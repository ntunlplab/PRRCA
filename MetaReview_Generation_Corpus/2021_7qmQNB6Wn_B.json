{"year": "2021", "forum": "7qmQNB6Wn_B", "title": "Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration", "decision": "Reject", "meta_review": "First, I'd like to thank both the authors and the reviewers for extensive and constructive discussion. The paper proposes a generalization of SAC, which considers the entropy of both the current policy and the action samples in the replay pool. The method is motivated by better sample complexity, as it avoids retaking actions that already appear in the pool. The paper formulates a theoretical algorithm and proves its convergence, as well as a practical algorithm that is compared to SAC and SAC-Div in continuous sparse-reward tasks.\n\nGenerally, the reviewers found the method interesting. After rounds of discussion and revisions, the reviewers identified two remaining issues. Theoretical analysis still requires improvement and the positioning of the paper is not clear. Particularly, the method is motivated as an exploration method, and it should be evaluated as such, for example, by comparing to a more representative set of baseline methods. Therefore, I'm recommending rejection, but encourage the authors to improve the work bases on the reviews, and submit to a future conference.", "reviews": [{"review_id": "7qmQNB6Wn_B-0", "review_text": "Summary This paper proposes a novel exploration method in off-policy learning . Compared to previous methods which do not take care into account the distribution of the samples in the replay buffer , the proposed method maximizes the entropy of the mixture of the policy distribution and the distribution of the samples in the replay buffer , hereby making exploration efficient . Reasons for score I vote for accepting the paper . The paper proposes an intuitive and efficient exploration method that generalizes existing methods , including them as special cases . The authors provide a theoretical guarantee ( Theorem 1 ) that the policy obtained from the iteration of evaluation and improvement under this new regime converges to the optimal policy . The presentation is clear and concrete , and the experiments are convincing . Pros The experiment results are not limited to just showing that the proposed method achieves higher reward than state of the art methods , but they also address important questions such as ( i ) the pure exploration when rewards are assumed to be 0 ( i ) the necessity of the adaptation of alpha , the parameter that controls the ratio of the current policy to the sample distribution in the target distribution . ( ii ) the effect of controlling alpha , the entropy weighting factor beta , and the control coefficient c ( required for adapting alpha ) , and also , the robustness of the proposed method to these parameters . The authors have stated the experiment details clearly and the results are convincing . Cons The methodology part in Section 3 and 4 could be improved . Some notations are confusing . ( a ) In Section 3 , the policy \\pi is defined as a function from S to A . It looks like it is a fixed function over time . ( b ) An explanation on the definition of J_ { pi 1 } ( pi 2 ) would be helpful , e.g. , J_ { pi 1 } ( pi 2 ) is value of J ( pi_2 ) computed under pi_1 . Minor Comments It would be good to add the line of SAC and SAC-Div in Figure 5 ( c ) to show that the performance of DAC with adaptive alpha is robust to control coefficient c. For now , one has to go back to Figure 4 ( b ) to check that most of the case ( when c is not 0 ) , DAC with adaptive alpha performs better than SAC and SAC-Div . In Section 6 in the 5th line , J ( \\pi ) should be specified as \u201c J ( \\pi ) in ( 1 ) \u201d . It is done in the next sentence , but I prefer that it is done when it first appears . It was confusing", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the positive feedback and for highlighting the strength of our proposed method . Confusing notation and figure will be fixed as soon as possible ."}, {"review_id": "7qmQNB6Wn_B-1", "review_text": "# # # Summary The paper proposes DAC , an actor-critic method exploiting the replay buffer to do policy entropy regularisation . The main idea of DAC is to use the data from the replay buffer to induce a distribution $ q ( \\cdot , s_t ) $ and replace the entropy part of the Soft Actor-Critic objective with a convex combination of $ q $ and $ \\pi $ . This results positively on exploration properties and leads to sample-efficiency gains on some of the considered MuJoCo benchmarks . # # # Pros - Formulating the diversity using the entropy of the replay buffer frequences is an interesting idea . - Using the convex combination of $ q $ and $ \\pi $ for entropy regularisation is a nice way of generalising SAC for the considered purpose . - The paper shows the convergence of their method to an optimal policy and derives a surrogate objective whose gradient direction coincides with the original one , but which can be practically used . ( However , I have not checked the proofs which are in the appendix ) . # # # Cons - It is not clear , what is the problem the paper tackles . Is it exploration ? Is it a generic RL setup ? What kind of problems is DAC good for ? - If DAC is for improving exploration , then it should be compared with other exploration methods , not with vanilla SAC . Comparison with RND should not be in the appendix and there should be more details on this . Related work in this case should have a paragraph on exploration methods in RL . - The paper is based on assumptions not challenged/tested by the authors , e.g.policy entropy regularisation is inefficient , because it does not take the distribution of the samples into account . - The paper focuses more on the technical details of the solution rather than justifying the assumptions and making the research question clear . # # # Reasoning behind the score I believe , the paper has a great potential . However , at the moment I vote for rejection . The paper has to have a clear research question and its motivation . This should define the experimental part of the work . Lack of a clear positioning makes it unclear if the baselines of the experimental sections are the right ones and whether the claims have been properly supported by the results . # # # Questions to the authors - Can you formulate the exact problem you are solving ? - How can you justify the claim that 'entropy regularization is sample inefficient in off-policy learning since it does not take the distribution of previous samples stored in the replay buffer into account . - `` it is preferable that the old sample distribution in the replay buffer is uniformly distributed '' . Why is it true ? Does n't prioritized experience replay refute this claim ? - You define $ \\beta $ in Equation 1 in $ ( 0 , \\infty ) $ , can it really be infinite ? - `` The rationale behind this is that it is preferable to have as diverse actions stored in the replay buffer as possible for better Q estimation in off-policy learning . '' What are the assumptions for this ? Do you care more about better Q estimates or finding an better policy faster ? How can you support your rationale ? - In section 4.1. you define the target distribution as a convex combination of $ \\pi $ and $ q $ . You assume that the buffer is generated by $ q $ . Does such a policy always exist ? What are the assumptions for this ? - You prove the convergence of your algorithm ( I did not check the proof in the appendix ) , what are the assumptions for which the convergence is guaranteed ? - Why do you use sparse/delayed MuJoCo benchmarks , but not the original ones ? - The variance across different seeds seems to be huge for your method ( as well as for the others ) . What do you think is the reason behind this ? This also happens for the pure exploration task in 6.1 , why do you think it happens ? - For the adaptive $ \\alpha $ case , you restrict the range of possible values , what is the reasoning behind the left boundary ? - I think your paper can find an important application in Imitation Learning or off-line RL . Have you considered this ? Are you aware of works which do something similar in those subfields ? # # # Additional feedback not affecting the score - `` Reinforcement learning aims to maximize the discounted sum of rewards ... ' . Should be 'expected discounted sum ' . - There should be a distribution over initial states under the expectation sign in 3.1 . - ' A is the continuous action space ' . This is not true for the general MDP definition , specify that this is specific for your paper . - Section 3.1 , a policy is a mapping from states to distribution over actions , not to actions . - In off-policy , we can learn from any other samples , not only from 'previous samples ' from our policy . - typo `` propoed '' at the bottom of page 4 . - Equation 9 does not have a left hand side . - DAC acronym has been used in RL . I would choose a different one to avoid confusion .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable comments and constructive feedbacks . We provide our feedback below : \u2022 It is not clear , what is the problem the paper tackles . ... What kind of problems is DAC good for ? \u2022 Can you formulate the exact problem you are solving ? \u2022 How can you justify the claim that 'entropy regularization is sample inefficient in off-policy learning since it does not take the distribution of previous samples stored in the replay buffer into account . \u2022 `` The rationale behind this is that it is preferable to have as diverse actions stored in the replay buffer as possible for better Q estimation in off-policy learning . '' What are the assumptions for this ? Do you care more about better Q estimates or finding an better policy faster ? How can you support your rationale ? In order to guarantee the convergence of Q-learning [ R3-1 ] , there is a key assumption : Each state-action pair must be visited infinitely often . If the policy does not visit diverse state-action pairs many times , it converges to local optima . Therefore , exploration for visiting different state-action pairs is important for RL , and the original entropy regularization encourages exploration [ R3-2 ] . The simple policy entropy maximization will choose all actions with equal probability without considering the previous action samples . In contrast , if we consider maximizing the entropy of the mixture of $ \\pi $ and $ q $ ( it becomes the future sample distribution since the buffer stores samples generated by $ \\pi $ ) , $ \\pi $ should choose actions rare in the buffer with high probability and actions stored many times in the buffer with low probability to make the mixture distribution uniform . Hence , it considers the samples already stored in the buffer in choosing current action and encourages sample-efficient exploration . We provide a simple example below : Let us consider a simple 1-step MDP in which $ s_0 $ is the unique initial state , there exist $ N $ actions ( $ a_0\\in ( \\ { 1 , \\cdots , N\\ } ) $ ) , $ s_1 $ is the terminal state , and $ r $ is the deterministic reward function . Then , there exist $ N $ state-action pairs in total and let us assume that we already have $ N-1 $ state-action samples in the replay buffer as $ R= ( \\ { ( s_0,1 , r ( s_0,1 ) ) , \\cdots , ( s_0 , N-1 , r ( s_0 , N-1 ) ) \\ } ) $ . In order to estimate the Q-function for all state-action pairs , the policy should sample Action $ N $ ( After then , we can reuse all samples infinitely to estimate Q ) . Here , we will compare two exploration methods . 1 ) First , if we consider the simple entropy maximization , the policy will choose all actions with equal probability $ 1/N $ ( uniformly ) since $ \\max_\\pi \\mathcal { H } ( \\pi ) =\\min_\\pi KLD ( \\pi||U ) $ is achieved when $ \\pi=U $ , where $ U $ is a uniform distribution . Then , $ N $ samples should be taken on average by the policy to visit Action $ N $ . 2 ) Consider the sample-aware entropy maximization as in our paper . Here , the sample action distribution $ q $ in the buffer is defined as $ q ( a_0|s_0 ) =1/ ( N-1 ) $ for $ a_0\\in\\ { 0 , \\cdots , N-1\\ } $ , and $ q ( N|s_0 ) =0 $ . Now , we set the target sample distribution as the mixture of $ \\pi $ and $ q $ , as $ q_ { target } ^ { \\pi , \\alpha } =\\alpha\\pi+ ( 1-\\alpha ) q $ , and we set $ \\alpha=1/N $ . Then , in order to maximize the entropy of the target sample distribution $ \\max_\\pi \\mathcal { H } ( q_ { target } ^ { \\pi , \\alpha } ) = \\min_\\pi KLD ( q_ { target } ^ { \\pi , \\alpha } ||U ) $ , the policy distribution should be $ \\pi ( N|s_0 ) =1 $ to make $ q_ { target } ^ { \\pi , \\alpha } $ uniform . Thus , it only needs one sample to visit Action $ N $ . In this way , the simple entropy regularization is sample-inefficient for off-policy RL , and the proposed sample-aware entropy regularization enhances the sample-efficiency for exploration by using the previous sample distribution and choosing proper $ \\alpha $ . With this motivation , we propose the sample-aware entropy regularization and the corresponding $ \\alpha $ -adaptation . Therefore , the method in this paper addresses sample-efficient exploration . \u2022 `` it is preferable that the old sample distribution in the replay buffer is uniformly distributed '' . Why is it true ? Does n't prioritized experience replay refute this claim ? The claim means that the policy should choose samples to make the sample distribution uniform over the overall state-action space to visit all state-action pairs for convergence of Q [ R3-1 ] as explained the first comment ( I think that the word \u201c old sample \u201d is misleading ) . Note that uniform sampling from the replay buffer does not yield a uniform distribution over the overall state-action space since the sample distribution is usually non-uniform . Hence , it can be bad for TD error minimization and prioritized experience replay gives priority for samples based on TD error . Hence , regardless of that , the policy should select samples as uniformly as possible over the overall state-action space so that it can be used for convergence of Q ."}, {"review_id": "7qmQNB6Wn_B-2", "review_text": "This paper considers the exploration efficiency issues in off-policy deep reinforcement learning ( DRL ) . The authors identify a sample efficiency limitation in the classical entropy regularization , which does not take into account the existing samples in the replay buffer . To avoid repeated sampling of previously seen scenarios/actions , the authors propose to replace the current policy in the entropy term with a mixture of the empirical policy estimation from the replay buffer and the current policy , and term this approach as sample-aware entropy regularization . The authors then propose a theoretical algorithm called sample-aware entropy regularized policy iteration , which is a generalization of the soft policy iteration ( SPI ) algorithm , and show that it converges assuming that the empirical policy estimation is fixed . A practical algorithm based on the sample-aware entropy regularized policy iteration , called Diversity Actor-Critic ( DAC ) , is then proposed . This algorithm is a generalization of the well-known soft actor-critic ( SAC ) algorithm . Finally , numerical experiments show that DAC outperforms SAC and other SOTA RL algorithms , and some ablation studies are also provided to demonstrate the effect of hyper-parameter choices in DAC . In general , the approach is novel to my knowledge and the high level idea of using mixed policies in the entropy regularization to avoid repeated sampling and encourage unseen scenarios/actions is also interesting and reasonable . However , there are some clarity and technical issues that should be addressed and improved , as listed below : 1 . The authors study finite horizon MDPs , for which the optimal policy should be non-stationary in general . However , the authors only consider stationary policies . Instead , the authors should either change the underlying setting to infinite horizon MDPs or consider non-stationary policies . 2.In ( 2 ) , $ s_t $ should be replaced by an arbitrary $ s $ in the state space . Otherwise there may be contradicting definitions of the policy $ q $ if $ s_t $ and $ s_ { t \u2019 } $ are equal for some two different timestamps $ t $ and $ t \u2019 $ . And in ( 3 ) , it is better to write the $ q_ { \\rm target } ^ { \\pi , \\alpha } $ in the entropy term as $ q_ { \\rm target } ^ { \\pi , \\alpha } ( \\cdot|s_t ) $ , to be consistent with ( 1 ) . 3.It \u2019 s not very clear why the authors propose to estimate $ R^ { \\pi , \\alpha } $ with some ( neural network ) parametrized $ R^ { \\alpha } $ . The authors mention that one can only estimate $ R^ { \\pi_ { \\rm old } , \\alpha } $ for the previous policy $ \\pi_ { \\rm old } $ in practice . However , since in $ R^ { \\pi , \\alpha } $ , all the quantities including $ \\pi $ , $ q $ and $ \\alpha $ are known , I \u2019 m confused why one can not evaluate it directly . On a related point , it \u2019 s not very clear why the estimation procedure for $ \\eta $ ( the parameter of $ R^ { \\alpha } $ ) using hat $ J_ { R^ { \\alpha } } ( \\eta ) $ makes sense . The form of hat $ J_ { R^ { \\alpha } } ( \\eta ) $ looks like an entropy term extracted from the $ J_ { \\pi_ { \\rm old } } $ function , but it \u2019 s unclear why maximizing it gives a good estimation of $ R^ { \\pi , \\alpha } $ . Some more explanations are needed . 4.There seem to be several errors ( at least inaccuracies ) in the proof of Theorem 1 ( in the Appendix ) . Firstly , in the proof of Lemma 1 , the term \u201c correctly estimates \u201d is not very accurate , and should be simply stated as something like \u201c equals \u201d . Also , it \u2019 s not very clear when the assumption $ R^ { \\alpha } \\in ( 0,1 ) $ can be guaranteed ( e.g. , using Gaussian/soft-max policies ? ) . Secondly , in the main proof of Theorem 1 , convergence of $ Q^ { \\pi_i } $ to some $ Q^ { \\star } $ is correct , but this does not immediately imply convergence of $ J_ { \\pi_i } $ , let alone the convergence of $ \\pi_i $ to some policy $ \\pi^\\star $ . On a related point , the proof for the optimality of $ \\pi^\\star $ in terms of $ J $ is not clear . In particular , it is not clear why ( 7 ) and Lemma 2 implies the chained inequality $ J_ { \\pi_ { \\rm new } } ( \\pi_ { \\rm new } ) \\geq J_ { \\pi_ { \\rm old } } ( \\pi_ { \\rm new } ) \\geq J_ { \\pi_ { \\rm old } } ( \\pi_ { \\rm old } ) $ . I understand that the authors may feel that the proofs are similar to that of SPI , but indeed there are several significant differences ( e.g. , the definitions of $ \\pi_ { \\rm new } $ and $ J_ { \\pi } $ ) . More rigorous proofs are needed for these claims . 5.In Section 5 , it is unclear why the authors need to include the parameter $ c $ , how to choose it and what it serves for . Some additional explanations are needed . 6.On a high level , the eventual goal of the paper is not clearly stated . From the experiments , it seems that the average episode reward is the actual goal of concern . However , the problem setting and the theoretical results ( Theorem 1 ) seem to indicate that the problem of concern is the discounted entropy regularized reward . Some discussion about this is needed . Finally , here are some more minor comments and suggestions : 1 . In the analysis of the sample-aware entropy regularized policy iteration , the authors assume that $ q $ is fixed . However , in practice , especially in the long run ( as concerned in the analysis ) , such an assumption will not hold ( even in just an approximate sense ) . Can you still obtain some sort of convergence when taking into account the $ q $ changes ? 2.Why do you need to divide the reward and entropy regularization term in $ Q^ { \\pi } $ by $ \\beta $ ? 3.It \u2019 s better to write out the \u201c binary entropy function $ H $ '' explicitly for clarity . 4.At the beginning of Section 4.3 , \u201c propoed \u201d should be \u201c proposed \u201d , and In Section 5 , \u201c a function $ s_t $ \u201d should be \u201c a function of $ s_t $ \u201d . 5.Some high level explanations on why the $ ( 1-\\alpha ) $ term can also be dropped in ( 8 ) will be helpful . 6.The theoretical results only show that the algorithm converges , which is already guaranteed by SPI . Is there any possibility to show that there is also some theoretical improvement ? So in short , the paper proposes an interesting modification of the max-entropy regularization framework , but contains several technical and clarity issues . Hence I think it is not yet ready for publication in its current form .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable comments and constructive feedbacks . We provide our feedback below : \u2022 The authors study finite horizon MDPs , ... change the underlying setting to infinite horizon MDPs or consider non-stationary policies . Our paper considers infinite horizon MDP as basic RL setup , and the proof of the diverse policy iteration also assumes an infinite horizon MDP . We will mention it in Section 3.1 as the reviewer says . Episode length $ T $ also should be $ \\infty $ but many paper use \u201c $ T $ \u201d though they consider infinite horizon MDP and we followed this convention . In the revised we change $ T $ to $ \\infty $ . \u2022 In ( 2 ) , $ s_t $ should be replaced by an arbitrary $ s $ in the state space . We will change the both confusing notations . \u2022 It \u2019 s not very clear why the authors propose to estimate $ R^ { \\pi , \\alpha } $ ... I \u2019 m confused why one can not evaluate it directly . We defined the sample distribution $ q $ in equation ( 2 ) . However , we do not actually compute $ q $ from the sample buffer by using a method such as discretization and counting for continuous samples , which is not simple . Even if $ q $ is obtained by counting , it is difficult to generalize such $ q $ to yield the probability density value for arbitrary actions for any given state . Please note that we have to generalize $ q $ as such because we need to compute $ q $ of any policy action for any given state . We circumvented this difficulty by defining the ratio function $ R^ { \\pi , \\alpha } $ . Please note that what we need is the entropy of the target distribution , as seen in equation ( 3 ) . We show that this mixture entropy can be decomposed as $ \\mathcal { H } ( \\pi ) + D^\\alpha_ { JS } ( \\pi||q ) + ( 1-\\alpha ) \\mathcal { H } ( q ) $ . Then , we express $ D^\\alpha_ { JS } ( \\pi||q ) $ in terms of $ R^ { \\pi , \\alpha } $ not $ q $ , and $ H ( q ) $ can be handled without computing $ q $ explicitly again by using the property of the ratio function as eq ( B.7 ) in Appendix . Hence , at least regarding the entropy of the target distribution , $ R^ { \\pi , \\alpha } $ is sufficient . Now , the objective function for policy update is given by equation ( 7 ) , and note that $ \\pi $ in $ R^ { \\pi , \\alpha } $ is the optimization variable . Since we do not compute $ q $ , we can express $ R^ { \\pi , \\alpha } $ in terms of $ \\pi $ explicitly for optimization . We circumvented this difficulty by showing that maximizing the original objective function ( 7 ) is equivalent to maximizing the alternative objective function equation ( 8 ) in which $ R^ { \\pi_ { old } , \\alpha } $ appears . And , $ R^ { \\pi_ { old } , \\alpha } $ is estimated by using a neural network $ R^\\alpha $ . Note that this neural network has generalization effect . \u2022 On a related point , it \u2019 s not very clear why the estimation procedure for $ \\eta $ ... Some more explanations are needed . Please note that $ J ( R^\\alpha ) $ is just an $ \\alpha $ -skewed JS divergence except some constant terms . In the $ \\alpha=0.5 $ case , it becomes the usual JS divergence , which is considered in GAN [ R1-1 ] . [ R1-1 ] has shown that the ratio function for $ \\alpha=0.5 $ can be estimated by maximizing the JS divergence . In a similar way to that in [ R1-1 ] , we can show that maximizing $ J ( R^\\alpha ) $ can estimate our ratio function as below : For given $ s $ , $ J ( R^\\alpha ( s , \\cdot ) ) = \\int_a \\alpha \\pi ( a|s ) \\log R^ { \\alpha } ( s , a ) + ( 1-\\alpha ) q ( a|s ) \\log ( 1-R^ { \\alpha } ( s , a ) ) da $ . The integrand is in the form of the function $ y\\rightarrow a \\log y + b\\log ( 1-y ) $ with $ a=\\alpha\\pi $ and $ b= ( 1-\\alpha ) q $ , and for any positive $ ( a , b ) $ , the function its maximum at $ a/ ( a+b ) $ . Thus , the optimal $ { R^ { \\alpha } } $ $ ^ * $ maximizing $ J ( R^\\alpha ( s , \\cdot ) ) $ is $ { R^ { \\alpha } } $ $ ^ * $ $ ( s , a ) = \\alpha\\pi / ( \\alpha\\pi+ ( 1-\\alpha ) q ) $ . \u2022 There seem to be several errors in Theorem 1 ... More rigorous proofs are needed for these claims . 1 ) The term \u201c correctly estimates \u201d will be changed as reviewer 1 said . 2 ) $ R^\\alpha \\in ( 0,1 ) $ is guaranteed when $ \\pi $ and $ q $ are non-zero for all state-action pairs . For practical implementation , we clipped the ratio function as $ ( \\epsilon,1-\\epsilon ) $ for small $ \\epsilon > 0 $ since some $ q $ values can be close to zero before the replay buffer stores a sufficient amount of samples . $ \\pi $ is always non-zero since we consider Gaussian policy . 3 ) Assume an arbitrary state $ s\\in\\mathcal { S } $ . From the policy update , $ J_ { \\pi_ { old } } ( \\pi_ { new } ) \\geq J_ { \\pi_ { old } } ( \\pi_ { old } $ as stated int the proof of Lemma 2.Next , from the eq . ( 7 ) , all terms are the same for $ J_ { \\pi_ { new } } ( \\pi_ { new } ( \\cdot|s ) ) $ and $ J_ { \\pi_ { old } } ( \\pi_ { new } ( \\cdot|s ) ) $ except $ \\beta E_ { a\\sim \\pi_ { new } ( \\cdot|s ) } [ Q^ { \\pi_ { new } } ( s , a ) ] $ in $ J_ { \\pi_ { new } } ( \\pi_ { new } ( \\cdot|s ) ) $ and $ \\beta E_ { a\\sim \\pi_ { new } ( \\cdot|s ) } [ Q^ { \\pi_ { old } } ( s , a ) ] $ in $ J_ { \\pi_ { old } } ( \\pi_ { new } ( \\cdot|s ) ) $ . Since $ Q^ { \\pi_ { new } } ( s , a ) \\geq Q^ { \\pi_ { old } } ( s , a ) $ for any $ ( s , a ) \\in \\mathcal { S } \\times\\mathcal { A } $ by Lemma 2 , $ J_ { \\pi_ { new } } ( \\pi_ { new } ( \\cdot|s ) ) \\geq J_ { \\pi_ { old } } ( \\pi_ { new } ( \\cdot|s ) ) $ . Thus , $ J_ { \\pi_ { new } } ( \\pi_ { new } ( \\cdot|s ) ) \\geq J_ { \\pi_ { old } } ( \\pi_ { new } ( \\cdot|s ) ) \\geq J_ { \\pi_ { old } } ( \\pi_ { old } ( \\cdot|s ) ) $ for any state $ s\\in\\mathcal { S } $ ."}, {"review_id": "7qmQNB6Wn_B-3", "review_text": "This paper proposes diversity actor-critic ( DAC ) for exploration in reinforcement learning . The main idea of the proposed algorithm is to take advantage of the previous sample distribution from the replay buffer for sample-efficient exploration . The authors provide convergence analysis of DAC and conduct empirical investigations on several benchmarks . Pros The idea of using previous sample distribution from the replay buffer for better exploration seems interesting . The proposed exploration bonus $ \\mathcal { H } ( q^ { \\pi , \\alpha } _ { \\text { target } } ) $ can be decomposed into three terms as shown in ( 4 ) . Since the last term does not depend on $ \\pi $ , intuitively this exploration bonus encourages the exploration of $ \\pi $ ( first term ) , and tries to make $ \\pi $ different with previous policies approximated by the replay buffer ( second term ) . The authors provide a reasonable method to optimized the proposed objective , which can be naturally combined with state-of-the-art algorithms like SAC . Cons 1.Theorem 1 seems misleading . The diverse policy iteration can only guarantee the converge to the optimal policy with respect to the regularized value function , not the optimal policy of the original problem . The authors should make the definition of $ \\pi^ * $ clear . 2.It \u2019 s hard to see the motivation of using a mixture of $ q $ and $ \\pi $ . Could you explain more about this choice ? 3.It \u2019 s worth to provide the results of SAC-div with JS divergence as it \u2019 s more similar to the proposed objective ( 4 ) . 4.The experiment results are not convincing enough as some important baselines are missing . For example , [ 1 ] also uses a mixture of previous polices to encourage exploration with strong theoretical guarantees . I believe this is closely related to the proposed algorithms . Also , the experiment results are not very promising compared with the baseline algorithms based on SAC . [ 1 ] Hazan , E. , Kakade , S. , Singh , K. and Van Soest , A. , 2019 , May . Provably efficient maximum entropy exploration . In International Conference on Machine Learning ( pp.2681-2691 ) . Other suggestions The main idea of the proposed method is to make the current policy different with previous policies . The paper uses a nonparametric method ( 2 ) to approximate the previous policies . I think it \u2019 s also worth to try parametric $ q $ . For example , $ q $ could be learned by fitting the replay buffer , or use a moving average of previous policies .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable comments and constructive feedbacks . We provide our feedback below : \u2022 Theorem 1 seems misleading . The diverse policy iteration can only guarantee the converge to the optimal policy with respect to the regularized value function , not the optimal policy of the original problem . The authors should make the definition of $ \\pi^ * $ clear . We define the objective function as the sample-aware entropy regularized return as ( 3 ) ( not just return ) , so the optimal policy $ \\pi^ * $ in this paper maximizes ( 3 ) as explained Theorem 1 . Please note that the optimal policy of SAC is also defined as the policy that maximizes the entropy regularized return ( 1 ) [ R2-1 ] . In order to guarantee convergence to optimal policy for return maximization , we have to shrink the entropy coefficient $ \\beta $ to $ 0 $ . \u2022 It \u2019 s hard to see the motivation of using a mixture of q and \u03c0 . Could you explain more about this choice ? The detailed motivation of using the mixture distribution is as follows . In order to guarantee the convergence of Q-learning [ R2-2 ] , there is a key assumption : Each state-action pair must be visited infinitely often . If the policy does not visit diverse state-action pairs many times , it converges to local optima . Therefore , exploration that promotes visiting different state-action pairs is important for RL , so it is better to draw new samples while avoiding samples that have been drawn many times before . The simple policy entropy maximization will choose all actions with equal probability without considering the previous action samples . In contrast , if we consider maximizing the entropy of the mixture of $ \\pi $ and $ q $ ( it becomes the future sample distribution since the buffer stores samples generated by $ \\pi $ ) , $ \\pi $ should choose actions rare in the buffer with high probability and actions stored many times in the buffer with low probability to make the mixture distribution uniform . Hence , it considers the samples already stored in the buffer in choosing current action and encourages sample-efficient exploration . We provide a simple example below : Let us consider a simple 1-step MDP in which $ s_0 $ is the unique initial state , there exist $ N $ actions ( $ a_0\\in ( \\ { 1 , \\cdots , N\\ } ) $ ) , $ s_1 $ is the terminal state , and $ r $ is the deterministic reward function . Then , there exist $ N $ state-action pairs in total and let us assume that we already have $ N-1 $ state-action samples in the replay buffer as $ R= ( \\ { ( s_0,1 , r ( s_0,1 ) ) , \\cdots , ( s_0 , N-1 , r ( s_0 , N-1 ) ) \\ } ) $ . In order to estimate the Q-function for all state-action pairs , the policy should sample Action $ N $ ( After then , we can reuse all samples infinitely to estimate Q ) . Here , we will compare two exploration methods . 1 ) First , if we consider the simple entropy maximization , the policy will choose all actions with equal probability $ 1/N $ ( uniformly ) since $ \\max_\\pi \\mathcal { H } ( \\pi ) =\\min_\\pi KLD ( \\pi||U ) $ is achieved when $ \\pi=U $ , where $ U $ is a uniform distribution . Then , $ N $ samples should be taken on average by the policy to visit Action $ N $ . 2 ) Consider the sample-aware entropy maximization as in our paper . Here , the sample action distribution $ q $ in the buffer is defined as $ q ( a_0|s_0 ) =1/ ( N-1 ) $ for $ a_0\\in\\ { 0 , \\cdots , N-1\\ } $ , and $ q ( N|s_0 ) =0 $ . Now , we set the target sample distribution as the mixture of $ \\pi $ and $ q $ , as $ q_ { target } ^ { \\pi , \\alpha } =\\alpha\\pi+ ( 1-\\alpha ) q $ , and we set $ \\alpha=1/N $ . Then , in order to maximize the entropy of the target sample distribution $ \\max_\\pi \\mathcal { H } ( q_ { target } ^ { \\pi , \\alpha } ) = \\min_\\pi KLD ( q_ { target } ^ { \\pi , \\alpha } ||U ) $ , the policy distribution should be $ \\pi ( N|s_0 ) =1 $ to make $ q_ { target } ^ { \\pi , \\alpha } $ uniform . Thus , it only needs one sample to visit Action $ N $ . In this way , the simple entropy regularization is sample-inefficient for off-policy RL , and the proposed sample-aware entropy regularization enhances the sample-efficiency for exploration by using the previous sample distribution and choosing proper $ \\alpha $ . With this motivation , we propose the sample-aware entropy regularization and the corresponding $ \\alpha $ -adaptation . Therefore , the method in this paper addresses sample-efficient exploration ."}], "0": {"review_id": "7qmQNB6Wn_B-0", "review_text": "Summary This paper proposes a novel exploration method in off-policy learning . Compared to previous methods which do not take care into account the distribution of the samples in the replay buffer , the proposed method maximizes the entropy of the mixture of the policy distribution and the distribution of the samples in the replay buffer , hereby making exploration efficient . Reasons for score I vote for accepting the paper . The paper proposes an intuitive and efficient exploration method that generalizes existing methods , including them as special cases . The authors provide a theoretical guarantee ( Theorem 1 ) that the policy obtained from the iteration of evaluation and improvement under this new regime converges to the optimal policy . The presentation is clear and concrete , and the experiments are convincing . Pros The experiment results are not limited to just showing that the proposed method achieves higher reward than state of the art methods , but they also address important questions such as ( i ) the pure exploration when rewards are assumed to be 0 ( i ) the necessity of the adaptation of alpha , the parameter that controls the ratio of the current policy to the sample distribution in the target distribution . ( ii ) the effect of controlling alpha , the entropy weighting factor beta , and the control coefficient c ( required for adapting alpha ) , and also , the robustness of the proposed method to these parameters . The authors have stated the experiment details clearly and the results are convincing . Cons The methodology part in Section 3 and 4 could be improved . Some notations are confusing . ( a ) In Section 3 , the policy \\pi is defined as a function from S to A . It looks like it is a fixed function over time . ( b ) An explanation on the definition of J_ { pi 1 } ( pi 2 ) would be helpful , e.g. , J_ { pi 1 } ( pi 2 ) is value of J ( pi_2 ) computed under pi_1 . Minor Comments It would be good to add the line of SAC and SAC-Div in Figure 5 ( c ) to show that the performance of DAC with adaptive alpha is robust to control coefficient c. For now , one has to go back to Figure 4 ( b ) to check that most of the case ( when c is not 0 ) , DAC with adaptive alpha performs better than SAC and SAC-Div . In Section 6 in the 5th line , J ( \\pi ) should be specified as \u201c J ( \\pi ) in ( 1 ) \u201d . It is done in the next sentence , but I prefer that it is done when it first appears . It was confusing", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the positive feedback and for highlighting the strength of our proposed method . Confusing notation and figure will be fixed as soon as possible ."}, "1": {"review_id": "7qmQNB6Wn_B-1", "review_text": "# # # Summary The paper proposes DAC , an actor-critic method exploiting the replay buffer to do policy entropy regularisation . The main idea of DAC is to use the data from the replay buffer to induce a distribution $ q ( \\cdot , s_t ) $ and replace the entropy part of the Soft Actor-Critic objective with a convex combination of $ q $ and $ \\pi $ . This results positively on exploration properties and leads to sample-efficiency gains on some of the considered MuJoCo benchmarks . # # # Pros - Formulating the diversity using the entropy of the replay buffer frequences is an interesting idea . - Using the convex combination of $ q $ and $ \\pi $ for entropy regularisation is a nice way of generalising SAC for the considered purpose . - The paper shows the convergence of their method to an optimal policy and derives a surrogate objective whose gradient direction coincides with the original one , but which can be practically used . ( However , I have not checked the proofs which are in the appendix ) . # # # Cons - It is not clear , what is the problem the paper tackles . Is it exploration ? Is it a generic RL setup ? What kind of problems is DAC good for ? - If DAC is for improving exploration , then it should be compared with other exploration methods , not with vanilla SAC . Comparison with RND should not be in the appendix and there should be more details on this . Related work in this case should have a paragraph on exploration methods in RL . - The paper is based on assumptions not challenged/tested by the authors , e.g.policy entropy regularisation is inefficient , because it does not take the distribution of the samples into account . - The paper focuses more on the technical details of the solution rather than justifying the assumptions and making the research question clear . # # # Reasoning behind the score I believe , the paper has a great potential . However , at the moment I vote for rejection . The paper has to have a clear research question and its motivation . This should define the experimental part of the work . Lack of a clear positioning makes it unclear if the baselines of the experimental sections are the right ones and whether the claims have been properly supported by the results . # # # Questions to the authors - Can you formulate the exact problem you are solving ? - How can you justify the claim that 'entropy regularization is sample inefficient in off-policy learning since it does not take the distribution of previous samples stored in the replay buffer into account . - `` it is preferable that the old sample distribution in the replay buffer is uniformly distributed '' . Why is it true ? Does n't prioritized experience replay refute this claim ? - You define $ \\beta $ in Equation 1 in $ ( 0 , \\infty ) $ , can it really be infinite ? - `` The rationale behind this is that it is preferable to have as diverse actions stored in the replay buffer as possible for better Q estimation in off-policy learning . '' What are the assumptions for this ? Do you care more about better Q estimates or finding an better policy faster ? How can you support your rationale ? - In section 4.1. you define the target distribution as a convex combination of $ \\pi $ and $ q $ . You assume that the buffer is generated by $ q $ . Does such a policy always exist ? What are the assumptions for this ? - You prove the convergence of your algorithm ( I did not check the proof in the appendix ) , what are the assumptions for which the convergence is guaranteed ? - Why do you use sparse/delayed MuJoCo benchmarks , but not the original ones ? - The variance across different seeds seems to be huge for your method ( as well as for the others ) . What do you think is the reason behind this ? This also happens for the pure exploration task in 6.1 , why do you think it happens ? - For the adaptive $ \\alpha $ case , you restrict the range of possible values , what is the reasoning behind the left boundary ? - I think your paper can find an important application in Imitation Learning or off-line RL . Have you considered this ? Are you aware of works which do something similar in those subfields ? # # # Additional feedback not affecting the score - `` Reinforcement learning aims to maximize the discounted sum of rewards ... ' . Should be 'expected discounted sum ' . - There should be a distribution over initial states under the expectation sign in 3.1 . - ' A is the continuous action space ' . This is not true for the general MDP definition , specify that this is specific for your paper . - Section 3.1 , a policy is a mapping from states to distribution over actions , not to actions . - In off-policy , we can learn from any other samples , not only from 'previous samples ' from our policy . - typo `` propoed '' at the bottom of page 4 . - Equation 9 does not have a left hand side . - DAC acronym has been used in RL . I would choose a different one to avoid confusion .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable comments and constructive feedbacks . We provide our feedback below : \u2022 It is not clear , what is the problem the paper tackles . ... What kind of problems is DAC good for ? \u2022 Can you formulate the exact problem you are solving ? \u2022 How can you justify the claim that 'entropy regularization is sample inefficient in off-policy learning since it does not take the distribution of previous samples stored in the replay buffer into account . \u2022 `` The rationale behind this is that it is preferable to have as diverse actions stored in the replay buffer as possible for better Q estimation in off-policy learning . '' What are the assumptions for this ? Do you care more about better Q estimates or finding an better policy faster ? How can you support your rationale ? In order to guarantee the convergence of Q-learning [ R3-1 ] , there is a key assumption : Each state-action pair must be visited infinitely often . If the policy does not visit diverse state-action pairs many times , it converges to local optima . Therefore , exploration for visiting different state-action pairs is important for RL , and the original entropy regularization encourages exploration [ R3-2 ] . The simple policy entropy maximization will choose all actions with equal probability without considering the previous action samples . In contrast , if we consider maximizing the entropy of the mixture of $ \\pi $ and $ q $ ( it becomes the future sample distribution since the buffer stores samples generated by $ \\pi $ ) , $ \\pi $ should choose actions rare in the buffer with high probability and actions stored many times in the buffer with low probability to make the mixture distribution uniform . Hence , it considers the samples already stored in the buffer in choosing current action and encourages sample-efficient exploration . We provide a simple example below : Let us consider a simple 1-step MDP in which $ s_0 $ is the unique initial state , there exist $ N $ actions ( $ a_0\\in ( \\ { 1 , \\cdots , N\\ } ) $ ) , $ s_1 $ is the terminal state , and $ r $ is the deterministic reward function . Then , there exist $ N $ state-action pairs in total and let us assume that we already have $ N-1 $ state-action samples in the replay buffer as $ R= ( \\ { ( s_0,1 , r ( s_0,1 ) ) , \\cdots , ( s_0 , N-1 , r ( s_0 , N-1 ) ) \\ } ) $ . In order to estimate the Q-function for all state-action pairs , the policy should sample Action $ N $ ( After then , we can reuse all samples infinitely to estimate Q ) . Here , we will compare two exploration methods . 1 ) First , if we consider the simple entropy maximization , the policy will choose all actions with equal probability $ 1/N $ ( uniformly ) since $ \\max_\\pi \\mathcal { H } ( \\pi ) =\\min_\\pi KLD ( \\pi||U ) $ is achieved when $ \\pi=U $ , where $ U $ is a uniform distribution . Then , $ N $ samples should be taken on average by the policy to visit Action $ N $ . 2 ) Consider the sample-aware entropy maximization as in our paper . Here , the sample action distribution $ q $ in the buffer is defined as $ q ( a_0|s_0 ) =1/ ( N-1 ) $ for $ a_0\\in\\ { 0 , \\cdots , N-1\\ } $ , and $ q ( N|s_0 ) =0 $ . Now , we set the target sample distribution as the mixture of $ \\pi $ and $ q $ , as $ q_ { target } ^ { \\pi , \\alpha } =\\alpha\\pi+ ( 1-\\alpha ) q $ , and we set $ \\alpha=1/N $ . Then , in order to maximize the entropy of the target sample distribution $ \\max_\\pi \\mathcal { H } ( q_ { target } ^ { \\pi , \\alpha } ) = \\min_\\pi KLD ( q_ { target } ^ { \\pi , \\alpha } ||U ) $ , the policy distribution should be $ \\pi ( N|s_0 ) =1 $ to make $ q_ { target } ^ { \\pi , \\alpha } $ uniform . Thus , it only needs one sample to visit Action $ N $ . In this way , the simple entropy regularization is sample-inefficient for off-policy RL , and the proposed sample-aware entropy regularization enhances the sample-efficiency for exploration by using the previous sample distribution and choosing proper $ \\alpha $ . With this motivation , we propose the sample-aware entropy regularization and the corresponding $ \\alpha $ -adaptation . Therefore , the method in this paper addresses sample-efficient exploration . \u2022 `` it is preferable that the old sample distribution in the replay buffer is uniformly distributed '' . Why is it true ? Does n't prioritized experience replay refute this claim ? The claim means that the policy should choose samples to make the sample distribution uniform over the overall state-action space to visit all state-action pairs for convergence of Q [ R3-1 ] as explained the first comment ( I think that the word \u201c old sample \u201d is misleading ) . Note that uniform sampling from the replay buffer does not yield a uniform distribution over the overall state-action space since the sample distribution is usually non-uniform . Hence , it can be bad for TD error minimization and prioritized experience replay gives priority for samples based on TD error . Hence , regardless of that , the policy should select samples as uniformly as possible over the overall state-action space so that it can be used for convergence of Q ."}, "2": {"review_id": "7qmQNB6Wn_B-2", "review_text": "This paper considers the exploration efficiency issues in off-policy deep reinforcement learning ( DRL ) . The authors identify a sample efficiency limitation in the classical entropy regularization , which does not take into account the existing samples in the replay buffer . To avoid repeated sampling of previously seen scenarios/actions , the authors propose to replace the current policy in the entropy term with a mixture of the empirical policy estimation from the replay buffer and the current policy , and term this approach as sample-aware entropy regularization . The authors then propose a theoretical algorithm called sample-aware entropy regularized policy iteration , which is a generalization of the soft policy iteration ( SPI ) algorithm , and show that it converges assuming that the empirical policy estimation is fixed . A practical algorithm based on the sample-aware entropy regularized policy iteration , called Diversity Actor-Critic ( DAC ) , is then proposed . This algorithm is a generalization of the well-known soft actor-critic ( SAC ) algorithm . Finally , numerical experiments show that DAC outperforms SAC and other SOTA RL algorithms , and some ablation studies are also provided to demonstrate the effect of hyper-parameter choices in DAC . In general , the approach is novel to my knowledge and the high level idea of using mixed policies in the entropy regularization to avoid repeated sampling and encourage unseen scenarios/actions is also interesting and reasonable . However , there are some clarity and technical issues that should be addressed and improved , as listed below : 1 . The authors study finite horizon MDPs , for which the optimal policy should be non-stationary in general . However , the authors only consider stationary policies . Instead , the authors should either change the underlying setting to infinite horizon MDPs or consider non-stationary policies . 2.In ( 2 ) , $ s_t $ should be replaced by an arbitrary $ s $ in the state space . Otherwise there may be contradicting definitions of the policy $ q $ if $ s_t $ and $ s_ { t \u2019 } $ are equal for some two different timestamps $ t $ and $ t \u2019 $ . And in ( 3 ) , it is better to write the $ q_ { \\rm target } ^ { \\pi , \\alpha } $ in the entropy term as $ q_ { \\rm target } ^ { \\pi , \\alpha } ( \\cdot|s_t ) $ , to be consistent with ( 1 ) . 3.It \u2019 s not very clear why the authors propose to estimate $ R^ { \\pi , \\alpha } $ with some ( neural network ) parametrized $ R^ { \\alpha } $ . The authors mention that one can only estimate $ R^ { \\pi_ { \\rm old } , \\alpha } $ for the previous policy $ \\pi_ { \\rm old } $ in practice . However , since in $ R^ { \\pi , \\alpha } $ , all the quantities including $ \\pi $ , $ q $ and $ \\alpha $ are known , I \u2019 m confused why one can not evaluate it directly . On a related point , it \u2019 s not very clear why the estimation procedure for $ \\eta $ ( the parameter of $ R^ { \\alpha } $ ) using hat $ J_ { R^ { \\alpha } } ( \\eta ) $ makes sense . The form of hat $ J_ { R^ { \\alpha } } ( \\eta ) $ looks like an entropy term extracted from the $ J_ { \\pi_ { \\rm old } } $ function , but it \u2019 s unclear why maximizing it gives a good estimation of $ R^ { \\pi , \\alpha } $ . Some more explanations are needed . 4.There seem to be several errors ( at least inaccuracies ) in the proof of Theorem 1 ( in the Appendix ) . Firstly , in the proof of Lemma 1 , the term \u201c correctly estimates \u201d is not very accurate , and should be simply stated as something like \u201c equals \u201d . Also , it \u2019 s not very clear when the assumption $ R^ { \\alpha } \\in ( 0,1 ) $ can be guaranteed ( e.g. , using Gaussian/soft-max policies ? ) . Secondly , in the main proof of Theorem 1 , convergence of $ Q^ { \\pi_i } $ to some $ Q^ { \\star } $ is correct , but this does not immediately imply convergence of $ J_ { \\pi_i } $ , let alone the convergence of $ \\pi_i $ to some policy $ \\pi^\\star $ . On a related point , the proof for the optimality of $ \\pi^\\star $ in terms of $ J $ is not clear . In particular , it is not clear why ( 7 ) and Lemma 2 implies the chained inequality $ J_ { \\pi_ { \\rm new } } ( \\pi_ { \\rm new } ) \\geq J_ { \\pi_ { \\rm old } } ( \\pi_ { \\rm new } ) \\geq J_ { \\pi_ { \\rm old } } ( \\pi_ { \\rm old } ) $ . I understand that the authors may feel that the proofs are similar to that of SPI , but indeed there are several significant differences ( e.g. , the definitions of $ \\pi_ { \\rm new } $ and $ J_ { \\pi } $ ) . More rigorous proofs are needed for these claims . 5.In Section 5 , it is unclear why the authors need to include the parameter $ c $ , how to choose it and what it serves for . Some additional explanations are needed . 6.On a high level , the eventual goal of the paper is not clearly stated . From the experiments , it seems that the average episode reward is the actual goal of concern . However , the problem setting and the theoretical results ( Theorem 1 ) seem to indicate that the problem of concern is the discounted entropy regularized reward . Some discussion about this is needed . Finally , here are some more minor comments and suggestions : 1 . In the analysis of the sample-aware entropy regularized policy iteration , the authors assume that $ q $ is fixed . However , in practice , especially in the long run ( as concerned in the analysis ) , such an assumption will not hold ( even in just an approximate sense ) . Can you still obtain some sort of convergence when taking into account the $ q $ changes ? 2.Why do you need to divide the reward and entropy regularization term in $ Q^ { \\pi } $ by $ \\beta $ ? 3.It \u2019 s better to write out the \u201c binary entropy function $ H $ '' explicitly for clarity . 4.At the beginning of Section 4.3 , \u201c propoed \u201d should be \u201c proposed \u201d , and In Section 5 , \u201c a function $ s_t $ \u201d should be \u201c a function of $ s_t $ \u201d . 5.Some high level explanations on why the $ ( 1-\\alpha ) $ term can also be dropped in ( 8 ) will be helpful . 6.The theoretical results only show that the algorithm converges , which is already guaranteed by SPI . Is there any possibility to show that there is also some theoretical improvement ? So in short , the paper proposes an interesting modification of the max-entropy regularization framework , but contains several technical and clarity issues . Hence I think it is not yet ready for publication in its current form .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable comments and constructive feedbacks . We provide our feedback below : \u2022 The authors study finite horizon MDPs , ... change the underlying setting to infinite horizon MDPs or consider non-stationary policies . Our paper considers infinite horizon MDP as basic RL setup , and the proof of the diverse policy iteration also assumes an infinite horizon MDP . We will mention it in Section 3.1 as the reviewer says . Episode length $ T $ also should be $ \\infty $ but many paper use \u201c $ T $ \u201d though they consider infinite horizon MDP and we followed this convention . In the revised we change $ T $ to $ \\infty $ . \u2022 In ( 2 ) , $ s_t $ should be replaced by an arbitrary $ s $ in the state space . We will change the both confusing notations . \u2022 It \u2019 s not very clear why the authors propose to estimate $ R^ { \\pi , \\alpha } $ ... I \u2019 m confused why one can not evaluate it directly . We defined the sample distribution $ q $ in equation ( 2 ) . However , we do not actually compute $ q $ from the sample buffer by using a method such as discretization and counting for continuous samples , which is not simple . Even if $ q $ is obtained by counting , it is difficult to generalize such $ q $ to yield the probability density value for arbitrary actions for any given state . Please note that we have to generalize $ q $ as such because we need to compute $ q $ of any policy action for any given state . We circumvented this difficulty by defining the ratio function $ R^ { \\pi , \\alpha } $ . Please note that what we need is the entropy of the target distribution , as seen in equation ( 3 ) . We show that this mixture entropy can be decomposed as $ \\mathcal { H } ( \\pi ) + D^\\alpha_ { JS } ( \\pi||q ) + ( 1-\\alpha ) \\mathcal { H } ( q ) $ . Then , we express $ D^\\alpha_ { JS } ( \\pi||q ) $ in terms of $ R^ { \\pi , \\alpha } $ not $ q $ , and $ H ( q ) $ can be handled without computing $ q $ explicitly again by using the property of the ratio function as eq ( B.7 ) in Appendix . Hence , at least regarding the entropy of the target distribution , $ R^ { \\pi , \\alpha } $ is sufficient . Now , the objective function for policy update is given by equation ( 7 ) , and note that $ \\pi $ in $ R^ { \\pi , \\alpha } $ is the optimization variable . Since we do not compute $ q $ , we can express $ R^ { \\pi , \\alpha } $ in terms of $ \\pi $ explicitly for optimization . We circumvented this difficulty by showing that maximizing the original objective function ( 7 ) is equivalent to maximizing the alternative objective function equation ( 8 ) in which $ R^ { \\pi_ { old } , \\alpha } $ appears . And , $ R^ { \\pi_ { old } , \\alpha } $ is estimated by using a neural network $ R^\\alpha $ . Note that this neural network has generalization effect . \u2022 On a related point , it \u2019 s not very clear why the estimation procedure for $ \\eta $ ... Some more explanations are needed . Please note that $ J ( R^\\alpha ) $ is just an $ \\alpha $ -skewed JS divergence except some constant terms . In the $ \\alpha=0.5 $ case , it becomes the usual JS divergence , which is considered in GAN [ R1-1 ] . [ R1-1 ] has shown that the ratio function for $ \\alpha=0.5 $ can be estimated by maximizing the JS divergence . In a similar way to that in [ R1-1 ] , we can show that maximizing $ J ( R^\\alpha ) $ can estimate our ratio function as below : For given $ s $ , $ J ( R^\\alpha ( s , \\cdot ) ) = \\int_a \\alpha \\pi ( a|s ) \\log R^ { \\alpha } ( s , a ) + ( 1-\\alpha ) q ( a|s ) \\log ( 1-R^ { \\alpha } ( s , a ) ) da $ . The integrand is in the form of the function $ y\\rightarrow a \\log y + b\\log ( 1-y ) $ with $ a=\\alpha\\pi $ and $ b= ( 1-\\alpha ) q $ , and for any positive $ ( a , b ) $ , the function its maximum at $ a/ ( a+b ) $ . Thus , the optimal $ { R^ { \\alpha } } $ $ ^ * $ maximizing $ J ( R^\\alpha ( s , \\cdot ) ) $ is $ { R^ { \\alpha } } $ $ ^ * $ $ ( s , a ) = \\alpha\\pi / ( \\alpha\\pi+ ( 1-\\alpha ) q ) $ . \u2022 There seem to be several errors in Theorem 1 ... More rigorous proofs are needed for these claims . 1 ) The term \u201c correctly estimates \u201d will be changed as reviewer 1 said . 2 ) $ R^\\alpha \\in ( 0,1 ) $ is guaranteed when $ \\pi $ and $ q $ are non-zero for all state-action pairs . For practical implementation , we clipped the ratio function as $ ( \\epsilon,1-\\epsilon ) $ for small $ \\epsilon > 0 $ since some $ q $ values can be close to zero before the replay buffer stores a sufficient amount of samples . $ \\pi $ is always non-zero since we consider Gaussian policy . 3 ) Assume an arbitrary state $ s\\in\\mathcal { S } $ . From the policy update , $ J_ { \\pi_ { old } } ( \\pi_ { new } ) \\geq J_ { \\pi_ { old } } ( \\pi_ { old } $ as stated int the proof of Lemma 2.Next , from the eq . ( 7 ) , all terms are the same for $ J_ { \\pi_ { new } } ( \\pi_ { new } ( \\cdot|s ) ) $ and $ J_ { \\pi_ { old } } ( \\pi_ { new } ( \\cdot|s ) ) $ except $ \\beta E_ { a\\sim \\pi_ { new } ( \\cdot|s ) } [ Q^ { \\pi_ { new } } ( s , a ) ] $ in $ J_ { \\pi_ { new } } ( \\pi_ { new } ( \\cdot|s ) ) $ and $ \\beta E_ { a\\sim \\pi_ { new } ( \\cdot|s ) } [ Q^ { \\pi_ { old } } ( s , a ) ] $ in $ J_ { \\pi_ { old } } ( \\pi_ { new } ( \\cdot|s ) ) $ . Since $ Q^ { \\pi_ { new } } ( s , a ) \\geq Q^ { \\pi_ { old } } ( s , a ) $ for any $ ( s , a ) \\in \\mathcal { S } \\times\\mathcal { A } $ by Lemma 2 , $ J_ { \\pi_ { new } } ( \\pi_ { new } ( \\cdot|s ) ) \\geq J_ { \\pi_ { old } } ( \\pi_ { new } ( \\cdot|s ) ) $ . Thus , $ J_ { \\pi_ { new } } ( \\pi_ { new } ( \\cdot|s ) ) \\geq J_ { \\pi_ { old } } ( \\pi_ { new } ( \\cdot|s ) ) \\geq J_ { \\pi_ { old } } ( \\pi_ { old } ( \\cdot|s ) ) $ for any state $ s\\in\\mathcal { S } $ ."}, "3": {"review_id": "7qmQNB6Wn_B-3", "review_text": "This paper proposes diversity actor-critic ( DAC ) for exploration in reinforcement learning . The main idea of the proposed algorithm is to take advantage of the previous sample distribution from the replay buffer for sample-efficient exploration . The authors provide convergence analysis of DAC and conduct empirical investigations on several benchmarks . Pros The idea of using previous sample distribution from the replay buffer for better exploration seems interesting . The proposed exploration bonus $ \\mathcal { H } ( q^ { \\pi , \\alpha } _ { \\text { target } } ) $ can be decomposed into three terms as shown in ( 4 ) . Since the last term does not depend on $ \\pi $ , intuitively this exploration bonus encourages the exploration of $ \\pi $ ( first term ) , and tries to make $ \\pi $ different with previous policies approximated by the replay buffer ( second term ) . The authors provide a reasonable method to optimized the proposed objective , which can be naturally combined with state-of-the-art algorithms like SAC . Cons 1.Theorem 1 seems misleading . The diverse policy iteration can only guarantee the converge to the optimal policy with respect to the regularized value function , not the optimal policy of the original problem . The authors should make the definition of $ \\pi^ * $ clear . 2.It \u2019 s hard to see the motivation of using a mixture of $ q $ and $ \\pi $ . Could you explain more about this choice ? 3.It \u2019 s worth to provide the results of SAC-div with JS divergence as it \u2019 s more similar to the proposed objective ( 4 ) . 4.The experiment results are not convincing enough as some important baselines are missing . For example , [ 1 ] also uses a mixture of previous polices to encourage exploration with strong theoretical guarantees . I believe this is closely related to the proposed algorithms . Also , the experiment results are not very promising compared with the baseline algorithms based on SAC . [ 1 ] Hazan , E. , Kakade , S. , Singh , K. and Van Soest , A. , 2019 , May . Provably efficient maximum entropy exploration . In International Conference on Machine Learning ( pp.2681-2691 ) . Other suggestions The main idea of the proposed method is to make the current policy different with previous policies . The paper uses a nonparametric method ( 2 ) to approximate the previous policies . I think it \u2019 s also worth to try parametric $ q $ . For example , $ q $ could be learned by fitting the replay buffer , or use a moving average of previous policies .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable comments and constructive feedbacks . We provide our feedback below : \u2022 Theorem 1 seems misleading . The diverse policy iteration can only guarantee the converge to the optimal policy with respect to the regularized value function , not the optimal policy of the original problem . The authors should make the definition of $ \\pi^ * $ clear . We define the objective function as the sample-aware entropy regularized return as ( 3 ) ( not just return ) , so the optimal policy $ \\pi^ * $ in this paper maximizes ( 3 ) as explained Theorem 1 . Please note that the optimal policy of SAC is also defined as the policy that maximizes the entropy regularized return ( 1 ) [ R2-1 ] . In order to guarantee convergence to optimal policy for return maximization , we have to shrink the entropy coefficient $ \\beta $ to $ 0 $ . \u2022 It \u2019 s hard to see the motivation of using a mixture of q and \u03c0 . Could you explain more about this choice ? The detailed motivation of using the mixture distribution is as follows . In order to guarantee the convergence of Q-learning [ R2-2 ] , there is a key assumption : Each state-action pair must be visited infinitely often . If the policy does not visit diverse state-action pairs many times , it converges to local optima . Therefore , exploration that promotes visiting different state-action pairs is important for RL , so it is better to draw new samples while avoiding samples that have been drawn many times before . The simple policy entropy maximization will choose all actions with equal probability without considering the previous action samples . In contrast , if we consider maximizing the entropy of the mixture of $ \\pi $ and $ q $ ( it becomes the future sample distribution since the buffer stores samples generated by $ \\pi $ ) , $ \\pi $ should choose actions rare in the buffer with high probability and actions stored many times in the buffer with low probability to make the mixture distribution uniform . Hence , it considers the samples already stored in the buffer in choosing current action and encourages sample-efficient exploration . We provide a simple example below : Let us consider a simple 1-step MDP in which $ s_0 $ is the unique initial state , there exist $ N $ actions ( $ a_0\\in ( \\ { 1 , \\cdots , N\\ } ) $ ) , $ s_1 $ is the terminal state , and $ r $ is the deterministic reward function . Then , there exist $ N $ state-action pairs in total and let us assume that we already have $ N-1 $ state-action samples in the replay buffer as $ R= ( \\ { ( s_0,1 , r ( s_0,1 ) ) , \\cdots , ( s_0 , N-1 , r ( s_0 , N-1 ) ) \\ } ) $ . In order to estimate the Q-function for all state-action pairs , the policy should sample Action $ N $ ( After then , we can reuse all samples infinitely to estimate Q ) . Here , we will compare two exploration methods . 1 ) First , if we consider the simple entropy maximization , the policy will choose all actions with equal probability $ 1/N $ ( uniformly ) since $ \\max_\\pi \\mathcal { H } ( \\pi ) =\\min_\\pi KLD ( \\pi||U ) $ is achieved when $ \\pi=U $ , where $ U $ is a uniform distribution . Then , $ N $ samples should be taken on average by the policy to visit Action $ N $ . 2 ) Consider the sample-aware entropy maximization as in our paper . Here , the sample action distribution $ q $ in the buffer is defined as $ q ( a_0|s_0 ) =1/ ( N-1 ) $ for $ a_0\\in\\ { 0 , \\cdots , N-1\\ } $ , and $ q ( N|s_0 ) =0 $ . Now , we set the target sample distribution as the mixture of $ \\pi $ and $ q $ , as $ q_ { target } ^ { \\pi , \\alpha } =\\alpha\\pi+ ( 1-\\alpha ) q $ , and we set $ \\alpha=1/N $ . Then , in order to maximize the entropy of the target sample distribution $ \\max_\\pi \\mathcal { H } ( q_ { target } ^ { \\pi , \\alpha } ) = \\min_\\pi KLD ( q_ { target } ^ { \\pi , \\alpha } ||U ) $ , the policy distribution should be $ \\pi ( N|s_0 ) =1 $ to make $ q_ { target } ^ { \\pi , \\alpha } $ uniform . Thus , it only needs one sample to visit Action $ N $ . In this way , the simple entropy regularization is sample-inefficient for off-policy RL , and the proposed sample-aware entropy regularization enhances the sample-efficiency for exploration by using the previous sample distribution and choosing proper $ \\alpha $ . With this motivation , we propose the sample-aware entropy regularization and the corresponding $ \\alpha $ -adaptation . Therefore , the method in this paper addresses sample-efficient exploration ."}}