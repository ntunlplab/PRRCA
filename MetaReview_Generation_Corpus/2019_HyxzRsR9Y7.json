{"year": "2019", "forum": "HyxzRsR9Y7", "title": "Learning Self-Imitating Diverse Policies", "decision": "Accept (Poster)", "meta_review": "This paper proposes a reinforcement learning approach that better handles sparse reward environments, by using previously-experienced roll-outs that achieve high reward. The approach is intuitive, and the results in the paper are convincing. The authors addressed nearly all of the reviewer's concerns. The reviewers all agree that the paper should be accepted.", "reviews": [{"review_id": "HyxzRsR9Y7-0", "review_text": "The paper proposes how previously experienced high reward trajectories can be used to generate dense reward functions for more for efficient training of policies in context of reinforcement learning. The paper does this by computing the state-action pair distribution of high rewarding trajectories in the replay buffer, and using a surrogate reward that measures the distance between this distribution and the current state-action pair distribution. The paper derives approximate policy gradients for this surrogate reward function. The paper then describes limitations of doing this: possibility of getting stuck in the local neighborhood of currently well-performing trajectories. It also describes an extension based on Stein variational policy gradients to diversify behavior of an ensemble of policies that are learned together. The paper shows experimental results on a number of MuJoCo tasks. Strengths: 1. Adequately leveraging high-return roll-outs for effective learning of policies is an important problem in RL. The paper proposes and empirically investigates a reasonable approach for doing this. The paper shows how using the proposed additional rewards leads to better performance on the choses benchmarks than baseline methods without the proposed rewards. 2. I also like that the paper details the short-comings of the proposed approach, and how these could be fixed. Weaknesses: 1. The paper uses sparse rewards in RL as a motivation. However, the proposed approach crucially relies on the fact that a good trajectory has at least been encountered once in the past to be of any use. I am not sure if how the proposed approach does justice to the motivation in the paper. The paper should re-write the motivation, or better explain why the proposed method addresses the motivation. 2. Additionally, the paper does not provide adequate experimental validation. The experiment that I think will make the case for the paper is one that shows the sample efficiency of the proposed approach over other baseline methods, when given a successful past roll-out. The current experimental setup emphasizes the sparse reward scenario in RL, and it is just not clear to me as to why this is a good benchmark to study the effects of the proposed method. 3. The paper primarily makes comparisons to on-policy methods. This may not be a fair comparison, as the proposed method uses past trajectories from a replay buffer (to compute reward). Perhaps improvements are coming because of use of this off-policy information. The paper should design experiments to de-conflate this: perhaps by also comparing to how these additional rewards will compare in context of off-policy methods (like Q-learning). 4. I also do not understand how the benchmark tasks were chosen? Are the MuJoCo tasks studied here a fair representative of MuJoCo tasks studied in literature, or are these selected in any manner? While selecting and modifying benchmarks for the purpose of making a specific point is acceptable, it is important to include benchmark results on a full suite of tasks. This can help understand (desirable or un-desirable) side-effects of proposed ideas. After reading author response and the extra experiments, I have changed my rating to 6 (from the original rating of 5).", "rating": "6: Marginally above acceptance threshold", "reply_text": "1- Concerning \u201c Points 1. and 2. under Weaknesses \u201d : We do not wish to claim or motivate that self-imitation would suffice if the task is \u201c sparse \u201d in the sense that most of the episodes don \u2019 t see * any * rewards . This would fall under the limitations of self-imitation which we discuss in the paper ; we could rely on population-based exploration methods ( e.g.SVPG , Section 2.3 ) and draw on the rich literature on single-agent exploration methods like curiosity/novelty-search or parameter noise to alleviate this to an extent . Instead , we focus on scenarios where \u201c sparse \u201d feedback is available within an episode . We will make this very clear in our revision . For example , our experiments in Section 3.1 consider tasks where some feedback is available in an episode - either only once at the end of the episode , or at very few timesteps during an episode . We find self-imitation to be highly beneficial ( compared to standard policy gradients ) on these \u201c sparse \u201d constructions . Some practical situations of the kind include a . ) robotics tasks where rewards in an episode could be intermittent or delayed by arbitrary timesteps due to the inverse kinematics operations b . ) cases where a mild feedback on the overall quality of the episode is available , but designing a dense reward function manually is prohibitively hard ; an interesting example of this is [ 5 ] . Also , although our algorithm exploits \u201c good \u201d trajectories from agent \u2019 s past experience , the demands on the \u201c goodness \u201d of the trajectories are very relaxed . Indeed , the trajectories imitated during the initial phases of learning have quite low overall scores , and they gradually improve in quality . [ 5 ] Christiano , Paul F. , et al . `` Deep reinforcement learning from human preferences . '' Advances in Neural Information Processing Systems . 2017.2- Concerning \u201c Point 3. under Weaknesses -- comparison to off-policy RL methods \u201d : Our approach makes use of a replay memory to store and exploit past good rollouts of the agent . Off-policy RL methods such as DQN , DDPG also accumulate agent experience in a replay buffer and reuse them for learning ( e.g.by reducing TD-error ) . We run new experiments with a recent off-policy RL method based on DDPG - Twin Delayed Deep Deterministic policy gradient ( TD3 ; [ 2 ] ) . Appendix 5.10 evaluates its performance on MuJoCo tasks under the various reward distributions we used in our paper . We find that the performance of TD3 suffers appreciably under the episodic case and when the rewards are masked out with 90 % probability ( p_m=0.9 ) . We therefore believe that popular off-policy algorithms ( DDPG , TD3 ) do not exploit the past experience in a manner that accelerates learning when rewards are scarce during an episode . The per-timestep ( dense ) pseudo-rewards that we obtain with the divergence-minimization objective help in temporal credit assignment , resulting in good policies even under the episodic and noisy ( p_m=0.9 ) settings ( Table 1 , Section 3.1 ) . [ 2 ] Fujimoto , Scott , Herke van Hoof , and Dave Meger . `` Addressing Function Approximation Error in Actor-Critic Methods . '' International Conference on Machine Learning . 2018.4- Concerning \u201c Point 4. under Weaknesses \u201d : We have added Appendix 5.7 with results on more MuJoCo tasks . Combined with Table 1. in the paper , we believe our overall set to be fairly representative . For reference , the PPO paper [ 6 ] , which forms our baseline , uses the same set of benchmarks ( Figure 3 in their paper ) . [ 6 ] Schulman , John , et al . `` Proximal policy optimization algorithms . '' arXiv preprint arXiv:1707.06347 ( 2017 ) ."}, {"review_id": "HyxzRsR9Y7-1", "review_text": "The paper describes a method to improve reinforcement learning for task with sparse rewards signals. The basic idea is to select the best episodes from the system's experience, and learn to imitate them step by step as the system evolves, aiming at providing a less sparse learning signal. The math works out to a gradient that is of similar form as a policy gradient, which makes it easy to interpolate both of them. The resulting training procedure is a policy gradient that gets additional reinforcement of the system's best runs. The experiments show the validity especially for the most extreme case (episodic rewards), while, as expected, for the other extreme of dense rewards, the method's effect is not consistently positive. The paper then critiques its own method and identifies a critical weakness: the reliance on good exploration. I like that a lot. The paper goes on to suggest an extension to address this by training an ensemble, and shows the effectiveness of this for a number of tasks. However, I feel that the description of this extension is less clear than that of the core idea, and introduces too many new ideas and concepts in a too condensed text. The paper seems a significant in that it provides a notable improvement for sparse-rewards tasks, which are a common sub-class of real-world problems. My background is not RL. While I am quite confident in my understanding of the paper's math, I am not 100% familiar with the typical benchmark sets. Hence, I cannot judge whether the results include good baselines, or whether the task selection is biased. I can also not judge the completeness of the related work, and how novel the work is. For these questions, I hope that the other reviewers can provide more information. Pros: - intuitive idea for a common problem - solution elegantly has the form of a modified policy gradient - convincing experimental results - self-critique of core idea, and extension to address its main weakness - nicely written text, does not leave a lot of questions Cons: - while the core idea is nicely motivated and described and good to follow, Section 2.3 feels very dense and too short. Overall, I find the core idea quite intuitive and elegant. The paper's background, motivation, and core method are well-written and, with some effort, quite readable for someone who is not an RL expert. I found that several questions I had during reading were preempted promptly and addressed. However, the description of the secondary method (Section 2.3) is too dense. To me, the paper solidly meets the threshold of publication. Since I have no good comparison to other papers, I rate it a \"clear accept\" (8). Minor points: I noticed a few superfluous \"the\", please double-check. In Table 1, please use the same exponent for directly comparable numbers, e.g. instead of \"1.8e5 4.4e4\", say \"18e4 4.4e4\". Or best just print the full numbers without exponent, I think you have the space. When reading Table 1, I could bnot immediately line up \"PPO\" and \"Self-imitation\" in the caption with the table columns. It took a while to infer that PPO refers to \\nu=0, and SI to \\nu=0.8. Can you add PPO and SI to the table headings? You define p as \"the masking probability\", but it is not clear whether that is the probability for keeping a \"1\" in the mask, or for masking out the value. I can only guess from the results. I suggest to rephrase as \"the probability of retaining a reward\". Also, how about using plain words in Table 1's heading, such as \"Noisy rewards\\nSuppressing 10% of rewards\", so that one can understand the table without having to search for its description in the text? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "1- Concerning \u201c Section 2.3 being too dense \u201d : We have re-organized the writing . Specifically , we have added more details on SVPG exploration with the JS-kernel in Appendix 5.8 . Appendix 5.8.1 includes some more intuition and theory behind Stein Variational Gradient Descent ( SVGD ) and Stein Variational Policy Gradient ( SVPG ) . Appendix 5.8.2 contains details on our implementation such as calculation of SVPG exploration rewards by each agent , and state-value function baselines , along with better explanation of symbols used in our full algorithm ( Algorithm 2 ) . 2- Concerning \u201c Minor points \u201d : Thank you for pointing these out . We have changed Table 1. in the revision to include all the suggested changes , in the hope that the table becomes self-explanatory . We have also rephrased the text to clarify that we compare performance with two different reward masking values - suppressing each per-timestep reward r_t with 90 % probability ( p_m = 0.9 ) , and with 50 % probability ( p_m=0.5 ) ."}, {"review_id": "HyxzRsR9Y7-2", "review_text": "Overall impression: I think that this is a well written interesting paper with strong results. One thing I\u2019d have liked to see a bit more is an explanation of why self imitation is more effective than standard policy gradient? Where does the extra supervision/stability come from, and can this be explained intuitively? I\u2019ve suggested some small changes/clarifications to be made inline, and a few more comparisons to add. But overall, I very much like this line of work and I recommend accepting this paper. Abstract: We demonstrate its effectiveness on a number of challenging tasks. -> be more specific. The term single-timestep optimization is not very clear. Can this be clarified? they are more widely applicable in the sparse or episodic reward settings -> it is likely important to mention that they are agnostic to horizon of the task. Related works: Guided Policy Search also does divergence minimization. GAIL considers the imitation learning work as a sort of divergence minimization problem as well, which should be explicitly mentioned. Other work for good exploration include DIAYN (Eysenbach et al 2018). The difference in resulting updates between (Oh et al) and this work should be clearly discussed in the methods section. \u201cwe learn shaped, dense rewards\u201d-> too early in the paper for this to make sense. can provide some contextt Section 2.2: fully decides the expected return -> clarify this a bit. I think what you mean is that the dynamics are wrapped into this already, so it accounts for this, but this can be made explicit. Small typos in appendix 5.1 (r should be replaced by the density ratio) The update in (3) seems quite similar to what GAIL would do. What is the difference there? Or is the difference just in the fact that the experts are chosen from \u201cself\u201d experiences. How is the priority list threshold and size chosen? Would a softer version of the priority queue update do anything useful? Or would it just reduce to policy gradient when weighted by rewards? Appendices are very clear and very informative while being succinct! I would have liked to see Appendix 5.3 in the main text (maybe a shorter form) to clarify the whole algorithm What is psi in appendix 5.3? The algorithm remains a bit unclear without this clarification Experiments. Only 1 question to answer in this section is labelled? Put 2) and 3) appropriately. Can a comparison to Oh et al 2018 be added to this for the sake of completeness? Also can this be compared to using novelty/curiosity based exploration schemes? Can the authors comment on why the method reaches higher asymptotic performance but is often slower in the beginning than the other methods in Fig 3. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "1- Concerning \u201c Why is self-imitation more effective than standard policy gradients , and if the source of stability can be explained intuitively \u201d : We believe that learning pseudo-rewards with self-imitation helps in the temporal credit assignment problem in the sparse- or episodic-reward setting . For instance , in the episodic setting , where a reward is only provided at episode termination , standard policy gradient algorithms reinforce the actions towards the beginning of the episode based on a reward signal which is obtained after multiple timesteps and convolves the effect of many intermediate actions . This signal is potentially sparse and diluted , and may deteriorate with task horizon . With our approach , since we learn \u201c per-timestep \u201d pseudo-rewards with self-imitation , we expect this greedy signal to help in attributing credit to actions more effectively , leading to faster training . Qualitatively , the stability of the self-imitation algorithm could also be understood by viewing it as a form of curriculum learning [ 4 ] . Unlike learning from perfect demonstrations by external experts , our learner at any point in time is imitating only a slightly different version of itself . The demonstrations , therefore , increase in complexity gradually over time , resulting in an implicit , adaptive curriculum which stabilizes learning and avoids catastrophic forgetting of behaviors . [ 4 ] Bengio , Yoshua , et al . `` Curriculum learning . '' Proceedings of the 26th annual international conference on machine learning . ACM , 2009 . 2- Concerning \u201c Re-phrases in various sections \u201d : We have incorporated all the suggested changes in the revision with extra discussion . We have also added the missing reference to Guided Policy Search and expanded on GAIL . DIAYN ( Eysenbach et al 2018 ) is included in Appendix 5.6 . 3- Concerning \u201c Comparison to Oh et al . ( 2018 ) \u201d : We have added a new section ( Appendix 5.9 ) focussed on the algorithm ( SIL ) by Oh et al . ( 2018 ) .Therein , we mention the update rule for SIL and the performance of PPO+SIL on MuJoCo tasks under the various reward distributions we used in our paper . We summarize our observations here ( please see Appendix 5.9 for more details ) . The performance of PPO+SIL suffers under the episodic case and when the rewards are masked out with 90 % probability ( p_m=0.9 ) . Our intuition is that this is because PPO+SIL makes use of the \u201c cumulative return \u201d from each transition of a past good rollout for the update . When rewards are provided only at the end of the episode , for instance , cumulative return does not help with the temporal credit assignment problem and hence is not a strong learning signal . 4- Concerning \u201c Comparing SVPG exploration ( Figure 3 ) to novelty/curiosity based exploration schemes \u201d : We have added a new section ( Appendix 5.11 ) on comparing SVPG exploration to a novelty-based exploration baseline - EX2 [ 3 ] . The EX2 algorithm does implicit density estimation using discriminative modeling , and uses it for novelty-based exploration . We report results on the hard exploration MuJoCo tasks considered in Section 3.2 , using author provided code and hyperparameters . Table 5 in Appendix 5.11 shows that we compare favorably against EX2 on the tasks evaluated . [ 3 ] Fu , Justin , John Co-Reyes , and Sergey Levine . `` Ex2 : Exploration with exemplar models for deep reinforcement learning . '' Advances in Neural Information Processing Systems . 2017.5- Concerning \u201c What is psi in appendix 5.3 ? \u201d : We apologize for skimping the details on this . \u201c psi \u201d denotes the parameters of neural networks that are used to model the state-action visitation distribution ( rho ) of the policy . Therefore , for an ensemble of n policies , there are n \u201c psi \u201d networks . The motivation behind using these networks is as follows . To calculate the gradient of JS , we need the ratio denoted by r^ { \\phi } in the paper . This ratio can be obtained implicitly by training a parameterized discriminator network . However , when using SVPG exploration with JS kernel , this method would require us to train O ( n^2 ) discriminator networks , one each for calculating the gradient of JS between a policy pair ( i , j ) . To reduce the computational and memory resource burden to O ( n ) , we opt for explicit modeling of the state-action visitation distribution ( rho ) of the policy by a network with parameters \u201c psi \u201d . The \u201c psi \u201d networks are trained using the JS optimization ( Equation 2 . ) and we can then obtain the ratio explicitly from these \u201c psi \u201d networks . We have added these details ( and more ) to Appendix 5.8.2 . It also contains proper symbols ( in Latex ) for easier reading ."}], "0": {"review_id": "HyxzRsR9Y7-0", "review_text": "The paper proposes how previously experienced high reward trajectories can be used to generate dense reward functions for more for efficient training of policies in context of reinforcement learning. The paper does this by computing the state-action pair distribution of high rewarding trajectories in the replay buffer, and using a surrogate reward that measures the distance between this distribution and the current state-action pair distribution. The paper derives approximate policy gradients for this surrogate reward function. The paper then describes limitations of doing this: possibility of getting stuck in the local neighborhood of currently well-performing trajectories. It also describes an extension based on Stein variational policy gradients to diversify behavior of an ensemble of policies that are learned together. The paper shows experimental results on a number of MuJoCo tasks. Strengths: 1. Adequately leveraging high-return roll-outs for effective learning of policies is an important problem in RL. The paper proposes and empirically investigates a reasonable approach for doing this. The paper shows how using the proposed additional rewards leads to better performance on the choses benchmarks than baseline methods without the proposed rewards. 2. I also like that the paper details the short-comings of the proposed approach, and how these could be fixed. Weaknesses: 1. The paper uses sparse rewards in RL as a motivation. However, the proposed approach crucially relies on the fact that a good trajectory has at least been encountered once in the past to be of any use. I am not sure if how the proposed approach does justice to the motivation in the paper. The paper should re-write the motivation, or better explain why the proposed method addresses the motivation. 2. Additionally, the paper does not provide adequate experimental validation. The experiment that I think will make the case for the paper is one that shows the sample efficiency of the proposed approach over other baseline methods, when given a successful past roll-out. The current experimental setup emphasizes the sparse reward scenario in RL, and it is just not clear to me as to why this is a good benchmark to study the effects of the proposed method. 3. The paper primarily makes comparisons to on-policy methods. This may not be a fair comparison, as the proposed method uses past trajectories from a replay buffer (to compute reward). Perhaps improvements are coming because of use of this off-policy information. The paper should design experiments to de-conflate this: perhaps by also comparing to how these additional rewards will compare in context of off-policy methods (like Q-learning). 4. I also do not understand how the benchmark tasks were chosen? Are the MuJoCo tasks studied here a fair representative of MuJoCo tasks studied in literature, or are these selected in any manner? While selecting and modifying benchmarks for the purpose of making a specific point is acceptable, it is important to include benchmark results on a full suite of tasks. This can help understand (desirable or un-desirable) side-effects of proposed ideas. After reading author response and the extra experiments, I have changed my rating to 6 (from the original rating of 5).", "rating": "6: Marginally above acceptance threshold", "reply_text": "1- Concerning \u201c Points 1. and 2. under Weaknesses \u201d : We do not wish to claim or motivate that self-imitation would suffice if the task is \u201c sparse \u201d in the sense that most of the episodes don \u2019 t see * any * rewards . This would fall under the limitations of self-imitation which we discuss in the paper ; we could rely on population-based exploration methods ( e.g.SVPG , Section 2.3 ) and draw on the rich literature on single-agent exploration methods like curiosity/novelty-search or parameter noise to alleviate this to an extent . Instead , we focus on scenarios where \u201c sparse \u201d feedback is available within an episode . We will make this very clear in our revision . For example , our experiments in Section 3.1 consider tasks where some feedback is available in an episode - either only once at the end of the episode , or at very few timesteps during an episode . We find self-imitation to be highly beneficial ( compared to standard policy gradients ) on these \u201c sparse \u201d constructions . Some practical situations of the kind include a . ) robotics tasks where rewards in an episode could be intermittent or delayed by arbitrary timesteps due to the inverse kinematics operations b . ) cases where a mild feedback on the overall quality of the episode is available , but designing a dense reward function manually is prohibitively hard ; an interesting example of this is [ 5 ] . Also , although our algorithm exploits \u201c good \u201d trajectories from agent \u2019 s past experience , the demands on the \u201c goodness \u201d of the trajectories are very relaxed . Indeed , the trajectories imitated during the initial phases of learning have quite low overall scores , and they gradually improve in quality . [ 5 ] Christiano , Paul F. , et al . `` Deep reinforcement learning from human preferences . '' Advances in Neural Information Processing Systems . 2017.2- Concerning \u201c Point 3. under Weaknesses -- comparison to off-policy RL methods \u201d : Our approach makes use of a replay memory to store and exploit past good rollouts of the agent . Off-policy RL methods such as DQN , DDPG also accumulate agent experience in a replay buffer and reuse them for learning ( e.g.by reducing TD-error ) . We run new experiments with a recent off-policy RL method based on DDPG - Twin Delayed Deep Deterministic policy gradient ( TD3 ; [ 2 ] ) . Appendix 5.10 evaluates its performance on MuJoCo tasks under the various reward distributions we used in our paper . We find that the performance of TD3 suffers appreciably under the episodic case and when the rewards are masked out with 90 % probability ( p_m=0.9 ) . We therefore believe that popular off-policy algorithms ( DDPG , TD3 ) do not exploit the past experience in a manner that accelerates learning when rewards are scarce during an episode . The per-timestep ( dense ) pseudo-rewards that we obtain with the divergence-minimization objective help in temporal credit assignment , resulting in good policies even under the episodic and noisy ( p_m=0.9 ) settings ( Table 1 , Section 3.1 ) . [ 2 ] Fujimoto , Scott , Herke van Hoof , and Dave Meger . `` Addressing Function Approximation Error in Actor-Critic Methods . '' International Conference on Machine Learning . 2018.4- Concerning \u201c Point 4. under Weaknesses \u201d : We have added Appendix 5.7 with results on more MuJoCo tasks . Combined with Table 1. in the paper , we believe our overall set to be fairly representative . For reference , the PPO paper [ 6 ] , which forms our baseline , uses the same set of benchmarks ( Figure 3 in their paper ) . [ 6 ] Schulman , John , et al . `` Proximal policy optimization algorithms . '' arXiv preprint arXiv:1707.06347 ( 2017 ) ."}, "1": {"review_id": "HyxzRsR9Y7-1", "review_text": "The paper describes a method to improve reinforcement learning for task with sparse rewards signals. The basic idea is to select the best episodes from the system's experience, and learn to imitate them step by step as the system evolves, aiming at providing a less sparse learning signal. The math works out to a gradient that is of similar form as a policy gradient, which makes it easy to interpolate both of them. The resulting training procedure is a policy gradient that gets additional reinforcement of the system's best runs. The experiments show the validity especially for the most extreme case (episodic rewards), while, as expected, for the other extreme of dense rewards, the method's effect is not consistently positive. The paper then critiques its own method and identifies a critical weakness: the reliance on good exploration. I like that a lot. The paper goes on to suggest an extension to address this by training an ensemble, and shows the effectiveness of this for a number of tasks. However, I feel that the description of this extension is less clear than that of the core idea, and introduces too many new ideas and concepts in a too condensed text. The paper seems a significant in that it provides a notable improvement for sparse-rewards tasks, which are a common sub-class of real-world problems. My background is not RL. While I am quite confident in my understanding of the paper's math, I am not 100% familiar with the typical benchmark sets. Hence, I cannot judge whether the results include good baselines, or whether the task selection is biased. I can also not judge the completeness of the related work, and how novel the work is. For these questions, I hope that the other reviewers can provide more information. Pros: - intuitive idea for a common problem - solution elegantly has the form of a modified policy gradient - convincing experimental results - self-critique of core idea, and extension to address its main weakness - nicely written text, does not leave a lot of questions Cons: - while the core idea is nicely motivated and described and good to follow, Section 2.3 feels very dense and too short. Overall, I find the core idea quite intuitive and elegant. The paper's background, motivation, and core method are well-written and, with some effort, quite readable for someone who is not an RL expert. I found that several questions I had during reading were preempted promptly and addressed. However, the description of the secondary method (Section 2.3) is too dense. To me, the paper solidly meets the threshold of publication. Since I have no good comparison to other papers, I rate it a \"clear accept\" (8). Minor points: I noticed a few superfluous \"the\", please double-check. In Table 1, please use the same exponent for directly comparable numbers, e.g. instead of \"1.8e5 4.4e4\", say \"18e4 4.4e4\". Or best just print the full numbers without exponent, I think you have the space. When reading Table 1, I could bnot immediately line up \"PPO\" and \"Self-imitation\" in the caption with the table columns. It took a while to infer that PPO refers to \\nu=0, and SI to \\nu=0.8. Can you add PPO and SI to the table headings? You define p as \"the masking probability\", but it is not clear whether that is the probability for keeping a \"1\" in the mask, or for masking out the value. I can only guess from the results. I suggest to rephrase as \"the probability of retaining a reward\". Also, how about using plain words in Table 1's heading, such as \"Noisy rewards\\nSuppressing 10% of rewards\", so that one can understand the table without having to search for its description in the text? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "1- Concerning \u201c Section 2.3 being too dense \u201d : We have re-organized the writing . Specifically , we have added more details on SVPG exploration with the JS-kernel in Appendix 5.8 . Appendix 5.8.1 includes some more intuition and theory behind Stein Variational Gradient Descent ( SVGD ) and Stein Variational Policy Gradient ( SVPG ) . Appendix 5.8.2 contains details on our implementation such as calculation of SVPG exploration rewards by each agent , and state-value function baselines , along with better explanation of symbols used in our full algorithm ( Algorithm 2 ) . 2- Concerning \u201c Minor points \u201d : Thank you for pointing these out . We have changed Table 1. in the revision to include all the suggested changes , in the hope that the table becomes self-explanatory . We have also rephrased the text to clarify that we compare performance with two different reward masking values - suppressing each per-timestep reward r_t with 90 % probability ( p_m = 0.9 ) , and with 50 % probability ( p_m=0.5 ) ."}, "2": {"review_id": "HyxzRsR9Y7-2", "review_text": "Overall impression: I think that this is a well written interesting paper with strong results. One thing I\u2019d have liked to see a bit more is an explanation of why self imitation is more effective than standard policy gradient? Where does the extra supervision/stability come from, and can this be explained intuitively? I\u2019ve suggested some small changes/clarifications to be made inline, and a few more comparisons to add. But overall, I very much like this line of work and I recommend accepting this paper. Abstract: We demonstrate its effectiveness on a number of challenging tasks. -> be more specific. The term single-timestep optimization is not very clear. Can this be clarified? they are more widely applicable in the sparse or episodic reward settings -> it is likely important to mention that they are agnostic to horizon of the task. Related works: Guided Policy Search also does divergence minimization. GAIL considers the imitation learning work as a sort of divergence minimization problem as well, which should be explicitly mentioned. Other work for good exploration include DIAYN (Eysenbach et al 2018). The difference in resulting updates between (Oh et al) and this work should be clearly discussed in the methods section. \u201cwe learn shaped, dense rewards\u201d-> too early in the paper for this to make sense. can provide some contextt Section 2.2: fully decides the expected return -> clarify this a bit. I think what you mean is that the dynamics are wrapped into this already, so it accounts for this, but this can be made explicit. Small typos in appendix 5.1 (r should be replaced by the density ratio) The update in (3) seems quite similar to what GAIL would do. What is the difference there? Or is the difference just in the fact that the experts are chosen from \u201cself\u201d experiences. How is the priority list threshold and size chosen? Would a softer version of the priority queue update do anything useful? Or would it just reduce to policy gradient when weighted by rewards? Appendices are very clear and very informative while being succinct! I would have liked to see Appendix 5.3 in the main text (maybe a shorter form) to clarify the whole algorithm What is psi in appendix 5.3? The algorithm remains a bit unclear without this clarification Experiments. Only 1 question to answer in this section is labelled? Put 2) and 3) appropriately. Can a comparison to Oh et al 2018 be added to this for the sake of completeness? Also can this be compared to using novelty/curiosity based exploration schemes? Can the authors comment on why the method reaches higher asymptotic performance but is often slower in the beginning than the other methods in Fig 3. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "1- Concerning \u201c Why is self-imitation more effective than standard policy gradients , and if the source of stability can be explained intuitively \u201d : We believe that learning pseudo-rewards with self-imitation helps in the temporal credit assignment problem in the sparse- or episodic-reward setting . For instance , in the episodic setting , where a reward is only provided at episode termination , standard policy gradient algorithms reinforce the actions towards the beginning of the episode based on a reward signal which is obtained after multiple timesteps and convolves the effect of many intermediate actions . This signal is potentially sparse and diluted , and may deteriorate with task horizon . With our approach , since we learn \u201c per-timestep \u201d pseudo-rewards with self-imitation , we expect this greedy signal to help in attributing credit to actions more effectively , leading to faster training . Qualitatively , the stability of the self-imitation algorithm could also be understood by viewing it as a form of curriculum learning [ 4 ] . Unlike learning from perfect demonstrations by external experts , our learner at any point in time is imitating only a slightly different version of itself . The demonstrations , therefore , increase in complexity gradually over time , resulting in an implicit , adaptive curriculum which stabilizes learning and avoids catastrophic forgetting of behaviors . [ 4 ] Bengio , Yoshua , et al . `` Curriculum learning . '' Proceedings of the 26th annual international conference on machine learning . ACM , 2009 . 2- Concerning \u201c Re-phrases in various sections \u201d : We have incorporated all the suggested changes in the revision with extra discussion . We have also added the missing reference to Guided Policy Search and expanded on GAIL . DIAYN ( Eysenbach et al 2018 ) is included in Appendix 5.6 . 3- Concerning \u201c Comparison to Oh et al . ( 2018 ) \u201d : We have added a new section ( Appendix 5.9 ) focussed on the algorithm ( SIL ) by Oh et al . ( 2018 ) .Therein , we mention the update rule for SIL and the performance of PPO+SIL on MuJoCo tasks under the various reward distributions we used in our paper . We summarize our observations here ( please see Appendix 5.9 for more details ) . The performance of PPO+SIL suffers under the episodic case and when the rewards are masked out with 90 % probability ( p_m=0.9 ) . Our intuition is that this is because PPO+SIL makes use of the \u201c cumulative return \u201d from each transition of a past good rollout for the update . When rewards are provided only at the end of the episode , for instance , cumulative return does not help with the temporal credit assignment problem and hence is not a strong learning signal . 4- Concerning \u201c Comparing SVPG exploration ( Figure 3 ) to novelty/curiosity based exploration schemes \u201d : We have added a new section ( Appendix 5.11 ) on comparing SVPG exploration to a novelty-based exploration baseline - EX2 [ 3 ] . The EX2 algorithm does implicit density estimation using discriminative modeling , and uses it for novelty-based exploration . We report results on the hard exploration MuJoCo tasks considered in Section 3.2 , using author provided code and hyperparameters . Table 5 in Appendix 5.11 shows that we compare favorably against EX2 on the tasks evaluated . [ 3 ] Fu , Justin , John Co-Reyes , and Sergey Levine . `` Ex2 : Exploration with exemplar models for deep reinforcement learning . '' Advances in Neural Information Processing Systems . 2017.5- Concerning \u201c What is psi in appendix 5.3 ? \u201d : We apologize for skimping the details on this . \u201c psi \u201d denotes the parameters of neural networks that are used to model the state-action visitation distribution ( rho ) of the policy . Therefore , for an ensemble of n policies , there are n \u201c psi \u201d networks . The motivation behind using these networks is as follows . To calculate the gradient of JS , we need the ratio denoted by r^ { \\phi } in the paper . This ratio can be obtained implicitly by training a parameterized discriminator network . However , when using SVPG exploration with JS kernel , this method would require us to train O ( n^2 ) discriminator networks , one each for calculating the gradient of JS between a policy pair ( i , j ) . To reduce the computational and memory resource burden to O ( n ) , we opt for explicit modeling of the state-action visitation distribution ( rho ) of the policy by a network with parameters \u201c psi \u201d . The \u201c psi \u201d networks are trained using the JS optimization ( Equation 2 . ) and we can then obtain the ratio explicitly from these \u201c psi \u201d networks . We have added these details ( and more ) to Appendix 5.8.2 . It also contains proper symbols ( in Latex ) for easier reading ."}}