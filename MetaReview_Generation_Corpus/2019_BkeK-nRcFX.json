{"year": "2019", "forum": "BkeK-nRcFX", "title": "The Nonlinearity Coefficient - Predicting Generalization in Deep Neural Networks", "decision": "Reject", "meta_review": "This paper proposes the NonLinearity Coefficient (NLC), a metric which aims to predicts test-time performance of neural networks at initialization. The idea is interesting and novel, and has clear practical implications. Reviewers unanimously agreed that the direction is a worthwhile one to pursue. However, several reviewers also raised concerns about how well-justified the method is: in particular, Reviewer 3 believes that a quantitative comparison to the related work is necessary, and takes issue with the motivation for being ad-hoc. Reviewer 2 also is concerned about the soundness of the coefficient in truly measuring nonlinearity. \n\nThese concerns make it clear that the paper needs more work before it can be published. And, in particular, addressing the reviewers' concerns and providing proper comparison to related works will go a long way in that direction.", "reviews": [{"review_id": "BkeK-nRcFX-0", "review_text": "I do not understand the denomination of nonlinearity coefficient provided in definition 1: although the quantity indeed does equal to 1 under whitened data distribution or orthogonal matrix, the conjecture that it should be close to 1 does not seem to be close at all just under any data distribution. Using a similar construction that section 6, we can rescale a whitened input data with a diagonal matrix D with components all equal to one except for a very large one \\lambda and also multiply the input weights by D^{-1} to compensate (and have a similar function). If you look at such construction for the linear case with identity initialization of A, the NLC is sqrt((\\lambda^2 + n - 1) (\\lambda^{-2} + n - 1)) / n which can grow arbitrarily large with \\lambda *for a linear model*. However, because of its low capacity, we would expect a linear model to have reasonable generalization. This seems to compromise the initial NLC being low as a necessary condition for reasonable generalization. Conversely, it\u2019s possible to initialize arbitrarily large residual networks such that the resulting initial function is linear (by initializing the output weight of the incrementing block to 0). This initialization may also be done such that the initial NLC becomes close to 1. I would not think this wouldn\u2019t necessarily result in good generalization, which seems to agree with the experimental observation. Now given that this initial NLC is neither sufficient nor necessary to predict generalization, one can wonder what is correlating generalization and NLC together in the experiment section. Same remark applies to the correlation between nonlinearity and NLC. This is especially concerning since in the linear case, the NLC can vary whether we chose to whiten the data or not for example, so the other influencing factors need to be discovered. What were the architecture that resulted in small/high NLC? The experiment section still contains interesting bits, such as successful training of very deep architecture that are very sensitive to input perturbations but they are not part of the main thread of the paper.", "rating": "4: Ok but not good enough - rejection", "reply_text": "`` one can wonder what is correlating generalization and NLC together in the experiment section . Same remark applies to the correlation between nonlinearity and NLC . '' NLC is a measure of nonlinearity that is based on certain assumptions as explained in sections 3 / A . Figure 1 shows that those assumptions hold in practical networks . Figure 2 shows that good / bad performance is strongly associated with nonlinearity . At least one reason for this relationship is given in figure 3E , where we show that NLC is related to sensitivity of the output to small input changes . `` What were the architecture that resulted in small/high NLC ? '' The magnitude of the NLC is chiefly dependent on the linear approximability of nonlinearities , as explained in section 5 . Unfortunately , an in-depth discussion on why certain architectures have a certain NLC , for a large number of architectures , goes beyond the scope of the paper . Thank you and we look forward to your response ."}, {"review_id": "BkeK-nRcFX-1", "review_text": "In this paper the authors introduce a new quantity, the nonlinearity coefficient, and argue that its value at initialization is a useful predictor of test time performance for neural networks. The authors conduct a wide range of experiments over many different network architectures and activation functions to corroborate these results. The authors then extend their method to compute the local nonlinearity of activation functions instead. I am a bit torn on this paper. I appreciate the direction that the authors have chosen to pursue. The topic of identifying parameters that are predictive of trainability is certainly interesting and has the potential to be quite impactful. Moreover, the breadth of the experiments conducted by the authors is novel and significant. Finally, I find the the overall manner in which the authors have chosen to present their data refreshingly transparent. Together, this leads me to believe that the quantity proposed by the authors might be useful to researchers. Having said that, I am concerned by the author\u2019s exposition of the nonlinearity coefficient itself. Fundamentally, my concern stems from the fact that it seems a lot of relatively ad-hoc decisions were made in the construction of the nonlinearity coefficient and an insufficiently good job was done to compare it to other measures of nonlinearity. Specifically, it feels like an extremely weak definition of nonlinearity to say that the linear approximation of a function fails when it produces values that lie outside of the co-domain of the function. Moreover, I feel as though there is already a well defined notion of nonlinearity at a point that could be constructed by reference to the Hessian (or generally by the approximation error induced by truncating the Taylor series after the linear term). I would like to see some comparison between these two methods. This is made more troubling given that the correlation found by the authors is present but does not seem especially strong. For example, in fig. 2A it seems like the nonlinearity coefficient varies by at least two orders of magnitude in the inset of the figure where the test accuracy really does not seem sensitive to its value. Prior work (for example, [1] from last years ICLR) has shown strong correlations between the Frobenius norm of the Jacobian and test error (see fig. 5 and fig. 6). Since the definition of the nonlinearity coefficient seems somewhat ad-hoc I would love to see a comparison between it and just looking at the Jacobian norm in terms of predicting test accuracy. [1] - SENSITIVITY AND GENERALIZATION IN NEURAL NETWORKS: AN EMPIRICAL STUDY Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein ", "rating": "5: Marginally below acceptance threshold", "reply_text": "# # # The method of Novak et al # # # Our paper goes significantly beyond the scope of Novak et al , because we use the NLC computed * before * training to predict performance * after * training . Novak et al use the Jacobian * after * training to compare against performance * after * training . Predicting performance before training is much more useful because it enables architecture design / selection . Furthermore , it is also much harder . We predict the property ( test error ) of one network ( trained network ) by examining a property ( NLC ) of a different network ( untrained network ) . Novak et al make inference about the property of a network ( test error ) from properties of that same network . Let us detail just one reason why our task is harder . The Novak et al paper uses the Frobenius norm of the Jacobian of the softmax units with respect to the input and compares that value to test error . We can write that Jacobian as d softmax/d input , which is the same as d softmax/d logits * d logits/d input , where 'logits ' denotes the values that are fed into the softmax . Now it turns out that ||d softmax/d logits||_F tends to be smaller for a given input when the prediction of the network is correct for that input , and it tends to be larger when the prediction of the network is incorrect . This is shown in the Novak paper in figure 6 . Therefore ||d softmax/d logits||_F is strongly correlated with error not because of an interesting structural property of a network , but simply because of an idiosyncratic property of the softmax : it tends to have larger gradients for less confident predictions . Hence , it is likely that the correlation between d softmax/d logits = d softmax/d logits * d logits/d input and test error is also caused to a significant degree by this effect . By computing the NLC before training , we do not `` benefit '' from this spurious signal . Therefore , our task is not comparable to the task studied by Novak et al , and hence the raw correlation numbers are also not comparable . We would argue that if you consider the Novak paper to be an important contribution , our paper is at least an equally important contribution , because we study a task that is at least in certain ways significantly more useful . # # # Summary # # # The NLC is the first gradient-based metric that , when computed before traning , has been shown to be predictive of test error after training through a large-scale study involving a wide variety of networks . Additional benefits include : - it is an accurate measure of nonlinearity * in practice * ( figure 1 ) - it is intimiately related to the linear approximability of activation functions ( section 5 ) - it is more robust to confounders than comparable metrics ( section 6 ) - it is cheap to compute ( section G ) However , we do not claim that the NLC is the * correct * measure of nonlinearity for deep neural networks . We are happy to use heuristics in deriving the NLC as long as we attain the above benefits . In response to your criticism that `` it feels like an extremely weak definition of nonlinearity to say that the linear approximation of a function fails when it produces values that lie outside of the co-domain of the function . '' we would respond that our goal is not to define nonlinearity definitively , but to come up with a metric that has the benefits outlined above . Nonetheless , we think that our figure 1 and the shortcomings of the Hessian indicate that the NLC is the state-of-the-art in neural network nonlinearity estimation , and we think that the NLC as a performance predictor is better motivated than e.g . 'gradient explosion ' or 'correlation preservation ' , as well as the metric used by Novak et al.Finally , we strongly disagree with the statement that `` in fig . 2A it seems like the nonlinearity coefficient varies by at least two orders of magnitude in the inset of the figure where the test accuracy really does not seem sensitive to its value . '' See the correlation numbers and p-values below . While the correlation in the inset is a bit lower , this is to be expected as the correlation between random variables tends to decrease if the range of one variable is restricted . Scenario correlation p-value CIFAR10 0.72 2.34e-41 CIFAR10 ( inset ) 0.68 1.88e-21 MNIST 0.81 1.31e-57 MNIST ( inset ) 0.63 3.60e-19 wave 0.67 1.82e-33 wave ( inset ) 0.57 3.60e-13 We thank you again for your review . We would love to discuss further and look forward to your response . Please let us know whether you want us to include the above discussions in the next revision of the manuscript or not ."}, {"review_id": "BkeK-nRcFX-2", "review_text": "This paper proposes a metric to measure the \"nonlinearity\" of neural network, and presents evidence that the value of this metric at initialization time is predictive of generalization performance. Apart from a few problems I think this paper is well written and thorough. The contribution is solid, although not earth shattering given previous work on such metrics. There seems to be a basic error in some of the early math, although I don't think this will qualitatively affect the results in any significant way. ----------------- Detailed comments by section: ------------------ Section 3: It seems like a 1/sqrt(d) factor is missing from these Q_i(S_x x(i)) and Q_j(S_x f(x,j)) formulas. As far as I can tell this doesn't affect Def 1 because you seemed to use the correct formula there. However, the rewritten version with the traces doesn't seem to be correct. There should be a d_in factor in the denominator (inside the square root). This error seems unrelated to the other one. Assuming I'm correct and that this is an error, does this affect your results in the various figures? And what is the actual final definition of NLC that you used? In general, it's annoying for the reader to verify that all of these forms are equivalent. And it's fiddly enough with the sqrt(d) terms constantly disappearing and reappearing in the numerator and denominators that even you made multiple errors (as far as I can tell). I would suggest making this section more rigorous and writing out everything carefully. And you probably don't need to rewrite it in so many equivalent forms with different notation unless they are useful somehow. The use of the Q and S symbols feels superfluous and counterproductive. Standard notation with expectations and squares wouldn't take much more space and would be a lot clearer. Section 4: \"we plot the relative diameter of the linearly approximable regions of the network as defined in section 3\": but you don't seem to define \"relative diameter\" there. As far as I can tell it's only defined in Appendix E, and this is only mentioned in the caption of figure 1. It's impossible to interpret this result without knowing precisely what \"relative diameter\" is. If you can't afford to describe this in the main paper you should at least mention that it's a different (more expensive) way of estimating the same thing that the NLC estimates. In Figure 2, are the higher test errors due to the optimizer failing to lower the training error, or due to a greater generalization gap? I guess the Figure 3 results suggest the latter possibility, which is surprising to me. What does it mean to have a \"very biased output\". What does that inequality mean intuitively? Should there be an absolute value on the RHS? It would be much easier to parse it if it were written in plain notation without these S and Q symbols. Section 6: \"metric also an\" -> \"metric also has an\" Can you generate a failure case for \"correlation information\" that doesn't involve Batch Norm layers? I don't think the authors of those works meant for their results to deal with that. Note that there are actually a lot of papers going back to the 90s that discussed and proved representational benefits of depth in neural networks.", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 1 , Thank you for your review . We address your detailed comments below . It seems that your main criticism which prevented our paper from attaining a higher rating was your assessment that `` The contribution is solid , although not earth shattering given previous work on such metrics . '' I would love to know more detail regarding this statement . We believe that many properties demonstrated for the NLC throughout the paper ( e.g.predictiveness of error when computed before training across a wide range of architectures , predictiveness of nonlinearity , robustness to confounders , relationship to linear approximability of activation functions ) are either novel compared to other metrics or at least have not been demonstrated for them . If you are aware of prior work that contradicts those beliefs , we would love to know . ... * * * missing sqrt ( d ) factor You are absolutely correct . Thank you for pointing this out . For what it 's worth , we noticed this problem very soon after we submitted the paper and posted a correction on openreview . You can check our comment `` typo found '' , posted on October 3rd , below . We understand that it is annoying to see conflicting definitions , and we apologize for this . The highlighted `` Definition 1 '' at the top of page 3 is correct . We added the alternative definition using traces at the last moment , thereby producing typos . The reason for including the 'correct ' d_in / d_out factors in the definition is to avoid susceptibility of the NLC to changes in input dimension / output dimension that do not affect nonlinearity / performance . In the revision we just uploaded , we have fixed the d_in / d_out typos . If you think we should still also remove the Q/S notation , please let us know and we will upload an additional revision . * * * relative diameter We added a reference to section E.1 in the main text , where the formal definition of relative diameter is diven . The informal meaning is discussed in section 3 . * * * test error vs training error Yes , the high test error is mostly caused by bad generalization , at least on the waveform-noise dataset . Please see section B.1 for further details on this . * * * Biased output An individual neuron has biased activation values if the standard deviation of those values is much smaller than the absolute mean . The output bias as defined by the Q over S ratio is a way to average the bias across neurons in the output layer . The quantity can be written as \\sqrt { E_j , x [ f ( x , j ) ^2 ] / ( E_j , x [ f ( x , j ) ^2 ] - E_j [ [ E_x x ( i ) ] ^2 ] ) } . * * * Failure cases for correlation information The problem with correlation information is that it is susceptible to adding and removing constants . Consider a simple example : performing k-means on a dataset is equivalent to performing k-means on that dataset plus a large constant . Adding the constant destroys correlation information , but does not fundamentally alter the quality of the representation , at least as long as the constant is not comparable in size to the largest representable floating point number . Hence , any bias in the input that is removed by the network confounds correlation information . Batch normalization is just one way to do this . One can also simply initialize the trainable bias in the first layer to eliminate the bias of the dataset . Similarly , if the network corrupts correlation information by introducing bias , we can for example use an error function ( instead of vanila softmax-cross entropy ) that compares the output minus the bias to the labels instead of the output itself . Or we could add the same bias to the labels and use an L2 error function , for example . We agree that the paper that introduced correlation information possibly did not intend to deal with batchnorm . We do not call into question the validity of their results , but simply point out limitations of the metric . * * * Representational benefits of depth Admittedly , I am not a huge expert on the literature on representational benefits of depth . Since this is not a core topic for the paper , we hope that including 7 citations is sufficient . However , if there are other papers on depth that you think should be cited , please let us know . Thanks"}], "0": {"review_id": "BkeK-nRcFX-0", "review_text": "I do not understand the denomination of nonlinearity coefficient provided in definition 1: although the quantity indeed does equal to 1 under whitened data distribution or orthogonal matrix, the conjecture that it should be close to 1 does not seem to be close at all just under any data distribution. Using a similar construction that section 6, we can rescale a whitened input data with a diagonal matrix D with components all equal to one except for a very large one \\lambda and also multiply the input weights by D^{-1} to compensate (and have a similar function). If you look at such construction for the linear case with identity initialization of A, the NLC is sqrt((\\lambda^2 + n - 1) (\\lambda^{-2} + n - 1)) / n which can grow arbitrarily large with \\lambda *for a linear model*. However, because of its low capacity, we would expect a linear model to have reasonable generalization. This seems to compromise the initial NLC being low as a necessary condition for reasonable generalization. Conversely, it\u2019s possible to initialize arbitrarily large residual networks such that the resulting initial function is linear (by initializing the output weight of the incrementing block to 0). This initialization may also be done such that the initial NLC becomes close to 1. I would not think this wouldn\u2019t necessarily result in good generalization, which seems to agree with the experimental observation. Now given that this initial NLC is neither sufficient nor necessary to predict generalization, one can wonder what is correlating generalization and NLC together in the experiment section. Same remark applies to the correlation between nonlinearity and NLC. This is especially concerning since in the linear case, the NLC can vary whether we chose to whiten the data or not for example, so the other influencing factors need to be discovered. What were the architecture that resulted in small/high NLC? The experiment section still contains interesting bits, such as successful training of very deep architecture that are very sensitive to input perturbations but they are not part of the main thread of the paper.", "rating": "4: Ok but not good enough - rejection", "reply_text": "`` one can wonder what is correlating generalization and NLC together in the experiment section . Same remark applies to the correlation between nonlinearity and NLC . '' NLC is a measure of nonlinearity that is based on certain assumptions as explained in sections 3 / A . Figure 1 shows that those assumptions hold in practical networks . Figure 2 shows that good / bad performance is strongly associated with nonlinearity . At least one reason for this relationship is given in figure 3E , where we show that NLC is related to sensitivity of the output to small input changes . `` What were the architecture that resulted in small/high NLC ? '' The magnitude of the NLC is chiefly dependent on the linear approximability of nonlinearities , as explained in section 5 . Unfortunately , an in-depth discussion on why certain architectures have a certain NLC , for a large number of architectures , goes beyond the scope of the paper . Thank you and we look forward to your response ."}, "1": {"review_id": "BkeK-nRcFX-1", "review_text": "In this paper the authors introduce a new quantity, the nonlinearity coefficient, and argue that its value at initialization is a useful predictor of test time performance for neural networks. The authors conduct a wide range of experiments over many different network architectures and activation functions to corroborate these results. The authors then extend their method to compute the local nonlinearity of activation functions instead. I am a bit torn on this paper. I appreciate the direction that the authors have chosen to pursue. The topic of identifying parameters that are predictive of trainability is certainly interesting and has the potential to be quite impactful. Moreover, the breadth of the experiments conducted by the authors is novel and significant. Finally, I find the the overall manner in which the authors have chosen to present their data refreshingly transparent. Together, this leads me to believe that the quantity proposed by the authors might be useful to researchers. Having said that, I am concerned by the author\u2019s exposition of the nonlinearity coefficient itself. Fundamentally, my concern stems from the fact that it seems a lot of relatively ad-hoc decisions were made in the construction of the nonlinearity coefficient and an insufficiently good job was done to compare it to other measures of nonlinearity. Specifically, it feels like an extremely weak definition of nonlinearity to say that the linear approximation of a function fails when it produces values that lie outside of the co-domain of the function. Moreover, I feel as though there is already a well defined notion of nonlinearity at a point that could be constructed by reference to the Hessian (or generally by the approximation error induced by truncating the Taylor series after the linear term). I would like to see some comparison between these two methods. This is made more troubling given that the correlation found by the authors is present but does not seem especially strong. For example, in fig. 2A it seems like the nonlinearity coefficient varies by at least two orders of magnitude in the inset of the figure where the test accuracy really does not seem sensitive to its value. Prior work (for example, [1] from last years ICLR) has shown strong correlations between the Frobenius norm of the Jacobian and test error (see fig. 5 and fig. 6). Since the definition of the nonlinearity coefficient seems somewhat ad-hoc I would love to see a comparison between it and just looking at the Jacobian norm in terms of predicting test accuracy. [1] - SENSITIVITY AND GENERALIZATION IN NEURAL NETWORKS: AN EMPIRICAL STUDY Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein ", "rating": "5: Marginally below acceptance threshold", "reply_text": "# # # The method of Novak et al # # # Our paper goes significantly beyond the scope of Novak et al , because we use the NLC computed * before * training to predict performance * after * training . Novak et al use the Jacobian * after * training to compare against performance * after * training . Predicting performance before training is much more useful because it enables architecture design / selection . Furthermore , it is also much harder . We predict the property ( test error ) of one network ( trained network ) by examining a property ( NLC ) of a different network ( untrained network ) . Novak et al make inference about the property of a network ( test error ) from properties of that same network . Let us detail just one reason why our task is harder . The Novak et al paper uses the Frobenius norm of the Jacobian of the softmax units with respect to the input and compares that value to test error . We can write that Jacobian as d softmax/d input , which is the same as d softmax/d logits * d logits/d input , where 'logits ' denotes the values that are fed into the softmax . Now it turns out that ||d softmax/d logits||_F tends to be smaller for a given input when the prediction of the network is correct for that input , and it tends to be larger when the prediction of the network is incorrect . This is shown in the Novak paper in figure 6 . Therefore ||d softmax/d logits||_F is strongly correlated with error not because of an interesting structural property of a network , but simply because of an idiosyncratic property of the softmax : it tends to have larger gradients for less confident predictions . Hence , it is likely that the correlation between d softmax/d logits = d softmax/d logits * d logits/d input and test error is also caused to a significant degree by this effect . By computing the NLC before training , we do not `` benefit '' from this spurious signal . Therefore , our task is not comparable to the task studied by Novak et al , and hence the raw correlation numbers are also not comparable . We would argue that if you consider the Novak paper to be an important contribution , our paper is at least an equally important contribution , because we study a task that is at least in certain ways significantly more useful . # # # Summary # # # The NLC is the first gradient-based metric that , when computed before traning , has been shown to be predictive of test error after training through a large-scale study involving a wide variety of networks . Additional benefits include : - it is an accurate measure of nonlinearity * in practice * ( figure 1 ) - it is intimiately related to the linear approximability of activation functions ( section 5 ) - it is more robust to confounders than comparable metrics ( section 6 ) - it is cheap to compute ( section G ) However , we do not claim that the NLC is the * correct * measure of nonlinearity for deep neural networks . We are happy to use heuristics in deriving the NLC as long as we attain the above benefits . In response to your criticism that `` it feels like an extremely weak definition of nonlinearity to say that the linear approximation of a function fails when it produces values that lie outside of the co-domain of the function . '' we would respond that our goal is not to define nonlinearity definitively , but to come up with a metric that has the benefits outlined above . Nonetheless , we think that our figure 1 and the shortcomings of the Hessian indicate that the NLC is the state-of-the-art in neural network nonlinearity estimation , and we think that the NLC as a performance predictor is better motivated than e.g . 'gradient explosion ' or 'correlation preservation ' , as well as the metric used by Novak et al.Finally , we strongly disagree with the statement that `` in fig . 2A it seems like the nonlinearity coefficient varies by at least two orders of magnitude in the inset of the figure where the test accuracy really does not seem sensitive to its value . '' See the correlation numbers and p-values below . While the correlation in the inset is a bit lower , this is to be expected as the correlation between random variables tends to decrease if the range of one variable is restricted . Scenario correlation p-value CIFAR10 0.72 2.34e-41 CIFAR10 ( inset ) 0.68 1.88e-21 MNIST 0.81 1.31e-57 MNIST ( inset ) 0.63 3.60e-19 wave 0.67 1.82e-33 wave ( inset ) 0.57 3.60e-13 We thank you again for your review . We would love to discuss further and look forward to your response . Please let us know whether you want us to include the above discussions in the next revision of the manuscript or not ."}, "2": {"review_id": "BkeK-nRcFX-2", "review_text": "This paper proposes a metric to measure the \"nonlinearity\" of neural network, and presents evidence that the value of this metric at initialization time is predictive of generalization performance. Apart from a few problems I think this paper is well written and thorough. The contribution is solid, although not earth shattering given previous work on such metrics. There seems to be a basic error in some of the early math, although I don't think this will qualitatively affect the results in any significant way. ----------------- Detailed comments by section: ------------------ Section 3: It seems like a 1/sqrt(d) factor is missing from these Q_i(S_x x(i)) and Q_j(S_x f(x,j)) formulas. As far as I can tell this doesn't affect Def 1 because you seemed to use the correct formula there. However, the rewritten version with the traces doesn't seem to be correct. There should be a d_in factor in the denominator (inside the square root). This error seems unrelated to the other one. Assuming I'm correct and that this is an error, does this affect your results in the various figures? And what is the actual final definition of NLC that you used? In general, it's annoying for the reader to verify that all of these forms are equivalent. And it's fiddly enough with the sqrt(d) terms constantly disappearing and reappearing in the numerator and denominators that even you made multiple errors (as far as I can tell). I would suggest making this section more rigorous and writing out everything carefully. And you probably don't need to rewrite it in so many equivalent forms with different notation unless they are useful somehow. The use of the Q and S symbols feels superfluous and counterproductive. Standard notation with expectations and squares wouldn't take much more space and would be a lot clearer. Section 4: \"we plot the relative diameter of the linearly approximable regions of the network as defined in section 3\": but you don't seem to define \"relative diameter\" there. As far as I can tell it's only defined in Appendix E, and this is only mentioned in the caption of figure 1. It's impossible to interpret this result without knowing precisely what \"relative diameter\" is. If you can't afford to describe this in the main paper you should at least mention that it's a different (more expensive) way of estimating the same thing that the NLC estimates. In Figure 2, are the higher test errors due to the optimizer failing to lower the training error, or due to a greater generalization gap? I guess the Figure 3 results suggest the latter possibility, which is surprising to me. What does it mean to have a \"very biased output\". What does that inequality mean intuitively? Should there be an absolute value on the RHS? It would be much easier to parse it if it were written in plain notation without these S and Q symbols. Section 6: \"metric also an\" -> \"metric also has an\" Can you generate a failure case for \"correlation information\" that doesn't involve Batch Norm layers? I don't think the authors of those works meant for their results to deal with that. Note that there are actually a lot of papers going back to the 90s that discussed and proved representational benefits of depth in neural networks.", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 1 , Thank you for your review . We address your detailed comments below . It seems that your main criticism which prevented our paper from attaining a higher rating was your assessment that `` The contribution is solid , although not earth shattering given previous work on such metrics . '' I would love to know more detail regarding this statement . We believe that many properties demonstrated for the NLC throughout the paper ( e.g.predictiveness of error when computed before training across a wide range of architectures , predictiveness of nonlinearity , robustness to confounders , relationship to linear approximability of activation functions ) are either novel compared to other metrics or at least have not been demonstrated for them . If you are aware of prior work that contradicts those beliefs , we would love to know . ... * * * missing sqrt ( d ) factor You are absolutely correct . Thank you for pointing this out . For what it 's worth , we noticed this problem very soon after we submitted the paper and posted a correction on openreview . You can check our comment `` typo found '' , posted on October 3rd , below . We understand that it is annoying to see conflicting definitions , and we apologize for this . The highlighted `` Definition 1 '' at the top of page 3 is correct . We added the alternative definition using traces at the last moment , thereby producing typos . The reason for including the 'correct ' d_in / d_out factors in the definition is to avoid susceptibility of the NLC to changes in input dimension / output dimension that do not affect nonlinearity / performance . In the revision we just uploaded , we have fixed the d_in / d_out typos . If you think we should still also remove the Q/S notation , please let us know and we will upload an additional revision . * * * relative diameter We added a reference to section E.1 in the main text , where the formal definition of relative diameter is diven . The informal meaning is discussed in section 3 . * * * test error vs training error Yes , the high test error is mostly caused by bad generalization , at least on the waveform-noise dataset . Please see section B.1 for further details on this . * * * Biased output An individual neuron has biased activation values if the standard deviation of those values is much smaller than the absolute mean . The output bias as defined by the Q over S ratio is a way to average the bias across neurons in the output layer . The quantity can be written as \\sqrt { E_j , x [ f ( x , j ) ^2 ] / ( E_j , x [ f ( x , j ) ^2 ] - E_j [ [ E_x x ( i ) ] ^2 ] ) } . * * * Failure cases for correlation information The problem with correlation information is that it is susceptible to adding and removing constants . Consider a simple example : performing k-means on a dataset is equivalent to performing k-means on that dataset plus a large constant . Adding the constant destroys correlation information , but does not fundamentally alter the quality of the representation , at least as long as the constant is not comparable in size to the largest representable floating point number . Hence , any bias in the input that is removed by the network confounds correlation information . Batch normalization is just one way to do this . One can also simply initialize the trainable bias in the first layer to eliminate the bias of the dataset . Similarly , if the network corrupts correlation information by introducing bias , we can for example use an error function ( instead of vanila softmax-cross entropy ) that compares the output minus the bias to the labels instead of the output itself . Or we could add the same bias to the labels and use an L2 error function , for example . We agree that the paper that introduced correlation information possibly did not intend to deal with batchnorm . We do not call into question the validity of their results , but simply point out limitations of the metric . * * * Representational benefits of depth Admittedly , I am not a huge expert on the literature on representational benefits of depth . Since this is not a core topic for the paper , we hope that including 7 citations is sufficient . However , if there are other papers on depth that you think should be cited , please let us know . Thanks"}}