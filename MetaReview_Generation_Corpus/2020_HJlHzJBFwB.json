{"year": "2020", "forum": "HJlHzJBFwB", "title": "Accelerating Monte Carlo Bayesian Inference via Approximating Predictive Uncertainty over the Simplex", "decision": "Reject", "meta_review": "This paper proposes to speed up Bayesian deep learning at test time by training a student network to approximate the BNN's output distribution. The idea is certainly a reasonable thing to try, and the writing is mostly good (though as some reviewers point out, certain sections might not be necessary). The idea is fairly obvious, though, so the question is whether the experimental results are impressive enough by themselves to justify acceptance. The method is able to get close to the performance achieved by Monte Carlo estimators with much lower cost, although there is a nontrivial drop in accuracy. This is probably worth paying if it achieves 500x computation reduction as claimed in the paper, though the practical gains are probably much smaller since Monte Carlo methods are rarely used with 500 samples. Overall, this seems a bit below the bar for ICLR.\n", "reviews": [{"review_id": "HJlHzJBFwB-0", "review_text": "Thank the authors for your detailed rebuttal. I agree with the authors that the proposed method acts as a useful tool for \"real-time evaluation of induced predictive uncertainty\", and the experiments also validate that the method indeed achieves comparable performance with smaller computations. But for now, I am inclined to not change my score. ################### Bayesian models maintain the posterior distribution for predictions, which might bring up big computational costs of multiple forwards or big memory costs of multiple particles. To resolve the computational and memory issues at predictions, this paper proposes to distill Bayesian models into an amortized prediction model, avoiding the original multiple forwards. Specifically, in classification, they distill the predictive probabilities into an amortized Dirichlet distribution. They evaluated different distillation metrics, including KL divergence, Earth moving distance, and Maximum mean discrepancy. Empirically, they evaluate the proposed method over out-of-distribution detection. They demonstrate that their method achieves comparable performance with much speedup. Strengths, 1, This paper is well-written and the ideas are well-presented. They evaluated the proposed method over different Bayesian models (MCDP & SGLD) as well different metrics (KL, EMD, MMD), and demonstrate the effectiveness of their method. Overall, this paper is very comprehensive. 2, As evaluated and validated in the experiments, the proposed method vastly reduces the inference time at test phase. Weakness, 1, The paper kind of lacks of novelty. Basically the proposed method distills a Bayesian models into an amortized Dirichlet distribution, which is straightforward. 2, The baselines such as MCDP-KL, MCDP-EMD are strange, it is wired why you would distill the predictive distribution of a single point to a Dirichlet distribution. And I think it is probably unfair, as distilling the single-point distribution to the Dirichlet under KL, EMD, MMD might require large amount of particles, which they don't have. 3, Related to (2), more baselines should be compared with to better demonstrate the method's effectiveness. 1) performance of the un-distilled MCDP and SGLD models. 2) BDK and DPN for the MCDP models. 3) MCDP and SGLD with fewer particles. The paper claims to achieve 500x speed up, while I reckon the performance of MCDP and SGLD won't deteriorate a lot if you use only fewer particles. 4, It would be interesting to see experiments other than out-of-distribution detection, such as calibration. Minor Issues, 1, The paper has several un-complied references, such as above eq(4) and appendix D. 2, The \\Tau(x | \\theta) in Figure 1 is a typo. 3, Assumption 1 should be put forward to the main articles for comprehensiveness of Lemma 1.", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the comments and would like to answer the questions as follows : Q1 : Lacks of novelty and straight forward . This paper proposes a framework that solves the practical problem of real-time evaluation of induced predictive uncertainty . Different from previous knowledge distillation [ 1,2 ] , we provide a new view of induced distribution \\pi which isolates the dependence between y and x , as shown by the graphic model in Fig.3 ( b ) in the Appendix . The \u201c isolation view \u201d is meaningful not only in classification , but also in all applications where predictive uncertainty are required , e.g. , image object detection and segmentation . The \u201c isolation view \u201d also enables a richer characterization of student model ( all previous works use a simple categorical distribution ) . As this kind of distillation is an unexplored problem , different evaluation metrics are considered and adapted to our framework . We also propose use the unamortized version to study how amortization affects the approximation , which decomposes the total approximation error into model error and amortization gap ( Eq.7 , Page 5 ) . ( see the new experimental results on EMNIST below ) The local amortization gap can be used as an evaluation metric for amortized approximation . This also appears to be novel to the literature . The student distribution does not necessarily need to be a Dirichlet . The framework allows to use various choices of student distribution , and the algorithms based on KL , EMD and MMD as well as the analysis still apply . More results on EMNIST OPU-MCDP-MMD : Average approximation error [ Eq.7 on page 5 ] : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( q_ { \\mathbf { x } _i } , p_ { \\mathbf { x } _i } ) $ . The averaged MMD between teacher \u2019 s particles ( for each x ) and the predicted Dirichlet by OPU : 6.5 * 10^ ( -2 ) . Average model error [ Eq.7 on page 5 ] : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( p_ { \\mathbf { x } _i } , \\bar { q } _ { \\mathbf { x } _i } ^\\ast ) $ . The averaged MMD between teacher \u2019 s particles and locally fitted Dirichlet : 6.01 * 10^ ( -2 ) . Average local amortization error : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( q_ { \\mathbf { x } _i } , \\bar { q } _ { \\mathbf { x } _i } ^\\ast ) $ . The averaged MMD between locally fitted Dirichlet ( for each x ) and the predicted Dirichlet by OPU : 5.3 * 10^ ( -3 ) . Note that the \u201c error \u201d is different from \u201c amortization gap \u201d \u0394 ( x ) defined in the paper [ Sec.2.4 on page 5 ] . The relationship between \u201c amortization error \u201d defined here and \u201c amortization gap \u201d is given by Eq.8 [ Sec 2.4 on page 5 ] . It can be observed that the local amortization error 5.3 * 10^ ( -3 ) is low and it bounds the local amortization gap \u0394 ( x ) = Avg.Apx.Err \u2013 Avg.Mdl.Err = 4.9 * 10^ ( -3 ) , which is consistent with Eq.8 [ Sec 2.4 on page 5 ] . The approximation error is mainly determined by the model error , which turns out to be acceptably small . This is consistent with the analysis and also shows the effectiveness and suitableness of using Dirichlet family . Q2 : The `` single-point '' baselines are strange . We argue that this baseline is fair . Let the \u2018 single-point distribution \u2019 be understood as the \u2018 local \u2019 distribution for each input x we have defined , either local induced conditional distribution or local Dirichlet approximation . This baseline is consistent with the analysis , where the approximation loss equals the model loss and the amortization loss is zero , which should be the best performance OPU can achieve ( within Dirichlet family ) theoretically . When training OPU , we first extract 700 posterior samples from the pretrained MCDP/ SGLD model . ( for fairness , the baselines and OPU use the same set of posterior samples ) For each input x , this induces 700 particles over the simplex for OPU to approximate . When training each local distribution , for each x , a Dirichlet with a k-dim ( k=10 for MNIST and Cifar10 , k=47 for EMNIST ) vector parameter is fitted on the 700 particles induced by the 700 posterior samples , which is enough particles to learn the Dirichlet well . The vector is also disentangled into a probability vector and a concentration scalar to be consistent , with no neural network parameterizing them ( no amortization ) . References : [ 1 ] Bayesian dark knowledge . Advances in Neural Information Processing Systems 28 , pp . 3438\u20133446 . [ 2 ] Distilling the knowledge in a neural network . In NIPS Deep Learning and Representation Learning Workshop , 2015 ."}, {"review_id": "HJlHzJBFwB-1", "review_text": "Overall I liked several results presented in this paper. The findings in Figure 2 gives clear illustration on how Bayesian classification models distinguish between in-distribution difficult-to-classify data and out-of-distribution data, namely uncertain predicted mean and large predicted variance. Though I believe this eventually depends on what kind of \"kernel\"s are used to correlate data points in the prior, throughout the paper I assume meaningful \"kernel\"s are used (for Bayesian NNs this is rooted in the inductive bias of neural networks). Another result that I liked is in experiments we can clearly see the advantage of considering the bayesian predictive distribution over a single predictive mean. As demonstrated by BDK-SGLD vs. BDK-DIR-SGLD. The proposed idea is a simple and meaningful improvement over previous works. Though the contribution is quite limited, the authors present it with great clarity, which I appreciated. However, I do find some discussions in the paper unnecessary and would expect for more technical contributions. For example, I didn't see the argument for the whole section discussing amortization gap. Everything seems straightforward given the hypothesis F is of enough capacity, which obviously does not hold in practice. Many other concerns are summarized below: * What if the teacher predictive distribution is far unlike a Dirichlet? How much is the discrepancy between teacher and student predictive distribution? Theoretical or empirical evidence is needed for this modeling choice. * One importance advantage of Bayesian classification models is that they can capture the covariance between predictions of different data points. By amortization this advantage no longer exists. * In the paper the authors keep mentioning that the method can be applied to GPs but I don't see experiments or algorithms for it? * The concentration model is parameterized using an exponential activation, how does this activation affect the performance? * The distilling process is done on a held-out dataset. Which may not be wanted because an advantage of Bayesian classification models (eg. GPs) is that all hyperparameters can be automatically selected by marginal likelihoods and don't need a held-out validation set. * MMD/wasserstein distances are cool but they require also samples from the student, which adds more variance to the distillation process. * The experiment setup is extremely unclear to me. What is \"uncertainty measures\", are they used as metrics for detecting out-of-distribution data, how are AUROC/AUPR calculated using the uncertainty measures? I can guess the meaning but the paper should be more clear about this. * I found most numbers convincing except that sometimes BDK-SGLD outperforms BDK-DIR-SGLD, if I understand it right, the predicted mean of BDK-DIR-SGLD should be as good as BDK-SGLD? Minor: * On page 4, above Eq. (4) there is a broken figure link. * On Page 7, \"To save space, we only present the best performing uncertainty measure (E, P or C)\". What is \"C\" here?", "rating": "3: Weak Reject", "reply_text": "Review # 4 We thank the reviewer for the comments and would like to answer the questions as follows : Q1 : However , I do find some discussions in the paper unnecessary and would expect for more technical contributions . For example , I did n't see the argument for the whole section discussing amortization gap . Everything seems straightforward given the hypothesis F is of enough capacity , which obviously does not hold in practice . To understand the amortized approximation problem better , we show the total approximation error can be decomposed into model error and amortization gap , and that the amortization gap can be reduced to zero given enough capacity . The analysis aims to show that the idea of \u201c amortization \u201d is appropriate in our particular scenario . However , in our application , a strong model doesn \u2019 t always translate to small approximation error . For example , consider inference gap in VAE [ 1 ] . The local amortization gap is also useful as a general evaluation metric in the amortized knowledge distillation problems . The assumption of F having enough capacity is an assumption also used in the analyses of GAN and WGAN [ 2,3 ] . In the experiment ( Sec.4.2 on page 7 ) , we show that the amortized student model approximates each local distribution ( that with only model error ) in high-fidelity , indicating a low amortization loss . We add an experiment on the EMNIST dataset to verify the decomposition of total approximation error empirically . More results on EMNIST OPU-MCDP-MMD : Average approximation error [ Eq.7 on page 5 ] : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( q_ { \\mathbf { x } _i } , p_ { \\mathbf { x } _i } ) $ . The averaged MMD between teacher \u2019 s particles ( for each x ) and the predicted Dirichlet by OPU : 6.5 * 10^ ( -2 ) . Average model error [ Eq.7 on page 5 ] : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( p_ { \\mathbf { x } _i } , \\bar { q } _ { \\mathbf { x } _i } ^\\ast ) $ . The averaged MMD between teacher \u2019 s particles and locally fitted Dirichlet : 6.01 * 10^ ( -2 ) . Average local amortization error : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( q_ { \\mathbf { x } _i } , \\bar { q } _ { \\mathbf { x } _i } ^\\ast ) $ . The averaged MMD between locally fitted Dirichlet ( for each x ) and the predicted Dirichlet by OPU : 5.3 * 10^ ( -3 ) . Note that the \u201c error \u201d is different from \u201c amortization gap \u201d \u0394 ( x ) defined in the paper [ Sec.2.4 on page 5 ] . The relationship between \u201c amortization error \u201d defined here and \u201c amortization gap \u201d is given by Eq.8 [ Sec 2.4 on page 5 ] . It can be observed that the local amortization error is low , even with a student model having limited capacity capacity . This effectively shows that the function space considered is enough to cover the essential target , which is in fact \u201c of enough capacity \u201d . The local amortization error 5.3 * 10^ ( -3 ) bounds the local amortization gap \u0394 ( x ) = Avg.Apx.Err \u2013 Avg.Mdl.Err = 4.9 * 10^ ( -3 ) , which is consistent with Eq.8 [ Sec 2.4 on page 5 ] . The approximation error is mainly determined by the model error , which turns out to be acceptably small . This is consistent with the analysis and also shows the effectiveness and suitableness of using Dirichlet family . Q2 : What if the teacher predictive distribution is far unlike a Dirichlet ? How much is the discrepancy between teacher and student predictive distribution ? Theoretical or empirical evidence is needed for this modeling choice . Using a Dirichlet for the student is modeling choice , similar to assuming Gaussian posteriors for variational approximations . Our framework is general and any student distribution can be adopted , e.g. , generalized Dirichlet , mixture of Dirichlets . To do this requires : 1 ) a suitable parametrization of the model that can capture uncertainty ; 2 ) deriving/computing the approximation loss ( KL , MMD , EMD ) ; 3 ) reparameterization trick of expectations for efficient gradient estimation . The algorithms of KL , EMD and MMD as well as the analysis still apply . EMNIST is a more challenging dataset compared with Pima , Spambase , MNIST and Cifar10 as the number of classes is larger ( 47 ) . The empirical evidence for the choice of Dirichlet is in this experiment , where we obtain nearly the same performance ( accuracy ) as using particles , while obtaining better OOD performance . We also show that the approximation error is low ( see Q1 ) , and thus the Dirichlet is a good fit for the teacher \u2019 s predictive distribution in this case . References : [ 1 ] Kingma , Diederik P. , and Max Welling . `` Auto-encoding variational bayes . '' arXiv preprint arXiv:1312.6114 ( 2013 ) . [ 2 ] Goodfellow , Ian , et al . `` Generative adversarial nets . '' Advances in neural information processing systems . 2014 . [ 3 ] Arjovsky , Martin , Soumith Chintala , and L\u00e9on Bottou . `` Wasserstein generative adversarial networks . '' International conference on machine learning . 2017 ."}, {"review_id": "HJlHzJBFwB-2", "review_text": "This paper studies the problem of avoiding Monte Carlo (MC) estimate for the predictive distribution during the test for Bayesian methods. MC estimate will incur multiple passes where the number of passes depends on the number of samples and therefore the cost can be huge. The authors propose One-Pass Uncertainty (OPU) methods to approximate the predictive distribution through distillation. Experiments on Bayesian neural networks are conducted to demonstrate the proposed method. Quality: The proposed method appears to be technically sound. The view of approximating the predictive distribution over simplex is interesting and may inspire future studies under this formulation. Although the restriction of the student distribution to be tractable seems to limit the design of the student model significantly. And this restrictive distribution family may cause large amortization error, as suggested by Lemma 1 in the paper. The experiments are well-conducted, and the proposed method is well-evaluated. Significance: This paper studies an important problem in Bayesian machine learning and the proposed method can be combined with many Bayesian methods to reduce the computational cost during the test. Originality: As far as I know, the method is novel. The related work is adequately cited. Clarity: This paper is well-written and easy to follow. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the comments and would like to answer the questions as follows : Q1 : Although the restriction of the student distribution to be tractable seems to limit the design of the student model significantly . And this restrictive distribution family may cause large amortization error , as suggested by Lemma 1 in the paper . The restrictive distribution family causes \u201c large \u201d total approximation error if the ground truth induced distribution is considerably different from a Dirichlet . However the amortization error is low and depends on capacity of neural network used to construct the approximation [ point 2 in Para 1 . Sec 2.2 on page 3 ] . We show the three types of errors with the experimental results on EMNIST . Results on EMNIST OPU-MCDP-MMD : Average approximation error [ Eq.7 on page 5 ] : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( q_ { \\mathbf { x } _i } , p_ { \\mathbf { x } _i } ) $ . The averaged MMD between teacher \u2019 s particles ( for each x ) and the predicted Dirichlet by OPU : 6.5 * 10^ ( -2 ) . Average model error [ Eq.7 on page 5 ] : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( p_ { \\mathbf { x } _i } , \\bar { q } _ { \\mathbf { x } _i } ^\\ast ) $ . The averaged MMD between teacher \u2019 s particles and locally fitted Dirichlet : 6.01 * 10^ ( -2 ) . Average local amortization error : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( q_ { \\mathbf { x } _i } , \\bar { q } _ { \\mathbf { x } _i } ^\\ast ) $ . The averaged MMD between locally fitted Dirichlet ( for each x ) and the predicted Dirichlet by OPU : 5.3 * 10^ ( -3 ) . Note that the \u201c error \u201d is different from \u201c amortization gap \u201d \u0394 ( x ) defined in the paper [ Sec.2.4 on page 5 ] . The relationship between \u201c amortization error \u201d defined here and \u201c amortization gap \u201d is given by Eq.8 [ Sec 2.4 on page 5 ] . It can be observed that the local amortization error 5.3 * 10^ ( -3 ) is low and it bounds the local amortization gap \u0394 ( x ) = Avg.Approx.Err \u2013 Avg.Model.Err = 4.9 * 10^ ( -3 ) , which is consistent with Eq.8 [ Sec 2.4 on page 5 ] . The approximation error is mainly determined by the model error , which turns out to be acceptably small . This is consistent with the analysis and also shows the effectiveness and suitableness of using Dirichlet family . More experiments on MCDP-Cifar10 . ( Code available via the code link ) Method || MisC . AUROC|| MisC . AUPR || OOD . AUROC || OOD . AUPR || Acc MCDP-KL : || 92.2 ( P ) || 47.0 ( P ) || 90.5 ( E ) || 88.7 ( E ) || 92.4 MCDP-EMD : || 92.2 ( P ) || 47.0 ( P ) || 91.4 ( D ) || 89.1 ( D ) || 92.4 MCDP-MMD : || 92.2 ( P ) || 47.0 ( P ) || 91.0 ( D ) || 89.3 ( D ) || 92.4 OPU-MCDP-KL : || 87.2 ( P ) || 45.9 ( P ) || 86.1 ( E ) || 85.5 ( E ) || 89.9 OPU-MCDP-EMD : || 91.8 ( E ) || 46.9 ( P ) || 93.5 ( C ) || 92.0 ( C ) || 91.8 OPU-MCDP-MMD : || 91.3 ( E ) || 46.6 ( P ) || 92.9 ( C ) || 91.7 ( C ) || 91.8"}], "0": {"review_id": "HJlHzJBFwB-0", "review_text": "Thank the authors for your detailed rebuttal. I agree with the authors that the proposed method acts as a useful tool for \"real-time evaluation of induced predictive uncertainty\", and the experiments also validate that the method indeed achieves comparable performance with smaller computations. But for now, I am inclined to not change my score. ################### Bayesian models maintain the posterior distribution for predictions, which might bring up big computational costs of multiple forwards or big memory costs of multiple particles. To resolve the computational and memory issues at predictions, this paper proposes to distill Bayesian models into an amortized prediction model, avoiding the original multiple forwards. Specifically, in classification, they distill the predictive probabilities into an amortized Dirichlet distribution. They evaluated different distillation metrics, including KL divergence, Earth moving distance, and Maximum mean discrepancy. Empirically, they evaluate the proposed method over out-of-distribution detection. They demonstrate that their method achieves comparable performance with much speedup. Strengths, 1, This paper is well-written and the ideas are well-presented. They evaluated the proposed method over different Bayesian models (MCDP & SGLD) as well different metrics (KL, EMD, MMD), and demonstrate the effectiveness of their method. Overall, this paper is very comprehensive. 2, As evaluated and validated in the experiments, the proposed method vastly reduces the inference time at test phase. Weakness, 1, The paper kind of lacks of novelty. Basically the proposed method distills a Bayesian models into an amortized Dirichlet distribution, which is straightforward. 2, The baselines such as MCDP-KL, MCDP-EMD are strange, it is wired why you would distill the predictive distribution of a single point to a Dirichlet distribution. And I think it is probably unfair, as distilling the single-point distribution to the Dirichlet under KL, EMD, MMD might require large amount of particles, which they don't have. 3, Related to (2), more baselines should be compared with to better demonstrate the method's effectiveness. 1) performance of the un-distilled MCDP and SGLD models. 2) BDK and DPN for the MCDP models. 3) MCDP and SGLD with fewer particles. The paper claims to achieve 500x speed up, while I reckon the performance of MCDP and SGLD won't deteriorate a lot if you use only fewer particles. 4, It would be interesting to see experiments other than out-of-distribution detection, such as calibration. Minor Issues, 1, The paper has several un-complied references, such as above eq(4) and appendix D. 2, The \\Tau(x | \\theta) in Figure 1 is a typo. 3, Assumption 1 should be put forward to the main articles for comprehensiveness of Lemma 1.", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the comments and would like to answer the questions as follows : Q1 : Lacks of novelty and straight forward . This paper proposes a framework that solves the practical problem of real-time evaluation of induced predictive uncertainty . Different from previous knowledge distillation [ 1,2 ] , we provide a new view of induced distribution \\pi which isolates the dependence between y and x , as shown by the graphic model in Fig.3 ( b ) in the Appendix . The \u201c isolation view \u201d is meaningful not only in classification , but also in all applications where predictive uncertainty are required , e.g. , image object detection and segmentation . The \u201c isolation view \u201d also enables a richer characterization of student model ( all previous works use a simple categorical distribution ) . As this kind of distillation is an unexplored problem , different evaluation metrics are considered and adapted to our framework . We also propose use the unamortized version to study how amortization affects the approximation , which decomposes the total approximation error into model error and amortization gap ( Eq.7 , Page 5 ) . ( see the new experimental results on EMNIST below ) The local amortization gap can be used as an evaluation metric for amortized approximation . This also appears to be novel to the literature . The student distribution does not necessarily need to be a Dirichlet . The framework allows to use various choices of student distribution , and the algorithms based on KL , EMD and MMD as well as the analysis still apply . More results on EMNIST OPU-MCDP-MMD : Average approximation error [ Eq.7 on page 5 ] : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( q_ { \\mathbf { x } _i } , p_ { \\mathbf { x } _i } ) $ . The averaged MMD between teacher \u2019 s particles ( for each x ) and the predicted Dirichlet by OPU : 6.5 * 10^ ( -2 ) . Average model error [ Eq.7 on page 5 ] : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( p_ { \\mathbf { x } _i } , \\bar { q } _ { \\mathbf { x } _i } ^\\ast ) $ . The averaged MMD between teacher \u2019 s particles and locally fitted Dirichlet : 6.01 * 10^ ( -2 ) . Average local amortization error : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( q_ { \\mathbf { x } _i } , \\bar { q } _ { \\mathbf { x } _i } ^\\ast ) $ . The averaged MMD between locally fitted Dirichlet ( for each x ) and the predicted Dirichlet by OPU : 5.3 * 10^ ( -3 ) . Note that the \u201c error \u201d is different from \u201c amortization gap \u201d \u0394 ( x ) defined in the paper [ Sec.2.4 on page 5 ] . The relationship between \u201c amortization error \u201d defined here and \u201c amortization gap \u201d is given by Eq.8 [ Sec 2.4 on page 5 ] . It can be observed that the local amortization error 5.3 * 10^ ( -3 ) is low and it bounds the local amortization gap \u0394 ( x ) = Avg.Apx.Err \u2013 Avg.Mdl.Err = 4.9 * 10^ ( -3 ) , which is consistent with Eq.8 [ Sec 2.4 on page 5 ] . The approximation error is mainly determined by the model error , which turns out to be acceptably small . This is consistent with the analysis and also shows the effectiveness and suitableness of using Dirichlet family . Q2 : The `` single-point '' baselines are strange . We argue that this baseline is fair . Let the \u2018 single-point distribution \u2019 be understood as the \u2018 local \u2019 distribution for each input x we have defined , either local induced conditional distribution or local Dirichlet approximation . This baseline is consistent with the analysis , where the approximation loss equals the model loss and the amortization loss is zero , which should be the best performance OPU can achieve ( within Dirichlet family ) theoretically . When training OPU , we first extract 700 posterior samples from the pretrained MCDP/ SGLD model . ( for fairness , the baselines and OPU use the same set of posterior samples ) For each input x , this induces 700 particles over the simplex for OPU to approximate . When training each local distribution , for each x , a Dirichlet with a k-dim ( k=10 for MNIST and Cifar10 , k=47 for EMNIST ) vector parameter is fitted on the 700 particles induced by the 700 posterior samples , which is enough particles to learn the Dirichlet well . The vector is also disentangled into a probability vector and a concentration scalar to be consistent , with no neural network parameterizing them ( no amortization ) . References : [ 1 ] Bayesian dark knowledge . Advances in Neural Information Processing Systems 28 , pp . 3438\u20133446 . [ 2 ] Distilling the knowledge in a neural network . In NIPS Deep Learning and Representation Learning Workshop , 2015 ."}, "1": {"review_id": "HJlHzJBFwB-1", "review_text": "Overall I liked several results presented in this paper. The findings in Figure 2 gives clear illustration on how Bayesian classification models distinguish between in-distribution difficult-to-classify data and out-of-distribution data, namely uncertain predicted mean and large predicted variance. Though I believe this eventually depends on what kind of \"kernel\"s are used to correlate data points in the prior, throughout the paper I assume meaningful \"kernel\"s are used (for Bayesian NNs this is rooted in the inductive bias of neural networks). Another result that I liked is in experiments we can clearly see the advantage of considering the bayesian predictive distribution over a single predictive mean. As demonstrated by BDK-SGLD vs. BDK-DIR-SGLD. The proposed idea is a simple and meaningful improvement over previous works. Though the contribution is quite limited, the authors present it with great clarity, which I appreciated. However, I do find some discussions in the paper unnecessary and would expect for more technical contributions. For example, I didn't see the argument for the whole section discussing amortization gap. Everything seems straightforward given the hypothesis F is of enough capacity, which obviously does not hold in practice. Many other concerns are summarized below: * What if the teacher predictive distribution is far unlike a Dirichlet? How much is the discrepancy between teacher and student predictive distribution? Theoretical or empirical evidence is needed for this modeling choice. * One importance advantage of Bayesian classification models is that they can capture the covariance between predictions of different data points. By amortization this advantage no longer exists. * In the paper the authors keep mentioning that the method can be applied to GPs but I don't see experiments or algorithms for it? * The concentration model is parameterized using an exponential activation, how does this activation affect the performance? * The distilling process is done on a held-out dataset. Which may not be wanted because an advantage of Bayesian classification models (eg. GPs) is that all hyperparameters can be automatically selected by marginal likelihoods and don't need a held-out validation set. * MMD/wasserstein distances are cool but they require also samples from the student, which adds more variance to the distillation process. * The experiment setup is extremely unclear to me. What is \"uncertainty measures\", are they used as metrics for detecting out-of-distribution data, how are AUROC/AUPR calculated using the uncertainty measures? I can guess the meaning but the paper should be more clear about this. * I found most numbers convincing except that sometimes BDK-SGLD outperforms BDK-DIR-SGLD, if I understand it right, the predicted mean of BDK-DIR-SGLD should be as good as BDK-SGLD? Minor: * On page 4, above Eq. (4) there is a broken figure link. * On Page 7, \"To save space, we only present the best performing uncertainty measure (E, P or C)\". What is \"C\" here?", "rating": "3: Weak Reject", "reply_text": "Review # 4 We thank the reviewer for the comments and would like to answer the questions as follows : Q1 : However , I do find some discussions in the paper unnecessary and would expect for more technical contributions . For example , I did n't see the argument for the whole section discussing amortization gap . Everything seems straightforward given the hypothesis F is of enough capacity , which obviously does not hold in practice . To understand the amortized approximation problem better , we show the total approximation error can be decomposed into model error and amortization gap , and that the amortization gap can be reduced to zero given enough capacity . The analysis aims to show that the idea of \u201c amortization \u201d is appropriate in our particular scenario . However , in our application , a strong model doesn \u2019 t always translate to small approximation error . For example , consider inference gap in VAE [ 1 ] . The local amortization gap is also useful as a general evaluation metric in the amortized knowledge distillation problems . The assumption of F having enough capacity is an assumption also used in the analyses of GAN and WGAN [ 2,3 ] . In the experiment ( Sec.4.2 on page 7 ) , we show that the amortized student model approximates each local distribution ( that with only model error ) in high-fidelity , indicating a low amortization loss . We add an experiment on the EMNIST dataset to verify the decomposition of total approximation error empirically . More results on EMNIST OPU-MCDP-MMD : Average approximation error [ Eq.7 on page 5 ] : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( q_ { \\mathbf { x } _i } , p_ { \\mathbf { x } _i } ) $ . The averaged MMD between teacher \u2019 s particles ( for each x ) and the predicted Dirichlet by OPU : 6.5 * 10^ ( -2 ) . Average model error [ Eq.7 on page 5 ] : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( p_ { \\mathbf { x } _i } , \\bar { q } _ { \\mathbf { x } _i } ^\\ast ) $ . The averaged MMD between teacher \u2019 s particles and locally fitted Dirichlet : 6.01 * 10^ ( -2 ) . Average local amortization error : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( q_ { \\mathbf { x } _i } , \\bar { q } _ { \\mathbf { x } _i } ^\\ast ) $ . The averaged MMD between locally fitted Dirichlet ( for each x ) and the predicted Dirichlet by OPU : 5.3 * 10^ ( -3 ) . Note that the \u201c error \u201d is different from \u201c amortization gap \u201d \u0394 ( x ) defined in the paper [ Sec.2.4 on page 5 ] . The relationship between \u201c amortization error \u201d defined here and \u201c amortization gap \u201d is given by Eq.8 [ Sec 2.4 on page 5 ] . It can be observed that the local amortization error is low , even with a student model having limited capacity capacity . This effectively shows that the function space considered is enough to cover the essential target , which is in fact \u201c of enough capacity \u201d . The local amortization error 5.3 * 10^ ( -3 ) bounds the local amortization gap \u0394 ( x ) = Avg.Apx.Err \u2013 Avg.Mdl.Err = 4.9 * 10^ ( -3 ) , which is consistent with Eq.8 [ Sec 2.4 on page 5 ] . The approximation error is mainly determined by the model error , which turns out to be acceptably small . This is consistent with the analysis and also shows the effectiveness and suitableness of using Dirichlet family . Q2 : What if the teacher predictive distribution is far unlike a Dirichlet ? How much is the discrepancy between teacher and student predictive distribution ? Theoretical or empirical evidence is needed for this modeling choice . Using a Dirichlet for the student is modeling choice , similar to assuming Gaussian posteriors for variational approximations . Our framework is general and any student distribution can be adopted , e.g. , generalized Dirichlet , mixture of Dirichlets . To do this requires : 1 ) a suitable parametrization of the model that can capture uncertainty ; 2 ) deriving/computing the approximation loss ( KL , MMD , EMD ) ; 3 ) reparameterization trick of expectations for efficient gradient estimation . The algorithms of KL , EMD and MMD as well as the analysis still apply . EMNIST is a more challenging dataset compared with Pima , Spambase , MNIST and Cifar10 as the number of classes is larger ( 47 ) . The empirical evidence for the choice of Dirichlet is in this experiment , where we obtain nearly the same performance ( accuracy ) as using particles , while obtaining better OOD performance . We also show that the approximation error is low ( see Q1 ) , and thus the Dirichlet is a good fit for the teacher \u2019 s predictive distribution in this case . References : [ 1 ] Kingma , Diederik P. , and Max Welling . `` Auto-encoding variational bayes . '' arXiv preprint arXiv:1312.6114 ( 2013 ) . [ 2 ] Goodfellow , Ian , et al . `` Generative adversarial nets . '' Advances in neural information processing systems . 2014 . [ 3 ] Arjovsky , Martin , Soumith Chintala , and L\u00e9on Bottou . `` Wasserstein generative adversarial networks . '' International conference on machine learning . 2017 ."}, "2": {"review_id": "HJlHzJBFwB-2", "review_text": "This paper studies the problem of avoiding Monte Carlo (MC) estimate for the predictive distribution during the test for Bayesian methods. MC estimate will incur multiple passes where the number of passes depends on the number of samples and therefore the cost can be huge. The authors propose One-Pass Uncertainty (OPU) methods to approximate the predictive distribution through distillation. Experiments on Bayesian neural networks are conducted to demonstrate the proposed method. Quality: The proposed method appears to be technically sound. The view of approximating the predictive distribution over simplex is interesting and may inspire future studies under this formulation. Although the restriction of the student distribution to be tractable seems to limit the design of the student model significantly. And this restrictive distribution family may cause large amortization error, as suggested by Lemma 1 in the paper. The experiments are well-conducted, and the proposed method is well-evaluated. Significance: This paper studies an important problem in Bayesian machine learning and the proposed method can be combined with many Bayesian methods to reduce the computational cost during the test. Originality: As far as I know, the method is novel. The related work is adequately cited. Clarity: This paper is well-written and easy to follow. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the comments and would like to answer the questions as follows : Q1 : Although the restriction of the student distribution to be tractable seems to limit the design of the student model significantly . And this restrictive distribution family may cause large amortization error , as suggested by Lemma 1 in the paper . The restrictive distribution family causes \u201c large \u201d total approximation error if the ground truth induced distribution is considerably different from a Dirichlet . However the amortization error is low and depends on capacity of neural network used to construct the approximation [ point 2 in Para 1 . Sec 2.2 on page 3 ] . We show the three types of errors with the experimental results on EMNIST . Results on EMNIST OPU-MCDP-MMD : Average approximation error [ Eq.7 on page 5 ] : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( q_ { \\mathbf { x } _i } , p_ { \\mathbf { x } _i } ) $ . The averaged MMD between teacher \u2019 s particles ( for each x ) and the predicted Dirichlet by OPU : 6.5 * 10^ ( -2 ) . Average model error [ Eq.7 on page 5 ] : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( p_ { \\mathbf { x } _i } , \\bar { q } _ { \\mathbf { x } _i } ^\\ast ) $ . The averaged MMD between teacher \u2019 s particles and locally fitted Dirichlet : 6.01 * 10^ ( -2 ) . Average local amortization error : $ \\frac { 1 } { N } \\sum_ { i=1 } ^ { N } MMD ( q_ { \\mathbf { x } _i } , \\bar { q } _ { \\mathbf { x } _i } ^\\ast ) $ . The averaged MMD between locally fitted Dirichlet ( for each x ) and the predicted Dirichlet by OPU : 5.3 * 10^ ( -3 ) . Note that the \u201c error \u201d is different from \u201c amortization gap \u201d \u0394 ( x ) defined in the paper [ Sec.2.4 on page 5 ] . The relationship between \u201c amortization error \u201d defined here and \u201c amortization gap \u201d is given by Eq.8 [ Sec 2.4 on page 5 ] . It can be observed that the local amortization error 5.3 * 10^ ( -3 ) is low and it bounds the local amortization gap \u0394 ( x ) = Avg.Approx.Err \u2013 Avg.Model.Err = 4.9 * 10^ ( -3 ) , which is consistent with Eq.8 [ Sec 2.4 on page 5 ] . The approximation error is mainly determined by the model error , which turns out to be acceptably small . This is consistent with the analysis and also shows the effectiveness and suitableness of using Dirichlet family . More experiments on MCDP-Cifar10 . ( Code available via the code link ) Method || MisC . AUROC|| MisC . AUPR || OOD . AUROC || OOD . AUPR || Acc MCDP-KL : || 92.2 ( P ) || 47.0 ( P ) || 90.5 ( E ) || 88.7 ( E ) || 92.4 MCDP-EMD : || 92.2 ( P ) || 47.0 ( P ) || 91.4 ( D ) || 89.1 ( D ) || 92.4 MCDP-MMD : || 92.2 ( P ) || 47.0 ( P ) || 91.0 ( D ) || 89.3 ( D ) || 92.4 OPU-MCDP-KL : || 87.2 ( P ) || 45.9 ( P ) || 86.1 ( E ) || 85.5 ( E ) || 89.9 OPU-MCDP-EMD : || 91.8 ( E ) || 46.9 ( P ) || 93.5 ( C ) || 92.0 ( C ) || 91.8 OPU-MCDP-MMD : || 91.3 ( E ) || 46.6 ( P ) || 92.9 ( C ) || 91.7 ( C ) || 91.8"}}