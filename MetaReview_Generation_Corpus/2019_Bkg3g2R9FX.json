{"year": "2019", "forum": "Bkg3g2R9FX", "title": "Adaptive Gradient Methods with Dynamic Bound of Learning Rate", "decision": "Accept (Poster)", "meta_review": "The paper was found to be well-written and conveys interesting idea. However the AC notices a large body of clarifications that were provided to the reviewers (regarding the theory, experiments, and setting in general) that need to be well addressed in the paper. ", "reviews": [{"review_id": "Bkg3g2R9FX-0", "review_text": "This paper presents new variants of ADAM and AMSGrad that bound the gradients above and below to avoid potential negative effects on generalization of excessively large and small gradients; and the paper demonstrates the effectiveness on a few commonly used machine learning test cases. The paper also presents detailed proofs that there exists a convex optimization problem for which the ADAM regret does not converge to zero. This paper is very well written and easy to read. For that I thank the authors for their hard word. I also believe that their approach to bound is well structured in that it converges to SGD in the infinite limit and allows the algorithm to get teh best of both worlds - faster convergence and better generalization. The authors' experimental results support the value of their proposed algorithms. In sum, this is an important result that I believe will be of interest to a wide audience at ICLR. The proofs in the paper, although impressive, are not very compelling for the point that the authors want to get across. That fact that such cases of poor performance can exists, says nothing about the average performance of the algorithms, which is practice is what really matters. The paper could be improved by including more and larger data sets. For example, the authors ran on CIFAR-10. They could have done CIFAR-100, for example, to get more believable results. The authors add a useful section on notation, but go on to abuse it a bit. This could be improved. Specifically, they use an \"i\" subscript to indicate the i-th coordinate of a vector and then in the Table 1 sum over t using i as a subscript. Also, superscript on vectors are said to element-wise powers. If so, why is a diag() operation required? Either make the outproduct explicit, or get rid of the diag().", "rating": "7: Good paper, accept", "reply_text": "Thanks for your comments ! We deeply agree that the average performance of different algorithms is very important in practice . But as also mentioned in the reply to anonymous comments before ( on 11.12 ) , our understanding of the generalization behavior of deep neural networks is still very shallow by now . It is a big challenge of investigating from theoretical aspects . Actually , the theoretical analysis of most recent related work is still under strong or particular assumptions . I believe if one could conduct convincing theoretical proof without strong assumptions , that work is totally worth an individual publication . We are conducting more experiments on larger datasets such as CIFAR-100 and on more tasks in other fields , and the results are very positive too . We will add the results and analysis in the final revision if there is space left in the paper . We want to argue that the use of diag ( ) is necessary since \\phi_t is a matrix rather than a vector . Also , $ g $ is not a vector but $ g_t $ is , and $ g_ { t , i } $ is coordinate . It is true that the expression $ x_i $ might be ambiguous without context : 1 ) $ x $ is a vector and it means the i-th coordinate of $ x $ or 2 ) $ x $ is not a vector and $ x_i $ is a vector at time $ i $ . But since $ x $ can not be or not be a vector at the same time , it is clear in a specific context . This kind of notation is also used in many other works . We re-check the math expressions in our paper and think they are ok ."}, {"review_id": "Bkg3g2R9FX-1", "review_text": "The authors introduce AdaBound, a method that starts off as Adam but eventually transitions to SGD. The motivation is to benefit from the rapid training process of Adam in the beginning and the improved convergence of SGD at the end. The authors do so by clipping the weight updates of Adam in a dynamic way. They show numerical results and theoretical guarantees. The numerical results are presented on CIFAR-10 and PTB while the theoretical results are shown on assumptions similar to AMSGrad (& using similar proof strategies). As it stands, I have some foundational concerns about the paper and believe that it needs significant improvement before it can be published. I request the authors to please let me know if I misunderstood any aspect of the algorithm, I will adjust my rating promptly. I detail my key criticisms below: - I'm somewhat confused by the formulation of \\eta_u and \\eta_l. The way it is set up (end of Section 4), the final learning rate for the algorithm converges to 0.1 as t goes to infinity. In the Appendix, the authors show results also with final convergence to 1. Are the results coincidental with the fact that SGD works well with those learning rates? It is a bit odd that we indirectly encode the final learning rate of the algorithm into the \\eta s. - Am I correct in saying that with t=100 (i.e., the 100th iteration), the \\eta s constrain the learning rates to be in a tight bound around 0.1? If beta=0.9, then \\eta_l(1) = 0.1 - 0.1 / (0.1*100+1) = 0.091. After t=1000 iterations, \\eta_l becomes 0.099. Again, are the good results coincidental with the fact that SGD with learning rate 0.1 works well for this setup? In the scheme of the 200 epochs of training (equaling almost 100-150k iterations), if \\eta s are almost 0.099 / 0.10099, for over 99% of the training, we're only doing SGD with learning rate 0.1. - Along the same lines, what learning rates on the grid were chosen for each of the problems? Does the setup still work if SGD needs a small step size and we still have \\eta converge to 1? A VGG-11 without batch normalization typically needs a smaller learning rate than usual; could you try the algorithms on that? - Can the authors plot the evolution of learning rate of the algorithm over time? You could pick the min/median/max of the learning rates and plot them against epochs in the same way as accuracy.This would be a good meta-result to show how gradual the transition from Adam to SGD is. - The core observation of extreme learning rates and the proposal of clipping the updates is not novel; Keskar and Socher (which the authors cite for other claims) motivates their setup with the same idea (Section 2 of their paper). I feel that the authors should clarify what they are proposing as novel. Is it correct that a careful theoretical analysis of this framework is what stands as the authors' major contribution? - Can you try experimenting with/suggesting trajectories for \\eta which converge to SGD stepsize more slower? - Similarly, can you suggest ways to automate the choice for the \\eta^\\star? It seems that the 0.1 in the numerator is an additional hyperparameter that still might need tuning? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your questions and suggestions . We separate the questions into 3 parts ( bound functions , contributions , and extra details & experiments ) and post the responses below . We hope they can address your questions . [ About bound functions ] We want to clarify the following facts about the bound function : 1 . The convergence speed ( indicated by \\beta in current settings ) and convergence target ( indicated by \\alpha * ) exert minor impacts on the performance of AdaBound . 2.In other words , AdaBound is not sensitive to the form of bound functions , and therefore we don \u2019 t have to waste much time fine-tuning the hyperparameters , especially compared with SGD ( M ) . 3.Moreover , even not carefully fine-tuned AdaBound can beat SGD ( M ) with the optimal step size . We conducted the empirical study in Appendix G in order to illustrate the above points . But as you have raised a few questions about the bound function , it seems that our original experiments are not enough . We expand the experiments in an attempt to give more evidence to support the above statements and hope this can answer some questions you mentioned . > > > I 'm somewhat confused by the formulation of \\eta_u and \\eta_l . The way it is set up ( end of Section 4 ) , the final learning rate for the algorithm converges to 0.1 as t goes to infinity . In the Appendix , the authors show results also with final convergence to 1 . Are the results coincidental with the fact that SGD works well with those learning rates ? It is a bit odd that we indirectly encode the final learning rate of the algorithm into the \\eta s. ( Note : SGD and SGDM have similar performance in our experiments.Here we directly use SGD to generally indicate SGD or SGDM ) It is not a coincidence . SGD is very sensitive to the step size . \\alpha=0.1 is the best setting and other settings have large performance gaps compared with the optimal one ( see Figure 6a ) . But AdaBound has stable performance in different final step sizes ( see Figure 6b ) . Moreover , for all the step sizes , AdaBound outperforms SGD ( see Figure 7 ) . > > > Can you try experimenting with/suggesting trajectories for \\eta which converge to SGD stepsize more slower ? We further test \\beta for { 1-1/10 , 1-1/50 , 1-1/100 , 1-1/500 , 1-1/1000 } , which translates to some slower convergence speed of bound functions . Their performances are really close ( see Figure 5 ) . > > > Similarly , can you suggest ways to automate the choice for the \\eta^\\star ? It seems that the 0.1 in the numerator is an additional hyperparameter that still might need tuning ? In the current form of bound functions , yes , it is an additional hyperparameter . But as illustrated by the experiments , AdaBound is very robust and not sensitive to hyperparameters ( we can randomly use \\alpha from 0.001 to 1 and still get stable and good performance ) . I think in practice , we can somehow treat it as \u201c no need of tuning \u201d , and 0.1 can be a default setting ."}, {"review_id": "Bkg3g2R9FX-2", "review_text": "*Summary : The paper explores variants of popular adaptive optimization methods. The idea is to clip the magnitude of the gradients from above and below in order to prevent too aggressive/conservative updates. The authors provide regret bound to this algorithm in the online convex setting and perform several illustrative experiments. *Significance: -There is not much novelty in Theorems 1,2,3 since similar results already appeared in Reddi et al. -Also, the theoretical part does not demonstrate the benefit of the clipping idea. Concretely, the regret bounds seem to be similar to the bounds of AMSBound. Ideally, I would like to see an analysis that discusses a situation where AdaGrad/AMSBound fail or perfrom really bad, yet the clipped versions do well. -The experimental part on the other hand is impressive, and the results illustrate the usefulness of the clipping idea. *Clarity: The idea and motivation are very clear and so are the experiments. *Presentation: The presentation is mostly good. Summary of review: The paper suggests a simple idea to avoid extreme behaviour of the learning rate in standard adaptive methods. The theory is not so satisfying, since it does not illustrate the benefit of the method over standard adaptive methods. The experiments are more thorough and illustrate the applicability of the method. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments . > > > There is not much novelty in Theorems 1,2,3 since similar results already appeared in Reddi et al.We argue that Reddi et al . ( 2018 ) did not prove \u300cfor all the initial learning rates\u300d , Adam has bad behavior , and this condition is important for showing the necessity of our idea of restricting the actual learning rates . That \u2019 s why we complete the proof with this weaker assumption . We would not claim the theoretical analysis as our main contribution in this paper , but it is a necessary part that serves for our actual main contribution\u300cproposing the idea of an optimization algorithm that can gradually transform from adaptive methods to SGD ( M ) , combining both of their advantages\u300d . All the other parts in the paper , including preliminary empirical study , theoretical proofs , experiments , and further analysis , serve for this main contribution . > > > Also , the theoretical part does not demonstrate the benefit of the clipping idea . Concretely , the regret bounds seem to be similar to the bounds of AMSBound . Ideally , I would like to see an analysis that discusses a situation where AdaGrad/AMSBound fail or perform really bad , yet the clipped versions do well . First , the name of our new proposed methods are AdaBound and AMSBound . I guess you mean AMSGrad in your suggestion ? Actually , it is easy to use a setting similar to that of Wilson et al . ( 2017 ) , to show AdaGrad/Adam achieve really bad performance while our methods do well . But I don \u2019 t think it is very meaningful since it is only a bunch of examples . As also mentioned by review 2 , the average performance of the algorithms is what really matters . But due to its difficulty , most similar works on optimizers tend to use experiments to support their arguments and lack the theoretical proofs for this part ."}], "0": {"review_id": "Bkg3g2R9FX-0", "review_text": "This paper presents new variants of ADAM and AMSGrad that bound the gradients above and below to avoid potential negative effects on generalization of excessively large and small gradients; and the paper demonstrates the effectiveness on a few commonly used machine learning test cases. The paper also presents detailed proofs that there exists a convex optimization problem for which the ADAM regret does not converge to zero. This paper is very well written and easy to read. For that I thank the authors for their hard word. I also believe that their approach to bound is well structured in that it converges to SGD in the infinite limit and allows the algorithm to get teh best of both worlds - faster convergence and better generalization. The authors' experimental results support the value of their proposed algorithms. In sum, this is an important result that I believe will be of interest to a wide audience at ICLR. The proofs in the paper, although impressive, are not very compelling for the point that the authors want to get across. That fact that such cases of poor performance can exists, says nothing about the average performance of the algorithms, which is practice is what really matters. The paper could be improved by including more and larger data sets. For example, the authors ran on CIFAR-10. They could have done CIFAR-100, for example, to get more believable results. The authors add a useful section on notation, but go on to abuse it a bit. This could be improved. Specifically, they use an \"i\" subscript to indicate the i-th coordinate of a vector and then in the Table 1 sum over t using i as a subscript. Also, superscript on vectors are said to element-wise powers. If so, why is a diag() operation required? Either make the outproduct explicit, or get rid of the diag().", "rating": "7: Good paper, accept", "reply_text": "Thanks for your comments ! We deeply agree that the average performance of different algorithms is very important in practice . But as also mentioned in the reply to anonymous comments before ( on 11.12 ) , our understanding of the generalization behavior of deep neural networks is still very shallow by now . It is a big challenge of investigating from theoretical aspects . Actually , the theoretical analysis of most recent related work is still under strong or particular assumptions . I believe if one could conduct convincing theoretical proof without strong assumptions , that work is totally worth an individual publication . We are conducting more experiments on larger datasets such as CIFAR-100 and on more tasks in other fields , and the results are very positive too . We will add the results and analysis in the final revision if there is space left in the paper . We want to argue that the use of diag ( ) is necessary since \\phi_t is a matrix rather than a vector . Also , $ g $ is not a vector but $ g_t $ is , and $ g_ { t , i } $ is coordinate . It is true that the expression $ x_i $ might be ambiguous without context : 1 ) $ x $ is a vector and it means the i-th coordinate of $ x $ or 2 ) $ x $ is not a vector and $ x_i $ is a vector at time $ i $ . But since $ x $ can not be or not be a vector at the same time , it is clear in a specific context . This kind of notation is also used in many other works . We re-check the math expressions in our paper and think they are ok ."}, "1": {"review_id": "Bkg3g2R9FX-1", "review_text": "The authors introduce AdaBound, a method that starts off as Adam but eventually transitions to SGD. The motivation is to benefit from the rapid training process of Adam in the beginning and the improved convergence of SGD at the end. The authors do so by clipping the weight updates of Adam in a dynamic way. They show numerical results and theoretical guarantees. The numerical results are presented on CIFAR-10 and PTB while the theoretical results are shown on assumptions similar to AMSGrad (& using similar proof strategies). As it stands, I have some foundational concerns about the paper and believe that it needs significant improvement before it can be published. I request the authors to please let me know if I misunderstood any aspect of the algorithm, I will adjust my rating promptly. I detail my key criticisms below: - I'm somewhat confused by the formulation of \\eta_u and \\eta_l. The way it is set up (end of Section 4), the final learning rate for the algorithm converges to 0.1 as t goes to infinity. In the Appendix, the authors show results also with final convergence to 1. Are the results coincidental with the fact that SGD works well with those learning rates? It is a bit odd that we indirectly encode the final learning rate of the algorithm into the \\eta s. - Am I correct in saying that with t=100 (i.e., the 100th iteration), the \\eta s constrain the learning rates to be in a tight bound around 0.1? If beta=0.9, then \\eta_l(1) = 0.1 - 0.1 / (0.1*100+1) = 0.091. After t=1000 iterations, \\eta_l becomes 0.099. Again, are the good results coincidental with the fact that SGD with learning rate 0.1 works well for this setup? In the scheme of the 200 epochs of training (equaling almost 100-150k iterations), if \\eta s are almost 0.099 / 0.10099, for over 99% of the training, we're only doing SGD with learning rate 0.1. - Along the same lines, what learning rates on the grid were chosen for each of the problems? Does the setup still work if SGD needs a small step size and we still have \\eta converge to 1? A VGG-11 without batch normalization typically needs a smaller learning rate than usual; could you try the algorithms on that? - Can the authors plot the evolution of learning rate of the algorithm over time? You could pick the min/median/max of the learning rates and plot them against epochs in the same way as accuracy.This would be a good meta-result to show how gradual the transition from Adam to SGD is. - The core observation of extreme learning rates and the proposal of clipping the updates is not novel; Keskar and Socher (which the authors cite for other claims) motivates their setup with the same idea (Section 2 of their paper). I feel that the authors should clarify what they are proposing as novel. Is it correct that a careful theoretical analysis of this framework is what stands as the authors' major contribution? - Can you try experimenting with/suggesting trajectories for \\eta which converge to SGD stepsize more slower? - Similarly, can you suggest ways to automate the choice for the \\eta^\\star? It seems that the 0.1 in the numerator is an additional hyperparameter that still might need tuning? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your questions and suggestions . We separate the questions into 3 parts ( bound functions , contributions , and extra details & experiments ) and post the responses below . We hope they can address your questions . [ About bound functions ] We want to clarify the following facts about the bound function : 1 . The convergence speed ( indicated by \\beta in current settings ) and convergence target ( indicated by \\alpha * ) exert minor impacts on the performance of AdaBound . 2.In other words , AdaBound is not sensitive to the form of bound functions , and therefore we don \u2019 t have to waste much time fine-tuning the hyperparameters , especially compared with SGD ( M ) . 3.Moreover , even not carefully fine-tuned AdaBound can beat SGD ( M ) with the optimal step size . We conducted the empirical study in Appendix G in order to illustrate the above points . But as you have raised a few questions about the bound function , it seems that our original experiments are not enough . We expand the experiments in an attempt to give more evidence to support the above statements and hope this can answer some questions you mentioned . > > > I 'm somewhat confused by the formulation of \\eta_u and \\eta_l . The way it is set up ( end of Section 4 ) , the final learning rate for the algorithm converges to 0.1 as t goes to infinity . In the Appendix , the authors show results also with final convergence to 1 . Are the results coincidental with the fact that SGD works well with those learning rates ? It is a bit odd that we indirectly encode the final learning rate of the algorithm into the \\eta s. ( Note : SGD and SGDM have similar performance in our experiments.Here we directly use SGD to generally indicate SGD or SGDM ) It is not a coincidence . SGD is very sensitive to the step size . \\alpha=0.1 is the best setting and other settings have large performance gaps compared with the optimal one ( see Figure 6a ) . But AdaBound has stable performance in different final step sizes ( see Figure 6b ) . Moreover , for all the step sizes , AdaBound outperforms SGD ( see Figure 7 ) . > > > Can you try experimenting with/suggesting trajectories for \\eta which converge to SGD stepsize more slower ? We further test \\beta for { 1-1/10 , 1-1/50 , 1-1/100 , 1-1/500 , 1-1/1000 } , which translates to some slower convergence speed of bound functions . Their performances are really close ( see Figure 5 ) . > > > Similarly , can you suggest ways to automate the choice for the \\eta^\\star ? It seems that the 0.1 in the numerator is an additional hyperparameter that still might need tuning ? In the current form of bound functions , yes , it is an additional hyperparameter . But as illustrated by the experiments , AdaBound is very robust and not sensitive to hyperparameters ( we can randomly use \\alpha from 0.001 to 1 and still get stable and good performance ) . I think in practice , we can somehow treat it as \u201c no need of tuning \u201d , and 0.1 can be a default setting ."}, "2": {"review_id": "Bkg3g2R9FX-2", "review_text": "*Summary : The paper explores variants of popular adaptive optimization methods. The idea is to clip the magnitude of the gradients from above and below in order to prevent too aggressive/conservative updates. The authors provide regret bound to this algorithm in the online convex setting and perform several illustrative experiments. *Significance: -There is not much novelty in Theorems 1,2,3 since similar results already appeared in Reddi et al. -Also, the theoretical part does not demonstrate the benefit of the clipping idea. Concretely, the regret bounds seem to be similar to the bounds of AMSBound. Ideally, I would like to see an analysis that discusses a situation where AdaGrad/AMSBound fail or perfrom really bad, yet the clipped versions do well. -The experimental part on the other hand is impressive, and the results illustrate the usefulness of the clipping idea. *Clarity: The idea and motivation are very clear and so are the experiments. *Presentation: The presentation is mostly good. Summary of review: The paper suggests a simple idea to avoid extreme behaviour of the learning rate in standard adaptive methods. The theory is not so satisfying, since it does not illustrate the benefit of the method over standard adaptive methods. The experiments are more thorough and illustrate the applicability of the method. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments . > > > There is not much novelty in Theorems 1,2,3 since similar results already appeared in Reddi et al.We argue that Reddi et al . ( 2018 ) did not prove \u300cfor all the initial learning rates\u300d , Adam has bad behavior , and this condition is important for showing the necessity of our idea of restricting the actual learning rates . That \u2019 s why we complete the proof with this weaker assumption . We would not claim the theoretical analysis as our main contribution in this paper , but it is a necessary part that serves for our actual main contribution\u300cproposing the idea of an optimization algorithm that can gradually transform from adaptive methods to SGD ( M ) , combining both of their advantages\u300d . All the other parts in the paper , including preliminary empirical study , theoretical proofs , experiments , and further analysis , serve for this main contribution . > > > Also , the theoretical part does not demonstrate the benefit of the clipping idea . Concretely , the regret bounds seem to be similar to the bounds of AMSBound . Ideally , I would like to see an analysis that discusses a situation where AdaGrad/AMSBound fail or perform really bad , yet the clipped versions do well . First , the name of our new proposed methods are AdaBound and AMSBound . I guess you mean AMSGrad in your suggestion ? Actually , it is easy to use a setting similar to that of Wilson et al . ( 2017 ) , to show AdaGrad/Adam achieve really bad performance while our methods do well . But I don \u2019 t think it is very meaningful since it is only a bunch of examples . As also mentioned by review 2 , the average performance of the algorithms is what really matters . But due to its difficulty , most similar works on optimizers tend to use experiments to support their arguments and lack the theoretical proofs for this part ."}}