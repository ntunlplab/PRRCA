{"year": "2017", "forum": "ryEGFD9gl", "title": "Submodular Sum-product Networks for Scene Understanding", "decision": "Reject", "meta_review": "This paper was reviewed by three experts. While they all find merits in the paper (interesting new model class SSPN, new MAP inference algorithm), they all consistently point to deficiencies in the current manuscript (lack of parameter learning, emphasis on evaluation on energies, lack of improvements in accuracy). \n \n One problem that (I believe) is that manuscript as it stands makes neither a compelling impact on the chosen application (semantic segmentation) nor does it convincing establish the broad applicability of the proposed model (how do I run SSPNs on activity recognition or social network modeling). \n \n To be clear, we all agree that there is promising content here. However, I agree with the reviewers that the significance has not been established.", "reviews": [{"review_id": "ryEGFD9gl-0", "review_text": "The paper discusses sub modular sum-product networks as a tractable extension for classical sum-product networks. The proposed approach is evaluated on semantic segmentation tasks and some early promising results are provided. Summary: \u2014\u2014\u2014 I think the paper presents a compelling technique for hierarchical reasoning in MRFs but the experimental results are not yet convincing. Moreover the writing is confusing at times. See below for details. Quality: I think some of the techniques could be described more carefully to better convey the intuition. Clarity: Some of the derivations and intuitions could be explained in more detail. Originality: The suggested idea is great. Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time. Detailed comments: \u2014\u2014\u2014 1. I think the clarity of the paper would benefit significantly from fixes to inaccuracies. E.g., \\alpha-expansion and belief propagation are not `scene-understanding algorithms\u2019 but rather approaches for optimizing energy functions. Computing the MAP state of an SSPN in time sub-linear in the network size seems counterintuitive because it means we are not allowed to visit all the nodes in the network. The term `deep probabilistic model\u2019 should probably be defined. The paper states that InferSSPN computes `the approximate MAP state of the SSPN (equivalently, the optimal parse of the image)\u2019 and I\u2019m wondering how the `approximate MAP state' can be optimal. Etc. 2. Albeit being formulated for scene understanding tasks, no experiments demonstrate the obtained results of the proposed technique. To assess the applicability of the proposed approach a more detailed analysis is required. More specifically, the technique is evaluated on a subset of images which makes comparison to any other approach impossible. According to my opinion, either a conclusive experimental evaluation using, e.g., IoU metric should be given in the paper, or a comparison to publicly available results is possible. 3. To simplify the understanding of the paper a more intuitive high-level description is desirable. Maybe the authors can even provide an intuitive visualization of their approach.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your comments and feedback . 1.We \u2019 ve edited and re-written parts of the paper to fix inaccuracies . For example , with respect to alpha-expansion and BP as scene-understanding algorithms , we simply meant that they are used for this task , not that they are explicitly designed for it . This has been clarified , as has the language regarding the approximate MAP state ( where the intention was that the MAP state is the optimal parse , but this was poorly worded ) . We note , however , that inference in time sub-linear in network size is not uncommon , depending on the definition of the network . Techniques such as A * and branch-and-bound are able to explore graphs and networks in time sub-linear in their size . 2.See our general response to this point above . 3.In editing the paper , we \u2019 ve re-structured and re-written parts of the inference section to try to make the intuitions more clear and have included a figure to help visualize the key parts of InferSSPN ."}, {"review_id": "ryEGFD9gl-1", "review_text": "This paper is about submodular sum product networks applied to scene understanding. SPNs have shown great success in deep linear models since the work of Poon 2011. The authors propose an extension to the initial SPNs model to be submodular, introducing submodular unary and pairwise potentials. The authors propose a new inference algorithm. The authors evaluated their results on Stanford Background Dataset and compared against multiple baselines. Pros: + New formulation of SPNs + New inference algorithm Cons: - The authors did not discuss how the SSPN structure is learned and how the generative process chooses the a symbol (operation) at each level) - The evaluations is lacking. The authors only showed results on their own approach and baselines, leaving out every other approach. Evaluations could have been also done on BSD for regular image segmentation (hierarchical segmentation). The idea is great, however, the paper needs more work to be published. I would also recommend for the authors to include more details about their approach and present a full paper with extended experiments and full learning approach.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . 1.We do not do any learning in this paper , which is why it is not discussed . The grammar structures used in the experiments are algorithmically defined and the corresponding weights are generated heuristically by counting symbol appearances in the evaluated images . The same structures and parameters are used when evaluating each inference algorithm . The generative process does not select symbols , instead it always begins at the start symbol over the entire image , chooses a production of the start symbol ( from a categorical distribution over said productions \u2013 the paper has been updated to make this more explicit ) , chooses a partition of the current region into a labeling based on the constituent symbols of the chosen production ( by sampling from the MRF defined by this production over the pixels in the current region ) , and then recurses on each ( constituent , subregion ) pair . The production-selection process is equivalent to that found in PCFGs . 2.See our general response to this point above . It \u2019 s not clear to us how evaluating on the Berkeley segmentation dataset ( BSD ) would be meaningful , as our model and inference algorithm are about identifying semantic classes and relationships such as part-subpart and class-subclass , not about identifying image contours or segmenting based on intensity values ."}, {"review_id": "ryEGFD9gl-2", "review_text": "This paper develops Submodular Sum Product Networks (SSPNs) and an efficient inference algorithm for approximately computing the most probable labeling of variables in the model. The main application in the paper is on scene parsing. In this context, SSPNs define an energy function with a grammar component for representing a hierarchy of labels and an MRF for encoding smoothness of labels over space. To perform inference, the authors develop a move-making algorithm, somewhat in the spirit of fusion moves (Lempitsky et al., 2010) that repeatedly improves a solution by considering a large neighborhood of alternative segmentations and solving an optimization problem to choose the best neighbor. Empirical results show that the proposed algorithm achieves better energy that belief propagation of alpha expansion and is much faster. This is generally a well-executed paper. The model is interesting and clearly defined, the algorithm is well presented with proper analysis of the relevant runtimes and guarantees on the behavior. Overall, the algorithm seems effective at minimizing the energy of SSPN models. Having said that, I don't think this paper is a great fit for ICLR. The model is even somewhat to the antithesis of the idea of learning representations, in that a highly structured form of energy function is asserted by the human modeller, and then inference is performed. I don't see the connection to learning representations. One additional issue is that while the proposed algorithm is faster than alternatives, the times are still on the order of 1-287 seconds per image, which means that the applicability of this method (as is) to something like training ConvNets is limited. Finally, there is no attempt to argue that the model produces better segmentations than alternative models. The only evaluations in the paper are on energy values achieved and on training data. So overall I think this is a good paper that should be published at a good machine learning conference, but I don't think ICLR is the right fit.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your detailed comments and feedback . We disagree that SSPNs are antithetical to ICLR . While our results do not currently include learning , SSPNs are absolutely learnable , just as SPNs , MRFs , and CRFs , and convolutional neural networks are learnable . While structure learning is possible for most of these models , the vast majority of learning ( in ICLR and in other conferences ) is parameter learning , meaning that the aforementioned models all have equivalent amounts of human-specified structure . The only difference between SSPNs and these other models ( ignoring SPNs ) is that SSPNs can be ( but do not need to be ) defined with respect to a grammar , as a convenient method for specifying the architecture of the network and to provide them with additional semantic information such as constituency and sub-categorization relationships . With respect to inference time , many convolutional neural network architectures for semantic segmentation end up down-sampling images quite extensively ( e.g. , Deeplab , FCN ) due to the use of max-pooling or strided convolution . In our preliminary work on end-to-end training of SSPNs with deep networks , the input image to the SSPN ends up being much smaller than the full image and thus inference only takes a small fraction of a second . As such , inference time during training is similar to that of a forward pass through resnet101 , making end-to-end training quite realistic . See our general response to this point above ."}], "0": {"review_id": "ryEGFD9gl-0", "review_text": "The paper discusses sub modular sum-product networks as a tractable extension for classical sum-product networks. The proposed approach is evaluated on semantic segmentation tasks and some early promising results are provided. Summary: \u2014\u2014\u2014 I think the paper presents a compelling technique for hierarchical reasoning in MRFs but the experimental results are not yet convincing. Moreover the writing is confusing at times. See below for details. Quality: I think some of the techniques could be described more carefully to better convey the intuition. Clarity: Some of the derivations and intuitions could be explained in more detail. Originality: The suggested idea is great. Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time. Detailed comments: \u2014\u2014\u2014 1. I think the clarity of the paper would benefit significantly from fixes to inaccuracies. E.g., \\alpha-expansion and belief propagation are not `scene-understanding algorithms\u2019 but rather approaches for optimizing energy functions. Computing the MAP state of an SSPN in time sub-linear in the network size seems counterintuitive because it means we are not allowed to visit all the nodes in the network. The term `deep probabilistic model\u2019 should probably be defined. The paper states that InferSSPN computes `the approximate MAP state of the SSPN (equivalently, the optimal parse of the image)\u2019 and I\u2019m wondering how the `approximate MAP state' can be optimal. Etc. 2. Albeit being formulated for scene understanding tasks, no experiments demonstrate the obtained results of the proposed technique. To assess the applicability of the proposed approach a more detailed analysis is required. More specifically, the technique is evaluated on a subset of images which makes comparison to any other approach impossible. According to my opinion, either a conclusive experimental evaluation using, e.g., IoU metric should be given in the paper, or a comparison to publicly available results is possible. 3. To simplify the understanding of the paper a more intuitive high-level description is desirable. Maybe the authors can even provide an intuitive visualization of their approach.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your comments and feedback . 1.We \u2019 ve edited and re-written parts of the paper to fix inaccuracies . For example , with respect to alpha-expansion and BP as scene-understanding algorithms , we simply meant that they are used for this task , not that they are explicitly designed for it . This has been clarified , as has the language regarding the approximate MAP state ( where the intention was that the MAP state is the optimal parse , but this was poorly worded ) . We note , however , that inference in time sub-linear in network size is not uncommon , depending on the definition of the network . Techniques such as A * and branch-and-bound are able to explore graphs and networks in time sub-linear in their size . 2.See our general response to this point above . 3.In editing the paper , we \u2019 ve re-structured and re-written parts of the inference section to try to make the intuitions more clear and have included a figure to help visualize the key parts of InferSSPN ."}, "1": {"review_id": "ryEGFD9gl-1", "review_text": "This paper is about submodular sum product networks applied to scene understanding. SPNs have shown great success in deep linear models since the work of Poon 2011. The authors propose an extension to the initial SPNs model to be submodular, introducing submodular unary and pairwise potentials. The authors propose a new inference algorithm. The authors evaluated their results on Stanford Background Dataset and compared against multiple baselines. Pros: + New formulation of SPNs + New inference algorithm Cons: - The authors did not discuss how the SSPN structure is learned and how the generative process chooses the a symbol (operation) at each level) - The evaluations is lacking. The authors only showed results on their own approach and baselines, leaving out every other approach. Evaluations could have been also done on BSD for regular image segmentation (hierarchical segmentation). The idea is great, however, the paper needs more work to be published. I would also recommend for the authors to include more details about their approach and present a full paper with extended experiments and full learning approach.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . 1.We do not do any learning in this paper , which is why it is not discussed . The grammar structures used in the experiments are algorithmically defined and the corresponding weights are generated heuristically by counting symbol appearances in the evaluated images . The same structures and parameters are used when evaluating each inference algorithm . The generative process does not select symbols , instead it always begins at the start symbol over the entire image , chooses a production of the start symbol ( from a categorical distribution over said productions \u2013 the paper has been updated to make this more explicit ) , chooses a partition of the current region into a labeling based on the constituent symbols of the chosen production ( by sampling from the MRF defined by this production over the pixels in the current region ) , and then recurses on each ( constituent , subregion ) pair . The production-selection process is equivalent to that found in PCFGs . 2.See our general response to this point above . It \u2019 s not clear to us how evaluating on the Berkeley segmentation dataset ( BSD ) would be meaningful , as our model and inference algorithm are about identifying semantic classes and relationships such as part-subpart and class-subclass , not about identifying image contours or segmenting based on intensity values ."}, "2": {"review_id": "ryEGFD9gl-2", "review_text": "This paper develops Submodular Sum Product Networks (SSPNs) and an efficient inference algorithm for approximately computing the most probable labeling of variables in the model. The main application in the paper is on scene parsing. In this context, SSPNs define an energy function with a grammar component for representing a hierarchy of labels and an MRF for encoding smoothness of labels over space. To perform inference, the authors develop a move-making algorithm, somewhat in the spirit of fusion moves (Lempitsky et al., 2010) that repeatedly improves a solution by considering a large neighborhood of alternative segmentations and solving an optimization problem to choose the best neighbor. Empirical results show that the proposed algorithm achieves better energy that belief propagation of alpha expansion and is much faster. This is generally a well-executed paper. The model is interesting and clearly defined, the algorithm is well presented with proper analysis of the relevant runtimes and guarantees on the behavior. Overall, the algorithm seems effective at minimizing the energy of SSPN models. Having said that, I don't think this paper is a great fit for ICLR. The model is even somewhat to the antithesis of the idea of learning representations, in that a highly structured form of energy function is asserted by the human modeller, and then inference is performed. I don't see the connection to learning representations. One additional issue is that while the proposed algorithm is faster than alternatives, the times are still on the order of 1-287 seconds per image, which means that the applicability of this method (as is) to something like training ConvNets is limited. Finally, there is no attempt to argue that the model produces better segmentations than alternative models. The only evaluations in the paper are on energy values achieved and on training data. So overall I think this is a good paper that should be published at a good machine learning conference, but I don't think ICLR is the right fit.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your detailed comments and feedback . We disagree that SSPNs are antithetical to ICLR . While our results do not currently include learning , SSPNs are absolutely learnable , just as SPNs , MRFs , and CRFs , and convolutional neural networks are learnable . While structure learning is possible for most of these models , the vast majority of learning ( in ICLR and in other conferences ) is parameter learning , meaning that the aforementioned models all have equivalent amounts of human-specified structure . The only difference between SSPNs and these other models ( ignoring SPNs ) is that SSPNs can be ( but do not need to be ) defined with respect to a grammar , as a convenient method for specifying the architecture of the network and to provide them with additional semantic information such as constituency and sub-categorization relationships . With respect to inference time , many convolutional neural network architectures for semantic segmentation end up down-sampling images quite extensively ( e.g. , Deeplab , FCN ) due to the use of max-pooling or strided convolution . In our preliminary work on end-to-end training of SSPNs with deep networks , the input image to the SSPN ends up being much smaller than the full image and thus inference only takes a small fraction of a second . As such , inference time during training is similar to that of a forward pass through resnet101 , making end-to-end training quite realistic . See our general response to this point above ."}}