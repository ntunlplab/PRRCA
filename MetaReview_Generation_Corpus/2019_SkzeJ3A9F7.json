{"year": "2019", "forum": "SkzeJ3A9F7", "title": "Beyond Games: Bringing Exploration to Robots in Real-world", "decision": "Reject", "meta_review": "The authors propose implementing intrinsic motivation as a differentiable supervised loss coming from the error of a forward model, rather than the black box style of curiosity reward. The motivation is that this approach will lead to more sample efficient exploration for real robots. The use of a differentiable loss for policy optimization is interesting and has some novelty. However, the reviewers were unanimous in their criticism of the paper for poor baselines, unclear experiments and results, and unsupported claims. Even after substantial revisions to the paper, the AC and reviewers were unconvinced of the basic claims of the paper.", "reviews": [{"review_id": "SkzeJ3A9F7-0", "review_text": "This paper presents an interesting way to reformulate intrinsic curiosity as a differentiable function. The authors compare the differentiable function against using prediction error via REINFORCE and DQN, showing that their intrinsic curiosity method results in more interactions with unseen objects than the other two methods. For DQN this is to be expected, but it shows that backprop through this function is more efficient than reinforce in getting to unseen state spaces. I think this is an interesting method/proposal and is a somewhat novel reformulation of intrinsic error, but I do have some concerns in comparisons/claims. In the introduction, the authors say that the intrinsic curiosity method proposed by Pathak et al. is sample inefficient and isn\u2019t tested in robots. However, to my understanding the REINFORCE baseline isn\u2019t really equivalent (though it may be possible that it is, it was unclear how exactly the loss was formulated in the baseline, did include the other components from Pathak et al.?). If the claim is that this method is more efficient, I think it should have compared against that method directly. Moreover, I think the description of the experiments doesn\u2019t provide enough information. For example, the method says that different learning rates were used for the min-max game to stabilize it, but doesn\u2019t say what they were. Also, for the DQN baseline what were the parameters? Was there an epsilon greedy policy on top of the exploration reward? Was this annealed as in other work? Generally, I think more detail is needed throughout (even if it just refers to a more detailed appendix). Overall, I think this work needs to be revised to include more details on hyperaparameters, details on the baselines, and describing differences between Pathak et al.\u2019s method and the REINFORCE baseline. Moreover, feedback from other comments on this work should be addressed which reflect in more detail my comments below on opinionated claims (e.g., https://openreview.net/forum?id=SkzeJ3A9F7&noteId=HJlFlZOa2X ) Comments/Thoughts: + I think in the introduction there are some statements that probably need citations. For example, \u201cBut the same formulation from an optimization viewpoint, it suffers from all the bad properties of extrinsic rewards. The reward is a function of environment behavior with respect to the performed action. Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.\u201d \u2014> Why is this true? Is there a citation that can back this? Do you prove it later in the paper? + \u201cYes, 54 environments but no real-world physical robots\u201d \u2014> this and the intro seems like a blogpost at times. That can be fine (some would argue it\u2019s a good thing), but there seem to be some opinions without citations/backing, I suggest trying to back up statements wherever possible and avoid opinions. For example in this statement, robots aren't a requirement for evaluating intrinsic motivation. + \u201cSince the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.\u201d \u2014> citation/backing? it might be nice to point to the experiment section here to back it (e.g., \"As will be shown in Section X and in \\citet{something}, REINFORCE can be quite sample inefficient\") + \u201cIn practice, the existing on-policy algorithms, e.g., A3C (Mnih et al., 2016), PPO (Schulman et al., 2017) etc. are deployed off-the shelf -> This is confusing, so is this using REINFORCE or PPO/A3C? what is this statement referring to? + \u201cregress to rti to learn value estimates (i.e., off-policy) as discussed in the previous section\u201d \u2014> regress to \\sum r_t{I} for a value estimate?? Value is the expected return so not sure if this is a typo or i missed something earlier + What is the actual loss function used for the baseline? Is it the same as Pathak et al.? + What are the hyper parameters for DQN exploration? What are all the hyper parameters for any/all the algorithms? + Was a variance-reducing baseline used in REINFORCE? + What is the variance representing in the graphs, std across several trials? Maybe I missed it, but how many trials represent this standard deviation? + \u201cHence, we train the forward predictor slightly faster than the policy by keeping higher learning rate to stabilize the learning process. \u201c \u2014> what were the learning rates? Linguistic/Typos: Also, some minor, but frequent, grammatical issues/typos that I\u2019ve added below could be fixed. I would ask that the authors please have the submission proof-read for English style and grammar issues. There are many minor mistakes, some of which I\u2019ve tried to point out below. + \u201cThis leads to a significantly sample efficient exploration policy. \u201c \u2014> significantly more (?) sample efficient ? \u201cWhy is that? To understand the reason behind sample inefficiency of curiosity or intrinsic rewards, notice how the intrinsic rewards are given by agent\u201d \u2014> by the agent? \u201cForward model f\u03b8F is trained to minimize its loss which amounts to minimizing rti with respect to \u03b8F\u201d \u2014> the forward model \u201cHowever, policy is optimized to maximize the objective\u201d \u2014> However, the policy \u201cWe can also optimize for policy parameters \u03b8P via differentiable loss function\u201d \u2014> We can also optimize for (the) policy parameters \\theta via (a) differentiable loss function? \u201cTo optimize policy to maximize a discounted sum \u201c \u2014> To optimize the policy \u201cHow good is Forward Prediction Model\u201d \u2014> How good is the forward prediction model There are several other spots, but basically another pass over the paper might be worth it to check for these sorts of issues. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to first thank the reviewer for the comments . Please see the meta response in the combined reply for the common concerns among reviewers . The reviewer found the formulation to be `` interesting '' and `` novel '' but has raised concerns about lack of details about the experiment hyper-parameters . We answer them below : [ R2 ] : REINFORCE baseline vs. Pathak et . al . ? Is it the same as Pathak et al . ? = > Please refer to the combined response to all reviewers above . It is indeed the same . [ R2 ] : `` different learning rates .. but does n't say what they were '' [ R2 ] : `` What were DQN parameters ? Epsilon greedy '' [ R2 ] : `` include more details on hyper-parameters ... '' = > Yes , in our case , we can simply train the forward model faster than policy to stabilize the training . The learning rate for the model was 5e-4 and policy 1e-4 with an entropy loss coefficient of 1e-4 . The optimizer used was Adam . Yes , we used standard DQN algorithm with prioritized experience replay [ Schaul et . al.2016 ] and epsilon-greedy to maximize the curiosity objective [ Pathak et.al . 2017 ] .Q-value was trained with a learning rate of 1e-4 . In epsilon-greedy , epsilon was initialized at 0.5 then annealed over training to 0.1 . We agree that these details are important for the reproducibility of work , and we will include them in supplementary -- we apologize that we missed adding them in the supplementary . That being said , we will make our code , environment , and models public upon release . [ R2 ] : `` Feedback from the other commentator should be addressed '' = > We have answered both the factual and stylistic comments with proper citations in our reply . [ R2 ] : `` ... This is confusing , so is this using REINFORCE or PPO/A3C ? .. '' [ R2 ] : `` it might be nice to point to the experiment section here to back '' = > With due respect , we believe reviewer has misunderstood the background here . REINFORCE is an operator that was proposed in ( Williams , 1992 ) , and then became the main ingredient of policy gradient algorithms . All policy gradient algorithms , whether PPO or A3C , use reinforce operator to update the policy . PPO , in addition to reward maximization , penalizes KL-divergence change while A3C relies on regularization from multiple workers , but both these algorithms use reinforce at the core of it . [ Schulman et.al . 2016 ] provides a nice discussion on how REINFORCE helps to optimize any black-box function in expectation using only samples from the function . More detailed and fundamentals of policy gradients are described in [ Sutton & Barto , 1998 ] . We implemented the policy-gradient algorithm as optimizing the curiosity objective in [ Pathak et.al . 2017 ] as a baseline which we call `` REINFORCE '' . We will update the legend of graphs to say [ Pathak et.al . 2017 ] in future to avoid confusion . [ R2 ] : `` regress to rti .. discussed in the previous section .. so not sure if this is a typo '' = > Thanks , it should be indeed summed . However , note that in the previous section it was mentioned correctly . We will correct this typo . [ R2 ] : `` Some minor , but frequent , grammatical issues/typos .. '' = > Thank you . We will fix all these writing typos and make sure there no other ones ."}, {"review_id": "SkzeJ3A9F7-1", "review_text": "Beyond Games: Bringing Exploration to Robots in Real-world =========================================================== This paper tackles the laudable goal of making an algorithm for efficient exploration in \"real-world\" RL. To do this, they augment the \"curiosity\" algorithm of Pathak et al with a differentiable approximation to the reward prediction model. They motivate this algorithm through several intuitive arguments together with a series of experiments where the algorithm outperforms vanilla DQN/REINFORCE. There are several things to like about this paper: - The problem of making \"real-world\" practical algorithms for exploration is clearly one of the biggest outstanding problems in reinforcement learning. - The authors have sucessfully gone from ideas, to algorithm, to real robot and their algorithm really seems to outperform the baselines. - The authors clearly make an effort to survey a wide variety of recent papers in the field However, there are several important places where this paper falls down: - In a paper that posits a new, groundbreaking, real-world application of \"exploration\" there is remarkably little discussion of the key issues of \"efficient exploration\". Indeed, I don't think that this paper even presents a clear metric for how we can tell if something *is* a good method for exploration. + This is a huge shortcoming, since we know that it is possible to guarantee polynomial regret bounds for many settings (mostly tabular, but some with function approximation too... see UCRL2, PSRL and more)... there is no discussion of whether the proposed algorithm would also satisfy these bounds? + Of course, this is not a paper designed for \"tabular MDPs\", but we already have exploration algorithms like UCB / Thompson sampling that *are* widely used in online advertising... so why is this method not compared/contrasted to these approaches? - There is very little *science* in this paper, beyond the experiments pitting \"improved algorithm\" vs DQN/REINFORCE, which nobody ever claimed would be a good approach to exploration! I don't think it's possible to assess if their algorithm (which I don't think has a clear name beyond \"sample-efficient exploration formulation\") performs better than the myriad of other exploration approaches listed. Although many intuitive arguments are presented, I did not find these convincing, and the overall narrative ends up being a little jumbled. - A lot of the writing is generally imprecise, and alludes to claims/statements that make no sense to me: + \"... most of these sucesses have been demonstrated in either video games or simulation environments. This is primarily becuase the rewards (even the intrinsic ones) are non-differentiable ...\" + \"Again these approaches have mostly been considered in context of external rewards and hence turn out to be sample inefficient\" I would suggest that each statement/claim is backed up by some material reasoning/statement/experiment unless extremely obvious - at the moment these are not! - Nothing in this algorithm really seems specific to real-world... or at least nothing in the competing algorithms seems to preclude them from being run on a real-world robot... I think that the main issue is that if people want to iterate fast (or don't have a robot) they prefer to do things in simulation. If your point is really that findings from simulation don't translate to real robots, then I think that is really interesting, but I don't see any evidence for that in this paper. Overall, it is clear that this is an interesting area to do work in. The goal of making a practical algorithm for real-world exploration tasks is exciting. However, in its current form, this paper falls well short of the level of science and insight I would expect for ICLR.", "rating": "3: Clear rejection", "reply_text": "We would like to first thank the reviewer for the comments . Please see the meta response in the combined reply for the common concerns among reviewers . We answer other individual concerns below : [ R1 ] : Only comparison to `` DQN/REINFORCE , which nobody ever claimed would be a good approach to exploration ! '' = > We respectfully disagree with the reviewer as there seems to be some misunderstanding . Here , DQN/REINFORCE do not refer to the vanilla RL algorithm but are optimizing the curiosity exploration objective proposed in Pathak et.al . [ 2017 ] .Please see the combined response above for details . We hope that this answers the reviewer 's major concern about the comparison to other exploration methods . [ R1 ] : Comparison to UCB / Thompson sampling ; UCRL2 , PSRL . = > As the reviewer agrees , most of these algorithms have been primarily studied in tabular cases and there have been some extensions to function approximators . These algorithms do not scale to the high dimensional image input ( 320x320x4 ) and a high dimensional action space ( location , angle , push , grasp ) used in our paper . Further , we respectfully believe there is a misunderstanding about the context in which the term `` exploration '' has been used in this paper . Our robotic agent explores the environment completely out of its own ; using only its intrinsic reward and no external reward at all . UCB-like algorithms are parameter-space exploration and are successful in learning a policy in presence of external reward ( even if sparse ) . One can not use UCB when there is no external reward as the value functions would collapse . The algorithms UCRL2/PSRL integrate the prior over dynamics or reward function into learning a policy in a natural and efficient manner . These have been great advancement but not relevant to the problem setup or the goal of this paper because our agent 's start completely from scratch without any prior , and has access to no external rewards . As far as bounds are concerned , our algorithm draws a similarity to Experiment-Design literature from optimization . In a linear case and explicit policy , optimizing our objective is the same as finding a max-variance action which is an efficient way to discover the space satisfied by the model . While this is out of the scope of the paper , we have mentioned the pointers in Appendix Section-D.1 and elaborate more to spur discussion for future work to build upon . [ R1 ] : `` how we can tell if something * is * a good method for exploration '' . = > It is difficult to measure exploration as an end in itself . There are several domain-specific metrics can be created that could act as a useful proxy to measure the quality of exploration . For instance , we measure how frequently the robot touches the object kept on the table purely out its exploration , and found out that this frequency increases to almost perfect as our training proceeds ( please see the video on the website link provided ) . Another proxy to measure exploration is to see if a purely exploration-driven learning without using any external reward can help the agent obtain an external reward , as discussed in ( Burda et.al.2018 ) [ 2 ] . However , this latter proxy is only applicable to games-like environments . In this paper , we argue that the true measure of exploration is how well it is able to contribute to learning planning models later . Using the data collected by the exploration , we build models and report the success of those models for grasping , pushing etc . If the robot explored well during exploration , then it should have gathered good data for learning planning models for manipulation . This discussion is present in Section-5.1 , 5.2 of paper . We will clarify it further . [ R1 ] : `` Nothing ... seems specific to real-world ... or at least nothing in the competing algorithms seems to preclude them from being run on a real-world robot\u2026 '' = > The REINFORCE comparison is indeed existing curiosity-driven exploration from Pathak et.al . [ 2017 ] and being run on the real robot ( please refer to meta combined response above for details ) We further provide a comparison for long-term horizon dependency tasks in video games . Please refer to the combined response for details ."}, {"review_id": "SkzeJ3A9F7-2", "review_text": "Summary: This paper proposes a novel differentiable approximation to the curiosity reward by Pathak et al. that allows a learning agent to optimize a policy for greedy exploration directly by supervised learning, rather than RL. The authors motivate this work with arguments about the sample-efficiency required by real robot learning, and demonstrate basic results using a real robot. Comments: The paper has serious style and tone issues that must be addressed before publication. The rest of my review will focus solely on the technical details. The experimental details are lacking (learning rates? rollout lengths for REINFORCE? what are the inner and outer loops and what are their sizes? what are the plots measuring - extrinsic reward? intrinsic reward? what is \"multi-step learning\" in Table 2?). Without these details, the results will be difficult to validate and reproduce independently. The approach is compared only against very weak baselines. Why vanilla REINFORCE and not any of the modern policy-gradient algorithms (A3C, PPO, TRPO, DDPG, ...)? The ability to deal with this large action space is certainly impressive, but it is likely far too large for DQN or REINFORCE to work, so the comparison is questionable to begin with: you can make any algorithm fail if you give it an unnecessarily difficult interface. Did the authors try a smaller, more traditional action space? The authors claim the sample-efficiency improvements by many existing exploration approaches are insignificant. In what way are the results in this paper more significant? Table 1 shows very minor improvements to a MPC planning task, Appendix Figure 1 shows barely any improvement over the baseline, and Appendix Figure 4 shows that learning from extrinsic rewards using REINFORCE seems to work just fine. Why use intrinsic rewards at all in this case? It appears that maybe some of the results look significant because the baselines are so weak. The paper contains many factual errors and unsupported claims. For example: - \"the field of RL was born out of need to make our robots learn\" - \"none of the recent advances have translated to success in the field of robotics\" (see e.g. the proceedings of CoRL 2017 and 2018) - \"Building a good model will require enormous number of interactions\" (see e.g. PILCO) - \"[our approach enables us to] for the first time ever, implement exploration on a real-world physical robot\" (PILCO and many others) - describing Pathak et al. curiosity as a \"gaussian density model\" in eq1; it's a deterministic forward model - in sec3, \"regress r^i_t to learn value estimates\", this is probably meant to be the discounted sum of rewards - also sec3, \"[REINFORCE] gives no signal as to what action to take\"; the signal has high variance but it works (see all policy gradient work) These errors can be easily corrected. However, the contribution of the paper is based on a more serious error: - sec3.1, \"If the policy could be optimized using direct gradients, the rewarder could ... inform the agent to change its action space in the direction where forward prediction loss is high.\" This is incorrect. The paper is based on using the gradient of the forward model to directly optimize the policy to produce higher prediction errors, as in Pathak et al. But in order to make the prediction error differentiable, it makes the severe assumption that the next state x_{t+1} is constant and does not depend on a_t, which is false and invalidates the idea of optimizing actions for prediction error. As a result, the gradient obtained does not actually move the policy toward higher prediction errors. To understand what the author's approximation actually does, consider a perfect forward model. No matter what actions the policy produces, the prediction error is always zero, but the authors' gradient is not. So it can't be optimizing for higher prediction errors. Instead of optimizing for high prediction errors as the authors claim, the policy is being optimized for state transitions that are maximally different from the observed state x_{t+1}. This is an interesting objective to optimize. I can see how it could result in interesting exploration. But it's not what the authors say they're proposing. It's much more like a count-based exploration strategy, which prefers visiting states that are maximally different from the states visited so far. It is much less like the prediction-error based curiosity of Pathak et al. that the authors are motivated by. I would like to see focused analysis of this particular objective. For example, would this not result in the policy oscillating between different parts of the state space, since it's only optimizing for maximal difference to what it just saw, rather than long-term knowledge gain? This issue requires more discussion. Finally, the approach is not really robot-specific despite the title and arguments in the paper. I recommend pursuing a more general investigation, because if this objective is truly as effective as the authors believe, then it should be applicable in a wide variety of domains (many of which are very easy to evaluate in: Atari, OpenAI Gym, DMLab, VizDoom, Mujoco, etc.). Conclusion: The paper proposes an interesting new objective, but it is motivated by a very naive approximation that completely changes the behavior of the exploration compared to what the authors want to approximate. The idea is novel and worth exploring, but the paper should be heavily rewritten to emphasize what the authors are actually doing with this new objective, and should include thorough analysis of its behavior, before I can recommend acceptance.", "rating": "3: Clear rejection", "reply_text": "We would like to first thank the reviewer for the comments . These comments [ R3 ] : `` The experimental details are lacking '' = > We agree that these details are important for the reproducibility of work , and we will include them in supplementary -- we apologize that we missed adding them in the supplementary . That being said , we will make our code , environment , and models public upon release . The learning rate for the model was 5e-4 and policy 1e-4 with an entropy loss coefficient of 1e-4 . The optimizer used was Adam . We used standard DQN algorithm with prioritized experience replay [ Schaul et . al.2016 ] and epsilon-greedy to maximize the curiosity objective [ Pathak et.al . 2017 ] .Q-value was trained with a learning rate of 1e-4 . In epsilon-greedy , epsilon was initialized at 0.5 then annealed over training to 0.1 . [ R3 ] : What are the plots measuring - extrinsic reward ? intrinsic reward ? = > The plots are measuring the how frequently the robotic arm touches the object which is a proxy for measuring how good the exploration is . Note that this reward was not used for training the policy , but just used to measure the quality of exploration at training . The robot was trained with an intrinsic reward only . [ R3 ] : `` The approach is compared only against very weak baselines . '' [ R3 ] : `` I recommend pursuing a more general investigation .. to evaluate in : Atari '' = > Inspired by reviewer 's suggestion , we have added a comparison on games in the supplementary using the state-of-the-art implementation proposed couple of months ago in ( Burda et.al.2018 ) [ 2 ] . Please see the combined response above for more details . Our paper is just a first step towards explicitly using the structure of the prediction model in training the policy in a supervised manner . It relies on the model being good which is true for short-horizon tasks , but learning good long-term models is still an active research area [ Finn et.al. , 2017 , Ebert et.al . 2017 ] .Hence , our approach might provide diminishing gains for long-horizon tasks . [ R3 ] : `` this large action space is certainly impressive , but it is likely far too large for DQN or REINFORCE '' = > Our fundamental goal is to test exploration algorithms on real robotics setup where it is crucial to experiment in presence of large input space as well as large action space . We believe working with larger action space should be seen as a positive feature , and not something made to fail baselines . However , upon the reviewer 's suggestion , we have added a comparison on games in the supplementary . Please see the combined response above for more details ."}], "0": {"review_id": "SkzeJ3A9F7-0", "review_text": "This paper presents an interesting way to reformulate intrinsic curiosity as a differentiable function. The authors compare the differentiable function against using prediction error via REINFORCE and DQN, showing that their intrinsic curiosity method results in more interactions with unseen objects than the other two methods. For DQN this is to be expected, but it shows that backprop through this function is more efficient than reinforce in getting to unseen state spaces. I think this is an interesting method/proposal and is a somewhat novel reformulation of intrinsic error, but I do have some concerns in comparisons/claims. In the introduction, the authors say that the intrinsic curiosity method proposed by Pathak et al. is sample inefficient and isn\u2019t tested in robots. However, to my understanding the REINFORCE baseline isn\u2019t really equivalent (though it may be possible that it is, it was unclear how exactly the loss was formulated in the baseline, did include the other components from Pathak et al.?). If the claim is that this method is more efficient, I think it should have compared against that method directly. Moreover, I think the description of the experiments doesn\u2019t provide enough information. For example, the method says that different learning rates were used for the min-max game to stabilize it, but doesn\u2019t say what they were. Also, for the DQN baseline what were the parameters? Was there an epsilon greedy policy on top of the exploration reward? Was this annealed as in other work? Generally, I think more detail is needed throughout (even if it just refers to a more detailed appendix). Overall, I think this work needs to be revised to include more details on hyperaparameters, details on the baselines, and describing differences between Pathak et al.\u2019s method and the REINFORCE baseline. Moreover, feedback from other comments on this work should be addressed which reflect in more detail my comments below on opinionated claims (e.g., https://openreview.net/forum?id=SkzeJ3A9F7&noteId=HJlFlZOa2X ) Comments/Thoughts: + I think in the introduction there are some statements that probably need citations. For example, \u201cBut the same formulation from an optimization viewpoint, it suffers from all the bad properties of extrinsic rewards. The reward is a function of environment behavior with respect to the performed action. Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.\u201d \u2014> Why is this true? Is there a citation that can back this? Do you prove it later in the paper? + \u201cYes, 54 environments but no real-world physical robots\u201d \u2014> this and the intro seems like a blogpost at times. That can be fine (some would argue it\u2019s a good thing), but there seem to be some opinions without citations/backing, I suggest trying to back up statements wherever possible and avoid opinions. For example in this statement, robots aren't a requirement for evaluating intrinsic motivation. + \u201cSince the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.\u201d \u2014> citation/backing? it might be nice to point to the experiment section here to back it (e.g., \"As will be shown in Section X and in \\citet{something}, REINFORCE can be quite sample inefficient\") + \u201cIn practice, the existing on-policy algorithms, e.g., A3C (Mnih et al., 2016), PPO (Schulman et al., 2017) etc. are deployed off-the shelf -> This is confusing, so is this using REINFORCE or PPO/A3C? what is this statement referring to? + \u201cregress to rti to learn value estimates (i.e., off-policy) as discussed in the previous section\u201d \u2014> regress to \\sum r_t{I} for a value estimate?? Value is the expected return so not sure if this is a typo or i missed something earlier + What is the actual loss function used for the baseline? Is it the same as Pathak et al.? + What are the hyper parameters for DQN exploration? What are all the hyper parameters for any/all the algorithms? + Was a variance-reducing baseline used in REINFORCE? + What is the variance representing in the graphs, std across several trials? Maybe I missed it, but how many trials represent this standard deviation? + \u201cHence, we train the forward predictor slightly faster than the policy by keeping higher learning rate to stabilize the learning process. \u201c \u2014> what were the learning rates? Linguistic/Typos: Also, some minor, but frequent, grammatical issues/typos that I\u2019ve added below could be fixed. I would ask that the authors please have the submission proof-read for English style and grammar issues. There are many minor mistakes, some of which I\u2019ve tried to point out below. + \u201cThis leads to a significantly sample efficient exploration policy. \u201c \u2014> significantly more (?) sample efficient ? \u201cWhy is that? To understand the reason behind sample inefficiency of curiosity or intrinsic rewards, notice how the intrinsic rewards are given by agent\u201d \u2014> by the agent? \u201cForward model f\u03b8F is trained to minimize its loss which amounts to minimizing rti with respect to \u03b8F\u201d \u2014> the forward model \u201cHowever, policy is optimized to maximize the objective\u201d \u2014> However, the policy \u201cWe can also optimize for policy parameters \u03b8P via differentiable loss function\u201d \u2014> We can also optimize for (the) policy parameters \\theta via (a) differentiable loss function? \u201cTo optimize policy to maximize a discounted sum \u201c \u2014> To optimize the policy \u201cHow good is Forward Prediction Model\u201d \u2014> How good is the forward prediction model There are several other spots, but basically another pass over the paper might be worth it to check for these sorts of issues. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to first thank the reviewer for the comments . Please see the meta response in the combined reply for the common concerns among reviewers . The reviewer found the formulation to be `` interesting '' and `` novel '' but has raised concerns about lack of details about the experiment hyper-parameters . We answer them below : [ R2 ] : REINFORCE baseline vs. Pathak et . al . ? Is it the same as Pathak et al . ? = > Please refer to the combined response to all reviewers above . It is indeed the same . [ R2 ] : `` different learning rates .. but does n't say what they were '' [ R2 ] : `` What were DQN parameters ? Epsilon greedy '' [ R2 ] : `` include more details on hyper-parameters ... '' = > Yes , in our case , we can simply train the forward model faster than policy to stabilize the training . The learning rate for the model was 5e-4 and policy 1e-4 with an entropy loss coefficient of 1e-4 . The optimizer used was Adam . Yes , we used standard DQN algorithm with prioritized experience replay [ Schaul et . al.2016 ] and epsilon-greedy to maximize the curiosity objective [ Pathak et.al . 2017 ] .Q-value was trained with a learning rate of 1e-4 . In epsilon-greedy , epsilon was initialized at 0.5 then annealed over training to 0.1 . We agree that these details are important for the reproducibility of work , and we will include them in supplementary -- we apologize that we missed adding them in the supplementary . That being said , we will make our code , environment , and models public upon release . [ R2 ] : `` Feedback from the other commentator should be addressed '' = > We have answered both the factual and stylistic comments with proper citations in our reply . [ R2 ] : `` ... This is confusing , so is this using REINFORCE or PPO/A3C ? .. '' [ R2 ] : `` it might be nice to point to the experiment section here to back '' = > With due respect , we believe reviewer has misunderstood the background here . REINFORCE is an operator that was proposed in ( Williams , 1992 ) , and then became the main ingredient of policy gradient algorithms . All policy gradient algorithms , whether PPO or A3C , use reinforce operator to update the policy . PPO , in addition to reward maximization , penalizes KL-divergence change while A3C relies on regularization from multiple workers , but both these algorithms use reinforce at the core of it . [ Schulman et.al . 2016 ] provides a nice discussion on how REINFORCE helps to optimize any black-box function in expectation using only samples from the function . More detailed and fundamentals of policy gradients are described in [ Sutton & Barto , 1998 ] . We implemented the policy-gradient algorithm as optimizing the curiosity objective in [ Pathak et.al . 2017 ] as a baseline which we call `` REINFORCE '' . We will update the legend of graphs to say [ Pathak et.al . 2017 ] in future to avoid confusion . [ R2 ] : `` regress to rti .. discussed in the previous section .. so not sure if this is a typo '' = > Thanks , it should be indeed summed . However , note that in the previous section it was mentioned correctly . We will correct this typo . [ R2 ] : `` Some minor , but frequent , grammatical issues/typos .. '' = > Thank you . We will fix all these writing typos and make sure there no other ones ."}, "1": {"review_id": "SkzeJ3A9F7-1", "review_text": "Beyond Games: Bringing Exploration to Robots in Real-world =========================================================== This paper tackles the laudable goal of making an algorithm for efficient exploration in \"real-world\" RL. To do this, they augment the \"curiosity\" algorithm of Pathak et al with a differentiable approximation to the reward prediction model. They motivate this algorithm through several intuitive arguments together with a series of experiments where the algorithm outperforms vanilla DQN/REINFORCE. There are several things to like about this paper: - The problem of making \"real-world\" practical algorithms for exploration is clearly one of the biggest outstanding problems in reinforcement learning. - The authors have sucessfully gone from ideas, to algorithm, to real robot and their algorithm really seems to outperform the baselines. - The authors clearly make an effort to survey a wide variety of recent papers in the field However, there are several important places where this paper falls down: - In a paper that posits a new, groundbreaking, real-world application of \"exploration\" there is remarkably little discussion of the key issues of \"efficient exploration\". Indeed, I don't think that this paper even presents a clear metric for how we can tell if something *is* a good method for exploration. + This is a huge shortcoming, since we know that it is possible to guarantee polynomial regret bounds for many settings (mostly tabular, but some with function approximation too... see UCRL2, PSRL and more)... there is no discussion of whether the proposed algorithm would also satisfy these bounds? + Of course, this is not a paper designed for \"tabular MDPs\", but we already have exploration algorithms like UCB / Thompson sampling that *are* widely used in online advertising... so why is this method not compared/contrasted to these approaches? - There is very little *science* in this paper, beyond the experiments pitting \"improved algorithm\" vs DQN/REINFORCE, which nobody ever claimed would be a good approach to exploration! I don't think it's possible to assess if their algorithm (which I don't think has a clear name beyond \"sample-efficient exploration formulation\") performs better than the myriad of other exploration approaches listed. Although many intuitive arguments are presented, I did not find these convincing, and the overall narrative ends up being a little jumbled. - A lot of the writing is generally imprecise, and alludes to claims/statements that make no sense to me: + \"... most of these sucesses have been demonstrated in either video games or simulation environments. This is primarily becuase the rewards (even the intrinsic ones) are non-differentiable ...\" + \"Again these approaches have mostly been considered in context of external rewards and hence turn out to be sample inefficient\" I would suggest that each statement/claim is backed up by some material reasoning/statement/experiment unless extremely obvious - at the moment these are not! - Nothing in this algorithm really seems specific to real-world... or at least nothing in the competing algorithms seems to preclude them from being run on a real-world robot... I think that the main issue is that if people want to iterate fast (or don't have a robot) they prefer to do things in simulation. If your point is really that findings from simulation don't translate to real robots, then I think that is really interesting, but I don't see any evidence for that in this paper. Overall, it is clear that this is an interesting area to do work in. The goal of making a practical algorithm for real-world exploration tasks is exciting. However, in its current form, this paper falls well short of the level of science and insight I would expect for ICLR.", "rating": "3: Clear rejection", "reply_text": "We would like to first thank the reviewer for the comments . Please see the meta response in the combined reply for the common concerns among reviewers . We answer other individual concerns below : [ R1 ] : Only comparison to `` DQN/REINFORCE , which nobody ever claimed would be a good approach to exploration ! '' = > We respectfully disagree with the reviewer as there seems to be some misunderstanding . Here , DQN/REINFORCE do not refer to the vanilla RL algorithm but are optimizing the curiosity exploration objective proposed in Pathak et.al . [ 2017 ] .Please see the combined response above for details . We hope that this answers the reviewer 's major concern about the comparison to other exploration methods . [ R1 ] : Comparison to UCB / Thompson sampling ; UCRL2 , PSRL . = > As the reviewer agrees , most of these algorithms have been primarily studied in tabular cases and there have been some extensions to function approximators . These algorithms do not scale to the high dimensional image input ( 320x320x4 ) and a high dimensional action space ( location , angle , push , grasp ) used in our paper . Further , we respectfully believe there is a misunderstanding about the context in which the term `` exploration '' has been used in this paper . Our robotic agent explores the environment completely out of its own ; using only its intrinsic reward and no external reward at all . UCB-like algorithms are parameter-space exploration and are successful in learning a policy in presence of external reward ( even if sparse ) . One can not use UCB when there is no external reward as the value functions would collapse . The algorithms UCRL2/PSRL integrate the prior over dynamics or reward function into learning a policy in a natural and efficient manner . These have been great advancement but not relevant to the problem setup or the goal of this paper because our agent 's start completely from scratch without any prior , and has access to no external rewards . As far as bounds are concerned , our algorithm draws a similarity to Experiment-Design literature from optimization . In a linear case and explicit policy , optimizing our objective is the same as finding a max-variance action which is an efficient way to discover the space satisfied by the model . While this is out of the scope of the paper , we have mentioned the pointers in Appendix Section-D.1 and elaborate more to spur discussion for future work to build upon . [ R1 ] : `` how we can tell if something * is * a good method for exploration '' . = > It is difficult to measure exploration as an end in itself . There are several domain-specific metrics can be created that could act as a useful proxy to measure the quality of exploration . For instance , we measure how frequently the robot touches the object kept on the table purely out its exploration , and found out that this frequency increases to almost perfect as our training proceeds ( please see the video on the website link provided ) . Another proxy to measure exploration is to see if a purely exploration-driven learning without using any external reward can help the agent obtain an external reward , as discussed in ( Burda et.al.2018 ) [ 2 ] . However , this latter proxy is only applicable to games-like environments . In this paper , we argue that the true measure of exploration is how well it is able to contribute to learning planning models later . Using the data collected by the exploration , we build models and report the success of those models for grasping , pushing etc . If the robot explored well during exploration , then it should have gathered good data for learning planning models for manipulation . This discussion is present in Section-5.1 , 5.2 of paper . We will clarify it further . [ R1 ] : `` Nothing ... seems specific to real-world ... or at least nothing in the competing algorithms seems to preclude them from being run on a real-world robot\u2026 '' = > The REINFORCE comparison is indeed existing curiosity-driven exploration from Pathak et.al . [ 2017 ] and being run on the real robot ( please refer to meta combined response above for details ) We further provide a comparison for long-term horizon dependency tasks in video games . Please refer to the combined response for details ."}, "2": {"review_id": "SkzeJ3A9F7-2", "review_text": "Summary: This paper proposes a novel differentiable approximation to the curiosity reward by Pathak et al. that allows a learning agent to optimize a policy for greedy exploration directly by supervised learning, rather than RL. The authors motivate this work with arguments about the sample-efficiency required by real robot learning, and demonstrate basic results using a real robot. Comments: The paper has serious style and tone issues that must be addressed before publication. The rest of my review will focus solely on the technical details. The experimental details are lacking (learning rates? rollout lengths for REINFORCE? what are the inner and outer loops and what are their sizes? what are the plots measuring - extrinsic reward? intrinsic reward? what is \"multi-step learning\" in Table 2?). Without these details, the results will be difficult to validate and reproduce independently. The approach is compared only against very weak baselines. Why vanilla REINFORCE and not any of the modern policy-gradient algorithms (A3C, PPO, TRPO, DDPG, ...)? The ability to deal with this large action space is certainly impressive, but it is likely far too large for DQN or REINFORCE to work, so the comparison is questionable to begin with: you can make any algorithm fail if you give it an unnecessarily difficult interface. Did the authors try a smaller, more traditional action space? The authors claim the sample-efficiency improvements by many existing exploration approaches are insignificant. In what way are the results in this paper more significant? Table 1 shows very minor improvements to a MPC planning task, Appendix Figure 1 shows barely any improvement over the baseline, and Appendix Figure 4 shows that learning from extrinsic rewards using REINFORCE seems to work just fine. Why use intrinsic rewards at all in this case? It appears that maybe some of the results look significant because the baselines are so weak. The paper contains many factual errors and unsupported claims. For example: - \"the field of RL was born out of need to make our robots learn\" - \"none of the recent advances have translated to success in the field of robotics\" (see e.g. the proceedings of CoRL 2017 and 2018) - \"Building a good model will require enormous number of interactions\" (see e.g. PILCO) - \"[our approach enables us to] for the first time ever, implement exploration on a real-world physical robot\" (PILCO and many others) - describing Pathak et al. curiosity as a \"gaussian density model\" in eq1; it's a deterministic forward model - in sec3, \"regress r^i_t to learn value estimates\", this is probably meant to be the discounted sum of rewards - also sec3, \"[REINFORCE] gives no signal as to what action to take\"; the signal has high variance but it works (see all policy gradient work) These errors can be easily corrected. However, the contribution of the paper is based on a more serious error: - sec3.1, \"If the policy could be optimized using direct gradients, the rewarder could ... inform the agent to change its action space in the direction where forward prediction loss is high.\" This is incorrect. The paper is based on using the gradient of the forward model to directly optimize the policy to produce higher prediction errors, as in Pathak et al. But in order to make the prediction error differentiable, it makes the severe assumption that the next state x_{t+1} is constant and does not depend on a_t, which is false and invalidates the idea of optimizing actions for prediction error. As a result, the gradient obtained does not actually move the policy toward higher prediction errors. To understand what the author's approximation actually does, consider a perfect forward model. No matter what actions the policy produces, the prediction error is always zero, but the authors' gradient is not. So it can't be optimizing for higher prediction errors. Instead of optimizing for high prediction errors as the authors claim, the policy is being optimized for state transitions that are maximally different from the observed state x_{t+1}. This is an interesting objective to optimize. I can see how it could result in interesting exploration. But it's not what the authors say they're proposing. It's much more like a count-based exploration strategy, which prefers visiting states that are maximally different from the states visited so far. It is much less like the prediction-error based curiosity of Pathak et al. that the authors are motivated by. I would like to see focused analysis of this particular objective. For example, would this not result in the policy oscillating between different parts of the state space, since it's only optimizing for maximal difference to what it just saw, rather than long-term knowledge gain? This issue requires more discussion. Finally, the approach is not really robot-specific despite the title and arguments in the paper. I recommend pursuing a more general investigation, because if this objective is truly as effective as the authors believe, then it should be applicable in a wide variety of domains (many of which are very easy to evaluate in: Atari, OpenAI Gym, DMLab, VizDoom, Mujoco, etc.). Conclusion: The paper proposes an interesting new objective, but it is motivated by a very naive approximation that completely changes the behavior of the exploration compared to what the authors want to approximate. The idea is novel and worth exploring, but the paper should be heavily rewritten to emphasize what the authors are actually doing with this new objective, and should include thorough analysis of its behavior, before I can recommend acceptance.", "rating": "3: Clear rejection", "reply_text": "We would like to first thank the reviewer for the comments . These comments [ R3 ] : `` The experimental details are lacking '' = > We agree that these details are important for the reproducibility of work , and we will include them in supplementary -- we apologize that we missed adding them in the supplementary . That being said , we will make our code , environment , and models public upon release . The learning rate for the model was 5e-4 and policy 1e-4 with an entropy loss coefficient of 1e-4 . The optimizer used was Adam . We used standard DQN algorithm with prioritized experience replay [ Schaul et . al.2016 ] and epsilon-greedy to maximize the curiosity objective [ Pathak et.al . 2017 ] .Q-value was trained with a learning rate of 1e-4 . In epsilon-greedy , epsilon was initialized at 0.5 then annealed over training to 0.1 . [ R3 ] : What are the plots measuring - extrinsic reward ? intrinsic reward ? = > The plots are measuring the how frequently the robotic arm touches the object which is a proxy for measuring how good the exploration is . Note that this reward was not used for training the policy , but just used to measure the quality of exploration at training . The robot was trained with an intrinsic reward only . [ R3 ] : `` The approach is compared only against very weak baselines . '' [ R3 ] : `` I recommend pursuing a more general investigation .. to evaluate in : Atari '' = > Inspired by reviewer 's suggestion , we have added a comparison on games in the supplementary using the state-of-the-art implementation proposed couple of months ago in ( Burda et.al.2018 ) [ 2 ] . Please see the combined response above for more details . Our paper is just a first step towards explicitly using the structure of the prediction model in training the policy in a supervised manner . It relies on the model being good which is true for short-horizon tasks , but learning good long-term models is still an active research area [ Finn et.al. , 2017 , Ebert et.al . 2017 ] .Hence , our approach might provide diminishing gains for long-horizon tasks . [ R3 ] : `` this large action space is certainly impressive , but it is likely far too large for DQN or REINFORCE '' = > Our fundamental goal is to test exploration algorithms on real robotics setup where it is crucial to experiment in presence of large input space as well as large action space . We believe working with larger action space should be seen as a positive feature , and not something made to fail baselines . However , upon the reviewer 's suggestion , we have added a comparison on games in the supplementary . Please see the combined response above for more details ."}}