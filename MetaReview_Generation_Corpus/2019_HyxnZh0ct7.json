{"year": "2019", "forum": "HyxnZh0ct7", "title": "Meta-learning with differentiable closed-form solvers", "decision": "Accept (Poster)", "meta_review": "The reviewers disagree strongly on this paper. Reviewer 2 was the most positive, believing it to be an interesting contribution with strong results. Reviewer 3 however, was underwhelmed by the results. Reviewer 1 does not believe that the contribution is sufficiently novel, seeing it as too close to existing multi-task learning approaches.\n\nAfter considering all of the discussion so far, I have to agree with reviewer 2 on their assessment. Much of the meta learning literature involves changing the base learner *for a fixed architecture* and seeing how it affects performance. There is a temptation to chase performance by changing the architecture, adding new regularizers, etc., and while this is important for practical reasons, it does not help to shed light on the underlying fundamentals. This is best done by considering carefully controlled and well understood experimental settings. Even still, the performance is quite good relative to popular base learners.\n\nRegarding novelty, I agree it is a simple change to the base learner, using a technique that has been tried before in other settings (linear regression as opposed to classification), however its use in a meta learning setup is novel in my opinion, and the new experimental comparison regression on top of pre-trained CNN features helps to demonstrate the utility of its use in the meta-learning settings.\n\nWhile the novelty can certainly be debated, I want to highlight two reasons why I am opting to accept this paper: 1) simple and effective ideas are often some of the most impactful. 2) sometimes taking ideas from one area (e.g., multi-task learning) and demonstrating that they can be effective in other settings (e.g., meta-learning) can itself be a valuable contribution. I believe that the meta-learning community would benefit from reading this paper.\n", "reviews": [{"review_id": "HyxnZh0ct7-0", "review_text": "This paper proposes a new meta-learning method based on closed-form solutions for task specific classifiers such as ridge regression and logistic regression (iterative). The idea of the paper is quite interesting, comparing to the existing metric learning based methods and optimization based methods. I have two concerns on this paper. First, the motivation and the rationale of the proposed approach is not clear. In particular, why one can simply treat \\hat{Y} as a scaled and shifted version of X\u2019W? Second, the empirical performance of the proposed approach is not very promising and it does not outperform the comparison methods, e.g., SNAIL. It is not clear what is the advantage. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments and questions . > \u201c Why one can simply treat \\hat { Y } as a scaled and shifted version of X \u2019 W ? \u201d In the case of logistic regression , the scaling and shifting is not needed , and we have \\hat { Y } =X \u2019 W . This is because logistic regression is a classification algorithm , and directly outputs class scores . These scores are fed to the ( cross-entropy ) loss L. However , ridge regression is a regression algorithm , and its regression targets are one-hot encoded labels , which is only an approximation of the discrete problem ( classification ) . This means that an extra calibration step is needed ( eq.6 ) , to allow the network to tune the regressed outputs into classification scores for the cross-entropy loss L. > \u201c The empirical performance of the proposed approach is not very promising and it does not outperform the comparison methods , e.g. , SNAIL \u201d Our method actually outperforms SNAIL on an apples-to-apples comparison , with the same number of layers . We would like to draw the reviewer \u2019 s attention to the last paragraph of the \u201c Multi-class classification \u201d subsection ( page 8 ) . The result mentioned by the reviewer uses a ResNet , while we use a 4-layer CNN to remain comparable to prior work . SNAIL with a 4-layer CNN ( [ 11 ] Appendix B ) performs much worse than our method ( 7.4 % to 10.0 % accuracy improvement ) . Even disregarding the great difference in architecture capacity , our proposal 's performance coincides with SNAIL on miniImageNet 5way-5shot and it is comparable on 3 out of 4 Omniglot setups . We would have liked to establish a comparison also on CIFAR , but unfortunately the official code for SNAIL hasn \u2019 t been released . Borrowing the words of AnonReviewer2 : \u201c Notably , the ridge regression variant can reach results competitive with SNAIL that uses significantly more weights and is shown to suffer when its capacity is reduced. \u201d We hope that this addresses the two concerns raised by the reviewer . We will be happy to answer any other question about the paper ."}, {"review_id": "HyxnZh0ct7-1", "review_text": "Summary: The paper proposes an algorithm for meta-learning which amounts to fixing the features (ie all hidden layers of a deep NN), and treating each task as having its own final layer which could be a ridge regression or a logistic regression. The paper also proposes to separate the data for each task into a training set used to optimize the last, task specific layer, and a validation set used to optimize all previous layers and hyper parameters. Novelty: This reviewer is unsure what the paper claims as a novel contribution. In particular training multi-task neural nets with shared feature representation and task specific final layer is probably 20-30 years old by now and entirely common. It is also common freeze the feature representation learned from the first set of tasks, and to simply use it for new tasks by modifying the last (few) layer(s) which would according to this paper qualify as meta-learning since the new task can be learned with very few new examples. ", "rating": "2: Strong rejection", "reply_text": "We thank the reviewer for the comment . However , we believe that the low score originates from a misunderstanding of our proposal . Below , we try to bring some clarity by disambiguating between what the reviewer refers to and our method . If our interpretation of what the reviewer refers to as \u201c entirely common \u201d is incorrect , it would be great to be provided with at least one reference , so that we can continue the conversation on the same ground . > \u201c novel contribution ? \u201d , \u201c training multi-task neural nets with shared feature representation and task specific final layer is probably 20-30 years old by now and entirely common. \u201d \u201c It is also common freeze the feature representation learned from the first set of tasks , and to simply use it for new tasks by modifying the last layer \u201d We understand that the reviewer is hinting at the common multi-task scenario with a shared network and task-specific layers ( e.g.Caruana 1993 ) . He/she also refers to basic transfer learning approaches in which a CNN is first pre-trained on one dataset/task and then adapted to a different dataset/task by simply adapting the final layer ( s ) ( e.g.Yosinski et al. \u201c How transferable are features in deep neural Networks ? \u201d - NIPS 2014 ; Chu et al. \u201c Best Practices for Fine-tuning Visual Classifiers to New Domains \u201d - ECCVw 2016 ) . If so , then this is significantly different to our work , and in general to all of the previous literature on meta-learning applied to few-shot classification ( e.g.Finn et al.2017 , Ravi & Larochelle 2017 , Vinyals et al.2016 , etc ) . Notably , these methods and ours take into account adaptation * already during the training process * , which requires back-propagating errors through the very fine-tuning process . Within this setup , our main contribution is to propose an adaptation procedure based on closed-form regressors , which have the important characteristic of allowing different models for different episodes while still being fast because of 1 ) their convergence in one ( R2-D2 ) or few ( LR-D2 ) steps , 2 ) the use of the Woodbury identity , which is particularly convenient in the few-shot data regime , and 3 ) back-propagation through the closed-form regressor can be made efficient . To better illustrate our point , we conducted a baseline experiment . First , we pre-trained the same 4-layers CNN architecture , but for a standard classification problem , using the same training samples as our method . We simply added a final fully-connected layer ( with 64 outputs , like the number of classes in the training splits ) and used the cross-entropy loss . Then , we used the convolutional part of this trained network as a feature extractor and fed its activation to our ridge-regression layer to produce a per-episode set of weights . On miniImagenet , the drop in performance w.r.t.our proposed R2-D2 is very significant : 13.8 % and 11.6 % accuracy for the 1 and 5 shot problems respectively . Results are consistent on CIFAR , though less drastic : 11.5 % and 5.9 % . This confirms that simply using a \u201c shared feature representation and task specific final layer \u201d as commented by the reviewer is not what we are doing and it is not a good strategy to obtain results competitive with the state-of-the-art in few-shot classification . Instead , it is necessary to enforce the generality of the underlying features during training explicitly , which we do by back-propagating through the fine-tuning procedure ( the closed-form regressors ) . We would like to conclude remarking that , probably , the source of confusion arises from the overlap that exists in general between the few-shot learning and the transfer/multi-task learning sub-communities . We realize that the two have developed fairly separately while trying to solve very related problems , and unfortunately the similarities/differences are not acknowledged enough in few-shot classification papers , including our own . We intend to alleviate this problem in our related work section , and invite the reviewer to suggest more relevant works from this area ."}, {"review_id": "HyxnZh0ct7-2", "review_text": "This paper proposes a meta-learning approach for the problem of few-shot classification. Their method, based on parametrizing the learner for each task by a closed-form solver, strikes an interesting compromise between not performing any adaptation for each new task (as is the case in pure metric learning methods) and performing an expensive iterative procedure, such as MAML or Meta-Learner LSTM where there is no guarantee that after taking the few steps prescribed by the respective algorithms the learner has converged. For this reason, I find that leveraging existing solvers that admit closed-form solutions is an attractive and natural choice. Specifically, they propose ridge regression as their closed-form solver (R2-D2 variant). This is easily incorporated into the meta-learning loop with any hyperparameters of this solver being meta-learned, along with the embedding weights as is usually done. The use of the Woodbury equation allows to rewrite the closed form solution in a way that scales with the number of examples instead of the dimensionality of the features; therefore taking advantage of the fact that we are operating in a few-shot setting. While regression may seem to be a strange choice for eventually solving a classification task, it is used as far as I understand due to the availability of this widely-known closed-form solution. They treat the one-hot encoded labels of the support set as the regression targets, and additionally calibrate the output of the network (via a transformation by a scale and bias) in order to make it appropriate for classification. Based on the loss of ridge regression on the support set of a task, a parameter matrix is learned for that task that maps from the embedding dimensionality to the number of classes. This matrix can then be used directly to multiply the embedded (via the fixed for the purposes of the episode embedding function) query points, and for each query point, the entry with the maximum value in the corresponding row of the resulting matrix will constitute the predicted class label. They also experimented with a logistic regression variant (LR-D2) that does not admit a closed-form solution but can be solved efficiently via Newton\u2019s Method under the form of Iteratively Reweighted Least Squares. When using this variant they restrict to tackling the case of binary-classification. A question that comes to mind about the LR-D2 variant: while I understand that a single logistic regression classifier is only capable of binary classification, there seems to be a straightforward extension to the case of multiple classes, where one classifier per class is learned, leading to a total of N one-vs-all classifiers (where N is the way of the episode). I\u2019m curious how this would compare in terms of performance against the ridge regression variant which is naturally multi-class. This would allow to directly apply this variant in the common setting and would enable for example still oversampling classes at meta-training time as is done usually. I would also be curious to see an ablation where for the LR-D2 variant SGD was used as the optimizer instead of Newton\u2019s method. That variant may require more steps (similar to MAML), but I\u2019m curious in practice how this performs. A few other minor comments: - In the related work section, the authors write: \u201cOn the other side of the spectrum, methods that optimize standard iterative learning algorithms, [...] are accurate but slow.\u201d Note however that neither MAML nor MetaLearner LSTM have been showed to be as effective as Prototypical Networks for example. So I wouldn\u2019t really present this as a trade-off between accuracy and speed. - I find the term multinomial classification strange. Why not use multi-class classification? - In page 8, there is a sentence that is not entirely grammatically correct: \u2018Interestingly, increasing the capacity of the other method it is not particularly helpful\u2019. Overall, I think this is good work. The idea is natural and attractive. The writing is clear and comprehensive. I enjoyed how the explanation of meta learning and the usual episodic framework was presented. I found the related work section thorough and accurate too. The experiments are thorough as well, with appropriate ablations to account for different numbers of parameters used between different methods being compared. This approach is evidently effective for few-shot learning, as demonstrated on the common two benchmarks as well as on a newly-introduced variant of cifar that is tailored to few-shot classification. Notably, the ridge regression variant can reach results competitive with SNAIL that uses significantly more weights and is shown to suffer when its capacity is reduced. Interestingly, other models such as MAML actually suffer when given additional capacity, potentially due to overfitting. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the insightful comments and analysis . > \u201c One-vs-all classifiers \u201d for LR-D2 This is a great suggestion , and we are not quite sure how we missed it . We will update the results for 5-way classification incorporating this method . > \u201c ablation where for the LR-D2 variant SGD was used ... instead of Newton \u2019 s method \u201d We previously did exactly this experiment , although for the R2-D2 ( ridge regression ) variant . We did not include it due to space constraints . It is equivalent to MAML , which also uses SGD , but adapting only the classification layer for new tasks ( instead of adapting all parameters ) . We tested this variant on miniImageNet with 5 classes , with the lowest-capacity CNN ( which is the most favorable model for MAML/SGD ) . It yields 45.4\u00b11.6 % accuracy for 1-shot classification and 61.7\u00b11.0 % for 5-shot classification . Comparing it to Table 1 , there \u2019 s a drop in performance compared to our closed form solver ( 3.5 % and 4.4 % less accuracy , respectively ) , and also compared to the original MAML ( 3.3 % and 1.4 % respectively ) . Although we expect the conclusions for logistic regression ( LR-D2 ) to be similar , we will extend the experiment to this case and report the results . > \u201c Neither MAML nor MetaLearner LSTM have been showed to be as effective as Prototypical Networks for example \u201d We agree , and will amend the text . Their interest may lie more in their technical novelty . > Suggestions on multinomial term and sentence grammar These do improve the readability of the text and will be corrected ."}], "0": {"review_id": "HyxnZh0ct7-0", "review_text": "This paper proposes a new meta-learning method based on closed-form solutions for task specific classifiers such as ridge regression and logistic regression (iterative). The idea of the paper is quite interesting, comparing to the existing metric learning based methods and optimization based methods. I have two concerns on this paper. First, the motivation and the rationale of the proposed approach is not clear. In particular, why one can simply treat \\hat{Y} as a scaled and shifted version of X\u2019W? Second, the empirical performance of the proposed approach is not very promising and it does not outperform the comparison methods, e.g., SNAIL. It is not clear what is the advantage. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments and questions . > \u201c Why one can simply treat \\hat { Y } as a scaled and shifted version of X \u2019 W ? \u201d In the case of logistic regression , the scaling and shifting is not needed , and we have \\hat { Y } =X \u2019 W . This is because logistic regression is a classification algorithm , and directly outputs class scores . These scores are fed to the ( cross-entropy ) loss L. However , ridge regression is a regression algorithm , and its regression targets are one-hot encoded labels , which is only an approximation of the discrete problem ( classification ) . This means that an extra calibration step is needed ( eq.6 ) , to allow the network to tune the regressed outputs into classification scores for the cross-entropy loss L. > \u201c The empirical performance of the proposed approach is not very promising and it does not outperform the comparison methods , e.g. , SNAIL \u201d Our method actually outperforms SNAIL on an apples-to-apples comparison , with the same number of layers . We would like to draw the reviewer \u2019 s attention to the last paragraph of the \u201c Multi-class classification \u201d subsection ( page 8 ) . The result mentioned by the reviewer uses a ResNet , while we use a 4-layer CNN to remain comparable to prior work . SNAIL with a 4-layer CNN ( [ 11 ] Appendix B ) performs much worse than our method ( 7.4 % to 10.0 % accuracy improvement ) . Even disregarding the great difference in architecture capacity , our proposal 's performance coincides with SNAIL on miniImageNet 5way-5shot and it is comparable on 3 out of 4 Omniglot setups . We would have liked to establish a comparison also on CIFAR , but unfortunately the official code for SNAIL hasn \u2019 t been released . Borrowing the words of AnonReviewer2 : \u201c Notably , the ridge regression variant can reach results competitive with SNAIL that uses significantly more weights and is shown to suffer when its capacity is reduced. \u201d We hope that this addresses the two concerns raised by the reviewer . We will be happy to answer any other question about the paper ."}, "1": {"review_id": "HyxnZh0ct7-1", "review_text": "Summary: The paper proposes an algorithm for meta-learning which amounts to fixing the features (ie all hidden layers of a deep NN), and treating each task as having its own final layer which could be a ridge regression or a logistic regression. The paper also proposes to separate the data for each task into a training set used to optimize the last, task specific layer, and a validation set used to optimize all previous layers and hyper parameters. Novelty: This reviewer is unsure what the paper claims as a novel contribution. In particular training multi-task neural nets with shared feature representation and task specific final layer is probably 20-30 years old by now and entirely common. It is also common freeze the feature representation learned from the first set of tasks, and to simply use it for new tasks by modifying the last (few) layer(s) which would according to this paper qualify as meta-learning since the new task can be learned with very few new examples. ", "rating": "2: Strong rejection", "reply_text": "We thank the reviewer for the comment . However , we believe that the low score originates from a misunderstanding of our proposal . Below , we try to bring some clarity by disambiguating between what the reviewer refers to and our method . If our interpretation of what the reviewer refers to as \u201c entirely common \u201d is incorrect , it would be great to be provided with at least one reference , so that we can continue the conversation on the same ground . > \u201c novel contribution ? \u201d , \u201c training multi-task neural nets with shared feature representation and task specific final layer is probably 20-30 years old by now and entirely common. \u201d \u201c It is also common freeze the feature representation learned from the first set of tasks , and to simply use it for new tasks by modifying the last layer \u201d We understand that the reviewer is hinting at the common multi-task scenario with a shared network and task-specific layers ( e.g.Caruana 1993 ) . He/she also refers to basic transfer learning approaches in which a CNN is first pre-trained on one dataset/task and then adapted to a different dataset/task by simply adapting the final layer ( s ) ( e.g.Yosinski et al. \u201c How transferable are features in deep neural Networks ? \u201d - NIPS 2014 ; Chu et al. \u201c Best Practices for Fine-tuning Visual Classifiers to New Domains \u201d - ECCVw 2016 ) . If so , then this is significantly different to our work , and in general to all of the previous literature on meta-learning applied to few-shot classification ( e.g.Finn et al.2017 , Ravi & Larochelle 2017 , Vinyals et al.2016 , etc ) . Notably , these methods and ours take into account adaptation * already during the training process * , which requires back-propagating errors through the very fine-tuning process . Within this setup , our main contribution is to propose an adaptation procedure based on closed-form regressors , which have the important characteristic of allowing different models for different episodes while still being fast because of 1 ) their convergence in one ( R2-D2 ) or few ( LR-D2 ) steps , 2 ) the use of the Woodbury identity , which is particularly convenient in the few-shot data regime , and 3 ) back-propagation through the closed-form regressor can be made efficient . To better illustrate our point , we conducted a baseline experiment . First , we pre-trained the same 4-layers CNN architecture , but for a standard classification problem , using the same training samples as our method . We simply added a final fully-connected layer ( with 64 outputs , like the number of classes in the training splits ) and used the cross-entropy loss . Then , we used the convolutional part of this trained network as a feature extractor and fed its activation to our ridge-regression layer to produce a per-episode set of weights . On miniImagenet , the drop in performance w.r.t.our proposed R2-D2 is very significant : 13.8 % and 11.6 % accuracy for the 1 and 5 shot problems respectively . Results are consistent on CIFAR , though less drastic : 11.5 % and 5.9 % . This confirms that simply using a \u201c shared feature representation and task specific final layer \u201d as commented by the reviewer is not what we are doing and it is not a good strategy to obtain results competitive with the state-of-the-art in few-shot classification . Instead , it is necessary to enforce the generality of the underlying features during training explicitly , which we do by back-propagating through the fine-tuning procedure ( the closed-form regressors ) . We would like to conclude remarking that , probably , the source of confusion arises from the overlap that exists in general between the few-shot learning and the transfer/multi-task learning sub-communities . We realize that the two have developed fairly separately while trying to solve very related problems , and unfortunately the similarities/differences are not acknowledged enough in few-shot classification papers , including our own . We intend to alleviate this problem in our related work section , and invite the reviewer to suggest more relevant works from this area ."}, "2": {"review_id": "HyxnZh0ct7-2", "review_text": "This paper proposes a meta-learning approach for the problem of few-shot classification. Their method, based on parametrizing the learner for each task by a closed-form solver, strikes an interesting compromise between not performing any adaptation for each new task (as is the case in pure metric learning methods) and performing an expensive iterative procedure, such as MAML or Meta-Learner LSTM where there is no guarantee that after taking the few steps prescribed by the respective algorithms the learner has converged. For this reason, I find that leveraging existing solvers that admit closed-form solutions is an attractive and natural choice. Specifically, they propose ridge regression as their closed-form solver (R2-D2 variant). This is easily incorporated into the meta-learning loop with any hyperparameters of this solver being meta-learned, along with the embedding weights as is usually done. The use of the Woodbury equation allows to rewrite the closed form solution in a way that scales with the number of examples instead of the dimensionality of the features; therefore taking advantage of the fact that we are operating in a few-shot setting. While regression may seem to be a strange choice for eventually solving a classification task, it is used as far as I understand due to the availability of this widely-known closed-form solution. They treat the one-hot encoded labels of the support set as the regression targets, and additionally calibrate the output of the network (via a transformation by a scale and bias) in order to make it appropriate for classification. Based on the loss of ridge regression on the support set of a task, a parameter matrix is learned for that task that maps from the embedding dimensionality to the number of classes. This matrix can then be used directly to multiply the embedded (via the fixed for the purposes of the episode embedding function) query points, and for each query point, the entry with the maximum value in the corresponding row of the resulting matrix will constitute the predicted class label. They also experimented with a logistic regression variant (LR-D2) that does not admit a closed-form solution but can be solved efficiently via Newton\u2019s Method under the form of Iteratively Reweighted Least Squares. When using this variant they restrict to tackling the case of binary-classification. A question that comes to mind about the LR-D2 variant: while I understand that a single logistic regression classifier is only capable of binary classification, there seems to be a straightforward extension to the case of multiple classes, where one classifier per class is learned, leading to a total of N one-vs-all classifiers (where N is the way of the episode). I\u2019m curious how this would compare in terms of performance against the ridge regression variant which is naturally multi-class. This would allow to directly apply this variant in the common setting and would enable for example still oversampling classes at meta-training time as is done usually. I would also be curious to see an ablation where for the LR-D2 variant SGD was used as the optimizer instead of Newton\u2019s method. That variant may require more steps (similar to MAML), but I\u2019m curious in practice how this performs. A few other minor comments: - In the related work section, the authors write: \u201cOn the other side of the spectrum, methods that optimize standard iterative learning algorithms, [...] are accurate but slow.\u201d Note however that neither MAML nor MetaLearner LSTM have been showed to be as effective as Prototypical Networks for example. So I wouldn\u2019t really present this as a trade-off between accuracy and speed. - I find the term multinomial classification strange. Why not use multi-class classification? - In page 8, there is a sentence that is not entirely grammatically correct: \u2018Interestingly, increasing the capacity of the other method it is not particularly helpful\u2019. Overall, I think this is good work. The idea is natural and attractive. The writing is clear and comprehensive. I enjoyed how the explanation of meta learning and the usual episodic framework was presented. I found the related work section thorough and accurate too. The experiments are thorough as well, with appropriate ablations to account for different numbers of parameters used between different methods being compared. This approach is evidently effective for few-shot learning, as demonstrated on the common two benchmarks as well as on a newly-introduced variant of cifar that is tailored to few-shot classification. Notably, the ridge regression variant can reach results competitive with SNAIL that uses significantly more weights and is shown to suffer when its capacity is reduced. Interestingly, other models such as MAML actually suffer when given additional capacity, potentially due to overfitting. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the insightful comments and analysis . > \u201c One-vs-all classifiers \u201d for LR-D2 This is a great suggestion , and we are not quite sure how we missed it . We will update the results for 5-way classification incorporating this method . > \u201c ablation where for the LR-D2 variant SGD was used ... instead of Newton \u2019 s method \u201d We previously did exactly this experiment , although for the R2-D2 ( ridge regression ) variant . We did not include it due to space constraints . It is equivalent to MAML , which also uses SGD , but adapting only the classification layer for new tasks ( instead of adapting all parameters ) . We tested this variant on miniImageNet with 5 classes , with the lowest-capacity CNN ( which is the most favorable model for MAML/SGD ) . It yields 45.4\u00b11.6 % accuracy for 1-shot classification and 61.7\u00b11.0 % for 5-shot classification . Comparing it to Table 1 , there \u2019 s a drop in performance compared to our closed form solver ( 3.5 % and 4.4 % less accuracy , respectively ) , and also compared to the original MAML ( 3.3 % and 1.4 % respectively ) . Although we expect the conclusions for logistic regression ( LR-D2 ) to be similar , we will extend the experiment to this case and report the results . > \u201c Neither MAML nor MetaLearner LSTM have been showed to be as effective as Prototypical Networks for example \u201d We agree , and will amend the text . Their interest may lie more in their technical novelty . > Suggestions on multinomial term and sentence grammar These do improve the readability of the text and will be corrected ."}}