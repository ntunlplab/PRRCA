{"year": "2019", "forum": "H1gL-2A9Ym", "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "decision": "Accept (Poster)", "meta_review": "There were several ambivalent reviews for this submission and one favorable one. Although this is a difficult case, I am recommending accepting the paper.\n\nThere were two main questions in my mind.\n1. Did the authors justify that the limited neighborhood problem they try to fix with their method is a real problem and that they fixed it? If so, accept.\n\nHere I believe evidence has been presented, but the case remains undecided.\n\n2. If they have not, is the method/experiments sufficiently useful to be interesting anyway?\n\nThis question I would lean towards answering in the affirmative.\n\nI believe the paper as a whole is sufficiently interesting and executed sufficiently well to be accepted, although I was not convinced of the first point (1) above. One review voting to reject did not find the conceptual contribution very valuable but still thought the paper was not severely flawed. I am partly down-weighting the conceptual criticism they made. I am more concerned with experimental issues. However, I did not see sufficiently severe issues raised by the reviewers to justify rejection.\n\nUltimately, I could go either way on this case, but I think some members of the community will benefit from reading this work enough that it should be accepted.", "reviews": [{"review_id": "H1gL-2A9Ym-0", "review_text": "The thurst behind this paper is that graph convolutional networks (GCNs) are constrained by construction to focus on small neighborhoods around any given node. Large neighborhoods introduce in principle a large number of parameters (while as the authors point out, weight sharing is an option to avoid this issue), plus even worse oversmoothing may occur. Specifically, Xu et al. (2018) showed that for a k-layer GCN one can think of the influence score of a node x on node y as the probability that a walker that starts at x, lands on y after k steps of random walk (modulo some details). Therefore, as k increases the random walks reaches its stationary distribution, forgetting any local information that is useful, e.g., for node classification. To avoid this problem, the authors propose the following: use personalized Pagerank instead of the standard Markov chain of Pagerank. In PPR there is a restart probability, which allows their algorithm to avoid \u201cforgetting\u201d the local information around a walk, thus allowing for an arbitrary number of steps in their random walk. The authors define two methods PEP, and PEPa based on PPR. The latter method is faster in practice since it approximates the PPR. A key advantage of the proposed method is the separation of the node embedding part from the propagation scheme. In this sense, following the categorization of existing methods into three categories, PEP is a hybrid of message passing algorithms, and random walk based node embeddings. The experimental evaluation tests certain basic properties of the proposed method. One interesting performance feature of PEP and PEPa is that they can perform well using few training examples. This is valuable especially when obtaining labeled examples is expensive. Finally, the authors compare their proposed methods against state-of-the-art GCN-based methods. Some remarks follow. - The idea of using PPR for node embeddings has been suggested in recent prior work \u201cLASAGNE: Locality and structure aware graph node embeddings\u201d By Faerman et al. While according to the authors\u2019 categorization of the existing methods in the intro, LASAGNE falls under the \u201crandom walk\u201d family of methods, the authors should compare against it. - Continuing the previous point, even simpler baselines would be desirable. How inferior is for instance an approach on one-vs-all classification using the approximate personalized Pagerank node embedding and support vector machines? - Also, the authors mention \u201csince our datasets are somewhat similar\u2026\u201d. Please clarify with respect to which aspects? Also, please use datasets that are different. For instance, see the LASAGNE paper for more datasets that have different number of classes. - In the experiments the authors use two layers for fair comparison. Given that one of the advantages of the proposed method is the ability to have more layers without suffering from the GCN shortcomings with large neighborhood exploration, it would be interesting to see an experiment where the number of layers is a variable. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and feedback ! We want to clarify that the principle and task performed by LASAGNE is fundamentally different to ours . The LASAGNE method learns individual node embeddings in an unsupervised setting . Our goal is not to learn individual node embeddings but to learn a transformation from attributes to class labels in the semi-supervised setting , as graph convolutional network ( GCN ) -like models do . Moreover , LASAGNE only considers structural information . Generally , it has been shown that approaches that consider both structure and attributes outperform methods that only consider the structure ( see e.g.Kipf Welling 2017 ) . Therefore , we only compare with methods that consider both , but we added a reference to LASAGNE in the paper . We feel that this confusion was due to a bad framing of our model . To make things clearer we have decided to rename the model and replace the term \u201c embedding \u201d with \u201c prediction \u201d in the revised version ( see also our general comment ) . We can not run the proposed baseline , since as we clarified above we do not learn any personalized pagerank embeddings to begin with . However , we do already include a comparatively simple baseline which is the bootstrapped Laplacian feature propagation . This method propagates features in a similar way as we do and then uses a one-vs-all classifier . We significantly outperform this baseline . In the revised version of the paper we clarified that the datasets are similar in that they contain bag-of-words features and use scientific networks . However , these graphs have very different numbers of nodes , edges , features , and classes , and different topology , as shown in Table 1 . The datasets you suggested from the LASAGNE paper are not suitable for the kind of semi-supervised classification we consider since they do not contain node attributes . Thank you for suggesting the interesting experiment of varying neural network depth ! The investigated datasets do not benefit from deeper networks . You can find the results in Figure 11 of the updated version of the paper ."}, {"review_id": "H1gL-2A9Ym-1", "review_text": "This paper proposed a variant of graph neural network, which added additional pagerank-like propagations (with constant aggregation weights), in additional to the normal message-passing like propagation layers. Experiments on some benchmark transductive node classification tasks show some empirical gains. Using more propagations with constant aggregation weights is an interesting idea to help propagate the information in a graph. However, this idea is not completely new. In the very first graph neural network [1], the propagation is done until convergence. If the operator in each layer is a contraction map, then according to the Banach Fixed Point theorem [2], a unique solution can be guaranteed. The constant operator used in this paper is thus a special case of this contraction map. Also, the closed form solution in (3) is not practical. It may not be suitable for large graphs (e.g., graphs with >10k nodes). And that\u2019s why this approach is not suitable for Pubmed and Microsoft dataset. The PEP_A is more practical. However, in this case I\u2019m curious how it would compare with a GNN having same number of layers, but with proper gating/skip connections like ResNet. The experiments show some marginal gains on the small graphs. However, I think it would be important to test on large graphs. Since small graphs typically have small diameter, thus several GNN layers would already cover the entire graph, and the additional propagation done by pagerank here might not be super helpful. Finally, I think the author should properly cite another relevant paper [3], which uses fixed point iteration to help propagate the local information. [1] Scarselli et.al, \u201cThe Graph Neural Network Model\u201d, IEEE Transactions on Neural Networks, 2009 [2] Mohamed A. Khamsi, An Introduction to Metric Spaces and Fixed Point Theory [3] Dai et.al, Learning Steady-States of Iterative Algorithms over Graphs, ICML 2018", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and feedback ! The connection to the GNN-framework is certainly interesting and we \u2019 ve added it in the revised version of the paper ( in Section 3 , after introducing APPNP ) . However , our main contribution is not the usage of fixed-point iterations for node classification , which has already been used e.g.in label propagation and belief propagation algorithms . Our contribution is the improvement of GCN-like models by solving the limited range problem through the development and thorough evaluation of an end-to-end trained model utilizing one specific fixed-point iteration . As you correctly noticed , the exact model is not applicable to larger data -- this is exactly the reason why we have developed its approximation . The discussion can be found under `` efficiency analysis '' in Section 3 . We have edited the experimental section to make this more clear . Furthermore , we would like to highlight that we have already performed an analysis on large graphs . As shown in Table 1 , our experimental evaluation includes two graphs with 20k nodes , which follows the suggestion you gave ( > 10k nodes ) . Please note that we have already compared our model to jumping knowledge networks ( JK ) , which is similar to the GNN that uses proper gating/skip connections you suggested . As we show in the experimental section , we significantly outperform this model . You state that we show `` some marginal gains '' . However , we show that our results are significant . Previous methods have reported \u201c large \u201d gains that actually were not statistically significant and vanish when thoroughly evaluated , as we show in the paper . We paid a lot of attention to performing a fair comparison and a rigorous statistical analysis of our results , which shows that we significantly outperform previous models . The different evaluation may make the improvements seem smaller . But in fact they are larger than those reported in previous , less careful evaluations . We have edited the section to further clarify this . Furthermore , we \u2019 ve included a reference to the work by Dai et al ."}, {"review_id": "H1gL-2A9Ym-2", "review_text": "This paper proposes a GCN variant that addresses a limitation of the original model, where embedding is propagated in only a few hops. The architectural difference may be explained in the following: GCN interleaves the individual node feature transformation and the single-hop propagation, whereas the proposed architecture first transforms the node features, followed by a propagation with an (in)finite number of hops. The propagation in the proposed method follows personalized PageRank, where in addition to following direct links, there is a nonzero probably jumping to a target node. I find the idea interesting. The experiments are comprehensive, covering important points including data split, training set size, number of hops, teleport probability, and ablation study. Two interesting take-home messages are that (1) GCN-like propagation without teleportation leads to degrading performance as the number of hops increases, whereas propagation with teleportation leads to converging performance; and (2) the best-performing teleport probability generally falls within a narrow range. Question: The current propagation approach uses the normalized adjacency matrix proposed by GCN, which is, strictly speaking, not the transition matrix used by PageRank. What prevents from using the transition matrix? Note that this matrix naturally handles directed graphs. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and feedback ! You are right , nothing prevents the model from using the standard transition matrix . During model development , however , we have found that the added self-loops of the GCN-matrix are beneficial to performance . The symmetrical normalization actually does n't make any difference in the limit k- > infinity . However , we found this style of normalization to be beneficial for the finite-step approximation ."}], "0": {"review_id": "H1gL-2A9Ym-0", "review_text": "The thurst behind this paper is that graph convolutional networks (GCNs) are constrained by construction to focus on small neighborhoods around any given node. Large neighborhoods introduce in principle a large number of parameters (while as the authors point out, weight sharing is an option to avoid this issue), plus even worse oversmoothing may occur. Specifically, Xu et al. (2018) showed that for a k-layer GCN one can think of the influence score of a node x on node y as the probability that a walker that starts at x, lands on y after k steps of random walk (modulo some details). Therefore, as k increases the random walks reaches its stationary distribution, forgetting any local information that is useful, e.g., for node classification. To avoid this problem, the authors propose the following: use personalized Pagerank instead of the standard Markov chain of Pagerank. In PPR there is a restart probability, which allows their algorithm to avoid \u201cforgetting\u201d the local information around a walk, thus allowing for an arbitrary number of steps in their random walk. The authors define two methods PEP, and PEPa based on PPR. The latter method is faster in practice since it approximates the PPR. A key advantage of the proposed method is the separation of the node embedding part from the propagation scheme. In this sense, following the categorization of existing methods into three categories, PEP is a hybrid of message passing algorithms, and random walk based node embeddings. The experimental evaluation tests certain basic properties of the proposed method. One interesting performance feature of PEP and PEPa is that they can perform well using few training examples. This is valuable especially when obtaining labeled examples is expensive. Finally, the authors compare their proposed methods against state-of-the-art GCN-based methods. Some remarks follow. - The idea of using PPR for node embeddings has been suggested in recent prior work \u201cLASAGNE: Locality and structure aware graph node embeddings\u201d By Faerman et al. While according to the authors\u2019 categorization of the existing methods in the intro, LASAGNE falls under the \u201crandom walk\u201d family of methods, the authors should compare against it. - Continuing the previous point, even simpler baselines would be desirable. How inferior is for instance an approach on one-vs-all classification using the approximate personalized Pagerank node embedding and support vector machines? - Also, the authors mention \u201csince our datasets are somewhat similar\u2026\u201d. Please clarify with respect to which aspects? Also, please use datasets that are different. For instance, see the LASAGNE paper for more datasets that have different number of classes. - In the experiments the authors use two layers for fair comparison. Given that one of the advantages of the proposed method is the ability to have more layers without suffering from the GCN shortcomings with large neighborhood exploration, it would be interesting to see an experiment where the number of layers is a variable. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and feedback ! We want to clarify that the principle and task performed by LASAGNE is fundamentally different to ours . The LASAGNE method learns individual node embeddings in an unsupervised setting . Our goal is not to learn individual node embeddings but to learn a transformation from attributes to class labels in the semi-supervised setting , as graph convolutional network ( GCN ) -like models do . Moreover , LASAGNE only considers structural information . Generally , it has been shown that approaches that consider both structure and attributes outperform methods that only consider the structure ( see e.g.Kipf Welling 2017 ) . Therefore , we only compare with methods that consider both , but we added a reference to LASAGNE in the paper . We feel that this confusion was due to a bad framing of our model . To make things clearer we have decided to rename the model and replace the term \u201c embedding \u201d with \u201c prediction \u201d in the revised version ( see also our general comment ) . We can not run the proposed baseline , since as we clarified above we do not learn any personalized pagerank embeddings to begin with . However , we do already include a comparatively simple baseline which is the bootstrapped Laplacian feature propagation . This method propagates features in a similar way as we do and then uses a one-vs-all classifier . We significantly outperform this baseline . In the revised version of the paper we clarified that the datasets are similar in that they contain bag-of-words features and use scientific networks . However , these graphs have very different numbers of nodes , edges , features , and classes , and different topology , as shown in Table 1 . The datasets you suggested from the LASAGNE paper are not suitable for the kind of semi-supervised classification we consider since they do not contain node attributes . Thank you for suggesting the interesting experiment of varying neural network depth ! The investigated datasets do not benefit from deeper networks . You can find the results in Figure 11 of the updated version of the paper ."}, "1": {"review_id": "H1gL-2A9Ym-1", "review_text": "This paper proposed a variant of graph neural network, which added additional pagerank-like propagations (with constant aggregation weights), in additional to the normal message-passing like propagation layers. Experiments on some benchmark transductive node classification tasks show some empirical gains. Using more propagations with constant aggregation weights is an interesting idea to help propagate the information in a graph. However, this idea is not completely new. In the very first graph neural network [1], the propagation is done until convergence. If the operator in each layer is a contraction map, then according to the Banach Fixed Point theorem [2], a unique solution can be guaranteed. The constant operator used in this paper is thus a special case of this contraction map. Also, the closed form solution in (3) is not practical. It may not be suitable for large graphs (e.g., graphs with >10k nodes). And that\u2019s why this approach is not suitable for Pubmed and Microsoft dataset. The PEP_A is more practical. However, in this case I\u2019m curious how it would compare with a GNN having same number of layers, but with proper gating/skip connections like ResNet. The experiments show some marginal gains on the small graphs. However, I think it would be important to test on large graphs. Since small graphs typically have small diameter, thus several GNN layers would already cover the entire graph, and the additional propagation done by pagerank here might not be super helpful. Finally, I think the author should properly cite another relevant paper [3], which uses fixed point iteration to help propagate the local information. [1] Scarselli et.al, \u201cThe Graph Neural Network Model\u201d, IEEE Transactions on Neural Networks, 2009 [2] Mohamed A. Khamsi, An Introduction to Metric Spaces and Fixed Point Theory [3] Dai et.al, Learning Steady-States of Iterative Algorithms over Graphs, ICML 2018", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and feedback ! The connection to the GNN-framework is certainly interesting and we \u2019 ve added it in the revised version of the paper ( in Section 3 , after introducing APPNP ) . However , our main contribution is not the usage of fixed-point iterations for node classification , which has already been used e.g.in label propagation and belief propagation algorithms . Our contribution is the improvement of GCN-like models by solving the limited range problem through the development and thorough evaluation of an end-to-end trained model utilizing one specific fixed-point iteration . As you correctly noticed , the exact model is not applicable to larger data -- this is exactly the reason why we have developed its approximation . The discussion can be found under `` efficiency analysis '' in Section 3 . We have edited the experimental section to make this more clear . Furthermore , we would like to highlight that we have already performed an analysis on large graphs . As shown in Table 1 , our experimental evaluation includes two graphs with 20k nodes , which follows the suggestion you gave ( > 10k nodes ) . Please note that we have already compared our model to jumping knowledge networks ( JK ) , which is similar to the GNN that uses proper gating/skip connections you suggested . As we show in the experimental section , we significantly outperform this model . You state that we show `` some marginal gains '' . However , we show that our results are significant . Previous methods have reported \u201c large \u201d gains that actually were not statistically significant and vanish when thoroughly evaluated , as we show in the paper . We paid a lot of attention to performing a fair comparison and a rigorous statistical analysis of our results , which shows that we significantly outperform previous models . The different evaluation may make the improvements seem smaller . But in fact they are larger than those reported in previous , less careful evaluations . We have edited the section to further clarify this . Furthermore , we \u2019 ve included a reference to the work by Dai et al ."}, "2": {"review_id": "H1gL-2A9Ym-2", "review_text": "This paper proposes a GCN variant that addresses a limitation of the original model, where embedding is propagated in only a few hops. The architectural difference may be explained in the following: GCN interleaves the individual node feature transformation and the single-hop propagation, whereas the proposed architecture first transforms the node features, followed by a propagation with an (in)finite number of hops. The propagation in the proposed method follows personalized PageRank, where in addition to following direct links, there is a nonzero probably jumping to a target node. I find the idea interesting. The experiments are comprehensive, covering important points including data split, training set size, number of hops, teleport probability, and ablation study. Two interesting take-home messages are that (1) GCN-like propagation without teleportation leads to degrading performance as the number of hops increases, whereas propagation with teleportation leads to converging performance; and (2) the best-performing teleport probability generally falls within a narrow range. Question: The current propagation approach uses the normalized adjacency matrix proposed by GCN, which is, strictly speaking, not the transition matrix used by PageRank. What prevents from using the transition matrix? Note that this matrix naturally handles directed graphs. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and feedback ! You are right , nothing prevents the model from using the standard transition matrix . During model development , however , we have found that the added self-loops of the GCN-matrix are beneficial to performance . The symmetrical normalization actually does n't make any difference in the limit k- > infinity . However , we found this style of normalization to be beneficial for the finite-step approximation ."}}