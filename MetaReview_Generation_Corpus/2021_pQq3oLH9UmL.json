{"year": "2021", "forum": "pQq3oLH9UmL", "title": "Achieving Explainability in a Visual Hard Attention Model through Content Prediction", "decision": "Reject", "meta_review": "This paper proposes a new hard attention model for the image classification as a way to achieve explainability. Two of the reviewers do not find the output of the system interpretable, which is a fatal weakness for a paper on XAI.\nR1: The visualization in Fig.5 shows only that the region selected in each timestep indeed has the maximum EIG. But how to interpret the explainability from the glimpse sequence is still confusing. I can hardly perceive the sequence using my knowledge.\nR2: However the output of the system is not so appealing either in performance or explainability.\nR3: Post-discussion note: For me, it's a bit hard to say the proposed methodology is novel. Authors needs to explain why the proposed model is different from pre-existing methodologies regarding attention mechanism.  \nR4: Due to the above, the recommendation is Reject - but the authors are strongly encouraged to do experiments on more challenging data and compare to a newer baseline.\n", "reviews": [{"review_id": "pQq3oLH9UmL-0", "review_text": "This paper proposes an alternative way to conduct hard attention . Specifically , it estimates the expected information gain ( EIG ) of attending various regions and at each timestep chooses the region with the maximum EIG . The proposed method is tested on image classification . Issues : 1. what do you mean `` achieving explainability '' in the title ? I do n't quite get it . The visualization in Fig.5 shows only that the region selected in each timestep indeed has the maximum EIG . But how to interpret the explainability from the glimpse sequence is still confusing . I can hardly perceive the sequence using my knowledge . 2. why the proposed method needs three building blocks is not well explained , especially the partial VAE . 3. what is the data used to train the partial VAE ? 4. no ablation study is conducted on these building blocks , such as the normalizing flow module in the partial VAE . 5.I think image classification is not the best task to demonstrate the effectiveness of hard attention method . As shown in the experiments , simply using a standard CNN that takes the whole image as the input obtains the best result .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for devoting their attention to our paper and providing us thorough and thoughtful reviews . We address the reviewer \u2019 s queries below . 1 . * * Explainability : * * The hard attention models make two predictions , the class-label , and the next glimpse location . While the first prediction is explainable by design , the second prediction is not . In this paper , we propose a model that explains the second prediction , i.e. , attention policies . Value at location $ l $ in the EIG map suggests the amount of information gained in the class probabilities if the model attends to $ l $ . We can explain that our model seeks a glimpse that seems to provide maximum information about the class-label . 2 . * * Building blocks and Partial VAE : * * We use a recurrent feature aggregator to extract fixed-length features from a variable number of glimpses , a classifier to predict the class label , and a partial VAE to predict the content yet unobserved . Our model does not have access to the content of the entire image due to the partial observability . The model uses the predicted content to compute the EIG in the absence of actual content . 3 . * * Training data for Partial VAE : * * We train our entire model in an end-to-end fashion on four image classification datasets : MNIST , SVHN , CIFAR-10 , and CINIC-10 . Specifically , we use the input image $ x $ to evaluate the log-likelihood ( eq 3 ) in the ELBO of the partial VAE . 4 . * * Ablation study : * * We include an ablation study on the normalizing flows in section 5.5 in the revised manuscript . 5 . * * Classification task for hard attention models : * * Researchers frequently use classification task to evaluate the visual hard attention models . Many seminal papers in this area demonstrate the effectiveness of the attention models on image classification ( Zheng et al . ( 2015 ) , Larochelle & Hinton ( 2010 ) , Mnih et al . ( 2014 ) , Ba et al . ( 2015 ) , Jaderberg et al. ( 2015 ) ) . * * Performance of a CNN : * * We expect any efficient model , including CNN , that observes the entire image to attain the best accuracy . We do not expect hard attention models that observe the image partially or at low resolution to match the former 's accuracy . The former provides a better perceptive about the comparison of the latter . We thank the reviewer again and request them to inform us if further clarification is required . References : * Zheng , Yin , et al . `` A neural autoregressive approach to attention-based recognition . '' International Journal of Computer Vision 113.1 ( 2015 ) : 67-79 . * Larochelle , Hugo , and Geoffrey E. Hinton . `` Learning to combine foveal glimpses with a third-order Boltzmann machine . '' Advances in neural information processing systems . 2010 . * Mnih , Volodymyr , Nicolas Heess , and Alex Graves . `` Recurrent models of visual attention . '' Advances in neural information processing systems . 2014 . * Ba , Jimmy , et al . `` Learning wake-sleep recurrent attention models . '' Advances in Neural Information Processing Systems . 2015 . * Jaderberg , Max , Karen Simonyan , and Andrew Zisserman . `` Spatial transformer networks . '' Advances in neural information processing systems . 2015 ."}, {"review_id": "pQq3oLH9UmL-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper proposed a new hard attention model for the image classification . They designed hard attention mechanism as a bayesian optimal experimental setting . Compare to other hard attention model , the policies of proposed hard attention can be explainable and differentiable , which is non-parametric . They evaluated their model to four different image classification dataset and their model outperformed than other baseline models . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Even though this is an interesting setting and the technical solutions presented in the paper look reasonable , the idea seems to be pretty incremental as it stacks multiple existing techniques without many innovations . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : The proposed hard attention model finds an optimal location using partial variational auto-encoder . Their attention policy is non-parametric and explainable . They validated their model on four different datasets with qualitatively analyzed results . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . There \u2019 s needs for more throughly designed experimental settings with more datasets . 2.Authors need to perform ablation studies with other form of attention mechanisms . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : For me , it 's a bit hard to say the proposed methodology is novel . Authors needs to explain why the proposed model is different from pre-existing methodologies regarding attention mechanism . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for spending time and providing honest reviews about our paper . And we are glad that the reviewer found the problem setting interesting . We address concerns about the novelty of our work below . 1 . * * Datasets : * * We evaluate our model on four datasets . The MNIST and SVHN datasets include grayscale and color images of 0-9 digits . The CIFAR-10 and CINIC-10 include color images of natural objects . Furthermore , CINIC-10 includes images from the ImageNet dataset . 2 . * * Ablation Study : * * We have added an ablation study on normalizing flows in Section 5.5 in the revised manuscript . All other modules in our model are essential and implemented with an indispensable number of layers , as mentioned in appendix A . 3 . * * Contributions : * * Our contributions , especially explainable attention policies for partially observable scenes , set our work apart from the pre-existing methodologies . We note that most existing models observe an entire image and use global information to predict unexplainable attention policies . In contrast , we focus on partially observable scenes where explainable attention policies are critical . We have added a paragraph at the end of the related works section , which highlights these differences . 4 . * * Novelty : * * We agree with the reviewer that the proposed model builds on the existing techniques . However , these techniques are less explored in the context of hard attention . We introduce these techniques to the visual attention community in a principled manner . We emphasize that our paper aims at innovating a unified approach towards hard attention where the attention policies are explainable . In this context , the presented work is unique and novel . Again , we thank the reviewer and request them to inform us if there are any further questions or suggestions ."}, {"review_id": "pQq3oLH9UmL-2", "review_text": "* * Summary : * * This paper follows a less explored strategy for achieving explainability via hard attention . They proposed a recurrent architecture which sequentially observe regions ( glimpse ) from an image . To decide where to look next , the model maintains a hidden state and use it to estimate the full image ( or features of the image ) . This `` content prediction '' module allows the model to look ahead and make a decision based on the expected information gain ( EIG ) over different locations . The objective function ( i.e.partial VAE loss and classification loss ) in this system is differentiable thus the system can be trained with gradient descent . The authors validated the system on several benchmarks and show comparable performance with baselines . * * Reasons for score : * * The system seems extremely complicated to me , as it involves multiple components and each component by themselves is very complex . However the output of the system is not so appealing either in performance or explainability . Probably I missed something but I do n't quite understand the advantages of the proposed system . * * Pros : * * 1 . The core idea for training the attention policy is intuitive , as the information gain is a natural choice for determining the location of next glimpse . It 's also appealing that the predicted content can provide sufficient signals to estimate the information gain . 2.The equation is clear and makes the paper easy to follow . 3.The careful analysis of the experiments is very informative . * * Cons : * * 1 . In general , my biggest concern about this paper is the complexity of the EXPAC system with the moderate performance . It seems that all benchmark datasets used in the paper are not so difficult ( e.g. , 10-way classification with 32x32 images ) and the performance of the proposed system is still far from satisfactory . Therefore , it 's questionable whether the system could be scaled to even more challenging ( but more practical ) datasets like ImageNet . Also I 'm curious about the robustness of training such an intricate system . 2.Regarding explainability , there are quite a few methods ( not limited to hard attention ) that target the same goal , such as Grad-CAM . However this paper seems to only compare with RAM ( and a gist-initialized variation of GRAM ) . Even if the core algorithm might be different , it 's still good to compare with other designs . ( Selvaraju , Ramprasaath R. , et al . `` Grad-cam : Visual explanations from deep networks via gradient-based localization . '' Proceedings of the IEEE international conference on computer vision . 2017 . ) 3.It seems to me that the content prediction module ( $ S $ and $ D $ ) is critical as the predicted $ \\tilde { x } $ is used for `` lookahead '' to estimate the next $ l $ . However the predicted $ \\tilde { x } $ is never shown in the paper . I think it would be more straightforward to better understand the performance of the system by comparing the predicted content with the original . * * Questions during rebuttal period : * * Please address my questions in the cons section .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for assessing our paper and providing invaluable suggestions and comments . We also acknowledge the reviewer for recognizing the spirit of our work i.e.exploring a less explored strategy for explainable hard attention . # # # # # Q1 * * Complexity : * * We note that the complexity of EXPAC is minimal . We use only bare essential modules , namely , a recurrent feature aggregator , a classifier , and a partial VAE . As mentioned in a newly added Appendix A , we implement EXPAC with a small number of layers . * * Robustness : * * As mentioned in Appendix B ( previously Appendix A ) , we optimize the model for different datasets with the same hyperparameters , indicating robustness in training our model . * * Performance : * * We implement EXPAC with an indispensable number of layers . Adding more layers may help in achieving higher accuracy . We do not invest in performing extensive architecture search . Nonetheless , we endeavor to perform a careful and rigorous analysis of our model . * * Datasets : * * Classifying a 32x32 image into ten classes is certainly not difficult when an entire image is observed . However , we observe images through a series of small glimpses , which makes a classification task challenging . Furthermore , we evaluate our model on the CINIC-10 dataset , which includes images from the ImageNet dataset . * * Scalability : * * One can implement the classifier and Partial VAE using large-scale models such as ResNet ( He et al . ( 2016 ) ) and NVAE ( Vahdat & Kautz ( 2017 ) ) . Note that a classifier and a generator are frequently used for multiple tasks in practice , making them readily available for a hard attention model . # # # # # Q2 * * Comparison with Grad-CAM : * * A feed-forward classifier observes the complete image and predicts the class-label . On the other hand , a recurrent hard attention model actively senses glimpses and makes two predictions , namely , the class-label and the next glimpse location . A seminal work by Selvaraju et al . ( 2017 ) explains the first type of model . It can not explain the prediction of the next glimpse location based on the partially observed scenes as in hard attention . We focus on the latter problem and develop an attention model with in-build explainability . As our problems are fundamentally different , we can not compare our work with Selvaraju et al . ( 2017 ) . # # # # # Q3 * * Visualization : * * We have added Figure 5 in the revised manuscript , displaying the predicted $ \\tilde { x } $ . We again thank the reviewer for their time and request them to inform us of additional queries . References : * He , Kaiming , et al . `` Deep residual learning for image recognition . '' Proceedings of the IEEE conference on computer vision and pattern recognition . 2016 . * Vahdat , Arash , and Jan Kautz . `` Nvae : A deep hierarchical variational autoencoder . '' Advances in Neural Information Processing Systems 33 ( 2020 ) . * Selvaraju , Ramprasaath R. , et al . `` Grad-cam : Visual explanations from deep networks via gradient-based localization . '' Proceedings of the IEEE international conference on computer vision . 2017 ."}, {"review_id": "pQq3oLH9UmL-3", "review_text": "This paper presents a visual hard-attention image classification model . The difference to standard classification methods such as CNN is that the model provides an explainable inner structure by default , that can be inspected to see what the model focused on . The difference to other state-or-the-art hard-attention models is that this model is differentiable , allowing for more robust and stable optimization . On a positive note , the presented method is sound and mathematically principled , and the description of it is complete and technically correct . The paper is also well written , well organized , and easy to read . The relevant related work is cited . However , the paper suffers from two major flaws . Firstly , the contribution of the proposed method with respect to other recent hard-attention models based on reinforcement learning it is not well motivated - other than that this model is differentiable . The last paragraph in the Related Work provide no statement whatsoever as to what the present method contributes over the latest methods in the literature . Secondly , the baseline hard-attention model in the experiments , ( Mnih et al.2014 ) , is very old and it is not surprising that the proposed method outperforms it . A more interesting baseline would be a later hard-attention model such as ( Elsayed et al.2019 ) .Moreover , the used datasets are all quite simplistic , and it would be more interesting with a more realistic one . Due to the above , the recommendation is Reject - but the authors are strongly encouraged to do experiments on more challenging data and compare to a newer baseline .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We are grateful to the reviewer for considering our work and providing us invaluable feedback . We also thank the reviewer for recognizing and acknowledging the strengths of our paper . We address the reviewer 's concerns below . * * Contribution : * * We delineate our contributions in the last paragraph of the introduction section . Most recent hard attention models based on reinforcement learning observe entire images and use unexplainable attention policies . In contrast , we present * explainable attention policies * for * partially observable * scenes , which is a valuable contribution . As suggested by the reviewer , we have added a paragraph at the end of the related works section , highlighting our contribution to the latest methods . * * Comparison : * * Saccader , a great model presented by Elsayed et al . ( 2019 ) , observes an entire image and gathers global contextual information in the attention network . In contrast , we observe an image only partially through a series of glimpses . As our model does not use global contextual information , we can not make a fair comparison between the two methods . We compare our method with Minh et al . ( 2014 ) as they also work with partially observable scenes . * * Datasets : * * Note that we focus on developing a systematic approach to explainable attention policies under partial observability . The partial observability in our problem setting imposes additional constraints , making classification task on simplistic datasets more challenging . Furthermore , the CIFAR-10 and CINIC-10 are real-world image datasets . The latter also includes images from the ImageNet dataset . We thank the reviewer for their time and request them to inform us of further questions . References : * Elsayed , Gamaleldin , Simon Kornblith , and Quoc V. Le . `` Saccader : improving accuracy of hard attention models for vision . '' Advances in Neural Information Processing Systems . 2019 . * Mnih , Volodymyr , Nicolas Heess , and Alex Graves . `` Recurrent models of visual attention . '' Advances in neural information processing systems . 2014 ."}], "0": {"review_id": "pQq3oLH9UmL-0", "review_text": "This paper proposes an alternative way to conduct hard attention . Specifically , it estimates the expected information gain ( EIG ) of attending various regions and at each timestep chooses the region with the maximum EIG . The proposed method is tested on image classification . Issues : 1. what do you mean `` achieving explainability '' in the title ? I do n't quite get it . The visualization in Fig.5 shows only that the region selected in each timestep indeed has the maximum EIG . But how to interpret the explainability from the glimpse sequence is still confusing . I can hardly perceive the sequence using my knowledge . 2. why the proposed method needs three building blocks is not well explained , especially the partial VAE . 3. what is the data used to train the partial VAE ? 4. no ablation study is conducted on these building blocks , such as the normalizing flow module in the partial VAE . 5.I think image classification is not the best task to demonstrate the effectiveness of hard attention method . As shown in the experiments , simply using a standard CNN that takes the whole image as the input obtains the best result .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for devoting their attention to our paper and providing us thorough and thoughtful reviews . We address the reviewer \u2019 s queries below . 1 . * * Explainability : * * The hard attention models make two predictions , the class-label , and the next glimpse location . While the first prediction is explainable by design , the second prediction is not . In this paper , we propose a model that explains the second prediction , i.e. , attention policies . Value at location $ l $ in the EIG map suggests the amount of information gained in the class probabilities if the model attends to $ l $ . We can explain that our model seeks a glimpse that seems to provide maximum information about the class-label . 2 . * * Building blocks and Partial VAE : * * We use a recurrent feature aggregator to extract fixed-length features from a variable number of glimpses , a classifier to predict the class label , and a partial VAE to predict the content yet unobserved . Our model does not have access to the content of the entire image due to the partial observability . The model uses the predicted content to compute the EIG in the absence of actual content . 3 . * * Training data for Partial VAE : * * We train our entire model in an end-to-end fashion on four image classification datasets : MNIST , SVHN , CIFAR-10 , and CINIC-10 . Specifically , we use the input image $ x $ to evaluate the log-likelihood ( eq 3 ) in the ELBO of the partial VAE . 4 . * * Ablation study : * * We include an ablation study on the normalizing flows in section 5.5 in the revised manuscript . 5 . * * Classification task for hard attention models : * * Researchers frequently use classification task to evaluate the visual hard attention models . Many seminal papers in this area demonstrate the effectiveness of the attention models on image classification ( Zheng et al . ( 2015 ) , Larochelle & Hinton ( 2010 ) , Mnih et al . ( 2014 ) , Ba et al . ( 2015 ) , Jaderberg et al. ( 2015 ) ) . * * Performance of a CNN : * * We expect any efficient model , including CNN , that observes the entire image to attain the best accuracy . We do not expect hard attention models that observe the image partially or at low resolution to match the former 's accuracy . The former provides a better perceptive about the comparison of the latter . We thank the reviewer again and request them to inform us if further clarification is required . References : * Zheng , Yin , et al . `` A neural autoregressive approach to attention-based recognition . '' International Journal of Computer Vision 113.1 ( 2015 ) : 67-79 . * Larochelle , Hugo , and Geoffrey E. Hinton . `` Learning to combine foveal glimpses with a third-order Boltzmann machine . '' Advances in neural information processing systems . 2010 . * Mnih , Volodymyr , Nicolas Heess , and Alex Graves . `` Recurrent models of visual attention . '' Advances in neural information processing systems . 2014 . * Ba , Jimmy , et al . `` Learning wake-sleep recurrent attention models . '' Advances in Neural Information Processing Systems . 2015 . * Jaderberg , Max , Karen Simonyan , and Andrew Zisserman . `` Spatial transformer networks . '' Advances in neural information processing systems . 2015 ."}, "1": {"review_id": "pQq3oLH9UmL-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper proposed a new hard attention model for the image classification . They designed hard attention mechanism as a bayesian optimal experimental setting . Compare to other hard attention model , the policies of proposed hard attention can be explainable and differentiable , which is non-parametric . They evaluated their model to four different image classification dataset and their model outperformed than other baseline models . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Even though this is an interesting setting and the technical solutions presented in the paper look reasonable , the idea seems to be pretty incremental as it stacks multiple existing techniques without many innovations . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : The proposed hard attention model finds an optimal location using partial variational auto-encoder . Their attention policy is non-parametric and explainable . They validated their model on four different datasets with qualitatively analyzed results . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . There \u2019 s needs for more throughly designed experimental settings with more datasets . 2.Authors need to perform ablation studies with other form of attention mechanisms . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : For me , it 's a bit hard to say the proposed methodology is novel . Authors needs to explain why the proposed model is different from pre-existing methodologies regarding attention mechanism . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for spending time and providing honest reviews about our paper . And we are glad that the reviewer found the problem setting interesting . We address concerns about the novelty of our work below . 1 . * * Datasets : * * We evaluate our model on four datasets . The MNIST and SVHN datasets include grayscale and color images of 0-9 digits . The CIFAR-10 and CINIC-10 include color images of natural objects . Furthermore , CINIC-10 includes images from the ImageNet dataset . 2 . * * Ablation Study : * * We have added an ablation study on normalizing flows in Section 5.5 in the revised manuscript . All other modules in our model are essential and implemented with an indispensable number of layers , as mentioned in appendix A . 3 . * * Contributions : * * Our contributions , especially explainable attention policies for partially observable scenes , set our work apart from the pre-existing methodologies . We note that most existing models observe an entire image and use global information to predict unexplainable attention policies . In contrast , we focus on partially observable scenes where explainable attention policies are critical . We have added a paragraph at the end of the related works section , which highlights these differences . 4 . * * Novelty : * * We agree with the reviewer that the proposed model builds on the existing techniques . However , these techniques are less explored in the context of hard attention . We introduce these techniques to the visual attention community in a principled manner . We emphasize that our paper aims at innovating a unified approach towards hard attention where the attention policies are explainable . In this context , the presented work is unique and novel . Again , we thank the reviewer and request them to inform us if there are any further questions or suggestions ."}, "2": {"review_id": "pQq3oLH9UmL-2", "review_text": "* * Summary : * * This paper follows a less explored strategy for achieving explainability via hard attention . They proposed a recurrent architecture which sequentially observe regions ( glimpse ) from an image . To decide where to look next , the model maintains a hidden state and use it to estimate the full image ( or features of the image ) . This `` content prediction '' module allows the model to look ahead and make a decision based on the expected information gain ( EIG ) over different locations . The objective function ( i.e.partial VAE loss and classification loss ) in this system is differentiable thus the system can be trained with gradient descent . The authors validated the system on several benchmarks and show comparable performance with baselines . * * Reasons for score : * * The system seems extremely complicated to me , as it involves multiple components and each component by themselves is very complex . However the output of the system is not so appealing either in performance or explainability . Probably I missed something but I do n't quite understand the advantages of the proposed system . * * Pros : * * 1 . The core idea for training the attention policy is intuitive , as the information gain is a natural choice for determining the location of next glimpse . It 's also appealing that the predicted content can provide sufficient signals to estimate the information gain . 2.The equation is clear and makes the paper easy to follow . 3.The careful analysis of the experiments is very informative . * * Cons : * * 1 . In general , my biggest concern about this paper is the complexity of the EXPAC system with the moderate performance . It seems that all benchmark datasets used in the paper are not so difficult ( e.g. , 10-way classification with 32x32 images ) and the performance of the proposed system is still far from satisfactory . Therefore , it 's questionable whether the system could be scaled to even more challenging ( but more practical ) datasets like ImageNet . Also I 'm curious about the robustness of training such an intricate system . 2.Regarding explainability , there are quite a few methods ( not limited to hard attention ) that target the same goal , such as Grad-CAM . However this paper seems to only compare with RAM ( and a gist-initialized variation of GRAM ) . Even if the core algorithm might be different , it 's still good to compare with other designs . ( Selvaraju , Ramprasaath R. , et al . `` Grad-cam : Visual explanations from deep networks via gradient-based localization . '' Proceedings of the IEEE international conference on computer vision . 2017 . ) 3.It seems to me that the content prediction module ( $ S $ and $ D $ ) is critical as the predicted $ \\tilde { x } $ is used for `` lookahead '' to estimate the next $ l $ . However the predicted $ \\tilde { x } $ is never shown in the paper . I think it would be more straightforward to better understand the performance of the system by comparing the predicted content with the original . * * Questions during rebuttal period : * * Please address my questions in the cons section .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for assessing our paper and providing invaluable suggestions and comments . We also acknowledge the reviewer for recognizing the spirit of our work i.e.exploring a less explored strategy for explainable hard attention . # # # # # Q1 * * Complexity : * * We note that the complexity of EXPAC is minimal . We use only bare essential modules , namely , a recurrent feature aggregator , a classifier , and a partial VAE . As mentioned in a newly added Appendix A , we implement EXPAC with a small number of layers . * * Robustness : * * As mentioned in Appendix B ( previously Appendix A ) , we optimize the model for different datasets with the same hyperparameters , indicating robustness in training our model . * * Performance : * * We implement EXPAC with an indispensable number of layers . Adding more layers may help in achieving higher accuracy . We do not invest in performing extensive architecture search . Nonetheless , we endeavor to perform a careful and rigorous analysis of our model . * * Datasets : * * Classifying a 32x32 image into ten classes is certainly not difficult when an entire image is observed . However , we observe images through a series of small glimpses , which makes a classification task challenging . Furthermore , we evaluate our model on the CINIC-10 dataset , which includes images from the ImageNet dataset . * * Scalability : * * One can implement the classifier and Partial VAE using large-scale models such as ResNet ( He et al . ( 2016 ) ) and NVAE ( Vahdat & Kautz ( 2017 ) ) . Note that a classifier and a generator are frequently used for multiple tasks in practice , making them readily available for a hard attention model . # # # # # Q2 * * Comparison with Grad-CAM : * * A feed-forward classifier observes the complete image and predicts the class-label . On the other hand , a recurrent hard attention model actively senses glimpses and makes two predictions , namely , the class-label and the next glimpse location . A seminal work by Selvaraju et al . ( 2017 ) explains the first type of model . It can not explain the prediction of the next glimpse location based on the partially observed scenes as in hard attention . We focus on the latter problem and develop an attention model with in-build explainability . As our problems are fundamentally different , we can not compare our work with Selvaraju et al . ( 2017 ) . # # # # # Q3 * * Visualization : * * We have added Figure 5 in the revised manuscript , displaying the predicted $ \\tilde { x } $ . We again thank the reviewer for their time and request them to inform us of additional queries . References : * He , Kaiming , et al . `` Deep residual learning for image recognition . '' Proceedings of the IEEE conference on computer vision and pattern recognition . 2016 . * Vahdat , Arash , and Jan Kautz . `` Nvae : A deep hierarchical variational autoencoder . '' Advances in Neural Information Processing Systems 33 ( 2020 ) . * Selvaraju , Ramprasaath R. , et al . `` Grad-cam : Visual explanations from deep networks via gradient-based localization . '' Proceedings of the IEEE international conference on computer vision . 2017 ."}, "3": {"review_id": "pQq3oLH9UmL-3", "review_text": "This paper presents a visual hard-attention image classification model . The difference to standard classification methods such as CNN is that the model provides an explainable inner structure by default , that can be inspected to see what the model focused on . The difference to other state-or-the-art hard-attention models is that this model is differentiable , allowing for more robust and stable optimization . On a positive note , the presented method is sound and mathematically principled , and the description of it is complete and technically correct . The paper is also well written , well organized , and easy to read . The relevant related work is cited . However , the paper suffers from two major flaws . Firstly , the contribution of the proposed method with respect to other recent hard-attention models based on reinforcement learning it is not well motivated - other than that this model is differentiable . The last paragraph in the Related Work provide no statement whatsoever as to what the present method contributes over the latest methods in the literature . Secondly , the baseline hard-attention model in the experiments , ( Mnih et al.2014 ) , is very old and it is not surprising that the proposed method outperforms it . A more interesting baseline would be a later hard-attention model such as ( Elsayed et al.2019 ) .Moreover , the used datasets are all quite simplistic , and it would be more interesting with a more realistic one . Due to the above , the recommendation is Reject - but the authors are strongly encouraged to do experiments on more challenging data and compare to a newer baseline .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We are grateful to the reviewer for considering our work and providing us invaluable feedback . We also thank the reviewer for recognizing and acknowledging the strengths of our paper . We address the reviewer 's concerns below . * * Contribution : * * We delineate our contributions in the last paragraph of the introduction section . Most recent hard attention models based on reinforcement learning observe entire images and use unexplainable attention policies . In contrast , we present * explainable attention policies * for * partially observable * scenes , which is a valuable contribution . As suggested by the reviewer , we have added a paragraph at the end of the related works section , highlighting our contribution to the latest methods . * * Comparison : * * Saccader , a great model presented by Elsayed et al . ( 2019 ) , observes an entire image and gathers global contextual information in the attention network . In contrast , we observe an image only partially through a series of glimpses . As our model does not use global contextual information , we can not make a fair comparison between the two methods . We compare our method with Minh et al . ( 2014 ) as they also work with partially observable scenes . * * Datasets : * * Note that we focus on developing a systematic approach to explainable attention policies under partial observability . The partial observability in our problem setting imposes additional constraints , making classification task on simplistic datasets more challenging . Furthermore , the CIFAR-10 and CINIC-10 are real-world image datasets . The latter also includes images from the ImageNet dataset . We thank the reviewer for their time and request them to inform us of further questions . References : * Elsayed , Gamaleldin , Simon Kornblith , and Quoc V. Le . `` Saccader : improving accuracy of hard attention models for vision . '' Advances in Neural Information Processing Systems . 2019 . * Mnih , Volodymyr , Nicolas Heess , and Alex Graves . `` Recurrent models of visual attention . '' Advances in neural information processing systems . 2014 ."}}