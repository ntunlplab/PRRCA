{"year": "2019", "forum": "Hylyui09tm", "title": "EMI: Exploration with Mutual Information Maximizing State and Action Embeddings", "decision": "Reject", "meta_review": "This paper proposes a method to compute embeddings of states and actions that facilitate computing measures of surprise for intrinsic reward. Though some of the ideas are quite interesting, there are currently issues with the experiments and the motivation.\n\nThe experiments have high variance across the 5 runs, with significant overlap of shaded regions representing just one standard deviation from the mean. It is hard to draw any conclusions about improved performance, and statements like the following are much too strong: \"For vision-based exploration tasks, our results in Figure 5 show that EMI achieves the state of the art performance on Freeway, Frostbite, Venture, and Montezuma\u2019s Revenge in comparison to the baseline exploration methods.\" Further, the proposed approach has three new hyperparameters (lambdas), without much understanding into how to set them or their effect on the results. Specific values are reported for the different game types, without explanation for how or why these values were chosen. \n\nSimilarly strong claims, that are not well substantiated, are given for the proposed approach. This paper seems to suggest that this is a principled approach to using surprise for exploration, contrasted to other ad-hoc approaches (\"Other approaches utilize more ad-hoc measures (Pathak et al., 2017; Tang et al., 2017) that aim to approximate surprise.\"). Yet, the paper does not define surprise (say by citing work by Itti and Baldi on Bayesian surprise), and then proposes what is largely a intuitive approach to providing a good intrinsic reward related to surprise. For example, \"we show that imposing linear topology on the learned embedding representation space (such that the transitions are linear), thereby offloading most of the modeling burden onto the embedding function itself, provides an essential informative measure of surprise when visiting novel states.\" This might be intuitively true, but I do not see a clear demonstration in Section 4.2 actually showing that this restriction provides a measure of surprise. Additionally, some of the choices in Section 4.2 are about estimating \"irreducible error under the linear dynamics model\", but irreducible error is about inherent uncertainty (due to stochasticity and partial observability), not due to the choice of modeling class. In general, many intuitive choices in the algorithm need to be better justified, and some claims disparaging other work for being ad-hoc should be toned down. \n\nOverall, this paper is as yet a bit preliminary, in terms of clarity and experiments. In a further iteration, with some improvements, it could be a useful contribution for exploration in image-based environments. ", "reviews": [{"review_id": "Hylyui09tm-0", "review_text": "The paper proposes an approach for exploration via reward bonuses based on a form of surprise. The surprise factor is based on the next state of a particular transition, and the error in the embedding space to satisfy a linear dynamics formulation. The embedding space of the states and actions are optimized to increase the mutual-information in predicting next state, and current action - encouraging meaningful embeddings with more training, and hence gradual fading away of the extrinsic rewards. The paper is mostly well-written, and the idea is interesting. The experimental results do show that the proposed reward augmentation leads to better performing policies, but the claims in the experimental section need to be less strong (\"outperforms the baseline by a large margin\" - Figure 4 - overlapping error bars; \"state of the art\" - Figure 5 - again, error bars, and no improvement in some domains.) But overall, I think the paper can be accepted as it is an interesting approach. Below are some comments that I hope the authors address in their rebuttal, followed by some possible typos in the current draft. - Theorem 1 content placement: The organization here is rather unclear. Currently, you present Theorem 1, and then talk about using \"JSD instead of MI\". Maybe this is a last minute mistake. In either case, it is strongly suggested that the section be reworked to be clearer. - JSD is upper bounded by ln(2); your bounds (7) and (8) would change consequently too. - Training regime employed: 3 epochs-512 minibatch -> assuming distinct minibatches are sampled, (512x3) samples used Collected (5k*500; sparsehalfcheetah)/(50*500; swimmercatcher)/(100k*4500; atari) Is this the sample usage for training? Axis labels for all plots are missing -- specifically scale of x-axis. Commenting on the sample complexity -- especially as the embedding network seems easy-to-train (or insufficiently trained), would be good; optimizing a lower-bound insufficiently leads one to doubt if the bound is meaningful at all. Is the huge batch of samples mostly used in TRPO/RL part of the infrastructure? - Discussing extrinsic rewards: the pros. vs. cons of the two reward formulations, why both are used etc. would be useful. - Embedding dimension: d=8 in Gravitar and Solaris, but performance is less significant (no significance) in these domains. Is this due to insufficient training? - RL method: Including details about form of TRPO used in appendix would be good (vine/single-path). Further if entropy regularization is used, how does the exploration interplay work. - A.2 is an interesting section. A linear dynamics model being effective in MuJoCo tasks seems plausible. But an Atari example is definitely more interesting. Therefore this section can be clearer - specifically distinction between residual error and sample error. Typos: - Appendix \\lambda parameters unclear - Appendix step-size information contradictory. Post-response comment: while I do think the approach is interesting, the utility of it is mostly demonstrated currently through empirical experiments. These experiments are preliminary, but used to make strong arguments for the effectiveness of the proposed approach. Further, upon highlighting this in my review, the authors disagree and think it\u2019s empirical validity is rather superior. This leaves me concerned, and after thinking about it further, I do not think this is sufficient for acceptance. Therefore, I\u2019m reducing my score to a 5. PS: the characterization of irreducible error as a product of the limitation of a linear model may be inaccurate.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer , We would like to thank you for the time and effort spent providing the feedback . We address the questions below and also updated corresponding response in the pdf submission . 1. \u201c Theorem 1 organization \u201d : We reorganized the structure around Theorem 1 in the paper . 2. \u201c JSD is upper bounded by ln ( 2 ) \u201d : In f-GAN [ 1 ] , they actually derived the lower bound of D_ { JSD } = D_ { KL } ( P||M ) +D_ { KL } ( Q||M ) , instead of D_ { JSD } =1/2 ( D_ { KL } ( P||M ) +D_ { KL } ( Q||M ) ) . Thus our D_ { JSD } formulation coherent with [ 1 ] is upper-bounded by log ( 4 ) . As there was a typo in the upper-bound of D_ { JSD } , we fixed it from log ( 2 ) to log ( 4 ) . 3. \u201c Sample usage for training and sample complexity \u201d : At each iteration in the atari environment , 100k ( state , action , reward ) pairs are sampled as a batch . The number 4500 is the max path length , which means that the length of each episode should not exceed 4500 . ( When the 4500th state is reached , the game is reset even though the agent does not reach the terminal state . ) Thus , if we assume that every episode is reset at 4500th state then we have about 100k / 4500 = 22 episodes in the batch at each iteration . After the batch of 100k samples is collected , we split the batch into minibatches of size 512 and train our network . ( We repeat this minibatch training 3 times because epoch is 3 . ) For the plot axes , the x-axis is iteration and the y-axis is the mean reward . We clarified the axes in the figure captions . For the sample complexity , the huge batch is used to train both TRPO policy network and EMI embedding network at approximately the equal amount . It is not the case that the embedding network underfits . 4. \u201c Pros.vs . cons of the two reward formulations \u201d : We propose two different intrinsic reward functions to show that EMI can be utilized with diverse intrinsic reward functions and is not constrained to the certain intrinsic reward function . As RL agents behave differently with different intrinsic reward functions in various environments , we can use EMI with appropriate intrinsic reward functions depending on environments . ( Example : 1 ) Diversity reward gives good performance in SparseHalfCheetah ( Mujoco ) where the agent should go as far from the origin . 2 ) Prediction error reward works well in Montezuma \u2019 s Revenge ( Atari ) where the agent can receive high prediction error by entering different rooms . ) 5. \u201c Performance is less significant in Gravitar and Solaris with d=8 \u201d : We used higher dimension embedding for Gravitar and Solaris because their environment dynamics are much challenging to predict with d=2 linear embeddings . Games like Freeway and Montezuma \u2019 s Revenge give discrete actions related to 4-way direction moves in 2D space . However , Gravitar and Solaris have different kinds of action dynamics and consist of visually more complex states . These factors make the performance of Gravitar and Solaris to be less significant compared to other games . 6. \u201c RL method : details about TRPO and entropy regularization \u201d : We added the details about TRPO in appendix . ( We use Single path method for TRPO . ) The entropy regularization is a technique that is widely used for policy-based RL algorithms . We think that it would be possible to combine these two as entropy regularization seeks to diversify sampled actions and EMI seeks to diversify sampled states . 7. \u201c Section A.2 can be clearer \u201d : We polished Appendix 2 in terms of the distinction between residual errors and sample errors ( error terms ) . 8. \u201c Claims to be less strong \u201d : As RL problems exhibit high variance on the return especially on environments with sparse rewards , we do not agree that one method is not better than the other just because of the overlapping error bars . Our experiments are all performed in sparse reward setting and the return plots in Figure 4 and a subset of Figure 5 show clear advantage of our method over other exploration baselines . Having said that , we moderated the claim a bit . 9. \u201c Typos : lambda , step size \u201d : Thank you for pointing this out . We modified Equation 9 to make it coherent with lambdas in Appendix . TRPO step size is hyper-parameter related to KL divergence in TRPO algorithm which uses conjugate gradients to update policy network . [ 1 ] Sebastian Nowozin , Botond Cseke , and Ryota Tomioka . f-gan : Training generative neural samplers using variational divergence minimization . In Advances in Neural Information Processing Systems , pp . 271\u2013279 , 2016 ."}, {"review_id": "Hylyui09tm-1", "review_text": "This is a very interesting paper about a novel approach to exploration in agents with state and action representations, making heavy use of recent progress in the use of deep learning for estimating and maximizing mutual information, as well as introducing an approach to model the latent space dynamics with a linear models with sparse errors. A closely related work which is not mentioned is the work of Thomas et al 2017 arXiv:1708.01289 where they also maximize mutual information between distributed representations of actions (policies, actually) and of distributed representations of changes in the state (as the result of applying the policy). The phrase 'functionally similar states' is used several times and would require a bit of explanation. I would also like to see more motivations for the two different reward functions r_e and r_d, and why one should be computed before the update while the other should be computed after. Regarding the experiments, and this is probably the weakest part of this paper, I would have expected to see comparisons against several of the numerous exploration methods which have been proposed in the past and are discussed in the paper (with many negative comments about their weakness, but no empirical support provided). Only one (EX2) was compared. The comparison with TRPO is without exploration (if I understand well, but should be stated clearly). It's also not clear how these results compare to the best reported results on these games (whether or not exploration is used). ", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer , We would like to thank you for the time and effort spent providing the feedback . We address the questions below and also updated corresponding response in the pdf submission . 1. \u201c Mention the work of Thomas et al 2017 \u201d : Thank you for spotting this . We added the work to the related works section . 2. \u201c Motivations for the two different reward functions r_e and r_d , and updating order \u201d : We propose two different intrinsic reward functions to show that EMI can be utilized with diverse intrinsic reward functions and is not constrained to the particular form of intrinsic reward function . As RL agents behave differently with different intrinsic reward functions in various environments , we can use EMI with appropriate intrinsic reward functions depending on environments . ( Example : 1 ) Diversity reward gives good performance in SparseHalfCheetah ( Mujuco ) where the task is to go as far away from the origin . 2 ) Prediction error reward works well in Montezuma \u2019 s Revenge ( Atari ) where the agent can receive high prediction error by entering different rooms . ) 3. \u201c Comparisons against several of the numerous exploration methods \u201d : We had experiments for both EX2 [ 1 ] and ICM [ 2 ] on Atari experiments . But for locomotion tasks , we only compared against EX2 . This was because ICM was mainly designed for discrete actions . To this end , we extended ICM to accommodate continuous actions ( by replacing the cross entropy loss for categorical policy with L2 loss for continuous policy ) ran it on continuous locomotion tasks to as another baseline in the experiments section . Also , we added Table 1 to further compare the final performance of EMI against other exploration methods . [ 1 ] Justin Fu , John Co-Reyes , and Sergey Levine . Ex2 : Exploration with exemplar models for deep reinforcement learning . In Advances in Neural Information Processing Systems , pp . 2577\u20132587 , 2017 . [ 2 ] Deepak Pathak , Pulkit Agrawal , Alexei A Efros , and Trevor Darrell . Curiosity-driven exploration by self-supervised prediction . In International Conference on Machine Learning , volume 2017 , 2017 ."}, {"review_id": "Hylyui09tm-2", "review_text": "This paper introduces actions as a co-predictor of next-states and the predicted (from current and next state) in the context of (model-based) RL. In addition they incorporate the idea of using a JSD-based objective do prediction (as the Deep InfoMax paper), which is novel to RL. The enforce a linear structure between current / next states and actions with an additional sparse nonlinear term computed from both current states and actions. From this, they are able to quantify the amount of novelty in the representation space as a measure of exploration, which can be used as an intrinsic reward. I found the paper to be very well-written and easy to understand. The prediction part is similar to that used in CPC structurally, except they include the action in two different prediction tasks and they have some built-in intrinsic rewards, which is good. I had some issues with the motivations of some of the loss functions. - The JSD-based objective makes sense, but I don't think it's correct to call it an \"approximation\" to the KL (this is only true where the log-ratio of the joint and the product of marginals is small). Rather, it would be better to describe this choice as simply using a different measure between the joint and marginals. - It seems like the best motivation for having linear relations is you can do multiple predictions using the same state / action encodings. - For measuring exploration (11) couldn't one just use the predictor models T? How does the output of T (perhaps correctly normalized with the marginals) correlate with (11)? Other notes: Page 2: Figure 1 is awfully confusing. Could this be clarified a little bit? I\u2019m not sure what the small dots or their colors are supposed to represent. Could diversity also be added by adding a prior to the state representations (as is done in Deep InfoMax)? Why were the vision experiments stopped at 500 x 100k (500 million) frames? I can\u2019t validate the SOTA claims, but it seems like the model is still improving: are there\u2019s further experiments? An ablation study would be nice comparing the different hyper parameters (intrinsic rewards, diversity, etc).", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer , We would like to thank you for the time and effort spent providing the feedback . We address the questions below and also updated corresponding response in the pdf submission . 1. \u201c JSD as an `` approximation '' to the KL \u201d : We agree that it would be better not to state JSD as \u201c approximation \u201d to the KL . We modified our statement to call it \u201c different measure \u201d in section 4.1 . 2. \u201c Using predictor models with output of T \u201d : This is a valid suggestion , as by the mutual information maximizing objective , the network T is encouraged to output smaller values for novel samples . We ran experiments in SparseHalfCheetah environment , and were able to confirm some degree of exploration effect of the intrinsic rewards derived by the outputs of the network T. Although the results are not comparable to our proposed method ( average return of 75 vs. 200 after 1,000 iterations in SparseHalfCheetah ) , it will be an interesting future research direction . 3. \u201c Figure 1 is awfully confusing \u201d : Sorry for the confusion . We updated Figure 1 and hope it clarifies the big picture . 4. \u201c Adding a prior to the state representations \u201d : Regularizing the distribution of state embeddings instead causes the optimization process to be much more unstable . This is because the distribution of states is much more likely to be skewed than the distribution of actions , especially during the initial stage of optimization , so the Gaussian approximation of the distribution of state embeddings in the KL regularization term becomes much less accurate in contrast to the distribution of actions . We added this statement to section 4.2 . 5. \u201c Vision experiments stopped at 50 frames \u201d : We stopped at 50 million frames to perform fair comparisons to other baseline methods . TRPO-based exploration methods such as EX2 [ 1 ] , SimHash [ 2 ] stopped at 50 million frames . 6. \u201d An ablation study would be nice \u201d : We added ablation study in Appendix . In figure 7 , We ablated on each loss terms of Equation 9 to show the impact of each term . In figure 8 , we tested on different intrinsic reward hyper-parameters to show the robustness of our method . [ 1 ] Justin Fu , John Co-Reyes , and Sergey Levine . Ex2 : Exploration with exemplar models for deep reinforcement learning . In Advances in Neural Information Processing Systems , pp . 2577\u20132587 , 2017 . [ 2 ] Haoran Tang , Rein Houthooft , Davis Foote , Adam Stooke , Xi Chen , Yan Duan , John Schulman , Filip DeTurck , and Pieter Abbeel . # exploration : A study of count-based exploration for deep reinforcement learning . In Advances in Neural Information Processing Systems , pp . 2753\u20132762 , 2017 ."}], "0": {"review_id": "Hylyui09tm-0", "review_text": "The paper proposes an approach for exploration via reward bonuses based on a form of surprise. The surprise factor is based on the next state of a particular transition, and the error in the embedding space to satisfy a linear dynamics formulation. The embedding space of the states and actions are optimized to increase the mutual-information in predicting next state, and current action - encouraging meaningful embeddings with more training, and hence gradual fading away of the extrinsic rewards. The paper is mostly well-written, and the idea is interesting. The experimental results do show that the proposed reward augmentation leads to better performing policies, but the claims in the experimental section need to be less strong (\"outperforms the baseline by a large margin\" - Figure 4 - overlapping error bars; \"state of the art\" - Figure 5 - again, error bars, and no improvement in some domains.) But overall, I think the paper can be accepted as it is an interesting approach. Below are some comments that I hope the authors address in their rebuttal, followed by some possible typos in the current draft. - Theorem 1 content placement: The organization here is rather unclear. Currently, you present Theorem 1, and then talk about using \"JSD instead of MI\". Maybe this is a last minute mistake. In either case, it is strongly suggested that the section be reworked to be clearer. - JSD is upper bounded by ln(2); your bounds (7) and (8) would change consequently too. - Training regime employed: 3 epochs-512 minibatch -> assuming distinct minibatches are sampled, (512x3) samples used Collected (5k*500; sparsehalfcheetah)/(50*500; swimmercatcher)/(100k*4500; atari) Is this the sample usage for training? Axis labels for all plots are missing -- specifically scale of x-axis. Commenting on the sample complexity -- especially as the embedding network seems easy-to-train (or insufficiently trained), would be good; optimizing a lower-bound insufficiently leads one to doubt if the bound is meaningful at all. Is the huge batch of samples mostly used in TRPO/RL part of the infrastructure? - Discussing extrinsic rewards: the pros. vs. cons of the two reward formulations, why both are used etc. would be useful. - Embedding dimension: d=8 in Gravitar and Solaris, but performance is less significant (no significance) in these domains. Is this due to insufficient training? - RL method: Including details about form of TRPO used in appendix would be good (vine/single-path). Further if entropy regularization is used, how does the exploration interplay work. - A.2 is an interesting section. A linear dynamics model being effective in MuJoCo tasks seems plausible. But an Atari example is definitely more interesting. Therefore this section can be clearer - specifically distinction between residual error and sample error. Typos: - Appendix \\lambda parameters unclear - Appendix step-size information contradictory. Post-response comment: while I do think the approach is interesting, the utility of it is mostly demonstrated currently through empirical experiments. These experiments are preliminary, but used to make strong arguments for the effectiveness of the proposed approach. Further, upon highlighting this in my review, the authors disagree and think it\u2019s empirical validity is rather superior. This leaves me concerned, and after thinking about it further, I do not think this is sufficient for acceptance. Therefore, I\u2019m reducing my score to a 5. PS: the characterization of irreducible error as a product of the limitation of a linear model may be inaccurate.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer , We would like to thank you for the time and effort spent providing the feedback . We address the questions below and also updated corresponding response in the pdf submission . 1. \u201c Theorem 1 organization \u201d : We reorganized the structure around Theorem 1 in the paper . 2. \u201c JSD is upper bounded by ln ( 2 ) \u201d : In f-GAN [ 1 ] , they actually derived the lower bound of D_ { JSD } = D_ { KL } ( P||M ) +D_ { KL } ( Q||M ) , instead of D_ { JSD } =1/2 ( D_ { KL } ( P||M ) +D_ { KL } ( Q||M ) ) . Thus our D_ { JSD } formulation coherent with [ 1 ] is upper-bounded by log ( 4 ) . As there was a typo in the upper-bound of D_ { JSD } , we fixed it from log ( 2 ) to log ( 4 ) . 3. \u201c Sample usage for training and sample complexity \u201d : At each iteration in the atari environment , 100k ( state , action , reward ) pairs are sampled as a batch . The number 4500 is the max path length , which means that the length of each episode should not exceed 4500 . ( When the 4500th state is reached , the game is reset even though the agent does not reach the terminal state . ) Thus , if we assume that every episode is reset at 4500th state then we have about 100k / 4500 = 22 episodes in the batch at each iteration . After the batch of 100k samples is collected , we split the batch into minibatches of size 512 and train our network . ( We repeat this minibatch training 3 times because epoch is 3 . ) For the plot axes , the x-axis is iteration and the y-axis is the mean reward . We clarified the axes in the figure captions . For the sample complexity , the huge batch is used to train both TRPO policy network and EMI embedding network at approximately the equal amount . It is not the case that the embedding network underfits . 4. \u201c Pros.vs . cons of the two reward formulations \u201d : We propose two different intrinsic reward functions to show that EMI can be utilized with diverse intrinsic reward functions and is not constrained to the certain intrinsic reward function . As RL agents behave differently with different intrinsic reward functions in various environments , we can use EMI with appropriate intrinsic reward functions depending on environments . ( Example : 1 ) Diversity reward gives good performance in SparseHalfCheetah ( Mujoco ) where the agent should go as far from the origin . 2 ) Prediction error reward works well in Montezuma \u2019 s Revenge ( Atari ) where the agent can receive high prediction error by entering different rooms . ) 5. \u201c Performance is less significant in Gravitar and Solaris with d=8 \u201d : We used higher dimension embedding for Gravitar and Solaris because their environment dynamics are much challenging to predict with d=2 linear embeddings . Games like Freeway and Montezuma \u2019 s Revenge give discrete actions related to 4-way direction moves in 2D space . However , Gravitar and Solaris have different kinds of action dynamics and consist of visually more complex states . These factors make the performance of Gravitar and Solaris to be less significant compared to other games . 6. \u201c RL method : details about TRPO and entropy regularization \u201d : We added the details about TRPO in appendix . ( We use Single path method for TRPO . ) The entropy regularization is a technique that is widely used for policy-based RL algorithms . We think that it would be possible to combine these two as entropy regularization seeks to diversify sampled actions and EMI seeks to diversify sampled states . 7. \u201c Section A.2 can be clearer \u201d : We polished Appendix 2 in terms of the distinction between residual errors and sample errors ( error terms ) . 8. \u201c Claims to be less strong \u201d : As RL problems exhibit high variance on the return especially on environments with sparse rewards , we do not agree that one method is not better than the other just because of the overlapping error bars . Our experiments are all performed in sparse reward setting and the return plots in Figure 4 and a subset of Figure 5 show clear advantage of our method over other exploration baselines . Having said that , we moderated the claim a bit . 9. \u201c Typos : lambda , step size \u201d : Thank you for pointing this out . We modified Equation 9 to make it coherent with lambdas in Appendix . TRPO step size is hyper-parameter related to KL divergence in TRPO algorithm which uses conjugate gradients to update policy network . [ 1 ] Sebastian Nowozin , Botond Cseke , and Ryota Tomioka . f-gan : Training generative neural samplers using variational divergence minimization . In Advances in Neural Information Processing Systems , pp . 271\u2013279 , 2016 ."}, "1": {"review_id": "Hylyui09tm-1", "review_text": "This is a very interesting paper about a novel approach to exploration in agents with state and action representations, making heavy use of recent progress in the use of deep learning for estimating and maximizing mutual information, as well as introducing an approach to model the latent space dynamics with a linear models with sparse errors. A closely related work which is not mentioned is the work of Thomas et al 2017 arXiv:1708.01289 where they also maximize mutual information between distributed representations of actions (policies, actually) and of distributed representations of changes in the state (as the result of applying the policy). The phrase 'functionally similar states' is used several times and would require a bit of explanation. I would also like to see more motivations for the two different reward functions r_e and r_d, and why one should be computed before the update while the other should be computed after. Regarding the experiments, and this is probably the weakest part of this paper, I would have expected to see comparisons against several of the numerous exploration methods which have been proposed in the past and are discussed in the paper (with many negative comments about their weakness, but no empirical support provided). Only one (EX2) was compared. The comparison with TRPO is without exploration (if I understand well, but should be stated clearly). It's also not clear how these results compare to the best reported results on these games (whether or not exploration is used). ", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer , We would like to thank you for the time and effort spent providing the feedback . We address the questions below and also updated corresponding response in the pdf submission . 1. \u201c Mention the work of Thomas et al 2017 \u201d : Thank you for spotting this . We added the work to the related works section . 2. \u201c Motivations for the two different reward functions r_e and r_d , and updating order \u201d : We propose two different intrinsic reward functions to show that EMI can be utilized with diverse intrinsic reward functions and is not constrained to the particular form of intrinsic reward function . As RL agents behave differently with different intrinsic reward functions in various environments , we can use EMI with appropriate intrinsic reward functions depending on environments . ( Example : 1 ) Diversity reward gives good performance in SparseHalfCheetah ( Mujuco ) where the task is to go as far away from the origin . 2 ) Prediction error reward works well in Montezuma \u2019 s Revenge ( Atari ) where the agent can receive high prediction error by entering different rooms . ) 3. \u201c Comparisons against several of the numerous exploration methods \u201d : We had experiments for both EX2 [ 1 ] and ICM [ 2 ] on Atari experiments . But for locomotion tasks , we only compared against EX2 . This was because ICM was mainly designed for discrete actions . To this end , we extended ICM to accommodate continuous actions ( by replacing the cross entropy loss for categorical policy with L2 loss for continuous policy ) ran it on continuous locomotion tasks to as another baseline in the experiments section . Also , we added Table 1 to further compare the final performance of EMI against other exploration methods . [ 1 ] Justin Fu , John Co-Reyes , and Sergey Levine . Ex2 : Exploration with exemplar models for deep reinforcement learning . In Advances in Neural Information Processing Systems , pp . 2577\u20132587 , 2017 . [ 2 ] Deepak Pathak , Pulkit Agrawal , Alexei A Efros , and Trevor Darrell . Curiosity-driven exploration by self-supervised prediction . In International Conference on Machine Learning , volume 2017 , 2017 ."}, "2": {"review_id": "Hylyui09tm-2", "review_text": "This paper introduces actions as a co-predictor of next-states and the predicted (from current and next state) in the context of (model-based) RL. In addition they incorporate the idea of using a JSD-based objective do prediction (as the Deep InfoMax paper), which is novel to RL. The enforce a linear structure between current / next states and actions with an additional sparse nonlinear term computed from both current states and actions. From this, they are able to quantify the amount of novelty in the representation space as a measure of exploration, which can be used as an intrinsic reward. I found the paper to be very well-written and easy to understand. The prediction part is similar to that used in CPC structurally, except they include the action in two different prediction tasks and they have some built-in intrinsic rewards, which is good. I had some issues with the motivations of some of the loss functions. - The JSD-based objective makes sense, but I don't think it's correct to call it an \"approximation\" to the KL (this is only true where the log-ratio of the joint and the product of marginals is small). Rather, it would be better to describe this choice as simply using a different measure between the joint and marginals. - It seems like the best motivation for having linear relations is you can do multiple predictions using the same state / action encodings. - For measuring exploration (11) couldn't one just use the predictor models T? How does the output of T (perhaps correctly normalized with the marginals) correlate with (11)? Other notes: Page 2: Figure 1 is awfully confusing. Could this be clarified a little bit? I\u2019m not sure what the small dots or their colors are supposed to represent. Could diversity also be added by adding a prior to the state representations (as is done in Deep InfoMax)? Why were the vision experiments stopped at 500 x 100k (500 million) frames? I can\u2019t validate the SOTA claims, but it seems like the model is still improving: are there\u2019s further experiments? An ablation study would be nice comparing the different hyper parameters (intrinsic rewards, diversity, etc).", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer , We would like to thank you for the time and effort spent providing the feedback . We address the questions below and also updated corresponding response in the pdf submission . 1. \u201c JSD as an `` approximation '' to the KL \u201d : We agree that it would be better not to state JSD as \u201c approximation \u201d to the KL . We modified our statement to call it \u201c different measure \u201d in section 4.1 . 2. \u201c Using predictor models with output of T \u201d : This is a valid suggestion , as by the mutual information maximizing objective , the network T is encouraged to output smaller values for novel samples . We ran experiments in SparseHalfCheetah environment , and were able to confirm some degree of exploration effect of the intrinsic rewards derived by the outputs of the network T. Although the results are not comparable to our proposed method ( average return of 75 vs. 200 after 1,000 iterations in SparseHalfCheetah ) , it will be an interesting future research direction . 3. \u201c Figure 1 is awfully confusing \u201d : Sorry for the confusion . We updated Figure 1 and hope it clarifies the big picture . 4. \u201c Adding a prior to the state representations \u201d : Regularizing the distribution of state embeddings instead causes the optimization process to be much more unstable . This is because the distribution of states is much more likely to be skewed than the distribution of actions , especially during the initial stage of optimization , so the Gaussian approximation of the distribution of state embeddings in the KL regularization term becomes much less accurate in contrast to the distribution of actions . We added this statement to section 4.2 . 5. \u201c Vision experiments stopped at 50 frames \u201d : We stopped at 50 million frames to perform fair comparisons to other baseline methods . TRPO-based exploration methods such as EX2 [ 1 ] , SimHash [ 2 ] stopped at 50 million frames . 6. \u201d An ablation study would be nice \u201d : We added ablation study in Appendix . In figure 7 , We ablated on each loss terms of Equation 9 to show the impact of each term . In figure 8 , we tested on different intrinsic reward hyper-parameters to show the robustness of our method . [ 1 ] Justin Fu , John Co-Reyes , and Sergey Levine . Ex2 : Exploration with exemplar models for deep reinforcement learning . In Advances in Neural Information Processing Systems , pp . 2577\u20132587 , 2017 . [ 2 ] Haoran Tang , Rein Houthooft , Davis Foote , Adam Stooke , Xi Chen , Yan Duan , John Schulman , Filip DeTurck , and Pieter Abbeel . # exploration : A study of count-based exploration for deep reinforcement learning . In Advances in Neural Information Processing Systems , pp . 2753\u20132762 , 2017 ."}}