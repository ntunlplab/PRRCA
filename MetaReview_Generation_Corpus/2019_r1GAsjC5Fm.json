{"year": "2019", "forum": "r1GAsjC5Fm", "title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "decision": "Accept (Poster)", "meta_review": "The authors have described a navigation method that uses co-grounding between language and vision as well as an explicit self-assessment of progress. The method is used for room 2 room navigation and is tested in unseen environments. On the positive side, the approach is well-analyzed, with multiple ablations and baseline comparisons. The method is interesting and could be a good starting point for a more ambitious grounded language-vision agent. The approach seems to work well and achieves a high score using the metric of successful goal acquisition. On the negative side, the method relies on beam search, which is certainly unrealistic for real-world navigation, the evaluation metric is very simple and may be misleading, and the architecture is quite complex, may not scale or survive the test of time, and has little relevance for the greater ML community. There was a long discussion between the authors and the reviewers and other members of the public that resolved many of these points, with the authors being extremely responsive in giving additional results and details, and the reviewers' conclusion is that the paper should be accepted. ", "reviews": [{"review_id": "r1GAsjC5Fm-0", "review_text": "This submission introduces a new method for vision+language navigation which tracks progress on the instruction using a progress monitor and a visual-textual co-grounding module. The method is shown to perform well on a standard benchmark. Ablation tests indicate the importance of each component of the model. Qualitative examples show that the proposed method attends to different parts of the instruction as the agent moves. Here are some comments/questions: - I like the underlying idea behind the method. The manuscript is written well for most parts. - The qualitative examples and Figure 2 are really helpful in understanding the reasons behind the improved performance. - There is a lot of confusion regarding the use of beam search. It's unclear from the current manuscript which results are with and without beam search. It seems like beam search was added from Ours 1 to Ours 2 in Table 2. It's not clear which rows involve beam search in Table 1. Some concerns about beam width were raised in the comments which I agree with. Please modify the submission to clearly indicate the use of beam search for each result and specify the beam width. - The use of beam search seems unrealistic to me as I can not think of any way a navigational model using beam search can be transferred or applied to real-world. I understand that one of the baselines uses beam search, so it's fair for performance comparison purposes, but could you provide any justification of how it might be useful in real-world? If there's no reasonable justification, could you also provide all the results (along with SPL metric) without beam search, including ablation, comparing with only methods without beam search? - I do not understand why the OSR in the submission is 0.64 and 0.70 for Speaker-Follower and proposed method and 0.96 and 0.97 in the comments. - It seems like the proposed method is tailored for the VLN task. In many real-world scenarios, an agent might be given an instruction which only describes the goal (such as in Chaplot et al. 2017 and Hermann et al. 2017) and not the path to the goal, could the authors provide their thoughts on whether the proposed would work well for such instructions? What would the progress monitor and textual attention distribution learn in such a scenario? Due to confusion about results and concerns about beam search, I give a rating of 5. I am willing to increase the rating if the authors address the above concerns.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Hi , We would like to thank the reviewer for the thoughtful and constructive feedback . 1.Regarding \u201c confusion regarding the result of beam search \u201d : The updated ablation study table is shown below , and the same table has been added to the revised paper . In this table , we show the performance improvement of each component with different inference modes ( which are described in the revised paper ) . We have also added the recently introduced SPL metric under all settings to table . From the results , we can again see that the proposed components improved the performance across different evaluation metrics . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Inference Mode Validation-Seen Validation-Unseen Co- Progress Greedy Progress Beam Data # Grounding Monitor Decoding Inference Search Aug. NE\u2193 SR\u2191 OSR\u2191 SPL\u2191 NE\u2193 SR\u2191 OSR\u2191 SPL\u2191 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Baseline 4.36 0.54 0.68 - 7.22 0.27 0.39 - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Regarding \u201c The use of beam search and SPL metric \u201d : We have provided all the results without beam search in the updated ablation study table ( see table above or from the revised paper ) , and the comparison with methods without beam search is shown in the table below ( for Speaker-Follower , the Pragmatic Inference which relies on beam search is removed for comparison purpose ) . This table is also included in the revised paper ."}, {"review_id": "r1GAsjC5Fm-1", "review_text": "This paper describes a model for vision-and-language navigation. The proposed model adds two components to the baseline model proposed by Fried et al. (2018): - a panoramic visual attention (referred to in this paper as \"visual--textual co-grounding\"), in which the full scene around the agent's current position is attended to prior to selecting a direction to follow - an auxiliary \"progress monitoring\" loss which encourages the agent to to produce textual attentions from which the distance to the goal can be directly inferred The two components combine to give state-of-the-art results on the Room2Room dataset: small improvements over existing approaches on the \"-seen\" evaluation set and larger improvements on the \"-unseen\" evaluation sets. These improvements also stack with the data-augmentation approach of Fried et al. I think this is a reasonable submission and should probably be accepted. However, I have some concerns about presentation and a number of specific questions about model implementation and evaluation. PRESENTATION AND NAMING First off: I implore the authors to find some descriptor other than \"self-aware\" for the proposed model. \"Self-aware\" is an imprecise description of the agent in this paper---the agent is specifically \"aware\" of its visual surroundings and its distance from the goal, neither of which is meaningfully an aspect of \"self\". Moreover, self-awareness means something quite different in adjacent areas of cognitive science and philosophy; overloading the term in the specific (and comparatively mundane) way used here creates confusion. See section 3.4 of https://arxiv.org/abs/1807.03341 for broader discussion. Perhaps something like \"visual / temporal context-sensitivity\" to describe what's new here? A bit clunky, but I think it makes the contributions of this work much clearer. As suggested in the summary above, I also think \"visual--textual co-attention\" is also an unhelpfully vague description of this aspect of the contribution. The textual attention mechanism used in this paper is the same as in all previous work on the task. Representations of language don't even interact with the visual attention mechanism except by way of the hidden state, and the salient new feature of the visual attention is the fact that it considers the full panoramic context before choosing a direction. MODELING QUESTIONS - p4: $y_t^{pm}$ is defined as the \"normalized distance from the current viewpoint to the goal\". Is this distance in units of length (as defined by the simulator) or units of time (i.e. the number of discrete \"steps\" needed to reach the goal)? The authors have already clarified on OpenReview that the progress monitor objective uses an MSE loss rather than a likelihood loss. Do I understand correctly that ground-truth distances are in [0, 1] but model predictions are in [-1, 1]? Why not use a sigmoid? Also, how does scoring beam-search candidates as $p_t^{pm} \\times p_{k,t}$ work if $p_t^{pm}$ can flip the sign? - The input to the progress monitor is formed by concatenating the attention vector $\\alpha_t$ to a vector of state features, and then multiplying by a fixed weight matrix. How is this possible? The size of $\\alpha_t$ varies depending on the length of the instruction sequence. Are attentions padded out to the length of the longest instruction in the training set? If so, how can the model learn when it's reached the end of a short instruction sequence? What would happen if the agent encountered a sequence that was too long? EVALUATION QUESTIONS - The progress monitor is used both as an auxiliary training objective and as a beam search heuristic. Is it possible to disentangle these two contributions? (E.g. by ignoring the scores during beam search, or by doing augmented beam search in a model that was trained without the auxiliary objective.) - Not critical, but it would be nice to know if the contributions here stack with the pragmatic inference procedure in Fried et al. - While, as pointed out on OpenReview, it is not required to include SPL evaluations, I think it would be informative to do so---the preliminary results with no beam search look good! MISCELLANEOUS p1: \"without a map\" If you can do beam search, you effectively have a map. p1: \"...smoothly\" What does \"smoothly\" mean in this context? p2: \"the position of grounded instruction can follow past and future instructions\". Is the claim here that if instructions are of the form \"ACB\" and the agent is supposed to do \"ABC\", that the proposed model will execute these instructions successfully and the baseline will not? This claim does not appear to be evaluated anywhere in the body of the paper. p4: \"intelligently prunes\" \"Intelligently\" is unnecessary. p4: \"for empirical reasons\" What does this mean? p5: \"Intuitively, an instruction-following agent is required...\" The existence of non-attentive models that do reasonably well at these instruction-following tasks suggest that this is not actually a requirement.", "rating": "7: Good paper, accept", "reply_text": "Hi , We would like to thank the reviewer for their thoughtful and constructive feedback . 1.Regarding \u201c presentation and naming \u201d : We thank the reviewer for pointing out the potential issue regarding the naming of the paper . We agree with the reviewer and we have been trying our best to find a better name . For now , one of the options that we come up with is \u201c Self-Monitoring Navigation Agent via Auxiliary Progress Estimation \u201d . We hope this new title is more clear and suitable for the work . If the reviewers have other suggestions , please kindly let us know . 2.Regarding \u201c the definition of distance \u201d : The distance is defined in units of length the same as the simulator . We also agree that using the number of steps is also a reasonable approach , and it would be interesting to explore in the future . We have clarified the definition in the revision . 3.Regarding \u201d using sigmoid as opposed to tanh \u201d : We have found that using sigmoid performs similarly to using the tanh function . The results on different metrics are shown below : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Val-seen Val-unseen NE\u2193 SR\u2191 OSR\u2191 SPL\u2191 NE\u2193 SR\u2191 OSR\u2191 SPL\u2191 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Ours ( Tanh ) 3.72 0.63 0.75 0.56 5.98 0.44 0.58 0.30 Ours ( Sigmoid ) 3.72 0.64 0.72 0.59 5.92 0.44 0.56 0.33 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - The main incentive for using tanh is to allow the agent to wander around when it \u2019 s distance to As a result , this allows the progress monitor to output values lower than 0 indicating that the agent has lost the track of textual grounding . We simply normalize the output of progress monitor between 0 to 1 before combining the score for beam search . We have revised the paper to clarify this accordingly . 4.Regarding \u201c instructions with various lengths \u201d : We use zero-padding to handle instructions with various lengths . We have also explored ideas like using interpolation to upsample the attention weights of short instructions to a fixed length of 80 , but it produces a similar performance as without interpolation . Generally , we have observed that the validation samples with longer instructions are more likely to fail , but this may due to the fact that the required total number of steps of these samples is larger . Hence , the agent is prone to errors since it needs to predict actions correctly for more steps ."}, {"review_id": "r1GAsjC5Fm-2", "review_text": "The paper considers the problem of following natural language route instructions in an unknown environment given only images. Integral to the proposed (\"self-aware\") approach is its ability to reason over which aspects of the instruction have been completed, which are to be followed next, which direction to go in next, as well as the agents current progress. This involves two primary components of the architecture. The first is a visual-textual module that grounds to the completed instruction, the next instruction, and the next direction based upon the visual input. The second is a \"progress monitor\" that takes the grounded instruction as input and captures the agent's progress towards completing the instruction. STRENGTHS + The paper describes an interesting approach to reasoning over which aspects of a given instruction have been correctly followed and which aspect to act on next. This takes the form of a visual-textual co-grounding model that identifies the instruction previously completed, the instruction corresponding to the next action, and the subsequent direction in which to move. The inclusion of a \"progress monitor\" allows the method to reason over whether the navigational progress matches the instruction. + The paper provides a thorough evaluation on a challenging benchmark language understanding dataset. This evaluation includes detailed comparisons to state-of-the-art baselines together with ablation studies to understand the contribution of the different components of the architecture. + The paper is well written and provides a thorough description of the framework with sufficient details to support replication of the results. WEAKNESSES - The paper would benefit from a more compelling argument for the importance of reasoning over which aspects of the instruction have been completed vs. which to act on next. - The paper emphasizes the use of images, the visual grounding reasons over visual features. - The paper incorrectly states that existing methods for language understanding require an explicit representation of the target. Several existing methods do not have this requirement. For example, Matuszek et al., 2012 parse free-form language into a formal logic representation for a downstream controller that interprets these instructions in unknown environments. Meanwhile, Duvallet et al., 2014 and Hemachandra et al., 2015 exploit language (together with vision and LIDAR) to learn a distribution over the unknown environment that guides grounding. Meanwhile, Mei et al., 2016 reason only over natural language text and parsed images, without knowledge of the environment or an explicit representation of the goal. C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox, \u201cLearning to parse natural language commands to a robot control system,\u201d in Proceedings of the International Symposium on Experimental Robotics (ISER), 2012. S. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz, and M. R. Walter, \u201cLearning models for following natural language directions in unknown environments,\u201d in Proc. IEEE Int\u2019l Conf. on Robotics and Automation (ICRA), 2015 F. Duvallet, M. R. Walter, T. Howard, S. Hemachandra, J. Oh, S. Teller, N. Roy, and A. Stentz, \u201cInferring maps and behaviors from natural language instructions,\u201d in Proceedings of the International Symposium on Experimental Robotics (ISER), 2014. - While it's not a neural approach, the work of Arkin et al., 2017 which reasons over the entire instruction history when deciding on actions (through a statistical symbol grounding formulation)\u2060 J. Arkin, M. Walter, A. Boteanu, M. Napoli, H. Biggie, H. Kress-Gazit, and T. Howard. \"Contextual Awareness: Understanding Monologic Natural Language Instructions for Autonomous Robots,\" In Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2017 - The paper misses the large body of literature on grounded language acquisition for robotics. QUESTIONS * What is the effect of using positional encoding for textual grounding as opposed to standard alignment methods such as those used by Mei et al., 2016? * Perhaps I missed it, but what happens if instructions are specified in such a way that their ordering is not consistent with the correct action ordering (e.g., with corrections interjected)? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Hi , We would like to thank the reviewer for the thoughtful and constructive feedback . We thank the reviewer for bringing the additional literature from related fields to our attention . We have included and discussed them in the revised paper ( please see both introduction and related work sections for the changes we made ) . 1.Regarding \u201c importance of reasoning over completed or next instruction \u201d : We rewrote the sentences in the second paragraph in the introduction and try to make the reason why reasoning over both past and present instructions is important and essential . Please see the revised paper for the changes we made . We emphasize that the transition between past and next part of the instructions is a soft boundary , in order to determine when to transit and to follow the instruction correctly the agent is required to keep track of both grounded instructions . 2.Regarding \u201c visual grounding over visual features \u201d : In order to provide a fair comparison with prior arts , we chose to use the image feature vector provided directly with this task . Our current visual grounding module performs attention over different parts of the panoramic image and grounds the located instruction to a part of the panoramic image . To further provide fine-grained visual grounding , we agree that it would be interesting to use panoramic images directly as input and perform visual grounding on feature maps or object-level bounding boxes . 3.Regarding \u201c effect of positional encoding \u201d : In our early experiments , we found that , although removing positional encoding can achieve better results on val-seen , the agent overfits quickly on val-unseen . We believe that the agent \u2019 s ability to generalize to unseen environments is more important than achieving good results on val-seen . Thus , we use positional encoding for ablation study and produce the final result . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Val-seen Val-unseen NE\u2193 SR\u2191 OSR\u2191 SPL\u2191 NE\u2193 SR\u2191 OSR\u2191 SPL\u2191 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Ours ( No PE ) 3.37 0.69 0.78 0.61 6.04 0.42 0.55 0.30 Ours 3.72 0.63 0.75 0.56 5.98 0.44 0.58 0.30 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - 4 . Regarding \u201c what if the orderings of the instruction and actions are inconsistent \u201d : The Room-to-Room dataset comes with the ground-truth starting location , goal location , and the instruction associated with it . The ground-truth trajectory is computed as the shortest path from the navigational graph from starting location to the goal , and the quality of given instructions is verified by humans which achieved 86 % success rate on the test set . From our own observation , the ordering of actions is consistent with the ordering of instructions . If this was not the case , our current language grounding mechanism may still applicable since it can represent an arbitrary weighting over the sentence . Similarly , since progress monitoring is a learned function over this it could still learn to estimate progress . However , note that our assessment depends on how different the orders of instruction and actions are . If the difference is small , our agent is very likely to be able to recover from incorrect instruction . We believe this can be an interesting direction for future work ."}], "0": {"review_id": "r1GAsjC5Fm-0", "review_text": "This submission introduces a new method for vision+language navigation which tracks progress on the instruction using a progress monitor and a visual-textual co-grounding module. The method is shown to perform well on a standard benchmark. Ablation tests indicate the importance of each component of the model. Qualitative examples show that the proposed method attends to different parts of the instruction as the agent moves. Here are some comments/questions: - I like the underlying idea behind the method. The manuscript is written well for most parts. - The qualitative examples and Figure 2 are really helpful in understanding the reasons behind the improved performance. - There is a lot of confusion regarding the use of beam search. It's unclear from the current manuscript which results are with and without beam search. It seems like beam search was added from Ours 1 to Ours 2 in Table 2. It's not clear which rows involve beam search in Table 1. Some concerns about beam width were raised in the comments which I agree with. Please modify the submission to clearly indicate the use of beam search for each result and specify the beam width. - The use of beam search seems unrealistic to me as I can not think of any way a navigational model using beam search can be transferred or applied to real-world. I understand that one of the baselines uses beam search, so it's fair for performance comparison purposes, but could you provide any justification of how it might be useful in real-world? If there's no reasonable justification, could you also provide all the results (along with SPL metric) without beam search, including ablation, comparing with only methods without beam search? - I do not understand why the OSR in the submission is 0.64 and 0.70 for Speaker-Follower and proposed method and 0.96 and 0.97 in the comments. - It seems like the proposed method is tailored for the VLN task. In many real-world scenarios, an agent might be given an instruction which only describes the goal (such as in Chaplot et al. 2017 and Hermann et al. 2017) and not the path to the goal, could the authors provide their thoughts on whether the proposed would work well for such instructions? What would the progress monitor and textual attention distribution learn in such a scenario? Due to confusion about results and concerns about beam search, I give a rating of 5. I am willing to increase the rating if the authors address the above concerns.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Hi , We would like to thank the reviewer for the thoughtful and constructive feedback . 1.Regarding \u201c confusion regarding the result of beam search \u201d : The updated ablation study table is shown below , and the same table has been added to the revised paper . In this table , we show the performance improvement of each component with different inference modes ( which are described in the revised paper ) . We have also added the recently introduced SPL metric under all settings to table . From the results , we can again see that the proposed components improved the performance across different evaluation metrics . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Inference Mode Validation-Seen Validation-Unseen Co- Progress Greedy Progress Beam Data # Grounding Monitor Decoding Inference Search Aug. NE\u2193 SR\u2191 OSR\u2191 SPL\u2191 NE\u2193 SR\u2191 OSR\u2191 SPL\u2191 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Baseline 4.36 0.54 0.68 - 7.22 0.27 0.39 - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Regarding \u201c The use of beam search and SPL metric \u201d : We have provided all the results without beam search in the updated ablation study table ( see table above or from the revised paper ) , and the comparison with methods without beam search is shown in the table below ( for Speaker-Follower , the Pragmatic Inference which relies on beam search is removed for comparison purpose ) . This table is also included in the revised paper ."}, "1": {"review_id": "r1GAsjC5Fm-1", "review_text": "This paper describes a model for vision-and-language navigation. The proposed model adds two components to the baseline model proposed by Fried et al. (2018): - a panoramic visual attention (referred to in this paper as \"visual--textual co-grounding\"), in which the full scene around the agent's current position is attended to prior to selecting a direction to follow - an auxiliary \"progress monitoring\" loss which encourages the agent to to produce textual attentions from which the distance to the goal can be directly inferred The two components combine to give state-of-the-art results on the Room2Room dataset: small improvements over existing approaches on the \"-seen\" evaluation set and larger improvements on the \"-unseen\" evaluation sets. These improvements also stack with the data-augmentation approach of Fried et al. I think this is a reasonable submission and should probably be accepted. However, I have some concerns about presentation and a number of specific questions about model implementation and evaluation. PRESENTATION AND NAMING First off: I implore the authors to find some descriptor other than \"self-aware\" for the proposed model. \"Self-aware\" is an imprecise description of the agent in this paper---the agent is specifically \"aware\" of its visual surroundings and its distance from the goal, neither of which is meaningfully an aspect of \"self\". Moreover, self-awareness means something quite different in adjacent areas of cognitive science and philosophy; overloading the term in the specific (and comparatively mundane) way used here creates confusion. See section 3.4 of https://arxiv.org/abs/1807.03341 for broader discussion. Perhaps something like \"visual / temporal context-sensitivity\" to describe what's new here? A bit clunky, but I think it makes the contributions of this work much clearer. As suggested in the summary above, I also think \"visual--textual co-attention\" is also an unhelpfully vague description of this aspect of the contribution. The textual attention mechanism used in this paper is the same as in all previous work on the task. Representations of language don't even interact with the visual attention mechanism except by way of the hidden state, and the salient new feature of the visual attention is the fact that it considers the full panoramic context before choosing a direction. MODELING QUESTIONS - p4: $y_t^{pm}$ is defined as the \"normalized distance from the current viewpoint to the goal\". Is this distance in units of length (as defined by the simulator) or units of time (i.e. the number of discrete \"steps\" needed to reach the goal)? The authors have already clarified on OpenReview that the progress monitor objective uses an MSE loss rather than a likelihood loss. Do I understand correctly that ground-truth distances are in [0, 1] but model predictions are in [-1, 1]? Why not use a sigmoid? Also, how does scoring beam-search candidates as $p_t^{pm} \\times p_{k,t}$ work if $p_t^{pm}$ can flip the sign? - The input to the progress monitor is formed by concatenating the attention vector $\\alpha_t$ to a vector of state features, and then multiplying by a fixed weight matrix. How is this possible? The size of $\\alpha_t$ varies depending on the length of the instruction sequence. Are attentions padded out to the length of the longest instruction in the training set? If so, how can the model learn when it's reached the end of a short instruction sequence? What would happen if the agent encountered a sequence that was too long? EVALUATION QUESTIONS - The progress monitor is used both as an auxiliary training objective and as a beam search heuristic. Is it possible to disentangle these two contributions? (E.g. by ignoring the scores during beam search, or by doing augmented beam search in a model that was trained without the auxiliary objective.) - Not critical, but it would be nice to know if the contributions here stack with the pragmatic inference procedure in Fried et al. - While, as pointed out on OpenReview, it is not required to include SPL evaluations, I think it would be informative to do so---the preliminary results with no beam search look good! MISCELLANEOUS p1: \"without a map\" If you can do beam search, you effectively have a map. p1: \"...smoothly\" What does \"smoothly\" mean in this context? p2: \"the position of grounded instruction can follow past and future instructions\". Is the claim here that if instructions are of the form \"ACB\" and the agent is supposed to do \"ABC\", that the proposed model will execute these instructions successfully and the baseline will not? This claim does not appear to be evaluated anywhere in the body of the paper. p4: \"intelligently prunes\" \"Intelligently\" is unnecessary. p4: \"for empirical reasons\" What does this mean? p5: \"Intuitively, an instruction-following agent is required...\" The existence of non-attentive models that do reasonably well at these instruction-following tasks suggest that this is not actually a requirement.", "rating": "7: Good paper, accept", "reply_text": "Hi , We would like to thank the reviewer for their thoughtful and constructive feedback . 1.Regarding \u201c presentation and naming \u201d : We thank the reviewer for pointing out the potential issue regarding the naming of the paper . We agree with the reviewer and we have been trying our best to find a better name . For now , one of the options that we come up with is \u201c Self-Monitoring Navigation Agent via Auxiliary Progress Estimation \u201d . We hope this new title is more clear and suitable for the work . If the reviewers have other suggestions , please kindly let us know . 2.Regarding \u201c the definition of distance \u201d : The distance is defined in units of length the same as the simulator . We also agree that using the number of steps is also a reasonable approach , and it would be interesting to explore in the future . We have clarified the definition in the revision . 3.Regarding \u201d using sigmoid as opposed to tanh \u201d : We have found that using sigmoid performs similarly to using the tanh function . The results on different metrics are shown below : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Val-seen Val-unseen NE\u2193 SR\u2191 OSR\u2191 SPL\u2191 NE\u2193 SR\u2191 OSR\u2191 SPL\u2191 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Ours ( Tanh ) 3.72 0.63 0.75 0.56 5.98 0.44 0.58 0.30 Ours ( Sigmoid ) 3.72 0.64 0.72 0.59 5.92 0.44 0.56 0.33 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - The main incentive for using tanh is to allow the agent to wander around when it \u2019 s distance to As a result , this allows the progress monitor to output values lower than 0 indicating that the agent has lost the track of textual grounding . We simply normalize the output of progress monitor between 0 to 1 before combining the score for beam search . We have revised the paper to clarify this accordingly . 4.Regarding \u201c instructions with various lengths \u201d : We use zero-padding to handle instructions with various lengths . We have also explored ideas like using interpolation to upsample the attention weights of short instructions to a fixed length of 80 , but it produces a similar performance as without interpolation . Generally , we have observed that the validation samples with longer instructions are more likely to fail , but this may due to the fact that the required total number of steps of these samples is larger . Hence , the agent is prone to errors since it needs to predict actions correctly for more steps ."}, "2": {"review_id": "r1GAsjC5Fm-2", "review_text": "The paper considers the problem of following natural language route instructions in an unknown environment given only images. Integral to the proposed (\"self-aware\") approach is its ability to reason over which aspects of the instruction have been completed, which are to be followed next, which direction to go in next, as well as the agents current progress. This involves two primary components of the architecture. The first is a visual-textual module that grounds to the completed instruction, the next instruction, and the next direction based upon the visual input. The second is a \"progress monitor\" that takes the grounded instruction as input and captures the agent's progress towards completing the instruction. STRENGTHS + The paper describes an interesting approach to reasoning over which aspects of a given instruction have been correctly followed and which aspect to act on next. This takes the form of a visual-textual co-grounding model that identifies the instruction previously completed, the instruction corresponding to the next action, and the subsequent direction in which to move. The inclusion of a \"progress monitor\" allows the method to reason over whether the navigational progress matches the instruction. + The paper provides a thorough evaluation on a challenging benchmark language understanding dataset. This evaluation includes detailed comparisons to state-of-the-art baselines together with ablation studies to understand the contribution of the different components of the architecture. + The paper is well written and provides a thorough description of the framework with sufficient details to support replication of the results. WEAKNESSES - The paper would benefit from a more compelling argument for the importance of reasoning over which aspects of the instruction have been completed vs. which to act on next. - The paper emphasizes the use of images, the visual grounding reasons over visual features. - The paper incorrectly states that existing methods for language understanding require an explicit representation of the target. Several existing methods do not have this requirement. For example, Matuszek et al., 2012 parse free-form language into a formal logic representation for a downstream controller that interprets these instructions in unknown environments. Meanwhile, Duvallet et al., 2014 and Hemachandra et al., 2015 exploit language (together with vision and LIDAR) to learn a distribution over the unknown environment that guides grounding. Meanwhile, Mei et al., 2016 reason only over natural language text and parsed images, without knowledge of the environment or an explicit representation of the goal. C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox, \u201cLearning to parse natural language commands to a robot control system,\u201d in Proceedings of the International Symposium on Experimental Robotics (ISER), 2012. S. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz, and M. R. Walter, \u201cLearning models for following natural language directions in unknown environments,\u201d in Proc. IEEE Int\u2019l Conf. on Robotics and Automation (ICRA), 2015 F. Duvallet, M. R. Walter, T. Howard, S. Hemachandra, J. Oh, S. Teller, N. Roy, and A. Stentz, \u201cInferring maps and behaviors from natural language instructions,\u201d in Proceedings of the International Symposium on Experimental Robotics (ISER), 2014. - While it's not a neural approach, the work of Arkin et al., 2017 which reasons over the entire instruction history when deciding on actions (through a statistical symbol grounding formulation)\u2060 J. Arkin, M. Walter, A. Boteanu, M. Napoli, H. Biggie, H. Kress-Gazit, and T. Howard. \"Contextual Awareness: Understanding Monologic Natural Language Instructions for Autonomous Robots,\" In Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2017 - The paper misses the large body of literature on grounded language acquisition for robotics. QUESTIONS * What is the effect of using positional encoding for textual grounding as opposed to standard alignment methods such as those used by Mei et al., 2016? * Perhaps I missed it, but what happens if instructions are specified in such a way that their ordering is not consistent with the correct action ordering (e.g., with corrections interjected)? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Hi , We would like to thank the reviewer for the thoughtful and constructive feedback . We thank the reviewer for bringing the additional literature from related fields to our attention . We have included and discussed them in the revised paper ( please see both introduction and related work sections for the changes we made ) . 1.Regarding \u201c importance of reasoning over completed or next instruction \u201d : We rewrote the sentences in the second paragraph in the introduction and try to make the reason why reasoning over both past and present instructions is important and essential . Please see the revised paper for the changes we made . We emphasize that the transition between past and next part of the instructions is a soft boundary , in order to determine when to transit and to follow the instruction correctly the agent is required to keep track of both grounded instructions . 2.Regarding \u201c visual grounding over visual features \u201d : In order to provide a fair comparison with prior arts , we chose to use the image feature vector provided directly with this task . Our current visual grounding module performs attention over different parts of the panoramic image and grounds the located instruction to a part of the panoramic image . To further provide fine-grained visual grounding , we agree that it would be interesting to use panoramic images directly as input and perform visual grounding on feature maps or object-level bounding boxes . 3.Regarding \u201c effect of positional encoding \u201d : In our early experiments , we found that , although removing positional encoding can achieve better results on val-seen , the agent overfits quickly on val-unseen . We believe that the agent \u2019 s ability to generalize to unseen environments is more important than achieving good results on val-seen . Thus , we use positional encoding for ablation study and produce the final result . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Val-seen Val-unseen NE\u2193 SR\u2191 OSR\u2191 SPL\u2191 NE\u2193 SR\u2191 OSR\u2191 SPL\u2191 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Ours ( No PE ) 3.37 0.69 0.78 0.61 6.04 0.42 0.55 0.30 Ours 3.72 0.63 0.75 0.56 5.98 0.44 0.58 0.30 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - 4 . Regarding \u201c what if the orderings of the instruction and actions are inconsistent \u201d : The Room-to-Room dataset comes with the ground-truth starting location , goal location , and the instruction associated with it . The ground-truth trajectory is computed as the shortest path from the navigational graph from starting location to the goal , and the quality of given instructions is verified by humans which achieved 86 % success rate on the test set . From our own observation , the ordering of actions is consistent with the ordering of instructions . If this was not the case , our current language grounding mechanism may still applicable since it can represent an arbitrary weighting over the sentence . Similarly , since progress monitoring is a learned function over this it could still learn to estimate progress . However , note that our assessment depends on how different the orders of instruction and actions are . If the difference is small , our agent is very likely to be able to recover from incorrect instruction . We believe this can be an interesting direction for future work ."}}