{"year": "2020", "forum": "r1x0lxrFPS", "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "decision": "Accept (Poster)", "meta_review": "Three reviewers suggest acceptance. Reviewers were impressed by the thoroughness of the author response. Please take reviewer comments into account in the camera ready. Congratulations!", "reviews": [{"review_id": "r1x0lxrFPS-0", "review_text": "This paper proposes a new measure of gradient mismatch for training binary networks, and additionally proposes a method for getting better performance out of binary networks by initializing them to behave like a ternary network. I found the new measure of gradient deviation fairly underdeveloped, and I suspect the method of converting ternary activations into binary activations works for a different reason than that proposed by the authors. There were English language issues that somewhat reduced clarity, though the intended meaning was always understandable. Detailed comments: \"Binary Neural Network (BNN) has been gaining interest thanks to its computing cost reduction and memory saving.\" --> \"Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings.\" (will stop making English language corrections from here on) \"Therefore, we argue that the sharp accuracy drop for the binary activation stems from the inefficient training method, not the capacity of the model.\" This could also be due to poor initialization in the binary case. e.g., it might make sense to initialize the binary network with bias=-0.5, so that the nonlinearity has a kink at pre-activation=0, rather than pre-activation=0.5. \"Unfortunately, it is not possible to measure the amount of gradient mismatch directly because the true gradient of a quantized activation function is zero almost everywhere. \" It *is* possible to measure the mismatch to the true gradient exactly. One could even train using the true gradient. It's just that the true gradient is useless. Fig 1b -- this is a nice baseline. \"the steepest descent direction, which is the direction toward the point with the smallest loss at given distance\" This is not the usual definition of steepest descent direction. If you're going to redefine this, should do so mathematically and precisely (for instance, you are going to run into trouble with the word \"distance\", since your coordinate discrete gradient more closely resembles an L\\infty-ball perturbation, rather than an L2-ball perturbation. eq. 3: Note that this equation is equivalent to taking the true gradient of a function which has been boxcar-smoothed along each parameter. This may more closely resemble existing measures of deviation than you like. You should also consider the relationship to an evolutionary strategies style gradient estimate, which similarly provides an unbiased gradient estimate for a smoothed function, and which allows that estimate to be computed with fewer samples (at the cost of higher error). Sec. 4.2 / Figure 3: The results in this section will be *highly* sensitive to the choice of epsilon. You should discuss this, specify the epsilon used, and experimentally explore the dependence on epsilon. \"The results indicate that the cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better than previous approaches. \" Don't know that I followed this. Gradient mismatch is never formally defined, so it's hard to know what this says about its relationship. Additionally, CDG sounds more like something which is correlated with, rather than an explanation for, performance. \" cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better \" --> \" cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better \" \"we shift the bias of BN layer which comes right before the activation function layer. \" Did you try using these bias values without pre-training as a ternary network? I suspect it would work just as well! \"Please note that BN layers followed by binary activation layer can be merged to the threshold of the binary activation layer, incurring no overhead at inference stage.\" Did not understand this. \"it is expected that the fine-tuning increases the accuracy even further\" Does it improve the accuracy further? Should state this as result, not prediction, and should have an ablation experiment showing this. \"Table 2 shows the validation accuracy of BNN in various schemes.\" Why not test accuracy? Figure 6: What are the filled circles? What was the sampling grid for the HP search? The images have high spatial frequency structure that I suspect is an artifact of the interpolation function, rather than in the data. ---- Update post-rebuttal: The authors have addressed the majority of my concerns, through both text changes and significant additional experiments. I am therefore increasing my score. Thank you for your hard work!", "rating": "6: Weak Accept", "reply_text": "Q11 : `` it is expected that the fine-tuning increases the accuracy even further '' Does it improve the accuracy further ? Should state this as result , not prediction , and should have an ablation experiment showing this . A11 : We agree to state this as result rather than prediction . Our experimental results ( Figure 6 in Section 6.2 ) indeed show that the fine-tuning procedure actually increases the accuracy further . Please note that decoupling the ternary model without the fine-tuning does not change computation results of the network . Therefore , the accuracy of the decoupled binary model is same as that of the coupled ternary model when fine-tuning is not applied . For example , in case of VGG-7 on CIFAR-10 dataset , the accuracy of decoupled binary model without fine-tuning is 89.69 % which is same as that of the coupled ternary model ( shown in Section 6.1 ) . In contrast , during the fine-tuning process , the weight for each of the decoupled binary network is tuned separately . The results for VGG7 on CIFAR-10 dataset shows that 0.75 % of accuracy improvement can be achieved by fine-tuning which results in 90.44 % accuracy ( shown in Section 6.1 again ) . The improvement was also observed on ImageNet dataset . Accuracy results before and after fine-tuning for AlexNet , ResNet-18 , and ResNet-18 ( +sc ) are shown below . ================================================================= | | Before fine-tuning | After fine-tuning | | Network | Top-1 ( % ) | Top-5 ( % ) | Top-1 ( % ) | Top-5 ( % ) | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - | AlexNet | 50.7 | 74.4 | 52.7 | 76.0 | | ResNet-18 | 58.8 | 81.3 | 60.4 | 82.3 | | ResNet-18 ( +sc ) | 59.1 | 81.3 | 60.9 | 82.6 | ================================================================= To reflect the update , we revised the sentence as follows . \u201c experimental results show that the fine-tuning increases the accuracy even further . Detailed experimental results will be discussed in the next section. \u201d Q12 : `` Table 2 shows the validation accuracy of BNN in various schemes . '' Why not test accuracy ? A12 : Table 2 shows the validation accuracy of BNN in various schemes on the ImageNet Large Scale Visual Recognition Challenge ( ILSVRC ) 2012 dataset . ImageNet dataset has 1.28M images for training set and 50K images for validation set . Because the test set is not available to the public , ( the test set is privately used for the ImageNet Large Scale Visual Recognition Challenge competition only ) , all the results of previous work in Table 2 are validation accuracy . For fair comparison with previous results , we also provided validation accuracy for our scheme . If required , it is possible to split the provided training set to train/valid sets and to use the provided validation set as a test set . However , the test accuracy from the experiment can not be fairly compared with other results . Nevertheless , if reviewer asks for the test accuracy , we are willing to conduct addition training and report the test accuracy . Please let us know . Q13 : Figure 6 : What are the filled circles ? What was the sampling grid for the HP search ? The images have high spatial frequency structure that I suspect is an artifact of the interpolation function , rather than in the data . A13 : We apologize for lack of detailed description of the figure . We mistakenly moved the information along with other details to the Appendix A while we made the original submission fit to 8-page limit . For the experiment on CIFAR-10 dataset , we tried 13 weight decay values ( from 1e-6 to 1e-2 ) and 10 different initial learning rates ( from 1e-4 to 1e-1 ) . Therefore , 130 different data are plotted in each contour plot . Since our goal for hyper-parameter search is to ensure that we are not with completely wrong hyper-parameters , we believe that this amount of sampling grid is large enough for our experiment . The 5 circles represent top 5 test accuracy points and the red circle is for the best result . We updated the caption of Figure 6 as follows . \u201c Figure 6 : Training results of VGG-7 on CIFAR-10 dataset in the order of coupled ternary model , decoupled binary model and decoupled binary model trained from scratch ( left ) . 5 circles represent the top 5 test accuracy points and the red circle is for the best result . The best accuracy result is shown at the top left corner of each contour plot . Test accuracy of various models with different activation precision and training schemes ( right ) . \u201d"}, {"review_id": "r1x0lxrFPS-1", "review_text": "To be honest, I only read several papers on the neural network quantization. I am not familiar with this research topic, so I provide my judgement based on my own limited knowledge rather than thorough comparison with other related works. 1. The motivation is clear. The 1-bit activation networks usually deteriorates the performance greatly. 2. The gradient mismatch for discrete variable did bring difficult for optimization. Do you mean 1-bit activation has larger gradient mismatch than other bits, at least in the defined cosine similarity by this paper? 3. As to Eq(3), Appendix C.1 describes the way to choose step size. I understand the logic, but for the detailed method, is it cross-validation with grid search or some other tricks? 4. Is there any relation between the decoupling method in Section 5 and the proposed estimated gradient mismatch in Section 4.2?", "rating": "6: Weak Accept", "reply_text": "Thank you very much for your constructive comments . We could improve the quality of our work significantly by responding to your comments . Below , we summarize your questions and address them in order . Q1 : The motivation is clear . The 1-bit activation networks usually deteriorates the performance greatly . A1 : Thank you very much for the positive comment . Q2 : The gradient mismatch for discrete variable did bring difficult for optimization . Do you mean 1-bit activation has larger gradient mismatch than other bits , at least in the defined cosine similarity by this paper ? A2 : Yes.We think that 1-bit activation has larger gradient mismatch than other bit cases because the cosine similarity between the gradient estimate and the coarse gradient shows a large difference between ternary and 1-bit activation cases while similar cosine similarity values are exhibited for 1-bit activation cases with various STEs ( Fig.3 ) Q3 : As to Eq.3 , Appendix C.1 describes the way to choose step size . I understand the logic , but for the detailed method , is it cross-validation with grid search or some other tricks ? A3 : We measured the cosine similarities for various epsilon values ( epsilon = 1e-4 to 1e-2 ) and the results show that overall trend is maintained regardless of the epsilon value although the absolute values change depending on the epsilon value . Detailed experimental results with various epsilon values have been added in Appendix C.1 . Q4.Is there any relation between the decoupling method in Section 5 and the proposed estimated gradient mismatch in Section 4.2 ? A4 : The results from gradient mismatch analysis confirm that there is a large accuracy gap between ternary activation neural network and binary activation neural network with any STEs . The results gave us strong justification to apply ternary activation neural network to train binary activation neural network , which is a key idea in the decoupling method used in the proposed BinaryDuo scheme ."}, {"review_id": "r1x0lxrFPS-2", "review_text": "This paper studies activation quantization in deep networks. The authors first compare the coordinate discrete gradient and those obtained by various kinds of straight-through estimators, and found 1-bit activation networks have much poorer gradient estimation than 2-bit ones. Thus they speculate that this explains the poorer performance of 1-bit activation networks than 2-bit ones. To utilize higher precision of activation, the authors then propose to decouple a ternary activation into two binary ones, and achieve competitive results on typical image classification data sets CIFAR-10 and ImageNet. The paper is overall well-written and easy to follow. The decoupling method is simple and straightforward. The experiments are also well conducted. One main concern is that since the computation of the decoupled binary model and the coupled ternary model are the same, why does the decoupled binary model can finally to tuned to perform better than the original ternary model? Is there any intuition or theoretical explanation? Yet another concern is that ternary activation basically can be viewed as binary+sparse activations, can it be even more computationally cheaper than the decoupled binary activation? Question: 1. One line below eq (2), does STE mean the estimated gradient? How can the difference be calculated based on different things (i.e., activations and gradients)?", "rating": "6: Weak Accept", "reply_text": "Q3 : One line below eq ( 2 ) , does STE mean the estimated gradient ? How can the difference be calculated based on different things ( i.e. , activations and gradients ) ? A3 : After reading reviewer \u2019 s question , we noticed that there might be a confusion over the terminology \u2018 STE \u2019 . Therefore , we updated our manuscript to clarify our intention as follows . We use the term \u2018 STE \u2019 to indicate the derivative of the approximation of the binary activation function used at backward pass . For example , derivative of HardTanh function was used as STE in Courbariaux et al . ( 2016 ) .In Figure 2 , g \u2019 ( x ) is for STE . We call the presumed activation function which is used at backward pass as differentiable approximation of binary activation function . For example , HardTanh or SwishSign is one of the differentiable approximations of the binary activation function . In Figure 2 and Eq.2 , g ( x ) is for the differentiable approximation of the binary activation function . Therefore , STE ( g \u2019 ( x ) ) is the derivative of the differentiable approximation of binary activation function ( g ( x ) ) . The cumulative difference in Eq.2 was used to measure the difference between the actual binary activation function and its differentiable approximation . Yellow area in Fig.2 will help understanding Eq.2 graphically . We thank the reviewer for pointing this out and helping us to improve our manuscript for better understanding ."}], "0": {"review_id": "r1x0lxrFPS-0", "review_text": "This paper proposes a new measure of gradient mismatch for training binary networks, and additionally proposes a method for getting better performance out of binary networks by initializing them to behave like a ternary network. I found the new measure of gradient deviation fairly underdeveloped, and I suspect the method of converting ternary activations into binary activations works for a different reason than that proposed by the authors. There were English language issues that somewhat reduced clarity, though the intended meaning was always understandable. Detailed comments: \"Binary Neural Network (BNN) has been gaining interest thanks to its computing cost reduction and memory saving.\" --> \"Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings.\" (will stop making English language corrections from here on) \"Therefore, we argue that the sharp accuracy drop for the binary activation stems from the inefficient training method, not the capacity of the model.\" This could also be due to poor initialization in the binary case. e.g., it might make sense to initialize the binary network with bias=-0.5, so that the nonlinearity has a kink at pre-activation=0, rather than pre-activation=0.5. \"Unfortunately, it is not possible to measure the amount of gradient mismatch directly because the true gradient of a quantized activation function is zero almost everywhere. \" It *is* possible to measure the mismatch to the true gradient exactly. One could even train using the true gradient. It's just that the true gradient is useless. Fig 1b -- this is a nice baseline. \"the steepest descent direction, which is the direction toward the point with the smallest loss at given distance\" This is not the usual definition of steepest descent direction. If you're going to redefine this, should do so mathematically and precisely (for instance, you are going to run into trouble with the word \"distance\", since your coordinate discrete gradient more closely resembles an L\\infty-ball perturbation, rather than an L2-ball perturbation. eq. 3: Note that this equation is equivalent to taking the true gradient of a function which has been boxcar-smoothed along each parameter. This may more closely resemble existing measures of deviation than you like. You should also consider the relationship to an evolutionary strategies style gradient estimate, which similarly provides an unbiased gradient estimate for a smoothed function, and which allows that estimate to be computed with fewer samples (at the cost of higher error). Sec. 4.2 / Figure 3: The results in this section will be *highly* sensitive to the choice of epsilon. You should discuss this, specify the epsilon used, and experimentally explore the dependence on epsilon. \"The results indicate that the cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better than previous approaches. \" Don't know that I followed this. Gradient mismatch is never formally defined, so it's hard to know what this says about its relationship. Additionally, CDG sounds more like something which is correlated with, rather than an explanation for, performance. \" cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better \" --> \" cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better \" \"we shift the bias of BN layer which comes right before the activation function layer. \" Did you try using these bias values without pre-training as a ternary network? I suspect it would work just as well! \"Please note that BN layers followed by binary activation layer can be merged to the threshold of the binary activation layer, incurring no overhead at inference stage.\" Did not understand this. \"it is expected that the fine-tuning increases the accuracy even further\" Does it improve the accuracy further? Should state this as result, not prediction, and should have an ablation experiment showing this. \"Table 2 shows the validation accuracy of BNN in various schemes.\" Why not test accuracy? Figure 6: What are the filled circles? What was the sampling grid for the HP search? The images have high spatial frequency structure that I suspect is an artifact of the interpolation function, rather than in the data. ---- Update post-rebuttal: The authors have addressed the majority of my concerns, through both text changes and significant additional experiments. I am therefore increasing my score. Thank you for your hard work!", "rating": "6: Weak Accept", "reply_text": "Q11 : `` it is expected that the fine-tuning increases the accuracy even further '' Does it improve the accuracy further ? Should state this as result , not prediction , and should have an ablation experiment showing this . A11 : We agree to state this as result rather than prediction . Our experimental results ( Figure 6 in Section 6.2 ) indeed show that the fine-tuning procedure actually increases the accuracy further . Please note that decoupling the ternary model without the fine-tuning does not change computation results of the network . Therefore , the accuracy of the decoupled binary model is same as that of the coupled ternary model when fine-tuning is not applied . For example , in case of VGG-7 on CIFAR-10 dataset , the accuracy of decoupled binary model without fine-tuning is 89.69 % which is same as that of the coupled ternary model ( shown in Section 6.1 ) . In contrast , during the fine-tuning process , the weight for each of the decoupled binary network is tuned separately . The results for VGG7 on CIFAR-10 dataset shows that 0.75 % of accuracy improvement can be achieved by fine-tuning which results in 90.44 % accuracy ( shown in Section 6.1 again ) . The improvement was also observed on ImageNet dataset . Accuracy results before and after fine-tuning for AlexNet , ResNet-18 , and ResNet-18 ( +sc ) are shown below . ================================================================= | | Before fine-tuning | After fine-tuning | | Network | Top-1 ( % ) | Top-5 ( % ) | Top-1 ( % ) | Top-5 ( % ) | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - | AlexNet | 50.7 | 74.4 | 52.7 | 76.0 | | ResNet-18 | 58.8 | 81.3 | 60.4 | 82.3 | | ResNet-18 ( +sc ) | 59.1 | 81.3 | 60.9 | 82.6 | ================================================================= To reflect the update , we revised the sentence as follows . \u201c experimental results show that the fine-tuning increases the accuracy even further . Detailed experimental results will be discussed in the next section. \u201d Q12 : `` Table 2 shows the validation accuracy of BNN in various schemes . '' Why not test accuracy ? A12 : Table 2 shows the validation accuracy of BNN in various schemes on the ImageNet Large Scale Visual Recognition Challenge ( ILSVRC ) 2012 dataset . ImageNet dataset has 1.28M images for training set and 50K images for validation set . Because the test set is not available to the public , ( the test set is privately used for the ImageNet Large Scale Visual Recognition Challenge competition only ) , all the results of previous work in Table 2 are validation accuracy . For fair comparison with previous results , we also provided validation accuracy for our scheme . If required , it is possible to split the provided training set to train/valid sets and to use the provided validation set as a test set . However , the test accuracy from the experiment can not be fairly compared with other results . Nevertheless , if reviewer asks for the test accuracy , we are willing to conduct addition training and report the test accuracy . Please let us know . Q13 : Figure 6 : What are the filled circles ? What was the sampling grid for the HP search ? The images have high spatial frequency structure that I suspect is an artifact of the interpolation function , rather than in the data . A13 : We apologize for lack of detailed description of the figure . We mistakenly moved the information along with other details to the Appendix A while we made the original submission fit to 8-page limit . For the experiment on CIFAR-10 dataset , we tried 13 weight decay values ( from 1e-6 to 1e-2 ) and 10 different initial learning rates ( from 1e-4 to 1e-1 ) . Therefore , 130 different data are plotted in each contour plot . Since our goal for hyper-parameter search is to ensure that we are not with completely wrong hyper-parameters , we believe that this amount of sampling grid is large enough for our experiment . The 5 circles represent top 5 test accuracy points and the red circle is for the best result . We updated the caption of Figure 6 as follows . \u201c Figure 6 : Training results of VGG-7 on CIFAR-10 dataset in the order of coupled ternary model , decoupled binary model and decoupled binary model trained from scratch ( left ) . 5 circles represent the top 5 test accuracy points and the red circle is for the best result . The best accuracy result is shown at the top left corner of each contour plot . Test accuracy of various models with different activation precision and training schemes ( right ) . \u201d"}, "1": {"review_id": "r1x0lxrFPS-1", "review_text": "To be honest, I only read several papers on the neural network quantization. I am not familiar with this research topic, so I provide my judgement based on my own limited knowledge rather than thorough comparison with other related works. 1. The motivation is clear. The 1-bit activation networks usually deteriorates the performance greatly. 2. The gradient mismatch for discrete variable did bring difficult for optimization. Do you mean 1-bit activation has larger gradient mismatch than other bits, at least in the defined cosine similarity by this paper? 3. As to Eq(3), Appendix C.1 describes the way to choose step size. I understand the logic, but for the detailed method, is it cross-validation with grid search or some other tricks? 4. Is there any relation between the decoupling method in Section 5 and the proposed estimated gradient mismatch in Section 4.2?", "rating": "6: Weak Accept", "reply_text": "Thank you very much for your constructive comments . We could improve the quality of our work significantly by responding to your comments . Below , we summarize your questions and address them in order . Q1 : The motivation is clear . The 1-bit activation networks usually deteriorates the performance greatly . A1 : Thank you very much for the positive comment . Q2 : The gradient mismatch for discrete variable did bring difficult for optimization . Do you mean 1-bit activation has larger gradient mismatch than other bits , at least in the defined cosine similarity by this paper ? A2 : Yes.We think that 1-bit activation has larger gradient mismatch than other bit cases because the cosine similarity between the gradient estimate and the coarse gradient shows a large difference between ternary and 1-bit activation cases while similar cosine similarity values are exhibited for 1-bit activation cases with various STEs ( Fig.3 ) Q3 : As to Eq.3 , Appendix C.1 describes the way to choose step size . I understand the logic , but for the detailed method , is it cross-validation with grid search or some other tricks ? A3 : We measured the cosine similarities for various epsilon values ( epsilon = 1e-4 to 1e-2 ) and the results show that overall trend is maintained regardless of the epsilon value although the absolute values change depending on the epsilon value . Detailed experimental results with various epsilon values have been added in Appendix C.1 . Q4.Is there any relation between the decoupling method in Section 5 and the proposed estimated gradient mismatch in Section 4.2 ? A4 : The results from gradient mismatch analysis confirm that there is a large accuracy gap between ternary activation neural network and binary activation neural network with any STEs . The results gave us strong justification to apply ternary activation neural network to train binary activation neural network , which is a key idea in the decoupling method used in the proposed BinaryDuo scheme ."}, "2": {"review_id": "r1x0lxrFPS-2", "review_text": "This paper studies activation quantization in deep networks. The authors first compare the coordinate discrete gradient and those obtained by various kinds of straight-through estimators, and found 1-bit activation networks have much poorer gradient estimation than 2-bit ones. Thus they speculate that this explains the poorer performance of 1-bit activation networks than 2-bit ones. To utilize higher precision of activation, the authors then propose to decouple a ternary activation into two binary ones, and achieve competitive results on typical image classification data sets CIFAR-10 and ImageNet. The paper is overall well-written and easy to follow. The decoupling method is simple and straightforward. The experiments are also well conducted. One main concern is that since the computation of the decoupled binary model and the coupled ternary model are the same, why does the decoupled binary model can finally to tuned to perform better than the original ternary model? Is there any intuition or theoretical explanation? Yet another concern is that ternary activation basically can be viewed as binary+sparse activations, can it be even more computationally cheaper than the decoupled binary activation? Question: 1. One line below eq (2), does STE mean the estimated gradient? How can the difference be calculated based on different things (i.e., activations and gradients)?", "rating": "6: Weak Accept", "reply_text": "Q3 : One line below eq ( 2 ) , does STE mean the estimated gradient ? How can the difference be calculated based on different things ( i.e. , activations and gradients ) ? A3 : After reading reviewer \u2019 s question , we noticed that there might be a confusion over the terminology \u2018 STE \u2019 . Therefore , we updated our manuscript to clarify our intention as follows . We use the term \u2018 STE \u2019 to indicate the derivative of the approximation of the binary activation function used at backward pass . For example , derivative of HardTanh function was used as STE in Courbariaux et al . ( 2016 ) .In Figure 2 , g \u2019 ( x ) is for STE . We call the presumed activation function which is used at backward pass as differentiable approximation of binary activation function . For example , HardTanh or SwishSign is one of the differentiable approximations of the binary activation function . In Figure 2 and Eq.2 , g ( x ) is for the differentiable approximation of the binary activation function . Therefore , STE ( g \u2019 ( x ) ) is the derivative of the differentiable approximation of binary activation function ( g ( x ) ) . The cumulative difference in Eq.2 was used to measure the difference between the actual binary activation function and its differentiable approximation . Yellow area in Fig.2 will help understanding Eq.2 graphically . We thank the reviewer for pointing this out and helping us to improve our manuscript for better understanding ."}}