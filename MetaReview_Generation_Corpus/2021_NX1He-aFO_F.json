{"year": "2021", "forum": "NX1He-aFO_F", "title": "Learning Value Functions in Deep Policy Gradients using Residual Variance", "decision": "Accept (Poster)", "meta_review": "This paper is accepted, however, it could be much stronger by addressing the concerns below.\n\nThe theoretical analysis of the proposed methods is weak.\n* As far as I can tell, the proposition has more to do with the compatible feature assumption than their method. Furthermore the compatible feature assumption is very strong and not satisfied in any of their experiments.\n* Sec 4.2 does not provide strong support for their method. R2 points out issues with their statements about variance and the next subsection argues from an overly simplistic diagram.\n\nThe experimental results are promising, however, R3 brought up important issues in the private discussion:\n* Their implementation of SAC systematically produces results worse than reported in the original paper (they use a version of SAC with automatically tuned temperature https://arxiv.org/pdf/1812.05905.pdf); 1a) Their SAC gets average returns of 2.5k at 500k steps while the original implementation gets 3k at 500k steps; 1b) Their SAC on HalfCheetah 10k at 1M steps, original paper - 11k at 1M steps; 1c) The same applies to Humanoid, there is no improvement with respect to the original SAC;\n* Their approach degrades performance on Hopper. \n* They use non-standard hyper parameters for SAC. 0.98 instead of 0.99 for the discount and 0.01 instead of 0.005 for the soft target updates. That might be the main reason why their SAC works worse than the original implementation. \n* The authors use the hyper-parameters suggested for HalfCheetahBulletEnv for all continuous control tasks. For HalfCheetah, however, the authors of the stable-baselines repository (which this paper uses) suggest to use the hyper parameters from the original SAC paper (https://github.com/araffin/rl-baselines-zoo/blob/master/hyperparams/sac.yml#L48). Nonetheless, the results for the unmodified SAC reported in this work for HalfCheetah/Hopper/Walker/Ant are subpar to the original results, suggesting that the hyper-parameters for HalfCheetahBulletEnv are suboptimal for these tasks.\n\nGiven the simplicity of the change and the promising experimental results (with some caveats), I believe the community will find this paper interesting and will lead to followup work that can patch the theoretical gaps.", "reviews": [{"review_id": "NX1He-aFO_F-0", "review_text": "This paper presents AVEC , a new critic loss for model-free actor-critic Reinforcement Learning algorithms . The AVEC loss can be used with any actor-critic algorithm , with PPO , TRPO and SAC being evaluated in the paper . The loss builds on the mean-squared-error , and adds a term that minimizes $ E_s [ f_ { \\\\phi } ( s ) - \\\\hat { V } ^ { \\\\pi_ { \\\\theta_k } } ( s ) ] $ . The addition of that extra term is motivated by recent research on the stability of actor-critic algorithms , and the benefits obtained by the AVEC loss are empirically demonstrated in numerous environments , with AVEC+PPO , AVEC+SAC and AVEC+TRPO . Quality : the paper presents an interesting idea , that is simple but well-motivated , and leads to encouraging empirical results . Both the theoretical and empirical motivations are strong . Clarity : the paper flows well and is quite clear . However , an intuition for what the added term in the AVEC loss is missing . Section 4.2 motivates the added term in a mathematical way , but a few sentences explaining what the added term does , in simple terms , may help the readers understand why AVEC is a better loss than simple MSE . Originality : the contribution of this paper seems original . It builds on recent work , but the recent work identifies problems while this paper offers an original solution to these problems . Significance : the fact that AVEC provides good empirical results , and can be used as the critic loss of any actor-critic Reinforcement Learning algorithm , points at the high significance of this work . Many actor-critic implementations can easily be improved by using the AVEC loss . Another positive point is that the paper discusses how to implement the AVEC loss in algorithms that fit a neural network on batches of samples . This really helps implementing the proposed loss , that contains an expectation in an expectation and is therefore not trivial to properly implement . In general , I like this paper and recommend acceptance . A few questions/issues : - An explicit mention of the gradient of the loss , or at least a discussion of where to stop back-propagating gradients , would have been interesting . $ f_ { \\phi } $ appears two times in the AVEC loss , and it is unclear whether the loss contributes to gradients in $ f_ { \\phi } $ two times , or if the expectation over states is first computed ( without computing any gradients ) , and then used as a constant in the rest of the evaluation of the loss . - As mentioned in `` clarity '' , an intuition of what the added term of the AVEC loss does , especially since it is `` inserted '' in the mean-squared-error ( inside the square ) , would help the less mathematics-savvy readers . It is not crucial to understand the paper , but the generality of the approach proposed in the paper may lead it to be used often by students , and so an intuition of why AVEC works and what it does would greatly help . Author response : the authors clarified my questions , so I maintain my recommendation for acceptance .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We would like to thank the reviewer for the positive feedback and thoroughly written review . Concerning the issues raised by the reviewer : * When using AVEC , all the gradients in the total objective remain the same for the coupled method except the gradient of $ \\mathcal { L } _ { \\text { AVEC } } $ . No stop gradient is used for the value ( resp.state-value ) function loss : both gradients contribute to the total gradient . * Thank you for your comment regarding clarity , we now provide a general intuition at the end of Section 4.2 where we emphasize that this added term is used to focus more on the relative values than on absolute values ."}, {"review_id": "NX1He-aFO_F-1", "review_text": "The paper explores an alternative loss function for fitting critic in Reinforcement Learning . Instead of using the standard mean squared loss between critic predictions and value estimates , the authors propose to use a loss function that also incorporates a variance term . The authors dub the approach AVEC . The authors combine their approach with popular RL algorithms such as SAC and PPO and evaluated on the standard benchmarks for continuous control . Although the paper demonstrates interesting empirical results , I think that the current experimental evaluation has a number of flaws that prevent me from recommending this paper for acceptance . The paper provides basic motivation but it is lacking thorough theoretical investigation of the phenomena . Also the proposed loss is biased in the stochastic mini batch optimization due to the expectation under the squared term that is not addressed in the paper either . Finally , I have major concerns regarding the experimental evaluation . The set of OpenAI mujoco tasks is different from commonly used tasks in literature . In particular , Hopper and Walker2d , which are used in the vast majority of the literature , are ignored in table 1 and figure 2 . This fact raises major concerns regarding generality of the approach . In conclusion , the paper presents interesting results on some tasks for continuous control . However , the paper requires more thorough experimental evaluation to confirm the statements . Also a deeper theoretical analysis will greatly benefit this work . I strongly encourage the authors to continuous working this approach and revise the paper to improve the theoretical and empirical analysis . This paper presents a very interesting idea but in the current form it is not ready for acceptance .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their time and comments . Below we address the remarks of the reviewer . First , we consider it important to insist that the main contribution of this paper is to introduce a new loss to learn the critic ; we demonstrate the modification does not bias the gradient . Regarding the remark on the bias in the loss , we address this point at the end of Section 4.3 and in Appendix D : \u201c state-values for a non-optimal policy are dependent and the variance is not tractable without access to the joint law of state-values . Consequently , to implement AVEC in practice we use the best-known proxy at hand , which is the empirical variance formula assuming independence \u201c . Moreover , we emphasize that theoretical results in deep policy gradient research are merely a source of motivation as even the most basic assumptions ( parameterization assumption for example ) are unrealistic : Tucker et al . [ 1 ] suggest that gradients resulting from these assumptions are always biased in empirical tasks . Hence the focus of this paper is to provide theoretical insights for both cases : when the critic fits a state value function or a state-action value function . Could the reviewer expand on the additional results considered appropriate ? We will do our best to pursue such paths to provide further insights for our method . Nevertheless , we believe this would not belong to this work and would be more appropriate for future investigations . Concerning the experimental evaluation , we respectfully disagree with the remark of the reviewer . For instance , HalfCheetah and Humanoid are used at least as much commonly in literature and Humanoid is more challenging than Hopper or Walker2d ( much larger state and action space ) . In our experimental protocol we chose a representative set of tasks ranging from moderate ( Reacher : $ \\mathbb { R } ^ { 11 } \\times \\mathbb { R } ^ { 2 } $ ) to very large ( Humanoid : $ \\mathbb { R } ^ { 376 } \\times \\mathbb { R } ^ { 17 } $ ) state and action spaces . Although we are confused by this comment as the reviewer stated earlier that we used \u201c standard benchmarks for continuous control \u201d . Moreover , note that we have already included in Appendix B.2 variance reduction graphs using the ( open-source ) PyBullet versions of Hopper and Walker2d . Nevertheless , because we sincerely want to address any concern that may come from the reviewer , we include in Table 1 and Appendix B.1 of the revised version of the paper the scores and performance graphs for Walker2d , which compared to Hopper , is more challenging and has a larger state action space ( $ \\mathbb { R } ^ { 17 } \\times \\mathbb { R } ^ { 6 } $ vs. $ \\mathbb { R } ^ { 11 } \\times \\mathbb { R } ^ { 3 } $ ) . The results show that using AVEC produces comparable gains in performance : 26 % for SAC and 33 % for PPO . [ 1 ] Tucker , George , Surya Bhupatiraju , Shixiang Gu , Richard Turner , Zoubin Ghahramani , and Sergey Levine . `` The Mirage of Action-Dependent Baselines in Reinforcement Learning . '' In International Conference on Machine Learning , pp . 5015-5024 . 2018 ."}, {"review_id": "NX1He-aFO_F-2", "review_text": "# # # Strengths The paper proposes a simple and elegant idea for changing the value function objectives in deep RL and demonstrates reasonable empirical evidence of it 's potential usefulness . The authors also provide a clearly articulated intuitive motivation and provide experiments to support the proposal . The idea complements several other algorithms and is therefore quite widely applicable ( and easy to try ) . The analysis of the experiments is also quite interesting and clearly presented . # # # Weaknesses The paper is mostly well written and has interesting theoretical insights as well as empirical analysis . Here are a some weaknesses . * The theoretical justification for the variance reduction while technically correct , seems like it should be miniscule in theory . For the $ T $ independent RV case being analyzed , the condition required for the improvement is that $ \\Delta \\triangleq 2 \\mathbb { V } ( X_i ) - \\frac { 1 } { T } \\sum_ { j=1 } ^T \\mathbb { V } ( X_j ) > 0 $ , which seems reasonable unless the sample in question is an outlier with a very small variance to begin with . However , the overall reduction itself has another $ \\frac { 1 } { T } $ scaling , i.e.the variance reduction over the squared error case is equal to $ \\frac { \\Delta } { T } $ , which seems to be vanishingly small as the number of samples $ T $ is large even if $ \\Delta \\gg 0 $ . Note that for the situation where this core idea is being applied , the parameter $ T $ is approximately , the number of samples in the expectation over $ ( s , a ) $ , which is large in practice . * The improvements are a good sanity check , but somewhat marginal in many cases ( especially given the error bars ) . # # # Additional comments/feedback * In Section 4.2 paragraph on State-value function estimation line 3 , should the targets be $ \\widehat { V } ^\\pi $ rather than $ V^\\pi $ ? * In Figure 1 , some additional detail on the claims seems necessary ( e.g.what parameterization is being considered ? ) * In the discussion below the specification for $ \\mathcal { L } ^1_ { AVEC } , \\mathcal { L } ^2_ { AVEC } $ , the authors say `` the reader may have noticed that these equations slightly differ from Eq.3 '' , but I am not able to see what difference is being alluded to . * Figure 4 looks quite surprising in terms of the large qualitative difference between the baseline and AVEC-baseline graphs . Just to be sure , do you measure the fit with respect to $ f_\\phi $ or the bias corrected version , $ g_\\phi $ ? ( obviously , the latter makes more sense ? ) . * The Ablation study in Section 5.4 seems intriguing , but what the conclusions imply seems unclear . It appears the authors were expecting to see some non-zero value of $ \\alpha $ to improve over $ \\alpha=0 $ ( AVEC ) , but this is n't the case ? Some additional clarification here would be useful . Also , it is a bit confusing to separate the plots into two depending on whether the weighting is less than one ; as I 'm guessing the exact same plot is used for the non-alpha versions in each pair of these graphs ? * In Figure 5 , the distance to the true value function seems to be relatively flat ( or even mildly increasing ) through the entire horizon in both graphs . Is this simply due to the resolution , as I 'd expect there to be a drop at least in the initial phase over time .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their time , positive feedback , and insightful comments . We agree that the variance reduction for a given state value ( resp.state-action value ) scales with $ \\frac { 1 } { T } $ . However , we note that to compare AVEC to the squared error case accurately , one should account for the sum of these reductions over visited states in a trajectory , which is equal to $ \\frac { 2T-1 } { T } \\sum_ { j=1 } ^T \\mathbb { V } ( X_j ) $ and does not scale with $ \\frac { 1 } { T } $ . Furthermore , we emphasize that $ T $ is not very large , it represents the size of the trajectories used to approximate the gradient , for example in our experiments it is equal to $ 2048 $ . Concerning the comment of the reviewer on the improvements of our method , we can not agree with them being marginal : AVEC brings an improvement over the baseline of on average +26 % for SAC and +39 % for PPO . Moreover , from Table 1 , we find that the coefficients of variation ( std/mean ) are on average 11 % for SAC and 9.5 % for PPO . Consequently , we believe that empirical improvement conclusions are reasonable . Thank you for the many additional comments , to which we reply below : * It was a typo , we fixed it . * The problem depicted in Fig.1 is a simple example of regression with one variable . We clarify this in Section 4.2 . * What we would like to highlight in this part of the paper is that the inner empirical expectation is empirically-biased and that it is not possible to propose a bias-corrected version without further restrictive assumptions on the joint law of state-values . * We do confirm to the reviewer that Fig.4 is the L2 distance with respect to the bias-corrected version $ g_\\phi $ . Indeed it is large , which might be surprising at first sight but is not inconsistent with Fig.5 which shows that PPO \u2019 s value estimator is farther from the true target than AVEC-PPO \u2019 s from the empirical target . * In the ablation study , we question whether there exists a value of $ \\alpha $ that yields better performance than $ \\alpha=0 $ , empirically we find that this is not the case which is why we consider $ \\mathcal { L } _ { \\text { AVEC } } $ . This is also favorable as it suggests that introducing a weighting with the need to be tuned would not be beneficial . Regarding the separation of the plots , indeed for the same task , AVEC-PPO and PPO are the same curves when the weighting is less than one or not . This was done for readability purposes only . We clarify this in Section 5.4 . * The flatness of the curves is simply a resolution matter : we decided to plot 5 values ( $ t \\in \\ { 1,2,4,6,9\\ } .10^5 $ ) corresponding to the order of magnitude chosen in Ilyas et al [ 1 ] . mainly because it is computationally demanding and because it suffices in order to compare AVEC to the base algorithm . [ 1 ] Ilyas , Andrew , Logan Engstrom , Shibani Santurkar , Dimitris Tsipras , Firdaus Janoos , Larry Rudolph , and Aleksander Madry . `` A Closer Look at Deep Policy Gradients . '' In International Conference on Learning Representations . 2019 ."}], "0": {"review_id": "NX1He-aFO_F-0", "review_text": "This paper presents AVEC , a new critic loss for model-free actor-critic Reinforcement Learning algorithms . The AVEC loss can be used with any actor-critic algorithm , with PPO , TRPO and SAC being evaluated in the paper . The loss builds on the mean-squared-error , and adds a term that minimizes $ E_s [ f_ { \\\\phi } ( s ) - \\\\hat { V } ^ { \\\\pi_ { \\\\theta_k } } ( s ) ] $ . The addition of that extra term is motivated by recent research on the stability of actor-critic algorithms , and the benefits obtained by the AVEC loss are empirically demonstrated in numerous environments , with AVEC+PPO , AVEC+SAC and AVEC+TRPO . Quality : the paper presents an interesting idea , that is simple but well-motivated , and leads to encouraging empirical results . Both the theoretical and empirical motivations are strong . Clarity : the paper flows well and is quite clear . However , an intuition for what the added term in the AVEC loss is missing . Section 4.2 motivates the added term in a mathematical way , but a few sentences explaining what the added term does , in simple terms , may help the readers understand why AVEC is a better loss than simple MSE . Originality : the contribution of this paper seems original . It builds on recent work , but the recent work identifies problems while this paper offers an original solution to these problems . Significance : the fact that AVEC provides good empirical results , and can be used as the critic loss of any actor-critic Reinforcement Learning algorithm , points at the high significance of this work . Many actor-critic implementations can easily be improved by using the AVEC loss . Another positive point is that the paper discusses how to implement the AVEC loss in algorithms that fit a neural network on batches of samples . This really helps implementing the proposed loss , that contains an expectation in an expectation and is therefore not trivial to properly implement . In general , I like this paper and recommend acceptance . A few questions/issues : - An explicit mention of the gradient of the loss , or at least a discussion of where to stop back-propagating gradients , would have been interesting . $ f_ { \\phi } $ appears two times in the AVEC loss , and it is unclear whether the loss contributes to gradients in $ f_ { \\phi } $ two times , or if the expectation over states is first computed ( without computing any gradients ) , and then used as a constant in the rest of the evaluation of the loss . - As mentioned in `` clarity '' , an intuition of what the added term of the AVEC loss does , especially since it is `` inserted '' in the mean-squared-error ( inside the square ) , would help the less mathematics-savvy readers . It is not crucial to understand the paper , but the generality of the approach proposed in the paper may lead it to be used often by students , and so an intuition of why AVEC works and what it does would greatly help . Author response : the authors clarified my questions , so I maintain my recommendation for acceptance .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We would like to thank the reviewer for the positive feedback and thoroughly written review . Concerning the issues raised by the reviewer : * When using AVEC , all the gradients in the total objective remain the same for the coupled method except the gradient of $ \\mathcal { L } _ { \\text { AVEC } } $ . No stop gradient is used for the value ( resp.state-value ) function loss : both gradients contribute to the total gradient . * Thank you for your comment regarding clarity , we now provide a general intuition at the end of Section 4.2 where we emphasize that this added term is used to focus more on the relative values than on absolute values ."}, "1": {"review_id": "NX1He-aFO_F-1", "review_text": "The paper explores an alternative loss function for fitting critic in Reinforcement Learning . Instead of using the standard mean squared loss between critic predictions and value estimates , the authors propose to use a loss function that also incorporates a variance term . The authors dub the approach AVEC . The authors combine their approach with popular RL algorithms such as SAC and PPO and evaluated on the standard benchmarks for continuous control . Although the paper demonstrates interesting empirical results , I think that the current experimental evaluation has a number of flaws that prevent me from recommending this paper for acceptance . The paper provides basic motivation but it is lacking thorough theoretical investigation of the phenomena . Also the proposed loss is biased in the stochastic mini batch optimization due to the expectation under the squared term that is not addressed in the paper either . Finally , I have major concerns regarding the experimental evaluation . The set of OpenAI mujoco tasks is different from commonly used tasks in literature . In particular , Hopper and Walker2d , which are used in the vast majority of the literature , are ignored in table 1 and figure 2 . This fact raises major concerns regarding generality of the approach . In conclusion , the paper presents interesting results on some tasks for continuous control . However , the paper requires more thorough experimental evaluation to confirm the statements . Also a deeper theoretical analysis will greatly benefit this work . I strongly encourage the authors to continuous working this approach and revise the paper to improve the theoretical and empirical analysis . This paper presents a very interesting idea but in the current form it is not ready for acceptance .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their time and comments . Below we address the remarks of the reviewer . First , we consider it important to insist that the main contribution of this paper is to introduce a new loss to learn the critic ; we demonstrate the modification does not bias the gradient . Regarding the remark on the bias in the loss , we address this point at the end of Section 4.3 and in Appendix D : \u201c state-values for a non-optimal policy are dependent and the variance is not tractable without access to the joint law of state-values . Consequently , to implement AVEC in practice we use the best-known proxy at hand , which is the empirical variance formula assuming independence \u201c . Moreover , we emphasize that theoretical results in deep policy gradient research are merely a source of motivation as even the most basic assumptions ( parameterization assumption for example ) are unrealistic : Tucker et al . [ 1 ] suggest that gradients resulting from these assumptions are always biased in empirical tasks . Hence the focus of this paper is to provide theoretical insights for both cases : when the critic fits a state value function or a state-action value function . Could the reviewer expand on the additional results considered appropriate ? We will do our best to pursue such paths to provide further insights for our method . Nevertheless , we believe this would not belong to this work and would be more appropriate for future investigations . Concerning the experimental evaluation , we respectfully disagree with the remark of the reviewer . For instance , HalfCheetah and Humanoid are used at least as much commonly in literature and Humanoid is more challenging than Hopper or Walker2d ( much larger state and action space ) . In our experimental protocol we chose a representative set of tasks ranging from moderate ( Reacher : $ \\mathbb { R } ^ { 11 } \\times \\mathbb { R } ^ { 2 } $ ) to very large ( Humanoid : $ \\mathbb { R } ^ { 376 } \\times \\mathbb { R } ^ { 17 } $ ) state and action spaces . Although we are confused by this comment as the reviewer stated earlier that we used \u201c standard benchmarks for continuous control \u201d . Moreover , note that we have already included in Appendix B.2 variance reduction graphs using the ( open-source ) PyBullet versions of Hopper and Walker2d . Nevertheless , because we sincerely want to address any concern that may come from the reviewer , we include in Table 1 and Appendix B.1 of the revised version of the paper the scores and performance graphs for Walker2d , which compared to Hopper , is more challenging and has a larger state action space ( $ \\mathbb { R } ^ { 17 } \\times \\mathbb { R } ^ { 6 } $ vs. $ \\mathbb { R } ^ { 11 } \\times \\mathbb { R } ^ { 3 } $ ) . The results show that using AVEC produces comparable gains in performance : 26 % for SAC and 33 % for PPO . [ 1 ] Tucker , George , Surya Bhupatiraju , Shixiang Gu , Richard Turner , Zoubin Ghahramani , and Sergey Levine . `` The Mirage of Action-Dependent Baselines in Reinforcement Learning . '' In International Conference on Machine Learning , pp . 5015-5024 . 2018 ."}, "2": {"review_id": "NX1He-aFO_F-2", "review_text": "# # # Strengths The paper proposes a simple and elegant idea for changing the value function objectives in deep RL and demonstrates reasonable empirical evidence of it 's potential usefulness . The authors also provide a clearly articulated intuitive motivation and provide experiments to support the proposal . The idea complements several other algorithms and is therefore quite widely applicable ( and easy to try ) . The analysis of the experiments is also quite interesting and clearly presented . # # # Weaknesses The paper is mostly well written and has interesting theoretical insights as well as empirical analysis . Here are a some weaknesses . * The theoretical justification for the variance reduction while technically correct , seems like it should be miniscule in theory . For the $ T $ independent RV case being analyzed , the condition required for the improvement is that $ \\Delta \\triangleq 2 \\mathbb { V } ( X_i ) - \\frac { 1 } { T } \\sum_ { j=1 } ^T \\mathbb { V } ( X_j ) > 0 $ , which seems reasonable unless the sample in question is an outlier with a very small variance to begin with . However , the overall reduction itself has another $ \\frac { 1 } { T } $ scaling , i.e.the variance reduction over the squared error case is equal to $ \\frac { \\Delta } { T } $ , which seems to be vanishingly small as the number of samples $ T $ is large even if $ \\Delta \\gg 0 $ . Note that for the situation where this core idea is being applied , the parameter $ T $ is approximately , the number of samples in the expectation over $ ( s , a ) $ , which is large in practice . * The improvements are a good sanity check , but somewhat marginal in many cases ( especially given the error bars ) . # # # Additional comments/feedback * In Section 4.2 paragraph on State-value function estimation line 3 , should the targets be $ \\widehat { V } ^\\pi $ rather than $ V^\\pi $ ? * In Figure 1 , some additional detail on the claims seems necessary ( e.g.what parameterization is being considered ? ) * In the discussion below the specification for $ \\mathcal { L } ^1_ { AVEC } , \\mathcal { L } ^2_ { AVEC } $ , the authors say `` the reader may have noticed that these equations slightly differ from Eq.3 '' , but I am not able to see what difference is being alluded to . * Figure 4 looks quite surprising in terms of the large qualitative difference between the baseline and AVEC-baseline graphs . Just to be sure , do you measure the fit with respect to $ f_\\phi $ or the bias corrected version , $ g_\\phi $ ? ( obviously , the latter makes more sense ? ) . * The Ablation study in Section 5.4 seems intriguing , but what the conclusions imply seems unclear . It appears the authors were expecting to see some non-zero value of $ \\alpha $ to improve over $ \\alpha=0 $ ( AVEC ) , but this is n't the case ? Some additional clarification here would be useful . Also , it is a bit confusing to separate the plots into two depending on whether the weighting is less than one ; as I 'm guessing the exact same plot is used for the non-alpha versions in each pair of these graphs ? * In Figure 5 , the distance to the true value function seems to be relatively flat ( or even mildly increasing ) through the entire horizon in both graphs . Is this simply due to the resolution , as I 'd expect there to be a drop at least in the initial phase over time .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their time , positive feedback , and insightful comments . We agree that the variance reduction for a given state value ( resp.state-action value ) scales with $ \\frac { 1 } { T } $ . However , we note that to compare AVEC to the squared error case accurately , one should account for the sum of these reductions over visited states in a trajectory , which is equal to $ \\frac { 2T-1 } { T } \\sum_ { j=1 } ^T \\mathbb { V } ( X_j ) $ and does not scale with $ \\frac { 1 } { T } $ . Furthermore , we emphasize that $ T $ is not very large , it represents the size of the trajectories used to approximate the gradient , for example in our experiments it is equal to $ 2048 $ . Concerning the comment of the reviewer on the improvements of our method , we can not agree with them being marginal : AVEC brings an improvement over the baseline of on average +26 % for SAC and +39 % for PPO . Moreover , from Table 1 , we find that the coefficients of variation ( std/mean ) are on average 11 % for SAC and 9.5 % for PPO . Consequently , we believe that empirical improvement conclusions are reasonable . Thank you for the many additional comments , to which we reply below : * It was a typo , we fixed it . * The problem depicted in Fig.1 is a simple example of regression with one variable . We clarify this in Section 4.2 . * What we would like to highlight in this part of the paper is that the inner empirical expectation is empirically-biased and that it is not possible to propose a bias-corrected version without further restrictive assumptions on the joint law of state-values . * We do confirm to the reviewer that Fig.4 is the L2 distance with respect to the bias-corrected version $ g_\\phi $ . Indeed it is large , which might be surprising at first sight but is not inconsistent with Fig.5 which shows that PPO \u2019 s value estimator is farther from the true target than AVEC-PPO \u2019 s from the empirical target . * In the ablation study , we question whether there exists a value of $ \\alpha $ that yields better performance than $ \\alpha=0 $ , empirically we find that this is not the case which is why we consider $ \\mathcal { L } _ { \\text { AVEC } } $ . This is also favorable as it suggests that introducing a weighting with the need to be tuned would not be beneficial . Regarding the separation of the plots , indeed for the same task , AVEC-PPO and PPO are the same curves when the weighting is less than one or not . This was done for readability purposes only . We clarify this in Section 5.4 . * The flatness of the curves is simply a resolution matter : we decided to plot 5 values ( $ t \\in \\ { 1,2,4,6,9\\ } .10^5 $ ) corresponding to the order of magnitude chosen in Ilyas et al [ 1 ] . mainly because it is computationally demanding and because it suffices in order to compare AVEC to the base algorithm . [ 1 ] Ilyas , Andrew , Logan Engstrom , Shibani Santurkar , Dimitris Tsipras , Firdaus Janoos , Larry Rudolph , and Aleksander Madry . `` A Closer Look at Deep Policy Gradients . '' In International Conference on Learning Representations . 2019 ."}}