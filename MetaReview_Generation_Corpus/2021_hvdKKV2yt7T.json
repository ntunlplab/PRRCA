{"year": "2021", "forum": "hvdKKV2yt7T", "title": "Dataset Inference: Ownership Resolution in Machine Learning", "decision": "Accept (Spotlight)", "meta_review": "This paper proposed to defend against model stealing attacks by dataset inference. The paper received unanimous rating of \"Good paper\" and \"accept\". The reviewers praise this paper insightful and well written. There are active discussion between the reviewers and authors, which further clarify some of the issues. Given the positive review and overall rating, the AC recommends it to be an spotlight paper.", "reviews": [{"review_id": "hvdKKV2yt7T-0", "review_text": "* * Update * * : Since nearly all of my issues have been addressed , I have changed my rating from 5 to 7 . Best of luck : ) Summary : The authors study a subset of model stealing by introducing dataset inference , a process to identify whether a suspected adversary 's model has private knowledge from the original model 's dataset . Through their experiments on two related tasks in the vision domain ( CIFAR-10,100 ) , the authors demonstrate their approach for dataset inference that combines statistical tests and decision boundary-based checks to make claims of model theft . They base their algorithms on theoretical results in toy settings ( proofs shifted to additional materials ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I feel the final observations are quite useful and pave new methods for asserting model theft in real-world settings . Although the proposed techniques are not novel ( in fact , a lot of existing literature is not cited , giving the impression that a lot of proposed techniques are contributions of the paper itself ) , the end results and observations can be useful in practical settings . I feel the authors need to make a lot of changes to the current draft before it can be ready for acceptance . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : - Mentioning points such as how issues like these can actually get to courts in real life and are covered by intellectual theft laws is great to see . Since the proposed solution tackles a real-world ( potentially ) problem , it makes sense to incorporate real-world implications as well . - Using theoretical analyses to justify the proposed methods is a great way to approach a problem like this . Testing them out on simpler problems to see how and most importantly , why they work , is important . - Evaluation over multiple model-stealing attacks is a great way to both show the robustness of the proposed attack method , as well as identify strong ( fine-tuned models ) as well as weak ( label-query attacks ) points of the algorithm . - The query efficiency of the proposed method is quite remarkable . A method that can use these little samples in a real-world setting to assert model stealing ( or check for it ) with high confidence can be very helpful . - Page 8 : 'gradient-based approaches are sensitive to numerical instabilities ' is a great and very valid point . An adversary could in fact even deliberately resort to gradient masking to avoid such an attack . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : - By biggest and foremost problem is with the threat model . In all of these settings , it is assumed that the party accusing the adversary of stealing dataset access provides actual data from its own training dataset . However , since this is a real-world scenario , there should also be some way for the accuser to provably show that the data points it is using for the test are in fact from its own training set . Also , it should be made clear what level of overlap in the datasets used is problematic . This parameter would vary from problem to problem and thus can not be one fixed value . Nonetheless , it should be a parameter of the testing process . - The term 'dataset inference ' is slightly misleading . In some portions , the authors say `` we propose to identify stolen copies by showing that they were trained on the same dataset '' . In other places , they imply that the adversary used a `` subset '' of that knowledge . It is important to both be consistent in this definition , and to quantify it : nowhere in the paper is it mentioned how much overlap is acceptable as `` sheer chance/possible via domain knowledge '' , and how much is `` definitely stealing '' . - Key implementation details , such as how the datasets were split to be used by the target and adversary to train their models are omitted . - CIFAR-10 and CIFAR-100 are highly related datasets ( same images ) . Claims made in the paper would hold more value if they can show similar trends on some other datasets . The authors could try some high-dimensional datasets such as tiny-Imagenet , or even something as simple as MNIST . - In several places in the paper , authors claim that watermarking models does not work in the sense that trying to copy a model does not copy the watermarks , since there is a difference in distributions . This is not true : recent works have shown honeypot attacks that transfer watermark behavior even when someone tries to copy a model using normal data . Please refer to Section 3.2.2 of [ this work ] ( https : //arxiv.org/pdf/2009.12153.pdf ) - When using sample points to assert model stealing , does the party arguing for their case release them all together , or one by one . One may argue that the sequential release of sample points could potentially be games ( a malicious party could easily look at the target model and accuse them wrongfully ) . What are the countermeasures against such a scenario ? It seems the final judgment can only be made by a third party by the other two parties revealing large subsets of their training sets and algorithms . This might not be possible in real-world scenarios , where datasets used to train models are bound by certain privacy-protection laws ( GDPR , etc ) . - Section 2 `` V suspects theft '' : what are the criteria here ? Is it based on similarity in model performance , or pure suspicion ? Since this section sets up the threat model , it might be beneficial to be as exact as possible . ( a ) `` may gain access to a subset of '' . As asked above , how big of a subset is a problem ? Since the subset is of Kv and not Sv , it may be possible that they are not overlapping at all , or barely overlap . Would it still be a problem ? ( b ) Definition 1 : `` prove that some knowledge '' since this is a definition , it should not be hand-wavy . In my opinion , the `` some '' here should be quantified perhaps by a parameter that captures the overlap in the two datasets and integrates them in the definition . ( c ) Definition 2 : Please rewrite the last point as a conditional assignment . It is hard to read in its current form - Section 3.3 , 'Failure if Membership Inference ' refers to two labels : b=1 when the sample is from S , or b=0 when the sample is from D. However , S is a subset of D and thus the first case will always be true . Assuming you mean 'sample is from S - D ' , please fix this . - Insufficient literature review . There are many works in the field that deal with the same problem and similar approaches , even though they do not define them as `` dataset inference '' . Please search for some works on 'property inference ' : the current problem can be posed as differentiating between the dataset suspected to be used , versus some other dataset . - Although the white-box method looks at finding the 'closest ' data points , the black-box method samples random directions and takes steps . Assuming this is intended to be a proxy of the white-box setting , it should take the form of sampling random initial directions and then , over all the samples , picking the closest directions . Also , the white-box setting may give significantly different results if either of the models were trained with an adversarial robustness objective . It would be interesting to see how the attack performs in that case . - Section 4.2 claims that membership inference attacks do not take confidence into account and that this is a key difference from the proposed method in this work . This is not true : ROC curves for such attacks can be analyzed to set rejection regions for such claims . - Section 4.2 , last line : `` publicly available data that is not used for training of Fv '' Is there a specific reason to use data that was not used to train Fv ? - Page 8 , 'White-box access is not essential to DI ' claims the Blind-Walk approach is 'non-targeted ' . This can also be made true for the white-box setting by adjusting the minimization objective to be non-targeted , instead of making it targeted and targeting the top-k classes . Please address and clarify the cons above", "rating": "7: Good paper, accept", "reply_text": "> * * * [ 11 ] Section 4.2 claims that membership inference attacks do not take confidence into account and that this is a key difference from the proposed method in this work . This is not true : ROC curves for such attacks can be analyzed to set rejection regions for such claims . * * * Dataset Inference utilizes statistical confidence measures . Such confidence measures naturally take into account the likelihood of failure because they are evaluated by sampling multiple data points from given distributions . On the contrary , MI works on pre-set rejection thresholds defined using ROC curves . The former can give us the likelihood of failure of dataset inference , whereas the latter is only as representative as the examples tested on for creating the ROC curve . In a new distribution , the generalization of the ROC curve based threshold may or may not hold , but since DI is able to draw multiple samples from the true distribution , we can set statistical confidence regions . > * * * [ 12 ] Section 4.2 , last line : \u201c publicly available data that is not used for training of Fv \u201d Is there a specific reason to use data that was not used to train Fv ? * * * The alternate dataset should not have been seen by Fv at training time . Otherwise , the ` prediction margin \u2019 for these data points would have been altered during optimization . The confidence regressor would hence not be able to distinguish between the two types of data points ( private training and public training ) , since they were both optimized on . That said , the reviewer is correct that the alternate dataset need not be publicly available . It could for instance be the private test set of the victim . > * * * [ 13 ] Page 8 , \u2018 White-box access is not essential to DI \u2019 claims the Blind-Walk approach is \u2018 non-targeted \u2019 . This can also be made true for the white-box setting by adjusting the minimization objective to be non-targeted , instead of making it targeted and targeting the top-k classes . * * * The point we wanted to highlight here is that the blind-walk attack is not ranking possible classes based on their distance to the current point ( which would be the case even if we used an untargeted minimization objective in the white-box approach ) but rather measures the average distance to randomly picked target classes . This follows from our discussion over comment [ 10 ] . We have edited the text to : * \u201c ( b ) the approach is stochastic and aims to find the expected prediction margin rather than the worst-case ( it searches for any incorrect neighboring class in a randomly chosen direction rather than focusing on the distance to possible target classes ) \u201d *"}, {"review_id": "hvdKKV2yt7T-1", "review_text": "This paper tackles a timely problem of detecting model stealing attacks . The proposed identifies the stolen model by investigating whether the model is trained on the same dataset as the victim model . Pros : - The proposed method of dataset inference for model stealing detection is novel . - The experiments provided by the paper are comprehensive , including different assumptions of data access , model access , and query access , etc . Adaptive attacks are considered in the paper as well . - The paper is well-written and easy to follow . Cons : - Zero-shot learning ( [ 9 ] in the paper ) can not be directly used as a model stealing attack , because the proposed approach in [ 9 ] requires more information of the victim model than the input and output pair . It would be better if the paper could investigate other data-free model stealing approaches ( e.g. , [ 1 ] [ 2 ] ) . Also , it is not clear to me how the proposed dataset inference approach is applied to data-free model stealing attacks , since the training dataset used by the attacker is different from the victim \u2019 s dataset . If the attackers use some synthetic data to steal the model , will the proposed approach work ? - What are the white-box and black-box settings in Section 4.1 ? [ 1 ] Mika Juuti , Sebastian Szyller , Samuel Marchal , and N Asokan . Prada : protecting against dnn model stealing attacks . In 2019 IEEE European Symposium on Security and Privacy ( EuroS & P ) , pp . 512\u2013527.IEEE , 2019 . [ 2 ] Nicolas Papernot , Patrick McDaniel , Ian Goodfellow , Somesh Jha , Z Berkay Celik , and Ananthram Swami . Practical black-box attacks against machine learning . In Proceedings of the 2017 ACM on Asia conference on computer and communications security , pp . 506\u2013519 , 2017 .", "rating": "7: Good paper, accept", "reply_text": "> * * * Zero-shot learning ( [ 9 ] in the paper ) can not be directly used as a model stealing attack , because the proposed approach in [ 9 ] requires more information of the victim model than the input and output pair * * * As detailed in the introduction ( paragraph 2 , point * * ( 3 ) * * ) and Section 5.2 ( point ( 2 ) ) on the type of threat models , our work on dataset inference extends beyond the realms of model extraction alone . We also consider the following two scenarios : - complete data theft ( i.e. , the training dataset itself is leaked to the adversary ) - model availability : in the age of open-sourced code and pre-trained models , the threat of commercialization of an ML model , otherwise intended only for academic purposes , is practical and prevalent . For instance , consider the scenarios where a company ( such as OpenAI ) may want to open-source their model ( like GPT-3 ) for academic purposes but does not permit its commercialization . In both situations , an adversary has sufficient information for staging zero-shot learning attacks on the victim model . An additional advantage of considering zero-shot learning is that it allows us to strengthen our evaluation with an analysis of dataset inference against adaptive adversaries : the queries used in zero-shot learning negate any resemblance to real-world queries since they only utilize synthetic inputs . > * * * What are the white-box and black-box settings in Section 4.1 ? * * * The white-box and black-box settings indicate access levels that the victim has to the potential adversary \u2019 s model . For instance , if the victim suspects that a competitor \u2019 s API has stolen their ML model , they can only gain black-box access to the predictions of their MLaaS . However , if the victim files a legal complaint , then in the presence of a neutral arbitrator , the victim may be presented white-box access to demonstrate \u2018 dataset inference \u2019 on the suspect adversary . Section 4.1 introduces an embedding generation procedure for each of the two settings . Thank you for your time . We would be happy to answer any further queries ."}, {"review_id": "hvdKKV2yt7T-2", "review_text": "The submission proposes to defend against model stealing attacks by dataset inference . It is argued that other watermarking techniques are not robust enough to be transferred during model stealing . Ownership resolution is performed by kind of a membership inference attack that the model owner runs on the potential copy . As only the owner knows the true training data , the `` attack '' can be amplified by a few samples . This leads to a rather strong signal in order to show model provenance . Final decision is formulated as a hypothesis test in order to get a confidence . The argumentation why for larger models this is not dependent on some sort of over fitting is not clear . While the idea is interesting , it is somewhat at odds with others goals in machine learning . In general , it is not a desirable property if the trained model contains `` artifacts '' related to the training data . This on the one hand could be overfitting artifacts that hamper generalization or more prominently could mean that the model does not preserve privacy . If such overfitting is not a major issues and privacy is not a concern , the approach seems viable . However , there should be an extended discussion under which situations/models the method is feasible . ( e.g.privacy preserving learning ? ) I somewhat disagree with the practice of moving the related work to the appendix . this is a vital part of a paper and not an appendix . this looks like breaking the page limitation . overall , having a 19 page submission is not quite following the submission guideline ( although the additional material is useful ) . this should not become practice - in particular as some parts in the main submission could be more concise . In particular , as the actual algorithm is quite simple ( which I do n't want to see as a disadvantage as it 's well motivated ) Overall , the paper is insightful -- also adding to the understanding of the connections to membership inference . Also the attention to important details like query efficiency is taken care of . It is bit disappointing that the method is not evaluated on more complex classification problems . In particular , as membership inference ( which this method is based on ) can be even weaker on those larger and well trained models . The claim about other watermarking techniques no being effective - although probably true - was not substantiated ( unless I 've missed ) . at least there is no comparison to other attribution/provenance techniques . While this submission is a very interesting and enjoyable read , in the end it fails to fully convince how large the impact will be . Questions are open w.r.t.models with privacy guarantees and larger models ... and also in the end , this technique is still effected by fine tuning . It is unclear to what extend this is an issue .", "rating": "7: Good paper, accept", "reply_text": "> * * * Not evaluated on more complex classification problems . In particular , membership inference ( which this method is based on ) can be even weaker on those larger and well-trained models . * * * We followed your suggestion and supplemented our paper with results showing the success of * dataset inference * on large models trained on complex tasks like the ImageNet dataset . We first remark that prior work in model extraction has not successfully demonstrated the efficacy of model stealing on such complex tasks , since these methods require a high number of queries to steal a model , and are hence not practical yet . As a proof of validity of our approach , we demonstrate dataset inference ( DI ) in the threat model that assumes complete data theft : the adversary directly steals the dataset used by the victim to train a model rather than querying the victim model . We use 3 pre-trained models on ImageNet using different architectures , and treat the one with a Wide ResNet-50-2 backbone as the victim . We then observe if DI is able to correctly identify that the two other pre-trained models ( AlexNet and Inception V3 ) were also trained on the same dataset ( i.e. , ImageNet ) . We confirm that DI is able to claim ownership ( i.e. , that the suspect models were indeed trained using knowledge of the victim \u2019 s training set ) with a p-value of $ 10^ { -3 } $ in less than 10 samples . The results are provided below : Success of DI on ImageNet ( Blind Walk ) | Attack Type | Model Stealing Attack | Architecture | Mean Difference | p-value | | -- || -- ||| | $ \\mathcal { V } $ | Source | Wide Resnet-50-2 | 1.868 | $ 10^ { -34 } $ | | $ \\mathcal { A_D } $ | Different Architecture | AlexNet | 0.790 | $ 10^ { -3 } $ | | $ \\mathcal { A_D } $ | Different Architecture | Inception V3 | 1.085 | $ 10^ { -5 } $ | Recall that these models are trained on large datasets where works in MI attacks train victims on small subsets of training datasets to enable overfitting ( Yeom et.al.2018 ) .We believe this demonstrates that DI scales to complex tasks . These results were added in Appendix E of the updated draft . > * * * The claim about other watermarking techniques not being effective - although probably true - was not substantiated ( unless I \u2019 ve missed ) . at least there is no comparison to other attribution/provenance techniques . * * * A key difference between our approach and watermarking techniques is that watermarking involves modifications to the training procedure whereas we can apply dataset inference to any model already trained and deployed . Further , watermarking has a trade-off on task accuracy . Regarding the effectiveness of watermarking , we cited [ Jia et al ( 2020 ) ] ( http : //arxiv.org/abs/2002.12200 ) where it is shown that common watermarking techniques fail in the presence of model extraction via distillation . This is because the distribution of watermarks differs from the distribution of the data that is being modeled . This is also supported by [ Yang et al . ( 2019 ) ] ( http : //arxiv.org/abs/1906.06046 ) for the particular case of distillation . We agree with the reviewer that a more substantiated and scoped claim is desirable here . Therefore , we qualified our claim about the limited effectiveness of the watermarking technique with the caveat that both [ Jia et al . ( 2020 ) ] ( http : //arxiv.org/abs/2002.12200 ) and [ Yang et al . ( 2019 ) ] ( http : //arxiv.org/abs/1906.06046 ) , have proposed alternative watermarking strategies that prevail distillation and fine-tuning ( to some extent\u2014see the next response ) . We made this qualification more prominent in the updated submission . > * * * ... this technique is still effected by fine tuning . * * * We would like to clarify that fine-tuning is the least effective attack against DI among the different threat models we considered in our paper . The effect size for fine-tuning is the largest across our experiments ( as highlighted in red in Table 1 ) . This is also indicated by the p-values which are correspondingly the lowest for fine-tuning . We have made this more explicit in the table caption . These two observations mean that our DI method was most resilient against fine-tuning based attacks . Thank you for your time . We would be happy to answer any further queries ."}], "0": {"review_id": "hvdKKV2yt7T-0", "review_text": "* * Update * * : Since nearly all of my issues have been addressed , I have changed my rating from 5 to 7 . Best of luck : ) Summary : The authors study a subset of model stealing by introducing dataset inference , a process to identify whether a suspected adversary 's model has private knowledge from the original model 's dataset . Through their experiments on two related tasks in the vision domain ( CIFAR-10,100 ) , the authors demonstrate their approach for dataset inference that combines statistical tests and decision boundary-based checks to make claims of model theft . They base their algorithms on theoretical results in toy settings ( proofs shifted to additional materials ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I feel the final observations are quite useful and pave new methods for asserting model theft in real-world settings . Although the proposed techniques are not novel ( in fact , a lot of existing literature is not cited , giving the impression that a lot of proposed techniques are contributions of the paper itself ) , the end results and observations can be useful in practical settings . I feel the authors need to make a lot of changes to the current draft before it can be ready for acceptance . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : - Mentioning points such as how issues like these can actually get to courts in real life and are covered by intellectual theft laws is great to see . Since the proposed solution tackles a real-world ( potentially ) problem , it makes sense to incorporate real-world implications as well . - Using theoretical analyses to justify the proposed methods is a great way to approach a problem like this . Testing them out on simpler problems to see how and most importantly , why they work , is important . - Evaluation over multiple model-stealing attacks is a great way to both show the robustness of the proposed attack method , as well as identify strong ( fine-tuned models ) as well as weak ( label-query attacks ) points of the algorithm . - The query efficiency of the proposed method is quite remarkable . A method that can use these little samples in a real-world setting to assert model stealing ( or check for it ) with high confidence can be very helpful . - Page 8 : 'gradient-based approaches are sensitive to numerical instabilities ' is a great and very valid point . An adversary could in fact even deliberately resort to gradient masking to avoid such an attack . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : - By biggest and foremost problem is with the threat model . In all of these settings , it is assumed that the party accusing the adversary of stealing dataset access provides actual data from its own training dataset . However , since this is a real-world scenario , there should also be some way for the accuser to provably show that the data points it is using for the test are in fact from its own training set . Also , it should be made clear what level of overlap in the datasets used is problematic . This parameter would vary from problem to problem and thus can not be one fixed value . Nonetheless , it should be a parameter of the testing process . - The term 'dataset inference ' is slightly misleading . In some portions , the authors say `` we propose to identify stolen copies by showing that they were trained on the same dataset '' . In other places , they imply that the adversary used a `` subset '' of that knowledge . It is important to both be consistent in this definition , and to quantify it : nowhere in the paper is it mentioned how much overlap is acceptable as `` sheer chance/possible via domain knowledge '' , and how much is `` definitely stealing '' . - Key implementation details , such as how the datasets were split to be used by the target and adversary to train their models are omitted . - CIFAR-10 and CIFAR-100 are highly related datasets ( same images ) . Claims made in the paper would hold more value if they can show similar trends on some other datasets . The authors could try some high-dimensional datasets such as tiny-Imagenet , or even something as simple as MNIST . - In several places in the paper , authors claim that watermarking models does not work in the sense that trying to copy a model does not copy the watermarks , since there is a difference in distributions . This is not true : recent works have shown honeypot attacks that transfer watermark behavior even when someone tries to copy a model using normal data . Please refer to Section 3.2.2 of [ this work ] ( https : //arxiv.org/pdf/2009.12153.pdf ) - When using sample points to assert model stealing , does the party arguing for their case release them all together , or one by one . One may argue that the sequential release of sample points could potentially be games ( a malicious party could easily look at the target model and accuse them wrongfully ) . What are the countermeasures against such a scenario ? It seems the final judgment can only be made by a third party by the other two parties revealing large subsets of their training sets and algorithms . This might not be possible in real-world scenarios , where datasets used to train models are bound by certain privacy-protection laws ( GDPR , etc ) . - Section 2 `` V suspects theft '' : what are the criteria here ? Is it based on similarity in model performance , or pure suspicion ? Since this section sets up the threat model , it might be beneficial to be as exact as possible . ( a ) `` may gain access to a subset of '' . As asked above , how big of a subset is a problem ? Since the subset is of Kv and not Sv , it may be possible that they are not overlapping at all , or barely overlap . Would it still be a problem ? ( b ) Definition 1 : `` prove that some knowledge '' since this is a definition , it should not be hand-wavy . In my opinion , the `` some '' here should be quantified perhaps by a parameter that captures the overlap in the two datasets and integrates them in the definition . ( c ) Definition 2 : Please rewrite the last point as a conditional assignment . It is hard to read in its current form - Section 3.3 , 'Failure if Membership Inference ' refers to two labels : b=1 when the sample is from S , or b=0 when the sample is from D. However , S is a subset of D and thus the first case will always be true . Assuming you mean 'sample is from S - D ' , please fix this . - Insufficient literature review . There are many works in the field that deal with the same problem and similar approaches , even though they do not define them as `` dataset inference '' . Please search for some works on 'property inference ' : the current problem can be posed as differentiating between the dataset suspected to be used , versus some other dataset . - Although the white-box method looks at finding the 'closest ' data points , the black-box method samples random directions and takes steps . Assuming this is intended to be a proxy of the white-box setting , it should take the form of sampling random initial directions and then , over all the samples , picking the closest directions . Also , the white-box setting may give significantly different results if either of the models were trained with an adversarial robustness objective . It would be interesting to see how the attack performs in that case . - Section 4.2 claims that membership inference attacks do not take confidence into account and that this is a key difference from the proposed method in this work . This is not true : ROC curves for such attacks can be analyzed to set rejection regions for such claims . - Section 4.2 , last line : `` publicly available data that is not used for training of Fv '' Is there a specific reason to use data that was not used to train Fv ? - Page 8 , 'White-box access is not essential to DI ' claims the Blind-Walk approach is 'non-targeted ' . This can also be made true for the white-box setting by adjusting the minimization objective to be non-targeted , instead of making it targeted and targeting the top-k classes . Please address and clarify the cons above", "rating": "7: Good paper, accept", "reply_text": "> * * * [ 11 ] Section 4.2 claims that membership inference attacks do not take confidence into account and that this is a key difference from the proposed method in this work . This is not true : ROC curves for such attacks can be analyzed to set rejection regions for such claims . * * * Dataset Inference utilizes statistical confidence measures . Such confidence measures naturally take into account the likelihood of failure because they are evaluated by sampling multiple data points from given distributions . On the contrary , MI works on pre-set rejection thresholds defined using ROC curves . The former can give us the likelihood of failure of dataset inference , whereas the latter is only as representative as the examples tested on for creating the ROC curve . In a new distribution , the generalization of the ROC curve based threshold may or may not hold , but since DI is able to draw multiple samples from the true distribution , we can set statistical confidence regions . > * * * [ 12 ] Section 4.2 , last line : \u201c publicly available data that is not used for training of Fv \u201d Is there a specific reason to use data that was not used to train Fv ? * * * The alternate dataset should not have been seen by Fv at training time . Otherwise , the ` prediction margin \u2019 for these data points would have been altered during optimization . The confidence regressor would hence not be able to distinguish between the two types of data points ( private training and public training ) , since they were both optimized on . That said , the reviewer is correct that the alternate dataset need not be publicly available . It could for instance be the private test set of the victim . > * * * [ 13 ] Page 8 , \u2018 White-box access is not essential to DI \u2019 claims the Blind-Walk approach is \u2018 non-targeted \u2019 . This can also be made true for the white-box setting by adjusting the minimization objective to be non-targeted , instead of making it targeted and targeting the top-k classes . * * * The point we wanted to highlight here is that the blind-walk attack is not ranking possible classes based on their distance to the current point ( which would be the case even if we used an untargeted minimization objective in the white-box approach ) but rather measures the average distance to randomly picked target classes . This follows from our discussion over comment [ 10 ] . We have edited the text to : * \u201c ( b ) the approach is stochastic and aims to find the expected prediction margin rather than the worst-case ( it searches for any incorrect neighboring class in a randomly chosen direction rather than focusing on the distance to possible target classes ) \u201d *"}, "1": {"review_id": "hvdKKV2yt7T-1", "review_text": "This paper tackles a timely problem of detecting model stealing attacks . The proposed identifies the stolen model by investigating whether the model is trained on the same dataset as the victim model . Pros : - The proposed method of dataset inference for model stealing detection is novel . - The experiments provided by the paper are comprehensive , including different assumptions of data access , model access , and query access , etc . Adaptive attacks are considered in the paper as well . - The paper is well-written and easy to follow . Cons : - Zero-shot learning ( [ 9 ] in the paper ) can not be directly used as a model stealing attack , because the proposed approach in [ 9 ] requires more information of the victim model than the input and output pair . It would be better if the paper could investigate other data-free model stealing approaches ( e.g. , [ 1 ] [ 2 ] ) . Also , it is not clear to me how the proposed dataset inference approach is applied to data-free model stealing attacks , since the training dataset used by the attacker is different from the victim \u2019 s dataset . If the attackers use some synthetic data to steal the model , will the proposed approach work ? - What are the white-box and black-box settings in Section 4.1 ? [ 1 ] Mika Juuti , Sebastian Szyller , Samuel Marchal , and N Asokan . Prada : protecting against dnn model stealing attacks . In 2019 IEEE European Symposium on Security and Privacy ( EuroS & P ) , pp . 512\u2013527.IEEE , 2019 . [ 2 ] Nicolas Papernot , Patrick McDaniel , Ian Goodfellow , Somesh Jha , Z Berkay Celik , and Ananthram Swami . Practical black-box attacks against machine learning . In Proceedings of the 2017 ACM on Asia conference on computer and communications security , pp . 506\u2013519 , 2017 .", "rating": "7: Good paper, accept", "reply_text": "> * * * Zero-shot learning ( [ 9 ] in the paper ) can not be directly used as a model stealing attack , because the proposed approach in [ 9 ] requires more information of the victim model than the input and output pair * * * As detailed in the introduction ( paragraph 2 , point * * ( 3 ) * * ) and Section 5.2 ( point ( 2 ) ) on the type of threat models , our work on dataset inference extends beyond the realms of model extraction alone . We also consider the following two scenarios : - complete data theft ( i.e. , the training dataset itself is leaked to the adversary ) - model availability : in the age of open-sourced code and pre-trained models , the threat of commercialization of an ML model , otherwise intended only for academic purposes , is practical and prevalent . For instance , consider the scenarios where a company ( such as OpenAI ) may want to open-source their model ( like GPT-3 ) for academic purposes but does not permit its commercialization . In both situations , an adversary has sufficient information for staging zero-shot learning attacks on the victim model . An additional advantage of considering zero-shot learning is that it allows us to strengthen our evaluation with an analysis of dataset inference against adaptive adversaries : the queries used in zero-shot learning negate any resemblance to real-world queries since they only utilize synthetic inputs . > * * * What are the white-box and black-box settings in Section 4.1 ? * * * The white-box and black-box settings indicate access levels that the victim has to the potential adversary \u2019 s model . For instance , if the victim suspects that a competitor \u2019 s API has stolen their ML model , they can only gain black-box access to the predictions of their MLaaS . However , if the victim files a legal complaint , then in the presence of a neutral arbitrator , the victim may be presented white-box access to demonstrate \u2018 dataset inference \u2019 on the suspect adversary . Section 4.1 introduces an embedding generation procedure for each of the two settings . Thank you for your time . We would be happy to answer any further queries ."}, "2": {"review_id": "hvdKKV2yt7T-2", "review_text": "The submission proposes to defend against model stealing attacks by dataset inference . It is argued that other watermarking techniques are not robust enough to be transferred during model stealing . Ownership resolution is performed by kind of a membership inference attack that the model owner runs on the potential copy . As only the owner knows the true training data , the `` attack '' can be amplified by a few samples . This leads to a rather strong signal in order to show model provenance . Final decision is formulated as a hypothesis test in order to get a confidence . The argumentation why for larger models this is not dependent on some sort of over fitting is not clear . While the idea is interesting , it is somewhat at odds with others goals in machine learning . In general , it is not a desirable property if the trained model contains `` artifacts '' related to the training data . This on the one hand could be overfitting artifacts that hamper generalization or more prominently could mean that the model does not preserve privacy . If such overfitting is not a major issues and privacy is not a concern , the approach seems viable . However , there should be an extended discussion under which situations/models the method is feasible . ( e.g.privacy preserving learning ? ) I somewhat disagree with the practice of moving the related work to the appendix . this is a vital part of a paper and not an appendix . this looks like breaking the page limitation . overall , having a 19 page submission is not quite following the submission guideline ( although the additional material is useful ) . this should not become practice - in particular as some parts in the main submission could be more concise . In particular , as the actual algorithm is quite simple ( which I do n't want to see as a disadvantage as it 's well motivated ) Overall , the paper is insightful -- also adding to the understanding of the connections to membership inference . Also the attention to important details like query efficiency is taken care of . It is bit disappointing that the method is not evaluated on more complex classification problems . In particular , as membership inference ( which this method is based on ) can be even weaker on those larger and well trained models . The claim about other watermarking techniques no being effective - although probably true - was not substantiated ( unless I 've missed ) . at least there is no comparison to other attribution/provenance techniques . While this submission is a very interesting and enjoyable read , in the end it fails to fully convince how large the impact will be . Questions are open w.r.t.models with privacy guarantees and larger models ... and also in the end , this technique is still effected by fine tuning . It is unclear to what extend this is an issue .", "rating": "7: Good paper, accept", "reply_text": "> * * * Not evaluated on more complex classification problems . In particular , membership inference ( which this method is based on ) can be even weaker on those larger and well-trained models . * * * We followed your suggestion and supplemented our paper with results showing the success of * dataset inference * on large models trained on complex tasks like the ImageNet dataset . We first remark that prior work in model extraction has not successfully demonstrated the efficacy of model stealing on such complex tasks , since these methods require a high number of queries to steal a model , and are hence not practical yet . As a proof of validity of our approach , we demonstrate dataset inference ( DI ) in the threat model that assumes complete data theft : the adversary directly steals the dataset used by the victim to train a model rather than querying the victim model . We use 3 pre-trained models on ImageNet using different architectures , and treat the one with a Wide ResNet-50-2 backbone as the victim . We then observe if DI is able to correctly identify that the two other pre-trained models ( AlexNet and Inception V3 ) were also trained on the same dataset ( i.e. , ImageNet ) . We confirm that DI is able to claim ownership ( i.e. , that the suspect models were indeed trained using knowledge of the victim \u2019 s training set ) with a p-value of $ 10^ { -3 } $ in less than 10 samples . The results are provided below : Success of DI on ImageNet ( Blind Walk ) | Attack Type | Model Stealing Attack | Architecture | Mean Difference | p-value | | -- || -- ||| | $ \\mathcal { V } $ | Source | Wide Resnet-50-2 | 1.868 | $ 10^ { -34 } $ | | $ \\mathcal { A_D } $ | Different Architecture | AlexNet | 0.790 | $ 10^ { -3 } $ | | $ \\mathcal { A_D } $ | Different Architecture | Inception V3 | 1.085 | $ 10^ { -5 } $ | Recall that these models are trained on large datasets where works in MI attacks train victims on small subsets of training datasets to enable overfitting ( Yeom et.al.2018 ) .We believe this demonstrates that DI scales to complex tasks . These results were added in Appendix E of the updated draft . > * * * The claim about other watermarking techniques not being effective - although probably true - was not substantiated ( unless I \u2019 ve missed ) . at least there is no comparison to other attribution/provenance techniques . * * * A key difference between our approach and watermarking techniques is that watermarking involves modifications to the training procedure whereas we can apply dataset inference to any model already trained and deployed . Further , watermarking has a trade-off on task accuracy . Regarding the effectiveness of watermarking , we cited [ Jia et al ( 2020 ) ] ( http : //arxiv.org/abs/2002.12200 ) where it is shown that common watermarking techniques fail in the presence of model extraction via distillation . This is because the distribution of watermarks differs from the distribution of the data that is being modeled . This is also supported by [ Yang et al . ( 2019 ) ] ( http : //arxiv.org/abs/1906.06046 ) for the particular case of distillation . We agree with the reviewer that a more substantiated and scoped claim is desirable here . Therefore , we qualified our claim about the limited effectiveness of the watermarking technique with the caveat that both [ Jia et al . ( 2020 ) ] ( http : //arxiv.org/abs/2002.12200 ) and [ Yang et al . ( 2019 ) ] ( http : //arxiv.org/abs/1906.06046 ) , have proposed alternative watermarking strategies that prevail distillation and fine-tuning ( to some extent\u2014see the next response ) . We made this qualification more prominent in the updated submission . > * * * ... this technique is still effected by fine tuning . * * * We would like to clarify that fine-tuning is the least effective attack against DI among the different threat models we considered in our paper . The effect size for fine-tuning is the largest across our experiments ( as highlighted in red in Table 1 ) . This is also indicated by the p-values which are correspondingly the lowest for fine-tuning . We have made this more explicit in the table caption . These two observations mean that our DI method was most resilient against fine-tuning based attacks . Thank you for your time . We would be happy to answer any further queries ."}}