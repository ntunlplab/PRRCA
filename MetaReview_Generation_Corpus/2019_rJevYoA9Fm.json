{"year": "2019", "forum": "rJevYoA9Fm", "title": "The Singular Values of Convolutional Layers", "decision": "Accept (Poster)", "meta_review": "This paper proposes an efficient method to compute the singular values of the linear map represented by a convolutional layer. It makes uses of the special block-matrix form of convolutional layers to construct their more efficient method. Furthermore, it shows that this method can be used to devise new regularization schemes for DNNs. The reviewers did note that the diversity of the experiments could be improved, and R2 raised concerns that the wrong singular values were being computed. The authors should add a section clarifying why the singular values of a convolutional linear map are not found directly by performing SVD on the reshaped kernel - indeed the number of singular values would be wrong. A contrast with the singular values obtained by simple reshaping of the kernel would also be helpful.", "reviews": [{"review_id": "rJevYoA9Fm-0", "review_text": "The paper is dedicated to computation of singular values of convolutional layers. While singular values of convolutional layers represent sufficient interest for researchers, huge computational complexity made it difficult to investigate their properties in the case of layers of deep neural networks. Using the fact that operator matrix of the convolutional layer has a special form (i.e. can be represented as block-matrix, which blocks are doubly block circulant matrices) the authors proposed a more efficient method of computation of singular values. I really enjoyed reading this paper and I think that it opens a lot of interesting applications. As one of the possible applications the authors proposed a regularization method based on bounding of singular values. The paper from my point of view has two main drawbacks: 1. Diversity of experiments. While the paper has strong theoretical component, the part dedicated to experiments is not broad enough. It would be interesting to see regularization on other architectures and other datasets. 2.The system of references. I would recommend to add not only references to the sources, but also to the theorem numbers or the chapters. For example, I would recommend to replace \u2018Poposition 9 ((Lefkimmiatis et al., 2013))\u2019 with \u2018Poposition 9 ((Lefkimmiatis et al., 2013, Proposition 1))\u2019. In pure math papers, it is a standard rule to add such additional information since many papers contain a lot of theorems and it significantly simplifies reading and understanding the paper. Despite these disadvantages this is a great work with huge potential. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review . We have updated the references in the revised version as you requested ."}, {"review_id": "rJevYoA9Fm-1", "review_text": "This paper studies the problem on computing the spectrum of singular values of linear convolutional layers. This is an important problem with abundant applications on regularizing deep neural networks. However, there are several technical issues need to be addressed in its current form. First, in the section \"Summary of Results\", at first read of the paper I found it very confusing why the time complexity of computing the spectrum is a function of n, where n is the size of the input feature map. Intuitively, since the size of the convolutional kernel is m x m x k x k, it is expected that the time complexity is expressed as a function of (m, k). Later I realized that this is due to the unnecessary and redundant 0 padding in section 2.1 that leads to this artifact. I understand that in order to apply the described Fourier transform technique it is necessary to introduce the large nxn filter, which is of the same size as the input, but it also introduces redundant computation. This fact further emerges in the introduction of matrix A in Eq (1). More importantly, I think the authors didn't perform a detailed analysis on using the basic definition of convolutional filter to compute its spectrum, and this is the reason why they reached a misleading conclusion that simple SVD takes O(n^6 m^3) time. Specifically, each convolution operation corresponds to a inner product operation, so we can reshape the input 3D tensor with shape m x n x n into a 2D matrix, with shape n^2 x mk^2, denoted as X. Note that this creates a unnecessary redundancy in the input feature map, but it does not create redundant weight for the convolutional kernel. As a comparison, the introduced matrix A in the paper is heavily redundant. Similarly, for m channels, we can reshape the 4D convolutional kernel with shape m x m x k x k into a 2D matrix, with shape mk^2 x m, denoted as K. Then the usual convolution layer can be described as the following linear system: Y = X K, where Y with shape n^2 x m is the output, and can be easily reshaped into size m x n x n. Hence to compute the spectrum of the convolution layer corresponds to computing the singular values of the 2D matrix K with size mk^2 x m. Hence a naive application of SVD directly gives us the solution in time O(m^3 k^2) (Note that the time complexity of SVD for matrix with size a x b is O(min{a^2 b, a b^2})), which is much smaller than the one given in the paper O(m^3 n^2) since k << n. In experiment the authors made unfair comparison between their proposed method and the full matrix method: the full matrix A is fully redundant, due to its circulant pattern. As this implies a highly redundant information, nobody will form and compute matrix A explicitly in practice. So the time improvements demonstrated in the experiment section are meaningless. A valid baseline would be to compare the proposed method with the one introduced above. But in this case I would imagine the proposed method to be worse due to its unnecessary 0 padding leading to the worst time complexity. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "It can be proved that if a kernel of size ( k , k , m , m ) is applied to an input of size ( n , n , m ) , and the entries of the kernel are chosen from a continuous probability distribution like a Gaussian , then with probability 1 , the number of distinct singular values in the linear transformation of its convolutional layer is at least ( m n^2 ) /2 . Repeatedly applying the code on page two of our paper to random inputs has always produced at least this number of distinct singular values . ( We invite the reader to try this . ) Any reshaping of an ( k , k , m , m ) kernel can produce at most mk singular values , which is not enough to be correct . The reshaping outlined in your review can have at most m singular values , which is even smaller . As we noted in our answer to a question during the review period , an example in which the reshaping does not compute even the largest singular value correctly can be obtained by executing the following commands after importing numpy as np and defining SingularValues as in page two of our paper : kernel = np.array ( np.ones ( 4 ) ) .reshape ( 2,2,1,1 ) SingularValues ( kernel , [ 4,4 ] ) reshaped = kernel.reshape ( 4,1 ) np.linalg.svd ( reshaped , compute_uv=False ) The correct largest singular value of the convolutional layer is 4 . ( An all-ones feature map is turned into an all-fours feature map by applying this filter . ) The ( only ) singular value of the reshaping of the kernel , which is ( 1,1,1,1 ) ^T , is 2 . This is not an isolated instance -- - random inputs also produce counterexamples -- - we ran our algorithm with m = 1 , n = 16 , and various values of k. In each case we found 130 unique singular values , with ranges described below . The reshaping method produced 1 singular value in each case : k Range of Singular Values Singular Value from reshaping 4 [ 0.388872696514 , 7.6799308322 ] 3.76770352 5 [ 0.421739253704 , 10.7721306924 ] 5.04019121 7 [ 0.165159699404 , 14.9556902191 ] 6.7304902 7 [ 0.532454839614 , 16.38084538 ] 7.20513578 3 [ 0.4655241798 , 6.12218595024 ] 3.48952531 6 [ 0.418950277234 , 15.2821360286 ] 6.1289447"}, {"review_id": "rJevYoA9Fm-2", "review_text": "In this paper, the authors derive exact formulas for computing singular values of convolutional layers of deep neural networks. By appealing to fast FFT transformations, they show that computing the singular values can be done much faster than computing the full SVD of the convolution matrix. This obviates the needs to approximate the singular values. They use these results to then devise regularization schemes for DNN layers, and show that employing this regularization helps with model performance. They show that the algorithm with the operator norm regularization can be solved via an alternating projection scheme. They also postulate that since this might be expensive and unnecessary, one can also perform just 2 projections after every few SG iterations, and claim that this acts as a 'warm start' for subsequent iterations. Experiments reveal that this does not degrade the performance too much. The paper is well written and easy to understand. The proofs follow from standard linear algebra methods, and are easy to follow. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your kind review ."}], "0": {"review_id": "rJevYoA9Fm-0", "review_text": "The paper is dedicated to computation of singular values of convolutional layers. While singular values of convolutional layers represent sufficient interest for researchers, huge computational complexity made it difficult to investigate their properties in the case of layers of deep neural networks. Using the fact that operator matrix of the convolutional layer has a special form (i.e. can be represented as block-matrix, which blocks are doubly block circulant matrices) the authors proposed a more efficient method of computation of singular values. I really enjoyed reading this paper and I think that it opens a lot of interesting applications. As one of the possible applications the authors proposed a regularization method based on bounding of singular values. The paper from my point of view has two main drawbacks: 1. Diversity of experiments. While the paper has strong theoretical component, the part dedicated to experiments is not broad enough. It would be interesting to see regularization on other architectures and other datasets. 2.The system of references. I would recommend to add not only references to the sources, but also to the theorem numbers or the chapters. For example, I would recommend to replace \u2018Poposition 9 ((Lefkimmiatis et al., 2013))\u2019 with \u2018Poposition 9 ((Lefkimmiatis et al., 2013, Proposition 1))\u2019. In pure math papers, it is a standard rule to add such additional information since many papers contain a lot of theorems and it significantly simplifies reading and understanding the paper. Despite these disadvantages this is a great work with huge potential. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review . We have updated the references in the revised version as you requested ."}, "1": {"review_id": "rJevYoA9Fm-1", "review_text": "This paper studies the problem on computing the spectrum of singular values of linear convolutional layers. This is an important problem with abundant applications on regularizing deep neural networks. However, there are several technical issues need to be addressed in its current form. First, in the section \"Summary of Results\", at first read of the paper I found it very confusing why the time complexity of computing the spectrum is a function of n, where n is the size of the input feature map. Intuitively, since the size of the convolutional kernel is m x m x k x k, it is expected that the time complexity is expressed as a function of (m, k). Later I realized that this is due to the unnecessary and redundant 0 padding in section 2.1 that leads to this artifact. I understand that in order to apply the described Fourier transform technique it is necessary to introduce the large nxn filter, which is of the same size as the input, but it also introduces redundant computation. This fact further emerges in the introduction of matrix A in Eq (1). More importantly, I think the authors didn't perform a detailed analysis on using the basic definition of convolutional filter to compute its spectrum, and this is the reason why they reached a misleading conclusion that simple SVD takes O(n^6 m^3) time. Specifically, each convolution operation corresponds to a inner product operation, so we can reshape the input 3D tensor with shape m x n x n into a 2D matrix, with shape n^2 x mk^2, denoted as X. Note that this creates a unnecessary redundancy in the input feature map, but it does not create redundant weight for the convolutional kernel. As a comparison, the introduced matrix A in the paper is heavily redundant. Similarly, for m channels, we can reshape the 4D convolutional kernel with shape m x m x k x k into a 2D matrix, with shape mk^2 x m, denoted as K. Then the usual convolution layer can be described as the following linear system: Y = X K, where Y with shape n^2 x m is the output, and can be easily reshaped into size m x n x n. Hence to compute the spectrum of the convolution layer corresponds to computing the singular values of the 2D matrix K with size mk^2 x m. Hence a naive application of SVD directly gives us the solution in time O(m^3 k^2) (Note that the time complexity of SVD for matrix with size a x b is O(min{a^2 b, a b^2})), which is much smaller than the one given in the paper O(m^3 n^2) since k << n. In experiment the authors made unfair comparison between their proposed method and the full matrix method: the full matrix A is fully redundant, due to its circulant pattern. As this implies a highly redundant information, nobody will form and compute matrix A explicitly in practice. So the time improvements demonstrated in the experiment section are meaningless. A valid baseline would be to compare the proposed method with the one introduced above. But in this case I would imagine the proposed method to be worse due to its unnecessary 0 padding leading to the worst time complexity. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "It can be proved that if a kernel of size ( k , k , m , m ) is applied to an input of size ( n , n , m ) , and the entries of the kernel are chosen from a continuous probability distribution like a Gaussian , then with probability 1 , the number of distinct singular values in the linear transformation of its convolutional layer is at least ( m n^2 ) /2 . Repeatedly applying the code on page two of our paper to random inputs has always produced at least this number of distinct singular values . ( We invite the reader to try this . ) Any reshaping of an ( k , k , m , m ) kernel can produce at most mk singular values , which is not enough to be correct . The reshaping outlined in your review can have at most m singular values , which is even smaller . As we noted in our answer to a question during the review period , an example in which the reshaping does not compute even the largest singular value correctly can be obtained by executing the following commands after importing numpy as np and defining SingularValues as in page two of our paper : kernel = np.array ( np.ones ( 4 ) ) .reshape ( 2,2,1,1 ) SingularValues ( kernel , [ 4,4 ] ) reshaped = kernel.reshape ( 4,1 ) np.linalg.svd ( reshaped , compute_uv=False ) The correct largest singular value of the convolutional layer is 4 . ( An all-ones feature map is turned into an all-fours feature map by applying this filter . ) The ( only ) singular value of the reshaping of the kernel , which is ( 1,1,1,1 ) ^T , is 2 . This is not an isolated instance -- - random inputs also produce counterexamples -- - we ran our algorithm with m = 1 , n = 16 , and various values of k. In each case we found 130 unique singular values , with ranges described below . The reshaping method produced 1 singular value in each case : k Range of Singular Values Singular Value from reshaping 4 [ 0.388872696514 , 7.6799308322 ] 3.76770352 5 [ 0.421739253704 , 10.7721306924 ] 5.04019121 7 [ 0.165159699404 , 14.9556902191 ] 6.7304902 7 [ 0.532454839614 , 16.38084538 ] 7.20513578 3 [ 0.4655241798 , 6.12218595024 ] 3.48952531 6 [ 0.418950277234 , 15.2821360286 ] 6.1289447"}, "2": {"review_id": "rJevYoA9Fm-2", "review_text": "In this paper, the authors derive exact formulas for computing singular values of convolutional layers of deep neural networks. By appealing to fast FFT transformations, they show that computing the singular values can be done much faster than computing the full SVD of the convolution matrix. This obviates the needs to approximate the singular values. They use these results to then devise regularization schemes for DNN layers, and show that employing this regularization helps with model performance. They show that the algorithm with the operator norm regularization can be solved via an alternating projection scheme. They also postulate that since this might be expensive and unnecessary, one can also perform just 2 projections after every few SG iterations, and claim that this acts as a 'warm start' for subsequent iterations. Experiments reveal that this does not degrade the performance too much. The paper is well written and easy to understand. The proofs follow from standard linear algebra methods, and are easy to follow. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your kind review ."}}