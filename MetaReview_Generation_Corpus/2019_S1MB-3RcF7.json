{"year": "2019", "forum": "S1MB-3RcF7", "title": "Multi-objective training of Generative Adversarial Networks with multiple discriminators", "decision": "Reject", "meta_review": "The reviewers found that paper is well written, clear and that the authors did a good job placing the work in the relevant literature.  The proposed method for using multiple discriminators in a multi-objective setting to train GANs seems interesting and compelling.  However, all the reviewers found the paper to be on the borderline.  The main concern was the significance of the work in the context of existing literature.  Specifically, the reviewers did not find the experimental results significant enough to be convinced that this work presents a major advance in GAN training.  ", "reviews": [{"review_id": "S1MB-3RcF7-0", "review_text": "The paper investigates the use of multi-objective optimization techniques in GAN-setups where there are multiple discriminators. Using multiple discriminators was proposed in Durugkar et al, Arora et al, Neyshabur et al and others. The twist here is to focus on the Pareto front and to import multiple gradient descent and hypervolume-maximization based methods into GANs. The results are decent. The authors find that optimizing with respect to multiple discriminators increases diversity of samples for a computational cost. However, just scaling up (and carefully optimizing), can yield extremely impressive samples, https://arxiv.org/abs/1809.11096. It is unclear how the tradeoffs in optimizing against multiple discriminators stack-up against bigger GANs. From my perspective, the paper is interesting because it introduces new methods into GANs from another community. However, the results themselves are not sufficient for publication. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the feedback and taking the time for reading our paper . We are glad that the reviewer found our method interesting and hope that the following response , added to the new results included in the manuscript , will make her/him more confident about our contributions . Regarding the mentioned trade-off , we understood that the reviewer is referring to the addition of \u201c capacity \u201d ( in terms of the number of parameters ) on the discriminators side , as in the multiple-discriminators settings , or in the generator side , as in the pointed reference ( Brock et al.2018 ) .We would like to point out that such approaches have different goals : while adding capacity in the generator side is intended to yield generators able to scale to higher resolution settings and higher quality samples , multiple-discriminators are aimed at stabilizing training , avoiding common issues such as mode-collapse and divergence , which makes final performance of the generator highly dependent of careful hyperparameters tuning . If enough resources are available , both approaches should be used jointly . As we also observed multiple-discriminators training to yield higher quality and diversity when compared to their single-discriminator equivalents , we believe higher scales settings would also benefit . Regarding the insufficiency of results , we would like to respectfully highlight that we presented quantitative and qualitative results ( comparing with both single- and multiple-discriminators GANs ) in 4 datasets ( namely , MINIST , CIFAR-10 , Stacked MNIST , and CelebA ) , with consistent conclusions . We also included samples on a higher resolution for CelebA at 128x128 in Appendix D.2 , and are currently running experiments to compare different versions of GANs in the single vs. multiple discriminators settings . Some preliminary results which will be added to the manuscript as soon as we conclude the new experiments , show that adding discriminators yield the following relative improvement in terms of FID : DCGAN - 55.21 % ; LSGAN - 57.93 % ( we are currently running similar experiments on other GANs such as wGAN-GP and hingeGAN [ 2 ] ) . Moreover , we would highly appreciate if the reviewer could suggest any further experiment in order to increase her/his confidence in our results . [ 2 ] Miyato , Takeru , et al . `` Spectral normalization for generative adversarial networks . '' arXiv preprint arXiv:1802.05957 ( 2018 ) ."}, {"review_id": "S1MB-3RcF7-1", "review_text": "Clarity: The work is a clear introduction/overview of this area of research. The reviewer enjoyed the connections to Multiple-Gradient Descent and clear distinctions/contrasts with previous approaches to weighting the outputs of multiple discriminators. All in all, the paper is quite clear in what its contributions are and how it differs from previous approaches. The details and motivations of the Hypervolume Maximization (HVM) method (especially as it relates to and interacts with the slack method of picking the nadir point) were a bit harder to follow intuitively given the standalone information in the paper. Originality: Adapts a technique to approximate MGD called HVM (Miranda 2016) and applies it to multi-discriminator training in GANs. As far as the reviewer is aware, this is a novel application of HVM to this task and well motivated under the MGD interpretation of the problem. Significance: Unclear. This work in isolation appears to present an improvement over prior work in this sub-field, but it is not obvious that the findings in these experiments will continue to be robust in more competitive settings. For instance, the worst performing model on CIFAR10, WGAN-GP (according to the experiments run) WGAN-GP also holds near SOTA Inception scores on CIFAR10 when appropriately tuned. Without any experimental results extending beyond toy datasets like MNIST and CIFAR10 the reviewer is not confident whether fundamental issues with GAN training are being addressed or just artifacts of small scale setups. Closely related previous work (Neyshabur 2017) scaled to 128x128 resolution on a much more difficult dataset - Imagenet Dogs but the authors did not compare in this case. Quality: Some concerns about details of experiments (see cons list and significance section for further discussion). Pros: + The work provides a clear overview of previous work on approaches using multiple discriminators. + The connections of this line of work to MGD and the re-interpretation of various other approaches in this framework is valuable. + The author provides direct comparisons to similar methods, which increases confidence in the results. + On the experiments run, the HVM method appears to be an improvement over the two previous approaches of softmax weighting and straightforward averaging for multiple discriminators. Cons: - Performance of GANs is highly dependent on both model size and compute expended for a given experiment (see Miyato 2018 for model size and training iterations and Brock 2018 for batch size). Training multiple discriminators (in this paper up to 24) significantly increases compute cost and effective model size. No baselines controlling for the effects of larger models and batch sizes are done. - The paper lacks experiments beyond toy-ish tasks like MNIST and CIFAR10 and does not do a good job comparing to the broader established literature and contextualizing its results on certain tasks such as CIFAR10 (reporting ratios to a baseline instead of absolute values, for instance). The absolute inception score of the baseline DCGAN needs to be reported to allow for this. Is the Inception Score of the authors DCGAN implementation similar to the 6 to 6.5 reported in the literature? - Figure 3 is slightly strange in that the x axis is time to best result result instead of just overall wallclock time. Without additional information I can not determine whether it is admissible. Do all models achieve their best FID scores at similar points in training? Why is this not just a visualization of FID score as a function of wallclock time? A method which has lower variance or continues to make progress for longer than methods which begin to diverge would be unfairly represented by the current Figure. Additional comments: In section 3.1 Eq 5 appears to be wrong. The loss of the discriminator is presented in a form to be minimized so exponentiating the negative loss in the softmax weighting term as presented will do the opposite of what is desired and assign lower weight to higher loss discriminators. In Fig 6 FID scores computed on a set of 10K samples are shown. The authors appear to draw the line for the FID score of real data at 0. But since it is being estimated with only 10K samples there will be sampling error resulting in non-zero FID score. The authors should update this figure to show the box-plot for FID scores computed on random draws of 10K real samples. I have only worked with FID on Imagenet where FID scores for random batches of 10K samples are much higher than 0. I admit there is some chance the value is extremely low on CIFAR10 to make this point irrelevant, however. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the thoughtful comments and feedback . We quote the reviewer and address the respective comments below . \u201c The details and motivations of the Hypervolume Maximization ( HVM ) method [ ... ] \u201d We thank the reviewer for pointing this out . Given the limitation in space in the main text , we added two figures to Appendix G in the hope this will make more clear how the hypervolume interacts with nadir point coordinates values . \u201c Significance : Unclear . This work in isolation appears to present an improvement over prior work [ ... ] \u201d We included samples of generators trained on CelebA at 128x128 in Appendix D for different numbers of discriminators . Architectures correspond to the ones used for the 64x64 case , with 1 extra conv . layer in both models . In order to further emphasize the significance of our contributions , we would like to highlight the following points . 1-In the coverage evaluation performed on top of stacked MNIST , results reported were computed using 10k generated samples while results reported in previous literature are computed employing a sample of size 26k . Both scenarios are now included in the last uploaded version , and after repeating the evaluations using 26k images we were able to cover the maximum number of 1000 modes with 16 and 24 discriminators , and 776.8+-6.4 modes with 8 discriminators . 2-Regarding WGAN-GP , our implementation obtained worse FID-ResNet ( trained on CIFAR-10 ) than DCGAN . On the other hand , Inception Score and FID with Inception model were both better with WGAN-GP , as reported in literature . Cons 1- \u201c Performance of GANs is highly dependent on both model size and compute [ ... ] \u201d We focused in going from a single- into the multiple-discriminators case , while keeping the generator architecture and training setting unchanged . This is done to isolate the effect of the added discriminators . We acknowledge the fact that different architectures will benefit differently from the added discriminators , however we observed similar effects in all cases considered within this work . We further highlight the multiple-discriminator setting is not an alternative to other training schemes for GANs , but rather a complementary training strategy that can ( and should , in our view ) be used together with other methods . As such , our experiments are intended to ( i ) -show the effect given by the addition of discriminators , and ( ii ) -show that hypervolume maximization provides an effective \u201c policy \u201d to assign importance to different discriminators . 2- \u201c The paper lacks experiments beyond toy-ish tasks [ ... ] \u201d We decided to report Inception Scores as a ratio with respect to DCGAN since they are not directly comparable to most of the values reported in literature . In order to keep a consistent comparison with our main baseline , Neyshabur et al . ( 2017 ) , we decided to employ exactly the same architecture , which was designed for inputs of size 64x64 . We thus upscaled CIFAR-10 and this changes the scores range . Inception Score obtained by DCGAN in the 64x64 rescaled version of CIFAR-10 was 4.0697+-0.0861 ( 10 runs with 10k samples ) . Moreover , aiming to better contextualize our contribution with other approaches , we are running experiments with the 32x32 version of CIFAR-10 . 3- \u201c Figure 3 is slightly strange in that the x axis is time to best result instead of just overall wallclock time . [ ... ] \u201d Following the reviewer \u2019 s suggestion , we added the suggested plot to Appendix F. All the models are trained with a fixed budget in terms of iterations ( 93800 , corresponding to 100 epochs with a batch size of 64 ) . Our goal was indeed to emphasize a trade-off between faster convergence to a sub-optimal FID vs. later convergence to a better value . AVG and GMAN were not able to further improve FID after a few training iterations . On the other hand , HV was able to further improve the achieved best FID and MGD could take even more advantage of the available training budget , as it was able to decrease the FID almost until the end of training . Additional comments : 1- \u201c In section 3.1 Eq 5 appears to be wrong . [ ... ] \u201d Indeed the minus signs on the betas are typos on the definition of alpha_k \u2019 s ( we also double-checked our implementation and it is correct ) . We fixed this on the updated version of the manuscript . 2- \u201c In Fig 6 FID scores computed on a set of 10K samples are shown . [ ... ] \u201d We agree and to further investigate this we compared FID values obtained for real data using three different architectures , namely Inception-V3 , ResNet18 and VGG-16 . The model to calculate FID using Inception-V3 was trained on Imagenet . More specifically , we first compute the statistics of the training partition of CIFAR-10 and then compute the FID for the test set . Obtained values were 3.1796 , 0.0319 , and 0.0255 , respectively , with very small variation . As pointed out by the reviewer , since CIFAR-10 has only 10 classes , using 10k real samples to calculate FID should have a smaller sampling error in comparison with Imagenet ."}, {"review_id": "S1MB-3RcF7-2", "review_text": "This paper studies the problem of training of Generative Adversarial Networks employing a set of discriminators, as opposed to the traditional game involving one generator against a single model. Specifically, this paper claims two contributions: 1. We offer a new perspective on multiple-discriminator GAN training by framing it in the context of multi-objective optimization, and draw similarities between previous research in GANs variations and MGD, commonly employed as a general solver for multi-objective optimization. 2. We propose a new method for training multiple-discriminator GANs: Hypervolume maximization, which weighs the gradient contributions of each discriminator by its loss. Overall, the proposed method is empirical and the authors show its performance by experiments. First, I want to discuss the significance of this work (or this kind of work). As surveyed in the paper, the idea of training of Generative Adversarial Networks employing a set of discriminators has been explored by several previous work, and showed some performance improvement. However, this idea (methods along this line) is not popular in GAN applications, like image-to-image translation. I guess that the reason may be that: the significant computational cost (both in FLOPS and memory consumption) increase due to multiple discriminators destroys the benefit from the small performance improvement. Maybe I\u2019m wrong. In Appendix C Figure 10, the authors compares the wall-lock time between DCGAN, WGAN-GP and multiple-discriminator, and claims that the proposed approach is cheaper than WGAN-GP. However, WGAN-GP is more expensive due to its loss function involves gradients, while the proposed method does not. If directly compared with DCGAN, we can see an obvious increase in wall-clock time (FLOPS). In addition, the additional memory consumption is hidden there, which is a bigger problem in practice when the discriminators are large. SN-GAN have roughly the same computational cost and memory consumption of DC-GAN, but inception and FID are much higher. From my perspective, a fair comparison is under roughly the same FLOPS and memory consumption. The paper is well-written. The method is well-motivated by the multi-objective optimization perspective. Although the presentation of the Hypervolume maximization method (Section 3.2) is not clear, the resulting loss function (Equation 10) is simple, and shares the same form with other previous methods. The hyperparameter \\eta is problematic in the new formulation. The authors propose the Nadir Point Adaption to set this parameter. The authors conduct extensive experiments to compare different methods. The authors emphasize that the performance is improved with more discriminators, but it\u2019s good to contain comparison of the computational cost (FLOPS and memory consumption) at the same time. There are some small questions for the experiments. The reported FID is computed from a pretrained classifier that is specific to the dataset, instead of the commonly used Inception model. I recommend the authors also measure the FID with the Inception model, so that we have a direct comparison with existing reported scores. Overall, I found that this work is empirical, and I\u2019m not convinced by its experiments about the advantage of multiple-discriminator training, due to lacking of fair computational cost comparison with single-discriminator training. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the suggestions and constructive feedback . In the following , we quote the reviewer and respectively respond to the specific concern right below . \u201c Overall , the proposed method is empirical and the authors show its performance by experiments. \u201d We acknowledge the bulk of evidence for adopting our method is empirical . However , we specifically build upon earlier guarantees introduced by Neyshabur et al . ( 2017 ) , showing that when approximation along a sufficient number of projections ( and discriminators as a consequence ) is achieved , the distribution induced by the generator converges to the real data distribution . We thus introduce a more suitable optimization framework to ensure approximation along as many projections as possible , which is not enforced by simply optimizing for the loss average if there is some trade-off along different projections . \u201c Although the presentation of the Hypervolume maximization [ ... ] form with other previous methods. \u201d We apologize for the lack of clarity in this section . Section 3.2 is a brief review of the Hypervolume formal definition for the more general multi-solution case . We tried to make the single-solution case , employed in our work , more clear and intuitive in Section 4.1 , and illustrated it with an example in Fig.1 , in which a single solution l has its hypervolume highlighted for a given nadir point \\eta . Maximizing the highlighted volume implies minimizing l1 and l2 simultaneously . We added an illustration to Appendix F which might be useful to understand the loss behavior throughout training . \u201c First , I want to discuss the significance [ ... ] performance improvement . Maybe I \u2019 m wrong. \u201d We agree with the reviewer . The computational complexity of training GANs under a multiple discriminator setting is higher by design in terms of both FLOPS and memory , if compared with single-discriminators settings . However , such setting constitutes not an alternative approach for the recent advances in single-discriminator training , but rather a complementary method which can be used together with other methods . We would also like to make a few practical remarks regarding the use of multiple discriminators : 1-While using multiple discriminators may increase the cost of a single training run , the overall cost of training , if one accounts for the several training runs required for hyperparameters search , is reduced . We observed such a behavior in our experiments , as reported in Fig.5.The reduced variation in training outcomes makes it faster to find a stable training setting when more discriminators are employed . In our point-of-view , multiple-discriminators settings should be employed along with any training scheme of choice , if enough resources are available . As an example , which will be added to the manuscript as soon as we conclude the new experiments , adding discriminators yield the following relative improvement in terms of FID : DCGAN - 55.21 % ; LSGAN - 57.93 % ( we are currently running similar experiments on other GANs such as wGAN-GP and hingeGAN ) . 2-While the increase in cost in terms of FLOPS and memory is unavoidable , wall-clock time can be made close to single-discriminators cases since training with respect to different discriminators can be implemented in parallel . Extra cost in time introduced by other frameworks such wGAN or SNGAN can not be recovered . 3-All our experiments were performed in single-GPU settings , which supports the claim that multiple-discriminators training is practical enough to be employed in several common use cases . The main conclusions we were able to draw from our experiments is that employing multiple discriminators is a practical approach allowing us to trade extra capacity ( and thereby extra computational cost ) for higher quality and diversity of generated samples when compared to the single-discriminator equivalent setting , while avoiding mode-collapse and divergence during training for a wider set of hyperparameters . \u201c From my perspective , a fair comparison [ ... ] FLOPS and memory consumption. \u201d We understand the concern in terms of fairness of comparison and thank the reviewer for the valuable comment . However , the experiments in the paper were designed to show the effect of adding the extra complexity ( in terms of number total parameters ) specifically through increasing the number of discriminators ( and using random projections+HV loss ) in the generated samples . We wanted to show the added cost would translate into performance gain . \u201c The reported FID is computed from [ ... ] comparison with existing reported scores. \u201d We reported in Table 5 in Appendix C the Inception Score and FID with Inception model trained on Imagenet relative to DCGAN . We highlight that our scores are not directly comparable with values reported in other works since we used an upscaled version of CIFAR-10 at 64x64 in order to use the same setting as our main baseline , Neyshabur et al . ( 2017 ) ."}], "0": {"review_id": "S1MB-3RcF7-0", "review_text": "The paper investigates the use of multi-objective optimization techniques in GAN-setups where there are multiple discriminators. Using multiple discriminators was proposed in Durugkar et al, Arora et al, Neyshabur et al and others. The twist here is to focus on the Pareto front and to import multiple gradient descent and hypervolume-maximization based methods into GANs. The results are decent. The authors find that optimizing with respect to multiple discriminators increases diversity of samples for a computational cost. However, just scaling up (and carefully optimizing), can yield extremely impressive samples, https://arxiv.org/abs/1809.11096. It is unclear how the tradeoffs in optimizing against multiple discriminators stack-up against bigger GANs. From my perspective, the paper is interesting because it introduces new methods into GANs from another community. However, the results themselves are not sufficient for publication. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the feedback and taking the time for reading our paper . We are glad that the reviewer found our method interesting and hope that the following response , added to the new results included in the manuscript , will make her/him more confident about our contributions . Regarding the mentioned trade-off , we understood that the reviewer is referring to the addition of \u201c capacity \u201d ( in terms of the number of parameters ) on the discriminators side , as in the multiple-discriminators settings , or in the generator side , as in the pointed reference ( Brock et al.2018 ) .We would like to point out that such approaches have different goals : while adding capacity in the generator side is intended to yield generators able to scale to higher resolution settings and higher quality samples , multiple-discriminators are aimed at stabilizing training , avoiding common issues such as mode-collapse and divergence , which makes final performance of the generator highly dependent of careful hyperparameters tuning . If enough resources are available , both approaches should be used jointly . As we also observed multiple-discriminators training to yield higher quality and diversity when compared to their single-discriminator equivalents , we believe higher scales settings would also benefit . Regarding the insufficiency of results , we would like to respectfully highlight that we presented quantitative and qualitative results ( comparing with both single- and multiple-discriminators GANs ) in 4 datasets ( namely , MINIST , CIFAR-10 , Stacked MNIST , and CelebA ) , with consistent conclusions . We also included samples on a higher resolution for CelebA at 128x128 in Appendix D.2 , and are currently running experiments to compare different versions of GANs in the single vs. multiple discriminators settings . Some preliminary results which will be added to the manuscript as soon as we conclude the new experiments , show that adding discriminators yield the following relative improvement in terms of FID : DCGAN - 55.21 % ; LSGAN - 57.93 % ( we are currently running similar experiments on other GANs such as wGAN-GP and hingeGAN [ 2 ] ) . Moreover , we would highly appreciate if the reviewer could suggest any further experiment in order to increase her/his confidence in our results . [ 2 ] Miyato , Takeru , et al . `` Spectral normalization for generative adversarial networks . '' arXiv preprint arXiv:1802.05957 ( 2018 ) ."}, "1": {"review_id": "S1MB-3RcF7-1", "review_text": "Clarity: The work is a clear introduction/overview of this area of research. The reviewer enjoyed the connections to Multiple-Gradient Descent and clear distinctions/contrasts with previous approaches to weighting the outputs of multiple discriminators. All in all, the paper is quite clear in what its contributions are and how it differs from previous approaches. The details and motivations of the Hypervolume Maximization (HVM) method (especially as it relates to and interacts with the slack method of picking the nadir point) were a bit harder to follow intuitively given the standalone information in the paper. Originality: Adapts a technique to approximate MGD called HVM (Miranda 2016) and applies it to multi-discriminator training in GANs. As far as the reviewer is aware, this is a novel application of HVM to this task and well motivated under the MGD interpretation of the problem. Significance: Unclear. This work in isolation appears to present an improvement over prior work in this sub-field, but it is not obvious that the findings in these experiments will continue to be robust in more competitive settings. For instance, the worst performing model on CIFAR10, WGAN-GP (according to the experiments run) WGAN-GP also holds near SOTA Inception scores on CIFAR10 when appropriately tuned. Without any experimental results extending beyond toy datasets like MNIST and CIFAR10 the reviewer is not confident whether fundamental issues with GAN training are being addressed or just artifacts of small scale setups. Closely related previous work (Neyshabur 2017) scaled to 128x128 resolution on a much more difficult dataset - Imagenet Dogs but the authors did not compare in this case. Quality: Some concerns about details of experiments (see cons list and significance section for further discussion). Pros: + The work provides a clear overview of previous work on approaches using multiple discriminators. + The connections of this line of work to MGD and the re-interpretation of various other approaches in this framework is valuable. + The author provides direct comparisons to similar methods, which increases confidence in the results. + On the experiments run, the HVM method appears to be an improvement over the two previous approaches of softmax weighting and straightforward averaging for multiple discriminators. Cons: - Performance of GANs is highly dependent on both model size and compute expended for a given experiment (see Miyato 2018 for model size and training iterations and Brock 2018 for batch size). Training multiple discriminators (in this paper up to 24) significantly increases compute cost and effective model size. No baselines controlling for the effects of larger models and batch sizes are done. - The paper lacks experiments beyond toy-ish tasks like MNIST and CIFAR10 and does not do a good job comparing to the broader established literature and contextualizing its results on certain tasks such as CIFAR10 (reporting ratios to a baseline instead of absolute values, for instance). The absolute inception score of the baseline DCGAN needs to be reported to allow for this. Is the Inception Score of the authors DCGAN implementation similar to the 6 to 6.5 reported in the literature? - Figure 3 is slightly strange in that the x axis is time to best result result instead of just overall wallclock time. Without additional information I can not determine whether it is admissible. Do all models achieve their best FID scores at similar points in training? Why is this not just a visualization of FID score as a function of wallclock time? A method which has lower variance or continues to make progress for longer than methods which begin to diverge would be unfairly represented by the current Figure. Additional comments: In section 3.1 Eq 5 appears to be wrong. The loss of the discriminator is presented in a form to be minimized so exponentiating the negative loss in the softmax weighting term as presented will do the opposite of what is desired and assign lower weight to higher loss discriminators. In Fig 6 FID scores computed on a set of 10K samples are shown. The authors appear to draw the line for the FID score of real data at 0. But since it is being estimated with only 10K samples there will be sampling error resulting in non-zero FID score. The authors should update this figure to show the box-plot for FID scores computed on random draws of 10K real samples. I have only worked with FID on Imagenet where FID scores for random batches of 10K samples are much higher than 0. I admit there is some chance the value is extremely low on CIFAR10 to make this point irrelevant, however. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the thoughtful comments and feedback . We quote the reviewer and address the respective comments below . \u201c The details and motivations of the Hypervolume Maximization ( HVM ) method [ ... ] \u201d We thank the reviewer for pointing this out . Given the limitation in space in the main text , we added two figures to Appendix G in the hope this will make more clear how the hypervolume interacts with nadir point coordinates values . \u201c Significance : Unclear . This work in isolation appears to present an improvement over prior work [ ... ] \u201d We included samples of generators trained on CelebA at 128x128 in Appendix D for different numbers of discriminators . Architectures correspond to the ones used for the 64x64 case , with 1 extra conv . layer in both models . In order to further emphasize the significance of our contributions , we would like to highlight the following points . 1-In the coverage evaluation performed on top of stacked MNIST , results reported were computed using 10k generated samples while results reported in previous literature are computed employing a sample of size 26k . Both scenarios are now included in the last uploaded version , and after repeating the evaluations using 26k images we were able to cover the maximum number of 1000 modes with 16 and 24 discriminators , and 776.8+-6.4 modes with 8 discriminators . 2-Regarding WGAN-GP , our implementation obtained worse FID-ResNet ( trained on CIFAR-10 ) than DCGAN . On the other hand , Inception Score and FID with Inception model were both better with WGAN-GP , as reported in literature . Cons 1- \u201c Performance of GANs is highly dependent on both model size and compute [ ... ] \u201d We focused in going from a single- into the multiple-discriminators case , while keeping the generator architecture and training setting unchanged . This is done to isolate the effect of the added discriminators . We acknowledge the fact that different architectures will benefit differently from the added discriminators , however we observed similar effects in all cases considered within this work . We further highlight the multiple-discriminator setting is not an alternative to other training schemes for GANs , but rather a complementary training strategy that can ( and should , in our view ) be used together with other methods . As such , our experiments are intended to ( i ) -show the effect given by the addition of discriminators , and ( ii ) -show that hypervolume maximization provides an effective \u201c policy \u201d to assign importance to different discriminators . 2- \u201c The paper lacks experiments beyond toy-ish tasks [ ... ] \u201d We decided to report Inception Scores as a ratio with respect to DCGAN since they are not directly comparable to most of the values reported in literature . In order to keep a consistent comparison with our main baseline , Neyshabur et al . ( 2017 ) , we decided to employ exactly the same architecture , which was designed for inputs of size 64x64 . We thus upscaled CIFAR-10 and this changes the scores range . Inception Score obtained by DCGAN in the 64x64 rescaled version of CIFAR-10 was 4.0697+-0.0861 ( 10 runs with 10k samples ) . Moreover , aiming to better contextualize our contribution with other approaches , we are running experiments with the 32x32 version of CIFAR-10 . 3- \u201c Figure 3 is slightly strange in that the x axis is time to best result instead of just overall wallclock time . [ ... ] \u201d Following the reviewer \u2019 s suggestion , we added the suggested plot to Appendix F. All the models are trained with a fixed budget in terms of iterations ( 93800 , corresponding to 100 epochs with a batch size of 64 ) . Our goal was indeed to emphasize a trade-off between faster convergence to a sub-optimal FID vs. later convergence to a better value . AVG and GMAN were not able to further improve FID after a few training iterations . On the other hand , HV was able to further improve the achieved best FID and MGD could take even more advantage of the available training budget , as it was able to decrease the FID almost until the end of training . Additional comments : 1- \u201c In section 3.1 Eq 5 appears to be wrong . [ ... ] \u201d Indeed the minus signs on the betas are typos on the definition of alpha_k \u2019 s ( we also double-checked our implementation and it is correct ) . We fixed this on the updated version of the manuscript . 2- \u201c In Fig 6 FID scores computed on a set of 10K samples are shown . [ ... ] \u201d We agree and to further investigate this we compared FID values obtained for real data using three different architectures , namely Inception-V3 , ResNet18 and VGG-16 . The model to calculate FID using Inception-V3 was trained on Imagenet . More specifically , we first compute the statistics of the training partition of CIFAR-10 and then compute the FID for the test set . Obtained values were 3.1796 , 0.0319 , and 0.0255 , respectively , with very small variation . As pointed out by the reviewer , since CIFAR-10 has only 10 classes , using 10k real samples to calculate FID should have a smaller sampling error in comparison with Imagenet ."}, "2": {"review_id": "S1MB-3RcF7-2", "review_text": "This paper studies the problem of training of Generative Adversarial Networks employing a set of discriminators, as opposed to the traditional game involving one generator against a single model. Specifically, this paper claims two contributions: 1. We offer a new perspective on multiple-discriminator GAN training by framing it in the context of multi-objective optimization, and draw similarities between previous research in GANs variations and MGD, commonly employed as a general solver for multi-objective optimization. 2. We propose a new method for training multiple-discriminator GANs: Hypervolume maximization, which weighs the gradient contributions of each discriminator by its loss. Overall, the proposed method is empirical and the authors show its performance by experiments. First, I want to discuss the significance of this work (or this kind of work). As surveyed in the paper, the idea of training of Generative Adversarial Networks employing a set of discriminators has been explored by several previous work, and showed some performance improvement. However, this idea (methods along this line) is not popular in GAN applications, like image-to-image translation. I guess that the reason may be that: the significant computational cost (both in FLOPS and memory consumption) increase due to multiple discriminators destroys the benefit from the small performance improvement. Maybe I\u2019m wrong. In Appendix C Figure 10, the authors compares the wall-lock time between DCGAN, WGAN-GP and multiple-discriminator, and claims that the proposed approach is cheaper than WGAN-GP. However, WGAN-GP is more expensive due to its loss function involves gradients, while the proposed method does not. If directly compared with DCGAN, we can see an obvious increase in wall-clock time (FLOPS). In addition, the additional memory consumption is hidden there, which is a bigger problem in practice when the discriminators are large. SN-GAN have roughly the same computational cost and memory consumption of DC-GAN, but inception and FID are much higher. From my perspective, a fair comparison is under roughly the same FLOPS and memory consumption. The paper is well-written. The method is well-motivated by the multi-objective optimization perspective. Although the presentation of the Hypervolume maximization method (Section 3.2) is not clear, the resulting loss function (Equation 10) is simple, and shares the same form with other previous methods. The hyperparameter \\eta is problematic in the new formulation. The authors propose the Nadir Point Adaption to set this parameter. The authors conduct extensive experiments to compare different methods. The authors emphasize that the performance is improved with more discriminators, but it\u2019s good to contain comparison of the computational cost (FLOPS and memory consumption) at the same time. There are some small questions for the experiments. The reported FID is computed from a pretrained classifier that is specific to the dataset, instead of the commonly used Inception model. I recommend the authors also measure the FID with the Inception model, so that we have a direct comparison with existing reported scores. Overall, I found that this work is empirical, and I\u2019m not convinced by its experiments about the advantage of multiple-discriminator training, due to lacking of fair computational cost comparison with single-discriminator training. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the suggestions and constructive feedback . In the following , we quote the reviewer and respectively respond to the specific concern right below . \u201c Overall , the proposed method is empirical and the authors show its performance by experiments. \u201d We acknowledge the bulk of evidence for adopting our method is empirical . However , we specifically build upon earlier guarantees introduced by Neyshabur et al . ( 2017 ) , showing that when approximation along a sufficient number of projections ( and discriminators as a consequence ) is achieved , the distribution induced by the generator converges to the real data distribution . We thus introduce a more suitable optimization framework to ensure approximation along as many projections as possible , which is not enforced by simply optimizing for the loss average if there is some trade-off along different projections . \u201c Although the presentation of the Hypervolume maximization [ ... ] form with other previous methods. \u201d We apologize for the lack of clarity in this section . Section 3.2 is a brief review of the Hypervolume formal definition for the more general multi-solution case . We tried to make the single-solution case , employed in our work , more clear and intuitive in Section 4.1 , and illustrated it with an example in Fig.1 , in which a single solution l has its hypervolume highlighted for a given nadir point \\eta . Maximizing the highlighted volume implies minimizing l1 and l2 simultaneously . We added an illustration to Appendix F which might be useful to understand the loss behavior throughout training . \u201c First , I want to discuss the significance [ ... ] performance improvement . Maybe I \u2019 m wrong. \u201d We agree with the reviewer . The computational complexity of training GANs under a multiple discriminator setting is higher by design in terms of both FLOPS and memory , if compared with single-discriminators settings . However , such setting constitutes not an alternative approach for the recent advances in single-discriminator training , but rather a complementary method which can be used together with other methods . We would also like to make a few practical remarks regarding the use of multiple discriminators : 1-While using multiple discriminators may increase the cost of a single training run , the overall cost of training , if one accounts for the several training runs required for hyperparameters search , is reduced . We observed such a behavior in our experiments , as reported in Fig.5.The reduced variation in training outcomes makes it faster to find a stable training setting when more discriminators are employed . In our point-of-view , multiple-discriminators settings should be employed along with any training scheme of choice , if enough resources are available . As an example , which will be added to the manuscript as soon as we conclude the new experiments , adding discriminators yield the following relative improvement in terms of FID : DCGAN - 55.21 % ; LSGAN - 57.93 % ( we are currently running similar experiments on other GANs such as wGAN-GP and hingeGAN ) . 2-While the increase in cost in terms of FLOPS and memory is unavoidable , wall-clock time can be made close to single-discriminators cases since training with respect to different discriminators can be implemented in parallel . Extra cost in time introduced by other frameworks such wGAN or SNGAN can not be recovered . 3-All our experiments were performed in single-GPU settings , which supports the claim that multiple-discriminators training is practical enough to be employed in several common use cases . The main conclusions we were able to draw from our experiments is that employing multiple discriminators is a practical approach allowing us to trade extra capacity ( and thereby extra computational cost ) for higher quality and diversity of generated samples when compared to the single-discriminator equivalent setting , while avoiding mode-collapse and divergence during training for a wider set of hyperparameters . \u201c From my perspective , a fair comparison [ ... ] FLOPS and memory consumption. \u201d We understand the concern in terms of fairness of comparison and thank the reviewer for the valuable comment . However , the experiments in the paper were designed to show the effect of adding the extra complexity ( in terms of number total parameters ) specifically through increasing the number of discriminators ( and using random projections+HV loss ) in the generated samples . We wanted to show the added cost would translate into performance gain . \u201c The reported FID is computed from [ ... ] comparison with existing reported scores. \u201d We reported in Table 5 in Appendix C the Inception Score and FID with Inception model trained on Imagenet relative to DCGAN . We highlight that our scores are not directly comparable with values reported in other works since we used an upscaled version of CIFAR-10 at 64x64 in order to use the same setting as our main baseline , Neyshabur et al . ( 2017 ) ."}}