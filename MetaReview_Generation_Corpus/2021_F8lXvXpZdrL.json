{"year": "2021", "forum": "F8lXvXpZdrL", "title": "Reintroducing Straight-Through Estimators as Principled Methods for Stochastic Binary Networks", "decision": "Reject", "meta_review": "This paper collects a variety of results that cast straight-through estimators as arising as principled methods that make a linearization assumption on the loss for functions with binary arguments. R1 & R3 recommended against acceptance, citing clarity concerns and a lack of novelty. R2 & R4 recommended acceptance, but had low confidence. This paper had uncharacteristically low confidence on behalf of the reviewers, and this is my fault. I apologize to the authors for this. \n\nI have read the paper myself. I believe that this paper contains many interesting ideas, but I agree with R1 & R3 that the paper suffers from clarity issues. Unfortunately, these issues persist in the recent revisions, despite having been asked by R1 & R3 to improve the clarity. The authors asked for concrete reference points. Here are some:\n\n- \"proxy function\" is not well-defined, despite being critical to the arguments.\n- deterministic ST is not defined clearly before it is discussed.\n-  The section structure of Sec 2 could be improved. At the moment it seems to flow from the loss function to the standard ST algorithm through to a disjointed list of questions addressed in the paper.\n- The section titles are not particularly informative.\n- It is difficult to know which results are known and which results are new.\n\nIn general, I believe this work could benefit from a significant restructuring. It would be best to delineate preceding work in its own section, then lay out the new results, making sure that all of the important concepts are clearly defined. I think many of these results are valuable for the community, but the current draft makes it challenging for these great ideas to reach their full potential.\n", "reviews": [{"review_id": "F8lXvXpZdrL-0", "review_text": "The paper reintroduces the straight-through estimator with bias-variance analysis . It further discusses its relationship with some constrained optimization methods in convex optimization and In general , the novelty of the paper on the methodology side is not high . Its value may lie in the theoretical analysis of an existing method . However , the current theoretical analysis is not clear for the following issues : - On page 3 , it says `` can not interchange the gradient and the expectation in z '' . But z is a continuous variable , why it can not ? - There is no clear explanation of what does dL ( x ) /dx mean . x is discrete , so this notation without re-definition is incorrect . - On page 3 , it is unclear what `` define now dx/da = 2F ' ( a ) '' mean . Why should it be defined in this way ? - On page 4 III ) , why g ( x ) is assumed to be Lipschitz continuous ? - On page 8 , eq ( 18 ) , this metric seems strange since the ratio of expectation is not the expectation of ratio . And it only measures the angle between two vectors . Why the L2 norm can not be used here ? In general , the main paper lacks formal theory statements , while the informal statement does not clearly answer the questions it comes up with . Another major issue is the writing . The paper does not have a conclusion/discussion part which makes it incomplete . And the main paper has several missing citations/references with ( ? ? ) . In figure 1 caption , it is ELBO not ELOB . With the concerns listed above , the paper in its current version looks not fully ready for publication . ==POST-REBUTTAL UPDATES== Thanks to the authors for the response and the efforts in the updated draft . The updated paper improves writing . The response resolves a part of the queries . The viewer yet believes the page limits should not be a good excuse to squeeze necessary information into the appendix , otherwise , as AR2 suggests , it may be more proper for other venues . I raised my rating according to the author 's response .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for you questions , they will help us to make the paper more clear and accessible . They are quite simple to resolve . We will answer directly here and incorporate in a cumulative update of the paper ( to keep the discussion finite , we want to upload a revision only once ) . * On page 3 , it says `` can not interchange the gradient and the expectation in z '' . But z is a continuous variable , why it can not ? It actually says ( line 133 ) `` since it [ loss ] is not continuously differentiable we can not ... '' . Indeed , to make an interchange a suitable theorem is needed such as Leibniz integral rule ( https : //en.wikipedia.org/wiki/Leibniz_integral_rule ) that requires the function to be continuously differentiable . The fact that z is a continuous variable is not sufficient . A simple counter-example is that expectation of a step function with a continuous noise density is the noise cdf ( and is differentiable ) but the gradient of a step function is zero almost everywhere and so its expectation . * There is no clear explanation of what does $ dL ( x ) /dx $ mean . x is discrete , so this notation without re-definition is incorrect . In our context $ \\mathcal { L } $ is defined not by a look-up table for all possible discrete configurations but by composing linear and non-linear functions such as softmax ( W2 * ReLu ( W1 x ) ) . That is , it is defined for any real-valued $ x $ . We evaluate it only at discrete $ x $ , but we compute the derivative of the obvious continuous extension at that $ x $ . Please note ( line 154 ) `` provided that $ f $ alone is differentiable '' , here $ f $ is a typo and should be $ \\mathcal { L } $ . * On page 3 , it is unclear what `` define now dx/da = 2F ' ( a ) '' mean . Why should it be defined in this way ? If we define this Jacobian exactly in this way , the estimator that was obtained in ( 8 ) can be written as a chain rule ( 9 ) and computed using familiar backpropagation . * On page 4 III ) , why g ( x ) is assumed to be Lipschitz continuous ? For example can L ( x ) = x^3 ? The precise claim in Property III can be used for informal reasoning as follows : if g is Lipschitz continuous and gradients are strong enough ( as specified ) , then the estimator will be good . If it is not Lipschitz continuous , one should consider simplifying the loss function or applying a better estimator . * On page 8 , eq ( 18 ) , this metric seems strange since the ratio of expectation is not the expectation of ratio . And it only measures the angle between two vectors . Why the L2 norm can not be used here ? The equation ( 18 ) is motivated in more details in Appendix D.1 ( lines 1141-1162 ) . We intend to significantly revise this experiment , including measuring root mean squared error and the expected cosine similarity . The idea was that the expected improvement metric ( 18 ) is more natural for use in optimization , but perhaps it is more difficult to present and could be less clear . * the main paper lacks formal theory statements Since you write `` the main paper '' , you probably have noticed that all statements made in Section 2 are formally detailed in the appendix B and all formal statements made in Section 3 are proven in Appendix C ? The paper is organized in this way in order to give an accessible overview in the main part . We believe the statements in Section 2 are sufficiently precise . Writing all formal statements in the main paper would clutter it and reduce readability . Could you please clarify to us and other reviewers whether the comment is about layout or indeed about a lack of formal statements ( which specifically ) . * Conclusion/discussion We will be happy to add a conclusion using the 9th page available now . There was no space for it in 8 pages . * Novelty is not high While we do not dare to argue what is a high novelty , should not all the new facts and isights about ST that we obtained by the way of analysis be considered as novelty ? There is indeed lots of empirical applications and heuristics around ST ( and probably a dozen of new submissions to this ICLR using it ) , but very little theoretical understanding and no systematic study since the introduction in 2012 . Is n't our work timely and strikingly novel in that regard ?"}, {"review_id": "F8lXvXpZdrL-1", "review_text": "1.Summary & contributions This paper analyzes a number of existing straight through ( ST ) estimators and connects them using a unified view as estimators for stochastic binary networks ( SBNs ) with Bernoulli weights . Different estimators can be seen as design choices , either relating to the model or optimization procedure . This provides insights in the specific assumptions ( implicitly ) made by these estimators and allows to analyze their bias and performance . The paper proposes an extension of ST for deep SBNs to binary weights and conducts experiments to evaluate the quality of ST gradient estimates , showing that the reliability increases with the size of the ( boolean ) latent space for a VAE , and providing optimization results for a Deep SBN . 2.Strengths & weaknesses Strengths : This paper unifies a number of straight through estimators under a common framework which helps in motivating previously considered 'ad-hoc ' rules for gradient backpropagation , and helps understanding of the conditions which may affect the bias/performance of the estimators . The paper is technical but written in such a way that it is still relatively easy to follow , by keeping notations simple ( yet clear ) and deferring detailed derivations/proofs to the appendix ( which I did not go through in detail ) . The paper shows theoretically and experimentally that different design choices for the estimator are possible , as long as different parts ( model , initialization and training ) are well aligned . Weaknesses : As a weakness , the paper feels a bit as an enumeration of related but separate contributions ( summed up in the paragraph 'contributions ' ) . The experiments mainly concern section 4 and show somewhat the effect of Bias Analysis IV ) , but it would improve the paper if ( toy ) experiments would have been conducted relating to the other analysis in section 2.2 as well , which could then justify the practical relevance of these results . While the paper motivates the reintroduction of ST as principled methods , the experiments give some insights but do not fully convince of the practical use of the proposed extensions . It is positive that the paper is largely self-contained , but this makes it a bit difficult to distinguish novel derivations from recapitulations of results from previous work . Also , the paper lacks a discussion section and as a result feels a bit unfinished . 3.Recommendation My current recommendation is to accept the paper . 4.Arguments for recommendation This paper helps in the theoretical understanding and unifying view of a variety of straight through estimators , which may help advancing these estimators for training networks with binary weights and activations , which is a relevant and difficult problem . 5.Questions to authors - Why is a discussion section omitted ? - Figure 1 : looking at the y-axis for the top row , it seems that both ARM and ST get worse results with 256 bits than 8 or 64 bits , which hints at underfitting . How does this affect the conclusions ? 6.Additional feedback Minor comments - Line 152 : where does the symbol f for loss come from ? - Line 305 : a reference is missing : ? ? - Line 331,5 : reminder - > remainder ? - Line 372 : ELOB - > ELBO ? Or -ELBO ( negative ELBO ) ? - Some grammar could be improved , e.g.in abstract 'we \u2026 obtains , \u2026 , explains '", "rating": "7: Good paper, accept", "reply_text": "Thanks for you detailed comments . We tried to address the pointed weaknesses and questions . * enumeration of related but separate contributions We agree that some of the contributions are not in the main focus , but we view them as useful and connected to the common topic of ST estimators . * it would improve the paper if ( toy ) experiments would have been conducted relating to the other analysis in section 2.2 as well , which could then justify the practical relevance of these results . The improved experiments on stochastic auto-encoders now verify more predictions from the analysis 2.2 : we compare bias of different variants of ST estimators and demonstrate how it translates to the learning behavior and an accumulated bias . One experimentally validated consequence that is made clear is that identity ST is not suitable for activations but is perfectly suitable for weights , as we derived and explained . We further explained better our conclusions from the deep SBN experiment . * While the paper motivates the reintroduction of ST as principled methods , the experiments give some insights but do not fully convince of the practical use of the proposed extensions . At present , we only proposed how to synthesize correct ( from the point of view of our derivation ) methods that work well for any given noise distribution and latent weights model . We would agree with the reviewer about the Bayesian learning extension , for which we did not demonstrate improved results . We position it rather as assigning a meaning to the combination of latent weights and weight decay . We do not offer experiments with it partially because some toy experiments have been already demonstrated by Meng et al . ( 2020 ) and partially because making it work in a larger scale appears to require some further research . * It is positive that the paper is largely self-contained , but this makes it a bit difficult to distinguish novel derivations from recapitulations of results from previous work . We try to accurately cite the previous work . One our major motivation ( and we believe a contribution as well ) is to review this prior work in a clear way and systematize together different aspects . * ( Q1 ) Discussion section We extended the discussion of experiments and added the conclusion section * ( Q2 ) Figure 1 : looking at the y-axis for the top row , it seems that both ARM and ST get worse results with 256 bits than 8 or 64 bits , which hints at underfitting . How does this affect the conclusions ? This was indeed the case in the initial submitted version . Somewhat counterintuitive , it was actually very hard to achieve a comparable training loss using more bits ( even with a longer training budget ) . We think the reason for this is the partial posterior collapse in VAE with discrete latent states . We discuss the issue in more detail in Appendix D.1 . Basically , increasing the latent space dimensionality did not lead to the solutions that would really make use of these latent bits ( more bits are found in collapse ) but was making the optimization problem harder . This is the main reason why we switched to plain stochastic auto-encoders , in order to verify the theoretically predicted dependence on the bit length . It is seen now that the experiment in the revision ( Fig.1 ) does not have an underfitting issue . All y-axis are precisely aligned and with more latent bits lower training loss is reached . Please let us know if you have further questions / suggestions ."}, {"review_id": "F8lXvXpZdrL-2", "review_text": "Summary : The paper presents a principled derivation and analysis of the straight-through ( ST ) estimator , which is often used to train networks with binary weights and activations . A strong point of the paper is that the ST estimator is often used to train networks with binary weights and activations . Hence , a more theoretical investigation can be helpful to better understand the ST estimator . Furthermore , the paper seems theoretically sound . However , my assessment is unconfident due to lacking expertise in this area . The empirical experiments confirm that the ST performance improves as the number of latent bits is increased as suggested by the theoretical analysis . Another strong point is that the paper will make code available on Github , which can improve the reproducibility of the experiments . A weak point of the paper might be its limited potential impact on future works since it mainly provides an analysis of prior empirical ST approaches . Furthermore , a substantial amount of content is only described in the supplementary material . A venue that allows longer submission may be a better fit for this work . Minor important points : - Missing reference in Line 305 .", "rating": "7: Good paper, accept", "reply_text": "Thanks for you comments . We would like to say something in response to the mentioned weaknesses . * Limited potential impact Please see whether our updated experiments section improves on the potential impact in you view . We try to obtain more experimental insights confirming the analysis and helping to chose and apply ST methods with clarity and awareness of the limitations . Furthermore , we hope our work will facilitate the development of improved methods . * A substantial amount of content is only described in the supplementary material . A venue that allows longer submission may be a better fit for this work This is not uncommon for ICLR papers in our experience ( e.g.Cong et al. , 2019 `` GO Gradient for expectation based objectives '' , which we cite ) . To be honest , we find it rather hard to publish theoretical results reaching simultaneously a significant contribution , high clarity and a short length . And only the length of the technical part is relaxable in our view . We would appreciate suggestions ( from any of the reviewers ) for a better publishing strategy / other venues in machine learning that allow longer submissions . * Missing reference in Line 305 Fixed in the revision . The reference was to the Appendix C.2 `` Latent Weight Decay Implements Variational Bayesian Learning ''"}, {"review_id": "F8lXvXpZdrL-3", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper presents a systematic way of analysing straight through estimators and derive ST estimators for Stochastic Binary Networks . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . The paper is theoretically strong . 2.The paper covers a large body of relevant work , covering theory behind straight through estimators and stochastic binary networks . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . The novelty of the paper is n't clear , if this paper is an analysis paper , the empirical evidence is weak . 2.The paper is very hard to read , and it is difficult to understand the clear motivation . The paper is too dense while missing any key takeaway point . It misses key details in the experiment section . 3.The utility of the proposed MD estimator is unclear , it would be helpful if the authors would clarify the interpretation of Table 1 with their write up under `` Classification with Deep SBN '' : the authors state that their method performs as well as the empirical ST , while the table shows it performs worse than their baselines . 4.Overall , the experiment section is scattered , with hard to understand goals . For example , the takeaway from Figure 1 bottom is difficult to understand . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Typos : Line 305 ? ? The paper needs major revisions in terms of notation issues : the vectors should be bold , to distinguish the from scalars . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions : Please address the cons mentioned above .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear R3 , thanks for you comments . * Novelty is not clear . Motivation is not clear . Hard to read . Too dense . We already did our best to present the systematization and the key results in a clear and accessible way . We can not omit any part of the contribution to make it less dense either in order to devote more space to e.g.motivation . * Clarify the interpretation of Table 1 . Performs worse than baselines . Our observations from Table 1 are summarized in lines 456-467 . There are two kinds of baselines : ST-based methods and a method of Peters and Welling ( 2018 ) also derived for SBNs . First , we observe from a comparison in the deterministic mode ( 'det ' column ) that our derived ST method is not worse than empirical ST setups found by trial and error . The differences of 0.1-0.2 % can be neglected ( it is within an effect of random initialization ) . The derived ST takes care of the choice of the noise distribution and proper updates and is clearly interpretable . If the same networks we trained are tested in the stochastic mode ( 10-sample ) , there is a clear boost of performance , indicating an advantage of SBN models . The takeaway message here is that mirror descent , initialization , and the link between noise and gradient components all work correctly . While some empirical ST methods were already guessed well , they can be replaced now with a more transparent approach . Second , there is indeed some gap to Peters and Welling ( 2018 ) in the stochastic mode ( e.g.our 10-sample versus their 16-sample ) . Nevertheless , the results are very similar considering that the baseline uses a different estimation method , an initialization from a pretrained model and an early stopping ( we believe otherwise they would overfit to the relaxation of the SBN considered ) . The takeaway message here is that ST can be considered in the context of SBN models as a simple but proper baseline . Since we achieve near 100 % training accuracy , the optimization fully succeeds and thus the bias of ST is tolerable . * The empirical evidence is weak We plan to update the experiment on auto-encoders ( within this review process ) to provide a more clear evidence about properties of STE to address concerns and suggestions by R1 and R4 . A clear understanding of STE and its properties is the main focus of the paper ( both theoretical and experimental ) . * Misses key details in the experiment section . We will appreciate pointing out which details do you find missing relative to the specifications in Appendix D. * Notation issues : the vectors should be bold , to distinguish the from scalars Thanks for bringing this up . We will give a try to the bold notation . At present , we rarely work with scalars unless they are coordinates of vectors such as x_i , in which case the subindex serves as a distinction . We will check consistency of the vector and coordinate-wise notation in the paper . We believe this is a matter of a minor revision in any case . * Line 305 ? ? Thanks , this refers to the Appendix C.2 , where Prop.2 is proven , i.e.this is a part of the present contribution and not an external reference ."}], "0": {"review_id": "F8lXvXpZdrL-0", "review_text": "The paper reintroduces the straight-through estimator with bias-variance analysis . It further discusses its relationship with some constrained optimization methods in convex optimization and In general , the novelty of the paper on the methodology side is not high . Its value may lie in the theoretical analysis of an existing method . However , the current theoretical analysis is not clear for the following issues : - On page 3 , it says `` can not interchange the gradient and the expectation in z '' . But z is a continuous variable , why it can not ? - There is no clear explanation of what does dL ( x ) /dx mean . x is discrete , so this notation without re-definition is incorrect . - On page 3 , it is unclear what `` define now dx/da = 2F ' ( a ) '' mean . Why should it be defined in this way ? - On page 4 III ) , why g ( x ) is assumed to be Lipschitz continuous ? - On page 8 , eq ( 18 ) , this metric seems strange since the ratio of expectation is not the expectation of ratio . And it only measures the angle between two vectors . Why the L2 norm can not be used here ? In general , the main paper lacks formal theory statements , while the informal statement does not clearly answer the questions it comes up with . Another major issue is the writing . The paper does not have a conclusion/discussion part which makes it incomplete . And the main paper has several missing citations/references with ( ? ? ) . In figure 1 caption , it is ELBO not ELOB . With the concerns listed above , the paper in its current version looks not fully ready for publication . ==POST-REBUTTAL UPDATES== Thanks to the authors for the response and the efforts in the updated draft . The updated paper improves writing . The response resolves a part of the queries . The viewer yet believes the page limits should not be a good excuse to squeeze necessary information into the appendix , otherwise , as AR2 suggests , it may be more proper for other venues . I raised my rating according to the author 's response .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for you questions , they will help us to make the paper more clear and accessible . They are quite simple to resolve . We will answer directly here and incorporate in a cumulative update of the paper ( to keep the discussion finite , we want to upload a revision only once ) . * On page 3 , it says `` can not interchange the gradient and the expectation in z '' . But z is a continuous variable , why it can not ? It actually says ( line 133 ) `` since it [ loss ] is not continuously differentiable we can not ... '' . Indeed , to make an interchange a suitable theorem is needed such as Leibniz integral rule ( https : //en.wikipedia.org/wiki/Leibniz_integral_rule ) that requires the function to be continuously differentiable . The fact that z is a continuous variable is not sufficient . A simple counter-example is that expectation of a step function with a continuous noise density is the noise cdf ( and is differentiable ) but the gradient of a step function is zero almost everywhere and so its expectation . * There is no clear explanation of what does $ dL ( x ) /dx $ mean . x is discrete , so this notation without re-definition is incorrect . In our context $ \\mathcal { L } $ is defined not by a look-up table for all possible discrete configurations but by composing linear and non-linear functions such as softmax ( W2 * ReLu ( W1 x ) ) . That is , it is defined for any real-valued $ x $ . We evaluate it only at discrete $ x $ , but we compute the derivative of the obvious continuous extension at that $ x $ . Please note ( line 154 ) `` provided that $ f $ alone is differentiable '' , here $ f $ is a typo and should be $ \\mathcal { L } $ . * On page 3 , it is unclear what `` define now dx/da = 2F ' ( a ) '' mean . Why should it be defined in this way ? If we define this Jacobian exactly in this way , the estimator that was obtained in ( 8 ) can be written as a chain rule ( 9 ) and computed using familiar backpropagation . * On page 4 III ) , why g ( x ) is assumed to be Lipschitz continuous ? For example can L ( x ) = x^3 ? The precise claim in Property III can be used for informal reasoning as follows : if g is Lipschitz continuous and gradients are strong enough ( as specified ) , then the estimator will be good . If it is not Lipschitz continuous , one should consider simplifying the loss function or applying a better estimator . * On page 8 , eq ( 18 ) , this metric seems strange since the ratio of expectation is not the expectation of ratio . And it only measures the angle between two vectors . Why the L2 norm can not be used here ? The equation ( 18 ) is motivated in more details in Appendix D.1 ( lines 1141-1162 ) . We intend to significantly revise this experiment , including measuring root mean squared error and the expected cosine similarity . The idea was that the expected improvement metric ( 18 ) is more natural for use in optimization , but perhaps it is more difficult to present and could be less clear . * the main paper lacks formal theory statements Since you write `` the main paper '' , you probably have noticed that all statements made in Section 2 are formally detailed in the appendix B and all formal statements made in Section 3 are proven in Appendix C ? The paper is organized in this way in order to give an accessible overview in the main part . We believe the statements in Section 2 are sufficiently precise . Writing all formal statements in the main paper would clutter it and reduce readability . Could you please clarify to us and other reviewers whether the comment is about layout or indeed about a lack of formal statements ( which specifically ) . * Conclusion/discussion We will be happy to add a conclusion using the 9th page available now . There was no space for it in 8 pages . * Novelty is not high While we do not dare to argue what is a high novelty , should not all the new facts and isights about ST that we obtained by the way of analysis be considered as novelty ? There is indeed lots of empirical applications and heuristics around ST ( and probably a dozen of new submissions to this ICLR using it ) , but very little theoretical understanding and no systematic study since the introduction in 2012 . Is n't our work timely and strikingly novel in that regard ?"}, "1": {"review_id": "F8lXvXpZdrL-1", "review_text": "1.Summary & contributions This paper analyzes a number of existing straight through ( ST ) estimators and connects them using a unified view as estimators for stochastic binary networks ( SBNs ) with Bernoulli weights . Different estimators can be seen as design choices , either relating to the model or optimization procedure . This provides insights in the specific assumptions ( implicitly ) made by these estimators and allows to analyze their bias and performance . The paper proposes an extension of ST for deep SBNs to binary weights and conducts experiments to evaluate the quality of ST gradient estimates , showing that the reliability increases with the size of the ( boolean ) latent space for a VAE , and providing optimization results for a Deep SBN . 2.Strengths & weaknesses Strengths : This paper unifies a number of straight through estimators under a common framework which helps in motivating previously considered 'ad-hoc ' rules for gradient backpropagation , and helps understanding of the conditions which may affect the bias/performance of the estimators . The paper is technical but written in such a way that it is still relatively easy to follow , by keeping notations simple ( yet clear ) and deferring detailed derivations/proofs to the appendix ( which I did not go through in detail ) . The paper shows theoretically and experimentally that different design choices for the estimator are possible , as long as different parts ( model , initialization and training ) are well aligned . Weaknesses : As a weakness , the paper feels a bit as an enumeration of related but separate contributions ( summed up in the paragraph 'contributions ' ) . The experiments mainly concern section 4 and show somewhat the effect of Bias Analysis IV ) , but it would improve the paper if ( toy ) experiments would have been conducted relating to the other analysis in section 2.2 as well , which could then justify the practical relevance of these results . While the paper motivates the reintroduction of ST as principled methods , the experiments give some insights but do not fully convince of the practical use of the proposed extensions . It is positive that the paper is largely self-contained , but this makes it a bit difficult to distinguish novel derivations from recapitulations of results from previous work . Also , the paper lacks a discussion section and as a result feels a bit unfinished . 3.Recommendation My current recommendation is to accept the paper . 4.Arguments for recommendation This paper helps in the theoretical understanding and unifying view of a variety of straight through estimators , which may help advancing these estimators for training networks with binary weights and activations , which is a relevant and difficult problem . 5.Questions to authors - Why is a discussion section omitted ? - Figure 1 : looking at the y-axis for the top row , it seems that both ARM and ST get worse results with 256 bits than 8 or 64 bits , which hints at underfitting . How does this affect the conclusions ? 6.Additional feedback Minor comments - Line 152 : where does the symbol f for loss come from ? - Line 305 : a reference is missing : ? ? - Line 331,5 : reminder - > remainder ? - Line 372 : ELOB - > ELBO ? Or -ELBO ( negative ELBO ) ? - Some grammar could be improved , e.g.in abstract 'we \u2026 obtains , \u2026 , explains '", "rating": "7: Good paper, accept", "reply_text": "Thanks for you detailed comments . We tried to address the pointed weaknesses and questions . * enumeration of related but separate contributions We agree that some of the contributions are not in the main focus , but we view them as useful and connected to the common topic of ST estimators . * it would improve the paper if ( toy ) experiments would have been conducted relating to the other analysis in section 2.2 as well , which could then justify the practical relevance of these results . The improved experiments on stochastic auto-encoders now verify more predictions from the analysis 2.2 : we compare bias of different variants of ST estimators and demonstrate how it translates to the learning behavior and an accumulated bias . One experimentally validated consequence that is made clear is that identity ST is not suitable for activations but is perfectly suitable for weights , as we derived and explained . We further explained better our conclusions from the deep SBN experiment . * While the paper motivates the reintroduction of ST as principled methods , the experiments give some insights but do not fully convince of the practical use of the proposed extensions . At present , we only proposed how to synthesize correct ( from the point of view of our derivation ) methods that work well for any given noise distribution and latent weights model . We would agree with the reviewer about the Bayesian learning extension , for which we did not demonstrate improved results . We position it rather as assigning a meaning to the combination of latent weights and weight decay . We do not offer experiments with it partially because some toy experiments have been already demonstrated by Meng et al . ( 2020 ) and partially because making it work in a larger scale appears to require some further research . * It is positive that the paper is largely self-contained , but this makes it a bit difficult to distinguish novel derivations from recapitulations of results from previous work . We try to accurately cite the previous work . One our major motivation ( and we believe a contribution as well ) is to review this prior work in a clear way and systematize together different aspects . * ( Q1 ) Discussion section We extended the discussion of experiments and added the conclusion section * ( Q2 ) Figure 1 : looking at the y-axis for the top row , it seems that both ARM and ST get worse results with 256 bits than 8 or 64 bits , which hints at underfitting . How does this affect the conclusions ? This was indeed the case in the initial submitted version . Somewhat counterintuitive , it was actually very hard to achieve a comparable training loss using more bits ( even with a longer training budget ) . We think the reason for this is the partial posterior collapse in VAE with discrete latent states . We discuss the issue in more detail in Appendix D.1 . Basically , increasing the latent space dimensionality did not lead to the solutions that would really make use of these latent bits ( more bits are found in collapse ) but was making the optimization problem harder . This is the main reason why we switched to plain stochastic auto-encoders , in order to verify the theoretically predicted dependence on the bit length . It is seen now that the experiment in the revision ( Fig.1 ) does not have an underfitting issue . All y-axis are precisely aligned and with more latent bits lower training loss is reached . Please let us know if you have further questions / suggestions ."}, "2": {"review_id": "F8lXvXpZdrL-2", "review_text": "Summary : The paper presents a principled derivation and analysis of the straight-through ( ST ) estimator , which is often used to train networks with binary weights and activations . A strong point of the paper is that the ST estimator is often used to train networks with binary weights and activations . Hence , a more theoretical investigation can be helpful to better understand the ST estimator . Furthermore , the paper seems theoretically sound . However , my assessment is unconfident due to lacking expertise in this area . The empirical experiments confirm that the ST performance improves as the number of latent bits is increased as suggested by the theoretical analysis . Another strong point is that the paper will make code available on Github , which can improve the reproducibility of the experiments . A weak point of the paper might be its limited potential impact on future works since it mainly provides an analysis of prior empirical ST approaches . Furthermore , a substantial amount of content is only described in the supplementary material . A venue that allows longer submission may be a better fit for this work . Minor important points : - Missing reference in Line 305 .", "rating": "7: Good paper, accept", "reply_text": "Thanks for you comments . We would like to say something in response to the mentioned weaknesses . * Limited potential impact Please see whether our updated experiments section improves on the potential impact in you view . We try to obtain more experimental insights confirming the analysis and helping to chose and apply ST methods with clarity and awareness of the limitations . Furthermore , we hope our work will facilitate the development of improved methods . * A substantial amount of content is only described in the supplementary material . A venue that allows longer submission may be a better fit for this work This is not uncommon for ICLR papers in our experience ( e.g.Cong et al. , 2019 `` GO Gradient for expectation based objectives '' , which we cite ) . To be honest , we find it rather hard to publish theoretical results reaching simultaneously a significant contribution , high clarity and a short length . And only the length of the technical part is relaxable in our view . We would appreciate suggestions ( from any of the reviewers ) for a better publishing strategy / other venues in machine learning that allow longer submissions . * Missing reference in Line 305 Fixed in the revision . The reference was to the Appendix C.2 `` Latent Weight Decay Implements Variational Bayesian Learning ''"}, "3": {"review_id": "F8lXvXpZdrL-3", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper presents a systematic way of analysing straight through estimators and derive ST estimators for Stochastic Binary Networks . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . The paper is theoretically strong . 2.The paper covers a large body of relevant work , covering theory behind straight through estimators and stochastic binary networks . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . The novelty of the paper is n't clear , if this paper is an analysis paper , the empirical evidence is weak . 2.The paper is very hard to read , and it is difficult to understand the clear motivation . The paper is too dense while missing any key takeaway point . It misses key details in the experiment section . 3.The utility of the proposed MD estimator is unclear , it would be helpful if the authors would clarify the interpretation of Table 1 with their write up under `` Classification with Deep SBN '' : the authors state that their method performs as well as the empirical ST , while the table shows it performs worse than their baselines . 4.Overall , the experiment section is scattered , with hard to understand goals . For example , the takeaway from Figure 1 bottom is difficult to understand . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Typos : Line 305 ? ? The paper needs major revisions in terms of notation issues : the vectors should be bold , to distinguish the from scalars . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions : Please address the cons mentioned above .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear R3 , thanks for you comments . * Novelty is not clear . Motivation is not clear . Hard to read . Too dense . We already did our best to present the systematization and the key results in a clear and accessible way . We can not omit any part of the contribution to make it less dense either in order to devote more space to e.g.motivation . * Clarify the interpretation of Table 1 . Performs worse than baselines . Our observations from Table 1 are summarized in lines 456-467 . There are two kinds of baselines : ST-based methods and a method of Peters and Welling ( 2018 ) also derived for SBNs . First , we observe from a comparison in the deterministic mode ( 'det ' column ) that our derived ST method is not worse than empirical ST setups found by trial and error . The differences of 0.1-0.2 % can be neglected ( it is within an effect of random initialization ) . The derived ST takes care of the choice of the noise distribution and proper updates and is clearly interpretable . If the same networks we trained are tested in the stochastic mode ( 10-sample ) , there is a clear boost of performance , indicating an advantage of SBN models . The takeaway message here is that mirror descent , initialization , and the link between noise and gradient components all work correctly . While some empirical ST methods were already guessed well , they can be replaced now with a more transparent approach . Second , there is indeed some gap to Peters and Welling ( 2018 ) in the stochastic mode ( e.g.our 10-sample versus their 16-sample ) . Nevertheless , the results are very similar considering that the baseline uses a different estimation method , an initialization from a pretrained model and an early stopping ( we believe otherwise they would overfit to the relaxation of the SBN considered ) . The takeaway message here is that ST can be considered in the context of SBN models as a simple but proper baseline . Since we achieve near 100 % training accuracy , the optimization fully succeeds and thus the bias of ST is tolerable . * The empirical evidence is weak We plan to update the experiment on auto-encoders ( within this review process ) to provide a more clear evidence about properties of STE to address concerns and suggestions by R1 and R4 . A clear understanding of STE and its properties is the main focus of the paper ( both theoretical and experimental ) . * Misses key details in the experiment section . We will appreciate pointing out which details do you find missing relative to the specifications in Appendix D. * Notation issues : the vectors should be bold , to distinguish the from scalars Thanks for bringing this up . We will give a try to the bold notation . At present , we rarely work with scalars unless they are coordinates of vectors such as x_i , in which case the subindex serves as a distinction . We will check consistency of the vector and coordinate-wise notation in the paper . We believe this is a matter of a minor revision in any case . * Line 305 ? ? Thanks , this refers to the Appendix C.2 , where Prop.2 is proven , i.e.this is a part of the present contribution and not an external reference ."}}