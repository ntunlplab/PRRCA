{"year": "2019", "forum": "SJe8DsR9tm", "title": "Dynamic Early Terminating of Multiply Accumulate Operations for Saving Computation Cost in Convolutional Neural Networks", "decision": "Reject", "meta_review": "This paper proposes a new method for speeding up convolutional neural networks. It uses the idea of early terminating the computation of convolutional layers. It saves FLOPs, but the reviewers raised a critical concern that it doesn't save wall-clock time. The time overhead is about 4 or 5 times of the original model. There is not any reduced execution time but much longer. The authors agreed that \"the overhead on the inference time is certainly an issue of our method\". The work is not mature and practical. recommend for rejection. ", "reviews": [{"review_id": "SJe8DsR9tm-0", "review_text": "This paper proposes a new method for speeding up convolutional neural networks. Different from previous work, it uses the idea of early terminating the computation of convolutional layers. The method itself is intuitive and easy to understand. By sorting the parameters in a descending order and early stopping the computation in a filter, it can reduce the computation cost (MAC) while preserving accuracy. 1. The networks used in the experiments are very simple. I understand that in the formulation part the assumption is that ReLU layer is put directly after convolutional layer. However, in state-of-the-art network, batch normalization layer is put between convolutional layer and ReLU non-linearity. It would add much value if the authors could justify the use cases of the proposed method on the widely adopted networks such as ResNet. 2. I notice that there is a process that sort the parameters in the convolutional layers. However, the authors do not give any time complexity analysis about this process. I would like see how weight sorting influences the inference time. 3. The title contains the word \u201cdynamic\u201d. However, I notice that the parameter e used in the paper is predefined (or chosen from a set predefined of values). So i am not sure it is appropriate to use the word \u201cdynamic\u201d here. Correct me if i am wrong here. 4. In the experiment part, the authors choose two baselines: FPEC [1]. However, to my knowledge, their methods are performed on different networks. Also, the pruned models from their methods are carefully designed using sensitivity analysis. So I am curious how are the baselines designed in your experiments. Overall this paper is well-written and points a new direction to speeding up neural networks. I like the analysis in section 3. I will consider revising the score if the authors can address my concerns. [1] Pruning Filters for Efficient ConvNets. Li et al., ICLR 2017. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate the valuable comments and the efforts of the reviewer on this manuscript . The detailed remarks are very helpful for improving the manuscript . We have made necessary changes to respond to all the comments . Q1 : This paper proposes a new method for speeding up convolutional neural networks . ... A1 : We appreciate this comment . Q2 : The networks used in the experiments are very simple . ... A2 : We appreciate this comment . We think that our method could be extended to deal with the mentioned network . Our current method compare the intermediate result at the checkpoint to 0 to determine whether a MAC process should stop or not . If the result is less than 0 , we terminate the MAC process . To our understanding , the batch normalization method normalizes the outputs of the convolutional layers according to four learned parameters , mu , sigma , gamma , and beta , and then passes the results to the following activation layers . One possible solution to address the issue is to apply batch normalization to the intermediate result before it is compared to 0 . Another possible solution is to compare the intermediate result to a non-zero value . The value is determined according to the four learned parameters , so that when the intermediate result is less than the value , it implies that the normalized intermediate result will be less than 0 . Additionally , since the final fine-tuning process updates the four learned parameters , the value to be compared would be updated as well . Q3 : I notice that there is a process that sort the parameters in the convolutional layers . ... A3 : We appreciate this comment . As AnonReviewer1 mentioned , current GPUs have been well optimized for convolution operations . However , to implement the proposed technique , we need to design a new function to replace the Cuda function CuBLAS SGEMM for MACs . Thus , our method actually does not speed up the inference time . In our implementation , we use a look-up table to record the indexes of the sorted weights , so that we only need to sort the weights of a filter once . In addition , we use a counter for checking whether the checkpoint is reached . These extra efforts all cause time overhead . In fact , we think that the main benefit of the proposed method should be that we can turn off the unused GPU nodes or threads to save power/energy consumption . However , since the proposed method is currently executed on a workstation , it is difficult to measure the saved power/energy consumption . Thus , we use MAC operations as the measuring criteria in the experiments . We have conducted a simple experiment to demonstrate the time overhead of the proposed method . We used the optimized ( ours ) and the non-optimized ( baseline ) C10-Net and NiN to test the CIFAR-10 testing data and recorded the average execution time for a batch of 200 input images . The following table summarizes the results comparing the baseline and ours . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - | C10-Net , | NiN , | | et = 10 % | et = 5 % | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Baseline|116.554 ms|879.404 ms| -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Ours |585.659 ms|6141.77 ms | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Q4 : The title contains the word \u201c dynamic \u201d . ... A4 : We appreciate this comment . The parameter et is predefined by the user for trading off inference accuracy and MAC operations . In this paper , we use the word \u201c dynamic \u201d because whether a MAC process early stops or not is conditional and related to the input images . That is , a MAC process may early stop for one input image , but does not for another input image . We have revised the introduction section on Page 2 to clarify the word \u201c dynamic \u201d . Q5 : In the experiment part , the authors choose two baselines : CP [ 1 ] and FPEC [ 2 ] . ... A5 : We appreciate this comment . The programs of CP [ 1 ] and PFEC [ 2 ] we used in the experiments were downloaded from the github website [ 3 ] [ 4 ] . They were released by the authors or had been confirmed that they can reproduce the experiments in the original papers [ 1 ] [ 2 ] . Both the CP and the PFEC programs allow a user to determine the ratio of filters to be dropped . Thus , in our experiments , the results of CP and PFEC were obtained by applying several different ratios . Furthermore , in the future , we will try to apply our method to more networks and datasets , such as ResNet , to show that our extended method works well for the batch normalization operation and to have a more comprehensive comparison with CP and PFEC . [ 1 ] He , Yihui , et al . `` Channel pruning for accelerating very deep neural networks . '' ICCV.Vol.2.No.6.2017 . [ 2 ] Li , Hao , et al . `` Pruning filters for efficient convnets . '' arXiv:1608.08710 . 2016 . [ 3 ] CP : https : //github.com/yihui-he/channel-pruning [ 4 ] PFEC : https : //github.com/slothkong/DNN-Pruning Q6 : Overall this paper is well-written and points a new direction to speeding up neural networks . ... A6 : We appreciate this comment ."}, {"review_id": "SJe8DsR9tm-1", "review_text": "This paper motivates itself by observing that not all convolutional weights are required to make an accurate prediction. In the introduction the authors envision a system similar to a cascaded classifier [Viola and Jones 2001] (I draw this conclusion not the paper). However the wording of the introduction is not clear or it does not align with what is presented in the paper. The approach in the paper does not perform early stopping dynamically during the feedforward phase. The approach removes weights which do not impact the accuracy after training has completed and the fine tunes the resulting network. The clarity of the introduction must be addressed however the work is still interesting. I recommend the authors try to make the introduction as accessible as possible. Also there are very general statements like \"The activation layer introduces non-linearity into the system for obtaining better accuracy.\" which do not contribute to the message of the paper. The paper will be fine without these statements. Shorter is better. Section 3 is good as a motivating example. However the conclusion \u201cThus, our focus is to develop an effective method of choosing a good intermediate result for saving more MAC operations with less accuracy drop\u201d is not very clear. More insights written hear would be better. One major flaw is that no analysis with respect to time of computation was performed. GPUs offer the advantage of being optimized for convolutions so it is possible that there is no speedup. Because of this it is unclear if the method would save time. The results clearly show individual computations (MACs) are reduced but it is not clear how this correlates with wall clock time. Why do you start with the centre layers? I understand the heuristic you\u2019re using, that the middle layers won\u2019t have high or low-level features, and that they won\u2019t break the other layers as badly if you modify them, but I feel like this is core to your method and it\u2019s not adequately justified. I\u2019d like to see some experiments on that and whether it actually matters to the outcome. Also, you don\u2019t say if you start at the middle then go up a layer, or down a layer. I think this matters since your proposed C10-Net has only 3 convolutional layers. All the filters in the same layer share a common checkpoint. Is that a good idea? What is the cost of doing this on a per-filter level? What is the cost on a per-layer level? Discussing runtime estimates for the added computation at training would make it more clear what the cost of the method is. In section 4.4.3. you mention that the majority of weight distributions in CNNs follow the Gaussian manner. Could you cite something of this? You might also want to move that in Step 1 (section 4.1.), since it seems to be your motivation for selection of checkpoint locations (5% and 32%) and I had no idea why you selected those values at that point. Typos: Typo on page 3: \u201cexploits redundancies inter feature maps to prune filters and feature maps\u201d Structural: Maybe section 4.3 should be part of the description of Section 4. Proposed Method, not its own subsection. Maybe table 2 should go at the end of section 4.4.1, because you describe it the error and columns in section 4.4.1. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate the valuable comments and the efforts of the reviewer on this manuscript . The detailed remarks are very helpful for improving the manuscript . We have made necessary changes to respond to all the comments . Q1 : This paper motivates itself by observing that not all convolutional weights are required to make an accurate prediction . ... A1 : We appreciate this comment . In fact , the technique \u201c early terminating \u201d proposed in this paper terminates a MAC process before it completes for saving computation cost . The technique is different from that used in [ 1 ] [ 2 ] which early terminates the whole network and makes a prediction directly at an early layer . We have revised the introduction section on Page 2 to clarify the differences as suggested . [ 1 ] Viola , Paul , and Michael Jones . `` Rapid object detection using a boosted cascade of simple features . '' Computer Vision and Pattern Recognition , 2001 . CVPR 2001 . Proceedings of the 2001 IEEE Computer Society Conference on . Vol.1.IEEE , 2001 . [ 2 ] Teerapittayanon , Surat , Bradley McDanel , and H. T. Kung . `` Branchynet : Fast inference via early exiting from deep neural networks . '' Pattern Recognition ( ICPR ) , 2016 23rd International Conference on . IEEE , 2016 . Q2 : The approach in the paper does not perform early stopping dynamically during the feedforward phase . ... A2 : We appreciate this comment . We say that the proposed technique is dynamic because whether a MAC process early stops or not is related to the input data . That is , a MAC process may early stop for one input image , but does not for another input image . In addition , we do not remove weights , because the criticality of a weight differs for different input images . As suggested , we have revised the introduction section on Page 2 to clarify the proposed method . Q3 : The clarity of the introduction must be addressed however the work is still interesting . ... A3 : We appreciate this comment . As suggested , we have revised the introduction section on Page 2 to clarify the proposed method and clearly stated the differences of the proposed method compared with previous methods . Q4 : Also there are very general statements like `` The activation layer introduces non-linearity into the system for obtaining better accuracy . '' ... A4 : We appreciate this comment . We have revised the paper as suggested . Q5 : Section 3 is good as a motivating example . ... A5 : We appreciate this comment . We originally would like to emphasize that the proposed idea is promising , and our next step is to address the issue of how to set good checkpoints which lead to more MAC operation saving with less accuracy drop . We have revised the statements to clarify our intention . Q6 : One major flaw is that no analysis with respect to time of computation was performed . ... A6 : We appreciate this comment . We agree that wall-clock-time is an important performance criteria . However , as the reviewer mentioned , current GPUs have been well optimized for convolution operations . Thus , the proposed method actually does not save execution time , but instead spends more time . In our implementation , we design a new function to replace the Cuda function CuBLAS SGEMM , use a counter for checking whether the checkpoint is reached , and use a look-up table to record the indexes of the sorted weights . These modification and extra effort cause the main time overhead . We think that the main benefit of the proposed method is that we can turn off the unused GPU nodes or threads to save power/energy consumption . Since the proposed method is currently executed on a workstation , it is difficult to measure the saved power/energy consumption . Thus , we use MAC operations as the measuring criteria in the experiments . One possible way to address the issue is to port the programs onto a GPU development board , such as NVIDIA Jetson TX2 , so that we are able to measure the consumed power/energy . This will be our future work . We have conducted a simple experiment to demonstrate the time overhead of the proposed method . We used the optimized ( ours ) and the non-optimized ( baseline ) C10-Net and NiN to test the CIFAR-10 testing data and recorded the average execution time of 200 input images . The following table summarizes the results comparing the baseline and ours . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- |C10-Net , et = 10 % | NiN , et = 5 % | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Baseline| 116.554 ms | 879.404 ms | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ours | 585.659 ms | 6141.77 ms | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"}, {"review_id": "SJe8DsR9tm-2", "review_text": "In convolutional neural networks, a convolutional filter conducts a series of multiply-accumulate (MAC) operations, which is computationally heavy. To save computational cost, this manuscript proposes an algorithm to set a checkpoint in the MAC process to determine whether a filter could terminate early based on the intermediate result. The results empirically demonstrate that the proposed algorithm could save lots of MAC computations without losing too much accuracy. Significance wise, the results look promising, though it is not always the best method to preserve accuracy while doing the pruning. There are a few experiments where CP is better at preserving accuracy. In addition, it would be great if the manuscript could also compare with other compression methods like low-rank factorization and knowledge distillation. In this manuscript, the best results are roughly 50% MAC savings (which is not equal to the saving of the whole networks) with some accuracy drop. It seems that knowledge distillation could often make the model half size without losing accuracy if properly tuned. Quality wise, there seems to be some room to improve from a technical point of view. The proposed algorithm always starts from the center layers rather than shallow or deep layers based on the intuition that center layers have minimal influence on final output compared with other layers. This sounds about right, but lacks data support. In addition, even this intuition is correct, it might not be the optimal setting for pruning. We shall start from the layer where the ratio of accuracy drop by reducing one MAC operation is minimal. As a counterexample, if the shallow layer is the most computationally heavy, saving 10% MAC ops might save more computational cost than saving 5% in the center layer while preserving the same accuracy level. The above sub-optimality might be further magnified by the fact that the proposed algorithm is greedy because one layer might use up all the accuracy budget before moving on to the next candidate layer as shown in line 7 of Algorithm 1. The method is quite original, and the manuscript is very well written and easy to follow.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate the valuable comments and the efforts of the reviewer on this manuscript . The detailed remarks are very helpful for improving the manuscript . We have made necessary changes to respond to all the comments . Q1 : In convolutional neural networks , a convolutional filter conducts a series of multiply-accumulate ( MAC ) operations , which is computationally heavy . To save computational cost , this manuscript proposes an algorithm to set a checkpoint in the MAC process to determine whether a filter could terminate early based on the intermediate result . The results empirically demonstrate that the proposed algorithm could save lots of MAC computations without losing too much accuracy . A1 : We appreciate this comment . Q2 : Significance wise , the results look promising , though it is not always the best method to preserve accuracy while doing the pruning . There are a few experiments where CP is better at preserving accuracy . In addition , it would be great if the manuscript could also compare with other compression methods like low-rank factorization and knowledge distillation . In this manuscript , the best results are roughly 50 % MAC savings ( which is not equal to the saving of the whole networks ) with some accuracy drop . It seems that knowledge distillation could often make the model half size without losing accuracy if properly tuned . A2 : We appreciate this comment . In fact , CP uses low-rank factorization as one of its key techniques . Although we do not compare to a low-rank factorization-based method , our approach should be competitive as well . Furthermore , we do not compare to knowledge distillation , because our approach is compatible with it . They could be combined to achieve better effectiveness . For example , one can apply our approach to the student network obtained from the knowledge distillation method . We will try to conduct some experiments to see if the combination is promising . Q3 : Quality wise , there seems to be some room to improve from a technical point of view . The proposed algorithm always starts from the center layers rather than shallow or deep layers based on the intuition that center layers have minimal influence on final output compared with other layers . This sounds about right , but lacks data support . In addition , even this intuition is correct , it might not be the optimal setting for pruning . We shall start from the layer where the ratio of accuracy drop by reducing one MAC operation is minimal . As a counterexample , if the shallow layer is the most computationally heavy , saving 10 % MAC ops might save more computational cost than saving 5 % in the center layer while preserving the same accuracy level . A3 : We appreciate this comment . We are trying to develop a new method for setting checkpoints based on the suggestion . The new method has a pre-process that analyzes the error tolerance of each layer , and then it sets checkpoints starting from the layer with the highest error tolerance . Currently , we are looking for a good method to estimate the error tolerance of a layer and conducting some experiments to show its effectiveness . When the experimental results are ready , we will post the results onto the website . Q4 : The above sub-optimality might be further magnified by the fact that the proposed algorithm is greedy because one layer might use up all the accuracy budget before moving on to the next candidate layer as shown in line 7 of Algorithm 1 . A4 : We appreciate this comment . We agree with the reviewer that the proposed method for setting checkpoints is greedy and it may suffer from the mentioned issue , although the issue did not happen in the considered cases . We have adopted the reviewer \u2019 s suggestion to present a new method . It should be able to mitigate or solve the issue . Q5 : The method is quite original , and the manuscript is very well written and easy to follow . A5 : We appreciate this comment ."}], "0": {"review_id": "SJe8DsR9tm-0", "review_text": "This paper proposes a new method for speeding up convolutional neural networks. Different from previous work, it uses the idea of early terminating the computation of convolutional layers. The method itself is intuitive and easy to understand. By sorting the parameters in a descending order and early stopping the computation in a filter, it can reduce the computation cost (MAC) while preserving accuracy. 1. The networks used in the experiments are very simple. I understand that in the formulation part the assumption is that ReLU layer is put directly after convolutional layer. However, in state-of-the-art network, batch normalization layer is put between convolutional layer and ReLU non-linearity. It would add much value if the authors could justify the use cases of the proposed method on the widely adopted networks such as ResNet. 2. I notice that there is a process that sort the parameters in the convolutional layers. However, the authors do not give any time complexity analysis about this process. I would like see how weight sorting influences the inference time. 3. The title contains the word \u201cdynamic\u201d. However, I notice that the parameter e used in the paper is predefined (or chosen from a set predefined of values). So i am not sure it is appropriate to use the word \u201cdynamic\u201d here. Correct me if i am wrong here. 4. In the experiment part, the authors choose two baselines: FPEC [1]. However, to my knowledge, their methods are performed on different networks. Also, the pruned models from their methods are carefully designed using sensitivity analysis. So I am curious how are the baselines designed in your experiments. Overall this paper is well-written and points a new direction to speeding up neural networks. I like the analysis in section 3. I will consider revising the score if the authors can address my concerns. [1] Pruning Filters for Efficient ConvNets. Li et al., ICLR 2017. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate the valuable comments and the efforts of the reviewer on this manuscript . The detailed remarks are very helpful for improving the manuscript . We have made necessary changes to respond to all the comments . Q1 : This paper proposes a new method for speeding up convolutional neural networks . ... A1 : We appreciate this comment . Q2 : The networks used in the experiments are very simple . ... A2 : We appreciate this comment . We think that our method could be extended to deal with the mentioned network . Our current method compare the intermediate result at the checkpoint to 0 to determine whether a MAC process should stop or not . If the result is less than 0 , we terminate the MAC process . To our understanding , the batch normalization method normalizes the outputs of the convolutional layers according to four learned parameters , mu , sigma , gamma , and beta , and then passes the results to the following activation layers . One possible solution to address the issue is to apply batch normalization to the intermediate result before it is compared to 0 . Another possible solution is to compare the intermediate result to a non-zero value . The value is determined according to the four learned parameters , so that when the intermediate result is less than the value , it implies that the normalized intermediate result will be less than 0 . Additionally , since the final fine-tuning process updates the four learned parameters , the value to be compared would be updated as well . Q3 : I notice that there is a process that sort the parameters in the convolutional layers . ... A3 : We appreciate this comment . As AnonReviewer1 mentioned , current GPUs have been well optimized for convolution operations . However , to implement the proposed technique , we need to design a new function to replace the Cuda function CuBLAS SGEMM for MACs . Thus , our method actually does not speed up the inference time . In our implementation , we use a look-up table to record the indexes of the sorted weights , so that we only need to sort the weights of a filter once . In addition , we use a counter for checking whether the checkpoint is reached . These extra efforts all cause time overhead . In fact , we think that the main benefit of the proposed method should be that we can turn off the unused GPU nodes or threads to save power/energy consumption . However , since the proposed method is currently executed on a workstation , it is difficult to measure the saved power/energy consumption . Thus , we use MAC operations as the measuring criteria in the experiments . We have conducted a simple experiment to demonstrate the time overhead of the proposed method . We used the optimized ( ours ) and the non-optimized ( baseline ) C10-Net and NiN to test the CIFAR-10 testing data and recorded the average execution time for a batch of 200 input images . The following table summarizes the results comparing the baseline and ours . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - | C10-Net , | NiN , | | et = 10 % | et = 5 % | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Baseline|116.554 ms|879.404 ms| -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Ours |585.659 ms|6141.77 ms | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Q4 : The title contains the word \u201c dynamic \u201d . ... A4 : We appreciate this comment . The parameter et is predefined by the user for trading off inference accuracy and MAC operations . In this paper , we use the word \u201c dynamic \u201d because whether a MAC process early stops or not is conditional and related to the input images . That is , a MAC process may early stop for one input image , but does not for another input image . We have revised the introduction section on Page 2 to clarify the word \u201c dynamic \u201d . Q5 : In the experiment part , the authors choose two baselines : CP [ 1 ] and FPEC [ 2 ] . ... A5 : We appreciate this comment . The programs of CP [ 1 ] and PFEC [ 2 ] we used in the experiments were downloaded from the github website [ 3 ] [ 4 ] . They were released by the authors or had been confirmed that they can reproduce the experiments in the original papers [ 1 ] [ 2 ] . Both the CP and the PFEC programs allow a user to determine the ratio of filters to be dropped . Thus , in our experiments , the results of CP and PFEC were obtained by applying several different ratios . Furthermore , in the future , we will try to apply our method to more networks and datasets , such as ResNet , to show that our extended method works well for the batch normalization operation and to have a more comprehensive comparison with CP and PFEC . [ 1 ] He , Yihui , et al . `` Channel pruning for accelerating very deep neural networks . '' ICCV.Vol.2.No.6.2017 . [ 2 ] Li , Hao , et al . `` Pruning filters for efficient convnets . '' arXiv:1608.08710 . 2016 . [ 3 ] CP : https : //github.com/yihui-he/channel-pruning [ 4 ] PFEC : https : //github.com/slothkong/DNN-Pruning Q6 : Overall this paper is well-written and points a new direction to speeding up neural networks . ... A6 : We appreciate this comment ."}, "1": {"review_id": "SJe8DsR9tm-1", "review_text": "This paper motivates itself by observing that not all convolutional weights are required to make an accurate prediction. In the introduction the authors envision a system similar to a cascaded classifier [Viola and Jones 2001] (I draw this conclusion not the paper). However the wording of the introduction is not clear or it does not align with what is presented in the paper. The approach in the paper does not perform early stopping dynamically during the feedforward phase. The approach removes weights which do not impact the accuracy after training has completed and the fine tunes the resulting network. The clarity of the introduction must be addressed however the work is still interesting. I recommend the authors try to make the introduction as accessible as possible. Also there are very general statements like \"The activation layer introduces non-linearity into the system for obtaining better accuracy.\" which do not contribute to the message of the paper. The paper will be fine without these statements. Shorter is better. Section 3 is good as a motivating example. However the conclusion \u201cThus, our focus is to develop an effective method of choosing a good intermediate result for saving more MAC operations with less accuracy drop\u201d is not very clear. More insights written hear would be better. One major flaw is that no analysis with respect to time of computation was performed. GPUs offer the advantage of being optimized for convolutions so it is possible that there is no speedup. Because of this it is unclear if the method would save time. The results clearly show individual computations (MACs) are reduced but it is not clear how this correlates with wall clock time. Why do you start with the centre layers? I understand the heuristic you\u2019re using, that the middle layers won\u2019t have high or low-level features, and that they won\u2019t break the other layers as badly if you modify them, but I feel like this is core to your method and it\u2019s not adequately justified. I\u2019d like to see some experiments on that and whether it actually matters to the outcome. Also, you don\u2019t say if you start at the middle then go up a layer, or down a layer. I think this matters since your proposed C10-Net has only 3 convolutional layers. All the filters in the same layer share a common checkpoint. Is that a good idea? What is the cost of doing this on a per-filter level? What is the cost on a per-layer level? Discussing runtime estimates for the added computation at training would make it more clear what the cost of the method is. In section 4.4.3. you mention that the majority of weight distributions in CNNs follow the Gaussian manner. Could you cite something of this? You might also want to move that in Step 1 (section 4.1.), since it seems to be your motivation for selection of checkpoint locations (5% and 32%) and I had no idea why you selected those values at that point. Typos: Typo on page 3: \u201cexploits redundancies inter feature maps to prune filters and feature maps\u201d Structural: Maybe section 4.3 should be part of the description of Section 4. Proposed Method, not its own subsection. Maybe table 2 should go at the end of section 4.4.1, because you describe it the error and columns in section 4.4.1. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate the valuable comments and the efforts of the reviewer on this manuscript . The detailed remarks are very helpful for improving the manuscript . We have made necessary changes to respond to all the comments . Q1 : This paper motivates itself by observing that not all convolutional weights are required to make an accurate prediction . ... A1 : We appreciate this comment . In fact , the technique \u201c early terminating \u201d proposed in this paper terminates a MAC process before it completes for saving computation cost . The technique is different from that used in [ 1 ] [ 2 ] which early terminates the whole network and makes a prediction directly at an early layer . We have revised the introduction section on Page 2 to clarify the differences as suggested . [ 1 ] Viola , Paul , and Michael Jones . `` Rapid object detection using a boosted cascade of simple features . '' Computer Vision and Pattern Recognition , 2001 . CVPR 2001 . Proceedings of the 2001 IEEE Computer Society Conference on . Vol.1.IEEE , 2001 . [ 2 ] Teerapittayanon , Surat , Bradley McDanel , and H. T. Kung . `` Branchynet : Fast inference via early exiting from deep neural networks . '' Pattern Recognition ( ICPR ) , 2016 23rd International Conference on . IEEE , 2016 . Q2 : The approach in the paper does not perform early stopping dynamically during the feedforward phase . ... A2 : We appreciate this comment . We say that the proposed technique is dynamic because whether a MAC process early stops or not is related to the input data . That is , a MAC process may early stop for one input image , but does not for another input image . In addition , we do not remove weights , because the criticality of a weight differs for different input images . As suggested , we have revised the introduction section on Page 2 to clarify the proposed method . Q3 : The clarity of the introduction must be addressed however the work is still interesting . ... A3 : We appreciate this comment . As suggested , we have revised the introduction section on Page 2 to clarify the proposed method and clearly stated the differences of the proposed method compared with previous methods . Q4 : Also there are very general statements like `` The activation layer introduces non-linearity into the system for obtaining better accuracy . '' ... A4 : We appreciate this comment . We have revised the paper as suggested . Q5 : Section 3 is good as a motivating example . ... A5 : We appreciate this comment . We originally would like to emphasize that the proposed idea is promising , and our next step is to address the issue of how to set good checkpoints which lead to more MAC operation saving with less accuracy drop . We have revised the statements to clarify our intention . Q6 : One major flaw is that no analysis with respect to time of computation was performed . ... A6 : We appreciate this comment . We agree that wall-clock-time is an important performance criteria . However , as the reviewer mentioned , current GPUs have been well optimized for convolution operations . Thus , the proposed method actually does not save execution time , but instead spends more time . In our implementation , we design a new function to replace the Cuda function CuBLAS SGEMM , use a counter for checking whether the checkpoint is reached , and use a look-up table to record the indexes of the sorted weights . These modification and extra effort cause the main time overhead . We think that the main benefit of the proposed method is that we can turn off the unused GPU nodes or threads to save power/energy consumption . Since the proposed method is currently executed on a workstation , it is difficult to measure the saved power/energy consumption . Thus , we use MAC operations as the measuring criteria in the experiments . One possible way to address the issue is to port the programs onto a GPU development board , such as NVIDIA Jetson TX2 , so that we are able to measure the consumed power/energy . This will be our future work . We have conducted a simple experiment to demonstrate the time overhead of the proposed method . We used the optimized ( ours ) and the non-optimized ( baseline ) C10-Net and NiN to test the CIFAR-10 testing data and recorded the average execution time of 200 input images . The following table summarizes the results comparing the baseline and ours . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- |C10-Net , et = 10 % | NiN , et = 5 % | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Baseline| 116.554 ms | 879.404 ms | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ours | 585.659 ms | 6141.77 ms | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"}, "2": {"review_id": "SJe8DsR9tm-2", "review_text": "In convolutional neural networks, a convolutional filter conducts a series of multiply-accumulate (MAC) operations, which is computationally heavy. To save computational cost, this manuscript proposes an algorithm to set a checkpoint in the MAC process to determine whether a filter could terminate early based on the intermediate result. The results empirically demonstrate that the proposed algorithm could save lots of MAC computations without losing too much accuracy. Significance wise, the results look promising, though it is not always the best method to preserve accuracy while doing the pruning. There are a few experiments where CP is better at preserving accuracy. In addition, it would be great if the manuscript could also compare with other compression methods like low-rank factorization and knowledge distillation. In this manuscript, the best results are roughly 50% MAC savings (which is not equal to the saving of the whole networks) with some accuracy drop. It seems that knowledge distillation could often make the model half size without losing accuracy if properly tuned. Quality wise, there seems to be some room to improve from a technical point of view. The proposed algorithm always starts from the center layers rather than shallow or deep layers based on the intuition that center layers have minimal influence on final output compared with other layers. This sounds about right, but lacks data support. In addition, even this intuition is correct, it might not be the optimal setting for pruning. We shall start from the layer where the ratio of accuracy drop by reducing one MAC operation is minimal. As a counterexample, if the shallow layer is the most computationally heavy, saving 10% MAC ops might save more computational cost than saving 5% in the center layer while preserving the same accuracy level. The above sub-optimality might be further magnified by the fact that the proposed algorithm is greedy because one layer might use up all the accuracy budget before moving on to the next candidate layer as shown in line 7 of Algorithm 1. The method is quite original, and the manuscript is very well written and easy to follow.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate the valuable comments and the efforts of the reviewer on this manuscript . The detailed remarks are very helpful for improving the manuscript . We have made necessary changes to respond to all the comments . Q1 : In convolutional neural networks , a convolutional filter conducts a series of multiply-accumulate ( MAC ) operations , which is computationally heavy . To save computational cost , this manuscript proposes an algorithm to set a checkpoint in the MAC process to determine whether a filter could terminate early based on the intermediate result . The results empirically demonstrate that the proposed algorithm could save lots of MAC computations without losing too much accuracy . A1 : We appreciate this comment . Q2 : Significance wise , the results look promising , though it is not always the best method to preserve accuracy while doing the pruning . There are a few experiments where CP is better at preserving accuracy . In addition , it would be great if the manuscript could also compare with other compression methods like low-rank factorization and knowledge distillation . In this manuscript , the best results are roughly 50 % MAC savings ( which is not equal to the saving of the whole networks ) with some accuracy drop . It seems that knowledge distillation could often make the model half size without losing accuracy if properly tuned . A2 : We appreciate this comment . In fact , CP uses low-rank factorization as one of its key techniques . Although we do not compare to a low-rank factorization-based method , our approach should be competitive as well . Furthermore , we do not compare to knowledge distillation , because our approach is compatible with it . They could be combined to achieve better effectiveness . For example , one can apply our approach to the student network obtained from the knowledge distillation method . We will try to conduct some experiments to see if the combination is promising . Q3 : Quality wise , there seems to be some room to improve from a technical point of view . The proposed algorithm always starts from the center layers rather than shallow or deep layers based on the intuition that center layers have minimal influence on final output compared with other layers . This sounds about right , but lacks data support . In addition , even this intuition is correct , it might not be the optimal setting for pruning . We shall start from the layer where the ratio of accuracy drop by reducing one MAC operation is minimal . As a counterexample , if the shallow layer is the most computationally heavy , saving 10 % MAC ops might save more computational cost than saving 5 % in the center layer while preserving the same accuracy level . A3 : We appreciate this comment . We are trying to develop a new method for setting checkpoints based on the suggestion . The new method has a pre-process that analyzes the error tolerance of each layer , and then it sets checkpoints starting from the layer with the highest error tolerance . Currently , we are looking for a good method to estimate the error tolerance of a layer and conducting some experiments to show its effectiveness . When the experimental results are ready , we will post the results onto the website . Q4 : The above sub-optimality might be further magnified by the fact that the proposed algorithm is greedy because one layer might use up all the accuracy budget before moving on to the next candidate layer as shown in line 7 of Algorithm 1 . A4 : We appreciate this comment . We agree with the reviewer that the proposed method for setting checkpoints is greedy and it may suffer from the mentioned issue , although the issue did not happen in the considered cases . We have adopted the reviewer \u2019 s suggestion to present a new method . It should be able to mitigate or solve the issue . Q5 : The method is quite original , and the manuscript is very well written and easy to follow . A5 : We appreciate this comment ."}}