{"year": "2019", "forum": "SJxTroR9F7", "title": "Supervised Policy Update for Deep Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "The paper presents an interesting technique for constrained policy optimization, which is applicable to existing RL algorithms such as TRPO and PPO. All of the reviewers agree that the paper is above the bar and the authors have improved the exposition during the review process. I encourage the authors to address all of the comments in the final version.", "reviews": [{"review_id": "SJxTroR9F7-0", "review_text": "The authors formulate policy optimization as a two step iterative procedure: 1) solving a constrained optimization problem in the non-parameterized policy space, 2) using supervised regression to project this onto a parameterized policy. This approach generally applies to both continuous and discrete action spaces and can handle a variety of constraints. Their primary claims is that this approach improves sample-efficiency over TRPO on Mujoco tasks and over PPO on Atari games. The method proposed in the paper has strong similarities with existing methods, but lacks comparisons with these approaches. The authors have not clearly demonstrated that SPU provides novel insights beyond the existing literature. I'm happy to change my score if the authors can convince me otherwise. Main comments: The focus of the paper is sample-efficiency, but the intro restricts to the on-policy setting. The authors should justify this choice. It is well known that off-policy algorithms (e.g., SAC for continuous control and Implicit Quantile Networks for Atari) are much more sample-efficient. In Sec 4, what is the advantage of breaking the problem up into these 3 steps versus directly trying to solve (9),(10)? In fact, if we convert (10) into a penalty and take the derivative, we arrive at nearly the same gradient as (17). As this is central to the SPU framework, this needs to be justified. MPO (Abdolmaleki et al. 2018) is very closely related to SPU. It is unclear if SPU provides any additional insights or benefits over MPO. This needs to be discussed and compared. The experimental section could be strengthened by: * Given the similarity to SPU, comparisons to MPO and GAC should be made, or clear justification for why they are not comparable must be given. * Why is the comparison on Mujoco to TRPO in the main text and the comparison to PPO relegated to the appendix? It would make more sense to compare to PPO, so the authors need to justify this decision. * The results on Mujoco are quite poor compared to state-of-the-art methods (e.g., SAC). The authors should justify why we should care about their results. Comments: In Sec 2, the authors should be careful about the discounting. For example, they are almost surely not having A_{it} approximate \\gamma^t A^{\\pi_{\\theta_k}}, rather A^{\\pi_{\\theta_k}}. In Sec 2, the KL is denoted as KL(\\pi || pi_k), but in the text is described as the KL from \\pi_k to \\pi (reversed). From the equations, it appears that is an error, and it should read KL from \\pi to \\pi_k. In Sec 3, the description of NPG/TRPO is not accurate. The main goal of NPG/TRPO work was to establish monotonic improvement. In Sec 3, computational speed is cited as a major deficit of GAC, especially the solving linear systems with the Hessian (wrt to the actions). This seems rather surprising. Inverting a 1000x1000 matrix on a modern computer takes <1 second, so it doesn't seem like this should be the limiting step for any of the problems encountered. The KL penalty version of PPO seems closely related to SPU. Can the authors mention differences with that version of PPO in the related work? In Sec 4, step iii is described as supervised learning. Can the authors elaborate on why? I would typically think of the other direction as supervised learning as that leads to MLE. In Sec 5.1, what is the justification/reasoning for setting \\tilde{\\lambda_{s_i}} = \\lambda and introducing the indicator functions? Sec 5.2 is not evaluated and Sec 5.3 produces inferior results, so it may make sense to move these to the appendix. Otherwise, the authors should explain situations where we would expect these to be useful or provide some additional insight. It also should be noted that the proximity constraints in TRPO/PPO follow from a theoretical argument and are not arbitrary choices. Sec 5.3 seems to deviate quite a bit from the SPU framework. In addition to the differences pointed out in the text, the \"supervised\" loss changes too. Can the authors justify/explain the reasoning for these changes? ===== I appreciate the authors' efforts to improve the paper. However, there is still substantial room for improvement in writing clarity. For example, the authors optimize the reverse KL from typical supervised learning, which makes even the title of the method confusing. The method that was experimentally evaluated can be derived more simply without the two-step procedure by directly taking the gradient and add the heuristically motivated per state indicator. This in itself is interesting and the authors demonstrate that it works well experimentally. I think the paper would be substantially more useful to the community if the authors focused on that contribution alone. As it stands now, I find the paper difficult to read because most of the theoretical results have no bearing on the method.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your thorough comments ! We argue that SPU has the following advantages : - It is mathematically principled . - It is mathematically straightforward . It is much simpler and more understandable than MPO and GAC . The technique is comprehensible by undergraduates . Simplicity in RL algorithms has its own merits . This is especially useful when non-expert wants to apply RL algorithms to solve their problems , which is becoming a trend . The step-by-step description and implementation of SPU in only slightly more involved than PPO . - It is versatile since a wide variety of constraint types can be used . SPU provides a general framework , allowing practitioners to try out different constraint types . For example , the TRPO paper mentions that enforcing the disaggregated constraint is preferable to enforcing the aggregated constraints . However , for mathematical conveniences , they choose to work with the aggregated constraints : \u201c While it is motivated by the theory , this problem is impractical to solve due to the large number of constraints . Instead , we can use a heuristic approximation which considers the average KL divergence \u201d [ 6 ] . In our paper , we show that the SPU framework allows us to solve the optimization problem with the dis-aggregated constraint exactly and also experimentally demonstrates that doing so helps to bring performance of SPU to be higher than TRPO . - It is desirable to have a unified algorithm that applies to both continuous and discrete problems . In our understanding , GAC and SAC do not apply to the discrete case . MPO has only made preliminary progress with the discrete case . - To our knowledge , among all the on-policy algorithms , it gives the best performance for the continuous cases . Deeply understanding the on-policy case is beneficial to the community . In future work we will consider combining SPU with replay buffers . - It is a general approach to solving DRL problems . The algorithms tested in our paper are specific instances of this approach . In both the GAC and MPO paper , working in the non-parameterized policy space is a by-product of applying the main ideas in those papers to DRL . We will cite and discuss SAC and MPO paper in related work . We also argue that SAC is not directly comparable to SPU since SAC tunes environment-specific parameter ( reward scaling ) . In SPU , the hyper-parameters are shared and fixed across all environments . The neural networks used in the SAC paper are also 4 times larger than the neural networks used in our work and the versions of PPO and TRPO that we compared against . Also , the performance of SPU and SAC can not be compared by looking at the graphs in the respective papers since the Mujoco environments used in the SAC paper is version 1 while we used version 2 . That being said , in our paper , we want to compare SPU against algorithms that operate under the same constraints , one of which is being on-policy . Thus , the focus of the paper will remain comparing SPU with other on-policy schemes . We justify why in more detailed below ."}, {"review_id": "SJxTroR9F7-1", "review_text": "The paper proposes to perform a constraint optimization of an approximation of the expected reward function for unparameterized policy with subsequent projection of the solution to the nearest parameterized one. This approach allows fast (\"nearly closed form\") solutions for nonparametric policies and leads to an increase in sample efficiency. The proposed approach is interesting and the results are promising. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you so much for your comments ! We are more than happy to answer any thing about the paper should you have more comments in the future ."}, {"review_id": "SJxTroR9F7-2", "review_text": "Overall this paper is ok. The algorithm seems novel, but is clearly very closely related to other things in the literature. The paper is also let down by poor exposition in several areas. The numerical results seem reasonably strong, at least against relatively old baselines. Equation 8 is crucial to the final algorithm, but is presented with no proof or explanation. Just above theorem 1 the sentence does not parse \"Further, for each s, let \u03bbs be the solution to \", firstly there is no 'solution' to an equation, secondly should it be \u03bbs or pi? The discussion following theorem 1 is very messy and hard to follow and the notation is horrendous. I'm confused as to why the indicator function in the 'disaggregated' update only includes states for which the constraint is already satisfied, what about the states where it is not? I presume this is because you initialize from the previous policy, but this seems very approximate and even worse updating the parameters for one state might significantly move the policy in some other state meaning large violations are possible and not dealt with. The connections to the papers 'MAXIMUM A POSTERIORI POLICY OPTIMISATION' and 'Relative Entropy Policy Search' should be mentioned, as another commenter said previously. I don't think TRPO/PPO is SOTA anymore, so maybe these baselines aren't particularly interesting. Figure 2 is incomprehensible. Two of the references are repeated (Schulman et al, Wang et al). The appendices include long lists of equalities with no explanation (e.g. appendix B), how is a reader meant to reasonably follow those steps? Each non-trivial equality needs a sentence explaining what was used to get it.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you so much for your detailed and thoughtful comments , we will revise the paper and figures based on your comments to better explain ourselves . Below , we address some of the specific concerns you have for our paper : \u201c Equation 8 is crucial to the final algorithm , but is presented with no proof or explanation. \u201d Equation 8 is a well-known result with detailed proofs provided in [ 1 ] , [ 2 ] . We felt it more appropriate to refer readers to these works rather than repeating the results in the appendix . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - \u201c Just above theorem 1 the sentence does not parse `` Further , for each s , let \u03bbs be the solution to `` , firstly there is no 'solution ' to an equation , secondly should it be \u03bbs or pi ? \u201d Just above Theorem 1 , note that \ud835\udf0b^\ud835\udf40 is defined as a function of \ud835\udf40 . We then define \ud835\udf40_s as the \ud835\udf40 that makes the value of the KL divergence equal to epsilon . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - \u201c The discussion following theorem 1 is very messy and hard to follow and the notation is horrendous . I 'm confused as to why the indicator function in the 'disaggregated ' update only includes states for which the constraint is already satisfied , what about the states where it is not ? I presume this is because you initialize from the previous policy , but this seems very approximate and even worse updating the parameters for one state might significantly move the policy in some other state meaning large violations are possible and not dealt with. \u201d Please note that the indicator function in Equation ( 18 ) is applied to each state in the trajectories sampled by the current policy . The indicator function helps us to take into account the disaggregated constraints ( 14 ) in the optimization problem ( 12 ) - ( 14 ) . Please also note that the original policy improvement bound from TRPO [ 1 ] [ 2 ] were proved using the disaggregated constraints . We would like to quote the TRPO paper : \u201c This problem imposes a constraint that the KL divergence is bounded at every point in the state space. \u201d In the TRPO , the disaggregated constraint is too unwieldy to work with and they thus choose to work with the average KL divergence ( the aggregated KL constraints in our case ) . In our paper , we work directly with the disaggregated constraint , but make the approximation that we only enforce the disaggregated constraints for the states in the trajectories sampled by the current policy . Thank you for your comment here ! We will rewrite the discussion to better explain our reasoning regarding the indicator function . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - \u201c The connections to the papers 'MAXIMUM A POSTERIORI POLICY OPTIMISATION ' and 'Relative Entropy Policy Search ' should be mentioned , as another commenter said previously. \u201d In the revision , we will include a discussion of MPO and Relative Entropy Policy Search and their relationship to our work . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - \u201c I do n't think TRPO/PPO is SOTA anymore , so maybe these baselines are n't particularly interesting. \u201d We acknowledge that recent work such as SAC has show to improve performance on TRPO and PPO , however we would like to point out that the main focus of our paper is to explore the idea of separating finding the optimal policy into a two-step process : finding the optimal non-parameterized policy , and then parameterizing this optimal policy . As such , we wanted to compare with algorithms operating under the same algorithmic constraints , one of which is being on-policy . It is a general trend in RL that the performance of an on-policy algorithm can be improved by incorporating off-policy training ( such as in SAC ) , We thus leave the extension of our approach to off-policy training to future work . We also invite the community to join us in making this extension . To help with this effort , we will release code for push-button replication of the main results in the paper . Thank you again for your input , we look forward to further discussions . Reference [ 1 ] Joshua Achiam , David Held , Aviv Tamar , and Pieter Abbeel . Constrained policy optimization . In International Conference on Machine Learning , pp . 22\u201331 , 2017 . [ 2 ] John Schulman , Sergey Levine , Pieter Abbeel , Michael Jordan , and Philipp Moritz . Trust region policy optimization . In International Conference on Machine Learning , pp . 1889\u20131897 , 2015 ."}], "0": {"review_id": "SJxTroR9F7-0", "review_text": "The authors formulate policy optimization as a two step iterative procedure: 1) solving a constrained optimization problem in the non-parameterized policy space, 2) using supervised regression to project this onto a parameterized policy. This approach generally applies to both continuous and discrete action spaces and can handle a variety of constraints. Their primary claims is that this approach improves sample-efficiency over TRPO on Mujoco tasks and over PPO on Atari games. The method proposed in the paper has strong similarities with existing methods, but lacks comparisons with these approaches. The authors have not clearly demonstrated that SPU provides novel insights beyond the existing literature. I'm happy to change my score if the authors can convince me otherwise. Main comments: The focus of the paper is sample-efficiency, but the intro restricts to the on-policy setting. The authors should justify this choice. It is well known that off-policy algorithms (e.g., SAC for continuous control and Implicit Quantile Networks for Atari) are much more sample-efficient. In Sec 4, what is the advantage of breaking the problem up into these 3 steps versus directly trying to solve (9),(10)? In fact, if we convert (10) into a penalty and take the derivative, we arrive at nearly the same gradient as (17). As this is central to the SPU framework, this needs to be justified. MPO (Abdolmaleki et al. 2018) is very closely related to SPU. It is unclear if SPU provides any additional insights or benefits over MPO. This needs to be discussed and compared. The experimental section could be strengthened by: * Given the similarity to SPU, comparisons to MPO and GAC should be made, or clear justification for why they are not comparable must be given. * Why is the comparison on Mujoco to TRPO in the main text and the comparison to PPO relegated to the appendix? It would make more sense to compare to PPO, so the authors need to justify this decision. * The results on Mujoco are quite poor compared to state-of-the-art methods (e.g., SAC). The authors should justify why we should care about their results. Comments: In Sec 2, the authors should be careful about the discounting. For example, they are almost surely not having A_{it} approximate \\gamma^t A^{\\pi_{\\theta_k}}, rather A^{\\pi_{\\theta_k}}. In Sec 2, the KL is denoted as KL(\\pi || pi_k), but in the text is described as the KL from \\pi_k to \\pi (reversed). From the equations, it appears that is an error, and it should read KL from \\pi to \\pi_k. In Sec 3, the description of NPG/TRPO is not accurate. The main goal of NPG/TRPO work was to establish monotonic improvement. In Sec 3, computational speed is cited as a major deficit of GAC, especially the solving linear systems with the Hessian (wrt to the actions). This seems rather surprising. Inverting a 1000x1000 matrix on a modern computer takes <1 second, so it doesn't seem like this should be the limiting step for any of the problems encountered. The KL penalty version of PPO seems closely related to SPU. Can the authors mention differences with that version of PPO in the related work? In Sec 4, step iii is described as supervised learning. Can the authors elaborate on why? I would typically think of the other direction as supervised learning as that leads to MLE. In Sec 5.1, what is the justification/reasoning for setting \\tilde{\\lambda_{s_i}} = \\lambda and introducing the indicator functions? Sec 5.2 is not evaluated and Sec 5.3 produces inferior results, so it may make sense to move these to the appendix. Otherwise, the authors should explain situations where we would expect these to be useful or provide some additional insight. It also should be noted that the proximity constraints in TRPO/PPO follow from a theoretical argument and are not arbitrary choices. Sec 5.3 seems to deviate quite a bit from the SPU framework. In addition to the differences pointed out in the text, the \"supervised\" loss changes too. Can the authors justify/explain the reasoning for these changes? ===== I appreciate the authors' efforts to improve the paper. However, there is still substantial room for improvement in writing clarity. For example, the authors optimize the reverse KL from typical supervised learning, which makes even the title of the method confusing. The method that was experimentally evaluated can be derived more simply without the two-step procedure by directly taking the gradient and add the heuristically motivated per state indicator. This in itself is interesting and the authors demonstrate that it works well experimentally. I think the paper would be substantially more useful to the community if the authors focused on that contribution alone. As it stands now, I find the paper difficult to read because most of the theoretical results have no bearing on the method.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your thorough comments ! We argue that SPU has the following advantages : - It is mathematically principled . - It is mathematically straightforward . It is much simpler and more understandable than MPO and GAC . The technique is comprehensible by undergraduates . Simplicity in RL algorithms has its own merits . This is especially useful when non-expert wants to apply RL algorithms to solve their problems , which is becoming a trend . The step-by-step description and implementation of SPU in only slightly more involved than PPO . - It is versatile since a wide variety of constraint types can be used . SPU provides a general framework , allowing practitioners to try out different constraint types . For example , the TRPO paper mentions that enforcing the disaggregated constraint is preferable to enforcing the aggregated constraints . However , for mathematical conveniences , they choose to work with the aggregated constraints : \u201c While it is motivated by the theory , this problem is impractical to solve due to the large number of constraints . Instead , we can use a heuristic approximation which considers the average KL divergence \u201d [ 6 ] . In our paper , we show that the SPU framework allows us to solve the optimization problem with the dis-aggregated constraint exactly and also experimentally demonstrates that doing so helps to bring performance of SPU to be higher than TRPO . - It is desirable to have a unified algorithm that applies to both continuous and discrete problems . In our understanding , GAC and SAC do not apply to the discrete case . MPO has only made preliminary progress with the discrete case . - To our knowledge , among all the on-policy algorithms , it gives the best performance for the continuous cases . Deeply understanding the on-policy case is beneficial to the community . In future work we will consider combining SPU with replay buffers . - It is a general approach to solving DRL problems . The algorithms tested in our paper are specific instances of this approach . In both the GAC and MPO paper , working in the non-parameterized policy space is a by-product of applying the main ideas in those papers to DRL . We will cite and discuss SAC and MPO paper in related work . We also argue that SAC is not directly comparable to SPU since SAC tunes environment-specific parameter ( reward scaling ) . In SPU , the hyper-parameters are shared and fixed across all environments . The neural networks used in the SAC paper are also 4 times larger than the neural networks used in our work and the versions of PPO and TRPO that we compared against . Also , the performance of SPU and SAC can not be compared by looking at the graphs in the respective papers since the Mujoco environments used in the SAC paper is version 1 while we used version 2 . That being said , in our paper , we want to compare SPU against algorithms that operate under the same constraints , one of which is being on-policy . Thus , the focus of the paper will remain comparing SPU with other on-policy schemes . We justify why in more detailed below ."}, "1": {"review_id": "SJxTroR9F7-1", "review_text": "The paper proposes to perform a constraint optimization of an approximation of the expected reward function for unparameterized policy with subsequent projection of the solution to the nearest parameterized one. This approach allows fast (\"nearly closed form\") solutions for nonparametric policies and leads to an increase in sample efficiency. The proposed approach is interesting and the results are promising. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you so much for your comments ! We are more than happy to answer any thing about the paper should you have more comments in the future ."}, "2": {"review_id": "SJxTroR9F7-2", "review_text": "Overall this paper is ok. The algorithm seems novel, but is clearly very closely related to other things in the literature. The paper is also let down by poor exposition in several areas. The numerical results seem reasonably strong, at least against relatively old baselines. Equation 8 is crucial to the final algorithm, but is presented with no proof or explanation. Just above theorem 1 the sentence does not parse \"Further, for each s, let \u03bbs be the solution to \", firstly there is no 'solution' to an equation, secondly should it be \u03bbs or pi? The discussion following theorem 1 is very messy and hard to follow and the notation is horrendous. I'm confused as to why the indicator function in the 'disaggregated' update only includes states for which the constraint is already satisfied, what about the states where it is not? I presume this is because you initialize from the previous policy, but this seems very approximate and even worse updating the parameters for one state might significantly move the policy in some other state meaning large violations are possible and not dealt with. The connections to the papers 'MAXIMUM A POSTERIORI POLICY OPTIMISATION' and 'Relative Entropy Policy Search' should be mentioned, as another commenter said previously. I don't think TRPO/PPO is SOTA anymore, so maybe these baselines aren't particularly interesting. Figure 2 is incomprehensible. Two of the references are repeated (Schulman et al, Wang et al). The appendices include long lists of equalities with no explanation (e.g. appendix B), how is a reader meant to reasonably follow those steps? Each non-trivial equality needs a sentence explaining what was used to get it.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you so much for your detailed and thoughtful comments , we will revise the paper and figures based on your comments to better explain ourselves . Below , we address some of the specific concerns you have for our paper : \u201c Equation 8 is crucial to the final algorithm , but is presented with no proof or explanation. \u201d Equation 8 is a well-known result with detailed proofs provided in [ 1 ] , [ 2 ] . We felt it more appropriate to refer readers to these works rather than repeating the results in the appendix . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - \u201c Just above theorem 1 the sentence does not parse `` Further , for each s , let \u03bbs be the solution to `` , firstly there is no 'solution ' to an equation , secondly should it be \u03bbs or pi ? \u201d Just above Theorem 1 , note that \ud835\udf0b^\ud835\udf40 is defined as a function of \ud835\udf40 . We then define \ud835\udf40_s as the \ud835\udf40 that makes the value of the KL divergence equal to epsilon . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - \u201c The discussion following theorem 1 is very messy and hard to follow and the notation is horrendous . I 'm confused as to why the indicator function in the 'disaggregated ' update only includes states for which the constraint is already satisfied , what about the states where it is not ? I presume this is because you initialize from the previous policy , but this seems very approximate and even worse updating the parameters for one state might significantly move the policy in some other state meaning large violations are possible and not dealt with. \u201d Please note that the indicator function in Equation ( 18 ) is applied to each state in the trajectories sampled by the current policy . The indicator function helps us to take into account the disaggregated constraints ( 14 ) in the optimization problem ( 12 ) - ( 14 ) . Please also note that the original policy improvement bound from TRPO [ 1 ] [ 2 ] were proved using the disaggregated constraints . We would like to quote the TRPO paper : \u201c This problem imposes a constraint that the KL divergence is bounded at every point in the state space. \u201d In the TRPO , the disaggregated constraint is too unwieldy to work with and they thus choose to work with the average KL divergence ( the aggregated KL constraints in our case ) . In our paper , we work directly with the disaggregated constraint , but make the approximation that we only enforce the disaggregated constraints for the states in the trajectories sampled by the current policy . Thank you for your comment here ! We will rewrite the discussion to better explain our reasoning regarding the indicator function . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - \u201c The connections to the papers 'MAXIMUM A POSTERIORI POLICY OPTIMISATION ' and 'Relative Entropy Policy Search ' should be mentioned , as another commenter said previously. \u201d In the revision , we will include a discussion of MPO and Relative Entropy Policy Search and their relationship to our work . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - \u201c I do n't think TRPO/PPO is SOTA anymore , so maybe these baselines are n't particularly interesting. \u201d We acknowledge that recent work such as SAC has show to improve performance on TRPO and PPO , however we would like to point out that the main focus of our paper is to explore the idea of separating finding the optimal policy into a two-step process : finding the optimal non-parameterized policy , and then parameterizing this optimal policy . As such , we wanted to compare with algorithms operating under the same algorithmic constraints , one of which is being on-policy . It is a general trend in RL that the performance of an on-policy algorithm can be improved by incorporating off-policy training ( such as in SAC ) , We thus leave the extension of our approach to off-policy training to future work . We also invite the community to join us in making this extension . To help with this effort , we will release code for push-button replication of the main results in the paper . Thank you again for your input , we look forward to further discussions . Reference [ 1 ] Joshua Achiam , David Held , Aviv Tamar , and Pieter Abbeel . Constrained policy optimization . In International Conference on Machine Learning , pp . 22\u201331 , 2017 . [ 2 ] John Schulman , Sergey Levine , Pieter Abbeel , Michael Jordan , and Philipp Moritz . Trust region policy optimization . In International Conference on Machine Learning , pp . 1889\u20131897 , 2015 ."}}