{"year": "2021", "forum": "aD1_5zowqV", "title": "Learning Energy-Based Generative Models via Coarse-to-Fine Expanding and Sampling", "decision": "Accept (Poster)", "meta_review": "This work proposes to train EBMs using multi-stage sampling. The EBMs are then used for generating high dimensional images, performing image to image translation, and out-of-distribution detection. The reviewers are impressed with the results, but indicate that the novelty is limited. While I agree that the work can be seen as a combination of previously proposed techniques, demonstrating that this combination can be made to work well is still a significant contribution to the field. In addition, the paper demonstrates strong results in using Langevin dynamics to translate between images, which I do think is novel. I therefore recommend accepting the paper for a poster presentation.", "reviews": [{"review_id": "aD1_5zowqV-0", "review_text": "This work proposes a method to improve generation with energy based models . The work further shows how energy based models can also be extended to cross class image translation . Strong Points : Generated samples appear to look good Weak Points : My most major concern is that the overall technical novelty of the work is rather limited , and I do not think passes the bar of acceptance for ICLR . Past work [ 2 ] has shown that EBMs can composed with multi-scale sampling , while [ 1 ] notes that the smooth activation function Swish significantly improves EBM generation . Similarly , cross domain generation with EBMs have been previously demonstrated such as in [ 1 ] . While [ 2 ] uses a separate hierarchy of multi-scale EBMs , this work uses the sampling scheme in [ 3 ] . My second most major concern is other the theoretical framing of the proposed approach and well is issues with statements made in the paper . W The proposed sampler linearly decays the noise schedule to 0 , but what are the theoretical implication of such a behavior ? For Langevin sampling to valid , the step size of both the model and the noise must decay to 0 . The proposed approach generates samples by running a fixed number of Langevin steps to generate a sample . But also the approach claims to train a maximum likelihood objective , where Langevin steps are used to approximate the energy landscape . A good energy landscape should be able to support an arbitrary number of MCMC sampling steps . What happens when a larger of number of steps of Langevin is run ? The paper states that past approaches such as [ 1 ] have relied on adding white noise to samples . I do not think this is the case . Can the authors elaborate ? My next major concern is that overall text of the paper is somewhat confusing to read , with some awkward sentences in the text . For example : Section 3.2 : Assuming the total number of stages is S and the training starts from the min = > Given a total of S stages of training In what follows , be aware of the only difference = > The only difference between In this way , both stability and time efficiency in training EBMs are benefited . = > In this way , both stability and time efficiency in training EBMs benefit . Section 3.3 : We name the style that can differentiate different domains discriminative saliency . = > We call this approach discriminative saliency . Conclusion : We owe the success to the newly proposed network architecture , smoothing activations , the powerful interpretability of EBM = > This success is due to our newly proposed architecture , smoothing activations , and the flexibility of EBM . To improve the clarity of the text , it would be good to perhaps add an algorithm block on the precise sampling procedure used to train EBMs . For example , the term short-run samples was never defined and I had to look at referenced papers to figure out the meaning . The related work seems to incomplete and misses a lot of recent work towards training EBMs . Some examples of additional approaches towards training EBMs including score matching and minimum probability flow learning . There are also some issues with the empirical evaluation of the proposed approach : To showcase mode coverage of the EBM , it 's important to show diverse inpaintings , as opposed to a single inpainting of an image . Furthermore , as deep likelihood based model it is important to show some density/likelihood evaluation -- using some variant of AIS . Minor Comments : Why is the proposed approach much more efficient than past approaches for image to image translation ? It seems Langevin sampling steps would take more computational time . [ 1 ] Yilun Du and Igor Mordatch , Implicit Generation and Generalization with EBMs [ 2 ] Ruiqi Gao , Yang Lu , Junpei Zhou , Song-Chun Zhu , and Ying Nian Wu . Learning generative convnets via multi-grid modeling and sampling . [ 3 ] Tero Karras et al.Progressive Growing of GANs for Improved Quality , Stability , and Variation == Post Rebuttal Updates I thank the authors for responding to my comments . The work shows improved generative performance by using a multi-scale architecture , however the approach is the same that used in previous GAN works . Furthermore , the generation quality is not as good those of recent GAN works and I do n't believe the added EBM benefits are significant . In addition , other contributions , such as the use of the Swish activation and domain transfer has also been noted as used in previous work [ 1 ] . I also have additional empirical concerns over the experiments . I list additional comments below : 1 ) The diverse inpaintings ( Figure 11 ) do not really look very diverse to me and seems to suggest that the model is not learning a likelihood . To evaluate diverse inpaintings , it would be good to follow past work and evaluated on ImageNet images where only the top half of an image is conditioned . [ 3 ] 2 ) The likelihood evaluation are hard to interpret in A.5 . An issue with evaluating AIS based likelihood sampling on MNIST for upper and lower bounds of likelihood depends heavily on the large number of steps of sampling required . Upper and lower bounds depend heavily on the number of steps of sampling run ( with unrealistically high likelihoods obtained when running only a few steps of AIS ) . It seems unlikely to me that the proposed model obtains a significant boost to log-likelihood compared to past approaches , and it would be good to report both the number of AIS transition distribution ( and ensure that it is same used in [ 1 ] ) . In particular , I believe that this approach is likely to perform poorly with a large number of gradient steps ( as the rebuttal response noted ) , which is required for proper evaluation of likelihood . 3 ) The related work is still missing older work in the area of EBM training . Instead of adding additional references to recent work on score based generative modeling , I think the others should cite past works that have used score matching to training energy models . For examples , such works include [ 4 , 5 , 6 ] . 4 ) I did n't find the out-of-distribution results to be a particularly compelling application of the model ( although its good that it performs similarly to past approaches ) . The only results that appear to be better are uniform ( which in my experience performance across all models fluctuates ) and interpolation . Furthermore , the model from [ 1 ] used in section 4.2 is not conditional . 5 ) I would n't say this paper is the first to generate 512x512 samples with EBMs . For example see [ 3 ] . 6 ) It 's difficult to evaluate an open source code release , since the code is not provided at the time . 7 ) Regarding the approach in [ 1 ] , when doing source translation images are initialized from ground truth images from a seperate class . [ 1 ] Yilun Du and Igor Mordatch , Implicit Generation and Generalization with EBMs [ 2 ] Aaron van den Oord , Nal Kalchbrenner , Koray Kavukcuoglu , Pixel Recurrent Neural Networks [ 3 ] Tian Han et al.Divergence Triangle for Joint Training of Generator Model , Energy-based Model , and Inferential Model [ 4 ] Jascha Sohl-Dickstein et al.Minimum Probability Flow Learning [ 5 ] Saeed Saremi . Deep Energy Estimator Networks [ 6 ] Hyv\u00e4rinen , Aapo . Estimation of non-normalized statistical models by score matching .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your insightful feedback on our presentation and suggestions for improvements . We will first address your concern about the novelty of our paper . # # # # Clarification on the difference from MultiGrid Our work essentially differs from MultiGrid [ 2 ] in the following three aspects : 1 . * * Different modeling strategy * * MultiGrid creates multiple EBMs ( i.e. , multiple energy networks ) to model different resolutions simultaneously , while our CF-EBM only has one single EBM , which grows gradually . Our model always keeps one learnable EBM . 2 . * * Different training strategies * * The two models have totally different training strategies . To be specific , in MultiGrid , each EBM needs a separate optimization and a separate sampling chain in each iteration . Each EBM uses its own synthesized examples to update its energy function . It requires a careful balance to learn and design these EBMs because the lower resolution EBM needs to produce samples to initialize the MCMC of the subsequent higher resolution EBM . Since MultiGrid trains all EBMs together , the whole framework will fail if one EBM collapses . The MultiGrid learning scheme appeared to be difficult when we tried to scale up . Our work trains only one EBM progressively and runs only one MCMC chain in each iteration . We propose a smooth transition and a dedicated architecture for EBM such that the training is very stable . 3 . * * Different empirical analysis * * Compared with MultiGrid , our method has presented a much higher resolution and better qualitative image samples and demonstrated a promising application on the unpaired image-to-image translation . In Table 1-3 , the proposed model achieves the best performance among all prior EBM models including MultiGrid , while using the fewest parameters and fewest sampling steps . We have shown * * 256x256 * * samples , which are firstly seen in EBM generative models . We also prepare to add * * 512x512 * * samples to demonstrate our model 's scalability in the revision . Section 4.2 presents the superior performance even compared with notable GAN approaches to the image translation task . # # # # Clarification on the difference from PGAN Given the fact that GAN and EBM are totally different frameworks , our proposed coarse-to-fine learning based on a single neural network is definitely non-trivial and essentially different even though EBM and PGAN share the same motivation . 1 . * * Different learning and sampling * * [ 3 ] samples from the generator and applies a discriminator to guide the learning . In contrast , EBM integrates both learning and sampling in one neural network . 2 . * * Incremental learning in EBM is very difficult * * . In [ 3 ] , the generator always learns a mapping from a * * fixed * * lower-dimensional prior to the higher dimensional data space . But in CF-EBM , since there is only one neural network and no such a latent variable model like generator , when we increase the resolution , the initial distribution dimension in Langevin dynamics will also * * increase * * accordingly . It dramatically increases the difficulty of coordinating EBM learning ( Eq.2 ) and the sampling procedure ( Eq.3 ) . We have proposed a dedicated architecture and a smooth transition process to stabilize the incremental learning in EBM . We also demonstrate the efficiency and the better sample quality compared with other models in Table 1-3 . 3 . * * Flexibility * * Our CF-EBM is more flexible than PGAN on various image synthesis tasks . We can apply CF-EBM on image generation , image inpainting , image denoising , and unpaired image-to-image translation , without changing architectures and objectives ."}, {"review_id": "aD1_5zowqV-1", "review_text": "Much like progressive growing of GANs two years ago , this paper adopts a similar coarse-to-fine procedure for scaling EBMs to higher resolutions . In particular , the approach starts from learning EBMs on low-resolution images and then smoothly transitions to higher resolution by carefully designing an expand layer and a smooth sampling procedure . Authors were able to obtain competitive FID scores on CIFAR-10 , and demonstrate the first set of 256x256 image samples from EBMs . In addition , authors demonstrate successful application of EBMs to unpaired image-to-image translation . # # # # Pros * Strong experimental results . The FID scores obtained are among the best achievable results with maximum likelihood training . Scaling to 256x256 images is a great advancement of the field . Various experiments on denoising and inpainting demonstrates the versatile applications of EBMs . * Applications in unpaired image-to-image translation demonstrate strong potential of the approach in image editing * Sampling typically takes 50 Langevin steps . This is much faster than denoising score matching with Langevin dynamics or denoising diffusion probabilistic models , which typically need thousands of Langevin steps . # # # # Cons * FID computation is done using a PyTorch implementation . Although the results should be close to the original FID implementation based on TensorFlow , they are not exactly comparable to previous results . The numbers in tables are therefore not rigorous . * There is no widely agreed protocol on how to compute FIDs on CelebA 64 . The processing methods can be quite different in different papers , and the number of samples used for FID computation can also be different . Please include your settings explicitly in the paper , otherwise the comparison in the table is not that meaningful . * The progressive growing procedure requires multiple stages of separate training and also need careful tuning of model architecture and sampling to make transition smooth . * There is a factual error in Appendix A.3.1.The work of Song & Ermon ( 2020 ) does not apply a sequence of decayed perturbation from 1 to 0.01 . Quite the reverse , starting from noise scales different from 1 is one main point in that paper . - Post-rebuttal Reporting PyTorch-computed FIDs risk the fairness when comparing against previous work . The repo quoted by the authors had a well-known issue , leading to much lower numbers compared to those computed by the original TensorFlow repo . Although this issue has been alleviated following some code update this year , the numbers still wo n't exactly match those of TensorFlow . Therefore , from a scientific perspective , the FID numbers have to be recomputed with the original TensorFlow code for a rigorous publication . In addition , the FID number reported by the authors for their model is computed between 10k samples and the test dataset , while most other FID numbers in Table 1 are computed between 50k samples and the training set ( following the original settings of [ 1 ] ) . This also makes the comparison not fair . I completely agree that FID is a flawed metric and lower FID scores do not necessarily indicate better sample quality . However , since the authors choose to report FIDs and compare against those directly ported from previous work , they have to follow the convention and ensure a fair comparison . Since I did n't get a satisfying response from the authors ( authors claimed to `` have corrected it '' , they just changed phrasing of their response but did n't re-compute scores ) , I decide to lower my scores from 7 to 6 . I am still marginally inclined to acceptance , but will leave it to the discretion of the AC on whether this paper should be rejected due to flawed FID computation . [ 1 ] Heusel , M. , Ramsauer , H. , Unterthiner , T. , Nessler , B. and Hochreiter , S. , 2017 . Gans trained by a two time-scale update rule converge to a local nash equilibrium . In Advances in neural information processing systems ( pp.6626-6637 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your acknowledgment of our work ! Your concerns are addressed as : 1 . * * FID computation * * To give a fair FID comparison between Tensorflow and PyTorch implementations , the adopted repository is most widely used by researchers in PyTorch community , * e.g . * in short-run EBM [ 1 ] . 2 . * * CelebA FID comparison * * We preprocess CelebA to produce CelebA-64 in two settings . 1 ) Each image is firstly center-cropped into $ 140\\times140 $ and then resized to $ 64\\times64 $ . In Table 2 , three baselines NCSN , mr-Langevin [ 3 ] , and WGAN-GP applied this setting . The FID computation protocol is also based on NCSN where the score is calculated between 10k samples and all test images . 2 ) Each image is directly resized to $ 64\\times64 $ . FID is computed on 40k generated samples . In Table 2 , VAE , DCGAN , and short-run EBM exactly follow this setting . We will clarify these in the Appendix . 3 . * * Model design and training * * The proposed multi-stage learning scheme in EBM is very efficient with a smaller model size and fewer sampling steps . We list the model comparison in Table 3 . The current model is very easy to scale up with one single GPU . In contrast , some EBMs with very deep networks require multiple GPU for training . We will also release the code and contribute to the community . To facilitate the use , everyone can play with this model and only need to change the channel number of the CNN block or import a customized data loader . 4 . * * Clarification of Song \\ & Ermon ( 2020 ) * * Regarding the description of Song \\ & Ermon ( 2020 ) , thanks for pointing this out . The reference should be Song \\ & Ermon ( 2019 ) . We have corrected it . Thanks again for your insightful reviews , and we will include these suggestions in the revision soon . Please note the reminder of the upcoming revision . [ 1 ] Nijkamp E , Hill M , Zhu SC , Wu YN . Learning non-convergent non-persistent short-run MCMC toward energy-based model , in NeurIPS 2019 . [ 2 ] Song , Yang , and Stefano Ermon . `` Generative modeling by estimating gradients of the data distribution . `` , in NeurIPS 2019 . [ 3 ] Block , Adam , et al . `` Fast Mixing of Multi-Scale Langevin Dynamics under the Manifold Hypothesis . `` , 2020 ."}, {"review_id": "aD1_5zowqV-2", "review_text": "Summary : This paper presents a number of methods to scale up training and sampling of EBMs on image data . The main contribution consists of an approach for progressively growing the model by increasing the image resolution as training progresses . This approach echos similar approaches used for scaling up GAN training . The approach involves slowly annealing in new blocks to the model during training which processes the image at increasing resolutions . This allows training of image EBMs at notably larger resolutions than published in prior work . The authors also present a few additional architectural improvements such as using smooth nonlinearities over the currently-popular leaky ReLU . They find that this change allows us to remove the commonly-used Gaussian noise added to the input , improving sample quality . The authors demonstrate the performance of their approach , focusing on image generation . They present results with FID and compare to other generative models and EBMs . They also examine the quality of the learned energy functions with inpainting and denoising experiments . Beyond unconditional image generation , the authors also propose a simple method for image-translation based on EBMs . In this approach , unconditional EBMs are trained for each domain and an image is translated by taking a sample from 1 domain and running a Langevin MCMC sampler in the target domain EBM starting from the sample from the source domain . The resulting image retains many of the high level characteristics of the source image but adds low level characteristics from the target domain . Surprisingly this simple approach outperforms more involved methods based on adversarial training . Strong areas : The empirical results in this paper are impressive , made more impressive by the simple nature of the approach . It appears that the proposed method gives a notable improvement in image generation quality over prior EBM models while also giving an improvement in run-time and parameter-efficiency . Particularly interesting are the very strong results in the image-translation task . I am not familiar with this space so I can not say how much of an improvement is reported here over the adversarial methods , but it is quite shocking to me that this very simple approach would outperform two recent state-of-the-art methods for this task while also having no cycle consistency objective . This should certainly bring attention to EBMs for this task . Weaknesses : I felt that many of the experimental details in this paper could have been made more clear . I found myself having to re-read the paper to find out that the authors used short-run MCMC instead of PCD . As these training procedures are quite different , this should be made more clear . I also found the proposed architecture hard to understand . I could not tell if the full energy function continues to take in lower-resolution versions of the image as we add levels or if those inputs are replaced by the outputs of the added higher-resolution layers . The figures are not very clear on this . When sampling high-resolution images , do we first generate lower resolution samples , and use these to seed the high-resolution samples ? Figure 2a makes it look like this is what is happening . But then algorithm 1 makes it seem like we generate samples directly from noise for the current training scale . This is important for understanding the method and it could be made much more clear . I would recommend the authors split apart algorithm 1 into an algorithm for training and one for sampling . I think that would make the method much more clear . While the experimental results were strong , I am curious why the authors did not present any results on out-of-distribution detection ( aka anomaly detection ) . These are common nowadays and I am curious as to how this approach will perform given its improved scalability over many EBM models which we find perform very well at this task . My recommendation : This is an interesting work that presents a simple method that allows image EBMs to scale up considerably . The proposed approach notably improves image generation and allows models to be trained on much larger images than has been possible by EBMs in the past . Further , the image-translation application is intriguing and the strong results should make EBMs a standard baseline for that application . Because of these strong empirical results , I am recommending to accept this paper , but I am not advocating for a strong-accept . While the results are strong , the work does not improve our understanding of these models , or introduce any particularly novel techniques . I think the contributions of this work could be made greater with some experiments on some other applications such as OOD . I also think the work could be greatly improved by some rewrites to section 3.2 . This is the main contribution of the paper and the authors should take more care to ensure that their effort is easily understood .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your support of our work . Here we would like to address your main concerns briefly : 1 . Yes.This work is most related to short-run MCMC . The sampler always starts from a fixed uniform distribution , which is essentially different from PCD . We will make it more clear . 2.In Figure 2 ( a ) , we train the model stage by stage , and the previous low-resolution EBM is merged into the high-resolution EBM . This is consistent with Algorithm 1 . Therefore , we only keep one trainable EBM . For sampling , we directly generate samples from a uniform noise , which has the same dimension as the final output image , without starting from low-resolution . We will provide a new figure and make it more evident in the revision . 3.We are working on ODD and will update it in the revision very soon . 4.Thanks for your suggestion . We are working on revising the manuscript , especially the model part . Please note the reminder of the revision , and we appreciate more advice on improving this work ."}, {"review_id": "aD1_5zowqV-3", "review_text": "The authors propose an energy-based generative model for image-to-image generation that differs from previous methods by incorporating a progressive learning scheme that gradually increases the resolution of images being trained on . Strengths : -- The presence of a saliency map in image-to-image translation is a benefit that is harder to get out of other methods . -- The ability to infill missing parts of an image with the same model that does unsupervised translation is a nice benefit . -- The smoothing along resolutions during training seems like a good way to incorporate the progressive training technique into this model . Weaknesses : -- The novelty here is very limited . The difference between this and previous energy-based models is the scheme of progressively generating at 8x8 , 16x16 , 32x32 , etc ... which itself is a well-established technique from the Progressive GAN . Given neither of these things is new , the novelty lies in just using the one with the other . -- The authors claim the saliency map is a benefit of their method but offer no comparisons with other models , e.g.the marginal gradient methods for any CNN model including CycleGANs . -- The quantitative scores for the GAN methods are significantly worse than models can actually achieve now . The Big-GAN , which is several years old now and not even SOTA on CIFAR has a better score than the one proposed here . -- The qualitative results from the GANs are much poorer than can actually be achieved . For example , the orange- > apple not changing shape is a result of a poor architecture with too high of a cycle-consistency coefficient . -- The characterization of GANs in the previous work section is very poor . The authors claim , without evidence or citation , that the invertibility of the two directions in a CycleGAN `` may intensify long-standing instability issues '' when it very likely does the * opposite * . They also criticize GANs for requiring an `` elegant design '' like instance normalization , when this is a standard and easy-to-implement part and is no more problematic than any aspects of the authors ' proposed framework . Also masquerading as a `` critique '' is that GANs have a `` fancy architecture '' : I do n't think the authors could devise a definition of fancy that excludes their model . If this section is designed to motivate their energy-based approach , it can be done without using arbitrary , subjective , inaccurate insults of other methods .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate your efforts in providing constructive suggestions on the current presentation . We want to address your concerns as : * * Clarification on the difference from PGAN * * Given the fact that GAN and EBM are totally different frameworks , our proposed coarse-to-fine learning based on a single neural network is definitely non-trivial and essentially different even though EBM and PGAN share the same motivation . 1 . * * Different learning and sampling * * PGAN [ 3 ] samples from the generator and applies a discriminator to guide the learning . In contrast , EBM integrates both learning and sampling in one neural network . 2 . * * The incremental learning in EBM is very difficult * * In PGAN [ 3 ] , the generator always learns a mapping from a * * fixed * * lower-dimensional prior to the higher dimensional data space . But in CF-EBM , since there is only one neural network and no such a latent variable model like generator , when we increase the resolution , the initial distribution dimension in Langevin dynamics will also * * increase * * accordingly . It dramatically increases the difficulty of coordinating EBM learning ( Eq.2 ) and the sampling procedure ( Eq.3 ) . We have proposed a dedicated architecture and a smooth transition process to stabilize the incremental learning in EBM . We also demonstrate the efficiency and the better sample quality compared with other models in Table 1-3 . 3 . * * Flexibility * * Our CF-EBM is more flexible than PGAN on various image synthesis tasks . We can apply CF-EBM on image generation , image inpainting , image denoising and unpaired image-to-image translation , without changing architectures and objectives . * * Choosing baselines * * Thanks for your suggestion . In the current version , we follow previous EBM works [ 1 , 4 ] that choose SNGAN as a strong baseline . We also note that there are better models on CIFAR10 generation like StyleGAN2-ADA [ 5 ] . We have added it to the revision . * * About the saliency map * * 1 . * * The saliency map from our method is naturally integrated so we would n't say it 's a benefit * * Because to sample from the model , we have to calculate the score function from which the saliency map is very naturally derived . Under our assumption , the score function exactly points out the magnitude and direction that the source image is adjusted to match the target domain . The saliency map is the inherent foundation that can make our approach work and provides a convenient way to visualize the dynamic translation process . Therefore , the goal of introducing the saliency map is twofold : 1 ) Validate our assumption that our approach can implicitly transfer styles while preserving content . 2 ) Give an intuitive illustration about which region has been adjusted via MCMC . 2.Indeed , we can calculate the gradient of the discriminator output in terms of the fake output from a generator in GAN-based approaches . However , this gradient wo n't teach the generator the exact amount and direction to edit the input to match the target but to indicate the field discriminator will respond to distinguish the fake and the real . Therefore , the gradients have different meanings between GAN and CF-EBM ."}], "0": {"review_id": "aD1_5zowqV-0", "review_text": "This work proposes a method to improve generation with energy based models . The work further shows how energy based models can also be extended to cross class image translation . Strong Points : Generated samples appear to look good Weak Points : My most major concern is that the overall technical novelty of the work is rather limited , and I do not think passes the bar of acceptance for ICLR . Past work [ 2 ] has shown that EBMs can composed with multi-scale sampling , while [ 1 ] notes that the smooth activation function Swish significantly improves EBM generation . Similarly , cross domain generation with EBMs have been previously demonstrated such as in [ 1 ] . While [ 2 ] uses a separate hierarchy of multi-scale EBMs , this work uses the sampling scheme in [ 3 ] . My second most major concern is other the theoretical framing of the proposed approach and well is issues with statements made in the paper . W The proposed sampler linearly decays the noise schedule to 0 , but what are the theoretical implication of such a behavior ? For Langevin sampling to valid , the step size of both the model and the noise must decay to 0 . The proposed approach generates samples by running a fixed number of Langevin steps to generate a sample . But also the approach claims to train a maximum likelihood objective , where Langevin steps are used to approximate the energy landscape . A good energy landscape should be able to support an arbitrary number of MCMC sampling steps . What happens when a larger of number of steps of Langevin is run ? The paper states that past approaches such as [ 1 ] have relied on adding white noise to samples . I do not think this is the case . Can the authors elaborate ? My next major concern is that overall text of the paper is somewhat confusing to read , with some awkward sentences in the text . For example : Section 3.2 : Assuming the total number of stages is S and the training starts from the min = > Given a total of S stages of training In what follows , be aware of the only difference = > The only difference between In this way , both stability and time efficiency in training EBMs are benefited . = > In this way , both stability and time efficiency in training EBMs benefit . Section 3.3 : We name the style that can differentiate different domains discriminative saliency . = > We call this approach discriminative saliency . Conclusion : We owe the success to the newly proposed network architecture , smoothing activations , the powerful interpretability of EBM = > This success is due to our newly proposed architecture , smoothing activations , and the flexibility of EBM . To improve the clarity of the text , it would be good to perhaps add an algorithm block on the precise sampling procedure used to train EBMs . For example , the term short-run samples was never defined and I had to look at referenced papers to figure out the meaning . The related work seems to incomplete and misses a lot of recent work towards training EBMs . Some examples of additional approaches towards training EBMs including score matching and minimum probability flow learning . There are also some issues with the empirical evaluation of the proposed approach : To showcase mode coverage of the EBM , it 's important to show diverse inpaintings , as opposed to a single inpainting of an image . Furthermore , as deep likelihood based model it is important to show some density/likelihood evaluation -- using some variant of AIS . Minor Comments : Why is the proposed approach much more efficient than past approaches for image to image translation ? It seems Langevin sampling steps would take more computational time . [ 1 ] Yilun Du and Igor Mordatch , Implicit Generation and Generalization with EBMs [ 2 ] Ruiqi Gao , Yang Lu , Junpei Zhou , Song-Chun Zhu , and Ying Nian Wu . Learning generative convnets via multi-grid modeling and sampling . [ 3 ] Tero Karras et al.Progressive Growing of GANs for Improved Quality , Stability , and Variation == Post Rebuttal Updates I thank the authors for responding to my comments . The work shows improved generative performance by using a multi-scale architecture , however the approach is the same that used in previous GAN works . Furthermore , the generation quality is not as good those of recent GAN works and I do n't believe the added EBM benefits are significant . In addition , other contributions , such as the use of the Swish activation and domain transfer has also been noted as used in previous work [ 1 ] . I also have additional empirical concerns over the experiments . I list additional comments below : 1 ) The diverse inpaintings ( Figure 11 ) do not really look very diverse to me and seems to suggest that the model is not learning a likelihood . To evaluate diverse inpaintings , it would be good to follow past work and evaluated on ImageNet images where only the top half of an image is conditioned . [ 3 ] 2 ) The likelihood evaluation are hard to interpret in A.5 . An issue with evaluating AIS based likelihood sampling on MNIST for upper and lower bounds of likelihood depends heavily on the large number of steps of sampling required . Upper and lower bounds depend heavily on the number of steps of sampling run ( with unrealistically high likelihoods obtained when running only a few steps of AIS ) . It seems unlikely to me that the proposed model obtains a significant boost to log-likelihood compared to past approaches , and it would be good to report both the number of AIS transition distribution ( and ensure that it is same used in [ 1 ] ) . In particular , I believe that this approach is likely to perform poorly with a large number of gradient steps ( as the rebuttal response noted ) , which is required for proper evaluation of likelihood . 3 ) The related work is still missing older work in the area of EBM training . Instead of adding additional references to recent work on score based generative modeling , I think the others should cite past works that have used score matching to training energy models . For examples , such works include [ 4 , 5 , 6 ] . 4 ) I did n't find the out-of-distribution results to be a particularly compelling application of the model ( although its good that it performs similarly to past approaches ) . The only results that appear to be better are uniform ( which in my experience performance across all models fluctuates ) and interpolation . Furthermore , the model from [ 1 ] used in section 4.2 is not conditional . 5 ) I would n't say this paper is the first to generate 512x512 samples with EBMs . For example see [ 3 ] . 6 ) It 's difficult to evaluate an open source code release , since the code is not provided at the time . 7 ) Regarding the approach in [ 1 ] , when doing source translation images are initialized from ground truth images from a seperate class . [ 1 ] Yilun Du and Igor Mordatch , Implicit Generation and Generalization with EBMs [ 2 ] Aaron van den Oord , Nal Kalchbrenner , Koray Kavukcuoglu , Pixel Recurrent Neural Networks [ 3 ] Tian Han et al.Divergence Triangle for Joint Training of Generator Model , Energy-based Model , and Inferential Model [ 4 ] Jascha Sohl-Dickstein et al.Minimum Probability Flow Learning [ 5 ] Saeed Saremi . Deep Energy Estimator Networks [ 6 ] Hyv\u00e4rinen , Aapo . Estimation of non-normalized statistical models by score matching .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your insightful feedback on our presentation and suggestions for improvements . We will first address your concern about the novelty of our paper . # # # # Clarification on the difference from MultiGrid Our work essentially differs from MultiGrid [ 2 ] in the following three aspects : 1 . * * Different modeling strategy * * MultiGrid creates multiple EBMs ( i.e. , multiple energy networks ) to model different resolutions simultaneously , while our CF-EBM only has one single EBM , which grows gradually . Our model always keeps one learnable EBM . 2 . * * Different training strategies * * The two models have totally different training strategies . To be specific , in MultiGrid , each EBM needs a separate optimization and a separate sampling chain in each iteration . Each EBM uses its own synthesized examples to update its energy function . It requires a careful balance to learn and design these EBMs because the lower resolution EBM needs to produce samples to initialize the MCMC of the subsequent higher resolution EBM . Since MultiGrid trains all EBMs together , the whole framework will fail if one EBM collapses . The MultiGrid learning scheme appeared to be difficult when we tried to scale up . Our work trains only one EBM progressively and runs only one MCMC chain in each iteration . We propose a smooth transition and a dedicated architecture for EBM such that the training is very stable . 3 . * * Different empirical analysis * * Compared with MultiGrid , our method has presented a much higher resolution and better qualitative image samples and demonstrated a promising application on the unpaired image-to-image translation . In Table 1-3 , the proposed model achieves the best performance among all prior EBM models including MultiGrid , while using the fewest parameters and fewest sampling steps . We have shown * * 256x256 * * samples , which are firstly seen in EBM generative models . We also prepare to add * * 512x512 * * samples to demonstrate our model 's scalability in the revision . Section 4.2 presents the superior performance even compared with notable GAN approaches to the image translation task . # # # # Clarification on the difference from PGAN Given the fact that GAN and EBM are totally different frameworks , our proposed coarse-to-fine learning based on a single neural network is definitely non-trivial and essentially different even though EBM and PGAN share the same motivation . 1 . * * Different learning and sampling * * [ 3 ] samples from the generator and applies a discriminator to guide the learning . In contrast , EBM integrates both learning and sampling in one neural network . 2 . * * Incremental learning in EBM is very difficult * * . In [ 3 ] , the generator always learns a mapping from a * * fixed * * lower-dimensional prior to the higher dimensional data space . But in CF-EBM , since there is only one neural network and no such a latent variable model like generator , when we increase the resolution , the initial distribution dimension in Langevin dynamics will also * * increase * * accordingly . It dramatically increases the difficulty of coordinating EBM learning ( Eq.2 ) and the sampling procedure ( Eq.3 ) . We have proposed a dedicated architecture and a smooth transition process to stabilize the incremental learning in EBM . We also demonstrate the efficiency and the better sample quality compared with other models in Table 1-3 . 3 . * * Flexibility * * Our CF-EBM is more flexible than PGAN on various image synthesis tasks . We can apply CF-EBM on image generation , image inpainting , image denoising , and unpaired image-to-image translation , without changing architectures and objectives ."}, "1": {"review_id": "aD1_5zowqV-1", "review_text": "Much like progressive growing of GANs two years ago , this paper adopts a similar coarse-to-fine procedure for scaling EBMs to higher resolutions . In particular , the approach starts from learning EBMs on low-resolution images and then smoothly transitions to higher resolution by carefully designing an expand layer and a smooth sampling procedure . Authors were able to obtain competitive FID scores on CIFAR-10 , and demonstrate the first set of 256x256 image samples from EBMs . In addition , authors demonstrate successful application of EBMs to unpaired image-to-image translation . # # # # Pros * Strong experimental results . The FID scores obtained are among the best achievable results with maximum likelihood training . Scaling to 256x256 images is a great advancement of the field . Various experiments on denoising and inpainting demonstrates the versatile applications of EBMs . * Applications in unpaired image-to-image translation demonstrate strong potential of the approach in image editing * Sampling typically takes 50 Langevin steps . This is much faster than denoising score matching with Langevin dynamics or denoising diffusion probabilistic models , which typically need thousands of Langevin steps . # # # # Cons * FID computation is done using a PyTorch implementation . Although the results should be close to the original FID implementation based on TensorFlow , they are not exactly comparable to previous results . The numbers in tables are therefore not rigorous . * There is no widely agreed protocol on how to compute FIDs on CelebA 64 . The processing methods can be quite different in different papers , and the number of samples used for FID computation can also be different . Please include your settings explicitly in the paper , otherwise the comparison in the table is not that meaningful . * The progressive growing procedure requires multiple stages of separate training and also need careful tuning of model architecture and sampling to make transition smooth . * There is a factual error in Appendix A.3.1.The work of Song & Ermon ( 2020 ) does not apply a sequence of decayed perturbation from 1 to 0.01 . Quite the reverse , starting from noise scales different from 1 is one main point in that paper . - Post-rebuttal Reporting PyTorch-computed FIDs risk the fairness when comparing against previous work . The repo quoted by the authors had a well-known issue , leading to much lower numbers compared to those computed by the original TensorFlow repo . Although this issue has been alleviated following some code update this year , the numbers still wo n't exactly match those of TensorFlow . Therefore , from a scientific perspective , the FID numbers have to be recomputed with the original TensorFlow code for a rigorous publication . In addition , the FID number reported by the authors for their model is computed between 10k samples and the test dataset , while most other FID numbers in Table 1 are computed between 50k samples and the training set ( following the original settings of [ 1 ] ) . This also makes the comparison not fair . I completely agree that FID is a flawed metric and lower FID scores do not necessarily indicate better sample quality . However , since the authors choose to report FIDs and compare against those directly ported from previous work , they have to follow the convention and ensure a fair comparison . Since I did n't get a satisfying response from the authors ( authors claimed to `` have corrected it '' , they just changed phrasing of their response but did n't re-compute scores ) , I decide to lower my scores from 7 to 6 . I am still marginally inclined to acceptance , but will leave it to the discretion of the AC on whether this paper should be rejected due to flawed FID computation . [ 1 ] Heusel , M. , Ramsauer , H. , Unterthiner , T. , Nessler , B. and Hochreiter , S. , 2017 . Gans trained by a two time-scale update rule converge to a local nash equilibrium . In Advances in neural information processing systems ( pp.6626-6637 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your acknowledgment of our work ! Your concerns are addressed as : 1 . * * FID computation * * To give a fair FID comparison between Tensorflow and PyTorch implementations , the adopted repository is most widely used by researchers in PyTorch community , * e.g . * in short-run EBM [ 1 ] . 2 . * * CelebA FID comparison * * We preprocess CelebA to produce CelebA-64 in two settings . 1 ) Each image is firstly center-cropped into $ 140\\times140 $ and then resized to $ 64\\times64 $ . In Table 2 , three baselines NCSN , mr-Langevin [ 3 ] , and WGAN-GP applied this setting . The FID computation protocol is also based on NCSN where the score is calculated between 10k samples and all test images . 2 ) Each image is directly resized to $ 64\\times64 $ . FID is computed on 40k generated samples . In Table 2 , VAE , DCGAN , and short-run EBM exactly follow this setting . We will clarify these in the Appendix . 3 . * * Model design and training * * The proposed multi-stage learning scheme in EBM is very efficient with a smaller model size and fewer sampling steps . We list the model comparison in Table 3 . The current model is very easy to scale up with one single GPU . In contrast , some EBMs with very deep networks require multiple GPU for training . We will also release the code and contribute to the community . To facilitate the use , everyone can play with this model and only need to change the channel number of the CNN block or import a customized data loader . 4 . * * Clarification of Song \\ & Ermon ( 2020 ) * * Regarding the description of Song \\ & Ermon ( 2020 ) , thanks for pointing this out . The reference should be Song \\ & Ermon ( 2019 ) . We have corrected it . Thanks again for your insightful reviews , and we will include these suggestions in the revision soon . Please note the reminder of the upcoming revision . [ 1 ] Nijkamp E , Hill M , Zhu SC , Wu YN . Learning non-convergent non-persistent short-run MCMC toward energy-based model , in NeurIPS 2019 . [ 2 ] Song , Yang , and Stefano Ermon . `` Generative modeling by estimating gradients of the data distribution . `` , in NeurIPS 2019 . [ 3 ] Block , Adam , et al . `` Fast Mixing of Multi-Scale Langevin Dynamics under the Manifold Hypothesis . `` , 2020 ."}, "2": {"review_id": "aD1_5zowqV-2", "review_text": "Summary : This paper presents a number of methods to scale up training and sampling of EBMs on image data . The main contribution consists of an approach for progressively growing the model by increasing the image resolution as training progresses . This approach echos similar approaches used for scaling up GAN training . The approach involves slowly annealing in new blocks to the model during training which processes the image at increasing resolutions . This allows training of image EBMs at notably larger resolutions than published in prior work . The authors also present a few additional architectural improvements such as using smooth nonlinearities over the currently-popular leaky ReLU . They find that this change allows us to remove the commonly-used Gaussian noise added to the input , improving sample quality . The authors demonstrate the performance of their approach , focusing on image generation . They present results with FID and compare to other generative models and EBMs . They also examine the quality of the learned energy functions with inpainting and denoising experiments . Beyond unconditional image generation , the authors also propose a simple method for image-translation based on EBMs . In this approach , unconditional EBMs are trained for each domain and an image is translated by taking a sample from 1 domain and running a Langevin MCMC sampler in the target domain EBM starting from the sample from the source domain . The resulting image retains many of the high level characteristics of the source image but adds low level characteristics from the target domain . Surprisingly this simple approach outperforms more involved methods based on adversarial training . Strong areas : The empirical results in this paper are impressive , made more impressive by the simple nature of the approach . It appears that the proposed method gives a notable improvement in image generation quality over prior EBM models while also giving an improvement in run-time and parameter-efficiency . Particularly interesting are the very strong results in the image-translation task . I am not familiar with this space so I can not say how much of an improvement is reported here over the adversarial methods , but it is quite shocking to me that this very simple approach would outperform two recent state-of-the-art methods for this task while also having no cycle consistency objective . This should certainly bring attention to EBMs for this task . Weaknesses : I felt that many of the experimental details in this paper could have been made more clear . I found myself having to re-read the paper to find out that the authors used short-run MCMC instead of PCD . As these training procedures are quite different , this should be made more clear . I also found the proposed architecture hard to understand . I could not tell if the full energy function continues to take in lower-resolution versions of the image as we add levels or if those inputs are replaced by the outputs of the added higher-resolution layers . The figures are not very clear on this . When sampling high-resolution images , do we first generate lower resolution samples , and use these to seed the high-resolution samples ? Figure 2a makes it look like this is what is happening . But then algorithm 1 makes it seem like we generate samples directly from noise for the current training scale . This is important for understanding the method and it could be made much more clear . I would recommend the authors split apart algorithm 1 into an algorithm for training and one for sampling . I think that would make the method much more clear . While the experimental results were strong , I am curious why the authors did not present any results on out-of-distribution detection ( aka anomaly detection ) . These are common nowadays and I am curious as to how this approach will perform given its improved scalability over many EBM models which we find perform very well at this task . My recommendation : This is an interesting work that presents a simple method that allows image EBMs to scale up considerably . The proposed approach notably improves image generation and allows models to be trained on much larger images than has been possible by EBMs in the past . Further , the image-translation application is intriguing and the strong results should make EBMs a standard baseline for that application . Because of these strong empirical results , I am recommending to accept this paper , but I am not advocating for a strong-accept . While the results are strong , the work does not improve our understanding of these models , or introduce any particularly novel techniques . I think the contributions of this work could be made greater with some experiments on some other applications such as OOD . I also think the work could be greatly improved by some rewrites to section 3.2 . This is the main contribution of the paper and the authors should take more care to ensure that their effort is easily understood .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your support of our work . Here we would like to address your main concerns briefly : 1 . Yes.This work is most related to short-run MCMC . The sampler always starts from a fixed uniform distribution , which is essentially different from PCD . We will make it more clear . 2.In Figure 2 ( a ) , we train the model stage by stage , and the previous low-resolution EBM is merged into the high-resolution EBM . This is consistent with Algorithm 1 . Therefore , we only keep one trainable EBM . For sampling , we directly generate samples from a uniform noise , which has the same dimension as the final output image , without starting from low-resolution . We will provide a new figure and make it more evident in the revision . 3.We are working on ODD and will update it in the revision very soon . 4.Thanks for your suggestion . We are working on revising the manuscript , especially the model part . Please note the reminder of the revision , and we appreciate more advice on improving this work ."}, "3": {"review_id": "aD1_5zowqV-3", "review_text": "The authors propose an energy-based generative model for image-to-image generation that differs from previous methods by incorporating a progressive learning scheme that gradually increases the resolution of images being trained on . Strengths : -- The presence of a saliency map in image-to-image translation is a benefit that is harder to get out of other methods . -- The ability to infill missing parts of an image with the same model that does unsupervised translation is a nice benefit . -- The smoothing along resolutions during training seems like a good way to incorporate the progressive training technique into this model . Weaknesses : -- The novelty here is very limited . The difference between this and previous energy-based models is the scheme of progressively generating at 8x8 , 16x16 , 32x32 , etc ... which itself is a well-established technique from the Progressive GAN . Given neither of these things is new , the novelty lies in just using the one with the other . -- The authors claim the saliency map is a benefit of their method but offer no comparisons with other models , e.g.the marginal gradient methods for any CNN model including CycleGANs . -- The quantitative scores for the GAN methods are significantly worse than models can actually achieve now . The Big-GAN , which is several years old now and not even SOTA on CIFAR has a better score than the one proposed here . -- The qualitative results from the GANs are much poorer than can actually be achieved . For example , the orange- > apple not changing shape is a result of a poor architecture with too high of a cycle-consistency coefficient . -- The characterization of GANs in the previous work section is very poor . The authors claim , without evidence or citation , that the invertibility of the two directions in a CycleGAN `` may intensify long-standing instability issues '' when it very likely does the * opposite * . They also criticize GANs for requiring an `` elegant design '' like instance normalization , when this is a standard and easy-to-implement part and is no more problematic than any aspects of the authors ' proposed framework . Also masquerading as a `` critique '' is that GANs have a `` fancy architecture '' : I do n't think the authors could devise a definition of fancy that excludes their model . If this section is designed to motivate their energy-based approach , it can be done without using arbitrary , subjective , inaccurate insults of other methods .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate your efforts in providing constructive suggestions on the current presentation . We want to address your concerns as : * * Clarification on the difference from PGAN * * Given the fact that GAN and EBM are totally different frameworks , our proposed coarse-to-fine learning based on a single neural network is definitely non-trivial and essentially different even though EBM and PGAN share the same motivation . 1 . * * Different learning and sampling * * PGAN [ 3 ] samples from the generator and applies a discriminator to guide the learning . In contrast , EBM integrates both learning and sampling in one neural network . 2 . * * The incremental learning in EBM is very difficult * * In PGAN [ 3 ] , the generator always learns a mapping from a * * fixed * * lower-dimensional prior to the higher dimensional data space . But in CF-EBM , since there is only one neural network and no such a latent variable model like generator , when we increase the resolution , the initial distribution dimension in Langevin dynamics will also * * increase * * accordingly . It dramatically increases the difficulty of coordinating EBM learning ( Eq.2 ) and the sampling procedure ( Eq.3 ) . We have proposed a dedicated architecture and a smooth transition process to stabilize the incremental learning in EBM . We also demonstrate the efficiency and the better sample quality compared with other models in Table 1-3 . 3 . * * Flexibility * * Our CF-EBM is more flexible than PGAN on various image synthesis tasks . We can apply CF-EBM on image generation , image inpainting , image denoising and unpaired image-to-image translation , without changing architectures and objectives . * * Choosing baselines * * Thanks for your suggestion . In the current version , we follow previous EBM works [ 1 , 4 ] that choose SNGAN as a strong baseline . We also note that there are better models on CIFAR10 generation like StyleGAN2-ADA [ 5 ] . We have added it to the revision . * * About the saliency map * * 1 . * * The saliency map from our method is naturally integrated so we would n't say it 's a benefit * * Because to sample from the model , we have to calculate the score function from which the saliency map is very naturally derived . Under our assumption , the score function exactly points out the magnitude and direction that the source image is adjusted to match the target domain . The saliency map is the inherent foundation that can make our approach work and provides a convenient way to visualize the dynamic translation process . Therefore , the goal of introducing the saliency map is twofold : 1 ) Validate our assumption that our approach can implicitly transfer styles while preserving content . 2 ) Give an intuitive illustration about which region has been adjusted via MCMC . 2.Indeed , we can calculate the gradient of the discriminator output in terms of the fake output from a generator in GAN-based approaches . However , this gradient wo n't teach the generator the exact amount and direction to edit the input to match the target but to indicate the field discriminator will respond to distinguish the fake and the real . Therefore , the gradients have different meanings between GAN and CF-EBM ."}}