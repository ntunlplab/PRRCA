{"year": "2017", "forum": "HyAddcLge", "title": "Revisiting Distributed Synchronous SGD", "decision": "Reject", "meta_review": "Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that \"backup workers\" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware.", "reviews": [{"review_id": "HyAddcLge-0", "review_text": "This paper was easy to read, the main idea was presented very clearly. The main points of the paper (and my concerns are below) can be summarized as follows: 1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a \"MPI_Reduce\" would be used (even if we wait for the slowest guy)? 2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain. 3.they propose to take gradient from the first \"N\" workers out of \"N+b\" workers available. My concern here is that they focused only on the workers, but what if the \"parameter server\" will became to slow? What if the parameter server would be the bottleneck? How would you address this situation? But still if the number of nodes (N) is not large, and the deep DNN is used, I can imagine that the communciation will not take more than 30% of the run-time. My largest concern is with the experiments. Different batch size implies that different learning rate should be chosen, right? How did you tune the learning rates and other parameters for e.g. Figure 5 you provide some formulas in (A2) but clearly this can bias your Figures, right? meaning, that if you tune \"\\gamma, \\beta\" for each N, it could be somehow more representative? also it would be nicer if you run the experiment many times and then report average, best and worst case behaviour. because now it can be just coinsidence, right? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the review and suggestions , and address the concerns raised in detail below . 1.The reviewer suggests that stragglers are not an issue with dedicated machines and proper implementation of communications , which is counter to our experiences . The issue of stragglers is exacerbated with larger number of machines , posing a challenge to scalability -- perhaps the reviewer has been using a relatively smaller number of machines ? Our experiments were all conducted on Google 's internal cluster with containers ( https : //research.google.com/pubs/pub43438.html ) . We also point out that the straggler phenomenon has been observed by others ( Dean & Barroso ( 2013 ) ) in production clusters . As discussed in both our paper and Dean & Barroso ( 2013 ) , stragglers may occur from multiple reasons , including failing hardware , software bugs , contention of resources , pre-emption of jobs . Communication bottlenecks which the reviewer referred to are only one possible cause . Furthermore , not everyone has access to dedicated machines and clusters . In our cluster , containers share underlying physical hardware with imperfect isolation that shows up in the tail when pushing higher utilization . Even with elastic cloud resources such as EC2 , one has to share network resources with other jobs and processes . Pre-emption of machines are also possible with spot instances . Our solution is a simple , elegant solution that attempts to address the * effect * of stragglers , rather than having multiple technically challenging approach to solve each cause . The reviewer 's suggestion of an `` MPI_Reduce '' ( or AllReduce ) is interesting and could help to alleviate some communication bottlenecks ( which we emphasize again is not the only cause of stragglers ) in synchronous training . We do also point out that a similar approach for asynchronous training could be technically harder to implement and analyze . 3.It is true that our solution only addresses straggling workers but not parameter servers . In practice , however , one typically has an order of magnitude more workers than parameter servers . Hence , we have found that that straggling workers are a greater problem than straggling parameter servers . The issue of slow parameter servers is something we would leave for future work , should it become a significant bottleneck in practice . 4.The reviewer is correct that hyperparameters need to be tuned with each batch size . ( For more discussions , we refer the reviewer to the below comments / discussion with Ioannis Mitliagkas on tuning . ) In general , we tried our best to tune the hyperparameters for asynchronous training ; in contrast we spent less effort tuning the synchronous training , but nevertheless found it to produce better results for the reported models and datasets . Hence , we feel confident that our experiments demonstrate that synchronous training of deep models can outperform asynchronous training . We agree with the reviewer that our results could be strengthened by averaging over multiple runs . Unfortunately , doing so is rather expensive -- 10 runs of the Inception experiments could cost ~150,000 GPU hours . We would like to reassure the reviewer that we did not cherry-pick results , i.e.we reported results from a random run . We also found results to be pretty consistent as we were fine-tuning our implementation . In addition , other real users within the company , training similar models for their work , have also observed similar results ."}, {"review_id": "HyAddcLge-1", "review_text": "This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. My main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions: - provide more experiments to show the performance with different efficiency distributions of learners. - assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the comments and suggestion . Indeed , our solution was designed with the specific distribution of learners ' efficiencies in mind , and may not necessarily be the ideal solution for other distributions . However , we contend that this particular distribution is the one that matters in practice , since it is what we empirically observe in real production workloads ( see Figures 3 , 4 ) , and it is what makes a fully synchronous ( no backups ) solution difficult to use in practice . Similar tail distributions have been observed by Dean & Barroso ( 2013 ) in other production systems with similar latency-related problems . Our contribution lies in demonstrating the feasibility of our solution in a synchronous machine learning system . Furthermore , we believe that distributions that are likely to cause problems for our solution , e.g.extremely high variability in learners ' efficiencies , are also problematic for Async-Opt and naive synchronous optimization ( with no backups ) , due to increased staleness and straggler effects . That said , we again emphasize that these are not distributions we observe in practice , and thus of lesser concern to us ."}, {"review_id": "HyAddcLge-2", "review_text": "The paper claim that, when supported by a number of backup workers, synchronized-SGD actually works better than async-SGD. The paper first analyze the problem of staled updates in async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the authors shows the effectiveness of the proposed method in applications to Inception Net and PixelCNN. The idea is very simple, but in practice it can be quite useful in industry settings where adding some backup workders is not a big problem in cost. Nevertheless, I think the proposed solution is quite straightforward to come up with when we assume that each worker contains the full dataset and we have budge to add more workers. So, under this setting, it seems quite natural to have a better performance with the additional backup workers that avoid the staggering worker problem. And, with this assumtion I'm not sure if the proposed solution is solving difficult enough problem with novel enough idea. In the experiments, for fair comparison, I think the Async-SGD should also have a mechanism to cut off updates of too much staledness just as the proposed method ignores all the remaining updates after having N updates. For example, one can measure the average time spent to obtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD so that Async-SGD does not perform so poorly.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments and suggestions . We would like to point out that our solution does not require extra budget . Indeed , given a fixed number of machines N , we can reserve a small number b , so that running with b out of N workers as backups would be faster than a fully synchronous solution with N workers . This is the premise of discussion in Section 3.1 on straggler effects . While we agree that the proposed solution looks straightforward in retrospect , we would argue that our contributions lie in motivating and demonstrating the feasibility of such a solution in a real-world production machine learning system . It is counter-intuitive and not immediately obvious that discarding the work of a few workers can in fact improve overall running time , since this reduces throughput relative to Async-SGD , and increases number of iterations to convergence relative to synchronous training ( see Figure 5 ) . The reviewer 's suggestion for controlling Async-SGD 's staleness is an interesting one , and provides a different trade-off between accuracy and throughput , similar to the `` softsync '' approach of Zhang et al . ( 2015b ) .Our main goal in this paper , however , is to demonstrate that a synchronous solution is competitive with , and can outperform asynchronous solutions . We also contend that asynchronous approaches do not completely resolve the staleness issues highlighted in Section 2.1 , making analysis and understanding more difficult . On the other hand , synchronous solutions are straightforward to analyze , and more predictable behavior that aids debugging and improves reproducibility ."}], "0": {"review_id": "HyAddcLge-0", "review_text": "This paper was easy to read, the main idea was presented very clearly. The main points of the paper (and my concerns are below) can be summarized as follows: 1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a \"MPI_Reduce\" would be used (even if we wait for the slowest guy)? 2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain. 3.they propose to take gradient from the first \"N\" workers out of \"N+b\" workers available. My concern here is that they focused only on the workers, but what if the \"parameter server\" will became to slow? What if the parameter server would be the bottleneck? How would you address this situation? But still if the number of nodes (N) is not large, and the deep DNN is used, I can imagine that the communciation will not take more than 30% of the run-time. My largest concern is with the experiments. Different batch size implies that different learning rate should be chosen, right? How did you tune the learning rates and other parameters for e.g. Figure 5 you provide some formulas in (A2) but clearly this can bias your Figures, right? meaning, that if you tune \"\\gamma, \\beta\" for each N, it could be somehow more representative? also it would be nicer if you run the experiment many times and then report average, best and worst case behaviour. because now it can be just coinsidence, right? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the review and suggestions , and address the concerns raised in detail below . 1.The reviewer suggests that stragglers are not an issue with dedicated machines and proper implementation of communications , which is counter to our experiences . The issue of stragglers is exacerbated with larger number of machines , posing a challenge to scalability -- perhaps the reviewer has been using a relatively smaller number of machines ? Our experiments were all conducted on Google 's internal cluster with containers ( https : //research.google.com/pubs/pub43438.html ) . We also point out that the straggler phenomenon has been observed by others ( Dean & Barroso ( 2013 ) ) in production clusters . As discussed in both our paper and Dean & Barroso ( 2013 ) , stragglers may occur from multiple reasons , including failing hardware , software bugs , contention of resources , pre-emption of jobs . Communication bottlenecks which the reviewer referred to are only one possible cause . Furthermore , not everyone has access to dedicated machines and clusters . In our cluster , containers share underlying physical hardware with imperfect isolation that shows up in the tail when pushing higher utilization . Even with elastic cloud resources such as EC2 , one has to share network resources with other jobs and processes . Pre-emption of machines are also possible with spot instances . Our solution is a simple , elegant solution that attempts to address the * effect * of stragglers , rather than having multiple technically challenging approach to solve each cause . The reviewer 's suggestion of an `` MPI_Reduce '' ( or AllReduce ) is interesting and could help to alleviate some communication bottlenecks ( which we emphasize again is not the only cause of stragglers ) in synchronous training . We do also point out that a similar approach for asynchronous training could be technically harder to implement and analyze . 3.It is true that our solution only addresses straggling workers but not parameter servers . In practice , however , one typically has an order of magnitude more workers than parameter servers . Hence , we have found that that straggling workers are a greater problem than straggling parameter servers . The issue of slow parameter servers is something we would leave for future work , should it become a significant bottleneck in practice . 4.The reviewer is correct that hyperparameters need to be tuned with each batch size . ( For more discussions , we refer the reviewer to the below comments / discussion with Ioannis Mitliagkas on tuning . ) In general , we tried our best to tune the hyperparameters for asynchronous training ; in contrast we spent less effort tuning the synchronous training , but nevertheless found it to produce better results for the reported models and datasets . Hence , we feel confident that our experiments demonstrate that synchronous training of deep models can outperform asynchronous training . We agree with the reviewer that our results could be strengthened by averaging over multiple runs . Unfortunately , doing so is rather expensive -- 10 runs of the Inception experiments could cost ~150,000 GPU hours . We would like to reassure the reviewer that we did not cherry-pick results , i.e.we reported results from a random run . We also found results to be pretty consistent as we were fine-tuning our implementation . In addition , other real users within the company , training similar models for their work , have also observed similar results ."}, "1": {"review_id": "HyAddcLge-1", "review_text": "This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. My main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions: - provide more experiments to show the performance with different efficiency distributions of learners. - assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the comments and suggestion . Indeed , our solution was designed with the specific distribution of learners ' efficiencies in mind , and may not necessarily be the ideal solution for other distributions . However , we contend that this particular distribution is the one that matters in practice , since it is what we empirically observe in real production workloads ( see Figures 3 , 4 ) , and it is what makes a fully synchronous ( no backups ) solution difficult to use in practice . Similar tail distributions have been observed by Dean & Barroso ( 2013 ) in other production systems with similar latency-related problems . Our contribution lies in demonstrating the feasibility of our solution in a synchronous machine learning system . Furthermore , we believe that distributions that are likely to cause problems for our solution , e.g.extremely high variability in learners ' efficiencies , are also problematic for Async-Opt and naive synchronous optimization ( with no backups ) , due to increased staleness and straggler effects . That said , we again emphasize that these are not distributions we observe in practice , and thus of lesser concern to us ."}, "2": {"review_id": "HyAddcLge-2", "review_text": "The paper claim that, when supported by a number of backup workers, synchronized-SGD actually works better than async-SGD. The paper first analyze the problem of staled updates in async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the authors shows the effectiveness of the proposed method in applications to Inception Net and PixelCNN. The idea is very simple, but in practice it can be quite useful in industry settings where adding some backup workders is not a big problem in cost. Nevertheless, I think the proposed solution is quite straightforward to come up with when we assume that each worker contains the full dataset and we have budge to add more workers. So, under this setting, it seems quite natural to have a better performance with the additional backup workers that avoid the staggering worker problem. And, with this assumtion I'm not sure if the proposed solution is solving difficult enough problem with novel enough idea. In the experiments, for fair comparison, I think the Async-SGD should also have a mechanism to cut off updates of too much staledness just as the proposed method ignores all the remaining updates after having N updates. For example, one can measure the average time spent to obtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD so that Async-SGD does not perform so poorly.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments and suggestions . We would like to point out that our solution does not require extra budget . Indeed , given a fixed number of machines N , we can reserve a small number b , so that running with b out of N workers as backups would be faster than a fully synchronous solution with N workers . This is the premise of discussion in Section 3.1 on straggler effects . While we agree that the proposed solution looks straightforward in retrospect , we would argue that our contributions lie in motivating and demonstrating the feasibility of such a solution in a real-world production machine learning system . It is counter-intuitive and not immediately obvious that discarding the work of a few workers can in fact improve overall running time , since this reduces throughput relative to Async-SGD , and increases number of iterations to convergence relative to synchronous training ( see Figure 5 ) . The reviewer 's suggestion for controlling Async-SGD 's staleness is an interesting one , and provides a different trade-off between accuracy and throughput , similar to the `` softsync '' approach of Zhang et al . ( 2015b ) .Our main goal in this paper , however , is to demonstrate that a synchronous solution is competitive with , and can outperform asynchronous solutions . We also contend that asynchronous approaches do not completely resolve the staleness issues highlighted in Section 2.1 , making analysis and understanding more difficult . On the other hand , synchronous solutions are straightforward to analyze , and more predictable behavior that aids debugging and improves reproducibility ."}}