{"year": "2019", "forum": "SylLYsCcFm", "title": "Learning to Make Analogies by Contrasting Abstract Relational Structure", "decision": "Accept (Poster)", "meta_review": "\npros:\n- The paper is well-written and includes a lot of interesting connections to cog sci (though see specific clarity concerns)\n- The tasks considered (visual and symbolic) provide a nice opportunity to study analogy making in different settings.\n\ncons:\n- There was some concerns about baselines and novelty that I think the authors have largely addressed in revision\n\nThis is an intriguing paper and an exciting direction and I think it merits acceptance.", "reviews": [{"review_id": "SylLYsCcFm-0", "review_text": "This work investigates the ability of a neural network to learn analogy. They showed that a simple neural network is able to solve analogy problems with image or abstract input, given that the training data is selected to contrast abstract relational structures. The paper is relatively well-written with rich discussions. Some details about the experiments are missing like how many examples are used for training and testing. It is also important to show how much variations are in the dataset, and there should be some external baselines like those proposed in (Barrett et al, 2018). Although the performance is relatively high, some error analysis will provide more insights into what the neural network is missing and if it makes mistakes similar to human. Section 4 claims that \u201cFor a model trained via LABC, we found that these activities clustered according to relation type (e.g. progression ) more-so than domain\u201d. However, it is unclear whether Figure 4 can support this. Some quantitive measure should help, for example, the average distance within the clusters between clustering based on relation type and domain. The novelty of the proposed approach is limited. The difference between the proposed method and baseline in performance seems to be a result of whether there is a difference between train and test setting. For example, if trained in \u201ccontrasting\u201d will have better test performance on \u201ccontrasting\u201d but worse on \u201cnormal\u201d and vice versa. The problem is very interesting and the discussion is extensive. However, the proposed approach isn\u2019t very novel and the evaluation and analysis should be improved to provide a stronger support. Barrett, David GT, et al. \"Measuring abstract reasoning in neural networks.\" arXiv preprint arXiv:1807.04225 (2018). ------ Score updated after reading authors' response. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review -- we \u2019 re glad you find the problem interesting ! You are right that we could have included more details of the visual analogy dataset to give a sense of its variety and scope . The training set contained 600,000 samples , 10,000 for validation , and 100,000 for testing . Regarding the variation in the dataset : are you referring to the number of possible questions ? While the number of domains and relations is small , the combinatorics of the puzzles ( born out through the choices for source and target domains , the relations , the values of the attributes , etc . ) ensures that the dataset is highly variable . For example , a quick back-of-the-envelope calculation shows that there are nearly 8 billion possible analogy questions ( source domain + target domain ) that the generator could produce . Moreover , variability in the possible incorrect choices further increases the number of possible questions to an astronomical amount . I hope this gives some sense of the scope of the dataset : we will add a table detailing these statistics to the appendix . We agree that it would be interesting to see how the model fails and whether the ways are similar to the ways in which humans fail . We \u2019 re performing this analysis now , and will be able to share specific failure cases with you soon . Additionally , we are testing a wider range of existing models models . We \u2019 d like to note , though , that the appropriate baselines are comparisons * within * model-type ; that is , using normal or LABC training on the same model . Our choice of model ( an RNN -- not even an LSTM ! ) was deliberately as simple as possible so as to emphasize the effect of the training method ; we agree , however , that it would be useful to corroborate the effect we observed using other well-known ( and closer to state-of-the-art ) architectures . Regarding quantitative measures for the clustering analysis . We performed this analysis and found that the inter-class cluster means are greater in LABC compared to normal training ( 5.2 vs. 4.4 ) , confirming the intuition that can be gleaned from the figure . We will soon compile further analyses with results of new experiments into a new comment addressed to all reviewers . Would you mind clarifying your opinion that the method is not novel ? As far as we are aware , there is no prior work that has analyzed the effects of negative examples on out-of-distribution generalization . Indeed , there is very little work at all addressing out-of-distribution generalization in abstract relational spaces . The closest work that we are aware is the triplet loss in computer vision , and some recent work in generative adversarial training . However , this work did not study the effects of the training method on out-of-distribution generalization , as we have . If you can provide specific examples of similar work , we 'll try to explain where this study differs ."}, {"review_id": "SylLYsCcFm-1", "review_text": "The paper describes an approach to train neural networks for analogical reasoning tasks. General analogical reasoning is quite a significant milestone in Machine learning. Therefore, the paper tackles an extremely challenging problem. The paper does a good job of constructing various tasks to show that analogies can be learned in different scenarios which are complex analogy tasks. Specifically, visual analogy and symbolic analogies are considered. The main idea is to choose training examples such that the model is forced to learn the relational structure rather than simply learn superficial features. One weakness is that we need to hand code the training examples to force it to have contrasting relational structure for different tasks. Is this realistic in different problems? That is maybe a limiting factor of this work. An automated method for generating such examples is given, but there is not too much detail on this (5.3). Maybe this needs to be expanded. Also, is the idea of LABC different from SMT. The novelty may be a bit weak in this aspect. If LABC can be described in a more general manner, it would help a reader not familiar with the other related work. Since the baseline comparison is with a very weak method (randomly chosen examples), it is hard to judge the impact of the proposed approach. In summary, I think the paper has nice ideas, particularly, if we can automatically generate examples using LABC. but maybe there is a need to work on better organizing the ideas, more general formulation of LABC and a more convincing experimental evaluation that includes a state-of-the-art method if available", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review ! You mention that one weakness is the need to hand code the training examples such that they have contrasting relational structure . We believe that there may be some deep , important links between LABC , generative adversarial training , and even self-play dynamics . Indeed , the latter two approaches may be seen as automated methods that can approximate LABC . For example , in self-play , agents continuously challenge their opponents by proposing maximally challenging data . In the context of our work , maximally challenging data are analogous to answer choices that are congruent at the level of abstract relational structure , as opposed to simply perceptual structure . Although we believe there are links to these other methods that may naturally automate candidate generation , no work , to our knowledge , has explored the effects of generative adversarial training or self-play on out-of-distribution generalization , especially in a training setup as we have demonstrated . And so , since this link is as of now just intuitive , we did not want to overclaim in the paper and propose these as natural methods of automating an LABC-like procedure to induce out-of-distribution generalization . We had hoped that our experiment on generative candidate proposals in the symbolic analogy task could point in this direction without overclaiming . Do you think it is appropriate to expand on these points in the paper ? There is a very important point that we \u2019 d like to clarify . You note that \u201c [ s ] ince the baseline comparison is with a very weak method ( randomly chosen examples ) , it is hard to judge the impact of the proposed approach \u201d . We believe this may be a misunderstanding . The baseline comparison is actually quite strong , as we do not choose * truly random * candidates in this condition . Rather , the candidates in this baseline condition are * perceptually plausible * given the visuals of the context panels ( i.e.they are always taken from the target domain ) . For example , if the context panels contain 1 circle , then 2 circles , then a possible answer choice would have 4 circles . A possible answer choice would * not * contain something as truly random as various colored lines ; if the training questions had such truly random candidates we think it is obvious that the model would learn very little of interest at all - it would learn the most superficial ability to match the visuals of the question and the answer , and yield no generalisation whatsoever . If you would like , we can run an experiment to demonstrate this , and will update the text to reflect our procedure for generating baseline candidates . Finally , regarding LABC and SMT , there are indeed some crucial differences . First , SMT is a psychological description of a human phenomena . It describes how humans tend to make analogies by mapping relational structures from one domain to another . LABC , on the other hand , is a * training * method . LABC contends that if a model is coaxed into * learning * to map relational structure , then it will be better at making analogies , as evidenced by out-of-distribution generalization . And indeed , our results show that this may be the case . Thank you for pointing out that these ideas are not clear -- we will try to update the description of LABC to better situate it in the previous literature ."}, {"review_id": "SylLYsCcFm-2", "review_text": "Cons 1. It\u2019s unclear why LABC produces lower scores than \u2018normal\u2019 training on \u2018normal\u2019 testing. 2. The text says nothing I can find to explain why in Fig 5 the \u2018entity\u2019 vectors have all 0s except in one dimension, which seems to make the problem considerably easier. 3. In a sense, there is no cross-domain adaptation required in the symbolic task: min is min, whether it operates on dimension k of the source vectors or dimension j of the target vectors. On the other hand, dimensions are processed independently in the model, as far as I can tell, so there\u2019s no free transfer of learning min on dimension k to knowing min on dimension j. It would be good to comment on this issue. 4. There seem to be obvious analogies (so to speak) to GANs, and it is very curious that this is not mentioned anywhere that I can see. This is particularly glaring in Sec. 5.3. 5. The quantitative results are scattered throughout the prose; it would be challenging, but worthwhile, to gather them into an actual table. Pros 6. The basic idea (\u201cWe should aspire to select as negative examples those examples that are plausible considering the most abstract principles that describe the data\u201d, p. 14) is very intuitive, common-sensical, bordering on obvious. But it is not at all obvious that the idea has as much power as is demonstrated in the experiments. The transfer to novel domain combinations, novel domains, and novel values of dimensions is impressive and surprising. 7. The result that the proposed training, designed to promote generalization on analogy tasks, also seems to promote improved sensory processing is interesting. Whether it really instantiates the parallel connection argued for by the High-Level Perception view from psychology/philosophy is debatable, but that is itself an interesting connection that the authors should be praised for identifying. 8. In general, the connection to the cognitive literature is creative and tantalizing and provides good scientific grounding for the work. 9. The linking to the flexibility of word meanings in the final paragraph pushes the limit of the plausibility of connection to broader cognitive issues, but I\u2019m inclined to indulge the authors for at least bringing up this important and relevant issue. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your review , we 're grateful you like the work ! We have given a lot of thought to your point 1 . We 're running experiments now to mix clever ( semantically-plausible ) and random ( perceptually-plausible ) candidates in a more refined way , hoping to train a model that does not degrade at all on test questions involving perceptually-plausible candidates while retaining the strong ability to generalise in the case of semantically-plausible candidates . Would such a result satisfy your reservations here ? Having said all this , we do n't believe this uncertainty detracts from the fact that a model trained to contrast abstract relational structure generalises more accurately and in a wider-range of out-of-distribution cases than one trained otherwise . Regarding the entity vectors having all 0 \u2019 s in the unused dimensions , we agree that this makes the problem easier . This experiment was designed to explicitly test domain-transfer generalization moreso than an ability to discern the domains that need to be considered . The idea was to strip away any difficulties in perception ( i.e. , in identifying the relevant domains ) to see if the effect of LABC persisted . As you note , \u201c dimensions are processed independently in the model ... so there \u2019 s no free transfer of learning min on dimension k to knowing min on dimension j. \u201d This is indeed the case , and we will clarify this . At test time the model should have an easy time identifying the relevant dimensions , but it will never have seen the particular transfer from dimension i to dimension j . So , even though it may have an easy time identifying and processing each dimension , it may be incapable ( without LABC ) of integrating the information processed from each of these dimensions , which we demonstrate . Regarding the link to GANs -- we believe there is a very interesting , potentially deep connection to GAN training , as well as self-play . Please see our reply to R2 for more thoughts here . We think that drawing connections between these methods is a very promising line of future work . We erred on the side of not overclaiming , and not drawing links that we have not rigorously proved , but our minds are definitely oriented in this direction , and we can add some text alluding to these ideas if you believe it necessary ."}], "0": {"review_id": "SylLYsCcFm-0", "review_text": "This work investigates the ability of a neural network to learn analogy. They showed that a simple neural network is able to solve analogy problems with image or abstract input, given that the training data is selected to contrast abstract relational structures. The paper is relatively well-written with rich discussions. Some details about the experiments are missing like how many examples are used for training and testing. It is also important to show how much variations are in the dataset, and there should be some external baselines like those proposed in (Barrett et al, 2018). Although the performance is relatively high, some error analysis will provide more insights into what the neural network is missing and if it makes mistakes similar to human. Section 4 claims that \u201cFor a model trained via LABC, we found that these activities clustered according to relation type (e.g. progression ) more-so than domain\u201d. However, it is unclear whether Figure 4 can support this. Some quantitive measure should help, for example, the average distance within the clusters between clustering based on relation type and domain. The novelty of the proposed approach is limited. The difference between the proposed method and baseline in performance seems to be a result of whether there is a difference between train and test setting. For example, if trained in \u201ccontrasting\u201d will have better test performance on \u201ccontrasting\u201d but worse on \u201cnormal\u201d and vice versa. The problem is very interesting and the discussion is extensive. However, the proposed approach isn\u2019t very novel and the evaluation and analysis should be improved to provide a stronger support. Barrett, David GT, et al. \"Measuring abstract reasoning in neural networks.\" arXiv preprint arXiv:1807.04225 (2018). ------ Score updated after reading authors' response. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review -- we \u2019 re glad you find the problem interesting ! You are right that we could have included more details of the visual analogy dataset to give a sense of its variety and scope . The training set contained 600,000 samples , 10,000 for validation , and 100,000 for testing . Regarding the variation in the dataset : are you referring to the number of possible questions ? While the number of domains and relations is small , the combinatorics of the puzzles ( born out through the choices for source and target domains , the relations , the values of the attributes , etc . ) ensures that the dataset is highly variable . For example , a quick back-of-the-envelope calculation shows that there are nearly 8 billion possible analogy questions ( source domain + target domain ) that the generator could produce . Moreover , variability in the possible incorrect choices further increases the number of possible questions to an astronomical amount . I hope this gives some sense of the scope of the dataset : we will add a table detailing these statistics to the appendix . We agree that it would be interesting to see how the model fails and whether the ways are similar to the ways in which humans fail . We \u2019 re performing this analysis now , and will be able to share specific failure cases with you soon . Additionally , we are testing a wider range of existing models models . We \u2019 d like to note , though , that the appropriate baselines are comparisons * within * model-type ; that is , using normal or LABC training on the same model . Our choice of model ( an RNN -- not even an LSTM ! ) was deliberately as simple as possible so as to emphasize the effect of the training method ; we agree , however , that it would be useful to corroborate the effect we observed using other well-known ( and closer to state-of-the-art ) architectures . Regarding quantitative measures for the clustering analysis . We performed this analysis and found that the inter-class cluster means are greater in LABC compared to normal training ( 5.2 vs. 4.4 ) , confirming the intuition that can be gleaned from the figure . We will soon compile further analyses with results of new experiments into a new comment addressed to all reviewers . Would you mind clarifying your opinion that the method is not novel ? As far as we are aware , there is no prior work that has analyzed the effects of negative examples on out-of-distribution generalization . Indeed , there is very little work at all addressing out-of-distribution generalization in abstract relational spaces . The closest work that we are aware is the triplet loss in computer vision , and some recent work in generative adversarial training . However , this work did not study the effects of the training method on out-of-distribution generalization , as we have . If you can provide specific examples of similar work , we 'll try to explain where this study differs ."}, "1": {"review_id": "SylLYsCcFm-1", "review_text": "The paper describes an approach to train neural networks for analogical reasoning tasks. General analogical reasoning is quite a significant milestone in Machine learning. Therefore, the paper tackles an extremely challenging problem. The paper does a good job of constructing various tasks to show that analogies can be learned in different scenarios which are complex analogy tasks. Specifically, visual analogy and symbolic analogies are considered. The main idea is to choose training examples such that the model is forced to learn the relational structure rather than simply learn superficial features. One weakness is that we need to hand code the training examples to force it to have contrasting relational structure for different tasks. Is this realistic in different problems? That is maybe a limiting factor of this work. An automated method for generating such examples is given, but there is not too much detail on this (5.3). Maybe this needs to be expanded. Also, is the idea of LABC different from SMT. The novelty may be a bit weak in this aspect. If LABC can be described in a more general manner, it would help a reader not familiar with the other related work. Since the baseline comparison is with a very weak method (randomly chosen examples), it is hard to judge the impact of the proposed approach. In summary, I think the paper has nice ideas, particularly, if we can automatically generate examples using LABC. but maybe there is a need to work on better organizing the ideas, more general formulation of LABC and a more convincing experimental evaluation that includes a state-of-the-art method if available", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review ! You mention that one weakness is the need to hand code the training examples such that they have contrasting relational structure . We believe that there may be some deep , important links between LABC , generative adversarial training , and even self-play dynamics . Indeed , the latter two approaches may be seen as automated methods that can approximate LABC . For example , in self-play , agents continuously challenge their opponents by proposing maximally challenging data . In the context of our work , maximally challenging data are analogous to answer choices that are congruent at the level of abstract relational structure , as opposed to simply perceptual structure . Although we believe there are links to these other methods that may naturally automate candidate generation , no work , to our knowledge , has explored the effects of generative adversarial training or self-play on out-of-distribution generalization , especially in a training setup as we have demonstrated . And so , since this link is as of now just intuitive , we did not want to overclaim in the paper and propose these as natural methods of automating an LABC-like procedure to induce out-of-distribution generalization . We had hoped that our experiment on generative candidate proposals in the symbolic analogy task could point in this direction without overclaiming . Do you think it is appropriate to expand on these points in the paper ? There is a very important point that we \u2019 d like to clarify . You note that \u201c [ s ] ince the baseline comparison is with a very weak method ( randomly chosen examples ) , it is hard to judge the impact of the proposed approach \u201d . We believe this may be a misunderstanding . The baseline comparison is actually quite strong , as we do not choose * truly random * candidates in this condition . Rather , the candidates in this baseline condition are * perceptually plausible * given the visuals of the context panels ( i.e.they are always taken from the target domain ) . For example , if the context panels contain 1 circle , then 2 circles , then a possible answer choice would have 4 circles . A possible answer choice would * not * contain something as truly random as various colored lines ; if the training questions had such truly random candidates we think it is obvious that the model would learn very little of interest at all - it would learn the most superficial ability to match the visuals of the question and the answer , and yield no generalisation whatsoever . If you would like , we can run an experiment to demonstrate this , and will update the text to reflect our procedure for generating baseline candidates . Finally , regarding LABC and SMT , there are indeed some crucial differences . First , SMT is a psychological description of a human phenomena . It describes how humans tend to make analogies by mapping relational structures from one domain to another . LABC , on the other hand , is a * training * method . LABC contends that if a model is coaxed into * learning * to map relational structure , then it will be better at making analogies , as evidenced by out-of-distribution generalization . And indeed , our results show that this may be the case . Thank you for pointing out that these ideas are not clear -- we will try to update the description of LABC to better situate it in the previous literature ."}, "2": {"review_id": "SylLYsCcFm-2", "review_text": "Cons 1. It\u2019s unclear why LABC produces lower scores than \u2018normal\u2019 training on \u2018normal\u2019 testing. 2. The text says nothing I can find to explain why in Fig 5 the \u2018entity\u2019 vectors have all 0s except in one dimension, which seems to make the problem considerably easier. 3. In a sense, there is no cross-domain adaptation required in the symbolic task: min is min, whether it operates on dimension k of the source vectors or dimension j of the target vectors. On the other hand, dimensions are processed independently in the model, as far as I can tell, so there\u2019s no free transfer of learning min on dimension k to knowing min on dimension j. It would be good to comment on this issue. 4. There seem to be obvious analogies (so to speak) to GANs, and it is very curious that this is not mentioned anywhere that I can see. This is particularly glaring in Sec. 5.3. 5. The quantitative results are scattered throughout the prose; it would be challenging, but worthwhile, to gather them into an actual table. Pros 6. The basic idea (\u201cWe should aspire to select as negative examples those examples that are plausible considering the most abstract principles that describe the data\u201d, p. 14) is very intuitive, common-sensical, bordering on obvious. But it is not at all obvious that the idea has as much power as is demonstrated in the experiments. The transfer to novel domain combinations, novel domains, and novel values of dimensions is impressive and surprising. 7. The result that the proposed training, designed to promote generalization on analogy tasks, also seems to promote improved sensory processing is interesting. Whether it really instantiates the parallel connection argued for by the High-Level Perception view from psychology/philosophy is debatable, but that is itself an interesting connection that the authors should be praised for identifying. 8. In general, the connection to the cognitive literature is creative and tantalizing and provides good scientific grounding for the work. 9. The linking to the flexibility of word meanings in the final paragraph pushes the limit of the plausibility of connection to broader cognitive issues, but I\u2019m inclined to indulge the authors for at least bringing up this important and relevant issue. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your review , we 're grateful you like the work ! We have given a lot of thought to your point 1 . We 're running experiments now to mix clever ( semantically-plausible ) and random ( perceptually-plausible ) candidates in a more refined way , hoping to train a model that does not degrade at all on test questions involving perceptually-plausible candidates while retaining the strong ability to generalise in the case of semantically-plausible candidates . Would such a result satisfy your reservations here ? Having said all this , we do n't believe this uncertainty detracts from the fact that a model trained to contrast abstract relational structure generalises more accurately and in a wider-range of out-of-distribution cases than one trained otherwise . Regarding the entity vectors having all 0 \u2019 s in the unused dimensions , we agree that this makes the problem easier . This experiment was designed to explicitly test domain-transfer generalization moreso than an ability to discern the domains that need to be considered . The idea was to strip away any difficulties in perception ( i.e. , in identifying the relevant domains ) to see if the effect of LABC persisted . As you note , \u201c dimensions are processed independently in the model ... so there \u2019 s no free transfer of learning min on dimension k to knowing min on dimension j. \u201d This is indeed the case , and we will clarify this . At test time the model should have an easy time identifying the relevant dimensions , but it will never have seen the particular transfer from dimension i to dimension j . So , even though it may have an easy time identifying and processing each dimension , it may be incapable ( without LABC ) of integrating the information processed from each of these dimensions , which we demonstrate . Regarding the link to GANs -- we believe there is a very interesting , potentially deep connection to GAN training , as well as self-play . Please see our reply to R2 for more thoughts here . We think that drawing connections between these methods is a very promising line of future work . We erred on the side of not overclaiming , and not drawing links that we have not rigorously proved , but our minds are definitely oriented in this direction , and we can add some text alluding to these ideas if you believe it necessary ."}}