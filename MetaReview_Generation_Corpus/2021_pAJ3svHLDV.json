{"year": "2021", "forum": "pAJ3svHLDV", "title": "R-MONet: Region-Based Unsupervised Scene Decomposition and Representation via Consistency of Object Representations", "decision": "Reject", "meta_review": "The paper has good contributions to a challenging problem, leveraging a Faster-RCNN framework with a novel self-supervised learning loss. However reviewer 4 and other chairs (in calibration) considered that the paper does not meet the bar for acceptance. The other reviewers did not champion the paper either, hence i am proposing rejection. \n\nPros:\n- R1 and R3 agree that the proposed model improves over related models such as MONET.\n- The value of the proposed self-supervised loss connecting bounding boxes and segmentations is well validated in experiments.\n\nCons:\n- R4 gives good suggestions that may be useful to reach a broader readership, namely introducing more of the concepts used in the paper., e.g. \"stick breaking, spatial broadcast decoder, multi-otsu thresholding\" so it becomes more self-contained. R4 also suggests improving the writing more generally.\n- R4 still finds the proposed \"method quite complex yet derivative\" after the rebuttal.\n- All reviewers complain about lack of experiments in real data, but the authors did revise their paper and add some coco results in the appendix. These could be part of the main paper in a future version.", "reviews": [{"review_id": "pAJ3svHLDV-0", "review_text": "This paper presents a variation of the MONet model where an additional Region Proposal Network generates bounding boxes for various objects in the scene . An additional loss is introduced during training to make the segmentations produced by the MONet segmenter consistent with the proposed bounding boxes . Results are demonstrated on multi d-Sprites and CLEVR with modest performance gains . The paper is somewhat middle of the road in most aspects - the proposed method is , in my opinion , only a slight variation on the existing MONet model . Though presented clearly , I do n't feel that adding that loss makes the model better in any fundamental way , even if performance numbers are slightly better in some circumstances . Furthermore , though there is some ablation analysis , I feel the level of analysis of the results is sub-par - when a relatively simple variation of a model like here is proposed I would want to see an effort to analyse the contribution beyond how it affects the numbers - do we learn anything new by introducing the variation ? does it tell us some fallacy or failure of the original model and if so , does it fix it ? These are lacking here . A few more concrete points : * In Table 1 - why is the ResNet18 + FPN missing from d-Sprites dataset ? This ablation is probably the single most important experiment present in the paper - I would want to see it reported on both datasets . * In general - I feel the performance on these datasets is quite saturated and I hope to see results on more challenging data in the future - the proposed method included * There is very little discussion about the choice of hyper-parameters in the paper - how were they chosen ? is the system sensitive to these choices ? Post rebuttal comments : Thank you authors for the detailed response - I think some of of my concerns have been answered - the paper may be a valid contribution to the community and I am raising my score .", "rating": "6: Marginally above acceptance threshold", "reply_text": "1 . `` In Table 1 - why is the ResNet18 + FPN missing from d-Sprites dataset ? This ablation is probably the single most important experiment present in the paper - I would want to see it reported on both datasets . '' Thanks for pointing out . We add MONet ( ResNet18+FPN ) and R-MONet ( ROI Align ) on Multi-dSprites dataset . We also add MONet ( ResNet18+FPN+UNet ) on both datasets to prove that R-MONet ( UNet ) 's performance will downgrade if getting rid of the object detection branch and self-supervised method . 2 . `` In general - I feel the performance on these datasets is quite saturated and I hope to see results on more challenging data in the future - the proposed method included '' We think the performance on these datasets is not saturated especially on the metrics mAP used to evaluate object detection . While +1 % mAP can be seen as a huge improvement in supervised area , our model achieves +10 % $ mAP_ { 50:95 } $ , +10 % $ mAP_ { 50 } $ and +20 % $ mAP_ { 75 } $ , comparing with the best unsupervised object detection model we can find ( SPACE ) . Regarding segmentation performance , we still have non-trivial quantitative improvements over the baseline MONet , IODINE , and SPACE . This quantitative difference can also be verified through newly added qualitative figures . We agree that the proposed model still requires significant change to apply it to the real image dataset ( the more challenging dataset you were referring to ) . But none of the previous models as we know can achieve acceptable performance on these datasets . In Sec.5 of the related work IODINE [ 8 ] ( accepted in ICML2019 ) : `` IODINE groups ImageNet not into meaningful objects but mostly into regions of similar color '' . In SPACE [ 19 ] ( accepted in ICLR2020 ) : `` Interesting future directions are to ... and to improve the model for natural images '' . The experiment only on synthetic datasets is widely adopted by SOTA ( State-of-the-Art ) works . Even though , comparing with previous models like AIR , SPAIR , GENESIS , and SPACE , the CLEVR dataset we use contains complex lighting effects and relatively closer to real images . The Multi-dSprites dataset contains complex shapes that challenge many related works such as IODINE and SPACE . 3 . `` There is very little discussion about the choice of hyper-parameters in the paper - how were they chosen ? is the system sensitive to these choices ? '' IODINE , SPACE , MONet , R-MONet ( Lite ) , and R-MONet ( UNet ) are only sensitive to the VAE decoder scale ( standard deviation ) . The best hyperparameter settings are reported in Appendix.B . The rest hyperparameters are robust in a reasonable range . We add this in Appendix.B in the revision . 4 . `` do we learn anything new by introducing the variation ? does it tell us some fallacy or failure of the original model and if so , does it fix it ? These are lacking here . '' We add a more detailed analysis in section4.1 in the revised paper . To summary it , MONet \u2019 s loss does not explicitly restrict multiple objects with similar colors existing in the same mask . As we can see in Figure ( 2 , 4 , 8 ) , MONet can not separate close objects with similar colors . More than that , since MONet performs segmentation on the entire image , it often suffers from small objects . When two objects are visually connected with each other , MONet may group them together as a single object even with different colors . This case is shown in Figure ( 5 , 6 ) . We can also find in Figure ( 9 , 10 ) , MONet may split a single object in multiple masks under certain lighting effects such as reflection or shadow . On the contrary , both R-MONet ( Lite ) and R-MONet ( UNet ) will not have this problem ."}, {"review_id": "pAJ3svHLDV-1", "review_text": "In this paper , the authors introduce a region-based approach for unsupervised scene decomposition . It extends the previous MONet by introducing the region-based self-supervised training . Instead of purely generating foreground masks in an RNN , they simultaneously predict the bounding boxes and segmentation masks using a Faster-RCNN-based framework . The supervision comes from the object reconstruction loss and the self-supervised loss of classification and regression of anchors in the RPN module . The experiments and comparisons are only conducted on the synthetic CLEVR and Multi-dSprites dataset . [ Paper strength ] - The paper is well motivated , and the proposed approach seems to be reasonable . - The self-supervised idea is interesting that uses the segmentation mask to get the pseudo bounding box label for object detection , which could ensure the consistency of object mask and bounding box . [ Paper Weakness ] 1 . The self-supervision between segmentation masks and detection bounding boxes is the main contribution . While incorporating the self-supervision into MONet is meaningful and interesting , the overall novelty does not look significant . 2.Clarification of Methods : - How to learn $ m_k $ in a self-supervise way is unclear ? MONet uses spatial attention to identify the most salient object one by one , which makes senses . But here you segment all the objects in one step . How could this be possible in an unsupervised way ? From the example in Fig.2 , it looks R-MONet could pick out some small and far-away objects first , which is not intuitive . - In Faster-RCNN , the positive/negative samples are selected by calculating the IoU threshold between the sampled bbox and the ground truth bbox . However , in this self-supervised approach , there is no ground truth bbox . Although the authors proposed to use the pseudo bbox from the segmentation mask $ m_k $ , how could this be reliable since $ m_k $ is likely of poor quality , especially at the initial stage . - The selected K value is unclear . In the original MONet , the spatial attention network is an RNN-like structure , they decompose the scene step-by-step . Therefore , they define the K steps . However , in this Faster-RCNN-based framework , the objects are selected in one step , how to select the K-1 objects in all proposals ? 3.Results : - Most of the results are with toy images . There is no result on real images . - There is no result to really demonstrate the effectiveness of the self-supervised loss . The author should compare their R-MONet ( UNet ) with the baseline of R-MONet ( UNet ) w/o the self-supervised loss , i.e.removing the object detection branch . Another missing baseline is MONet ( UNet ) . - In Table 1 , the MONet ( ResNet18+FPN ) is 10 percent lower than the original MONet . Does this means the network structure has a greater influence on the performance than the self-supervision component . - In Table 1 , the R-MONet ( Lite ) performs worse . Once again , I guess this poor performance comes from the network structure , as the input image is 64 * 64 , the Resnet downsamples the image to a very low resolution which losses the spatial information . - The visual results can not demonstrate the advantage of the proposed approach . For example , in Figure 3 , the visual performance of MONet and R-MONet ( UNet ) are quite similar . Update : In general , I am happy with the authors ' responses . They did show the advantage of the introduced self-supervised loss . Although the self-supervised loss is intuitive , incorporating it into MONet is non-trivial and it does outperform MONet . Despite that such self-supervised works are hard to work on real scenes , this paper does have some merits . I am willing to increase my rating .", "rating": "6: Marginally above acceptance threshold", "reply_text": "1 . `` How to learn $ m_k $ in a self-supervise way is unclear ? MONet uses spatial attention to identify the most salient object one by one , which makes sense . But here you segment all the objects in one step . How could this be possible in an unsupervised way ? From the example in Fig.2 , it looks R-MONet could pick out some small and far-away objects first , which is not intuitive . '' R-MONet generates ROIs and segmentation masks in parallel since we do not assume the objects in the scene are dependent . The recurrent attention structure in MONet is not necessary . Saliency objects can be captured in the local region of an image without having information about the rest of the scene . The ability to detect objects in a given ROI can be transferred well to detecting objects in other sub-regions . After the inference network generates segmentation masks , it passed them into the VAE to learn object appearance representation . 2 . `` In Faster-RCNN , the positive/negative samples are selected by calculating the IoU threshold between the sampled bbox and the ground truth bbox . However , in this self-supervised approach , there is no ground truth bbox . Although the authors proposed to use the pseudo bbox from the segmentation mask $ m_k $ , how could this be reliable since $ m_k $ is likely of poor quality , especially at the initial stage . '' We add Fig.17 to visualize the training of R-MONet during the initial stage . At the very early stage like epoch 480 , ROIs are random across the image . The spatial attention network tends to learn segmentation inside the ROIs . After the spatial attention network learns the rough segmentation masks , the pseudo ground truth bboxs generated from rough segmentation mask can guide object detection branch to find more accurate ROIs . In this stage , if segmentation masks contain more than one object , pseudo ground truth bbox will separate them with Multi-Otsu algorithm . At the middle stage ( epoch 7008 ) , the evolving segmentation masks help VAE to learn object appearance representations . In the last stage ( epoch 19296 ) , segmentation masks , bboxs and object appearance representations from VAE keep evolving at the same time . 3 . `` The selected K value is unclear . In the original MONet , the spatial attention network is an RNN-like structure , they decompose the scene step-by-step . Therefore , they define the K steps . However , in this Faster-RCNN-based framework , the objects are selected in one step , how to select the K-1 objects in all proposals ? '' After RPN , only ROIs with top K-1 prediction scores will be selected for further processing . The K-1 in R-MONet represents the max number of objects it will detect . We set the same K as MONet since we want to compare two models fairly . A good model should be able to capture objects with the least amount of ROIs . We choose 0.5 for the prediction score threshold to filter out void ROIs . We can see in the qualitative comparison figures , the bounding boxes are filtered with the threshold ( sometimes less than K-1 ) . We still keep the blank masks in the figures to make sure visualization is consistent . 4 . `` Most of the results are with toy images . There is no result on real images . '' The datasets we used is widely adopted by many related works such as MONet and IODINE . Comparing with previous models like AIR , SPAIR , GENESIS , and SPACE , the CLEVR dataset we use contains complex lighting effects and relatively closer to real images . However , we want to mention that the unsupervised scene decomposing task on the real image dataset is still a challenging problem and none of the previous models as we know can achieve acceptable performance on these datasets . In ` Sec.5 ` of the related work ` IODINE [ 8 ] ` ( accepted in ICML2019 ) : `` IODINE groups ImageNet not into meaningful objects but mostly into regions of similar color '' . In ` SPACE [ 19 ] ` ( accepted in ICLR2020 ) : `` Interesting future directions are to ... and to improve the model for natural images '' . Experiment only on synthetic datasets is widely adopted by SOTA ( State-of-the-Art ) works . 5 . `` There is no result to really demonstrate the effectiveness of the self-supervised loss . The author should compare their R-MONet ( UNet ) with the baseline of R-MONet ( UNet ) w/o the self-supervised loss , i.e.removing the object detection branch . Another missing baseline is MONet ( UNet ) . '' We add experiment MONet ( ResNet18+FPN+UNet ) ( equals to R-MONet ( UNet ) w/o the self-supervised loss and object detection branch ) in the revised paper . Original MONet is basically MONet ( ResNet18+UNet ) . We believe the MONet ( UNet ) you mentioned is the same as MONet ( ResNet18+FPN+UNet ) . MONet ( ResNet18+FPN+UNet ) performs significantly worse than MONet . FPN has a negative effect on segmentation by adding feature maps from different levels ."}, {"review_id": "pAJ3svHLDV-2", "review_text": "SUMMARY The paper presents a method to decompose scenes into its constituent objects . This is done with a generative framework that generates both bounding boxes and segmentation masks for each object . It relies on several previously existing technologies . Its main contribution is enforcing consistency between bounding boxes and segmentation masks . PROS * Outperforms the baselines . CONS * The paper can be hard to read . * Contributions seem minor . * Good results , but on two toy datasets only . COMMENTS The writing should be improved , as the paper can be hard to follow . One one hand , this includes broken sentences ( `` Inspired by the observation that foreground segmentation masks and bounding boxes both contain object geometric information and should be consistent with each other . `` ) , grammatical errors ( `` It proves that there are still many useful information can be discovered in those unlabeled data . `` ) , and sentences which are just hard to parse ( `` In the former type of models , the scene is encoded into the object-oriented disentangled spatial and appearance encoding explicitly . `` ) . On the other , the authors cite many concepts without introducing them in the paper ( stick breaking , spatial broadcast decoder , multi-otsu thresholding , etc ) which makes it non self-contained . The paper presents what seem like engineering improvements over previous works ( e.g.combining bounding boxes and segmentation masks ) by adding more components to the framework , which is quite convoluted ( see Fig.1 : ResNet , FPN , RPN , segmentation , VAEs , etc ) . It is hard to know where performance comes from , despite the ablation tests . The experiments are limited to two toy datasets with a fixed number of simple objects ( which must be known beforehand ) , which show no background interference and little occlusion . In all , I do not think it meets the ICLR bar . I am not an expert on the topic so I may have missed relevant datasets/baselines . Detail : `` Region of Interest '' introduced after ROI has been mentioned several times .", "rating": "3: Clear rejection", "reply_text": "1 . `` The writing should be improved , as the paper can be hard to follow . One one hand , this includes broken sentences ( `` Inspired by the observation that foreground segmentation masks and bounding boxes both contain object geometric information and should be consistent with each other . `` ) , grammatical errors ( `` It proves that there are still many useful information can be discovered in those unlabeled data . `` ) , and sentences which are just hard to parse ( `` In the former type of models , the scene is encoded into the object-oriented disentangled spatial and appearance encoding explicitly. '' ) . '' Thank you for pointing out that . We will update them in the revised paper . 2 . `` the authors cite many concepts without introducing them in the paper ( stick breaking , spatial broadcast decoder , multi-otsu thresholding , etc ) which makes it non self-contained . '' The stick-breaking process is a well-known analogy in statistics . We used equations 4-7 in the appendix to explain that . We explained the Multi-Otsu Thresholding method in the appendix . We add more details about it in the revised paper . Spatial broadcast decoder is almost the default VAE decoder used in the unsupervised scene decomposition topic such as MONet , IODINE ( ICML2019 ) , GENESIS ( ICLR2020 ) , SPACE ( ICLR2020 ) . We add some explanation about it in sec.2 of the revised paper . 3 . `` The experiments are limited to two toy datasets with a fixed number of simple objects ( which must be known beforehand ) , which show no background interference and little occlusion . '' The datasets we used is widely adopted by many related works such as MONet and IODINE . Comparing with previous models like AIR , SPAIR , GENESIS , and SPACE , the CLEVR dataset we use contains complex lighting effects and relatively closer to real images . However , we want to mention that the unsupervised scene decomposing task on the real image dataset is still a challenging problem and none of the previous models as we know can achieve acceptable performance on these datasets . In Sec.5 of the related work IODINE [ 8 ] ( accepted in ICML2019 ) : `` IODINE groups ImageNet not into meaningful objects but mostly into regions of similar color '' . In SPACE [ 19 ] ( accepted in ICLR2020 ) : `` Interesting future directions are to ... and to improve the model for natural images '' . Experiment only on synthetic datasets is widely adopted by SOTA ( State-of-the-Art ) works ."}], "0": {"review_id": "pAJ3svHLDV-0", "review_text": "This paper presents a variation of the MONet model where an additional Region Proposal Network generates bounding boxes for various objects in the scene . An additional loss is introduced during training to make the segmentations produced by the MONet segmenter consistent with the proposed bounding boxes . Results are demonstrated on multi d-Sprites and CLEVR with modest performance gains . The paper is somewhat middle of the road in most aspects - the proposed method is , in my opinion , only a slight variation on the existing MONet model . Though presented clearly , I do n't feel that adding that loss makes the model better in any fundamental way , even if performance numbers are slightly better in some circumstances . Furthermore , though there is some ablation analysis , I feel the level of analysis of the results is sub-par - when a relatively simple variation of a model like here is proposed I would want to see an effort to analyse the contribution beyond how it affects the numbers - do we learn anything new by introducing the variation ? does it tell us some fallacy or failure of the original model and if so , does it fix it ? These are lacking here . A few more concrete points : * In Table 1 - why is the ResNet18 + FPN missing from d-Sprites dataset ? This ablation is probably the single most important experiment present in the paper - I would want to see it reported on both datasets . * In general - I feel the performance on these datasets is quite saturated and I hope to see results on more challenging data in the future - the proposed method included * There is very little discussion about the choice of hyper-parameters in the paper - how were they chosen ? is the system sensitive to these choices ? Post rebuttal comments : Thank you authors for the detailed response - I think some of of my concerns have been answered - the paper may be a valid contribution to the community and I am raising my score .", "rating": "6: Marginally above acceptance threshold", "reply_text": "1 . `` In Table 1 - why is the ResNet18 + FPN missing from d-Sprites dataset ? This ablation is probably the single most important experiment present in the paper - I would want to see it reported on both datasets . '' Thanks for pointing out . We add MONet ( ResNet18+FPN ) and R-MONet ( ROI Align ) on Multi-dSprites dataset . We also add MONet ( ResNet18+FPN+UNet ) on both datasets to prove that R-MONet ( UNet ) 's performance will downgrade if getting rid of the object detection branch and self-supervised method . 2 . `` In general - I feel the performance on these datasets is quite saturated and I hope to see results on more challenging data in the future - the proposed method included '' We think the performance on these datasets is not saturated especially on the metrics mAP used to evaluate object detection . While +1 % mAP can be seen as a huge improvement in supervised area , our model achieves +10 % $ mAP_ { 50:95 } $ , +10 % $ mAP_ { 50 } $ and +20 % $ mAP_ { 75 } $ , comparing with the best unsupervised object detection model we can find ( SPACE ) . Regarding segmentation performance , we still have non-trivial quantitative improvements over the baseline MONet , IODINE , and SPACE . This quantitative difference can also be verified through newly added qualitative figures . We agree that the proposed model still requires significant change to apply it to the real image dataset ( the more challenging dataset you were referring to ) . But none of the previous models as we know can achieve acceptable performance on these datasets . In Sec.5 of the related work IODINE [ 8 ] ( accepted in ICML2019 ) : `` IODINE groups ImageNet not into meaningful objects but mostly into regions of similar color '' . In SPACE [ 19 ] ( accepted in ICLR2020 ) : `` Interesting future directions are to ... and to improve the model for natural images '' . The experiment only on synthetic datasets is widely adopted by SOTA ( State-of-the-Art ) works . Even though , comparing with previous models like AIR , SPAIR , GENESIS , and SPACE , the CLEVR dataset we use contains complex lighting effects and relatively closer to real images . The Multi-dSprites dataset contains complex shapes that challenge many related works such as IODINE and SPACE . 3 . `` There is very little discussion about the choice of hyper-parameters in the paper - how were they chosen ? is the system sensitive to these choices ? '' IODINE , SPACE , MONet , R-MONet ( Lite ) , and R-MONet ( UNet ) are only sensitive to the VAE decoder scale ( standard deviation ) . The best hyperparameter settings are reported in Appendix.B . The rest hyperparameters are robust in a reasonable range . We add this in Appendix.B in the revision . 4 . `` do we learn anything new by introducing the variation ? does it tell us some fallacy or failure of the original model and if so , does it fix it ? These are lacking here . '' We add a more detailed analysis in section4.1 in the revised paper . To summary it , MONet \u2019 s loss does not explicitly restrict multiple objects with similar colors existing in the same mask . As we can see in Figure ( 2 , 4 , 8 ) , MONet can not separate close objects with similar colors . More than that , since MONet performs segmentation on the entire image , it often suffers from small objects . When two objects are visually connected with each other , MONet may group them together as a single object even with different colors . This case is shown in Figure ( 5 , 6 ) . We can also find in Figure ( 9 , 10 ) , MONet may split a single object in multiple masks under certain lighting effects such as reflection or shadow . On the contrary , both R-MONet ( Lite ) and R-MONet ( UNet ) will not have this problem ."}, "1": {"review_id": "pAJ3svHLDV-1", "review_text": "In this paper , the authors introduce a region-based approach for unsupervised scene decomposition . It extends the previous MONet by introducing the region-based self-supervised training . Instead of purely generating foreground masks in an RNN , they simultaneously predict the bounding boxes and segmentation masks using a Faster-RCNN-based framework . The supervision comes from the object reconstruction loss and the self-supervised loss of classification and regression of anchors in the RPN module . The experiments and comparisons are only conducted on the synthetic CLEVR and Multi-dSprites dataset . [ Paper strength ] - The paper is well motivated , and the proposed approach seems to be reasonable . - The self-supervised idea is interesting that uses the segmentation mask to get the pseudo bounding box label for object detection , which could ensure the consistency of object mask and bounding box . [ Paper Weakness ] 1 . The self-supervision between segmentation masks and detection bounding boxes is the main contribution . While incorporating the self-supervision into MONet is meaningful and interesting , the overall novelty does not look significant . 2.Clarification of Methods : - How to learn $ m_k $ in a self-supervise way is unclear ? MONet uses spatial attention to identify the most salient object one by one , which makes senses . But here you segment all the objects in one step . How could this be possible in an unsupervised way ? From the example in Fig.2 , it looks R-MONet could pick out some small and far-away objects first , which is not intuitive . - In Faster-RCNN , the positive/negative samples are selected by calculating the IoU threshold between the sampled bbox and the ground truth bbox . However , in this self-supervised approach , there is no ground truth bbox . Although the authors proposed to use the pseudo bbox from the segmentation mask $ m_k $ , how could this be reliable since $ m_k $ is likely of poor quality , especially at the initial stage . - The selected K value is unclear . In the original MONet , the spatial attention network is an RNN-like structure , they decompose the scene step-by-step . Therefore , they define the K steps . However , in this Faster-RCNN-based framework , the objects are selected in one step , how to select the K-1 objects in all proposals ? 3.Results : - Most of the results are with toy images . There is no result on real images . - There is no result to really demonstrate the effectiveness of the self-supervised loss . The author should compare their R-MONet ( UNet ) with the baseline of R-MONet ( UNet ) w/o the self-supervised loss , i.e.removing the object detection branch . Another missing baseline is MONet ( UNet ) . - In Table 1 , the MONet ( ResNet18+FPN ) is 10 percent lower than the original MONet . Does this means the network structure has a greater influence on the performance than the self-supervision component . - In Table 1 , the R-MONet ( Lite ) performs worse . Once again , I guess this poor performance comes from the network structure , as the input image is 64 * 64 , the Resnet downsamples the image to a very low resolution which losses the spatial information . - The visual results can not demonstrate the advantage of the proposed approach . For example , in Figure 3 , the visual performance of MONet and R-MONet ( UNet ) are quite similar . Update : In general , I am happy with the authors ' responses . They did show the advantage of the introduced self-supervised loss . Although the self-supervised loss is intuitive , incorporating it into MONet is non-trivial and it does outperform MONet . Despite that such self-supervised works are hard to work on real scenes , this paper does have some merits . I am willing to increase my rating .", "rating": "6: Marginally above acceptance threshold", "reply_text": "1 . `` How to learn $ m_k $ in a self-supervise way is unclear ? MONet uses spatial attention to identify the most salient object one by one , which makes sense . But here you segment all the objects in one step . How could this be possible in an unsupervised way ? From the example in Fig.2 , it looks R-MONet could pick out some small and far-away objects first , which is not intuitive . '' R-MONet generates ROIs and segmentation masks in parallel since we do not assume the objects in the scene are dependent . The recurrent attention structure in MONet is not necessary . Saliency objects can be captured in the local region of an image without having information about the rest of the scene . The ability to detect objects in a given ROI can be transferred well to detecting objects in other sub-regions . After the inference network generates segmentation masks , it passed them into the VAE to learn object appearance representation . 2 . `` In Faster-RCNN , the positive/negative samples are selected by calculating the IoU threshold between the sampled bbox and the ground truth bbox . However , in this self-supervised approach , there is no ground truth bbox . Although the authors proposed to use the pseudo bbox from the segmentation mask $ m_k $ , how could this be reliable since $ m_k $ is likely of poor quality , especially at the initial stage . '' We add Fig.17 to visualize the training of R-MONet during the initial stage . At the very early stage like epoch 480 , ROIs are random across the image . The spatial attention network tends to learn segmentation inside the ROIs . After the spatial attention network learns the rough segmentation masks , the pseudo ground truth bboxs generated from rough segmentation mask can guide object detection branch to find more accurate ROIs . In this stage , if segmentation masks contain more than one object , pseudo ground truth bbox will separate them with Multi-Otsu algorithm . At the middle stage ( epoch 7008 ) , the evolving segmentation masks help VAE to learn object appearance representations . In the last stage ( epoch 19296 ) , segmentation masks , bboxs and object appearance representations from VAE keep evolving at the same time . 3 . `` The selected K value is unclear . In the original MONet , the spatial attention network is an RNN-like structure , they decompose the scene step-by-step . Therefore , they define the K steps . However , in this Faster-RCNN-based framework , the objects are selected in one step , how to select the K-1 objects in all proposals ? '' After RPN , only ROIs with top K-1 prediction scores will be selected for further processing . The K-1 in R-MONet represents the max number of objects it will detect . We set the same K as MONet since we want to compare two models fairly . A good model should be able to capture objects with the least amount of ROIs . We choose 0.5 for the prediction score threshold to filter out void ROIs . We can see in the qualitative comparison figures , the bounding boxes are filtered with the threshold ( sometimes less than K-1 ) . We still keep the blank masks in the figures to make sure visualization is consistent . 4 . `` Most of the results are with toy images . There is no result on real images . '' The datasets we used is widely adopted by many related works such as MONet and IODINE . Comparing with previous models like AIR , SPAIR , GENESIS , and SPACE , the CLEVR dataset we use contains complex lighting effects and relatively closer to real images . However , we want to mention that the unsupervised scene decomposing task on the real image dataset is still a challenging problem and none of the previous models as we know can achieve acceptable performance on these datasets . In ` Sec.5 ` of the related work ` IODINE [ 8 ] ` ( accepted in ICML2019 ) : `` IODINE groups ImageNet not into meaningful objects but mostly into regions of similar color '' . In ` SPACE [ 19 ] ` ( accepted in ICLR2020 ) : `` Interesting future directions are to ... and to improve the model for natural images '' . Experiment only on synthetic datasets is widely adopted by SOTA ( State-of-the-Art ) works . 5 . `` There is no result to really demonstrate the effectiveness of the self-supervised loss . The author should compare their R-MONet ( UNet ) with the baseline of R-MONet ( UNet ) w/o the self-supervised loss , i.e.removing the object detection branch . Another missing baseline is MONet ( UNet ) . '' We add experiment MONet ( ResNet18+FPN+UNet ) ( equals to R-MONet ( UNet ) w/o the self-supervised loss and object detection branch ) in the revised paper . Original MONet is basically MONet ( ResNet18+UNet ) . We believe the MONet ( UNet ) you mentioned is the same as MONet ( ResNet18+FPN+UNet ) . MONet ( ResNet18+FPN+UNet ) performs significantly worse than MONet . FPN has a negative effect on segmentation by adding feature maps from different levels ."}, "2": {"review_id": "pAJ3svHLDV-2", "review_text": "SUMMARY The paper presents a method to decompose scenes into its constituent objects . This is done with a generative framework that generates both bounding boxes and segmentation masks for each object . It relies on several previously existing technologies . Its main contribution is enforcing consistency between bounding boxes and segmentation masks . PROS * Outperforms the baselines . CONS * The paper can be hard to read . * Contributions seem minor . * Good results , but on two toy datasets only . COMMENTS The writing should be improved , as the paper can be hard to follow . One one hand , this includes broken sentences ( `` Inspired by the observation that foreground segmentation masks and bounding boxes both contain object geometric information and should be consistent with each other . `` ) , grammatical errors ( `` It proves that there are still many useful information can be discovered in those unlabeled data . `` ) , and sentences which are just hard to parse ( `` In the former type of models , the scene is encoded into the object-oriented disentangled spatial and appearance encoding explicitly . `` ) . On the other , the authors cite many concepts without introducing them in the paper ( stick breaking , spatial broadcast decoder , multi-otsu thresholding , etc ) which makes it non self-contained . The paper presents what seem like engineering improvements over previous works ( e.g.combining bounding boxes and segmentation masks ) by adding more components to the framework , which is quite convoluted ( see Fig.1 : ResNet , FPN , RPN , segmentation , VAEs , etc ) . It is hard to know where performance comes from , despite the ablation tests . The experiments are limited to two toy datasets with a fixed number of simple objects ( which must be known beforehand ) , which show no background interference and little occlusion . In all , I do not think it meets the ICLR bar . I am not an expert on the topic so I may have missed relevant datasets/baselines . Detail : `` Region of Interest '' introduced after ROI has been mentioned several times .", "rating": "3: Clear rejection", "reply_text": "1 . `` The writing should be improved , as the paper can be hard to follow . One one hand , this includes broken sentences ( `` Inspired by the observation that foreground segmentation masks and bounding boxes both contain object geometric information and should be consistent with each other . `` ) , grammatical errors ( `` It proves that there are still many useful information can be discovered in those unlabeled data . `` ) , and sentences which are just hard to parse ( `` In the former type of models , the scene is encoded into the object-oriented disentangled spatial and appearance encoding explicitly. '' ) . '' Thank you for pointing out that . We will update them in the revised paper . 2 . `` the authors cite many concepts without introducing them in the paper ( stick breaking , spatial broadcast decoder , multi-otsu thresholding , etc ) which makes it non self-contained . '' The stick-breaking process is a well-known analogy in statistics . We used equations 4-7 in the appendix to explain that . We explained the Multi-Otsu Thresholding method in the appendix . We add more details about it in the revised paper . Spatial broadcast decoder is almost the default VAE decoder used in the unsupervised scene decomposition topic such as MONet , IODINE ( ICML2019 ) , GENESIS ( ICLR2020 ) , SPACE ( ICLR2020 ) . We add some explanation about it in sec.2 of the revised paper . 3 . `` The experiments are limited to two toy datasets with a fixed number of simple objects ( which must be known beforehand ) , which show no background interference and little occlusion . '' The datasets we used is widely adopted by many related works such as MONet and IODINE . Comparing with previous models like AIR , SPAIR , GENESIS , and SPACE , the CLEVR dataset we use contains complex lighting effects and relatively closer to real images . However , we want to mention that the unsupervised scene decomposing task on the real image dataset is still a challenging problem and none of the previous models as we know can achieve acceptable performance on these datasets . In Sec.5 of the related work IODINE [ 8 ] ( accepted in ICML2019 ) : `` IODINE groups ImageNet not into meaningful objects but mostly into regions of similar color '' . In SPACE [ 19 ] ( accepted in ICLR2020 ) : `` Interesting future directions are to ... and to improve the model for natural images '' . Experiment only on synthetic datasets is widely adopted by SOTA ( State-of-the-Art ) works ."}}