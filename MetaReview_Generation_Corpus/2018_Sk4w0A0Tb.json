{"year": "2018", "forum": "Sk4w0A0Tb", "title": "Rotational Unit of Memory ", "decision": "Invite to Workshop Track", "meta_review": "although the authors argue that their experiments were selected from the earlier work from which major comparing approaches were taken, the reviewers found the empirical result to be weak. why not some real tasks (i do not believe bAbI nor PTB could be considered real) that could clearly reveal the superiority of the proposed unit against existing ones?", "reviews": [{"review_id": "Sk4w0A0Tb-0", "review_text": "The authors of this paper propose a new type of RNN architecture that modifies the reset gate of GRU with a rotational operator, where this rotational operator serves as an associative memory of their RNN model. The idea is sound, and the way they conduct experiments also make sense. The motivation and the details of the rotational memory are explained clearly. However, the experimental results reported in the paper seem to be a bit weak to support the claims made by the authors. The performance improvements are not so clear to me. Especially, in the character level language modeling, the BPC improvement is only 0.001 when choosing the SOTA model of this dataset as the base architecture. The test BPC score is obtained as a single-run experiment on the PTB dataset, and the improvement seems to be too small. In the copying memory task shown in Section 4.1, how did GORU performed when T=200? On the Q&A task, using the bAbI set (Section 4.3), RUM is said to be *significantly outperforming* GORU when the performance gap is 13.2%, and then, it is also said that RUM\u2019s performance *is close to* the MeMN2N when the performance gap is 12.8%. Both performance gaps seem to be very close to each other, but the way they are interpreted in the paper is not. Overall, the writing is clear, and the idea sounds interesting, but the experimental results are not strongly correlated with the claims made in the paper. In the visual analysis, the authors assume that RUM architecture might be the architecture that utilizes the full representational power of models like RNNs. If this is the case, I would expect to see more impressive improvements in the performance, assuming that all the other conditions are properly controlled. I would suggest evaluating the model on more datasets. Minor comments: In Section 2.2: Hopflied -> Hopfield In Section 3.2: I believe the dimension of b_t should be 2*N_h", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear Reviewer , We thank you for the constructive review ! For evaluation of RUM we wanted to test the model on diverse benchmark tasks , ranging from the Copying Memory Task and Character-level Language Modeling on PTB , which require a varied set of skills , including long-term memory capacity , associative skill , short-term forgetting mechanisms , etc . Our confidence in RUM is motivated by the state-of-the-art-like performance of the model in all those tasks . Thank you for suggesting to expand the experimental section . As far as the current results are concerned , we are working on a finer grid search , which can yield more impressive improvements . We are also evaluating RUM on a larger data set -- enwik8 : it is possible these simulations will not finish in time ( before the deadline ) , but we \u2019 ll try . We agree with your comment on the Q & A task , and we will rephrase this part of the experimental discussion . However , we want to explain why our result in this task is strong . Attention mechanism models hold the record for all Q & A tasks nowadays . Nevertheless , RNN models are still more responsible for long-term memory which should improve the SOTA when combined with attention mechanisms . Frankly , there is still a lack of studies on combining novel RNNs with attention mechanisms to achieve SOTA . Thus , this should not prevent studies on better fundamental RNN models , e.g.Cooijmans et al ( 2016 ) . For future work , we plan to apply RUM to other Q & A data sets . Finally , GORU learns the Copying Memory Task for T=200 ; we will update our figure . We will also implement your minor comments and update the paper accordingly . Thank you ! References : Tim Cooijmans , Nicolas Ballas , C\u00e9sar Laurent , \u00c7aglar G\u00fcl\u00e7ehre & Aaron Courville . Recurrent Batch Normalization . ICLR 2017 arXiv preprint arXiv:1603.09025 , 2016 ."}, {"review_id": "Sk4w0A0Tb-1", "review_text": "The paper proposes a RNN memory cell updating using an orthogonal rotation operator. This approach falls into the phase-encoding architectures. Overall the author's idea of generating a rotation operator using the embedded input and the transformed hidden state at the previous step is clever. Modelling this way makes the 'generating matrix' W_hh learn to couple the input to the hidden state (which contain information in the past) via the Rotation operator. I have several concerns: - The author should discuss the intuition why the rotation has to be from the generated memory target \u03c4 to the embeded input \u03b5 but not the other way around or other direction in this 2D subspace. - The description of parameter meter \u03c4 is not clear. Perhaps the author meant \u03c4 is the generated parameter via the parameter matrix W_hh acting upon the hidden state h_{t-1} - The idea of evolving the hidden state by an orthogonal matrix, of which the rotation is a special case, is similar to the GORU paper, which directly parametrizes the 'rotation' matrix. Therefore I am wondering if the better performance of this work than the GORU is because of the difference in parameterization or by limiting the orthogonal transform to only rotations (hence modelling only the phase of the hidden state). Perhaps an additional experiment is needed to verify this. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer , We thank you for the thoughtful review ! We believe that your question about the choice of the initial and final vectors , encoding the rotation , within the 2D subspace can lead to new interesting results . We might introduce two new parameters\u2014alpha and beta\u2014that define the rotation from the embedded input to a linear combination alpha * u+beta * v , where u and v form an orthonormal basis of the 2D subspace . The coefficients alpha and beta can be learned by backpropagation . Currently , we rotate from the embedded input to the target vector ; if we decide to flip the encoding ( from target to embedded input ) we expect to obtain comparable results to the current ones since we only reverse the orientation of the rotation . We thank you for your comments on the description of tau and will update the discussion accordingly . We will also conduct an additional experiment that will answer your questions about the comparison between RUM and GORU . Thank you !"}, {"review_id": "Sk4w0A0Tb-2", "review_text": "Summary: This paper proposes a way to incorporate rotation memories into gated RNNs. They use a specific parametrization of the rotation matrices. They run experiments on several toy tasks and on language modelling with PTB character-level language modeling (which I would still consider to be toyish.) Question: Can the rotation proposed here cause unintentional forgetting by interleaving the memories? Because in some sense rotations are glorified summation in high-dimensions, if you do a full-rotation of a vector (360 degrees) you can end up in the same location. Thus the model might overwrite into its past memories. Pros: Proposes an interesting way to incorporate the rotation operations into the gated architectures. Cons: The specific choice of rotation operation is not very well justified. This paper more or less uses the same architecture from Jing et al 2017 from EU-RNNs with a different parametrization for the rotation matrices. The experiments are still limited to simple small-scale tasks. General Comments: The idea and the premise of this paper is interesting. In general the paper seems to be well-written. However the most important part of the paper section 3.1 is not very well justified. Why this particular parameterization of the rotation matrices is used and where does actually that come from? Can you point out to some citation? I think the RUM architecture section also requires better explanation on for instance why why R_t is parameterized that way (as a multiplicative function of R_{t-1}). A detailed ablation study would help too. The model seems to perform really close to the GORU on Copying Task. I would be interested in seeing comparisons to GORU on \u201cAssociative Recall\u201d as well. On QA task, which subset of bAbI dataset have you used? 1k or 10k training sets? On language modelling there is only insignificant difference between the FS-LSTM-2 with FS-RUM model. This does not tell us much. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer , We thank you for the insightful review ! We believe that our concept of using rotation memories can be used in a large set of deep learning models , including RNNs . Our paper serves to introduce a particular construction ( Rotation ) that realizes the concept of rotation memories , and then to illustrate advantages of that construction by modifying gated models ( RUM ) . We agree that testing RUM on tasks with larger data sets would bolster the case for RUM . Currently we are running our model on enwik8 : it is possible our simulations will not finish in time ( before the deadline ) , but we \u2019 ll try . We believe that your comment about \u201c unintentional forgetting \u201d is interesting and will investigate it further . The RUM model utilizes rotations defined by projections into different ( in general ) 2d planes ( defined by the embedded input vector and the target vector ) under which going back to the same point unintentionally ( after making a cycle of 360 degrees ) is unlikely . Another way to think about this is by viewing the rotations as rotating a unit vector on an ( N_h-1 ) -sphere , where N_h is the hidden size . Since N_h is typically not small , the probability of ending at the same point after a full cycle is negligible . While RUM is only partially motivated by GORU , the RUM model introduces two crucial new concepts , which , we believe , substantially bolster its performance compared to GORU , and many other approaches : 1 . The rotation operation is not parameterized directly , as in GORU , but instead it is extracted from the new input and the previous hidden state . In this sense , to parallel our model with the literature , RUM is a \u201c firmware \u201d structure instead of a \u201c learnware \u201d structure as discussed in Balduzzi et al . ( 2016 ) : our rotation does not require additional parameters to be defined . 2.RUM has an associative memory structure , which is not present in GORU , and more importantly , it is vital for the learning of the Associative Recall task ( soon we will report on the inability of GORU to learn the task for T=30 and 50 ; note that RUM succeeds for T=30 and 50 ) . Moreover , the multiplicative recursive definition of R_t is required to maintain an orthogonal matrix and have an interpretation of phase accumulation because of the multiplicative nature of rotations . We believe that this is the first example of a multiplicative function used for associative memory , contrasting the recursions in Ba et al ( 2016 ) and Zhang et al ( 2017 ) . As far as rotations are concerned , they are key objects in a variety of fields such as quantum physics and the theory of Lie groups . If one wants to find inspirations for constructions , similar to ours in section 3.1. , they could consult standard books on those subjects ( Sakurai et.al ( 2010 ) , Artin ( 2011 ) ) . This particular parameterization for the rotation is a natural way to define a differentiable orthogonal operation within the RNN cell . Other ways to extract an orthogonal operation from elements in the RNN cell are still possible . Some approaches are as follows : 1 . Use a skew-symmetric matrix A to define the orthogonal operator e^A ; 2 . Use a permutation operator . However , those constructions are difficult to implement and do not offer a natural intuition about encoding memory . We recognize that other constructions are also feasible and potentially interesting for research ; however , we believe that our construction of the Rotation is simple and offers enough intuition ( and results ) to spur more research in constructing successful models other than RUM . We will update the discussion about the motivation of the rotational memory accordingly , but we will leave other constructions as a topic for further work ( i.e.for another conference ) . Finally , we used the 10k training set for the QA task . Thank you ! References : Jimmy Ba , Geoffrey Hinton , Volodymyr Mnih , Joel Z. Leibo , Catalin Ionescu . Using Fast Weights to Attend to the Recent Past . arXiv preprint arXiv:1610.06258 , 2016 . Wei Zhang and Bowen Zhou . Learning to update auto-associative memory in recurrent neural networks for improving sequence memorization . arXiv preprint arXiv:1709.06493 , 2017 . David Balduzzi and Muhammad Ghifary . Strongly-Typed Recurrent Neural Networks . Proceeding ICML'16 Proceedings of the 33rd International Conference on International Conference on Machine Learning . 48.1292-1300 , 2016 . J. J. Sakurai and Jim J. Napolitano . Modern Quantum Mechanics ( 2nd edition ) . Pearson , 2010 . Michael Artin . Algebra ( 2nd edition ) . Pearson , 2011 ."}], "0": {"review_id": "Sk4w0A0Tb-0", "review_text": "The authors of this paper propose a new type of RNN architecture that modifies the reset gate of GRU with a rotational operator, where this rotational operator serves as an associative memory of their RNN model. The idea is sound, and the way they conduct experiments also make sense. The motivation and the details of the rotational memory are explained clearly. However, the experimental results reported in the paper seem to be a bit weak to support the claims made by the authors. The performance improvements are not so clear to me. Especially, in the character level language modeling, the BPC improvement is only 0.001 when choosing the SOTA model of this dataset as the base architecture. The test BPC score is obtained as a single-run experiment on the PTB dataset, and the improvement seems to be too small. In the copying memory task shown in Section 4.1, how did GORU performed when T=200? On the Q&A task, using the bAbI set (Section 4.3), RUM is said to be *significantly outperforming* GORU when the performance gap is 13.2%, and then, it is also said that RUM\u2019s performance *is close to* the MeMN2N when the performance gap is 12.8%. Both performance gaps seem to be very close to each other, but the way they are interpreted in the paper is not. Overall, the writing is clear, and the idea sounds interesting, but the experimental results are not strongly correlated with the claims made in the paper. In the visual analysis, the authors assume that RUM architecture might be the architecture that utilizes the full representational power of models like RNNs. If this is the case, I would expect to see more impressive improvements in the performance, assuming that all the other conditions are properly controlled. I would suggest evaluating the model on more datasets. Minor comments: In Section 2.2: Hopflied -> Hopfield In Section 3.2: I believe the dimension of b_t should be 2*N_h", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear Reviewer , We thank you for the constructive review ! For evaluation of RUM we wanted to test the model on diverse benchmark tasks , ranging from the Copying Memory Task and Character-level Language Modeling on PTB , which require a varied set of skills , including long-term memory capacity , associative skill , short-term forgetting mechanisms , etc . Our confidence in RUM is motivated by the state-of-the-art-like performance of the model in all those tasks . Thank you for suggesting to expand the experimental section . As far as the current results are concerned , we are working on a finer grid search , which can yield more impressive improvements . We are also evaluating RUM on a larger data set -- enwik8 : it is possible these simulations will not finish in time ( before the deadline ) , but we \u2019 ll try . We agree with your comment on the Q & A task , and we will rephrase this part of the experimental discussion . However , we want to explain why our result in this task is strong . Attention mechanism models hold the record for all Q & A tasks nowadays . Nevertheless , RNN models are still more responsible for long-term memory which should improve the SOTA when combined with attention mechanisms . Frankly , there is still a lack of studies on combining novel RNNs with attention mechanisms to achieve SOTA . Thus , this should not prevent studies on better fundamental RNN models , e.g.Cooijmans et al ( 2016 ) . For future work , we plan to apply RUM to other Q & A data sets . Finally , GORU learns the Copying Memory Task for T=200 ; we will update our figure . We will also implement your minor comments and update the paper accordingly . Thank you ! References : Tim Cooijmans , Nicolas Ballas , C\u00e9sar Laurent , \u00c7aglar G\u00fcl\u00e7ehre & Aaron Courville . Recurrent Batch Normalization . ICLR 2017 arXiv preprint arXiv:1603.09025 , 2016 ."}, "1": {"review_id": "Sk4w0A0Tb-1", "review_text": "The paper proposes a RNN memory cell updating using an orthogonal rotation operator. This approach falls into the phase-encoding architectures. Overall the author's idea of generating a rotation operator using the embedded input and the transformed hidden state at the previous step is clever. Modelling this way makes the 'generating matrix' W_hh learn to couple the input to the hidden state (which contain information in the past) via the Rotation operator. I have several concerns: - The author should discuss the intuition why the rotation has to be from the generated memory target \u03c4 to the embeded input \u03b5 but not the other way around or other direction in this 2D subspace. - The description of parameter meter \u03c4 is not clear. Perhaps the author meant \u03c4 is the generated parameter via the parameter matrix W_hh acting upon the hidden state h_{t-1} - The idea of evolving the hidden state by an orthogonal matrix, of which the rotation is a special case, is similar to the GORU paper, which directly parametrizes the 'rotation' matrix. Therefore I am wondering if the better performance of this work than the GORU is because of the difference in parameterization or by limiting the orthogonal transform to only rotations (hence modelling only the phase of the hidden state). Perhaps an additional experiment is needed to verify this. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer , We thank you for the thoughtful review ! We believe that your question about the choice of the initial and final vectors , encoding the rotation , within the 2D subspace can lead to new interesting results . We might introduce two new parameters\u2014alpha and beta\u2014that define the rotation from the embedded input to a linear combination alpha * u+beta * v , where u and v form an orthonormal basis of the 2D subspace . The coefficients alpha and beta can be learned by backpropagation . Currently , we rotate from the embedded input to the target vector ; if we decide to flip the encoding ( from target to embedded input ) we expect to obtain comparable results to the current ones since we only reverse the orientation of the rotation . We thank you for your comments on the description of tau and will update the discussion accordingly . We will also conduct an additional experiment that will answer your questions about the comparison between RUM and GORU . Thank you !"}, "2": {"review_id": "Sk4w0A0Tb-2", "review_text": "Summary: This paper proposes a way to incorporate rotation memories into gated RNNs. They use a specific parametrization of the rotation matrices. They run experiments on several toy tasks and on language modelling with PTB character-level language modeling (which I would still consider to be toyish.) Question: Can the rotation proposed here cause unintentional forgetting by interleaving the memories? Because in some sense rotations are glorified summation in high-dimensions, if you do a full-rotation of a vector (360 degrees) you can end up in the same location. Thus the model might overwrite into its past memories. Pros: Proposes an interesting way to incorporate the rotation operations into the gated architectures. Cons: The specific choice of rotation operation is not very well justified. This paper more or less uses the same architecture from Jing et al 2017 from EU-RNNs with a different parametrization for the rotation matrices. The experiments are still limited to simple small-scale tasks. General Comments: The idea and the premise of this paper is interesting. In general the paper seems to be well-written. However the most important part of the paper section 3.1 is not very well justified. Why this particular parameterization of the rotation matrices is used and where does actually that come from? Can you point out to some citation? I think the RUM architecture section also requires better explanation on for instance why why R_t is parameterized that way (as a multiplicative function of R_{t-1}). A detailed ablation study would help too. The model seems to perform really close to the GORU on Copying Task. I would be interested in seeing comparisons to GORU on \u201cAssociative Recall\u201d as well. On QA task, which subset of bAbI dataset have you used? 1k or 10k training sets? On language modelling there is only insignificant difference between the FS-LSTM-2 with FS-RUM model. This does not tell us much. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer , We thank you for the insightful review ! We believe that our concept of using rotation memories can be used in a large set of deep learning models , including RNNs . Our paper serves to introduce a particular construction ( Rotation ) that realizes the concept of rotation memories , and then to illustrate advantages of that construction by modifying gated models ( RUM ) . We agree that testing RUM on tasks with larger data sets would bolster the case for RUM . Currently we are running our model on enwik8 : it is possible our simulations will not finish in time ( before the deadline ) , but we \u2019 ll try . We believe that your comment about \u201c unintentional forgetting \u201d is interesting and will investigate it further . The RUM model utilizes rotations defined by projections into different ( in general ) 2d planes ( defined by the embedded input vector and the target vector ) under which going back to the same point unintentionally ( after making a cycle of 360 degrees ) is unlikely . Another way to think about this is by viewing the rotations as rotating a unit vector on an ( N_h-1 ) -sphere , where N_h is the hidden size . Since N_h is typically not small , the probability of ending at the same point after a full cycle is negligible . While RUM is only partially motivated by GORU , the RUM model introduces two crucial new concepts , which , we believe , substantially bolster its performance compared to GORU , and many other approaches : 1 . The rotation operation is not parameterized directly , as in GORU , but instead it is extracted from the new input and the previous hidden state . In this sense , to parallel our model with the literature , RUM is a \u201c firmware \u201d structure instead of a \u201c learnware \u201d structure as discussed in Balduzzi et al . ( 2016 ) : our rotation does not require additional parameters to be defined . 2.RUM has an associative memory structure , which is not present in GORU , and more importantly , it is vital for the learning of the Associative Recall task ( soon we will report on the inability of GORU to learn the task for T=30 and 50 ; note that RUM succeeds for T=30 and 50 ) . Moreover , the multiplicative recursive definition of R_t is required to maintain an orthogonal matrix and have an interpretation of phase accumulation because of the multiplicative nature of rotations . We believe that this is the first example of a multiplicative function used for associative memory , contrasting the recursions in Ba et al ( 2016 ) and Zhang et al ( 2017 ) . As far as rotations are concerned , they are key objects in a variety of fields such as quantum physics and the theory of Lie groups . If one wants to find inspirations for constructions , similar to ours in section 3.1. , they could consult standard books on those subjects ( Sakurai et.al ( 2010 ) , Artin ( 2011 ) ) . This particular parameterization for the rotation is a natural way to define a differentiable orthogonal operation within the RNN cell . Other ways to extract an orthogonal operation from elements in the RNN cell are still possible . Some approaches are as follows : 1 . Use a skew-symmetric matrix A to define the orthogonal operator e^A ; 2 . Use a permutation operator . However , those constructions are difficult to implement and do not offer a natural intuition about encoding memory . We recognize that other constructions are also feasible and potentially interesting for research ; however , we believe that our construction of the Rotation is simple and offers enough intuition ( and results ) to spur more research in constructing successful models other than RUM . We will update the discussion about the motivation of the rotational memory accordingly , but we will leave other constructions as a topic for further work ( i.e.for another conference ) . Finally , we used the 10k training set for the QA task . Thank you ! References : Jimmy Ba , Geoffrey Hinton , Volodymyr Mnih , Joel Z. Leibo , Catalin Ionescu . Using Fast Weights to Attend to the Recent Past . arXiv preprint arXiv:1610.06258 , 2016 . Wei Zhang and Bowen Zhou . Learning to update auto-associative memory in recurrent neural networks for improving sequence memorization . arXiv preprint arXiv:1709.06493 , 2017 . David Balduzzi and Muhammad Ghifary . Strongly-Typed Recurrent Neural Networks . Proceeding ICML'16 Proceedings of the 33rd International Conference on International Conference on Machine Learning . 48.1292-1300 , 2016 . J. J. Sakurai and Jim J. Napolitano . Modern Quantum Mechanics ( 2nd edition ) . Pearson , 2010 . Michael Artin . Algebra ( 2nd edition ) . Pearson , 2011 ."}}