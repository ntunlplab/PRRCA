{"year": "2021", "forum": "Jr8XGtK04Pw", "title": "Hippocampal representations emerge when training recurrent neural networks on a memory dependent maze navigation task", "decision": "Reject", "meta_review": "This paper analyses a recurrent neural network model trained to perform a simple maze task, and reports that the network exhibits multiple hallmarks of neural selectivity reported in neurophysiological recordings from the hippocampus\u2014 in particular, they find place cells which also are tuned to task-relevant locations, cells which anticipate possible future paths, and a high proportion of neurons tuned to task variables.  \n\nThe reviewers appreciated the interesting empirical analysis, and the demonstration that multiple such features could arise in the same neural network\u2014 to the best of my knowledge, this had not been demonstrated explicitly before. However, there were also multiple concerns, which lead to this paper beeing discussed extensively and controversially. In particular, it is not clear which features arise from which learning objective, for example, for place cells to arise, do we  just need sensory prediction, or do we need q-learning? In addition, there were some points in which the tightness of the analogy between model and biology is questionable\u2014 in particular, this refers to the comprising between hippocampal recordings and the evaluation of the network.  Finally, it is also clear that  some of these observations reported in the paper are, indeed, empirical observations rather than explanations. Because of these shortcomings, there was no consensus and strong support from the reviewers for acceptance of the paper.\n\nAfter extensive discussion between both the reviewers, the AC and the program chair, the final decision was to not accept the paper. We do hope that the reviews will help you in improving the study and its presentation. It clearly has potential to be a valuable contribution to the literature. \n", "reviews": [{"review_id": "Jr8XGtK04Pw-0", "review_text": "Summary : The authors trained a recurrent network to perform a sensory prediction task and this gave rise to units that resembled hippocampal place fields . Then they augmented the network with a Q-learning objective and shown that the activity in the network sweep forward in space if the agent is fixed at a decision point . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I think the paper should be rejected . The work is interesting , but the clarity of the paper is not at the level of the findings . I think the authors should enhance exposition , and strengthen some analysis . The findings are really interesting , but at the current stage I don \u2019 t think the paper is ready to be published . However , I \u2019 m happy to revise my score if authors addresses my comments . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . Combination of unsupervised learning and RL to show that units in a recurrent network can be used to understand spatial and non-spatial firing patterns in the hippocampus 2 . Figure 4 and 5 are convincing . Cons : 1.While explaining the task the authors claim that the colours are chosen at random , however it is not clear whether these stay fixed across episodes or they changed . Intuitively it looks like , once the colour are generated , then they stay fixed , but in this case it is difficult to understand why a simple Q-leaning objective won \u2019 t be able to solve this task ( as claimed by the author ) . The only reason I can think about is if the steps between the cue and the reward are longer that the ones allowed by the discount factor chosen ( 0.8 ) . However , in the paper there are no details about the number of steps or how the discount affect the results . I think this is a serious issue . 2.The authors claim that they first pre-train on the predictive task , but then in the loss of eq.3 they report a combined loss . Does it mean that the loss_ { rgb } is also fine-tuned while training with the Q-learning objective ? This point need clarification . 3.The paper doesn \u2019 t report any details about the learning rates or the sizes of the linear layers used in eq.1 and 2.This way is impossible to replicate this results . This is a serious issue . 4.How the threshold for place cells are defined ? Why 30 % ? How many units show firing above that threshold ? This needs further analysis to support the decision , which otherwise seem very arbitrary . 5.Are figure 4 and 5 just cherry picked run or are these averaged across several testing runs ? Also in the captions of these figures the authors are using the singular \u201c observation \u201d in figure 4 and the plural \u201c observations \u201d in figure 5 . Does it means that the analysis have been performed differently ? Or is it just a mistake ? . This is an important point as I think the results will be more powerful with the same image fed as input , or even better with no image , just with 0ed input to simulate pondering . Minors : 1 . 380 units seems quite an unconventional number of units , why not 256 or 512 ? Can you please explain . Also is this number affecting the representations ? Have you done a sweep and settle on this number because it support your findings better ? If so , it would be important to mention it . 2.It would have been nice to have analysis to support the following sentence on page 4 : Generally , when the network loses its ability to self-localise the agent , state-action values are no longer reliable indicators of future reward potential as the current environmental state is not clearly discernible . Otherwise please correct it .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their feedback and for being transparent with regards to their evaluation . We have clarified the points made and have made substantial updates to the manuscript to address the reviewer \u2019 s comments . Cons : 1.Wall colours fixed or variable , Q-learning objective solving this task , steps between the cue and the reward and discount factor chosen ( 0.8 ) : We very much agree with the reviewer that this is ambiguous in our current explanation and we have updated the paper ( page 2 ) , explicitly stating that the wall colours remain fixed throughout training . In the paper we state that Q-learning alone can not solve the task after the network has been pre-trained on the predictive task . This is the case when using the rate of epsilon decay ( in epsilon-greedy RL training ) we use for the joint Q-learning and colour prediction training ( Eq.2 - Eq.3 in original paper ) described in the paper . We have run more rigorous analyses of running Q-learning alone with the same network without pre-training , Q-learning alone with pre-training and the joint Q-learning and colour prediction loss ( Eq.2 ) both with pre-training ( shown in the paper ) and without pre-training . We have updated the paper ( Figure 3 , page 5 ) to summarise these results . Taken together , training with a joint loss after pre-training converges with far fewer training iterations than the other cases . We have previously run analyses with differing discount factor values and find that a value higher than 0.8 regularly causes the model to converge without taking the most direct route to reward locations ( i.e converging on solutions with backtracking at secondary choice points when discount factor is higher than 0.8 ) . The number of steps in a single training episode is 30 ( if the agent takes the most direct path without backtracking ) . There are 5 steps from the cue to the choice point with 7 steps from the choice point to the first reward site . We have updated the paper ( page 4 and page 3 ) explaining the discount factor used and the number of steps inherent in the task . 2.Loss_rgb fine-tuned while training with the Q-learning objective : We agree this point is ambiguous and we have clarified this in the paper ( page 4 ) . The predictive pre-training task where loss_rgb is optimised converges completely . It is not fine-tuned while training the Q-learning objective to navigate to reward locations , but is required so the non-metric representation of space is maintained during learning . When loss_rgb is not included during Q-learning , the place fields are lost , and the training converges much slower to a different solution without an explicit representation of space . 3.Learning rates and sizes of linear layers used in eq.1 and 2 : We have updated the paper to include the following details on pages 2 , 3 and 4 of the manuscript . The linear layers in eq.1 and eq.2 ( Eq.3 in the updated manuscript ) are simply single layered readout layers for the LSTM . To clarify , when we have a 380 unit LSTM , the shapes of these readout layers are 380 x 12 ( when predicting four RGB wall colours ) and 380 x 4 ( when predicting Q-values ) respectively . We use a learning rate of 0.0005 with an Adam optimiser for reward training , as we find this learning rate gives good convergence ( without backtracking at secondary points ) with a greater range of training hyperparameters and initial conditions than with a learning rate of 0.001 . We use a learning rate of 0.001 for pre-training ."}, {"review_id": "Jr8XGtK04Pw-1", "review_text": "Motivated by biological considerations , this paper shows that recurrent networks trained with a predictive and goal-based objective on a maze finding task , qualitatively recapitulate experimental findings in hippocampal recordings in rodents trained on the same task . In particular , these LSTM networks demonstrate both metric representations of their environment and nonlocal extrafield firing at decision points along the maze ( anticipating the future trajectory of the agent ) . Strengths : + I like that the authors take a normative approach that exhibits both metric and non-metric place cell representations of the environment , unifying prior findings in one model . + I appreciate that no velocity input is given to their model , in contrast to prior approaches . + I also liked the qualitative comparisons to hippocampal recordings from rodents trained on the same task ( especially Figures 6 and 7 ) . Weaknesses : - The primary conclusion , namely , training an RNN on a maze-like environment gives you place cells , is really not all that new , especially considering that the network is still supervised to predict position and landmarks . - Given that lack of novelty in the modeling conclusion , it would have therefore been nice to have seen more quantitative comparisons to hippocampal recordings in rodents . Does their approach explain more variance in these neurons than prior approaches ? Otherwise , it seems that they simply recapitulate prior qualitative comparisons . Minor comments : \u201c Recurrence based \u201d should be changed everywhere to \u201c recurrent \u201d ( e.g.on pg.1 ) , and \u201c Neuroscience \u201d is not capitalized . The motivation to use double Q learning should be expanded on in pg . 3 prior to equation 3 . Question : The authors mention that Q-learning performs poorly on tasks in dynamic environments \u2013 however , I do not see any evidence of this in the paper , it would be imperative to show this explicitly for the environments they consider . Suppose this is in fact the case , could you clarify what makes your approach more successful at this task than others ? Is it because of the pretraining to predict the subsequent observation of wall colors from the current wall color observations ? As it stands , I think the ideas of this paper are interesting and think it unifies prior approaches , but I do not think the conclusions from the modeling add all that much novel insight from prior approaches . Therefore , I recommend a weak accept .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their feedback and overall positive evaluation of our work . However , we believe the reviewer may be unfamiliar with the previous work in this area . We address the points made below and in our updated manuscript . Weaknesses : 1 . The primary conclusion , namely , training an RNN on a maze-like environment gives you place cells , is really not all that new , especially considering that the network is still supervised to predict position and landmarks . We think the reviewer is incorrect here , to the best of our knowledge training an RNN on an environment with emerging place cells has only been shown once before ( Recanatesi et al. , bioRxiv 2019 ) and for the first time in a maze-like environment in this work . Our main result is not purely the emergence of place cells , but the combination of predictive and reinforcement learning to solve a navigation task which results in network units displaying hippocampal neuron characteristics . It is important to point out that the network in our model is not supervised to predict position or landmarks at all , it is only instructed to predict subsequent visual stimuli in pre-training and extended to predict global reward in Q-learning . We think the reviewer may have been misled by this line in our discussion section : \u201c This is similar to the purely contextual input received by the model pre-trained by Xu & Barak ( 2020 ) where no velocity input is given , however , the network here is still trained on position and landmark prediction in a supervised way. \u201d To clarify , here we are referring to the work by Xu & Barak , not our work . We have updated our paper to make this clearer . Our hypothesis was that predictive learning is well suited to be combined with reinforcement learning ( we add a prediction of the Q-value for reward training ) . We show that the resulting representation not only allows efficient learning but also yields cells with properties observed in the hippocampus . It is important to stress that previous work where training was based on trajectories of velocity and position did not report such behaviour and only demonstrate the formation of activations with the form of entorhinal cortex grid cells which are used in navigation differently to place cells . These authors also did not analyse resulting dynamics . 2.Given that lack of novelty in the modeling conclusion , it would have therefore been nice to have seen more quantitative comparisons to hippocampal recordings in rodents . Does their approach explain more variance in these neurons than prior approaches ? Otherwise , it seems that they simply recapitulate prior qualitative comparisons . We agree with the reviewer that direct quantitative comparisons would be interesting yet they may be misguided given the significant differences between neural and neuronal networks . We believe in this context our main novel contribution is the demonstration that the combination of predictive and reinforcement learning results in dynamics also observed in the hippocampus , specifically we show extrafield firing of network units at locations outside of their apparent place fields ( Johnson & Redish , 2007 ) , non-local forward sweeping representation of the network ( Johnson & Redish , 2007 ) , place fields drifting towards reward locations throughout training ( Lee et al. , 2006 ) , a high proportion of units with place fields at the maze start location encode reward locations ( Ainge et al. , 2007 ) and that a higher proportion of units encode task phase than turn direction ( Griffin et al. , 2007 ) . These specific comparisons between RNNs and hippocampal neurons have not been demonstrated before as far as we are aware ."}, {"review_id": "Jr8XGtK04Pw-2", "review_text": "* * Paper summary * * The main goal of this paper is to show that LSTM units in a network trained to solve a T-maze task , show similar activity patterns as neurons in rats solving a similar task . Specifically , the authors make the following claims : ( 1 ) an RNN learning the task by a combination of reinforcement and predictive learning produces internal representations with consistent extrafield firing associated with consequential decision points , ( 2 ) the network \u2019 s representation , once trained , follows a forward sweeping pattern similar to those found in rats and ( 3 ) a higher proportion of units in the trained network show strong selectivity for the choice phase of the task than for spatial topology , as seen in rats . * * Pros * * 1 . The submission is clear , well-written and the execution is competent . 2.I find the approach well motivated and the problem interesting for the current state of the field . 3.The authors provide a sufficient amount of details so that reproducibility should be possible . * * Cons * * I have concerns about key points of the paper and the interpretation of the results : 1 . One of the main results of the paper is the observation that the network produces a forward-moving representation similar to the one observed in rats at the decision point . However , the way the authors simulate this is by freezing the agent at such point and keeping the LSTM running , repeating the same constant observation . The LSTM was trained on trajectories on the maze , so in this trajectory ( which was never used during training ) , the network is completely out of distribution . The activity of any network in this situation is difficult to interpret . Because the network was trained with a predictive loss on two very specific sequences of observations , it seems plausible that it is robust to the change of input statistics and follows the same sequence , maybe with some instabilities . Note that the cue was present , which explains why the correct sequence is followed . In the network trained also by RL in particular , the authors interpret this as \u201c The agent appears to be sampling the trajectory concerning the alternate return arm of the maze before ultimately settling on the rewarding return arm \u201d . But this is , in my opinion , an over-interpretation , as the agent has no sampling capability in the first place ( there is no generative model of observations ) , nor any particular planning mechanism . Alternatively , the authors may be claiming that this jumping behavior happens only after the RL training and not before , in which case they should emphasize this difference and quantify it explicitly . Although in this case , a simple explanation for this could be that due to the epsilon-greedy , only during the RL training the network is exposed to the wrong cue-arm combination . Therefore , the LSTM would be less able to rely on the cue , which could explain the jump between attractors in the out-of-distribution case . In the discussion section , the authors claim \u201c We demonstrate that extrafield firing activity [ .. ] emerges when a simulated agent [ ... ] pauses at decision points - suggesting intrinsic dynamics are encoding the future planned trajectory of the agent. \u201d . I find this to be an over-claim , as the agent doesn \u2019 t pause ( it can \u2019 t ) and doesn \u2019 t plan ( for any common definition of planning ) . I would be more convinced if the agent was able to pause ( as an additional action ) and this behavior was observed in the LSTM activity in this situation , which is closer to the biological case . 2.The pre-train stage is done on trajectories that correspond to the solved task . This means that the LSTM trained by the predictive loss is not exposed to the general structure of the environment but to the specific solution of the task , including the cue-choice association and the exact sequence of observations in each of the two correct trajectories . The authors draw a parallel with the pre-training phase in behavioral experiments ( Johnson & Redish , 2007 ) in which rats usually run each trajectory separately ( by having the other one blocked ) . However , in my opinion , this is problematic for their analysis . First it \u2019 s unclear to what extent the network is learning by RL as during the pre-training it has already learnt to predict the observation corresponding to the correct turn ( wich corresponds to one of the two actions ) . Second , the LSTM is exposed only to the correct cue-arm trajectories , which I think is the reason why the forward-looking sweeps only follow these trajectories ( see previous point ) . This is more similar to a demonstration than to pre-training . A more conventional pre-training would leave the agent to explore freely to implicitly learn the structure of the environment ( a la Tollman ) . On the other hand , the argument of following the protocol of the behavioral experiments also doesn \u2019 t fully work as the rats are still producing motor outputs and even being rewarded during the pre-training phase ( Johnson & Redish , 2007 ) . 3.Finally , a more general concern is the main point of the paper . If I understand correctly , the main claim is the similarity of the observations between the RNN agent and the experimental findings in rats . However , given that there are plenty of arbitrary choices when training an RNN , I believe the results are not particularly explanatory . I would encourage the authors to formulate better alternative hypothesis and controlled experiments . For example , I would find it interesting to show that the forward-sweeping observations done in rats , which is often interpreted as a signature of planning or prediction of the consequences of future actions , arises simply from a next-step prediction loss in an overtrained rat . Minor concerns : - \u201c As such , a network of Gated Recurrent Units [ ... ] or vanilla RNN units was unable to perform well in either the pre-training or joint RL task due to these prevalent long term dependencies. \u201d How many steps are there between cue and choice , and between choice and reward ? - Related to the previous point : \u201c We attempted to run the reinforcement learning task alone in a maze with no wall colours or environment statistics except the cue . In this scenario the network is not able to learn the task due to a lack of self-localisation. \u201d If I understand correctly , there is a constant number of steps between the cue and the moment where the choice has to be made . I would tend to believe that an LSTM can learn to make a prediction only based on the number of timesteps , regardless of the lack of wall observations ( e.g.2 sequence problem in Hochreiter and Schmidhuber , 1997 ) . Details : - It would be good to clarify what exactly is the action set of the agent . - I would like to know how exactly are activity maps obtained . The authors mention \u201c Place fields determined by contiguous locality with average activity exceeding 30 % peak unit activity during a single left trajectory followed by a right trajectory \u201d but I don \u2019 t find this particularly clear . I also found it difficult to understand the bottom row of Fig 3 . - Fig 5 should be referred to in the paragraph starting with \u201c In stark contrast to the dynamics of the LSTM network following predictive pre-training ... \u201d - Why is the return to start representation in Fig.6 different for right and left trajectories ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their careful reading of our paper and for their scrupulous evaluation of our results . We have made substantial updates and alterations to the manuscript to address the reviewer \u2019 s comments and we respond to feedback below . Cons : 1 . * \u201c One of the main results of the paper [ ... ] , repeating the same constant observation. \u201d This is what the rodent is doing experimentally in ( Johnson & Redish , 2007 ) , and we replicate this experiment directly , so we feel prudent to follow this as a reasonable simulation for subsequent comparison of hippocampal characteristics . In page 4 of the original manuscript we state that , in experiments , rodents seem to pause at high consequence decision points ( Johnson & Redish , 2007 ) with alternating head movement behaviour signifying vicarious trial and error ( VTE ) ( Muenzinger , 1938 ; Hu & Amsel , 1995 ) . * \u201c The LSTM was trained on trajectories on the maze , [ ... ] , which explains why the correct sequence is followed. \u201d As the reviewer states , the activity of the network is difficult to interpret in this situation but we agree this is a plausible interpretation of network dynamics after pre-training and may in fact be what is occurring experimentally . * \u201c In the network trained also by RL in particular , the authors interpret this as [ ... ] , nor any particular planning mechanism. \u201d We agree that this may be an over-interpretation as the agent has no active sampling capability . Our wording here is ambiguous and we have removed references to agent sampling in the updated manuscript . However , sweeping behaviour ( as shown experimentally ) by Johnson & Redish is undoubtedly occurring . * \u201c Alternatively , the authors may be claiming [ ... ] quantify it explicitly. \u201d The path switching behaviour does only occur after reward training and not before , we have further emphasised this in the paper on page 7 . We have updated the paper to indicate that path switching occurs reliably in a very similar way after reward training with differing numbers of LSTM units and initial conditions as long as the reward task is solved without backtracking at secondary cue locations and is trained on the combined loss ( Eq.2 , this was Eq.3 in the original paper ) . Further analysis of this can be seen in Figure 3 which we introduce in the updated manuscript . * \u201c Although in this case , [ ... ] out-of-distribution case. \u201d We would argue that the network is more able to rely on the cue during reward training due to the spatial map of the maze formed during pre-training . Exposure to the wrong-arm combination is not sufficient for the network to exhibit the path switching behaviour shown in Figure 6 for various reasons . Firstly , the network converges to a solution which does not include backtracking at secondary points , therefore the network is choosing actions at the choice points which leads the agent to take a direct path to reward locations . Thus path switching is not part of the network \u2019 s inherent behaviour . Secondly , in the out of distribution case when the agent is paused at the primary choice point , the instantaneous representation jump from the rewarding maze arm to the opposing maze arm is not occurring anywhere in training , even at the beginning of epsilon-greedy reward training when actions are chosen completely randomly . Lastly , simply being out of distribution can not possibly explain the secondary path switch shown at timestep 32 in Figure 6 . * \u201c In the discussion section , [ ... ] the biological case. \u201d We agree that this may be an overclaim and we have removed suggestions of active planning in our updated manuscript , however stark similarity in terms of dynamics to the experimental case is a significant insight we believe . We thank the reviewer for the idea of having agent induced pausing as additional trainable behaviour which would certainly be closer to the biological case . We believe this would be too great a change to the current work as a revision but we will certainly explore this in future work ."}, {"review_id": "Jr8XGtK04Pw-3", "review_text": "In this paper , the authors train a recurrent neural network on a navigation task , and observe the emergence of several phenomena reminiscent of the hippocampus : appearance of place cells with a secondary receptive field at task-relevant locations ; anticipation of possible future paths in the activity of the model , with alternation in time between possible future paths ; a high proportion of neurons tuned to task variables rather than animal trajectory . Strong points : - these findings are compelling , they account for some hallmark properties of the activity of hippocampus , and they could lead to a better understanding of the role and function of the hippocampus . - the experiments are rigorous and convincing . Weak points : - some technical aspects of the paper could be clarified ( see below ) - it is unclear how this model improved our understanding of the hippocampus function , and whether the model makes any testable predictions about the hippocampus . I recommend to accept this paper because of its strengths listed above . Clarification questions : 1 ) I did not understand the role of secondary cue point . Why were these required in addition to the primary cue point and choice point ? 2 ) vocabulary : What are `` metric '' and `` non-metric '' representations ? 3 ) How reliable is the alternative path visiting phenomenon ? Can this phenomenon be observed reliably in networks trained from different initial conditions ? 4 ) I did not understand the consequences/take-homes of the second and third paragraph discussions . Additional feedback : 1 ) It would be interesting to see a discussion on what we learned about the function of the hippocampus from this model , and/or what predictions this model makes about neural activity in hippocampus . 2 ) Are there any oscillations and phase-precessions phenomena in the model ? If not , it would be interesting to discuss why these oscillations might be present in the brain but not in the model . 3 ) Could this network be used to simulate difficult experiments , e.g.understand how future paths exploration works in an open-field setting ? 4 ) The first sentence of the abstract is difficult to understand . In general , shorter sentences could improve clarity .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their careful reading and positive evaluation of our paper , in addition to the constructive feedback . We have clarified the points raised and updated the manuscript accordingly . Weak point : Using our model to improve understanding of the hippocampus and make testable predictions : We fully agree that this would be the ultimate goal of our model , to improve understanding of the brain with a relatively small RNN model which can be used to test hypotheses regarding hippocampal dynamics . In this work we aim to show that the LSTM dynamics resulting from training the network using the combination of predictive and reinforcement learning on the maze reward task , mirrors hippocampal neuronal dynamics found experimentally , and as such provides evidence that the underlying learning rules may be similar in the hippocampus . Specifically , we show that non-metric attractors form in the activation space of our network units in the way of place cells , we show extrafield firing of these units at location outside of their apparent place fields , non-local forward sweeping representation of the network , place fields drifting towards reward locations throughout training , a high proportion of units with place fields at the maze start location encode reward locations and that a higher proportion of units encode task phase than turn direction . As far as we know , this is the first model to replicate all these behaviours , and as such provides evidence that the underlying learning rules may be similar in the hippocampus . We therefore expect that our model can be used to generate new predictions in other tasks , and are currently working on this question . We have added a section on how our model could improve hippocampus understanding in the discussion section of the paper . Clarification Questions : 1 . Role of the secondary cue : The reviewer makes an important point on the necessity of the secondary cue points in this task . Our overall aim is to compare our trained model \u2019 s dynamics with that of hippocampal neurons as captured using experimental data . To this end we aim to mirror these experimental set ups as closely as possible - in this case we mirror the maze set up of Jonson and Redish , 2007 in order to optimally compare our resulting dynamics to that of hippocampal neurons . In terms of the task , including secondary cue/choice points gives the agent the opportunity to backtrack on its decision made at the primary choice point in light of further environmental observation ( the presentation or lack thereof of the secondary cue ) . We believe that experimentally , the presence of secondary cue points gives rise to some of the extrafield firing observed by Johnson and Redish as the rodent reevaluates its prior primary choice at a secondary cue point , resulting in the firing of place cells with place fields on the opposing side of the maze . We have updated the paper ( page 3 ) justifying the inclusion of the secondary cue . 2.Metric and non-metric representations : Metric representations relate to a Euclidean spatial map of an environment and are biologically akin to grid cells in the entorhinal cortex , whereas non-metric representations relate to associative landmark maps of the environment and are comparable to place cells in the hippocampus . We feel this is adequately explained in the introduction ."}], "0": {"review_id": "Jr8XGtK04Pw-0", "review_text": "Summary : The authors trained a recurrent network to perform a sensory prediction task and this gave rise to units that resembled hippocampal place fields . Then they augmented the network with a Q-learning objective and shown that the activity in the network sweep forward in space if the agent is fixed at a decision point . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I think the paper should be rejected . The work is interesting , but the clarity of the paper is not at the level of the findings . I think the authors should enhance exposition , and strengthen some analysis . The findings are really interesting , but at the current stage I don \u2019 t think the paper is ready to be published . However , I \u2019 m happy to revise my score if authors addresses my comments . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . Combination of unsupervised learning and RL to show that units in a recurrent network can be used to understand spatial and non-spatial firing patterns in the hippocampus 2 . Figure 4 and 5 are convincing . Cons : 1.While explaining the task the authors claim that the colours are chosen at random , however it is not clear whether these stay fixed across episodes or they changed . Intuitively it looks like , once the colour are generated , then they stay fixed , but in this case it is difficult to understand why a simple Q-leaning objective won \u2019 t be able to solve this task ( as claimed by the author ) . The only reason I can think about is if the steps between the cue and the reward are longer that the ones allowed by the discount factor chosen ( 0.8 ) . However , in the paper there are no details about the number of steps or how the discount affect the results . I think this is a serious issue . 2.The authors claim that they first pre-train on the predictive task , but then in the loss of eq.3 they report a combined loss . Does it mean that the loss_ { rgb } is also fine-tuned while training with the Q-learning objective ? This point need clarification . 3.The paper doesn \u2019 t report any details about the learning rates or the sizes of the linear layers used in eq.1 and 2.This way is impossible to replicate this results . This is a serious issue . 4.How the threshold for place cells are defined ? Why 30 % ? How many units show firing above that threshold ? This needs further analysis to support the decision , which otherwise seem very arbitrary . 5.Are figure 4 and 5 just cherry picked run or are these averaged across several testing runs ? Also in the captions of these figures the authors are using the singular \u201c observation \u201d in figure 4 and the plural \u201c observations \u201d in figure 5 . Does it means that the analysis have been performed differently ? Or is it just a mistake ? . This is an important point as I think the results will be more powerful with the same image fed as input , or even better with no image , just with 0ed input to simulate pondering . Minors : 1 . 380 units seems quite an unconventional number of units , why not 256 or 512 ? Can you please explain . Also is this number affecting the representations ? Have you done a sweep and settle on this number because it support your findings better ? If so , it would be important to mention it . 2.It would have been nice to have analysis to support the following sentence on page 4 : Generally , when the network loses its ability to self-localise the agent , state-action values are no longer reliable indicators of future reward potential as the current environmental state is not clearly discernible . Otherwise please correct it .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their feedback and for being transparent with regards to their evaluation . We have clarified the points made and have made substantial updates to the manuscript to address the reviewer \u2019 s comments . Cons : 1.Wall colours fixed or variable , Q-learning objective solving this task , steps between the cue and the reward and discount factor chosen ( 0.8 ) : We very much agree with the reviewer that this is ambiguous in our current explanation and we have updated the paper ( page 2 ) , explicitly stating that the wall colours remain fixed throughout training . In the paper we state that Q-learning alone can not solve the task after the network has been pre-trained on the predictive task . This is the case when using the rate of epsilon decay ( in epsilon-greedy RL training ) we use for the joint Q-learning and colour prediction training ( Eq.2 - Eq.3 in original paper ) described in the paper . We have run more rigorous analyses of running Q-learning alone with the same network without pre-training , Q-learning alone with pre-training and the joint Q-learning and colour prediction loss ( Eq.2 ) both with pre-training ( shown in the paper ) and without pre-training . We have updated the paper ( Figure 3 , page 5 ) to summarise these results . Taken together , training with a joint loss after pre-training converges with far fewer training iterations than the other cases . We have previously run analyses with differing discount factor values and find that a value higher than 0.8 regularly causes the model to converge without taking the most direct route to reward locations ( i.e converging on solutions with backtracking at secondary choice points when discount factor is higher than 0.8 ) . The number of steps in a single training episode is 30 ( if the agent takes the most direct path without backtracking ) . There are 5 steps from the cue to the choice point with 7 steps from the choice point to the first reward site . We have updated the paper ( page 4 and page 3 ) explaining the discount factor used and the number of steps inherent in the task . 2.Loss_rgb fine-tuned while training with the Q-learning objective : We agree this point is ambiguous and we have clarified this in the paper ( page 4 ) . The predictive pre-training task where loss_rgb is optimised converges completely . It is not fine-tuned while training the Q-learning objective to navigate to reward locations , but is required so the non-metric representation of space is maintained during learning . When loss_rgb is not included during Q-learning , the place fields are lost , and the training converges much slower to a different solution without an explicit representation of space . 3.Learning rates and sizes of linear layers used in eq.1 and 2 : We have updated the paper to include the following details on pages 2 , 3 and 4 of the manuscript . The linear layers in eq.1 and eq.2 ( Eq.3 in the updated manuscript ) are simply single layered readout layers for the LSTM . To clarify , when we have a 380 unit LSTM , the shapes of these readout layers are 380 x 12 ( when predicting four RGB wall colours ) and 380 x 4 ( when predicting Q-values ) respectively . We use a learning rate of 0.0005 with an Adam optimiser for reward training , as we find this learning rate gives good convergence ( without backtracking at secondary points ) with a greater range of training hyperparameters and initial conditions than with a learning rate of 0.001 . We use a learning rate of 0.001 for pre-training ."}, "1": {"review_id": "Jr8XGtK04Pw-1", "review_text": "Motivated by biological considerations , this paper shows that recurrent networks trained with a predictive and goal-based objective on a maze finding task , qualitatively recapitulate experimental findings in hippocampal recordings in rodents trained on the same task . In particular , these LSTM networks demonstrate both metric representations of their environment and nonlocal extrafield firing at decision points along the maze ( anticipating the future trajectory of the agent ) . Strengths : + I like that the authors take a normative approach that exhibits both metric and non-metric place cell representations of the environment , unifying prior findings in one model . + I appreciate that no velocity input is given to their model , in contrast to prior approaches . + I also liked the qualitative comparisons to hippocampal recordings from rodents trained on the same task ( especially Figures 6 and 7 ) . Weaknesses : - The primary conclusion , namely , training an RNN on a maze-like environment gives you place cells , is really not all that new , especially considering that the network is still supervised to predict position and landmarks . - Given that lack of novelty in the modeling conclusion , it would have therefore been nice to have seen more quantitative comparisons to hippocampal recordings in rodents . Does their approach explain more variance in these neurons than prior approaches ? Otherwise , it seems that they simply recapitulate prior qualitative comparisons . Minor comments : \u201c Recurrence based \u201d should be changed everywhere to \u201c recurrent \u201d ( e.g.on pg.1 ) , and \u201c Neuroscience \u201d is not capitalized . The motivation to use double Q learning should be expanded on in pg . 3 prior to equation 3 . Question : The authors mention that Q-learning performs poorly on tasks in dynamic environments \u2013 however , I do not see any evidence of this in the paper , it would be imperative to show this explicitly for the environments they consider . Suppose this is in fact the case , could you clarify what makes your approach more successful at this task than others ? Is it because of the pretraining to predict the subsequent observation of wall colors from the current wall color observations ? As it stands , I think the ideas of this paper are interesting and think it unifies prior approaches , but I do not think the conclusions from the modeling add all that much novel insight from prior approaches . Therefore , I recommend a weak accept .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their feedback and overall positive evaluation of our work . However , we believe the reviewer may be unfamiliar with the previous work in this area . We address the points made below and in our updated manuscript . Weaknesses : 1 . The primary conclusion , namely , training an RNN on a maze-like environment gives you place cells , is really not all that new , especially considering that the network is still supervised to predict position and landmarks . We think the reviewer is incorrect here , to the best of our knowledge training an RNN on an environment with emerging place cells has only been shown once before ( Recanatesi et al. , bioRxiv 2019 ) and for the first time in a maze-like environment in this work . Our main result is not purely the emergence of place cells , but the combination of predictive and reinforcement learning to solve a navigation task which results in network units displaying hippocampal neuron characteristics . It is important to point out that the network in our model is not supervised to predict position or landmarks at all , it is only instructed to predict subsequent visual stimuli in pre-training and extended to predict global reward in Q-learning . We think the reviewer may have been misled by this line in our discussion section : \u201c This is similar to the purely contextual input received by the model pre-trained by Xu & Barak ( 2020 ) where no velocity input is given , however , the network here is still trained on position and landmark prediction in a supervised way. \u201d To clarify , here we are referring to the work by Xu & Barak , not our work . We have updated our paper to make this clearer . Our hypothesis was that predictive learning is well suited to be combined with reinforcement learning ( we add a prediction of the Q-value for reward training ) . We show that the resulting representation not only allows efficient learning but also yields cells with properties observed in the hippocampus . It is important to stress that previous work where training was based on trajectories of velocity and position did not report such behaviour and only demonstrate the formation of activations with the form of entorhinal cortex grid cells which are used in navigation differently to place cells . These authors also did not analyse resulting dynamics . 2.Given that lack of novelty in the modeling conclusion , it would have therefore been nice to have seen more quantitative comparisons to hippocampal recordings in rodents . Does their approach explain more variance in these neurons than prior approaches ? Otherwise , it seems that they simply recapitulate prior qualitative comparisons . We agree with the reviewer that direct quantitative comparisons would be interesting yet they may be misguided given the significant differences between neural and neuronal networks . We believe in this context our main novel contribution is the demonstration that the combination of predictive and reinforcement learning results in dynamics also observed in the hippocampus , specifically we show extrafield firing of network units at locations outside of their apparent place fields ( Johnson & Redish , 2007 ) , non-local forward sweeping representation of the network ( Johnson & Redish , 2007 ) , place fields drifting towards reward locations throughout training ( Lee et al. , 2006 ) , a high proportion of units with place fields at the maze start location encode reward locations ( Ainge et al. , 2007 ) and that a higher proportion of units encode task phase than turn direction ( Griffin et al. , 2007 ) . These specific comparisons between RNNs and hippocampal neurons have not been demonstrated before as far as we are aware ."}, "2": {"review_id": "Jr8XGtK04Pw-2", "review_text": "* * Paper summary * * The main goal of this paper is to show that LSTM units in a network trained to solve a T-maze task , show similar activity patterns as neurons in rats solving a similar task . Specifically , the authors make the following claims : ( 1 ) an RNN learning the task by a combination of reinforcement and predictive learning produces internal representations with consistent extrafield firing associated with consequential decision points , ( 2 ) the network \u2019 s representation , once trained , follows a forward sweeping pattern similar to those found in rats and ( 3 ) a higher proportion of units in the trained network show strong selectivity for the choice phase of the task than for spatial topology , as seen in rats . * * Pros * * 1 . The submission is clear , well-written and the execution is competent . 2.I find the approach well motivated and the problem interesting for the current state of the field . 3.The authors provide a sufficient amount of details so that reproducibility should be possible . * * Cons * * I have concerns about key points of the paper and the interpretation of the results : 1 . One of the main results of the paper is the observation that the network produces a forward-moving representation similar to the one observed in rats at the decision point . However , the way the authors simulate this is by freezing the agent at such point and keeping the LSTM running , repeating the same constant observation . The LSTM was trained on trajectories on the maze , so in this trajectory ( which was never used during training ) , the network is completely out of distribution . The activity of any network in this situation is difficult to interpret . Because the network was trained with a predictive loss on two very specific sequences of observations , it seems plausible that it is robust to the change of input statistics and follows the same sequence , maybe with some instabilities . Note that the cue was present , which explains why the correct sequence is followed . In the network trained also by RL in particular , the authors interpret this as \u201c The agent appears to be sampling the trajectory concerning the alternate return arm of the maze before ultimately settling on the rewarding return arm \u201d . But this is , in my opinion , an over-interpretation , as the agent has no sampling capability in the first place ( there is no generative model of observations ) , nor any particular planning mechanism . Alternatively , the authors may be claiming that this jumping behavior happens only after the RL training and not before , in which case they should emphasize this difference and quantify it explicitly . Although in this case , a simple explanation for this could be that due to the epsilon-greedy , only during the RL training the network is exposed to the wrong cue-arm combination . Therefore , the LSTM would be less able to rely on the cue , which could explain the jump between attractors in the out-of-distribution case . In the discussion section , the authors claim \u201c We demonstrate that extrafield firing activity [ .. ] emerges when a simulated agent [ ... ] pauses at decision points - suggesting intrinsic dynamics are encoding the future planned trajectory of the agent. \u201d . I find this to be an over-claim , as the agent doesn \u2019 t pause ( it can \u2019 t ) and doesn \u2019 t plan ( for any common definition of planning ) . I would be more convinced if the agent was able to pause ( as an additional action ) and this behavior was observed in the LSTM activity in this situation , which is closer to the biological case . 2.The pre-train stage is done on trajectories that correspond to the solved task . This means that the LSTM trained by the predictive loss is not exposed to the general structure of the environment but to the specific solution of the task , including the cue-choice association and the exact sequence of observations in each of the two correct trajectories . The authors draw a parallel with the pre-training phase in behavioral experiments ( Johnson & Redish , 2007 ) in which rats usually run each trajectory separately ( by having the other one blocked ) . However , in my opinion , this is problematic for their analysis . First it \u2019 s unclear to what extent the network is learning by RL as during the pre-training it has already learnt to predict the observation corresponding to the correct turn ( wich corresponds to one of the two actions ) . Second , the LSTM is exposed only to the correct cue-arm trajectories , which I think is the reason why the forward-looking sweeps only follow these trajectories ( see previous point ) . This is more similar to a demonstration than to pre-training . A more conventional pre-training would leave the agent to explore freely to implicitly learn the structure of the environment ( a la Tollman ) . On the other hand , the argument of following the protocol of the behavioral experiments also doesn \u2019 t fully work as the rats are still producing motor outputs and even being rewarded during the pre-training phase ( Johnson & Redish , 2007 ) . 3.Finally , a more general concern is the main point of the paper . If I understand correctly , the main claim is the similarity of the observations between the RNN agent and the experimental findings in rats . However , given that there are plenty of arbitrary choices when training an RNN , I believe the results are not particularly explanatory . I would encourage the authors to formulate better alternative hypothesis and controlled experiments . For example , I would find it interesting to show that the forward-sweeping observations done in rats , which is often interpreted as a signature of planning or prediction of the consequences of future actions , arises simply from a next-step prediction loss in an overtrained rat . Minor concerns : - \u201c As such , a network of Gated Recurrent Units [ ... ] or vanilla RNN units was unable to perform well in either the pre-training or joint RL task due to these prevalent long term dependencies. \u201d How many steps are there between cue and choice , and between choice and reward ? - Related to the previous point : \u201c We attempted to run the reinforcement learning task alone in a maze with no wall colours or environment statistics except the cue . In this scenario the network is not able to learn the task due to a lack of self-localisation. \u201d If I understand correctly , there is a constant number of steps between the cue and the moment where the choice has to be made . I would tend to believe that an LSTM can learn to make a prediction only based on the number of timesteps , regardless of the lack of wall observations ( e.g.2 sequence problem in Hochreiter and Schmidhuber , 1997 ) . Details : - It would be good to clarify what exactly is the action set of the agent . - I would like to know how exactly are activity maps obtained . The authors mention \u201c Place fields determined by contiguous locality with average activity exceeding 30 % peak unit activity during a single left trajectory followed by a right trajectory \u201d but I don \u2019 t find this particularly clear . I also found it difficult to understand the bottom row of Fig 3 . - Fig 5 should be referred to in the paragraph starting with \u201c In stark contrast to the dynamics of the LSTM network following predictive pre-training ... \u201d - Why is the return to start representation in Fig.6 different for right and left trajectories ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their careful reading of our paper and for their scrupulous evaluation of our results . We have made substantial updates and alterations to the manuscript to address the reviewer \u2019 s comments and we respond to feedback below . Cons : 1 . * \u201c One of the main results of the paper [ ... ] , repeating the same constant observation. \u201d This is what the rodent is doing experimentally in ( Johnson & Redish , 2007 ) , and we replicate this experiment directly , so we feel prudent to follow this as a reasonable simulation for subsequent comparison of hippocampal characteristics . In page 4 of the original manuscript we state that , in experiments , rodents seem to pause at high consequence decision points ( Johnson & Redish , 2007 ) with alternating head movement behaviour signifying vicarious trial and error ( VTE ) ( Muenzinger , 1938 ; Hu & Amsel , 1995 ) . * \u201c The LSTM was trained on trajectories on the maze , [ ... ] , which explains why the correct sequence is followed. \u201d As the reviewer states , the activity of the network is difficult to interpret in this situation but we agree this is a plausible interpretation of network dynamics after pre-training and may in fact be what is occurring experimentally . * \u201c In the network trained also by RL in particular , the authors interpret this as [ ... ] , nor any particular planning mechanism. \u201d We agree that this may be an over-interpretation as the agent has no active sampling capability . Our wording here is ambiguous and we have removed references to agent sampling in the updated manuscript . However , sweeping behaviour ( as shown experimentally ) by Johnson & Redish is undoubtedly occurring . * \u201c Alternatively , the authors may be claiming [ ... ] quantify it explicitly. \u201d The path switching behaviour does only occur after reward training and not before , we have further emphasised this in the paper on page 7 . We have updated the paper to indicate that path switching occurs reliably in a very similar way after reward training with differing numbers of LSTM units and initial conditions as long as the reward task is solved without backtracking at secondary cue locations and is trained on the combined loss ( Eq.2 , this was Eq.3 in the original paper ) . Further analysis of this can be seen in Figure 3 which we introduce in the updated manuscript . * \u201c Although in this case , [ ... ] out-of-distribution case. \u201d We would argue that the network is more able to rely on the cue during reward training due to the spatial map of the maze formed during pre-training . Exposure to the wrong-arm combination is not sufficient for the network to exhibit the path switching behaviour shown in Figure 6 for various reasons . Firstly , the network converges to a solution which does not include backtracking at secondary points , therefore the network is choosing actions at the choice points which leads the agent to take a direct path to reward locations . Thus path switching is not part of the network \u2019 s inherent behaviour . Secondly , in the out of distribution case when the agent is paused at the primary choice point , the instantaneous representation jump from the rewarding maze arm to the opposing maze arm is not occurring anywhere in training , even at the beginning of epsilon-greedy reward training when actions are chosen completely randomly . Lastly , simply being out of distribution can not possibly explain the secondary path switch shown at timestep 32 in Figure 6 . * \u201c In the discussion section , [ ... ] the biological case. \u201d We agree that this may be an overclaim and we have removed suggestions of active planning in our updated manuscript , however stark similarity in terms of dynamics to the experimental case is a significant insight we believe . We thank the reviewer for the idea of having agent induced pausing as additional trainable behaviour which would certainly be closer to the biological case . We believe this would be too great a change to the current work as a revision but we will certainly explore this in future work ."}, "3": {"review_id": "Jr8XGtK04Pw-3", "review_text": "In this paper , the authors train a recurrent neural network on a navigation task , and observe the emergence of several phenomena reminiscent of the hippocampus : appearance of place cells with a secondary receptive field at task-relevant locations ; anticipation of possible future paths in the activity of the model , with alternation in time between possible future paths ; a high proportion of neurons tuned to task variables rather than animal trajectory . Strong points : - these findings are compelling , they account for some hallmark properties of the activity of hippocampus , and they could lead to a better understanding of the role and function of the hippocampus . - the experiments are rigorous and convincing . Weak points : - some technical aspects of the paper could be clarified ( see below ) - it is unclear how this model improved our understanding of the hippocampus function , and whether the model makes any testable predictions about the hippocampus . I recommend to accept this paper because of its strengths listed above . Clarification questions : 1 ) I did not understand the role of secondary cue point . Why were these required in addition to the primary cue point and choice point ? 2 ) vocabulary : What are `` metric '' and `` non-metric '' representations ? 3 ) How reliable is the alternative path visiting phenomenon ? Can this phenomenon be observed reliably in networks trained from different initial conditions ? 4 ) I did not understand the consequences/take-homes of the second and third paragraph discussions . Additional feedback : 1 ) It would be interesting to see a discussion on what we learned about the function of the hippocampus from this model , and/or what predictions this model makes about neural activity in hippocampus . 2 ) Are there any oscillations and phase-precessions phenomena in the model ? If not , it would be interesting to discuss why these oscillations might be present in the brain but not in the model . 3 ) Could this network be used to simulate difficult experiments , e.g.understand how future paths exploration works in an open-field setting ? 4 ) The first sentence of the abstract is difficult to understand . In general , shorter sentences could improve clarity .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their careful reading and positive evaluation of our paper , in addition to the constructive feedback . We have clarified the points raised and updated the manuscript accordingly . Weak point : Using our model to improve understanding of the hippocampus and make testable predictions : We fully agree that this would be the ultimate goal of our model , to improve understanding of the brain with a relatively small RNN model which can be used to test hypotheses regarding hippocampal dynamics . In this work we aim to show that the LSTM dynamics resulting from training the network using the combination of predictive and reinforcement learning on the maze reward task , mirrors hippocampal neuronal dynamics found experimentally , and as such provides evidence that the underlying learning rules may be similar in the hippocampus . Specifically , we show that non-metric attractors form in the activation space of our network units in the way of place cells , we show extrafield firing of these units at location outside of their apparent place fields , non-local forward sweeping representation of the network , place fields drifting towards reward locations throughout training , a high proportion of units with place fields at the maze start location encode reward locations and that a higher proportion of units encode task phase than turn direction . As far as we know , this is the first model to replicate all these behaviours , and as such provides evidence that the underlying learning rules may be similar in the hippocampus . We therefore expect that our model can be used to generate new predictions in other tasks , and are currently working on this question . We have added a section on how our model could improve hippocampus understanding in the discussion section of the paper . Clarification Questions : 1 . Role of the secondary cue : The reviewer makes an important point on the necessity of the secondary cue points in this task . Our overall aim is to compare our trained model \u2019 s dynamics with that of hippocampal neurons as captured using experimental data . To this end we aim to mirror these experimental set ups as closely as possible - in this case we mirror the maze set up of Jonson and Redish , 2007 in order to optimally compare our resulting dynamics to that of hippocampal neurons . In terms of the task , including secondary cue/choice points gives the agent the opportunity to backtrack on its decision made at the primary choice point in light of further environmental observation ( the presentation or lack thereof of the secondary cue ) . We believe that experimentally , the presence of secondary cue points gives rise to some of the extrafield firing observed by Johnson and Redish as the rodent reevaluates its prior primary choice at a secondary cue point , resulting in the firing of place cells with place fields on the opposing side of the maze . We have updated the paper ( page 3 ) justifying the inclusion of the secondary cue . 2.Metric and non-metric representations : Metric representations relate to a Euclidean spatial map of an environment and are biologically akin to grid cells in the entorhinal cortex , whereas non-metric representations relate to associative landmark maps of the environment and are comparable to place cells in the hippocampus . We feel this is adequately explained in the introduction ."}}