{"year": "2017", "forum": "Sk8csP5ex", "title": "The loss surface of residual networks: Ensembles and the role of batch normalization", "decision": "Reject", "meta_review": "The paper presents an analysis of residual networks and argues that the residual networks behave as ensembles of shallow networks, whose depths are dynamic. The authors argue that their model provides a concrete explanation to the effectiveness of resnets. \n \n However, I have to agree with reviewer 1 that the assumption of path independence is deeply flawed. In my opinion, it was also flawed in the original paper. Using that as a justification to continue this line of research is not the right approach. We cannot construct a single practical scenario where path independence may be expected to hold. So we should not be encouraging papers to continue this line of flawed reasoning.\n \n I thus cannot recommend acceptance of this paper.", "reviews": [{"review_id": "Sk8csP5ex-0", "review_text": " This paper extend the Spin Glass analysis of Choromanska et al. (2015a) to Res Nets which yield the novel dynamic ensemble results for Res Nets and the connection to Batch Normalization and the analysis of their loss surface of Res Nets. The paper is well-written with many insightful explanation of results. Although the technical contributions extend the Spin Glass model analysis of the ones by Choromanska et al. (2015a), the updated version could eliminate one of the unrealistic assumptions and the analysis further provides novel dynamic ensemble results and the connection to Batch Normalization that gives more insightful results about the structure of Res Nets. It is essential to show this dynamic behaviour in a regime without batch normalization to untangle the normalization effect on ensemble feature. Hence authors claim that steady increase in the L_2 norm of the weights will maintain the this feature but setting for Figure 1 is restrictive to empirically support the claim. At least results on CIFAR 10 without batch normalization for showing effect of L_2 norm increase and results that support claims about Theorem 4 would strengthen the paper. This work provides an initial rigorous framework to analyze better the inherent structure of the current state of art Res Net architectures and its variants which can stimulate potentially more significant results towards careful understanding of current state of art models (Rather than always to attempting to improve the performance of Res Nets by applying intuitive incremental heuristics, it is important to progress on some solid understanding too).", "rating": "7: Good paper, accept", "reply_text": "We thank AnonReviewer3 for the supportive review . The reviewer asked that we provide CIFAR-10 results for ResNets without Batch Normalization . These results are provided below . The experiments are almost identical to the previous experiments with a few exceptions : ( 1 ) All Batch Normalization layers were removed ; ( 2 ) Since it is harder to train without Batch Normalization , we focus on ResNets of depth 20 ; and ( 3 ) We employ an initial learning rate that is ten times smaller , otherwise NaN occurs after a few epochs . CIFAR-10 Norm of the convolutional layers \u2019 weights per layer for multiple epochs http : //imgur.com/JdRAM7j CIFAR-10 Mean norm of weights per epoch http : //imgur.com/46ngxb6 CIFAR-100 Norm of weights per layer for multiple epochs http : //imgur.com/ANKeRz0 CIFAR-100 Mean norm of weights per epoch http : //imgur.com/MzHOFgE Note that there is a steady increase in the weight norm until epoch 81 when the learning rate is reduced , at which point the norms stabilize . This could stem from the networks being slower to train without Batch Normalization and epoch 81 being too early for reducing the learning rate ."}, {"review_id": "Sk8csP5ex-1", "review_text": "This paper shows how spin glass techniques that were introduced in Choromanska et al. to analyze surface loss of deep neural networks can be applied to deep residual networks. This is an interesting contribution but it seems to me that the results are too similar to the ones in Choromanska et al. and thus the novelty is seriously limited. Main theoretical techniques described in the paper were already introduced and main theoretical results mentioned there were in fact already proved. The authors also did not get rid of lots of assumptions from Choromanska et al. (path-independence, assumptions about weights distributions, etc.).", "rating": "3: Clear rejection", "reply_text": "We completely disagree with the novelty assertions made by the AnonReviewer1 . Most of our main results are very distant from the results of Choromanska et al.and are entirely novel both technically and conceptually . For example , the detailed study of the driving force in ResNets . We also disagree with the assertion that \u201c The authors also did not get rid of lots of assumptions from Choromanska et al. \u201d , since , as the previous discussion on openreview.net reveals , we make selective use of these assumptions and Theorems 3 and 4 are virtually assumption free . Please refer to the discussion section of the revised manuscript , uploaded a few days ago . We are at a great disadvantage in this discussion since the assertions of AnonReviewer1 are made in general terms , which makes it difficult for us to understand ( 1 ) which results are deemed as incremental and ( 2 ) in what way these are judged as incremental , ( 3 ) which theoretical techniques are claimed to have already been done and ( 4 ) where these were done . We would be grateful if the reviewer could point to specific theorems that might be problematic and would agree to weigh their importance in the context of the entire body of presented results . In order to promote such a discussion , we address below each theorem and some lemmas along the dimensions of closest analogy in the literature and technical novelty . We believe that the facts clearly support the novelty of our work both conceptually and technically . If AnonReviewer1 disagrees , we respectfully ask to consider addressing specific results . Theorem 1 ======== What it shows : An expression for the effective depth of a deep ResNet , in the limit of infinite depth . Closest analogy in the literature : It has been argued that ResNets exhibit the properties of ensembles , however we are not aware of a similar mathematical analysis . Technical novelty : This theorem presents a novel mathematical treatment of the effective depth of ResNets . Theorem 2 ======== What it shows : Deep ResNets behave as ensembles concentrated around a narrow band near the maximum . Closest analogy in the literature : We are not aware of a similar claim . Technical novelty : The proof of this claim is quite involved and is not trivial . Theorem 3 ======== What it shows : The driving force behind the capacity increase of residual nets during training , when batch normalization is applied . Closest analogy in the literature : As far as we are aware , there is no other work which points out to this type dynamic behaviour in ResNets or elsewhere . Technical novelty : We are not aware of any similar analysis . Nothing in the proof follows existing results . Theorem 4 ======== What it shows : The driving force behind the capacity increase of residual nets during training , without batch normalization . Closest analogy in the literature : As far as we are aware , no similar analysis exists . Technical novelty : We are not aware of any similar analysis . Theorem 5 ======== What it shows : The loss surface of ensembles , in the context of general spin glass models . Here we use results of spin glass theory to demonstrate the landscape of the loss in ensembles , compared with single models , in terms of the number of critical points . Closest analogy in the literature : This comparison presents a novel viewpoint on ensembles , as far as we are aware . Technical novelty : In this theorem we use the results of spin glass theory . Most of the technical heavy lifting for this specific theorem was done in Auffinger 2013 Lemma 1+2 ========= What is shown : The similarity between the loss of ResNets , and the hamiltonian of a general spin glass model . Closest analogy in the literature : Although a similar analysis was performed of traditional networks , this analogy presented additional technical difficulties , and is novel in its claim . Technical novelty : Some technical aspects of this analogy are novel and not presented in the original work of Choromanska et al.especially Lemma 2 . Lemma 4 ======== What it shows : The effective depth of ResNets can be controlled through weight scaling . Closest analogy in the literature : We are not aware of a similar claim . Technical novelty : We are not aware of any similar analysis ."}, {"review_id": "Sk8csP5ex-2", "review_text": "Summary: In this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks. Clarity: This paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading. A summary of some of the proofs in the main text might have been helpful. Specific Comments: - In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?) - The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped? (Can the same analysis be used if only one layer is skipped? It seems like the skipping mostly affects the number of paths there are of a certain length?) - The new experiments supporting the scale increase in practice are interesting! I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2? ", "rating": "7: Good paper, accept", "reply_text": "Specifically for the proof of Lemma 2 , \\beta is the multiplicity of each input value from the p-dimensional input vector x in the expression \\xi . We will change it to another symbol and clarify this point . The next version to be released very soon , will include these additional clarifications and we hope that the issue will thus be resolved . In our framework , as you pointed out , the skipping connectiones affect the number of paths of a specific length . This number is manifested in the parameter gamma_r . The architecture used in our analysis includes a skip connection per layer and gives rise to paths of every length between 1 the r. This simplifies the notation and was therefore beneficial in our analysis . Exactly the same analysis as in Sec.3 can be performed with 2 layers or more per skipped block . The difference would change gamma_r while keeping everything else the same . The results of Sec.5 employ the general spherical spin glass model as presented and analyzed in [ Auffinger 2013 ] . This model includes every order of spin interaction between 1 and infinity , and thus fits the one skip per layer model , for large p. The extrapolation of Theorem 5 to the case of skipping a multi-layer block is technically possible , however it requires additional approximations . Sec.4 , Theorems 3 and 4 , as pointed out , studies each skip connection by itself . It can be applied to a block of arbitrary size . Since it holds for each block individually , it holds also for a network of multiple blocks ."}], "0": {"review_id": "Sk8csP5ex-0", "review_text": " This paper extend the Spin Glass analysis of Choromanska et al. (2015a) to Res Nets which yield the novel dynamic ensemble results for Res Nets and the connection to Batch Normalization and the analysis of their loss surface of Res Nets. The paper is well-written with many insightful explanation of results. Although the technical contributions extend the Spin Glass model analysis of the ones by Choromanska et al. (2015a), the updated version could eliminate one of the unrealistic assumptions and the analysis further provides novel dynamic ensemble results and the connection to Batch Normalization that gives more insightful results about the structure of Res Nets. It is essential to show this dynamic behaviour in a regime without batch normalization to untangle the normalization effect on ensemble feature. Hence authors claim that steady increase in the L_2 norm of the weights will maintain the this feature but setting for Figure 1 is restrictive to empirically support the claim. At least results on CIFAR 10 without batch normalization for showing effect of L_2 norm increase and results that support claims about Theorem 4 would strengthen the paper. This work provides an initial rigorous framework to analyze better the inherent structure of the current state of art Res Net architectures and its variants which can stimulate potentially more significant results towards careful understanding of current state of art models (Rather than always to attempting to improve the performance of Res Nets by applying intuitive incremental heuristics, it is important to progress on some solid understanding too).", "rating": "7: Good paper, accept", "reply_text": "We thank AnonReviewer3 for the supportive review . The reviewer asked that we provide CIFAR-10 results for ResNets without Batch Normalization . These results are provided below . The experiments are almost identical to the previous experiments with a few exceptions : ( 1 ) All Batch Normalization layers were removed ; ( 2 ) Since it is harder to train without Batch Normalization , we focus on ResNets of depth 20 ; and ( 3 ) We employ an initial learning rate that is ten times smaller , otherwise NaN occurs after a few epochs . CIFAR-10 Norm of the convolutional layers \u2019 weights per layer for multiple epochs http : //imgur.com/JdRAM7j CIFAR-10 Mean norm of weights per epoch http : //imgur.com/46ngxb6 CIFAR-100 Norm of weights per layer for multiple epochs http : //imgur.com/ANKeRz0 CIFAR-100 Mean norm of weights per epoch http : //imgur.com/MzHOFgE Note that there is a steady increase in the weight norm until epoch 81 when the learning rate is reduced , at which point the norms stabilize . This could stem from the networks being slower to train without Batch Normalization and epoch 81 being too early for reducing the learning rate ."}, "1": {"review_id": "Sk8csP5ex-1", "review_text": "This paper shows how spin glass techniques that were introduced in Choromanska et al. to analyze surface loss of deep neural networks can be applied to deep residual networks. This is an interesting contribution but it seems to me that the results are too similar to the ones in Choromanska et al. and thus the novelty is seriously limited. Main theoretical techniques described in the paper were already introduced and main theoretical results mentioned there were in fact already proved. The authors also did not get rid of lots of assumptions from Choromanska et al. (path-independence, assumptions about weights distributions, etc.).", "rating": "3: Clear rejection", "reply_text": "We completely disagree with the novelty assertions made by the AnonReviewer1 . Most of our main results are very distant from the results of Choromanska et al.and are entirely novel both technically and conceptually . For example , the detailed study of the driving force in ResNets . We also disagree with the assertion that \u201c The authors also did not get rid of lots of assumptions from Choromanska et al. \u201d , since , as the previous discussion on openreview.net reveals , we make selective use of these assumptions and Theorems 3 and 4 are virtually assumption free . Please refer to the discussion section of the revised manuscript , uploaded a few days ago . We are at a great disadvantage in this discussion since the assertions of AnonReviewer1 are made in general terms , which makes it difficult for us to understand ( 1 ) which results are deemed as incremental and ( 2 ) in what way these are judged as incremental , ( 3 ) which theoretical techniques are claimed to have already been done and ( 4 ) where these were done . We would be grateful if the reviewer could point to specific theorems that might be problematic and would agree to weigh their importance in the context of the entire body of presented results . In order to promote such a discussion , we address below each theorem and some lemmas along the dimensions of closest analogy in the literature and technical novelty . We believe that the facts clearly support the novelty of our work both conceptually and technically . If AnonReviewer1 disagrees , we respectfully ask to consider addressing specific results . Theorem 1 ======== What it shows : An expression for the effective depth of a deep ResNet , in the limit of infinite depth . Closest analogy in the literature : It has been argued that ResNets exhibit the properties of ensembles , however we are not aware of a similar mathematical analysis . Technical novelty : This theorem presents a novel mathematical treatment of the effective depth of ResNets . Theorem 2 ======== What it shows : Deep ResNets behave as ensembles concentrated around a narrow band near the maximum . Closest analogy in the literature : We are not aware of a similar claim . Technical novelty : The proof of this claim is quite involved and is not trivial . Theorem 3 ======== What it shows : The driving force behind the capacity increase of residual nets during training , when batch normalization is applied . Closest analogy in the literature : As far as we are aware , there is no other work which points out to this type dynamic behaviour in ResNets or elsewhere . Technical novelty : We are not aware of any similar analysis . Nothing in the proof follows existing results . Theorem 4 ======== What it shows : The driving force behind the capacity increase of residual nets during training , without batch normalization . Closest analogy in the literature : As far as we are aware , no similar analysis exists . Technical novelty : We are not aware of any similar analysis . Theorem 5 ======== What it shows : The loss surface of ensembles , in the context of general spin glass models . Here we use results of spin glass theory to demonstrate the landscape of the loss in ensembles , compared with single models , in terms of the number of critical points . Closest analogy in the literature : This comparison presents a novel viewpoint on ensembles , as far as we are aware . Technical novelty : In this theorem we use the results of spin glass theory . Most of the technical heavy lifting for this specific theorem was done in Auffinger 2013 Lemma 1+2 ========= What is shown : The similarity between the loss of ResNets , and the hamiltonian of a general spin glass model . Closest analogy in the literature : Although a similar analysis was performed of traditional networks , this analogy presented additional technical difficulties , and is novel in its claim . Technical novelty : Some technical aspects of this analogy are novel and not presented in the original work of Choromanska et al.especially Lemma 2 . Lemma 4 ======== What it shows : The effective depth of ResNets can be controlled through weight scaling . Closest analogy in the literature : We are not aware of a similar claim . Technical novelty : We are not aware of any similar analysis ."}, "2": {"review_id": "Sk8csP5ex-2", "review_text": "Summary: In this paper, the authors study ResNets through a theoretical formulation of a spin glass model. The conclusions are that ResNets behave as an ensemble of shallow networks at the start of training (by examining the magnitude of the weights for paths of a specific length) but this changes through training, through which the scaling parameter C (from assumption A4) increases, causing it to behave as an ensemble of deeper and deeper networks. Clarity: This paper was somewhat difficult to follow, being heavy in notation, with perhaps some notation overloading. A summary of some of the proofs in the main text might have been helpful. Specific Comments: - In the proof of Lemma 2, I'm not sure where the sequence beta comes from (I don't see how it follows from 11?) - The ResNet structure used in the paper is somewhat different from normal with multiple layers being skipped? (Can the same analysis be used if only one layer is skipped? It seems like the skipping mostly affects the number of paths there are of a certain length?) - The new experiments supporting the scale increase in practice are interesting! I'm not sure about Theorems 3, 4 necessarily proving this link theoretically however, particularly given the simplifying assumption at the start of Section 4.2? ", "rating": "7: Good paper, accept", "reply_text": "Specifically for the proof of Lemma 2 , \\beta is the multiplicity of each input value from the p-dimensional input vector x in the expression \\xi . We will change it to another symbol and clarify this point . The next version to be released very soon , will include these additional clarifications and we hope that the issue will thus be resolved . In our framework , as you pointed out , the skipping connectiones affect the number of paths of a specific length . This number is manifested in the parameter gamma_r . The architecture used in our analysis includes a skip connection per layer and gives rise to paths of every length between 1 the r. This simplifies the notation and was therefore beneficial in our analysis . Exactly the same analysis as in Sec.3 can be performed with 2 layers or more per skipped block . The difference would change gamma_r while keeping everything else the same . The results of Sec.5 employ the general spherical spin glass model as presented and analyzed in [ Auffinger 2013 ] . This model includes every order of spin interaction between 1 and infinity , and thus fits the one skip per layer model , for large p. The extrapolation of Theorem 5 to the case of skipping a multi-layer block is technically possible , however it requires additional approximations . Sec.4 , Theorems 3 and 4 , as pointed out , studies each skip connection by itself . It can be applied to a block of arbitrary size . Since it holds for each block individually , it holds also for a network of multiple blocks ."}}