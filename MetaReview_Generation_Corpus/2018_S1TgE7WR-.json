{"year": "2018", "forum": "S1TgE7WR-", "title": "Covariant Compositional Networks For Learning Graphs", "decision": "Invite to Workshop Track", "meta_review": "This is a good contribution, with the potential to become extremely good and significant if presentation is substantially improved.\nAll reviewers comment on the lack of clarity of the paper, especially concerning its central contributions (Section 4 and 5), as illustrated also by the relatively low confidence scores.\nReviewers also mention the current imbalance between the generality of high-order compositional networks and the motivation and empirical evaluation of these models. Generalizations of graph neural representations based on higher order local interactions are particularly interesting in contexts such as combinatorial optimization, where heuristics typically exploit high-order interactions.\n\nIn summary, we believe this work deserves a further iteration before it can be in proceedings in order to improve the exposition and the motivation of compositional networks, that will greatly improve its exposure to the community.  That said, the idea it lays forward is of potential interest, and thus the AC recommends resubmission to the workshop track. ", "reviews": [{"review_id": "S1TgE7WR--0", "review_text": "Thank you for your contribution to ICLR. The paper covers a very interesting topic and presents some though-provoking ideas. The paper introduces \"covariant compositional networks\" with the purpose of learning graph representations. An example application also covered in the experimental section is graph classification. Given a finite set S, a compositional network is simply a partially ordered set P where each element of P is a subset of S and where P contains all sets of cardinality 1 and the set S itself. Unfortunately, the presentation of the approach is extremely verbose and introduces old concepts (e.g., partially ordered set) under new names. The basic idea (which is not new) of this work is that we need to impose some sort of hierarchical order on the nodes of the graph so as to learn hierarchical feature representations. Moreover, the hierarchical order of the nodes should be invariant to valid permutations of the nodes, that is, two isomorphic graphs should have the same hierarchical order on their nodes and the same feature representations. Since this is the case for graph embedding methods that collect feature representations from their neighbors in the graph (and where the feature aggregation functions are symmetric) it makes sense that \"compositional networks\" generalize graph convolutional networks (and the more general message passing neural networks framework). The most challenging problem, however, namely the problem of finding a concrete and suitable permutation invariant hierarchical decomposition of the nodes plus some aggregation/pooling functions to compute the feature representations is not addressed in sufficient detail. The paper spends a lot of time on some theoretical definitions and (trivial) proofs but then fails to make the connection to an approach that works in practice. The description of the experiments and which compositional network is chosen and how it is chosen seems to be missing. The only part hinting at the model that was actually used in the experiments is the second paragraph of the section 'Experimental Setup', consisting of one long sentence that is incomprehensible to me. Instead of spending a lot of effort on the definitions and (somewhat trivial) propositions in the first half of the paper, the authors should spend much more time on detailing the experiments and the actual model that they used. In an effort to make the framework as general as possible, you ended up making the paper highly verbose and difficult to follow. Please address the following points or clarify in your rebuttal if I misunderstood something: - what precisely is the novel contribution of your work (it cannot be \"compositional networks\" and the propositions concerning those because these are just old concepts under new names)? - explain precisely (and/or more directly/less convoluted) how your model used in the experiments looks like; why do you think it is better than the other methods? - given that compositional network is a very general concept (partially ordered set imposed on subsets of the graph vertices), what is the principled set of steps one has to follow to arrive at such a compositional network tailored to a particular graph collection? isn't (or shouldn't) that be the contribution of this work? Am I missing something? In general, you should write the paper much more to the point and leave out unnecessary math (or move to an appendix). The paper is currently highly inaccessible.", "rating": "5: Marginally below acceptance threshold", "reply_text": "The reviewer 's comments would be valid if our method really was based on imposing a partial order on the vertices of the input graph ( i.e. , if we were turning it into a DAG ) . However this is not what we do . Our algorithm operates with two separate graphs : the original graph G , and the composition scheme M constructed from G. M is indeed a DAG ( i.e. , it defines a partial order ) , but G is whatever the input is , so there is no need to impose `` some sort of hierarchical order '' on its nodes that the reviewer is missing from the presentation . The nodes of M correspond to * subsets * of the nodes of G , specifically neighborhoods of increasing radii , which naturally form a partial order by inclusion . This is explicitly stated in the paper in multiple places , eg. , `` In composition networks for graphs , the atoms will usually be the vertices , and the P_i parts will correspond to clusters of nodes or neighborhoods of different radii '' ( p.4 ) . The construction of M is given explicitly on in points M1-M3 on page 5 . Furthermore , Figure 5 shows how the neighborhoods are nested in the case of a simple input graph . The reason for all the definitions in Sections 3 and 4 is that we need to define how M is constructed and what covariance conditions the corresponding activations must satisfy . Incidentally , this is also the main novelty of the paper . If the reviewer did n't understand these sections , it is not surprising that the `` Experiments '' section seems seems hazy and the tensor contractions in Section 5 just seem like unnecessary fluff . Given the above misunderstanding it is also not surprising that the reviewer thinks that the paper is not very novel . The word `` compositional network '' may have been used before in different contexts , but the general way to construct a DAG from a structured input object , and the invariance properties discussed in Section 4 have not been discussed before in the literature . As we explain , some existing networks are special cases , but the general framework is entirely novel . By breaking down our presentation into a sequence of fairly precise definitions and propositions , and drawing figures such as Figure 2 , which depicts the composition scheme , and Figure 5 , which depicts the neighborhoods in G that correspond to each of the nodes of the composition scheme , we tried to make the notion of composition scheme as clear as possible . We are disappointed that the message still did n't get through , and we would welcome suggestions on how to better convey what is really the main message and main novelty in the paper . ( Maybe by a figure with the original graph and the composition scheme side by side ? ) We would appreciate it if the reviewer revised his evaluation in light of the above clarification and would very much welcome suggestions on how to change the presentation so as to avoid other leaders falling prey to the same misunderstanding . However , we can vouch that the math is not unnecessary : without it , this construction would simply not work ."}, {"review_id": "S1TgE7WR--1", "review_text": "The paper presents a generalized architecture for representing generic compositional objects, such as graphs, which is called covariant compositional networks (CCN). Although, the paper is well structured and quite well written, its dense information and its long size made it hard to follow in depth. Some parts of the paper should have been moved to appendix. As far as the evaluation, the proposed method seems to outperform in a number of tasks/datasets compare to SoA methods, but it is not really clear whether the out-performance is of statistical significance. Moreover, in Table 1, training performances shouldn't be shown, while in Table 3, RMSE it would be nice to be shown in order to gain a complete image of the actual performance.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your comments . The paper is longer than usual because rather than just proposing a tweak on some existing algorithm , it derives a general framework for dealing with the issue of covariance in compositional-type neural architectures . We could n't figure out how to present all this in less than 15 pages without making the paper unreadably dense . Our results on HCEP are quite spectacular . While we did n't perform formal tests of statistical significance , it is quite clear that CCN far outperforms the other methods . For the MUTAG , etc . benchmarks it is harder to show statistical significance , simply because these datasets are small and multiple algorithms are neck and neck . For QM9 , although PSCN is admittedly close , we consistently beat it ( as well as the other two competitors ) on all 13 regression tasks in both MAE and RMSE , which again gives us confidence that the results are not just a statistical fluke . Thank you for your suggestion to include RMSE , we added a separate table to show that . Incidentally , the experiments required writing our own custom deep learning library in C++ ( which now can also use GPUs ) . The code has been released on GitHub , unfortunately we can not provide a link at this point because it would break the anonymity of the submission . Notwithstanding the above , we feel that sate-of-the-art experimental results are just one part of the paper 's contribution . The new conceptual framework of compositional networks and our mathematical results on how to make them covariant is even more important . We would appreciate it if the review was revised to reflect on the main body of the paper as well , not just the `` Experiments '' section ."}, {"review_id": "S1TgE7WR--2", "review_text": "The paper introduces a formalism to perform graph classification and regression, so-called \"covariant compositional networks\", which can be seen as a generalization of the recently proposed neural message passing algorithms. The authors argue that neural message passing algorithms are not able to sufficiently capture the structure of the graph since their neighborhood aggregation function is permutation invariant. They argue that relying on permutation invariance will led to some loss of structural information. In order to address this issue they introduce covariant comp-nets, which are a hierarchical decompositon of the set of vertices, and propose corresponding aggregation rules based on tensor arithmetic. Their new method is evaluated on several graph regression and classification benchmark data sets, showing that it improves the state-of-the-art on a subset of them. Strong points: + New method that generalizes existing methods Weak Points: - Paper should be made more accessible, especially pages 10-11 - Should include more data sets for graph classification experiments, e.g., larger data sets such as REDDIT-* - Paper does not include proofs, should be included in the appendix - Review of literature could be extended Some Remarks: * Section 1: The reference Feragen et al., 2013 is not adequate for kernels based on walks. * Section 3 is rather lengthy. I wonder if its contents are really needed in the following. * Section 6.5, 2nd paragraph: The sentence is difficult to understand. Moreover, the reference (Kriege et al., 2016) appears not to be adequate: The vectors obtained from one-hot encodings are summed and concatenated, which is different from the approach cited. This step should be clarified. * unify first names in references (Marion Neumann vs. R. Kondor) * P. 5 (bottom) broken reference", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We tried to fix everything that you mention . In particular : we have added an appendix that contains all the proofs ; added one more reference for kernels based on counting walks ; rewrote the paragraph you mention in the `` Experiments '' section ; generally extended and cleaned up the experimental results . We appreciate your comment about Section 3 . The reason that we structured the paper as we did was to emphasize that we are have a new general architecture for learning from structured ( multiscale ) objects and that graphs are just a special case . In fact , this work started out significantly more abstract and general , revolving around ideas from representation theory , and we were pretty happy that by reformulating it in the compositional framework we could condense it to something that can be described in 15 pages and ultimately reduces to just tensor products and contractions . It seems like maybe we are not doing a very good job of conveying the generality and power of the approach , though , because all three reviews complain about `` why do n't you just get down to describe your graph algorithm ? `` . Pages 10-11 may be dense , but the truth is that , at the end of the day , the tensor operations that they describe are mercifully straightforward , almost trivial . Having said this , we could not find any existing deep learning software that implements the type of contractions that we need , so we had to write our own library . The library has now been released , but we can not include a link here because it would break the anonymity of the submission . At the time of the submission the library could only use CPUs , that 's why the datasets are relatively small . Since then , we have extended the library to be able to use GPU 's so we now have the capability of running larger experiments and we will definitely try REDDIT . Thanks for the suggestion ."}], "0": {"review_id": "S1TgE7WR--0", "review_text": "Thank you for your contribution to ICLR. The paper covers a very interesting topic and presents some though-provoking ideas. The paper introduces \"covariant compositional networks\" with the purpose of learning graph representations. An example application also covered in the experimental section is graph classification. Given a finite set S, a compositional network is simply a partially ordered set P where each element of P is a subset of S and where P contains all sets of cardinality 1 and the set S itself. Unfortunately, the presentation of the approach is extremely verbose and introduces old concepts (e.g., partially ordered set) under new names. The basic idea (which is not new) of this work is that we need to impose some sort of hierarchical order on the nodes of the graph so as to learn hierarchical feature representations. Moreover, the hierarchical order of the nodes should be invariant to valid permutations of the nodes, that is, two isomorphic graphs should have the same hierarchical order on their nodes and the same feature representations. Since this is the case for graph embedding methods that collect feature representations from their neighbors in the graph (and where the feature aggregation functions are symmetric) it makes sense that \"compositional networks\" generalize graph convolutional networks (and the more general message passing neural networks framework). The most challenging problem, however, namely the problem of finding a concrete and suitable permutation invariant hierarchical decomposition of the nodes plus some aggregation/pooling functions to compute the feature representations is not addressed in sufficient detail. The paper spends a lot of time on some theoretical definitions and (trivial) proofs but then fails to make the connection to an approach that works in practice. The description of the experiments and which compositional network is chosen and how it is chosen seems to be missing. The only part hinting at the model that was actually used in the experiments is the second paragraph of the section 'Experimental Setup', consisting of one long sentence that is incomprehensible to me. Instead of spending a lot of effort on the definitions and (somewhat trivial) propositions in the first half of the paper, the authors should spend much more time on detailing the experiments and the actual model that they used. In an effort to make the framework as general as possible, you ended up making the paper highly verbose and difficult to follow. Please address the following points or clarify in your rebuttal if I misunderstood something: - what precisely is the novel contribution of your work (it cannot be \"compositional networks\" and the propositions concerning those because these are just old concepts under new names)? - explain precisely (and/or more directly/less convoluted) how your model used in the experiments looks like; why do you think it is better than the other methods? - given that compositional network is a very general concept (partially ordered set imposed on subsets of the graph vertices), what is the principled set of steps one has to follow to arrive at such a compositional network tailored to a particular graph collection? isn't (or shouldn't) that be the contribution of this work? Am I missing something? In general, you should write the paper much more to the point and leave out unnecessary math (or move to an appendix). The paper is currently highly inaccessible.", "rating": "5: Marginally below acceptance threshold", "reply_text": "The reviewer 's comments would be valid if our method really was based on imposing a partial order on the vertices of the input graph ( i.e. , if we were turning it into a DAG ) . However this is not what we do . Our algorithm operates with two separate graphs : the original graph G , and the composition scheme M constructed from G. M is indeed a DAG ( i.e. , it defines a partial order ) , but G is whatever the input is , so there is no need to impose `` some sort of hierarchical order '' on its nodes that the reviewer is missing from the presentation . The nodes of M correspond to * subsets * of the nodes of G , specifically neighborhoods of increasing radii , which naturally form a partial order by inclusion . This is explicitly stated in the paper in multiple places , eg. , `` In composition networks for graphs , the atoms will usually be the vertices , and the P_i parts will correspond to clusters of nodes or neighborhoods of different radii '' ( p.4 ) . The construction of M is given explicitly on in points M1-M3 on page 5 . Furthermore , Figure 5 shows how the neighborhoods are nested in the case of a simple input graph . The reason for all the definitions in Sections 3 and 4 is that we need to define how M is constructed and what covariance conditions the corresponding activations must satisfy . Incidentally , this is also the main novelty of the paper . If the reviewer did n't understand these sections , it is not surprising that the `` Experiments '' section seems seems hazy and the tensor contractions in Section 5 just seem like unnecessary fluff . Given the above misunderstanding it is also not surprising that the reviewer thinks that the paper is not very novel . The word `` compositional network '' may have been used before in different contexts , but the general way to construct a DAG from a structured input object , and the invariance properties discussed in Section 4 have not been discussed before in the literature . As we explain , some existing networks are special cases , but the general framework is entirely novel . By breaking down our presentation into a sequence of fairly precise definitions and propositions , and drawing figures such as Figure 2 , which depicts the composition scheme , and Figure 5 , which depicts the neighborhoods in G that correspond to each of the nodes of the composition scheme , we tried to make the notion of composition scheme as clear as possible . We are disappointed that the message still did n't get through , and we would welcome suggestions on how to better convey what is really the main message and main novelty in the paper . ( Maybe by a figure with the original graph and the composition scheme side by side ? ) We would appreciate it if the reviewer revised his evaluation in light of the above clarification and would very much welcome suggestions on how to change the presentation so as to avoid other leaders falling prey to the same misunderstanding . However , we can vouch that the math is not unnecessary : without it , this construction would simply not work ."}, "1": {"review_id": "S1TgE7WR--1", "review_text": "The paper presents a generalized architecture for representing generic compositional objects, such as graphs, which is called covariant compositional networks (CCN). Although, the paper is well structured and quite well written, its dense information and its long size made it hard to follow in depth. Some parts of the paper should have been moved to appendix. As far as the evaluation, the proposed method seems to outperform in a number of tasks/datasets compare to SoA methods, but it is not really clear whether the out-performance is of statistical significance. Moreover, in Table 1, training performances shouldn't be shown, while in Table 3, RMSE it would be nice to be shown in order to gain a complete image of the actual performance.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your comments . The paper is longer than usual because rather than just proposing a tweak on some existing algorithm , it derives a general framework for dealing with the issue of covariance in compositional-type neural architectures . We could n't figure out how to present all this in less than 15 pages without making the paper unreadably dense . Our results on HCEP are quite spectacular . While we did n't perform formal tests of statistical significance , it is quite clear that CCN far outperforms the other methods . For the MUTAG , etc . benchmarks it is harder to show statistical significance , simply because these datasets are small and multiple algorithms are neck and neck . For QM9 , although PSCN is admittedly close , we consistently beat it ( as well as the other two competitors ) on all 13 regression tasks in both MAE and RMSE , which again gives us confidence that the results are not just a statistical fluke . Thank you for your suggestion to include RMSE , we added a separate table to show that . Incidentally , the experiments required writing our own custom deep learning library in C++ ( which now can also use GPUs ) . The code has been released on GitHub , unfortunately we can not provide a link at this point because it would break the anonymity of the submission . Notwithstanding the above , we feel that sate-of-the-art experimental results are just one part of the paper 's contribution . The new conceptual framework of compositional networks and our mathematical results on how to make them covariant is even more important . We would appreciate it if the review was revised to reflect on the main body of the paper as well , not just the `` Experiments '' section ."}, "2": {"review_id": "S1TgE7WR--2", "review_text": "The paper introduces a formalism to perform graph classification and regression, so-called \"covariant compositional networks\", which can be seen as a generalization of the recently proposed neural message passing algorithms. The authors argue that neural message passing algorithms are not able to sufficiently capture the structure of the graph since their neighborhood aggregation function is permutation invariant. They argue that relying on permutation invariance will led to some loss of structural information. In order to address this issue they introduce covariant comp-nets, which are a hierarchical decompositon of the set of vertices, and propose corresponding aggregation rules based on tensor arithmetic. Their new method is evaluated on several graph regression and classification benchmark data sets, showing that it improves the state-of-the-art on a subset of them. Strong points: + New method that generalizes existing methods Weak Points: - Paper should be made more accessible, especially pages 10-11 - Should include more data sets for graph classification experiments, e.g., larger data sets such as REDDIT-* - Paper does not include proofs, should be included in the appendix - Review of literature could be extended Some Remarks: * Section 1: The reference Feragen et al., 2013 is not adequate for kernels based on walks. * Section 3 is rather lengthy. I wonder if its contents are really needed in the following. * Section 6.5, 2nd paragraph: The sentence is difficult to understand. Moreover, the reference (Kriege et al., 2016) appears not to be adequate: The vectors obtained from one-hot encodings are summed and concatenated, which is different from the approach cited. This step should be clarified. * unify first names in references (Marion Neumann vs. R. Kondor) * P. 5 (bottom) broken reference", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We tried to fix everything that you mention . In particular : we have added an appendix that contains all the proofs ; added one more reference for kernels based on counting walks ; rewrote the paragraph you mention in the `` Experiments '' section ; generally extended and cleaned up the experimental results . We appreciate your comment about Section 3 . The reason that we structured the paper as we did was to emphasize that we are have a new general architecture for learning from structured ( multiscale ) objects and that graphs are just a special case . In fact , this work started out significantly more abstract and general , revolving around ideas from representation theory , and we were pretty happy that by reformulating it in the compositional framework we could condense it to something that can be described in 15 pages and ultimately reduces to just tensor products and contractions . It seems like maybe we are not doing a very good job of conveying the generality and power of the approach , though , because all three reviews complain about `` why do n't you just get down to describe your graph algorithm ? `` . Pages 10-11 may be dense , but the truth is that , at the end of the day , the tensor operations that they describe are mercifully straightforward , almost trivial . Having said this , we could not find any existing deep learning software that implements the type of contractions that we need , so we had to write our own library . The library has now been released , but we can not include a link here because it would break the anonymity of the submission . At the time of the submission the library could only use CPUs , that 's why the datasets are relatively small . Since then , we have extended the library to be able to use GPU 's so we now have the capability of running larger experiments and we will definitely try REDDIT . Thanks for the suggestion ."}}