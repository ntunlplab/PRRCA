{"year": "2020", "forum": "S1lF8xHYwS", "title": "Unsupervised Domain Adaptation through Self-Supervision", "decision": "Reject", "meta_review": "Thanks for your detailed replies to the reviewers, which helped us a lot to clarify several issues.\nAlthough the paper discusses an interesting topic and contains potentially interesting idea, its novelty is limited.\nGiven the high competition of ICLR2020, this paper is still below the bar unfortunately.", "reviews": [{"review_id": "S1lF8xHYwS-0", "review_text": "This paper presents a novel unsupervised domain adaptation framework for neural networks. Similarly to existing approaches, it performs adaptation by aligning representations of the source and the target domains. The main difference is that this alignment is achieved not through explicitly minimizing some distribution discrepancy (this usually leads to challenging minimax optimization problems). Instead, the authors propose to use a battery of auxiliary self-supervised learning (SSL) tasks for both domains simultaneously. Each task is meant to align the source and the target representations along a direction of variation relevant to that task. Assuming that the battery is diverse enough, optimizing the representation for all the tasks leads to matching of the distributions. Pros: + The paper is well-written and easy to read. + I like the simplicity of the idea and the fact that it achieves competitive performance without any adversarial learning (which may be very tricky to deal with). + The paper presents a reasonable procedure for hyper-parameter tuning and early stopping which seems to work well in practice. Cons: - The paper is purely practical with no theory backing the approach. As a result, the discussion of guarantees and limitations is quite brief. - It\u2019s unclear how easy it is to come up with a reasonable set of SSL tasks for a particular pair of domains. It seems that it may become a serious problem when the method is applied to something other than benchmarks. Table 2 reveals that there is no consistent improvement over the existing approaches which suggests that the chosen battery of SSL tasks is not universal (as the authors themselves admit). On a related note, it\u2019s a bit disappointing that the authors mention SVHN results as a failure case but never provide a way to address the issue. - It would be nice to some results for the Office dataset for completeness. The authors could use a pre-trained network as a starting points just like it\u2019s done in other papers. According to the last paragraph of Section 6 this experiment should be feasible. Notes/questions: * Table 2, last column: The performance of DIRT-T seems to be better than that of the proposed method and yet the latter is highlighted and not the former. Overall, I think it\u2019s a good paper presenting a thought-provoking idea. In my opinion, the weakest point of the work is the lack of any (neither principled nor practical) guidance as to how to choose the set of self-supervised tasks. Despite this I feel that this submission should be accepted but at the same time I\u2019m curious to see what the authors have to say regarding the concerns I raised in my review.", "rating": "6: Weak Accept", "reply_text": "Thank you and we are happy you found our paper thought-provoking . Here we address the cons you wrote : 1 . It is true that we have no theory backing our approach . On the other hand , it is rarely to see any deep learning paper with theory adequate enough to give \u201c guarantees \u201d for datasets we actually care about . 2.This connects well with the comment at the end of the review , asking us for \u201c guidance as to how to choose the set of self-supervised tasks. \u201d We have in fact given some practical guidance in the paper , which we summarize below as two necessary conditions : - The self-supervised task is well defined and nontrivial on both domains . This rules out the case of rotation prediction on SVHN , since as we explain in the paper , \u201c the rotation head learns to look at the periphery and cheat \u201d . - \u201c The labels created by self-supervision should not require capturing information on the very factors where the domains are meaninglessly different. \u201d as said and explained in section 3 . This is rules out tasks such as colorization and autoencoder , for which it is important to learn the low-level details of the image . These two conditions are easy to reason about in practice . If the \u201c battery of self-supervised tasks \u201d satisfy them , there should be notable improvement on top of the source only baseline as we observe empirically , but there won \u2019 t be a guarantee . In addition , we would like this paper to add to the toolbox of available domain adaptation methods instead of becoming the only tool . When a good self-supervised task satisfying the two conditions can not be found ( SVHN ) , previous methods have provided different tools to use . When a good self-supervised task naturally exists , our method provides a simple and effective choice . In the end , this is a valuable question from the reviewer and we plan to be more explicit about those conditions in the next revision . 3.Please see results on Office-31 in our reply to R3 . Your notes / questions : Thank you very much for pointing out our error with the highlighting . This is an honest typo . In the latest revision , we have improved our results to match that of DIRT-T ; the modification we made for the improved results , as well as the original results , can be found in the last paragraph of Appendix B ."}, {"review_id": "S1lF8xHYwS-1", "review_text": "This paper describes an approach to domain adaptation that uses self-supervised losses to encourage source/target domain alignment for unsupervised domain adaptation. The authors propose to use four self-supervised tasks (variants of tasks used in the self-supervised representation learning for object recognition literature) that are used with a combined loss including unlabeled source and target training samples. The authors also propose an alignment heuristic for guiding early stopping. Experimental results on a standard battery of domain adaptation problems are given, plus some intriguing baseline results for semantic segmentation. The paper is written very well and the technical development and motivations for each decision are well discussed and argued. 1. The experimental evaluation is a bit limited as the object recognition datasets are a bit limited. Results on Office or Office-Home would be nice. 2. Using location classification for semantic segmentation seems intuitively to be encouraging the network to learn coarse spatial priors (which should be invariant across the two domains). Have you looked at how alignment is actually happening? More qualitative analysis in this direction would be useful to appreciate the proposed approach. 3. Related to the previous point, it would be interesting to see how semgmentations in the unsupervised domain gradually change and improve with increasing alignment. In summary: the ideas are simple, intuitive, and well-explained -- I think the results reported would be easy to reproduce with minimal head scratching. The experiments are interesting and not overstated. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your thoughtful review . We have added qualitative comparisons in Appendix G of our latest revision ( page 16 ) ."}, {"review_id": "S1lF8xHYwS-2", "review_text": "This paper introduces an unsupervised domain adaptation method that uses self-supervised tasks to bring the two different domains closer together. It runs experiments on some classic benchmarks. My score for this paper is weakly rejected because (1) the concept of self-supervision is not first proposed by this paper. The proposed method is not novel. It introduces three simple self-supervision tasks: flip, rotation and location, and the performance is not better than previous results such as DIRT-T; (2) there are 7 benchmarks in Table2, but only 2 of 7 has result on R+L+F. In the paper, it mentioned because the result is not better, but the author should still provide them. (3) it emphasizes the contribution of encouraging more study of self-supervision for unsupervised domain adaptation. It doesn\u2019t provide any way for how to design self-supervision task or whether more tasks is better. I think it is an interesting paper, but not enough as a conference paper, maybe a workshop paper. (4) there are some classic unsupervised domain adaption benchmarks like Office Dataset, and Bing-Caltech dataset, why not run the method on them? (5) In ICCV 2019, there is a paper \"S4L: Self-Supervised Semi-Supervised Learning\". The proposed method is almost same. I think the difference is this paper changes the setting and considers the unsupervised data as target domain and supervised data as source domain. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your time giving us feedback . Here we answer your numbered concerns . 1. \u201c The concept of self-supervision is not first proposed by this paper. \u201d Since being proposed in the 1990s , self-supervised learning has become a wide and vibrant field of inquiry , with hundreds , if not thousands of papers published in respected venues . So , we are perplexed by the statement : is this arguing that all these papers were published in error ? \u201c The proposed method is not novel. \u201d Such statements are unhelpful without references to prior work . We have stated in the introduction what we perceive to be the novelties of our method . Please provide references to previously published papers that render our novelties invalid . \u201c Performance is not better than previous results such as DIRT-T. \u201d Our results are shown in Table 2 , and many of them are better than DIRT-T. Our method is also simpler and derived from a different perspective . 2.Below are the requested results for R+L+F : \u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014- Source MNIST MNIST SVHN MNIST MNIST Target MNIST-M SVHN MNIST USPS USPS \u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014- Accuracy ( % ) 98.7 63.2 85.7 95.8 87.0 \u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014- There is not much difference between these numbers and the ones for R only . 3. \u201c [ The authors ] do not provide any way for how to design self-supervision task \u201d . Please see Section 3 titled \u201c designing self-supervised tasks for adaptation \u201d . 4.Please see results on Office-31 in our reply to R3 . 5.First , please note that ICCV 2019 papers are considered concurrent work , not prior work , to ICLR 2020 ( ICCV \u2019 19 happened in November , whereas deadline for ICLR was in September ) . Second , S4L , which is designed for semi-supervised learning , differs from ours both algorithmically and conceptually . We have already discussed this in the related work section in the context of semi-supervised learning methods , but to make our point clearer , here are the results for our implementation of the algorithm described in their equation ( 1 ) and ( 2 ) on MNIST - > MNIST-M , where improving upon the source only ( no adaptation ) baseline should have been very easy : \u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014 | Accuracy ( % ) \u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014 Source only | 44.9 S4L method | 56.6 Our method | 98.9 \u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014 The S4L result is barely better than source only , and qualitatively different from ours i.e.the difference should not come from merely implementation details . The most important difference between their algorithm and ours is that they train the supervised task on labeled data , and self-supervised task on unlabeled data , while we train the self-supervised task ( s ) simultaneously on both domains ( labeled and unlabeled ) . Conceptually , training the self-supervised task on both domains is critical for alignment , which is the main objective for adaptation . Because for semi-supervised learning , the labeled and unlabeled data come from the same domain , methods for semi-supervised learning e.g.S4L do not need to consider the alignment problem . These comments are not intended to criticize S4L , as it is solving a different problem . In fact , theoretical analysis for semi-supervised learning [ Cohen , Cozman ] [ Ghifary et al ] suggests that training the self-supervised task on both domains is not helpful for semi-supervised learning ; it is interesting to see how this picture is different for domain adaptation . \u201c I think it is an interesting paper , but not enough as a conference paper , maybe a workshop paper. \u201d We are happy you found the paper interesting . We do ask you to please reconsider your recommendation in light of the arguments presented above . Also , as similar works using self-supervision as a tool , e.g.S4L , were published at respectable conferences instead of workshops , it seems reasonable to argue that this work , too , deserves to be accepted to ICLR . References : Cohen , I. , Cozman , F.G. : Risks of semi-supervised learning : how unlabeled data can degrade performance of generative classifiers . In : Semi-Supervised Learning . MIT Press ( 2006 ) Muhammad Ghifary , W Bastiaan Kleijn , Mengjie Zhang , David Balduzzi , and Wen Li . Deep reconstruction-classification networks for unsupervised domain adaptation . In European Conference on Computer Vision , pp . 597\u2013613.Springer , 2016 ."}], "0": {"review_id": "S1lF8xHYwS-0", "review_text": "This paper presents a novel unsupervised domain adaptation framework for neural networks. Similarly to existing approaches, it performs adaptation by aligning representations of the source and the target domains. The main difference is that this alignment is achieved not through explicitly minimizing some distribution discrepancy (this usually leads to challenging minimax optimization problems). Instead, the authors propose to use a battery of auxiliary self-supervised learning (SSL) tasks for both domains simultaneously. Each task is meant to align the source and the target representations along a direction of variation relevant to that task. Assuming that the battery is diverse enough, optimizing the representation for all the tasks leads to matching of the distributions. Pros: + The paper is well-written and easy to read. + I like the simplicity of the idea and the fact that it achieves competitive performance without any adversarial learning (which may be very tricky to deal with). + The paper presents a reasonable procedure for hyper-parameter tuning and early stopping which seems to work well in practice. Cons: - The paper is purely practical with no theory backing the approach. As a result, the discussion of guarantees and limitations is quite brief. - It\u2019s unclear how easy it is to come up with a reasonable set of SSL tasks for a particular pair of domains. It seems that it may become a serious problem when the method is applied to something other than benchmarks. Table 2 reveals that there is no consistent improvement over the existing approaches which suggests that the chosen battery of SSL tasks is not universal (as the authors themselves admit). On a related note, it\u2019s a bit disappointing that the authors mention SVHN results as a failure case but never provide a way to address the issue. - It would be nice to some results for the Office dataset for completeness. The authors could use a pre-trained network as a starting points just like it\u2019s done in other papers. According to the last paragraph of Section 6 this experiment should be feasible. Notes/questions: * Table 2, last column: The performance of DIRT-T seems to be better than that of the proposed method and yet the latter is highlighted and not the former. Overall, I think it\u2019s a good paper presenting a thought-provoking idea. In my opinion, the weakest point of the work is the lack of any (neither principled nor practical) guidance as to how to choose the set of self-supervised tasks. Despite this I feel that this submission should be accepted but at the same time I\u2019m curious to see what the authors have to say regarding the concerns I raised in my review.", "rating": "6: Weak Accept", "reply_text": "Thank you and we are happy you found our paper thought-provoking . Here we address the cons you wrote : 1 . It is true that we have no theory backing our approach . On the other hand , it is rarely to see any deep learning paper with theory adequate enough to give \u201c guarantees \u201d for datasets we actually care about . 2.This connects well with the comment at the end of the review , asking us for \u201c guidance as to how to choose the set of self-supervised tasks. \u201d We have in fact given some practical guidance in the paper , which we summarize below as two necessary conditions : - The self-supervised task is well defined and nontrivial on both domains . This rules out the case of rotation prediction on SVHN , since as we explain in the paper , \u201c the rotation head learns to look at the periphery and cheat \u201d . - \u201c The labels created by self-supervision should not require capturing information on the very factors where the domains are meaninglessly different. \u201d as said and explained in section 3 . This is rules out tasks such as colorization and autoencoder , for which it is important to learn the low-level details of the image . These two conditions are easy to reason about in practice . If the \u201c battery of self-supervised tasks \u201d satisfy them , there should be notable improvement on top of the source only baseline as we observe empirically , but there won \u2019 t be a guarantee . In addition , we would like this paper to add to the toolbox of available domain adaptation methods instead of becoming the only tool . When a good self-supervised task satisfying the two conditions can not be found ( SVHN ) , previous methods have provided different tools to use . When a good self-supervised task naturally exists , our method provides a simple and effective choice . In the end , this is a valuable question from the reviewer and we plan to be more explicit about those conditions in the next revision . 3.Please see results on Office-31 in our reply to R3 . Your notes / questions : Thank you very much for pointing out our error with the highlighting . This is an honest typo . In the latest revision , we have improved our results to match that of DIRT-T ; the modification we made for the improved results , as well as the original results , can be found in the last paragraph of Appendix B ."}, "1": {"review_id": "S1lF8xHYwS-1", "review_text": "This paper describes an approach to domain adaptation that uses self-supervised losses to encourage source/target domain alignment for unsupervised domain adaptation. The authors propose to use four self-supervised tasks (variants of tasks used in the self-supervised representation learning for object recognition literature) that are used with a combined loss including unlabeled source and target training samples. The authors also propose an alignment heuristic for guiding early stopping. Experimental results on a standard battery of domain adaptation problems are given, plus some intriguing baseline results for semantic segmentation. The paper is written very well and the technical development and motivations for each decision are well discussed and argued. 1. The experimental evaluation is a bit limited as the object recognition datasets are a bit limited. Results on Office or Office-Home would be nice. 2. Using location classification for semantic segmentation seems intuitively to be encouraging the network to learn coarse spatial priors (which should be invariant across the two domains). Have you looked at how alignment is actually happening? More qualitative analysis in this direction would be useful to appreciate the proposed approach. 3. Related to the previous point, it would be interesting to see how semgmentations in the unsupervised domain gradually change and improve with increasing alignment. In summary: the ideas are simple, intuitive, and well-explained -- I think the results reported would be easy to reproduce with minimal head scratching. The experiments are interesting and not overstated. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your thoughtful review . We have added qualitative comparisons in Appendix G of our latest revision ( page 16 ) ."}, "2": {"review_id": "S1lF8xHYwS-2", "review_text": "This paper introduces an unsupervised domain adaptation method that uses self-supervised tasks to bring the two different domains closer together. It runs experiments on some classic benchmarks. My score for this paper is weakly rejected because (1) the concept of self-supervision is not first proposed by this paper. The proposed method is not novel. It introduces three simple self-supervision tasks: flip, rotation and location, and the performance is not better than previous results such as DIRT-T; (2) there are 7 benchmarks in Table2, but only 2 of 7 has result on R+L+F. In the paper, it mentioned because the result is not better, but the author should still provide them. (3) it emphasizes the contribution of encouraging more study of self-supervision for unsupervised domain adaptation. It doesn\u2019t provide any way for how to design self-supervision task or whether more tasks is better. I think it is an interesting paper, but not enough as a conference paper, maybe a workshop paper. (4) there are some classic unsupervised domain adaption benchmarks like Office Dataset, and Bing-Caltech dataset, why not run the method on them? (5) In ICCV 2019, there is a paper \"S4L: Self-Supervised Semi-Supervised Learning\". The proposed method is almost same. I think the difference is this paper changes the setting and considers the unsupervised data as target domain and supervised data as source domain. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your time giving us feedback . Here we answer your numbered concerns . 1. \u201c The concept of self-supervision is not first proposed by this paper. \u201d Since being proposed in the 1990s , self-supervised learning has become a wide and vibrant field of inquiry , with hundreds , if not thousands of papers published in respected venues . So , we are perplexed by the statement : is this arguing that all these papers were published in error ? \u201c The proposed method is not novel. \u201d Such statements are unhelpful without references to prior work . We have stated in the introduction what we perceive to be the novelties of our method . Please provide references to previously published papers that render our novelties invalid . \u201c Performance is not better than previous results such as DIRT-T. \u201d Our results are shown in Table 2 , and many of them are better than DIRT-T. Our method is also simpler and derived from a different perspective . 2.Below are the requested results for R+L+F : \u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014- Source MNIST MNIST SVHN MNIST MNIST Target MNIST-M SVHN MNIST USPS USPS \u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014- Accuracy ( % ) 98.7 63.2 85.7 95.8 87.0 \u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014- There is not much difference between these numbers and the ones for R only . 3. \u201c [ The authors ] do not provide any way for how to design self-supervision task \u201d . Please see Section 3 titled \u201c designing self-supervised tasks for adaptation \u201d . 4.Please see results on Office-31 in our reply to R3 . 5.First , please note that ICCV 2019 papers are considered concurrent work , not prior work , to ICLR 2020 ( ICCV \u2019 19 happened in November , whereas deadline for ICLR was in September ) . Second , S4L , which is designed for semi-supervised learning , differs from ours both algorithmically and conceptually . We have already discussed this in the related work section in the context of semi-supervised learning methods , but to make our point clearer , here are the results for our implementation of the algorithm described in their equation ( 1 ) and ( 2 ) on MNIST - > MNIST-M , where improving upon the source only ( no adaptation ) baseline should have been very easy : \u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014 | Accuracy ( % ) \u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014 Source only | 44.9 S4L method | 56.6 Our method | 98.9 \u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014\u2014-\u2014\u2014\u2014 The S4L result is barely better than source only , and qualitatively different from ours i.e.the difference should not come from merely implementation details . The most important difference between their algorithm and ours is that they train the supervised task on labeled data , and self-supervised task on unlabeled data , while we train the self-supervised task ( s ) simultaneously on both domains ( labeled and unlabeled ) . Conceptually , training the self-supervised task on both domains is critical for alignment , which is the main objective for adaptation . Because for semi-supervised learning , the labeled and unlabeled data come from the same domain , methods for semi-supervised learning e.g.S4L do not need to consider the alignment problem . These comments are not intended to criticize S4L , as it is solving a different problem . In fact , theoretical analysis for semi-supervised learning [ Cohen , Cozman ] [ Ghifary et al ] suggests that training the self-supervised task on both domains is not helpful for semi-supervised learning ; it is interesting to see how this picture is different for domain adaptation . \u201c I think it is an interesting paper , but not enough as a conference paper , maybe a workshop paper. \u201d We are happy you found the paper interesting . We do ask you to please reconsider your recommendation in light of the arguments presented above . Also , as similar works using self-supervision as a tool , e.g.S4L , were published at respectable conferences instead of workshops , it seems reasonable to argue that this work , too , deserves to be accepted to ICLR . References : Cohen , I. , Cozman , F.G. : Risks of semi-supervised learning : how unlabeled data can degrade performance of generative classifiers . In : Semi-Supervised Learning . MIT Press ( 2006 ) Muhammad Ghifary , W Bastiaan Kleijn , Mengjie Zhang , David Balduzzi , and Wen Li . Deep reconstruction-classification networks for unsupervised domain adaptation . In European Conference on Computer Vision , pp . 597\u2013613.Springer , 2016 ."}}