{"year": "2017", "forum": "Hk8TGSKlg", "title": "Reasoning with Memory Augmented Neural Networks for Language Comprehension", "decision": "Accept (Poster)", "meta_review": "This paper proposes a memory-enhanced RNN in the vein of NTM, and a novel training method for this architecture of cloze-style QA. The results seem convincing, and the training method is decently novel according to reviewers, although the evaluation seemed somewhat incomplete according to reviewers and my own reading. For instance, it is questionable whether or not the advertised human performance on CNN/DM is accurate (based on 100 samples from a 300k+ dataset), so I'm not sure this warrants not evaluating or reporting performance on it. Overall this looks like an acceptable paper, although there is room for improvement.", "reviews": [{"review_id": "Hk8TGSKlg-0", "review_text": "First I would like to apologize for the delay in reviewing. Summary : this work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art. The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one. In order to get a better sense of the reason for improvement it would be interesting to have a complexity and/or a time analysis of the algorithm. I might be mistaken but I don't see you reporting anything on the actual number of loops necessary in the reported experiments. The dataset description in section 2.2, should be moved to section 4 where the other datasets are described.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful review . > In order to get a better sense of the reason for improvement it would be interesting to have a complexity and/or a time analysis of the algorithm . I might be mistaken but I do n't see you reporting anything on the actual number of loops necessary in the reported experiments . The computational complexity of the model depends on length of the document and query , and the parameter T which is the maximum number of allowed steps . Our ablation study on parameter T ( Table 1 and 2 along with the discussion ) shows that the larger value of T , the longer the training and the more likely the models overfit . Therefore T is usually constant and defines the actual number of loops . It is orders of magnitude smaller to compare with the length of the document which is ~500 . The main bottleneck is the memory initialization with bi-directional LSTM because it has to sequentially read each document word . The memory read and write is as fast as NSE uses the dot product for content similarity . It is even faster than neural turing machines since NSE calculates the memory key vector z once and uses it for both read and write ."}, {"review_id": "Hk8TGSKlg-1", "review_text": "Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good. For the paper, I have some comments: 1. Actually the model in the paper is not single model, it proposed two models. One consists of \"reading\", \"writing\", \"adaptive computation\" and \" Answer module 2\", the other one is \"reading\", \"composing\", \"writing\", \"gate querying\" and \"Answer module 1\". Based on the method section and the experiment, it seems the \"adaptive computation\" model is simpler and performs better. And without two time memory update in single iteration and composing module, the model is similar to neural turing machine. 2. What is the MLP setting in the composing module? 3. This paper tested different size of hidden state:[256, 368, 436, 512], I do not find any relation between those numbers, how could you find 436? Is there any tricks helping you find those numbers? 4. It needs more ablation study about using different T such as T=1,2.. 5. According to my understanding, for the adaptive computation, it would stop when the P_T <0. So what is the distribution of T in the testing data?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the thoughtful review . 1.Actually the model in the paper is not single model , it proposed two models . One consists of `` reading '' , `` writing '' , `` adaptive computation '' and `` Answer module 2 '' , the other one is `` reading '' , `` composing '' , `` writing '' , `` gate querying '' and `` Answer module 1 '' . Based on the method section and the experiment , it seems the `` adaptive computation '' model is simpler and performs better . And without two time memory update in single iteration and composing module , the model is similar to neural turing machine . Comparing NSE to NTM : NTM has a single centralized controller , which is usually an MLP or RNN while NSE takes a modular approach . The main controller in NSE is decomposed into three separate modules , each of which performs for read , compose or write operation . In NSE , the compose module is introduced in addition to the standard memory update operations ( i.e.read-write ) in order to process the memory entries and input information . The main advantage of NSE is in its memory update . The NTM controller does not have mechanism to avoid information collision in the memory . Particularly the NTM controller emits two separate set of access weights ( i.e.read weight and erase and write weights ) that do not explicitly encode the knowledge about where information is read from and written to . Moreover NTM has a fixed-size memory with no memory allocation or de-allocation protocol . Therefore unless the controller is intelligent enough to track the previous read/write information , which is hard for an RNN when processing long sequences , the memory content is overlapped and information is overwritten throughout different time scales . We think that this is a potential reason that makes NTM hard to train and makes the training not stable . We also note that the effectiveness of the location based addressing introduced in NTM is unclear . In NSE , we introduce a systematic memory update approach based on the soft attention mechanism . NSE writes new information to the most recently read memory locations . This is accomplished by sharing the same memory key vector between the read and write modules . The NSE memory update is scalable and potentially more robust to train . NSE is provided with a variable sized memory and thus unlike NTM , the size of the NSE memory is more relaxed . The novel memory update mechanism and the variable sized memory together prevent NSE from the information collision issue and avoid the need of the memory allocation and de-allocation protocols . Each memory location of the NSE memory stores a token representation in input sequence during encoding . This provides NSE with an anytime-access to the entire input sequence including the tokens from the future time scales , which is not permitted in NTM , RNN and attention-based encoders . 2.What is the MLP setting in the composing module ? We used a single layer MLP with 436 hidden units and ReLU activation for the composition module . 3.This paper tested different size of hidden state : [ 256 , 368 , 436 , 512 ] , I do not find any relation between those numbers , how could you find 436 ? Is there any tricks helping you find those numbers ? It is usually not trivial to find those hyper-parameter settings as the search space is huge . Grid search is preferred for small tasks that we can afford multiple runs . For the large scale tasks like ones addressed in this paper , we turn to a random search or rely on previous experiences . Those hidden state sizes mainly came from the previous work , except 436 . 436 was selected by a random search and it worked better than 256 , 368 and 512 that were taken from the related work . 4.It needs more ablation study about using different T such as T=1,2 .. 5 . According to my understanding , for the adaptive computation , it would stop when the P_T < 0 . So what is the distribution of T in the testing data ? For the given time and resource , we were able to conduct an additional experiment with 12 different runs with T=1,2 . In the updated manuscript we also discussed about using different T , maximum number of permitted steps . In general given a small number of permitted steps , we found that our models less likely overfit although the final performance is not high . As the number of permitted steps increases both dev and test accuracy improves yielding an overall higher performance . However , this holds up to a certain point . When the permitted steps are large , we no longer observe a significant performance improvement in terms of testing accuracy . For example , NSE Query Gating model with T=15 achieved 79.2 % dev accuracy and 71.4 % test accuracy on CBT-NE task , showing the highest accuracy on the dev set yet massively overfitting on the test set . Furthermore it becomes expensive to train a model with a large number of allowed steps . During the qualitative analysis , we found out that the distribution of T is mostly concentrated at 2 . This is also indicated in the query regression visualizations in Appendix A . As noted in the manuscript , our adaptive computation model adjusts T for a particular task ( i.e document and question pair ) . More recently and following our work , the adaptive computation framework was successfully adapted for image classification and object recognition tasks [ 1 ] . Particularly the adaptive computation framework was deployed within residual networks to decide when to halt the computation . Finally for the CNN news task , we ran our NSE adaptive model without any hyper-parameter tuning . It achieved ~74 % accuracy ( 74 % ~75 % ) on the CNN test set . The result is not included in our paper as we speculate that the results could be improved further if we have more time . We think that ~74 % accuracy is reasonable considering the fact that the human performance on the task is ~75 % . Ref : 1.Figurnov , Michael , et al . `` Spatially Adaptive Computation Time for Residual Networks . '' arXiv preprint arXiv:1612.02297 ( 7 Dec 2016 ) ."}, {"review_id": "Hk8TGSKlg-2", "review_text": "This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read/compose/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond Cloze-style QA. Another advantage of the proposed model is to learn when to terminate the iteration by the so-called adaptive computation model, such that it avoids the issue of treating the number of iterations as another hyper-parameter, which is a common practice of iterative models/multi-hop reasoning in previous papers. There are a couple places that this paper can improve. First, I would like to see the results from CNN/Daily Mail as well to have a more comprehensive comparison. Secondly, it will be useful to visualize the entire M^q sequence over time t (not just z or the query gating) to help understand better the query regression and if it is human interpretable.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful review and useful suggestions . > First , I would like to see the results from CNN/Daily Mail as well to have a more comprehensive comparison . For the CNN news task , we ran our NSE adaptive model without any hyper-parameter tuning . It achieved ~74 % accuracy ( 74 % ~75 % ) on the CNN test set . The result is not included in our paper as we speculate that the results could be improved further if we have more time . We think that ~74 % accuracy is reasonable considering the fact that the human performance on the task is ~75 % . > Secondly , it will be useful to visualize the entire M^q sequence over time t ( not just z or the query gating ) to help understand better the query regression and if it is human interpretable . Since the query regression happens in a continuous vector space it is not trivial to directly visualize it . However , we think that by looking at the z and gating states one could interpret how the query is regressed . As shown in Figure 3 , word information provides a cue how answer is kept . As shown in Figure 3 , content words ( e.g. , \u201c Rabbit \u201d ) are not overwritten throughout the regression loop while non-content words like ' . ' and < BOS > are replaced in the very first step ."}], "0": {"review_id": "Hk8TGSKlg-0", "review_text": "First I would like to apologize for the delay in reviewing. Summary : this work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art. The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one. In order to get a better sense of the reason for improvement it would be interesting to have a complexity and/or a time analysis of the algorithm. I might be mistaken but I don't see you reporting anything on the actual number of loops necessary in the reported experiments. The dataset description in section 2.2, should be moved to section 4 where the other datasets are described.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful review . > In order to get a better sense of the reason for improvement it would be interesting to have a complexity and/or a time analysis of the algorithm . I might be mistaken but I do n't see you reporting anything on the actual number of loops necessary in the reported experiments . The computational complexity of the model depends on length of the document and query , and the parameter T which is the maximum number of allowed steps . Our ablation study on parameter T ( Table 1 and 2 along with the discussion ) shows that the larger value of T , the longer the training and the more likely the models overfit . Therefore T is usually constant and defines the actual number of loops . It is orders of magnitude smaller to compare with the length of the document which is ~500 . The main bottleneck is the memory initialization with bi-directional LSTM because it has to sequentially read each document word . The memory read and write is as fast as NSE uses the dot product for content similarity . It is even faster than neural turing machines since NSE calculates the memory key vector z once and uses it for both read and write ."}, "1": {"review_id": "Hk8TGSKlg-1", "review_text": "Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good. For the paper, I have some comments: 1. Actually the model in the paper is not single model, it proposed two models. One consists of \"reading\", \"writing\", \"adaptive computation\" and \" Answer module 2\", the other one is \"reading\", \"composing\", \"writing\", \"gate querying\" and \"Answer module 1\". Based on the method section and the experiment, it seems the \"adaptive computation\" model is simpler and performs better. And without two time memory update in single iteration and composing module, the model is similar to neural turing machine. 2. What is the MLP setting in the composing module? 3. This paper tested different size of hidden state:[256, 368, 436, 512], I do not find any relation between those numbers, how could you find 436? Is there any tricks helping you find those numbers? 4. It needs more ablation study about using different T such as T=1,2.. 5. According to my understanding, for the adaptive computation, it would stop when the P_T <0. So what is the distribution of T in the testing data?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the thoughtful review . 1.Actually the model in the paper is not single model , it proposed two models . One consists of `` reading '' , `` writing '' , `` adaptive computation '' and `` Answer module 2 '' , the other one is `` reading '' , `` composing '' , `` writing '' , `` gate querying '' and `` Answer module 1 '' . Based on the method section and the experiment , it seems the `` adaptive computation '' model is simpler and performs better . And without two time memory update in single iteration and composing module , the model is similar to neural turing machine . Comparing NSE to NTM : NTM has a single centralized controller , which is usually an MLP or RNN while NSE takes a modular approach . The main controller in NSE is decomposed into three separate modules , each of which performs for read , compose or write operation . In NSE , the compose module is introduced in addition to the standard memory update operations ( i.e.read-write ) in order to process the memory entries and input information . The main advantage of NSE is in its memory update . The NTM controller does not have mechanism to avoid information collision in the memory . Particularly the NTM controller emits two separate set of access weights ( i.e.read weight and erase and write weights ) that do not explicitly encode the knowledge about where information is read from and written to . Moreover NTM has a fixed-size memory with no memory allocation or de-allocation protocol . Therefore unless the controller is intelligent enough to track the previous read/write information , which is hard for an RNN when processing long sequences , the memory content is overlapped and information is overwritten throughout different time scales . We think that this is a potential reason that makes NTM hard to train and makes the training not stable . We also note that the effectiveness of the location based addressing introduced in NTM is unclear . In NSE , we introduce a systematic memory update approach based on the soft attention mechanism . NSE writes new information to the most recently read memory locations . This is accomplished by sharing the same memory key vector between the read and write modules . The NSE memory update is scalable and potentially more robust to train . NSE is provided with a variable sized memory and thus unlike NTM , the size of the NSE memory is more relaxed . The novel memory update mechanism and the variable sized memory together prevent NSE from the information collision issue and avoid the need of the memory allocation and de-allocation protocols . Each memory location of the NSE memory stores a token representation in input sequence during encoding . This provides NSE with an anytime-access to the entire input sequence including the tokens from the future time scales , which is not permitted in NTM , RNN and attention-based encoders . 2.What is the MLP setting in the composing module ? We used a single layer MLP with 436 hidden units and ReLU activation for the composition module . 3.This paper tested different size of hidden state : [ 256 , 368 , 436 , 512 ] , I do not find any relation between those numbers , how could you find 436 ? Is there any tricks helping you find those numbers ? It is usually not trivial to find those hyper-parameter settings as the search space is huge . Grid search is preferred for small tasks that we can afford multiple runs . For the large scale tasks like ones addressed in this paper , we turn to a random search or rely on previous experiences . Those hidden state sizes mainly came from the previous work , except 436 . 436 was selected by a random search and it worked better than 256 , 368 and 512 that were taken from the related work . 4.It needs more ablation study about using different T such as T=1,2 .. 5 . According to my understanding , for the adaptive computation , it would stop when the P_T < 0 . So what is the distribution of T in the testing data ? For the given time and resource , we were able to conduct an additional experiment with 12 different runs with T=1,2 . In the updated manuscript we also discussed about using different T , maximum number of permitted steps . In general given a small number of permitted steps , we found that our models less likely overfit although the final performance is not high . As the number of permitted steps increases both dev and test accuracy improves yielding an overall higher performance . However , this holds up to a certain point . When the permitted steps are large , we no longer observe a significant performance improvement in terms of testing accuracy . For example , NSE Query Gating model with T=15 achieved 79.2 % dev accuracy and 71.4 % test accuracy on CBT-NE task , showing the highest accuracy on the dev set yet massively overfitting on the test set . Furthermore it becomes expensive to train a model with a large number of allowed steps . During the qualitative analysis , we found out that the distribution of T is mostly concentrated at 2 . This is also indicated in the query regression visualizations in Appendix A . As noted in the manuscript , our adaptive computation model adjusts T for a particular task ( i.e document and question pair ) . More recently and following our work , the adaptive computation framework was successfully adapted for image classification and object recognition tasks [ 1 ] . Particularly the adaptive computation framework was deployed within residual networks to decide when to halt the computation . Finally for the CNN news task , we ran our NSE adaptive model without any hyper-parameter tuning . It achieved ~74 % accuracy ( 74 % ~75 % ) on the CNN test set . The result is not included in our paper as we speculate that the results could be improved further if we have more time . We think that ~74 % accuracy is reasonable considering the fact that the human performance on the task is ~75 % . Ref : 1.Figurnov , Michael , et al . `` Spatially Adaptive Computation Time for Residual Networks . '' arXiv preprint arXiv:1612.02297 ( 7 Dec 2016 ) ."}, "2": {"review_id": "Hk8TGSKlg-2", "review_text": "This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read/compose/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond Cloze-style QA. Another advantage of the proposed model is to learn when to terminate the iteration by the so-called adaptive computation model, such that it avoids the issue of treating the number of iterations as another hyper-parameter, which is a common practice of iterative models/multi-hop reasoning in previous papers. There are a couple places that this paper can improve. First, I would like to see the results from CNN/Daily Mail as well to have a more comprehensive comparison. Secondly, it will be useful to visualize the entire M^q sequence over time t (not just z or the query gating) to help understand better the query regression and if it is human interpretable.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful review and useful suggestions . > First , I would like to see the results from CNN/Daily Mail as well to have a more comprehensive comparison . For the CNN news task , we ran our NSE adaptive model without any hyper-parameter tuning . It achieved ~74 % accuracy ( 74 % ~75 % ) on the CNN test set . The result is not included in our paper as we speculate that the results could be improved further if we have more time . We think that ~74 % accuracy is reasonable considering the fact that the human performance on the task is ~75 % . > Secondly , it will be useful to visualize the entire M^q sequence over time t ( not just z or the query gating ) to help understand better the query regression and if it is human interpretable . Since the query regression happens in a continuous vector space it is not trivial to directly visualize it . However , we think that by looking at the z and gating states one could interpret how the query is regressed . As shown in Figure 3 , word information provides a cue how answer is kept . As shown in Figure 3 , content words ( e.g. , \u201c Rabbit \u201d ) are not overwritten throughout the regression loop while non-content words like ' . ' and < BOS > are replaced in the very first step ."}}