{"year": "2018", "forum": "By5ugjyCb", "title": "PACT: Parameterized Clipping Activation for Quantized Neural Networks", "decision": "Reject", "meta_review": "All of the reviewers agree that the experimental results are promising and the proposed activation function enables a decent degree of quantization. However, the main concern with the approach is its limited novelty compared to previous work on clipped activation functions.\n\nminor comments:\n- Even though PACT is very similar to Relu, the names are very different.\n- Please include a plot showing the proposed activation function as well.\n", "reviews": [{"review_id": "By5ugjyCb-0", "review_text": "The authors have addressed my concerns, and clarified a misunderstanding of the baseline that I had, which I appreciate. I do think that it is a solid contribution with thorough experiments. I still keep my original rating of the paper because the method presented is heavily based on previous works, which limits the novelty of the paper. It uses previously proposed clipping activation function for quantization of neural networks, adding a learnable parameter to this function. _______________ ORIGINAL REVIEW: This paper proposes to use a clipping activation function as a replacement of ReLu to train a neural network with quantized weights and activations. It shows empirically that even though the clipping activation function obtains a larger training error for full-precision model, it maintains the same error when applying quantization, whereas training with quantized ReLu activation function does not work in practice because it is unbounded. The experiments are thorough, and report results on many datasets, showing that PACT can reduce down to 4 bits of quantization of weights and activation with a slight loss in accuracy compared to the full-precision model. Related to that, it seams a bit an over claim to state that the accuracy decrease of quantizing the DNN with PACT in comparison with previous quantization methods is much less because the decrease is smaller or equal than 1%, when competing methods accuracy decrease compared to the full-precision model is more than 1%. Also, it is unfair to compare to the full-precision model using clipping, because ReLu activation function in full-precision is the standard and gives much better results, and this should be the reference accuracy. Also, previous methods take as reference the model with ReLu activation function, so it could be that in absolute value the accuracy performance of competing methods is actually higher than when using PACT for quantizing DNN. OTHER COMMENTS: - the list of contributions is a bit strange. It seams that the true contribution is number 1 on the list, which is to introduce the parameter \\alpha in the activation function that is learned with back-propagation, which reduces the quantization error with respect to using ReLu as activation function. To provide an analysis of why it works and quantitative results, is part of the same contribution I would say.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the detail comments . Here are our answers : Q1 . Over-claim that PACT \u2019 s accuracy degradation is much less than others ? A1.There are two aspects to consider for PACT 's accuracy degradation . First , PACT outperforms ( in terms of accuracy degradation ) for all the bit-width configuration we compared , demonstrating superior robustness of PACT compared to the other quantization schemes . This clear trend can be seen in Tables 3-8 , where the bold numbers indicate the one with lowest accuracy degradation for each column . We added Appendix A and B to analyze why PACT can outperform ReLU based activation quantization schemes . Second , PACT 's accuracy degradation is much lower for the challenging activation quantization ( e.g. , quantizing activation of binary/ternary weight networks ) for ResNet-50 . For example , as shown in Table 7 , accuracy degradations for HWGQ and FGQ are 11.4 % and 6.7 % , respectively , whereas PACT 's accuracy degradations are 9.1 % and 2.4 % for the same bit-precision . This gap in accuracy degradation becomes even larger when PACT is compared to the LPBN technique . In case of 3-bit activation with full-precision weight , LPBN 's accuracy degradation is 19.9 % , whereas PACT 's accuracy degradation is only 1.4 % . Q2.Baseline uses Clipping activation function ? A2.No , our full-precision baselines use the same activation function ( i.e. , ReLU ) as the network structure is proposed in the original paper . Tables 3-8 show that the accuracies for our full-precision baselines are comparable to the full-precision reference of the other work we compared . We will clarify this more in Section 5 and Appendix D. Q3 . Do not separate contribution for \u201c Why PACT works \u201d with \u201c PACT \u201d A3 . Thanks for the suggestion . We will merge the first two contributions to one . Furthermore , we now include enhanced analysis on PACT in Appendix A and B to provide deeper understanding about why PACT outperforms previous ReLU based activation quantization schemes ."}, {"review_id": "By5ugjyCb-1", "review_text": "The parameterized clipping activation (PACT) idea is very clear: extend clipping activation by learning the clipping parameter. Then, PACT is combined with quantizing the activations. The proposed technique sounds. The performance improvement is expected and validated by experiments. But I am not sure if the novelty is strong enough for an ICLR paper. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and showing interest to our work . To answer your question on the novelty of PACT , we put a detail response in the first comment above . And here 's a brief summary : We claim that PACT is a new activation function that is best suitable for activation quantization . We claim that ( 1 ) PACT demonstrates ( for the first time ) no-accuracy-degraded 4-bit quantization ( both weight and activation ) for challenging ResNet-50 for ImageNet dataset , and ( 2 ) PACT UNIVERSALLY outperforms ReLU based activation quantization schemes for all the CNN models we tested . To better explain why PACT outperforms ReLU based activation quantization schemes , we newly added Appendix A and B for deeper analysis of PACT . We showed that ( 1 ) PACT is as expressive as ReLU , and ( 2 ) PACT balances clipping and quantization errors when activation is quantized . Also , please note that all the robust accuracies we achieved with PACT do NOT require any modification in the original hyper-parameters and network structures the baselines use ."}, {"review_id": "By5ugjyCb-2", "review_text": "This paper presents a new idea to use PACT to quantize networks, and showed improved compression and comparable accuracy to the original network. The idea is interesting and novel that PACT has not been applied to compressing networks in the past. The results from this paper is also promising that it showed convincing compression results. The experiments in this paper is also solid and has done extensive experiments on state of the art datasets and networks. Results look promising too. Overall the paper is a descent one, but with limited novelty. I am a weak reject", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and showing interest to our work . To answer your question on the novelty of PACT , we put a detail response in the first comment above . And here 's a brief summary : We claim that PACT is a new activation function that is best suitable for activation quantization . We claim that ( 1 ) PACT demonstrates ( for the first time ) no-accuracy-degraded 4-bit quantization ( both weight and activation ) for challenging ResNet-50 for ImageNet dataset , and ( 2 ) PACT UNIVERSALLY outperforms ReLU based activation quantization schemes for all the CNN models we tested . To better explain why PACT outperforms ReLU based activation quantization schemes , we newly added Appendix A and B for deeper analysis of PACT . We showed that ( 1 ) PACT is as expressive as ReLU , and ( 2 ) PACT balances clipping and quantization errors when activation is quantized . Also , please note that all the robust accuracies we achieved with PACT do NOT require any modification in the original hyper-parameters and network structures the baselines use ."}], "0": {"review_id": "By5ugjyCb-0", "review_text": "The authors have addressed my concerns, and clarified a misunderstanding of the baseline that I had, which I appreciate. I do think that it is a solid contribution with thorough experiments. I still keep my original rating of the paper because the method presented is heavily based on previous works, which limits the novelty of the paper. It uses previously proposed clipping activation function for quantization of neural networks, adding a learnable parameter to this function. _______________ ORIGINAL REVIEW: This paper proposes to use a clipping activation function as a replacement of ReLu to train a neural network with quantized weights and activations. It shows empirically that even though the clipping activation function obtains a larger training error for full-precision model, it maintains the same error when applying quantization, whereas training with quantized ReLu activation function does not work in practice because it is unbounded. The experiments are thorough, and report results on many datasets, showing that PACT can reduce down to 4 bits of quantization of weights and activation with a slight loss in accuracy compared to the full-precision model. Related to that, it seams a bit an over claim to state that the accuracy decrease of quantizing the DNN with PACT in comparison with previous quantization methods is much less because the decrease is smaller or equal than 1%, when competing methods accuracy decrease compared to the full-precision model is more than 1%. Also, it is unfair to compare to the full-precision model using clipping, because ReLu activation function in full-precision is the standard and gives much better results, and this should be the reference accuracy. Also, previous methods take as reference the model with ReLu activation function, so it could be that in absolute value the accuracy performance of competing methods is actually higher than when using PACT for quantizing DNN. OTHER COMMENTS: - the list of contributions is a bit strange. It seams that the true contribution is number 1 on the list, which is to introduce the parameter \\alpha in the activation function that is learned with back-propagation, which reduces the quantization error with respect to using ReLu as activation function. To provide an analysis of why it works and quantitative results, is part of the same contribution I would say.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the detail comments . Here are our answers : Q1 . Over-claim that PACT \u2019 s accuracy degradation is much less than others ? A1.There are two aspects to consider for PACT 's accuracy degradation . First , PACT outperforms ( in terms of accuracy degradation ) for all the bit-width configuration we compared , demonstrating superior robustness of PACT compared to the other quantization schemes . This clear trend can be seen in Tables 3-8 , where the bold numbers indicate the one with lowest accuracy degradation for each column . We added Appendix A and B to analyze why PACT can outperform ReLU based activation quantization schemes . Second , PACT 's accuracy degradation is much lower for the challenging activation quantization ( e.g. , quantizing activation of binary/ternary weight networks ) for ResNet-50 . For example , as shown in Table 7 , accuracy degradations for HWGQ and FGQ are 11.4 % and 6.7 % , respectively , whereas PACT 's accuracy degradations are 9.1 % and 2.4 % for the same bit-precision . This gap in accuracy degradation becomes even larger when PACT is compared to the LPBN technique . In case of 3-bit activation with full-precision weight , LPBN 's accuracy degradation is 19.9 % , whereas PACT 's accuracy degradation is only 1.4 % . Q2.Baseline uses Clipping activation function ? A2.No , our full-precision baselines use the same activation function ( i.e. , ReLU ) as the network structure is proposed in the original paper . Tables 3-8 show that the accuracies for our full-precision baselines are comparable to the full-precision reference of the other work we compared . We will clarify this more in Section 5 and Appendix D. Q3 . Do not separate contribution for \u201c Why PACT works \u201d with \u201c PACT \u201d A3 . Thanks for the suggestion . We will merge the first two contributions to one . Furthermore , we now include enhanced analysis on PACT in Appendix A and B to provide deeper understanding about why PACT outperforms previous ReLU based activation quantization schemes ."}, "1": {"review_id": "By5ugjyCb-1", "review_text": "The parameterized clipping activation (PACT) idea is very clear: extend clipping activation by learning the clipping parameter. Then, PACT is combined with quantizing the activations. The proposed technique sounds. The performance improvement is expected and validated by experiments. But I am not sure if the novelty is strong enough for an ICLR paper. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and showing interest to our work . To answer your question on the novelty of PACT , we put a detail response in the first comment above . And here 's a brief summary : We claim that PACT is a new activation function that is best suitable for activation quantization . We claim that ( 1 ) PACT demonstrates ( for the first time ) no-accuracy-degraded 4-bit quantization ( both weight and activation ) for challenging ResNet-50 for ImageNet dataset , and ( 2 ) PACT UNIVERSALLY outperforms ReLU based activation quantization schemes for all the CNN models we tested . To better explain why PACT outperforms ReLU based activation quantization schemes , we newly added Appendix A and B for deeper analysis of PACT . We showed that ( 1 ) PACT is as expressive as ReLU , and ( 2 ) PACT balances clipping and quantization errors when activation is quantized . Also , please note that all the robust accuracies we achieved with PACT do NOT require any modification in the original hyper-parameters and network structures the baselines use ."}, "2": {"review_id": "By5ugjyCb-2", "review_text": "This paper presents a new idea to use PACT to quantize networks, and showed improved compression and comparable accuracy to the original network. The idea is interesting and novel that PACT has not been applied to compressing networks in the past. The results from this paper is also promising that it showed convincing compression results. The experiments in this paper is also solid and has done extensive experiments on state of the art datasets and networks. Results look promising too. Overall the paper is a descent one, but with limited novelty. I am a weak reject", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and showing interest to our work . To answer your question on the novelty of PACT , we put a detail response in the first comment above . And here 's a brief summary : We claim that PACT is a new activation function that is best suitable for activation quantization . We claim that ( 1 ) PACT demonstrates ( for the first time ) no-accuracy-degraded 4-bit quantization ( both weight and activation ) for challenging ResNet-50 for ImageNet dataset , and ( 2 ) PACT UNIVERSALLY outperforms ReLU based activation quantization schemes for all the CNN models we tested . To better explain why PACT outperforms ReLU based activation quantization schemes , we newly added Appendix A and B for deeper analysis of PACT . We showed that ( 1 ) PACT is as expressive as ReLU , and ( 2 ) PACT balances clipping and quantization errors when activation is quantized . Also , please note that all the robust accuracies we achieved with PACT do NOT require any modification in the original hyper-parameters and network structures the baselines use ."}}