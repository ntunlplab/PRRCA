{"year": "2017", "forum": "Sks3zF9eg", "title": "Taming the waves: sine as activation function in deep neural networks", "decision": "Reject", "meta_review": "The reviewers unanimously recommend rejecting the paper.", "reviews": [{"review_id": "Sks3zF9eg-0", "review_text": "Summary: In this paper, the authors explore the advantages/disadvantages of using a sin activation function. They first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions. They then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well. They then try different algorithmic tasks, where the periodicity of the functions is helpful. Pros: The closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational. Cons: Seems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable. The toy algorithmic tasks are hard to conclude something concrete from.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Review : Summary : In this paper , the authors explore the advantages/disadvantages of using a sin activation function . They first demonstrate that even with simple tasks , using sin activations can result in complex to optimize loss functions . They then compare networks trained with different activations on the MNIST dataset , and discover that the periodicity of the sin activation is not necessary for learning the task well . They then try different algorithmic tasks , where the periodicity of the functions is helpful . This is an accurate summary . More specifically concerning point 2 : from our experiments it emerges that when the task is learned well the periodicity is not used , and when the task is not learned ( i.e.the network just overfits to the training set ) the periodicity is used heavily . Pros : The closed form derivations of the loss surface were interesting to see , and the clarity of tone on the advantages * and * disadvantages was educational . Cons : Seems like more of a preliminary investigation of the potential benefits of sin , and more evidence ( to support or in contrary ) is needed to conclude anything significant We agree that more evidence would make for a stronger case . We are now considering a few options in this direction , in order to support the main contribution of the paper . O2 would benefit from more experiments similar to the MNIST experiment to validate/falsify it . We are planning to choose a couple more datasets to test the claim on , for now we added the Reuters dataset . What else would you recommend to investigate in order to further improve the impact of the paper ? -- the results on MNIST seem to indicate truncated sin is just as good , and while it is interesting that tanh maybe uses more of the saturated part , the two seem relatively interchangeable . This is correct . However , the main contribution of the results on MNIST is evidence showing that when training is successful for typical datasets ( as it 's the case for most previous works using sine presented in Section 2 ) the network is not relying on the periodicity of the function . The network relies almost exclusively on the central part of the function , that is monotonic and similar to a tanh . -- The toy algorithmic tasks are hard to conclude something concrete from . Our conclusion from the toy tasks is the following : There might still be room for sine as activation function in certain artificial/algorithmic tasks where sine is intuitively beneficial , since there are at least two simple tasks ( i.e.sum and diff ) where sin can outperform a standard RNN baseline using tanh ."}, {"review_id": "Sks3zF9eg-1", "review_text": "Authors propose using periodic activation functions (sin) instead of tanh for gradient descent training of neural networks. This change goes against common sense and there would need to be strong evidence to show that it's a good idea in practice. The experiments show slight improvement (98.0 -> 98.1) for some MNIST configurations. They show strong improvement (almost 100% higher accuracy after 1500 iterations) on a toy algorithmic task. It's not clear that this activation function is good for a broad class of algorithmic tasks or just for the two they present. Hence evidence shown is insufficient to be convincing that this is a good idea for practical tasks.", "rating": "4: Ok but not good enough - rejection", "reply_text": "There has probably been a misunderstanding . The scope of the paper is not to show that sine is overall better than tanh as an activation function . The main point of the paper is to impartially analyze the effect of having sine as an activation function , how it affects the representation learned and what consequences it has on learning . Our findings actually indicate that sine is probably not a good choice for typical datasets , as the analysis conducted in Section 3 and the experiments on MNIST in Section 4 demonstrate . We also offer a simple and plausible explanation to its previous successful uses ( presented Section 2 ) , i.e.that when it works it \u2019 s acting very similarly to a tanh in its non-saturated region . The algorithmic experiments are only supposed to show that there might still be tasks where sine turns out to be helpful , see O3 for more details . In order to make the main contributions of the paper more clear , we have more explicitly mentioned these in the introduction to the paper ."}, {"review_id": "Sks3zF9eg-2", "review_text": "An interesting study of using Sine as activation function showing successful training of models using Sine. However the scope of tasks this is applied to is a bit too limited to be convincing. Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?", "rating": "4: Ok but not good enough - rejection", "reply_text": "The other main points of the paper , that go beyond showing that there are tasks where sin is potentially beneficial , is evidence that sine as activation function makes for a difficult optimization problem and that when learning is successful the periodic part of the function is often largely ignored for typical datasets ( as sine has already been successfully used in several works mentioned in Section 2 ) . Please , see the Comments to all reviewers . We do agree that having more evidence of sine working on larger tasks would make for a stronger case supporting its use . However we did n't have this as a goal of the paper , and \u2014 instead of finding what tasks sin excels at \u2014 at this stage our work has been mostly focused on analyzing what the consequences are on the representation learned and in characterizing what makes these nets difficult to train . From the results presented in the paper ( and the appendix , with the case of larger networks closing or reversing the gap between sin and tanh ) it does seem that it might be difficult to find tasks and conditions where sin is more beneficial , so we would propose this for investigation in future work ."}], "0": {"review_id": "Sks3zF9eg-0", "review_text": "Summary: In this paper, the authors explore the advantages/disadvantages of using a sin activation function. They first demonstrate that even with simple tasks, using sin activations can result in complex to optimize loss functions. They then compare networks trained with different activations on the MNIST dataset, and discover that the periodicity of the sin activation is not necessary for learning the task well. They then try different algorithmic tasks, where the periodicity of the functions is helpful. Pros: The closed form derivations of the loss surface were interesting to see, and the clarity of tone on the advantages *and* disadvantages was educational. Cons: Seems like more of a preliminary investigation of the potential benefits of sin, and more evidence (to support or in contrary) is needed to conclude anything significant -- the results on MNIST seem to indicate truncated sin is just as good, and while it is interesting that tanh maybe uses more of the saturated part, the two seem relatively interchangeable. The toy algorithmic tasks are hard to conclude something concrete from.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Review : Summary : In this paper , the authors explore the advantages/disadvantages of using a sin activation function . They first demonstrate that even with simple tasks , using sin activations can result in complex to optimize loss functions . They then compare networks trained with different activations on the MNIST dataset , and discover that the periodicity of the sin activation is not necessary for learning the task well . They then try different algorithmic tasks , where the periodicity of the functions is helpful . This is an accurate summary . More specifically concerning point 2 : from our experiments it emerges that when the task is learned well the periodicity is not used , and when the task is not learned ( i.e.the network just overfits to the training set ) the periodicity is used heavily . Pros : The closed form derivations of the loss surface were interesting to see , and the clarity of tone on the advantages * and * disadvantages was educational . Cons : Seems like more of a preliminary investigation of the potential benefits of sin , and more evidence ( to support or in contrary ) is needed to conclude anything significant We agree that more evidence would make for a stronger case . We are now considering a few options in this direction , in order to support the main contribution of the paper . O2 would benefit from more experiments similar to the MNIST experiment to validate/falsify it . We are planning to choose a couple more datasets to test the claim on , for now we added the Reuters dataset . What else would you recommend to investigate in order to further improve the impact of the paper ? -- the results on MNIST seem to indicate truncated sin is just as good , and while it is interesting that tanh maybe uses more of the saturated part , the two seem relatively interchangeable . This is correct . However , the main contribution of the results on MNIST is evidence showing that when training is successful for typical datasets ( as it 's the case for most previous works using sine presented in Section 2 ) the network is not relying on the periodicity of the function . The network relies almost exclusively on the central part of the function , that is monotonic and similar to a tanh . -- The toy algorithmic tasks are hard to conclude something concrete from . Our conclusion from the toy tasks is the following : There might still be room for sine as activation function in certain artificial/algorithmic tasks where sine is intuitively beneficial , since there are at least two simple tasks ( i.e.sum and diff ) where sin can outperform a standard RNN baseline using tanh ."}, "1": {"review_id": "Sks3zF9eg-1", "review_text": "Authors propose using periodic activation functions (sin) instead of tanh for gradient descent training of neural networks. This change goes against common sense and there would need to be strong evidence to show that it's a good idea in practice. The experiments show slight improvement (98.0 -> 98.1) for some MNIST configurations. They show strong improvement (almost 100% higher accuracy after 1500 iterations) on a toy algorithmic task. It's not clear that this activation function is good for a broad class of algorithmic tasks or just for the two they present. Hence evidence shown is insufficient to be convincing that this is a good idea for practical tasks.", "rating": "4: Ok but not good enough - rejection", "reply_text": "There has probably been a misunderstanding . The scope of the paper is not to show that sine is overall better than tanh as an activation function . The main point of the paper is to impartially analyze the effect of having sine as an activation function , how it affects the representation learned and what consequences it has on learning . Our findings actually indicate that sine is probably not a good choice for typical datasets , as the analysis conducted in Section 3 and the experiments on MNIST in Section 4 demonstrate . We also offer a simple and plausible explanation to its previous successful uses ( presented Section 2 ) , i.e.that when it works it \u2019 s acting very similarly to a tanh in its non-saturated region . The algorithmic experiments are only supposed to show that there might still be tasks where sine turns out to be helpful , see O3 for more details . In order to make the main contributions of the paper more clear , we have more explicitly mentioned these in the introduction to the paper ."}, "2": {"review_id": "Sks3zF9eg-2", "review_text": "An interesting study of using Sine as activation function showing successful training of models using Sine. However the scope of tasks this is applied to is a bit too limited to be convincing. Maybe showing good results on more important tasks in addition to current toy tasks would make a stronger case?", "rating": "4: Ok but not good enough - rejection", "reply_text": "The other main points of the paper , that go beyond showing that there are tasks where sin is potentially beneficial , is evidence that sine as activation function makes for a difficult optimization problem and that when learning is successful the periodic part of the function is often largely ignored for typical datasets ( as sine has already been successfully used in several works mentioned in Section 2 ) . Please , see the Comments to all reviewers . We do agree that having more evidence of sine working on larger tasks would make for a stronger case supporting its use . However we did n't have this as a goal of the paper , and \u2014 instead of finding what tasks sin excels at \u2014 at this stage our work has been mostly focused on analyzing what the consequences are on the representation learned and in characterizing what makes these nets difficult to train . From the results presented in the paper ( and the appendix , with the case of larger networks closing or reversing the gap between sin and tanh ) it does seem that it might be difficult to find tasks and conditions where sin is more beneficial , so we would propose this for investigation in future work ."}}