{"year": "2019", "forum": "HJMjW3RqtX", "title": "One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL", "decision": "Reject", "meta_review": "The paper introduces a setting called high-fidelity imitation where the goal one-shot generalization to new trajectories in a given environment. The authors contrast this with more standard one-shot imitation approaches where one-shot generalization is to a task rather than a precise trajectory. The authors propose a technique that works off of only state information, which is coupled with an RL algorithm that learns from a replay buffer that is populated by the imitator. The authors emphasize that their approach can leverage very large deep learning models, and demonstrate strong empirical performance in a (simulated) robotics setting. \n\nA key weakness of the paper is its clarity. All reviewers were unclear about the precise setting as well as relation to prior work in one-shot imitation learning. As a result, there were substantial challenges in assessing the technical contribution of the paper. There were many requests for clarification, including for the motivation, difference between the present setting and those addressed in previous work, algorithmic details, and experiment details.\n\nI believe that a further concern was the lack of a wide range of baselines. The authors construct several baselines that are relevant in the given setting, but did not consider \"naive baseline\" approaches proposed by the reviewers. For example, behavior cloning is mentioned as a potential baseline several times. The authors argue that this is not applicable as it would require expert actions. Instead of considering it a baseline, BC could be used as an \"oracle\" - performance that could be achieved if demonstration actions were known. As long as the access to additional information is clearly marked, such a comparison with a privileged oracle can be properly placed by the reader. Without including such commonly known reference approaches, it is very challenging to assess the proposed method's performance in the context of the difficulty of the task. Generally, whenever a paper introduces both a new task and a new approach, a lot of care needs to be taken to build up insights into whether the task appropriately reflects the domain / challenge the paper claims to address, how challenging the task is in comparison to those addressed in prior work, and to place the performance of the novel proposed method in the context of prior work. In the present paper, on top of the task and approach being novel, the pure RL baseline D4PG is not yet widely known in the community and it's performance relative to common approaches is not well understood. Including commonly known RL approaches would help put all these results in context.\n\nThe authors took great care to respond to the reviewer comments, providing thorough discussion of related work and clarifications of the task and approach, and these were very helpful to the AC to understand the paper. The AC believes that the paper has excellent potential. At the same time, a much more thorough empirical evaluation is needed to demonstrate the value of the proposed approach in this novel setting, as well as to provide additional conceptual insights into why and in what kinds of settings the algorithm performance well, or where its limitations are. \n", "reviews": [{"review_id": "HJMjW3RqtX-0", "review_text": "**Summary** The paper looks at the problem of one-shot imitation with high accuracy of imitation. The main contributions: 1. learning technique for high fidelity one-shot imitation at test time. 2. Policies to improve the expert performance through RL. The main improvements of this method is that demo action and rewards are not needed only state trajectories are sufficient. ** Comments ** - The novelty of algorithm block The main method is very similar to D4PG-fd. The off-policy method samples from a replay buffer which comprises of both the demos and the agent experience from the previous learner iterates. 1. From a technical perspective, what is the advantage of training an imitation learner from a memory buffer of the total experience? If the task reward is not accessed, then when the imitation learner is training, then the data should not be used for training the task policy learner. On the other hand if task reward is indeed available then what is the advantage of not using it. 2. A comparison with a BC policy to generate more experience data for the task policy agent/learning might also be useful. * Improved Comparisons - Compare with One-Shot Performance Since this is one of the main contributions, explicit comparison with other one-shot imitation papers needs to be quantified with a clearly defined metric for generalization. This comparison should be both for short-term tasks such as block pick and place (Finn et al, Pathak et al, Sermanet et al.) and also for long-term tasks as shown in (Duan et al. 2017 and also in Neural Task Programming/Neural Task Graph line of work from 2018) - Compare High-Fidelity Performance It is used as a differentiator of this method but without experimental evidence. The results showing imitation reward are insufficient. The metric should be independent of the method. An evaluation might compare trajectory tracking error: for objects, end-effector, and joint positions. This is available as privileged information since the setup is in a simulation. Furthermore, a comparison with a model-based trajectory tracking with a learned or fitted model of dynamics is also very useful. - Compare Policy Learning Performance In addition to D4PG variants, performance comparison with GAIL will ascertain that unconditional imitation is better than SoTA. * Tracking a reference (from either sim or demos) is a good idea that has been explored in sim2real literature[2,3] and imitation learning [4]. It is not by itself novel. The authors fail to acknowledge any work in this line as well as provide insight why is this good and when is this valid. For instance, with highly stochastic dynamics this may not work! - \"Diverse Novel Skills\" The experiments are limited to a rather singular pick and place task with a 3-step structured reward model. It is unfair to characterize this domain as very diverse or complex from a robotics perspective. More experiments on continuous control would help. - Bigger networks \"In fig. 3 we demonstrate that indeed a large ResNet34-style network (He et al., 2016) clearly outperforms\" -- but Fig 3 is a network architecture diagram. It is probably fig 6! - The authors are commended for presenting a broad overview of imitation based methods in table 2 ** Questions ** 1. How different if the imitation learner (trained with imitation reward) from a Behaviour Cloning Policy. 2. How is the local context considered in action generation in sec 2.1. The authors reset the simulation environment to o_1 = d_1. Then actions are generated with \\pi_{theta} (o_t, d_{t+1}). a. Is the environment reset every time step? b. If not how is the deviation of the trajectory handled over time? c. how is the time horizon for this open loop roll out chosen. 3. How is this different for a using a tracking based MPC with the same horizon? The cost can be set the same the similarity metric between states. 4. The architecture uses a deep but simplistic model. When the major attribution of the model success is to state similarity -- especially image similarity -- why did the authors not use image comparators something like the Siamese model? Suggestion: The whole set of experiments are in a simulation. The authors go above and beyond in using Mitsuba for rendering images. But the images used are Mujoco rendered default. It would nice if the authors were more forthcoming about this. All image captions should clearly state -- Simulated robot results, show images used for agent training. The Mitsuba renders are only used for images but nowhere in the algorithm. So why do this at all, and if it has to be used please do it with a disclaimer. Right now this detail is rather buried in the text. References: 1. Neural Task Programming, Xu et al. 2018 (https://arxiv.org/abs/1710.01813) 2. Preparing for the Unknown: Learning a Universal Policy with Online System Identification (https://arxiv.org/abs/1702.02453) 3. Adapt: zero-shot adaptive policy transfer for stochastic dynamical systems (https://arxiv.org/abs/1707.04674) 4. A survey of robot learning from demonstration, Argall et al. 2009 ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for detailed and extensive feedback . In addition to the contribution pointed out above , we would like to emphasize that this work demonstrates that it is possible to train massive deep neural networks ( larger than any previous attempt by at least an order of magnitude ) for RL . Moreover , the paper shows through ablations that such architectures are essential to achieve good generalization in one-shot imitation . Smaller networks fail to generalize . We feel the problem of generalization in one-shot learning is central to AI , and as such we believe this paper presents important results showing how to advance this research frontier . A few years ago , we certainly did not know whether RL , with its considerable variance , would allow us to train such large policies . We also did not know whether big nets were necessary at all in control tasks , as pointed out by Emo Todorov and colleagues at the previous NIPS . This paper provides empirical evidence and answers to these important questions . > The main method is very similar to D4PG-fd . The off-policy method samples from a replay buffer which comprises of both the demos and the agent experience from the previous learner iterates . For clarification , the D4PGfd algorithm was introduced in this paper , with the previous existing work -- - the DDPGfg of Vecerik et al ( 2017 ) -- - missing the distributed and distributional aspects of the policy optimizer . Additionally , the D4PGfd method requires actions , but in contrast MetaMimic does not need access to actions as you note above . Moreover , MetaMimic does two things ( i ) one-shot high-fidelity imitation and ( ii ) task policy learning . The D4PGfd method only applies to task policy learning ( ii ) . That is , it is missing an important core feature of MetaMimic . > 1.From a technical perspective , what is the advantage of training an imitation learner from a memory buffer of the total experience ? If the task reward is not accessed , then when the imitation learner is training , then the data should not be used for training the task policy learner . On the other hand if task reward is indeed available then what is the advantage of not using it . Excellent question . The purpose of MetaMimic is twofold . The first goal is to deploy policies that users ( say someone at a factory ) can easily adapt , via demonstrations , to solve new tasks . Moreover , the case studied in this paper aims to meet the need for imitating the user observation trajectory with high-fidelity ( eg in high precision engineering or surgery ) . That is , it is not only important to accomplish the goal , but also it is important to achieve this in a very precise and specific manner . The second purpose of MetaMimic is to act as a general task policy learner by capitalizing on demonstrations of observations and rewards . Here , the process of following the demonstrations should be understood as an auxiliary task to address the problem of exploration . The final goal is a task policy . In this sense , the closest competitor to MetaMimic is DDPG-fd , but as pointed out , MetaMimic works from observations while DDPG-fd requires additional access to actions . Interestingly , our results in Figure 8 , using our proposed D4PG-fd method , show that both methods perform similarly , despite MetaMimic requiring less information . We feel much of the confusion we \u2019 ve created comes from the fact that we are proposing a method that does two things . It can be useful to do ( i ) only , ( ii ) only or both ( i ) and ( ii ) . It really depends on the deployment case . > 2.A comparison with a BC policy to generate more experience data for the task policy agent/learning might also be useful . This may work quite well . However , BC requires expert actions while our method does not . We already provide a strong baseline for training the task policy with access to expert actions , D4PGfD ."}, {"review_id": "HJMjW3RqtX-1", "review_text": "Summary This work porposes a approach for one-shot imitation with high accuracy, called \"high fidelity imitation learning\" by the authors. Furthermore, the work addresses the common problem of exploration in imitation learning, which would help to rescue from off-policy states. Review In my opinion, the main claims of this paper are not validated sufficiently in the experiments. I would expect the experiments to be designed specifically to support the claims made, but little evidence is provided: - The authors claim that the method allows one-shot generalization to an unknown trajectory. To test this hypothesis the authors only provide experiments of generalization towards trajectories of a different demonstrator on the same task of stacking cubes. I would expect experiments with truly different trajectories on a different task than stacking cubes to test the hypothesis of one-shot imitation. Until then I see no evidence for a \"one-shot\" imitation capability of the proposed method. - That storing the trajectories of early training can act as replacement for exploration as rescue from off-policy states: This is never experimentally validated. This hypothesis could easiliy be validated with an ablation study, were the results of early would not be added to the replay buffer. - High fidelity imitation: In the caption of Figure 7 the authors note that the unconditional task policy is able to outperform the demonstration videos. Thus the trajectories of the unconditional task policy allow a higher reward then the demonstrations. Could the authors please comment on how the method still achieves high fidelity imitation even when the results of the unconditional task policy are added to the replay buffer? In prinicipal these trajectories allow a higher reward than the demonstration trajectories that should be imitated. Mainly due to the missing experimental validation of the claims made I recommend to reject the paper.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for taking the time to provide feedback on the paper . We have taken great care to ensure our claims are directly supported by our experiments . We will address the two claims your refer to below . > The authors claim that the method allows one-shot generalization to an unknown trajectory . To test this hypothesis the authors only provide experiments of generalization towards trajectories of a different demonstrator on the same task of stacking cubes . I would expect experiments with truly different trajectories on a different task than stacking cubes to test the hypothesis of one-shot imitation . Until then I see no evidence for a `` one-shot '' imitation capability of the proposed method . There is a question of terminology here , and we agree that we need to address this more precisely in the paper . Most papers on one shot imitation sample a task t~p ( t ) and conditional on this sample a demonstration d~p ( d|t ) . In this setting , accomplishing t is what matters and significant deviations in the demonstration d are tolerated . In our work , we are sampling d~p ( d ) , and for us it is important to minimize deviations in d ( i.e.we want high-fidelity ) . Other one-shot imitation learning methods mostly focus on object diversity , e.g.pushing unseen objects or placing unseen objects ( see eg the excellent website of Yu and Finn : https : //bair.berkeley.edu/blog/2018/06/28/daml/ ) We instead chose a difficult control task , block stacking , which allows for many diverse ways of solving the task . We focused on demonstration diversity , following a distinctly different trajectory to solve a new task instance . While generalizing to different colours and objects is difficult , generalizing to different motions in one shot imitation is equally hard . As pointed out by Yu , Finn et al ( 2018 ) \u201c While our work enables one-shot learning for manipulating new objects from one video of a human , our current experiments do not yet demonstrate the ability to learn entirely new motions in one shot \u201d . Admittedly , all the different sources of variation are important and we need to make progress in all of them . Our work clearly does not address object variety , and we do need to do this in the future . > That storing the trajectories of early training can act as replacement for exploration as rescue from off-policy states : This is never experimentally validated . This hypothesis could easiliy be validated with an ablation study , were the results of early would not be added to the replay buffer . Actually , this is experimentally validated in Figure 8 . Using D4PG to train the task policy , is equivalent to using our method without adding experiences from the imitation policy to the replay memory . We believe this is strong evidence our method overcomes the exploration problem , because the same RL method is used , with the same hyper-parameters , same number of actors etc , but now the transitions in the replay are more likely to see task reward . We will add a note to the paper to ensure this is clear . > High fidelity imitation : In the caption of Figure 7 the authors note that the unconditional task policy is able to outperform the demonstration videos . Thus the trajectories of the unconditional task policy allow a higher reward then the demonstrations . Could the authors please comment on how the method still achieves high fidelity imitation even when the results of the unconditional task policy are added to the replay buffer ? In prinicipal these trajectories allow a higher reward than the demonstration trajectories that should be imitated . The task policy is able to achieve higher task reward , but at the moment we do n't calculate or store its imitation reward . This is illustrated in Figure 1 . The imitation policy is trying to maximize imitation reward , as a result the task policy trajectories do not interfere with the imitation policy . > Mainly due to the missing experimental validation of the claims made I recommend to reject the paper . We hope we have made clear how our claims are supported by our experiments , and that you would reconsider your evaluation . Regardless , thank you for the feedback . It has been very valuable ."}, {"review_id": "HJMjW3RqtX-2", "review_text": "This paper presents an RL method for learning from video demonstration without access to expert actions. The agent first learn to imitate the expert demonstration (observed image sequence and proprioceptive information) by producing a sequence of actions that will lead to the similar observations (require a renderer that takes actions and outputs images). The imitation loss is a similarity metric. Next, the agent explores the environment with both the imitation policy and task policy being learned; an off-policy RL algorithm D4PG is used for policy learning. Experiments are conducted on a simulated robot block stacking task. The paper is really clearly written, but presenting the approach as \"high-fidelity\", \"one-shot\" learning is a bit confusing. First, it's not clear what's the motivation for high-fidelity. To me this is an artifact due to having to imitate the visual observation instead of the actions, which is a legitimate constraint, but not the original goal. Second, the one-shot learning setting consists of training on a set of stochastic demonstrations and testing on another set collected from a different person; both for the same task. Usually one-shot learning tests on slightly different tasks or environments, whereas here the goal is to generalize to novel demonstrations. It's not clear why do we care imitation per se in addition to the task reward. What I find interesting is the proposed approach for learning for video demonstration without action labels. Currently this requires an executor to render the actions to images, what if we don't have such an executor or only have a noisy / approximate renderer? In the real world it's probably hard to find a good renderer, it would be interesting to see how this constraint can be relaxed. Questions: - While the authors have shown the average rewards of the two sets are different, I wonder what's the variance of each person's demonstration. - In Fig 5, on the validation set, in terms of imitation loss there aren't that much difference between the policies, but in terms of task reward, the 'red' policy goes to zero while others policies' rewards are still similar. Any intuition for why seemingly okay imitation doesn't translate to task reward? Overall, I enjoyed reading the paper and the experiments are comprehensive. The current presentation angle seems a bit off though.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable feedback . We will address some of your comments and questions below . > The paper is really clearly written , but presenting the approach as `` high-fidelity '' , `` one-shot '' learning is a bit confusing . First , it 's not clear what 's the motivation for high-fidelity . To me this is an artifact due to having to imitate the visual observation instead of the actions , which is a legitimate constraint , but not the original goal . Second , the one-shot learning setting consists of training on a set of stochastic demonstrations and testing on another set collected from a different person ; both for the same task . Usually one-shot learning tests on slightly different tasks or environments , whereas here the goal is to generalize to novel demonstrations . It 's not clear why do we care imitation per se in addition to the task reward . Given the review scores , we can only agree with you that the paper is somewhat confusing and we have failed to motivate high-fidelity imitation properly . We think that what makes the paper confusing is that as it stands it tells two stories ( Hi-Fi imitation and task policies ) . These two stories are fundamentally linked , however we admit the presentation did not make these links clear . We will try to address this , but we very much look forward to any advice on how to change the presentation to make it more understandable . We used to the terminology One-Shot High-Fidelity Imitation to clarify both how our method works , and how it differs from existing methods in the space . First , high-fidelity is about mimicking the trajectory precisely . In precision engineering or surgery , where we don \u2019 t want the actuator doing anything other than what was demonstrated , this seems like a valuable idea . Existing few-shot imitation works focus on solving tasks , but not on following the trajectory precisely ( see for example https : //bair.berkeley.edu/blog/2018/06/28/daml/ ) . In relation to this work , Yu and Finn point out : \u201c While our work enables one-shot learning for manipulating new objects from one video of a human , our current experiments do not yet demonstrate the ability to learn entirely new motions in one shot \u201d . The latter is what is demonstrated in our generalization experiments . That is , given a new demonstration motion , our policy is able to follow it closely as shown in Figure 4 . Second , we use the phrase `` one-shot '' to distinguish our method from other tracking based methods , which can require many thousands of environment interactions to learn to track a single trajectory . In contrast , our method requires no additional environment interactions to track a novel trajectory . It achieves this in the same way one-shot methods do , by learning a policy that works well across a large dataset of `` tasks '' where each `` task '' is a demonstration . > What I find interesting is the proposed approach for learning for video demonstration without action labels . Currently this requires an executor to render the actions to images , what if we do n't have such an executor or only have a noisy / approximate renderer ? In the real world it 's probably hard to find a good renderer , it would be interesting to see how this constraint can be relaxed . We agree that relaxing the constraint of an exact environmental renderer is an interesting research direction . This would be helpful for our method , as well as many other RL based methods . But we think it is beyond the scope of this paper . > While the authors have shown the average rewards of the two sets are different , I wonder what 's the variance of each person 's demonstration . There is notable variance between the two demonstrators , and between each demonstration . We will provide some additional examples in the appendix . > In Fig 5 , on the validation set , in terms of imitation loss there are n't that much difference between the policies , but in terms of task reward , the 'red ' policy goes to zero while others policies ' rewards are still similar . Any intuition for why seemingly okay imitation does n't translate to task reward ? Yes , we have noticed two types of behavior that have reasonably high imitation rewards , but do not successfully complete the task : ( i ) the policy closely imitates the arm but ignores the block entirely , ( ii ) the policy successfully imitates both the arm and block position in the beginning of the trajectory , but fails to place the block on a stable position during the stack . As the imitation reward increases we see these behaviors less . > Overall , I enjoyed reading the paper and the experiments are comprehensive . The current presentation angle seems a bit off though . Thanks ! We are really glad you enjoyed the paper , and the experiments , and hope we can align the presentation a bit better ."}, {"review_id": "HJMjW3RqtX-3", "review_text": " Summary: This paper proposes MetaMimic, an algorithm that does the following: (i) Learn to imitate with high-fidelity with one-shot. The setting is that we have access to several demonstrations (only states, no actions) of the same task. During training, we have pixel observations plus proprioceptive measurements). At test time, the learned policy can imitate a single new demonstration (consisting of only pixel observations) of the same task. (ii) When given access to rewards, the policy can exceed the human demonstrator by augmenting its experience replay buffer with the experience gained while learning (i). Therefore, even in a setting with sparse rewards and no access to expert actions (only states), the policy can learn to solve the task. Overall Evaluation: This is a good paper. In my opinion however, it does not pass the bar for ICLR. Pros: - The paper is well written. The contributions are clearly listed, the methods section is easy to follow and the authors explain the choices they make. The illustrations are clear and intuitive. - The overview of hyperparameter choice and tuning / importance factor in the Appendix is useful. - Interesting pipeline of learning policies that can use demonstrations without actions. - The results on the simulated robot arm (block stacking task with two blocks) are good. Cons: - The abstracts oversells the contribution a bit when saying that MetaMimic can learn \"policies for high-fidelity one-shot imitation of diverse novel skills\". The setting that's considered in the paper is that of a single task, but different demonstrations (different humans from different starting points). This seems restrictive, and could have been motivated better. - Experimental results are shown only for one task; block stacking with a robot arm in simulation. - Might not be a good topical fit for ICLR, but more suited for a conference like CoRL or a workshop. The paper is very specific to imitation learning for a manipulation / control tasks, where we can (1) reset the environment to the exact starting position of the demonstrations, (2) the eucledian distance between states in the demonstration and visited by the policy is meaningful (3) we have access to both pixel observations and proprioceptive measurements. The proposed method is an elegant way to solve this, but it's unclear how well it would perform on different types of control problems, or when we want to transfer policies between different (but related) tasks. Questions: - Where does the \"task stochasticity\" come from? Only from the starting state, and from having different demonstrations? Or is the transition function also stochastic? - The learned policy is able to do one-shot imitation, i.e., given a new demonstration (of the same task) the policy can follow this demonstration. Do I understand correct that this mean that there is *no* additional learning required at test time? - It is not immediately clear to me why the setting of a single task but new demonstrations is interesting. Could the authors comment on this? One setting I could imagine is that the policy is trained in simulation, but then executed in the real-world, given a new demonstration. (If that's the main motivation though, then the experiments might have to support that this is possible - if no real-world robot is available, maybe the same simulator with a slightly different camera angle / light conditons or so.) - The x-axis in the figures says \"time (hours)\" - is that computation time or simulated time? Other Comments: - In 3.2, I would be interested in seeing the following baseline comparison: Learn the test task from scratch using the one available demonstration, with the RL procedure (Equation 2, but possibly without the second term to make it fair). In Figure 5, we can see that the performance on the training tasks is much better when training on only 10 tasks, compared to 500. Then why not overfit to a single task, if that's what we're interested in? - An interesting baseline for 3.3 might be an RL algorithm with shaped rewards: using an additional reward term that is the eucledian distance to the *closest* datapoint from the demonstration. Compared to the baselines shown in the results section, this would be a fairer comparison because (1) unlike D4PG we also have access to information from the demonstrations and (2) no additional information is needed like the action information in D4PGfD and (3) we don't have the need for a curriculum. Nitpick (no influence on score): [1. Introduction] - I find the first sentence, \"One-shot imitation is a powerful way to show agents how to solve a task\" a bit confusing. I'd say one-shot imitation is a method, not a way to show how to solve a task. Maybe an introductory sentence like \"Expert demonstrations are a powerful way to show agents how to solve a task.\" works better? - Second sentence, the chosen example is \"manufacturing\" tasks - do you mean manipulation? When reading this, I had to think of car manufacturing - a task I could certainly not imitate with just a few demonstrations. - Add note that with \"unconditional policy\" you mean not conditioned on a demonstration. [2. MetaMimic] - [2.1] Third paragraph: write \"Figure 2, Algorithm 1\" or split the algorithm and figure up so you can refer to them separately. - [2.1] Last paragraph, second line: remove second \"to\"", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your feedback . We address your comments below . > The abstracts oversells the contribution a bit when saying that MetaMimic can learn `` policies for high-fidelity one-shot imitation of diverse novel skills '' . The setting that 's considered in the paper is that of a single task , but different demonstrations ( different humans from different starting points ) . This seems restrictive , and could have been motivated better . We fully agree . We should have been more specific , e.g . `` policies for high-fidelity one-shot imitation of diverse novel motions in block stacking '' . We plan to be much more specific in the writeup explaining the sources of variation in the task . > Experimental results are shown only for one task ; block stacking with a robot arm in simulation . This is correct . However there is significant variation in the motions , and since we are interested in high-fidelity imitation ( and not just imitation for the purposes of solving the task ) , we believe this is a significant source of variation . This is in line with our experiment showing that one needs a neural net with very large capacity ( the largest ever trained with RL ) to generalize to novel test demonstrations in high-fidelity imitation . > Might not be a good topical fit for ICLR , but more suited for a conference like CoRL or a workshop . The paper is very specific to imitation learning for a manipulation / control tasks , where we can ( 1 ) reset the environment to the exact starting position of the demonstrations , ( 2 ) the eucledian distance between states in the demonstration and visited by the policy is meaningful ( 3 ) we have access to both pixel observations and proprioceptive measurements . The proposed method is an elegant way to solve this , but it 's unclear how well it would perform on different types of control problems , or when we want to transfer policies between different ( but related ) tasks . We feel there are important questions of representation here that make the work interesting for ICLR . For instance , prior to this work we didn \u2019 t know we could train such massive neural nets with RL , and that increasing the size of these particular models is needed for generalization . We fully agree that ( 1 ) is an important limitation of the present approach . We were more interested in motion variation ( the style in which any user solves a specific task ) that on say object variation . Our focus is on high-fidelity imitation - if the focus is on imitation , then the techniques of pointed out in our reply to reviewer 4 are better choices ( eg the works of Silvio Savarese , Chelsea Finn and colleagues. > Where does the `` task stochasticity '' come from ? Only from the starting state , and from having different demonstrations ? Or is the transition function also stochastic ? Correct , the task stochasticity only comes from the starting state , and from having different demonstrations . We called it stochastic to distinguish it from environments like atari which are completely deterministic , i.e.without different starting conditions or different goals to achieve . > The learned policy is able to do one-shot imitation , i.e. , given a new demonstration ( of the same task ) the policy can follow this demonstration . Do I understand correct that this mean that there is * no * additional learning required at test time ? Yes , that is correct . At test time , the imitation policy is able to follow ( never-seen-before ) trajectories with no additional learning very closely . This is what we believe is a very cool result , especially because other groups have struggled to achieve this . > It is not immediately clear to me why the setting of a single task but new demonstrations is interesting . Could the authors comment on this ? One setting I could imagine is that the policy is trained in simulation , but then executed in the real-world , given a new demonstration . ( If that 's the main motivation though , then the experiments might have to support that this is possible - if no real-world robot is available , maybe the same simulator with a slightly different camera angle / light conditons or so . ) We would argue it is interesting because it is a skill that humans have , that is nontrivial for agents . Humans can observe a demonstration and imitate it very closely using just observations . In a factory , a manager might demonstrate to a new worker what to do with a set of objects , and then the new worker repeats the task with the same objects . Of course , humans can do this in a much more general way : e.g.from a third person perspective , and with abstract notions of perceptual similarity . Admittedly , we are all far from solving the full problem . Still , we think this is an interesting and useful step in that direction . One that demonstrates learning a complex mapping from perception to motor control through experience with an environment . We agree that the sim-to-real version of the problem is quite interesting ."}], "0": {"review_id": "HJMjW3RqtX-0", "review_text": "**Summary** The paper looks at the problem of one-shot imitation with high accuracy of imitation. The main contributions: 1. learning technique for high fidelity one-shot imitation at test time. 2. Policies to improve the expert performance through RL. The main improvements of this method is that demo action and rewards are not needed only state trajectories are sufficient. ** Comments ** - The novelty of algorithm block The main method is very similar to D4PG-fd. The off-policy method samples from a replay buffer which comprises of both the demos and the agent experience from the previous learner iterates. 1. From a technical perspective, what is the advantage of training an imitation learner from a memory buffer of the total experience? If the task reward is not accessed, then when the imitation learner is training, then the data should not be used for training the task policy learner. On the other hand if task reward is indeed available then what is the advantage of not using it. 2. A comparison with a BC policy to generate more experience data for the task policy agent/learning might also be useful. * Improved Comparisons - Compare with One-Shot Performance Since this is one of the main contributions, explicit comparison with other one-shot imitation papers needs to be quantified with a clearly defined metric for generalization. This comparison should be both for short-term tasks such as block pick and place (Finn et al, Pathak et al, Sermanet et al.) and also for long-term tasks as shown in (Duan et al. 2017 and also in Neural Task Programming/Neural Task Graph line of work from 2018) - Compare High-Fidelity Performance It is used as a differentiator of this method but without experimental evidence. The results showing imitation reward are insufficient. The metric should be independent of the method. An evaluation might compare trajectory tracking error: for objects, end-effector, and joint positions. This is available as privileged information since the setup is in a simulation. Furthermore, a comparison with a model-based trajectory tracking with a learned or fitted model of dynamics is also very useful. - Compare Policy Learning Performance In addition to D4PG variants, performance comparison with GAIL will ascertain that unconditional imitation is better than SoTA. * Tracking a reference (from either sim or demos) is a good idea that has been explored in sim2real literature[2,3] and imitation learning [4]. It is not by itself novel. The authors fail to acknowledge any work in this line as well as provide insight why is this good and when is this valid. For instance, with highly stochastic dynamics this may not work! - \"Diverse Novel Skills\" The experiments are limited to a rather singular pick and place task with a 3-step structured reward model. It is unfair to characterize this domain as very diverse or complex from a robotics perspective. More experiments on continuous control would help. - Bigger networks \"In fig. 3 we demonstrate that indeed a large ResNet34-style network (He et al., 2016) clearly outperforms\" -- but Fig 3 is a network architecture diagram. It is probably fig 6! - The authors are commended for presenting a broad overview of imitation based methods in table 2 ** Questions ** 1. How different if the imitation learner (trained with imitation reward) from a Behaviour Cloning Policy. 2. How is the local context considered in action generation in sec 2.1. The authors reset the simulation environment to o_1 = d_1. Then actions are generated with \\pi_{theta} (o_t, d_{t+1}). a. Is the environment reset every time step? b. If not how is the deviation of the trajectory handled over time? c. how is the time horizon for this open loop roll out chosen. 3. How is this different for a using a tracking based MPC with the same horizon? The cost can be set the same the similarity metric between states. 4. The architecture uses a deep but simplistic model. When the major attribution of the model success is to state similarity -- especially image similarity -- why did the authors not use image comparators something like the Siamese model? Suggestion: The whole set of experiments are in a simulation. The authors go above and beyond in using Mitsuba for rendering images. But the images used are Mujoco rendered default. It would nice if the authors were more forthcoming about this. All image captions should clearly state -- Simulated robot results, show images used for agent training. The Mitsuba renders are only used for images but nowhere in the algorithm. So why do this at all, and if it has to be used please do it with a disclaimer. Right now this detail is rather buried in the text. References: 1. Neural Task Programming, Xu et al. 2018 (https://arxiv.org/abs/1710.01813) 2. Preparing for the Unknown: Learning a Universal Policy with Online System Identification (https://arxiv.org/abs/1702.02453) 3. Adapt: zero-shot adaptive policy transfer for stochastic dynamical systems (https://arxiv.org/abs/1707.04674) 4. A survey of robot learning from demonstration, Argall et al. 2009 ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for detailed and extensive feedback . In addition to the contribution pointed out above , we would like to emphasize that this work demonstrates that it is possible to train massive deep neural networks ( larger than any previous attempt by at least an order of magnitude ) for RL . Moreover , the paper shows through ablations that such architectures are essential to achieve good generalization in one-shot imitation . Smaller networks fail to generalize . We feel the problem of generalization in one-shot learning is central to AI , and as such we believe this paper presents important results showing how to advance this research frontier . A few years ago , we certainly did not know whether RL , with its considerable variance , would allow us to train such large policies . We also did not know whether big nets were necessary at all in control tasks , as pointed out by Emo Todorov and colleagues at the previous NIPS . This paper provides empirical evidence and answers to these important questions . > The main method is very similar to D4PG-fd . The off-policy method samples from a replay buffer which comprises of both the demos and the agent experience from the previous learner iterates . For clarification , the D4PGfd algorithm was introduced in this paper , with the previous existing work -- - the DDPGfg of Vecerik et al ( 2017 ) -- - missing the distributed and distributional aspects of the policy optimizer . Additionally , the D4PGfd method requires actions , but in contrast MetaMimic does not need access to actions as you note above . Moreover , MetaMimic does two things ( i ) one-shot high-fidelity imitation and ( ii ) task policy learning . The D4PGfd method only applies to task policy learning ( ii ) . That is , it is missing an important core feature of MetaMimic . > 1.From a technical perspective , what is the advantage of training an imitation learner from a memory buffer of the total experience ? If the task reward is not accessed , then when the imitation learner is training , then the data should not be used for training the task policy learner . On the other hand if task reward is indeed available then what is the advantage of not using it . Excellent question . The purpose of MetaMimic is twofold . The first goal is to deploy policies that users ( say someone at a factory ) can easily adapt , via demonstrations , to solve new tasks . Moreover , the case studied in this paper aims to meet the need for imitating the user observation trajectory with high-fidelity ( eg in high precision engineering or surgery ) . That is , it is not only important to accomplish the goal , but also it is important to achieve this in a very precise and specific manner . The second purpose of MetaMimic is to act as a general task policy learner by capitalizing on demonstrations of observations and rewards . Here , the process of following the demonstrations should be understood as an auxiliary task to address the problem of exploration . The final goal is a task policy . In this sense , the closest competitor to MetaMimic is DDPG-fd , but as pointed out , MetaMimic works from observations while DDPG-fd requires additional access to actions . Interestingly , our results in Figure 8 , using our proposed D4PG-fd method , show that both methods perform similarly , despite MetaMimic requiring less information . We feel much of the confusion we \u2019 ve created comes from the fact that we are proposing a method that does two things . It can be useful to do ( i ) only , ( ii ) only or both ( i ) and ( ii ) . It really depends on the deployment case . > 2.A comparison with a BC policy to generate more experience data for the task policy agent/learning might also be useful . This may work quite well . However , BC requires expert actions while our method does not . We already provide a strong baseline for training the task policy with access to expert actions , D4PGfD ."}, "1": {"review_id": "HJMjW3RqtX-1", "review_text": "Summary This work porposes a approach for one-shot imitation with high accuracy, called \"high fidelity imitation learning\" by the authors. Furthermore, the work addresses the common problem of exploration in imitation learning, which would help to rescue from off-policy states. Review In my opinion, the main claims of this paper are not validated sufficiently in the experiments. I would expect the experiments to be designed specifically to support the claims made, but little evidence is provided: - The authors claim that the method allows one-shot generalization to an unknown trajectory. To test this hypothesis the authors only provide experiments of generalization towards trajectories of a different demonstrator on the same task of stacking cubes. I would expect experiments with truly different trajectories on a different task than stacking cubes to test the hypothesis of one-shot imitation. Until then I see no evidence for a \"one-shot\" imitation capability of the proposed method. - That storing the trajectories of early training can act as replacement for exploration as rescue from off-policy states: This is never experimentally validated. This hypothesis could easiliy be validated with an ablation study, were the results of early would not be added to the replay buffer. - High fidelity imitation: In the caption of Figure 7 the authors note that the unconditional task policy is able to outperform the demonstration videos. Thus the trajectories of the unconditional task policy allow a higher reward then the demonstrations. Could the authors please comment on how the method still achieves high fidelity imitation even when the results of the unconditional task policy are added to the replay buffer? In prinicipal these trajectories allow a higher reward than the demonstration trajectories that should be imitated. Mainly due to the missing experimental validation of the claims made I recommend to reject the paper.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for taking the time to provide feedback on the paper . We have taken great care to ensure our claims are directly supported by our experiments . We will address the two claims your refer to below . > The authors claim that the method allows one-shot generalization to an unknown trajectory . To test this hypothesis the authors only provide experiments of generalization towards trajectories of a different demonstrator on the same task of stacking cubes . I would expect experiments with truly different trajectories on a different task than stacking cubes to test the hypothesis of one-shot imitation . Until then I see no evidence for a `` one-shot '' imitation capability of the proposed method . There is a question of terminology here , and we agree that we need to address this more precisely in the paper . Most papers on one shot imitation sample a task t~p ( t ) and conditional on this sample a demonstration d~p ( d|t ) . In this setting , accomplishing t is what matters and significant deviations in the demonstration d are tolerated . In our work , we are sampling d~p ( d ) , and for us it is important to minimize deviations in d ( i.e.we want high-fidelity ) . Other one-shot imitation learning methods mostly focus on object diversity , e.g.pushing unseen objects or placing unseen objects ( see eg the excellent website of Yu and Finn : https : //bair.berkeley.edu/blog/2018/06/28/daml/ ) We instead chose a difficult control task , block stacking , which allows for many diverse ways of solving the task . We focused on demonstration diversity , following a distinctly different trajectory to solve a new task instance . While generalizing to different colours and objects is difficult , generalizing to different motions in one shot imitation is equally hard . As pointed out by Yu , Finn et al ( 2018 ) \u201c While our work enables one-shot learning for manipulating new objects from one video of a human , our current experiments do not yet demonstrate the ability to learn entirely new motions in one shot \u201d . Admittedly , all the different sources of variation are important and we need to make progress in all of them . Our work clearly does not address object variety , and we do need to do this in the future . > That storing the trajectories of early training can act as replacement for exploration as rescue from off-policy states : This is never experimentally validated . This hypothesis could easiliy be validated with an ablation study , were the results of early would not be added to the replay buffer . Actually , this is experimentally validated in Figure 8 . Using D4PG to train the task policy , is equivalent to using our method without adding experiences from the imitation policy to the replay memory . We believe this is strong evidence our method overcomes the exploration problem , because the same RL method is used , with the same hyper-parameters , same number of actors etc , but now the transitions in the replay are more likely to see task reward . We will add a note to the paper to ensure this is clear . > High fidelity imitation : In the caption of Figure 7 the authors note that the unconditional task policy is able to outperform the demonstration videos . Thus the trajectories of the unconditional task policy allow a higher reward then the demonstrations . Could the authors please comment on how the method still achieves high fidelity imitation even when the results of the unconditional task policy are added to the replay buffer ? In prinicipal these trajectories allow a higher reward than the demonstration trajectories that should be imitated . The task policy is able to achieve higher task reward , but at the moment we do n't calculate or store its imitation reward . This is illustrated in Figure 1 . The imitation policy is trying to maximize imitation reward , as a result the task policy trajectories do not interfere with the imitation policy . > Mainly due to the missing experimental validation of the claims made I recommend to reject the paper . We hope we have made clear how our claims are supported by our experiments , and that you would reconsider your evaluation . Regardless , thank you for the feedback . It has been very valuable ."}, "2": {"review_id": "HJMjW3RqtX-2", "review_text": "This paper presents an RL method for learning from video demonstration without access to expert actions. The agent first learn to imitate the expert demonstration (observed image sequence and proprioceptive information) by producing a sequence of actions that will lead to the similar observations (require a renderer that takes actions and outputs images). The imitation loss is a similarity metric. Next, the agent explores the environment with both the imitation policy and task policy being learned; an off-policy RL algorithm D4PG is used for policy learning. Experiments are conducted on a simulated robot block stacking task. The paper is really clearly written, but presenting the approach as \"high-fidelity\", \"one-shot\" learning is a bit confusing. First, it's not clear what's the motivation for high-fidelity. To me this is an artifact due to having to imitate the visual observation instead of the actions, which is a legitimate constraint, but not the original goal. Second, the one-shot learning setting consists of training on a set of stochastic demonstrations and testing on another set collected from a different person; both for the same task. Usually one-shot learning tests on slightly different tasks or environments, whereas here the goal is to generalize to novel demonstrations. It's not clear why do we care imitation per se in addition to the task reward. What I find interesting is the proposed approach for learning for video demonstration without action labels. Currently this requires an executor to render the actions to images, what if we don't have such an executor or only have a noisy / approximate renderer? In the real world it's probably hard to find a good renderer, it would be interesting to see how this constraint can be relaxed. Questions: - While the authors have shown the average rewards of the two sets are different, I wonder what's the variance of each person's demonstration. - In Fig 5, on the validation set, in terms of imitation loss there aren't that much difference between the policies, but in terms of task reward, the 'red' policy goes to zero while others policies' rewards are still similar. Any intuition for why seemingly okay imitation doesn't translate to task reward? Overall, I enjoyed reading the paper and the experiments are comprehensive. The current presentation angle seems a bit off though.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable feedback . We will address some of your comments and questions below . > The paper is really clearly written , but presenting the approach as `` high-fidelity '' , `` one-shot '' learning is a bit confusing . First , it 's not clear what 's the motivation for high-fidelity . To me this is an artifact due to having to imitate the visual observation instead of the actions , which is a legitimate constraint , but not the original goal . Second , the one-shot learning setting consists of training on a set of stochastic demonstrations and testing on another set collected from a different person ; both for the same task . Usually one-shot learning tests on slightly different tasks or environments , whereas here the goal is to generalize to novel demonstrations . It 's not clear why do we care imitation per se in addition to the task reward . Given the review scores , we can only agree with you that the paper is somewhat confusing and we have failed to motivate high-fidelity imitation properly . We think that what makes the paper confusing is that as it stands it tells two stories ( Hi-Fi imitation and task policies ) . These two stories are fundamentally linked , however we admit the presentation did not make these links clear . We will try to address this , but we very much look forward to any advice on how to change the presentation to make it more understandable . We used to the terminology One-Shot High-Fidelity Imitation to clarify both how our method works , and how it differs from existing methods in the space . First , high-fidelity is about mimicking the trajectory precisely . In precision engineering or surgery , where we don \u2019 t want the actuator doing anything other than what was demonstrated , this seems like a valuable idea . Existing few-shot imitation works focus on solving tasks , but not on following the trajectory precisely ( see for example https : //bair.berkeley.edu/blog/2018/06/28/daml/ ) . In relation to this work , Yu and Finn point out : \u201c While our work enables one-shot learning for manipulating new objects from one video of a human , our current experiments do not yet demonstrate the ability to learn entirely new motions in one shot \u201d . The latter is what is demonstrated in our generalization experiments . That is , given a new demonstration motion , our policy is able to follow it closely as shown in Figure 4 . Second , we use the phrase `` one-shot '' to distinguish our method from other tracking based methods , which can require many thousands of environment interactions to learn to track a single trajectory . In contrast , our method requires no additional environment interactions to track a novel trajectory . It achieves this in the same way one-shot methods do , by learning a policy that works well across a large dataset of `` tasks '' where each `` task '' is a demonstration . > What I find interesting is the proposed approach for learning for video demonstration without action labels . Currently this requires an executor to render the actions to images , what if we do n't have such an executor or only have a noisy / approximate renderer ? In the real world it 's probably hard to find a good renderer , it would be interesting to see how this constraint can be relaxed . We agree that relaxing the constraint of an exact environmental renderer is an interesting research direction . This would be helpful for our method , as well as many other RL based methods . But we think it is beyond the scope of this paper . > While the authors have shown the average rewards of the two sets are different , I wonder what 's the variance of each person 's demonstration . There is notable variance between the two demonstrators , and between each demonstration . We will provide some additional examples in the appendix . > In Fig 5 , on the validation set , in terms of imitation loss there are n't that much difference between the policies , but in terms of task reward , the 'red ' policy goes to zero while others policies ' rewards are still similar . Any intuition for why seemingly okay imitation does n't translate to task reward ? Yes , we have noticed two types of behavior that have reasonably high imitation rewards , but do not successfully complete the task : ( i ) the policy closely imitates the arm but ignores the block entirely , ( ii ) the policy successfully imitates both the arm and block position in the beginning of the trajectory , but fails to place the block on a stable position during the stack . As the imitation reward increases we see these behaviors less . > Overall , I enjoyed reading the paper and the experiments are comprehensive . The current presentation angle seems a bit off though . Thanks ! We are really glad you enjoyed the paper , and the experiments , and hope we can align the presentation a bit better ."}, "3": {"review_id": "HJMjW3RqtX-3", "review_text": " Summary: This paper proposes MetaMimic, an algorithm that does the following: (i) Learn to imitate with high-fidelity with one-shot. The setting is that we have access to several demonstrations (only states, no actions) of the same task. During training, we have pixel observations plus proprioceptive measurements). At test time, the learned policy can imitate a single new demonstration (consisting of only pixel observations) of the same task. (ii) When given access to rewards, the policy can exceed the human demonstrator by augmenting its experience replay buffer with the experience gained while learning (i). Therefore, even in a setting with sparse rewards and no access to expert actions (only states), the policy can learn to solve the task. Overall Evaluation: This is a good paper. In my opinion however, it does not pass the bar for ICLR. Pros: - The paper is well written. The contributions are clearly listed, the methods section is easy to follow and the authors explain the choices they make. The illustrations are clear and intuitive. - The overview of hyperparameter choice and tuning / importance factor in the Appendix is useful. - Interesting pipeline of learning policies that can use demonstrations without actions. - The results on the simulated robot arm (block stacking task with two blocks) are good. Cons: - The abstracts oversells the contribution a bit when saying that MetaMimic can learn \"policies for high-fidelity one-shot imitation of diverse novel skills\". The setting that's considered in the paper is that of a single task, but different demonstrations (different humans from different starting points). This seems restrictive, and could have been motivated better. - Experimental results are shown only for one task; block stacking with a robot arm in simulation. - Might not be a good topical fit for ICLR, but more suited for a conference like CoRL or a workshop. The paper is very specific to imitation learning for a manipulation / control tasks, where we can (1) reset the environment to the exact starting position of the demonstrations, (2) the eucledian distance between states in the demonstration and visited by the policy is meaningful (3) we have access to both pixel observations and proprioceptive measurements. The proposed method is an elegant way to solve this, but it's unclear how well it would perform on different types of control problems, or when we want to transfer policies between different (but related) tasks. Questions: - Where does the \"task stochasticity\" come from? Only from the starting state, and from having different demonstrations? Or is the transition function also stochastic? - The learned policy is able to do one-shot imitation, i.e., given a new demonstration (of the same task) the policy can follow this demonstration. Do I understand correct that this mean that there is *no* additional learning required at test time? - It is not immediately clear to me why the setting of a single task but new demonstrations is interesting. Could the authors comment on this? One setting I could imagine is that the policy is trained in simulation, but then executed in the real-world, given a new demonstration. (If that's the main motivation though, then the experiments might have to support that this is possible - if no real-world robot is available, maybe the same simulator with a slightly different camera angle / light conditons or so.) - The x-axis in the figures says \"time (hours)\" - is that computation time or simulated time? Other Comments: - In 3.2, I would be interested in seeing the following baseline comparison: Learn the test task from scratch using the one available demonstration, with the RL procedure (Equation 2, but possibly without the second term to make it fair). In Figure 5, we can see that the performance on the training tasks is much better when training on only 10 tasks, compared to 500. Then why not overfit to a single task, if that's what we're interested in? - An interesting baseline for 3.3 might be an RL algorithm with shaped rewards: using an additional reward term that is the eucledian distance to the *closest* datapoint from the demonstration. Compared to the baselines shown in the results section, this would be a fairer comparison because (1) unlike D4PG we also have access to information from the demonstrations and (2) no additional information is needed like the action information in D4PGfD and (3) we don't have the need for a curriculum. Nitpick (no influence on score): [1. Introduction] - I find the first sentence, \"One-shot imitation is a powerful way to show agents how to solve a task\" a bit confusing. I'd say one-shot imitation is a method, not a way to show how to solve a task. Maybe an introductory sentence like \"Expert demonstrations are a powerful way to show agents how to solve a task.\" works better? - Second sentence, the chosen example is \"manufacturing\" tasks - do you mean manipulation? When reading this, I had to think of car manufacturing - a task I could certainly not imitate with just a few demonstrations. - Add note that with \"unconditional policy\" you mean not conditioned on a demonstration. [2. MetaMimic] - [2.1] Third paragraph: write \"Figure 2, Algorithm 1\" or split the algorithm and figure up so you can refer to them separately. - [2.1] Last paragraph, second line: remove second \"to\"", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your feedback . We address your comments below . > The abstracts oversells the contribution a bit when saying that MetaMimic can learn `` policies for high-fidelity one-shot imitation of diverse novel skills '' . The setting that 's considered in the paper is that of a single task , but different demonstrations ( different humans from different starting points ) . This seems restrictive , and could have been motivated better . We fully agree . We should have been more specific , e.g . `` policies for high-fidelity one-shot imitation of diverse novel motions in block stacking '' . We plan to be much more specific in the writeup explaining the sources of variation in the task . > Experimental results are shown only for one task ; block stacking with a robot arm in simulation . This is correct . However there is significant variation in the motions , and since we are interested in high-fidelity imitation ( and not just imitation for the purposes of solving the task ) , we believe this is a significant source of variation . This is in line with our experiment showing that one needs a neural net with very large capacity ( the largest ever trained with RL ) to generalize to novel test demonstrations in high-fidelity imitation . > Might not be a good topical fit for ICLR , but more suited for a conference like CoRL or a workshop . The paper is very specific to imitation learning for a manipulation / control tasks , where we can ( 1 ) reset the environment to the exact starting position of the demonstrations , ( 2 ) the eucledian distance between states in the demonstration and visited by the policy is meaningful ( 3 ) we have access to both pixel observations and proprioceptive measurements . The proposed method is an elegant way to solve this , but it 's unclear how well it would perform on different types of control problems , or when we want to transfer policies between different ( but related ) tasks . We feel there are important questions of representation here that make the work interesting for ICLR . For instance , prior to this work we didn \u2019 t know we could train such massive neural nets with RL , and that increasing the size of these particular models is needed for generalization . We fully agree that ( 1 ) is an important limitation of the present approach . We were more interested in motion variation ( the style in which any user solves a specific task ) that on say object variation . Our focus is on high-fidelity imitation - if the focus is on imitation , then the techniques of pointed out in our reply to reviewer 4 are better choices ( eg the works of Silvio Savarese , Chelsea Finn and colleagues. > Where does the `` task stochasticity '' come from ? Only from the starting state , and from having different demonstrations ? Or is the transition function also stochastic ? Correct , the task stochasticity only comes from the starting state , and from having different demonstrations . We called it stochastic to distinguish it from environments like atari which are completely deterministic , i.e.without different starting conditions or different goals to achieve . > The learned policy is able to do one-shot imitation , i.e. , given a new demonstration ( of the same task ) the policy can follow this demonstration . Do I understand correct that this mean that there is * no * additional learning required at test time ? Yes , that is correct . At test time , the imitation policy is able to follow ( never-seen-before ) trajectories with no additional learning very closely . This is what we believe is a very cool result , especially because other groups have struggled to achieve this . > It is not immediately clear to me why the setting of a single task but new demonstrations is interesting . Could the authors comment on this ? One setting I could imagine is that the policy is trained in simulation , but then executed in the real-world , given a new demonstration . ( If that 's the main motivation though , then the experiments might have to support that this is possible - if no real-world robot is available , maybe the same simulator with a slightly different camera angle / light conditons or so . ) We would argue it is interesting because it is a skill that humans have , that is nontrivial for agents . Humans can observe a demonstration and imitate it very closely using just observations . In a factory , a manager might demonstrate to a new worker what to do with a set of objects , and then the new worker repeats the task with the same objects . Of course , humans can do this in a much more general way : e.g.from a third person perspective , and with abstract notions of perceptual similarity . Admittedly , we are all far from solving the full problem . Still , we think this is an interesting and useful step in that direction . One that demonstrates learning a complex mapping from perception to motor control through experience with an environment . We agree that the sim-to-real version of the problem is quite interesting ."}}