{"year": "2020", "forum": "BJxeHyrKPB", "title": "RATE-DISTORTION OPTIMIZATION GUIDED AUTOENCODER FOR GENERATIVE APPROACH", "decision": "Reject", "meta_review": "Agreement by the reviewers: although the idea is good, the paper is very hard to read and not accurately enough formulated to merit publication.  \n\nThis can be repaired, and the authors should try again after a thorough revision and rewrite.", "reviews": [{"review_id": "BJxeHyrKPB-0", "review_text": "The paper propose a noisy autoencoder that considers the jacobian between data and latent spaces to match the corresponding densities. This idea has already been proposed elsewhere, and here it is applied to autoencoders. Overall I had hard time understanding the paper, the motivation, the main contribution or the claim, the model definition and the jacobian method. The paper is poorly written, with lots of issues in math notation and poor motivation and explication of what the sections are introducing, and what parts of the presentation is novel and what is already known. Lots of the math machinery is too vague to follow. The distribution p(z) is unclear, and whether z is random variable or not. It seems that \u201cz\" is a non-random variable, and then adding noise \\eps makes it stochastic. However, then p(z) without \\eps does not make any sense since z is not random. It seems that p(z) is maybe a prior distribution instead (or maybe the variational posterior?), but then adding \\eps noise to an already stochastic variable is strange. Overall I have hard time understanding the motivation of the two discrepancies in eq 4, what is the point of adding more noise to \u201cz\u201d? This seems some kind of noisy or perhaps robust AE variant, but the paper does not explicate this. I have hard time following the eqs 8-15. I am not convinced of the orthogonality argument, and I fail to see what this section tries to show or demonstrate. It seems that eq 14 is the final result, but its difficult to follow due to most terms in eq 14 being undefined. Optimizing eq 14 seems trivial since we can always match pz and pz_\\varphi easily with neural networks, or similarly the two x-distributions. In the experiment 4.1. the proposed method seems to achieve matching densities, although the distributions are wrongly normalized. How does the density matching improve? All three methods seem to have equally good scatters. The benchmarks on table 1 show clear improvement with the method. The face experiment is unconvincing since the VAE spreads variance across all latent dimensions while RADO seems to compress them to just first 20 or so. If one would visualise the z_100 there would be no variance in RADO and possibly some variance in VAE. The paper also should compare their model to simple MNIST/VAE to highlight what problems are there in standard approaches (such as VAE), and how does the proposed method alleviate them. Overall the paper is poorly presented and difficult to follow. Despite this the method does seem to work remarkably well, and the Jacobian idea is clearly very promising. Nevertheless in its current form the paper is badly premature for ICLR, and needs a lot more work and polish to be made understandable for wider ML audience. Minor comments o Px(x), x1, x2 are probably missing subscripts o The point of eq 5 is unclear, it seems unnecessary. It also does not contain h(), which is claimed after eq6 o The log pz(z) in eq 4 is not entropy o eq 8 is unclear, is the dx a derivative, distance or change? o the $^t$ prefix notation is confusing, what does it mean? o what is the \\sim and line notation in eq 5? o what are the products in eq9, are these inner products? o in eq 13 pz, pxd or hat(pxd) have not been introduced or defined ", "rating": "1: Reject", "reply_text": "> > Regarding experimental results > > In the experiment 4.1. the proposed method seems to achieve matching densities , although the distributions are wrongly normalized . How does the density matching improve ? All three methods seem to have equally good scatters . As written in the result section , even though the baseline method ( DAGMM ) also captured good scatter , the density is not estimated adequately . Figure 5 ( in revised version ) depicts plot of Px ( x ) ( x-axis ) and Pz\u03c8 ( z ) ( y-axis ) . It is obvious that we can see the proportionality between Px ( x ) and Pz\u03c8 ( z ) , while we can not see the tendency in the baseline . This is also quantitatively evaluated . The correlation coefficients are 0.691 ( baseline ) vs 0.997 or more ( ours ) . ( Originally we showed residual of linear regression though , for intuitive understanding , we replaced it by correlation coefficients . ) For the easy following , we also updated the caption and description in the main text . > > The face experiment is unconvincing since the VAE spreads variance across all latent dimensions while RADO seems to compress them to just first 20 or so . If one would visualize the z_100 there would be no variance in RADO and possibly some variance in VAE . Yes , what you said happens . This means that our model correctly works as PCA as in theory . The point is the variance of latent variables directly correlated This is the experiment to confirm that latent variables of our model work as PCA components . To make it further explicit , we added this statement at the beginning of the section . Let \u2019 s say if we want to find an important latent variable in terms of the influence on the metrics function , what should we do ? In our model , we can find that latent variable easily , since the variance of z is directly related to the visual . In ( beta- ) VAE , as we described , variance and impact to visual are uncorrelated . Thus , for example , we need to visualize all latent variables to find the important ones . Moreover , even if you come up with you run PCA for the latent variables of VAE , you need to set the number of PCA components and may struggle to decide how many components are appropriate . In our model , it is automatically optimized in terms of minimizing entropy . Consequently , the latent variable in our model is quantitatively understandable . We believe this character is helpful for the interpretation of latent variables and meta-prior of data . > > The paper also should compare their model to simple MNIST/VAE to highlight what problems are there in standard approaches ( such as VAE ) , and how does the proposed method alleviate them . An Experiment with MNIST could be worth-doing though , we demonstrated the above characteristic clearly with CelebA dataset . In terms of the interpretation of latent variables , some of the standard approaches are visualizing or evaluate independencies of variables as in ( Lopez et al. , 2018 ; Chen et al. , 2018 ; Kim & Mnih , 2018 ; Chen et al. , 2016 ) . They do not directly evaluate the importance of latent variables on the metric function ( such as MSE or SSIM ) . In our method , this can be quantitatively measured like PCA . Note that , we do not intend to claim this way of analysis is always better than previous ways . We argue that making use of PCA like analysis as an option and incorporating them will promote further interpretation of latent variables . > > For minor comments o The point of eq 5 is unclear , it seems unnecessary . It also does not contain h ( ) , which is claimed after eq6 Equation 5 ( 6 in the revised version ) is a condition for function D ( \u30fb , \u30fb ) . As long as D ( \u30fb , \u30fb ) can be approximated as eq 5 , it can be applied . We added this explanation . o The log pz ( z ) in eq 4 is not entropy We fixed it . o eq 8 is unclear , is the dx a derivative , distance or change ? It is derivative . We added the notation . o the t prefix notation is confusing , what does it mean ? The t denotes the transpose of a matrix . We added the notation . o what is the \\sim and line notation in eq 5 \\simeq denotes approximation . o what are the products in eq9 , are these inner products ? It is a multiplication . We removed dots . oi n eq 13 pz , pxd or hat ( pxd ) have not been introduced or defined hat ( pxd ) is defined as \u201c let hat ( pxd ) be estimated probability of xd. \u201d We added definition pz , pxd as the true PDF of z and xd"}, {"review_id": "BJxeHyrKPB-1", "review_text": "This paper aims to obtain latent representation of data such that probability density for the real space can be calculated correctly from that in the latent space. The authors optimize a loss function that has components related to parametric probabilistic distribution and auto encoder simultaneously. While this might be an important problem (I am not sure), the paper is not written and organized well which makes a through evaluation very difficult. I provide below some of the problems with this the paper: Why the introduced method is better than VAE as a generative model for capturing the latest representation is not explained well. It is not also used as a baseline in most of the experiments. The motivation for having the third term in Equation (4) needs to be explained. Also what is h() in the second term. The authors only describe briefly both terms together after they used it here but failed to describe what each term is. Why there is an h for the second term but not for the third term. h() becomes more clear much later in the paper but when it is used the first time, it not defined. I believe A in Equation (5) should be also positive-definite. What is L(x) in Equation (8). It needs to be defined. Experiments: 1- It is useful to also plot the original data in space s to see how the results in Figure 2 make sense. 2- Figure 3 is not clear. 3- In the Anomaly detection experiments, the authors make two assumptions that usually do not exist in real-worlds: (1) they assume that they have access to training set that only contains normal cases. (2) They assume that they know the correct rate of anomaly. I think both these assumptions are very restrictive and unreal. While these assumptions are used for all the comparing methods, it is not obvious how different algorithms behave in real scenario. 4- Figure 4 and what it represents is not clear. Writing Problems: 1- In the text of paragraph before Figure 1, Eq. (5) in \u201cin the second term of Eq. (5)\u201d is a typo and should be Eq. (4). 2- In the paragraph before Figure 1, the following sentence is not complete: \u201cThen, averaging Eq. (4) according to distribution, x~P_x(x) and epsilon~ P(epsilon).\u201d 3- Section 4.2.1: \u201cthere is a difference is PDF \u2192 \u201cthere is a difference in PDF\u201d", "rating": "3: Weak Reject", "reply_text": "Regarding experiments : > > 1- It is useful to also plot the original data in space s to see how the results in Figure 2 make sense . Thanks for your point . We added the plot of the original data source . > > 2- Figure 3 is not clear . Figure 3 ( Figure 5 in revised version ) depicts plot of Px ( x ) ( x-axis ) and Pz_\\psi ( z ) ( y-axis ) . A linear plot means that the probability density of Px ( x ) is tidily mapped into the latent space . Thanks to this property , Px ( x ) can be estimated by Pz_\\psi ( z ) in our model . It is obvious that DAGMM does not have this trait . This is also quantitatively evaluated . The correlation coefficients are 0.691 ( baseline ) vs 0.997 or more ( ours ) . For the easy following , we revised the caption and description in the main text . > > 3- In the Anomaly detection experiments , the authors make two assumptions that usually do not exist in real-worlds : ( 1 ) they assume that they have access to a training set that only contains normal cases . ( 2 ) They assume that they know the correct rate of anomaly . I think both these assumptions are very restrictive and unreal . While these assumptions are used for all the comparing methods , it is not obvious how different algorithms behave in a real scenario . I understand that there is an unrealistic assumption , but this setting is established and widely admitted in this anomaly detection task . Regardless of this assumption is realistic or not , density estimation remains a critical issue , and better estimation provides better performance . Although investigating the performance in a truly real scenario might be future work , we argue this point is not a defect to show the validity of our method . > > 4- Figure 4 and what it represents is not clear . This is caused because we could not tell you the purpose of this experiment sufficiently . This is an experiment to show an important property of our model : our model behaves as PCA , where the energy of acquired latent space is concentrated on several principal components and the influence of each component can be evaluated quantitatively The two on the left of Fig.4 ( Fig.6 in the revised version ) is the variance of the latent space . Since our model works as PCA , the variance is concentrated in a few dimensions . Two on the right shows that the influence of minute displacement of each z to the real image is the almost constant in our model while it is varied in beta-VAE . Thus , we can evaluate the importance of latent variables by variance like PCA . We added the caption and enhanced the purpose of the experiment . > > Regarding writing issue Thank you for pointing . We fixed them ."}, {"review_id": "BJxeHyrKPB-2", "review_text": "Summary of the paper: The authors propose a latent variable model RaDOGAGA, a generative autoencoding model. The model is trained via a tradeoff between distortion (the reconstruction error) and the rate (the capacity of the latent space, measured by entropy). The paper provides an analysis of theoretical properties of their approach, and presents supporting experimental results. Review tl;dr: weak reject, for three main reasons: (i) While the existing literature around VAEs, beta-VAEs, and Rate-Distortion theory is mentioned in the related work, the connections are not nearly discussed sufficiently. (ii) On top of (i), the derivation of their loss function and architecture is not sufficiently motivated. This is in astonishing contrast to 1.5 pages of main text and 8 pages of (much appreciated!) analysis of properties. (iii) Given the paper is clearly related to existing approaches in the literature, the experiments would require a much more careful comparison to existing models. It remains unclear why an interested user should favor your model over conceptually simpler generative models with fewer hyperparameters. Detailed review: Nota bene: This review is a late reassignment. While I reviewed the paper to the best of my ability, time constraints did not allow me to review parts of the paper in depth. I am open to reassess my review during the second stage. Connection to prior art: As a probabilistic, neural autoencoding model, the connections to the family of VAE models are obvious. The loss function (eq. (4)) still looks very much like the ELBO, where the typical conditional log-likelihood was split into two distortion terms. How is this different from e.g. a beta-VAE? Particularly, what is the connection between the rate-distortion analysis of beta-VAE by Alemi et al. and yours? These things need to be discussed explicitly, with more than a sentence or two in the related work section. A lesser, but still important omission in your discussion of prior work: The Jacobian of the generator has also been studied, even for the VAE, cf. e.g. [1]. I believe this deserves more attention in your assessment of prior art. Motivation: You use two distortion terms: actual sample vs. undistorted reconstruction. Why is that? What is the interpretation of the multipliers? How do I choose them? Why is a large part of your architecture (the pipeline from x to \\hat(x)) actually deterministic? Why are you using the entropy of the prior over the latents, rather than the KL divergence between encoder and a prior? I think an interested reader could learn much more from your paper if you discussed your model embedded in th related work rather than in isolation. Theory: Due to aforementioned time constraints, I was not able to review the extensive theoretical analysis in depth. Still, I would strongly recommend structuring the respective sections more clearly. Separate model and architecture description from the theoretical analysis; precisely formulate your claims. In particular, state your assumptions clearly. For instance, you assume \"that each function's parameter is rich enough to fit ideally\" (and similar e.g. in Appendix A). Does this only mean that the true distributions are part of the parametric family? What if this is not the case? Do your parameters need to be in the optimum for your analysis to hold true? Given that the full 20-page manuscript spends 10 pages on theory, I think this contribution is not given appropriate space in the main text. Experiments: There are three experiments: a simple 3D proof of concept; anomaly detection; analysis of the latent state in CelebA. As mentioned in my review of the methods section, I believe the approach to be very similar to established models. None of the experiments provides convincing evidence why I should prefer the new, arguably more complex model. For instance, I would have much preferred that you investigate properties of your model against alternatives over the anomaly detection experiments, which did not further my understanding of the proposed model. Summary: The paper tackles an important problem, namely the lack of control over the latent embedding in autoencoding generative models. I believe the author's contribution can be valuable, and I particularly appreciate the effort to investigate theoretical properties. As is, the case is not sufficiently convincing to be accepted, but I encourage the authors to improve the paper. Minor comments: 1. While I appreciate a pun, I would recommend to rename the model along with the acronym to a more concise name. 2. Please revise your notation and typsetting. Examples: x1 instead of x_1, f of f(\\cdot) instead of f(), \\log instead of log. 3. Introduce acronyms before using them (e.g. VAE, MSE, SSIM), even when they seem obvious to you. 4. Please carefully check the manuscript for typos, missing articles, missing spaces etc. 5. Your citations are inconsistent, in that they sometimes use first names, sometimes first name initials, and sometimes no first names. 6. To my knowledge, the term scale function does not have an obvious definition. I think you are simply referring to monotonically increasing functions. Please clarify! 7. Your figures should be understandable without too much context, they need more detailed captions. [1] http://proceedings.mlr.press/v84/chen18e.html", "rating": "3: Weak Reject", "reply_text": "> > Regarding the experimental result . First of all , actually , our model does not increase model complexity even though you concerned about this point . When we compared with our model and DAGMM ( baseline model ) , the number of network parameters is completely the same . We added this point explicitly . Nevertheless , our model provides a significant performance boost in the anomaly detection task . Experiment with toy data is executed to confirm our model \u2019 s property though , it also supports the result of anomaly detection . In DAGMM , the relation PDF of x and z is unclear . On the other hand , in our model , the PDFs of x and z are close to proportional . That means , our model can capture the probability of real data methodically in the latent space . This fact should be very intuitive to explain the performance boost in the anomaly detection task in which PDF estimation is a critical issue . Other comparison methods also essentially lead the disorder in the density estimation like DAGMM because the Jacobian is not controlled . In the analysis of the latent state in CelebA , we assume that since we could not tell you the purpose of the experiment enough , it was not convincing for you . This is an experiment to confirm that the latent variable in our model works as PCA components , and the influence of each component can be evaluated quantitatively as in theory while ( beta- ) VAE does not have this property . We revised this sections and captions for easy following . The two on the right of Fig.6 in the revised version show the scaling between the latent and metric dependent data space . ( c ) shows the scaling in VAE is anisometric , and ( d ) shows the scaling in ours is isometric . The two on the left of Fig.6 in the revised version is the variance of the latent space . Since the scaling of z in our model is isometric , the variance shows the importance of each latent variable like PCA . Consequently , we believe our experimental results demonstrate the validity of our method decently . PCA can simultaneously disentangle the data and estimate the importance of latent variables by variance . We believe this trait is very helpful to the interpretation of the latent variable of deep models . > > minor issues We fixed the minor issues you pointed ( we would appreciate if you could be indulgent of a bit long model acronym ) . We also promise to request a grammatical check by a native no later than the camera-ready version ."}], "0": {"review_id": "BJxeHyrKPB-0", "review_text": "The paper propose a noisy autoencoder that considers the jacobian between data and latent spaces to match the corresponding densities. This idea has already been proposed elsewhere, and here it is applied to autoencoders. Overall I had hard time understanding the paper, the motivation, the main contribution or the claim, the model definition and the jacobian method. The paper is poorly written, with lots of issues in math notation and poor motivation and explication of what the sections are introducing, and what parts of the presentation is novel and what is already known. Lots of the math machinery is too vague to follow. The distribution p(z) is unclear, and whether z is random variable or not. It seems that \u201cz\" is a non-random variable, and then adding noise \\eps makes it stochastic. However, then p(z) without \\eps does not make any sense since z is not random. It seems that p(z) is maybe a prior distribution instead (or maybe the variational posterior?), but then adding \\eps noise to an already stochastic variable is strange. Overall I have hard time understanding the motivation of the two discrepancies in eq 4, what is the point of adding more noise to \u201cz\u201d? This seems some kind of noisy or perhaps robust AE variant, but the paper does not explicate this. I have hard time following the eqs 8-15. I am not convinced of the orthogonality argument, and I fail to see what this section tries to show or demonstrate. It seems that eq 14 is the final result, but its difficult to follow due to most terms in eq 14 being undefined. Optimizing eq 14 seems trivial since we can always match pz and pz_\\varphi easily with neural networks, or similarly the two x-distributions. In the experiment 4.1. the proposed method seems to achieve matching densities, although the distributions are wrongly normalized. How does the density matching improve? All three methods seem to have equally good scatters. The benchmarks on table 1 show clear improvement with the method. The face experiment is unconvincing since the VAE spreads variance across all latent dimensions while RADO seems to compress them to just first 20 or so. If one would visualise the z_100 there would be no variance in RADO and possibly some variance in VAE. The paper also should compare their model to simple MNIST/VAE to highlight what problems are there in standard approaches (such as VAE), and how does the proposed method alleviate them. Overall the paper is poorly presented and difficult to follow. Despite this the method does seem to work remarkably well, and the Jacobian idea is clearly very promising. Nevertheless in its current form the paper is badly premature for ICLR, and needs a lot more work and polish to be made understandable for wider ML audience. Minor comments o Px(x), x1, x2 are probably missing subscripts o The point of eq 5 is unclear, it seems unnecessary. It also does not contain h(), which is claimed after eq6 o The log pz(z) in eq 4 is not entropy o eq 8 is unclear, is the dx a derivative, distance or change? o the $^t$ prefix notation is confusing, what does it mean? o what is the \\sim and line notation in eq 5? o what are the products in eq9, are these inner products? o in eq 13 pz, pxd or hat(pxd) have not been introduced or defined ", "rating": "1: Reject", "reply_text": "> > Regarding experimental results > > In the experiment 4.1. the proposed method seems to achieve matching densities , although the distributions are wrongly normalized . How does the density matching improve ? All three methods seem to have equally good scatters . As written in the result section , even though the baseline method ( DAGMM ) also captured good scatter , the density is not estimated adequately . Figure 5 ( in revised version ) depicts plot of Px ( x ) ( x-axis ) and Pz\u03c8 ( z ) ( y-axis ) . It is obvious that we can see the proportionality between Px ( x ) and Pz\u03c8 ( z ) , while we can not see the tendency in the baseline . This is also quantitatively evaluated . The correlation coefficients are 0.691 ( baseline ) vs 0.997 or more ( ours ) . ( Originally we showed residual of linear regression though , for intuitive understanding , we replaced it by correlation coefficients . ) For the easy following , we also updated the caption and description in the main text . > > The face experiment is unconvincing since the VAE spreads variance across all latent dimensions while RADO seems to compress them to just first 20 or so . If one would visualize the z_100 there would be no variance in RADO and possibly some variance in VAE . Yes , what you said happens . This means that our model correctly works as PCA as in theory . The point is the variance of latent variables directly correlated This is the experiment to confirm that latent variables of our model work as PCA components . To make it further explicit , we added this statement at the beginning of the section . Let \u2019 s say if we want to find an important latent variable in terms of the influence on the metrics function , what should we do ? In our model , we can find that latent variable easily , since the variance of z is directly related to the visual . In ( beta- ) VAE , as we described , variance and impact to visual are uncorrelated . Thus , for example , we need to visualize all latent variables to find the important ones . Moreover , even if you come up with you run PCA for the latent variables of VAE , you need to set the number of PCA components and may struggle to decide how many components are appropriate . In our model , it is automatically optimized in terms of minimizing entropy . Consequently , the latent variable in our model is quantitatively understandable . We believe this character is helpful for the interpretation of latent variables and meta-prior of data . > > The paper also should compare their model to simple MNIST/VAE to highlight what problems are there in standard approaches ( such as VAE ) , and how does the proposed method alleviate them . An Experiment with MNIST could be worth-doing though , we demonstrated the above characteristic clearly with CelebA dataset . In terms of the interpretation of latent variables , some of the standard approaches are visualizing or evaluate independencies of variables as in ( Lopez et al. , 2018 ; Chen et al. , 2018 ; Kim & Mnih , 2018 ; Chen et al. , 2016 ) . They do not directly evaluate the importance of latent variables on the metric function ( such as MSE or SSIM ) . In our method , this can be quantitatively measured like PCA . Note that , we do not intend to claim this way of analysis is always better than previous ways . We argue that making use of PCA like analysis as an option and incorporating them will promote further interpretation of latent variables . > > For minor comments o The point of eq 5 is unclear , it seems unnecessary . It also does not contain h ( ) , which is claimed after eq6 Equation 5 ( 6 in the revised version ) is a condition for function D ( \u30fb , \u30fb ) . As long as D ( \u30fb , \u30fb ) can be approximated as eq 5 , it can be applied . We added this explanation . o The log pz ( z ) in eq 4 is not entropy We fixed it . o eq 8 is unclear , is the dx a derivative , distance or change ? It is derivative . We added the notation . o the t prefix notation is confusing , what does it mean ? The t denotes the transpose of a matrix . We added the notation . o what is the \\sim and line notation in eq 5 \\simeq denotes approximation . o what are the products in eq9 , are these inner products ? It is a multiplication . We removed dots . oi n eq 13 pz , pxd or hat ( pxd ) have not been introduced or defined hat ( pxd ) is defined as \u201c let hat ( pxd ) be estimated probability of xd. \u201d We added definition pz , pxd as the true PDF of z and xd"}, "1": {"review_id": "BJxeHyrKPB-1", "review_text": "This paper aims to obtain latent representation of data such that probability density for the real space can be calculated correctly from that in the latent space. The authors optimize a loss function that has components related to parametric probabilistic distribution and auto encoder simultaneously. While this might be an important problem (I am not sure), the paper is not written and organized well which makes a through evaluation very difficult. I provide below some of the problems with this the paper: Why the introduced method is better than VAE as a generative model for capturing the latest representation is not explained well. It is not also used as a baseline in most of the experiments. The motivation for having the third term in Equation (4) needs to be explained. Also what is h() in the second term. The authors only describe briefly both terms together after they used it here but failed to describe what each term is. Why there is an h for the second term but not for the third term. h() becomes more clear much later in the paper but when it is used the first time, it not defined. I believe A in Equation (5) should be also positive-definite. What is L(x) in Equation (8). It needs to be defined. Experiments: 1- It is useful to also plot the original data in space s to see how the results in Figure 2 make sense. 2- Figure 3 is not clear. 3- In the Anomaly detection experiments, the authors make two assumptions that usually do not exist in real-worlds: (1) they assume that they have access to training set that only contains normal cases. (2) They assume that they know the correct rate of anomaly. I think both these assumptions are very restrictive and unreal. While these assumptions are used for all the comparing methods, it is not obvious how different algorithms behave in real scenario. 4- Figure 4 and what it represents is not clear. Writing Problems: 1- In the text of paragraph before Figure 1, Eq. (5) in \u201cin the second term of Eq. (5)\u201d is a typo and should be Eq. (4). 2- In the paragraph before Figure 1, the following sentence is not complete: \u201cThen, averaging Eq. (4) according to distribution, x~P_x(x) and epsilon~ P(epsilon).\u201d 3- Section 4.2.1: \u201cthere is a difference is PDF \u2192 \u201cthere is a difference in PDF\u201d", "rating": "3: Weak Reject", "reply_text": "Regarding experiments : > > 1- It is useful to also plot the original data in space s to see how the results in Figure 2 make sense . Thanks for your point . We added the plot of the original data source . > > 2- Figure 3 is not clear . Figure 3 ( Figure 5 in revised version ) depicts plot of Px ( x ) ( x-axis ) and Pz_\\psi ( z ) ( y-axis ) . A linear plot means that the probability density of Px ( x ) is tidily mapped into the latent space . Thanks to this property , Px ( x ) can be estimated by Pz_\\psi ( z ) in our model . It is obvious that DAGMM does not have this trait . This is also quantitatively evaluated . The correlation coefficients are 0.691 ( baseline ) vs 0.997 or more ( ours ) . For the easy following , we revised the caption and description in the main text . > > 3- In the Anomaly detection experiments , the authors make two assumptions that usually do not exist in real-worlds : ( 1 ) they assume that they have access to a training set that only contains normal cases . ( 2 ) They assume that they know the correct rate of anomaly . I think both these assumptions are very restrictive and unreal . While these assumptions are used for all the comparing methods , it is not obvious how different algorithms behave in a real scenario . I understand that there is an unrealistic assumption , but this setting is established and widely admitted in this anomaly detection task . Regardless of this assumption is realistic or not , density estimation remains a critical issue , and better estimation provides better performance . Although investigating the performance in a truly real scenario might be future work , we argue this point is not a defect to show the validity of our method . > > 4- Figure 4 and what it represents is not clear . This is caused because we could not tell you the purpose of this experiment sufficiently . This is an experiment to show an important property of our model : our model behaves as PCA , where the energy of acquired latent space is concentrated on several principal components and the influence of each component can be evaluated quantitatively The two on the left of Fig.4 ( Fig.6 in the revised version ) is the variance of the latent space . Since our model works as PCA , the variance is concentrated in a few dimensions . Two on the right shows that the influence of minute displacement of each z to the real image is the almost constant in our model while it is varied in beta-VAE . Thus , we can evaluate the importance of latent variables by variance like PCA . We added the caption and enhanced the purpose of the experiment . > > Regarding writing issue Thank you for pointing . We fixed them ."}, "2": {"review_id": "BJxeHyrKPB-2", "review_text": "Summary of the paper: The authors propose a latent variable model RaDOGAGA, a generative autoencoding model. The model is trained via a tradeoff between distortion (the reconstruction error) and the rate (the capacity of the latent space, measured by entropy). The paper provides an analysis of theoretical properties of their approach, and presents supporting experimental results. Review tl;dr: weak reject, for three main reasons: (i) While the existing literature around VAEs, beta-VAEs, and Rate-Distortion theory is mentioned in the related work, the connections are not nearly discussed sufficiently. (ii) On top of (i), the derivation of their loss function and architecture is not sufficiently motivated. This is in astonishing contrast to 1.5 pages of main text and 8 pages of (much appreciated!) analysis of properties. (iii) Given the paper is clearly related to existing approaches in the literature, the experiments would require a much more careful comparison to existing models. It remains unclear why an interested user should favor your model over conceptually simpler generative models with fewer hyperparameters. Detailed review: Nota bene: This review is a late reassignment. While I reviewed the paper to the best of my ability, time constraints did not allow me to review parts of the paper in depth. I am open to reassess my review during the second stage. Connection to prior art: As a probabilistic, neural autoencoding model, the connections to the family of VAE models are obvious. The loss function (eq. (4)) still looks very much like the ELBO, where the typical conditional log-likelihood was split into two distortion terms. How is this different from e.g. a beta-VAE? Particularly, what is the connection between the rate-distortion analysis of beta-VAE by Alemi et al. and yours? These things need to be discussed explicitly, with more than a sentence or two in the related work section. A lesser, but still important omission in your discussion of prior work: The Jacobian of the generator has also been studied, even for the VAE, cf. e.g. [1]. I believe this deserves more attention in your assessment of prior art. Motivation: You use two distortion terms: actual sample vs. undistorted reconstruction. Why is that? What is the interpretation of the multipliers? How do I choose them? Why is a large part of your architecture (the pipeline from x to \\hat(x)) actually deterministic? Why are you using the entropy of the prior over the latents, rather than the KL divergence between encoder and a prior? I think an interested reader could learn much more from your paper if you discussed your model embedded in th related work rather than in isolation. Theory: Due to aforementioned time constraints, I was not able to review the extensive theoretical analysis in depth. Still, I would strongly recommend structuring the respective sections more clearly. Separate model and architecture description from the theoretical analysis; precisely formulate your claims. In particular, state your assumptions clearly. For instance, you assume \"that each function's parameter is rich enough to fit ideally\" (and similar e.g. in Appendix A). Does this only mean that the true distributions are part of the parametric family? What if this is not the case? Do your parameters need to be in the optimum for your analysis to hold true? Given that the full 20-page manuscript spends 10 pages on theory, I think this contribution is not given appropriate space in the main text. Experiments: There are three experiments: a simple 3D proof of concept; anomaly detection; analysis of the latent state in CelebA. As mentioned in my review of the methods section, I believe the approach to be very similar to established models. None of the experiments provides convincing evidence why I should prefer the new, arguably more complex model. For instance, I would have much preferred that you investigate properties of your model against alternatives over the anomaly detection experiments, which did not further my understanding of the proposed model. Summary: The paper tackles an important problem, namely the lack of control over the latent embedding in autoencoding generative models. I believe the author's contribution can be valuable, and I particularly appreciate the effort to investigate theoretical properties. As is, the case is not sufficiently convincing to be accepted, but I encourage the authors to improve the paper. Minor comments: 1. While I appreciate a pun, I would recommend to rename the model along with the acronym to a more concise name. 2. Please revise your notation and typsetting. Examples: x1 instead of x_1, f of f(\\cdot) instead of f(), \\log instead of log. 3. Introduce acronyms before using them (e.g. VAE, MSE, SSIM), even when they seem obvious to you. 4. Please carefully check the manuscript for typos, missing articles, missing spaces etc. 5. Your citations are inconsistent, in that they sometimes use first names, sometimes first name initials, and sometimes no first names. 6. To my knowledge, the term scale function does not have an obvious definition. I think you are simply referring to monotonically increasing functions. Please clarify! 7. Your figures should be understandable without too much context, they need more detailed captions. [1] http://proceedings.mlr.press/v84/chen18e.html", "rating": "3: Weak Reject", "reply_text": "> > Regarding the experimental result . First of all , actually , our model does not increase model complexity even though you concerned about this point . When we compared with our model and DAGMM ( baseline model ) , the number of network parameters is completely the same . We added this point explicitly . Nevertheless , our model provides a significant performance boost in the anomaly detection task . Experiment with toy data is executed to confirm our model \u2019 s property though , it also supports the result of anomaly detection . In DAGMM , the relation PDF of x and z is unclear . On the other hand , in our model , the PDFs of x and z are close to proportional . That means , our model can capture the probability of real data methodically in the latent space . This fact should be very intuitive to explain the performance boost in the anomaly detection task in which PDF estimation is a critical issue . Other comparison methods also essentially lead the disorder in the density estimation like DAGMM because the Jacobian is not controlled . In the analysis of the latent state in CelebA , we assume that since we could not tell you the purpose of the experiment enough , it was not convincing for you . This is an experiment to confirm that the latent variable in our model works as PCA components , and the influence of each component can be evaluated quantitatively as in theory while ( beta- ) VAE does not have this property . We revised this sections and captions for easy following . The two on the right of Fig.6 in the revised version show the scaling between the latent and metric dependent data space . ( c ) shows the scaling in VAE is anisometric , and ( d ) shows the scaling in ours is isometric . The two on the left of Fig.6 in the revised version is the variance of the latent space . Since the scaling of z in our model is isometric , the variance shows the importance of each latent variable like PCA . Consequently , we believe our experimental results demonstrate the validity of our method decently . PCA can simultaneously disentangle the data and estimate the importance of latent variables by variance . We believe this trait is very helpful to the interpretation of the latent variable of deep models . > > minor issues We fixed the minor issues you pointed ( we would appreciate if you could be indulgent of a bit long model acronym ) . We also promise to request a grammatical check by a native no later than the camera-ready version ."}}