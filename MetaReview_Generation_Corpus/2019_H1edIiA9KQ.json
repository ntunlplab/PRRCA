{"year": "2019", "forum": "H1edIiA9KQ", "title": "Generating Multiple Objects at Spatially Distinct Locations", "decision": "Accept (Poster)", "meta_review": "The submission proposes a model to generate images where one can control the fine-grained locations of objects. This is achieved by adding an \"object pathway\" to the GAN architecture. Experiments on a number of baselines are performed, including a number of reviewer-suggested metrics that were added post-rebuttal.\n\nThe method needs bounding boxes of the objects to be placed (and labels). The proposed method is simple and likely novel and I like the evaluating done with Yolov3 to get a sense of the object detection performance on the generated images. I find the results (qual & quant) and write-up compelling and I think that the method will be of practical relevance, especially in creative applications.\n\nBecause of this, I recommend acceptance.", "reviews": [{"review_id": "H1edIiA9KQ-0", "review_text": "This paper proposed a model to generate location-controllable images built upon GANs. The experiments are conducted on several datasets. Although this problem seems interesting, here are several concerns I have: 1.Novelty: the overall framework is still conditional GAN framework. The multiple -generators-discriminators structure has been used in many other works (see the references). The global-local design is not new. Finally, compared with Reed et al. [2016], the novelty is limit. 2.Motivation: I still can not tell why the proposed method is better than ones with scene layout. For me, the cost of collecting annotated data is almost the same. 3. The experimental results are week. For such a task, it is difficult to find a good metric. Thus the qualitative comparison is important. I think the author should follow standard rule to do some design for user study instead of cherry pick some examples. Besides, it should include more baselines instead of StackGAN. References: a. Xi et al. Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond b. Yixiao et al. FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification Revision: Thanks for the work of the authors' and all the reviewers. I spent sometime reading the rebuttal as well as the revised paper. It addressed most of my concern. I would like to change my rating from 5 to 6. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "-- -- -- -- -- `` 2.Motivation : I still can not tell why the proposed method is better than ones with scene layout . For me , the cost of collecting annotated data is almost the same . '' -- -- -- -- -- Thank you for pointing to the lack of clarity here . In fact we think that using semantic scene layouts it not necessarily worse but just yields different properties . Our approach requires both the bounding boxes and the associated labels but arguably `` less '' information than an image scene layout ( bounding box level annotation versus pixel level annotation ) . Nevertheless , we agree that the cost of collecting the required data may be similar . On an intuitive level , if we want a human to generate an image we would usually only give them a general description ( image caption ) and possibly the location where we want the salient objects to be . This is essentially what we do with our model , as opposed to describing `` common sense '' knowledge in detail such as `` the sky should be in the top of the image '' and `` the grass should be at the bottom '' , which is essentially what semantic scene layouts do . Another advantage of using bounding boxes instead of semantic layout is that they make it easier for humans to manually change the layout of the scene ( semantic scene manipulation ) in potential downstream tasks since humans usually do this on a per-object basis and not on a per-pixel basis . -- -- -- -- -- \u201c 3 . The experimental results are week . For such a task , it is difficult to find a good metric . Thus the qualitative comparison is important . I think the author should follow standard rule to do some design for user study instead of cherry pick some examples . Besides , it should include more baselines instead of StackGAN. \u201d -- -- -- -- -- We agree that a user study would be a valuable next step for all generative approaches within the community , but is often difficult to do because of time and resources constraints . To reduce the tendency of cherry picking , we made an effort to include both : well-working examples as well as failure cases ( see e.g.Fig 2 rows D-F and last three examples of Fig.6 and Fig.7 respectively ) . Additionally , we report the IS and FID values in order to provide results that are comparable with related models ( see Table 1 ) . As a direct baseline for qualitatively comparing the images on the MS-COCO data set we used both the StackGAN since it is a well-known architecture that performs well on many different data sets and the AttnGAN since it provides the current SOTA in image generation on the MS-COCO data set ( based on IS score ) . We acknowledge that other interesting approaches are emerging e.g.from CVPR ( e.g . [ 1-3 ] ) and we will make an effort to provide further tests if the implementation and training are feasible in time ( e.g . [ 1 ] and [ 2 ] ) . Overall , we would like to thank you again for your valuable feedback and concerns . We will work to implement the feedback we got and will post an updated version of our submission by the end of next week ( latest on 16.November ) and will let you know once the updated version is online . [ 1 ] Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network , Zhang Zizhao et al , CVPR , 2018 [ 2 ] Image Generation from Scene Graphs , Justin Johnson et al , CVPR , 2018 [ 3 ] Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis , Seunghoon Hong et al , CVPR , 2018"}, {"review_id": "H1edIiA9KQ-1", "review_text": "The authors are proposing a method for allowing the generation of multiple objects in generated images given simple supervision such as bounding boxes and their associated labels. They control the spatial location of generated objects by the mean of an object pathway added to the architecture of both Generator and Discriminator within a GAN framework. They show generated results on Multi-MNIST, CLEVR with discussions of their model's abilities and properties. they also provide quantitative results on MSCOCO (IS and FID) using StackGAN and AttGAN models with the object pathway modifications and show some improvements compared to the original models. However it must be noted (as commented by the authors) that these models are using image captions only and do not have explicit supervision of bounding box and object labels. This paper proposes a simple approach to generating requested objects in GAN-based image generation task, The method is supervised and requests (in its current form) the Bounding Boxes and Labels of the objects to integrate into the image generation. This task of controlling the nature (identity) and size of objects to integrate in a generated image is an important one and is significant to the GAN-base image generation community. In terms of originality, the approach is a nice simple architecture that takes care of the spatial location problem head-on. It seems like an obvious step but this does not take away from the merits of the proposed method. The generator Global path is given a noise component. From the text, it does not seem that the Object path is given a noise component. Do you generate always the same object given the same label and Bounding Box then? Why not integrate some noise in this pipeline too? Multi-MNIST: The authors present results on Multi-MNIST 50K customed data to present the ability of the model to accurately put request images in the correct bounding box (BB) and do some ablation study. This is an interesting test as it shows that indeed the method proposed generates digits where it is expected to. Could you provide the ground truth labels for each/some image/s? For the failure cases it is often not clear what digit is what. For the Row E and F, 1s could be 7s and vice versa. Since it is a qualitative study, it would be nice to have the Ground Truth (GT) (which you provide to G at for generation). For the failure case of Row D (right) an interesting results would have been to have example of a digit bounding box from top to bottom with few pixel vertical shift to visualize when the model starts to mess-up the generation. This seems to point that your model (exposed to the location from BB for the object paths) is sensitive to what locations it has seen in training. How would you make the object path more robust to unseen location (overall you need to design an object of a given size, then locate it in your empty canvas prior to the CNN for generation)? CLEVR: The images resolution make it hard to really see the shape of the images (here too, the GT would be great). The bounding boxes make the images even harder to parse. I know the colors change but \"We can confirm that the model can control both location and object's shape\". For the location, it is true, for the shape is hard to completely tell at this resolution without GT. MS-COCO: Just a comment in passing on the fact that resizing images from COCO to 256x256 will inherently distort quite a bit of images, the median size (for each dim) for COCO is 640x480, if I am not mistaken. Most, if not all images in COCO are not 1.0 size ratio. The quantitative results on COCO seem to confirm that the proposed method is generating \"better\" images according to IS and FID. This is a good thing, however the technique is strongly supervised (Bounding Box and Object Labels, caption compared to solely captions for StackGAN and AttGAN) so this result should be expected and really put into perspective as your are not comparing models w/ the same supervision (which you mention in the Discussion). Discussion: I appreciate that the authors addressed the limitations of their approach in this section. The overlapping BBs seems to be an interesting challenge. Did you try to normalize the embeddings in overlapping area? A simple sum does not seem to be a good solution. In Figure 7 w/ overlapping zebras, the generation seems completely lost. In terms of clarity, the paper is well-written but would benefit *greatly* from using variables names when discussing 'layout embedding', 'generated local labels', etc. Variable names and equations, while not necessary, can go a long way to clearly express a model's internal blocks (most of the papers you referenced are using this approach). The paper employs none of this commonly used standard and suffers from it. I myself had to write down on the margin the different variables used at each step described in text to have an understanding of what was done (with help of Figure 1). You should reference Figure 1 in the Introduction, as you cover your approach there and the Figure is useful to grasp your contributions. Another comment concerning clarity is, while it is fine to rely on previously published papers for description of our own work, you should not assume full knowledge from the reader and your paper should stand on its own without having the reader lookup for several papers to have an understanding of your training procedures. If one uses GAN training, it should be expected to cover/formulate quickly the min max game and the various losses you are trying to minimize. I am afraid that \"using common GAN procedures\" is not enough. When describing your experimental setup, pointing to another paper as \"hyperparameters remain the same as in the original training procedure\" should not be a substitution for covering it too, even if lightly in the Appendix. For instance: in the Appendix, it is mentioned that training was stopped at 20 epochs for Multi-MNIST, 40 for CLEVR... How did you decide on the epoch (early stopping, stopped before instabillity of GAN training, etc.) Did you use SGD? ADAM? Did you adjust the learning rate, which schedule? etc. for your GAN training. This information in the Appendix would make the paper overall stronger. Last comment: In terms of generation multiple objects. Have you had the chance to run an object detector on your generated image (you can build one on MSCOCO given the bounding box and label, finetune an ImageNet pretrained model). It would be interesting to see if the generated images are good enough for object detection. Post-Rebuttal: Given the work from the authors on improving the clarity of the paper as well as investigating the use of object detection metrics to compare their methods, I decided to move my rating upward to 7 ", "rating": "7: Good paper, accept", "reply_text": "\u201c Another comment concerning clarity is , while it is fine to rely on previously published papers for description of our own work , you should not assume full knowledge from the reader and your paper should stand on its own without having the reader lookup for several papers to have an understanding of your training procedures . If one uses GAN training , it should be expected to cover/formulate quickly the min max game and the various losses you are trying to minimize . I am afraid that `` using common GAN procedures '' is not enough . When describing your experimental setup , pointing to another paper as `` hyperparameters remain the same as in the original training procedure '' should not be a substitution for covering it too , even if lightly in the Appendix . For instance : in the Appendix , it is mentioned that training was stopped at 20 epochs for Multi-MNIST , 40 for CLEVR ... How did you decide on the epoch ( early stopping , stopped before instabillity of GAN training , etc . ) Did you use SGD ? ADAM ? Did you adjust the learning rate , which schedule ? etc.for your GAN training . This information in the Appendix would make the paper overall stronger. \u201c -- -- -- -- -- Because of the space constrains we aimed at providing all these information via our Github repository . Hovewer , we will make an effort to also include the information about the general GAN training procedure into our approach section to make the paper more self-contained . Additionally , we will update our appendix to describe the exact training procedure and hyperparameters in more detail . -- -- -- -- -- \u201c Last comment : In terms of generation multiple objects . Have you had the chance to run an object detector on your generated image ( you can build one on MSCOCO given the bounding box and label , finetune an ImageNet pretrained model ) . It would be interesting to see if the generated images are good enough for object detection. \u201d -- -- -- -- -- We have not tried this yet . This is an interesting idea and would pose a good additional metric for our paper and the community . We will look into this and keep you updated , whether we can do this in time . Thank you again for all your valuable comments and your feedback . We will work to implement the feedback we got and will post an updated version of our submission by the end of next week ( latest on 16.November ) and will let you know once the updated version is online ."}, {"review_id": "H1edIiA9KQ-2", "review_text": "The paper proposes a simple but effective method for controlling the location of objects in image generation using generative adversarial networks. Experiments on MNIST and CLEVR are toy examples but illustrate that the model is indeed performing as expected. The experiments on COCO produce results that while containing obvious artefacts are producing output consistent with the input control signal (i.e., bounding boxes). It would however have been interesting to see more varied bounding box locations for the same caption. In short, the paper makes an interesting addition to image generation works and likely to be incorporated into future image generation and inpainting methods.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Dear reviewer , thank you very much for your review . We will update our submission with examples of images based on MS-COCO captions in which we vary the location of the various bounding boxes . We will work to implement the feedback we got and will post an updated version of our submission by the end of next week ( latest on 16.November ) and will let you know once the updated version is online ."}], "0": {"review_id": "H1edIiA9KQ-0", "review_text": "This paper proposed a model to generate location-controllable images built upon GANs. The experiments are conducted on several datasets. Although this problem seems interesting, here are several concerns I have: 1.Novelty: the overall framework is still conditional GAN framework. The multiple -generators-discriminators structure has been used in many other works (see the references). The global-local design is not new. Finally, compared with Reed et al. [2016], the novelty is limit. 2.Motivation: I still can not tell why the proposed method is better than ones with scene layout. For me, the cost of collecting annotated data is almost the same. 3. The experimental results are week. For such a task, it is difficult to find a good metric. Thus the qualitative comparison is important. I think the author should follow standard rule to do some design for user study instead of cherry pick some examples. Besides, it should include more baselines instead of StackGAN. References: a. Xi et al. Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond b. Yixiao et al. FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification Revision: Thanks for the work of the authors' and all the reviewers. I spent sometime reading the rebuttal as well as the revised paper. It addressed most of my concern. I would like to change my rating from 5 to 6. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "-- -- -- -- -- `` 2.Motivation : I still can not tell why the proposed method is better than ones with scene layout . For me , the cost of collecting annotated data is almost the same . '' -- -- -- -- -- Thank you for pointing to the lack of clarity here . In fact we think that using semantic scene layouts it not necessarily worse but just yields different properties . Our approach requires both the bounding boxes and the associated labels but arguably `` less '' information than an image scene layout ( bounding box level annotation versus pixel level annotation ) . Nevertheless , we agree that the cost of collecting the required data may be similar . On an intuitive level , if we want a human to generate an image we would usually only give them a general description ( image caption ) and possibly the location where we want the salient objects to be . This is essentially what we do with our model , as opposed to describing `` common sense '' knowledge in detail such as `` the sky should be in the top of the image '' and `` the grass should be at the bottom '' , which is essentially what semantic scene layouts do . Another advantage of using bounding boxes instead of semantic layout is that they make it easier for humans to manually change the layout of the scene ( semantic scene manipulation ) in potential downstream tasks since humans usually do this on a per-object basis and not on a per-pixel basis . -- -- -- -- -- \u201c 3 . The experimental results are week . For such a task , it is difficult to find a good metric . Thus the qualitative comparison is important . I think the author should follow standard rule to do some design for user study instead of cherry pick some examples . Besides , it should include more baselines instead of StackGAN. \u201d -- -- -- -- -- We agree that a user study would be a valuable next step for all generative approaches within the community , but is often difficult to do because of time and resources constraints . To reduce the tendency of cherry picking , we made an effort to include both : well-working examples as well as failure cases ( see e.g.Fig 2 rows D-F and last three examples of Fig.6 and Fig.7 respectively ) . Additionally , we report the IS and FID values in order to provide results that are comparable with related models ( see Table 1 ) . As a direct baseline for qualitatively comparing the images on the MS-COCO data set we used both the StackGAN since it is a well-known architecture that performs well on many different data sets and the AttnGAN since it provides the current SOTA in image generation on the MS-COCO data set ( based on IS score ) . We acknowledge that other interesting approaches are emerging e.g.from CVPR ( e.g . [ 1-3 ] ) and we will make an effort to provide further tests if the implementation and training are feasible in time ( e.g . [ 1 ] and [ 2 ] ) . Overall , we would like to thank you again for your valuable feedback and concerns . We will work to implement the feedback we got and will post an updated version of our submission by the end of next week ( latest on 16.November ) and will let you know once the updated version is online . [ 1 ] Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network , Zhang Zizhao et al , CVPR , 2018 [ 2 ] Image Generation from Scene Graphs , Justin Johnson et al , CVPR , 2018 [ 3 ] Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis , Seunghoon Hong et al , CVPR , 2018"}, "1": {"review_id": "H1edIiA9KQ-1", "review_text": "The authors are proposing a method for allowing the generation of multiple objects in generated images given simple supervision such as bounding boxes and their associated labels. They control the spatial location of generated objects by the mean of an object pathway added to the architecture of both Generator and Discriminator within a GAN framework. They show generated results on Multi-MNIST, CLEVR with discussions of their model's abilities and properties. they also provide quantitative results on MSCOCO (IS and FID) using StackGAN and AttGAN models with the object pathway modifications and show some improvements compared to the original models. However it must be noted (as commented by the authors) that these models are using image captions only and do not have explicit supervision of bounding box and object labels. This paper proposes a simple approach to generating requested objects in GAN-based image generation task, The method is supervised and requests (in its current form) the Bounding Boxes and Labels of the objects to integrate into the image generation. This task of controlling the nature (identity) and size of objects to integrate in a generated image is an important one and is significant to the GAN-base image generation community. In terms of originality, the approach is a nice simple architecture that takes care of the spatial location problem head-on. It seems like an obvious step but this does not take away from the merits of the proposed method. The generator Global path is given a noise component. From the text, it does not seem that the Object path is given a noise component. Do you generate always the same object given the same label and Bounding Box then? Why not integrate some noise in this pipeline too? Multi-MNIST: The authors present results on Multi-MNIST 50K customed data to present the ability of the model to accurately put request images in the correct bounding box (BB) and do some ablation study. This is an interesting test as it shows that indeed the method proposed generates digits where it is expected to. Could you provide the ground truth labels for each/some image/s? For the failure cases it is often not clear what digit is what. For the Row E and F, 1s could be 7s and vice versa. Since it is a qualitative study, it would be nice to have the Ground Truth (GT) (which you provide to G at for generation). For the failure case of Row D (right) an interesting results would have been to have example of a digit bounding box from top to bottom with few pixel vertical shift to visualize when the model starts to mess-up the generation. This seems to point that your model (exposed to the location from BB for the object paths) is sensitive to what locations it has seen in training. How would you make the object path more robust to unseen location (overall you need to design an object of a given size, then locate it in your empty canvas prior to the CNN for generation)? CLEVR: The images resolution make it hard to really see the shape of the images (here too, the GT would be great). The bounding boxes make the images even harder to parse. I know the colors change but \"We can confirm that the model can control both location and object's shape\". For the location, it is true, for the shape is hard to completely tell at this resolution without GT. MS-COCO: Just a comment in passing on the fact that resizing images from COCO to 256x256 will inherently distort quite a bit of images, the median size (for each dim) for COCO is 640x480, if I am not mistaken. Most, if not all images in COCO are not 1.0 size ratio. The quantitative results on COCO seem to confirm that the proposed method is generating \"better\" images according to IS and FID. This is a good thing, however the technique is strongly supervised (Bounding Box and Object Labels, caption compared to solely captions for StackGAN and AttGAN) so this result should be expected and really put into perspective as your are not comparing models w/ the same supervision (which you mention in the Discussion). Discussion: I appreciate that the authors addressed the limitations of their approach in this section. The overlapping BBs seems to be an interesting challenge. Did you try to normalize the embeddings in overlapping area? A simple sum does not seem to be a good solution. In Figure 7 w/ overlapping zebras, the generation seems completely lost. In terms of clarity, the paper is well-written but would benefit *greatly* from using variables names when discussing 'layout embedding', 'generated local labels', etc. Variable names and equations, while not necessary, can go a long way to clearly express a model's internal blocks (most of the papers you referenced are using this approach). The paper employs none of this commonly used standard and suffers from it. I myself had to write down on the margin the different variables used at each step described in text to have an understanding of what was done (with help of Figure 1). You should reference Figure 1 in the Introduction, as you cover your approach there and the Figure is useful to grasp your contributions. Another comment concerning clarity is, while it is fine to rely on previously published papers for description of our own work, you should not assume full knowledge from the reader and your paper should stand on its own without having the reader lookup for several papers to have an understanding of your training procedures. If one uses GAN training, it should be expected to cover/formulate quickly the min max game and the various losses you are trying to minimize. I am afraid that \"using common GAN procedures\" is not enough. When describing your experimental setup, pointing to another paper as \"hyperparameters remain the same as in the original training procedure\" should not be a substitution for covering it too, even if lightly in the Appendix. For instance: in the Appendix, it is mentioned that training was stopped at 20 epochs for Multi-MNIST, 40 for CLEVR... How did you decide on the epoch (early stopping, stopped before instabillity of GAN training, etc.) Did you use SGD? ADAM? Did you adjust the learning rate, which schedule? etc. for your GAN training. This information in the Appendix would make the paper overall stronger. Last comment: In terms of generation multiple objects. Have you had the chance to run an object detector on your generated image (you can build one on MSCOCO given the bounding box and label, finetune an ImageNet pretrained model). It would be interesting to see if the generated images are good enough for object detection. Post-Rebuttal: Given the work from the authors on improving the clarity of the paper as well as investigating the use of object detection metrics to compare their methods, I decided to move my rating upward to 7 ", "rating": "7: Good paper, accept", "reply_text": "\u201c Another comment concerning clarity is , while it is fine to rely on previously published papers for description of our own work , you should not assume full knowledge from the reader and your paper should stand on its own without having the reader lookup for several papers to have an understanding of your training procedures . If one uses GAN training , it should be expected to cover/formulate quickly the min max game and the various losses you are trying to minimize . I am afraid that `` using common GAN procedures '' is not enough . When describing your experimental setup , pointing to another paper as `` hyperparameters remain the same as in the original training procedure '' should not be a substitution for covering it too , even if lightly in the Appendix . For instance : in the Appendix , it is mentioned that training was stopped at 20 epochs for Multi-MNIST , 40 for CLEVR ... How did you decide on the epoch ( early stopping , stopped before instabillity of GAN training , etc . ) Did you use SGD ? ADAM ? Did you adjust the learning rate , which schedule ? etc.for your GAN training . This information in the Appendix would make the paper overall stronger. \u201c -- -- -- -- -- Because of the space constrains we aimed at providing all these information via our Github repository . Hovewer , we will make an effort to also include the information about the general GAN training procedure into our approach section to make the paper more self-contained . Additionally , we will update our appendix to describe the exact training procedure and hyperparameters in more detail . -- -- -- -- -- \u201c Last comment : In terms of generation multiple objects . Have you had the chance to run an object detector on your generated image ( you can build one on MSCOCO given the bounding box and label , finetune an ImageNet pretrained model ) . It would be interesting to see if the generated images are good enough for object detection. \u201d -- -- -- -- -- We have not tried this yet . This is an interesting idea and would pose a good additional metric for our paper and the community . We will look into this and keep you updated , whether we can do this in time . Thank you again for all your valuable comments and your feedback . We will work to implement the feedback we got and will post an updated version of our submission by the end of next week ( latest on 16.November ) and will let you know once the updated version is online ."}, "2": {"review_id": "H1edIiA9KQ-2", "review_text": "The paper proposes a simple but effective method for controlling the location of objects in image generation using generative adversarial networks. Experiments on MNIST and CLEVR are toy examples but illustrate that the model is indeed performing as expected. The experiments on COCO produce results that while containing obvious artefacts are producing output consistent with the input control signal (i.e., bounding boxes). It would however have been interesting to see more varied bounding box locations for the same caption. In short, the paper makes an interesting addition to image generation works and likely to be incorporated into future image generation and inpainting methods.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Dear reviewer , thank you very much for your review . We will update our submission with examples of images based on MS-COCO captions in which we vary the location of the various bounding boxes . We will work to implement the feedback we got and will post an updated version of our submission by the end of next week ( latest on 16.November ) and will let you know once the updated version is online ."}}