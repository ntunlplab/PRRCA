{"year": "2018", "forum": "rkmu5b0a-", "title": "MGAN: Training Generative Adversarial Nets with Multiple Generators", "decision": "Accept (Poster)", "meta_review": "This paper presents an analysis of using multiple generators in a GAN setup, to address the mode-collapse problem. R1 was generally positive about the paper, raising the concern on how to choose the number of generators, and also whether parameter sharing was essential. The authors reported back on parameter sharing, showing its benefits yet did not have any principled method of selecting the number of generators. R2 was less positive about the paper, pointing out that mixture GANs and multiple generators have been tried before. They also raised concern with the (flawed) Inception score as the basis for comparison. R2 also pointed out that fixing the mixing proportions to uniform was an unrealistic assumption. The authors responded to these claims, clarifying the differences between this paper and the previous mixture GAN/multiple generator papers, and reporting FID scores. R3 was generally positive, also citing some novelty concerns similar to that of R2. I acknowledge the authors detailed responses to the reviews (in particular in response to R2) and I believe that the majority of concerns expressed have now been addressed. I also encourage the authors to include the FID scores in the final version of the paper.", "reviews": [{"review_id": "rkmu5b0a--0", "review_text": "The present manuscript attempts to address the problem of mode collapse in GANs using a constrained mixture distribution for the generator, and an auxiliary classifier which predicts the source mixture component, plus a loss term which encourages diversity amongst components. All told the proposed method is quite incremental, as mixture GANs/multi-generators have been done before. The Inception scores are good but it's widely known now that Inception scores are a deeply flawed measure, and presenting it as the only quantitative measure in a manuscript which makes strong claims about mode collapse unfortunately will not suffice. If the generator were to generate one template per class for which the Inception network's p(y|x) had low entropy, the Inception score would be quite high even though the model had only memorized one image per class. For claims surrounding mode collapse in particular, evaluation against a parameter count matched baseline using the AIS log likelihood estimation procedure in Wu et al (2017) would be the gold standard. Frechet Inception distance has also been proposed which at least has some favourable properties relative to Inception score. The mixing proportions are fixed to the uniform distribution, and therefore this method also makes the unrealistic assumption that modes are equiprobable and require an equal amount of modeling capacity. This seems quite dubious. Finally, their own qualitative results indicate that they've simply moved the problem, with clear evidence of mode collapse in one of their mixture components in figure 5c, 4th row from the bottom. Indeed, this does nothing to address the problem of mode collapse in general, as there is nothing preventing individual mixture component GANs from collapsing. Uncited prior work includes Generative Adversarial Parallelization of Im et al (2016). Also, if I'm not mistaken this is quite similar to an AC-GAN, where the classes are instead randomly assigned and the generator conditioning is done in a certain way; namely the first layer activations are the sum of K embeddings which are gated by the active mixture component. More discussion of this would be warranted. Other notes: - The introduction contains no discussion of the ill-posedness of the GAN game as it is played in practice. - \"As a result, the optimization order in 1 can be reversed\" this does not accurately characterize the source of the issues, see, e.g. Goodfellow (2015) \"On distinguishability criteria...\". - Section 3: the second last sentence of the third paragraph is vague and doesn't really say anything. Of course parameter sharing leverages common information. How does this help to train the model effectively? - Section 3: Since JSD is defined between two distributions, it is not clear what JSD_pi(P_G1, P_G2, ...) refers to. The last line of the proof of theorem 2 leaps to calling this term a Jensen-Shannon divergence but it's not clear what the steps are; it looks like a regular KL divergence to me. - Section 3: Also, is the classifier being trained to maximize this divergence or just the generator? I assume the latter. - The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions (pi). - \"... which further minimizes the objective value\" -- it minimizes a term that you introduced which is constant with respect to your learnable parameters. This is not a selling point, and I'm not sure why you bothered mentioning it. - There's no mention of the substitution of log (1 - D(x)) for -log(D(x)) and its effect on the interpretation as a Jensen-Shannon divergence (which I'm not sure was quite right in the first place) - Section 4: does the DAE introduced in DFM really introduce that much of a computational burden? - \"Symmetric Kullback Liebler divergence\" is not a well-known measure. The standard KL is asymmetric. Please define it. - Figure 2 is illegible in grayscale. - Improved-GAN score in Table 1 is misleading, as this was their no-label baseline. It's fine to include it but indicate it as such. Update: many of my concerns were adequately addressed, however I still feel that calling this an avenue to \"overcome mode collapse\" is misleading. This seems aimed at improving coverage of the support of the data distribution; test log likelihood bounds via AIS (there are GAN baselines for MNIST in the Wu et al manuscript I mentioned) would have been more compelling quantitative evidence. I've raised my score to a 5.", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * * * Note 1 : The introduction contains no discussion of the ill-posedness of the GAN game as it is played in practice . ==== Answer : We do not understand exactly what you meant by ill-posedness . Can please you further clarify this note ? * * * * Note 2 : `` As a result , the optimization order in 1 can be reversed '' this does not accurately characterize the source of the issues , see , e.g.Goodfellow ( 2015 ) `` On distinguishability criteria ... '' . ==== Answer : Here , we simply mentioned the issue discussed in The GAN tutorial ( Goodfellow , 2016 ) : \u201c Simultaneous gradient descent does not clearly privilege min max over max min or vice versa . We use it in the hope that it will behave like min max but it often behaves like max min. \u201d * * * * Note 3 : Section 3 : the second last sentence of the third paragraph is vague and does n't really say anything . Of course parameter sharing leverages common informaNtion . How does this help to train the model effectively ? ==== Answer : We discussed in Section 5.2 , Model Architectures that our experiment showed that when the parameters are not tied between the classifier and discriminator , the model learns slowly and eventually yields lower performance . * * * * Note 4 : Section 3 : Since JSD is defined between two distributions , it is not clear what JSD_pi ( P_G1 , P_G2 , ... ) refers to . The last line of the proof of theorem 2 leaps to calling this term a Jensen-Shannon divergence but it 's not clear what the steps are ; it looks like a regular KL divergence to me . ==== Answer : The general definition of JSD is : JSD_pi ( P_1 , P_2 , \u2026P_n ) = H ( sum_ { i=1 .. n } ( pi_i * P_i ) ) - sum_ { i=1 .. n } ( pi_i * H ( P_i ) Where H ( P ) is the Shannon entropy for distribution P. Due to limited space , we showed more details of the derivation of L ( G_1 : K ) in Appendix B . * * * * Note 5 : Section 3 : Also , is the classifier being trained to maximize this divergence or just the generator ? I assume the latter . ==== Answer : It is the latter . Based on Eq.2 , the classifier is trained to minimize its softmax loss , and based on the optimal solution for the classifier , the generators , by minimizing their objective function , will maximize the JSD divergence . * * * * Note 6 : The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions ( pi ) . - `` ... which further minimizes the objective value '' \u2013 it minimizes a term that you introduced which is constant with respect to your learnable parameters . This is not a selling point , and I 'm not sure why you bothered mentioning it . ==== Answer : Please refer to our answer to comment 3 . * * * * Note 7 : There 's no mention of the substitution of log ( 1 - D ( x ) ) for -log ( D ( x ) ) and its effect on the interpretation as a Jensen-Shannon divergence ( which I 'm not sure was quite right in the first place ) ==== Answer : We said in the end of Section 3 : \u201c In addition , we adopt the non-saturating heuristic proposed in ( Goodfellow et al. , 2014 ) to train G_ { 1 : K } by maximizing log D ( G_k ( z ) ) instead of minimizing log D ( 1 - G_k ( z ) ) . \u201d * * * * Note 8 : Section 4 : does the DAE introduced in DFM really introduce that much of a computational burden ? ==== Answer : It was stated in Section 5.3 , paragraph 2 in ( Warde-Farley & Bengio , 2017 ) that : \u201c we achieve a higher Inception score using denoising feature matching , using denoiser with 10 hidden layers of 2,048 rectified linear units each. \u201d That means the DAE adds more than 40 million parameters . * * * * Note 9 : \u201c Symmetric Kullback Liebler divergence \u201d is not a well-known measure . The standard KL is asymmetric . Please define it . - Figure 2 is illegible in grayscale . ==== Answer : Symmetric Kullback Liebler is the average of the KL and reverse KL divergence . As per your suggestion , we will define it in the paper . Regarding Figure 2 , we tried different shapes for the real and generated data points , but due the small size if figure , they are just clusters of red and blue points . We will try different approaches to make the figure more legible . * * * * Note 10 : Improved-GAN score in Table 1 is misleading , as this was their no-label baseline . It 's fine to include it but indicate it as such . ==== Answer : We will take your advice and make it clear that Improve-GAN score in Table 1 is for the unsupervised version ."}, {"review_id": "rkmu5b0a--1", "review_text": "Summary: The paper proposes a mixture of generators to train GANs. The generators used have tied weights except the first layer that maps the random codes is generator specific, hence no extra computational cost is added. Quality/clarity: The paper is well written and easy to follow. clarity: The appendix states how the weight tying is done , not the main paper, which might confuse the reader, would be better to state this weight tying that keeps the first layer free in the main text. Originality: Using multiple generators for GAN training has been proposed in many previous work that are cited in the paper, the difference in this paper is in weight tying between generators of the mixture, the first layer is kept free for each generator. General review: - when only the first layer is free between generators, I think it is not suitable to talk about multiple generators, but rather it is just a multimodal prior on the z, in this case z is a mixture of Gaussians with learned covariances (the weights of the first layer). This angle should be stressed in the paper, it is in fine, *one generator* with a multimodal learned prior on z! - Taking the multimodal z further , can you try adding a mean to be learned, together with the covariances also? see if this also helps? - in the tied weight case, in the synthetic example, can you show what each \"generator\" of the mixture learn? are they really learning modes of the data? - the theory is for general untied generators, can you comment on the tied case? I don't think the theory is any more valid, for this case, because again your implementation is one generator with a multimodal z prior. would be good to have some experiments and see how much we loose for example in term of inception scores, between tied and untied weights of generators. ", "rating": "7: Good paper, accept", "reply_text": "We gratefully thank the reviewer for the thoughtful and insightful comments . It took us a while to answer all the reviews as well as to run additional experiments as suggested . Our answers are the following : * * * * Comment 1 : when only the first layer is free between generators , I think it is not suitable to talk about multiple generators , but rather it is just a multimodal prior on the z , in this case z is a mixture of Gaussians with learned covariances ( the weights of the first layer ) . This angle should be stressed in the paper , it is in fine , * one generator * with a multimodal learned prior on z ! ==== Answer : The first hidden layer actually has 4x4x512 = 8,192 dimensions ( for Cifar-10 ) . So , untying weights in the first layer effectively maps the noise prior to a different distribution in R^8192 ( with a different mean and covariances ) for each generator . So , our proposed method is different from a GAN with a multimodal prior . * * * * Comment 2 : taking the multimodal z further , can you try adding a mean to be learned , together with the covariances also ? see if this also helps ? ==== Answer : We tried to learn the mean and covariance of the prior for each generator , but the result was not much different from the standard GAN . * * * * Comment 3 : in the tied weight case , in the synthetic example , can you show what each `` generator '' of the mixture learn ? are they really learning modes of the data ? ==== Answer : Following your suggestion , we revised figure 6 so that data points generated by different generators have different colors . As you can see , generators learned different modes of the data . * * * * Comment 4 : the theory is for general untied generators , can you comment on the tied case ? I do n't think the theory is any more valid , for this case , because again your implementation is one generator with a multimodal z prior . would be good to have some experiments and see how much we loose for example in term of inception scores , between tied and untied weights of generators . ==== Answer : In theory , tying weights will add constraints to the optimization of the objective function for G_ { 1 : K } in Eq.4.For example , if we tie weights in all layers and generators differ only in the mean and variance of the noise prior , the result was similar to the standard GAN like we reported in comment 2 . Untying weights in the first layer , however , achieved good results like we discussed in the paper . Finally , as per your request , we conducted experiments without parameter sharing . Surprisingly , when we trained 4 generators without parameter sharing and each generator has 128 feature maps in the penultimate layer , the model failed to learn . The model even failed to learn when we set beta to 0 . When we reduced the number of feature maps in the penultimate layer for each generator to 32 , they managed to learn and achieved an Inception Score of 7.42 . So , we hypothesize that added benefit of our parameter sharing scheme is to balance the capacity of generators and that of the discriminator/classifier ."}, {"review_id": "rkmu5b0a--2", "review_text": "MGAN aims to overcome model collapsing problem by mixture generators. Compare to traditional GAN, there is a classifier added to minimax formulation. In training, MGAN is optimized towards minimizing the Jensen-Shannon Divergence between mixture distributions from generator and data distribution. The author also present that using MGAN to achive state-of-art results. The paper is easy to follow. Comment: 1. Seems there still no principle to choose correct number of generators but try different setting. Although most parameters of generators are shared, the result various. 2. Parameter sharing seems is a trick in MGAN model. Could you provide experiment results w/o parameter sharing. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We gratefully thank reviewers for the insightful comments . We have endeavored to address as much as we can , including running additional experiments as suggested , thus it has taken us a while . * * * * Comment 1 : Seems there still no principle to choose correct number of generators but try different setting . Although most parameters of generators are shared , the result various . ==== Answer : We agree that we don \u2019 t have any principle to choose the correct number of generators for our proposed model , as choosing the correct number of clusters for Gaussian mixture model ( GMM ) and other clustering methods . If we wish to specify an appropriate number of generators automatically , we would need to go for a Bayesian nonparametric extension , similarly to going from GMM to Dirichlet Process Mixtures . Within the scope of this work , our motivation is that GAN works pretty well on narrow-domain dataset but poorly on diverse dataset ; So , if we can efficiently train many generators while enforcing divergence among them , they can work well too . In general , more generators tend to work better . * * * * Comment 2 : Parameter sharing seems is a trick in MGAN model . Could you provide experiment results w/o parameter sharing . ==== Answer : We did experiment without parameters sharing among generators and found an interesting behavior . When we trained 4 generators without parameter sharing and each generator has 128 feature maps in the penultimate layer , the model failed to learn . The model even failed to learn when we set beta to 0 . When we reduced the number of feature maps in the penultimate layer for each generator to 32 , they managed to learn and achieved an Inception Score of 7.42 . So , we hypothesize that added benefit of parameter sharing is to help balance the capacity of generators and that of the discriminator/classifier ."}], "0": {"review_id": "rkmu5b0a--0", "review_text": "The present manuscript attempts to address the problem of mode collapse in GANs using a constrained mixture distribution for the generator, and an auxiliary classifier which predicts the source mixture component, plus a loss term which encourages diversity amongst components. All told the proposed method is quite incremental, as mixture GANs/multi-generators have been done before. The Inception scores are good but it's widely known now that Inception scores are a deeply flawed measure, and presenting it as the only quantitative measure in a manuscript which makes strong claims about mode collapse unfortunately will not suffice. If the generator were to generate one template per class for which the Inception network's p(y|x) had low entropy, the Inception score would be quite high even though the model had only memorized one image per class. For claims surrounding mode collapse in particular, evaluation against a parameter count matched baseline using the AIS log likelihood estimation procedure in Wu et al (2017) would be the gold standard. Frechet Inception distance has also been proposed which at least has some favourable properties relative to Inception score. The mixing proportions are fixed to the uniform distribution, and therefore this method also makes the unrealistic assumption that modes are equiprobable and require an equal amount of modeling capacity. This seems quite dubious. Finally, their own qualitative results indicate that they've simply moved the problem, with clear evidence of mode collapse in one of their mixture components in figure 5c, 4th row from the bottom. Indeed, this does nothing to address the problem of mode collapse in general, as there is nothing preventing individual mixture component GANs from collapsing. Uncited prior work includes Generative Adversarial Parallelization of Im et al (2016). Also, if I'm not mistaken this is quite similar to an AC-GAN, where the classes are instead randomly assigned and the generator conditioning is done in a certain way; namely the first layer activations are the sum of K embeddings which are gated by the active mixture component. More discussion of this would be warranted. Other notes: - The introduction contains no discussion of the ill-posedness of the GAN game as it is played in practice. - \"As a result, the optimization order in 1 can be reversed\" this does not accurately characterize the source of the issues, see, e.g. Goodfellow (2015) \"On distinguishability criteria...\". - Section 3: the second last sentence of the third paragraph is vague and doesn't really say anything. Of course parameter sharing leverages common information. How does this help to train the model effectively? - Section 3: Since JSD is defined between two distributions, it is not clear what JSD_pi(P_G1, P_G2, ...) refers to. The last line of the proof of theorem 2 leaps to calling this term a Jensen-Shannon divergence but it's not clear what the steps are; it looks like a regular KL divergence to me. - Section 3: Also, is the classifier being trained to maximize this divergence or just the generator? I assume the latter. - The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions (pi). - \"... which further minimizes the objective value\" -- it minimizes a term that you introduced which is constant with respect to your learnable parameters. This is not a selling point, and I'm not sure why you bothered mentioning it. - There's no mention of the substitution of log (1 - D(x)) for -log(D(x)) and its effect on the interpretation as a Jensen-Shannon divergence (which I'm not sure was quite right in the first place) - Section 4: does the DAE introduced in DFM really introduce that much of a computational burden? - \"Symmetric Kullback Liebler divergence\" is not a well-known measure. The standard KL is asymmetric. Please define it. - Figure 2 is illegible in grayscale. - Improved-GAN score in Table 1 is misleading, as this was their no-label baseline. It's fine to include it but indicate it as such. Update: many of my concerns were adequately addressed, however I still feel that calling this an avenue to \"overcome mode collapse\" is misleading. This seems aimed at improving coverage of the support of the data distribution; test log likelihood bounds via AIS (there are GAN baselines for MNIST in the Wu et al manuscript I mentioned) would have been more compelling quantitative evidence. I've raised my score to a 5.", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * * * Note 1 : The introduction contains no discussion of the ill-posedness of the GAN game as it is played in practice . ==== Answer : We do not understand exactly what you meant by ill-posedness . Can please you further clarify this note ? * * * * Note 2 : `` As a result , the optimization order in 1 can be reversed '' this does not accurately characterize the source of the issues , see , e.g.Goodfellow ( 2015 ) `` On distinguishability criteria ... '' . ==== Answer : Here , we simply mentioned the issue discussed in The GAN tutorial ( Goodfellow , 2016 ) : \u201c Simultaneous gradient descent does not clearly privilege min max over max min or vice versa . We use it in the hope that it will behave like min max but it often behaves like max min. \u201d * * * * Note 3 : Section 3 : the second last sentence of the third paragraph is vague and does n't really say anything . Of course parameter sharing leverages common informaNtion . How does this help to train the model effectively ? ==== Answer : We discussed in Section 5.2 , Model Architectures that our experiment showed that when the parameters are not tied between the classifier and discriminator , the model learns slowly and eventually yields lower performance . * * * * Note 4 : Section 3 : Since JSD is defined between two distributions , it is not clear what JSD_pi ( P_G1 , P_G2 , ... ) refers to . The last line of the proof of theorem 2 leaps to calling this term a Jensen-Shannon divergence but it 's not clear what the steps are ; it looks like a regular KL divergence to me . ==== Answer : The general definition of JSD is : JSD_pi ( P_1 , P_2 , \u2026P_n ) = H ( sum_ { i=1 .. n } ( pi_i * P_i ) ) - sum_ { i=1 .. n } ( pi_i * H ( P_i ) Where H ( P ) is the Shannon entropy for distribution P. Due to limited space , we showed more details of the derivation of L ( G_1 : K ) in Appendix B . * * * * Note 5 : Section 3 : Also , is the classifier being trained to maximize this divergence or just the generator ? I assume the latter . ==== Answer : It is the latter . Based on Eq.2 , the classifier is trained to minimize its softmax loss , and based on the optimal solution for the classifier , the generators , by minimizing their objective function , will maximize the JSD divergence . * * * * Note 6 : The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions ( pi ) . - `` ... which further minimizes the objective value '' \u2013 it minimizes a term that you introduced which is constant with respect to your learnable parameters . This is not a selling point , and I 'm not sure why you bothered mentioning it . ==== Answer : Please refer to our answer to comment 3 . * * * * Note 7 : There 's no mention of the substitution of log ( 1 - D ( x ) ) for -log ( D ( x ) ) and its effect on the interpretation as a Jensen-Shannon divergence ( which I 'm not sure was quite right in the first place ) ==== Answer : We said in the end of Section 3 : \u201c In addition , we adopt the non-saturating heuristic proposed in ( Goodfellow et al. , 2014 ) to train G_ { 1 : K } by maximizing log D ( G_k ( z ) ) instead of minimizing log D ( 1 - G_k ( z ) ) . \u201d * * * * Note 8 : Section 4 : does the DAE introduced in DFM really introduce that much of a computational burden ? ==== Answer : It was stated in Section 5.3 , paragraph 2 in ( Warde-Farley & Bengio , 2017 ) that : \u201c we achieve a higher Inception score using denoising feature matching , using denoiser with 10 hidden layers of 2,048 rectified linear units each. \u201d That means the DAE adds more than 40 million parameters . * * * * Note 9 : \u201c Symmetric Kullback Liebler divergence \u201d is not a well-known measure . The standard KL is asymmetric . Please define it . - Figure 2 is illegible in grayscale . ==== Answer : Symmetric Kullback Liebler is the average of the KL and reverse KL divergence . As per your suggestion , we will define it in the paper . Regarding Figure 2 , we tried different shapes for the real and generated data points , but due the small size if figure , they are just clusters of red and blue points . We will try different approaches to make the figure more legible . * * * * Note 10 : Improved-GAN score in Table 1 is misleading , as this was their no-label baseline . It 's fine to include it but indicate it as such . ==== Answer : We will take your advice and make it clear that Improve-GAN score in Table 1 is for the unsupervised version ."}, "1": {"review_id": "rkmu5b0a--1", "review_text": "Summary: The paper proposes a mixture of generators to train GANs. The generators used have tied weights except the first layer that maps the random codes is generator specific, hence no extra computational cost is added. Quality/clarity: The paper is well written and easy to follow. clarity: The appendix states how the weight tying is done , not the main paper, which might confuse the reader, would be better to state this weight tying that keeps the first layer free in the main text. Originality: Using multiple generators for GAN training has been proposed in many previous work that are cited in the paper, the difference in this paper is in weight tying between generators of the mixture, the first layer is kept free for each generator. General review: - when only the first layer is free between generators, I think it is not suitable to talk about multiple generators, but rather it is just a multimodal prior on the z, in this case z is a mixture of Gaussians with learned covariances (the weights of the first layer). This angle should be stressed in the paper, it is in fine, *one generator* with a multimodal learned prior on z! - Taking the multimodal z further , can you try adding a mean to be learned, together with the covariances also? see if this also helps? - in the tied weight case, in the synthetic example, can you show what each \"generator\" of the mixture learn? are they really learning modes of the data? - the theory is for general untied generators, can you comment on the tied case? I don't think the theory is any more valid, for this case, because again your implementation is one generator with a multimodal z prior. would be good to have some experiments and see how much we loose for example in term of inception scores, between tied and untied weights of generators. ", "rating": "7: Good paper, accept", "reply_text": "We gratefully thank the reviewer for the thoughtful and insightful comments . It took us a while to answer all the reviews as well as to run additional experiments as suggested . Our answers are the following : * * * * Comment 1 : when only the first layer is free between generators , I think it is not suitable to talk about multiple generators , but rather it is just a multimodal prior on the z , in this case z is a mixture of Gaussians with learned covariances ( the weights of the first layer ) . This angle should be stressed in the paper , it is in fine , * one generator * with a multimodal learned prior on z ! ==== Answer : The first hidden layer actually has 4x4x512 = 8,192 dimensions ( for Cifar-10 ) . So , untying weights in the first layer effectively maps the noise prior to a different distribution in R^8192 ( with a different mean and covariances ) for each generator . So , our proposed method is different from a GAN with a multimodal prior . * * * * Comment 2 : taking the multimodal z further , can you try adding a mean to be learned , together with the covariances also ? see if this also helps ? ==== Answer : We tried to learn the mean and covariance of the prior for each generator , but the result was not much different from the standard GAN . * * * * Comment 3 : in the tied weight case , in the synthetic example , can you show what each `` generator '' of the mixture learn ? are they really learning modes of the data ? ==== Answer : Following your suggestion , we revised figure 6 so that data points generated by different generators have different colors . As you can see , generators learned different modes of the data . * * * * Comment 4 : the theory is for general untied generators , can you comment on the tied case ? I do n't think the theory is any more valid , for this case , because again your implementation is one generator with a multimodal z prior . would be good to have some experiments and see how much we loose for example in term of inception scores , between tied and untied weights of generators . ==== Answer : In theory , tying weights will add constraints to the optimization of the objective function for G_ { 1 : K } in Eq.4.For example , if we tie weights in all layers and generators differ only in the mean and variance of the noise prior , the result was similar to the standard GAN like we reported in comment 2 . Untying weights in the first layer , however , achieved good results like we discussed in the paper . Finally , as per your request , we conducted experiments without parameter sharing . Surprisingly , when we trained 4 generators without parameter sharing and each generator has 128 feature maps in the penultimate layer , the model failed to learn . The model even failed to learn when we set beta to 0 . When we reduced the number of feature maps in the penultimate layer for each generator to 32 , they managed to learn and achieved an Inception Score of 7.42 . So , we hypothesize that added benefit of our parameter sharing scheme is to balance the capacity of generators and that of the discriminator/classifier ."}, "2": {"review_id": "rkmu5b0a--2", "review_text": "MGAN aims to overcome model collapsing problem by mixture generators. Compare to traditional GAN, there is a classifier added to minimax formulation. In training, MGAN is optimized towards minimizing the Jensen-Shannon Divergence between mixture distributions from generator and data distribution. The author also present that using MGAN to achive state-of-art results. The paper is easy to follow. Comment: 1. Seems there still no principle to choose correct number of generators but try different setting. Although most parameters of generators are shared, the result various. 2. Parameter sharing seems is a trick in MGAN model. Could you provide experiment results w/o parameter sharing. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We gratefully thank reviewers for the insightful comments . We have endeavored to address as much as we can , including running additional experiments as suggested , thus it has taken us a while . * * * * Comment 1 : Seems there still no principle to choose correct number of generators but try different setting . Although most parameters of generators are shared , the result various . ==== Answer : We agree that we don \u2019 t have any principle to choose the correct number of generators for our proposed model , as choosing the correct number of clusters for Gaussian mixture model ( GMM ) and other clustering methods . If we wish to specify an appropriate number of generators automatically , we would need to go for a Bayesian nonparametric extension , similarly to going from GMM to Dirichlet Process Mixtures . Within the scope of this work , our motivation is that GAN works pretty well on narrow-domain dataset but poorly on diverse dataset ; So , if we can efficiently train many generators while enforcing divergence among them , they can work well too . In general , more generators tend to work better . * * * * Comment 2 : Parameter sharing seems is a trick in MGAN model . Could you provide experiment results w/o parameter sharing . ==== Answer : We did experiment without parameters sharing among generators and found an interesting behavior . When we trained 4 generators without parameter sharing and each generator has 128 feature maps in the penultimate layer , the model failed to learn . The model even failed to learn when we set beta to 0 . When we reduced the number of feature maps in the penultimate layer for each generator to 32 , they managed to learn and achieved an Inception Score of 7.42 . So , we hypothesize that added benefit of parameter sharing is to help balance the capacity of generators and that of the discriminator/classifier ."}}