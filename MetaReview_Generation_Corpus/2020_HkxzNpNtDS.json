{"year": "2020", "forum": "HkxzNpNtDS", "title": "Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning", "decision": "Reject", "meta_review": "The paper proposes a multitask navigation model that can be trained on both vision-language navigation (VLN) and navigation from dialog history (NDH) tasks. The authors provide experiments that demonstrate that their model can outperformance single-task baseline models.\n\nThe paper received borderline scores with two weak accept and one weak reject.  Overall, the reviewers found the paper to be well-written and easy to understand, with thorough experiments.\n\nThe reviewers had minor concerns about the following:\n1. The generalizability of the work.  No results are reported on the test set, only on val.\n2. The gains for val unseen are pretty small and there are other models (e.g. Ke et al, Tan et al) that have better results.  Would the proposed environment-agnostic multitask learning be able to improve those models as well?  Or is the gains limited to having a weak baseline?\n3. It's unclear if the gains are due to the multitasking or just having more data available to train on.\n4. There are some minor issues with the misspellings/typos.  Some examples are given:\nPage 1: \"Manolis Savva* et al\" --> \"Savva et al\"\nPage 5: \"x_1, x2, ..., x_3\" --> Should the x_3 be something like x_k where k is the length of the utterance?\n\nThe AC agrees with the reviewers that the paper is interesting and is mostly solid work.  The AC also feels that there are some valid concerns about the generalizability of the work and that the paper would benefit from a more careful consideration of the issues raised by the reviewers.  The authors are encouraged to refine the work and resubmit.", "reviews": [{"review_id": "HkxzNpNtDS-0", "review_text": "This paper addresses some challenges of following natural language instructions for navigating in visual environments. The main challenge in such tasks is the scarcity of available training data, which results in generalization problems where the agent has difficulty navigating in unseen environments. Therefore, the authors propose two key ideas to tackle this issue and incorporate them in the reinforced cross-modal matching (RCM) model (Wang et al, 2019). First, they use a generalized multitask learning model to transfer knowledge across two different tasks: Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH). This results in learning features that explain both tasks simultaneously and hence generalize better. Moreover, by training on both tasks the effective size of training data is increased significantly. Second, they propose an environment-agnostic learning technique in order to learn invariant representations that are still efficient for navigation. This prevents overfitting to specific visual features of the environment and therefore helps improving generalization. The contribution of this paper is combining and incorporating these two key ideas in the RCM framework and verifying it on VLN and NDH tasks. This approach is novel compared to prior results in tackling the VLN task. Their experimental results show that the two proposed techniques improve generalization in a complementary fashion, measured by decreased performance gap between seen and unseen environments. They demonstrate that their technique outperforms state-of-the-art methods on unseen data on some evaluation metrics. Even though the two key ideas proposed in the paper has been explored in the literature in other contexts, this paper contributes to natural language guided navigation by incorporating these ideas in a unified framework and demonstrates promising results on two datasets. Therefore, I would recommend accepting this paper if some issues in delivery and clarity are addressed. In particular, Section 3, where the authors introduce the novelty of the paper in more detail, could be better explained and a cleaner line should be drawn between prior results from other papers and novel results proposed by this paper. In general, the new ideas would require more emphasis, since they are somewhat lost between adaptations from prior work. Most importantly, Eq. (3) is stated without sufficient motivation and would require a more detailed explanation. In addition to these points, I would like to disclose some recommendations that might improve the paper but are not strictly part of my decision. In some cases the notation is not clear and some variables are not defined or explained. For instance, after Eq. (8) Attention(.) is used without citation or definition, in Eq. (9) Wc and Wu are not defined and some of the notation in Eq. (6)-(7) are not defined. Moreover, reading the decrease in performance gap from the presented table format is inconvenient and a better visual representation might help demonstrating the improvement in generalization better. Lastly, I noticed that the navigation error for shared decoder is slightly higher than for separate encoders in Table 3, even though it outperforms the separate encoders in every other measures. Is there a particular explanation for this?", "rating": "6: Weak Accept", "reply_text": "Thank you for the recommendation and valuable suggestions to improve the clarity of this paper . Below we provide the responses to your suggestions . Note that we have uploaded a paper revision according to the suggestions . 1.Re : Emphasis on new ideas and results We mainly discuss the difference from prior art in Section 2 \u201c background \u201d . As R3 suggested , we explain the motivation of Equation 3 in the revision and draw more clear separation of results of previous work and our new results in Table 1 and Table 5 . We will keep working on the clarity till the camera-ready version if it is accepted . 2.Re : Notations of Equations We have improved the notations and citations , and made them clearer in the revision . 3.Re : Better visualization of performance gaps We added a figure ( Fig 4 ) in the Appendix to better visualize the performance gaps between seen and unseen environments in the revision . 4.Re : Separate encoders vs. shared encoder From Table 3 , we can observe that the shared encoder outperforms the separate encoders almost on all metrics on both seen and unseen environments . The navigation errors on seen environments are actually equivalent ( 5.09 vs. 5.02 , the difference is 0.07 ) . Besides , the navigation error is the average distance between the last node in the predicted path and the target destination , which is often but not necessarily aligned to the primary measures like Success Rate , because 5 meters and 10 meters to the destination are both considered as failure cases ."}, {"review_id": "HkxzNpNtDS-1", "review_text": "This paper aims to apply the model of Wang 2019 to the new NDH task of Thomason '19. Both of these datasets are built on the same room-to-room environment and both are for natural language instruction following. Thomason's work extends the R2R paradigm to include a dialogue history which is collapsed into a single instruction. The contribution of this paper is to build a single model which alternatingly samples trajectories from each of the two datasets to train a more general actor and the authors also believe that the presence of an environment classifier assists in generalization. The claims of the paper focus on being \"environment-agnostic\" and notions of generality. As hinted by the authors in their future work, to properly show this would probably require two different environments or tasks (e.g. Touchdown), not training on two tasks that use the same environments and pre-computed visual features. Am I incorrect that the only difference between the NDH and VLN formulation is the structure of the sentences? Figure 3 is the most compelling component of the paper. However, I am still not convinced it will generalize and all other components of the paper are largely minor tweaks to existing work. Figure 2 mostly leads me to believe that we have a simple data-augmentation situation which makes the bump in performance somewhat predictable. Minor: Is there any reason why in Table 1 we can't simply run the VLN models on NDH and NDH on VLN? I commend the authors for putting this all together in the two months between the release of CVDN and the ICLR deadline. I think training a joint model on these two datasets is a completely natural experiment that many of us wanted to see, and so I appreciate the effort of the authors and the benefit to the community of having these numbers, but I'm not convinced there is that much meat otherwise in this paper.", "rating": "3: Weak Reject", "reply_text": "Thank you for the review and acknowledging the usefulness of this work . We would like to use this opportunity to resolve some potential confusion and misunderstanding . 1.Contributions First , we would like to reiterate the main contributions : ( 1 ) We introduce the first generalized multitask learning framework for natural language grounded navigation tasks such as VLN and NDH , which adopts an interleaved multitask sampling strategy and allows different learning objectives for different tasks . Note that we simultaneously sample data for different tasks within the same mini-batch ( not alternately batch by batch ) to avoid overfitting to individual tasks . ( 2 ) To further improve generalizability , we propose an environment-agnostic training method , which is unified with the multitask learning framework , to learn environment-invariant representations and thus reduce the gap between seen and unseen environments . ( 3 ) We have done thorough and extensive experiments on VLN and NDH tasks and prove that the proposed methods are very effective , which improve the baselines by a large margin and establish new SOTA on NDH ( with ~120 % improvement in terms of goal progress ) . 2.Re : \u201c environments \u201d and \u201c environment-agnostic \u201d We would also like to clarify the notion of \u201c environments \u201d . We use the term \u201c environments \u201d to refer to different houses in the datasets , which are split into seen and unseen sets . The navigation models are trained on seen environments but tested on previously unseen environments . The objective is to improve the performance on unseen environments and reduce the performance gap between them . Therefore , \u201c environment-agnostic \u201d learning is to learn invariant indoor representations among different houses within tasks such as VLN and NDH ( not among different tasks ) . In contrast , the idea of multitask learning is to utilize knowledge across tasks . 3.Re : differences between VLN and NDH Even though both VLN and NDH tasks use the same Matterport3D indoor environments [ 1 ] , there are significant differences in the motivations as well as the overall objectives of the two tasks -- ( 1 ) Collecting data for VLN task involved single human player describing a fixed path through the environment resulting in instructions that can be followed step-by-step to reach the destination . On the other hand , collecting data for NDH involved two human players ( oracle and navigator ) co-operating to find a specified object ( e.g. , \u201c trashcan \u201d ) in the environment . The navigator can elicit assistance from the oracle who can view the navigator \u2019 s path so far and the future 5 steps towards the goal . As a result , NDH involves a series of question/answer interactions ( dialog ) between two players that may not necessarily be step-by-step . NDH is actually a more practical setting than VLN . In real-world applications such as disaster relief , it is impossible to just give a one-time instruction , and send the robot to finish the job , so how to navigate from the interactive dialog history is crucially important . ( 2 ) The two tasks have different data characteristics , e.g. , while the VLN task has an average instruction length of 29 words , the average dialog length in NDH is 81 words . Similarly , the average path length in NDH is much larger ( roughly 3x ) than that in VLN . ( 3 ) The two tasks differ in input representations as well as overall objectives . While VLN instructions can be tokenized and represented as word embeddings , NDH instructions require care in adding markers to indicate start/end of a navigator \u2019 s question and oracle \u2019 s answer . Furthermore , while the objective in VLN is to find the exact goal node in the environment ( i.e. , point navigation ) , the objective in NDH is the find the goal room that contains the specified object ( i.e. , room navigation ) . Due to the above differences , multitask learning involving the two tasks aids learning better representations that improve performance on both tasks simultaneously . The difference in data characteristics leads to effective inductive transfer between the two tasks ( e.g. , MT-RCM model can benefit from following shorter paths in VLN to break down longer paths into smaller achievable chunks in NDH ) . Extending our work to include different environments ( e.g. , Touchdown ) will indeed be a worthy future extension of our work but that should not discount the contributions of our present work . [ 1 ] \u201c Matterport3D : Learning from RGB-D Data in Indoor Environments \u201d , Chang et al.2017"}, {"review_id": "HkxzNpNtDS-2", "review_text": "Summary: There have been two recent related tasks proposed in vision-langauge settings: vision-langauge navigation (VLN) where natural language turn-by-turn instructions must be decoded by an agent in an indoor environment to reach the goal location and Navigation from Dialog History (NDH) where dialog between two humans trying to reach a goal is input to an agent to try to reach a goal location. This paper uses these two tasks' data in a multi-task manner to try to generalize better between indoor environments especially unseen environments which are not in the training set of the agent. Another proposed innovation is to use an auxiliary task of environment classification but via a gradient reversal layer such that the learnt latent representation input to the classifier should not overfit to foibles of the environment but should (hopefully) learn a representation that captures the intrinsics of the environment necessary for generalization. Comments: - The paper is well-written and easy to understand! Experiments are thorough and has all the ablations one would ask for. Thanks! - Overall I like the paper but have a number of comments: 1. Why not try more sophisticated methods of multi-task learning like 'MetaLearning' by Finn et al 2017 (MAML). It is common knowledge that straight up averaging across tasks is not as effective as doing the bilevel optimization in MAML. 2. Why do RL at all? Already the authors are doing BC (naive form of imitation learning) but they could just do robust imitation learning like DAgger, AggreVateD, etc. The setting is already such that one has a natural oracle (which the authors are already using via BC in the objective) which is the shortest path planner during training time. Then combined with MAML one can do Meta-IL as in 'One-Shot Imitation Learning via Meta-Learning' Finn et al 2017. Note that imitation learning is exponenially more sample efficient than RL and removes all the reward-shaping complications. 3. In Section 2 for error correction \"Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention\" by Nguyen et al. CVPR 2019 is directly relevant. 4. Also for generalizaton performance these papers are directly relevant: \"Building Generalizable Agents with a Realistic and Rich 3D Environment\" Wu et al ICLR 2018 \"Learning and Planning with a Semantic Model\" Wu et al ", "rating": "6: Weak Accept", "reply_text": "We appreciate your constructive feedback that brings in deeper thinking and helps improve the comprehension of this paper . The responses to your comments are shown below . 1.Re : why not try more sophisticated methods of multi-task learning like MAML [ 1 ] . One of the focuses of this paper is to validate multitask learning for VLN and NDH , so we believe , focusing on simplicity and effectiveness is an elegant solution to initiate the study of language grounded navigation tasks along this direction . MAML [ 1 ] has been proved very effective in learning generalizable parameters from multiple tasks that can quickly adapt to a new task ( few-shot learning ) . In this paper , we are solving the standard setup of VLN and NDH , where the model trained on seen environments is supposed to directly applied to unseen environments ( similar to zero-shot learning ) . We agree that adapting MAML on these practical tasks would also be very useful , but we also believe it would not devalue our contributions . [ 1 ] \u201c Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks \u201d , Finn et al.2017 2.Re : why do RL instead of more complicated imitation learning . Anderson et al [ 2 ] introduce an online version of DAgger , student forcing , which imitates the shortest-path actions but takes actions sampled from its own policy . It has been proven to be more effective than teacher forcing ( or behavior cloning ) , but less effective than RL methods [ 3 , 4 ] . Note that imitation learning methods tend to utilize underlying navigation graphs of seen environments more , resulting in higher performance on seen but lower performance on unseen environments . In contrast , RL methods seem to generalize better on unseen environments . But if we modify the evaluation setup and allow pre-exploration of the unseen environments before testing , then one-shot imitation learning method [ 5 ] would fit and can possibly achieve promising performance . [ 2 ] \u201c Vision-and-Language Navigation : Interpreting visually-grounded navigation instructions in real environments \u201d , Anderson et al.2018 [ 3 ] \u201c Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation \u201d Wang et al 2019 [ 4 ] \u201c Learning to Navigate Unseen Environments : Back Translation with Environmental Dropout \u201d , Tan et al.2019 [ 5 ] \u201c One-Shot Imitation Learning via Meta-Learning \u201d , Finn et al.2017 3.Re : more related work . Thanks for pointing out [ 6,7,8 ] in terms of error correction and generalization , we have added them and other related work like [ 9 ] in the revision . [ 6 ] \u201c Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention \u201d , Nguyen et al.2019 [ 7 ] \u201c Building Generalizable Agents with a Realistic and Rich 3D Environment \u201d , Wu et al.2018 [ 8 ] `` Learning and Planning with a Semantic Model '' Wu et al.2018 [ 9 ] \u201c help , anna ! vision-based navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning \u201d , Nguyen et al.2019"}], "0": {"review_id": "HkxzNpNtDS-0", "review_text": "This paper addresses some challenges of following natural language instructions for navigating in visual environments. The main challenge in such tasks is the scarcity of available training data, which results in generalization problems where the agent has difficulty navigating in unseen environments. Therefore, the authors propose two key ideas to tackle this issue and incorporate them in the reinforced cross-modal matching (RCM) model (Wang et al, 2019). First, they use a generalized multitask learning model to transfer knowledge across two different tasks: Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH). This results in learning features that explain both tasks simultaneously and hence generalize better. Moreover, by training on both tasks the effective size of training data is increased significantly. Second, they propose an environment-agnostic learning technique in order to learn invariant representations that are still efficient for navigation. This prevents overfitting to specific visual features of the environment and therefore helps improving generalization. The contribution of this paper is combining and incorporating these two key ideas in the RCM framework and verifying it on VLN and NDH tasks. This approach is novel compared to prior results in tackling the VLN task. Their experimental results show that the two proposed techniques improve generalization in a complementary fashion, measured by decreased performance gap between seen and unseen environments. They demonstrate that their technique outperforms state-of-the-art methods on unseen data on some evaluation metrics. Even though the two key ideas proposed in the paper has been explored in the literature in other contexts, this paper contributes to natural language guided navigation by incorporating these ideas in a unified framework and demonstrates promising results on two datasets. Therefore, I would recommend accepting this paper if some issues in delivery and clarity are addressed. In particular, Section 3, where the authors introduce the novelty of the paper in more detail, could be better explained and a cleaner line should be drawn between prior results from other papers and novel results proposed by this paper. In general, the new ideas would require more emphasis, since they are somewhat lost between adaptations from prior work. Most importantly, Eq. (3) is stated without sufficient motivation and would require a more detailed explanation. In addition to these points, I would like to disclose some recommendations that might improve the paper but are not strictly part of my decision. In some cases the notation is not clear and some variables are not defined or explained. For instance, after Eq. (8) Attention(.) is used without citation or definition, in Eq. (9) Wc and Wu are not defined and some of the notation in Eq. (6)-(7) are not defined. Moreover, reading the decrease in performance gap from the presented table format is inconvenient and a better visual representation might help demonstrating the improvement in generalization better. Lastly, I noticed that the navigation error for shared decoder is slightly higher than for separate encoders in Table 3, even though it outperforms the separate encoders in every other measures. Is there a particular explanation for this?", "rating": "6: Weak Accept", "reply_text": "Thank you for the recommendation and valuable suggestions to improve the clarity of this paper . Below we provide the responses to your suggestions . Note that we have uploaded a paper revision according to the suggestions . 1.Re : Emphasis on new ideas and results We mainly discuss the difference from prior art in Section 2 \u201c background \u201d . As R3 suggested , we explain the motivation of Equation 3 in the revision and draw more clear separation of results of previous work and our new results in Table 1 and Table 5 . We will keep working on the clarity till the camera-ready version if it is accepted . 2.Re : Notations of Equations We have improved the notations and citations , and made them clearer in the revision . 3.Re : Better visualization of performance gaps We added a figure ( Fig 4 ) in the Appendix to better visualize the performance gaps between seen and unseen environments in the revision . 4.Re : Separate encoders vs. shared encoder From Table 3 , we can observe that the shared encoder outperforms the separate encoders almost on all metrics on both seen and unseen environments . The navigation errors on seen environments are actually equivalent ( 5.09 vs. 5.02 , the difference is 0.07 ) . Besides , the navigation error is the average distance between the last node in the predicted path and the target destination , which is often but not necessarily aligned to the primary measures like Success Rate , because 5 meters and 10 meters to the destination are both considered as failure cases ."}, "1": {"review_id": "HkxzNpNtDS-1", "review_text": "This paper aims to apply the model of Wang 2019 to the new NDH task of Thomason '19. Both of these datasets are built on the same room-to-room environment and both are for natural language instruction following. Thomason's work extends the R2R paradigm to include a dialogue history which is collapsed into a single instruction. The contribution of this paper is to build a single model which alternatingly samples trajectories from each of the two datasets to train a more general actor and the authors also believe that the presence of an environment classifier assists in generalization. The claims of the paper focus on being \"environment-agnostic\" and notions of generality. As hinted by the authors in their future work, to properly show this would probably require two different environments or tasks (e.g. Touchdown), not training on two tasks that use the same environments and pre-computed visual features. Am I incorrect that the only difference between the NDH and VLN formulation is the structure of the sentences? Figure 3 is the most compelling component of the paper. However, I am still not convinced it will generalize and all other components of the paper are largely minor tweaks to existing work. Figure 2 mostly leads me to believe that we have a simple data-augmentation situation which makes the bump in performance somewhat predictable. Minor: Is there any reason why in Table 1 we can't simply run the VLN models on NDH and NDH on VLN? I commend the authors for putting this all together in the two months between the release of CVDN and the ICLR deadline. I think training a joint model on these two datasets is a completely natural experiment that many of us wanted to see, and so I appreciate the effort of the authors and the benefit to the community of having these numbers, but I'm not convinced there is that much meat otherwise in this paper.", "rating": "3: Weak Reject", "reply_text": "Thank you for the review and acknowledging the usefulness of this work . We would like to use this opportunity to resolve some potential confusion and misunderstanding . 1.Contributions First , we would like to reiterate the main contributions : ( 1 ) We introduce the first generalized multitask learning framework for natural language grounded navigation tasks such as VLN and NDH , which adopts an interleaved multitask sampling strategy and allows different learning objectives for different tasks . Note that we simultaneously sample data for different tasks within the same mini-batch ( not alternately batch by batch ) to avoid overfitting to individual tasks . ( 2 ) To further improve generalizability , we propose an environment-agnostic training method , which is unified with the multitask learning framework , to learn environment-invariant representations and thus reduce the gap between seen and unseen environments . ( 3 ) We have done thorough and extensive experiments on VLN and NDH tasks and prove that the proposed methods are very effective , which improve the baselines by a large margin and establish new SOTA on NDH ( with ~120 % improvement in terms of goal progress ) . 2.Re : \u201c environments \u201d and \u201c environment-agnostic \u201d We would also like to clarify the notion of \u201c environments \u201d . We use the term \u201c environments \u201d to refer to different houses in the datasets , which are split into seen and unseen sets . The navigation models are trained on seen environments but tested on previously unseen environments . The objective is to improve the performance on unseen environments and reduce the performance gap between them . Therefore , \u201c environment-agnostic \u201d learning is to learn invariant indoor representations among different houses within tasks such as VLN and NDH ( not among different tasks ) . In contrast , the idea of multitask learning is to utilize knowledge across tasks . 3.Re : differences between VLN and NDH Even though both VLN and NDH tasks use the same Matterport3D indoor environments [ 1 ] , there are significant differences in the motivations as well as the overall objectives of the two tasks -- ( 1 ) Collecting data for VLN task involved single human player describing a fixed path through the environment resulting in instructions that can be followed step-by-step to reach the destination . On the other hand , collecting data for NDH involved two human players ( oracle and navigator ) co-operating to find a specified object ( e.g. , \u201c trashcan \u201d ) in the environment . The navigator can elicit assistance from the oracle who can view the navigator \u2019 s path so far and the future 5 steps towards the goal . As a result , NDH involves a series of question/answer interactions ( dialog ) between two players that may not necessarily be step-by-step . NDH is actually a more practical setting than VLN . In real-world applications such as disaster relief , it is impossible to just give a one-time instruction , and send the robot to finish the job , so how to navigate from the interactive dialog history is crucially important . ( 2 ) The two tasks have different data characteristics , e.g. , while the VLN task has an average instruction length of 29 words , the average dialog length in NDH is 81 words . Similarly , the average path length in NDH is much larger ( roughly 3x ) than that in VLN . ( 3 ) The two tasks differ in input representations as well as overall objectives . While VLN instructions can be tokenized and represented as word embeddings , NDH instructions require care in adding markers to indicate start/end of a navigator \u2019 s question and oracle \u2019 s answer . Furthermore , while the objective in VLN is to find the exact goal node in the environment ( i.e. , point navigation ) , the objective in NDH is the find the goal room that contains the specified object ( i.e. , room navigation ) . Due to the above differences , multitask learning involving the two tasks aids learning better representations that improve performance on both tasks simultaneously . The difference in data characteristics leads to effective inductive transfer between the two tasks ( e.g. , MT-RCM model can benefit from following shorter paths in VLN to break down longer paths into smaller achievable chunks in NDH ) . Extending our work to include different environments ( e.g. , Touchdown ) will indeed be a worthy future extension of our work but that should not discount the contributions of our present work . [ 1 ] \u201c Matterport3D : Learning from RGB-D Data in Indoor Environments \u201d , Chang et al.2017"}, "2": {"review_id": "HkxzNpNtDS-2", "review_text": "Summary: There have been two recent related tasks proposed in vision-langauge settings: vision-langauge navigation (VLN) where natural language turn-by-turn instructions must be decoded by an agent in an indoor environment to reach the goal location and Navigation from Dialog History (NDH) where dialog between two humans trying to reach a goal is input to an agent to try to reach a goal location. This paper uses these two tasks' data in a multi-task manner to try to generalize better between indoor environments especially unseen environments which are not in the training set of the agent. Another proposed innovation is to use an auxiliary task of environment classification but via a gradient reversal layer such that the learnt latent representation input to the classifier should not overfit to foibles of the environment but should (hopefully) learn a representation that captures the intrinsics of the environment necessary for generalization. Comments: - The paper is well-written and easy to understand! Experiments are thorough and has all the ablations one would ask for. Thanks! - Overall I like the paper but have a number of comments: 1. Why not try more sophisticated methods of multi-task learning like 'MetaLearning' by Finn et al 2017 (MAML). It is common knowledge that straight up averaging across tasks is not as effective as doing the bilevel optimization in MAML. 2. Why do RL at all? Already the authors are doing BC (naive form of imitation learning) but they could just do robust imitation learning like DAgger, AggreVateD, etc. The setting is already such that one has a natural oracle (which the authors are already using via BC in the objective) which is the shortest path planner during training time. Then combined with MAML one can do Meta-IL as in 'One-Shot Imitation Learning via Meta-Learning' Finn et al 2017. Note that imitation learning is exponenially more sample efficient than RL and removes all the reward-shaping complications. 3. In Section 2 for error correction \"Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention\" by Nguyen et al. CVPR 2019 is directly relevant. 4. Also for generalizaton performance these papers are directly relevant: \"Building Generalizable Agents with a Realistic and Rich 3D Environment\" Wu et al ICLR 2018 \"Learning and Planning with a Semantic Model\" Wu et al ", "rating": "6: Weak Accept", "reply_text": "We appreciate your constructive feedback that brings in deeper thinking and helps improve the comprehension of this paper . The responses to your comments are shown below . 1.Re : why not try more sophisticated methods of multi-task learning like MAML [ 1 ] . One of the focuses of this paper is to validate multitask learning for VLN and NDH , so we believe , focusing on simplicity and effectiveness is an elegant solution to initiate the study of language grounded navigation tasks along this direction . MAML [ 1 ] has been proved very effective in learning generalizable parameters from multiple tasks that can quickly adapt to a new task ( few-shot learning ) . In this paper , we are solving the standard setup of VLN and NDH , where the model trained on seen environments is supposed to directly applied to unseen environments ( similar to zero-shot learning ) . We agree that adapting MAML on these practical tasks would also be very useful , but we also believe it would not devalue our contributions . [ 1 ] \u201c Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks \u201d , Finn et al.2017 2.Re : why do RL instead of more complicated imitation learning . Anderson et al [ 2 ] introduce an online version of DAgger , student forcing , which imitates the shortest-path actions but takes actions sampled from its own policy . It has been proven to be more effective than teacher forcing ( or behavior cloning ) , but less effective than RL methods [ 3 , 4 ] . Note that imitation learning methods tend to utilize underlying navigation graphs of seen environments more , resulting in higher performance on seen but lower performance on unseen environments . In contrast , RL methods seem to generalize better on unseen environments . But if we modify the evaluation setup and allow pre-exploration of the unseen environments before testing , then one-shot imitation learning method [ 5 ] would fit and can possibly achieve promising performance . [ 2 ] \u201c Vision-and-Language Navigation : Interpreting visually-grounded navigation instructions in real environments \u201d , Anderson et al.2018 [ 3 ] \u201c Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation \u201d Wang et al 2019 [ 4 ] \u201c Learning to Navigate Unseen Environments : Back Translation with Environmental Dropout \u201d , Tan et al.2019 [ 5 ] \u201c One-Shot Imitation Learning via Meta-Learning \u201d , Finn et al.2017 3.Re : more related work . Thanks for pointing out [ 6,7,8 ] in terms of error correction and generalization , we have added them and other related work like [ 9 ] in the revision . [ 6 ] \u201c Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention \u201d , Nguyen et al.2019 [ 7 ] \u201c Building Generalizable Agents with a Realistic and Rich 3D Environment \u201d , Wu et al.2018 [ 8 ] `` Learning and Planning with a Semantic Model '' Wu et al.2018 [ 9 ] \u201c help , anna ! vision-based navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning \u201d , Nguyen et al.2019"}}