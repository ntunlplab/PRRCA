{"year": "2021", "forum": "RqCC_00Bg7V", "title": "Blending MPC & Value Function Approximation for Efficient Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "The authors put a lot of effort in replying to questions and improving the paper (to a point that the reviewers felt overwhelmed).\n\nPros:\n- An interesting way of dealing with model bias in MPC\n- They successfully managed to address the most important concerns of the reviewers, with lots of additional experiments and insights\n- R3's concerns have also been successfully addressed by the authors, the review & score were unfortunately not updated\n\nCons:\n- The only remaining point is that the simulations seem to be everything but physically realistic (update at end of R1's review), which is probably a problem of the benchmarks and not the authors faults.", "reviews": [{"review_id": "RqCC_00Bg7V-0", "review_text": "Summary : The paper provides and interesting analysis of and new method for model-predictive control in reinforcement learning . Specifically , it proposes a new framework MPQ ( lambda ) for joining model-based MPC with learned value estimates in RL . The authors develop an formulation to find an optimal prediction horizon and and how this works in an online reinforcement learning framework . The new approach is evaluated on 3 continuous control tasks and compared to some other baselines . -- Score reasoning : This paper has interesting theoretical contributions to multiple areas of machine learning : model-based RL , MPC , and value estimation , but the somewhat limited experimental evaluation make the efficacy of the method more difficult to judge . Overall , the paper is well written and I enjoyed reading it . I now will address my conceptual comments followed by more minor suggestions . -- Rebuttal update : the authors have gone beyond the normal scope of a rebuttal phase to update their experiments and the motivation of the work , and for that reason I have improved my recommendation to be above the acceptance threshold . -- Experimental Validation Questions . I am breaking this section of the review into it 's own section because it is where the majority of my questions are . E1 ) The authors bring up model-based RL algorithms , but do not baseline against other algorithms generally considered sample efficient . PPO is not always easy to use ( authors mention it not converging , and not substantial parameter tuning ) , how about SAC ? E1b ) It would be very interesting to compare something similar to the PETS optimizer for MPC ( cited in intro , but not really mentioned ) . These baseline changes could make the results much more believable . E2 ) `` All parameters were found using a coarse grid search '' This makes the results suspect to me . Please clarify how coarse ? Is the same search space used for all algorithms ? Were defaults used for algorithms with previously published results ? Do the results match ? E3 ) Was tuning of the reward functions done by hand in A.2.1 , or are they referenced elsewhere ? Are states like x-position and pole angle normalized in cartpole ? This can have bigger effects in more complicated environments . E4 ) `` shaded regions represent standard deviation '' for MPPI , is this over the same 30x3 evaluations ? Very important to standard dev . is the number of samples . E5 ) Cartpole swing up is a very similar task . It seems to be the only one where MPQ ( lambda ) substantially outperforms the baselines given ( no error bars on MPQ too ) . How do the ablation studies of figure 2 reproduce on more challenging environments ? -- Comments : 1 ) The authors refer to MPC as a `` simpler , more practical alternative '' to RL or a `` more pragmatic approach '' for `` simple '' policies . Some would argue that RL is a simpler approach because it does not require any model in the case of model-free RL . MPC also has many design decisions such as which optimizer to use or the planning horizon ( multiple papers written on this topic ) . I would like the authors to explain this with more detail , or defend their stance . 2 ) The authors may consider including these two other papers that relate to model-based RL , model-bias , and MPC horizon https : //arxiv.org/pdf/2009.09593.pdf , https : //arxiv.org/pdf/2002.04523.pdf . 3 ) How does model bias differ from model inaccuracy ? In MBRL , model-bias often refers to the model being more accurate in some areas of the state-space than others , and how this impacts the downstream ranking of action choices . Do the authors consider this difference at all ? How does model-accuracy drop when the bias terms are introduced in some basic metrics like mean-squared-error or negative-log-likelihood ( metrics used in MBRL to quantify model-accuracy ) . 4 ) The position of the contribution in related works could be made stronger . I was unaware that MPQ was not the proposal of this paper until section 4 . The difference between the two and why this matters should be in the introduction ( unless the authors decide to add a dedicated related works section ) . 4b ) how does entropy-regularized formulation impact the results ? From my reading , that is an important part of the original MPQ paper , so I think it should be explained . 5 ) The conclusion to this paper is weak . It re-iterates what is done , but the authors should make a case how this impacts developments in robotics & control to better match up with the experiments and introduction . What should I take away from studying this paper ? 6 ) It would be interesting to see the authors propose how to combine the MPQ framework with other forms of MPC that do n't have an implicit terminal cost included . This may be for future work , but I would be interested in a comment . -- Minor comments : 1 ) There are some typos that impede reading , but overall the paper is well written . - intro , paragraph 2 , `` owing to its ability to '' is weird - section 2.2 `` since it plans ... '' it is vague here - some missing commas in first paragraph of section 3 `` First ... '' - Missing period at end of paragraph `` Baselines '' , missing period Figure 3 end , double period before Conclusion - Typo in PPO A.2.2 `` The '' 2 ) in 3.2 , the authors show how to blend the model-based and model-free methods , but point to a reference that is not obviously connected to me and call the approach `` common '' . I would suggest adding more references , or adjusting the claim . 3 ) Why was MPPI chosen as the MPC algorithm ? It is a suitable choice , but could be added . 4 ) there is a lot of visuals in Figures 2 and 3 . Maybe have fewer lines ? The font could be enlarged and it is very confusing that the y-axis 's are not all the same for similar data types . 4b ) Figure 4b ) has strange shading from the MPPI variance - it 's not readable .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for providing valuable feedback on our paper . We address your concerns below # # # Experimental Evaluation * * * * * * * * * * * * * * * * * * * * * * * * * * * E1 ) - * * Model-free Baselines : * * We have updated the paper draft to include the SAC baseline for all three environments and the new results further strengthen our claim regarding the sample efficiency of MPQ ( $ \\lambda $ ) versus model-free RL . Please refer to the \u201c Overall Response \u201d comment for more details . - * * Comparison to PETS Optimizer * * We stress that the main motivation of our work is to improve upon MPC from experience in order to mitigate errors introduced due to model bias , value function error and limited horizon . Our proposed MPQ ( $ \\lambda $ ) framework is theoretically well-founded and systematically trades-off different sources of errors in MPC leading to improved performance over time . In this work , our aim was not to benchmark different MPC algorithms against each other . We provide a novel interpretation of MPC as Q-function approximation and a framework to combine it with learned value functions that can be used with any underlying MPC algorithm . While it is true that a better MPC optimizer could potentially improve performance and is a great avenue for future work , an exhaustive benchmarking of different MPC algorithms is beyond the scope of this paper . Please refer to \u201c Overall Response \u201d for a detailed discussion . E2 ) * * Hyperparameter Tuning Details : * * - MPPI : The temperature , initial covariance and step size parameters for MPPI were tuned with true dynamics using a coarse grid search . Temperature and initial covariance were searched within the range of [ 0.0 , 1.0 ] and step size from [ 0.5,1.0 ] with a discretization of 0.05 for all of them . The number of particles were searched from [ 40,120 ] with a step size of 10 and the horizon was chosen from 4 different values from [ 16 , 20 , 32 , 64 ] . The best performing parameters were then chosen based on average reward over 30 episodes with a fixed seed value to ensure reproducibility . The same parameters were used in the case of biased dynamics and MPQ ( \\lambda ) , to clearly demonstrate that MPQ ( $ \\lambda $ ) can overcome sources of error in the base MPC implementation . - While the CartpoleSwingup and PegInsertion environments do not have previously published results , the InhandManipulation environment was first introduced in [ 1 ] and was used without any modification . MPPI has previously been used to solve this environment in [ 5 ] , and our results match their reported performance in terms of average reward and success rate as shown in Figure 3 ( a ) and 3 ( b ) . Furthermore , we have provided curves for average success rate for PegInsertion ( Figure 3 ( d ) ) along with asymptotic performance of model-free baselines to show that our implementation achieves strong performance on all the tasks . - The only parameters in MPQ ( $ \\lambda $ ) that were tuned were batch size , buffer size and $ \\lambda $ decay rate . For batch size we did a search from [ 16 , 64 ] with a step size of 16 and buffer size was chosen from { 1500 , 3000 , 5000 } . While batch size was tuned for cartpole and then fixed for the remaining two environments , the buffer size was chosen independently for all three . We have added these details to the appendix of the revised draft . Further , we would like to note that even though better settings of parameters might exist , the current settings are sufficient to prove the efficacy of MPQ ( $ \\lambda $ ) in overcoming the major sources of bias ( from model and terminal cost ) in MPC . This can be evidenced from the following - In CartpoleSwingup , MPPI with true dynamics and $ H=32 $ achieves similar average reward as optimal performance of model-free SAC trained for 1M timesteps ( Figure 2 ) . - MPPI with true dynamics reliably solves over 90 % of the problems in InhandManipulation and 100 % of the problems in PegInsertion . - MPQ ( $ \\lambda $ ) achieves similar or better performance as MPPI with true dynamics and model-free RL in the limit in all the problems . E3 ) The simple reward function used for CartpoleSwingup and PegInsertion were tuned by hand since we constructed these environments . The InHandManipulation environment is provided in the open-source code implementation for [ 1 ] and is available at ( https : //github.com/vikashplus/mj_envs ) and was used without any modification . For CartpoleSwingup , states such as x-position were not normalized whereas pole-angle was kept to be between $ [ -\\pi , \\pi ] $ . We have added discussion on this in the Appendix in the revised draft . E4 ) We have updated the plots to include error bars in terms of standard error of the mean for all algorithms . We chose this metric to denote confidence over the estimated average performance of the algorithm by normalizing for finite samples . We believe this is a more representative metric than standard deviation , since it quantifies a bound on average case performance of our algorithm ."}, {"review_id": "RqCC_00Bg7V-1", "review_text": "# # * * Summary : * * The paper proposes to combine MPC and model-free RL to overcome the possible modelling errors . Thereby the approach achieves the sample-efficiency of MPC and the control quality of model-free RL . The resulting MPQ ( \\lambda ) algorithm uses MPPI to obtain the actions by optimizing the blended MPC objective . The Q-targets for fitting the q function also use the blended Q-estimate . # # * * Quality , Originality & Significance : * * The idea of combining MPC and model-free RL is straight forward and not novel ( the paper also does not claim this ) . However , the exact instantiation is novel , very well motivated and feels natural . My biggest concerns are the experiments . The cartpole experiments show the improved performance compared to MPPI on the biased model and the impact of the lambda and model bias . However , the PPO baseline is missing for the cartpole , right ? Furthermore , is PPO a fair comparison for the MPQ ( $ \\lambda $ ) algorithm to evaluate sample complexity ? While PPO is a batched update , the MPQ ( $ \\lambda $ ) uses step based updates . Would n't a model-free step-based update algorithm such as DDPG , SAC etc . be a better baseline to evaluate the improved sample complexity ? Regarding the high-dimensional tasks , the provided evaluations do not enable an evaluation whether the task is solved or not . Could you please provide videos of the final policies , otherwise the achieved reward is just a random number . Furthermore , the paper shows confidence bounds for the MPPI baselines but not for the MPQ ( $ \\lambda $ ) algorithm . Also the learning curves are cut before converging , could you train every instance until convergence ? Furthermore , could you please include the asymptotic performance of your baseline in the plots . The definition of 'validation iteration ' remains unclear . Given the current evaluations the stated claim of applicability to high-dim tasks can not be made as the evaluations are not sufficient . The used modelling bias is also very limited as the paper only compares to biases of the model parameters but not against other sources of biases . Ultimately the increased performance can only be shown on the physical system . # # * * Clarity & Style : * * The paper is really well written and understandable ! A few sections could be improved , e.g. , text between Eq.6 & Eq.8.In this section it is a bit unclear what is expanded and how it is expanded . It would be beneficial to rethink the labeling of the Q-functions as they can be quite confusing . Maybe a table of the different subscript/superscript definitions would simplify the reading , as I had to search for the exact definitions frequently . Furthermore , there are minor styling issues : * Inline equations are consuming too much space to mess up line spacing , e.g , Section 2.1 argmin , Theorem 3.1 , norms , * the min in Equation 13 needs two spaces and a subscript * Experiment O4 ends with two dots * White space around figure 2 can be optimized # # * * Conclusion : * * All in all the paper is nicely written with a clear and well motivated idea of combining MPC & model-free rl . Right now the main problem is the execution of the evaluations . The performance on the high-dim tasks is unclear and the baseline is missing for the cartpole . I would be happy to improve my score to * * * weak accept * * if a step based model free RL algorithm is added to cartpole , claims regarding high-dim tasks are adapted and videos of the high-dimensional tasks are released . * * * accept * * if the high-dimensional tasks are working properly with MPQ ( $ \\lambda $ ) * * * strong accept * * if MPQ ( $ \\lambda $ ) shows this performance on a physical system P.S . You might also try to get medium dimension tasks working such as hopper or cheetah . That might be a bit easier . # # * * Post Discussion Comments : * * So the author did a * * filibuster * * and * * flooded the discussions * * with bloated comments . In this manner it was close to impossible to keep track of anything . * * There has to be character limit for responses otherwise this is not feasible * * . I looked at the videos and your physics simulators looks * * really catchy * * . At one point in time , the pole of the cartpole is at 10-11 o'clock and the cart starts moving right ( the pole has close to no velocity and hence only a very small angular momentum ) . In this setting it would be natural that the pole would fall down if this state is maintained for a longer period ( which it is in the video ) . However the pole goes upwards into the balancing position . This is really weird . And do n't get me started on the pen-orientation as the pen sometimes floats mid-air . For this setting the gravitational constant really does not seem right . I also would n't consider the task solved as this is more an really uncoordinated movements for three specific configurations . For the simulation studies some doubts remain , but the authors improved the paper . Therefore , I am going to increase my score to weak accept . Nevertheless , the experimental evaluation could be improved and the paper would really benefit from real experiments .", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # Experimental Evaluation * * * * * * * * * * * * * * * * * * * * * * * * * * * 1 . * * Model-free baselines , task success evaluation and updated learning curves * * - We have updated the paper draft to include the * * SAC baseline * * for all three environments . Please refer to the \u201c Overall Response \u201d comment for more details . - MPQ ( $ \\lambda $ ) and model-free baselines were * * re-trained for a larger number of iterations * * and the curves have been updated accordingly . - * * Asymptotic performance * * of model-free baseline has also been included ( defined as average reward/success of last 10 iterations ) . - * * Videos of final policies * * after MPQ ( $ \\lambda $ ) training have been uploaded in the supplementary material . - To further demonstrate the ability of MPQ ( $ \\lambda $ ) in solving the high dimensional problems , we have also provided * * curves for average success rate * * achieved by different algorithms on the InHandManipulation and SawyerPegInsertion . Please refer to the \u201c Overall Response \u201d comment for a detailed discussion . The updated results further solidify our claim that MPQ ( $ \\lambda $ ) can mitigate large amounts of model-bias in MPC even in high-dimensional problems . - * * Error bars in terms of standard error of the mean * * have also been provided for all algorithms . We chose this metric to denote confidence over the estimated average performance of the algorithm by normalizing for finite samples . We believe this is a more representative metric than standard deviation , since it quantifies a bound on average case performance of our algorithm . With these updates to the draft we believe we have fulfilled the reviewer \u2019 s criteria for an * * accept * * . We would be happy to answer any follow-up queries about the experimental results . Next , we address the remaining remarks from the reviewer . 2 . * * Meaning of validation iteration * * Validation is performed after every $ N $ training episodes during training for $ N_ { eval } $ episodes using a fixed set of start states that the environment is reset to . We ensure that the same start states are sampled at every validation iteration by setting the seed value to a pre-defined validation seed , which is kept constant across different runs of the algorithm with different training seeds . This helps ensure consistency in evaluating different runs of the algorithm . In all our experiments we use $ N=40 $ and $ N_ { eval } =30 $ . We have added this discussion to the appendix . 3 . * * Modelling Bias * * We agree that the true test for the efficacy of MPQ ( $ \\lambda $ ) is deployment on a physical system , which is our top priority for future work . Although , model bias can manifest in MPC algorithms in several different ways , through our experiments we have tried to test MPQ ( $ \\lambda $ ) on qualitatively different forms of biased models that often occur in the real-world , with the common thread being that biased models can lead to * persistent errors * . In the CartpoleSwingup and InhandManipulation tasks , model bias occurs in the form of unknown dynamics parameters . This is a common problem in robotic control tasks , especially ones that involve a robot manipulating an unknown object . In CartpoleSwingup we specifically set the masses of the cart and pole to lower values than reality which biases our controller to * persistently input smaller controls * than required to swingup the pendulum . On the contrary , in InhandManipulation , we set the mass , inertia and friction coefficients of the pen to be higher than true values . This causes the MPC to * persistently optimize for overly aggressive policies * making it hard to recover from mistakes . The PegInsertion task tests the ability of MPQ ( $ \\lambda $ ) to adapt to errors in perception . Here , we simulate a noisy sensor at the target location inside the hole which provides position measurements corrupted by Gaussian noise with a large standard deviation of 10cm . The MPC algorithm does not simulate sensor noise and uses the current measurement as the true target . This causes MPC to * persistently miss the target * and hover around the hole location . Through our experiments , we demonstrate that MPQ ( $ \\lambda $ ) can successfully overcome qualitatively different kinds of model bias by blending in a value function learned from real-world data . # # # Clarity & Style * * * * * * * * * * * * * * * * * We would like to thank the reviewer for the helpful comments on improving clarity , style and other grammatical errors . We will try to incorporate as many of the suggestions as possible in the final draft given the space constraints . We hope that we have sufficiently addressed the reviewer \u2019 s concerns and incorporated all the suggested updates to experimental evaluation . We would be happy to answer any remaining queries that the reviewer might have ."}, {"review_id": "RqCC_00Bg7V-2", "review_text": "Blending MPC & Value Function Approximation for Efficient Reinforcement Learning review : summarization : In this paper , the authors consider using a blending of Q value which is predicted directly from the neural network , and a Q value which is predicted by unrolling the learnt dynamics . The empirical results suggest improved performance , sample efficiency and good robustness in choosing different values of blending coefficient . Pros : 1.The idea of blending the Q values is novel . I also think the connection to GAE is quite natural and interesting , where both algorithms consider trade off between bias and variance ( in this paper \u2019 s case , bias in learned dynamics ) . 2.The supporting experiments consider some of the interesting questions . For example in section 5.1 , the question of how sensitive lambda is is addressed . 3.The direction of combining value estimation in model-predictive control is interesting and under-explored . That being said , this paper can be inspiring and helpful towards future research . Cons : 1.The experiment section lacks comparison among state-of-the-art algorithms . While MPPI and PPO were generally considered state-of-the-art at the time when they are published ( 2017 ) , their performance is now outperformed heavily given the fast development in the research direction . It would be great if some of the strong baselines between 2019-2020 are included ( SAC , TD3 , MBPO etc . ) . Also given the similarity to MCTS algorithms , it would be more convincing to include one variant of it as a baseline . 2.Experiments on more environments are also appreciated . Questions : I didn \u2019 t see study on the training time and testing time ( how much time needed to generate one action during testing ) , but it seems to be referred to in the introduction ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for investing the time to review our paper and providing valuable feedback . We address your concerns below . # # # Baselines * * * * * * * * * * * * * - * * Model Free Baselines * * : We have updated the paper draft to include the Soft Actor-Critic ( SAC ) baseline for all environments as well as curves for average success rate for high-dimensional InHandManipulation and PegInsertion tasks . Please refer to the `` Overall Response '' comment for more details . - * * Monte-Carlo Tree Search ( MCTS ) Baseline * * : While MCTS-based approaches have demonstrated incredible performance in discrete control problems such as game playing [ 1 , 2 ] , it is not straightforward to extend them to continuous control problems such as the ones studied in this paper . Although approaches such as POMCP [ 3 ] do attempt to extend MCTS to continuous state spaces , the action space is still limited to be discrete and the applicability beyond toy problems is largely unexplored . Furthermore , we would like to stress that the main aim of the work is to provide a general framework for dealing with model-bias in MPC and not to compare different MPC algorithms against each other . While it is true that a better MPC approach could potentially improve performance , an exhaustive comparison is beyond the scope of this paper . Please refer to the \u201c Overall Response \u201d comment for a detailed discussion . # # # Timing Benchmark * * * * * * * * * * * * * * * * * * * * * In this work , we focused solely on the problem of mitigating the effects of different sources of bias on the performance of MPC algorithms and not on providing a highly efficient implementation for a particular MPC algorithm . Such implementations often require a significant amount of engineering effort and domain expertise . For example , in the case of sampling based controllers such as MPPI , approaches such as [ 4 ] are able to achieve real-time control by employing significant GPU acceleration along with neural network dynamics models . The choice of programming language can have a great impact on the speed of the controller as well . Our current implementation is based on Python which allows us to leverage efficient deep learning libraries such as PyTorch , but makes the controller slower compared to an implementation based on a compiled language such as C++ . Since , the speed of the controller does not affect the assertions and empirical results in the current paper , we do not believe that it would be informative to include a timing analysis with the current implementation . As part of future work , we aim to deploy MPQ ( $ \\lambda $ ) on a real-robot platform which would warrant a more in-depth study of different engineering trade-offs involved in coming up with an efficient MPC implementation . An example of recent work that attempts to provide a nice middle-ground is [ 5 ] , where the authors provide a highly efficient yet easy to use Julia framework for MPC and RL algorithms . We hope that our updated experiments and responses satisfactorily address the reviewers concerns and they would consider updating the score on that basis . We look forward to more in-depth discussions regarding the work . * * References * * [ 1 ] Silver , David , Aja Huang , Chris J. Maddison , Arthur Guez , Laurent Sifre , George Van Den Driessche , Julian Schrittwieser et al . `` Mastering the game of Go with deep neural networks and tree search . '' nature 529 , no . 7587 ( 2016 ) : 484-489 . [ 2 ] Silver , David , Thomas Hubert , Julian Schrittwieser , Ioannis Antonoglou , Matthew Lai , Arthur Guez , Marc Lanctot et al . `` Mastering chess and shogi by self-play with a general reinforcement learning algorithm . '' arXiv preprint arXiv:1712.01815 ( 2017 ) . [ 3 ] Silver , David , and Joel Veness . `` Monte-Carlo planning in large POMDPs . '' In Advances in neural information processing systems , pp . 2164-2172 . 2010 . [ 4 ] Williams , Grady and Wagener , Nolan and Goldfain , Brian and Drews , Paul and Rehg , James M and Boots , Byron and Theodorou , Evangelos A . Information theoretic MPC for model-based reinforcement learning . 2017 IEEE International Conference on Robotics and Automation ( ICRA ) [ 5 ] Summers , Colin , Kendall Lowrey , Aravind Rajeswaran , Siddhartha Srinivasa , and Emanuel Todorov . `` Lyceum : An efficient and scalable ecosystem for robot learning . '' arXiv preprint arXiv:2001.07343 ( 2020 ) ."}, {"review_id": "RqCC_00Bg7V-3", "review_text": "# # Summary The paper describes an algorithm to tackle model bias in the MPC . They address the question of optimal horizon length as well the model errors observed in MPC based systems . The paper is well motivated and written in clear concise manner . Experiments with cart-pole and robot models demonstrate the practical feasibility of the proposed method . # # # Strong Points 1 . Good description of the sources of errors in MPC based models 2 . The Theorems are useful though I was not able to check their algebraic accuracy , the limiting constructs are intuitively correct . 3.Choosing the horizon limit and tackling model errors are import challenges for MPC and the method proposed in this paper would be a good addition to the knowledge , hence recommendation for accept . # # # To improve 1 . I am not sure the MPPI is the SOTA baseline for this comparison , there are other MPC methods that achieve better results than MPPI . 2.Although citations from the machine learning community seem to be covered the standard MPC literature seem to be completely ignored . At the bare minimum , when speak of stability and fast horizon planning , no reference to tube based MPC [ see Mayne 2011 ] 3 . Major advantage of MPC is the ability to deal with constraints again see [ Mayne et ref 2 below ] . These are not recent developments but classic position papers that address lot of questions you pose and attempt to answer in the paper . 4.The figures need a significant improvements . In their current form the plots are too thin to read them correctly . # # Refs 1 . Mayne , D.Q. , Kerrigan , E.C. , van Wyk , E.J . and Falugi , P. ( 2011 ) , Tube\u2010based robust nonlinear model predictive control . Int.J.Robust Nonlinear Control , 21 : 1341-1353. doi:10.1002/rnc.1758 2 . D.Q.Mayne , J.B. Rawlings , C.V. Rao , P.O.M . Scokaert , Constrained model predictive control : Stability and optimality , Automatica , Volume 36 , Issue 6,2000 , Pages 789-814 , ISSN 0005-1098 ,", "rating": "7: Good paper, accept", "reply_text": "Thank you for reviewing our paper and providing valuable feedback . We are glad that you found the paper well motivated and clear to understand . We address your concerns below # # # Citations for key MPC papers * * * * * * * * * * * We agree that the paper can be made stronger by adding in more citations to relevant papers from MPC literature such as tube MPC and other MPC approaches that focus on safety and constraints satisfaction . We will incorporate references to the papers mentioned by the reviewer as well as a more in-depth review of related MPC literature in the final version . # # # Figures * * * * * * * * * * * We note the lack of clarity in the figures and have updated them in the current revision of the paper accordingly ."}], "0": {"review_id": "RqCC_00Bg7V-0", "review_text": "Summary : The paper provides and interesting analysis of and new method for model-predictive control in reinforcement learning . Specifically , it proposes a new framework MPQ ( lambda ) for joining model-based MPC with learned value estimates in RL . The authors develop an formulation to find an optimal prediction horizon and and how this works in an online reinforcement learning framework . The new approach is evaluated on 3 continuous control tasks and compared to some other baselines . -- Score reasoning : This paper has interesting theoretical contributions to multiple areas of machine learning : model-based RL , MPC , and value estimation , but the somewhat limited experimental evaluation make the efficacy of the method more difficult to judge . Overall , the paper is well written and I enjoyed reading it . I now will address my conceptual comments followed by more minor suggestions . -- Rebuttal update : the authors have gone beyond the normal scope of a rebuttal phase to update their experiments and the motivation of the work , and for that reason I have improved my recommendation to be above the acceptance threshold . -- Experimental Validation Questions . I am breaking this section of the review into it 's own section because it is where the majority of my questions are . E1 ) The authors bring up model-based RL algorithms , but do not baseline against other algorithms generally considered sample efficient . PPO is not always easy to use ( authors mention it not converging , and not substantial parameter tuning ) , how about SAC ? E1b ) It would be very interesting to compare something similar to the PETS optimizer for MPC ( cited in intro , but not really mentioned ) . These baseline changes could make the results much more believable . E2 ) `` All parameters were found using a coarse grid search '' This makes the results suspect to me . Please clarify how coarse ? Is the same search space used for all algorithms ? Were defaults used for algorithms with previously published results ? Do the results match ? E3 ) Was tuning of the reward functions done by hand in A.2.1 , or are they referenced elsewhere ? Are states like x-position and pole angle normalized in cartpole ? This can have bigger effects in more complicated environments . E4 ) `` shaded regions represent standard deviation '' for MPPI , is this over the same 30x3 evaluations ? Very important to standard dev . is the number of samples . E5 ) Cartpole swing up is a very similar task . It seems to be the only one where MPQ ( lambda ) substantially outperforms the baselines given ( no error bars on MPQ too ) . How do the ablation studies of figure 2 reproduce on more challenging environments ? -- Comments : 1 ) The authors refer to MPC as a `` simpler , more practical alternative '' to RL or a `` more pragmatic approach '' for `` simple '' policies . Some would argue that RL is a simpler approach because it does not require any model in the case of model-free RL . MPC also has many design decisions such as which optimizer to use or the planning horizon ( multiple papers written on this topic ) . I would like the authors to explain this with more detail , or defend their stance . 2 ) The authors may consider including these two other papers that relate to model-based RL , model-bias , and MPC horizon https : //arxiv.org/pdf/2009.09593.pdf , https : //arxiv.org/pdf/2002.04523.pdf . 3 ) How does model bias differ from model inaccuracy ? In MBRL , model-bias often refers to the model being more accurate in some areas of the state-space than others , and how this impacts the downstream ranking of action choices . Do the authors consider this difference at all ? How does model-accuracy drop when the bias terms are introduced in some basic metrics like mean-squared-error or negative-log-likelihood ( metrics used in MBRL to quantify model-accuracy ) . 4 ) The position of the contribution in related works could be made stronger . I was unaware that MPQ was not the proposal of this paper until section 4 . The difference between the two and why this matters should be in the introduction ( unless the authors decide to add a dedicated related works section ) . 4b ) how does entropy-regularized formulation impact the results ? From my reading , that is an important part of the original MPQ paper , so I think it should be explained . 5 ) The conclusion to this paper is weak . It re-iterates what is done , but the authors should make a case how this impacts developments in robotics & control to better match up with the experiments and introduction . What should I take away from studying this paper ? 6 ) It would be interesting to see the authors propose how to combine the MPQ framework with other forms of MPC that do n't have an implicit terminal cost included . This may be for future work , but I would be interested in a comment . -- Minor comments : 1 ) There are some typos that impede reading , but overall the paper is well written . - intro , paragraph 2 , `` owing to its ability to '' is weird - section 2.2 `` since it plans ... '' it is vague here - some missing commas in first paragraph of section 3 `` First ... '' - Missing period at end of paragraph `` Baselines '' , missing period Figure 3 end , double period before Conclusion - Typo in PPO A.2.2 `` The '' 2 ) in 3.2 , the authors show how to blend the model-based and model-free methods , but point to a reference that is not obviously connected to me and call the approach `` common '' . I would suggest adding more references , or adjusting the claim . 3 ) Why was MPPI chosen as the MPC algorithm ? It is a suitable choice , but could be added . 4 ) there is a lot of visuals in Figures 2 and 3 . Maybe have fewer lines ? The font could be enlarged and it is very confusing that the y-axis 's are not all the same for similar data types . 4b ) Figure 4b ) has strange shading from the MPPI variance - it 's not readable .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for providing valuable feedback on our paper . We address your concerns below # # # Experimental Evaluation * * * * * * * * * * * * * * * * * * * * * * * * * * * E1 ) - * * Model-free Baselines : * * We have updated the paper draft to include the SAC baseline for all three environments and the new results further strengthen our claim regarding the sample efficiency of MPQ ( $ \\lambda $ ) versus model-free RL . Please refer to the \u201c Overall Response \u201d comment for more details . - * * Comparison to PETS Optimizer * * We stress that the main motivation of our work is to improve upon MPC from experience in order to mitigate errors introduced due to model bias , value function error and limited horizon . Our proposed MPQ ( $ \\lambda $ ) framework is theoretically well-founded and systematically trades-off different sources of errors in MPC leading to improved performance over time . In this work , our aim was not to benchmark different MPC algorithms against each other . We provide a novel interpretation of MPC as Q-function approximation and a framework to combine it with learned value functions that can be used with any underlying MPC algorithm . While it is true that a better MPC optimizer could potentially improve performance and is a great avenue for future work , an exhaustive benchmarking of different MPC algorithms is beyond the scope of this paper . Please refer to \u201c Overall Response \u201d for a detailed discussion . E2 ) * * Hyperparameter Tuning Details : * * - MPPI : The temperature , initial covariance and step size parameters for MPPI were tuned with true dynamics using a coarse grid search . Temperature and initial covariance were searched within the range of [ 0.0 , 1.0 ] and step size from [ 0.5,1.0 ] with a discretization of 0.05 for all of them . The number of particles were searched from [ 40,120 ] with a step size of 10 and the horizon was chosen from 4 different values from [ 16 , 20 , 32 , 64 ] . The best performing parameters were then chosen based on average reward over 30 episodes with a fixed seed value to ensure reproducibility . The same parameters were used in the case of biased dynamics and MPQ ( \\lambda ) , to clearly demonstrate that MPQ ( $ \\lambda $ ) can overcome sources of error in the base MPC implementation . - While the CartpoleSwingup and PegInsertion environments do not have previously published results , the InhandManipulation environment was first introduced in [ 1 ] and was used without any modification . MPPI has previously been used to solve this environment in [ 5 ] , and our results match their reported performance in terms of average reward and success rate as shown in Figure 3 ( a ) and 3 ( b ) . Furthermore , we have provided curves for average success rate for PegInsertion ( Figure 3 ( d ) ) along with asymptotic performance of model-free baselines to show that our implementation achieves strong performance on all the tasks . - The only parameters in MPQ ( $ \\lambda $ ) that were tuned were batch size , buffer size and $ \\lambda $ decay rate . For batch size we did a search from [ 16 , 64 ] with a step size of 16 and buffer size was chosen from { 1500 , 3000 , 5000 } . While batch size was tuned for cartpole and then fixed for the remaining two environments , the buffer size was chosen independently for all three . We have added these details to the appendix of the revised draft . Further , we would like to note that even though better settings of parameters might exist , the current settings are sufficient to prove the efficacy of MPQ ( $ \\lambda $ ) in overcoming the major sources of bias ( from model and terminal cost ) in MPC . This can be evidenced from the following - In CartpoleSwingup , MPPI with true dynamics and $ H=32 $ achieves similar average reward as optimal performance of model-free SAC trained for 1M timesteps ( Figure 2 ) . - MPPI with true dynamics reliably solves over 90 % of the problems in InhandManipulation and 100 % of the problems in PegInsertion . - MPQ ( $ \\lambda $ ) achieves similar or better performance as MPPI with true dynamics and model-free RL in the limit in all the problems . E3 ) The simple reward function used for CartpoleSwingup and PegInsertion were tuned by hand since we constructed these environments . The InHandManipulation environment is provided in the open-source code implementation for [ 1 ] and is available at ( https : //github.com/vikashplus/mj_envs ) and was used without any modification . For CartpoleSwingup , states such as x-position were not normalized whereas pole-angle was kept to be between $ [ -\\pi , \\pi ] $ . We have added discussion on this in the Appendix in the revised draft . E4 ) We have updated the plots to include error bars in terms of standard error of the mean for all algorithms . We chose this metric to denote confidence over the estimated average performance of the algorithm by normalizing for finite samples . We believe this is a more representative metric than standard deviation , since it quantifies a bound on average case performance of our algorithm ."}, "1": {"review_id": "RqCC_00Bg7V-1", "review_text": "# # * * Summary : * * The paper proposes to combine MPC and model-free RL to overcome the possible modelling errors . Thereby the approach achieves the sample-efficiency of MPC and the control quality of model-free RL . The resulting MPQ ( \\lambda ) algorithm uses MPPI to obtain the actions by optimizing the blended MPC objective . The Q-targets for fitting the q function also use the blended Q-estimate . # # * * Quality , Originality & Significance : * * The idea of combining MPC and model-free RL is straight forward and not novel ( the paper also does not claim this ) . However , the exact instantiation is novel , very well motivated and feels natural . My biggest concerns are the experiments . The cartpole experiments show the improved performance compared to MPPI on the biased model and the impact of the lambda and model bias . However , the PPO baseline is missing for the cartpole , right ? Furthermore , is PPO a fair comparison for the MPQ ( $ \\lambda $ ) algorithm to evaluate sample complexity ? While PPO is a batched update , the MPQ ( $ \\lambda $ ) uses step based updates . Would n't a model-free step-based update algorithm such as DDPG , SAC etc . be a better baseline to evaluate the improved sample complexity ? Regarding the high-dimensional tasks , the provided evaluations do not enable an evaluation whether the task is solved or not . Could you please provide videos of the final policies , otherwise the achieved reward is just a random number . Furthermore , the paper shows confidence bounds for the MPPI baselines but not for the MPQ ( $ \\lambda $ ) algorithm . Also the learning curves are cut before converging , could you train every instance until convergence ? Furthermore , could you please include the asymptotic performance of your baseline in the plots . The definition of 'validation iteration ' remains unclear . Given the current evaluations the stated claim of applicability to high-dim tasks can not be made as the evaluations are not sufficient . The used modelling bias is also very limited as the paper only compares to biases of the model parameters but not against other sources of biases . Ultimately the increased performance can only be shown on the physical system . # # * * Clarity & Style : * * The paper is really well written and understandable ! A few sections could be improved , e.g. , text between Eq.6 & Eq.8.In this section it is a bit unclear what is expanded and how it is expanded . It would be beneficial to rethink the labeling of the Q-functions as they can be quite confusing . Maybe a table of the different subscript/superscript definitions would simplify the reading , as I had to search for the exact definitions frequently . Furthermore , there are minor styling issues : * Inline equations are consuming too much space to mess up line spacing , e.g , Section 2.1 argmin , Theorem 3.1 , norms , * the min in Equation 13 needs two spaces and a subscript * Experiment O4 ends with two dots * White space around figure 2 can be optimized # # * * Conclusion : * * All in all the paper is nicely written with a clear and well motivated idea of combining MPC & model-free rl . Right now the main problem is the execution of the evaluations . The performance on the high-dim tasks is unclear and the baseline is missing for the cartpole . I would be happy to improve my score to * * * weak accept * * if a step based model free RL algorithm is added to cartpole , claims regarding high-dim tasks are adapted and videos of the high-dimensional tasks are released . * * * accept * * if the high-dimensional tasks are working properly with MPQ ( $ \\lambda $ ) * * * strong accept * * if MPQ ( $ \\lambda $ ) shows this performance on a physical system P.S . You might also try to get medium dimension tasks working such as hopper or cheetah . That might be a bit easier . # # * * Post Discussion Comments : * * So the author did a * * filibuster * * and * * flooded the discussions * * with bloated comments . In this manner it was close to impossible to keep track of anything . * * There has to be character limit for responses otherwise this is not feasible * * . I looked at the videos and your physics simulators looks * * really catchy * * . At one point in time , the pole of the cartpole is at 10-11 o'clock and the cart starts moving right ( the pole has close to no velocity and hence only a very small angular momentum ) . In this setting it would be natural that the pole would fall down if this state is maintained for a longer period ( which it is in the video ) . However the pole goes upwards into the balancing position . This is really weird . And do n't get me started on the pen-orientation as the pen sometimes floats mid-air . For this setting the gravitational constant really does not seem right . I also would n't consider the task solved as this is more an really uncoordinated movements for three specific configurations . For the simulation studies some doubts remain , but the authors improved the paper . Therefore , I am going to increase my score to weak accept . Nevertheless , the experimental evaluation could be improved and the paper would really benefit from real experiments .", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # Experimental Evaluation * * * * * * * * * * * * * * * * * * * * * * * * * * * 1 . * * Model-free baselines , task success evaluation and updated learning curves * * - We have updated the paper draft to include the * * SAC baseline * * for all three environments . Please refer to the \u201c Overall Response \u201d comment for more details . - MPQ ( $ \\lambda $ ) and model-free baselines were * * re-trained for a larger number of iterations * * and the curves have been updated accordingly . - * * Asymptotic performance * * of model-free baseline has also been included ( defined as average reward/success of last 10 iterations ) . - * * Videos of final policies * * after MPQ ( $ \\lambda $ ) training have been uploaded in the supplementary material . - To further demonstrate the ability of MPQ ( $ \\lambda $ ) in solving the high dimensional problems , we have also provided * * curves for average success rate * * achieved by different algorithms on the InHandManipulation and SawyerPegInsertion . Please refer to the \u201c Overall Response \u201d comment for a detailed discussion . The updated results further solidify our claim that MPQ ( $ \\lambda $ ) can mitigate large amounts of model-bias in MPC even in high-dimensional problems . - * * Error bars in terms of standard error of the mean * * have also been provided for all algorithms . We chose this metric to denote confidence over the estimated average performance of the algorithm by normalizing for finite samples . We believe this is a more representative metric than standard deviation , since it quantifies a bound on average case performance of our algorithm . With these updates to the draft we believe we have fulfilled the reviewer \u2019 s criteria for an * * accept * * . We would be happy to answer any follow-up queries about the experimental results . Next , we address the remaining remarks from the reviewer . 2 . * * Meaning of validation iteration * * Validation is performed after every $ N $ training episodes during training for $ N_ { eval } $ episodes using a fixed set of start states that the environment is reset to . We ensure that the same start states are sampled at every validation iteration by setting the seed value to a pre-defined validation seed , which is kept constant across different runs of the algorithm with different training seeds . This helps ensure consistency in evaluating different runs of the algorithm . In all our experiments we use $ N=40 $ and $ N_ { eval } =30 $ . We have added this discussion to the appendix . 3 . * * Modelling Bias * * We agree that the true test for the efficacy of MPQ ( $ \\lambda $ ) is deployment on a physical system , which is our top priority for future work . Although , model bias can manifest in MPC algorithms in several different ways , through our experiments we have tried to test MPQ ( $ \\lambda $ ) on qualitatively different forms of biased models that often occur in the real-world , with the common thread being that biased models can lead to * persistent errors * . In the CartpoleSwingup and InhandManipulation tasks , model bias occurs in the form of unknown dynamics parameters . This is a common problem in robotic control tasks , especially ones that involve a robot manipulating an unknown object . In CartpoleSwingup we specifically set the masses of the cart and pole to lower values than reality which biases our controller to * persistently input smaller controls * than required to swingup the pendulum . On the contrary , in InhandManipulation , we set the mass , inertia and friction coefficients of the pen to be higher than true values . This causes the MPC to * persistently optimize for overly aggressive policies * making it hard to recover from mistakes . The PegInsertion task tests the ability of MPQ ( $ \\lambda $ ) to adapt to errors in perception . Here , we simulate a noisy sensor at the target location inside the hole which provides position measurements corrupted by Gaussian noise with a large standard deviation of 10cm . The MPC algorithm does not simulate sensor noise and uses the current measurement as the true target . This causes MPC to * persistently miss the target * and hover around the hole location . Through our experiments , we demonstrate that MPQ ( $ \\lambda $ ) can successfully overcome qualitatively different kinds of model bias by blending in a value function learned from real-world data . # # # Clarity & Style * * * * * * * * * * * * * * * * * We would like to thank the reviewer for the helpful comments on improving clarity , style and other grammatical errors . We will try to incorporate as many of the suggestions as possible in the final draft given the space constraints . We hope that we have sufficiently addressed the reviewer \u2019 s concerns and incorporated all the suggested updates to experimental evaluation . We would be happy to answer any remaining queries that the reviewer might have ."}, "2": {"review_id": "RqCC_00Bg7V-2", "review_text": "Blending MPC & Value Function Approximation for Efficient Reinforcement Learning review : summarization : In this paper , the authors consider using a blending of Q value which is predicted directly from the neural network , and a Q value which is predicted by unrolling the learnt dynamics . The empirical results suggest improved performance , sample efficiency and good robustness in choosing different values of blending coefficient . Pros : 1.The idea of blending the Q values is novel . I also think the connection to GAE is quite natural and interesting , where both algorithms consider trade off between bias and variance ( in this paper \u2019 s case , bias in learned dynamics ) . 2.The supporting experiments consider some of the interesting questions . For example in section 5.1 , the question of how sensitive lambda is is addressed . 3.The direction of combining value estimation in model-predictive control is interesting and under-explored . That being said , this paper can be inspiring and helpful towards future research . Cons : 1.The experiment section lacks comparison among state-of-the-art algorithms . While MPPI and PPO were generally considered state-of-the-art at the time when they are published ( 2017 ) , their performance is now outperformed heavily given the fast development in the research direction . It would be great if some of the strong baselines between 2019-2020 are included ( SAC , TD3 , MBPO etc . ) . Also given the similarity to MCTS algorithms , it would be more convincing to include one variant of it as a baseline . 2.Experiments on more environments are also appreciated . Questions : I didn \u2019 t see study on the training time and testing time ( how much time needed to generate one action during testing ) , but it seems to be referred to in the introduction ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for investing the time to review our paper and providing valuable feedback . We address your concerns below . # # # Baselines * * * * * * * * * * * * * - * * Model Free Baselines * * : We have updated the paper draft to include the Soft Actor-Critic ( SAC ) baseline for all environments as well as curves for average success rate for high-dimensional InHandManipulation and PegInsertion tasks . Please refer to the `` Overall Response '' comment for more details . - * * Monte-Carlo Tree Search ( MCTS ) Baseline * * : While MCTS-based approaches have demonstrated incredible performance in discrete control problems such as game playing [ 1 , 2 ] , it is not straightforward to extend them to continuous control problems such as the ones studied in this paper . Although approaches such as POMCP [ 3 ] do attempt to extend MCTS to continuous state spaces , the action space is still limited to be discrete and the applicability beyond toy problems is largely unexplored . Furthermore , we would like to stress that the main aim of the work is to provide a general framework for dealing with model-bias in MPC and not to compare different MPC algorithms against each other . While it is true that a better MPC approach could potentially improve performance , an exhaustive comparison is beyond the scope of this paper . Please refer to the \u201c Overall Response \u201d comment for a detailed discussion . # # # Timing Benchmark * * * * * * * * * * * * * * * * * * * * * In this work , we focused solely on the problem of mitigating the effects of different sources of bias on the performance of MPC algorithms and not on providing a highly efficient implementation for a particular MPC algorithm . Such implementations often require a significant amount of engineering effort and domain expertise . For example , in the case of sampling based controllers such as MPPI , approaches such as [ 4 ] are able to achieve real-time control by employing significant GPU acceleration along with neural network dynamics models . The choice of programming language can have a great impact on the speed of the controller as well . Our current implementation is based on Python which allows us to leverage efficient deep learning libraries such as PyTorch , but makes the controller slower compared to an implementation based on a compiled language such as C++ . Since , the speed of the controller does not affect the assertions and empirical results in the current paper , we do not believe that it would be informative to include a timing analysis with the current implementation . As part of future work , we aim to deploy MPQ ( $ \\lambda $ ) on a real-robot platform which would warrant a more in-depth study of different engineering trade-offs involved in coming up with an efficient MPC implementation . An example of recent work that attempts to provide a nice middle-ground is [ 5 ] , where the authors provide a highly efficient yet easy to use Julia framework for MPC and RL algorithms . We hope that our updated experiments and responses satisfactorily address the reviewers concerns and they would consider updating the score on that basis . We look forward to more in-depth discussions regarding the work . * * References * * [ 1 ] Silver , David , Aja Huang , Chris J. Maddison , Arthur Guez , Laurent Sifre , George Van Den Driessche , Julian Schrittwieser et al . `` Mastering the game of Go with deep neural networks and tree search . '' nature 529 , no . 7587 ( 2016 ) : 484-489 . [ 2 ] Silver , David , Thomas Hubert , Julian Schrittwieser , Ioannis Antonoglou , Matthew Lai , Arthur Guez , Marc Lanctot et al . `` Mastering chess and shogi by self-play with a general reinforcement learning algorithm . '' arXiv preprint arXiv:1712.01815 ( 2017 ) . [ 3 ] Silver , David , and Joel Veness . `` Monte-Carlo planning in large POMDPs . '' In Advances in neural information processing systems , pp . 2164-2172 . 2010 . [ 4 ] Williams , Grady and Wagener , Nolan and Goldfain , Brian and Drews , Paul and Rehg , James M and Boots , Byron and Theodorou , Evangelos A . Information theoretic MPC for model-based reinforcement learning . 2017 IEEE International Conference on Robotics and Automation ( ICRA ) [ 5 ] Summers , Colin , Kendall Lowrey , Aravind Rajeswaran , Siddhartha Srinivasa , and Emanuel Todorov . `` Lyceum : An efficient and scalable ecosystem for robot learning . '' arXiv preprint arXiv:2001.07343 ( 2020 ) ."}, "3": {"review_id": "RqCC_00Bg7V-3", "review_text": "# # Summary The paper describes an algorithm to tackle model bias in the MPC . They address the question of optimal horizon length as well the model errors observed in MPC based systems . The paper is well motivated and written in clear concise manner . Experiments with cart-pole and robot models demonstrate the practical feasibility of the proposed method . # # # Strong Points 1 . Good description of the sources of errors in MPC based models 2 . The Theorems are useful though I was not able to check their algebraic accuracy , the limiting constructs are intuitively correct . 3.Choosing the horizon limit and tackling model errors are import challenges for MPC and the method proposed in this paper would be a good addition to the knowledge , hence recommendation for accept . # # # To improve 1 . I am not sure the MPPI is the SOTA baseline for this comparison , there are other MPC methods that achieve better results than MPPI . 2.Although citations from the machine learning community seem to be covered the standard MPC literature seem to be completely ignored . At the bare minimum , when speak of stability and fast horizon planning , no reference to tube based MPC [ see Mayne 2011 ] 3 . Major advantage of MPC is the ability to deal with constraints again see [ Mayne et ref 2 below ] . These are not recent developments but classic position papers that address lot of questions you pose and attempt to answer in the paper . 4.The figures need a significant improvements . In their current form the plots are too thin to read them correctly . # # Refs 1 . Mayne , D.Q. , Kerrigan , E.C. , van Wyk , E.J . and Falugi , P. ( 2011 ) , Tube\u2010based robust nonlinear model predictive control . Int.J.Robust Nonlinear Control , 21 : 1341-1353. doi:10.1002/rnc.1758 2 . D.Q.Mayne , J.B. Rawlings , C.V. Rao , P.O.M . Scokaert , Constrained model predictive control : Stability and optimality , Automatica , Volume 36 , Issue 6,2000 , Pages 789-814 , ISSN 0005-1098 ,", "rating": "7: Good paper, accept", "reply_text": "Thank you for reviewing our paper and providing valuable feedback . We are glad that you found the paper well motivated and clear to understand . We address your concerns below # # # Citations for key MPC papers * * * * * * * * * * * We agree that the paper can be made stronger by adding in more citations to relevant papers from MPC literature such as tube MPC and other MPC approaches that focus on safety and constraints satisfaction . We will incorporate references to the papers mentioned by the reviewer as well as a more in-depth review of related MPC literature in the final version . # # # Figures * * * * * * * * * * * We note the lack of clarity in the figures and have updated them in the current revision of the paper accordingly ."}}