{"year": "2019", "forum": "rylDfnCqF7", "title": "Lagging Inference Networks and Posterior Collapse in Variational Autoencoders", "decision": "Accept (Poster)", "meta_review": "This paper introduces a method that aims to solve the problem of 'posterior collapse' in variational autoencoders (VAEs). The problem of posterior collapse is well-documented in the VAE literature, and various solutions have been proposed. Existing proposed solutions, however, aim to solve the problem by either changing the objective function (e.g. beta-VAE) or by changing the prior and/or approximate posterior models. The proposed method, in contrast, aims to solve the problem by bringing the VAE optimization procedure closer to the EM optimization procedure. Every iteration in optimization consists of SGD updates to the inference model (E-step), performed until the approximate posterior converges. This is followed by a single SGD update of the generative model. The multi-update E-step makes sure that the M-step optimizes something closer to the marginal log-likelihood, compared to what we would normaly do in VAEs (joint optimization of both inference model and generative model).\n\nThe experiments are relatively small-scale, but convincing.\n\nThe reviewers agree that the method is clearly described, and that the proposed technique is well supported by the experiments. We think that this work will probably be of high interest to the ICLR community.", "reviews": [{"review_id": "rylDfnCqF7-0", "review_text": "This work looks into the phenomenon of posterior collapse, and shows that training the inference network more can reduce this problem, and lead to better optima. The exposition is clear. The proposed training procedure is simple and effective. Experiments were carried out in multiple settings, though I would've liked to see more analysis. Overall, I think this is a nice contribution. I have some concerns which I hope the authors can address. Comments: - I think [1] should be cited as they first mentioned Eq 5 and also performed similar analysis. - Were you able to form an unbiased estimate for the log of the aggregate posterior which is used extensively in this paper (e.g. MI)? Some recents works also estimate this but they use biased estimators. If your estimator is biased, please add a sentence clarifying this so readers aren't mislead. - Apart from KL and (biased?) MI, a metric I really would've liked to see is the number of active/inactive units as measured in [2]. I think this is a more reliable and very explainable metric for posterior collapse, whereas real-valued information-theoretic quantites can be hard to interpret. Questions: - Has this approach truly completely solved posterior collapse? (e.g. can you show that the mutual information between z and x is maximal or the number of inactive units is zero?) - How robust is this approach to the effects of randomness during training such as initialization and use of minibatches? (e.g. can you show some standard deviations of the metrics you report in Table 1?) - (minor) I wasn't able to understand why the top right is optimal, as opposed to anywhere on the dashed line, in Figures 1(b) and 3? [1] Hoffman, Matthew D., and Matthew J. Johnson. \"Elbo surgery: yet another way to carve up the variational evidence lower bound.\" [2] Burda, Yuri, Roger Grosse, and Ruslan Salakhutdinov. \"Importance weighted autoencoders.\" --REVISION-- The paper has significantly improved since the revision and I am happy to increase my score. I do still think that the claim of \"preventing\" or \"avoiding\" posterior collapse is too strong, as I agree with the authors that \"it is unknown whether there is a better local optimum that [activates] more or all latent units\". I would suggest not to emphasize it too strongly (ie. in the abstract) or using words like \"reducing\" or \"mitigate\" instead.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your comments ! Your review is helpful and we are currently running additional experiments based on some of your suggestions . This will take some time and we will submit a revised version once we collect the results . For now we will quickly answer some of your questions , and describe our revision plan given your concerns . # # Q1 : Estimator of MI The estimator we used for MI is biased because the estimator for the log of the aggregate posterior is biased . More specifically , it is a Monte Carlo estimate of an upper bound on MI . In future revisions we will be sure to provide more details and point to related work that uses the same estimate of MI . Thanks for catching the lack of detail here -- this was an oversight on our part . # # Q2 : Active units Great idea ! We are currently re-running experiments and keeping track of active units . In the revised version , we will include this measure for all models in Table 1 . # # Q3 : Robustness to the effects of randomness We agree that quantifying robustness to initialization is important . We are currently re-running all the models with different random seeds . Once these experiments complete , we will update the draft with mean and variance across random restarts . In our implementation , different random seeds lead to different initialization and minibatch traversal . # # Q4 : Presentation suggestions ( 1 ) Thanks for pointing out this related paper [ 1 ] . We will be sure cite and include it in the discussion of related work . ( 2 ) Actually , the optimum in this cartoon might be anywhere on the dashed x=y line depending on the data and specific shape of the objective . We intended Figure 1 ( b ) to convey that the global optimum is not located at origin , that the origin is a local optimum , and that the global optimum is somewhere on the dashed x=y line . In Figure 3 we arbitrarily chose to show a point that happens to move to top right , which must had added to the confusion . Thanks for catching this ambiguity . We will clarify the meaning of these figures in future revisions . [ 1 ] Hoffman , Matthew D. , and Matthew J. Johnson . `` Elbo surgery : yet another way to carve up the variational evidence lower bound . ''"}, {"review_id": "rylDfnCqF7-1", "review_text": "Response to Authors ------------- I've read all other reviews and the author responses. Most responses to my issues seem to be \"we will run more experiments\", so my review scores haven't changed. I'm glad the authors are planning many revised experiments, and I understand that these take time. It's too bad revised results won't be available before the review revision deadline (tomorrow 11/26). I guess I'm willing to take the author's promises to update in good faith. Thus, I think this is an \"accept\", but only if the authors really do follow through on promises to add uncertainty quantification and include some complete comparisons to KL annealing strategies. Review Summary -------------- Overall, I think the paper offers a reasonable story for why its proposed innovation -- an alternative scheduling of parameter-specific updates where encoder parameters are always trained to convergence during early iterations -- might offer a reliable way to avoid posterior collapse that is far faster and easier-to-implement than other options that require some per-example iterations (e.g. semi-amortized VAE). My biggest concerns are that relative performance gains (in bound quality) over alternatives are not too large and hard to judge as significant because no uncertainty in these estimates is quantified. Additionally, I'd like to see more careful evaluation of the KL annealing baseline and more attention to within-model comparisons (do you really need to update until convergence?). Given the method's simplicity and speed, I think with a satisfactory rebuttal and plan for revision I would lean towards acceptance. Paper Summary ------------- The paper investigates a common problem known as \"posterior collapse\" observed when training generative models such as VAEs (Kingma & Welling 2014) with high-capacity neural networks. Posterior collapse occurs when the encoder distribution q(z|x) (parameterized by a NN) becomes indistinguishable from the generative prior on codes p(z), which is often a local optima of the VI ELBO objective. While other better fixed points exist, once this one is reached during optimization it is hard to escape using the typical local gradient steps for VAEs that jointly update the parameters of an encoder and a decoder with each gradient step. The proposed solution (presented in Alg. 1) is to avoid joint gradient updates early in training, and instead use an alternating update scheme where after each single-gradient-step decoder parameter update, the encoder is updated with as many gradient steps as are needed to reach convergence. This proposed scheme, which the paper terms \"aggressive updates\", forces the encoder to better approximate the true posterior p(z|x) at each step. Experiments study a synthetic task where visualizing the evolution of true posterior mean of p(z|x) side-by-side with approximate q(z|x) is possible in 2D, as well as benchmark comparisons to several other methods that address posterior collapse on text modeling (Yahoo, Yelp15) and image modeling (Omniglot). Studied baselines include annealing the KL term in the VI objective, the \\beta VAE (which keeps the KL term fixed with a weight \\beta), and semi-amortized VAEs (SA-VAEs, Kim et al. 2018). The presented approach is said to reach better values of the log likelihood while also being ~10x faster to train than the Kim et al. approach on large datasets. Significance and Originality ---------------------------- There exists strong interest in deploying amortized VI to fit sophisticated models efficiently while avoiding posterior collapse, so the topic is definitely relevant to ICLR. Certainly solutions to this issue are welcome, though I worry with the crowded field that performance is starting to saturate and it is becoming hard to identify significant vs. marginal contributions. Thus it's important to interpret results across multiple axes (e.g. speed and heldout likelihood). The paper does a nice job of highlighting related work on this problem, and I'd rate its methodological contributions as clearly distinct from prior work, even though the eventual procedure is simple. The closest related works in my view are: * Krishnan et al. AISTATS 2018, where VAE joint-training algorithms for nonlinear factor analysis problems are shown to be improved by an algorithm that uses the encoder NN as an *initialization* and then doing several standard SVI updates to refine per-example parameters. Encoder parameters are updated via gradient updates, *after* the decoder parameters are updated (not jointly). * SA-VAEs (Kim et al. ICML 2018) which studies VAEs for deep text models and develops an algorithm that at each a new batch uses the encoder to initialize per-example parameters, updates these via several iterations of SVI, then *backpropagates* through those updates to compute a gradient update of the encoder NN. Compared to these, the detailed algorithm presented in this work is both distinct and simpler. It does not require any per-example parameter updates, instead it only requires a different scheduling of when encoder and decoder NN updates occur. Concerns about Technical Quality (prioritized) ---------------------------------------------- ## C1: Without error bars in Table 1 and 3, hard to know which gaps are significant Are 500 Monte Carlo samples enough to be sure that the numbers reported in Table 1 are precise estimates and not too noisy? How much error is there in the estimation of various quantities like the NLL or the KL if we repeated 500-MC samples 5x or 10x or 25x? My experience is that even with 100 or more samples, evaluations of the ELBO bound for classic VAEs can differ non-trivally. I'd like to see evidence that these quantities are estimated with certainty, or (even better) some direct reporting of the uncertainties across several estimates. ## C2: Baseline comparison to KL annealing needs to be more thorough The current paper dismisses the strategy that annealing the KL term as ineffective in addressing posterior collapse (e.g. VAE + anneal has a 0.0 KL term in Table 1). However, it's not clear that a reasonable annealing schedule was used, or even that any reasonable effort was made to try more than one schedule. For example, if we set the KL term to exactly 0.0 weight, the optimization has no incentive to push q towards the prior, and thus posterior collapse *cannot* occur. It may be that this leads to other problems, but it's unclear to me why a schedule that keeps the KL term weight exactly at 0 for a few updates and then gradually increases the weight should lead to collapse. To me, the KL annealing story is much simpler than the presented approach and I think as a community we should invest in giving it a fair shot. If the answer is that annealing takes too long or the schedule is tough to tune, that's sensible, but I think the claim that annealing still leads to collapse just means the schedule probably wasn't set right. Notice that \"Ours\" is improved by \"Ours+Annealing\" for 2 datasets in Table 1. So annealing *can* be effective. Krishnan et al. 2018's Supplementary Fig. 10 suggests that if annealing is slow enough (unfolding over 100000 updates instead of 10000 updates), then KL annealing will get close to pure SVI in effective, non-collapsed posterior approximation. The present paper's Sec. B.3 indicates that the attempted annealing schedule was 0.1 to 1.0 linearly over 10 epochs with batch size 32 and train set size 100k, which sounds like only 30k updates of annealing were performed. I'd suggest comparing against KL annealing that both starts with a smaller weight (perhaps exactly at 0.0) and grows much slower. ## C3: Results do not analyze variability due to random initialization or random minibatch traversal Many factors can impact the final performance values of a model trained via VI, including the random initialization of its parameters and the random order of minibatches used during gradient updates. Due to local optima, often best practice is to take the best of many separate initializations (see several figures in Bishop's PRML textbook). The present paper doesn't make clear whether it's reporting single runs or the best of many runs. I suggest a revision is needed to clarify. Quantifying robustness to initialization is important. ## C4: Results do not analyze relative sensitivity of encoder and decoder to using the same learning rate One possible explanation for \"lagging\" might be that the gradient vectors of the encoder and the decoder have different magnitudes, and thus using the same fixed learning rate for both (as seems to be done from a skim of Sec. B) might not be optimal. Perhaps a quick experiment that separately tunes learning rates of encoder and decoder is necessary? If the learning rate for encoder is too small, this could easily explain the lagging when using joint updates. ## C5: Is it necessary to update until convergence? Or would a fixed budget of 25 or 100 updates to the encoder suffice? In Alg. 1, during the \"aggressive\" phase the encoder is updated until convergence. I'd like to see some coverage of how long this typically takes (10 updates? 100 updates?). I'd also like to know if there are significant time-savings to be had by not going *all* the way to convergence. It's concerning that in Fig. 1 convergence on a toy dataset takes more than 2000 iterations. ## C6: Sensitivity to the initialization of the encoder is not discussed and could matter In the synthetic example figure, it seems the encoder is initialized so that across many examples, the typical encoding will be near the origin and thus favored under the prior. Thus, the *initialization* is in some ways setting optimization up for posterior collapse. I wonder if some more diverse initialization might avoid the problem. Presentation comments --------------------- Overall the paper reads reasonably. I'd suggest mentioning the KL annealing comparison a bit earlier, but otherwise I have few complaints. I'm not sure I like the chosen terminology of \"aggressive\" update. The procedure is more accurately a \"repeat-until-convergence\" update. There's nothing aggressive about it, it's just repeated. Line-by-line Detailed comments ------------------------------ Citations for \"traditional\" VI with per-example parameters should go much further back than 2013. For example, Matthew Beal's thesis, work by Blei in 2003 on LDA, or work by MacKay or M.I. Jordan or others even further back. Alg 1 Line 12: This update should be to \\theta (model parameters), not \\phi (approx posterior parameters). Alg 1: Might consider using notation like g_\\theta to denote the grad. of specific parameters, rather than have the same symbol \"g\" overloaded as the gradient of \\theta, \\phi, and both in the same Algo. Fig. 3: This is interesting, but I think it's missing something as a visualization of the algorithm. There's nothing obvious visually that indicates the encoder update involves *many* steps, but the decoder update is only one step. I'd suggest at least turning each vertical arrow into *many* short arrows stacked end-to-end, indicating many steps. Also use a different color (not green for both). Fig. 4: Shows various quantities like KL(q, prior) traced over optimization. This figure would be more illuminating if it also showed the complete ELBO objective and the expected log likelihood term. Then it would be clear why annealing is failing to avoid posterior collapse. Table 1: How exactly is the negative log likelihood (NLL) computed? Is it the expected value of the data likelihood: -1 * E_q[log p(x|z)]? Or is it the variational lower bound on marginal likelihood? ", "rating": "7: Good paper, accept", "reply_text": "# # Q4 : Separate learning rates of encoder and decoder This is a good point ! When we first observed the `` lagging '' behaviour we also found that the gradient of the encoder and decoder had very different magnitudes . We tried doing exactly what you propose : tuning the learning rates for the encoder and decoder separately , as well as experimenting with alternative optimization methods as potential solutions -- but nothing worked . We realize that readers might be curious about this matter , thus we will include further discussion in the paper and additional negative experimental results as support . # # Q5 : Is it necessary to update until convergence ? This is a good question ! In practice , of course , we never reach * exact * convergence , thus the question is really about how close to convergence is required in the inner loop update . In our current implementation , we break the inner loop when the ELBO objective stays the same or decreases across 10 iterations . Note that we do n't perform separate learning rate decay in the inner loop so this convergence condition is not strict , but empirically we found it to be sufficient . Across all four datasets ( on synthetic , Yahoo , Yelp , and OMNIGLOT ) in practice this yields roughly 30 - 100 updates per inner loop update . We also want to clarify that Fig.2 does n't imply our approach takes 2000 updates to converge in one single inner loop , the notation `` iter '' in Figure 2 represents outer loop iterations instead of inner loop iterations ( we will clarify this in future revisions -- thank you for pointing out the ambiguity ) . In preliminary experiments we tried using a fixed budget of encoder updates , similar to the approach you suggest . While not reported in the current revision , our takeaway from these experiments was the following : ( 1 ) Generally speaking , the final model fit is better when the encoder update is near convergence . ( 2 ) Performing a sufficient number of updates above some threshold in the inner loop is * critical * for avoiding posterior collapse -- we found that this `` sufficient number '' is sensitive to dataset and model architecture . ( 3 ) We found , empirically , that the minimal fixed budget of inner loop iterations required to avoid posterior collapse was not meaningfully smaller than the number of updates resulting from our proposed approach and implementation . Therefore , we concluded that the fixed budget approach would not lead to worthwhile speedups in practice , and that our simpler proposed approach represents a good tradeoff between performance and speed . We will include this discussion in future revisions . # # Q6 : Initialization of encoder We had also considered whether a different initialization for the encoder might help avoid posterior collapse , but did not conduct experiments to test this hypothesis . Considering your concern , we plan to at least conduct experiments where we initialize all the encoder parameters to positive values ( so that the approximate posterior mean is not located at origin upon initialization ) . We will discuss this point in future revisions and include experimental results if they are interesting ."}, {"review_id": "rylDfnCqF7-2", "review_text": "General: The paper tackles one of the most important problems of learning VAEs, namely, the posterior collapse. Typically, this problem is attacked by either proposing a new model or modifying the objective. Interestingly, the authors considered a third option, i.e., changing the training procedure only, leaving the model and the objective untouched. Moreover, they show that in fact the modified objective (beta-VAE) could drastically harm training a VAE. I find the idea very interesting and promising. The proposed algorithm is very easy to be applied, thus, it could be easily reproduced. I believe the paper should be presented at the ICLR 2019. Pros: + The paper is written in a lucid manner. All ideas are clearly presented. I find the toy problem (Figure 2) very illuminating. + It might seem that the idea follows from simple if not even trivial remarks. But this impression is fully due to the fashion the authors presented their idea. I am truly impressed by the writing style of the authors. + I find the proposed approach very appealing because it requires changes only in the optimization procedure while the model and the objective remain the same. Moreover, the paper formalizes some intuition that could be found in other papers (e.g., (Alemi et al., 2018)). + The presented results are fully convincing. Cons: - It would be beneficial to see samples for the same latent variables to verify whether the model utilizes the latent code. Additionally, a latent space interpolation could be also presented. - The choice of the stopping criterion seems to be rather arbitrary. Did the authors try other methods? If yes, what were they? If not, why the current stopping criterion is so unique? - The proposed approach was applied to the case when the prior is a standard Normal. What would happen if a different prior is considered? Neutral remark: * Another problem, next to the posterior collapse, is the \u201chole problem\u201d (see Rezende & Viola, \u201cTaming VAEs\u201d, 2018). A natural question is whether the proposed approach also helps to solve this issue? One possible solution to that problem is to take the aggregated posterior as the prior (e.g., (Tomczak & Welling, 2018)) or to ensure that the KL between the aggregated posterior and the prior is small. In Figure 4 it seems it is the case, however, I am really curious about the authors\u2019 opinion on this matter. * Can the authors relate the proposed algorithm to the wake-sleep algorithm? Obviously, the motivation is different, however, I find these two approaches a bit similar in spirit. --REVISION-- I would like to thank the authors for their comments. In my opinion the paper is very interesting and opens new directions for further research (as discussed by the authors in their reply). I strongly believe the paper should be accepted and presented at the ICLR.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your encouraging comments and advice ! Currently we are running additional experiments to address some of the reviewer comments . This is taking some time and we will submit a revised version once we have collected all the results . For now , we will quickly answer some of your questions . # # Q1 : Latent variable interpretation We agree that providing samples would be informative . We plan to add these experiments , along with additional analysis aimed at uncovering how the latent codes are used by the generative model . # # Q2 : Choice of stopping criterion In addition to the presented stopping criterion , we first tried switching back to traditional VAE training after a fixed number of epochs -- i.e.early stopping . We found that this approach can also work well , but introduces an additional hyperparameter ( number of epochs ) that is sensitive to datasets and model architectures . We found that stopping too early hurts the performance , and stopping too late of course hurts speed . This tradeoff needs to be tuned if epochs are specified explicitly . Intuitively , posterior collapse ( and a \u201c lagging \u201d encoder ) correspond to a lack of \u201c dependence \u201d between posterior samples and observed data . Based on its use in related literature , we experimented with mutual information ( MI ) as a simple quantitative surrogate for \u201c dependence \u201d . Stopping the aggressive training phase after MI stops increasing monotonically worked well in practice and avoided the need for data or model dependent tuning . Across multiple settings we found the proposed stopping criterion does n't sacrifice performance and maintains fast training . We agree that further analysis would be interesting and suspect that similar measurements of dependence and related stopping criteria might also strike a successful balance . # # Q3 : Different prior The effect of our approach under different priors would certainly be interesting to see , but is a bit beyond the scope of the current paper . We may explore this direction in future work . # # Q4 : Hole problem Our analysis and empirical results were focused specifically on the problem of posterior collapse . We agree , however , that it would be interesting to explore how the proposed procedure ( and related modifications to optimization ) might affect other known issues with VAE . We hope to explore this in the future welcome any suggestions for how to do so ! Regarding the \u201c hole \u201d problem : We were not aware of this paper , thank you for sharing it with us . Our current experimental results demonstrate that the proposed approach is able to maintain a relatively small KL ( q ( z ) | p ( z ) ) , but this real-valued quantity is hard to interpret . We think that it is necessary to visualize the aggregated posterior and prior ( or use another more direct metric ) to check if the proposed approach helps solve the `` hole problem '' . # # Q5 : Connection to the wake-sleep algorithm Good point ! The proposed algorithm is similar to the wake-sleep algorithm in the sense that we split encoder and decoder optimization into separate phases . Essentially , both the proposed algorithm and the wake-sleep algorithm are instances of block-coordinate ascent . The decoder update in the proposed method is analogous to the wake phase : the ELBO objective corresponds to the wake phase objective with an additional regularization term from the prior on code z . The encoder update in the proposed method is analogous to the sleep phase where decoder is fixed -- though , here , the ELBO objective is somewhat different from the sleep phase objective which aims to recover hidden code z instead of observations x ."}], "0": {"review_id": "rylDfnCqF7-0", "review_text": "This work looks into the phenomenon of posterior collapse, and shows that training the inference network more can reduce this problem, and lead to better optima. The exposition is clear. The proposed training procedure is simple and effective. Experiments were carried out in multiple settings, though I would've liked to see more analysis. Overall, I think this is a nice contribution. I have some concerns which I hope the authors can address. Comments: - I think [1] should be cited as they first mentioned Eq 5 and also performed similar analysis. - Were you able to form an unbiased estimate for the log of the aggregate posterior which is used extensively in this paper (e.g. MI)? Some recents works also estimate this but they use biased estimators. If your estimator is biased, please add a sentence clarifying this so readers aren't mislead. - Apart from KL and (biased?) MI, a metric I really would've liked to see is the number of active/inactive units as measured in [2]. I think this is a more reliable and very explainable metric for posterior collapse, whereas real-valued information-theoretic quantites can be hard to interpret. Questions: - Has this approach truly completely solved posterior collapse? (e.g. can you show that the mutual information between z and x is maximal or the number of inactive units is zero?) - How robust is this approach to the effects of randomness during training such as initialization and use of minibatches? (e.g. can you show some standard deviations of the metrics you report in Table 1?) - (minor) I wasn't able to understand why the top right is optimal, as opposed to anywhere on the dashed line, in Figures 1(b) and 3? [1] Hoffman, Matthew D., and Matthew J. Johnson. \"Elbo surgery: yet another way to carve up the variational evidence lower bound.\" [2] Burda, Yuri, Roger Grosse, and Ruslan Salakhutdinov. \"Importance weighted autoencoders.\" --REVISION-- The paper has significantly improved since the revision and I am happy to increase my score. I do still think that the claim of \"preventing\" or \"avoiding\" posterior collapse is too strong, as I agree with the authors that \"it is unknown whether there is a better local optimum that [activates] more or all latent units\". I would suggest not to emphasize it too strongly (ie. in the abstract) or using words like \"reducing\" or \"mitigate\" instead.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your comments ! Your review is helpful and we are currently running additional experiments based on some of your suggestions . This will take some time and we will submit a revised version once we collect the results . For now we will quickly answer some of your questions , and describe our revision plan given your concerns . # # Q1 : Estimator of MI The estimator we used for MI is biased because the estimator for the log of the aggregate posterior is biased . More specifically , it is a Monte Carlo estimate of an upper bound on MI . In future revisions we will be sure to provide more details and point to related work that uses the same estimate of MI . Thanks for catching the lack of detail here -- this was an oversight on our part . # # Q2 : Active units Great idea ! We are currently re-running experiments and keeping track of active units . In the revised version , we will include this measure for all models in Table 1 . # # Q3 : Robustness to the effects of randomness We agree that quantifying robustness to initialization is important . We are currently re-running all the models with different random seeds . Once these experiments complete , we will update the draft with mean and variance across random restarts . In our implementation , different random seeds lead to different initialization and minibatch traversal . # # Q4 : Presentation suggestions ( 1 ) Thanks for pointing out this related paper [ 1 ] . We will be sure cite and include it in the discussion of related work . ( 2 ) Actually , the optimum in this cartoon might be anywhere on the dashed x=y line depending on the data and specific shape of the objective . We intended Figure 1 ( b ) to convey that the global optimum is not located at origin , that the origin is a local optimum , and that the global optimum is somewhere on the dashed x=y line . In Figure 3 we arbitrarily chose to show a point that happens to move to top right , which must had added to the confusion . Thanks for catching this ambiguity . We will clarify the meaning of these figures in future revisions . [ 1 ] Hoffman , Matthew D. , and Matthew J. Johnson . `` Elbo surgery : yet another way to carve up the variational evidence lower bound . ''"}, "1": {"review_id": "rylDfnCqF7-1", "review_text": "Response to Authors ------------- I've read all other reviews and the author responses. Most responses to my issues seem to be \"we will run more experiments\", so my review scores haven't changed. I'm glad the authors are planning many revised experiments, and I understand that these take time. It's too bad revised results won't be available before the review revision deadline (tomorrow 11/26). I guess I'm willing to take the author's promises to update in good faith. Thus, I think this is an \"accept\", but only if the authors really do follow through on promises to add uncertainty quantification and include some complete comparisons to KL annealing strategies. Review Summary -------------- Overall, I think the paper offers a reasonable story for why its proposed innovation -- an alternative scheduling of parameter-specific updates where encoder parameters are always trained to convergence during early iterations -- might offer a reliable way to avoid posterior collapse that is far faster and easier-to-implement than other options that require some per-example iterations (e.g. semi-amortized VAE). My biggest concerns are that relative performance gains (in bound quality) over alternatives are not too large and hard to judge as significant because no uncertainty in these estimates is quantified. Additionally, I'd like to see more careful evaluation of the KL annealing baseline and more attention to within-model comparisons (do you really need to update until convergence?). Given the method's simplicity and speed, I think with a satisfactory rebuttal and plan for revision I would lean towards acceptance. Paper Summary ------------- The paper investigates a common problem known as \"posterior collapse\" observed when training generative models such as VAEs (Kingma & Welling 2014) with high-capacity neural networks. Posterior collapse occurs when the encoder distribution q(z|x) (parameterized by a NN) becomes indistinguishable from the generative prior on codes p(z), which is often a local optima of the VI ELBO objective. While other better fixed points exist, once this one is reached during optimization it is hard to escape using the typical local gradient steps for VAEs that jointly update the parameters of an encoder and a decoder with each gradient step. The proposed solution (presented in Alg. 1) is to avoid joint gradient updates early in training, and instead use an alternating update scheme where after each single-gradient-step decoder parameter update, the encoder is updated with as many gradient steps as are needed to reach convergence. This proposed scheme, which the paper terms \"aggressive updates\", forces the encoder to better approximate the true posterior p(z|x) at each step. Experiments study a synthetic task where visualizing the evolution of true posterior mean of p(z|x) side-by-side with approximate q(z|x) is possible in 2D, as well as benchmark comparisons to several other methods that address posterior collapse on text modeling (Yahoo, Yelp15) and image modeling (Omniglot). Studied baselines include annealing the KL term in the VI objective, the \\beta VAE (which keeps the KL term fixed with a weight \\beta), and semi-amortized VAEs (SA-VAEs, Kim et al. 2018). The presented approach is said to reach better values of the log likelihood while also being ~10x faster to train than the Kim et al. approach on large datasets. Significance and Originality ---------------------------- There exists strong interest in deploying amortized VI to fit sophisticated models efficiently while avoiding posterior collapse, so the topic is definitely relevant to ICLR. Certainly solutions to this issue are welcome, though I worry with the crowded field that performance is starting to saturate and it is becoming hard to identify significant vs. marginal contributions. Thus it's important to interpret results across multiple axes (e.g. speed and heldout likelihood). The paper does a nice job of highlighting related work on this problem, and I'd rate its methodological contributions as clearly distinct from prior work, even though the eventual procedure is simple. The closest related works in my view are: * Krishnan et al. AISTATS 2018, where VAE joint-training algorithms for nonlinear factor analysis problems are shown to be improved by an algorithm that uses the encoder NN as an *initialization* and then doing several standard SVI updates to refine per-example parameters. Encoder parameters are updated via gradient updates, *after* the decoder parameters are updated (not jointly). * SA-VAEs (Kim et al. ICML 2018) which studies VAEs for deep text models and develops an algorithm that at each a new batch uses the encoder to initialize per-example parameters, updates these via several iterations of SVI, then *backpropagates* through those updates to compute a gradient update of the encoder NN. Compared to these, the detailed algorithm presented in this work is both distinct and simpler. It does not require any per-example parameter updates, instead it only requires a different scheduling of when encoder and decoder NN updates occur. Concerns about Technical Quality (prioritized) ---------------------------------------------- ## C1: Without error bars in Table 1 and 3, hard to know which gaps are significant Are 500 Monte Carlo samples enough to be sure that the numbers reported in Table 1 are precise estimates and not too noisy? How much error is there in the estimation of various quantities like the NLL or the KL if we repeated 500-MC samples 5x or 10x or 25x? My experience is that even with 100 or more samples, evaluations of the ELBO bound for classic VAEs can differ non-trivally. I'd like to see evidence that these quantities are estimated with certainty, or (even better) some direct reporting of the uncertainties across several estimates. ## C2: Baseline comparison to KL annealing needs to be more thorough The current paper dismisses the strategy that annealing the KL term as ineffective in addressing posterior collapse (e.g. VAE + anneal has a 0.0 KL term in Table 1). However, it's not clear that a reasonable annealing schedule was used, or even that any reasonable effort was made to try more than one schedule. For example, if we set the KL term to exactly 0.0 weight, the optimization has no incentive to push q towards the prior, and thus posterior collapse *cannot* occur. It may be that this leads to other problems, but it's unclear to me why a schedule that keeps the KL term weight exactly at 0 for a few updates and then gradually increases the weight should lead to collapse. To me, the KL annealing story is much simpler than the presented approach and I think as a community we should invest in giving it a fair shot. If the answer is that annealing takes too long or the schedule is tough to tune, that's sensible, but I think the claim that annealing still leads to collapse just means the schedule probably wasn't set right. Notice that \"Ours\" is improved by \"Ours+Annealing\" for 2 datasets in Table 1. So annealing *can* be effective. Krishnan et al. 2018's Supplementary Fig. 10 suggests that if annealing is slow enough (unfolding over 100000 updates instead of 10000 updates), then KL annealing will get close to pure SVI in effective, non-collapsed posterior approximation. The present paper's Sec. B.3 indicates that the attempted annealing schedule was 0.1 to 1.0 linearly over 10 epochs with batch size 32 and train set size 100k, which sounds like only 30k updates of annealing were performed. I'd suggest comparing against KL annealing that both starts with a smaller weight (perhaps exactly at 0.0) and grows much slower. ## C3: Results do not analyze variability due to random initialization or random minibatch traversal Many factors can impact the final performance values of a model trained via VI, including the random initialization of its parameters and the random order of minibatches used during gradient updates. Due to local optima, often best practice is to take the best of many separate initializations (see several figures in Bishop's PRML textbook). The present paper doesn't make clear whether it's reporting single runs or the best of many runs. I suggest a revision is needed to clarify. Quantifying robustness to initialization is important. ## C4: Results do not analyze relative sensitivity of encoder and decoder to using the same learning rate One possible explanation for \"lagging\" might be that the gradient vectors of the encoder and the decoder have different magnitudes, and thus using the same fixed learning rate for both (as seems to be done from a skim of Sec. B) might not be optimal. Perhaps a quick experiment that separately tunes learning rates of encoder and decoder is necessary? If the learning rate for encoder is too small, this could easily explain the lagging when using joint updates. ## C5: Is it necessary to update until convergence? Or would a fixed budget of 25 or 100 updates to the encoder suffice? In Alg. 1, during the \"aggressive\" phase the encoder is updated until convergence. I'd like to see some coverage of how long this typically takes (10 updates? 100 updates?). I'd also like to know if there are significant time-savings to be had by not going *all* the way to convergence. It's concerning that in Fig. 1 convergence on a toy dataset takes more than 2000 iterations. ## C6: Sensitivity to the initialization of the encoder is not discussed and could matter In the synthetic example figure, it seems the encoder is initialized so that across many examples, the typical encoding will be near the origin and thus favored under the prior. Thus, the *initialization* is in some ways setting optimization up for posterior collapse. I wonder if some more diverse initialization might avoid the problem. Presentation comments --------------------- Overall the paper reads reasonably. I'd suggest mentioning the KL annealing comparison a bit earlier, but otherwise I have few complaints. I'm not sure I like the chosen terminology of \"aggressive\" update. The procedure is more accurately a \"repeat-until-convergence\" update. There's nothing aggressive about it, it's just repeated. Line-by-line Detailed comments ------------------------------ Citations for \"traditional\" VI with per-example parameters should go much further back than 2013. For example, Matthew Beal's thesis, work by Blei in 2003 on LDA, or work by MacKay or M.I. Jordan or others even further back. Alg 1 Line 12: This update should be to \\theta (model parameters), not \\phi (approx posterior parameters). Alg 1: Might consider using notation like g_\\theta to denote the grad. of specific parameters, rather than have the same symbol \"g\" overloaded as the gradient of \\theta, \\phi, and both in the same Algo. Fig. 3: This is interesting, but I think it's missing something as a visualization of the algorithm. There's nothing obvious visually that indicates the encoder update involves *many* steps, but the decoder update is only one step. I'd suggest at least turning each vertical arrow into *many* short arrows stacked end-to-end, indicating many steps. Also use a different color (not green for both). Fig. 4: Shows various quantities like KL(q, prior) traced over optimization. This figure would be more illuminating if it also showed the complete ELBO objective and the expected log likelihood term. Then it would be clear why annealing is failing to avoid posterior collapse. Table 1: How exactly is the negative log likelihood (NLL) computed? Is it the expected value of the data likelihood: -1 * E_q[log p(x|z)]? Or is it the variational lower bound on marginal likelihood? ", "rating": "7: Good paper, accept", "reply_text": "# # Q4 : Separate learning rates of encoder and decoder This is a good point ! When we first observed the `` lagging '' behaviour we also found that the gradient of the encoder and decoder had very different magnitudes . We tried doing exactly what you propose : tuning the learning rates for the encoder and decoder separately , as well as experimenting with alternative optimization methods as potential solutions -- but nothing worked . We realize that readers might be curious about this matter , thus we will include further discussion in the paper and additional negative experimental results as support . # # Q5 : Is it necessary to update until convergence ? This is a good question ! In practice , of course , we never reach * exact * convergence , thus the question is really about how close to convergence is required in the inner loop update . In our current implementation , we break the inner loop when the ELBO objective stays the same or decreases across 10 iterations . Note that we do n't perform separate learning rate decay in the inner loop so this convergence condition is not strict , but empirically we found it to be sufficient . Across all four datasets ( on synthetic , Yahoo , Yelp , and OMNIGLOT ) in practice this yields roughly 30 - 100 updates per inner loop update . We also want to clarify that Fig.2 does n't imply our approach takes 2000 updates to converge in one single inner loop , the notation `` iter '' in Figure 2 represents outer loop iterations instead of inner loop iterations ( we will clarify this in future revisions -- thank you for pointing out the ambiguity ) . In preliminary experiments we tried using a fixed budget of encoder updates , similar to the approach you suggest . While not reported in the current revision , our takeaway from these experiments was the following : ( 1 ) Generally speaking , the final model fit is better when the encoder update is near convergence . ( 2 ) Performing a sufficient number of updates above some threshold in the inner loop is * critical * for avoiding posterior collapse -- we found that this `` sufficient number '' is sensitive to dataset and model architecture . ( 3 ) We found , empirically , that the minimal fixed budget of inner loop iterations required to avoid posterior collapse was not meaningfully smaller than the number of updates resulting from our proposed approach and implementation . Therefore , we concluded that the fixed budget approach would not lead to worthwhile speedups in practice , and that our simpler proposed approach represents a good tradeoff between performance and speed . We will include this discussion in future revisions . # # Q6 : Initialization of encoder We had also considered whether a different initialization for the encoder might help avoid posterior collapse , but did not conduct experiments to test this hypothesis . Considering your concern , we plan to at least conduct experiments where we initialize all the encoder parameters to positive values ( so that the approximate posterior mean is not located at origin upon initialization ) . We will discuss this point in future revisions and include experimental results if they are interesting ."}, "2": {"review_id": "rylDfnCqF7-2", "review_text": "General: The paper tackles one of the most important problems of learning VAEs, namely, the posterior collapse. Typically, this problem is attacked by either proposing a new model or modifying the objective. Interestingly, the authors considered a third option, i.e., changing the training procedure only, leaving the model and the objective untouched. Moreover, they show that in fact the modified objective (beta-VAE) could drastically harm training a VAE. I find the idea very interesting and promising. The proposed algorithm is very easy to be applied, thus, it could be easily reproduced. I believe the paper should be presented at the ICLR 2019. Pros: + The paper is written in a lucid manner. All ideas are clearly presented. I find the toy problem (Figure 2) very illuminating. + It might seem that the idea follows from simple if not even trivial remarks. But this impression is fully due to the fashion the authors presented their idea. I am truly impressed by the writing style of the authors. + I find the proposed approach very appealing because it requires changes only in the optimization procedure while the model and the objective remain the same. Moreover, the paper formalizes some intuition that could be found in other papers (e.g., (Alemi et al., 2018)). + The presented results are fully convincing. Cons: - It would be beneficial to see samples for the same latent variables to verify whether the model utilizes the latent code. Additionally, a latent space interpolation could be also presented. - The choice of the stopping criterion seems to be rather arbitrary. Did the authors try other methods? If yes, what were they? If not, why the current stopping criterion is so unique? - The proposed approach was applied to the case when the prior is a standard Normal. What would happen if a different prior is considered? Neutral remark: * Another problem, next to the posterior collapse, is the \u201chole problem\u201d (see Rezende & Viola, \u201cTaming VAEs\u201d, 2018). A natural question is whether the proposed approach also helps to solve this issue? One possible solution to that problem is to take the aggregated posterior as the prior (e.g., (Tomczak & Welling, 2018)) or to ensure that the KL between the aggregated posterior and the prior is small. In Figure 4 it seems it is the case, however, I am really curious about the authors\u2019 opinion on this matter. * Can the authors relate the proposed algorithm to the wake-sleep algorithm? Obviously, the motivation is different, however, I find these two approaches a bit similar in spirit. --REVISION-- I would like to thank the authors for their comments. In my opinion the paper is very interesting and opens new directions for further research (as discussed by the authors in their reply). I strongly believe the paper should be accepted and presented at the ICLR.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your encouraging comments and advice ! Currently we are running additional experiments to address some of the reviewer comments . This is taking some time and we will submit a revised version once we have collected all the results . For now , we will quickly answer some of your questions . # # Q1 : Latent variable interpretation We agree that providing samples would be informative . We plan to add these experiments , along with additional analysis aimed at uncovering how the latent codes are used by the generative model . # # Q2 : Choice of stopping criterion In addition to the presented stopping criterion , we first tried switching back to traditional VAE training after a fixed number of epochs -- i.e.early stopping . We found that this approach can also work well , but introduces an additional hyperparameter ( number of epochs ) that is sensitive to datasets and model architectures . We found that stopping too early hurts the performance , and stopping too late of course hurts speed . This tradeoff needs to be tuned if epochs are specified explicitly . Intuitively , posterior collapse ( and a \u201c lagging \u201d encoder ) correspond to a lack of \u201c dependence \u201d between posterior samples and observed data . Based on its use in related literature , we experimented with mutual information ( MI ) as a simple quantitative surrogate for \u201c dependence \u201d . Stopping the aggressive training phase after MI stops increasing monotonically worked well in practice and avoided the need for data or model dependent tuning . Across multiple settings we found the proposed stopping criterion does n't sacrifice performance and maintains fast training . We agree that further analysis would be interesting and suspect that similar measurements of dependence and related stopping criteria might also strike a successful balance . # # Q3 : Different prior The effect of our approach under different priors would certainly be interesting to see , but is a bit beyond the scope of the current paper . We may explore this direction in future work . # # Q4 : Hole problem Our analysis and empirical results were focused specifically on the problem of posterior collapse . We agree , however , that it would be interesting to explore how the proposed procedure ( and related modifications to optimization ) might affect other known issues with VAE . We hope to explore this in the future welcome any suggestions for how to do so ! Regarding the \u201c hole \u201d problem : We were not aware of this paper , thank you for sharing it with us . Our current experimental results demonstrate that the proposed approach is able to maintain a relatively small KL ( q ( z ) | p ( z ) ) , but this real-valued quantity is hard to interpret . We think that it is necessary to visualize the aggregated posterior and prior ( or use another more direct metric ) to check if the proposed approach helps solve the `` hole problem '' . # # Q5 : Connection to the wake-sleep algorithm Good point ! The proposed algorithm is similar to the wake-sleep algorithm in the sense that we split encoder and decoder optimization into separate phases . Essentially , both the proposed algorithm and the wake-sleep algorithm are instances of block-coordinate ascent . The decoder update in the proposed method is analogous to the wake phase : the ELBO objective corresponds to the wake phase objective with an additional regularization term from the prior on code z . The encoder update in the proposed method is analogous to the sleep phase where decoder is fixed -- though , here , the ELBO objective is somewhat different from the sleep phase objective which aims to recover hidden code z instead of observations x ."}}