{"year": "2018", "forum": "S1680_1Rb", "title": "CAYLEYNETS: SPECTRAL GRAPH CNNS WITH COMPLEX RATIONAL FILTERS", "decision": "Reject", "meta_review": "This paper considers graph neural representations that use Cayley polynomials of the graph Laplacian as generators. These polynomials offer better frequency localization than Chebyshev polynomials. The authors illustrate the advantages of Cayleynets on several benchmarks, producing modest improvements.\n\nReviewers were mixed in the assessment of this work, highlighting on the one hand the good quality of the presentation and the theoretical background, but on the other hand skeptical about the experimental section significance. In particular, some concerns were centered about the analysis of complexity of Cayley versus the existing alternatives.\n\nOverall, the AC believes this paper is perhaps more suited to an audience more savvy in signal processing than ICLR, which may fail to appreciate the contributions. ", "reviews": [{"review_id": "S1680_1Rb-0", "review_text": "The paper proposes a new filter for spectral analysis on graphs for graph CNNs. The filter is a rational function based on the Cayley transform. Unlike other popular variants, it is not strictly supported on a small graph neighborhood, but the paper proves an exponential-decay property on the norm of a filtered vertex indicator function. The paper argues that Cayley filters allow better spectral localization than Chebyshev filters. While Chebyshev filters can be applied efficiently using a recursive method, evaluation of a Cayley filter of order r requires solving r linear system in dimension corresponding to the number of vertices, which is expensive. The paper proposes to stop after a small number of iterations of Jacobi's method to alleviate this problem. The paper is clear and well written. The proposed method seems of interest, although I find the experimental section only partly convincing. There seems to be a tradeoff here. The paper demonstrates that CayleyNet achieves similar efficiency as ChebNet in multiple experiments while using smaller filter orders. Although using smaller filter orders (and better-localized filters) is an interesting property, it is not necessarily a key objective, especially as this seems to come at the cost of a significantly increased computational complexity. The paper could help us understand this tradeoff better. For instance: - Middle and right panels of Figure 4 could use a more precise Y scale. How much slower is CayleyNet here with respect to the ChebNet? - Figure 4 mentions time corresponds to \"test times on batches of 100 samples\". Is this an average value over multiple 100-sample batches? What is the standard deviation? How do the training times compare? - MNIST accuracies are very similar (and near perfect) -- how did the training and testing time compare? Same for the MovieLens experiment. The improvement in performance is rather small, what is the corresponding computational cost? - CORA results are a bit confusing to me. The filter orders used here are very small, and the best amongst the values considered seems to be r=1. Is there a reason only such small values have been considered? Is this a fair evaluation of ChebNet which may possibly perform better with larger filter orders? - The paper could provide some insights as to why ChebNet is unable to work with unnormalized Laplacians while CayleyNet is (and why the ChebNet performance seems to get worse and worse as r increases?). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank anonymous Reviewer2 for the work he/she provided . We present here various insights on the highlighted points . We first note that smaller filter orders are not the key objective . They lead to more regular filter spaces , which ultimately leads to less overfitting , and better accuracy , which is the key goal . This is evident in the experimental results . 1.Community dataset test/training times We agree with the reviewer that the scale proposed on the y axis of Fig.4 is not detailed enough , which will be fixed in the revision . All test times have been computed running 30 times our models with batches of 100 samples and averaging times across batches ( thus the reported times should be considered as mean test times per batch ) . In order to provide a better understanding , we attach here 2 anonymous links to figures showing ratios between times obtained with CayleyNet and ChebNet ( i.e.test_time_CayleyNet / test_time_ChebNet ) : https : //ibb.co/bD4xNG https : //ibb.co/jbNdUw We will add these plots as supplementary material in our final revisions . Standard deviations have been avoided in our analysis since do not add much to what already presented in Fig.4 . For completeness , we attach here 2 links showing mean test times and corresponding standard deviations : https : //ibb.co/jnODwb https : //ibb.co/nezWhG Finally , training times have been avoided in the paper for reasons of space . In general they present a similar trend to test times : https : //ibb.co/gUjE2G , https : //ibb.co/bHAnNG , https : //ibb.co/kbrYwb , https : //ibb.co/kcYP2G . We will add training times in the final version of this work . 2.CORA accuracies As also requested by Reviewer 3 , we further extended our analysis with additional orders . The best CayleyNet still outperform the competitor requiring at the same time a smaller amount of parameters ( see point 1 and 2 of our response to Reviewer 3 ) . 3.MNIST/MovieLens performance and test times Performance obtained over the MNIST dataset have been computed by means of 11 Jacobi iterations . Test time required by the proposed approach thus appears equal to 0.1776 +/- 0.06079 sec wrt the 0.0268 +/- 0.00841 sec required by ChebNet per batch ( batch size = 100 images ) . We stress that MNIST digit classification just represents a toy example for ensuring the performance of our approach on a well known benchmark in standard conditions and should not be considered as a valuable example for proving the superior capabilities of the proposed spectral filters . For what concern MovieLens , ChebNets with order 4 and 8 respectively require 0.0698 +/- 0.00275 sec and 0.0877 +/- 0.00362 sec at test time , CayleyNet with order 4 and 15 jacobi iterations requires instead 0.165 +/- 0.00332 sec.As presented to Reviewer 3 , the only modest improvement obtained by CayleyNet on this dataset is due to the construction of the graph . 4.ChebNet and unnormalized Laplacians Chebyshev polynomials are only well defined in the interval [ -1 , -1 ] , and plugging in values away from this interval leads an ill behaved system . Following the comments of Reviewer 3 we updated the paper to compare the two methods over the scaled version of the unnormalized laplace operator proposed by Defferrard et al.The eigenvalues of the unnormalized laplacian \\Delta are bounded by max { d ( u ) +d ( v ) : uv\u2208E } ( where d ( u ) corresponds to the degree of node u and E is the edge set , doi.org/10.1080/03081088508817681 ) . In ChebNet , Defferrard et al.proposed to divide the unnormalized laplace operator by the maximum eigenvalue ( thus producing a contraction from the original laplacian ) . The Chebyshev polynomial basis is well defined on this normalized version of the Laplacian . However , a side effect of this normalization is that all of the `` macroscopic frequencies \u2019 \u2019 get squeezed near zero , and thus Chebyshev polynomials can not separate them . This phenomenon is avoided in CayleyNets , as explained in the \u201c Cayley vs Chebyshev \u201d section . In the updated comparison , CayleyNet still achieves better performance while requiring a lower amount of parameters . Regarding the last remark of the reviewer , the performance gets worse as the filter order increases due to overfitting ."}, {"review_id": "S1680_1Rb-1", "review_text": "Summary: This paper proposes a new graph-convolution architecture, based on Cayley transform of the matrix. Succinctly, if L denotes the Laplacian of a graph, this filter corresponds to an operator that is a low degree polynomial of C(L) = (hL - i)/(hL+i), where h is a scalar and i denotes sqrt(-1). The authors contend that such filters are interesting because they can 'zoom' into a part of the spectrum, depending on the choice of h, and that C(L) is always a rotation matrix with eigenvalues with magnitude 1. The authors propose to compute them using Jacobi iteration (using the diagonal as a preconditioner), and present experimental results. Opinion: Though the Cayley filters seem to have interesting properties, I find the authors theoretical and experimental justification insufficient to conclude that they offer sufficient advantage over existing methods. I list my major criticisms below: 1. The comparison to Chebyshev filters (small degree polynomials in the Chebyshev basis) at several places is unconvincing. The results on CORA (Fig 5a) compare filters with the same order, though Cayley filters have twice the number of variables for the same order as Chebyshev filters. Similarly for Fig 1, order 3 Cayley should be compared to Order 6 Chebyshev (roughly). 2. Since Chebyshev polynomials blow up exponentially when applied to values larger than 1, applying Chebyshev filters to unnormalized Laplacians (Fig 5b) is an unfair comparison. 3. The authors basically apply Jacobi iteration (gradient descent using a diagonal preconditioner) to estimate the Cayley filters, and contend that a constant number of iterations of Jacobi are sufficient. This ignores the fact that their convergence rate scales quadratically in h and the max-degree of the graph. Moreover, this means that the Filter is effectively a low degree polynomial in (D^(-1)A)^K, where A is the adjacency matrix of the graph, and K is the number of Jacobi iterations. It's unclear how (or why) a choice of K might be good, or why does it make sense to throw away all powers of D^(-1)Af, even though we're computing all of them. Also, note that this means a K-fold increase in the runtime for each evaluation of the network, compared to the Chebyshev filter. Among the other experimental results, the synthetic results do clearly convey a significant advantage at least over Chebyshev filters with the same number of parameters. The CORA results (table 2) do convey a small but clear advantage. The MNIST result seems a tie, and the comparison for MovieLens doesn't make it obvious that the number of parameters is the same. Overall, this leads me to conclude that the paper presents insufficient justification to conclude that Cayley filters offer a significant advantage over existing work.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank anonymous Reviewer3 for thorough and insightful comments . We have run extensive experiments requested by the reviewer and provide these results as well as our detailed response to his/her main concerns below . We will revise the paper to address these issues and our responses to them . 1.Number of coefficients : We agree that , since Cayley filters use complex coefficients while Chebyshev filters use real coefficients , in principle complex coefficients should be counted as twice more parameters . There are two different ways to make the number of parameters fairly comparable : ( i ) compare Cayley filters of order r vs Chebyshev filters of order 2 * r ( as suggested by Reviewer3 ) , or ( ii ) use real coefficients in Cayley filters ( as we note in our paper on p.4 , in paragraph preceding Fig 1 ) . We produce these two comparisons below , using the Cora dataset with symmetric normalized Laplacian ( # params = # real coefficients ; 1 complex coefficient is counted as 2 parameters ) : ( i ) Cayley filter with complex coefficients , twice lower order than Chebyshev filter : ChebNet order r=2 ( # params = 69136 ) - Accuracy = 86.607986 +/- 0.65477967 ( reported in paper ) ChebNet order r=4 ( # params = 115216 ) - Accuracy = 85.203995 +/- 0.83185506 ChebNet order r=6 ( # params = 161296 ) - Accuracy = 84.487999 +/- 0.83249897 Complex CayleyNet order r=1 ( # params = 69136 ) - Accuracy = 87.9 +/- 0.97508276 ( reported in paper ) Complex CayleyNet order r=2 ( # params = 115216 ) - Accuracy = 86.9 +/- 0.28902602 ( reported in paper ) Complex CayleyNet order r=3 ( # params = 161296 ) - Accuracy = 87.1 +/- 0.30883133 ( reported in paper ) ( ii ) Cayley filter using real coefficients , same order as Chebyshev filter : Real CayleyNet order r=1 ( # params = 46096 ) - Accuracy = 87.311989 +/- 0.50936872 Real CayleyNet order r=2 ( # params = 69136 ) - Accuracy = 86.863991 +/- 0.57611096 Real CayleyNet order r=3 ( # params = 92176 ) - Accuracy = 86.147995 +/- 0.56823856 Real CayleyNet order r=4 ( # params = 115216 ) - Accuracy = 86.395996 +/- 0.62544805 Real CayleyNet order r=5 ( # params = 138256 ) - Furthermore , for the MovieLens experiment , the CayleyNet outperforms ChebNet ( lower RMS error ) when using lower polynomial order : Complex CayleyNet order r=4 ( # params = 23126 ) - RMSE = 0.922 ( reported in paper ) ChebNet order r=8 ( # params = 23124 ) - RMSE = 0.925 We will include these results and a more detailed discussion regarding a fair comparison of the number of parameters . 2.Normalized vs Unnormalized Laplacian : We agree that poor performance of ChebNet in case of unnormalized Laplacian can be attributed to large eigenvalues . As requested by Reviewer3 , we reproduce this experiment using scaled unnormalized Laplacian ( 2 * Delta/lambda_max - I ) to ensure the magnitude of its eigenvalues is < = 1 , thus avoiding the numerical instability raised by the reviewer . We note that in our approach , no such scaling is necessary , since the eigenvalues of any Laplacian are mapped to the complex unit circle and thus automatically numerically stable . The best performing models on Cora dataset with scaled unnormalized Laplacian are reported below : ChebNet order r=7 ( # params = 184336 ) - Accuracy = 87.232002 +/- 0.68511164 CayleyNet order r=1 ( # params = 69136 ) - Accuracy = 87.676003 +/- 0.13199957 Thus , CayleyNet outperforms ChebNet ( accuracy of 87.68 % vs 87.23 % ) at the same time requiring significantly less parameters ( 69K vs 184K ) . We will update the results reported in the paper for the unnormalized Laplacian by this experiment ."}, {"review_id": "S1680_1Rb-2", "review_text": "This paper is on construction graph CNN using spectral techniques. The originality of this work is the use of Cayley polynomials to compute spectral filters on graphs, related to the work of Defferrard et al. (2016) and Monto et al. (2017) where Chebyshev filters were used. Theoretical and experimental results show the relevance of the Cayley polynomials as filters for graph CNN. The paper is well written, and connections to related works are highlighted. We recommend the authors to talk about some future work.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the Reviewer for a positive evaluation of our work . Future work : One of the key issues in our method ( and deep learning on graphs in general ) is the assumption of a given graph . In many settings , such as recommender systems , the graph has to be estimated from the data/side information . Learning the graph together with the filters on the graph is the next logical step which we will address in future works . In particular , for graphs constructed in some feature space ( e.g.demographic information of users in the recommender system examples ) , \u201c learning the graph \u201d boils down to learning a metric on the feature space , which in turn determines the graph edge weights . Second , as we note in the response to Reviewer3 , the behavior of our approximate matrix inversion is akin to \u201c model compression \u201d . In future work , we will analyze this phenomenon in light of previous results on learnable iterative algorithms ."}], "0": {"review_id": "S1680_1Rb-0", "review_text": "The paper proposes a new filter for spectral analysis on graphs for graph CNNs. The filter is a rational function based on the Cayley transform. Unlike other popular variants, it is not strictly supported on a small graph neighborhood, but the paper proves an exponential-decay property on the norm of a filtered vertex indicator function. The paper argues that Cayley filters allow better spectral localization than Chebyshev filters. While Chebyshev filters can be applied efficiently using a recursive method, evaluation of a Cayley filter of order r requires solving r linear system in dimension corresponding to the number of vertices, which is expensive. The paper proposes to stop after a small number of iterations of Jacobi's method to alleviate this problem. The paper is clear and well written. The proposed method seems of interest, although I find the experimental section only partly convincing. There seems to be a tradeoff here. The paper demonstrates that CayleyNet achieves similar efficiency as ChebNet in multiple experiments while using smaller filter orders. Although using smaller filter orders (and better-localized filters) is an interesting property, it is not necessarily a key objective, especially as this seems to come at the cost of a significantly increased computational complexity. The paper could help us understand this tradeoff better. For instance: - Middle and right panels of Figure 4 could use a more precise Y scale. How much slower is CayleyNet here with respect to the ChebNet? - Figure 4 mentions time corresponds to \"test times on batches of 100 samples\". Is this an average value over multiple 100-sample batches? What is the standard deviation? How do the training times compare? - MNIST accuracies are very similar (and near perfect) -- how did the training and testing time compare? Same for the MovieLens experiment. The improvement in performance is rather small, what is the corresponding computational cost? - CORA results are a bit confusing to me. The filter orders used here are very small, and the best amongst the values considered seems to be r=1. Is there a reason only such small values have been considered? Is this a fair evaluation of ChebNet which may possibly perform better with larger filter orders? - The paper could provide some insights as to why ChebNet is unable to work with unnormalized Laplacians while CayleyNet is (and why the ChebNet performance seems to get worse and worse as r increases?). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank anonymous Reviewer2 for the work he/she provided . We present here various insights on the highlighted points . We first note that smaller filter orders are not the key objective . They lead to more regular filter spaces , which ultimately leads to less overfitting , and better accuracy , which is the key goal . This is evident in the experimental results . 1.Community dataset test/training times We agree with the reviewer that the scale proposed on the y axis of Fig.4 is not detailed enough , which will be fixed in the revision . All test times have been computed running 30 times our models with batches of 100 samples and averaging times across batches ( thus the reported times should be considered as mean test times per batch ) . In order to provide a better understanding , we attach here 2 anonymous links to figures showing ratios between times obtained with CayleyNet and ChebNet ( i.e.test_time_CayleyNet / test_time_ChebNet ) : https : //ibb.co/bD4xNG https : //ibb.co/jbNdUw We will add these plots as supplementary material in our final revisions . Standard deviations have been avoided in our analysis since do not add much to what already presented in Fig.4 . For completeness , we attach here 2 links showing mean test times and corresponding standard deviations : https : //ibb.co/jnODwb https : //ibb.co/nezWhG Finally , training times have been avoided in the paper for reasons of space . In general they present a similar trend to test times : https : //ibb.co/gUjE2G , https : //ibb.co/bHAnNG , https : //ibb.co/kbrYwb , https : //ibb.co/kcYP2G . We will add training times in the final version of this work . 2.CORA accuracies As also requested by Reviewer 3 , we further extended our analysis with additional orders . The best CayleyNet still outperform the competitor requiring at the same time a smaller amount of parameters ( see point 1 and 2 of our response to Reviewer 3 ) . 3.MNIST/MovieLens performance and test times Performance obtained over the MNIST dataset have been computed by means of 11 Jacobi iterations . Test time required by the proposed approach thus appears equal to 0.1776 +/- 0.06079 sec wrt the 0.0268 +/- 0.00841 sec required by ChebNet per batch ( batch size = 100 images ) . We stress that MNIST digit classification just represents a toy example for ensuring the performance of our approach on a well known benchmark in standard conditions and should not be considered as a valuable example for proving the superior capabilities of the proposed spectral filters . For what concern MovieLens , ChebNets with order 4 and 8 respectively require 0.0698 +/- 0.00275 sec and 0.0877 +/- 0.00362 sec at test time , CayleyNet with order 4 and 15 jacobi iterations requires instead 0.165 +/- 0.00332 sec.As presented to Reviewer 3 , the only modest improvement obtained by CayleyNet on this dataset is due to the construction of the graph . 4.ChebNet and unnormalized Laplacians Chebyshev polynomials are only well defined in the interval [ -1 , -1 ] , and plugging in values away from this interval leads an ill behaved system . Following the comments of Reviewer 3 we updated the paper to compare the two methods over the scaled version of the unnormalized laplace operator proposed by Defferrard et al.The eigenvalues of the unnormalized laplacian \\Delta are bounded by max { d ( u ) +d ( v ) : uv\u2208E } ( where d ( u ) corresponds to the degree of node u and E is the edge set , doi.org/10.1080/03081088508817681 ) . In ChebNet , Defferrard et al.proposed to divide the unnormalized laplace operator by the maximum eigenvalue ( thus producing a contraction from the original laplacian ) . The Chebyshev polynomial basis is well defined on this normalized version of the Laplacian . However , a side effect of this normalization is that all of the `` macroscopic frequencies \u2019 \u2019 get squeezed near zero , and thus Chebyshev polynomials can not separate them . This phenomenon is avoided in CayleyNets , as explained in the \u201c Cayley vs Chebyshev \u201d section . In the updated comparison , CayleyNet still achieves better performance while requiring a lower amount of parameters . Regarding the last remark of the reviewer , the performance gets worse as the filter order increases due to overfitting ."}, "1": {"review_id": "S1680_1Rb-1", "review_text": "Summary: This paper proposes a new graph-convolution architecture, based on Cayley transform of the matrix. Succinctly, if L denotes the Laplacian of a graph, this filter corresponds to an operator that is a low degree polynomial of C(L) = (hL - i)/(hL+i), where h is a scalar and i denotes sqrt(-1). The authors contend that such filters are interesting because they can 'zoom' into a part of the spectrum, depending on the choice of h, and that C(L) is always a rotation matrix with eigenvalues with magnitude 1. The authors propose to compute them using Jacobi iteration (using the diagonal as a preconditioner), and present experimental results. Opinion: Though the Cayley filters seem to have interesting properties, I find the authors theoretical and experimental justification insufficient to conclude that they offer sufficient advantage over existing methods. I list my major criticisms below: 1. The comparison to Chebyshev filters (small degree polynomials in the Chebyshev basis) at several places is unconvincing. The results on CORA (Fig 5a) compare filters with the same order, though Cayley filters have twice the number of variables for the same order as Chebyshev filters. Similarly for Fig 1, order 3 Cayley should be compared to Order 6 Chebyshev (roughly). 2. Since Chebyshev polynomials blow up exponentially when applied to values larger than 1, applying Chebyshev filters to unnormalized Laplacians (Fig 5b) is an unfair comparison. 3. The authors basically apply Jacobi iteration (gradient descent using a diagonal preconditioner) to estimate the Cayley filters, and contend that a constant number of iterations of Jacobi are sufficient. This ignores the fact that their convergence rate scales quadratically in h and the max-degree of the graph. Moreover, this means that the Filter is effectively a low degree polynomial in (D^(-1)A)^K, where A is the adjacency matrix of the graph, and K is the number of Jacobi iterations. It's unclear how (or why) a choice of K might be good, or why does it make sense to throw away all powers of D^(-1)Af, even though we're computing all of them. Also, note that this means a K-fold increase in the runtime for each evaluation of the network, compared to the Chebyshev filter. Among the other experimental results, the synthetic results do clearly convey a significant advantage at least over Chebyshev filters with the same number of parameters. The CORA results (table 2) do convey a small but clear advantage. The MNIST result seems a tie, and the comparison for MovieLens doesn't make it obvious that the number of parameters is the same. Overall, this leads me to conclude that the paper presents insufficient justification to conclude that Cayley filters offer a significant advantage over existing work.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank anonymous Reviewer3 for thorough and insightful comments . We have run extensive experiments requested by the reviewer and provide these results as well as our detailed response to his/her main concerns below . We will revise the paper to address these issues and our responses to them . 1.Number of coefficients : We agree that , since Cayley filters use complex coefficients while Chebyshev filters use real coefficients , in principle complex coefficients should be counted as twice more parameters . There are two different ways to make the number of parameters fairly comparable : ( i ) compare Cayley filters of order r vs Chebyshev filters of order 2 * r ( as suggested by Reviewer3 ) , or ( ii ) use real coefficients in Cayley filters ( as we note in our paper on p.4 , in paragraph preceding Fig 1 ) . We produce these two comparisons below , using the Cora dataset with symmetric normalized Laplacian ( # params = # real coefficients ; 1 complex coefficient is counted as 2 parameters ) : ( i ) Cayley filter with complex coefficients , twice lower order than Chebyshev filter : ChebNet order r=2 ( # params = 69136 ) - Accuracy = 86.607986 +/- 0.65477967 ( reported in paper ) ChebNet order r=4 ( # params = 115216 ) - Accuracy = 85.203995 +/- 0.83185506 ChebNet order r=6 ( # params = 161296 ) - Accuracy = 84.487999 +/- 0.83249897 Complex CayleyNet order r=1 ( # params = 69136 ) - Accuracy = 87.9 +/- 0.97508276 ( reported in paper ) Complex CayleyNet order r=2 ( # params = 115216 ) - Accuracy = 86.9 +/- 0.28902602 ( reported in paper ) Complex CayleyNet order r=3 ( # params = 161296 ) - Accuracy = 87.1 +/- 0.30883133 ( reported in paper ) ( ii ) Cayley filter using real coefficients , same order as Chebyshev filter : Real CayleyNet order r=1 ( # params = 46096 ) - Accuracy = 87.311989 +/- 0.50936872 Real CayleyNet order r=2 ( # params = 69136 ) - Accuracy = 86.863991 +/- 0.57611096 Real CayleyNet order r=3 ( # params = 92176 ) - Accuracy = 86.147995 +/- 0.56823856 Real CayleyNet order r=4 ( # params = 115216 ) - Accuracy = 86.395996 +/- 0.62544805 Real CayleyNet order r=5 ( # params = 138256 ) - Furthermore , for the MovieLens experiment , the CayleyNet outperforms ChebNet ( lower RMS error ) when using lower polynomial order : Complex CayleyNet order r=4 ( # params = 23126 ) - RMSE = 0.922 ( reported in paper ) ChebNet order r=8 ( # params = 23124 ) - RMSE = 0.925 We will include these results and a more detailed discussion regarding a fair comparison of the number of parameters . 2.Normalized vs Unnormalized Laplacian : We agree that poor performance of ChebNet in case of unnormalized Laplacian can be attributed to large eigenvalues . As requested by Reviewer3 , we reproduce this experiment using scaled unnormalized Laplacian ( 2 * Delta/lambda_max - I ) to ensure the magnitude of its eigenvalues is < = 1 , thus avoiding the numerical instability raised by the reviewer . We note that in our approach , no such scaling is necessary , since the eigenvalues of any Laplacian are mapped to the complex unit circle and thus automatically numerically stable . The best performing models on Cora dataset with scaled unnormalized Laplacian are reported below : ChebNet order r=7 ( # params = 184336 ) - Accuracy = 87.232002 +/- 0.68511164 CayleyNet order r=1 ( # params = 69136 ) - Accuracy = 87.676003 +/- 0.13199957 Thus , CayleyNet outperforms ChebNet ( accuracy of 87.68 % vs 87.23 % ) at the same time requiring significantly less parameters ( 69K vs 184K ) . We will update the results reported in the paper for the unnormalized Laplacian by this experiment ."}, "2": {"review_id": "S1680_1Rb-2", "review_text": "This paper is on construction graph CNN using spectral techniques. The originality of this work is the use of Cayley polynomials to compute spectral filters on graphs, related to the work of Defferrard et al. (2016) and Monto et al. (2017) where Chebyshev filters were used. Theoretical and experimental results show the relevance of the Cayley polynomials as filters for graph CNN. The paper is well written, and connections to related works are highlighted. We recommend the authors to talk about some future work.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the Reviewer for a positive evaluation of our work . Future work : One of the key issues in our method ( and deep learning on graphs in general ) is the assumption of a given graph . In many settings , such as recommender systems , the graph has to be estimated from the data/side information . Learning the graph together with the filters on the graph is the next logical step which we will address in future works . In particular , for graphs constructed in some feature space ( e.g.demographic information of users in the recommender system examples ) , \u201c learning the graph \u201d boils down to learning a metric on the feature space , which in turn determines the graph edge weights . Second , as we note in the response to Reviewer3 , the behavior of our approximate matrix inversion is akin to \u201c model compression \u201d . In future work , we will analyze this phenomenon in light of previous results on learnable iterative algorithms ."}}