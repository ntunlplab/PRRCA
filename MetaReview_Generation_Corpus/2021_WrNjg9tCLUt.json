{"year": "2021", "forum": "WrNjg9tCLUt", "title": "BAFFLE: TOWARDS RESOLVING FEDERATED LEARNING\u2019S DILEMMA - THWARTING BACKDOOR  AND INFERENCE ATTACKS", "decision": "Reject", "meta_review": "The paper makes an attempt towards byzantine resilient federated learning, in the pressneece of backdoor attacks. \n\nThe method presented combines a clustering step with a poison elimination step, and seems to be effective against a range of current attacks. \n\nBoth steps are a bit ad hoc in nature, and do not come with provable guarantees.\n\nMoreover, the algorithms presented will have a big negative impact on personalization as several models may be incorrectly discarded during and FL round.\n\nThe authors further point in their response that \" no existing defense against backdoor attacks preserves the privacy of the clients\u2019 data.\" This is in fact not true, as the differential privacy defense presented by the \"Can you really backdoor FL\" paper is in fact fully respective of user privacy.\n\nAt the same time, the work on backdoor attacks and defenses is reminiscent of the \"cat and mouse\" work in adversarial examples: an attack comes out, then a defense claims to protect against it, then an attack that incorporates that defense can be made stronger, and so on. This is similar in the context of backdoor attacks.\n\nIn fact, a recent work [1] proposes that detecting backdoors is in the general computationally unlikely, rendering the generality of the proposed algorithm questionable, and also suggest a set of attacks that seem very hard to defend against. (it is fine that the authors do not reference this work as it was published just recently)\n\nAs the paper lacks significant algorithmic novelty, solid guarantees, and also is unclear whether it is universally sound, the overall contribution is limited. \n\n[1] Wang et al. Attack of the tails: Yes, you really can backdoor federated learning, neurips 2020\nhttps://papers.nips.cc/paper/2020/file/b8ffa41d4e492f0fad2f13e29e1762eb-Paper.pdf\n", "reviews": [{"review_id": "WrNjg9tCLUt-0", "review_text": "In the paper , the authors proposed a novel privacy-preserving defense approach BAFFLE for federated learning which could simultaneously impede backdoor and inference attacks . To impede backdoor attacks , the Model Filtering layer ( i.e. , by dynamic clustering ) and Poison Elimination layer ( i.e. , by noising and clipping ) were presented respectively for the malicious updates and the weak manipulations of the model . To thwart inference attacks , private BAFFLE was built to evaluate the BAFFLE algorithm under encryption using secure computation techniques . [ Strengths ] 1 . The paper is clear , logical , and easy to follow . 2.The topic of simultaneously defending against the backdoor and the inference attacks is significant . 3.Evaluations were conducted on multiple datasets and applications , including image classi\ufb01cation , word prediction , and IoT intrusion detection . [ Weaknesses ] 1 . The topic is significant but the contributions to the proposed approach are limited . 2.To impede backdoor attacks , many models are marked as outliers and discarded , clipped , and noised , generally speaking , which could lead to performance degradation . However , there is only a negligible effect on performance . What is the cause of this phenomenon\uff1f 3 . In FL , clients locally train model updates using private data and provide these to a central aggregator . If some models were directly discarded in the central aggregator , the corresponding private data are not utilized for model training which is not an ideal approach , especially , the private data is irreplaceable . 4.Clipping and noising are the means of eliminating weak manipulations . How about the settings of them affect the results ? Some analysis and ablation experiments are needed . After reading the response , I still think that the work is promising and would like to keep my recommendation .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your valuable comments ! We are looking forward to improve our paper with your guidance . Please find below our responses and a list of changes that we implemented : # Clarifications : * * Limited contributions . * * We would kindly ask the reviewer to specify more precisely * what * the limitations referred to are . Our main contribution is to propose a _novel_ and _generic_ approach to tackle state-of-the-art backdoor and inference attacks . These attacks have been recently introduced in top-tier machine learning as well as security conferences , e.g. , ICLR ( Xie et al. , 2020 ) , AISTATS ( Bagdasaryan et al. , 2020 ) , USENIX Security ( Fang et al. , 2020 ) , and IEEE S & P ( Melis et al. , 2019 ) . Our extensive evaluation shows that our approach mitigates all these attacks as well as adaptive attacks effectively . It is common sense in security research community to show that any defenses ( also generic ones ) can successfully mitigate known attacks in the literature . Thus , we address the deficiency of state-of-the-art defenses in a way that to the best of our knowledge has not been proposed before . And , on top of that we also solve another challenging problem by designing these defenses in a privacy-preserving manner . * * Minimum effect on the performance of the global model . * * Mitigating backdoors , while retaining the performance of the model is the main goal of our paper . We aim at minimizing the following parameters : ( 1 ) the number of models that are filtered out , ( 2 ) the clipping bound , and ( 3 ) the noise level , as all of these would negatively impact the benign performance of the model . Our dynamic clustering approach only removes models that potentially have high attack impact in order to avoid falsely rejecting benign models ( cf.\u00a73.1 ) .We introduce adaptive clipping ( \u00a73.2 ) and adaptive noising ( \u00a73.3 ) so that the clipping bound and noise level dynamically adapt to the changes in the models in different training iterations . Moreover , since the poisoned models having high attack impact are filtered out by the Model Filtering layer , this reduces the burden to the Poison Elimination layer to mitigate backdoors , i.e. , the clipping bound is increased ( the model is clipped less ) and noise level is decreased ( less noise needs to be added ) . As a result , our clipping and noising approaches do not degrade the performance of the model . * * Effect of the discarded models on the performance . * * We agree that in the ideal case all benign models should be kept . Our approach , therefore , aims at minimizing the number of models that are falsely filtered out . By doing this , we significantly improve over state-of-the-art approaches , e.g. , Krum ( Blanchard et al. , 2017 ) or Auror ( Shen et al. , 2016 ) . For example , Krum only selects a single centroid local model as the global model , or collects a small subset of local models ( Multi-Krum ) to be aggregated in the global model , and thus , ignores contributions of a significant amount of benign models . In addition , Auror clusters model updates into two classes using K-means clustering , which is too coarse-grained and generates many false positives . Even worse , it can not handle the simultaneous injection of multiple backdoors ( cf.\u00a73.1 ) . * * How the settings of clipping and noising affect the results . * * We agree that eliminating weak manipulations is the primary goal of our clipping and noising components ( the Poison Elimination layer ) . In \u00a75.2 , we analyzed and evaluated the effect of these components . As shown in Fig.5 , Poison Elimination is only effective if the poisoned data rate is below 13 % . However , Model Filtering is effective above the poisoned data rate of 13 % , as it then can reliably identify poisoned models . Moreover , we also evaluated different clipping and noising thresholds to justify our choices in Appendix F.1 , Paragraph 3 and 4 and illustrated the results in Fig.7 and 8. # Changes : * We added a summary of the discussion on clipping and noising in Sect . 5.2 and refer to Appendix F.1 , Paragraph 3 and 4 . Please let us kindly know of any further clarifications or changes required to clear all possibly remaining doubts and to secure your support ."}, {"review_id": "WrNjg9tCLUt-1", "review_text": "This paper suggests a new solution to protect FL models from backdoor attacks . Under the backdoor attack , an adversary can manipulate a few clients ' weight matrices to affect the final global model . In particular , the adversary wants the final model to make an incorrect prediction for certain inputs . This paper 's main idea to defend against a backdoor attack is to use clustering and adaptive clipping and noising . In the clustering phase , the aggregator uses a clustering technique to identifies the weight matrices that have been manipulated by the adversary . In the clipping and noising phase , the aggregator tries to mitigate the effect of manipulated weight matrices that could not be identified in the filtering phase . Strengths : This paper uses an existing clustering algorithm ( the HDBSCAN clustering algorithm ( Campello et al. , 2013 ) ) that works best in the FL problem in identifying manipulated weight matrices . Using this clustering algorithm combined with adaptive clipping and noising , the proposed method can mitigate the backdoor attack . Moreover , BAFFLE , combined with the Secure-Two-Party Computation method , can protect the FL model from Inference attacks . The extensive numerical examples show that the proposed method outperforms other defense methods most of the time . Weaknesses : This paper does not provide any theoretical guarantee and only applies the existing methods to mitigate the backdoor attacks . Therefore , we can not make sure that the proposed method works in any problem or on any dataset . As the numerical examples show ( e.g. , see Table 2 ) , BAFFLE can not outperform other defense methods all the time . Due to the lack of theoretical analysis or developing a new method/algorithm , I vote for weak acceptance ( 6 ) . I ask the authors to clarify/explain the $ \\bf { \\underline { novelty } } $ of their algorithms during the discussion period .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your valuable comments ! We are looking very forward to improve our paper with your guidance . Please find below our responses and a list of changes that we implemented : # Clarifications : * * The novelty of our approach . * * 1.A novel generic privacy-preserving FL backdoor system that simultaneously protects both the security and data privacy of FL-based applications by effectively preventing backdoor and inference attacks . To the best of our knowledge , this is the first work that discusses and tackles this dilemmatic challenge , i.e. , _no_ existing defense against backdoor attacks preserves the privacy of the clients \u2019 data . 2.A novel generic backdoor defense ( cf.Algorithm 1 ) that has three-folds of novelty : ( 1 ) a novel two-layer defense , ( 2 ) a new clustering approach , and ( 3 ) a new adaptive threshold tuning scheme for clipping and noising as follows : * A novel two-layer defense concept : To the best of our knowledge , we are the first to point out that combining two classes of defenses can prevent the adversary to trade-off between attack impact and attack stealthiness . The reason is that clustering-based defenses are effective for mitigating attacks with intense manipulations ( \u00a73.1 ) while \u201c clipping and noising-based \u201d defenses are useful for eliminating weak manipulations ( stealthiness ) ( \u00a73.2 and \u00a73.3 ) . However , the na\u00efve combination of these two classes of defenses is not effective . Our evaluation ( as discussed in the last paragraph in \u00a7F.1 ) shows that simply stacking clustering using K-means ( e.g. , Shen et al. , 2016 ) and clipping and noising ( e.g. , Bagdasaryan et al.2020 ) is not effective to mitigate sophisticated backdoor attacks . Therefore , we introduce a new clustering approach as well as clipping and noising approaches to mitigate powerful backdoor attacks . * A novel clustering approach tackling dynamic attack scenarios ( \u00a73.1 ) : We utilize a completely different clustering approach ( density-based clustering , HDBSCAN/DBSCAN ) with appropriate parameterization so that clusters containing poisoned models are identified as outliers , irrespective of the number of backdoors ( Alg.1 , Lines 6 and 7 ) . Existing approaches cluster model updates either into two classes using K-means ( e.g. , Shen et al. , 2016 ) , or only select one or a small subset of models ( Blanchard et al. , 2017 ) , which is too coarse-grained and generates many false positives . Even worse , it can not handle the simultaneous injection of multiple backdoors . * A novel adaptive threshold tuning scheme for finding effective clipping bound ( \u00a73.2 ) and noise level ( \u00a73.3 ) : Existing approaches do not discuss/consider this at all . Since Euclidean distances between the local models and the global model indicating how much the local models have changed after local training , and these distances reduce every round ( cf.Fig.3 ) , it can be used to specify the clipping bound ( Alg.1.Lines 8 and 9 ) and the noise level ( Alg.1.Lines 8 , 9 , and 13 ) to adapt to the changes in each training iterations . # Changes : * We made the novelty aspects of our approach clearer in the contribution section in \u00a71 . Please let us kindly know of any further clarifications or changes required to clear all possibly remaining doubts and to get your support ."}, {"review_id": "WrNjg9tCLUt-2", "review_text": "This paper provides an interesting research direction for the cross-domain of federating learning and backdoor attacks . This direction has very limited work until the recent 2 years . The work being proposed in this manuscript is simple and straightforward to implement . The pipeline has been clearly demonstrated . The experiments have multiple aspects presented and show promising results in various metrics . pros : - Novel problem , may attract massive attention - Simple and intuitive pipeline , conceptually easy to implement - Results are versatile , many comparison tables are provided cons : - Whole article is not self-contained , feel the connections between modules are very loose - The design of the pipeline is very ad-hoc , so many engineering aspects can be tweaked and the performance could be dramatically altered . - The experiments could use a few popular trojan attack methods , the baselines are not comprehensive . concerns : - My major concern comes from the design of the pipeline and the experiments . The author ( s ) have created many splendid terms to describe the modules used in this work , however , their implementation uses both clustering and median , which is very engineering and may not reliable with a different clustering algorithm or data set is severely unbalanced ( just like the non-iid data sets among clients ) . This kind of uncertainty due to the ad-hoc nature of the pipeline causes me to wonder : how bad this framework can be if any of the carefully cherry-picked modules fails its purpose ? The mathematical motivation of this paper is missing and this causes the impression of untrustedness on the model design . It would be better if the author ( s ) can 1. provide some mathematical proofs or derivations to support your design . 2. provide a lower bound or upper bound for performance guarantee . - Please compare it with a few Trojan attack methods in recent years . I believe no matter what kind of backdoor and Trojan attack , can be easily applied to FL by applying them individually on each client without too much trouble . An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks . KDD 2020 - For federated learning , one important experimental factor is the number of clients ( K= 5 , 10 , 50 , 100 , etc ) , the portion of data on each client ( 1 % , 5 % , 10 % , etc ) , and data distribution assumption ( iid , non-iid with feature shift or label shift , etc ) . Please evaluate the results by changing these important hyperparameters . - The holomorphic encryption is a pretty standard concept in FL , I do n't understand why the author ( s ) have listed this as the major contribution for the work . Especially , only one paper from 1986 is mentioned and nothing especially has been proposed in this work . It is just an unusual way to list your contribution . - Code is not provided , I can not see the reproducibility of this work . minor : please attach your main context pdf in the submission and submit the appendix in the supplementary material", "rating": "4: Ok but not good enough - rejection", "reply_text": "# Clarifications : * * Secure two-party computation ( from 1986 ) . * * It seems that there is a misunderstanding here . _Nowhere_ in the paper do we mention that we would use homomorphic encryption ( HE ) for BAFFLE . Instead , we use secure two-party computation ( STPC ) techniques . STPC is more efficient than HE while also being provably secure . STPC effectively prevents the aggregator from accessing the model updates and hence mitigates inference attacks on the local models . Indeed , the pioneering work by Andrew Yao in 1986 , that we cite , was ground-breaking for secure computation : Yao showed that it is possible to evaluate any efficiently computable function securely ( i.e. , in a privacy-preserving manner ) . This is why citing the paper by Yao ( Yao , 1986 ) is standard practice in the security and privacy research community due to the role of Yao \u2018 s work as a seminal paper in this area . However , since Yao \u2019 s publication , an extensive line of research work followed his paradigm and introduced optimized secure computation protocols , implementations , and various efficiency improvements , e.g. , point-and-permute ( Beaver et al.1990 ) , ( Kolesnikov & Schneider , 2008 ) , FastGC ( Huang et al.,2011 ) , fixed-key AES ( Bellare et al. , 2013 ) , and half-gates ( Zahur et al. , 2015 ) to name some . We use ABY ( Demmler et al. , 2015 ) , a state-of-the-art STPC-framework that implements three techniques for secure computation : Yao \u2019 s Garbled Circuits ( originally introduced by Yao in 1986 ) , Boolean- and Arithmetic-Sharing ( building upon a work by Goldreich et al.in 1987 ) .Note , that ABY also includes recent advancements that make the original protocols from 1986/87 significantly more efficient . For details , please refer to ( Demmler et al. , 2015 ) . * * Our STPC contributions . * * Our contribution in private BAFFLE goes significantly beyond simply applying existing STPC protocols and the ABY framework . We carefully designed all components and operations as Boolean circuits in a very efficient way . For example , we synthesized _novel_ ( previously not existing ) and highly optimized circuits ( we will highlight this more prominently in the revised version of the paper ) for the square root calculation that is also of independent interest and can be used for other applications that need a privacy-preserving computation of the square root ( e.g. , any protocol that uses the Euclidean distance like privacy-preserving face recognition ( Osadchy et al. , 2010 ) ) . For the circuit generation , we customized the flow of the commercial hardware logic synthesis tools ( DC , 2010 ) to generate circuits optimized for the Garbled Circuits technique . For example , with the free-XOR technique ( Kolesnikov & Schneider , 2008 ; which allows to calculate all XOR operations for _free_ ) , one has to minimize the number of non-XOR gates in the Boolean representation . We developed a technology library to guide the mapping of the logic to the circuit with no manufacturing rules defined , similar to what was done in TinyGarble ( Songhori et al. , 2015 ) . More concretely , to generate efficient Boolean circuits for private BAFFLE , we constrained the mapping to free XOR gates and non-free AND gates . We enhanced the cost functions of the single gates : We set the delay and area of XOR gates to 0 , the delay and area of the inverters to 0 ( as they can be replaced with XOR gates with the constant input 1 ) , and the delay and area of AND gates to a non-0 value . We provide details on the protocol design of private BAFFLE \u2019 s design in \u00a7C and we will extend the details for private BAFFLE in the main part of the paper ( e.g. , we will elaborate more on the circuit generation ) . * * Separating the appendix . * * ( \u201c minor : please attach your main context pdf in the submission and submit the appendix in the supplementary material \u201d ) We surely can do this if it is required . However , we would like to point out that the structure of our paper follows closely the explicit requirements laid out in the CfP and resembles the structure of numerous other papers published in recent years at ICLR . It is therefore not quite clear to us what benefit it would provide to the reader to push materials from the appendices to supplementary materials as this would likely result in making these materials somewhat more difficult to access and therefore decrease the readability of the paper as a self-contained publication . # Changes : * We clarified our contributions when designing and implementing private BAFFLE in \u00a74 and \u00a7D . Please let us kindly know of any further clarifications or changes required to clear all possibly remaining doubts and to get your support ."}, {"review_id": "WrNjg9tCLUt-3", "review_text": "The paper proposes a backdoor-resilient federated learning method to defend the backdoor attack of poisoning the models . Their method consists of the dynamic clustering , adaptive clipping and noise adding . There are extensive experiments to demonstrate the effectiveness . The cosine distance calculation between W_i and W_j is conducted of every pair of the models . The clustering is based on the assumption that the poisonous and benign models can be classified into two parts . In Algorithm 1 , line 9 , is fetching median euclidean distance a safe clipping threshold to remove the outliers ? Some study is encouraged to discover such choice . How to choose the parameter \\lambda such that adding the noise N ( 0 , sigma ) do not flooding the model G_t ? As training the global model G_t is with many iterations , each iteration adding certain level of noise , how to guarantee the model training convergence ? As the framework under federated learning , would the framework consider the communication cost between the global model and the local models ? i.e. , how many model synchronizations are needed ? What are the overall operation complexity ? For Private BAFFLE , what is the consideration to choose STPC ? As in experiments , there are differential privacy based method ? Would the authors provide the reasoning to compare different privacy methods ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your valuable comments ! We are looking very forward to improve our paper with your guidance . Please find below our responses and a list of changes that we implemented : # Clarifications : * * A safe clipping threshold . * * Our adaptive clipping threshold ( the median euclidean distance threshold ) is safe and effective because : ( 1 ) We select the median Euclidean distance as a basis for clipping . Since we assume that more than 50 % of clients are benign ( a common assumption in the literature ) , we can be sure that this value is always computed between a benign local model and the global model . ( 2 ) The Euclidean distances are reduced after each training round since the global model converges . Hence , the clipping bound is adaptively reduced to avoid damaging the performance of the global model ( cf.\u00a73.2 , Fig.3 ) .We justify our clipping threshold choice in Appendix F.1 , Paragraph 3 ( Effectiveness of Clipping ) , in which we empirically compare our choice to the static approach as well as to other potential thresholds . Figure 7 shows that using the median Euclidean distance as the threshold can effectively mitigate backdoors while retaining the main task accuracy . Note : Further , it is worth clarifying that the median Euclidean distance threshold is not used to remove outliers like it is done with the clustering approach . Instead , it is used to eliminate the poisoned model weights by scaling down local models with high Euclidean distances to the global model ( cf.Alg.1 , Line 11 ) in order to reduce attack impact . In particular , it hinders adversaries to scale up malicious model updates , e.g. , as done in the model replacement-attack ( Bagdasaryan et al. , 2020 ) . * * Specifying $ \\lambda $ and noise level . * * In BAFFLE , the noise level $ \\sigma $ is calculated based on $ \\lambda $ and the median of the Euclidean distances $ S_t $ ( cf.Alg.1 , Line 13 ) . $ S_t $ weighs the noise level according to the difference between local and global models : the level of noise is dynamically reduced after each training round as the local models converge towards the global model ( Bagdasaryan et al. , 2020 ) . We empirically determined $ \\lambda = 0.001 $ for image classification and word prediction , and $ \\lambda = 0.01 $ for the IoT datasets . With this , we ensure that we do not add too much noise , as this would damage the main task accuracy of the global model $ G_t $ , and that we still effectively mitigate backdoors in combination with our dynamic clustering and adaptive clipping approach . To justify our choice , we have run an experiment to compare the effectiveness of different $ \\lambda $ values and noise levels in Appendix F.1 , Paragraph 4 ( Effectiveness of Adding Noise ) , and depict the results in Fig.8 . * * Overhead of BAFFLE . * * We made experiments where all participants started from a randomly initialized model . In average , 1.67 additional rounds are needed . The details are discussed in appendix F.10 . Besides these small number of additional rounds , _no_ further overhead is created by BAFFLE . * * STPC vs. DP for private BAFFLE . * * As pointed out by the reviewer , differential privacy ( DP ) might be a tempting choice to reduce the information leaking from model updates . DP is a statistical approach that can be relatively efficiently implemented , however , it can only offer effective privacy protection at the cost of a severe loss in model accuracy due to the high amount of noise that needs to be added to the models ( see Zhang et al. , 2020 ; Aono et al. , 2017 ; So et al. , 2019 ) . This is the reason why we chose to use Secure Two-Party Computation ( STPC ) : It guarantees strong privacy as well as high efficiency ( compared to other cryptographic techniques such as homomorphic encryption [ 1 ] ) . Hence , STPC represents the best possible choice and trade-off to achieve provable privacy , efficiency , and accuracy . We discuss this in \u00a74 . # Changes : * We added a summary of the discussion on clipping and noising in \u00a73.2 and \u00a75.2 and refer to Appendix F.1 , Paragraph 3 and 4 . * We also added an experiment to show that our backdoor defense does not require too many more iterations ( cf.Appendix F.10 ) . Please let us kindly know of any further clarifications or changes required to clear all possibly remaining doubts and to secure your support . # References : [ 1 ] Gentry , Craig , and Dan Boneh . A fully homomorphic encryption scheme . Stanford University , 2009 ."}], "0": {"review_id": "WrNjg9tCLUt-0", "review_text": "In the paper , the authors proposed a novel privacy-preserving defense approach BAFFLE for federated learning which could simultaneously impede backdoor and inference attacks . To impede backdoor attacks , the Model Filtering layer ( i.e. , by dynamic clustering ) and Poison Elimination layer ( i.e. , by noising and clipping ) were presented respectively for the malicious updates and the weak manipulations of the model . To thwart inference attacks , private BAFFLE was built to evaluate the BAFFLE algorithm under encryption using secure computation techniques . [ Strengths ] 1 . The paper is clear , logical , and easy to follow . 2.The topic of simultaneously defending against the backdoor and the inference attacks is significant . 3.Evaluations were conducted on multiple datasets and applications , including image classi\ufb01cation , word prediction , and IoT intrusion detection . [ Weaknesses ] 1 . The topic is significant but the contributions to the proposed approach are limited . 2.To impede backdoor attacks , many models are marked as outliers and discarded , clipped , and noised , generally speaking , which could lead to performance degradation . However , there is only a negligible effect on performance . What is the cause of this phenomenon\uff1f 3 . In FL , clients locally train model updates using private data and provide these to a central aggregator . If some models were directly discarded in the central aggregator , the corresponding private data are not utilized for model training which is not an ideal approach , especially , the private data is irreplaceable . 4.Clipping and noising are the means of eliminating weak manipulations . How about the settings of them affect the results ? Some analysis and ablation experiments are needed . After reading the response , I still think that the work is promising and would like to keep my recommendation .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your valuable comments ! We are looking forward to improve our paper with your guidance . Please find below our responses and a list of changes that we implemented : # Clarifications : * * Limited contributions . * * We would kindly ask the reviewer to specify more precisely * what * the limitations referred to are . Our main contribution is to propose a _novel_ and _generic_ approach to tackle state-of-the-art backdoor and inference attacks . These attacks have been recently introduced in top-tier machine learning as well as security conferences , e.g. , ICLR ( Xie et al. , 2020 ) , AISTATS ( Bagdasaryan et al. , 2020 ) , USENIX Security ( Fang et al. , 2020 ) , and IEEE S & P ( Melis et al. , 2019 ) . Our extensive evaluation shows that our approach mitigates all these attacks as well as adaptive attacks effectively . It is common sense in security research community to show that any defenses ( also generic ones ) can successfully mitigate known attacks in the literature . Thus , we address the deficiency of state-of-the-art defenses in a way that to the best of our knowledge has not been proposed before . And , on top of that we also solve another challenging problem by designing these defenses in a privacy-preserving manner . * * Minimum effect on the performance of the global model . * * Mitigating backdoors , while retaining the performance of the model is the main goal of our paper . We aim at minimizing the following parameters : ( 1 ) the number of models that are filtered out , ( 2 ) the clipping bound , and ( 3 ) the noise level , as all of these would negatively impact the benign performance of the model . Our dynamic clustering approach only removes models that potentially have high attack impact in order to avoid falsely rejecting benign models ( cf.\u00a73.1 ) .We introduce adaptive clipping ( \u00a73.2 ) and adaptive noising ( \u00a73.3 ) so that the clipping bound and noise level dynamically adapt to the changes in the models in different training iterations . Moreover , since the poisoned models having high attack impact are filtered out by the Model Filtering layer , this reduces the burden to the Poison Elimination layer to mitigate backdoors , i.e. , the clipping bound is increased ( the model is clipped less ) and noise level is decreased ( less noise needs to be added ) . As a result , our clipping and noising approaches do not degrade the performance of the model . * * Effect of the discarded models on the performance . * * We agree that in the ideal case all benign models should be kept . Our approach , therefore , aims at minimizing the number of models that are falsely filtered out . By doing this , we significantly improve over state-of-the-art approaches , e.g. , Krum ( Blanchard et al. , 2017 ) or Auror ( Shen et al. , 2016 ) . For example , Krum only selects a single centroid local model as the global model , or collects a small subset of local models ( Multi-Krum ) to be aggregated in the global model , and thus , ignores contributions of a significant amount of benign models . In addition , Auror clusters model updates into two classes using K-means clustering , which is too coarse-grained and generates many false positives . Even worse , it can not handle the simultaneous injection of multiple backdoors ( cf.\u00a73.1 ) . * * How the settings of clipping and noising affect the results . * * We agree that eliminating weak manipulations is the primary goal of our clipping and noising components ( the Poison Elimination layer ) . In \u00a75.2 , we analyzed and evaluated the effect of these components . As shown in Fig.5 , Poison Elimination is only effective if the poisoned data rate is below 13 % . However , Model Filtering is effective above the poisoned data rate of 13 % , as it then can reliably identify poisoned models . Moreover , we also evaluated different clipping and noising thresholds to justify our choices in Appendix F.1 , Paragraph 3 and 4 and illustrated the results in Fig.7 and 8. # Changes : * We added a summary of the discussion on clipping and noising in Sect . 5.2 and refer to Appendix F.1 , Paragraph 3 and 4 . Please let us kindly know of any further clarifications or changes required to clear all possibly remaining doubts and to secure your support ."}, "1": {"review_id": "WrNjg9tCLUt-1", "review_text": "This paper suggests a new solution to protect FL models from backdoor attacks . Under the backdoor attack , an adversary can manipulate a few clients ' weight matrices to affect the final global model . In particular , the adversary wants the final model to make an incorrect prediction for certain inputs . This paper 's main idea to defend against a backdoor attack is to use clustering and adaptive clipping and noising . In the clustering phase , the aggregator uses a clustering technique to identifies the weight matrices that have been manipulated by the adversary . In the clipping and noising phase , the aggregator tries to mitigate the effect of manipulated weight matrices that could not be identified in the filtering phase . Strengths : This paper uses an existing clustering algorithm ( the HDBSCAN clustering algorithm ( Campello et al. , 2013 ) ) that works best in the FL problem in identifying manipulated weight matrices . Using this clustering algorithm combined with adaptive clipping and noising , the proposed method can mitigate the backdoor attack . Moreover , BAFFLE , combined with the Secure-Two-Party Computation method , can protect the FL model from Inference attacks . The extensive numerical examples show that the proposed method outperforms other defense methods most of the time . Weaknesses : This paper does not provide any theoretical guarantee and only applies the existing methods to mitigate the backdoor attacks . Therefore , we can not make sure that the proposed method works in any problem or on any dataset . As the numerical examples show ( e.g. , see Table 2 ) , BAFFLE can not outperform other defense methods all the time . Due to the lack of theoretical analysis or developing a new method/algorithm , I vote for weak acceptance ( 6 ) . I ask the authors to clarify/explain the $ \\bf { \\underline { novelty } } $ of their algorithms during the discussion period .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your valuable comments ! We are looking very forward to improve our paper with your guidance . Please find below our responses and a list of changes that we implemented : # Clarifications : * * The novelty of our approach . * * 1.A novel generic privacy-preserving FL backdoor system that simultaneously protects both the security and data privacy of FL-based applications by effectively preventing backdoor and inference attacks . To the best of our knowledge , this is the first work that discusses and tackles this dilemmatic challenge , i.e. , _no_ existing defense against backdoor attacks preserves the privacy of the clients \u2019 data . 2.A novel generic backdoor defense ( cf.Algorithm 1 ) that has three-folds of novelty : ( 1 ) a novel two-layer defense , ( 2 ) a new clustering approach , and ( 3 ) a new adaptive threshold tuning scheme for clipping and noising as follows : * A novel two-layer defense concept : To the best of our knowledge , we are the first to point out that combining two classes of defenses can prevent the adversary to trade-off between attack impact and attack stealthiness . The reason is that clustering-based defenses are effective for mitigating attacks with intense manipulations ( \u00a73.1 ) while \u201c clipping and noising-based \u201d defenses are useful for eliminating weak manipulations ( stealthiness ) ( \u00a73.2 and \u00a73.3 ) . However , the na\u00efve combination of these two classes of defenses is not effective . Our evaluation ( as discussed in the last paragraph in \u00a7F.1 ) shows that simply stacking clustering using K-means ( e.g. , Shen et al. , 2016 ) and clipping and noising ( e.g. , Bagdasaryan et al.2020 ) is not effective to mitigate sophisticated backdoor attacks . Therefore , we introduce a new clustering approach as well as clipping and noising approaches to mitigate powerful backdoor attacks . * A novel clustering approach tackling dynamic attack scenarios ( \u00a73.1 ) : We utilize a completely different clustering approach ( density-based clustering , HDBSCAN/DBSCAN ) with appropriate parameterization so that clusters containing poisoned models are identified as outliers , irrespective of the number of backdoors ( Alg.1 , Lines 6 and 7 ) . Existing approaches cluster model updates either into two classes using K-means ( e.g. , Shen et al. , 2016 ) , or only select one or a small subset of models ( Blanchard et al. , 2017 ) , which is too coarse-grained and generates many false positives . Even worse , it can not handle the simultaneous injection of multiple backdoors . * A novel adaptive threshold tuning scheme for finding effective clipping bound ( \u00a73.2 ) and noise level ( \u00a73.3 ) : Existing approaches do not discuss/consider this at all . Since Euclidean distances between the local models and the global model indicating how much the local models have changed after local training , and these distances reduce every round ( cf.Fig.3 ) , it can be used to specify the clipping bound ( Alg.1.Lines 8 and 9 ) and the noise level ( Alg.1.Lines 8 , 9 , and 13 ) to adapt to the changes in each training iterations . # Changes : * We made the novelty aspects of our approach clearer in the contribution section in \u00a71 . Please let us kindly know of any further clarifications or changes required to clear all possibly remaining doubts and to get your support ."}, "2": {"review_id": "WrNjg9tCLUt-2", "review_text": "This paper provides an interesting research direction for the cross-domain of federating learning and backdoor attacks . This direction has very limited work until the recent 2 years . The work being proposed in this manuscript is simple and straightforward to implement . The pipeline has been clearly demonstrated . The experiments have multiple aspects presented and show promising results in various metrics . pros : - Novel problem , may attract massive attention - Simple and intuitive pipeline , conceptually easy to implement - Results are versatile , many comparison tables are provided cons : - Whole article is not self-contained , feel the connections between modules are very loose - The design of the pipeline is very ad-hoc , so many engineering aspects can be tweaked and the performance could be dramatically altered . - The experiments could use a few popular trojan attack methods , the baselines are not comprehensive . concerns : - My major concern comes from the design of the pipeline and the experiments . The author ( s ) have created many splendid terms to describe the modules used in this work , however , their implementation uses both clustering and median , which is very engineering and may not reliable with a different clustering algorithm or data set is severely unbalanced ( just like the non-iid data sets among clients ) . This kind of uncertainty due to the ad-hoc nature of the pipeline causes me to wonder : how bad this framework can be if any of the carefully cherry-picked modules fails its purpose ? The mathematical motivation of this paper is missing and this causes the impression of untrustedness on the model design . It would be better if the author ( s ) can 1. provide some mathematical proofs or derivations to support your design . 2. provide a lower bound or upper bound for performance guarantee . - Please compare it with a few Trojan attack methods in recent years . I believe no matter what kind of backdoor and Trojan attack , can be easily applied to FL by applying them individually on each client without too much trouble . An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks . KDD 2020 - For federated learning , one important experimental factor is the number of clients ( K= 5 , 10 , 50 , 100 , etc ) , the portion of data on each client ( 1 % , 5 % , 10 % , etc ) , and data distribution assumption ( iid , non-iid with feature shift or label shift , etc ) . Please evaluate the results by changing these important hyperparameters . - The holomorphic encryption is a pretty standard concept in FL , I do n't understand why the author ( s ) have listed this as the major contribution for the work . Especially , only one paper from 1986 is mentioned and nothing especially has been proposed in this work . It is just an unusual way to list your contribution . - Code is not provided , I can not see the reproducibility of this work . minor : please attach your main context pdf in the submission and submit the appendix in the supplementary material", "rating": "4: Ok but not good enough - rejection", "reply_text": "# Clarifications : * * Secure two-party computation ( from 1986 ) . * * It seems that there is a misunderstanding here . _Nowhere_ in the paper do we mention that we would use homomorphic encryption ( HE ) for BAFFLE . Instead , we use secure two-party computation ( STPC ) techniques . STPC is more efficient than HE while also being provably secure . STPC effectively prevents the aggregator from accessing the model updates and hence mitigates inference attacks on the local models . Indeed , the pioneering work by Andrew Yao in 1986 , that we cite , was ground-breaking for secure computation : Yao showed that it is possible to evaluate any efficiently computable function securely ( i.e. , in a privacy-preserving manner ) . This is why citing the paper by Yao ( Yao , 1986 ) is standard practice in the security and privacy research community due to the role of Yao \u2018 s work as a seminal paper in this area . However , since Yao \u2019 s publication , an extensive line of research work followed his paradigm and introduced optimized secure computation protocols , implementations , and various efficiency improvements , e.g. , point-and-permute ( Beaver et al.1990 ) , ( Kolesnikov & Schneider , 2008 ) , FastGC ( Huang et al.,2011 ) , fixed-key AES ( Bellare et al. , 2013 ) , and half-gates ( Zahur et al. , 2015 ) to name some . We use ABY ( Demmler et al. , 2015 ) , a state-of-the-art STPC-framework that implements three techniques for secure computation : Yao \u2019 s Garbled Circuits ( originally introduced by Yao in 1986 ) , Boolean- and Arithmetic-Sharing ( building upon a work by Goldreich et al.in 1987 ) .Note , that ABY also includes recent advancements that make the original protocols from 1986/87 significantly more efficient . For details , please refer to ( Demmler et al. , 2015 ) . * * Our STPC contributions . * * Our contribution in private BAFFLE goes significantly beyond simply applying existing STPC protocols and the ABY framework . We carefully designed all components and operations as Boolean circuits in a very efficient way . For example , we synthesized _novel_ ( previously not existing ) and highly optimized circuits ( we will highlight this more prominently in the revised version of the paper ) for the square root calculation that is also of independent interest and can be used for other applications that need a privacy-preserving computation of the square root ( e.g. , any protocol that uses the Euclidean distance like privacy-preserving face recognition ( Osadchy et al. , 2010 ) ) . For the circuit generation , we customized the flow of the commercial hardware logic synthesis tools ( DC , 2010 ) to generate circuits optimized for the Garbled Circuits technique . For example , with the free-XOR technique ( Kolesnikov & Schneider , 2008 ; which allows to calculate all XOR operations for _free_ ) , one has to minimize the number of non-XOR gates in the Boolean representation . We developed a technology library to guide the mapping of the logic to the circuit with no manufacturing rules defined , similar to what was done in TinyGarble ( Songhori et al. , 2015 ) . More concretely , to generate efficient Boolean circuits for private BAFFLE , we constrained the mapping to free XOR gates and non-free AND gates . We enhanced the cost functions of the single gates : We set the delay and area of XOR gates to 0 , the delay and area of the inverters to 0 ( as they can be replaced with XOR gates with the constant input 1 ) , and the delay and area of AND gates to a non-0 value . We provide details on the protocol design of private BAFFLE \u2019 s design in \u00a7C and we will extend the details for private BAFFLE in the main part of the paper ( e.g. , we will elaborate more on the circuit generation ) . * * Separating the appendix . * * ( \u201c minor : please attach your main context pdf in the submission and submit the appendix in the supplementary material \u201d ) We surely can do this if it is required . However , we would like to point out that the structure of our paper follows closely the explicit requirements laid out in the CfP and resembles the structure of numerous other papers published in recent years at ICLR . It is therefore not quite clear to us what benefit it would provide to the reader to push materials from the appendices to supplementary materials as this would likely result in making these materials somewhat more difficult to access and therefore decrease the readability of the paper as a self-contained publication . # Changes : * We clarified our contributions when designing and implementing private BAFFLE in \u00a74 and \u00a7D . Please let us kindly know of any further clarifications or changes required to clear all possibly remaining doubts and to get your support ."}, "3": {"review_id": "WrNjg9tCLUt-3", "review_text": "The paper proposes a backdoor-resilient federated learning method to defend the backdoor attack of poisoning the models . Their method consists of the dynamic clustering , adaptive clipping and noise adding . There are extensive experiments to demonstrate the effectiveness . The cosine distance calculation between W_i and W_j is conducted of every pair of the models . The clustering is based on the assumption that the poisonous and benign models can be classified into two parts . In Algorithm 1 , line 9 , is fetching median euclidean distance a safe clipping threshold to remove the outliers ? Some study is encouraged to discover such choice . How to choose the parameter \\lambda such that adding the noise N ( 0 , sigma ) do not flooding the model G_t ? As training the global model G_t is with many iterations , each iteration adding certain level of noise , how to guarantee the model training convergence ? As the framework under federated learning , would the framework consider the communication cost between the global model and the local models ? i.e. , how many model synchronizations are needed ? What are the overall operation complexity ? For Private BAFFLE , what is the consideration to choose STPC ? As in experiments , there are differential privacy based method ? Would the authors provide the reasoning to compare different privacy methods ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your valuable comments ! We are looking very forward to improve our paper with your guidance . Please find below our responses and a list of changes that we implemented : # Clarifications : * * A safe clipping threshold . * * Our adaptive clipping threshold ( the median euclidean distance threshold ) is safe and effective because : ( 1 ) We select the median Euclidean distance as a basis for clipping . Since we assume that more than 50 % of clients are benign ( a common assumption in the literature ) , we can be sure that this value is always computed between a benign local model and the global model . ( 2 ) The Euclidean distances are reduced after each training round since the global model converges . Hence , the clipping bound is adaptively reduced to avoid damaging the performance of the global model ( cf.\u00a73.2 , Fig.3 ) .We justify our clipping threshold choice in Appendix F.1 , Paragraph 3 ( Effectiveness of Clipping ) , in which we empirically compare our choice to the static approach as well as to other potential thresholds . Figure 7 shows that using the median Euclidean distance as the threshold can effectively mitigate backdoors while retaining the main task accuracy . Note : Further , it is worth clarifying that the median Euclidean distance threshold is not used to remove outliers like it is done with the clustering approach . Instead , it is used to eliminate the poisoned model weights by scaling down local models with high Euclidean distances to the global model ( cf.Alg.1 , Line 11 ) in order to reduce attack impact . In particular , it hinders adversaries to scale up malicious model updates , e.g. , as done in the model replacement-attack ( Bagdasaryan et al. , 2020 ) . * * Specifying $ \\lambda $ and noise level . * * In BAFFLE , the noise level $ \\sigma $ is calculated based on $ \\lambda $ and the median of the Euclidean distances $ S_t $ ( cf.Alg.1 , Line 13 ) . $ S_t $ weighs the noise level according to the difference between local and global models : the level of noise is dynamically reduced after each training round as the local models converge towards the global model ( Bagdasaryan et al. , 2020 ) . We empirically determined $ \\lambda = 0.001 $ for image classification and word prediction , and $ \\lambda = 0.01 $ for the IoT datasets . With this , we ensure that we do not add too much noise , as this would damage the main task accuracy of the global model $ G_t $ , and that we still effectively mitigate backdoors in combination with our dynamic clustering and adaptive clipping approach . To justify our choice , we have run an experiment to compare the effectiveness of different $ \\lambda $ values and noise levels in Appendix F.1 , Paragraph 4 ( Effectiveness of Adding Noise ) , and depict the results in Fig.8 . * * Overhead of BAFFLE . * * We made experiments where all participants started from a randomly initialized model . In average , 1.67 additional rounds are needed . The details are discussed in appendix F.10 . Besides these small number of additional rounds , _no_ further overhead is created by BAFFLE . * * STPC vs. DP for private BAFFLE . * * As pointed out by the reviewer , differential privacy ( DP ) might be a tempting choice to reduce the information leaking from model updates . DP is a statistical approach that can be relatively efficiently implemented , however , it can only offer effective privacy protection at the cost of a severe loss in model accuracy due to the high amount of noise that needs to be added to the models ( see Zhang et al. , 2020 ; Aono et al. , 2017 ; So et al. , 2019 ) . This is the reason why we chose to use Secure Two-Party Computation ( STPC ) : It guarantees strong privacy as well as high efficiency ( compared to other cryptographic techniques such as homomorphic encryption [ 1 ] ) . Hence , STPC represents the best possible choice and trade-off to achieve provable privacy , efficiency , and accuracy . We discuss this in \u00a74 . # Changes : * We added a summary of the discussion on clipping and noising in \u00a73.2 and \u00a75.2 and refer to Appendix F.1 , Paragraph 3 and 4 . * We also added an experiment to show that our backdoor defense does not require too many more iterations ( cf.Appendix F.10 ) . Please let us kindly know of any further clarifications or changes required to clear all possibly remaining doubts and to secure your support . # References : [ 1 ] Gentry , Craig , and Dan Boneh . A fully homomorphic encryption scheme . Stanford University , 2009 ."}}