{"year": "2020", "forum": "ryeFY0EFwS", "title": "Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization", "decision": "Accept (Poster)", "meta_review": "The paper proposes an intuitive causal explanation for the generalization properties of GD methods. The reviewers appreciated the insights, with one reviewer claiming that there was significant overlap with existing work.\n\nI ultimately decided to accept this paper as I believe intuitive explanations are critical to the propagation of ideas. That being said, there is a tendency in this community to erase past, especially theoretical, work, for that very reason that theoretical work is less popular.\n\nHence, I want to make it clear that the acceptance of this paper is based on the premise that the authors will incorporate all of reviewer 3's comments and give enough credit to all relevant work (namely, all the papers cited by the reviewer) with a proper discussion on the link between these.", "reviews": [{"review_id": "ryeFY0EFwS-0", "review_text": "This paper posits that similar input examples will have similar gradients, leading to a gradient \"coherence\" phenomenon. A simple argument then suggests that the loss should decrease much more rapidly when gradients cohere than when they do not. This hypothesis and analysis is supported with clever experiments that confirm some of the predictions of this theory. Furthermore, since, as the authors emphasize, their hypothesis is prescriptive, they are able to suggest a novel regularization technique and show that it is effective in a simple setting. I find the coherent gradient hypothesis to be simple and reasonable. Furthermore, the paper is written very clearly, and as far as I know the main idea is original (although since it is a rather simple phenomenon, it's possible something similar could have appeared elsewhere in the literature). Perhaps more importantly, the associated experiments are very cleverly designed and are very supportive of the hypothesis. For instance, Figure 1 provides compelling evidence for the coherent gradient hypothesis and in particular motivates the way phenomenon of early stopping arises a natural consequence. Overall, the paper is of very high quality, and I recommend its acceptance. One criticism perhaps is whether these results are sufficiently significant. On the one hand, most of the experiments were done on small network and dataset combinations -- and the proposed regularization scheme as is will not scale to practical problems of interest. On the other hand, I really feel like I learned something interesting about gradient descent from reading this paper and absorbing the experimental results -- which is often not something I can say given the large array of reported experimental results in this field. It's clear that the authors themselves are aware that it's of interest to extend their results to more realistic settings, and regardless I think that this paper stands alone as is and should be accepted to ICLR.", "rating": "8: Accept", "reply_text": "Thank you for your encouragement and we hope that our work inspires others to study this line of argument for understanding generalization . ( We are also working on larger scale studies as mentioned in the Future Work and so far our experiments on other datasets have been encouraging . )"}, {"review_id": "ryeFY0EFwS-1", "review_text": "Summary The surprising generalization properties of neural networks trained with stochastic gradient descent are still poorly understood. The present work suggests that they can be explained at least partly by the fact that patterns shared across many data points will lead to gradients pointing in similar directions, thus reinforcing each other. Artefacts specific to small numbers of data points however will not have this property and thus have a substantially smaller impact on the learning. Numerical experiments on MNIST with label-noise indeed show that even though the neural network is able to perfectly fit even the flipped labels, the \"pristine\" labels are fittet much earlier during training. The authors also experiment with explicitly clipping \"outlier gradients\" and show that the resulting algorithm drastically reduces overfitting, thus further supporting the coherent gradient hypothesis. Decision The present work proposes a plausible, simple mechanism that might be contributing to the generalization of Neural Networks trained with gradient descent. Parts of the discussion stay informal as the authors themselves admit, but I appreciate that rather than providing mathematical decoration the authors focus on well-designed experiments that support their claims. Overall, the paper is of high quality and provides an interesting perspective on an important topic, which is why I think it should be accepted. Questions for the authors The coherent gradient hypothesis seems equally valid in the absence of stochasticity. However, the latter is often seen as an explanation of the generalization performance of SGD. My understanding is that you are also using minibatched gradient descent. Would you expect your experiments to still be valid when using deterministic gradient descent (full batch)? Did you study the effects of large batch sizes on the experiments?", "rating": "8: Accept", "reply_text": "Thank you for your feedback and the encouragement . Your question is very insightful . Indeed we believe that one of the predictions/consequences of Coherent Gradients is that stochasticity is not fundamental to generalization . Our experiments bear that out ( we allude to it in Section 4 \u201c from our experiments .. it appears not to be necessary. \u201d ) . These are some results with full batch training using otherwise the exact setup as Section 2 ( thus each step is also an epoch ) . We see that in the 0 % label noise case , there is good generalization throughout . Label Noise = 0 % , Step = 200 , Training accuracy = 0.918 , Test accuracy = 0.921 Label Noise = 100 % , Step = 200 , Training accuracy = 0.125 , Test accuracy = 0.110 Label Noise = 0 % , Step = 10,000 , Training accuracy = 0.996 , Test accuracy = 0.981 Label Noise = 100 % , Step = 10,000 , Training accuracy = 0.396 , Test accuracy = 0.104 Label Noise = 0 % , Step = 170,000 , Training accuracy = 1.000 , Test accuracy = 0.984 Label Noise = 100 % , Step = 170,000 , Training accuracy = 1.000 , Test accuracy = 0.108 Note that this is also consistent with the findings in the recent large scale study ( https : //arxiv.org/abs/1811.03600 ) who find \u201c no evidence that larger batch sizes degrade out-of-sample performance . \u201d"}, {"review_id": "ryeFY0EFwS-2", "review_text": " The paper studies the link between alignment of the gradients computed on different examples, and generalization of deep neural networks. The paper tackles an important research question, is very clearly written, and proposes an insightful metric. In particular, through the lenses of the metric it is possible to understand better the learning dynamics on random labels. However, the submission seems to have limited novelty, based on which I am leaning towards rejecting the paper. Detailed comments 1. The prior and concurrent work is not discussed sufficiently: a) The novelty of the \"Coherent Gradients hypothesis\" is not clear to me. First, the empirical fact that some examples are easier to learn than others in training of deep networks was the key focus of [5]. Hence, \"Coherent Graident Hypothesis\" should be mostly considered an explanation for why simple examples are/simple function are learned first. \"Coherent Gradient Hypothesis\" proposes that the key mechanism behind this phenomena is that simple examples/functions have co-aligned gradients and hence a larger \"effective\" learning rate. However, there are already quite convincing and closely related hypotheses. For example, the spectral bias interpretation of deep networks [2] and (2) suggests the same view actually. Just expressed in a different formalism, but can be also casted as having a higher effective learning rate for the strongest modes. Similarly, [3] proposes that SGD learns functions of increased complexity. A detailed comparison between these hypotheses is needed. b) \"Gradient coherence\" metric is very closely related to Stiffness studied in [1] (01.2019 on arXiv). [1] studies the cosine (or sign) between gradients coming from different examples, and reach quite similar conclusions. It is also worth noting that [6, 7] propose and study a very similar metric as well. While arXiv submissions is not consider prior work, these three preprints should be discussed in detail in the submission. c) It should be also remarked that \"Coherent Gradient hypothesis\" is to some extend folk knowledge. In particular, it is quite well known and also brought to the attention of the deep learning community that in linear regression strongest modes of the datasets as learned first when training using GD (see for instance [4]), which causally speaking stems directly from gradient coherence; these modes correspond to the largest eigenvalues of the (constant) Hessian. To make it more precise: consider that GD solving linear regression can be seen as having higher \"effective\" learning rates along the strongest modes in the dataset. 2. Experiments on random labels and restricting gradient norms are interesting. However, [5] should be cited. They experimented with regularization impact on memorization, which due to the addition of noise, probably also supresses weak gradients. 3. Experiments on MNIST do not feel adequate. While I do not doubt the validity of the experimental results, the paper should include results on another dataset; ideally from other domain than vision. 4. Plots in Figure 4 are too small to read. I would recommend moving half of them to the Supplement? 5. \"Understanding why solutions of the optimization problem on the training sample carry over to the population at large\" - Not sure what do you mean here. Could you please clarify? 6. \"Furthermore, while SGD is critical for computational speed, from our experiments and others (Keskar et al., 2016; Wu et al., 2017; Zhang et al., 2017) it appears not to be necessary.\". Please note there is very little work on training with GD large models. Also, citing in this context Keskar is misleading. Wasn't the whole point of Keskar to show why large batch size training overfits? Finally, there are many papers on studying the role of learning rate and batch size in generalization (not computational speed). I think this sentence should be rewritten to clarify what is the experimental data that GD is \"sufficient\", and SGD is just needed for \"computational speed\". References [1] Stanislav Fort et al, Stiffness: A New Perspective on Generalization in Neural Networks, https://arxiv.org/abs/1901.09491 [2] Rahaman et al, On the Spectral Bias of Neural Networks, https://arxiv.org/abs/1806.08734 [3] Nakkiran et al, SGD on Neural Networks Learns Functions of Increasing Complexity, https://arxiv.org/abs/1905.11604 [4] Goh, Why Momentum Really Works, https://distill.pub/2017/momentum/ [5] Arpit et al, A Closer Look at Memorization in Deep Networks, https://arxiv.org/abs/1706.05394 [6] He and Su, The Local Elasticity of Neural Networks, https://arxiv.org/abs/1910.06943 [7] Sankararaman, The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent, https://arxiv.org/abs/1904.06963", "rating": "3: Weak Reject", "reply_text": "Please see our overall response first . ( The comment numbering below corresponds to the numbering in the original review . ) 1 ( a ) ( b ) . We believe we have addressed this in our comments above but please let us know if not . 1 ( c ) .We are not sure what you mean by \u201c mode \u201d in the context of [ 4 ] . The word does not appear in [ 4 ] which is a tutorial on why momentum works . ( To clarify for other readers , we do not look at momentum or other optimizers in our paper focussing only on vanilla GD . ) It is not always easy to explain generalization in linear models either . For instance see discussion in Section 5 in https : //arxiv.org/abs/1611.03530 on precisely this topic . We would love to include a discussion of the folklore , but we and our immediate network of colleagues are not familiar with this . Also we would expect [ 2 , 3 ] to also reference such folk knowledge , so maybe there \u2019 s something we can point to there ? 2.We cite [ 5 ] ( see Section 5 , first para ) . Also see discussion above on simple examples above . 3.This is fair . Our thinking is that even in the MNIST case we do not understand why generalization happens , so why not explain that first . Furthermore , even on MNIST experiments can computationally expensive ( e.g.our expt in Section 3 as well as full-batch gradient ) . We hope this work inspires others to study this on other architectures , optimizers , datasets . 4.We \u2019 ll consider this . We thought having the 5x5 grid makes it easier to get the qualitative big picture . On the screen , the PDF should be zoomable to see full detail but do let us know if that is not your experience . 5.We are referring to generalization proper . We \u2019 ll rephrase as \u201c Understanding generalization proper , i.e. , .. \u201d . 6.Yes , we should drop the Keskar reference since they do not train with full batch . However , do note that even their large batch models generalize . The experiment in Wu et al.2017 ( e.g.Figure 1 ) shows that ( full batch ) gradient descent generalizes well ( albeit not as well as stochastic gradient descent ) . We find the same thing in our experiments with full batch ( one benefit of restricting our focus to MNIST ) -- see our response to Reviewer # 2 for the data . Thus we know that at least in these cases , stochasticity is not fundamentally responsible for generalization . Also note in this context that the recent large scale study ( https : //arxiv.org/abs/1811.03600 ) found no evidence that larger batch sizes degrade out-of-sample performance . We will rephrase to say that based on our experiments we believe that stochasticity is not fundamental to generalization ( though may help with it ) and that his has also been found in other experiments in the literature such as [ Wu et al.2017 ( Figure 1 ) ] . Furthermore , this is consistent with what is known about large batches [ https : //arxiv.org/abs/1811.03600 ] . Please let us know if this does not sound fair ."}], "0": {"review_id": "ryeFY0EFwS-0", "review_text": "This paper posits that similar input examples will have similar gradients, leading to a gradient \"coherence\" phenomenon. A simple argument then suggests that the loss should decrease much more rapidly when gradients cohere than when they do not. This hypothesis and analysis is supported with clever experiments that confirm some of the predictions of this theory. Furthermore, since, as the authors emphasize, their hypothesis is prescriptive, they are able to suggest a novel regularization technique and show that it is effective in a simple setting. I find the coherent gradient hypothesis to be simple and reasonable. Furthermore, the paper is written very clearly, and as far as I know the main idea is original (although since it is a rather simple phenomenon, it's possible something similar could have appeared elsewhere in the literature). Perhaps more importantly, the associated experiments are very cleverly designed and are very supportive of the hypothesis. For instance, Figure 1 provides compelling evidence for the coherent gradient hypothesis and in particular motivates the way phenomenon of early stopping arises a natural consequence. Overall, the paper is of very high quality, and I recommend its acceptance. One criticism perhaps is whether these results are sufficiently significant. On the one hand, most of the experiments were done on small network and dataset combinations -- and the proposed regularization scheme as is will not scale to practical problems of interest. On the other hand, I really feel like I learned something interesting about gradient descent from reading this paper and absorbing the experimental results -- which is often not something I can say given the large array of reported experimental results in this field. It's clear that the authors themselves are aware that it's of interest to extend their results to more realistic settings, and regardless I think that this paper stands alone as is and should be accepted to ICLR.", "rating": "8: Accept", "reply_text": "Thank you for your encouragement and we hope that our work inspires others to study this line of argument for understanding generalization . ( We are also working on larger scale studies as mentioned in the Future Work and so far our experiments on other datasets have been encouraging . )"}, "1": {"review_id": "ryeFY0EFwS-1", "review_text": "Summary The surprising generalization properties of neural networks trained with stochastic gradient descent are still poorly understood. The present work suggests that they can be explained at least partly by the fact that patterns shared across many data points will lead to gradients pointing in similar directions, thus reinforcing each other. Artefacts specific to small numbers of data points however will not have this property and thus have a substantially smaller impact on the learning. Numerical experiments on MNIST with label-noise indeed show that even though the neural network is able to perfectly fit even the flipped labels, the \"pristine\" labels are fittet much earlier during training. The authors also experiment with explicitly clipping \"outlier gradients\" and show that the resulting algorithm drastically reduces overfitting, thus further supporting the coherent gradient hypothesis. Decision The present work proposes a plausible, simple mechanism that might be contributing to the generalization of Neural Networks trained with gradient descent. Parts of the discussion stay informal as the authors themselves admit, but I appreciate that rather than providing mathematical decoration the authors focus on well-designed experiments that support their claims. Overall, the paper is of high quality and provides an interesting perspective on an important topic, which is why I think it should be accepted. Questions for the authors The coherent gradient hypothesis seems equally valid in the absence of stochasticity. However, the latter is often seen as an explanation of the generalization performance of SGD. My understanding is that you are also using minibatched gradient descent. Would you expect your experiments to still be valid when using deterministic gradient descent (full batch)? Did you study the effects of large batch sizes on the experiments?", "rating": "8: Accept", "reply_text": "Thank you for your feedback and the encouragement . Your question is very insightful . Indeed we believe that one of the predictions/consequences of Coherent Gradients is that stochasticity is not fundamental to generalization . Our experiments bear that out ( we allude to it in Section 4 \u201c from our experiments .. it appears not to be necessary. \u201d ) . These are some results with full batch training using otherwise the exact setup as Section 2 ( thus each step is also an epoch ) . We see that in the 0 % label noise case , there is good generalization throughout . Label Noise = 0 % , Step = 200 , Training accuracy = 0.918 , Test accuracy = 0.921 Label Noise = 100 % , Step = 200 , Training accuracy = 0.125 , Test accuracy = 0.110 Label Noise = 0 % , Step = 10,000 , Training accuracy = 0.996 , Test accuracy = 0.981 Label Noise = 100 % , Step = 10,000 , Training accuracy = 0.396 , Test accuracy = 0.104 Label Noise = 0 % , Step = 170,000 , Training accuracy = 1.000 , Test accuracy = 0.984 Label Noise = 100 % , Step = 170,000 , Training accuracy = 1.000 , Test accuracy = 0.108 Note that this is also consistent with the findings in the recent large scale study ( https : //arxiv.org/abs/1811.03600 ) who find \u201c no evidence that larger batch sizes degrade out-of-sample performance . \u201d"}, "2": {"review_id": "ryeFY0EFwS-2", "review_text": " The paper studies the link between alignment of the gradients computed on different examples, and generalization of deep neural networks. The paper tackles an important research question, is very clearly written, and proposes an insightful metric. In particular, through the lenses of the metric it is possible to understand better the learning dynamics on random labels. However, the submission seems to have limited novelty, based on which I am leaning towards rejecting the paper. Detailed comments 1. The prior and concurrent work is not discussed sufficiently: a) The novelty of the \"Coherent Gradients hypothesis\" is not clear to me. First, the empirical fact that some examples are easier to learn than others in training of deep networks was the key focus of [5]. Hence, \"Coherent Graident Hypothesis\" should be mostly considered an explanation for why simple examples are/simple function are learned first. \"Coherent Gradient Hypothesis\" proposes that the key mechanism behind this phenomena is that simple examples/functions have co-aligned gradients and hence a larger \"effective\" learning rate. However, there are already quite convincing and closely related hypotheses. For example, the spectral bias interpretation of deep networks [2] and (2) suggests the same view actually. Just expressed in a different formalism, but can be also casted as having a higher effective learning rate for the strongest modes. Similarly, [3] proposes that SGD learns functions of increased complexity. A detailed comparison between these hypotheses is needed. b) \"Gradient coherence\" metric is very closely related to Stiffness studied in [1] (01.2019 on arXiv). [1] studies the cosine (or sign) between gradients coming from different examples, and reach quite similar conclusions. It is also worth noting that [6, 7] propose and study a very similar metric as well. While arXiv submissions is not consider prior work, these three preprints should be discussed in detail in the submission. c) It should be also remarked that \"Coherent Gradient hypothesis\" is to some extend folk knowledge. In particular, it is quite well known and also brought to the attention of the deep learning community that in linear regression strongest modes of the datasets as learned first when training using GD (see for instance [4]), which causally speaking stems directly from gradient coherence; these modes correspond to the largest eigenvalues of the (constant) Hessian. To make it more precise: consider that GD solving linear regression can be seen as having higher \"effective\" learning rates along the strongest modes in the dataset. 2. Experiments on random labels and restricting gradient norms are interesting. However, [5] should be cited. They experimented with regularization impact on memorization, which due to the addition of noise, probably also supresses weak gradients. 3. Experiments on MNIST do not feel adequate. While I do not doubt the validity of the experimental results, the paper should include results on another dataset; ideally from other domain than vision. 4. Plots in Figure 4 are too small to read. I would recommend moving half of them to the Supplement? 5. \"Understanding why solutions of the optimization problem on the training sample carry over to the population at large\" - Not sure what do you mean here. Could you please clarify? 6. \"Furthermore, while SGD is critical for computational speed, from our experiments and others (Keskar et al., 2016; Wu et al., 2017; Zhang et al., 2017) it appears not to be necessary.\". Please note there is very little work on training with GD large models. Also, citing in this context Keskar is misleading. Wasn't the whole point of Keskar to show why large batch size training overfits? Finally, there are many papers on studying the role of learning rate and batch size in generalization (not computational speed). I think this sentence should be rewritten to clarify what is the experimental data that GD is \"sufficient\", and SGD is just needed for \"computational speed\". References [1] Stanislav Fort et al, Stiffness: A New Perspective on Generalization in Neural Networks, https://arxiv.org/abs/1901.09491 [2] Rahaman et al, On the Spectral Bias of Neural Networks, https://arxiv.org/abs/1806.08734 [3] Nakkiran et al, SGD on Neural Networks Learns Functions of Increasing Complexity, https://arxiv.org/abs/1905.11604 [4] Goh, Why Momentum Really Works, https://distill.pub/2017/momentum/ [5] Arpit et al, A Closer Look at Memorization in Deep Networks, https://arxiv.org/abs/1706.05394 [6] He and Su, The Local Elasticity of Neural Networks, https://arxiv.org/abs/1910.06943 [7] Sankararaman, The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent, https://arxiv.org/abs/1904.06963", "rating": "3: Weak Reject", "reply_text": "Please see our overall response first . ( The comment numbering below corresponds to the numbering in the original review . ) 1 ( a ) ( b ) . We believe we have addressed this in our comments above but please let us know if not . 1 ( c ) .We are not sure what you mean by \u201c mode \u201d in the context of [ 4 ] . The word does not appear in [ 4 ] which is a tutorial on why momentum works . ( To clarify for other readers , we do not look at momentum or other optimizers in our paper focussing only on vanilla GD . ) It is not always easy to explain generalization in linear models either . For instance see discussion in Section 5 in https : //arxiv.org/abs/1611.03530 on precisely this topic . We would love to include a discussion of the folklore , but we and our immediate network of colleagues are not familiar with this . Also we would expect [ 2 , 3 ] to also reference such folk knowledge , so maybe there \u2019 s something we can point to there ? 2.We cite [ 5 ] ( see Section 5 , first para ) . Also see discussion above on simple examples above . 3.This is fair . Our thinking is that even in the MNIST case we do not understand why generalization happens , so why not explain that first . Furthermore , even on MNIST experiments can computationally expensive ( e.g.our expt in Section 3 as well as full-batch gradient ) . We hope this work inspires others to study this on other architectures , optimizers , datasets . 4.We \u2019 ll consider this . We thought having the 5x5 grid makes it easier to get the qualitative big picture . On the screen , the PDF should be zoomable to see full detail but do let us know if that is not your experience . 5.We are referring to generalization proper . We \u2019 ll rephrase as \u201c Understanding generalization proper , i.e. , .. \u201d . 6.Yes , we should drop the Keskar reference since they do not train with full batch . However , do note that even their large batch models generalize . The experiment in Wu et al.2017 ( e.g.Figure 1 ) shows that ( full batch ) gradient descent generalizes well ( albeit not as well as stochastic gradient descent ) . We find the same thing in our experiments with full batch ( one benefit of restricting our focus to MNIST ) -- see our response to Reviewer # 2 for the data . Thus we know that at least in these cases , stochasticity is not fundamentally responsible for generalization . Also note in this context that the recent large scale study ( https : //arxiv.org/abs/1811.03600 ) found no evidence that larger batch sizes degrade out-of-sample performance . We will rephrase to say that based on our experiments we believe that stochasticity is not fundamental to generalization ( though may help with it ) and that his has also been found in other experiments in the literature such as [ Wu et al.2017 ( Figure 1 ) ] . Furthermore , this is consistent with what is known about large batches [ https : //arxiv.org/abs/1811.03600 ] . Please let us know if this does not sound fair ."}}