{"year": "2020", "forum": "Ske5UANYDB", "title": "Benefit of Interpolation in Nearest Neighbor Algorithms", "decision": "Reject", "meta_review": "The authors show that data interpolation in the context of nearest neighbor algorithms, can sometime strictly improve performance. The paper is poorly written for an ICLR audience and the added value compared to extensive prior work in the area is not clearly demonstrated.", "reviews": [{"review_id": "Ske5UANYDB-0", "review_text": "The paper studies theoretical perspective of double descent phenomenon for the interpolated K-NN classifier. The paper is works in several interesting directions and gives theoretical reasoning to how interpolated K-NN could exhibit the double descent phenomenon. They give theoretical justifications albeit with strong assumptions. I think the paper is a good paper. However, I have concerns with the presentation quality of the paper. It is very tough to get through the paper till the end. In my view, it would have been an Accept if the paper was well written.", "rating": "6: Weak Accept", "reply_text": "Thanks for pointing out our writing problem . The theorem 1 in the first submission shall serve as an important intermediate ( i.e lemma ) result . The corollaries derived from theorem 1 is actually our main point . We adjusted the displays of main theorems in a revision submission to enhance the readability ."}, {"review_id": "Ske5UANYDB-1", "review_text": "This paper is about interpolation schemes in the particular case of the Nearest Neighbor algorithm. The authors investigate the bene t, mainly theoretical, of the proposed interpolation. They study minimax rates of the proposed interpolated-NN for both classi cation and regression. The statistical stability of the Interpolated-NN is adressed. The paper is easy to understand and correctly written. Nevertheless, It is a particular case of ( \\Over tting or perfect tting? Risk bounds for classi cation and regression rules that interpolate\", Belkin, M., Hsu, D., and Mitra, P. (2018a)) with an explicit interpolation schemes given by the euclidien distance power gamma. It appears as an application of the above paper which brings only few theoretical advantages and not enough to justify, although intuitive, the choice of these weights. Only few discussions and no comparison to others bounds (as those in the above paper) of the main theorem are given. The paper is too much incremental from the papers of Belkin et al. (2018) and Xing et al. (2018) and the bene ts of the proposed interpolation are limited. Furthermore, the empirical performance of the interpolate-NN is clearly not convincing and show no signi cant practical advantages of the proposed method. As the goal of the paper is clearly theoretical, the 'real data analysis' part is not necessary in my opinion.", "rating": "1: Reject", "reply_text": "In Belkin ( 2018 ) , they technically only obtains a suboptimal bound for classification . And from their theorem , even if it is an optimal rate , it is only sufficient to state that `` interpolated-NN is not hurt by interpolation '' . Our work , by proving that interpolated NN yields a smaller multiplicative constant , asserts that `` interpolation helps improve the performance '' ."}, {"review_id": "Ske5UANYDB-2", "review_text": "This paper studies the interpolated k-nearest neighbors algorithm from a theoretical perspective. Specifically, it studies how the performance of the algorithm is affected by reweighting the k nearest neighbors according to their relative distance. This regime has been considered in prior work, particularly Belkin et al. (2018). Under various niceness conditions, the paper proves error bounds for interpolated k-nearest neighbors for both regression (i.e. squared loss) and classification (i.e. 0-1 loss after thresholding). Overall, I have the impression that this paper contains interesting ideas, but the presentation is very poor. It should be revised and resubmitted before it can be accepted. In particular, the paper does not make its contribution clear. The main theorem only appears on page 4 and the reader must consult the appendix to see the definition of all the terms that appear in the theorem. I have no idea how to interpret the (complicated) expression in the theorem. The theorem needs to be explained in intuitive terms. More context needs to be given by comparing the main theorem to prior works (which I am not familiar with). ", "rating": "3: Weak Reject", "reply_text": "Thanks for pointing out our writing problem . The theorem 1 in the first submission shall serve as an important intermediate ( i.e lemma ) result . The corollaries derived from theorem 1 is actually our main point . We adjusted the displays of main theorems in a revision submission to enhance the readability ."}], "0": {"review_id": "Ske5UANYDB-0", "review_text": "The paper studies theoretical perspective of double descent phenomenon for the interpolated K-NN classifier. The paper is works in several interesting directions and gives theoretical reasoning to how interpolated K-NN could exhibit the double descent phenomenon. They give theoretical justifications albeit with strong assumptions. I think the paper is a good paper. However, I have concerns with the presentation quality of the paper. It is very tough to get through the paper till the end. In my view, it would have been an Accept if the paper was well written.", "rating": "6: Weak Accept", "reply_text": "Thanks for pointing out our writing problem . The theorem 1 in the first submission shall serve as an important intermediate ( i.e lemma ) result . The corollaries derived from theorem 1 is actually our main point . We adjusted the displays of main theorems in a revision submission to enhance the readability ."}, "1": {"review_id": "Ske5UANYDB-1", "review_text": "This paper is about interpolation schemes in the particular case of the Nearest Neighbor algorithm. The authors investigate the bene t, mainly theoretical, of the proposed interpolation. They study minimax rates of the proposed interpolated-NN for both classi cation and regression. The statistical stability of the Interpolated-NN is adressed. The paper is easy to understand and correctly written. Nevertheless, It is a particular case of ( \\Over tting or perfect tting? Risk bounds for classi cation and regression rules that interpolate\", Belkin, M., Hsu, D., and Mitra, P. (2018a)) with an explicit interpolation schemes given by the euclidien distance power gamma. It appears as an application of the above paper which brings only few theoretical advantages and not enough to justify, although intuitive, the choice of these weights. Only few discussions and no comparison to others bounds (as those in the above paper) of the main theorem are given. The paper is too much incremental from the papers of Belkin et al. (2018) and Xing et al. (2018) and the bene ts of the proposed interpolation are limited. Furthermore, the empirical performance of the interpolate-NN is clearly not convincing and show no signi cant practical advantages of the proposed method. As the goal of the paper is clearly theoretical, the 'real data analysis' part is not necessary in my opinion.", "rating": "1: Reject", "reply_text": "In Belkin ( 2018 ) , they technically only obtains a suboptimal bound for classification . And from their theorem , even if it is an optimal rate , it is only sufficient to state that `` interpolated-NN is not hurt by interpolation '' . Our work , by proving that interpolated NN yields a smaller multiplicative constant , asserts that `` interpolation helps improve the performance '' ."}, "2": {"review_id": "Ske5UANYDB-2", "review_text": "This paper studies the interpolated k-nearest neighbors algorithm from a theoretical perspective. Specifically, it studies how the performance of the algorithm is affected by reweighting the k nearest neighbors according to their relative distance. This regime has been considered in prior work, particularly Belkin et al. (2018). Under various niceness conditions, the paper proves error bounds for interpolated k-nearest neighbors for both regression (i.e. squared loss) and classification (i.e. 0-1 loss after thresholding). Overall, I have the impression that this paper contains interesting ideas, but the presentation is very poor. It should be revised and resubmitted before it can be accepted. In particular, the paper does not make its contribution clear. The main theorem only appears on page 4 and the reader must consult the appendix to see the definition of all the terms that appear in the theorem. I have no idea how to interpret the (complicated) expression in the theorem. The theorem needs to be explained in intuitive terms. More context needs to be given by comparing the main theorem to prior works (which I am not familiar with). ", "rating": "3: Weak Reject", "reply_text": "Thanks for pointing out our writing problem . The theorem 1 in the first submission shall serve as an important intermediate ( i.e lemma ) result . The corollaries derived from theorem 1 is actually our main point . We adjusted the displays of main theorems in a revision submission to enhance the readability ."}}