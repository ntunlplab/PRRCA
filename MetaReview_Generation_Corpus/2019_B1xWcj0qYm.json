{"year": "2019", "forum": "B1xWcj0qYm", "title": "On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data", "decision": "Accept (Poster)", "meta_review": "This paper studies the task of learning a binary classifier from only unlabeled data. They first provide a negative result, i.e., they show it is impossible to learn an unbiased estimator from a set of unlabeled data. Then they provide an empirical risk minimization method which works when given two sets of unlabeled data, as well as the class priors. \n\nThe four submitted reviews were unanimous in their vote to accept. The results are impactful, and might make for an interesting oral presentation.", "reviews": [{"review_id": "B1xWcj0qYm-0", "review_text": "The authors propose an unbiased estimator that allows for training models with weak supervision on two unlabeled datasets with known class priors. The theoretical properties of the estimator are discussed and an empirical evaluation shows promising performance. The paper provides a thorough overview of the related work. The experiments compare to the relevant baselines. Minor remarks: The writing seems like it could be improved in multiple places and the main thing that makes some sections of the paper hard to follow is that the concepts often get mentioned and discussed before they are formally defined/introduced. Concepts that are introduced via citations should also be explained even if not in-depth. Figure 2: the curves suggest that the models should have been left to train for a longer time - some of the small PN and small PN prior-shift risks are still decreasing Figure 2: the scaling seems inconsistent - the leftmost subplot in each row doesn\u2019t start at (0,0) in the lower left corner, unlike the other subplots in each row - and it should probably be the same throughout - no need to be showing the negative space. Figure 2: maybe it would be good to plot the different lines in different styles (not just colors) - for BW print and colorblind readers For small PN and small PN prior-shift, the choice of 10% seems arbitrary. At what percentage do the supervised methods start displaying a clear advantage - for the experiments in the paper? When looking into the robustness wrt noise in the training class priors, both are multiplied by the same epsilon coefficient. In a more realistic setting the priors might be perturbed independently, potentially even in a different direction. It would be nice to have a more general experiment here, measuring the robustness of the proposed approach in such a way. 5.2 typo: benchmarksand ; datasetsfor", "rating": "7: Good paper, accept", "reply_text": "And please find our responses below . Q : For small PN and small PN prior-shift , the choice of 10 % seems arbitrary A : Yes , using 10 % data for training is a bit arbitrary , but it follows the tradition in semi-supervised learning where it is common to give 10 % labeled data . Some recent semi-supervised papers give slightly less than 10 % labeled data , for example , 4k labeled data for CIFAR-10 in \u201c temporal ensembling \u201d from ICLR 2017 , \u201c mean teachers \u201d from NIPS 2017 , \u201c smooth neighbors on teacher graphs \u201d from CVPR 2018 , and \u201c compact latent space clustering \u201d from ICML 2018 . Note that section 5.1 is more a proof of concept and illustration of properties , and hence this arbitrary choice should be a safe choice . Q : At what percentage do the supervised methods start displaying a clear advantage ? A : This is a great question but hard to answer . The proposed UU learning is model-independent . However , this doesn \u2019 t mean the best model for PN learning is the best model for UU learning due to the memorization in deep networks ( \u201c a closer look at memorization in deep networks \u201d from ICML 2017 ) . At what percentage PN learning is clearly better than UU learning mainly depends on 4 factors : first of all , the dataset ; second , the values of theta and theta \u2019 , which naturally measure how far UU learning is away from PN learning ; third , the model capacity in terms of memorizing signals and noises with different speeds -- -it is conjectured that skip connections themselves have certain regularization effects against label noises ; finally , the optimization algorithm , especially the learning rate as a function of the epoch number . Q : The curves in Fig.2 suggest that the models should have been trained for longer time A : Thanks for the suggestion ! We are working on the experiments for extending the number of max epochs for training from 200 to 500 ; we will see if standard supervised learning with limited data can be significantly better than now . Q : Try a more realistic setting that the priors are perturbed in a different direction A : We have launched new experiments by scaling the training class priors differently . Experiments on MNIST are finished but experiments on CIFAR-10 are quite slow ; we will update the submission later . The experimental results on MNIST show that the proposed method still performs reasonably well ."}, {"review_id": "B1xWcj0qYm-1", "review_text": "This paper proposes a methodology for training any binary classifier from only unlabeled data. They proved that it is impossible to provide an unbiased estimator if having only a single set of unlabeled data, however, they provide an empirical risk minimization method for only two sets of unlabeled data where all the class priors are given. Some experiments and comparisons with state-of-the-art are provided, together with a study on the robustness of the method. pros: - The paper is clear, and it provides an interesting proven statement as well as a methodology that can be applied directly. Because they show that only two sets with different (and known) priors are sufficient to have an unbiased estimator, the paper has a clear contribution. - The impact of the method is a clear asset, because learning from unlabeled data is applicable to a large number of tasks and is raising attention in the last years. - The large literature on the subject has been well covered in the introduction. - The importance made on the integration of the method to state-of-the-art classifiers, such as the deep learning framework, is also a very positive point. - The effort made in the experiments, by testing the performance as well as the robustness of the method with noisy training class priors is very interesting. remarks: - part 4.1 : the simplification is interesting. However, the authors say that this simplification is easier to implement in many deep learning frameworks. Why is that? - part 4.2 : the consistency part is too condensed and not clear enough. - experiments : what about computation time? - More generally, I wonder if the authors can find examples of typical problems for classification from unlabeled data with known class priors and with at least two sets? minor comments: - part 1: 'but also IN weakly-supervised learning' - part 2. related work : post- precessing --> post-processing - part 2. related work : it is proven THAT the minimal number of U sets... - part 2. related work : In fact, these two are fairly different --> not clear, did you mean 'Actually, ..' ? - part 4.1 : definition 3. Why naming l- and l+ the corrected loss functions? both of them integrate l(z) and l(-z), so it can be confusing. - part 5.1 Analysis of moving ... closer: ... is exactly THE same as before. - part 5.2 : Missing spaces : 'from the webpage of authors.Note ...' and 'USPS datasetsfor the experiment ...' ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for pointing out many typos ; we will fix them accordingly . Please find our detailed responses below . Q : Why is the simplification easier to implement in deep learning frameworks ? A : Sorry for not explaining it . The simplified risk estimator is standard cost-sensitive learning , and thus we can reuse existing codes for cost-sensitive learning , such as importance reweighting by plugging alpha and alpha \u2019 into the codes . However , the original risk estimator needs to be implemented since it is new and can not be reduced to existing objective functions . Q : What about computation time in experiments ? A : Please note that the proposed method just offers a new objective function . After specifying a model , this objective function can be minimized by any optimization algorithm . Specifically , we applied standard SGD for MNIST and Fashion-MNIST and Adam for SVHN and CIFAR-10 . Here the proposed method would not add any more computational burden , so that the computation time simply depends on how many epochs we would like to train the model . Q : Examples of typical problems for classification from two sets of U data with known class priors A : Two sets of U data with different class priors may be collected from different places or time points . For example , considering morbidity rates , they can be potential patient data collected from urban and rural areas ; considering food preferences , they can be potential customer data collected from the Northern and Southern China ; likewise , considering approval rates , they can be unlabeled voter data collected in two years . Note that in the seminal paper on learning from label proportions \u201c N . Quadrianto , A. J. Smola , T. S. Caetano , and Q. V. Le . Estimating labels from label proportions . JMLR , 2009 \u201d , there are many potential applications in areas like e-commerce , politics , spam filtering and improper content detection . The two problem settings are different yet closely related , and thus those can also be our potential applications . Q : Why naming l- and l+ the corrected loss functions ? A : We apologize for the confusion . The notation ( i.e. , l+ and l- ) indicates the U set with larger/smaller class prior is regarded as the corrupted P/N dataset . The name is also from learning with noisy labels . Since our training data are corrupted data , using the original loss l means regarding the corrupted data as clean data and will cause learning to be biased and inconsistent . In order to \u201c correct \u201d this effect , the loss has to be corrected so that the corrected loss is perfectly compatible with the corrupted data ."}, {"review_id": "B1xWcj0qYm-2", "review_text": "Summary: The authors introduce the task of learning from unlabeled data clearly and concisely with sufficient reference to background material. They propose a learning approach, called UU, from two unlabeled datasets with known class priors and prove consistency and convergence rates. Their experiments are insightful to the problem, revealing how the two datasets must be sufficiently separated and how UU learning outperforms state-of-the-art approaches. The writing is clear and the idea is an original refinement of earlier work, justified by its exceeding state-of-the-art approaches. However, the paper needs more experimentation. Further details: While the introduction and set-up is long, it positions the paper well by making it approachable to someone not directly in the subject area and delineating how the approach differs from existing theory. The paper flows smoothly and the arguments build sequentially. A few issues are left unaddressed: - How does the natural extension of UU learning extend beyond the binary setting? - As the authors state, in the wild the class priors may not be known. Their experiment is not completely satisfying because it scales both priors the same. It would be more interesting to experimentally consider them with two different unknown error rates. If this were theoretically addressed (even under the symmetrical single epsilon) this paper would be much better. - In Table 2, using an epsilon greater than 1 seems to always decrease the error with a seeming greater impact when theta and theta' are close. This trend should be explained. In general, the real-world application was the weakest section. Expounding up on it more, running more revealing experiments (potentially on an actual problem in addition to benchmarks), and providing theoretical motivation would greatly improve the paper. - In the introduction is is emphasized how this compares to supervised learning but the explanation is how this compares to unsupervised clustering is much more terse. Another sentence or two explaining why using the resulting cluster identifications for binary labeling is inferior to the \"arbitrary binary classifier\" would help. It's clear in the author's application because one would like to use all data available, including the class priors, for classification. Minor issues: -At the bottom of page 3 the authors state, \" In fact, these two are fairly different, and the differences are reviewed and discussed in Menon et al. (2015) and van Rooyen & Williamson (2018). \" It would be clearer to immediately state the key difference instead of waiting until the end of the paragraph. - In the first sentence of Section 3.1 \"imagining\" is mistyped as \"imaging.\" - What does \"classifier-calibrated\" mean in Section 3.1? - In Section 3.1, \"That is why by choosing a model G, g\u2217 = arg ming\u2208G R(g) is changed as the target to which\" was a bit unclear at first. The phrase \"is changed as the target to which\" was confusing because of the phrasing. Upon second read, the meaning was clear. - In the introduction it was stated \"impossibility is a proof by contradiction, and the possibility is a proof by construction.\" It would be better to (re)state this with each theorem. I was immediately curious about the proof technique after reading the theorem but no elaboration was provided (other than see the appendix). The footnote with the latter theorem is helpful as it alludes to the kind of construction used without being overly detailed. - In section 5.2, in the next to last sentence of the first paragraph there are some issues with missing spaces. - Some more experiment details, e.g. hyperparameter tuning, could be explained in the appendix for reproducibility. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We are trying to expand Table 2 by adding noises of different directions to the training class priors ( experiments on MNIST are finished but experiments on CIFAR-10 are quite slow ; we will update the submission later ) . Please find our detailed responses below ."}, {"review_id": "B1xWcj0qYm-3", "review_text": "This paper studies the weak supervision setting of learning a general binary classifier from two unlabeled (U) datasets with known class balances. The authors establish that this is possible by constructing an unbiased estimator, analyze its convergence theoretically, and then run experiments using modern image classification models. Pros: - This work demonstrates, theoretically and empirically, a simple way to train generic models using only the known class balances of several sets of unlabeled data (having the same conditional distributions p(x|y))---a very interesting configuration of weak supervision, an increasingly popular and important area - The treatment is thorough, proceeding from establishing the minimum number of U datasets, constructing the estimator, analyzing convergence, and implementing thorough experiments Cons: - This is a crowded area (as covered in their related work section). As they cite, (Quadrianto et al., 2009) proposed this setting and considered linear models for k-wise classification. Moreover, the two U datasets with known class balances can equivalently be viewed as two weak / noisy label sources with known accuracies. Thus this work connects to many areas- both in noisy learning, as they cite heavily, but also in methods (in e.g. crowdsourcing and multi-source weak supervision) where several sources label unlabeled datasets with unknown accuracies (which are often estimated in an unsupervised fashion). - The overall clarity of the paper's writing could be improved. For example, the introduction and related work sections take up a large portion of the paper, but are very dense and heavy with jargon that is not internally defined upfront; for example \"risk rewrite\" is introduced in paragraph 2 with no internal definition and then used subsequently throughout the paper (this defn would be simple enough to give: in the context of this paper, \"risk rewrite\" means a linear combination of the class-conditional losses; or more generally, the expected loss w.r.t. distribution over classes...). Also intuition could be briefly given about the theorem proof strategies. - The difference between the two class distributions over the U datasets seems like an important quantity (akin, in e.g. weak supervision / crowd source modeling papers, to quantity of how bounded away from random noise the labelers are). This is treated empirically, but would be stronger to have this show up in the theory somewhere. - Other prior work here has handled k classes with k U sets; could have extended to cover this setting too, since seems natural Overall take: This learning from label proportions setting has been covered before, but this paper presents it in an overall clean and general way, testing it empirically on modern models and datasets, which is an interesting contribution. Other minor points: - The argument for / distinction between using eqns. (3) and (4) seems a bit ad hoc / informal (\"we argue that...\"). This is an important point... - Theorem 1 proof seems fine, but some intuition in the main body would be nice. - What does \"classification calibrated\" mean? - Saying that three U sets are needed, where this includes the test set, seems a bit non-standard? Also I'm confused- isn't a labeled test set used? So what is this third U set for? - The labels l_+ and l_- in Defn. 3 seem to imply that the two U sets are positive vs. negative; but this is not the case, correct\u2026? - Stating both Lemma 5 and Thm 6 seems unnecessary - In Fig. 2, seems like could have trained for longer and perhaps some of the losses would have continued decreasing? In particular, small PN? Also, a table of the final test set accuracies would have been very helpful. - More detail on experimental protocol would be helpful: what kind of hyperparameter tuning was done? repeated runs averaging? It seems odd, for example in Fig. 3, that the green lines are so different in (a) vs. (c), and not in the way that one would expect given the decrease in theta ", "rating": "7: Good paper, accept", "reply_text": "We are still working on the experiments for extending the number of max epochs for training from 200 to 500 and investigating whether standard supervised learning with limited data can be significantly better than now . After that we will go to improve the clarity of the paper following your comments . Please find our detailed responses below ."}], "0": {"review_id": "B1xWcj0qYm-0", "review_text": "The authors propose an unbiased estimator that allows for training models with weak supervision on two unlabeled datasets with known class priors. The theoretical properties of the estimator are discussed and an empirical evaluation shows promising performance. The paper provides a thorough overview of the related work. The experiments compare to the relevant baselines. Minor remarks: The writing seems like it could be improved in multiple places and the main thing that makes some sections of the paper hard to follow is that the concepts often get mentioned and discussed before they are formally defined/introduced. Concepts that are introduced via citations should also be explained even if not in-depth. Figure 2: the curves suggest that the models should have been left to train for a longer time - some of the small PN and small PN prior-shift risks are still decreasing Figure 2: the scaling seems inconsistent - the leftmost subplot in each row doesn\u2019t start at (0,0) in the lower left corner, unlike the other subplots in each row - and it should probably be the same throughout - no need to be showing the negative space. Figure 2: maybe it would be good to plot the different lines in different styles (not just colors) - for BW print and colorblind readers For small PN and small PN prior-shift, the choice of 10% seems arbitrary. At what percentage do the supervised methods start displaying a clear advantage - for the experiments in the paper? When looking into the robustness wrt noise in the training class priors, both are multiplied by the same epsilon coefficient. In a more realistic setting the priors might be perturbed independently, potentially even in a different direction. It would be nice to have a more general experiment here, measuring the robustness of the proposed approach in such a way. 5.2 typo: benchmarksand ; datasetsfor", "rating": "7: Good paper, accept", "reply_text": "And please find our responses below . Q : For small PN and small PN prior-shift , the choice of 10 % seems arbitrary A : Yes , using 10 % data for training is a bit arbitrary , but it follows the tradition in semi-supervised learning where it is common to give 10 % labeled data . Some recent semi-supervised papers give slightly less than 10 % labeled data , for example , 4k labeled data for CIFAR-10 in \u201c temporal ensembling \u201d from ICLR 2017 , \u201c mean teachers \u201d from NIPS 2017 , \u201c smooth neighbors on teacher graphs \u201d from CVPR 2018 , and \u201c compact latent space clustering \u201d from ICML 2018 . Note that section 5.1 is more a proof of concept and illustration of properties , and hence this arbitrary choice should be a safe choice . Q : At what percentage do the supervised methods start displaying a clear advantage ? A : This is a great question but hard to answer . The proposed UU learning is model-independent . However , this doesn \u2019 t mean the best model for PN learning is the best model for UU learning due to the memorization in deep networks ( \u201c a closer look at memorization in deep networks \u201d from ICML 2017 ) . At what percentage PN learning is clearly better than UU learning mainly depends on 4 factors : first of all , the dataset ; second , the values of theta and theta \u2019 , which naturally measure how far UU learning is away from PN learning ; third , the model capacity in terms of memorizing signals and noises with different speeds -- -it is conjectured that skip connections themselves have certain regularization effects against label noises ; finally , the optimization algorithm , especially the learning rate as a function of the epoch number . Q : The curves in Fig.2 suggest that the models should have been trained for longer time A : Thanks for the suggestion ! We are working on the experiments for extending the number of max epochs for training from 200 to 500 ; we will see if standard supervised learning with limited data can be significantly better than now . Q : Try a more realistic setting that the priors are perturbed in a different direction A : We have launched new experiments by scaling the training class priors differently . Experiments on MNIST are finished but experiments on CIFAR-10 are quite slow ; we will update the submission later . The experimental results on MNIST show that the proposed method still performs reasonably well ."}, "1": {"review_id": "B1xWcj0qYm-1", "review_text": "This paper proposes a methodology for training any binary classifier from only unlabeled data. They proved that it is impossible to provide an unbiased estimator if having only a single set of unlabeled data, however, they provide an empirical risk minimization method for only two sets of unlabeled data where all the class priors are given. Some experiments and comparisons with state-of-the-art are provided, together with a study on the robustness of the method. pros: - The paper is clear, and it provides an interesting proven statement as well as a methodology that can be applied directly. Because they show that only two sets with different (and known) priors are sufficient to have an unbiased estimator, the paper has a clear contribution. - The impact of the method is a clear asset, because learning from unlabeled data is applicable to a large number of tasks and is raising attention in the last years. - The large literature on the subject has been well covered in the introduction. - The importance made on the integration of the method to state-of-the-art classifiers, such as the deep learning framework, is also a very positive point. - The effort made in the experiments, by testing the performance as well as the robustness of the method with noisy training class priors is very interesting. remarks: - part 4.1 : the simplification is interesting. However, the authors say that this simplification is easier to implement in many deep learning frameworks. Why is that? - part 4.2 : the consistency part is too condensed and not clear enough. - experiments : what about computation time? - More generally, I wonder if the authors can find examples of typical problems for classification from unlabeled data with known class priors and with at least two sets? minor comments: - part 1: 'but also IN weakly-supervised learning' - part 2. related work : post- precessing --> post-processing - part 2. related work : it is proven THAT the minimal number of U sets... - part 2. related work : In fact, these two are fairly different --> not clear, did you mean 'Actually, ..' ? - part 4.1 : definition 3. Why naming l- and l+ the corrected loss functions? both of them integrate l(z) and l(-z), so it can be confusing. - part 5.1 Analysis of moving ... closer: ... is exactly THE same as before. - part 5.2 : Missing spaces : 'from the webpage of authors.Note ...' and 'USPS datasetsfor the experiment ...' ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for pointing out many typos ; we will fix them accordingly . Please find our detailed responses below . Q : Why is the simplification easier to implement in deep learning frameworks ? A : Sorry for not explaining it . The simplified risk estimator is standard cost-sensitive learning , and thus we can reuse existing codes for cost-sensitive learning , such as importance reweighting by plugging alpha and alpha \u2019 into the codes . However , the original risk estimator needs to be implemented since it is new and can not be reduced to existing objective functions . Q : What about computation time in experiments ? A : Please note that the proposed method just offers a new objective function . After specifying a model , this objective function can be minimized by any optimization algorithm . Specifically , we applied standard SGD for MNIST and Fashion-MNIST and Adam for SVHN and CIFAR-10 . Here the proposed method would not add any more computational burden , so that the computation time simply depends on how many epochs we would like to train the model . Q : Examples of typical problems for classification from two sets of U data with known class priors A : Two sets of U data with different class priors may be collected from different places or time points . For example , considering morbidity rates , they can be potential patient data collected from urban and rural areas ; considering food preferences , they can be potential customer data collected from the Northern and Southern China ; likewise , considering approval rates , they can be unlabeled voter data collected in two years . Note that in the seminal paper on learning from label proportions \u201c N . Quadrianto , A. J. Smola , T. S. Caetano , and Q. V. Le . Estimating labels from label proportions . JMLR , 2009 \u201d , there are many potential applications in areas like e-commerce , politics , spam filtering and improper content detection . The two problem settings are different yet closely related , and thus those can also be our potential applications . Q : Why naming l- and l+ the corrected loss functions ? A : We apologize for the confusion . The notation ( i.e. , l+ and l- ) indicates the U set with larger/smaller class prior is regarded as the corrupted P/N dataset . The name is also from learning with noisy labels . Since our training data are corrupted data , using the original loss l means regarding the corrupted data as clean data and will cause learning to be biased and inconsistent . In order to \u201c correct \u201d this effect , the loss has to be corrected so that the corrected loss is perfectly compatible with the corrupted data ."}, "2": {"review_id": "B1xWcj0qYm-2", "review_text": "Summary: The authors introduce the task of learning from unlabeled data clearly and concisely with sufficient reference to background material. They propose a learning approach, called UU, from two unlabeled datasets with known class priors and prove consistency and convergence rates. Their experiments are insightful to the problem, revealing how the two datasets must be sufficiently separated and how UU learning outperforms state-of-the-art approaches. The writing is clear and the idea is an original refinement of earlier work, justified by its exceeding state-of-the-art approaches. However, the paper needs more experimentation. Further details: While the introduction and set-up is long, it positions the paper well by making it approachable to someone not directly in the subject area and delineating how the approach differs from existing theory. The paper flows smoothly and the arguments build sequentially. A few issues are left unaddressed: - How does the natural extension of UU learning extend beyond the binary setting? - As the authors state, in the wild the class priors may not be known. Their experiment is not completely satisfying because it scales both priors the same. It would be more interesting to experimentally consider them with two different unknown error rates. If this were theoretically addressed (even under the symmetrical single epsilon) this paper would be much better. - In Table 2, using an epsilon greater than 1 seems to always decrease the error with a seeming greater impact when theta and theta' are close. This trend should be explained. In general, the real-world application was the weakest section. Expounding up on it more, running more revealing experiments (potentially on an actual problem in addition to benchmarks), and providing theoretical motivation would greatly improve the paper. - In the introduction is is emphasized how this compares to supervised learning but the explanation is how this compares to unsupervised clustering is much more terse. Another sentence or two explaining why using the resulting cluster identifications for binary labeling is inferior to the \"arbitrary binary classifier\" would help. It's clear in the author's application because one would like to use all data available, including the class priors, for classification. Minor issues: -At the bottom of page 3 the authors state, \" In fact, these two are fairly different, and the differences are reviewed and discussed in Menon et al. (2015) and van Rooyen & Williamson (2018). \" It would be clearer to immediately state the key difference instead of waiting until the end of the paragraph. - In the first sentence of Section 3.1 \"imagining\" is mistyped as \"imaging.\" - What does \"classifier-calibrated\" mean in Section 3.1? - In Section 3.1, \"That is why by choosing a model G, g\u2217 = arg ming\u2208G R(g) is changed as the target to which\" was a bit unclear at first. The phrase \"is changed as the target to which\" was confusing because of the phrasing. Upon second read, the meaning was clear. - In the introduction it was stated \"impossibility is a proof by contradiction, and the possibility is a proof by construction.\" It would be better to (re)state this with each theorem. I was immediately curious about the proof technique after reading the theorem but no elaboration was provided (other than see the appendix). The footnote with the latter theorem is helpful as it alludes to the kind of construction used without being overly detailed. - In section 5.2, in the next to last sentence of the first paragraph there are some issues with missing spaces. - Some more experiment details, e.g. hyperparameter tuning, could be explained in the appendix for reproducibility. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We are trying to expand Table 2 by adding noises of different directions to the training class priors ( experiments on MNIST are finished but experiments on CIFAR-10 are quite slow ; we will update the submission later ) . Please find our detailed responses below ."}, "3": {"review_id": "B1xWcj0qYm-3", "review_text": "This paper studies the weak supervision setting of learning a general binary classifier from two unlabeled (U) datasets with known class balances. The authors establish that this is possible by constructing an unbiased estimator, analyze its convergence theoretically, and then run experiments using modern image classification models. Pros: - This work demonstrates, theoretically and empirically, a simple way to train generic models using only the known class balances of several sets of unlabeled data (having the same conditional distributions p(x|y))---a very interesting configuration of weak supervision, an increasingly popular and important area - The treatment is thorough, proceeding from establishing the minimum number of U datasets, constructing the estimator, analyzing convergence, and implementing thorough experiments Cons: - This is a crowded area (as covered in their related work section). As they cite, (Quadrianto et al., 2009) proposed this setting and considered linear models for k-wise classification. Moreover, the two U datasets with known class balances can equivalently be viewed as two weak / noisy label sources with known accuracies. Thus this work connects to many areas- both in noisy learning, as they cite heavily, but also in methods (in e.g. crowdsourcing and multi-source weak supervision) where several sources label unlabeled datasets with unknown accuracies (which are often estimated in an unsupervised fashion). - The overall clarity of the paper's writing could be improved. For example, the introduction and related work sections take up a large portion of the paper, but are very dense and heavy with jargon that is not internally defined upfront; for example \"risk rewrite\" is introduced in paragraph 2 with no internal definition and then used subsequently throughout the paper (this defn would be simple enough to give: in the context of this paper, \"risk rewrite\" means a linear combination of the class-conditional losses; or more generally, the expected loss w.r.t. distribution over classes...). Also intuition could be briefly given about the theorem proof strategies. - The difference between the two class distributions over the U datasets seems like an important quantity (akin, in e.g. weak supervision / crowd source modeling papers, to quantity of how bounded away from random noise the labelers are). This is treated empirically, but would be stronger to have this show up in the theory somewhere. - Other prior work here has handled k classes with k U sets; could have extended to cover this setting too, since seems natural Overall take: This learning from label proportions setting has been covered before, but this paper presents it in an overall clean and general way, testing it empirically on modern models and datasets, which is an interesting contribution. Other minor points: - The argument for / distinction between using eqns. (3) and (4) seems a bit ad hoc / informal (\"we argue that...\"). This is an important point... - Theorem 1 proof seems fine, but some intuition in the main body would be nice. - What does \"classification calibrated\" mean? - Saying that three U sets are needed, where this includes the test set, seems a bit non-standard? Also I'm confused- isn't a labeled test set used? So what is this third U set for? - The labels l_+ and l_- in Defn. 3 seem to imply that the two U sets are positive vs. negative; but this is not the case, correct\u2026? - Stating both Lemma 5 and Thm 6 seems unnecessary - In Fig. 2, seems like could have trained for longer and perhaps some of the losses would have continued decreasing? In particular, small PN? Also, a table of the final test set accuracies would have been very helpful. - More detail on experimental protocol would be helpful: what kind of hyperparameter tuning was done? repeated runs averaging? It seems odd, for example in Fig. 3, that the green lines are so different in (a) vs. (c), and not in the way that one would expect given the decrease in theta ", "rating": "7: Good paper, accept", "reply_text": "We are still working on the experiments for extending the number of max epochs for training from 200 to 500 and investigating whether standard supervised learning with limited data can be significantly better than now . After that we will go to improve the clarity of the paper following your comments . Please find our detailed responses below ."}}