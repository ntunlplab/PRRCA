{"year": "2020", "forum": "Hkx6hANtwH", "title": "LambdaNet: Probabilistic Type Inference using Graph Neural Networks", "decision": "Accept (Poster)", "meta_review": "This paper proposes an approach to type inference in dynamically typed languages using graph neural networks. The reviewers (and the area chair) love this novel and useful application of GNNs to a practical problem, the presentation, the results. Clear accept.", "reviews": [{"review_id": "Hkx6hANtwH-0", "review_text": "This paper proposed to use Graph Neural Networks (GNN) to do type inference for dynamically typed languages. The key technique is to construct a type dependency graph and infer the type on top of it. The type dependency graph contains edges specifying hard constraints derived from the static analysis, as well as soft relationships specified by humans. Experiments on type predictions for TypeScript have shown better performance than the previous methods, with or without user specified types. Overall this paper tackles a nice application of GNN, which is the type prediction problem that utilizes structural information of the code. Also the proposed type dependency graph seems interesting to me. Also the pointer mechanism used for predicting user specified types is a good strategy that advances the previous method. However, I have several concerns below: About formulation: 1) I\u2019m not sure if the predicted types for individual variable would be very helpful in general. Since the work only cares about individual predictions while no global consistency is enforced, it is somewhat limited. For example, in order to (partially) compile a program, does it require all the variable types to be correct in that part? If so, then the predicted types here might not be that helpful. I\u2019m not sure about this, so any discussion would be appreciated. About type dependency graph: 1) Comparing to previous work (e.g, Allamanis et.al, ICLR 18), it seems the construction of the task specific graph is the major contribution, where the novelty is a bit limited. 2) The construction of the dependency graph is heuristic. For example, why the three contextual constraints are good? Would there be other good ones? Also why only include such limited set of logical constraints. For example, would expression like (x + y) induce some interesting relationships? Because such hand-crafted graph is lossy (unlike raw source code), all the questions here lead to the concern of such design choices. 3) The usage of graph is somewhat straightforward to me. For example, although the hard-constraints are there, there\u2019s no such constraints reflected in the prediction. Adding the constraints on the predictions would be more interesting. About experiments: 1) I think one ablation study I\u2019m most interested in is to simply run GNN on the AST (or simply use Allamanis et.al\u2019s method). This is to verify and support the usage of proposed type dependency graph. 2) As the authors claimed in Introduction, \u2018plenty of training data is available\u2019. However in experiment only 300 projects are involved. Also it seems that these are not fully annotated, and the \u2018forward type inference functionality from TypeScript\u2019 is required to obtain labels. It would be good to explain such discrepancy. 3) Continue with 2), as the experiment results shown in Table 2, TS compiler performs poorly. So how would it be possible to train with poor annotations, while generalize much better? Some explanations would be helpful here. 4) I think only predicting non-polymorphic types is another limitation. Would it be possible to predict structured types? like nested list, or function types with arguments? ", "rating": "6: Weak Accept", "reply_text": "Thank you very much for your comments and questions about our paper ! Due to length limit , we break our responses into two comments . Please see the parts about formulation and type dependency graph below : ======= \u201c I \u2019 m not sure if the predicted types for individual variable would be very helpful in general . Since the work only cares about individual predictions while no global consistency is enforced , it is somewhat limited. \u201d Response : We believe that predicted types would still be helpful because the user only needs to focus on parts of the codebase that do not type check ( combined with techniques for localizing type checking errors ) . That said , we are definitely interested in using structured prediction techniques ( e.g. , structured SVM ) to enforce that the predicted types pass the type checker at learning and inference time . However , this aspect is orthogonal to the techniques presented in our submission and brings several new research challenges . ======= \u201c For example , in order to ( partially ) compile a program , does it require all the variable types to be correct in that part ? If so , then the predicted types here might not be that helpful . I \u2019 m not sure about this , so any discussion would be appreciated. \u201d Response : If the program does not type check , the TypeScript compiler will report a warning but still emit the Javascript program anyway . Thus , mistakes in type error prediction neither affect the compilation process nor cause any additional run-time errors . ======= `` Comparing to previous work ( e.g , Allamanis et.al , ICLR 18 ) , it seems the construction of the task specific graph is the major contribution , where the novelty is a bit limited . '' Response : Our architecture is only related to Allamanis et al.insofar as it is a graph neural network over source code . Most components of our model introduce substantial new elements . First , the type embedding scheme for user-defined types is entirely new , which we believe would also be useful for other tasks involving types ( e.g.program language models , code repair , static analysis ... ) . Second , the lack of static type information makes the problem fundamentally different and requires much more sophisticated attention mechanism ( in the form of usage hyperedges ) to help propagating information between syntactically distant but semantically relevant program elements . ======= \u201c The construction of the dependency graph is heuristic . For example , why the three contextual constraints are good ? Would there be other good ones ? Also why only include such limited set of logical constraints . For example , would expression like ( x + y ) induce some interesting relationships ? Because such hand-crafted graph is lossy ( unlike raw source code ) , all the questions here lead to the concern of such design choices . \u201d Response : The set of logical constraints contains all the hard constraints that a rule-based type inference algorithm would use . But as our ablation studies show , adding extra statistical information via contextual edges can be very helpful . Our choices of contextual edges aim to preserve the statistical hints that are most relevant to make correct type prediction while exclude type-irrelevant aspects of code that are more likely leading to overfitting . In the case of the expression \u201c x+y \u201d , it would be translated into the Call relation \u201c $ \\textrm { Call } ( N_z , N_+ , N_x , N_y ) $ \u201d asserting that $ N_z=N_+ ( N_x , N_y ) $ , where $ N_x $ , $ N_y $ , and $ N_+ $ are the nodes corresponding to \u201c x \u201d , \u201c y \u201d , and the library function \u201c + \u201d . And $ N_z $ is the node generated for the entire expression . Since $ N_+ $ is a library node whose embedding vector is fine-tuned during the training process , information can propagate from it into other three nodes via the Call edge and hint the GNN that they are all likely to be of type \u201c number \u201d ."}, {"review_id": "Hkx6hANtwH-1", "review_text": "This paper presents a GNN-based method for predicting type annotations in JavaScript and TypeScript code. By constructing a \"type dependency graph\" that encodes the relationships among variables and appropriately modifying GNNs to the hypergraph setting, the authors show that the good performance improvements can be made. Overall, this paper is well-written, the experiments are quite convincing and the methods are reasonable and interesting. I believe that there are a few things that need to be clarified within the evaluation, but I would argue that this paper should be accepted. * It is unclear to me what is the scope of the type dependency graph construction. Is it a whole file? Is it a single function/class? A whole project? * The DeepTyper paper seems to suggest that it predicts type annotations one function at a time. Does the comparison (in 5.1) use the same granularity as LambdaNet? If for example, LambdaNet looks at whole files, then the comparison is not exact. Could you please clarify? Performing the comparison on equal-sized samples would make sense. * If my reading of DeepTyper is correct, it processes all identifiers as a single unit. In contrast, this work (correctly, in my opinion), breaks the identifiers into \"word tokens\". This may be an important difference between the two methods. To test this, an ablation where LambdaNet does *not* split the identifiers, would provide a better comparison among the sequential representation of DeepTyper and the type constraint graph of LambdaNet. * Some comparison with JSNice is missing (Raychev et al. 2015). The DeepTyper paper suggests that the two approaches are somewhat complementary. It would be useful to know how LambdaNet compares to JSNice too. * There is some literature that suggests that code duplication exists in automatically scraped corpora and that it hurts the evaluation of machine learning models [a,b]. At the very least, the authors should report the percent of duplicates (if any) in their corpus. Another option would be to _not_ evaluate predictions in any duplicated file. ## Secondary question: * I do not understand why a separate edge is needed for Subtype() and Assign(). Isn't Assign (a,b) == Subype(a,b)? * The authors correctly exclude the `any` annotations produced by the TypeScript compiler. Do they also exclude any other annotations? For example, functions that do not return a value (i.e. their return value is `void`) would also need to be excluded. What about `object`? * I would encourage the authors to make the dataset (source code and extracted graphs), and the LambdaNet code public upon acceptance. ## Minor * Please capitalize GitHub, TypeScript, etc throughout the paper? * Sec 4: \"we first to compute\" -> \"we first compute\" [a] Lopes, Cristina V., et al. \"D\u00e9j\u00e0Vu: a map of code duplicates on GitHub.\" Proceedings of the ACM on Programming Languages 1.OOPSLA (2017): 84. [b] Allamanis, Miltiadis. \"The Adverse Effects of Code Duplication in Machine Learning Models of Code.\" arXiv preprint arXiv:1812.06469 (2018).", "rating": "8: Accept", "reply_text": "Thank you very much for your comments and questions about our paper ! Please see our responses below : ======= \u201c It is unclear to me what is the scope of the type dependency graph construction . Is it a whole file ? Is it a single function/class ? A whole project ? \u201d Response : Our approach takes an entire Typescript project as its input ( see section 2.1 paragraph 1 ) . We can make this clearer in the evaluation sections . ======= \u201c The DeepTyper paper seems to suggest that it predicts type annotations one function at a time . Does the comparison ( in 5.1 ) use the same granularity as LambdaNet ? If for example , LambdaNet looks at whole files , then the comparison is not exact . Could you please clarify ? Performing the comparison on equal-sized samples would make sense. \u201d Response : The original DeepTyper architecture takes a Typescript source file as its input and performs inter-file batching to reduce training and inference time , so we did the same in our evaluation . To compare on equal-sized inputs , we could theoretically concatenate all source files into one large file and run DeepTyper on top of it . However , this is extremely unlikely to make any difference due to the limited memory of the RNN . ======= \u201c If my reading of DeepTyper is correct , it processes all identifiers as a single unit . In contrast , this work ( correctly , in my opinion ) , breaks the identifiers into `` word tokens '' . This may be an important difference between the two methods . To test this , an ablation where LambdaNet does * not * split the identifiers , would provide a better comparison among the sequential representation of DeepTyper and the type constraint graph of LambdaNet. \u201d Response : In our comparison , we made sure both DeepTyper and LambdaNet are using the same naming feature that splits words into tokens , through a modification to DeepTyper \u2019 s architecture . We did this because we believe word tokens are clearly better , and our results demonstrate LambdaNet \u2019 s advantage over DeepTyper even when DeepTyper is using better naming features . ======= \u201c Some comparison with JSNice is missing ( Raychev et al.2015 ) .The DeepTyper paper suggests that the two approaches are somewhat complementary . It would be useful to know how LambdaNet compares to JSNice too. \u201d Response : We found it very hard to do a comprehensive comparison with JSNice because their tool predicts over a different and very restricted set of types ( mostly primitive Javascript types like number , string , etc ) , so we did not compare with it . DeepTyper \u2019 s original experiment only manually compares with JSNice on 30 functions . We could perform a similar manual comparison against JSNice on randomly selected functions and include this comparison in our revision . ======= \u201c There is some literature that suggests that code duplication exists in automatically scraped corpora and that it hurts the evaluation of machine learning models [ a , b ] . At the very least , the authors should report the percent of duplicates ( if any ) in their corpus . Another option would be to _not_ evaluate predictions in any duplicated file. \u201d Response : We investigated this question by running jscpd ( a popular code duplication detection tool , https : //github.com/kucherenko/jscpd ) on our entire data set and found that only 2.7 % code is duplicated . Furthermore , most of these duplicates are intra-project . Thus , we believe that code duplication is not a severe problem in our dataset , and we will include more details about this result in any future revision . ======= \u201c I do not understand why a separate edge is needed for Subtype ( ) and Assign ( ) . Is n't Assign ( a , b ) == Subype ( a , b ) ? \u201d Response : It is true that Assign is a special case of Subtype from a typing perspective . We differentiate them because these edges appear in different contexts in the graph and , thus , having uncoupled parameters for these two edge types is likely to be beneficial . Moreover , assignments constitute a substantial portion of subtyping constraints , so we have enough data to train an additional edge type . ======= \u201c The authors correctly exclude the ` any ` annotations produced by the TypeScript compiler . Do they also exclude any other annotations ? For example , functions that do not return a value ( i.e.their return value is ` void ` ) would also need to be excluded . What about ` object ` ? \u201d Response : As we mentioned in the paper , we only evaluate on type annotations that the programmers have manually added . This means that types like \u201c void \u201d would only be evaluated if the programmer feels it is necessary to add them ( which is quite rare but possible ) . ======= \u201c I would encourage the authors to make the dataset ( source code and extracted graphs ) , and the LambdaNet code public upon acceptance. \u201d Response : Yes , we will ."}, {"review_id": "Hkx6hANtwH-2", "review_text": "= Summary A method to predict likely type of program variables in TypeScript is presented. It consists of a translation of a program's type constraints and defined objects into a (hyper)graph, and a specialised neural message passing architecture to learn from the generated graphs. Experiments show that the method substantially outperforms sound typing in the TypeScript compiler, as well as a recent method based on deep neural networks. = Strong/Weak Points + The graph representation of the problem is novel, and draws both on core ideas from Hindley-Milner typing (in the subtyping/assignment graph bits) as well as neural ideas (in name similiarity) + The neural message passing architecture is adapted to the problem, handling features not present in the standard GNN literature (hyperedges, ...) + Experiments compare with relevant baselines and consider interesting ablations, studying the effect of the GNN extensions in detail. - The hyperparameter selection regime (and the experiments used to find them) is not described = Recommendation This is an application-driven paper with nice practical results. The fact that standard neural architectures are extended and adapted to the task, and the way domain knowledge is used to design the graph representation makes this interesting even to people outside the task-specific audience, and hence I strongly recommend acceptance. = Minor Comments - page 2: \"network's type to be class\" -> \"to be a class\" - Evaluation Datasets: Did you take duplication in the crawled datasets into account? (Lopes et al. 2017 (D\u00e9j\u00e0Vu: a map of code duplicates on GitHub) suggests that this is particularly problematic for JavaScript/TypeScript) ", "rating": "8: Accept", "reply_text": "Thank you very much for your review ! Please see our responses below regarding your comments : \u201c Evaluation Datasets : Did you take duplication in the crawled datasets into account ? ( Lopes et al.2017 ( D\u00e9j\u00e0Vu : a map of code duplicates on GitHub ) suggests that this is particularly problematic for JavaScript/TypeScript ) \u201d Response : We investigated this question by running jscpd ( a popular code duplication detection tool , https : //github.com/kucherenko/jscpd ) on our entire data set and found that only 2.7 % code is duplicated . Furthermore , most of these duplicates are intra-project . Thus , we believe that code duplication is not a severe problem in our dataset , and we will include more details about this result in any future revision . ======== \u201c The hyperparameter selection regime ( and the experiments used to find them ) is not described \u201d Response : We selected hyperparameters in a standard way by tuning on a validation set as we were developing our model . We \u2019 ll include more details about hyperparameters and hyperparameter selection in any future revision ."}], "0": {"review_id": "Hkx6hANtwH-0", "review_text": "This paper proposed to use Graph Neural Networks (GNN) to do type inference for dynamically typed languages. The key technique is to construct a type dependency graph and infer the type on top of it. The type dependency graph contains edges specifying hard constraints derived from the static analysis, as well as soft relationships specified by humans. Experiments on type predictions for TypeScript have shown better performance than the previous methods, with or without user specified types. Overall this paper tackles a nice application of GNN, which is the type prediction problem that utilizes structural information of the code. Also the proposed type dependency graph seems interesting to me. Also the pointer mechanism used for predicting user specified types is a good strategy that advances the previous method. However, I have several concerns below: About formulation: 1) I\u2019m not sure if the predicted types for individual variable would be very helpful in general. Since the work only cares about individual predictions while no global consistency is enforced, it is somewhat limited. For example, in order to (partially) compile a program, does it require all the variable types to be correct in that part? If so, then the predicted types here might not be that helpful. I\u2019m not sure about this, so any discussion would be appreciated. About type dependency graph: 1) Comparing to previous work (e.g, Allamanis et.al, ICLR 18), it seems the construction of the task specific graph is the major contribution, where the novelty is a bit limited. 2) The construction of the dependency graph is heuristic. For example, why the three contextual constraints are good? Would there be other good ones? Also why only include such limited set of logical constraints. For example, would expression like (x + y) induce some interesting relationships? Because such hand-crafted graph is lossy (unlike raw source code), all the questions here lead to the concern of such design choices. 3) The usage of graph is somewhat straightforward to me. For example, although the hard-constraints are there, there\u2019s no such constraints reflected in the prediction. Adding the constraints on the predictions would be more interesting. About experiments: 1) I think one ablation study I\u2019m most interested in is to simply run GNN on the AST (or simply use Allamanis et.al\u2019s method). This is to verify and support the usage of proposed type dependency graph. 2) As the authors claimed in Introduction, \u2018plenty of training data is available\u2019. However in experiment only 300 projects are involved. Also it seems that these are not fully annotated, and the \u2018forward type inference functionality from TypeScript\u2019 is required to obtain labels. It would be good to explain such discrepancy. 3) Continue with 2), as the experiment results shown in Table 2, TS compiler performs poorly. So how would it be possible to train with poor annotations, while generalize much better? Some explanations would be helpful here. 4) I think only predicting non-polymorphic types is another limitation. Would it be possible to predict structured types? like nested list, or function types with arguments? ", "rating": "6: Weak Accept", "reply_text": "Thank you very much for your comments and questions about our paper ! Due to length limit , we break our responses into two comments . Please see the parts about formulation and type dependency graph below : ======= \u201c I \u2019 m not sure if the predicted types for individual variable would be very helpful in general . Since the work only cares about individual predictions while no global consistency is enforced , it is somewhat limited. \u201d Response : We believe that predicted types would still be helpful because the user only needs to focus on parts of the codebase that do not type check ( combined with techniques for localizing type checking errors ) . That said , we are definitely interested in using structured prediction techniques ( e.g. , structured SVM ) to enforce that the predicted types pass the type checker at learning and inference time . However , this aspect is orthogonal to the techniques presented in our submission and brings several new research challenges . ======= \u201c For example , in order to ( partially ) compile a program , does it require all the variable types to be correct in that part ? If so , then the predicted types here might not be that helpful . I \u2019 m not sure about this , so any discussion would be appreciated. \u201d Response : If the program does not type check , the TypeScript compiler will report a warning but still emit the Javascript program anyway . Thus , mistakes in type error prediction neither affect the compilation process nor cause any additional run-time errors . ======= `` Comparing to previous work ( e.g , Allamanis et.al , ICLR 18 ) , it seems the construction of the task specific graph is the major contribution , where the novelty is a bit limited . '' Response : Our architecture is only related to Allamanis et al.insofar as it is a graph neural network over source code . Most components of our model introduce substantial new elements . First , the type embedding scheme for user-defined types is entirely new , which we believe would also be useful for other tasks involving types ( e.g.program language models , code repair , static analysis ... ) . Second , the lack of static type information makes the problem fundamentally different and requires much more sophisticated attention mechanism ( in the form of usage hyperedges ) to help propagating information between syntactically distant but semantically relevant program elements . ======= \u201c The construction of the dependency graph is heuristic . For example , why the three contextual constraints are good ? Would there be other good ones ? Also why only include such limited set of logical constraints . For example , would expression like ( x + y ) induce some interesting relationships ? Because such hand-crafted graph is lossy ( unlike raw source code ) , all the questions here lead to the concern of such design choices . \u201d Response : The set of logical constraints contains all the hard constraints that a rule-based type inference algorithm would use . But as our ablation studies show , adding extra statistical information via contextual edges can be very helpful . Our choices of contextual edges aim to preserve the statistical hints that are most relevant to make correct type prediction while exclude type-irrelevant aspects of code that are more likely leading to overfitting . In the case of the expression \u201c x+y \u201d , it would be translated into the Call relation \u201c $ \\textrm { Call } ( N_z , N_+ , N_x , N_y ) $ \u201d asserting that $ N_z=N_+ ( N_x , N_y ) $ , where $ N_x $ , $ N_y $ , and $ N_+ $ are the nodes corresponding to \u201c x \u201d , \u201c y \u201d , and the library function \u201c + \u201d . And $ N_z $ is the node generated for the entire expression . Since $ N_+ $ is a library node whose embedding vector is fine-tuned during the training process , information can propagate from it into other three nodes via the Call edge and hint the GNN that they are all likely to be of type \u201c number \u201d ."}, "1": {"review_id": "Hkx6hANtwH-1", "review_text": "This paper presents a GNN-based method for predicting type annotations in JavaScript and TypeScript code. By constructing a \"type dependency graph\" that encodes the relationships among variables and appropriately modifying GNNs to the hypergraph setting, the authors show that the good performance improvements can be made. Overall, this paper is well-written, the experiments are quite convincing and the methods are reasonable and interesting. I believe that there are a few things that need to be clarified within the evaluation, but I would argue that this paper should be accepted. * It is unclear to me what is the scope of the type dependency graph construction. Is it a whole file? Is it a single function/class? A whole project? * The DeepTyper paper seems to suggest that it predicts type annotations one function at a time. Does the comparison (in 5.1) use the same granularity as LambdaNet? If for example, LambdaNet looks at whole files, then the comparison is not exact. Could you please clarify? Performing the comparison on equal-sized samples would make sense. * If my reading of DeepTyper is correct, it processes all identifiers as a single unit. In contrast, this work (correctly, in my opinion), breaks the identifiers into \"word tokens\". This may be an important difference between the two methods. To test this, an ablation where LambdaNet does *not* split the identifiers, would provide a better comparison among the sequential representation of DeepTyper and the type constraint graph of LambdaNet. * Some comparison with JSNice is missing (Raychev et al. 2015). The DeepTyper paper suggests that the two approaches are somewhat complementary. It would be useful to know how LambdaNet compares to JSNice too. * There is some literature that suggests that code duplication exists in automatically scraped corpora and that it hurts the evaluation of machine learning models [a,b]. At the very least, the authors should report the percent of duplicates (if any) in their corpus. Another option would be to _not_ evaluate predictions in any duplicated file. ## Secondary question: * I do not understand why a separate edge is needed for Subtype() and Assign(). Isn't Assign (a,b) == Subype(a,b)? * The authors correctly exclude the `any` annotations produced by the TypeScript compiler. Do they also exclude any other annotations? For example, functions that do not return a value (i.e. their return value is `void`) would also need to be excluded. What about `object`? * I would encourage the authors to make the dataset (source code and extracted graphs), and the LambdaNet code public upon acceptance. ## Minor * Please capitalize GitHub, TypeScript, etc throughout the paper? * Sec 4: \"we first to compute\" -> \"we first compute\" [a] Lopes, Cristina V., et al. \"D\u00e9j\u00e0Vu: a map of code duplicates on GitHub.\" Proceedings of the ACM on Programming Languages 1.OOPSLA (2017): 84. [b] Allamanis, Miltiadis. \"The Adverse Effects of Code Duplication in Machine Learning Models of Code.\" arXiv preprint arXiv:1812.06469 (2018).", "rating": "8: Accept", "reply_text": "Thank you very much for your comments and questions about our paper ! Please see our responses below : ======= \u201c It is unclear to me what is the scope of the type dependency graph construction . Is it a whole file ? Is it a single function/class ? A whole project ? \u201d Response : Our approach takes an entire Typescript project as its input ( see section 2.1 paragraph 1 ) . We can make this clearer in the evaluation sections . ======= \u201c The DeepTyper paper seems to suggest that it predicts type annotations one function at a time . Does the comparison ( in 5.1 ) use the same granularity as LambdaNet ? If for example , LambdaNet looks at whole files , then the comparison is not exact . Could you please clarify ? Performing the comparison on equal-sized samples would make sense. \u201d Response : The original DeepTyper architecture takes a Typescript source file as its input and performs inter-file batching to reduce training and inference time , so we did the same in our evaluation . To compare on equal-sized inputs , we could theoretically concatenate all source files into one large file and run DeepTyper on top of it . However , this is extremely unlikely to make any difference due to the limited memory of the RNN . ======= \u201c If my reading of DeepTyper is correct , it processes all identifiers as a single unit . In contrast , this work ( correctly , in my opinion ) , breaks the identifiers into `` word tokens '' . This may be an important difference between the two methods . To test this , an ablation where LambdaNet does * not * split the identifiers , would provide a better comparison among the sequential representation of DeepTyper and the type constraint graph of LambdaNet. \u201d Response : In our comparison , we made sure both DeepTyper and LambdaNet are using the same naming feature that splits words into tokens , through a modification to DeepTyper \u2019 s architecture . We did this because we believe word tokens are clearly better , and our results demonstrate LambdaNet \u2019 s advantage over DeepTyper even when DeepTyper is using better naming features . ======= \u201c Some comparison with JSNice is missing ( Raychev et al.2015 ) .The DeepTyper paper suggests that the two approaches are somewhat complementary . It would be useful to know how LambdaNet compares to JSNice too. \u201d Response : We found it very hard to do a comprehensive comparison with JSNice because their tool predicts over a different and very restricted set of types ( mostly primitive Javascript types like number , string , etc ) , so we did not compare with it . DeepTyper \u2019 s original experiment only manually compares with JSNice on 30 functions . We could perform a similar manual comparison against JSNice on randomly selected functions and include this comparison in our revision . ======= \u201c There is some literature that suggests that code duplication exists in automatically scraped corpora and that it hurts the evaluation of machine learning models [ a , b ] . At the very least , the authors should report the percent of duplicates ( if any ) in their corpus . Another option would be to _not_ evaluate predictions in any duplicated file. \u201d Response : We investigated this question by running jscpd ( a popular code duplication detection tool , https : //github.com/kucherenko/jscpd ) on our entire data set and found that only 2.7 % code is duplicated . Furthermore , most of these duplicates are intra-project . Thus , we believe that code duplication is not a severe problem in our dataset , and we will include more details about this result in any future revision . ======= \u201c I do not understand why a separate edge is needed for Subtype ( ) and Assign ( ) . Is n't Assign ( a , b ) == Subype ( a , b ) ? \u201d Response : It is true that Assign is a special case of Subtype from a typing perspective . We differentiate them because these edges appear in different contexts in the graph and , thus , having uncoupled parameters for these two edge types is likely to be beneficial . Moreover , assignments constitute a substantial portion of subtyping constraints , so we have enough data to train an additional edge type . ======= \u201c The authors correctly exclude the ` any ` annotations produced by the TypeScript compiler . Do they also exclude any other annotations ? For example , functions that do not return a value ( i.e.their return value is ` void ` ) would also need to be excluded . What about ` object ` ? \u201d Response : As we mentioned in the paper , we only evaluate on type annotations that the programmers have manually added . This means that types like \u201c void \u201d would only be evaluated if the programmer feels it is necessary to add them ( which is quite rare but possible ) . ======= \u201c I would encourage the authors to make the dataset ( source code and extracted graphs ) , and the LambdaNet code public upon acceptance. \u201d Response : Yes , we will ."}, "2": {"review_id": "Hkx6hANtwH-2", "review_text": "= Summary A method to predict likely type of program variables in TypeScript is presented. It consists of a translation of a program's type constraints and defined objects into a (hyper)graph, and a specialised neural message passing architecture to learn from the generated graphs. Experiments show that the method substantially outperforms sound typing in the TypeScript compiler, as well as a recent method based on deep neural networks. = Strong/Weak Points + The graph representation of the problem is novel, and draws both on core ideas from Hindley-Milner typing (in the subtyping/assignment graph bits) as well as neural ideas (in name similiarity) + The neural message passing architecture is adapted to the problem, handling features not present in the standard GNN literature (hyperedges, ...) + Experiments compare with relevant baselines and consider interesting ablations, studying the effect of the GNN extensions in detail. - The hyperparameter selection regime (and the experiments used to find them) is not described = Recommendation This is an application-driven paper with nice practical results. The fact that standard neural architectures are extended and adapted to the task, and the way domain knowledge is used to design the graph representation makes this interesting even to people outside the task-specific audience, and hence I strongly recommend acceptance. = Minor Comments - page 2: \"network's type to be class\" -> \"to be a class\" - Evaluation Datasets: Did you take duplication in the crawled datasets into account? (Lopes et al. 2017 (D\u00e9j\u00e0Vu: a map of code duplicates on GitHub) suggests that this is particularly problematic for JavaScript/TypeScript) ", "rating": "8: Accept", "reply_text": "Thank you very much for your review ! Please see our responses below regarding your comments : \u201c Evaluation Datasets : Did you take duplication in the crawled datasets into account ? ( Lopes et al.2017 ( D\u00e9j\u00e0Vu : a map of code duplicates on GitHub ) suggests that this is particularly problematic for JavaScript/TypeScript ) \u201d Response : We investigated this question by running jscpd ( a popular code duplication detection tool , https : //github.com/kucherenko/jscpd ) on our entire data set and found that only 2.7 % code is duplicated . Furthermore , most of these duplicates are intra-project . Thus , we believe that code duplication is not a severe problem in our dataset , and we will include more details about this result in any future revision . ======== \u201c The hyperparameter selection regime ( and the experiments used to find them ) is not described \u201d Response : We selected hyperparameters in a standard way by tuning on a validation set as we were developing our model . We \u2019 ll include more details about hyperparameters and hyperparameter selection in any future revision ."}}