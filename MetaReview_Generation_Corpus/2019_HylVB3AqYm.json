{"year": "2019", "forum": "HylVB3AqYm", "title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "decision": "Accept (Poster)", "meta_review": "This paper integrates a bunch of existing approaches for neural architecture search, including OneShot/DARTS, BinaryConnect, REINFORCE, etc. Although the novelty of the paper may be limited, empirical performance seems impressive. The source code is not available. I think this is a borderline paper but maybe good enough for acceptance.\n", "reviews": [{"review_id": "HylVB3AqYm-0", "review_text": "The algorithm described in this paper is part of the one-shot family of architecture search algorithms. In practice this means training an over-parameterized architecture, of which the architectures being searched for are sub-graphs. Once this bigger network is trained it is pruned into the desired sub-graph. The algorithm is similar to DARTS in that it it has weights that determine how important the various possible nodes are, but the interpretation here is stochastic, in that the weight indicates the probability of the component being active. Two methods to train those weights are being suggested, using REINFORCE and using BinaryConnect, both having different trade offs. - (minor) *cumbersome* network seems the wrong term, maybe over-parameterized network? - (minor) I do not think that the size of the search space a very meaningful metric Pros: - Good exposition - Interesting and fairly elegant idea - Good experimental results Cons - tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture. I think this is the main shortcoming, although shared by many NAS papers - No source code available Some typos: - Fo example, when proxy strategy -> Fo*r* example - normal training in following ways. -> in *the* following ways - we can then derive optimized compact architecture.", "rating": "7: Good paper, accept", "reply_text": "We sincerely thank you for the detailed comments on our paper . We have revised the paper and fixed the typos accordingly . > > > Response to \u201c limited amount of tested settings \u201d : As our proxy-less NAS has reduced the cost to the same level of normal training ( 100x more efficient on ImageNet ) , it is of great interest for us to apply proxy-less NAS to more settings and datasets . However , for this work , considering the resource constraints and time limits , we have strong reasons to believe that our experiment settings are sufficient : a ) Our experiments are conducted on two most representative benchmarks ( CIFAR and ImageNet ) . It is in line with previous NAS papers and also makes it possible to compare our method with previous NAS methods . We also experimented with 3 different hardware platforms and observed consistent latency improvement over previous work . b ) Moreover , on the challenging ImageNet classification task , we have conducted architecture search experiments under three different settings ( GPU , CPU and Mobile ) while previous NAS papers mainly transfer learned architectures from CIFAR-10 to ImageNet without conducting architecture search experiments on ImageNet [ 1 , 2 ] . > > > Response to \u201c no source code available \u201d : Reviewer 2 also has similar requests , based on the concern on our strong empirical results . Our pre-trained models and the evaluation code are provided in the following anonymous link : https : //goo.gl/QU3GhA . Besides , we have also uploaded the video visualizing the architecture search process : https : //goo.gl/VAzGJs . We plan to open source our project upon publication . > > > Response to \u201c the size of the search space is not a very meaningful metric \u201d : This might be a misunderstanding . We do not intend to use the size of our search space as a metric for comparison ; instead , it is an important reason why our accuracy is much better than previous NAS methods . Previous NAS methods forced different blocks to share the same structure and only explored a limited architecture space ( e.g.10^18 in [ 2 ] and 10^10 in [ 3 ] ) . Our method , breaking the constraints , allows all of the blocks to be specified and has much larger search space ( i.e.10^547 ) . [ 1 ] Zoph B , Vasudevan V , Shlens J , Le QV . Learning transferable architectures for scalable image recognition . CVPR 2018 . [ 2 ] Liu H , Simonyan K , Yang Y. Darts : Differentiable architecture search . arXiv preprint arXiv:1806.09055 . 2018 . [ 3 ] Bender G , Kindermans PJ , Zoph B , Vasudevan V , Le Q . Understanding and simplifying one-shot architecture search . ICML 2018 ."}, {"review_id": "HylVB3AqYm-1", "review_text": " This paper addresses the problem of architecture search, and specifically seeks to do this without having to train on \"proxy\" tasks where the problem is simplified through more limited optimization, architectural complexity, or dataset size. The paper puts together a set of existing complementary methods towards this end, specifically 1) Training \"cumbersome\" networks as in One Shot and DARTS, 2) Path binarization to address memory requirements (optimized using ideas in BinaryConnect), and 3) optimizing a non-differentiable architecture using REINFORCE. The end result is that this method is able to find efficient architectures that achieve state of art performance with fewer parameters, can be optimized for non-differentiable objectives such as latency, and can do so with smaller amounts of GPU memory and computation. Strengths + The paper is in general well-written and provides a clear description of the methods. + Different choices made are well-justified in terms of the challenge they seek to address (e.g. non-differentiable objectives, etc.) + The results achieve state of art while being able to trade off other objectives such as latency + There are some interesting findings such as the need for specialized blocks rather than repeating blocks, comparison of architectures for CPUs vs. GPUs, etc. Weaknesses - In the end, the method is really a combination of existing methods (One Shot/DART, BinaryConnect, use of RL/REINFORCE, etc.). One novel aspect seems to be factorizing the choice out of N candidates by making it a binary selection. In general, it would be good for the paper to make clear which aspects were already done by other approaches (or if it's a modification what exactly was modified/added in comparison) and highlight the novel elements. - The comparison with One Shot and DARTS seems strange, as there are limitations place on those methods (e.g. cell structure settings) that the authors state they chose \"to save time\". While that consideration has some validity, the authors should explicitly state why they think these differences don't unfairly bias the experiments towards the proposed approach. - It's not clear that the REINFORCE aspect is adding much; it achieves slightly higher parameters when compared against Proxyless-G, and while I understand the motivation to optimize a non-differentiable function in this case the latency example (on ImageNet) is never compared to Proxyless-G. It could be that optimized the normal differentiable objective achieves similar latency with the smaller number of parameters. Please show results for Proxyless-G in Table 4. - There were several typos throughout the paper (\"great impact BY automatically designing\", \"Fo example\", \"is build upon\", etc.) In summary, the paper presents work on an interesting topic. The set of methods seem to be largely pulled from work that already exists, but is able to achieve good results in a manner that uses less GPU memory and compute, while supporting non-differentiable objectives. Some of the methodological issues mentioned above should be addressed though in order to strengthen the argument that all parts of the the method (especially REINFORCE) are necessary. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely thank you for your comprehensive comments and constructive advices . > > > Response to \u201c combination of existing methods \u201d : Thanks for your kind advice on organizing the paper to make our contributions more clear . Here , we would like to emphasize our contributions : a ) Our proxy-less NAS is the first NAS algorithm that directly learns architectures on the large-scale dataset ( e.g.ImageNet ) without any proxy . We also solved an important problem improving the computation efficiency of NAS as we reduced the computational cost ( GPU hours and GPU memory ) of NAS to the same level as normal training . Moreover , the GPU memory requirement of our method keeps at O ( 1 ) complexity rather than grows linearly with the number of candidate operations O ( N ) [ 3 , 4 ] . Therefore , our method can easily support a large candidate set while DARTS and One-Shot can not . b ) Our proxy-less NAS is the first NAS algorithm that breaks the convention of repeating blocks in neural architecture design . From Alexnet and VGG to ResNet and MobileNet , manually designed CNNs used to repeat blocks within the same stage . Previous NAS works keep the tradition as otherwise the searching cost will be unaffordable . Our work breaks the constraints , and we found this is actually a stereotype that needs to be corrected . The new interesting design patterns , found by our method , can provide new insights for efficient neural architecture design . For example , people used to stack multiple 3x3 convs to replace a single large kernel conv , as this uses fewer parameters while keeping a similar receptive field . But we found this pattern may not be proper for designing efficient ( low latency ) networks : Two 3x3 depthwise separable convs actually run slower than a single 5x5 depthwise separable conv . Our GPU model , shown in Figure 4 , incorporates large kernel convs and aggressively pools at early stages to shrink network depth . Then the model chooses computation-expensive operations at low-resolution stages . It also tends to choose computation-expensive operations in the first block within each stage where the feature map is downsampled . As a consequence , our GPU model can outperform previous SOTA efficient architectures in accuracy performances ( e.g.3.1 % higher top-1 than MobileNetV2 ) , while running faster than them ( e.g.1.2x faster than MobileNetV2 ) . Such patterns can not be found by previous NAS , as they optimize on proxy task and force blocks to share structures . c ) Our method builds upon methods from two communities ( one-shot architecture search from NAS community and Pruning/BinaryConnect from model compression community ) . It is the first time to incorporate ideas from the model compression community to the NAS community and we also provide a new path-level pruning perspective for one-shot architecture search . Moreover , we provide a unified framework for both gradient-based updates and REINFORCE-based updates . d ) Our proxy-less NAS achieved very strong empirical results on two most representative benchmarks ( i.e.CIFAR and ImageNet ) . On CIFAR-10 , our optimized model reached 2.08 % error rate with only 5.7M parameters , outperforming previous state-of-the-art architecture ( AmeobaNet-B with 34.9M parameters ) . On ImageNet , we searched specialized neural network architectures for three different platforms ( GPU , CPU and mobile phone ) . With latency constraints , our optimized models also achieved state-of-the-art results ( 3.1 % higher top-1 accuracy while being 1.2x faster on GPU and 2.6 % higher top-1 accuracy with similar latency on mobile phone , compared to MobileNetV2 ) . Besides , we directly optimize the latency , rather than an inaccurate proxy ( i.e.FLOPs ) .It \u2019 s an important concept that low FLOPs doesn \u2019 t translate to low latency . All our speedup numbers are reported with real measured latency . We believe both our efficient search methodology and the resulting efficient models have big industry impact ."}, {"review_id": "HylVB3AqYm-2", "review_text": "It seems the authors propose an efficient method to search platform-aware network architecture aiming at high recognition accuracy and low latency. Their results on CIFAR-10 and ImageNet are surprisingly good. But it is still hard to believe that the author can achieve 2.08% error rate with only 5.7M parameter on CIFAR10 and 74.5% top-1 accuracy on ImageNet with less GPU hours/memories than prior arts. Given my concerns above, the author must release their code and detail pipelines since NAS papers are difficult to be reproduced. There is a small typo in reference part: Jing-Dong Dong's work should be DPP-Net instead of PPP-Net (https://eccv2018.org/openaccess/content_ECCV_2018/papers/Jin-Dong_Dong_DPP-Net_Device-aware_Progressive_ECCV_2018_paper.pdf) and I think this paper \"Neural Architecture Optimization\" shoud be cited.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely thanks for the detailed feedback . Our pre-trained models and the evaluation code are provided in the following anonymous link for verifying our results : https : //goo.gl/QU3GhA . We have also made a video to visualize the architecture search process : https : //goo.gl/VAzGJs . We would like to release the entire codebase upon publication . > > > Response to \u201c performances are too good to be true \u201d : We consider the comment as a compliment rather than a drawback . There are several reasons for our good results : a ) Our proxy-less NAS * directly * learns on the * target * task while previous NAS methods * indirectly * learn on * proxy * tasks . For example , on CIFAR-10 , DARTS [ 1 ] conducted architecture search experiments with 8 blocks due to their high memory consumption and then transferred the learned block structure to a much larger network with 20 blocks . This indirect optimization scheme would lead to suboptimal results while our proxy-less NAS does not suffer from this problem . b ) We broke the convention in neural architecture design by * not * repeating the same building block structure . Our method explores a much larger architecture space compared to previous NAS methods ( 10^547 vs 10^18 ) . Furthermore , our method has much larger block diversity and is able to learn preferences at different positions in the architecture . For example , our optimized neural network architectures for GPU , CPU and mobile phone prefer to choose more computation-expensive operations ( e.g.7x7 MBConv6 ) for the last few stages where the resolution of feature map is low . They also prefer to choose more computation-expensive operations in the first block within each stage where the feature map is downsampled . We consider the ability to learn such patterns which are absent in previous NAS papers also helps to improve our results . > > > Response to \u201c DPP-Net and NAO citations \u201d : Apologize for the typo and missing a relevant paper in our reference part . We have fixed typo and added a reference to \u201c Neural Architecture Optimization \u201d . Thanks for pointing out our mistakes . [ 1 ] Liu H , Simonyan K , Yang Y. Darts : Differentiable architecture search . arXiv preprint arXiv:1806.09055 . 2018 ."}], "0": {"review_id": "HylVB3AqYm-0", "review_text": "The algorithm described in this paper is part of the one-shot family of architecture search algorithms. In practice this means training an over-parameterized architecture, of which the architectures being searched for are sub-graphs. Once this bigger network is trained it is pruned into the desired sub-graph. The algorithm is similar to DARTS in that it it has weights that determine how important the various possible nodes are, but the interpretation here is stochastic, in that the weight indicates the probability of the component being active. Two methods to train those weights are being suggested, using REINFORCE and using BinaryConnect, both having different trade offs. - (minor) *cumbersome* network seems the wrong term, maybe over-parameterized network? - (minor) I do not think that the size of the search space a very meaningful metric Pros: - Good exposition - Interesting and fairly elegant idea - Good experimental results Cons - tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture. I think this is the main shortcoming, although shared by many NAS papers - No source code available Some typos: - Fo example, when proxy strategy -> Fo*r* example - normal training in following ways. -> in *the* following ways - we can then derive optimized compact architecture.", "rating": "7: Good paper, accept", "reply_text": "We sincerely thank you for the detailed comments on our paper . We have revised the paper and fixed the typos accordingly . > > > Response to \u201c limited amount of tested settings \u201d : As our proxy-less NAS has reduced the cost to the same level of normal training ( 100x more efficient on ImageNet ) , it is of great interest for us to apply proxy-less NAS to more settings and datasets . However , for this work , considering the resource constraints and time limits , we have strong reasons to believe that our experiment settings are sufficient : a ) Our experiments are conducted on two most representative benchmarks ( CIFAR and ImageNet ) . It is in line with previous NAS papers and also makes it possible to compare our method with previous NAS methods . We also experimented with 3 different hardware platforms and observed consistent latency improvement over previous work . b ) Moreover , on the challenging ImageNet classification task , we have conducted architecture search experiments under three different settings ( GPU , CPU and Mobile ) while previous NAS papers mainly transfer learned architectures from CIFAR-10 to ImageNet without conducting architecture search experiments on ImageNet [ 1 , 2 ] . > > > Response to \u201c no source code available \u201d : Reviewer 2 also has similar requests , based on the concern on our strong empirical results . Our pre-trained models and the evaluation code are provided in the following anonymous link : https : //goo.gl/QU3GhA . Besides , we have also uploaded the video visualizing the architecture search process : https : //goo.gl/VAzGJs . We plan to open source our project upon publication . > > > Response to \u201c the size of the search space is not a very meaningful metric \u201d : This might be a misunderstanding . We do not intend to use the size of our search space as a metric for comparison ; instead , it is an important reason why our accuracy is much better than previous NAS methods . Previous NAS methods forced different blocks to share the same structure and only explored a limited architecture space ( e.g.10^18 in [ 2 ] and 10^10 in [ 3 ] ) . Our method , breaking the constraints , allows all of the blocks to be specified and has much larger search space ( i.e.10^547 ) . [ 1 ] Zoph B , Vasudevan V , Shlens J , Le QV . Learning transferable architectures for scalable image recognition . CVPR 2018 . [ 2 ] Liu H , Simonyan K , Yang Y. Darts : Differentiable architecture search . arXiv preprint arXiv:1806.09055 . 2018 . [ 3 ] Bender G , Kindermans PJ , Zoph B , Vasudevan V , Le Q . Understanding and simplifying one-shot architecture search . ICML 2018 ."}, "1": {"review_id": "HylVB3AqYm-1", "review_text": " This paper addresses the problem of architecture search, and specifically seeks to do this without having to train on \"proxy\" tasks where the problem is simplified through more limited optimization, architectural complexity, or dataset size. The paper puts together a set of existing complementary methods towards this end, specifically 1) Training \"cumbersome\" networks as in One Shot and DARTS, 2) Path binarization to address memory requirements (optimized using ideas in BinaryConnect), and 3) optimizing a non-differentiable architecture using REINFORCE. The end result is that this method is able to find efficient architectures that achieve state of art performance with fewer parameters, can be optimized for non-differentiable objectives such as latency, and can do so with smaller amounts of GPU memory and computation. Strengths + The paper is in general well-written and provides a clear description of the methods. + Different choices made are well-justified in terms of the challenge they seek to address (e.g. non-differentiable objectives, etc.) + The results achieve state of art while being able to trade off other objectives such as latency + There are some interesting findings such as the need for specialized blocks rather than repeating blocks, comparison of architectures for CPUs vs. GPUs, etc. Weaknesses - In the end, the method is really a combination of existing methods (One Shot/DART, BinaryConnect, use of RL/REINFORCE, etc.). One novel aspect seems to be factorizing the choice out of N candidates by making it a binary selection. In general, it would be good for the paper to make clear which aspects were already done by other approaches (or if it's a modification what exactly was modified/added in comparison) and highlight the novel elements. - The comparison with One Shot and DARTS seems strange, as there are limitations place on those methods (e.g. cell structure settings) that the authors state they chose \"to save time\". While that consideration has some validity, the authors should explicitly state why they think these differences don't unfairly bias the experiments towards the proposed approach. - It's not clear that the REINFORCE aspect is adding much; it achieves slightly higher parameters when compared against Proxyless-G, and while I understand the motivation to optimize a non-differentiable function in this case the latency example (on ImageNet) is never compared to Proxyless-G. It could be that optimized the normal differentiable objective achieves similar latency with the smaller number of parameters. Please show results for Proxyless-G in Table 4. - There were several typos throughout the paper (\"great impact BY automatically designing\", \"Fo example\", \"is build upon\", etc.) In summary, the paper presents work on an interesting topic. The set of methods seem to be largely pulled from work that already exists, but is able to achieve good results in a manner that uses less GPU memory and compute, while supporting non-differentiable objectives. Some of the methodological issues mentioned above should be addressed though in order to strengthen the argument that all parts of the the method (especially REINFORCE) are necessary. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely thank you for your comprehensive comments and constructive advices . > > > Response to \u201c combination of existing methods \u201d : Thanks for your kind advice on organizing the paper to make our contributions more clear . Here , we would like to emphasize our contributions : a ) Our proxy-less NAS is the first NAS algorithm that directly learns architectures on the large-scale dataset ( e.g.ImageNet ) without any proxy . We also solved an important problem improving the computation efficiency of NAS as we reduced the computational cost ( GPU hours and GPU memory ) of NAS to the same level as normal training . Moreover , the GPU memory requirement of our method keeps at O ( 1 ) complexity rather than grows linearly with the number of candidate operations O ( N ) [ 3 , 4 ] . Therefore , our method can easily support a large candidate set while DARTS and One-Shot can not . b ) Our proxy-less NAS is the first NAS algorithm that breaks the convention of repeating blocks in neural architecture design . From Alexnet and VGG to ResNet and MobileNet , manually designed CNNs used to repeat blocks within the same stage . Previous NAS works keep the tradition as otherwise the searching cost will be unaffordable . Our work breaks the constraints , and we found this is actually a stereotype that needs to be corrected . The new interesting design patterns , found by our method , can provide new insights for efficient neural architecture design . For example , people used to stack multiple 3x3 convs to replace a single large kernel conv , as this uses fewer parameters while keeping a similar receptive field . But we found this pattern may not be proper for designing efficient ( low latency ) networks : Two 3x3 depthwise separable convs actually run slower than a single 5x5 depthwise separable conv . Our GPU model , shown in Figure 4 , incorporates large kernel convs and aggressively pools at early stages to shrink network depth . Then the model chooses computation-expensive operations at low-resolution stages . It also tends to choose computation-expensive operations in the first block within each stage where the feature map is downsampled . As a consequence , our GPU model can outperform previous SOTA efficient architectures in accuracy performances ( e.g.3.1 % higher top-1 than MobileNetV2 ) , while running faster than them ( e.g.1.2x faster than MobileNetV2 ) . Such patterns can not be found by previous NAS , as they optimize on proxy task and force blocks to share structures . c ) Our method builds upon methods from two communities ( one-shot architecture search from NAS community and Pruning/BinaryConnect from model compression community ) . It is the first time to incorporate ideas from the model compression community to the NAS community and we also provide a new path-level pruning perspective for one-shot architecture search . Moreover , we provide a unified framework for both gradient-based updates and REINFORCE-based updates . d ) Our proxy-less NAS achieved very strong empirical results on two most representative benchmarks ( i.e.CIFAR and ImageNet ) . On CIFAR-10 , our optimized model reached 2.08 % error rate with only 5.7M parameters , outperforming previous state-of-the-art architecture ( AmeobaNet-B with 34.9M parameters ) . On ImageNet , we searched specialized neural network architectures for three different platforms ( GPU , CPU and mobile phone ) . With latency constraints , our optimized models also achieved state-of-the-art results ( 3.1 % higher top-1 accuracy while being 1.2x faster on GPU and 2.6 % higher top-1 accuracy with similar latency on mobile phone , compared to MobileNetV2 ) . Besides , we directly optimize the latency , rather than an inaccurate proxy ( i.e.FLOPs ) .It \u2019 s an important concept that low FLOPs doesn \u2019 t translate to low latency . All our speedup numbers are reported with real measured latency . We believe both our efficient search methodology and the resulting efficient models have big industry impact ."}, "2": {"review_id": "HylVB3AqYm-2", "review_text": "It seems the authors propose an efficient method to search platform-aware network architecture aiming at high recognition accuracy and low latency. Their results on CIFAR-10 and ImageNet are surprisingly good. But it is still hard to believe that the author can achieve 2.08% error rate with only 5.7M parameter on CIFAR10 and 74.5% top-1 accuracy on ImageNet with less GPU hours/memories than prior arts. Given my concerns above, the author must release their code and detail pipelines since NAS papers are difficult to be reproduced. There is a small typo in reference part: Jing-Dong Dong's work should be DPP-Net instead of PPP-Net (https://eccv2018.org/openaccess/content_ECCV_2018/papers/Jin-Dong_Dong_DPP-Net_Device-aware_Progressive_ECCV_2018_paper.pdf) and I think this paper \"Neural Architecture Optimization\" shoud be cited.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely thanks for the detailed feedback . Our pre-trained models and the evaluation code are provided in the following anonymous link for verifying our results : https : //goo.gl/QU3GhA . We have also made a video to visualize the architecture search process : https : //goo.gl/VAzGJs . We would like to release the entire codebase upon publication . > > > Response to \u201c performances are too good to be true \u201d : We consider the comment as a compliment rather than a drawback . There are several reasons for our good results : a ) Our proxy-less NAS * directly * learns on the * target * task while previous NAS methods * indirectly * learn on * proxy * tasks . For example , on CIFAR-10 , DARTS [ 1 ] conducted architecture search experiments with 8 blocks due to their high memory consumption and then transferred the learned block structure to a much larger network with 20 blocks . This indirect optimization scheme would lead to suboptimal results while our proxy-less NAS does not suffer from this problem . b ) We broke the convention in neural architecture design by * not * repeating the same building block structure . Our method explores a much larger architecture space compared to previous NAS methods ( 10^547 vs 10^18 ) . Furthermore , our method has much larger block diversity and is able to learn preferences at different positions in the architecture . For example , our optimized neural network architectures for GPU , CPU and mobile phone prefer to choose more computation-expensive operations ( e.g.7x7 MBConv6 ) for the last few stages where the resolution of feature map is low . They also prefer to choose more computation-expensive operations in the first block within each stage where the feature map is downsampled . We consider the ability to learn such patterns which are absent in previous NAS papers also helps to improve our results . > > > Response to \u201c DPP-Net and NAO citations \u201d : Apologize for the typo and missing a relevant paper in our reference part . We have fixed typo and added a reference to \u201c Neural Architecture Optimization \u201d . Thanks for pointing out our mistakes . [ 1 ] Liu H , Simonyan K , Yang Y. Darts : Differentiable architecture search . arXiv preprint arXiv:1806.09055 . 2018 ."}}