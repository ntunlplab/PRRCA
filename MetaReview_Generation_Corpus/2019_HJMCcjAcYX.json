{"year": "2019", "forum": "HJMCcjAcYX", "title": "Learning Representations of Sets through Optimized Permutations", "decision": "Accept (Poster)", "meta_review": "The paper proposes an architecture to learn over sets, by proposing a way\nto have permutations differentiable end-to-end, hence learnable by gradient\ndescent. Reviewers pointed out to the computational limitation (quadratic in\nthe size of the set just to consider pairwise interactions, and cubic overall).\nOne reviewer (with low confidence) though the approach was not novel but\ndidn't appreciate the integration of learning-to-permute with a differentiable\nsetting, so I decided to down-weight their score. Overall, I found the paper\nborderline but would propose to accept it if possible.", "reviews": [{"review_id": "HJMCcjAcYX-0", "review_text": "Update: From the perspective of a \"broader ML\" audience, I cannot recommend acceptance of this paper. The paper does not provide even a clear and concrete problem statement due to which it is difficult for me to appreciate the results. This is the only paper out of all ICLR2019 papers that I have reviewed / read which has such an issue. Of course for the conference, the area chair / program chairs can choose how to weigh the acceptance decisions between interest to the broader ML audience and the audience in the area of the paper. ---------------------------------------------------------------------------------------------------------------------------------- This paper addresses the problem that often features are obtained as a set, whereas certain orders of these features are known to allow for easier learning. With this motivation the goal of this paper is to learn a permutation of the features. This paper makes the following three main contributions: 1. The idea of using pairwise comparison costs instead of position-based costs 2. The methodological crux of how to go from the pairwise comparison costs to the permutation (that is, solving Eqn. (2) using Eqn. (1) ) 3. An empirical evaluation I like the idea and the empirical evaluations are promising. However, I have a major concern about the second contribution on the method. There is a massive amount of literature on this very problem and a number of algorithms are proposed in the literature. This literature takes various forms including rank aggregation and most popularly the (weighted) minimum feedback arc set problem. The submitted paper is oblivious to this enormous literature both in the related work section as well as the empirical evaluations. I have listed below a few papers pertaining to various versions of the problem (this list is by no means exhaustive). With this issue, I cannot give a positive evaluation of this submitted paper since it is not clear whether the paper is just re-solving a solved problem. That said, I am happy to reconsider if the related work and the empirical evaluations are augmented with comparisons to the past literature on the methodological crux of the submitted paper (e.g., why off-the-shelf use of previously proposed algorithms may or may not suffice here.) Unweighted feedback arc set: A fast and effective heuristic for the feedback arc set problem, Eades et al. Efficient Computation of Feedback Arc Set at Web-Scale, Simpson et al. How to rank with few errors, Kenyon-Mathieu et al. Aggregating Inconsistent Information: Ranking and Clustering, Ailon et al. Hardness results: The Minimum Feedback Arc Set Problem is NP-hard for Tournaments, Charbit et al. Weighted feedback arc set: A branch-and-bound algorithm to solve the linear ordering problem for weighted tournaments, Charon et al. Exact and heuristic algorithms for the weighted feedback arc set problem: A special case of the skew\u2010symmetric quadratic assignment problem, Flood Approximating Minimum Feedback Sets and Multicuts in Directed Graphs, Even et al. Random inputs: Noisy sorting without resampling, Braverman et al. Stochastically transitive models for pairwise comparisons: Statistical and computational issues, Shah et al. On estimation in tournaments and graphs under monotonicity constraints, Chatterjee et al. Survey (slightly dated): An updated survey on the linear ordering problem for weighted or unweighted tournaments, Charon et al. Convex relaxation of permutation matrices: On convex relaxation of graph isomorphism, Afalo et al. Facets of the linear ordering polytope, Grotschel ", "rating": "3: Clear rejection", "reply_text": "Thank you for the review . The key difference you missed that separates our work from the papers you cite is that our method is differentiable . In our problem set-up , we are not given the pairwise costs ; they have to be learned . In order for these costs to be learnable with gradient descent , we have to be able to differentiate through the algorithm . This is possible with our method , but not possible with traditional literature on feedback arc sets . Experimental comparisons to the papers you list are thus not meaningful , since the costs that these algorithm operate on have to be learned first . Does this sufficiently clarify for you why our methodology is not reinventing the wheel ? We already cite the particular convex relaxation of permutations that we use ( Fogel et al. , 2013 ; Adams & Zemel , 2011 ) and the NP-hardness of the problem ( Pardalos & Vavasis , 1991 ) . Though we mention this matter of differentiability several times throughout the paper , we will add a sentence in the Related Works section to make this distinction with the work on feedback arc sets even clearer ."}, {"review_id": "HJMCcjAcYX-1", "review_text": "The authors introduce a method to learn to permute sets end-to-end. They define the cost of a permutation as the sum of pairwise costs induced by the permutation, where the pairwise costs are learned. Permutations are made differentiable by relaxing them to doubly stochastic matrices which are approximated with the Sinkhorn operator. In the forward pass of the algorithm, a good permutation (ie one with low cost) is obtained with a few steps of gradient descent (the forward pass itself contains an optimization procedure). This permutation is then either used directly as the output of the algorithm or is used to permute the original inputs and feed the permuted sequence to another module (such as an RNN or a CNN). The method can easily be adapted to other structures such as lattices by considering row-wise and column-wise pairwise relations. The proposed method is benchmarked on 4 tasks: 1. Sorting numbers, where they obtain very strong generalization results. 2. Re-assembling image mosaics, on which they obtain encouraging results. 3. Image classification through image mosaics. 4. Visual Question Answering where the permuted inputs are fed to an LSTM whose final latent state is fed back into the baseline model (a bilinear attention network). Doing so improves over feeding the inputs to an LSTM without learning the order.for which the output is the permutation itself and classification from image mosaics and visual question answering which require to learn an implicit permutation. The method is most similar to Learning Latent Permutations with Gumbel-Sinkhorn Networks (Mena et al) but considers pairwise relations when producing the permutation. This can have important advantages (such as taking local relations into account, as shown by the strong sorting results) but also drawbacks (inability to differentiate inputs with similar content), but in any case this represents a good step towards exploring with different cost functions. The method can be quite unpractical (cubic time complexity in set cardinality, optimization in forward pass, having to preprocess the set into a sequence for another module can be resource expensive). Experimental results on toy tasks (tasks 1, 2 and 3) are encouraging. The approach improves over a relatively strong baseline (task 4) although it isn't clear that it would still hold true when controlling for number of parameters and compute. I have a few comments about the presentation (for which I would be willing to change my score to a 6): - When possible, please use the numbers reported by Mena et al and consider reporting error (instead of accuracy) as they do to ease comparison. The results that you report using their method are quite worse than what they report, so I think it would be fair to include both your reimplementation and the initial results in the table. - It would be interested to have some insights on what function f is learned (for the sorting task and re-assembling image mosaics for example). - Clarity would be improved with figures representing which neural networks are used at what part of the process. ########################################### Updated review: The authors have greatly improved presentation and have addressed concerns about the increase in parameters and computation time. I have changed my score to a 6.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . In general , we intend our model to be useful for relatively small sets of complex objects like in VQA , rather than large sets of simple objects where the cubic time complexity indeed becomes a big problem . About your concern whether the results hold up when controlling for parameters and computation : the time increase by using our model compared to the baseline is about 30 % ( 4400 seconds per epoch instead of 3400 seconds ) . This is a similar increase in computation time as the change from BAN-8 to BAN-12 ( increasing number of attention glimpses to 12 ) in their paper ( Kim et al. , 2018 ) , Table 1 . Their difference is within one standard deviation ( 0.04 % increase , stdev of 0.11 % ) , so simply increasing the BAN model size alone is clearly running into diminishing returns already . Our model does not add a significant number of parameters compared to changing BAN-8 to BAN-12 . Our model also results in qualitatively different improvements : general improvements in VQA typically result in a roughly even improvements in all the categories , whereas our model improves on number questions significantly more than other categories . We will make this clearer in the revision . - We will swap the table of errors ( Appendix E Table 4 ) with the table of accuracies ( Table 1 ) in the main body . We initially decided on reporting accuracy in the main body because MSE may not be directly comparable : we do not know whether their pixel values were scaled to have unit variance ( our choice ) or to be between 0 -- 1 , the latter of which would make their errors seem lower than ours for the same reconstruction ( in personal communication , they said that they did not normalise the data , but our results suggest that they did , since our reproduced MSEs on MNIST closely match theirs ) . As Reviewer 1 seemed to have a slight misunderstanding with accuracy too , we agree that comparing MSE in the main body is likely clearer . Contrary to what you say , our reproductions on MNIST are fairly close to what they report ( our reproduced MSEs are roughly even , accuracies are slightly worse ) and only on ImageNet are our reproduced results of their model worse . To be clear , the relevant row in their results to compare our accuracy to is `` Prop . any wrong '' ( reconstruction is correct only if all tiles are correct , this is what we use ) , not `` Prop . wrong '' ( reconstructions of the tiles being individually correct ) . As per your suggestion , we will include their results in our tables to make the appropriate comparison easier . - We will perform some analysis of the learned f and include it as an appendix . For number sorting , it is enough for it to learn f ( x_i , x_j ) = x_i , so F ( x_i , x_j ) = x_i - x_j , which is a sensible comparison function . In initial analysis it appears to learn a scaled and shifted version of that . We are currently looking into what it learns for image mosaics . - We will add some figures to make the network architectures for the different tasks clearer ."}, {"review_id": "HJMCcjAcYX-2", "review_text": "This paper proposed an interesting idea of learning representations of sets by permutation optimizations. Through learning a permutation of the elements of a set, the proposed algorithm can learn a permutation-invariant representation of that set. To deal with the underlying difficult combinatorial optimization problem, the authors proposed to relax the optimization constraints and instead optimize over the set of doubly-stochastic matrices with reparameterization using the Sinkhorn operator. The cost function of this optimization is related to a pairwise ordering cost, which compares the order for each pair of the elements. The idea of using pairwise comparison information to learn permutations is interesting. The total cost function utilizes the comparison information and optimization over this cost function can lead to a permutation-invariant representation of the set. The idea of using the Sinkhorn operator to reparameterize the doubly-stochastic matrices makes the optimization objective differentiable. Also, the experiment results compared with some baseline algorithms showed the success of the proposed methods in many different tasks. My major concern of the proposed method is on whether this method can be applied to large sets. Since the algorithm compares all pairs of elements in the set, we need O(N^2) comparisons for a set of size N and hence the proposed method might be slow if N is large. Is it possible to improve the efficiency for large sets? Questions and Suggestions: 1. Since the authors wants to approximately solve the objective function in Equation (2), it is better if we can see a proof showing why this optimization problem is difficult. 2. For the experiment in Section 4.2, it seems that all methods (including the proposed methods and the baseline methods) are not performing well if the images are split to at least 4 * 4 equal-size tiles. I understand that currently the authors applied their method to the case of grid permutation by simply adding all cost functions of all rows and columns. Is it possible to extend the proposed method to the grid case in another way so that the results under this setting is better? 3. It will be better if the authors can propose some more insights (probably with some theoretical analysis) when can the PO-U method performs better and when can the PO-LA method performs better. 4. The authors mentioned that, the proposed method can get good permutations even for only T=4 steps. What if we continue running the algorithm? Will the permutation converges stably? 5. The authors proposed to update the permutation matrix parameters in an alternative way (Equation (7)) and mentioned that this update works significantly better in the experiments. It will be great if the authors can have a theoretical analysis on why this is true since P and \\tilde P can be quite different from each other for an arbitrary \\tilde P matrix. Minor comment: I think there is a typo in Equation (5). The entry \\tilde P_{pq} is related to not only the entry P_{pq}, but also the other entries of the matrix P. Hence, I think Equation (5) should be modified as a matrix multiplication.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . Scalability to large sets is indeed an issue with our model . As we discuss in section 5 , its intended use is more with small sets of complex elements ( e.g.objects in images ) rather than large sets of simple elements ( e.g.point clouds ) , with our VQA experiment being an instance of the former . For large sets there are some optimisations that can be made . For example , if there are many pairs of elements where we can guess that they will not affect each other 's local ordering much ( e.g.points in point clouds that are far away from each other ) , we can make the comparisons sparse by only comparing points that are reasonably close to each other , which just involves a pre-processing step . We also mention in the Discussion section that a divide-and-conquer strategy ( perhaps merge-sort-like ) could work , reducing the comparisons down to O ( n log n ) , though it might assume transitivity of the cost function . 1.Eqn ( 2 ) , without any relaxations , is a standard Quadratic Assignment Problem ( see Appendix C to turn ( 2 ) into a more standard formulation ) . These problems are known to be NP-hard , and even epsilon-approximability is NP-hard [ https : //dl.acm.org/citation.cfm ? id=321975 ] . For the quadratic programming formulation ( when constraints on P are relaxed ) , we already cite ( Pardalos & Vavasis , 1991 ) in our paper for NP-hardness . 2.There is a bit of a misunderstanding in interpreting the 4x4 and 5x5 results because we did not explain the metric sufficiently . Especially for MNIST , many of the tiles end up completely blank or very similar when the image is split into many small tiles . This means that there are many equally-valid solutions where blank tiles are assigned to different positions . However , the accuracy metric does not account for these multiple possible solutions properly ( only one of these is considered to be the ground truth ) , so all the blank tiles must happen to be assigned to the blank spots in the ground-truth order for the accuracy to be 1 . We will improve this by swapping accuracy ( Table 1 ) with mean squared error ( Appendix E Table 4 ) for this experiment . Until we have updated the paper , you can see in Table 4 that while there is still a worsening of error as we increase the number of tiles , the change is less abrupt and the models do decently on MNIST for higher number of tiles . As to ideas for improving results in the setting of many tiles for image mosaics : in the grid case , it is possible to not only have row-wise and column-wise permutations , but also over diagonals , which should help in constraining what permutations are considered good by the model . It is also possible to modify the row-wise comparison to not act on each row individually , but consider assignments to other rows simultaneously ( vice versa for columns ) . In some initial work we had some success on image mosaics with an alternative cost function for ( 1 ) that only considers correct ordering between direct neighbours , but it struggled even with small instances of the sorting task so we did not pursue this further . It may be possible to fix this issue , perhaps by doing a convex combination of the cost function in the paper and this modified cost function , which should give better results . 3.As theoretical analysis of these complex learned systems is rather difficult ( it is a bit like asking for theoretical analysis of when , for example in the context of sequence modeling , CNNs are better than RNNs ) , we instead point to the results in section 4 : PO-LA is suitable for when LinAssign is decent , which is the case when absolute positioning of set elements is useful ( since LinAssign only directly models absolute positioning ) . The PO part is then used to refine this . PO-U is suitable for when relative position between set elements becomes more important , resulting in LinAssign only learning things that are not useful and thus being a detractor . If the input set can have variable size ( e.g.in VQA ) , it is not possible to use PO-LA , only PO-U . 4.We took a net that was trained to sort 10 numbers with T=4 and evaluated it with T=100 000 . The accuracy degraded from 100 % accuracy to ~94 % accuracy . We also took a net trained with T=20 and evaluated it with T=100 000 , which resulted in 100 % accuracy this time . It appears that if the permutations seen in training are sufficiently converged already -- which is the case for T=20 -- it converges stably at evaluation time when run for signficantly longer too . 5.We will add this as an appendix to the paper . By looking at the gradients before and after the Sinkhorn , it becomes clear that gradients vanish when trying to differentiate through the Sinkhorn the further away from the uniform initialisation one gets . That means that it is difficult to learn Ps that are close to proper permutation matrices ( all 0 and 1 entries ) if we do not use the alternative gradient . Typo in equation ( 5 ) : Good catch ! We will fix this in the revision of our paper ."}], "0": {"review_id": "HJMCcjAcYX-0", "review_text": "Update: From the perspective of a \"broader ML\" audience, I cannot recommend acceptance of this paper. The paper does not provide even a clear and concrete problem statement due to which it is difficult for me to appreciate the results. This is the only paper out of all ICLR2019 papers that I have reviewed / read which has such an issue. Of course for the conference, the area chair / program chairs can choose how to weigh the acceptance decisions between interest to the broader ML audience and the audience in the area of the paper. ---------------------------------------------------------------------------------------------------------------------------------- This paper addresses the problem that often features are obtained as a set, whereas certain orders of these features are known to allow for easier learning. With this motivation the goal of this paper is to learn a permutation of the features. This paper makes the following three main contributions: 1. The idea of using pairwise comparison costs instead of position-based costs 2. The methodological crux of how to go from the pairwise comparison costs to the permutation (that is, solving Eqn. (2) using Eqn. (1) ) 3. An empirical evaluation I like the idea and the empirical evaluations are promising. However, I have a major concern about the second contribution on the method. There is a massive amount of literature on this very problem and a number of algorithms are proposed in the literature. This literature takes various forms including rank aggregation and most popularly the (weighted) minimum feedback arc set problem. The submitted paper is oblivious to this enormous literature both in the related work section as well as the empirical evaluations. I have listed below a few papers pertaining to various versions of the problem (this list is by no means exhaustive). With this issue, I cannot give a positive evaluation of this submitted paper since it is not clear whether the paper is just re-solving a solved problem. That said, I am happy to reconsider if the related work and the empirical evaluations are augmented with comparisons to the past literature on the methodological crux of the submitted paper (e.g., why off-the-shelf use of previously proposed algorithms may or may not suffice here.) Unweighted feedback arc set: A fast and effective heuristic for the feedback arc set problem, Eades et al. Efficient Computation of Feedback Arc Set at Web-Scale, Simpson et al. How to rank with few errors, Kenyon-Mathieu et al. Aggregating Inconsistent Information: Ranking and Clustering, Ailon et al. Hardness results: The Minimum Feedback Arc Set Problem is NP-hard for Tournaments, Charbit et al. Weighted feedback arc set: A branch-and-bound algorithm to solve the linear ordering problem for weighted tournaments, Charon et al. Exact and heuristic algorithms for the weighted feedback arc set problem: A special case of the skew\u2010symmetric quadratic assignment problem, Flood Approximating Minimum Feedback Sets and Multicuts in Directed Graphs, Even et al. Random inputs: Noisy sorting without resampling, Braverman et al. Stochastically transitive models for pairwise comparisons: Statistical and computational issues, Shah et al. On estimation in tournaments and graphs under monotonicity constraints, Chatterjee et al. Survey (slightly dated): An updated survey on the linear ordering problem for weighted or unweighted tournaments, Charon et al. Convex relaxation of permutation matrices: On convex relaxation of graph isomorphism, Afalo et al. Facets of the linear ordering polytope, Grotschel ", "rating": "3: Clear rejection", "reply_text": "Thank you for the review . The key difference you missed that separates our work from the papers you cite is that our method is differentiable . In our problem set-up , we are not given the pairwise costs ; they have to be learned . In order for these costs to be learnable with gradient descent , we have to be able to differentiate through the algorithm . This is possible with our method , but not possible with traditional literature on feedback arc sets . Experimental comparisons to the papers you list are thus not meaningful , since the costs that these algorithm operate on have to be learned first . Does this sufficiently clarify for you why our methodology is not reinventing the wheel ? We already cite the particular convex relaxation of permutations that we use ( Fogel et al. , 2013 ; Adams & Zemel , 2011 ) and the NP-hardness of the problem ( Pardalos & Vavasis , 1991 ) . Though we mention this matter of differentiability several times throughout the paper , we will add a sentence in the Related Works section to make this distinction with the work on feedback arc sets even clearer ."}, "1": {"review_id": "HJMCcjAcYX-1", "review_text": "The authors introduce a method to learn to permute sets end-to-end. They define the cost of a permutation as the sum of pairwise costs induced by the permutation, where the pairwise costs are learned. Permutations are made differentiable by relaxing them to doubly stochastic matrices which are approximated with the Sinkhorn operator. In the forward pass of the algorithm, a good permutation (ie one with low cost) is obtained with a few steps of gradient descent (the forward pass itself contains an optimization procedure). This permutation is then either used directly as the output of the algorithm or is used to permute the original inputs and feed the permuted sequence to another module (such as an RNN or a CNN). The method can easily be adapted to other structures such as lattices by considering row-wise and column-wise pairwise relations. The proposed method is benchmarked on 4 tasks: 1. Sorting numbers, where they obtain very strong generalization results. 2. Re-assembling image mosaics, on which they obtain encouraging results. 3. Image classification through image mosaics. 4. Visual Question Answering where the permuted inputs are fed to an LSTM whose final latent state is fed back into the baseline model (a bilinear attention network). Doing so improves over feeding the inputs to an LSTM without learning the order.for which the output is the permutation itself and classification from image mosaics and visual question answering which require to learn an implicit permutation. The method is most similar to Learning Latent Permutations with Gumbel-Sinkhorn Networks (Mena et al) but considers pairwise relations when producing the permutation. This can have important advantages (such as taking local relations into account, as shown by the strong sorting results) but also drawbacks (inability to differentiate inputs with similar content), but in any case this represents a good step towards exploring with different cost functions. The method can be quite unpractical (cubic time complexity in set cardinality, optimization in forward pass, having to preprocess the set into a sequence for another module can be resource expensive). Experimental results on toy tasks (tasks 1, 2 and 3) are encouraging. The approach improves over a relatively strong baseline (task 4) although it isn't clear that it would still hold true when controlling for number of parameters and compute. I have a few comments about the presentation (for which I would be willing to change my score to a 6): - When possible, please use the numbers reported by Mena et al and consider reporting error (instead of accuracy) as they do to ease comparison. The results that you report using their method are quite worse than what they report, so I think it would be fair to include both your reimplementation and the initial results in the table. - It would be interested to have some insights on what function f is learned (for the sorting task and re-assembling image mosaics for example). - Clarity would be improved with figures representing which neural networks are used at what part of the process. ########################################### Updated review: The authors have greatly improved presentation and have addressed concerns about the increase in parameters and computation time. I have changed my score to a 6.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . In general , we intend our model to be useful for relatively small sets of complex objects like in VQA , rather than large sets of simple objects where the cubic time complexity indeed becomes a big problem . About your concern whether the results hold up when controlling for parameters and computation : the time increase by using our model compared to the baseline is about 30 % ( 4400 seconds per epoch instead of 3400 seconds ) . This is a similar increase in computation time as the change from BAN-8 to BAN-12 ( increasing number of attention glimpses to 12 ) in their paper ( Kim et al. , 2018 ) , Table 1 . Their difference is within one standard deviation ( 0.04 % increase , stdev of 0.11 % ) , so simply increasing the BAN model size alone is clearly running into diminishing returns already . Our model does not add a significant number of parameters compared to changing BAN-8 to BAN-12 . Our model also results in qualitatively different improvements : general improvements in VQA typically result in a roughly even improvements in all the categories , whereas our model improves on number questions significantly more than other categories . We will make this clearer in the revision . - We will swap the table of errors ( Appendix E Table 4 ) with the table of accuracies ( Table 1 ) in the main body . We initially decided on reporting accuracy in the main body because MSE may not be directly comparable : we do not know whether their pixel values were scaled to have unit variance ( our choice ) or to be between 0 -- 1 , the latter of which would make their errors seem lower than ours for the same reconstruction ( in personal communication , they said that they did not normalise the data , but our results suggest that they did , since our reproduced MSEs on MNIST closely match theirs ) . As Reviewer 1 seemed to have a slight misunderstanding with accuracy too , we agree that comparing MSE in the main body is likely clearer . Contrary to what you say , our reproductions on MNIST are fairly close to what they report ( our reproduced MSEs are roughly even , accuracies are slightly worse ) and only on ImageNet are our reproduced results of their model worse . To be clear , the relevant row in their results to compare our accuracy to is `` Prop . any wrong '' ( reconstruction is correct only if all tiles are correct , this is what we use ) , not `` Prop . wrong '' ( reconstructions of the tiles being individually correct ) . As per your suggestion , we will include their results in our tables to make the appropriate comparison easier . - We will perform some analysis of the learned f and include it as an appendix . For number sorting , it is enough for it to learn f ( x_i , x_j ) = x_i , so F ( x_i , x_j ) = x_i - x_j , which is a sensible comparison function . In initial analysis it appears to learn a scaled and shifted version of that . We are currently looking into what it learns for image mosaics . - We will add some figures to make the network architectures for the different tasks clearer ."}, "2": {"review_id": "HJMCcjAcYX-2", "review_text": "This paper proposed an interesting idea of learning representations of sets by permutation optimizations. Through learning a permutation of the elements of a set, the proposed algorithm can learn a permutation-invariant representation of that set. To deal with the underlying difficult combinatorial optimization problem, the authors proposed to relax the optimization constraints and instead optimize over the set of doubly-stochastic matrices with reparameterization using the Sinkhorn operator. The cost function of this optimization is related to a pairwise ordering cost, which compares the order for each pair of the elements. The idea of using pairwise comparison information to learn permutations is interesting. The total cost function utilizes the comparison information and optimization over this cost function can lead to a permutation-invariant representation of the set. The idea of using the Sinkhorn operator to reparameterize the doubly-stochastic matrices makes the optimization objective differentiable. Also, the experiment results compared with some baseline algorithms showed the success of the proposed methods in many different tasks. My major concern of the proposed method is on whether this method can be applied to large sets. Since the algorithm compares all pairs of elements in the set, we need O(N^2) comparisons for a set of size N and hence the proposed method might be slow if N is large. Is it possible to improve the efficiency for large sets? Questions and Suggestions: 1. Since the authors wants to approximately solve the objective function in Equation (2), it is better if we can see a proof showing why this optimization problem is difficult. 2. For the experiment in Section 4.2, it seems that all methods (including the proposed methods and the baseline methods) are not performing well if the images are split to at least 4 * 4 equal-size tiles. I understand that currently the authors applied their method to the case of grid permutation by simply adding all cost functions of all rows and columns. Is it possible to extend the proposed method to the grid case in another way so that the results under this setting is better? 3. It will be better if the authors can propose some more insights (probably with some theoretical analysis) when can the PO-U method performs better and when can the PO-LA method performs better. 4. The authors mentioned that, the proposed method can get good permutations even for only T=4 steps. What if we continue running the algorithm? Will the permutation converges stably? 5. The authors proposed to update the permutation matrix parameters in an alternative way (Equation (7)) and mentioned that this update works significantly better in the experiments. It will be great if the authors can have a theoretical analysis on why this is true since P and \\tilde P can be quite different from each other for an arbitrary \\tilde P matrix. Minor comment: I think there is a typo in Equation (5). The entry \\tilde P_{pq} is related to not only the entry P_{pq}, but also the other entries of the matrix P. Hence, I think Equation (5) should be modified as a matrix multiplication.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . Scalability to large sets is indeed an issue with our model . As we discuss in section 5 , its intended use is more with small sets of complex elements ( e.g.objects in images ) rather than large sets of simple elements ( e.g.point clouds ) , with our VQA experiment being an instance of the former . For large sets there are some optimisations that can be made . For example , if there are many pairs of elements where we can guess that they will not affect each other 's local ordering much ( e.g.points in point clouds that are far away from each other ) , we can make the comparisons sparse by only comparing points that are reasonably close to each other , which just involves a pre-processing step . We also mention in the Discussion section that a divide-and-conquer strategy ( perhaps merge-sort-like ) could work , reducing the comparisons down to O ( n log n ) , though it might assume transitivity of the cost function . 1.Eqn ( 2 ) , without any relaxations , is a standard Quadratic Assignment Problem ( see Appendix C to turn ( 2 ) into a more standard formulation ) . These problems are known to be NP-hard , and even epsilon-approximability is NP-hard [ https : //dl.acm.org/citation.cfm ? id=321975 ] . For the quadratic programming formulation ( when constraints on P are relaxed ) , we already cite ( Pardalos & Vavasis , 1991 ) in our paper for NP-hardness . 2.There is a bit of a misunderstanding in interpreting the 4x4 and 5x5 results because we did not explain the metric sufficiently . Especially for MNIST , many of the tiles end up completely blank or very similar when the image is split into many small tiles . This means that there are many equally-valid solutions where blank tiles are assigned to different positions . However , the accuracy metric does not account for these multiple possible solutions properly ( only one of these is considered to be the ground truth ) , so all the blank tiles must happen to be assigned to the blank spots in the ground-truth order for the accuracy to be 1 . We will improve this by swapping accuracy ( Table 1 ) with mean squared error ( Appendix E Table 4 ) for this experiment . Until we have updated the paper , you can see in Table 4 that while there is still a worsening of error as we increase the number of tiles , the change is less abrupt and the models do decently on MNIST for higher number of tiles . As to ideas for improving results in the setting of many tiles for image mosaics : in the grid case , it is possible to not only have row-wise and column-wise permutations , but also over diagonals , which should help in constraining what permutations are considered good by the model . It is also possible to modify the row-wise comparison to not act on each row individually , but consider assignments to other rows simultaneously ( vice versa for columns ) . In some initial work we had some success on image mosaics with an alternative cost function for ( 1 ) that only considers correct ordering between direct neighbours , but it struggled even with small instances of the sorting task so we did not pursue this further . It may be possible to fix this issue , perhaps by doing a convex combination of the cost function in the paper and this modified cost function , which should give better results . 3.As theoretical analysis of these complex learned systems is rather difficult ( it is a bit like asking for theoretical analysis of when , for example in the context of sequence modeling , CNNs are better than RNNs ) , we instead point to the results in section 4 : PO-LA is suitable for when LinAssign is decent , which is the case when absolute positioning of set elements is useful ( since LinAssign only directly models absolute positioning ) . The PO part is then used to refine this . PO-U is suitable for when relative position between set elements becomes more important , resulting in LinAssign only learning things that are not useful and thus being a detractor . If the input set can have variable size ( e.g.in VQA ) , it is not possible to use PO-LA , only PO-U . 4.We took a net that was trained to sort 10 numbers with T=4 and evaluated it with T=100 000 . The accuracy degraded from 100 % accuracy to ~94 % accuracy . We also took a net trained with T=20 and evaluated it with T=100 000 , which resulted in 100 % accuracy this time . It appears that if the permutations seen in training are sufficiently converged already -- which is the case for T=20 -- it converges stably at evaluation time when run for signficantly longer too . 5.We will add this as an appendix to the paper . By looking at the gradients before and after the Sinkhorn , it becomes clear that gradients vanish when trying to differentiate through the Sinkhorn the further away from the uniform initialisation one gets . That means that it is difficult to learn Ps that are close to proper permutation matrices ( all 0 and 1 entries ) if we do not use the alternative gradient . Typo in equation ( 5 ) : Good catch ! We will fix this in the revision of our paper ."}}