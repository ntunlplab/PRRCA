{"year": "2018", "forum": "S1PWi_lC-", "title": "Multi-task Learning on MNIST Image Datasets", "decision": "Reject", "meta_review": "the paper validates the benefit of multi-task learning on MNIST datasets, which is not sufficient for ICLR publication", "reviews": [{"review_id": "S1PWi_lC--0", "review_text": "This paper presents a multi-task neural network for classification on MNIST-like datasets. The main concern is that the technical innovation is limited. It is well known that multi-task learning can lead to performance improvement on similar tasks/datasets. This does not need to be verified in MNIST-like datasets. The proposed multi-task model is to fine tune a pretrained model, which is already a standard approach for multi-task and transfer learning. So the novelty of this paper is very limited. The experiments do not bring too much insights.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the thorough analysis and insightful comments . Although it is well known that multi-task learning can lead to performance improvement on similar tasks , there is not too many research on FashionMNIST and NotMNIST datasets . And we also visualize the results obtained with/without multi-task learning to analyze our experiment result . Hopeful it contributes to the research community ."}, {"review_id": "S1PWi_lC--1", "review_text": "The manuscript mainly utilizing the data from all three MNIST-like datasets to pre-train the parameters of joint classification networks, and the pre-trained parameters are utilized to initialize the disjoint classification networks (of the three datasets). The presented idea is quite simple and the authors only re-affirm that multi-task learning can lead to performance improvement by simultaneously leverage the information of multiple tasks. There is no technique contribution. Pros: 1. The main idea is clearly presented. 2. It is interesting to visualize the results obtained with/without multi-task learning in Figure 6. Cons: 1. The contribution is quite limited since the authors only apply multi-task learning to the three MNIST-like datasets and there is no technique contribution. 2. There is no difference between the architecture of the single-task learning network and multi-task learning network. 3. Many unclear points, e.g., there is no description for \u201czero-padding\u201d and why it can enhance target label. What is the \u201ctwo-stage learning rate decay scheme\u201d and why it is implemented? It is also unclear what can we observed from Figure 4. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the very pertinent and exhaustive comments about our work . We humbly accept the shortcomings you said about our paper and have already revised the shortcomings in our paper . About the unclear points , our answers as below : In order to compare the result between multi-task learning and single task , we must keep the architecture of the network unchanging . The reason we implement `` zero-padding '' is when the multi-task learning process is executed , we combine the training data of MNIST-like datasets together . In order to train a 20 ( or 30 ) ways classifier , we have to extend target labels by zero-padding . The \u201c two-stage learning rate decay scheme \u201d we implemented is just a way to decrease learning rate . We found it is helpful to improve recognition accuracies ."}, {"review_id": "S1PWi_lC--2", "review_text": "The paper applies multi-task learning to MNIST (M), FashionNIST (F), and NotMNIST (N) datasets. That is, the authors first train a neural network (with a specific architecture; in this case, it is an all-convolutional network) on a combination of the datasets (M+F; F+N; N+M; M+F+N) and then use the learned weights (in all but the output layer) to initialize the weights for task-specific training on each of the datasets. The authors observe that for each of the combinations, the above approach does better than training on a dataset individually. Further, in all but one case, initializing weights based on training on M+F+N gives the best performance. The improvements are not striking but are noticeable. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the thorough analysis and insightful comments ."}], "0": {"review_id": "S1PWi_lC--0", "review_text": "This paper presents a multi-task neural network for classification on MNIST-like datasets. The main concern is that the technical innovation is limited. It is well known that multi-task learning can lead to performance improvement on similar tasks/datasets. This does not need to be verified in MNIST-like datasets. The proposed multi-task model is to fine tune a pretrained model, which is already a standard approach for multi-task and transfer learning. So the novelty of this paper is very limited. The experiments do not bring too much insights.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the thorough analysis and insightful comments . Although it is well known that multi-task learning can lead to performance improvement on similar tasks , there is not too many research on FashionMNIST and NotMNIST datasets . And we also visualize the results obtained with/without multi-task learning to analyze our experiment result . Hopeful it contributes to the research community ."}, "1": {"review_id": "S1PWi_lC--1", "review_text": "The manuscript mainly utilizing the data from all three MNIST-like datasets to pre-train the parameters of joint classification networks, and the pre-trained parameters are utilized to initialize the disjoint classification networks (of the three datasets). The presented idea is quite simple and the authors only re-affirm that multi-task learning can lead to performance improvement by simultaneously leverage the information of multiple tasks. There is no technique contribution. Pros: 1. The main idea is clearly presented. 2. It is interesting to visualize the results obtained with/without multi-task learning in Figure 6. Cons: 1. The contribution is quite limited since the authors only apply multi-task learning to the three MNIST-like datasets and there is no technique contribution. 2. There is no difference between the architecture of the single-task learning network and multi-task learning network. 3. Many unclear points, e.g., there is no description for \u201czero-padding\u201d and why it can enhance target label. What is the \u201ctwo-stage learning rate decay scheme\u201d and why it is implemented? It is also unclear what can we observed from Figure 4. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the very pertinent and exhaustive comments about our work . We humbly accept the shortcomings you said about our paper and have already revised the shortcomings in our paper . About the unclear points , our answers as below : In order to compare the result between multi-task learning and single task , we must keep the architecture of the network unchanging . The reason we implement `` zero-padding '' is when the multi-task learning process is executed , we combine the training data of MNIST-like datasets together . In order to train a 20 ( or 30 ) ways classifier , we have to extend target labels by zero-padding . The \u201c two-stage learning rate decay scheme \u201d we implemented is just a way to decrease learning rate . We found it is helpful to improve recognition accuracies ."}, "2": {"review_id": "S1PWi_lC--2", "review_text": "The paper applies multi-task learning to MNIST (M), FashionNIST (F), and NotMNIST (N) datasets. That is, the authors first train a neural network (with a specific architecture; in this case, it is an all-convolutional network) on a combination of the datasets (M+F; F+N; N+M; M+F+N) and then use the learned weights (in all but the output layer) to initialize the weights for task-specific training on each of the datasets. The authors observe that for each of the combinations, the above approach does better than training on a dataset individually. Further, in all but one case, initializing weights based on training on M+F+N gives the best performance. The improvements are not striking but are noticeable. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the thorough analysis and insightful comments ."}}