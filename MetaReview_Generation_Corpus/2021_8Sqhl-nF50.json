{"year": "2021", "forum": "8Sqhl-nF50", "title": "On the Curse of Memory in Recurrent Neural Networks: Approximation and Optimization Analysis", "decision": "Accept (Poster)", "meta_review": "This paper provides a theoretically rigorous treatment of approximation properties and convergence analysis of LINEAR RNNs. The reviewers were divided in their evaluation. On the positive side, the presented relation between approximation error and required memory size is not obvious and interesting. On the less positive side, two of the reviewers raised the necessity of mathematical machinery that were invoked. Furthermore, its applicability is unclear in ML, since they aren't applicable to the usual nonlinear RNNs. However, given that the theoretical contributions are clear, the final decision was to accept.", "reviews": [{"review_id": "8Sqhl-nF50-0", "review_text": "This paper studies approximation and optimization of linear RNNs for learning linear functions , from the perspective of the memory-properties of the temporal sequence . It shows that linear functionals can be approximated by a linear RNN , with the rate of approximation depending on the long-term memory of the process . It also shows that the training dynamics slow down for certain linear functionals with long-term memory . Strengths : 1 . The problem being studied in the paper is interesting and well-motivated . Capturing long-term memory is one of the major challenges for sequential models such as RNNs , and the paper makes progress towards understanding this . 2.The functional view of the process is interesting , and seems to shed light on interesting phenomenon regarding memory . The paper also brings in rich tools from functional analysis for analyzing RNNs , which could perhaps be more broadly useful if they can be made accessible enough . Weaknesses : 1 . I think the paper needs to be significantly rewritten for the ML audience to extract much out of it . Most of the ML community will not be familiar with the tools and terminology here , including classical results from representation theory such as the Riesz-Markov-Kakutani theorem . Providing more intuition and context for these results will be very helpful . For instance , it would be good to provide some intuition for the \\rho ( t ) function . Once this is introduced , the authors use it interchangeably in place of H_t for describing all their subsequent examples , but it would be better to provide some intuition for the examples directly . The underlying phenomenon are simple and elegant , and I think they can be explained effectively to the ML community . 2.As far as the main message of the paper regarding memory goes , I think it is interesting , but I am not sure if all the machinery is necessary for showing this result ? For instance , for a linear functional which does not decay fast enough on the constant input ( such as in the conditions of Thm 4.2 ) , would it not be possible to show that it can not be approximated using a small number of neurons even in the discrete case ? The reason being that the process \u201c remembers \u201d inputs over exponentially long windows , and hence you need an exponential number of units to approximate it ( at least with linear activations ) ? Can the authors shed light on the power of the continuous time view and representation theory for showing this ? 3.The optimization result seems a bit tailored to a particular functional . I think if the authors could explain more generally why the optimization is getting stuck at a plateau , even at an intuitive level , then that would be useful . I \u2019 m also curious about the same question as before , is it not possible to construct a specific worst case function even in the discrete case ? Overall , I think there are some nice ideas and tools here , but am not sure if the ML community will get a lot out of this currently . Some more comments : 1 . Please define/describe the airy function . 2.Typo above Eq ( 21 ) , We- > we . 3.Typo above conclusion , exponentially- > exponential . 4.Theorem 4.2 , y_i^ ( k ) ( t ) with the superscript ( k ) does not appear to be defined ? 5.Please clearly define what inputs and outputs are at the beginning of Section , for example x is input , y is output , h is not observed . Updates after response I thank the authors for the detailed response and the revision . I am still not completely convinced regarding the suitability for ICLR and have similar concerns to reviewer 2 , but am not opposed to acceptance . In light of this , I have increased my score to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "( Continued ) 3 . `` The optimization result seems a bit tailored to a particular functional . I think if the authors could explain more generally why the optimization is getting stuck at a plateau , even at an intuitive level , then that would be useful . I \u2019 m also curious about the same question as before , is it not possible to construct a specific worst case function even in the discrete case ? '' * First , let us emphasize that not all functionals will result in difficulty in training ( e.g.exponentially decaying $ \\rho $ ) , thus it is * necessary * to restrict the functional family . In fact , one of our main theoretical contributions in this portion is to give a precise family for which such exponential slow downs * provably * occurs , and that it is in fact related to memory that corroborates with previous practical findings , as well as our numerical experiments in Fig 2 in the appendix . * * We have made this point clearer in the discussion following Theorem 5.1 * * . * The family of functionals we constructed in here is in fact not that special -- if one were to construct a `` memory '' effect that diverges , it is likely to result in such a separation of terms . The template function $ \\rho_0 $ is general , and is only assumed to satisfy some smoothness and decay properties , and the $ \\bar { \\rho } $ is a universal approximator that efficiently approximates any sufficiently decaying $ \\rho $ as shown in our approximation analysis . * As discussed in reply point 2 , the analysis in discrete setting does not appear simpler to us , because we can not perform many of the exact integrations , and the application of Ito isometry , that we used to arrive at our result . Such operations are important in deriving * provable * slowdowns in optimization . We agree that there could be constructions in discrete time that gives a similar behavior ( e.g.the discretization of our construction ! ) , but we do not see how it is simpler , especially when we require a fine-grained analysis of the landscape and training dynamics . * * We modified the conclusion to reflect this point * * . * Intuition of why the plateau happens : Suppose that we currently have a good approximation $ \\hat { \\rho } _m $ of the short term part $ \\bar { \\rho } $ , then we can show that the loss is large ( $ J_m=\\mathcal { O } ( 1 ) $ ) because the long term part $ \\rho_ { 0 , \\omega } $ is not accounted for . However , we prove that the gradient is small ( $ \\nabla J_m=o ( 1 ) $ ) , since the gradient contribution comes mostly from the short term part . The gradient corresponding to the long term part is concentrated at large $ t $ , and thus modulated by a negative exponential function of $ t $ ( see Eq . ( 20 ) ) .This shows that we must have a slowdown in the training dynamics in the region $ \\hat { \\rho } _m \\approx \\bar { \\rho } $ . The escape time follows from precise estimates of the eigenvalues of the Hessian and local linearization analysis . * * We sketched the ideas of the result after the revised Theorem 5.1 statement . * * 4 . * * Minor issues are fixed . * *"}, {"review_id": "8Sqhl-nF50-1", "review_text": "The paper provides a theoretical examination of the challenge of fitting recurrent neural networks ( RNNs ) to fit processes with long memory ( or long-range dependence ) . Dubbed the \u201c curse of memory \u201d , the author ( s ) restrict to the case of linear activation functions , and show that for processes with increased spatial dependence : ( a ) the width of the layers must increase exponentially to _guarantee_ accurate approximations under a provided bound , and ( b ) a gradient-based optimization algorithm will take exponentially more time to converge . Sufficient details for reproducing the experiments are provided . I like the paper and believe the contributions to be both substantive and of wide interest in the RNN community . The analysis presented here is rigorous and comprehensive , and while the discussion is limited to linear RNNs , it provides a good starting point for further studies regarding long-range dependence with RNNs . The paper is reasonably well-written , and aside from a few minor typos \u2014 see the minor comments below \u2014 I did not encounter any serious errors . I do have some criticisms , however : 1 . Perhaps my biggest disappointment is that while the authors do a good summary of the relevant RNN literature , they have failed to mention any analogous ideas from the time series literature . For example , any discussion regarding Hurst parameters would be welcome , since this is precisely the type of \u201c long-range dependence \u201d discussed here . The lack of references to this literature is disheartening , since these concepts have played a key role in time series analysis for many decades , and it would have been nice to see this acknowledged , or a connection made . 2.The takeaway from Theorem 4.2 regarding the exponential increase in the required width for fitting a linear RNN is nice , but is undercut by the fact that Theorem 4.2 is an upper bound . I 'm satisfied with the result , but a lower bound for approximating a particular functional would be more convincing . 3.I was disappointed with the \u201c informal \u201d presentation here . I can appreciate the attempt to simplify the full statement , but I still had to go to the supplementary material to understand the statement , which I should hope most readers would not have to do . I think a more precise statement than \u201c trapped in a plateau with timescale \u201d would be better . Perhaps something involving the hitting time , or even more simply , $ ||\\theta ' ( t ) || = O ( ... ) $ as $ \\omega \\to 0^+ $ for sufficiently large $ t $ ? It is also worth noting that there is an extraordinary amount of supplementary material here , much of which , unfortunately , does not get the attention it probably deserves in this format . There are aspects of the paper , including the precise statement of Theorem 5.1 and the definition of the Airy target , that require the reader to visit this material . The proofs of Theorems 4.1 and 4.2 seem fine to me . Unfortunately , the proof of Theorem 5.1 is especially lengthy , so I did not get the opportunity to check each step in detail . However , the general argument appears sound . Overall , I am impressed with this paper and enjoyed reading it . The missing connections to classical ideas in time series analysis are unfortunate , and I would be willing to give the paper an 8 with a wider literature review and improved presentation of Theorem 5.1 . An additional lower bound for Theorem 4.2 would bring it to a 9 at least . But , even in its current form , I recommend this paper for acceptance to ICLR . Other comments : - I believe Theorem 4.1 also follows from the fact that matrix exponential distributions are dense ( see [ 1 ] ) , together with the Riesz-Markov-Kakutani representation theorem used here . - \u201c We first show that the training dynamics of $ \\mathbb { E } _x J_m $ exhibits very interesting behaviors depending on the form of target functionals. \u201d \u2014 This is a little too vague , maybe consider rewording ? - In multiple places : \u201c x being/is the white noise \u201d - > \u201c white noise x \u201d or \u201c x is white noise \u201d . - Check capitalization in the statement under Theorem A.1 . - \u201c Airy \u201d should be capitalized in each appearance . [ 1 ] He , Qi-Ming , and Hanqin Zhang . `` On matrix exponential distributions . '' Advances in Applied Probability 39.1 ( 2007 ) : 271-292 .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "1 . `` Perhaps my biggest disappointment is that while the authors do a good summary of the relevant RNN literature , they have failed to mention any analogous ideas from the time series literature . For example , any discussion regarding Hurst parameters would be welcome , since this is precisely the type of \u201c long-range dependence \u201d discussed here . The lack of references to this literature is disheartening , since these concepts have played a key role in time series analysis for many decades , and it would have been nice to see this acknowledged , or a connection made . '' * We thank the reviewer for the suggestion . We have added some literature review on time series analysis pertaining to memory . We feel that the key difference here is we consider memory of input-output relationship and their interaction with RNN approximation/optimization . On the other hand , these works mainly focus on characterizing the memory structures in the input temporal data themselves . * * We have added discussion on these prior works at the end of section 2 . * * 2 . `` The takeaway from Theorem 4.2 regarding the exponential increase in the required width for fitting a linear RNN is nice , but is undercut by the fact that Theorem 4.2 is an upper bound . I 'm satisfied with the result , but a lower bound for approximating a particular functional would be more convincing . '' * We fully agree with the reviewer 's comment that Theorem 4.2 gives an upper-bound of the approximation error . If we denote the minimum number of terms needed to achieve an $ L^1 $ error $ \\epsilon $ to the target density $ 1/t^ { ( 1+\\omega ) } $ as $ m ( \\omega , \\epsilon ) $ , the estimate derived from Theorem 4.2 shows an upper-bound of $ m ( \\omega , \\epsilon ) $ goes to infinity exponentially fast as $ \\omega\\rightarrow 0^+ $ with fixed $ \\epsilon $ . A stronger result would be that the lower bound of $ m ( \\omega , \\epsilon ) \\rightarrow \\infty $ exponentially fast as $ \\omega\\rightarrow 0^+ $ with fixed $ \\epsilon $ . The divergence of $ m ( \\omega , \\epsilon ) $ is somewhat natural , but the exponential divergence of its lower-bound is not obvious . We have some thoughts on it , but we have not proved it in general so far . This is a point of our future work . * * We have made the point about upper/lower bounds clearer at the end of Section 4 on Page 6 when discussing our estimate . * * 3 . `` I was disappointed with the \u201c informal \u201d presentation here . I can appreciate the attempt to simplify the full statement , but I still had to go to the supplementary material to understand the statement , which I should hope most readers would not have to do . I think a more precise statement than \u201c trapped in a plateau with timescale \u201d would be better . Perhaps something involving the hitting time , or even more simply , .... '' * * * We have improved the precision of Theorem 5.1 and added some explanatory paragraphs of the main proof ideas . * * 4 . `` It is also worth noting that there is an extraordinary amount of supplementary material here , much of which , unfortunately , does not get the attention it probably deserves in this format . There are aspects of the paper , including the precise statement of Theorem 5.1 and the definition of the Airy target , that require the reader to visit this material . The proofs of Theorems 4.1 and 4.2 seem fine to me . Unfortunately , the proof of Theorem 5.1 is especially lengthy , so I did not get the opportunity to check each step in detail . However , the general argument appears sound . '' * We agree that there is a large amount of technical details for the optimization part . This is because any precise analysis of non-convex optimization tends to be very complicated . * * We have tried to improve the overall clarity of the stated results and sketched the main ideas after Theorem 5.1 . We also improved the presentation in the appendix . * * 5 . `` I believe Theorem 4.1 also follows from the fact that matrix exponential distributions are dense ( see [ 1 ] ) , together with the Riesz-Markov-Kakutani representation theorem used here '' * Thank you for the suggested reference . Indeed , this result is related to the density of phase type distributions . Here , we consider signed measures instead of probability measures , but the approach should be generalizable ( e.g.using Jordan decomposition ) . * * We have added some references and stated the connection in the proof of Theorem 4.1 ( Lemma A.2 ) . * * 6 . * * Minor issues are fixed . * *"}, {"review_id": "8Sqhl-nF50-2", "review_text": "This paper reports a mathematical study of approximation properties of linear RNNs . The first part reports a universal approximation theorem , and presents an analysis of how efficient the approximation is . In particular , it is shown that approximating a slowly decaying , power-law temporal filter requires a large number of neurons , a property the authors refer to as the `` curse of memory '' . The second part of the paper examines the dynamics of learning under gradient descent , and gives arguments related to the existence of long plateaus seen in the loss as function of training epochs . The paper is written in a formal mathematical style ( Definitions/Theorems ) . Not being a mathematician , I am not able to assess the formal correctness of the proofs , and I have found some parts not easily accessible . Most importantly , as currently presented the main results seem to be of limited interest to the ICLR community ( see below for details ) . The paper as a whole is probably more appropriate for a more mathematical venue , where the novelty of the proofs may be better appreciated . Strengths : - attempts to put on a rigorous footing various experimental observations on RNN training - possibly novel mathematical derivations of approximation results Concerns : - the novelty of the results presented in the first part is limited . Previous works have reported various universal approximation theorems for RNNs . Several classical works on that topic are not mentioned , eg Doya 1993 , or Maass 2007 . The details of the mathematical derivation may well be novel , but I am not able to judge this aspect . - the fact that a diverging number of exponentials are needed to approximate a power-law function is also well known ; from that perspective the `` curse of memory '' is not very surprising . - I found it difficult to extract key results from the second part on Optimization dynamics .", "rating": "3: Clear rejection", "reply_text": "( Continued ) 3 . `` the fact that a diverging number of exponentials are needed to approximate a power-law function is also well known ; from that perspective the `` curse of memory '' is not very surprising . '' * First , we would like to point out the fact the approximation problem in RNNs can be mathematically understood as the approximation of decaying functions by exponential sums * is * one of our original contributions . In our viewpoint , this is not * a priori * obvious without a precise formulation and detailed analysis , and this has not been discussed previously in the literature to the best of our knowledge . * Second , it is * not * obvious whether a * fixed * power-law function can be efficiently approximated by a sum of exponentials . The answer depends on the type of approximation , and many problems are still open in the approximation theory literature . In Braess and Hackbusch 2005 ( IMA Journal of Numerical Analysis ( 2005 ) 25 , 685-697 ) ) , it is in fact proved that the $ L^ { \\infty } $ ( uniform-in-time ) approximation error of $ 1/t $ through exponential sums decreases like $ O ( \\text { exp } ( -c\\sqrt { m } ) ) $ with the order $ m $ of the exponential sum ( i.e.number of `` hidden nodes '' in our RNN formulation ) . Thus , it is known that the power law function $ 1/t $ * can * be efficiently approximated by exponentials in $ L^\\infty $ . However , in our case , we need $ L^1 $ approximation , and in this case lower bound on the order of approximation error remains unsolved . It is actually guessed in Kammler 1979 ( SIAM Journal on Numerical Analysis ( 1979 ) 16 , 30-45 ) that the $ L^1 $ approximation error to $ 1/t^2 $ , another power law function , should have a similar exponential order , but this is not proved . In all these cases , the statements can be understood as follows : let $ m ( \\epsilon ) $ be the minimum number of terms needed to achieve approximation of order $ \\epsilon $ . Exponential convergence means that $ m ( \\epsilon ) \\sim \\log ( 1/\\epsilon ) $ . This is different from what we need , as discussed next . * Here what we considered on the approximation rates is different from the above results in the literature . We are interested in the approximation error of a * sequence * of functions , namely , the $ L^1 $ approximation error of $ 1/t^ { ( 1+\\omega ) } $ as $ \\omega\\rightarrow 0^+ $ . Denote the minimum number of terms needed to achieve an $ L^1 $ error $ \\epsilon $ as $ m ( \\omega , \\epsilon ) $ . In the paper we show that an upper-bound of $ m ( \\omega , \\epsilon ) $ goes to infinity exponentially fast as $ \\omega\\rightarrow 0^+ $ with fixed $ \\epsilon $ . This is similar to the classical results on the curse of dimensionality in approximation theory , where now $ \\omega $ plays the same role as `` dimension '' . A stronger result would be that the lower bound of $ m ( \\omega , \\epsilon ) \\rightarrow \\infty $ exponentially as $ \\omega\\rightarrow 0^+ $ with fixed $ \\epsilon $ , and this is a point of future work . * * We also made the point about upper/lower bounds clearer at the end of Section 4 on Page 6 when discussing our estimate . * * 4 . `` I found it difficult to extract key results from the second part on Optimization dynamics . '' * The result here is a fine-grained analysis of the optimization landscape and dynamics of a particular class of functionals possessing long memory . These functionals are of the form of an addition of a short term memory part that is easily learned , and a long term memory part that poses difficulty in training ( see.Eq.21 ) .For such functionals , we show that the optimization dynamics can get trapped for exponentially large times near points where the short term memory is learned but the long term one is not . In fact , the trapping time has an exponentially diverging lower bound as the memory increases . The derived estimate can be checked against experiments ( Fig 2 , 3 in Appendix ) , and provably illustrates the difficulty of RNN training as a result of long term memory in the training data . This has been proposed heuristically in many practical works , but here we make it precise , albeit in a idealistic but representative setting . * * We improved the precision of the result ( Theorem 5.1 ) and added a intuitive explanatory paragraph of the main ideas following the proof . * *"}, {"review_id": "8Sqhl-nF50-3", "review_text": "The main contributions of the paper are the following ones ( informal ) : 1 . The approximation theorem of linear functionals with linear RNNs in continuous time settings . The main difference with the previous results is that the class of approximated functions is linear , but does not necessarily come from the same differential equation that describes the class of approximator models . 2.The upper bound on the approximation error for some `` exponentially decaying '' linear functionals , where the upper bound depends on the weights matrix size ( i.e.memory size ) . The memory growth rate is polynomial with respect to approximation error . It is no longer the case when linear functional is not `` decaying exponentially '' and memory growth rate is exponential . 3.The optimization dynamics analysis . In particular , the authors showed that under some conditions the optimization process can be stuck if the `` memory '' of the target functional is large . Overall , it is a good paper and I enjoyed reading it . It is very well written and easy to follow . In many cases , the authors provide clarification for used assumptions . The authors also emphasize the difference between their results and the previous results . Pros : the authors showed that difficulties encountered in practice , where the target functional has long term dependencies , emerge even in simple linear settings and can be explained from a theoretical point of view . Cons : in many settings we are interested in not just recovering some dependencies , but recovering it from the data or recovering the dependency only on some subset of the all possible input sentences . The role of input data is significantly ignored in the given analysis . Several questions and remarks : 1 . The condition on supremum in ( 14 ) seems purely technical ( at least based on provided proof in appendix ) . Could the authors please clarify whether this assumption has some qualitative explanation or can be replaced with stronger but more `` meaningful '' assumptions ( of course it will make the result weaker ) ? 2.In the dynamic analysis x is assumed to be white noise . This assumption seems too restrictive and is used to apply Ito 's isometry theorem . What else stochastic processes can be used here to make this result stronger ? 3.In ( 50 ) in ( - ( alpha + 1 ) / beta ) ^ { i } . i should be replaced with j .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "1 . `` in many settings we are interested in not just recovering some dependencies , but recovering it from the data or recovering the dependency only on some subset of the all possible input sentences . The role of input data is significantly ignored in the given analysis . '' [ Related concern that follows : `` In the dynamic analysis $ x $ is assumed to be white noise . This assumption seems too restrictive and is used to apply Ito 's isometry theorem . What else stochastic processes can be used here to make this result stronger ? `` . We answer both together below . ] * We agree that in our optimization analysis , our assumption of white noise does not highlight the role of specific input sequence distributions . The choice is made for simplicity , since a fine-grained optimization analysis for non-convex landscapes is in general very involved . It is certainly worthwhile to explore the role of input distribution in optimization in the future . * * With that said , we conducted some additional experiments with varying input distributions for our test cases in Fig 4 of the appendix , where we found that even for non-white-noise input ( $ x_t = \\sum_ { j=1 } ^ { J } \\alpha_j \\cos ( \\lambda_j t ) $ where $ \\lambda_j \\sim U [ 0,10 ] $ and $ \\alpha_j \\sim \\mathcal { N } ( 0,1 ) , J=5 $ ) , we still observe similar behavior . The results are reported in the revised appendix . This at least suggests that our setting is generally representative , but we agree that certainly more theoretical work can be done in this direction in the future to make this rigorous . * * 1 . `` The condition on supremum in ( 14 ) seems purely technical ( at least based on provided proof in appendix ) . Could the authors please clarify whether this assumption has some qualitative explanation or can be replaced with stronger but more `` meaningful '' assumptions ( of course it will make the result weaker ) ? '' * This condition can be interpreted as the bound of the function 's derivatives . It essentially requires the derivatives to decay rapidly enough . If $ \\gamma $ is finite , then it gives us a kind of norm . Similar kinds of norms is common in mathematical analysis , for instance , the Sobolev spaces and Besov spaces . 2 . * * Minor issues are fixed . * *"}], "0": {"review_id": "8Sqhl-nF50-0", "review_text": "This paper studies approximation and optimization of linear RNNs for learning linear functions , from the perspective of the memory-properties of the temporal sequence . It shows that linear functionals can be approximated by a linear RNN , with the rate of approximation depending on the long-term memory of the process . It also shows that the training dynamics slow down for certain linear functionals with long-term memory . Strengths : 1 . The problem being studied in the paper is interesting and well-motivated . Capturing long-term memory is one of the major challenges for sequential models such as RNNs , and the paper makes progress towards understanding this . 2.The functional view of the process is interesting , and seems to shed light on interesting phenomenon regarding memory . The paper also brings in rich tools from functional analysis for analyzing RNNs , which could perhaps be more broadly useful if they can be made accessible enough . Weaknesses : 1 . I think the paper needs to be significantly rewritten for the ML audience to extract much out of it . Most of the ML community will not be familiar with the tools and terminology here , including classical results from representation theory such as the Riesz-Markov-Kakutani theorem . Providing more intuition and context for these results will be very helpful . For instance , it would be good to provide some intuition for the \\rho ( t ) function . Once this is introduced , the authors use it interchangeably in place of H_t for describing all their subsequent examples , but it would be better to provide some intuition for the examples directly . The underlying phenomenon are simple and elegant , and I think they can be explained effectively to the ML community . 2.As far as the main message of the paper regarding memory goes , I think it is interesting , but I am not sure if all the machinery is necessary for showing this result ? For instance , for a linear functional which does not decay fast enough on the constant input ( such as in the conditions of Thm 4.2 ) , would it not be possible to show that it can not be approximated using a small number of neurons even in the discrete case ? The reason being that the process \u201c remembers \u201d inputs over exponentially long windows , and hence you need an exponential number of units to approximate it ( at least with linear activations ) ? Can the authors shed light on the power of the continuous time view and representation theory for showing this ? 3.The optimization result seems a bit tailored to a particular functional . I think if the authors could explain more generally why the optimization is getting stuck at a plateau , even at an intuitive level , then that would be useful . I \u2019 m also curious about the same question as before , is it not possible to construct a specific worst case function even in the discrete case ? Overall , I think there are some nice ideas and tools here , but am not sure if the ML community will get a lot out of this currently . Some more comments : 1 . Please define/describe the airy function . 2.Typo above Eq ( 21 ) , We- > we . 3.Typo above conclusion , exponentially- > exponential . 4.Theorem 4.2 , y_i^ ( k ) ( t ) with the superscript ( k ) does not appear to be defined ? 5.Please clearly define what inputs and outputs are at the beginning of Section , for example x is input , y is output , h is not observed . Updates after response I thank the authors for the detailed response and the revision . I am still not completely convinced regarding the suitability for ICLR and have similar concerns to reviewer 2 , but am not opposed to acceptance . In light of this , I have increased my score to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "( Continued ) 3 . `` The optimization result seems a bit tailored to a particular functional . I think if the authors could explain more generally why the optimization is getting stuck at a plateau , even at an intuitive level , then that would be useful . I \u2019 m also curious about the same question as before , is it not possible to construct a specific worst case function even in the discrete case ? '' * First , let us emphasize that not all functionals will result in difficulty in training ( e.g.exponentially decaying $ \\rho $ ) , thus it is * necessary * to restrict the functional family . In fact , one of our main theoretical contributions in this portion is to give a precise family for which such exponential slow downs * provably * occurs , and that it is in fact related to memory that corroborates with previous practical findings , as well as our numerical experiments in Fig 2 in the appendix . * * We have made this point clearer in the discussion following Theorem 5.1 * * . * The family of functionals we constructed in here is in fact not that special -- if one were to construct a `` memory '' effect that diverges , it is likely to result in such a separation of terms . The template function $ \\rho_0 $ is general , and is only assumed to satisfy some smoothness and decay properties , and the $ \\bar { \\rho } $ is a universal approximator that efficiently approximates any sufficiently decaying $ \\rho $ as shown in our approximation analysis . * As discussed in reply point 2 , the analysis in discrete setting does not appear simpler to us , because we can not perform many of the exact integrations , and the application of Ito isometry , that we used to arrive at our result . Such operations are important in deriving * provable * slowdowns in optimization . We agree that there could be constructions in discrete time that gives a similar behavior ( e.g.the discretization of our construction ! ) , but we do not see how it is simpler , especially when we require a fine-grained analysis of the landscape and training dynamics . * * We modified the conclusion to reflect this point * * . * Intuition of why the plateau happens : Suppose that we currently have a good approximation $ \\hat { \\rho } _m $ of the short term part $ \\bar { \\rho } $ , then we can show that the loss is large ( $ J_m=\\mathcal { O } ( 1 ) $ ) because the long term part $ \\rho_ { 0 , \\omega } $ is not accounted for . However , we prove that the gradient is small ( $ \\nabla J_m=o ( 1 ) $ ) , since the gradient contribution comes mostly from the short term part . The gradient corresponding to the long term part is concentrated at large $ t $ , and thus modulated by a negative exponential function of $ t $ ( see Eq . ( 20 ) ) .This shows that we must have a slowdown in the training dynamics in the region $ \\hat { \\rho } _m \\approx \\bar { \\rho } $ . The escape time follows from precise estimates of the eigenvalues of the Hessian and local linearization analysis . * * We sketched the ideas of the result after the revised Theorem 5.1 statement . * * 4 . * * Minor issues are fixed . * *"}, "1": {"review_id": "8Sqhl-nF50-1", "review_text": "The paper provides a theoretical examination of the challenge of fitting recurrent neural networks ( RNNs ) to fit processes with long memory ( or long-range dependence ) . Dubbed the \u201c curse of memory \u201d , the author ( s ) restrict to the case of linear activation functions , and show that for processes with increased spatial dependence : ( a ) the width of the layers must increase exponentially to _guarantee_ accurate approximations under a provided bound , and ( b ) a gradient-based optimization algorithm will take exponentially more time to converge . Sufficient details for reproducing the experiments are provided . I like the paper and believe the contributions to be both substantive and of wide interest in the RNN community . The analysis presented here is rigorous and comprehensive , and while the discussion is limited to linear RNNs , it provides a good starting point for further studies regarding long-range dependence with RNNs . The paper is reasonably well-written , and aside from a few minor typos \u2014 see the minor comments below \u2014 I did not encounter any serious errors . I do have some criticisms , however : 1 . Perhaps my biggest disappointment is that while the authors do a good summary of the relevant RNN literature , they have failed to mention any analogous ideas from the time series literature . For example , any discussion regarding Hurst parameters would be welcome , since this is precisely the type of \u201c long-range dependence \u201d discussed here . The lack of references to this literature is disheartening , since these concepts have played a key role in time series analysis for many decades , and it would have been nice to see this acknowledged , or a connection made . 2.The takeaway from Theorem 4.2 regarding the exponential increase in the required width for fitting a linear RNN is nice , but is undercut by the fact that Theorem 4.2 is an upper bound . I 'm satisfied with the result , but a lower bound for approximating a particular functional would be more convincing . 3.I was disappointed with the \u201c informal \u201d presentation here . I can appreciate the attempt to simplify the full statement , but I still had to go to the supplementary material to understand the statement , which I should hope most readers would not have to do . I think a more precise statement than \u201c trapped in a plateau with timescale \u201d would be better . Perhaps something involving the hitting time , or even more simply , $ ||\\theta ' ( t ) || = O ( ... ) $ as $ \\omega \\to 0^+ $ for sufficiently large $ t $ ? It is also worth noting that there is an extraordinary amount of supplementary material here , much of which , unfortunately , does not get the attention it probably deserves in this format . There are aspects of the paper , including the precise statement of Theorem 5.1 and the definition of the Airy target , that require the reader to visit this material . The proofs of Theorems 4.1 and 4.2 seem fine to me . Unfortunately , the proof of Theorem 5.1 is especially lengthy , so I did not get the opportunity to check each step in detail . However , the general argument appears sound . Overall , I am impressed with this paper and enjoyed reading it . The missing connections to classical ideas in time series analysis are unfortunate , and I would be willing to give the paper an 8 with a wider literature review and improved presentation of Theorem 5.1 . An additional lower bound for Theorem 4.2 would bring it to a 9 at least . But , even in its current form , I recommend this paper for acceptance to ICLR . Other comments : - I believe Theorem 4.1 also follows from the fact that matrix exponential distributions are dense ( see [ 1 ] ) , together with the Riesz-Markov-Kakutani representation theorem used here . - \u201c We first show that the training dynamics of $ \\mathbb { E } _x J_m $ exhibits very interesting behaviors depending on the form of target functionals. \u201d \u2014 This is a little too vague , maybe consider rewording ? - In multiple places : \u201c x being/is the white noise \u201d - > \u201c white noise x \u201d or \u201c x is white noise \u201d . - Check capitalization in the statement under Theorem A.1 . - \u201c Airy \u201d should be capitalized in each appearance . [ 1 ] He , Qi-Ming , and Hanqin Zhang . `` On matrix exponential distributions . '' Advances in Applied Probability 39.1 ( 2007 ) : 271-292 .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "1 . `` Perhaps my biggest disappointment is that while the authors do a good summary of the relevant RNN literature , they have failed to mention any analogous ideas from the time series literature . For example , any discussion regarding Hurst parameters would be welcome , since this is precisely the type of \u201c long-range dependence \u201d discussed here . The lack of references to this literature is disheartening , since these concepts have played a key role in time series analysis for many decades , and it would have been nice to see this acknowledged , or a connection made . '' * We thank the reviewer for the suggestion . We have added some literature review on time series analysis pertaining to memory . We feel that the key difference here is we consider memory of input-output relationship and their interaction with RNN approximation/optimization . On the other hand , these works mainly focus on characterizing the memory structures in the input temporal data themselves . * * We have added discussion on these prior works at the end of section 2 . * * 2 . `` The takeaway from Theorem 4.2 regarding the exponential increase in the required width for fitting a linear RNN is nice , but is undercut by the fact that Theorem 4.2 is an upper bound . I 'm satisfied with the result , but a lower bound for approximating a particular functional would be more convincing . '' * We fully agree with the reviewer 's comment that Theorem 4.2 gives an upper-bound of the approximation error . If we denote the minimum number of terms needed to achieve an $ L^1 $ error $ \\epsilon $ to the target density $ 1/t^ { ( 1+\\omega ) } $ as $ m ( \\omega , \\epsilon ) $ , the estimate derived from Theorem 4.2 shows an upper-bound of $ m ( \\omega , \\epsilon ) $ goes to infinity exponentially fast as $ \\omega\\rightarrow 0^+ $ with fixed $ \\epsilon $ . A stronger result would be that the lower bound of $ m ( \\omega , \\epsilon ) \\rightarrow \\infty $ exponentially fast as $ \\omega\\rightarrow 0^+ $ with fixed $ \\epsilon $ . The divergence of $ m ( \\omega , \\epsilon ) $ is somewhat natural , but the exponential divergence of its lower-bound is not obvious . We have some thoughts on it , but we have not proved it in general so far . This is a point of our future work . * * We have made the point about upper/lower bounds clearer at the end of Section 4 on Page 6 when discussing our estimate . * * 3 . `` I was disappointed with the \u201c informal \u201d presentation here . I can appreciate the attempt to simplify the full statement , but I still had to go to the supplementary material to understand the statement , which I should hope most readers would not have to do . I think a more precise statement than \u201c trapped in a plateau with timescale \u201d would be better . Perhaps something involving the hitting time , or even more simply , .... '' * * * We have improved the precision of Theorem 5.1 and added some explanatory paragraphs of the main proof ideas . * * 4 . `` It is also worth noting that there is an extraordinary amount of supplementary material here , much of which , unfortunately , does not get the attention it probably deserves in this format . There are aspects of the paper , including the precise statement of Theorem 5.1 and the definition of the Airy target , that require the reader to visit this material . The proofs of Theorems 4.1 and 4.2 seem fine to me . Unfortunately , the proof of Theorem 5.1 is especially lengthy , so I did not get the opportunity to check each step in detail . However , the general argument appears sound . '' * We agree that there is a large amount of technical details for the optimization part . This is because any precise analysis of non-convex optimization tends to be very complicated . * * We have tried to improve the overall clarity of the stated results and sketched the main ideas after Theorem 5.1 . We also improved the presentation in the appendix . * * 5 . `` I believe Theorem 4.1 also follows from the fact that matrix exponential distributions are dense ( see [ 1 ] ) , together with the Riesz-Markov-Kakutani representation theorem used here '' * Thank you for the suggested reference . Indeed , this result is related to the density of phase type distributions . Here , we consider signed measures instead of probability measures , but the approach should be generalizable ( e.g.using Jordan decomposition ) . * * We have added some references and stated the connection in the proof of Theorem 4.1 ( Lemma A.2 ) . * * 6 . * * Minor issues are fixed . * *"}, "2": {"review_id": "8Sqhl-nF50-2", "review_text": "This paper reports a mathematical study of approximation properties of linear RNNs . The first part reports a universal approximation theorem , and presents an analysis of how efficient the approximation is . In particular , it is shown that approximating a slowly decaying , power-law temporal filter requires a large number of neurons , a property the authors refer to as the `` curse of memory '' . The second part of the paper examines the dynamics of learning under gradient descent , and gives arguments related to the existence of long plateaus seen in the loss as function of training epochs . The paper is written in a formal mathematical style ( Definitions/Theorems ) . Not being a mathematician , I am not able to assess the formal correctness of the proofs , and I have found some parts not easily accessible . Most importantly , as currently presented the main results seem to be of limited interest to the ICLR community ( see below for details ) . The paper as a whole is probably more appropriate for a more mathematical venue , where the novelty of the proofs may be better appreciated . Strengths : - attempts to put on a rigorous footing various experimental observations on RNN training - possibly novel mathematical derivations of approximation results Concerns : - the novelty of the results presented in the first part is limited . Previous works have reported various universal approximation theorems for RNNs . Several classical works on that topic are not mentioned , eg Doya 1993 , or Maass 2007 . The details of the mathematical derivation may well be novel , but I am not able to judge this aspect . - the fact that a diverging number of exponentials are needed to approximate a power-law function is also well known ; from that perspective the `` curse of memory '' is not very surprising . - I found it difficult to extract key results from the second part on Optimization dynamics .", "rating": "3: Clear rejection", "reply_text": "( Continued ) 3 . `` the fact that a diverging number of exponentials are needed to approximate a power-law function is also well known ; from that perspective the `` curse of memory '' is not very surprising . '' * First , we would like to point out the fact the approximation problem in RNNs can be mathematically understood as the approximation of decaying functions by exponential sums * is * one of our original contributions . In our viewpoint , this is not * a priori * obvious without a precise formulation and detailed analysis , and this has not been discussed previously in the literature to the best of our knowledge . * Second , it is * not * obvious whether a * fixed * power-law function can be efficiently approximated by a sum of exponentials . The answer depends on the type of approximation , and many problems are still open in the approximation theory literature . In Braess and Hackbusch 2005 ( IMA Journal of Numerical Analysis ( 2005 ) 25 , 685-697 ) ) , it is in fact proved that the $ L^ { \\infty } $ ( uniform-in-time ) approximation error of $ 1/t $ through exponential sums decreases like $ O ( \\text { exp } ( -c\\sqrt { m } ) ) $ with the order $ m $ of the exponential sum ( i.e.number of `` hidden nodes '' in our RNN formulation ) . Thus , it is known that the power law function $ 1/t $ * can * be efficiently approximated by exponentials in $ L^\\infty $ . However , in our case , we need $ L^1 $ approximation , and in this case lower bound on the order of approximation error remains unsolved . It is actually guessed in Kammler 1979 ( SIAM Journal on Numerical Analysis ( 1979 ) 16 , 30-45 ) that the $ L^1 $ approximation error to $ 1/t^2 $ , another power law function , should have a similar exponential order , but this is not proved . In all these cases , the statements can be understood as follows : let $ m ( \\epsilon ) $ be the minimum number of terms needed to achieve approximation of order $ \\epsilon $ . Exponential convergence means that $ m ( \\epsilon ) \\sim \\log ( 1/\\epsilon ) $ . This is different from what we need , as discussed next . * Here what we considered on the approximation rates is different from the above results in the literature . We are interested in the approximation error of a * sequence * of functions , namely , the $ L^1 $ approximation error of $ 1/t^ { ( 1+\\omega ) } $ as $ \\omega\\rightarrow 0^+ $ . Denote the minimum number of terms needed to achieve an $ L^1 $ error $ \\epsilon $ as $ m ( \\omega , \\epsilon ) $ . In the paper we show that an upper-bound of $ m ( \\omega , \\epsilon ) $ goes to infinity exponentially fast as $ \\omega\\rightarrow 0^+ $ with fixed $ \\epsilon $ . This is similar to the classical results on the curse of dimensionality in approximation theory , where now $ \\omega $ plays the same role as `` dimension '' . A stronger result would be that the lower bound of $ m ( \\omega , \\epsilon ) \\rightarrow \\infty $ exponentially as $ \\omega\\rightarrow 0^+ $ with fixed $ \\epsilon $ , and this is a point of future work . * * We also made the point about upper/lower bounds clearer at the end of Section 4 on Page 6 when discussing our estimate . * * 4 . `` I found it difficult to extract key results from the second part on Optimization dynamics . '' * The result here is a fine-grained analysis of the optimization landscape and dynamics of a particular class of functionals possessing long memory . These functionals are of the form of an addition of a short term memory part that is easily learned , and a long term memory part that poses difficulty in training ( see.Eq.21 ) .For such functionals , we show that the optimization dynamics can get trapped for exponentially large times near points where the short term memory is learned but the long term one is not . In fact , the trapping time has an exponentially diverging lower bound as the memory increases . The derived estimate can be checked against experiments ( Fig 2 , 3 in Appendix ) , and provably illustrates the difficulty of RNN training as a result of long term memory in the training data . This has been proposed heuristically in many practical works , but here we make it precise , albeit in a idealistic but representative setting . * * We improved the precision of the result ( Theorem 5.1 ) and added a intuitive explanatory paragraph of the main ideas following the proof . * *"}, "3": {"review_id": "8Sqhl-nF50-3", "review_text": "The main contributions of the paper are the following ones ( informal ) : 1 . The approximation theorem of linear functionals with linear RNNs in continuous time settings . The main difference with the previous results is that the class of approximated functions is linear , but does not necessarily come from the same differential equation that describes the class of approximator models . 2.The upper bound on the approximation error for some `` exponentially decaying '' linear functionals , where the upper bound depends on the weights matrix size ( i.e.memory size ) . The memory growth rate is polynomial with respect to approximation error . It is no longer the case when linear functional is not `` decaying exponentially '' and memory growth rate is exponential . 3.The optimization dynamics analysis . In particular , the authors showed that under some conditions the optimization process can be stuck if the `` memory '' of the target functional is large . Overall , it is a good paper and I enjoyed reading it . It is very well written and easy to follow . In many cases , the authors provide clarification for used assumptions . The authors also emphasize the difference between their results and the previous results . Pros : the authors showed that difficulties encountered in practice , where the target functional has long term dependencies , emerge even in simple linear settings and can be explained from a theoretical point of view . Cons : in many settings we are interested in not just recovering some dependencies , but recovering it from the data or recovering the dependency only on some subset of the all possible input sentences . The role of input data is significantly ignored in the given analysis . Several questions and remarks : 1 . The condition on supremum in ( 14 ) seems purely technical ( at least based on provided proof in appendix ) . Could the authors please clarify whether this assumption has some qualitative explanation or can be replaced with stronger but more `` meaningful '' assumptions ( of course it will make the result weaker ) ? 2.In the dynamic analysis x is assumed to be white noise . This assumption seems too restrictive and is used to apply Ito 's isometry theorem . What else stochastic processes can be used here to make this result stronger ? 3.In ( 50 ) in ( - ( alpha + 1 ) / beta ) ^ { i } . i should be replaced with j .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "1 . `` in many settings we are interested in not just recovering some dependencies , but recovering it from the data or recovering the dependency only on some subset of the all possible input sentences . The role of input data is significantly ignored in the given analysis . '' [ Related concern that follows : `` In the dynamic analysis $ x $ is assumed to be white noise . This assumption seems too restrictive and is used to apply Ito 's isometry theorem . What else stochastic processes can be used here to make this result stronger ? `` . We answer both together below . ] * We agree that in our optimization analysis , our assumption of white noise does not highlight the role of specific input sequence distributions . The choice is made for simplicity , since a fine-grained optimization analysis for non-convex landscapes is in general very involved . It is certainly worthwhile to explore the role of input distribution in optimization in the future . * * With that said , we conducted some additional experiments with varying input distributions for our test cases in Fig 4 of the appendix , where we found that even for non-white-noise input ( $ x_t = \\sum_ { j=1 } ^ { J } \\alpha_j \\cos ( \\lambda_j t ) $ where $ \\lambda_j \\sim U [ 0,10 ] $ and $ \\alpha_j \\sim \\mathcal { N } ( 0,1 ) , J=5 $ ) , we still observe similar behavior . The results are reported in the revised appendix . This at least suggests that our setting is generally representative , but we agree that certainly more theoretical work can be done in this direction in the future to make this rigorous . * * 1 . `` The condition on supremum in ( 14 ) seems purely technical ( at least based on provided proof in appendix ) . Could the authors please clarify whether this assumption has some qualitative explanation or can be replaced with stronger but more `` meaningful '' assumptions ( of course it will make the result weaker ) ? '' * This condition can be interpreted as the bound of the function 's derivatives . It essentially requires the derivatives to decay rapidly enough . If $ \\gamma $ is finite , then it gives us a kind of norm . Similar kinds of norms is common in mathematical analysis , for instance , the Sobolev spaces and Besov spaces . 2 . * * Minor issues are fixed . * *"}}