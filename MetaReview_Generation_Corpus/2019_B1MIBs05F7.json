{"year": "2019", "forum": "B1MIBs05F7", "title": "On the Ineffectiveness of Variance Reduced Optimization for Deep Learning", "decision": "Reject", "meta_review": "All reviewers agreed that this paper addresses an important question in deep learning (why doesn't SVRG help for deep learning)? But the paper still has some issues that need to be addressed before publication, thus the AC recommends \"revise and resubmit\".", "reviews": [{"review_id": "B1MIBs05F7-0", "review_text": "This paper presents an analysis of SVRG style methods, which have shown remarkable progress in improving rates of convergence (in theory) for convex and non-convex optimization (Reddi et al 2016). This paper highlights some of the difficulties faced by SVRG in practice, especially for practically relevant deep learning problems. Issues such as dropout, batch norm, data augmentation (random crop/rotation/translations) tend to cause additional issues with regards to increasing bias (and/or variance) to resulting updates. Some fixes are proposed by this paper in order to remove these issues and then reconsider the resulting algorithm's behavior in practice. These fixes appear right headed and the observation about ratio of variances (of stochastic gradients) with or without variance reduction is an interesting way to track benefits of variance reduction. There are some issues that I'd like to raise in the paper's consideration of variance reduction: [1] I'd like more clarification (using an expression or two) to make sure I have the right understanding of the variance estimates computed by the authors for variance reduced stochastic gradient and the routine stochastic gradient that is computed. [2] In any case, the claim that if the ratio of the variance of the gradient computed with/without variance reduction is less than 1/3, thats when effective variance reduction is happening is true only in the regime when the variance (estimation error) dominates the bias (approximation error). At the start of the optimization, the bias is the dominating factor variance reduction isn't really useful. That gets us to the point below. [3] It is also important to note that variance reduction really doesn't matter at the initial stages of learning. This is noticed by the paper, which says that variance reduction doesn't matter when the iterates move rather quickly through the optimization landscape - which is the case when we are at the start of the optimization. In fact, performing SGD over the first few passes/until the error for SGD starts \"bouncing around\" is a very good idea that is recommended in practice (for ex., see the SDCA paper of Shalev-Shwarz and Zhang (2013)). Only when the variance of SGD's iterates starts dominating the initial error, one requires to use one of several possible alternatives, including (a) decaying learning rate or, (b) increasing batch size or, (c) iterate averaging or, (d) variance reduction. Note that, in a similar spirit, Reddi et al. (2016) mention that SVRG is more sensitive to the initial point than SGD for non-convex problems. With these issues in mind, I'd be interested in understanding how variance reduction behaves for all networks once we start at an epoch when SGD's iterates start bouncing around (i.e. the error flattens out). Basically, start out with SGD with a certain step size until the error starts bouncing around, then, switch to SVRG with all the fixes (or without these fixes) proposed by the paper. This will ensure that the variance dominates the error and this is where variance reduction should really matter. Without this comparison, while this paper's results and thoughts are somewhat interesting, the results are inconclusive. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for providing detailed comments . We will respond to your comments individually . 1 ) Yes , we can provide more detail here in our camera ready , although your understanding sounds correct . 2/3 ) We agree completely with your characterization of the error . Variance reduction can certainly not help when variance is not the limiting factor . However , for problems such as imagenet , there is strong evidence to suggest that variance is the limiting factor for the majority of the optimization time . For instance , the seminal imagenet-in-1-hour paper ( https : //arxiv.org/abs/1706.02677 ) established that the learning rate can be increased if the batchsize is also proportionally increased , but only after a warm-up period of 5-10 epochs . This shows that variance rather than curvature is the limiting factor , and that the majority of training occurs in this variance constrained situation . The fact that the learning rate is also commonly decreased at 30 epochs of 90 suggests ( although does n't prove ) that variance is the dominating factor by that point , as learning rate decreases primarily used to decrease variance . 4 ) Regarding the technique of switching to SVRG after using SGD in a warm-up phase , we did investigate this thoroughly in unreported experiments . In no situation were we able to get a non-trivial improvement . Switching to SVRG 160+ epochs into CIFAR-10 training is a viable technique , particularly for the smaller LeNet , however it does n't result in significant convergence rate improvements as the majority of the optimization time is spent in the earlier phases , and the variance reduction for DenseNet/ResNet-100 is too small even at those later stages , as shown in our primary plots in Figure 2 . The amount of variance reduction shown in Figure 2 at 160+ epochs is similar to the variance reduction you would see from using SGD for the earlier epochs then switching over ."}, {"review_id": "B1MIBs05F7-1", "review_text": "This work investigates the applicability of SVGD to modern neural networks and shows the naive application of SVGD typically fail. The authors find that the naive application of batch norm, dropout, and data augmentation deviate significantly from the assumptions of SVGD and variance reduction can fail easily in large nets due to the weight moves quickly in the training. This is a thorough exploration of a well-known algorithm - SVGD in deep neural networks. Although most results indicate that SGVD fails to reduce the variance in training deep neural networks, it might still provide insights for other researchers to improve SVGD algorithm in the context of deep learning. Therefore, I'm inclined to accept this paper. One weakness of this work is that it lacks instructive suggestion of how to improve SVGD in deep neural networks and no solution of variance reduction is given. Finally, I'd like to pose a question: Is it really useful to reduce variance in training deep neural networks? We've proposed tons of regularizers to increase the stochasticity of the gradient (e.g., small-batch training, dropout, Gaussian noise), which have been shown to improve the generalization. I agree optimization is important, but generalization is arguably the ultimate goal for most tasks.", "rating": "6: Marginally above acceptance threshold", "reply_text": "The question you pose is an interesting one . It is definitely the case that additional gradient noise is helpful right at the very beginning of optimization . In practice this phase where the noise is useful lasts for only a few epochs , as has been shown recently in a series of papers ( e.g.https : //arxiv.org/abs/1706.02677 ) applying large-batch ( 8k or greater ) to imagenet . The general result is that smaller batches ( i.e higher variance gradients ) help final validation set error when used for the first ~5 epochs , after which much larger batches can be used without harming generalization performance . We believe that these large-batch training results show quite clearly that there is significant gains to be had with variance reduction , as most of the training time is spent in the remaining 85 epochs ."}, {"review_id": "B1MIBs05F7-2", "review_text": "This paper is thorough and well-written. On first look, the paper seems to be addressing a topic that I believe is already well-known in the DL community that has typically been explained by memory constraints or light empirical evidence. However, a more in-depth reading of this paper shows that the authors provide a serious attempt at implementing SVRG methods. This is demonstrated by their detailed implementation that attempts to overcome the main practical algorithmic concerns for neural networks (which may be beneficial even in the implementation of other optimization algorithms for deep learning) and their in-depth experiments that give concrete evidence towards a reasonable explanation of why SVRG methods currently do not work for deep learning. In particular, they claim that because the SVRG estimator fails to significantly decrease the variance, the increased computation is not worthwhile in improving the efficiency of the algorithm. Because the empirical study is fairly in-depth and thorough and the paper itself is well-written (particularly for DL), I\u2019m more inclined to accept the paper; however, I still do not believe that the explanation is significant and novel enough to be worthy of publication, as I will explain below. 1. Performance of SVRG methods in convex setting It is fairly well-known that even in the convex optimization community (training logistic regression), SVRG methods fail to improve the performance of SG in the initial phase; see [1, 5]. Often, the improvements in the algorithm are seen in the later phases of the algorithm, once the iterates is sufficiently close to the solution and the linear convergence rate kicks in. The experiments for neural networks presented here seem to corroborate this; in particular, the variance reduction introduces an additional cost but without much benefit in the main initial phase (epochs up to 150). One also typically observes (in the convex setting) very little difference in test error between SG and SVRG methods since it is unnecessary to train logistic regression to a lower error for the test error to stabilize. Hence, the test error results in the neural network setting feels unsurprising. 2. Comments/questions on experiments (i) Batch Normalization: When you were obtaining poor results and divergence with applying SVRG directly with training mode on, what batch size were you using for the batch normalization? In particular, did you use full batch when computing the batch norm statistics at the snapshot point? Did you try fixing the batch normalization \u201cghost batch size\u201d [4]? (ii) Measuring Variance Reduction: When training the models, what other hyperparameters were tried? Was SVRG sensitive to the choices in hyperparameters? What happened when momentum was not used? How did the training loss behave? It may also be good to mention which datasets were used since these networks have been applied to a wider set of datasets. (iii) Streaming SVRG Variants: Have you considered SVRG with batching, as proposed in [3]? (iv) Convergence Rate Comparisons: Is it reasonable to use the same hyperparameter settings for all of the methods? One would expect that each method needs to be tuned independently; otherwise, this may indicate that SVRG/SCSG and the SG method are so similar that they can all be treated the same as the SG method. 3. Generalization For neural networks, the question of generalization is almost as important as finding a minimizer efficiently, which is not addressed in-depth in this paper. The SG method benefits from treating both the empirical risk and expected risk problems \u201cequally\u201d, whereas SVRG suffers from utilizing this finite-sum/full-batch structure, which may potentially lead to deficiencies in the testing error. In light of this, I would suggest the authors investigate more carefully the generalization properties of the solutions of SVRG methods for neural networks. This may be highly relevant to the work on large-batch training; see [6]. Summary: Overall, the paper is quite thorough and well-written, particularly for deep learning. However, the paper still lacks enough content and novelty, in my opinion, to warrant acceptance. They appeal to a simple empirical investigation of the variance as supportive evidence for their claim; if the paper had some stronger mathematical justification specific for neural networks demonstrating why the theory does not hold in this case, the paper would be a clear accept. For these reasons, I have given a weak reject. A response addressing my concerns above and emphasizing the novelty of these results for neural networks may push me the other way. References: [1] Bollapragada, Raghu, et al. \"A progressive batching L-BFGS method for machine learning.\" arXiv preprint arXiv:1802.05374(2018). [2] Friedlander, Michael P., and Mark Schmidt. \"Hybrid deterministic-stochastic methods for data fitting.\" SIAM Journal on Scientific Computing 34.3 (2012): A1380-A1405. [3] Harikandeh, Reza, et al. \"Stopwasting my gradients: Practical svrg.\" Advances in Neural Information Processing Systems. 2015. [4] Hoffer, Elad, Itay Hubara, and Daniel Soudry. \"Train longer, generalize better: closing the generalization gap in large batch training of neural networks.\" Advances in Neural Information Processing Systems. 2017. [5] Johnson, Rie, and Tong Zhang. \"Accelerating stochastic gradient descent using predictive variance reduction.\" Advances in neural information processing systems. 2013. [6] Keskar, Nitish Shirish, et al. \"On large-batch training for deep learning: Generalization gap and sharp minima.\" arXiv preprint arXiv:1609.04836 (2016). [7] Smith, Samuel L., Pieter-Jan Kindermans, and Quoc V. Le. \"Don't Decay the Learning Rate, Increase the Batch Size.\" arXiv preprint arXiv:1711.00489 (2017).", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the detailed comments ! We would like to elaborate a little on the significance of our contribution in the hopes you will change your mind . We strongly believe that a paper such as ours that points out a problem in a well-known existing method should be publishable . Currently , other researchers will end up duplicating effort in trying the promising SVRG method on deep learning problems , only to find that it does n't work . By exploring this in depth , and pointing out how existing papers testing it on tiny networks such as LeNet are misleading , we provide a valuable contribution that can potentially spur future research into SVRG variants that can work for neural network training . We will also address specific comments of yours individually : 1 ) It is definitely the case that typical convex logistic regression test problems show relatively little test loss advantage when using SVRG , compared to the rapid train loss convergence . We do n't believe that this is always the case though , and certainly the theory does n't rule out faster test loss convergence , particularly if the optimization problem is badly conditioned . So it is not aprior given that SVRG should fail here . 2 ) i ) We used batch-size 128 which is typical for these problems both at training , eval time and at the snapshot point . Ghost batch norm is recommended when training for much larger batches , and typically it is configured to have the same statistic variances as batch-size 128 or 256 , so it did not seem sensible to us to use it here . ii ) We did an extensive set of experiments covering learning rate , momentum and weight decay . In the end we found that SVRG behaved essentially the same as SGD with respect to these parameters . We can update the paper to include a comment to this effect if you wish . iii ) We have not tried the exact prcedure described in that paper for minibatching . It looks to rely on the Lipschitz constant heavily , which is hard to work with for non-smooth optimization problems such as ReLU neural networks . iv ) As mentioned in ( ii ) , we did try other parameter settings and we found that using the same settings actually did work the best . We should definitely elaborate more on this in the camera ready , as one of the other reviewers commented on this aspect as well . 3 ) Generalization with SVRG is certainly an interesting research direction ."}], "0": {"review_id": "B1MIBs05F7-0", "review_text": "This paper presents an analysis of SVRG style methods, which have shown remarkable progress in improving rates of convergence (in theory) for convex and non-convex optimization (Reddi et al 2016). This paper highlights some of the difficulties faced by SVRG in practice, especially for practically relevant deep learning problems. Issues such as dropout, batch norm, data augmentation (random crop/rotation/translations) tend to cause additional issues with regards to increasing bias (and/or variance) to resulting updates. Some fixes are proposed by this paper in order to remove these issues and then reconsider the resulting algorithm's behavior in practice. These fixes appear right headed and the observation about ratio of variances (of stochastic gradients) with or without variance reduction is an interesting way to track benefits of variance reduction. There are some issues that I'd like to raise in the paper's consideration of variance reduction: [1] I'd like more clarification (using an expression or two) to make sure I have the right understanding of the variance estimates computed by the authors for variance reduced stochastic gradient and the routine stochastic gradient that is computed. [2] In any case, the claim that if the ratio of the variance of the gradient computed with/without variance reduction is less than 1/3, thats when effective variance reduction is happening is true only in the regime when the variance (estimation error) dominates the bias (approximation error). At the start of the optimization, the bias is the dominating factor variance reduction isn't really useful. That gets us to the point below. [3] It is also important to note that variance reduction really doesn't matter at the initial stages of learning. This is noticed by the paper, which says that variance reduction doesn't matter when the iterates move rather quickly through the optimization landscape - which is the case when we are at the start of the optimization. In fact, performing SGD over the first few passes/until the error for SGD starts \"bouncing around\" is a very good idea that is recommended in practice (for ex., see the SDCA paper of Shalev-Shwarz and Zhang (2013)). Only when the variance of SGD's iterates starts dominating the initial error, one requires to use one of several possible alternatives, including (a) decaying learning rate or, (b) increasing batch size or, (c) iterate averaging or, (d) variance reduction. Note that, in a similar spirit, Reddi et al. (2016) mention that SVRG is more sensitive to the initial point than SGD for non-convex problems. With these issues in mind, I'd be interested in understanding how variance reduction behaves for all networks once we start at an epoch when SGD's iterates start bouncing around (i.e. the error flattens out). Basically, start out with SGD with a certain step size until the error starts bouncing around, then, switch to SVRG with all the fixes (or without these fixes) proposed by the paper. This will ensure that the variance dominates the error and this is where variance reduction should really matter. Without this comparison, while this paper's results and thoughts are somewhat interesting, the results are inconclusive. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for providing detailed comments . We will respond to your comments individually . 1 ) Yes , we can provide more detail here in our camera ready , although your understanding sounds correct . 2/3 ) We agree completely with your characterization of the error . Variance reduction can certainly not help when variance is not the limiting factor . However , for problems such as imagenet , there is strong evidence to suggest that variance is the limiting factor for the majority of the optimization time . For instance , the seminal imagenet-in-1-hour paper ( https : //arxiv.org/abs/1706.02677 ) established that the learning rate can be increased if the batchsize is also proportionally increased , but only after a warm-up period of 5-10 epochs . This shows that variance rather than curvature is the limiting factor , and that the majority of training occurs in this variance constrained situation . The fact that the learning rate is also commonly decreased at 30 epochs of 90 suggests ( although does n't prove ) that variance is the dominating factor by that point , as learning rate decreases primarily used to decrease variance . 4 ) Regarding the technique of switching to SVRG after using SGD in a warm-up phase , we did investigate this thoroughly in unreported experiments . In no situation were we able to get a non-trivial improvement . Switching to SVRG 160+ epochs into CIFAR-10 training is a viable technique , particularly for the smaller LeNet , however it does n't result in significant convergence rate improvements as the majority of the optimization time is spent in the earlier phases , and the variance reduction for DenseNet/ResNet-100 is too small even at those later stages , as shown in our primary plots in Figure 2 . The amount of variance reduction shown in Figure 2 at 160+ epochs is similar to the variance reduction you would see from using SGD for the earlier epochs then switching over ."}, "1": {"review_id": "B1MIBs05F7-1", "review_text": "This work investigates the applicability of SVGD to modern neural networks and shows the naive application of SVGD typically fail. The authors find that the naive application of batch norm, dropout, and data augmentation deviate significantly from the assumptions of SVGD and variance reduction can fail easily in large nets due to the weight moves quickly in the training. This is a thorough exploration of a well-known algorithm - SVGD in deep neural networks. Although most results indicate that SGVD fails to reduce the variance in training deep neural networks, it might still provide insights for other researchers to improve SVGD algorithm in the context of deep learning. Therefore, I'm inclined to accept this paper. One weakness of this work is that it lacks instructive suggestion of how to improve SVGD in deep neural networks and no solution of variance reduction is given. Finally, I'd like to pose a question: Is it really useful to reduce variance in training deep neural networks? We've proposed tons of regularizers to increase the stochasticity of the gradient (e.g., small-batch training, dropout, Gaussian noise), which have been shown to improve the generalization. I agree optimization is important, but generalization is arguably the ultimate goal for most tasks.", "rating": "6: Marginally above acceptance threshold", "reply_text": "The question you pose is an interesting one . It is definitely the case that additional gradient noise is helpful right at the very beginning of optimization . In practice this phase where the noise is useful lasts for only a few epochs , as has been shown recently in a series of papers ( e.g.https : //arxiv.org/abs/1706.02677 ) applying large-batch ( 8k or greater ) to imagenet . The general result is that smaller batches ( i.e higher variance gradients ) help final validation set error when used for the first ~5 epochs , after which much larger batches can be used without harming generalization performance . We believe that these large-batch training results show quite clearly that there is significant gains to be had with variance reduction , as most of the training time is spent in the remaining 85 epochs ."}, "2": {"review_id": "B1MIBs05F7-2", "review_text": "This paper is thorough and well-written. On first look, the paper seems to be addressing a topic that I believe is already well-known in the DL community that has typically been explained by memory constraints or light empirical evidence. However, a more in-depth reading of this paper shows that the authors provide a serious attempt at implementing SVRG methods. This is demonstrated by their detailed implementation that attempts to overcome the main practical algorithmic concerns for neural networks (which may be beneficial even in the implementation of other optimization algorithms for deep learning) and their in-depth experiments that give concrete evidence towards a reasonable explanation of why SVRG methods currently do not work for deep learning. In particular, they claim that because the SVRG estimator fails to significantly decrease the variance, the increased computation is not worthwhile in improving the efficiency of the algorithm. Because the empirical study is fairly in-depth and thorough and the paper itself is well-written (particularly for DL), I\u2019m more inclined to accept the paper; however, I still do not believe that the explanation is significant and novel enough to be worthy of publication, as I will explain below. 1. Performance of SVRG methods in convex setting It is fairly well-known that even in the convex optimization community (training logistic regression), SVRG methods fail to improve the performance of SG in the initial phase; see [1, 5]. Often, the improvements in the algorithm are seen in the later phases of the algorithm, once the iterates is sufficiently close to the solution and the linear convergence rate kicks in. The experiments for neural networks presented here seem to corroborate this; in particular, the variance reduction introduces an additional cost but without much benefit in the main initial phase (epochs up to 150). One also typically observes (in the convex setting) very little difference in test error between SG and SVRG methods since it is unnecessary to train logistic regression to a lower error for the test error to stabilize. Hence, the test error results in the neural network setting feels unsurprising. 2. Comments/questions on experiments (i) Batch Normalization: When you were obtaining poor results and divergence with applying SVRG directly with training mode on, what batch size were you using for the batch normalization? In particular, did you use full batch when computing the batch norm statistics at the snapshot point? Did you try fixing the batch normalization \u201cghost batch size\u201d [4]? (ii) Measuring Variance Reduction: When training the models, what other hyperparameters were tried? Was SVRG sensitive to the choices in hyperparameters? What happened when momentum was not used? How did the training loss behave? It may also be good to mention which datasets were used since these networks have been applied to a wider set of datasets. (iii) Streaming SVRG Variants: Have you considered SVRG with batching, as proposed in [3]? (iv) Convergence Rate Comparisons: Is it reasonable to use the same hyperparameter settings for all of the methods? One would expect that each method needs to be tuned independently; otherwise, this may indicate that SVRG/SCSG and the SG method are so similar that they can all be treated the same as the SG method. 3. Generalization For neural networks, the question of generalization is almost as important as finding a minimizer efficiently, which is not addressed in-depth in this paper. The SG method benefits from treating both the empirical risk and expected risk problems \u201cequally\u201d, whereas SVRG suffers from utilizing this finite-sum/full-batch structure, which may potentially lead to deficiencies in the testing error. In light of this, I would suggest the authors investigate more carefully the generalization properties of the solutions of SVRG methods for neural networks. This may be highly relevant to the work on large-batch training; see [6]. Summary: Overall, the paper is quite thorough and well-written, particularly for deep learning. However, the paper still lacks enough content and novelty, in my opinion, to warrant acceptance. They appeal to a simple empirical investigation of the variance as supportive evidence for their claim; if the paper had some stronger mathematical justification specific for neural networks demonstrating why the theory does not hold in this case, the paper would be a clear accept. For these reasons, I have given a weak reject. A response addressing my concerns above and emphasizing the novelty of these results for neural networks may push me the other way. References: [1] Bollapragada, Raghu, et al. \"A progressive batching L-BFGS method for machine learning.\" arXiv preprint arXiv:1802.05374(2018). [2] Friedlander, Michael P., and Mark Schmidt. \"Hybrid deterministic-stochastic methods for data fitting.\" SIAM Journal on Scientific Computing 34.3 (2012): A1380-A1405. [3] Harikandeh, Reza, et al. \"Stopwasting my gradients: Practical svrg.\" Advances in Neural Information Processing Systems. 2015. [4] Hoffer, Elad, Itay Hubara, and Daniel Soudry. \"Train longer, generalize better: closing the generalization gap in large batch training of neural networks.\" Advances in Neural Information Processing Systems. 2017. [5] Johnson, Rie, and Tong Zhang. \"Accelerating stochastic gradient descent using predictive variance reduction.\" Advances in neural information processing systems. 2013. [6] Keskar, Nitish Shirish, et al. \"On large-batch training for deep learning: Generalization gap and sharp minima.\" arXiv preprint arXiv:1609.04836 (2016). [7] Smith, Samuel L., Pieter-Jan Kindermans, and Quoc V. Le. \"Don't Decay the Learning Rate, Increase the Batch Size.\" arXiv preprint arXiv:1711.00489 (2017).", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the detailed comments ! We would like to elaborate a little on the significance of our contribution in the hopes you will change your mind . We strongly believe that a paper such as ours that points out a problem in a well-known existing method should be publishable . Currently , other researchers will end up duplicating effort in trying the promising SVRG method on deep learning problems , only to find that it does n't work . By exploring this in depth , and pointing out how existing papers testing it on tiny networks such as LeNet are misleading , we provide a valuable contribution that can potentially spur future research into SVRG variants that can work for neural network training . We will also address specific comments of yours individually : 1 ) It is definitely the case that typical convex logistic regression test problems show relatively little test loss advantage when using SVRG , compared to the rapid train loss convergence . We do n't believe that this is always the case though , and certainly the theory does n't rule out faster test loss convergence , particularly if the optimization problem is badly conditioned . So it is not aprior given that SVRG should fail here . 2 ) i ) We used batch-size 128 which is typical for these problems both at training , eval time and at the snapshot point . Ghost batch norm is recommended when training for much larger batches , and typically it is configured to have the same statistic variances as batch-size 128 or 256 , so it did not seem sensible to us to use it here . ii ) We did an extensive set of experiments covering learning rate , momentum and weight decay . In the end we found that SVRG behaved essentially the same as SGD with respect to these parameters . We can update the paper to include a comment to this effect if you wish . iii ) We have not tried the exact prcedure described in that paper for minibatching . It looks to rely on the Lipschitz constant heavily , which is hard to work with for non-smooth optimization problems such as ReLU neural networks . iv ) As mentioned in ( ii ) , we did try other parameter settings and we found that using the same settings actually did work the best . We should definitely elaborate more on this in the camera ready , as one of the other reviewers commented on this aspect as well . 3 ) Generalization with SVRG is certainly an interesting research direction ."}}