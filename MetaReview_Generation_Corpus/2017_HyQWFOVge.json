{"year": "2017", "forum": "HyQWFOVge", "title": "Significance of Softmax-Based Features over Metric Learning-Based Features", "decision": "Reject", "meta_review": "The paper aims to compare the representations learnt by metric learning and classification objectives. While this is an interesting topic, the presented evaluation is not sufficiently clear for the paper to be accepted.", "reviews": [{"review_id": "HyQWFOVge-0", "review_text": "I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al DML method. However, does it support the claim that softmax-based features work much better than DML learned features? I have doubts on this claim. Also the experiments are a little bit misleading. What is vanilla googleNet softmax finetuned results? It seems it is not Rippel et al. (softmax prob) result. I am wondering whether the improvement comes from a) using retrieval (nearest neighbor) for classification or b) adding a new layer on top of pool5 or c) L2 normalization of the features. It is not clear to me at all. It appears to me the comparison is not apple vs apple between the proposed method and Rippel et al. It would be great if we know adding feature reduction or adding another layer on top of pool5 can improve finetued softmax result. However, I am not sure what is the biggest contributing factor to the superior results. Before getting more clarifications from the authors, I lean toward rejection. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . This is a paper that shows the equitable comparisons between softmax features and DML features under the same networks , which leads to clarification of performance of softmax based feature embedding . It was not clear in the previous studies . Let us clarify questions you raised in the comments . a ) We used 1-NN classification for fair comparison to Rippel . Note that nearest neighbor is used for classification , such as Nearest Class Mean ( Mensink et al. , 2013 ) . We think the difference between classification and retrieval is whether the class sets of the training data and test data are the same or not . b ) In section 4.2 and 4.3 , we used vanilla GoogLeNet w/ batch normalization . We 'll revise to specify the network architectures . c ) We show w/ and w/o L2 normalization in our results . For Magnet loss , we note that L2 normalization reduces the performance . This is a natural result we can expect from Fig 1 . The reason of our improvement of softmax is the strategy of fine-tuning ( and this is only difference from Rippel 's softmax result ) . Rippel used the networks pretrained on ImageNet for only three epochs , and we used those of fully pretrained , as we described in our paper ."}, {"review_id": "HyQWFOVge-1", "review_text": "I have a huge, big picture concern about this paper and the papers it most closely addresses (MagnetLoss and Lifted Feature Structure Embedding). I don't understand why Distance Metric Learning (DML) is being used for classification tasks (Stanford Cars 196, UCSD Birds 200, Oxford 102 flowers, Stanford Dogs, ImageNet attributes, etc). As far as I can tell, there is really only a single \"retrieval\"-like benchmark being used here - the Stanford Online Products database. All the other datasets are used in a \"classification-by-retrieval\" approach which seems contrived. While ostensibly evaluating \"retrieval\", the retrieval ground truth is totally defined by category membership so these are still classification tasks with many instances in each category. With the Online Products dataset the correspondence between queries and correct results is much more fine grained so it makes sense to think of it as a retrieval task. It seems obvious that if your task is classification, a network trained with a classification loss will be best. Even when these datasets are used in a \"retrieval\" setting, the ground truth is still defined by category membership. It's still a classification task. I don't really see the point of using DML in these scenarios. I guess prior work claims to outperform SoftMax in these settings so this paper is fighting back against this and I should be thankful for this paper. But I think this paper's narrative is a bit off. The narrative shouldn't be \"We can get good retrieval features from softmax networks with appropriate normalization\". It should be \"It never made sense to train or evaluate these things as retrieval tasks. Direct classification is better\". For example, why are you taking the second to last layer or pool5 layer from these networks? Why aren't you taking the last layer? That should do well in these evaluations, right? Table 1 and 2 do show that using softmax probabilities directly tends to be better than doing classification-by-retrieval (works better or the same as doing retrieval with an earlier layer of features, except on Oxford flowers). GoogLeNet is quite deep and gets auxiliary supervision. By the second-to-last layer of the network, the activations could look a lot like category membership already. And category membership is all that's needed for the tasks in 4.2 and 4.3. I don't think my pre-review question was adequately addressed. I was getting at this concern by pointing out numerous scenarios where distance metric learning makes sense because you have fine-grained associations between instances at training time, NOT categorical associations -- e.g. this product photo corresponds to this photo of the object in a scene [Bell et al. 2015], this 3d model correspond to this sketch [Wang et al. 2015], this sketch corresponds to this photo [Sangkloy et al. 2016], this ground view corresponds to this aerial view [Lin et al., 2015]. DeepFace and follow-up works on LFW could also fit into this space because there are few training samples per class (few training samples per person identity). You cite DeepFace and Bell et al. 2015 but you don't compare on those benchmarks. I think those are exactly the tasks where DML makes sense. Maybe the \"retrieval on classification datasets\" would be a reasonable benchmark if the test and train classes were completely different. Then you could argue that softmax is learning a useful representation yet the last layer isn't directly useful since the categories change. But that's not the case here, is it? With all of this said, I'm not sure whether I'm positive or negative about this paper. I think you're onto something significant -- people have been using DML where it is not appropriate -- but addressed it in the wrong way -- by using softmax for \"classification by retrieval\". But you don't need to do retrieval! Softmax is already telling you the class prediction! Why go through the extra step of finding nearest neighbors with some intermediate feature? AnonReviewer3 also raises some good points and you should be thankful that a reviewer is willing to dig so deep to help make your experiments sound! I don't think his/her concerns are disqualifying for this paper, though, as long as it is fixed. I look forward to hearing your response. I want this paper to be published, but I think it needs to be tweaked. ", "rating": "7: Good paper, accept", "reply_text": "We thank you for your helpful review . 1 ) classification and retrieval We think there might be some misunderstanding about our experimental settings . In section 4.2 we evaluated the classification performances , as you pointed out , and it was not surprising that softmax-based features outperformed DML-based features . However , in section 4.3 `` we used only the images and their class labels during our training of the softmax classifier and did not use attributes '' ( described in our paper ) . And in section 4.4 , the classes of training data are completely different from those of test data , as in table 3 . Therefore we evaluated the retrieval ( clustering ) performance , not the classification performance . 2 ) comparison to DML We agree with that DML is suitable in cases of non-categorical problems ( e.g.Bell+ , Wang+ , etc . ) or problems with few training samples . Our main concern is `` learning beneficial deep features '' , i.e.useful for such as retrieval , clustering , one-shot and zero-shot learning . In these problems , we can often use class labels during the training , however the classes of test data is different from those of training . Therefore we can not use class prediction and extracting useful intermediate features is important for these problems . In section 4.4 , we showed that softmax-based features are superior to DML-based features in image retrieval and clustering if we have enough training samples . Even in the case of few training samples , softmax-based features achieved comparable performance in the OP dataset , which contains only about 5.5 training samples per class . 3 ) about Learning visual product design [ Bell+ , 2015 ] , DeepFace [ Taigman+ , 2014 ] , facenet [ Schroff+ , 2015 ] As far as we know , these studies used private datasets for training . Moreover , they used simple siamese or triplet based networks . Therefore we compared with more sophisticated methods , Magnet loss and Lifted structed feature embedding , on publicly available datasets ."}, {"review_id": "HyQWFOVge-2", "review_text": "There has been substantial recent interest in representation learning, and specifically, using distance metric learning (DML) to learn representations where semantic distance between inputs can be measured. This is a topic of particular relevance / interest to ICLR. This paper poses a simple yet provocative question: can a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches? Thorough experiments seem to indicate that this is indeed the case. Comparisons are made to recent DML approaches including Magnet Loss (ICLR2016) and Lifted structure embedding (CVPR2016) and superior results are shown across a number of datasets / tasks for which the DML approaches were designed. This main result is a bit surprising since SoftMax is a natural and trivial baseline, so it should have been properly evaluated in previous DML literature. The authors argue that previous approaches did not fully/properly tune the softmax baselines, or that comparisons were not apples-to-apples. Also, one change in the current paper is the addition of L2 normalization, which is well motivated and helps improve SoftMax feature distances. Different dimensionality reduction approaches are also tested. These changes are minor, but especially the L2 normalization proves to be a simple but effective improvement for SoftMax features. A big issue is with how pre-training is performed (in Magnet Loss the softmax baselines were pretrained for less time on ImageNet). The approach taken here is reasonable, but so is the approach in Magnet Loss (for different reasons). Ultimately, both are fine. Unfortunately, due to use of different schemes, the results are not comparable. Let me copy-paste what I wrote in an earlier comment: My main concern about the paper is that the comparisons in Tables 1 and 2 and Figure 4 to Rippel et al. are not apples-to-apples. Basically, the papers shows that absolute results of using SoftMax w full pre-training (PT) on ImageNet is superior to any of the results in Rippel's paper (including both the Softmax and Magnet results). But as the current results show, PT appears to be critical to obtaining such good numbers - The Rippel SoftMax numbers use only 3 PT stages and are dramatically worse than the full PT on ImageNet. As it stands, I am not convinced that SoftMax is actually better than Magnet. Here is the evidence we have (I'll use Stanford Dogs as an example, but any of the datasets have the same conclusion): (1) Softmax w 3 stages of PT: 26.6% (from Rippel paper) and 32.7% (from authors' reproduction) (2) Magnet w 3 stages of PT: 24.9% (3) Softmax w full PT: 18.3% (4) Magnet w full PT: not shown From this all I see is that PT is critical for getting absolute good results. However, what about Magnet w full PT? These results are not shown either here or in the original Rippel paper (I went back and looked). As such, I do not think it is justifiable to claim superiority of Softmax to Magnet based on available evidence. (Note: I looked back carefully at Rippel's paper, and it appears that the authors use 3 PT stages as a form of \"warmup\". There is a statement that using full PT would \"defeat the purpose of pursuing DML\". I'm not sure if I agree w Rippel's statement since in the present paper there is clear evidence that full PT is hugely helpful, at least for softmax. That being said, I did not see any evidence in the Rippel paper that PT is harmful or that DML wouldn't work with full PT.) The authors responded to my concern by claiming that \u201cfrom Rippel's results, it is no doubt that Magnet@3epochPT > Magnet@FullPT and Magnet@3epoch > Softmax@3epochPT.\u201d However, I went back to Rippel\u2019s paper, and simply the Magnet@FullPT experiment never appears. I further went and contacted Oren Rippel himself, and he verified he never ran the Magnet@FullPT experiment. I encourage the authors to contact Oren Rippel regarding this if they wish to verify (I have asked Oren Rippel to not reveal my identity). [Disclaimer: I am NOT Oren Rippel]. The authors mentioned that they are training Magnet@FullPT. If results were shown for Magnet@FullPT and also retrain the Magnet@3epochPT as a sanity check, that would help alleviate this concern. Alternatively, the language in Section 4 and the Tables could be altered to make clear that the methods use different pretraining and hence are not comparable. Overall, I am actually quite sympathetic to this work. I think it could serve as an important sanity-check paper for the community and quite relevant to ICLR. Having proper and strong SoftMax baselines should prove quite useful to the DML community and to this line of work. However, currently I find the main results (table 1, table 2, figure 4, etc.) to be misleading. If indeed it were the case that Magnet@3epochPT > Magnet@FullPT, then it would be fine. However, at this point as far as I know no one has actually tried Magnet@FullPT. And, given the general importance and effectiveness of pre-training, especially when transferring to small dataset, I would be hugely surprised if Magnet@FullPT was not superior by a large margin. I think either having this experiment in place or altering the writing / presentation of the results would be critical to allow for publishing.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review , We 're still doing experiments of Magnet loss . We tried Magnet @ fullPT and Magnet @ 3epochPT for comparison , but it reaches lower accuracies than those in Rippel 's paper . ( For example , we got about 25 % classification error on Oxford-102Flowers dataset for both Magnet @ 3epochPT and Magnet @ FullPT . ) We have sent an email to Rippel , and waiting for his reply . Please wait for a while ."}], "0": {"review_id": "HyQWFOVge-0", "review_text": "I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al DML method. However, does it support the claim that softmax-based features work much better than DML learned features? I have doubts on this claim. Also the experiments are a little bit misleading. What is vanilla googleNet softmax finetuned results? It seems it is not Rippel et al. (softmax prob) result. I am wondering whether the improvement comes from a) using retrieval (nearest neighbor) for classification or b) adding a new layer on top of pool5 or c) L2 normalization of the features. It is not clear to me at all. It appears to me the comparison is not apple vs apple between the proposed method and Rippel et al. It would be great if we know adding feature reduction or adding another layer on top of pool5 can improve finetued softmax result. However, I am not sure what is the biggest contributing factor to the superior results. Before getting more clarifications from the authors, I lean toward rejection. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . This is a paper that shows the equitable comparisons between softmax features and DML features under the same networks , which leads to clarification of performance of softmax based feature embedding . It was not clear in the previous studies . Let us clarify questions you raised in the comments . a ) We used 1-NN classification for fair comparison to Rippel . Note that nearest neighbor is used for classification , such as Nearest Class Mean ( Mensink et al. , 2013 ) . We think the difference between classification and retrieval is whether the class sets of the training data and test data are the same or not . b ) In section 4.2 and 4.3 , we used vanilla GoogLeNet w/ batch normalization . We 'll revise to specify the network architectures . c ) We show w/ and w/o L2 normalization in our results . For Magnet loss , we note that L2 normalization reduces the performance . This is a natural result we can expect from Fig 1 . The reason of our improvement of softmax is the strategy of fine-tuning ( and this is only difference from Rippel 's softmax result ) . Rippel used the networks pretrained on ImageNet for only three epochs , and we used those of fully pretrained , as we described in our paper ."}, "1": {"review_id": "HyQWFOVge-1", "review_text": "I have a huge, big picture concern about this paper and the papers it most closely addresses (MagnetLoss and Lifted Feature Structure Embedding). I don't understand why Distance Metric Learning (DML) is being used for classification tasks (Stanford Cars 196, UCSD Birds 200, Oxford 102 flowers, Stanford Dogs, ImageNet attributes, etc). As far as I can tell, there is really only a single \"retrieval\"-like benchmark being used here - the Stanford Online Products database. All the other datasets are used in a \"classification-by-retrieval\" approach which seems contrived. While ostensibly evaluating \"retrieval\", the retrieval ground truth is totally defined by category membership so these are still classification tasks with many instances in each category. With the Online Products dataset the correspondence between queries and correct results is much more fine grained so it makes sense to think of it as a retrieval task. It seems obvious that if your task is classification, a network trained with a classification loss will be best. Even when these datasets are used in a \"retrieval\" setting, the ground truth is still defined by category membership. It's still a classification task. I don't really see the point of using DML in these scenarios. I guess prior work claims to outperform SoftMax in these settings so this paper is fighting back against this and I should be thankful for this paper. But I think this paper's narrative is a bit off. The narrative shouldn't be \"We can get good retrieval features from softmax networks with appropriate normalization\". It should be \"It never made sense to train or evaluate these things as retrieval tasks. Direct classification is better\". For example, why are you taking the second to last layer or pool5 layer from these networks? Why aren't you taking the last layer? That should do well in these evaluations, right? Table 1 and 2 do show that using softmax probabilities directly tends to be better than doing classification-by-retrieval (works better or the same as doing retrieval with an earlier layer of features, except on Oxford flowers). GoogLeNet is quite deep and gets auxiliary supervision. By the second-to-last layer of the network, the activations could look a lot like category membership already. And category membership is all that's needed for the tasks in 4.2 and 4.3. I don't think my pre-review question was adequately addressed. I was getting at this concern by pointing out numerous scenarios where distance metric learning makes sense because you have fine-grained associations between instances at training time, NOT categorical associations -- e.g. this product photo corresponds to this photo of the object in a scene [Bell et al. 2015], this 3d model correspond to this sketch [Wang et al. 2015], this sketch corresponds to this photo [Sangkloy et al. 2016], this ground view corresponds to this aerial view [Lin et al., 2015]. DeepFace and follow-up works on LFW could also fit into this space because there are few training samples per class (few training samples per person identity). You cite DeepFace and Bell et al. 2015 but you don't compare on those benchmarks. I think those are exactly the tasks where DML makes sense. Maybe the \"retrieval on classification datasets\" would be a reasonable benchmark if the test and train classes were completely different. Then you could argue that softmax is learning a useful representation yet the last layer isn't directly useful since the categories change. But that's not the case here, is it? With all of this said, I'm not sure whether I'm positive or negative about this paper. I think you're onto something significant -- people have been using DML where it is not appropriate -- but addressed it in the wrong way -- by using softmax for \"classification by retrieval\". But you don't need to do retrieval! Softmax is already telling you the class prediction! Why go through the extra step of finding nearest neighbors with some intermediate feature? AnonReviewer3 also raises some good points and you should be thankful that a reviewer is willing to dig so deep to help make your experiments sound! I don't think his/her concerns are disqualifying for this paper, though, as long as it is fixed. I look forward to hearing your response. I want this paper to be published, but I think it needs to be tweaked. ", "rating": "7: Good paper, accept", "reply_text": "We thank you for your helpful review . 1 ) classification and retrieval We think there might be some misunderstanding about our experimental settings . In section 4.2 we evaluated the classification performances , as you pointed out , and it was not surprising that softmax-based features outperformed DML-based features . However , in section 4.3 `` we used only the images and their class labels during our training of the softmax classifier and did not use attributes '' ( described in our paper ) . And in section 4.4 , the classes of training data are completely different from those of test data , as in table 3 . Therefore we evaluated the retrieval ( clustering ) performance , not the classification performance . 2 ) comparison to DML We agree with that DML is suitable in cases of non-categorical problems ( e.g.Bell+ , Wang+ , etc . ) or problems with few training samples . Our main concern is `` learning beneficial deep features '' , i.e.useful for such as retrieval , clustering , one-shot and zero-shot learning . In these problems , we can often use class labels during the training , however the classes of test data is different from those of training . Therefore we can not use class prediction and extracting useful intermediate features is important for these problems . In section 4.4 , we showed that softmax-based features are superior to DML-based features in image retrieval and clustering if we have enough training samples . Even in the case of few training samples , softmax-based features achieved comparable performance in the OP dataset , which contains only about 5.5 training samples per class . 3 ) about Learning visual product design [ Bell+ , 2015 ] , DeepFace [ Taigman+ , 2014 ] , facenet [ Schroff+ , 2015 ] As far as we know , these studies used private datasets for training . Moreover , they used simple siamese or triplet based networks . Therefore we compared with more sophisticated methods , Magnet loss and Lifted structed feature embedding , on publicly available datasets ."}, "2": {"review_id": "HyQWFOVge-2", "review_text": "There has been substantial recent interest in representation learning, and specifically, using distance metric learning (DML) to learn representations where semantic distance between inputs can be measured. This is a topic of particular relevance / interest to ICLR. This paper poses a simple yet provocative question: can a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches? Thorough experiments seem to indicate that this is indeed the case. Comparisons are made to recent DML approaches including Magnet Loss (ICLR2016) and Lifted structure embedding (CVPR2016) and superior results are shown across a number of datasets / tasks for which the DML approaches were designed. This main result is a bit surprising since SoftMax is a natural and trivial baseline, so it should have been properly evaluated in previous DML literature. The authors argue that previous approaches did not fully/properly tune the softmax baselines, or that comparisons were not apples-to-apples. Also, one change in the current paper is the addition of L2 normalization, which is well motivated and helps improve SoftMax feature distances. Different dimensionality reduction approaches are also tested. These changes are minor, but especially the L2 normalization proves to be a simple but effective improvement for SoftMax features. A big issue is with how pre-training is performed (in Magnet Loss the softmax baselines were pretrained for less time on ImageNet). The approach taken here is reasonable, but so is the approach in Magnet Loss (for different reasons). Ultimately, both are fine. Unfortunately, due to use of different schemes, the results are not comparable. Let me copy-paste what I wrote in an earlier comment: My main concern about the paper is that the comparisons in Tables 1 and 2 and Figure 4 to Rippel et al. are not apples-to-apples. Basically, the papers shows that absolute results of using SoftMax w full pre-training (PT) on ImageNet is superior to any of the results in Rippel's paper (including both the Softmax and Magnet results). But as the current results show, PT appears to be critical to obtaining such good numbers - The Rippel SoftMax numbers use only 3 PT stages and are dramatically worse than the full PT on ImageNet. As it stands, I am not convinced that SoftMax is actually better than Magnet. Here is the evidence we have (I'll use Stanford Dogs as an example, but any of the datasets have the same conclusion): (1) Softmax w 3 stages of PT: 26.6% (from Rippel paper) and 32.7% (from authors' reproduction) (2) Magnet w 3 stages of PT: 24.9% (3) Softmax w full PT: 18.3% (4) Magnet w full PT: not shown From this all I see is that PT is critical for getting absolute good results. However, what about Magnet w full PT? These results are not shown either here or in the original Rippel paper (I went back and looked). As such, I do not think it is justifiable to claim superiority of Softmax to Magnet based on available evidence. (Note: I looked back carefully at Rippel's paper, and it appears that the authors use 3 PT stages as a form of \"warmup\". There is a statement that using full PT would \"defeat the purpose of pursuing DML\". I'm not sure if I agree w Rippel's statement since in the present paper there is clear evidence that full PT is hugely helpful, at least for softmax. That being said, I did not see any evidence in the Rippel paper that PT is harmful or that DML wouldn't work with full PT.) The authors responded to my concern by claiming that \u201cfrom Rippel's results, it is no doubt that Magnet@3epochPT > Magnet@FullPT and Magnet@3epoch > Softmax@3epochPT.\u201d However, I went back to Rippel\u2019s paper, and simply the Magnet@FullPT experiment never appears. I further went and contacted Oren Rippel himself, and he verified he never ran the Magnet@FullPT experiment. I encourage the authors to contact Oren Rippel regarding this if they wish to verify (I have asked Oren Rippel to not reveal my identity). [Disclaimer: I am NOT Oren Rippel]. The authors mentioned that they are training Magnet@FullPT. If results were shown for Magnet@FullPT and also retrain the Magnet@3epochPT as a sanity check, that would help alleviate this concern. Alternatively, the language in Section 4 and the Tables could be altered to make clear that the methods use different pretraining and hence are not comparable. Overall, I am actually quite sympathetic to this work. I think it could serve as an important sanity-check paper for the community and quite relevant to ICLR. Having proper and strong SoftMax baselines should prove quite useful to the DML community and to this line of work. However, currently I find the main results (table 1, table 2, figure 4, etc.) to be misleading. If indeed it were the case that Magnet@3epochPT > Magnet@FullPT, then it would be fine. However, at this point as far as I know no one has actually tried Magnet@FullPT. And, given the general importance and effectiveness of pre-training, especially when transferring to small dataset, I would be hugely surprised if Magnet@FullPT was not superior by a large margin. I think either having this experiment in place or altering the writing / presentation of the results would be critical to allow for publishing.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review , We 're still doing experiments of Magnet loss . We tried Magnet @ fullPT and Magnet @ 3epochPT for comparison , but it reaches lower accuracies than those in Rippel 's paper . ( For example , we got about 25 % classification error on Oxford-102Flowers dataset for both Magnet @ 3epochPT and Magnet @ FullPT . ) We have sent an email to Rippel , and waiting for his reply . Please wait for a while ."}}