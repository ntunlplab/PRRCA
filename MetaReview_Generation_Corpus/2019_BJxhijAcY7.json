{"year": "2019", "forum": "BJxhijAcY7", "title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "decision": "Accept (Poster)", "meta_review": "The Reviewers noticed that the paper undergone many editions and raise concern about the content. They encourage improving experimental section further and strengthening the message of the paper. ", "reviews": [{"review_id": "BJxhijAcY7-0", "review_text": "The authors present a distributed implementation of signSGD with majority vote as aggregation. The result is a communication efficient and byzantine robust distributed training method. This is an interesting and relevant problem. There are two parts in this paper: first the authors prove a convergence guarantee for signSGD, and then they prove that under a weak adversary attack signSGD will be robust to a constant fraction of adversarial nodes. The authors conclude with some limited experiments. Overall, the idea of combining low-communication methods with byzantine resilience is quite interesting. That is, by limiting the domain of the gradients one expects that the power of an adversary would be limited too. The application of the majority vote on the gradients is an intuitive technique that can resolve weak adversarial attacks. Overall, I found the premise quite interesting. There are several issues that if fixed this could be a great paper, however I am not sure if there is enough time between rebuttals to achieve this for this round of submissions. I will summarize these key issues below. 1) Although the authors claim that this is a communication efficient technique, signSGD (on its communication merit) is not compared with any state of the art communication efficient training algorithm, for example: - 1Bit SGD [1] - QSD [2] - TernGrad [3] - Deep Gradient compression [4] I think it is important to include at least one of those algorithms in a comparison. Due to the lack of comparisons with state of the art it is hard to argue on the relative performance of signSGD. 2) Although the authors claim byzantine resilience, this is against a very weak type of adversary, eg one that only sends back the opposite sign of the local stochastic gradient. An omniscient adversary can craft attacks that are significantly more sophisticated, for which a simple majority vote would not work. Please see the results in [b1]. 3) The authors although reference some limited literature on byzantine ML, they do not compare with other byzantine tolerant ML methods. For example check [eg, b1-b4] below. Again, due to the lack of comparisons with state of the art it is hard to argue on the relative performance of signSGD. Overall, although the presented ideas are promising, a substantial revision is needed before this paper is accepted for publication. I think it is extremely important that an extensive comparison is carried out with respect to both communication efficient algorithms, and/or byzantine tolerant algorithms, since signSGD aims to be competitive with both of these lines of work. This is a paper that has potential, but is currently limited by its lack of appropriate comparisons. [1] https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140694.pdf [2] https://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf [3] https://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf [4] https://arxiv.org/pdf/1712.01887.pdf [b1] https://arxiv.org/pdf/1802.07927.pdf [b2] https://arxiv.org/pdf/1803.01498.pdf [b3] https://dl.acm.org/citation.cfm?id=2933105 [b4] https://arxiv.org/pdf/1804.10140.pdf [b5] https://arxiv.org/pdf/1802.10116.pdf ######################## I would like to commend the authors for making a significant effort in revising their manuscript. Specifically, I think adding the experiments for QSGD and Krum are an important addition. However, I still have a few major that in my opinion are significant: - The experiments for QSGD are only carried for the 1-bit version of the algorithm. It has been well observed that this is by far the least well performing variant of QSGD. That is, 4 or 8 bit QSGD seems to be significantly more accurate for a given time budget. I think the goal of the experiments should not be to compare against other 1-bit algorithms (though to be precise, 1-bit QSGD is a ternary algorithm) , but against the fastest low-communication algorithm. As such, although the authors made an effort in adding more experiments, I am still not convinced that signSGD will be faster than 4 or 8 bit QSGD. I want to also acknowledge in this comment the fact that these experiments do take time, and are not easy to run, so I commend them again for this effort. - My second comment relates to comparisons with state of the art algorithms in byzantine ML. The authors indeed did compare against Krum, however, as noted in my original review there are many works following Blanchard et al. For example as I noted https://arxiv.org/pdf/1802.07927.pdf (the Bulyan algorithm) shows that there exist significantly stronger defense mechanisms for byzantine attacks. I think it would have been a much stronger comparison to compare with Bulyan. Overall, I think the paper has good content, and the authors significantly revised their paper according to the reviews. However, several more experiments are needed for convincing a potential reader of the main claims of the paper, i.e., that signSGD is a state of the art communication efficient and byzantine tolerant algorithm. I will increase my score from 5 to 6, and I will not oppose the paper being rejected or accepted. My personal opinion is that a resubmission for a future venue would yield a much stronger and more convincing paper assuming more extensive and thorough comparisons are added.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer1 , Thank you for your clear and precise review . We appreciate the comment that our work \u201c could be a great paper \u201d if we add some comparisons during the rebuttal . We want to contest your take on the weakness of our adversarial model , yet wholeheartedly agree with the need for adequate experimental comparisons to other techniques . > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > Comparison expts > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > We have added comparisons to QSGD ( compression ) Multi-Krum ( Byzantine fault tolerance ) . Please see the revisions in the post above . > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > Adversarial model > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > the adversary is \u201c very weak \u201d since it \u201c only sends back the opposite sign of the local stochastic gradient \u201d We have formulated an entire class of adversaries that our algorithm is robust to . Please see our revisions above . Thank you for pointing us to the paper [ b1 ] saying that \u201c convergence is not enough \u201d since , for example , a powerful adversary can steer convergence to bad local minimisers . This is a great point . For this reason we do not recommend using our algorithm to protect against \u201c omniscient \u201d adversaries . But for \u201c mere mortal \u201d adversaries , our results are interesting . An example of a \u201c mere mortal \u201d adversary could be a broken machine that sends random bits or stale gradients ."}, {"review_id": "BJxhijAcY7-1", "review_text": "This paper continues the study of the signSGD algorithm due to (Balles & Hennig, Bernstein et al), where only the sign of a stochastic gradient is used for updating. There are two main results: (1) a slightly refined analysis of two results in Bernstein et al. The authors proved that signSGD continues to converge at the 1/sqrt(T) rate even with minibatch size 1 (instead of T as in Bernstein et al), if the gradient noise is symmetric and unimodal; (2) a similar convergence rate is obtained even when half of the worker machines flip the sign of their stochastic gradients. These results appear to be relatively straightforward extensions of those in Bernstein et al. Clarity: The paper is mostly nicely written, with some occasionally imprecise claims. Page 5, right before Remark 1: it is wrongly claimed that signSGD converges to a critical point of the objective. This cannot be inferred from Theorem 1. (If the authors disagree, please give the complete details on how the random sequence x_t converges to some critical point x^*. or perhaps you are using the word \"convergence\" differently from its usual meaning?) Page 6, after Lemma 1. The authors claimed that \"the bound is elegant since ... even at low SNR we still have ... <= 1/2.\" In my opinion, this is not elegant at all. This is just your symmetric assumption on the noise, nothing more... Eq (1): are you assuming g_i > 0 here? this inequality is false as you need to discuss the two cases. \"Therefore signSGD cannot converge for these noise distributions, ..... point in the wrong direction.\" This is a claim based on intuitive arguments but not a proven fact. Please refrain from using definitive sentences like this. Footnote 1: where is the discussion? Originality: Compared to the existing work of Bernstein et al, the novelty of the current submission is moderate. The main results appear to be relatively straightforward refinements of those in Bernstein. The observation that majority voting is Byzantine fault tolerant is perhaps not very surprising but it is certainly nice to have a formal justification. Quality: At times this submission feels like half-baked: -- The theoretical results are about signSGD while the experiments are about sigNUM -- The adversaries must send the negation of the sign? why can't they send an arbitrary bit vector? -- From the authors' discussion \" we will include this feature in our open source code release\", \"plan to run more extensive experiments in the immediate future and will update the paper...\", and \"should be possible to extend the result to the mini-batch setting by combining ...\" Significance: This paper is certainly a nice addition to our understanding of signSGD. However, the current obtained results are not very significant compared to the existing results: Theorem 1 is a minor refinement of the two results in Bernstein et al, while Theorem 2 at its current form is not very interesting, as it heavily restricts what an adversary worker machine can do. It would be more realistic if the adversaries can send random bits (still non-cooperated though). ##### added after author response ##### I appreciate the authors' efforts in trying to improve the draft by incorporating the reviewers' comments. While I do like the authors' continued study of signSGD, the submission has gone through some significant revision (more complete experiments + stronger adversary). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer2 , Thank you for your clear and thorough review . We appreciate your comment that the paper is a \u201c nice addition to our understanding of signSGD \u201d . We will first contest the criticism about the significance of the work . We will then respond to the other comments in detail . > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > On matters of significance > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \u201c it heavily restricts what an adversary worker machine can do \u201d We have now formulated an entire class of adversaries that our algorithm is robust to . Please see our revisions above . This class contains machines that send random bits as a special case . > \u201c Theorem 1 is a minor refinement \u201d . Whilst `` algebraically '' the result is a minor refinement , conceptually it is a larger shift . It brings the signSGD work in line with modern machine learning practice . And we expect that it has ramifications on other active areas of ML research . For example : Reddi et al . ( 2018 ) showed how bimodal noise distributions can lead to divergence of Adam . This leaves a major outstanding question in the community : if Adam generally diverges , why does it work so well in practice ? Theorem 1 shows how signSGD -- -a special limit of Adam -- -may be guaranteed to converge in natural settings such as Gaussian noise distributions . It suggests that we may be able to prove convergence of Adam for Gaussian noise distributions . > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > Minor comments > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \u201c signSGD converges to a critical point of the objective \u201d To clarify , we mean convergence in the sense that the gradient norm goes to zero as N increases , which is exactly what Theorem 1 tells us . Points with zero gradient norm are critical points . The mixed norm on the left hand side is unusual , but by inspection it is clear that the mixed norm shrinking to zero implies that the L2-norm shrinks to zero . We will clarify this in the paper . > \u201c are you assuming g_i > 0 here \u201d Thanks for mentioning this . We did not signpost it , but we assumed , without loss of generality , that g_i > 0 . ( The case that g_i < 0 follows by totally analogous reasoning . ) > The claim \u201c signSGD can not converge for these noise distributions \u201d is only \u201c based on intuitive arguments \u201d . Thank you for pointing this out , we decided to simplify the discussion by just giving a simple example . > \u201d The theoretical results are about signSGD while the experiments are about sigNUM \u201d See [ 1 , Appendix , Figure A.4 ] for experiments across a range of momentum values . [ 1 ] also discusses the theoretical relation between Signum and signSGD . In general we suggest practitioners use Signum instead of signSGD in practice since it is only fair to give our algorithm as many hyperparameters as momentum SGD . [ 1 ] signSGD , compressed optimisation for non-convex problems https : //arxiv.org/abs/1802.04434 ."}, {"review_id": "BJxhijAcY7-2", "review_text": "The paper proposes a distributed optimization method based on signSGD. Majority vote is used when aggregating the updates from different workers. The method itself is naturally communication efficient. Convergence analysis is provided under certain assumptions on the gradient. It also theoretically shows that it is robust up to half of the workers behave independently adversarially. Experiments are carried out on parameter server environment and are shown to be effective in speeding up training. I find the paper to be solid and interesting. The idea of using signSGD for distributed optimization make it attractive as it is naturally communication efficient. The work provides theoretical convergence analysis under the small batch setting by further assuming the gradient is unimodal and symmetric, which is the main theoretical contribution. Another main theoretical contribution is showing it is Byzantine fault tolerant. The experiments are extensive, demonstrating running time speed-up comparison to normal SGD. It is interesting to see a test set gap in the experiments. It remains to be further experimented to see if the method itself inherently suffer from generalization problems or it is a result of imperfect parameter tuning. One thing that would be interesting to explore further is to see how asynchronous updates of signSGD affect the convergence both in theory and practice. For example, some workers might be lost during one iteration, how will this affect the overall convergence. Also, it would be interesting to see the comparison of the proposed method with SGD + batch normalization, especially on their generalization performance. It might be interesting to explore what kind of regularization technique would be suitable for signed update kind of method. Overall, I think the paper proposes a novel distributed optimization algorithm that has both theoretical and experimental contribution. The presentation of the paper is clear and easy to follow. Suggestions: I feel the experiments part could still be improved as also mentioned in the paper to achieve competitive results. More experiments on different tasks and DNN architectures could be performed. ", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer3 , Thank you for your positive review . We really appreciate the remarks that our \u201c experiments are extensive \u201d and our paper is \u201c solid and interesting \u201d . > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > More experiments > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \u201c More experiments on different tasks and DNN architectures could be performed \u201d Thanks for the suggestion , we have added experiments training the QRNN language model on the Wikitext-103 dataset . Please see the revisions above > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > Further thoughts > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \u201c some workers might be lost during one iteration \u201d Intuitively , dropping workers will slow down convergence but not prevent it . You can see this immediately since a dropped worker is strictly better for convergence than an adversarial worker . This is one of the reasons we are excited about our Byzantine fault tolerance results . > what \u201c regularization technique would be suitable for signed update kind of method \u201d ? We are particularly excited about this question for future work , thanks for suggesting it ."}], "0": {"review_id": "BJxhijAcY7-0", "review_text": "The authors present a distributed implementation of signSGD with majority vote as aggregation. The result is a communication efficient and byzantine robust distributed training method. This is an interesting and relevant problem. There are two parts in this paper: first the authors prove a convergence guarantee for signSGD, and then they prove that under a weak adversary attack signSGD will be robust to a constant fraction of adversarial nodes. The authors conclude with some limited experiments. Overall, the idea of combining low-communication methods with byzantine resilience is quite interesting. That is, by limiting the domain of the gradients one expects that the power of an adversary would be limited too. The application of the majority vote on the gradients is an intuitive technique that can resolve weak adversarial attacks. Overall, I found the premise quite interesting. There are several issues that if fixed this could be a great paper, however I am not sure if there is enough time between rebuttals to achieve this for this round of submissions. I will summarize these key issues below. 1) Although the authors claim that this is a communication efficient technique, signSGD (on its communication merit) is not compared with any state of the art communication efficient training algorithm, for example: - 1Bit SGD [1] - QSD [2] - TernGrad [3] - Deep Gradient compression [4] I think it is important to include at least one of those algorithms in a comparison. Due to the lack of comparisons with state of the art it is hard to argue on the relative performance of signSGD. 2) Although the authors claim byzantine resilience, this is against a very weak type of adversary, eg one that only sends back the opposite sign of the local stochastic gradient. An omniscient adversary can craft attacks that are significantly more sophisticated, for which a simple majority vote would not work. Please see the results in [b1]. 3) The authors although reference some limited literature on byzantine ML, they do not compare with other byzantine tolerant ML methods. For example check [eg, b1-b4] below. Again, due to the lack of comparisons with state of the art it is hard to argue on the relative performance of signSGD. Overall, although the presented ideas are promising, a substantial revision is needed before this paper is accepted for publication. I think it is extremely important that an extensive comparison is carried out with respect to both communication efficient algorithms, and/or byzantine tolerant algorithms, since signSGD aims to be competitive with both of these lines of work. This is a paper that has potential, but is currently limited by its lack of appropriate comparisons. [1] https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140694.pdf [2] https://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf [3] https://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf [4] https://arxiv.org/pdf/1712.01887.pdf [b1] https://arxiv.org/pdf/1802.07927.pdf [b2] https://arxiv.org/pdf/1803.01498.pdf [b3] https://dl.acm.org/citation.cfm?id=2933105 [b4] https://arxiv.org/pdf/1804.10140.pdf [b5] https://arxiv.org/pdf/1802.10116.pdf ######################## I would like to commend the authors for making a significant effort in revising their manuscript. Specifically, I think adding the experiments for QSGD and Krum are an important addition. However, I still have a few major that in my opinion are significant: - The experiments for QSGD are only carried for the 1-bit version of the algorithm. It has been well observed that this is by far the least well performing variant of QSGD. That is, 4 or 8 bit QSGD seems to be significantly more accurate for a given time budget. I think the goal of the experiments should not be to compare against other 1-bit algorithms (though to be precise, 1-bit QSGD is a ternary algorithm) , but against the fastest low-communication algorithm. As such, although the authors made an effort in adding more experiments, I am still not convinced that signSGD will be faster than 4 or 8 bit QSGD. I want to also acknowledge in this comment the fact that these experiments do take time, and are not easy to run, so I commend them again for this effort. - My second comment relates to comparisons with state of the art algorithms in byzantine ML. The authors indeed did compare against Krum, however, as noted in my original review there are many works following Blanchard et al. For example as I noted https://arxiv.org/pdf/1802.07927.pdf (the Bulyan algorithm) shows that there exist significantly stronger defense mechanisms for byzantine attacks. I think it would have been a much stronger comparison to compare with Bulyan. Overall, I think the paper has good content, and the authors significantly revised their paper according to the reviews. However, several more experiments are needed for convincing a potential reader of the main claims of the paper, i.e., that signSGD is a state of the art communication efficient and byzantine tolerant algorithm. I will increase my score from 5 to 6, and I will not oppose the paper being rejected or accepted. My personal opinion is that a resubmission for a future venue would yield a much stronger and more convincing paper assuming more extensive and thorough comparisons are added.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer1 , Thank you for your clear and precise review . We appreciate the comment that our work \u201c could be a great paper \u201d if we add some comparisons during the rebuttal . We want to contest your take on the weakness of our adversarial model , yet wholeheartedly agree with the need for adequate experimental comparisons to other techniques . > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > Comparison expts > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > We have added comparisons to QSGD ( compression ) Multi-Krum ( Byzantine fault tolerance ) . Please see the revisions in the post above . > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > Adversarial model > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > the adversary is \u201c very weak \u201d since it \u201c only sends back the opposite sign of the local stochastic gradient \u201d We have formulated an entire class of adversaries that our algorithm is robust to . Please see our revisions above . Thank you for pointing us to the paper [ b1 ] saying that \u201c convergence is not enough \u201d since , for example , a powerful adversary can steer convergence to bad local minimisers . This is a great point . For this reason we do not recommend using our algorithm to protect against \u201c omniscient \u201d adversaries . But for \u201c mere mortal \u201d adversaries , our results are interesting . An example of a \u201c mere mortal \u201d adversary could be a broken machine that sends random bits or stale gradients ."}, "1": {"review_id": "BJxhijAcY7-1", "review_text": "This paper continues the study of the signSGD algorithm due to (Balles & Hennig, Bernstein et al), where only the sign of a stochastic gradient is used for updating. There are two main results: (1) a slightly refined analysis of two results in Bernstein et al. The authors proved that signSGD continues to converge at the 1/sqrt(T) rate even with minibatch size 1 (instead of T as in Bernstein et al), if the gradient noise is symmetric and unimodal; (2) a similar convergence rate is obtained even when half of the worker machines flip the sign of their stochastic gradients. These results appear to be relatively straightforward extensions of those in Bernstein et al. Clarity: The paper is mostly nicely written, with some occasionally imprecise claims. Page 5, right before Remark 1: it is wrongly claimed that signSGD converges to a critical point of the objective. This cannot be inferred from Theorem 1. (If the authors disagree, please give the complete details on how the random sequence x_t converges to some critical point x^*. or perhaps you are using the word \"convergence\" differently from its usual meaning?) Page 6, after Lemma 1. The authors claimed that \"the bound is elegant since ... even at low SNR we still have ... <= 1/2.\" In my opinion, this is not elegant at all. This is just your symmetric assumption on the noise, nothing more... Eq (1): are you assuming g_i > 0 here? this inequality is false as you need to discuss the two cases. \"Therefore signSGD cannot converge for these noise distributions, ..... point in the wrong direction.\" This is a claim based on intuitive arguments but not a proven fact. Please refrain from using definitive sentences like this. Footnote 1: where is the discussion? Originality: Compared to the existing work of Bernstein et al, the novelty of the current submission is moderate. The main results appear to be relatively straightforward refinements of those in Bernstein. The observation that majority voting is Byzantine fault tolerant is perhaps not very surprising but it is certainly nice to have a formal justification. Quality: At times this submission feels like half-baked: -- The theoretical results are about signSGD while the experiments are about sigNUM -- The adversaries must send the negation of the sign? why can't they send an arbitrary bit vector? -- From the authors' discussion \" we will include this feature in our open source code release\", \"plan to run more extensive experiments in the immediate future and will update the paper...\", and \"should be possible to extend the result to the mini-batch setting by combining ...\" Significance: This paper is certainly a nice addition to our understanding of signSGD. However, the current obtained results are not very significant compared to the existing results: Theorem 1 is a minor refinement of the two results in Bernstein et al, while Theorem 2 at its current form is not very interesting, as it heavily restricts what an adversary worker machine can do. It would be more realistic if the adversaries can send random bits (still non-cooperated though). ##### added after author response ##### I appreciate the authors' efforts in trying to improve the draft by incorporating the reviewers' comments. While I do like the authors' continued study of signSGD, the submission has gone through some significant revision (more complete experiments + stronger adversary). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer2 , Thank you for your clear and thorough review . We appreciate your comment that the paper is a \u201c nice addition to our understanding of signSGD \u201d . We will first contest the criticism about the significance of the work . We will then respond to the other comments in detail . > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > On matters of significance > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \u201c it heavily restricts what an adversary worker machine can do \u201d We have now formulated an entire class of adversaries that our algorithm is robust to . Please see our revisions above . This class contains machines that send random bits as a special case . > \u201c Theorem 1 is a minor refinement \u201d . Whilst `` algebraically '' the result is a minor refinement , conceptually it is a larger shift . It brings the signSGD work in line with modern machine learning practice . And we expect that it has ramifications on other active areas of ML research . For example : Reddi et al . ( 2018 ) showed how bimodal noise distributions can lead to divergence of Adam . This leaves a major outstanding question in the community : if Adam generally diverges , why does it work so well in practice ? Theorem 1 shows how signSGD -- -a special limit of Adam -- -may be guaranteed to converge in natural settings such as Gaussian noise distributions . It suggests that we may be able to prove convergence of Adam for Gaussian noise distributions . > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > Minor comments > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \u201c signSGD converges to a critical point of the objective \u201d To clarify , we mean convergence in the sense that the gradient norm goes to zero as N increases , which is exactly what Theorem 1 tells us . Points with zero gradient norm are critical points . The mixed norm on the left hand side is unusual , but by inspection it is clear that the mixed norm shrinking to zero implies that the L2-norm shrinks to zero . We will clarify this in the paper . > \u201c are you assuming g_i > 0 here \u201d Thanks for mentioning this . We did not signpost it , but we assumed , without loss of generality , that g_i > 0 . ( The case that g_i < 0 follows by totally analogous reasoning . ) > The claim \u201c signSGD can not converge for these noise distributions \u201d is only \u201c based on intuitive arguments \u201d . Thank you for pointing this out , we decided to simplify the discussion by just giving a simple example . > \u201d The theoretical results are about signSGD while the experiments are about sigNUM \u201d See [ 1 , Appendix , Figure A.4 ] for experiments across a range of momentum values . [ 1 ] also discusses the theoretical relation between Signum and signSGD . In general we suggest practitioners use Signum instead of signSGD in practice since it is only fair to give our algorithm as many hyperparameters as momentum SGD . [ 1 ] signSGD , compressed optimisation for non-convex problems https : //arxiv.org/abs/1802.04434 ."}, "2": {"review_id": "BJxhijAcY7-2", "review_text": "The paper proposes a distributed optimization method based on signSGD. Majority vote is used when aggregating the updates from different workers. The method itself is naturally communication efficient. Convergence analysis is provided under certain assumptions on the gradient. It also theoretically shows that it is robust up to half of the workers behave independently adversarially. Experiments are carried out on parameter server environment and are shown to be effective in speeding up training. I find the paper to be solid and interesting. The idea of using signSGD for distributed optimization make it attractive as it is naturally communication efficient. The work provides theoretical convergence analysis under the small batch setting by further assuming the gradient is unimodal and symmetric, which is the main theoretical contribution. Another main theoretical contribution is showing it is Byzantine fault tolerant. The experiments are extensive, demonstrating running time speed-up comparison to normal SGD. It is interesting to see a test set gap in the experiments. It remains to be further experimented to see if the method itself inherently suffer from generalization problems or it is a result of imperfect parameter tuning. One thing that would be interesting to explore further is to see how asynchronous updates of signSGD affect the convergence both in theory and practice. For example, some workers might be lost during one iteration, how will this affect the overall convergence. Also, it would be interesting to see the comparison of the proposed method with SGD + batch normalization, especially on their generalization performance. It might be interesting to explore what kind of regularization technique would be suitable for signed update kind of method. Overall, I think the paper proposes a novel distributed optimization algorithm that has both theoretical and experimental contribution. The presentation of the paper is clear and easy to follow. Suggestions: I feel the experiments part could still be improved as also mentioned in the paper to achieve competitive results. More experiments on different tasks and DNN architectures could be performed. ", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer3 , Thank you for your positive review . We really appreciate the remarks that our \u201c experiments are extensive \u201d and our paper is \u201c solid and interesting \u201d . > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > More experiments > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \u201c More experiments on different tasks and DNN architectures could be performed \u201d Thanks for the suggestion , we have added experiments training the QRNN language model on the Wikitext-103 dataset . Please see the revisions above > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > Further thoughts > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \u201c some workers might be lost during one iteration \u201d Intuitively , dropping workers will slow down convergence but not prevent it . You can see this immediately since a dropped worker is strictly better for convergence than an adversarial worker . This is one of the reasons we are excited about our Byzantine fault tolerance results . > what \u201c regularization technique would be suitable for signed update kind of method \u201d ? We are particularly excited about this question for future work , thanks for suggesting it ."}}