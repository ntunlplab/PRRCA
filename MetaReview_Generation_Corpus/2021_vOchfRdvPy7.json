{"year": "2021", "forum": "vOchfRdvPy7", "title": "To be Robust or to be Fair: Towards Fairness in Adversarial Training", "decision": "Reject", "meta_review": "This paper examines adversarially trained robust models, and finds that accuracy disparity is higher than for standard models. The authors introduce a method they call Fair Robust Learning using Lagrange multipliers to minimize overall robust error while constraining the accuracy discrepancy between classes.  \n\nIn discussion, consensus was reached that the observations and approach are interesting but the paper is not yet ready for publication. The main concern is that it is not clear if the class accuracy disparity is due to adversarial training, or simply due to lower accuracy in general. Please see reviews and public discussion for further details.", "reviews": [{"review_id": "vOchfRdvPy7-0", "review_text": "The authors study adversarially trained classifiers and observe that the accuracy discrepancy between classes is larger than that of standard models . They then propose a theoretical model where this phenomenon provably arises . Finally , they propose a method to reduce the ( standard and robust ) accuracy discrepancy between classes , by adapting existing methods from the non-adversarial setting . I found the paper interesting . The original observation is intriguing and rigorously reproduced on a synthetic dataset . Moreover , the method proposed does seem to improve the class disparity of these models . However , I have one major concern : it is not clear that the phenomenon observed is a property of adversarial training . A different explanation could be that robust models have lower accuracy than standard ones , with their robust accuracy being even lower . Should the increase in error be multiplicative ( which is the most likely scenario ) then it would potentially explain the main observation of the paper without taking into account adversarially training at all . Specifically : - Figure 1 : Even the standard model accuracy fluctuates across classes . There are clearly classes that have at least 3 times the error rates of others . The situation for the robust model does not seem particularly different , there are also classes with similar multiplicative ratios in their error , the difference just looks larger because the error rate itself is larger . Measuring the ratio between different error rates might shed some light here . - Theoretical setting : Looking at equation ( 12 ) , consider a similar case where the standard error of the robust model is , say , 4 times higher than that of the standard one . In this case , $ \\Omega $ would be 4 purely based on the accuracy of the model . To summarize , based on the existing arguments in the paper , we can not tell apart the scenario where robust training causes class disparity due to an inherent property of the method or simply because it increases the model 's error rate . Should the latter be the case , the contribution of the paper would be significantly smaller since the underlying phenomenon would have little to do with adversarial training ( similar for the proposed method ) . Another issue that was not clear to me is whether the test set is used during training to compute the weighting of the different loss terms ( line 5 of algorithm 1 ) . From what I understand , this is the same test set used to report performance , in which case the methodology would be fundamentally flawed and a separate test set ( completely unseen ) would be needed . Overall , I do not believe that the paper is ready for publication but I would be willing to update my review based on further discussion . Other comments ( not affecting score ) : - Intro : `` tradition debiasing ... '' - > `` traditional debiasing ... '' - Figure 1 : I assume models are trained against an 8/255 adversary , is this correct ? - Eq 14 : The fourth error rate should be R_rob , right ? - Section 4.3 : broken references - Section 5 : `` fairly robust '' can be interpreted as `` somewhat robust '' which is not the intended meaning of `` fairly '' here POST-RESPONSE UPDATE I appreciate the authors ' response and the additional experimental results provided . While I do think that they are a step in the right direction ( I hence slightly increased my score to a 5 ) , they still do not address my concerns . Specifically : - * * Empirical results . * * Taking a closer look at Table 1 * of the response and the results on randomized smoothing ( provided in other responses ) , I do agree that not all errors increase by the exact same multiplicative factor . At the same time , the discrepancy does not seem to be particularly largei.e. , the ratios have a mean of 2.5 with a standard deviation of < 0.5 . Clearly , given that this is a real-world dataset , it is natural to expect that the effect of adversarial training is not perfectly linear across all classes . - * * Theoretical results . * * After reading the response , reading the discussion with Reviewer4 , and going through the paper again , I am still not convinced that the increase in class disparity can not be attributed to a large extent to the gap in clean accuracy introduced by robust training . Perhaps there is a cleaner way of formulating there results or highlighting the key components of the analysis that resolves this . However , unfortunately , I still do not find the analysis convincing in its current state . Overall , I believe that this point is quite nuanced and the existing empirical and theoretical analysis is not sufficient to draw a confident conclusion . Given how this point is at the core of the paper 's contribution , I still recommend rejection .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for taking the time to review our paper . We were happy to read that `` the observations are interesting '' and `` synthetic analysis is rigorous '' . In the following , we will clarify your key concerns . $ \\textbf { Question 1 } $ . Whether the observed \u201c unfair \u201d phenomenon is a property of adversarial training ? To investigate this question , we did the following further empirical and theoretical understandings -- ( 1 ) we check the clean test performance difference between adversarial training and clean training for each class in CIFAR10 ; ( 2 ) we check the robustness test performance difference between adversarial training and clean training for each class in CIFAR10 ; ( 3 ) we check another dataset ( i.e. , GTSRB ) in addition to CIFAR10 ; and ( 4 ) we further clarify our theoretical analysis . The evidence from these understandings suggests that adversarial training indeed introduces biased impact on different classes instead of only scaling up the error for each class in a multiplicative way . Next we present the details . $ \\textbf { ( 1 ) } $ Table ( 1 * ) presents the clean test error of clean training and adv training model for each class in CIFAR10 , including the corresponding error ratio and difference . From the results in Table ( 1 * ) , we can see that in terms of difference ( adv - clean ) , adv training causes the clean error of \u201c bird \u201d and \u201c cat \u201d increase by a large margin , which is around 20\\ % , much larger than other classes . In terms of ratio ( adv : clean ) , adv training almost triples the errors for class \u201c bird \u201d , `` cat '' and \u201c deer \u201d , but for class \u201c ship \u201d , it only increases the error by a ratio 1.6 . If we take a closer look , for classes `` frog '' and `` ship '' , they have comparable clean errors in the clean model , but have an obvious discrepancy in a robust model . Thus , the increase of class-wise robust errors are also not equal ( in terms of ratio \\ & difference ) . Table ( 1 * ) Clean Error in CIFAR10 | class ( in % ) | plane | car | bird | cat | deer | dog | frog | horse | ship | truck | avg . | | clean train . err.|5.6 $ \\pm $ 0.5 |2.8 $ \\pm $ 0.4 |9.5 $ \\pm $ 1.5 | 13.8 $ \\pm $ 1.3 | 6.1 $ \\pm $ 1.4 | 10.0 $ \\pm $ 1.4 | 5.1 $ \\pm $ 0.9| 5.2 $ \\pm $ 0.9 | 4.4 $ \\pm $ 0.6 | 4.3 $ \\pm $ 0.6 | 6.3 $ \\pm $ 0.7 | | adv . train.err.|12.9 $ \\pm $ 1.8|6.8 $ \\pm $ 0.9|27.9 $ \\pm $ 2.9|38.4 $ \\pm $ 2.1|20.8 $ \\pm $ 1.9|26.9 $ \\pm $ 3.3|10.2 $ \\pm $ 1.0|10.9 $ \\pm $ 1.0|7.1 $ \\pm $ 1.3|9.4 $ \\pm $ 1.1| 15.4 $ \\pm $ 1.5 | | ratio ( adv : clean ) | 2.3 | 2.4 | 2.9 | 2.9 | 3.4| 2.7 | 2.0 | 2.1 | 1.6 | 2.2 | 2.4 | | diff . ( adv - clean ) | 7.3 | 4.0 |18.3|22.6|14.6|16.9| 5.1 | 5.7 | 2.7 | 5.2 | 10.3 | $ \\textbf { ( 2 ) } $ Table ( 2 * ) demonstrates the robust error ( under PGD attack by 8/255 ) between clean trained and adversarial trained models for each class in CIFAR10 . In terms of adversarial robustness , the disparity between different classes is more obvious . For example , in Table 2 * and Figure 1 in the paper , we see for a clean trained model , the adversarial error under PGD attack for each class is around 100\\ % . However , after adv training , some classes ( such as \u201c car \u201d ) achieve high robustness with the robust error less than 35\\ % , but other classes ( such as \u201c cat \u201d ) still have high robust error which is around 80\\ % . Thus , the decreases of class-wise robust errors are also not equal ( in terms of ratio \\ & difference ) . Table ( 2 * ) Robust Error in CIFAR10 | class ( in % ) | plane | car | bird | cat | deer | dog | frog | horse | ship | truck | | clean train . rob err.|100| 100 | 100 | 100 | 100 | 100 | 100 | 100| 100 | | adv . train.rob err.| 48.4 $ \\pm $ 2.1| 34.0 $ \\pm $ 2.5 | 70.4 $ \\pm $ 3.2 | 82.1 $ \\pm $ 2.2| 74.6 $ \\pm $ 3.0| 64.4 $ \\pm $ 2.7| 50.0 $ \\pm $ 4.0 | 44.7 $ \\pm $ 2.3 | 39.5 $ \\pm $ 4.6 | 43.9 $ \\pm $ 2.6| | ratio ( adv : clean ) | 0.48 | 0.34| 0.70 | 0.82 | 0.75 | 0.64 | 0.50 | 0.45 | 0.40 | 4.44| | diff . ( adv - clean ) | -51.6 | -66 . | -29.6 | -17.9 | -25.4 | -35.6 | -50.0| -55.3| -60.5|-56.1| $ \\textbf { ( 3 ) } $ We also include another dataset ( GTSRB , which is composed of 43 classes of different traffic sign images ) . From the results in Figure 2 and Table 4 in the paper , for a clean trained model , more than 20 classes have almost 0\\ % error rates and the worst class has 8\\ % clean error . For an adv trained model , there are 10 classes that have large clean errors over 20\\ % but some classes maintain 0\\ % clean error . ( Note that the Figure 2 in the paper sorts the classes in descending order for the presentations of both clean and adversarial accuracy . ) As a case study , we find that in a clean model , the traffic sign `` Speed Limit 60 '' , `` Speed Limit 100 '' and `` Go Straight '' signs have 0-1\\ % clean error , but they have 14\\ % , 26\\ % , 33\\ % error in a robustly trained model . On the contrary , the class `` turn right or straight '' sign has 3\\ % clean error in both clean and robust models . It also suggests that the increase of clean error is not proportional between classes . These facts show that the `` unfair '' condition might be related to adv training and not because the total error is large ."}, {"review_id": "vOchfRdvPy7-1", "review_text": "Thank you for your response , which cleared up some of my questions so I increased my score to a 5 . I would have liked more analytical / empirical evidence to substantiate some of the claims made in response to the questions numbered 4 , 5 , 12 , and 13 in the comments provided by the authors . My overall opinion remains unchanged but I encourage the authors to continue this line of work on build on these results and incorporate the feedback provided by the other reviewers as well . # # # This work makes an interesting observation that techniques designed to maximize robustness to adversarial examples may have negative consequences on the model 's performance . Typically , this has been studied primarily with an emphasis on accuracy . Here the authors look at fairness instead . - The first sentence of the introduction makes strong claim about the implications of adversarial examples without providing appropriate references or supporting the claim with evidence . - Adversarial examples do not have to be `` imperceptible '' , the main requirement is that the perturbation introduced does not affect semantics of the input . - Adversarial training has been subsumed into verifiable approaches to defending against p-norm bounded adversaries ( e.g. , randomized smoothing , etc . ) . Could you comment on why these approaches were left out of the experiments here ? They would be able to prove robustness is achieved uniformly for each training example . - If we know that adversarial training impacts accuracy ( as recently re-demonstrated by Tsipras et al . ) , could you comment on why it would have been expected that fairness would not be impacted given that the training set is balanced among classes and certain pairs of classes are closer to one-another under the p-norm than other pairs of classes ? - The presentation is somewhat confusing given that it entangles the notions of fairness and safety . This seems likely to lead to confusion from the readers . I would recommend splitting the work into two distinct studies : one of the worst-case performance of the classifiers in the absence of an adversary ( i.e. , safety ) and the other being on fairness itself . Unless the authors wish to demonstrate explicitly the connection between safety and certain definitions of fairness , in which case this connection should be presented more upfront in the paper . - Typo/grammar in `` In the lower dimensional space , an optimal linear model is more sensitive to the inherent data distributional difference and be biased when making predictions . '' - Could you motivate the choice of dataset here ? Why is CIFAR10 a good dataset to study fairness ? - Figure 1 : I would recommend using another type of graph . Typically series like this are used to represent data that is sequentially meaningful whereas here a bar plot or something along these lines would probably convey your message better . - The paper does not explicitly state which fairness definition is used ( for instance in Section 2 ) while this is important here . In particular , the definition considered seems to be based on parity but this is not explicitly stated there . Would the approach considered here apply to other definitions ? - It is not clear what is the applicability of the analysis based on robust/non-robust features is beyond the toy dataset considered here . Could you motivate better why this toy dataset matters in the context you are considering here ? - Grammar error in the statement of Theorem 1 - Grammar in `` should not be too larger '' ( page 5 ) - Could you motivate the choice of different thresholds on the natural model error and adversarial model error ? - Missing Section reference on page 6 - Section 4.3 discusses the intricacies and possible tensions between natural and robust accuracies and fairness . The re-margin strategy proposed seems to be at odds with recent results demonstrating that improving robustness to p-norm perturbations conflicts with generalization ( see Tramer et al.in ICML20 ) . Have you thought about the suitability of the p-norm constraint placed on adversaries in the context of your approach ? Does this create an artificial tension with fairness for the same reason that it creates an artificial tension with accuracy ( because the p-norm is not aligned with class semantics ) . - Related to this , Table 1 suggests that there is increased tension with natural accuracy when employing FRL compared to the baselines considered . Is that due to the point mentioned above or to what is discussed in Section 4.2 ? - The experimental evaluation is limited . It is hard to draw conclusions from one dataset . In particular , I would encourage the authors to consider datasets which are not balanced , which are from other domains than images , and which have a natural motivation for achieving `` fairness '' due to the task being solved . - Experiments should also include multiple runs and report variance . - The discussion of related work is a bit `` thin '' .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your time . We were happy to read that `` the observations are interesting '' and the insightful suggestions to further polish the work . In the following , we will clarify your key concerns . ........................................................................ Q1 . The first sentence of the introduction makes strong claim without providing appropriate references or supporting the claim with evidence . A1.We add more references and fixed improper claims in Introduction . ........................................................................ Q2 . Missing or broken reference in the paper . A2.We fixed the broken references in the paper . ........................................................................ Q3 . Why verifiable approaches are left out of the experiments ? A3.To investigate the fairness under adv training , we choose to test two most popular and representative algorithms for adv training , i.e. , PGD training and TRADES . In addition to these , we further provide experiments on certified defense ( Random Smoothing , Cohen 2019 ) in Table ( 1 * ) , which also confirms the existence of unfairness issue . Table ( 1 * ) | class ( in % ) | plane | car | bird | cat | deer | dog | frog | horse | ship | truck | |clean acc | 78.08| 96.15| 78.12 |62.5 | 84.75 |73.53| 92.16 |90.91| 90.91| 90.91| |adv acc 0.2 | 64.38| 86.54| 59.38| 37.5 | 62.71 |51.47 |74.51| 72.73 |74.55| 78.18| |adv acc 0.4 [ 35.62| 63.46 |25 . | 10.94 |23.73| 27.94 |47.06| 58.18| 47.27 |69.09 | ........................................................................ Q4 . If we know that adversarial training impacts accuracy , why it would have been expected that fairness would not be impacted given that the training set is balanced among classes and certain pairs of classes are closer to one-another under the p-norm than other pairs of classes ? A4.This could be one of the reasons why the unfairness issue occurs in some real datasets , but may not be always the case . In CIFAR10 , ( see Figure 1 ( left ) ) , some classes such as `` cat '' and `` bird '' do have 3-5\\ % lower clean accuracy than average . This may suggest some pairs of classes are indeed `` closer '' to each other in this model 's feature space . However , robust and clean models might use totally different sets of features for prediction [ Tsipras 2018 , Ilyas 2019 ] . Thus , a pair of classes which are `` close '' in the feature space of a clean model might be not `` close '' or become much `` closer '' in the feature space of a robust model . Therefore , even though the total accuracy will drop after adversarial training , it is still hard to expect how the accuracy will change in each class . Moreover , in our theoretical findings in Section 3 , we show that the `` unfairness '' can even exist for binary classification problems where there is only one pair of classes . Thus , this theoretical study demonstrates that the `` unfairness '' issue can be indeed due to the classification model itself . ........................................................................ Q5 . The presentation is somewhat confusing given that it entangles the notions of fairness and safety . A5.In this work , we focus on the general problem of unfairness in adversarial training . Safety is one of the consequences of such unfairness since certain classes with worst-cast performance can be the attacking target of the adversary . For example , for autonomous vehicles which apply robust models for traffic sign recognition tasks , if the model is very inaccurate or easy to be attacked only for some specific signs , the vehicle will still be under huge safety risks . In the experiments on the GTSRB dataset , we show the possibility of this \u201c safety \u201d concern under different traffic signs . In a clean traffic-sign classifier trained on GTSRB , we observe that the classifier can achieve high accuracy for every class which is above 92\\ % ( Figure 2 and Table 4 in the paper ) . However , when we use the adversarial training algorithm , some classes ( e.g . `` Speed Limit under 100 km/h '' ) become very inaccurate ( only has 70\\ % accuracy ) and still very easy to be attacked ( only has 16\\ % adversarial accuracy ) . ........................................................................ Q6 . Typos and grammar error A6 . We have corrected all the typos and grammar errors being pointed out ."}, {"review_id": "vOchfRdvPy7-2", "review_text": "This paper begins with the empirical observation that adversarially trained models often exhibit a large different in clean ( and robust ) accuracies across different classes . This is an important observation , which I have not seen in published work ( though I believe is generally understood by many practitioners with the robustness community ) , and is an important contribution . The paper then proceeds to use a theoretical example ( adapted from the model in Tsipras et al 2018 ) where adversarial training increases the accuracy difference across classes . More on this later . The paper proposes an algorithm , Fair Robust Learning ( FRL ) , to address this issue . The starting point is a standard Lagrangian-based approach to approximately ensure constraints that the performance for each class should be close to the overall performance . The FRL algorithm then proposes 2 modifications . First , rather than applying the constraint on the robust errors directly , they decompose the robust error into clean error and a robustness term ( similar to TRADES ) and approximately maintain constraints on both terms . Second , for difficult classes , they propose increasing the adversarial radius , instead of/in addition to only increasing the corresponding Lagrange multiplier . The experiments demonstrate that their FRL approach improves the worst-class clean and robust errors , relative to both standard adversarial training and the baseline Lagrangian approach , without losing much in average-class errors . Overall , I believe the paper is fairly well-executed and investigates an important topic . The motivating empirical observations are an important contribution . The proposed approach is natural and well-motivated , and the experiments show improvements in the worst-class errors , as should be expected conceptually . The empirical explorations of different variations on the core idea are also valuable . There were some points in the paper which I believe could be improved . For me , the theoretical example does not add much insight regarding why this effect occurs . - Regarding Theorem 1 , the comment says that `` when the term A is large , the model has close clean errors between the 2 classes , namely $ R_ { nat } ( f , -1 ) ~ R_ { nat } ( f , +1 ) $ . Assuming A^2 > > q ( K ) , the terms simplify to : $ $ R_ { nat } ( f , -1 ) = P [ N ( 0 , 1 ) \\leq A ( 1- \\sqrt { K } ) ] $ $ $ $ R_ { nat } ( f , +1 ) = P [ N ( 0 , 1 ) \\leq A ( 1- K ) ] $ $ which means we have a $ \\sqrt { K } $ factor difference between the two errors . In this case , it seems that even before adversarial training , we see a difference in class errors between the classes . - Additionally , in Theorem 2 , using a similar approximation B^2 > > q ( K ) , we 'd have $ $ R_ { nat } ( f_ { adv } , -1 ) = P [ N ( 0 , 1 ) \\leq B ( 1- \\sqrt { K } ) ] $ $ $ $ R_ { nat } ( f_ { adv } , +1 ) = P [ N ( 0 , 1 ) \\leq B ( 1- K ) ] $ $ where the two terms differ by a $ \\sqrt { K } $ factor again , only the leading constant is different . - The claim in Equation 12 that `` the ratio $ \\Omega $ is large ( e.g. > 1 ) '' seems to be the result the section builds to , but it seems it is more naturally explained by the fact that if there is already a difference in clean errors , since adversarial training will increase these errors ( Tsipras et al 2018 ) , then adversarial training will also scale up the difference in these class-wise errors . It 's possible that this explanation is actually what 's happening , and it does n't seem clearly inconsistent with the empirical data , since in Figure 1 ( left ) , there are already class-wise differences , which generally match the shape of the class-wise differences after adversarial training . But it does seem very different from the story the paper is trying to tell , and it would be good to figure out which one it is ( or acknowledge both ) . Moving on to the experiments : - The TRADES baseline has robust accuracy almost 10 % lower than the standard TRADES model - can you comment on this ? - Section 4.3 comments `` we claim that only upweighting its cost ( or Reweight ) could not succeed to fulfill the cost-sensitive classification goal in adversarial setting '' - could you explain why this is ? ( after all , reweighting is the basic approach used here too ) - How does the Re-Margin approach compare to the Reweight approach , when the reweighting also involves sweeping over the training $ \\epsilon $ ? - It 's nice to see the code . Hopefully this can also be released along with the paper if accepted . Other notes : - It would be nice to connect this to the broader fairness literature . This is unfortunately not my area of expertise , but difference in performances between classes ( or demographic groups ) is a very common metric , and it would be nice to relate this work more closely to work there . `` Distributionally Robust Neural Networks for Group Shifts '' ( Sagawa et al 2020 ) is a ( somewhat arbitrary ) example of work which seems related , but I 'm sure there are many more . It would be great to connect to this , especially papers which address why/when difference in performance across classes is expected . I give the paper a 6 overall , though could adjust my rating in either direction depending on the author feedback . ________________ EDIT : Score changed from 6 to 5 during discussion , see comments below .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for taking the time to review our paper . We were happy to read that `` the observations are important '' and `` the investigations are well-executed '' . We are also very appreciated the reviewer 's effort to understand our theories . In the following , we will clarify your key concerns . $ \\textbf { Question 1 } $ . In the theory , does adversarial training only just scale up the difference in these class-wise errors ? To answer this question , we did the following further theoretical understandings -- ( 1 ) We agree with the point that : in this binary classification setting , there originally exist ( negligible ) difference between classes errors . ( 2 ) Adversarial training will enlarge the disparity but may not just scale up the errors ( Z-scores ) . $ \\textbf { ( 1 ) } $ Regarding Theorem 1 for clean models , we agree with the reviewer 's understanding about our induction . Because the term $ A\\propto\\sqrt { m+d } $ and $ A > > q ( K ) $ , so we can have the simplification to get the in-class errors : $ $ R_\\text { nat } ( f ; -1 ) = \\text { Pr . } [ \\mathcal { N } ( 0,1 ) \\leq A ( 1-\\sqrt { K } ) ] $ $ $ $ R_\\text { nat } ( f ; +1 ) = \\text { Pr . } [ \\mathcal { N } ( 0,1 ) \\leq A ( 1-K ) ] $ $ Thus , there already exist accuracy disparity between 2 classes in a clean model : $ R_\\text { nat } ( f ; -1 ) < R_\\text { nat } ( f ; +1 ) $ . One thing to note is that : because $ K > 1 $ and $ A \\propto\\sqrt { m+d } $ ( $ d $ is the dimension of non-robust features and $ d $ is large ) , we can assume these 2 Z-scores are very large negative numbers . Therefore , the errors of both two classes are small and their difference is also small and even negligible . $ \\textbf { ( 2 ) } $ Regarding Theorem 2 for robust models , because the term $ B\\propto\\sqrt { m } $ , where $ m $ is the dimension of robust features and much smaller than $ d $ ( dimension of non-robust features ) , we can not have the assumption that $ B > > q ( K ) $ . Thus , we can not directly have the similar simplifications for Eq . ( 8 ) ( 9 ) as above . As a result , the 2 classes ' error disparity condition are not the same between a clean model and a robust model . For a robust model , this ratio between the 2 Z-scores are closely related to the property of the robust model itself , such as the dimension $ m $ of robust features it uses . In our main claim in Eq.12 , Corollary 1 , we clearly stated that adversarial training will enlarge the 2 classes ' error difference , given the discussions above that the ratio of Z-scores are not always the same ."}, {"review_id": "vOchfRdvPy7-3", "review_text": "Summary : This paper introduces a fairness perspective on accuracy performance among distinct classes in the context of adversarial training . It makes an observation that adversarial training algorithms ( Madry et al.2017 , Zhang et al.2019 ) yield biased performances on CIFAR 10 . It also offers a theoretical study under a Gaussian mixture setting that respects Eq . ( 1 ) .Three versions of fair robust algorithms are proposed and evaluated on CIFAR 10 . Strength : The paper makes an interesting observation on a fairness perspective in adversarial training . It provides a theoretical insight that tries to explain the observation in a certain setting Eq . ( 1 ) in which low-dimensional robust features play a dominant role in adversarial training . Weakness : While the observation is new and interesting , this reviewer wonders whether it happens \u201c fundamentally \u201d in any adversarial training setting . The theoretical study definitely offers a deeper understanding , yet it is analyzed under a particular setting Eq . ( 1 ) which I am not sure if it represents most practically-relevant scenarios . Also , all the experimental results are provided for the two algorithms ( Madry et al.2017 , Zhang et al.2019 ) under only one dataset CIFAR 10 . As the authors mentioned in Section 6.2 , this may be due to particular algorithms and data distribution . If that is the case , it would be good to see that the observation may not happen in other settings . Otherwise ( if the observation is fundamental ) , a thorough theoretical study together with extensive experiments that cover more diverse datasets are desired to be presented . Theoretical analysis : I can barely follow Theorems 1/2 and their implications stated in the paper . But it was not intuitively clear to me why the unfairness issue occurs in the considered setting . Any elaboration on intuition and insight might help . There are many grammatical errors and typos .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback . Your comments of `` the observations are interesting '' and `` the theory part offers deeper understanding of this topic '' are very encouraging . In the following , we will clarify your key concerns . Question 1 : Whether the \u201c unfairness \u201d phenomenon is \u201c fundamental \u201d for any adversarial training setting ? Whether the analysis under setting Eq . ( 1 ) is practical ? To answer this question , ( 1 ) we first make clarifications on what kind of adversarial training methods that we claim to have this `` unfair '' phenomenon ; ( 2 ) We provide more empirical evidence to show the existence of the phenomenon ; and ( 3 ) We explain why our theoretical setting is general and representative for practical-relevant scenarios . $ \\textbf { ( 1 ) } $ In this work , we concentrate on general adversarial training approaches which follow the robust optimization problem to minimize the average robust error under the same perturbation budgets : $ $ \\text { minimize } _f \\text { Pr . } ( ||\\delta||\\leq\\epsilon , f ( x+\\delta ) \\neq y ) . $ $ Theoretically , the unfairness issue is analyzed under this adversarial training objective . Empirically , the algorithms being tested , i.e. , PGD training and TRADES , are two most popular and representative algorithms for such optimization problem . $ \\textbf { ( 2 ) } $ In addition , we further provide experiments on one more robust training algorithm in CIFAR10 , [ Random Smoothing , Cohen 2019 ] , as well as one more dataset ( GTSRB ) . Specifically , Randomized Smoothing is a certified defense which implicitly minimizes model 's ( certified ) robust error . In Table 1 * , we obverse the similar `` unfair '' phenomenon and disparity relation among classes , compared to PGD training and TRADES . In Table 2 * ( Figure 2 and Table 4 in the paper ) , we also consider another dataset , GTSRB , which is composed of 43 classes of traffic sign images . In Table ( 5 * ) , for a clean trained model , most classes have almost 0\\ % error rates . For an adv trained model , there are several classes that have large errors ( e.g.above 20\\ % ) but some classes also maintain 0\\ % error . Similar observations can be made for class-wise robustness performance in this dataset . All evidence supports that the `` unfair '' phenomenon is likely to be fundamental for adversarial training . Table ( 1 * ) Clean / Robust errors of Randomized Smoothing ( $ \\sigma = 0.5 $ ) . | class ( in % ) | plane | car | bird | cat | deer | dog | frog | horse| ship | truck | | clean acc | 78.08 | 96.15 | 78.12 | 62.5 | 84.75 | 73.53|92.16 | 90.91 | 90.91| 90.91| | adv acc 0.2 | 64.38 | 86.54 | 59.38 | 37.5 | 62.71 | 51.47 | 74.51| 72.73 | 74.55 | 78.18| | adv acc 0.4 | 35.62 | 63.46 | 25 . 0 |10.94 | 23.73| 27.94 | 47.06| 58.18| 47.27 |69.09| Table ( 2 * ) Clean / Robust errors in GTSRB dataset . | ( in % ) | Avg . Clean | Avg . Rob.| Best Clean | Best Rob . | Worst Clean | Worst Rob | | clean train . | 0.5 | 81.1 | 0.0 | 27.5 | 8.3 | 100 | | PGD adv train . | 5.5 | 55.6 | 0.0 | 10.0 | 50.0 | 98.6 | | TRADES train . | 8.8 | 52.8 | 0.0 | 8.0 | 65.7 | 96.6 | ( 3 ) The theoretical model being analyzed in Eq.1 is generally used in previous works [ Tsipras 2018 , Ilyas 2019 ] to reveal one key behavior of adversarial training : robust models use low dimensional robust features for prediction , while non-robust models use high dimensional features . This theoretical setting has been shown to well reflect many properties about adversarial training in real datasets [ Ilyas 2019 , Gourdeau 2019 ] . In Eq.1 , based on this setting , we consider a more general condition where the 2 classes have different variances under the mixture Gaussian distribution . Thus if the unfairness problem exists for such a simple model , it is likely to exist for more complicated models and data distribution . This assumption is verified in our empirical study ."}], "0": {"review_id": "vOchfRdvPy7-0", "review_text": "The authors study adversarially trained classifiers and observe that the accuracy discrepancy between classes is larger than that of standard models . They then propose a theoretical model where this phenomenon provably arises . Finally , they propose a method to reduce the ( standard and robust ) accuracy discrepancy between classes , by adapting existing methods from the non-adversarial setting . I found the paper interesting . The original observation is intriguing and rigorously reproduced on a synthetic dataset . Moreover , the method proposed does seem to improve the class disparity of these models . However , I have one major concern : it is not clear that the phenomenon observed is a property of adversarial training . A different explanation could be that robust models have lower accuracy than standard ones , with their robust accuracy being even lower . Should the increase in error be multiplicative ( which is the most likely scenario ) then it would potentially explain the main observation of the paper without taking into account adversarially training at all . Specifically : - Figure 1 : Even the standard model accuracy fluctuates across classes . There are clearly classes that have at least 3 times the error rates of others . The situation for the robust model does not seem particularly different , there are also classes with similar multiplicative ratios in their error , the difference just looks larger because the error rate itself is larger . Measuring the ratio between different error rates might shed some light here . - Theoretical setting : Looking at equation ( 12 ) , consider a similar case where the standard error of the robust model is , say , 4 times higher than that of the standard one . In this case , $ \\Omega $ would be 4 purely based on the accuracy of the model . To summarize , based on the existing arguments in the paper , we can not tell apart the scenario where robust training causes class disparity due to an inherent property of the method or simply because it increases the model 's error rate . Should the latter be the case , the contribution of the paper would be significantly smaller since the underlying phenomenon would have little to do with adversarial training ( similar for the proposed method ) . Another issue that was not clear to me is whether the test set is used during training to compute the weighting of the different loss terms ( line 5 of algorithm 1 ) . From what I understand , this is the same test set used to report performance , in which case the methodology would be fundamentally flawed and a separate test set ( completely unseen ) would be needed . Overall , I do not believe that the paper is ready for publication but I would be willing to update my review based on further discussion . Other comments ( not affecting score ) : - Intro : `` tradition debiasing ... '' - > `` traditional debiasing ... '' - Figure 1 : I assume models are trained against an 8/255 adversary , is this correct ? - Eq 14 : The fourth error rate should be R_rob , right ? - Section 4.3 : broken references - Section 5 : `` fairly robust '' can be interpreted as `` somewhat robust '' which is not the intended meaning of `` fairly '' here POST-RESPONSE UPDATE I appreciate the authors ' response and the additional experimental results provided . While I do think that they are a step in the right direction ( I hence slightly increased my score to a 5 ) , they still do not address my concerns . Specifically : - * * Empirical results . * * Taking a closer look at Table 1 * of the response and the results on randomized smoothing ( provided in other responses ) , I do agree that not all errors increase by the exact same multiplicative factor . At the same time , the discrepancy does not seem to be particularly largei.e. , the ratios have a mean of 2.5 with a standard deviation of < 0.5 . Clearly , given that this is a real-world dataset , it is natural to expect that the effect of adversarial training is not perfectly linear across all classes . - * * Theoretical results . * * After reading the response , reading the discussion with Reviewer4 , and going through the paper again , I am still not convinced that the increase in class disparity can not be attributed to a large extent to the gap in clean accuracy introduced by robust training . Perhaps there is a cleaner way of formulating there results or highlighting the key components of the analysis that resolves this . However , unfortunately , I still do not find the analysis convincing in its current state . Overall , I believe that this point is quite nuanced and the existing empirical and theoretical analysis is not sufficient to draw a confident conclusion . Given how this point is at the core of the paper 's contribution , I still recommend rejection .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for taking the time to review our paper . We were happy to read that `` the observations are interesting '' and `` synthetic analysis is rigorous '' . In the following , we will clarify your key concerns . $ \\textbf { Question 1 } $ . Whether the observed \u201c unfair \u201d phenomenon is a property of adversarial training ? To investigate this question , we did the following further empirical and theoretical understandings -- ( 1 ) we check the clean test performance difference between adversarial training and clean training for each class in CIFAR10 ; ( 2 ) we check the robustness test performance difference between adversarial training and clean training for each class in CIFAR10 ; ( 3 ) we check another dataset ( i.e. , GTSRB ) in addition to CIFAR10 ; and ( 4 ) we further clarify our theoretical analysis . The evidence from these understandings suggests that adversarial training indeed introduces biased impact on different classes instead of only scaling up the error for each class in a multiplicative way . Next we present the details . $ \\textbf { ( 1 ) } $ Table ( 1 * ) presents the clean test error of clean training and adv training model for each class in CIFAR10 , including the corresponding error ratio and difference . From the results in Table ( 1 * ) , we can see that in terms of difference ( adv - clean ) , adv training causes the clean error of \u201c bird \u201d and \u201c cat \u201d increase by a large margin , which is around 20\\ % , much larger than other classes . In terms of ratio ( adv : clean ) , adv training almost triples the errors for class \u201c bird \u201d , `` cat '' and \u201c deer \u201d , but for class \u201c ship \u201d , it only increases the error by a ratio 1.6 . If we take a closer look , for classes `` frog '' and `` ship '' , they have comparable clean errors in the clean model , but have an obvious discrepancy in a robust model . Thus , the increase of class-wise robust errors are also not equal ( in terms of ratio \\ & difference ) . Table ( 1 * ) Clean Error in CIFAR10 | class ( in % ) | plane | car | bird | cat | deer | dog | frog | horse | ship | truck | avg . | | clean train . err.|5.6 $ \\pm $ 0.5 |2.8 $ \\pm $ 0.4 |9.5 $ \\pm $ 1.5 | 13.8 $ \\pm $ 1.3 | 6.1 $ \\pm $ 1.4 | 10.0 $ \\pm $ 1.4 | 5.1 $ \\pm $ 0.9| 5.2 $ \\pm $ 0.9 | 4.4 $ \\pm $ 0.6 | 4.3 $ \\pm $ 0.6 | 6.3 $ \\pm $ 0.7 | | adv . train.err.|12.9 $ \\pm $ 1.8|6.8 $ \\pm $ 0.9|27.9 $ \\pm $ 2.9|38.4 $ \\pm $ 2.1|20.8 $ \\pm $ 1.9|26.9 $ \\pm $ 3.3|10.2 $ \\pm $ 1.0|10.9 $ \\pm $ 1.0|7.1 $ \\pm $ 1.3|9.4 $ \\pm $ 1.1| 15.4 $ \\pm $ 1.5 | | ratio ( adv : clean ) | 2.3 | 2.4 | 2.9 | 2.9 | 3.4| 2.7 | 2.0 | 2.1 | 1.6 | 2.2 | 2.4 | | diff . ( adv - clean ) | 7.3 | 4.0 |18.3|22.6|14.6|16.9| 5.1 | 5.7 | 2.7 | 5.2 | 10.3 | $ \\textbf { ( 2 ) } $ Table ( 2 * ) demonstrates the robust error ( under PGD attack by 8/255 ) between clean trained and adversarial trained models for each class in CIFAR10 . In terms of adversarial robustness , the disparity between different classes is more obvious . For example , in Table 2 * and Figure 1 in the paper , we see for a clean trained model , the adversarial error under PGD attack for each class is around 100\\ % . However , after adv training , some classes ( such as \u201c car \u201d ) achieve high robustness with the robust error less than 35\\ % , but other classes ( such as \u201c cat \u201d ) still have high robust error which is around 80\\ % . Thus , the decreases of class-wise robust errors are also not equal ( in terms of ratio \\ & difference ) . Table ( 2 * ) Robust Error in CIFAR10 | class ( in % ) | plane | car | bird | cat | deer | dog | frog | horse | ship | truck | | clean train . rob err.|100| 100 | 100 | 100 | 100 | 100 | 100 | 100| 100 | | adv . train.rob err.| 48.4 $ \\pm $ 2.1| 34.0 $ \\pm $ 2.5 | 70.4 $ \\pm $ 3.2 | 82.1 $ \\pm $ 2.2| 74.6 $ \\pm $ 3.0| 64.4 $ \\pm $ 2.7| 50.0 $ \\pm $ 4.0 | 44.7 $ \\pm $ 2.3 | 39.5 $ \\pm $ 4.6 | 43.9 $ \\pm $ 2.6| | ratio ( adv : clean ) | 0.48 | 0.34| 0.70 | 0.82 | 0.75 | 0.64 | 0.50 | 0.45 | 0.40 | 4.44| | diff . ( adv - clean ) | -51.6 | -66 . | -29.6 | -17.9 | -25.4 | -35.6 | -50.0| -55.3| -60.5|-56.1| $ \\textbf { ( 3 ) } $ We also include another dataset ( GTSRB , which is composed of 43 classes of different traffic sign images ) . From the results in Figure 2 and Table 4 in the paper , for a clean trained model , more than 20 classes have almost 0\\ % error rates and the worst class has 8\\ % clean error . For an adv trained model , there are 10 classes that have large clean errors over 20\\ % but some classes maintain 0\\ % clean error . ( Note that the Figure 2 in the paper sorts the classes in descending order for the presentations of both clean and adversarial accuracy . ) As a case study , we find that in a clean model , the traffic sign `` Speed Limit 60 '' , `` Speed Limit 100 '' and `` Go Straight '' signs have 0-1\\ % clean error , but they have 14\\ % , 26\\ % , 33\\ % error in a robustly trained model . On the contrary , the class `` turn right or straight '' sign has 3\\ % clean error in both clean and robust models . It also suggests that the increase of clean error is not proportional between classes . These facts show that the `` unfair '' condition might be related to adv training and not because the total error is large ."}, "1": {"review_id": "vOchfRdvPy7-1", "review_text": "Thank you for your response , which cleared up some of my questions so I increased my score to a 5 . I would have liked more analytical / empirical evidence to substantiate some of the claims made in response to the questions numbered 4 , 5 , 12 , and 13 in the comments provided by the authors . My overall opinion remains unchanged but I encourage the authors to continue this line of work on build on these results and incorporate the feedback provided by the other reviewers as well . # # # This work makes an interesting observation that techniques designed to maximize robustness to adversarial examples may have negative consequences on the model 's performance . Typically , this has been studied primarily with an emphasis on accuracy . Here the authors look at fairness instead . - The first sentence of the introduction makes strong claim about the implications of adversarial examples without providing appropriate references or supporting the claim with evidence . - Adversarial examples do not have to be `` imperceptible '' , the main requirement is that the perturbation introduced does not affect semantics of the input . - Adversarial training has been subsumed into verifiable approaches to defending against p-norm bounded adversaries ( e.g. , randomized smoothing , etc . ) . Could you comment on why these approaches were left out of the experiments here ? They would be able to prove robustness is achieved uniformly for each training example . - If we know that adversarial training impacts accuracy ( as recently re-demonstrated by Tsipras et al . ) , could you comment on why it would have been expected that fairness would not be impacted given that the training set is balanced among classes and certain pairs of classes are closer to one-another under the p-norm than other pairs of classes ? - The presentation is somewhat confusing given that it entangles the notions of fairness and safety . This seems likely to lead to confusion from the readers . I would recommend splitting the work into two distinct studies : one of the worst-case performance of the classifiers in the absence of an adversary ( i.e. , safety ) and the other being on fairness itself . Unless the authors wish to demonstrate explicitly the connection between safety and certain definitions of fairness , in which case this connection should be presented more upfront in the paper . - Typo/grammar in `` In the lower dimensional space , an optimal linear model is more sensitive to the inherent data distributional difference and be biased when making predictions . '' - Could you motivate the choice of dataset here ? Why is CIFAR10 a good dataset to study fairness ? - Figure 1 : I would recommend using another type of graph . Typically series like this are used to represent data that is sequentially meaningful whereas here a bar plot or something along these lines would probably convey your message better . - The paper does not explicitly state which fairness definition is used ( for instance in Section 2 ) while this is important here . In particular , the definition considered seems to be based on parity but this is not explicitly stated there . Would the approach considered here apply to other definitions ? - It is not clear what is the applicability of the analysis based on robust/non-robust features is beyond the toy dataset considered here . Could you motivate better why this toy dataset matters in the context you are considering here ? - Grammar error in the statement of Theorem 1 - Grammar in `` should not be too larger '' ( page 5 ) - Could you motivate the choice of different thresholds on the natural model error and adversarial model error ? - Missing Section reference on page 6 - Section 4.3 discusses the intricacies and possible tensions between natural and robust accuracies and fairness . The re-margin strategy proposed seems to be at odds with recent results demonstrating that improving robustness to p-norm perturbations conflicts with generalization ( see Tramer et al.in ICML20 ) . Have you thought about the suitability of the p-norm constraint placed on adversaries in the context of your approach ? Does this create an artificial tension with fairness for the same reason that it creates an artificial tension with accuracy ( because the p-norm is not aligned with class semantics ) . - Related to this , Table 1 suggests that there is increased tension with natural accuracy when employing FRL compared to the baselines considered . Is that due to the point mentioned above or to what is discussed in Section 4.2 ? - The experimental evaluation is limited . It is hard to draw conclusions from one dataset . In particular , I would encourage the authors to consider datasets which are not balanced , which are from other domains than images , and which have a natural motivation for achieving `` fairness '' due to the task being solved . - Experiments should also include multiple runs and report variance . - The discussion of related work is a bit `` thin '' .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your time . We were happy to read that `` the observations are interesting '' and the insightful suggestions to further polish the work . In the following , we will clarify your key concerns . ........................................................................ Q1 . The first sentence of the introduction makes strong claim without providing appropriate references or supporting the claim with evidence . A1.We add more references and fixed improper claims in Introduction . ........................................................................ Q2 . Missing or broken reference in the paper . A2.We fixed the broken references in the paper . ........................................................................ Q3 . Why verifiable approaches are left out of the experiments ? A3.To investigate the fairness under adv training , we choose to test two most popular and representative algorithms for adv training , i.e. , PGD training and TRADES . In addition to these , we further provide experiments on certified defense ( Random Smoothing , Cohen 2019 ) in Table ( 1 * ) , which also confirms the existence of unfairness issue . Table ( 1 * ) | class ( in % ) | plane | car | bird | cat | deer | dog | frog | horse | ship | truck | |clean acc | 78.08| 96.15| 78.12 |62.5 | 84.75 |73.53| 92.16 |90.91| 90.91| 90.91| |adv acc 0.2 | 64.38| 86.54| 59.38| 37.5 | 62.71 |51.47 |74.51| 72.73 |74.55| 78.18| |adv acc 0.4 [ 35.62| 63.46 |25 . | 10.94 |23.73| 27.94 |47.06| 58.18| 47.27 |69.09 | ........................................................................ Q4 . If we know that adversarial training impacts accuracy , why it would have been expected that fairness would not be impacted given that the training set is balanced among classes and certain pairs of classes are closer to one-another under the p-norm than other pairs of classes ? A4.This could be one of the reasons why the unfairness issue occurs in some real datasets , but may not be always the case . In CIFAR10 , ( see Figure 1 ( left ) ) , some classes such as `` cat '' and `` bird '' do have 3-5\\ % lower clean accuracy than average . This may suggest some pairs of classes are indeed `` closer '' to each other in this model 's feature space . However , robust and clean models might use totally different sets of features for prediction [ Tsipras 2018 , Ilyas 2019 ] . Thus , a pair of classes which are `` close '' in the feature space of a clean model might be not `` close '' or become much `` closer '' in the feature space of a robust model . Therefore , even though the total accuracy will drop after adversarial training , it is still hard to expect how the accuracy will change in each class . Moreover , in our theoretical findings in Section 3 , we show that the `` unfairness '' can even exist for binary classification problems where there is only one pair of classes . Thus , this theoretical study demonstrates that the `` unfairness '' issue can be indeed due to the classification model itself . ........................................................................ Q5 . The presentation is somewhat confusing given that it entangles the notions of fairness and safety . A5.In this work , we focus on the general problem of unfairness in adversarial training . Safety is one of the consequences of such unfairness since certain classes with worst-cast performance can be the attacking target of the adversary . For example , for autonomous vehicles which apply robust models for traffic sign recognition tasks , if the model is very inaccurate or easy to be attacked only for some specific signs , the vehicle will still be under huge safety risks . In the experiments on the GTSRB dataset , we show the possibility of this \u201c safety \u201d concern under different traffic signs . In a clean traffic-sign classifier trained on GTSRB , we observe that the classifier can achieve high accuracy for every class which is above 92\\ % ( Figure 2 and Table 4 in the paper ) . However , when we use the adversarial training algorithm , some classes ( e.g . `` Speed Limit under 100 km/h '' ) become very inaccurate ( only has 70\\ % accuracy ) and still very easy to be attacked ( only has 16\\ % adversarial accuracy ) . ........................................................................ Q6 . Typos and grammar error A6 . We have corrected all the typos and grammar errors being pointed out ."}, "2": {"review_id": "vOchfRdvPy7-2", "review_text": "This paper begins with the empirical observation that adversarially trained models often exhibit a large different in clean ( and robust ) accuracies across different classes . This is an important observation , which I have not seen in published work ( though I believe is generally understood by many practitioners with the robustness community ) , and is an important contribution . The paper then proceeds to use a theoretical example ( adapted from the model in Tsipras et al 2018 ) where adversarial training increases the accuracy difference across classes . More on this later . The paper proposes an algorithm , Fair Robust Learning ( FRL ) , to address this issue . The starting point is a standard Lagrangian-based approach to approximately ensure constraints that the performance for each class should be close to the overall performance . The FRL algorithm then proposes 2 modifications . First , rather than applying the constraint on the robust errors directly , they decompose the robust error into clean error and a robustness term ( similar to TRADES ) and approximately maintain constraints on both terms . Second , for difficult classes , they propose increasing the adversarial radius , instead of/in addition to only increasing the corresponding Lagrange multiplier . The experiments demonstrate that their FRL approach improves the worst-class clean and robust errors , relative to both standard adversarial training and the baseline Lagrangian approach , without losing much in average-class errors . Overall , I believe the paper is fairly well-executed and investigates an important topic . The motivating empirical observations are an important contribution . The proposed approach is natural and well-motivated , and the experiments show improvements in the worst-class errors , as should be expected conceptually . The empirical explorations of different variations on the core idea are also valuable . There were some points in the paper which I believe could be improved . For me , the theoretical example does not add much insight regarding why this effect occurs . - Regarding Theorem 1 , the comment says that `` when the term A is large , the model has close clean errors between the 2 classes , namely $ R_ { nat } ( f , -1 ) ~ R_ { nat } ( f , +1 ) $ . Assuming A^2 > > q ( K ) , the terms simplify to : $ $ R_ { nat } ( f , -1 ) = P [ N ( 0 , 1 ) \\leq A ( 1- \\sqrt { K } ) ] $ $ $ $ R_ { nat } ( f , +1 ) = P [ N ( 0 , 1 ) \\leq A ( 1- K ) ] $ $ which means we have a $ \\sqrt { K } $ factor difference between the two errors . In this case , it seems that even before adversarial training , we see a difference in class errors between the classes . - Additionally , in Theorem 2 , using a similar approximation B^2 > > q ( K ) , we 'd have $ $ R_ { nat } ( f_ { adv } , -1 ) = P [ N ( 0 , 1 ) \\leq B ( 1- \\sqrt { K } ) ] $ $ $ $ R_ { nat } ( f_ { adv } , +1 ) = P [ N ( 0 , 1 ) \\leq B ( 1- K ) ] $ $ where the two terms differ by a $ \\sqrt { K } $ factor again , only the leading constant is different . - The claim in Equation 12 that `` the ratio $ \\Omega $ is large ( e.g. > 1 ) '' seems to be the result the section builds to , but it seems it is more naturally explained by the fact that if there is already a difference in clean errors , since adversarial training will increase these errors ( Tsipras et al 2018 ) , then adversarial training will also scale up the difference in these class-wise errors . It 's possible that this explanation is actually what 's happening , and it does n't seem clearly inconsistent with the empirical data , since in Figure 1 ( left ) , there are already class-wise differences , which generally match the shape of the class-wise differences after adversarial training . But it does seem very different from the story the paper is trying to tell , and it would be good to figure out which one it is ( or acknowledge both ) . Moving on to the experiments : - The TRADES baseline has robust accuracy almost 10 % lower than the standard TRADES model - can you comment on this ? - Section 4.3 comments `` we claim that only upweighting its cost ( or Reweight ) could not succeed to fulfill the cost-sensitive classification goal in adversarial setting '' - could you explain why this is ? ( after all , reweighting is the basic approach used here too ) - How does the Re-Margin approach compare to the Reweight approach , when the reweighting also involves sweeping over the training $ \\epsilon $ ? - It 's nice to see the code . Hopefully this can also be released along with the paper if accepted . Other notes : - It would be nice to connect this to the broader fairness literature . This is unfortunately not my area of expertise , but difference in performances between classes ( or demographic groups ) is a very common metric , and it would be nice to relate this work more closely to work there . `` Distributionally Robust Neural Networks for Group Shifts '' ( Sagawa et al 2020 ) is a ( somewhat arbitrary ) example of work which seems related , but I 'm sure there are many more . It would be great to connect to this , especially papers which address why/when difference in performance across classes is expected . I give the paper a 6 overall , though could adjust my rating in either direction depending on the author feedback . ________________ EDIT : Score changed from 6 to 5 during discussion , see comments below .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for taking the time to review our paper . We were happy to read that `` the observations are important '' and `` the investigations are well-executed '' . We are also very appreciated the reviewer 's effort to understand our theories . In the following , we will clarify your key concerns . $ \\textbf { Question 1 } $ . In the theory , does adversarial training only just scale up the difference in these class-wise errors ? To answer this question , we did the following further theoretical understandings -- ( 1 ) We agree with the point that : in this binary classification setting , there originally exist ( negligible ) difference between classes errors . ( 2 ) Adversarial training will enlarge the disparity but may not just scale up the errors ( Z-scores ) . $ \\textbf { ( 1 ) } $ Regarding Theorem 1 for clean models , we agree with the reviewer 's understanding about our induction . Because the term $ A\\propto\\sqrt { m+d } $ and $ A > > q ( K ) $ , so we can have the simplification to get the in-class errors : $ $ R_\\text { nat } ( f ; -1 ) = \\text { Pr . } [ \\mathcal { N } ( 0,1 ) \\leq A ( 1-\\sqrt { K } ) ] $ $ $ $ R_\\text { nat } ( f ; +1 ) = \\text { Pr . } [ \\mathcal { N } ( 0,1 ) \\leq A ( 1-K ) ] $ $ Thus , there already exist accuracy disparity between 2 classes in a clean model : $ R_\\text { nat } ( f ; -1 ) < R_\\text { nat } ( f ; +1 ) $ . One thing to note is that : because $ K > 1 $ and $ A \\propto\\sqrt { m+d } $ ( $ d $ is the dimension of non-robust features and $ d $ is large ) , we can assume these 2 Z-scores are very large negative numbers . Therefore , the errors of both two classes are small and their difference is also small and even negligible . $ \\textbf { ( 2 ) } $ Regarding Theorem 2 for robust models , because the term $ B\\propto\\sqrt { m } $ , where $ m $ is the dimension of robust features and much smaller than $ d $ ( dimension of non-robust features ) , we can not have the assumption that $ B > > q ( K ) $ . Thus , we can not directly have the similar simplifications for Eq . ( 8 ) ( 9 ) as above . As a result , the 2 classes ' error disparity condition are not the same between a clean model and a robust model . For a robust model , this ratio between the 2 Z-scores are closely related to the property of the robust model itself , such as the dimension $ m $ of robust features it uses . In our main claim in Eq.12 , Corollary 1 , we clearly stated that adversarial training will enlarge the 2 classes ' error difference , given the discussions above that the ratio of Z-scores are not always the same ."}, "3": {"review_id": "vOchfRdvPy7-3", "review_text": "Summary : This paper introduces a fairness perspective on accuracy performance among distinct classes in the context of adversarial training . It makes an observation that adversarial training algorithms ( Madry et al.2017 , Zhang et al.2019 ) yield biased performances on CIFAR 10 . It also offers a theoretical study under a Gaussian mixture setting that respects Eq . ( 1 ) .Three versions of fair robust algorithms are proposed and evaluated on CIFAR 10 . Strength : The paper makes an interesting observation on a fairness perspective in adversarial training . It provides a theoretical insight that tries to explain the observation in a certain setting Eq . ( 1 ) in which low-dimensional robust features play a dominant role in adversarial training . Weakness : While the observation is new and interesting , this reviewer wonders whether it happens \u201c fundamentally \u201d in any adversarial training setting . The theoretical study definitely offers a deeper understanding , yet it is analyzed under a particular setting Eq . ( 1 ) which I am not sure if it represents most practically-relevant scenarios . Also , all the experimental results are provided for the two algorithms ( Madry et al.2017 , Zhang et al.2019 ) under only one dataset CIFAR 10 . As the authors mentioned in Section 6.2 , this may be due to particular algorithms and data distribution . If that is the case , it would be good to see that the observation may not happen in other settings . Otherwise ( if the observation is fundamental ) , a thorough theoretical study together with extensive experiments that cover more diverse datasets are desired to be presented . Theoretical analysis : I can barely follow Theorems 1/2 and their implications stated in the paper . But it was not intuitively clear to me why the unfairness issue occurs in the considered setting . Any elaboration on intuition and insight might help . There are many grammatical errors and typos .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback . Your comments of `` the observations are interesting '' and `` the theory part offers deeper understanding of this topic '' are very encouraging . In the following , we will clarify your key concerns . Question 1 : Whether the \u201c unfairness \u201d phenomenon is \u201c fundamental \u201d for any adversarial training setting ? Whether the analysis under setting Eq . ( 1 ) is practical ? To answer this question , ( 1 ) we first make clarifications on what kind of adversarial training methods that we claim to have this `` unfair '' phenomenon ; ( 2 ) We provide more empirical evidence to show the existence of the phenomenon ; and ( 3 ) We explain why our theoretical setting is general and representative for practical-relevant scenarios . $ \\textbf { ( 1 ) } $ In this work , we concentrate on general adversarial training approaches which follow the robust optimization problem to minimize the average robust error under the same perturbation budgets : $ $ \\text { minimize } _f \\text { Pr . } ( ||\\delta||\\leq\\epsilon , f ( x+\\delta ) \\neq y ) . $ $ Theoretically , the unfairness issue is analyzed under this adversarial training objective . Empirically , the algorithms being tested , i.e. , PGD training and TRADES , are two most popular and representative algorithms for such optimization problem . $ \\textbf { ( 2 ) } $ In addition , we further provide experiments on one more robust training algorithm in CIFAR10 , [ Random Smoothing , Cohen 2019 ] , as well as one more dataset ( GTSRB ) . Specifically , Randomized Smoothing is a certified defense which implicitly minimizes model 's ( certified ) robust error . In Table 1 * , we obverse the similar `` unfair '' phenomenon and disparity relation among classes , compared to PGD training and TRADES . In Table 2 * ( Figure 2 and Table 4 in the paper ) , we also consider another dataset , GTSRB , which is composed of 43 classes of traffic sign images . In Table ( 5 * ) , for a clean trained model , most classes have almost 0\\ % error rates . For an adv trained model , there are several classes that have large errors ( e.g.above 20\\ % ) but some classes also maintain 0\\ % error . Similar observations can be made for class-wise robustness performance in this dataset . All evidence supports that the `` unfair '' phenomenon is likely to be fundamental for adversarial training . Table ( 1 * ) Clean / Robust errors of Randomized Smoothing ( $ \\sigma = 0.5 $ ) . | class ( in % ) | plane | car | bird | cat | deer | dog | frog | horse| ship | truck | | clean acc | 78.08 | 96.15 | 78.12 | 62.5 | 84.75 | 73.53|92.16 | 90.91 | 90.91| 90.91| | adv acc 0.2 | 64.38 | 86.54 | 59.38 | 37.5 | 62.71 | 51.47 | 74.51| 72.73 | 74.55 | 78.18| | adv acc 0.4 | 35.62 | 63.46 | 25 . 0 |10.94 | 23.73| 27.94 | 47.06| 58.18| 47.27 |69.09| Table ( 2 * ) Clean / Robust errors in GTSRB dataset . | ( in % ) | Avg . Clean | Avg . Rob.| Best Clean | Best Rob . | Worst Clean | Worst Rob | | clean train . | 0.5 | 81.1 | 0.0 | 27.5 | 8.3 | 100 | | PGD adv train . | 5.5 | 55.6 | 0.0 | 10.0 | 50.0 | 98.6 | | TRADES train . | 8.8 | 52.8 | 0.0 | 8.0 | 65.7 | 96.6 | ( 3 ) The theoretical model being analyzed in Eq.1 is generally used in previous works [ Tsipras 2018 , Ilyas 2019 ] to reveal one key behavior of adversarial training : robust models use low dimensional robust features for prediction , while non-robust models use high dimensional features . This theoretical setting has been shown to well reflect many properties about adversarial training in real datasets [ Ilyas 2019 , Gourdeau 2019 ] . In Eq.1 , based on this setting , we consider a more general condition where the 2 classes have different variances under the mixture Gaussian distribution . Thus if the unfairness problem exists for such a simple model , it is likely to exist for more complicated models and data distribution . This assumption is verified in our empirical study ."}}