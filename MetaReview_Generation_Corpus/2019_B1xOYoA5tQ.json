{"year": "2019", "forum": "B1xOYoA5tQ", "title": "Multi-way Encoding for Robustness to Adversarial Attacks", "decision": "Reject", "meta_review": "This paper proposes a method for improving robustness to black-box adversarial attacks by replacing the cross-entropy layer with an output vector encoding scheme. The paper is well-written, and the approach appears to be novel. However, Reviewer 4 raises very relevant concerns regarding the experimental evaluation of the method, including (a) lack of robustness without AT in the whitebox case (which is very relevant as we still lack good understanding of blackbox vs whitebox robustness) (b) comparison with Kannan et al and (c) lack of some common strong attacks. Reviewer 1 echoes many of these concerns.", "reviews": [{"review_id": "B1xOYoA5tQ-0", "review_text": "This paper argues that the vulnerability of classifiers to (black-box) adversarial attacks stems from the use of a final cross-entropy layer trained on one-hot labels. The authors propose replacing this layer by encoding each label as a high-dimensional vector and then training the classifier to minimize the L2 distance of the classifier output from the encoding of the correct class. While the approach is interesting and the paper well-written, both the motivation and the experimental evaluation is insufficient. Hence I consider it below the ICLR bar. I find the approach weakly motivated. The argument in Figure 1 is very hand-wavy with no clear experimental or theoretical support. The authors argue that cross-entropy with one hot labels causes gradient correlation in the last layer and this propagates all the way through the network (bottom of page 3) but there are no experiments supporting this conjecture. Moreover, the approach is not fundamentally different from standard networks with cross-entropy training. One can consider adding an extra layer (with number of neurons equal to the encoding vector dimensions) and keeping the weights of these neurons fixed (the output weights are essentially the encoding dictionary). Then training with cross-entropy is increasing the inner product with these vectors. This is qualitatively very similar to the proposed approach of this paper. Is there a benefit from explicitly considering the encoding vectors? Moreover, why is the length of the encoding vectors important from a conceptual point of view? As far as I can tell, this is simply encouraging the output of the network to be large in norm. This could be leading to gradient masking, similar to the phenomena observed for defensive distillation. I find the proposed approach to watermark evasion interesting. However I consider it orthogonal to the rest of the results so it is hard to consider it as a contribution to the main point of the paper. Figure 3 is missing white-box evaluation of RO classifiers. Is this on purpose? It is important to understand if the claimed improvement in robustness actually stems from RO rather than mostly from combining it with adversarial training. The authors report an increase in white-box adversarial robustness. However, I don't believe that the evaluation of their method is thorough. There are plenty of examples by now where PGD has not been sufficient to evaluate the ground-truth robustness of a model. This can distort the relative robustness of different approaches. Given that the increase from baselines in white-box robustness is relatively small (<10% for most datasets) a much more thorough evaluation is required to conclusively demonstrate the benefit of this method. For instance, applying the SPSA attack from Uesato et al. (2018, https://arxiv.org/abs/1802.05666) or a variant of the CW (https://arxiv.org/abs/1608.04644) attack adapted to the particular method used. As an additional point of concern emphasizing this issue, the authors present the results of Kannan et al. (2018) as state-of-the-art. So far, there is no conclusive evidence about ALP improving the robustness of neural networks beyond adversarial training. The original paper was found to be not as robust as claimed and retracted from NIPS. A similar paper reporting ALP to improve robustness in smaller datasets (CIFAR10) was submitted to ICLR (https://openreview.net/forum?id=Bylj6oC5K7) but was withdrawn after the authors performed additional experiments. The fact that the authors find the approach of Kannan et al. (2018) to offer an increase over the robustness of Madry et al. (2017) thus raises concerns about the reliability of the evaluation. Other comments: -- When the authors perform PGD, what is exactly the loss it is applied on? Is it clear that this is the optimal loss to use when attacking RO classifiers?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . 1.In response to the reviewer , we measure the correlation of gradients between all convolutional layers of the different models . We first compute the gradients of the loss with respect to intermediate features of Conv1 and Conv2 . Then , we compute the Pearson correlation coefficient of the sign of the gradients with respect to such intermediate features between models . For further comparison , we train models \u2018 A \u2019 _1ofK \u2019 and \u2018 A \u2019 _RO \u2019 , that are independently initialized from \u2018 A_1ofK \u2019 and \u2018 A_RO \u2019 . This correlation analysis is reported in Appendix B of the manuscript . We find that the correlations of Conv1 and Conv2 between \u2018 1ofK \u2019 models are much higher than those of \u2018 RO \u2019 models . In addition , even though \u2018 RO \u2019 models used the same output encoding , they are not highly correlated . We also find that the correlations between \u2018 RO \u2019 and \u2018 1ofK \u2019 are low . In response to the reviewer \u2019 s request , we perform additional experiments with a model that we denote as \u2018 RO_softmax \u2019 . We train \u2018 A_RO_softmax \u2019 , \u2018 C_RO_softmax \u2019 models , and evaluate the models under FGSM attacks from \u2018 1ofK \u2019 substitute models . We also report Pearson correlation coefficients of the sign of the input gradients between the substitute and the target models as explained in Section 4.1.1 . The results are reported in Appendix A of the manuscript . Models with \u2018 RO_Softmax \u2019 and \u2018 1ofK_MSE \u2019 are less robust to attacks from \u2018 1ofK \u2019 compared to \u2018 RO \u2019 and have higher correlations between \u2018 1ofK \u2019 models . 2.Using a higher dimension ( > K ) results in a less constrained gradient space compared to that of 1ofK encoding . [ 1 ] addresses the importance of the norm in the input to the softmax layer ( Section 3 of [ 1 ] ) so that the norm has a massive impact on the softmax output . However , we do not use a softmax and our loss is determined by the relative difference between the output and the ground-truth vector which is not related to the norm of the output . Therefore the explanation in [ 1 ] is not directly applicable to our setup . 3.We demonstrate watermarking evasion to further demonstrate that our proposed approach decorrelates model predictions on watermarked images . We interpret a watermarked image used to deliberately cause a misclassification as an adversarial example . * When the encoding of the substitute and target models is different , adversarial examples become less transferable . * 4.Table 3 was intended to only show the results of Black-box attacks without adversarial training . The purpose of presenting the White-box ( 1ofK ) result is to show how transferable Black-box attacks ( 1ofK ) are for reference . We remove the row of White-box ( 1ofK ) , together with its referenced text , in order to avoid confusion . For the White-box ( RO ) without adversarial training , we achieve 0.1 % classification accuracy on untargeted attacks and 74.7 % attack success rate on targeted attacks for MNIST . Without adversarial training , the RO model is still vulnerable to untargeted attacks and shows improvements on targeted attacks . This indicates that our model does not break gradient descent . Considering the space constraints , we decided to instead demonstrate the main argument : RO gives better performance than 1ofK under White-box attacks with adversarial training as reported in Table 4 ."}, {"review_id": "B1xOYoA5tQ-1", "review_text": "Authors proposes new method against adversarial attacks. Paper is organized well and easy to follow. Basically, authors notice that gradients of a deep neural network when one hot encoding is used can be highly correlated and hence can be used in the design on an attack. Authors supply extensive experimental evidence to support their method. Those experiments shows significant amount of gains compared to baselines. Although proposed method is neat, I believe it has room to be improved. One question which is bothering me is: Given that one hot encoding is not optimal, can one find optimal (highly resistant to any attack) encoding? One may use evolutionary computing to empirically analyse such a encoding or one may come up an existence/non-existence proof (I am not expert in the field however I guess ecoc field should have investigates similar problems) of such encoding. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We agree that optimizing the output encoding to maximize robustness to attack is not sufficiently explored in deep models and is an interesting future direction for our research ."}, {"review_id": "B1xOYoA5tQ-2", "review_text": "This paper argues that a random orthogonal output vector encoding is more robust to adversarial attacks than the ubiquitous softmax. The reasoning is as follows: 1. different models that share the same final softmax layer will have highly correlated gradients in this final layer 2. this correlation can be carried all the way back to the input pertubations 3. the use of a multi-way encoding results in a weaker correlation in gradients between models I found (2) to be a surprising assumption, but it does seem to be supported by the experiments. These show a lower correlation in input gradients between models when using the proposed RO encoding. They also show an increased resiliance to attack in a number of different settings. Overall, the results seem to be impressive. However, I think the paper would be a lot stronger if there was a more thorough investigation of the correlation between gradients in all layers of the models. I did not find the discussion around Figure 1 to be very compelling, since it is only relevant to the encoding layer, while we are only interested in gradients at the input layer. The correlation numbers in Table 2 are unexpected and interesting. I would like to see a deeper investigation of these correlations. I am not familiar with the broader literature in this area, so giving myself low confidence. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . In response to the reviewer , we measure the correlation of gradients between all convolutional layers of the different models . We first compute the gradients of the loss with respect to intermediate features after Conv1 and Conv2 . Then , we compute the Pearson correlation coefficient of the sign of the gradients with respect to such intermediate features between models . For further comparison , we train models \u2018 A \u2019 _1ofK \u2019 and \u2018 A \u2019 _RO \u2019 , that are independently initialized from \u2018 A_1ofK \u2019 and \u2018 A_RO \u2019 . This correlation analysis is reported in Appendix B of the manuscript . We find that the correlations of Conv1 and Conv2 between \u2018 1ofK \u2019 models are much higher than those of \u2018 RO \u2019 models . In addition , even though \u2018 RO \u2019 models used the same output encoding , they are not highly correlated . We also find that the correlations between \u2018 RO \u2019 and \u2018 1ofK \u2019 are low ."}, {"review_id": "B1xOYoA5tQ-3", "review_text": "This work proposes an alternative loss function to train models robust to adversarial attacks. Specifically, instead of the common sparse, N-way softmax-crossentropy loss, they propose to minimize the MSE to the target column of a random, dense orthogonal matrix. I believe the high-level idea behind this work is that changing the target labelspace is a more effective means of defending against adversarial attacks than modifying the underlying architecture, as the loss-level gradients will be strongly correlated across all architectures in the latter scenario. Pros: -Paper was easy to follow -Using orthogonal encodings to decorrelate gradients is an interesting idea -Benchmark results appear promising compared to prior works Cons: -This work claims that their RO-formulation is fundamentally different from 1-of-K, but I'm not completely sure that's true. One could train a classification model where the final fully connected layer (C inputs K output logits) were a frozen matrix (updates disabled) of K orthogonal basis vector (ie, the same as the C_{RO}) codebook they propose. The inputs to this layer would probably have to be L2 normalized, and the output logits would then proceed through a softmax-crossentropy layer. Would this be any less effective than the proposed scheme? -Another baseline/sanity test that should probably included is how does the 1-of-K softmax/cross-entropy compare with the proposed method where encoding length l = k and the C_{RO} codebook is just the identity matrix? -Some of the numbers in Table 4 are pretty close. Since the authors are replicating Kannan et al, it would be best to included error bars when possible to account for differences in random initializations. -It is unclear the extent to which better classification performance on the clean input generalizes to datasets such as ImageNet Overall, I think the results are promising, but I'm not fully convinced that similar results cannot be achieved using standard cross-entropy losses with 1-hot labels.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . 1. and 2.In response to the reviewer \u2019 s request , we perform additional experiments with models that we denote as \u2018 RO_softmax \u2019 ( suggestion 1 ) and \u2018 1ofK_MSE \u2019 ( suggestion 2 ) . We train \u2018 A_RO_softmax \u2019 , \u2018 A_1ofK_MSE \u2019 , \u2018 C_RO_softmax \u2019 , and \u2018 C_1ofK_MSE \u2019 models , and evaluate the models under FGSM attacks from \u2018 1ofK \u2019 substitute models . We also report Pearson correlation coefficients of the sign of the input gradients between the substitute and the target models as explained in Section 4.1.1 . The results are reported in Appendix A of the manuscript . Models with \u2018 RO_Softmax \u2019 and \u2018 1ofK_MSE \u2019 are less robust to attacks from \u2018 1ofK \u2019 compared to \u2018 RO \u2019 and have higher correlations between \u2018 1ofK \u2019 models . 4.Adversarial robustness on ImageNet is still an open problem . Even using the conventional 1ofK encoding , it is hard to generalize to clean data when a model is trained with adversarial training [ 1 ] . Tsipras et al.adversarially train a model with L-infinity=0.05 on Restricted ImageNet , a smaller subset of ImageNet , and the clean accuracy is decreased by ~70 % compared to standard training ( without adversarial training ) . In addition , adversarial training on ImageNet is very costly . Kannan et al . [ 2 ] managed to perform adversarial training on ImageNet with 53 GPUs . Due to the computational constraints , most of the papers at ICLR 2018 on adversarial robustness do not experiment with ImageNet . ( e.g. , Madry et al.ICLR 2018 , Buckman et al ICLR 2018 , Na et al.ICLR 2018 ) . We would like to emphasize that scaling-up adversarial training for ImageNet is beyond the scope of this work , but we obtain promising generalizability on smaller scale datasets with adversarial training . [ 1 ] Tsipras , Dimitris , et al . `` Robustness may be at odds with accuracy . '' arXiv preprint arXiv:1805.12152 ( 2018 ) . [ 2 ] Kannan , Harini , Alexey Kurakin , and Ian Goodfellow . `` Adversarial Logit Pairing . '' arXiv preprint arXiv:1803.06373 ( 2018 ) . 3.We present the mean and standard deviation of five runs here : MNIST : 93.3 ( +/- 0.98 ) , 96.5 ( +/- 0.31 ) , 98.8 ( +/- 0.12 ) SVHN : 45.2 ( +/- 1.30 ) , 56.7 ( +/- 0.47 ) , 90 ( +/- 0.44 ) Cifar100 : 21.2 ( +/- 0.25 ) , 42.6 ( +/- 0.20 ) , 60.0 ( +/- 0.27 ) Cifar10 : 52.4 ( +/- 0.33 ) , 65.8 ( +/- 0.13 ) , 86.5 ( +/- 0.17 )"}], "0": {"review_id": "B1xOYoA5tQ-0", "review_text": "This paper argues that the vulnerability of classifiers to (black-box) adversarial attacks stems from the use of a final cross-entropy layer trained on one-hot labels. The authors propose replacing this layer by encoding each label as a high-dimensional vector and then training the classifier to minimize the L2 distance of the classifier output from the encoding of the correct class. While the approach is interesting and the paper well-written, both the motivation and the experimental evaluation is insufficient. Hence I consider it below the ICLR bar. I find the approach weakly motivated. The argument in Figure 1 is very hand-wavy with no clear experimental or theoretical support. The authors argue that cross-entropy with one hot labels causes gradient correlation in the last layer and this propagates all the way through the network (bottom of page 3) but there are no experiments supporting this conjecture. Moreover, the approach is not fundamentally different from standard networks with cross-entropy training. One can consider adding an extra layer (with number of neurons equal to the encoding vector dimensions) and keeping the weights of these neurons fixed (the output weights are essentially the encoding dictionary). Then training with cross-entropy is increasing the inner product with these vectors. This is qualitatively very similar to the proposed approach of this paper. Is there a benefit from explicitly considering the encoding vectors? Moreover, why is the length of the encoding vectors important from a conceptual point of view? As far as I can tell, this is simply encouraging the output of the network to be large in norm. This could be leading to gradient masking, similar to the phenomena observed for defensive distillation. I find the proposed approach to watermark evasion interesting. However I consider it orthogonal to the rest of the results so it is hard to consider it as a contribution to the main point of the paper. Figure 3 is missing white-box evaluation of RO classifiers. Is this on purpose? It is important to understand if the claimed improvement in robustness actually stems from RO rather than mostly from combining it with adversarial training. The authors report an increase in white-box adversarial robustness. However, I don't believe that the evaluation of their method is thorough. There are plenty of examples by now where PGD has not been sufficient to evaluate the ground-truth robustness of a model. This can distort the relative robustness of different approaches. Given that the increase from baselines in white-box robustness is relatively small (<10% for most datasets) a much more thorough evaluation is required to conclusively demonstrate the benefit of this method. For instance, applying the SPSA attack from Uesato et al. (2018, https://arxiv.org/abs/1802.05666) or a variant of the CW (https://arxiv.org/abs/1608.04644) attack adapted to the particular method used. As an additional point of concern emphasizing this issue, the authors present the results of Kannan et al. (2018) as state-of-the-art. So far, there is no conclusive evidence about ALP improving the robustness of neural networks beyond adversarial training. The original paper was found to be not as robust as claimed and retracted from NIPS. A similar paper reporting ALP to improve robustness in smaller datasets (CIFAR10) was submitted to ICLR (https://openreview.net/forum?id=Bylj6oC5K7) but was withdrawn after the authors performed additional experiments. The fact that the authors find the approach of Kannan et al. (2018) to offer an increase over the robustness of Madry et al. (2017) thus raises concerns about the reliability of the evaluation. Other comments: -- When the authors perform PGD, what is exactly the loss it is applied on? Is it clear that this is the optimal loss to use when attacking RO classifiers?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . 1.In response to the reviewer , we measure the correlation of gradients between all convolutional layers of the different models . We first compute the gradients of the loss with respect to intermediate features of Conv1 and Conv2 . Then , we compute the Pearson correlation coefficient of the sign of the gradients with respect to such intermediate features between models . For further comparison , we train models \u2018 A \u2019 _1ofK \u2019 and \u2018 A \u2019 _RO \u2019 , that are independently initialized from \u2018 A_1ofK \u2019 and \u2018 A_RO \u2019 . This correlation analysis is reported in Appendix B of the manuscript . We find that the correlations of Conv1 and Conv2 between \u2018 1ofK \u2019 models are much higher than those of \u2018 RO \u2019 models . In addition , even though \u2018 RO \u2019 models used the same output encoding , they are not highly correlated . We also find that the correlations between \u2018 RO \u2019 and \u2018 1ofK \u2019 are low . In response to the reviewer \u2019 s request , we perform additional experiments with a model that we denote as \u2018 RO_softmax \u2019 . We train \u2018 A_RO_softmax \u2019 , \u2018 C_RO_softmax \u2019 models , and evaluate the models under FGSM attacks from \u2018 1ofK \u2019 substitute models . We also report Pearson correlation coefficients of the sign of the input gradients between the substitute and the target models as explained in Section 4.1.1 . The results are reported in Appendix A of the manuscript . Models with \u2018 RO_Softmax \u2019 and \u2018 1ofK_MSE \u2019 are less robust to attacks from \u2018 1ofK \u2019 compared to \u2018 RO \u2019 and have higher correlations between \u2018 1ofK \u2019 models . 2.Using a higher dimension ( > K ) results in a less constrained gradient space compared to that of 1ofK encoding . [ 1 ] addresses the importance of the norm in the input to the softmax layer ( Section 3 of [ 1 ] ) so that the norm has a massive impact on the softmax output . However , we do not use a softmax and our loss is determined by the relative difference between the output and the ground-truth vector which is not related to the norm of the output . Therefore the explanation in [ 1 ] is not directly applicable to our setup . 3.We demonstrate watermarking evasion to further demonstrate that our proposed approach decorrelates model predictions on watermarked images . We interpret a watermarked image used to deliberately cause a misclassification as an adversarial example . * When the encoding of the substitute and target models is different , adversarial examples become less transferable . * 4.Table 3 was intended to only show the results of Black-box attacks without adversarial training . The purpose of presenting the White-box ( 1ofK ) result is to show how transferable Black-box attacks ( 1ofK ) are for reference . We remove the row of White-box ( 1ofK ) , together with its referenced text , in order to avoid confusion . For the White-box ( RO ) without adversarial training , we achieve 0.1 % classification accuracy on untargeted attacks and 74.7 % attack success rate on targeted attacks for MNIST . Without adversarial training , the RO model is still vulnerable to untargeted attacks and shows improvements on targeted attacks . This indicates that our model does not break gradient descent . Considering the space constraints , we decided to instead demonstrate the main argument : RO gives better performance than 1ofK under White-box attacks with adversarial training as reported in Table 4 ."}, "1": {"review_id": "B1xOYoA5tQ-1", "review_text": "Authors proposes new method against adversarial attacks. Paper is organized well and easy to follow. Basically, authors notice that gradients of a deep neural network when one hot encoding is used can be highly correlated and hence can be used in the design on an attack. Authors supply extensive experimental evidence to support their method. Those experiments shows significant amount of gains compared to baselines. Although proposed method is neat, I believe it has room to be improved. One question which is bothering me is: Given that one hot encoding is not optimal, can one find optimal (highly resistant to any attack) encoding? One may use evolutionary computing to empirically analyse such a encoding or one may come up an existence/non-existence proof (I am not expert in the field however I guess ecoc field should have investigates similar problems) of such encoding. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We agree that optimizing the output encoding to maximize robustness to attack is not sufficiently explored in deep models and is an interesting future direction for our research ."}, "2": {"review_id": "B1xOYoA5tQ-2", "review_text": "This paper argues that a random orthogonal output vector encoding is more robust to adversarial attacks than the ubiquitous softmax. The reasoning is as follows: 1. different models that share the same final softmax layer will have highly correlated gradients in this final layer 2. this correlation can be carried all the way back to the input pertubations 3. the use of a multi-way encoding results in a weaker correlation in gradients between models I found (2) to be a surprising assumption, but it does seem to be supported by the experiments. These show a lower correlation in input gradients between models when using the proposed RO encoding. They also show an increased resiliance to attack in a number of different settings. Overall, the results seem to be impressive. However, I think the paper would be a lot stronger if there was a more thorough investigation of the correlation between gradients in all layers of the models. I did not find the discussion around Figure 1 to be very compelling, since it is only relevant to the encoding layer, while we are only interested in gradients at the input layer. The correlation numbers in Table 2 are unexpected and interesting. I would like to see a deeper investigation of these correlations. I am not familiar with the broader literature in this area, so giving myself low confidence. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . In response to the reviewer , we measure the correlation of gradients between all convolutional layers of the different models . We first compute the gradients of the loss with respect to intermediate features after Conv1 and Conv2 . Then , we compute the Pearson correlation coefficient of the sign of the gradients with respect to such intermediate features between models . For further comparison , we train models \u2018 A \u2019 _1ofK \u2019 and \u2018 A \u2019 _RO \u2019 , that are independently initialized from \u2018 A_1ofK \u2019 and \u2018 A_RO \u2019 . This correlation analysis is reported in Appendix B of the manuscript . We find that the correlations of Conv1 and Conv2 between \u2018 1ofK \u2019 models are much higher than those of \u2018 RO \u2019 models . In addition , even though \u2018 RO \u2019 models used the same output encoding , they are not highly correlated . We also find that the correlations between \u2018 RO \u2019 and \u2018 1ofK \u2019 are low ."}, "3": {"review_id": "B1xOYoA5tQ-3", "review_text": "This work proposes an alternative loss function to train models robust to adversarial attacks. Specifically, instead of the common sparse, N-way softmax-crossentropy loss, they propose to minimize the MSE to the target column of a random, dense orthogonal matrix. I believe the high-level idea behind this work is that changing the target labelspace is a more effective means of defending against adversarial attacks than modifying the underlying architecture, as the loss-level gradients will be strongly correlated across all architectures in the latter scenario. Pros: -Paper was easy to follow -Using orthogonal encodings to decorrelate gradients is an interesting idea -Benchmark results appear promising compared to prior works Cons: -This work claims that their RO-formulation is fundamentally different from 1-of-K, but I'm not completely sure that's true. One could train a classification model where the final fully connected layer (C inputs K output logits) were a frozen matrix (updates disabled) of K orthogonal basis vector (ie, the same as the C_{RO}) codebook they propose. The inputs to this layer would probably have to be L2 normalized, and the output logits would then proceed through a softmax-crossentropy layer. Would this be any less effective than the proposed scheme? -Another baseline/sanity test that should probably included is how does the 1-of-K softmax/cross-entropy compare with the proposed method where encoding length l = k and the C_{RO} codebook is just the identity matrix? -Some of the numbers in Table 4 are pretty close. Since the authors are replicating Kannan et al, it would be best to included error bars when possible to account for differences in random initializations. -It is unclear the extent to which better classification performance on the clean input generalizes to datasets such as ImageNet Overall, I think the results are promising, but I'm not fully convinced that similar results cannot be achieved using standard cross-entropy losses with 1-hot labels.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . 1. and 2.In response to the reviewer \u2019 s request , we perform additional experiments with models that we denote as \u2018 RO_softmax \u2019 ( suggestion 1 ) and \u2018 1ofK_MSE \u2019 ( suggestion 2 ) . We train \u2018 A_RO_softmax \u2019 , \u2018 A_1ofK_MSE \u2019 , \u2018 C_RO_softmax \u2019 , and \u2018 C_1ofK_MSE \u2019 models , and evaluate the models under FGSM attacks from \u2018 1ofK \u2019 substitute models . We also report Pearson correlation coefficients of the sign of the input gradients between the substitute and the target models as explained in Section 4.1.1 . The results are reported in Appendix A of the manuscript . Models with \u2018 RO_Softmax \u2019 and \u2018 1ofK_MSE \u2019 are less robust to attacks from \u2018 1ofK \u2019 compared to \u2018 RO \u2019 and have higher correlations between \u2018 1ofK \u2019 models . 4.Adversarial robustness on ImageNet is still an open problem . Even using the conventional 1ofK encoding , it is hard to generalize to clean data when a model is trained with adversarial training [ 1 ] . Tsipras et al.adversarially train a model with L-infinity=0.05 on Restricted ImageNet , a smaller subset of ImageNet , and the clean accuracy is decreased by ~70 % compared to standard training ( without adversarial training ) . In addition , adversarial training on ImageNet is very costly . Kannan et al . [ 2 ] managed to perform adversarial training on ImageNet with 53 GPUs . Due to the computational constraints , most of the papers at ICLR 2018 on adversarial robustness do not experiment with ImageNet . ( e.g. , Madry et al.ICLR 2018 , Buckman et al ICLR 2018 , Na et al.ICLR 2018 ) . We would like to emphasize that scaling-up adversarial training for ImageNet is beyond the scope of this work , but we obtain promising generalizability on smaller scale datasets with adversarial training . [ 1 ] Tsipras , Dimitris , et al . `` Robustness may be at odds with accuracy . '' arXiv preprint arXiv:1805.12152 ( 2018 ) . [ 2 ] Kannan , Harini , Alexey Kurakin , and Ian Goodfellow . `` Adversarial Logit Pairing . '' arXiv preprint arXiv:1803.06373 ( 2018 ) . 3.We present the mean and standard deviation of five runs here : MNIST : 93.3 ( +/- 0.98 ) , 96.5 ( +/- 0.31 ) , 98.8 ( +/- 0.12 ) SVHN : 45.2 ( +/- 1.30 ) , 56.7 ( +/- 0.47 ) , 90 ( +/- 0.44 ) Cifar100 : 21.2 ( +/- 0.25 ) , 42.6 ( +/- 0.20 ) , 60.0 ( +/- 0.27 ) Cifar10 : 52.4 ( +/- 0.33 ) , 65.8 ( +/- 0.13 ) , 86.5 ( +/- 0.17 )"}}