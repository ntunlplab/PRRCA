{"year": "2020", "forum": "rklOg6EFwS", "title": "Improving Adversarial Robustness Requires Revisiting Misclassified Examples", "decision": "Accept (Poster)", "meta_review": "This paper presents modifications to the adversarial training loss that yield improvements in adversarial robustness.  While some reviewers were concerned by the lack of mathematical elegance in the proposed method, there is consensus that the proposed method clears a tough bar by increasing SOTA robustness on CIFAR-10. ", "reviews": [{"review_id": "rklOg6EFwS-0", "review_text": "The paper essentially presented a viewpoint, i.e. misclassified examples may have a significant impact on the final robustness. The authors conducted a series of qualitative experiments to verify 1) Misclassified examples have more impact on the final robustness than correctly classified examples. 2) For misclassified examples, different maximization techniques may have a negligible influence, but minimization techniques play a critical role on the final robustness. 3) The authors proposed a new defense algorithm which focus on generating adversarial examples from misclassified examples during the training. The algorithm was shown to improve the final robustness by revisiting these adversarial misclassified examples. Generally speaking, the idea, though somewhat straightforward and less elegant, is reasonably presented and also well-motivated. Empirical validations seemed to support the idea and indicated that the proposed approach could improve the adversarial robustness. There are several major concerns with the paper as follows: 1. It is good and reasonable to put more emphasis on the misclassification examples, since it is likely that the region of the \u201cperturbation\u201d largely overlaps with the region of the misclassified examples. 2. However, this may be dealt with in a more elegant or systematic way. In the viewpoint of the reviewer, actually a different emphasis can be imposed on each different data point including both the correctly-classified and mis-classified samples. In another word, a more systematical extension or a metric may actually be developed emphasizing more on mis-classified samples. It is suspected that different samples within the set of mis-classified samples (or even in the set correctly-classified samples) could also have a different influence. It is highly likely that a more elegant and mathematic way can be designed for this purpose. 3. Further to point 2, in particular, the regulation w.r.t the correctly classified examples may not be ignored. It is interesting to consider differently the correctly-classified examples due to the trade-off between robustness and standard accuracy. This can also be seen in another submission of ICLR2020 (titled Sensible adversarial learning\u201d) which actually discards the mis-classified samples. I would like to see some additional comments, clarification, or discussions on this point. 4. I am curious to know if outliers would be over-emphasized by the proposed idea. Some discussion or even some illustrations on a synthetic case would be interesting. Will the existence of outliers affect the robustness? 5. In Table 2, the best and the last results on FGSM are totally identically, I wonder if this is a wrong copy, which should be further clarified and discussed. 6. In the Unlabeled Data experiment part, it is better to compare the results with both UAT++ and RST at the same time rather than separately. 7. Equation (4) shows that the two parts on the right side are added together, is that right? This is inconsistent with Equation (7) and the following description. 8. In 3.2, the first paragraph, \u201cmax-margin adversarial training (MMA) (Ding et al., 2019)\u201d appears to be wrong. The correct reference is \u201cGavinWeiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Max-margin adversarial (mma) training: Direct input space margin maximization through adversarial training. arXiv preprint arXiv:1812.02637, 2018.\u201d. ================== I have read carefully the response from the authors. The newly added results appear satisfactory to me and I am generally happy with the clarifications. I have then adjusted my rating accordingly.", "rating": "6: Weak Accept", "reply_text": "Thanks for your valuable feedback . We address them in detail as follows . Q1 & 2 : A suggestion of different emphasis on each different data point including both the correctly-classified and misclassified samples . A more elegant and systematic way can be designed for this purpose . A1 & 2 : Actually , our approach has already imposed a different emphasis on each different data point according to whether it is correctly-classified or misclassified . As presented in Eq . ( 11 ) , the weight term $ 1-p $ is applied on both correctly-classified and misclassified examples : examples of high confidence ( e.g. , correctly classified ones ) will have small $ 1-p $ , while low confidence training examples ( e.g. , misclassified ones ) will have large $ 1-p $ , that is , different training examples are treated adaptively ( during training ) and differently . We agree that there might exist a more principled and elegant way to deal with misclassified examples/correctly classified examples , and we will explore it in our future work . Q3 : Request for additional comments on another submission of ICLR2020 ( titled \u201c Sensible adversarial learning \u201d ) which considers correctly classified examples differently and discards the misclassified ones . A3 : First of all , we would like to clarify that the ICLR2020 paper \u201c Sensible adversarial learning \u201d is a concurrent submission as our work . After reading the paper , we found that their method explores different treatments on correctly-classified and misclassified examples at the maximization step of adversarial training , while our work considers different treatments at the minimization step of adversarial training , i.e. , applying example-wise regularization term on both correctly-classified and misclassified examples based on the weight $ 1-p $ . Q4 : I am curious to know if the existence of outliers affects the robustness ? A4 : We would appreciate more information on the type of outliers the reviewer would like to discuss here : 1 ) outliers that are wrongly labeled , or 2 ) outliers that are correctly labeled but far away from other data points in the same class ? Q5 : In Table 4 , the best and the last results on FGSM are identical , I wonder if this is a wrong copy . A5 : This is not a wrong copy . For Table 4 , \u201c best \u201d refers to the best checkpoint for each defense under each attack method . For all defense methods against FGSM , the \u201c best \u201d checkpoint just happens to be at the last epoch , which is why the \u201c best \u201d and the \u201c last \u201d results are identical . We have clarified this in the revision . Q6 : In the Unlabeled Data experiment part , it is better to compare the results with both UAT++ and RST at the same time rather than separately . A6 : Thank you for your suggestion . We have updated Table 5 , comparing MART with both UAT++ and RST at the same time . Please note that our conclusion remains the same , i.e. , the proposed defense MART can also benefit from unlabeled data , and further improves the UAT++ and RST defenses , achieving the state-of-the-art robustness . Q7 : Equation ( 4 ) shows that the two parts on the right side are added together , is that right ? This is inconsistent with Equation ( 7 ) and the following description . A7 : We have added more steps of derivation in Equation ( 7 ) to show the consistency between Equation ( 4 ) and Equation ( 7 ) . Q8 : The reference in 3.2 , the first paragraph , \u201c max-margin adversarial training ( MMA ) ( Ding et al. , 2019 ) \u201d appears to be wrong . A8 : Thanks for the correction . We have fixed it in the revision ."}, {"review_id": "rklOg6EFwS-1", "review_text": "Summary: ======= Neural Networks (NN) have been shown to be susceptible to various adversarial attacks i.e. if we perturb the \"x\" just a little, the output prediction changes. So, there has been much research devoted to how we can make NNs robust to such attacks. Typically, the adversarial examples that are used to train adversarially robust methods are generated from the correctly classified examples. This paper first shows empirically that the adversarial examples generated from misclassified examples by the model h_{\\theta} are as important as the ones generated from correctly classified examples on the CIFAR dataset. The authors then propose a novel objective for the adversarial risk that incorporates the misclassified examples as a regularizer. Next, a surrogate convex objective is derived which is optimized to come up with a new kind of adversarial training called misclassification aware adversarial training (MART). Originality: ========== The proposed objective seems novel to me compared to the existing methods, perhaps less so to someone who is an expert in adversarial methods. That said, I definitely found the addition of a misclassification-based regularization term as intriguing. However, I was wondering that maybe the previous approaches did not distinguish between correctly vs incorrectly classified examples while generating adversarial examples was because that is a more general setting. This is so because the idea of classification is tied to a certain model h_{\\theta} and hence is not model-agnostic by definition. Perhaps the strength of the approach by Madry et. al. 18, which is the closest work to this paper, is in being model-agnostic. I hope I am not missing something! Quality: ======= The paper is technically sound and is a solid contribution to the literature on adversarial robustness. The motivation, experimental results, and ablation studies are all very well executed. The results are shown on MNIST and CIFAR-10 image datasets and it outperforms a host of competitive baseline algorithms. Clarity: ======= The paper is well organized and is very well written. The experimental results are very thorough and well explained. Significance: ============ The paper solves an important problem of dealing with adversarial attacks on deep neural networks. Further, the solution they propose is novel and a seems like a significant improvement over the extend state-of-the-art. ", "rating": "6: Weak Accept", "reply_text": "Thanks for your positive and valuable comments . Q1 : \u201c I was wondering that maybe the previous approaches did not distinguish between correctly vs incorrectly classified examples while generating adversarial examples was because that is a more general setting . This is so because the idea of classification is tied to a certain model h_ { \\theta } and hence is not model-agnostic by definition . Perhaps the strength of the approach by Madry et . al.18 , which is the closest work to this paper , is in being model-agnostic . I hope I am not missing something ! \u201d A1 : We would like to clarify that both our approach and the standard adversarial training by Madry et . al.18 are of the same nature . Like Madry et . al.18 , our algorithm can also be applied to any models as long as it can output class probability , i.e. , $ p_k ( x_i , \\theta ) $ in Equation ( 2 ) . The difference is that our method makes the outer minimization of adversarial training more data-adaptive ( with differentiation between misclassified and correctly classified examples ) ."}, {"review_id": "rklOg6EFwS-2", "review_text": "The paper improves adversarial training by introducing two modifications to the loss function: (i) a \"boosted\" version of the cross-entropy loss that involves a term similar to a large-margin loss, and (ii) weighting the adversarial loss differently depending on how correctly classified an example is. When put together, these modifications achieve state-of-the-art robustness on CIFAR-10, improving over the previously best robust accuracy by about 3.5%. The authors perform multiple ablation studies and demonstrate that their modified loss function also improves when additional unlabeled data is added (again achieving state-of-the-art robustness). I recommend accepting the paper. The modifications for the loss function are well motivated and improve over the state of the art by a non-trivial amount. Moreover, the authors nicely put their loss function in the context of prior work. Additional comments: - In Table 4, are the \"best\" columns the best checkpoint for the respective column (potentially different checkpoints for different columns) or does \"best\" refer to a single model (for each row)? - Is 65.04% (Table 5 b) now the best published robust accuracy on CIFAR-10 (at least to best of the authors' knowledge)? If so, it may be helpful to indicate this to the reader. - In Figure 2d, it could be insightful to expand the plot further to see the regime where the performance of MART drops substantially. - In Figure 1, the three plots would be easier to compare if the y-axes were the same. - From Figure 2, it looks like the gain from the BCE loss is as large as the gain from treating misclassified examples differently. Is this correct? - I strongly encourage the authors to release their models in a format that is easy for other researchers to use (e.g., PyTorch model checkpoints). This will make it substantially easier for future work to build on the results in this paper.", "rating": "8: Accept", "reply_text": "Thanks for your valuable and positive comments . We address them in detail as follows . Q1 : In Table 4 , are the `` best '' columns the best checkpoint for the respective column ( potentially different checkpoints for different columns ) or does `` best '' refer to a single model ( for each row ) ? A1 : Here , \u201c best \u201d refers to the best checkpoint for each defense method under each attack method . Empirically , we find that , against FGSM attack , the \u201c best \u201d was found at the \u201c last \u201d checkpoint , while against other attacks ( PGD and CW ) , the \u201c best \u201d was found right after the first time learning rate decay ( i.e. , epoch 76 ) . This is consistent for all defense methods . We have clarified this in the revision . Q2 : Is 65.04 % ( Table 5 b ) now the best published robust accuracy on CIFAR-10 ( at least to the best of the authors ' knowledge ) ? A2 : Yes.To the best of our knowledge , this is the best robust accuracy on CIFAR-10 ( with the help of 500K unlabeled data ) by our submission to ICLR2020 . We have indicated this in the revision . Q3 : In Figure 2d , it could be insightful to expand the plot further to see the regime where the performance of MART drops substantially . A3 : We have run additional experiments for larger lambda and updated Figure 2d accordingly . The performance of MART ( as well as TRADES ) drops gradually as lambda increases from 20 to 50 . Q4 : In Figure 1 , the three plots would be easier to compare if the y-axes were the same . A4 : Thanks for pointing this out . We have updated Figure 1 to the same y-axes . Q5 : From Figure 2 , it looks like the gain from the BCE loss is as large as the gain from treating misclassified examples differently . Is this correct ? A5 : Yes , it is correct . Q6 : Encourage to release the models . A6 : Yes , we will release our code and model upon acceptance ."}], "0": {"review_id": "rklOg6EFwS-0", "review_text": "The paper essentially presented a viewpoint, i.e. misclassified examples may have a significant impact on the final robustness. The authors conducted a series of qualitative experiments to verify 1) Misclassified examples have more impact on the final robustness than correctly classified examples. 2) For misclassified examples, different maximization techniques may have a negligible influence, but minimization techniques play a critical role on the final robustness. 3) The authors proposed a new defense algorithm which focus on generating adversarial examples from misclassified examples during the training. The algorithm was shown to improve the final robustness by revisiting these adversarial misclassified examples. Generally speaking, the idea, though somewhat straightforward and less elegant, is reasonably presented and also well-motivated. Empirical validations seemed to support the idea and indicated that the proposed approach could improve the adversarial robustness. There are several major concerns with the paper as follows: 1. It is good and reasonable to put more emphasis on the misclassification examples, since it is likely that the region of the \u201cperturbation\u201d largely overlaps with the region of the misclassified examples. 2. However, this may be dealt with in a more elegant or systematic way. In the viewpoint of the reviewer, actually a different emphasis can be imposed on each different data point including both the correctly-classified and mis-classified samples. In another word, a more systematical extension or a metric may actually be developed emphasizing more on mis-classified samples. It is suspected that different samples within the set of mis-classified samples (or even in the set correctly-classified samples) could also have a different influence. It is highly likely that a more elegant and mathematic way can be designed for this purpose. 3. Further to point 2, in particular, the regulation w.r.t the correctly classified examples may not be ignored. It is interesting to consider differently the correctly-classified examples due to the trade-off between robustness and standard accuracy. This can also be seen in another submission of ICLR2020 (titled Sensible adversarial learning\u201d) which actually discards the mis-classified samples. I would like to see some additional comments, clarification, or discussions on this point. 4. I am curious to know if outliers would be over-emphasized by the proposed idea. Some discussion or even some illustrations on a synthetic case would be interesting. Will the existence of outliers affect the robustness? 5. In Table 2, the best and the last results on FGSM are totally identically, I wonder if this is a wrong copy, which should be further clarified and discussed. 6. In the Unlabeled Data experiment part, it is better to compare the results with both UAT++ and RST at the same time rather than separately. 7. Equation (4) shows that the two parts on the right side are added together, is that right? This is inconsistent with Equation (7) and the following description. 8. In 3.2, the first paragraph, \u201cmax-margin adversarial training (MMA) (Ding et al., 2019)\u201d appears to be wrong. The correct reference is \u201cGavinWeiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Max-margin adversarial (mma) training: Direct input space margin maximization through adversarial training. arXiv preprint arXiv:1812.02637, 2018.\u201d. ================== I have read carefully the response from the authors. The newly added results appear satisfactory to me and I am generally happy with the clarifications. I have then adjusted my rating accordingly.", "rating": "6: Weak Accept", "reply_text": "Thanks for your valuable feedback . We address them in detail as follows . Q1 & 2 : A suggestion of different emphasis on each different data point including both the correctly-classified and misclassified samples . A more elegant and systematic way can be designed for this purpose . A1 & 2 : Actually , our approach has already imposed a different emphasis on each different data point according to whether it is correctly-classified or misclassified . As presented in Eq . ( 11 ) , the weight term $ 1-p $ is applied on both correctly-classified and misclassified examples : examples of high confidence ( e.g. , correctly classified ones ) will have small $ 1-p $ , while low confidence training examples ( e.g. , misclassified ones ) will have large $ 1-p $ , that is , different training examples are treated adaptively ( during training ) and differently . We agree that there might exist a more principled and elegant way to deal with misclassified examples/correctly classified examples , and we will explore it in our future work . Q3 : Request for additional comments on another submission of ICLR2020 ( titled \u201c Sensible adversarial learning \u201d ) which considers correctly classified examples differently and discards the misclassified ones . A3 : First of all , we would like to clarify that the ICLR2020 paper \u201c Sensible adversarial learning \u201d is a concurrent submission as our work . After reading the paper , we found that their method explores different treatments on correctly-classified and misclassified examples at the maximization step of adversarial training , while our work considers different treatments at the minimization step of adversarial training , i.e. , applying example-wise regularization term on both correctly-classified and misclassified examples based on the weight $ 1-p $ . Q4 : I am curious to know if the existence of outliers affects the robustness ? A4 : We would appreciate more information on the type of outliers the reviewer would like to discuss here : 1 ) outliers that are wrongly labeled , or 2 ) outliers that are correctly labeled but far away from other data points in the same class ? Q5 : In Table 4 , the best and the last results on FGSM are identical , I wonder if this is a wrong copy . A5 : This is not a wrong copy . For Table 4 , \u201c best \u201d refers to the best checkpoint for each defense under each attack method . For all defense methods against FGSM , the \u201c best \u201d checkpoint just happens to be at the last epoch , which is why the \u201c best \u201d and the \u201c last \u201d results are identical . We have clarified this in the revision . Q6 : In the Unlabeled Data experiment part , it is better to compare the results with both UAT++ and RST at the same time rather than separately . A6 : Thank you for your suggestion . We have updated Table 5 , comparing MART with both UAT++ and RST at the same time . Please note that our conclusion remains the same , i.e. , the proposed defense MART can also benefit from unlabeled data , and further improves the UAT++ and RST defenses , achieving the state-of-the-art robustness . Q7 : Equation ( 4 ) shows that the two parts on the right side are added together , is that right ? This is inconsistent with Equation ( 7 ) and the following description . A7 : We have added more steps of derivation in Equation ( 7 ) to show the consistency between Equation ( 4 ) and Equation ( 7 ) . Q8 : The reference in 3.2 , the first paragraph , \u201c max-margin adversarial training ( MMA ) ( Ding et al. , 2019 ) \u201d appears to be wrong . A8 : Thanks for the correction . We have fixed it in the revision ."}, "1": {"review_id": "rklOg6EFwS-1", "review_text": "Summary: ======= Neural Networks (NN) have been shown to be susceptible to various adversarial attacks i.e. if we perturb the \"x\" just a little, the output prediction changes. So, there has been much research devoted to how we can make NNs robust to such attacks. Typically, the adversarial examples that are used to train adversarially robust methods are generated from the correctly classified examples. This paper first shows empirically that the adversarial examples generated from misclassified examples by the model h_{\\theta} are as important as the ones generated from correctly classified examples on the CIFAR dataset. The authors then propose a novel objective for the adversarial risk that incorporates the misclassified examples as a regularizer. Next, a surrogate convex objective is derived which is optimized to come up with a new kind of adversarial training called misclassification aware adversarial training (MART). Originality: ========== The proposed objective seems novel to me compared to the existing methods, perhaps less so to someone who is an expert in adversarial methods. That said, I definitely found the addition of a misclassification-based regularization term as intriguing. However, I was wondering that maybe the previous approaches did not distinguish between correctly vs incorrectly classified examples while generating adversarial examples was because that is a more general setting. This is so because the idea of classification is tied to a certain model h_{\\theta} and hence is not model-agnostic by definition. Perhaps the strength of the approach by Madry et. al. 18, which is the closest work to this paper, is in being model-agnostic. I hope I am not missing something! Quality: ======= The paper is technically sound and is a solid contribution to the literature on adversarial robustness. The motivation, experimental results, and ablation studies are all very well executed. The results are shown on MNIST and CIFAR-10 image datasets and it outperforms a host of competitive baseline algorithms. Clarity: ======= The paper is well organized and is very well written. The experimental results are very thorough and well explained. Significance: ============ The paper solves an important problem of dealing with adversarial attacks on deep neural networks. Further, the solution they propose is novel and a seems like a significant improvement over the extend state-of-the-art. ", "rating": "6: Weak Accept", "reply_text": "Thanks for your positive and valuable comments . Q1 : \u201c I was wondering that maybe the previous approaches did not distinguish between correctly vs incorrectly classified examples while generating adversarial examples was because that is a more general setting . This is so because the idea of classification is tied to a certain model h_ { \\theta } and hence is not model-agnostic by definition . Perhaps the strength of the approach by Madry et . al.18 , which is the closest work to this paper , is in being model-agnostic . I hope I am not missing something ! \u201d A1 : We would like to clarify that both our approach and the standard adversarial training by Madry et . al.18 are of the same nature . Like Madry et . al.18 , our algorithm can also be applied to any models as long as it can output class probability , i.e. , $ p_k ( x_i , \\theta ) $ in Equation ( 2 ) . The difference is that our method makes the outer minimization of adversarial training more data-adaptive ( with differentiation between misclassified and correctly classified examples ) ."}, "2": {"review_id": "rklOg6EFwS-2", "review_text": "The paper improves adversarial training by introducing two modifications to the loss function: (i) a \"boosted\" version of the cross-entropy loss that involves a term similar to a large-margin loss, and (ii) weighting the adversarial loss differently depending on how correctly classified an example is. When put together, these modifications achieve state-of-the-art robustness on CIFAR-10, improving over the previously best robust accuracy by about 3.5%. The authors perform multiple ablation studies and demonstrate that their modified loss function also improves when additional unlabeled data is added (again achieving state-of-the-art robustness). I recommend accepting the paper. The modifications for the loss function are well motivated and improve over the state of the art by a non-trivial amount. Moreover, the authors nicely put their loss function in the context of prior work. Additional comments: - In Table 4, are the \"best\" columns the best checkpoint for the respective column (potentially different checkpoints for different columns) or does \"best\" refer to a single model (for each row)? - Is 65.04% (Table 5 b) now the best published robust accuracy on CIFAR-10 (at least to best of the authors' knowledge)? If so, it may be helpful to indicate this to the reader. - In Figure 2d, it could be insightful to expand the plot further to see the regime where the performance of MART drops substantially. - In Figure 1, the three plots would be easier to compare if the y-axes were the same. - From Figure 2, it looks like the gain from the BCE loss is as large as the gain from treating misclassified examples differently. Is this correct? - I strongly encourage the authors to release their models in a format that is easy for other researchers to use (e.g., PyTorch model checkpoints). This will make it substantially easier for future work to build on the results in this paper.", "rating": "8: Accept", "reply_text": "Thanks for your valuable and positive comments . We address them in detail as follows . Q1 : In Table 4 , are the `` best '' columns the best checkpoint for the respective column ( potentially different checkpoints for different columns ) or does `` best '' refer to a single model ( for each row ) ? A1 : Here , \u201c best \u201d refers to the best checkpoint for each defense method under each attack method . Empirically , we find that , against FGSM attack , the \u201c best \u201d was found at the \u201c last \u201d checkpoint , while against other attacks ( PGD and CW ) , the \u201c best \u201d was found right after the first time learning rate decay ( i.e. , epoch 76 ) . This is consistent for all defense methods . We have clarified this in the revision . Q2 : Is 65.04 % ( Table 5 b ) now the best published robust accuracy on CIFAR-10 ( at least to the best of the authors ' knowledge ) ? A2 : Yes.To the best of our knowledge , this is the best robust accuracy on CIFAR-10 ( with the help of 500K unlabeled data ) by our submission to ICLR2020 . We have indicated this in the revision . Q3 : In Figure 2d , it could be insightful to expand the plot further to see the regime where the performance of MART drops substantially . A3 : We have run additional experiments for larger lambda and updated Figure 2d accordingly . The performance of MART ( as well as TRADES ) drops gradually as lambda increases from 20 to 50 . Q4 : In Figure 1 , the three plots would be easier to compare if the y-axes were the same . A4 : Thanks for pointing this out . We have updated Figure 1 to the same y-axes . Q5 : From Figure 2 , it looks like the gain from the BCE loss is as large as the gain from treating misclassified examples differently . Is this correct ? A5 : Yes , it is correct . Q6 : Encourage to release the models . A6 : Yes , we will release our code and model upon acceptance ."}}