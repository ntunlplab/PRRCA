{"year": "2021", "forum": "BXewfAYMmJw", "title": "Counterfactual Generative Networks", "decision": "Accept (Poster)", "meta_review": "The paper presents a \"conceptual  advance connecting causality, disentangled representation learning, invariant representations and robust classification\". Authors propose to decompose the image generation process to independent mechanism that can be composed (foreground masks (shapes), forground texture, and backgrounds), allowing for a specific image to generate counterfactuals , by changing some variations factors, while keeping other fixed. One can use interventional data to augment classifiers, this can lead in certain cases to improvement in accuracy and in other in improving the robustness. \n\nThere was concerns about the clarity of the paper regarding the structured causal model considered and its applicability beyond image generation, experimental protocol for choosing hyperparameters (loss scaling and ratios of real data and interventional samples ) and some missing references. The rebuttal of the authors and their updated paper reflected comprehensively all those concerns and addressed them, highlighting limitations of the method and adding more examples of its failures. \n\nI liked the ideas and concepts in  this paper , and it will be exciting to generalize such generative approach to other domains, this  work is a first step. I think it will be good addition to ICLR program ", "reviews": [{"review_id": "BXewfAYMmJw-0", "review_text": "The main idea of the paper , i.e. , using independent causal mechanisms to generate interventional images , has already been explored by Kocaoglu et al.in Causalgan : Learning causal implicit generative models with adversarial training , ICLR'18 . Same as here , the authors there also `` view image generation as a causal process '' and `` structure a generator network as a structural causal model ( SCM ) '' and use a conditional gan to generate the image from the labels . The generation used here based on three variables , i.e. , shape , texture and background seem to be a special case . Therefore , the authors should definitely cite this work . My general remark is that there is very little causality in the approach . The causal structure that is used in the generation of data is not different than a conditional GAN . This makes the claims in the introduction very disconnected from the actual methodology and the experiments in my opinion. `` we can intervene on a subset of them and generate counterfactual images `` - > What the authors call counterfactual images are actually interventional images from a causal point of view . Please consider changing `` counterfactual '' to `` interventional '' throughout the paper . This will help clarify the distinction between interventional and counterfactual layers in Pearl 's hierarchy . `` From a causal perspective , we maximize the average causal effect ( ACE ) of one IM on the classifier \u2019 s decision , while minimizing the ACE of all other IMs . '' Can you formalize this claim ? This does not seem trivial . Could you explain `` alpha blending '' ? This step is not motivated well and seems specific to the used dataset . Even though the ImageNet experiments look impressive , I believe the main factor for success is in the deterministic and manually defined composition mechanism . Furthermore , I believe this composition is doing most of the disentangling during training . Foreground and background segmentation use an existing method U2-Net which is used to create masks , or values for the variables used in the graph . The intuition on comparing with other methods is missing . For example , why do you think training a classifier on interventionally augmented data performs better than IRM ? Should n't this depend on the number of environments and degree of correlation ? These are not reported . `` We , therefore , follow an augmentation strategy , i.e. , we augment ImageNet with additional counterfactual images . '' How many samples are added to the original data ? I believe the amount of augmentation relative to the original dataset size is important . % % AFTER REBUTTAL % % Thank you for all the updates . I would like to thank the authors for their humility in the rebuttal and for clarifying the paper 's contributions . Accordingly , I will increase my score . However , I still believe Section 3.1 's contribution , and the follow-up of using this to improve classifier robustness , is useful only for a very specific type of data and it is hard to assess its value from a practical point of view . The fact that the authors were able to showcase that such counterfactual data augmentation improves classification is , although expected , useful in itself . However , performance improvement is only evident in colored MNIST , relative to GAN augmentation . Furthermore , R4 points out the important issue that the relevant causal feature is assumed to be known in the experiments . This information is normally not available and must be inferred by the classifier . The additional experiments provided by the authors during the rebuttal are welcome but they should be in the main paper rather than the appendix since this is the main setting where spurious correlations create problems . I believe the experimental section should put more weight on this setting . In light of all this , I will provide a borderline score leaning towards rejection . I encourage the authors to expand section 3 to settings that do not restrict the images to have one foreground object and a single background .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . We will address your concerns in the following . __Q1 : The main idea of the paper , i.e. , using independent causal mechanisms to generate interventional images , has already been explored by Kocaoglu et al.in Causalgan : Learning causal implicit generative models with adversarial training , ICLR'18 . Same as here , the authors there also `` view image generation as a causal process '' and `` structure a generator network as a structural causal model ( SCM ) '' and use a conditional gan to generate the image from the labels . The generation used here based on three variables , i.e. , shape , texture and background seem to be a special case . Therefore , the authors should definitely cite this work.__ We are aware of the work by Kocaoglu et al . ; however , given the limited space for the initial submission , we decided to discuss more closely related work such as [ 1 , 2 ] . However , we agree that it is helpful and necessary to contrast your proposed reference . Indeed , Kocaoglu et al.propose to structure a generator as a structural causal model . Concretely , they utilize two separate generative models : ( i ) a `` causal controller , '' a model trained to learn the label distribution , the labels are binary ( mustache , young ) . ( ii ) a generator conditioned on the generated labels . There are several aspects in which the work of Kocaoglu et al.significantly differs from our work : 1 . There are substantial differences on a conceptual level . As we state : `` We follow the argument that rather than training a monolithic network to map from a latent space to images , the mapping should be decomposed into several functions '' ( Section 2.2 , Page 3 ) . In our case , this latent space comprises noise and labels . CausalGAN learns an SCM on label level , then maps to images using a monolithic network . We do not map from noise and labels to images -- rather , we view the image formation process as an SCM , where different mechanisms interact with each other . Kocaoglu et al.do not use independent causal mechanisms , nor do they claim to do so . 2.Structuring the generator as an SCM is only one of our several contributions . For instance , Kocaoglu et al.do not deploy their GAN to generate data to train invariant classifiers , an essential aspect of our work . 3.Kocaoglu et al.assume a data set with fine-grained labels , and they report results on CelebA . It would not be straightforward to scale to more complex domains , such as Imagenet , where these kinds of labels are not available or very hard to obtain . 4.Applying a CausalGAN on our proposed MNIST variants and Imagenet would result in trivial solutions . The causal controller can not observe any variation in the label correlations . Hence , it would only learn deterministic mappings , as all the relevant factors of variation are 100 % correlated , i.e. , a zero is always red ( colored MNIST ) or a banana is always `` banana-textured '' ( Imagenet ) . We include the reference and discussion points in the paper ."}, {"review_id": "BXewfAYMmJw-1", "review_text": "Deep neural network brittleness can be attributed to their tendency to latch on to spurious correlations in the training dataset . The proposal in the paper is to learn to generate samples where these correlations can be eliminated . To this end , the authors , distill trained conditional big gan into a transformation with explicit modules to capture the shape , texture of the foreground object , and the background . The distilled network is called Counterfactual Generator Network ( CGN ) . Thus , an image can be generated with a background of one class , the shape of another class , and the foreground texture of a different class . Then a classifier with multiple heads is learned where each head predicts a class based on only one of the factors among shape , texture , and background . The proposed approach is motivated by the assumption of independent mechanisms where different modules of the causal data generating process are independent of each other . Once the decomposition of a training image into shape , texture , and background is obtained , any component can be swapped to generate counterfactual data . Pros : + The solutions provided to extract object masks , background and texture are interesting and scale to Imagenet dataset . + Shows that augmenting the training dataset with the generated counterfactual images can help improve robustness . Cons + Questions : - The presentation of the paper can be improved . It is not always clear if the causal structure is assumed to known . In Sec 2.2 SCM is defined , but the SCM for the MNIST or Imagenet is not provided . Do all the nodes in the CGN share the same noise or exogenous variables ? - The proposed method appears to assume that the causal structure is known . In this , it assumes it is made up of three nodes shape , texture , and background , and thus can limit the counterfactual generation ability . Many semantic changes can not be achieved as evidenced by the fact that the counterfactual images are not realistic . - in the invariant MNIST classification task it appears that the results are based on the assumption that the invariant feature - shape is known apriori . In practice , this information is not available . IRM does not assume this knowledge , so it does not seem comparison with IRM is fair in this case . - Some related work that seems to be missing [ 1 ] [ 2 ] [ 1 ] Kocaoglu , Murat , et al . `` Causalgan : Learning causal implicit generative models with adversarial training . '' arXiv preprint arXiv:1709.02023 ( 2017 ) . [ 2 ] Kaushik , Divyansh , Eduard Hovy , and Zachary C. Lipton . `` Learning the difference that makes a difference with counterfactually-augmented data . '' arXiv preprint arXiv:1909.12434 ( 2019 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . We appreciate your assessment of our approach as interesting and scalable . We also appreciate your suggestions to improve the presentation of our work . We will address your concerns in the following . __Q1 : It is not always clear if the causal structure is assumed to known . In Sec 2.2 SCM is defined , but the SCM for the MNIST or Imagenet is not provided . Do all the nodes in the CGN share the same noise or exogenous variables ? __ Yes , we assume knowledge of the causal structure in all our experiments . As you correctly remark , the causal structure of our generative model is as follows \\begin { align * } \\mathbf { M } & : = f_ { shape } ( Y_1 , U_1 ) \\\\\\\\ \\mathbf { F } & : = f_ { text } ( Y_2 , U_2 ) \\\\\\\\ \\mathbf { B } & : = f_ { bg } ( Y_3 , U_3 ) \\\\\\\\ \\mathbf { X_ { gen } } & : = C ( \\mathbf { M } , \\mathbf { F } , \\mathbf { B } ) \\end { align * } where $ U_j $ is the exogenous noise , $ Y_j $ is the label , $ X_ { gen } $ is the generated image , and $ f_j $ and $ C $ are the independent mechanisms . The general structure is the same for all experiments , with a slight simplification for MNIST : instead of $ f_ { bg } $ we assume a second texture mechanism $ f_ { text,2 } $ . There is no need for a globally coherent background in the MNIST setting . For the MNIST experiments , the noise variables are independent both during training and inference . For Imagenet , we deploy a cGAN for supervision . Hence , the CGN needs to use the same latent space as the cGAN , and all mechanisms use the same noise variables as input . Strictly speaking , this violates the assumption of independent noise . We accept this violation for the sake of stable optimization . However , during inference , we can draw $ u $ independently for each independent mechanism ( IM ) , hence implementing a proper SCM . We include a section in the appendix explicitly formulating the assumed causal structures for all experimental settings . __Q2 : The proposed method appears to assume that the causal structure is known . In this , it assumes it is made up of three nodes shape , texture , and background , and thus can limit the counterfactual generation ability . Many semantic changes can not be achieved as evidenced by the fact that the counterfactual images are not realistic.__ This is indeed a limitation of our method ; we assume that the causal problem structure is known . We add a paragraph in the additional discussion section to highlight this issue . As we mention in our discussion with Reviewer 1 , an exciting future research direction might be to leverage causal discovery methods to find said structure . Further , our method is not limited to future extensions . One might add more IMs to model more fine-grained semantic changes . Also , as evidenced by our experiments , the counterfactual images are realistic enough to significantly influence a classifier \u2019 s preference on the main classification task ( Imagenet ) while maintaining performance ."}, {"review_id": "BXewfAYMmJw-2", "review_text": "Summary This paper proposes a new generative model that generate images from 3 seperate aspects : foreground masks ( shapes ) , forground texture , and backgrounds . Then they convexly mix these 3 aspects into one image . By doing so , they can vary each aspect individually without changing other aspects , enabling the model to generate counterfactual images . For example , we can generate a cat shape with telephene texture and sea background , which would not exist in natural images . They show that in several colorful MNISTs datasets their methods can generate new combinations of images . In ImageNet , by using the pre-trained BigNet GAN as backbones and pretrained U-Net for foreground object masks , they can distill the knowledge in BigNet into these 3 seperate aspects and generate counterfactual natural images . Pros 1.Intersting ideas of combining pre-trained models with novel loss function that helps disentangle these 3 aspects . 2.Plausible and intriguing counterfactual examples . 3.Strong improvements in colorful MNIST datasets . Major comments 1 . The claim that `` we are able to reduce the gap while achieving hig accuracy on IN-9 '' is not true . In Table 4 , the method IN+CGN has lower accuracy than IN alone , and it reduces the gap by lowering the original performance in Mixed-same . So it does not actually improve the performance . 2.The above makes me wonder the ImageNet experiment does not actually generate plausible enough images that help improve accuracy , probably due to its unnatural image generation as evidenced by relatively low IS scores ( 130 ) . Especially seeing there are 6 lambdas to tune listed in Appendix B.3 and other hyperparameters like learning rates etc , this method might not be very practical in high-dimensional natural images . Maybe authors can be honest about it . Or illustrate what 's the best way to tune this method , what has been attempted etc . 3.The current setting seems to a bit limited that requires a single foreground and background . For example , designing independent mechanisms for medical imaging might not be easy with no exact foreground/background boundaries . 4.In Table 3 , maybe authors can evaluate on some difficult ImageNet datasets like ImageNet-A to assess if the performance improves . Minor comments 1 . The loss descriptions in Appendix B should be moved to main text to help readers understand the method . Details like how to pick $ \\tau $ for shape loss should be mentioned . 2.The name `` pre-masks '' is confusing that originally I think it 's a binary mask , but instead it 's a colorful image . 3.More failure examples in Appendix E will better help readers understand its limitations . 4.The figure 5 and the Appendix D should also include the final image that combines m , f and b to better assess its improvement over the training stages . Also , having an arrow in Figure 5 or specifying epochs might better help readers understand it is showing the transitions . 5.The caption in Figure 24 is wrong : should be columns instead of rows . Evaluations Overall I like this paper . The method is interesting to read and the examples are interesting . The experiments are thorough and do show some improvements in colorful MNISTs . The method , unfortunately , does not seem to work that well in ImageNet , and does not improve generalization performance . I encourage the authors to be upfront about the limitations of this method and write better descriptions of loss and hyperparameters tuning .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . We appreciate your assessment of our generated counterfactual images as `` plausible and intriguing '' and our experiments as `` thorough , '' and you highlighting our idea of combining pre-trained models with novel loss functions . We address your raised concerns in the following , starting with your primary concerns . __Q1 : IN-9 results - IN+CGN has lower accuracy than IN alone , and it reduces the gap by lowering the original performance in Mixed-same . So it does not actually improve the performance . The above makes me wonder the ImageNet experiment does not actually generate plausible enough images that help improve accuracy , probably due to its unnatural image generation as evidenced by relatively low IS scores ( 130 ) .__ We conducted additional experiments for this rebuttal . Our updated results show a slight improvement on Mixed-Rand ( IN : 78.9 % , IN+CGN : 80.1 % ) . We achieve this improvement by increasing the overall batch size ( both real and counterfactual batches ) during training . However , we continue to observe a drop on Mixed-Same ( IN : 86.2 % , IN+CGN : 83.4 % ) . Your question still raises a valid point - why does the performance on Mixed-Same decline ? We hypothesize that there is an additional domain shift that our approach does not address . A model trained on IN achieves the following accuracies : IN-9 ( original ) : 95.6 % , Mixed-Same : 86.2 % , Mixed-Rand : 78.9 % . A stated in [ 1 ] , Mixed-Same controls for artifacts from image processing . These artifacts lead to a drop in performance of 9.4 % - even though the background is still 100 % correlated with the label . Hence , without handling these artifacts , 86.2 % appears to be the upper boundary if all available factors of variation are used for classification -- including the background . It seems natural that a classifier which uses the background signal less and does not address these artifacts would further drop in performance , i.e. , below 86.2 % We also believe that your argument of unnatural images is correct . As we remark in our answer to Reviewer 1 , we believe that the quality of the synthetic images is not high enough yet to * improve * performance on ImageNet . On the one hand , the constraints we enforce on our model result in less realistic images , as evidenced by the lower IS and the failure cases we present ( see below ) . On the other hand , even state-of-the-art generative models ( with higher IS ) are not good enough yet to generate data for training competitive ImageNet classifiers , see [ 2 ] . __Q2 : Especially seeing there are 6 lambdas to tune listed in Appendix B.3 and other hyperparameters like learning rates etc , this method might not be very practical in high-dimensional natural images . Maybe authors can be honest about it . Or illustrate what 's the best way to tune this method , what has been attempted etc.__ Thank you for bringing up this point . We agree our initial submission was lacking details about how to find good hyperparameters . As we previously described , during training , we measure the Inception score ( IS ) and the mean value of the masks $ \\mu_ { mask } $ to detect mask collapse . We also observe the generated images for a fixed noise vector ; see the outputs in Figure 5 . Our overall objective is a high IS and a stable $ \\mu_ { mask } $ . Further , we aim for high-quality output of all IMs ( Masks : binary , capture only class-specific parts ; Textures : no background/global shape visible , Background : no trace of foreground objects visible ) . We observe these outputs for several classes during optimization . As we show in our loss ablation study , all losses are necessary for disentanglement . However , in our experience , the hyperparameters can be tuned mostly independently from each other , i.e. , a better lambda for the mask loss does not influence the quality of the texture maps much . We include this description in the appendix . __Q3 : limited setting ( requires a single foreground and background ) . Designing independent mechanisms for medical imaging might not be easy with no exact foreground/background boundaries.__ The setting of a single foreground and background is indeed a limitation of our current work . Your suggested example is an exciting one . A possibility would be to avoid the foreground/background assumption and train IMs to generate domain-specific object instances . For example , for brain scans , different IMs could model different brain regions that can only be composed in a certain way . The composition function can be informed by domain-specific knowledge . We include a discussion of the current limitations regarding the points above in the discussion section ."}, {"review_id": "BXewfAYMmJw-3", "review_text": "This paper presents an interesting conceptual advance connecting causality , disentangled representation learning , invariant representations and robust classification . The authors propose a Counterfactual Generative Network ( CGN ) , which is basically `` modular '' generative adversarial network that can independently control the generation of independent factors of variations in the data corresponding to Independent Mechanism , i.e.independent factors in the structural causal model of the data . In the context of generating natural images like those comprising the ImageNet dataset , the CGN once trained can be used to generate high-quality counterfactual images with direct control over of factors of variations determining the content of an image shape , texture , and background . These generated samples obtained by independently and uniformly sampling over factors of variation can be used to train a classifier to achieve out-of-domain robustness . The authors show indeed show in simulation that this procedure works as a data augmentation procedure that increases out-of-domain robustness while only marginally degrades the overall accuracy . As the authors explain , this can be thought of as a generalization of `` domain randomization '' . Additionally , CGN can be used as a generative model of high-quality binary object masks and unsupervised image inpainting . The authors also carry out extensive ablation studies that quantify the contribution of the different training costs for CGN to the overall quality ( measures as Inception Score ) of the generated counterfactual images . This is first and foremost an `` idea paper '' putting forth a very interesting conceptual proposal . This is then empirically validated on out-of-distribution classification tasks in different versions of colored MNIST , and a coarse grained subsed of ImageNet . A natural question for the authors is whether and by how much this type of data augmentation might help in realistic large-scale classification scenarios . Another natural question would be to quantify the effect of the counterfactual data augmentation in terms of trade-off between performance on in-distribution and out-of-distribution samples . Lastly , it would be interesting to know whether the authors have any thought on how to generalize their architecture to other setting , i.e.how to isolate Independent Mechanisms in a general domain and what type of domain knowledge is necessary to do that effectively .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review . We appreciate your assessment of our work as an \u201c interesting conceptual advance. \u201d We , too , believe that our main contribution is connecting the fields of causality , disentanglement , and robust classification . With further advances in generative modeling and synthetic-to-real literature , we are optimistic that we can further improve results on realistic datasets like Imagenet . In the following , we like to address all the questions you raised . __Q1 : whether and by how much this type of data augmentation might help in realistic large-scale classification scenarios.__ As we show in Table 3 ( Shape vs. Texture ) , our invariant classifier ensemble is able to trade off the different factors of variation on the full Imagenet classification task . Interestingly , we can strongly influence the classifier \u2019 s preferences while maintaining good performance on the ( large-scale ) classification task - even more than augmenting with Stylized Imagenet . Our intuition is that the quality of the synthetic images is not high enough yet to * improve * performance on ImageNet itself . On the one hand , the constraints we enforce on our model result in less realistic images , as evidenced by the lower IS . On the other hand , even state-of-the-art generative models ( with higher IS ) are not good enough yet to generate data for training competitive ImageNet classifiers , see [ 1 ] . __Q2 : Effect of the counterfactual data augmentation in terms of trade-off between performance on in-distribution and out-of-distribution samples.__ This question is indeed interesting , and we ran additional experiments both on MNIST and Imagenet . We did not observe a difference in our initial experiments when training on both OOD and IID data , or only on OOD data . We increase the OOD/IID ratio beyond 1 ( the initially used ratio ) in our new experiments . Interestingly , on MNIST , there seems to be no downgrade in performance when we increase the ratio of OOD/IID samples - more OOD data always improves the performance on the test set . This effect holds up to a ratio of 20 ; beyond that , we see stagnation . By increasing this ratio , we achieve a substantial improvement over our initial reported numbers ( Colored MNIST 86.4 % - > 95.1 % , Double-Colored MNIST 85.3 % - > 89.0 % , Wildlife MNIST 80.5 % - > 85.7 % ) . These results illustrate the potential of generated counterfactual data , especially in less complex domains . We add these new results to the paper . As for Imagenet , we settled on a ratio of 1 for all of our experiments , based on similar results by [ 1 ] . A ratio below 1 leads to inferior performance on both Imagenet and IN-9 . For a ratio above 1 , the training time for a single epoch increases . However , the network also learns faster on the counterfactual data so that these two effects keep a balance , but we also do not observe an improvement over a ratio of 1 . __Q3 : Lastly , it would be interesting to know whether the authors have any thought on how to generalize their architecture to other setting , i.e.how to isolate Independent Mechanisms in a general domain and what type of domain knowledge is necessary to do that effectively.__ We , too , are very interested in how to generalize our ideas to different settings . A straightforward way to extend our proposed CGN is to add several branches for object instances . This type of disentanglement might produce useful training data for semantic ( instance ) segmentation . Another possible avenue is to build on advances in 3d generative modeling [ 2 , 3 ] to generate data to train classifiers invariant to object pose . It is generally hard to state what domain knowledge is necessary and may depend heavily on the respective domain . With our focus on image classification , we incorporated general information about compositional image formation , i.e. , alpha blending . That being said , it would be interesting to explore ideas from causal discovery to isolate IMs in a domain-agnostic manner , e.g. , via meta-learning [ 4 ] . __References__ [ 1 ] Suman , Vinyals , \u201c Seeing is not necessarily believing : Limitations of biggans for data augmentation \u201d , 2019 , ICLR Workshop LLD , https : //openreview.net/forum ? id=rJMw747l_4 [ 2 ] Nguyen-Phuoc , Li , Theis , Richardt , Yang , \u201c Hologan : Unsupervised learning of 3d representations from natural images \u201d , 2019 , ICCV , https : //arxiv.org/abs/1904.01326 [ 3 ] Liao , Schwarz , Mescheder , Geiger , \u201c Towards unsupervised learning of generative models for 3d controllable image synthesis \u201d , 2020 , CVPR , https : //arxiv.org/abs/1912.05237 [ 4 ] Bengio , Deleu , Rahaman , Ke , Lachapelle , Bilaniuk , Goyal , Pal \u201c A meta-transfer objective for learning to disentangle causal mechanisms \u201d , 2019 , ICLR , https : //arxiv.org/abs/1901.10912"}], "0": {"review_id": "BXewfAYMmJw-0", "review_text": "The main idea of the paper , i.e. , using independent causal mechanisms to generate interventional images , has already been explored by Kocaoglu et al.in Causalgan : Learning causal implicit generative models with adversarial training , ICLR'18 . Same as here , the authors there also `` view image generation as a causal process '' and `` structure a generator network as a structural causal model ( SCM ) '' and use a conditional gan to generate the image from the labels . The generation used here based on three variables , i.e. , shape , texture and background seem to be a special case . Therefore , the authors should definitely cite this work . My general remark is that there is very little causality in the approach . The causal structure that is used in the generation of data is not different than a conditional GAN . This makes the claims in the introduction very disconnected from the actual methodology and the experiments in my opinion. `` we can intervene on a subset of them and generate counterfactual images `` - > What the authors call counterfactual images are actually interventional images from a causal point of view . Please consider changing `` counterfactual '' to `` interventional '' throughout the paper . This will help clarify the distinction between interventional and counterfactual layers in Pearl 's hierarchy . `` From a causal perspective , we maximize the average causal effect ( ACE ) of one IM on the classifier \u2019 s decision , while minimizing the ACE of all other IMs . '' Can you formalize this claim ? This does not seem trivial . Could you explain `` alpha blending '' ? This step is not motivated well and seems specific to the used dataset . Even though the ImageNet experiments look impressive , I believe the main factor for success is in the deterministic and manually defined composition mechanism . Furthermore , I believe this composition is doing most of the disentangling during training . Foreground and background segmentation use an existing method U2-Net which is used to create masks , or values for the variables used in the graph . The intuition on comparing with other methods is missing . For example , why do you think training a classifier on interventionally augmented data performs better than IRM ? Should n't this depend on the number of environments and degree of correlation ? These are not reported . `` We , therefore , follow an augmentation strategy , i.e. , we augment ImageNet with additional counterfactual images . '' How many samples are added to the original data ? I believe the amount of augmentation relative to the original dataset size is important . % % AFTER REBUTTAL % % Thank you for all the updates . I would like to thank the authors for their humility in the rebuttal and for clarifying the paper 's contributions . Accordingly , I will increase my score . However , I still believe Section 3.1 's contribution , and the follow-up of using this to improve classifier robustness , is useful only for a very specific type of data and it is hard to assess its value from a practical point of view . The fact that the authors were able to showcase that such counterfactual data augmentation improves classification is , although expected , useful in itself . However , performance improvement is only evident in colored MNIST , relative to GAN augmentation . Furthermore , R4 points out the important issue that the relevant causal feature is assumed to be known in the experiments . This information is normally not available and must be inferred by the classifier . The additional experiments provided by the authors during the rebuttal are welcome but they should be in the main paper rather than the appendix since this is the main setting where spurious correlations create problems . I believe the experimental section should put more weight on this setting . In light of all this , I will provide a borderline score leaning towards rejection . I encourage the authors to expand section 3 to settings that do not restrict the images to have one foreground object and a single background .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . We will address your concerns in the following . __Q1 : The main idea of the paper , i.e. , using independent causal mechanisms to generate interventional images , has already been explored by Kocaoglu et al.in Causalgan : Learning causal implicit generative models with adversarial training , ICLR'18 . Same as here , the authors there also `` view image generation as a causal process '' and `` structure a generator network as a structural causal model ( SCM ) '' and use a conditional gan to generate the image from the labels . The generation used here based on three variables , i.e. , shape , texture and background seem to be a special case . Therefore , the authors should definitely cite this work.__ We are aware of the work by Kocaoglu et al . ; however , given the limited space for the initial submission , we decided to discuss more closely related work such as [ 1 , 2 ] . However , we agree that it is helpful and necessary to contrast your proposed reference . Indeed , Kocaoglu et al.propose to structure a generator as a structural causal model . Concretely , they utilize two separate generative models : ( i ) a `` causal controller , '' a model trained to learn the label distribution , the labels are binary ( mustache , young ) . ( ii ) a generator conditioned on the generated labels . There are several aspects in which the work of Kocaoglu et al.significantly differs from our work : 1 . There are substantial differences on a conceptual level . As we state : `` We follow the argument that rather than training a monolithic network to map from a latent space to images , the mapping should be decomposed into several functions '' ( Section 2.2 , Page 3 ) . In our case , this latent space comprises noise and labels . CausalGAN learns an SCM on label level , then maps to images using a monolithic network . We do not map from noise and labels to images -- rather , we view the image formation process as an SCM , where different mechanisms interact with each other . Kocaoglu et al.do not use independent causal mechanisms , nor do they claim to do so . 2.Structuring the generator as an SCM is only one of our several contributions . For instance , Kocaoglu et al.do not deploy their GAN to generate data to train invariant classifiers , an essential aspect of our work . 3.Kocaoglu et al.assume a data set with fine-grained labels , and they report results on CelebA . It would not be straightforward to scale to more complex domains , such as Imagenet , where these kinds of labels are not available or very hard to obtain . 4.Applying a CausalGAN on our proposed MNIST variants and Imagenet would result in trivial solutions . The causal controller can not observe any variation in the label correlations . Hence , it would only learn deterministic mappings , as all the relevant factors of variation are 100 % correlated , i.e. , a zero is always red ( colored MNIST ) or a banana is always `` banana-textured '' ( Imagenet ) . We include the reference and discussion points in the paper ."}, "1": {"review_id": "BXewfAYMmJw-1", "review_text": "Deep neural network brittleness can be attributed to their tendency to latch on to spurious correlations in the training dataset . The proposal in the paper is to learn to generate samples where these correlations can be eliminated . To this end , the authors , distill trained conditional big gan into a transformation with explicit modules to capture the shape , texture of the foreground object , and the background . The distilled network is called Counterfactual Generator Network ( CGN ) . Thus , an image can be generated with a background of one class , the shape of another class , and the foreground texture of a different class . Then a classifier with multiple heads is learned where each head predicts a class based on only one of the factors among shape , texture , and background . The proposed approach is motivated by the assumption of independent mechanisms where different modules of the causal data generating process are independent of each other . Once the decomposition of a training image into shape , texture , and background is obtained , any component can be swapped to generate counterfactual data . Pros : + The solutions provided to extract object masks , background and texture are interesting and scale to Imagenet dataset . + Shows that augmenting the training dataset with the generated counterfactual images can help improve robustness . Cons + Questions : - The presentation of the paper can be improved . It is not always clear if the causal structure is assumed to known . In Sec 2.2 SCM is defined , but the SCM for the MNIST or Imagenet is not provided . Do all the nodes in the CGN share the same noise or exogenous variables ? - The proposed method appears to assume that the causal structure is known . In this , it assumes it is made up of three nodes shape , texture , and background , and thus can limit the counterfactual generation ability . Many semantic changes can not be achieved as evidenced by the fact that the counterfactual images are not realistic . - in the invariant MNIST classification task it appears that the results are based on the assumption that the invariant feature - shape is known apriori . In practice , this information is not available . IRM does not assume this knowledge , so it does not seem comparison with IRM is fair in this case . - Some related work that seems to be missing [ 1 ] [ 2 ] [ 1 ] Kocaoglu , Murat , et al . `` Causalgan : Learning causal implicit generative models with adversarial training . '' arXiv preprint arXiv:1709.02023 ( 2017 ) . [ 2 ] Kaushik , Divyansh , Eduard Hovy , and Zachary C. Lipton . `` Learning the difference that makes a difference with counterfactually-augmented data . '' arXiv preprint arXiv:1909.12434 ( 2019 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . We appreciate your assessment of our approach as interesting and scalable . We also appreciate your suggestions to improve the presentation of our work . We will address your concerns in the following . __Q1 : It is not always clear if the causal structure is assumed to known . In Sec 2.2 SCM is defined , but the SCM for the MNIST or Imagenet is not provided . Do all the nodes in the CGN share the same noise or exogenous variables ? __ Yes , we assume knowledge of the causal structure in all our experiments . As you correctly remark , the causal structure of our generative model is as follows \\begin { align * } \\mathbf { M } & : = f_ { shape } ( Y_1 , U_1 ) \\\\\\\\ \\mathbf { F } & : = f_ { text } ( Y_2 , U_2 ) \\\\\\\\ \\mathbf { B } & : = f_ { bg } ( Y_3 , U_3 ) \\\\\\\\ \\mathbf { X_ { gen } } & : = C ( \\mathbf { M } , \\mathbf { F } , \\mathbf { B } ) \\end { align * } where $ U_j $ is the exogenous noise , $ Y_j $ is the label , $ X_ { gen } $ is the generated image , and $ f_j $ and $ C $ are the independent mechanisms . The general structure is the same for all experiments , with a slight simplification for MNIST : instead of $ f_ { bg } $ we assume a second texture mechanism $ f_ { text,2 } $ . There is no need for a globally coherent background in the MNIST setting . For the MNIST experiments , the noise variables are independent both during training and inference . For Imagenet , we deploy a cGAN for supervision . Hence , the CGN needs to use the same latent space as the cGAN , and all mechanisms use the same noise variables as input . Strictly speaking , this violates the assumption of independent noise . We accept this violation for the sake of stable optimization . However , during inference , we can draw $ u $ independently for each independent mechanism ( IM ) , hence implementing a proper SCM . We include a section in the appendix explicitly formulating the assumed causal structures for all experimental settings . __Q2 : The proposed method appears to assume that the causal structure is known . In this , it assumes it is made up of three nodes shape , texture , and background , and thus can limit the counterfactual generation ability . Many semantic changes can not be achieved as evidenced by the fact that the counterfactual images are not realistic.__ This is indeed a limitation of our method ; we assume that the causal problem structure is known . We add a paragraph in the additional discussion section to highlight this issue . As we mention in our discussion with Reviewer 1 , an exciting future research direction might be to leverage causal discovery methods to find said structure . Further , our method is not limited to future extensions . One might add more IMs to model more fine-grained semantic changes . Also , as evidenced by our experiments , the counterfactual images are realistic enough to significantly influence a classifier \u2019 s preference on the main classification task ( Imagenet ) while maintaining performance ."}, "2": {"review_id": "BXewfAYMmJw-2", "review_text": "Summary This paper proposes a new generative model that generate images from 3 seperate aspects : foreground masks ( shapes ) , forground texture , and backgrounds . Then they convexly mix these 3 aspects into one image . By doing so , they can vary each aspect individually without changing other aspects , enabling the model to generate counterfactual images . For example , we can generate a cat shape with telephene texture and sea background , which would not exist in natural images . They show that in several colorful MNISTs datasets their methods can generate new combinations of images . In ImageNet , by using the pre-trained BigNet GAN as backbones and pretrained U-Net for foreground object masks , they can distill the knowledge in BigNet into these 3 seperate aspects and generate counterfactual natural images . Pros 1.Intersting ideas of combining pre-trained models with novel loss function that helps disentangle these 3 aspects . 2.Plausible and intriguing counterfactual examples . 3.Strong improvements in colorful MNIST datasets . Major comments 1 . The claim that `` we are able to reduce the gap while achieving hig accuracy on IN-9 '' is not true . In Table 4 , the method IN+CGN has lower accuracy than IN alone , and it reduces the gap by lowering the original performance in Mixed-same . So it does not actually improve the performance . 2.The above makes me wonder the ImageNet experiment does not actually generate plausible enough images that help improve accuracy , probably due to its unnatural image generation as evidenced by relatively low IS scores ( 130 ) . Especially seeing there are 6 lambdas to tune listed in Appendix B.3 and other hyperparameters like learning rates etc , this method might not be very practical in high-dimensional natural images . Maybe authors can be honest about it . Or illustrate what 's the best way to tune this method , what has been attempted etc . 3.The current setting seems to a bit limited that requires a single foreground and background . For example , designing independent mechanisms for medical imaging might not be easy with no exact foreground/background boundaries . 4.In Table 3 , maybe authors can evaluate on some difficult ImageNet datasets like ImageNet-A to assess if the performance improves . Minor comments 1 . The loss descriptions in Appendix B should be moved to main text to help readers understand the method . Details like how to pick $ \\tau $ for shape loss should be mentioned . 2.The name `` pre-masks '' is confusing that originally I think it 's a binary mask , but instead it 's a colorful image . 3.More failure examples in Appendix E will better help readers understand its limitations . 4.The figure 5 and the Appendix D should also include the final image that combines m , f and b to better assess its improvement over the training stages . Also , having an arrow in Figure 5 or specifying epochs might better help readers understand it is showing the transitions . 5.The caption in Figure 24 is wrong : should be columns instead of rows . Evaluations Overall I like this paper . The method is interesting to read and the examples are interesting . The experiments are thorough and do show some improvements in colorful MNISTs . The method , unfortunately , does not seem to work that well in ImageNet , and does not improve generalization performance . I encourage the authors to be upfront about the limitations of this method and write better descriptions of loss and hyperparameters tuning .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . We appreciate your assessment of our generated counterfactual images as `` plausible and intriguing '' and our experiments as `` thorough , '' and you highlighting our idea of combining pre-trained models with novel loss functions . We address your raised concerns in the following , starting with your primary concerns . __Q1 : IN-9 results - IN+CGN has lower accuracy than IN alone , and it reduces the gap by lowering the original performance in Mixed-same . So it does not actually improve the performance . The above makes me wonder the ImageNet experiment does not actually generate plausible enough images that help improve accuracy , probably due to its unnatural image generation as evidenced by relatively low IS scores ( 130 ) .__ We conducted additional experiments for this rebuttal . Our updated results show a slight improvement on Mixed-Rand ( IN : 78.9 % , IN+CGN : 80.1 % ) . We achieve this improvement by increasing the overall batch size ( both real and counterfactual batches ) during training . However , we continue to observe a drop on Mixed-Same ( IN : 86.2 % , IN+CGN : 83.4 % ) . Your question still raises a valid point - why does the performance on Mixed-Same decline ? We hypothesize that there is an additional domain shift that our approach does not address . A model trained on IN achieves the following accuracies : IN-9 ( original ) : 95.6 % , Mixed-Same : 86.2 % , Mixed-Rand : 78.9 % . A stated in [ 1 ] , Mixed-Same controls for artifacts from image processing . These artifacts lead to a drop in performance of 9.4 % - even though the background is still 100 % correlated with the label . Hence , without handling these artifacts , 86.2 % appears to be the upper boundary if all available factors of variation are used for classification -- including the background . It seems natural that a classifier which uses the background signal less and does not address these artifacts would further drop in performance , i.e. , below 86.2 % We also believe that your argument of unnatural images is correct . As we remark in our answer to Reviewer 1 , we believe that the quality of the synthetic images is not high enough yet to * improve * performance on ImageNet . On the one hand , the constraints we enforce on our model result in less realistic images , as evidenced by the lower IS and the failure cases we present ( see below ) . On the other hand , even state-of-the-art generative models ( with higher IS ) are not good enough yet to generate data for training competitive ImageNet classifiers , see [ 2 ] . __Q2 : Especially seeing there are 6 lambdas to tune listed in Appendix B.3 and other hyperparameters like learning rates etc , this method might not be very practical in high-dimensional natural images . Maybe authors can be honest about it . Or illustrate what 's the best way to tune this method , what has been attempted etc.__ Thank you for bringing up this point . We agree our initial submission was lacking details about how to find good hyperparameters . As we previously described , during training , we measure the Inception score ( IS ) and the mean value of the masks $ \\mu_ { mask } $ to detect mask collapse . We also observe the generated images for a fixed noise vector ; see the outputs in Figure 5 . Our overall objective is a high IS and a stable $ \\mu_ { mask } $ . Further , we aim for high-quality output of all IMs ( Masks : binary , capture only class-specific parts ; Textures : no background/global shape visible , Background : no trace of foreground objects visible ) . We observe these outputs for several classes during optimization . As we show in our loss ablation study , all losses are necessary for disentanglement . However , in our experience , the hyperparameters can be tuned mostly independently from each other , i.e. , a better lambda for the mask loss does not influence the quality of the texture maps much . We include this description in the appendix . __Q3 : limited setting ( requires a single foreground and background ) . Designing independent mechanisms for medical imaging might not be easy with no exact foreground/background boundaries.__ The setting of a single foreground and background is indeed a limitation of our current work . Your suggested example is an exciting one . A possibility would be to avoid the foreground/background assumption and train IMs to generate domain-specific object instances . For example , for brain scans , different IMs could model different brain regions that can only be composed in a certain way . The composition function can be informed by domain-specific knowledge . We include a discussion of the current limitations regarding the points above in the discussion section ."}, "3": {"review_id": "BXewfAYMmJw-3", "review_text": "This paper presents an interesting conceptual advance connecting causality , disentangled representation learning , invariant representations and robust classification . The authors propose a Counterfactual Generative Network ( CGN ) , which is basically `` modular '' generative adversarial network that can independently control the generation of independent factors of variations in the data corresponding to Independent Mechanism , i.e.independent factors in the structural causal model of the data . In the context of generating natural images like those comprising the ImageNet dataset , the CGN once trained can be used to generate high-quality counterfactual images with direct control over of factors of variations determining the content of an image shape , texture , and background . These generated samples obtained by independently and uniformly sampling over factors of variation can be used to train a classifier to achieve out-of-domain robustness . The authors show indeed show in simulation that this procedure works as a data augmentation procedure that increases out-of-domain robustness while only marginally degrades the overall accuracy . As the authors explain , this can be thought of as a generalization of `` domain randomization '' . Additionally , CGN can be used as a generative model of high-quality binary object masks and unsupervised image inpainting . The authors also carry out extensive ablation studies that quantify the contribution of the different training costs for CGN to the overall quality ( measures as Inception Score ) of the generated counterfactual images . This is first and foremost an `` idea paper '' putting forth a very interesting conceptual proposal . This is then empirically validated on out-of-distribution classification tasks in different versions of colored MNIST , and a coarse grained subsed of ImageNet . A natural question for the authors is whether and by how much this type of data augmentation might help in realistic large-scale classification scenarios . Another natural question would be to quantify the effect of the counterfactual data augmentation in terms of trade-off between performance on in-distribution and out-of-distribution samples . Lastly , it would be interesting to know whether the authors have any thought on how to generalize their architecture to other setting , i.e.how to isolate Independent Mechanisms in a general domain and what type of domain knowledge is necessary to do that effectively .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review . We appreciate your assessment of our work as an \u201c interesting conceptual advance. \u201d We , too , believe that our main contribution is connecting the fields of causality , disentanglement , and robust classification . With further advances in generative modeling and synthetic-to-real literature , we are optimistic that we can further improve results on realistic datasets like Imagenet . In the following , we like to address all the questions you raised . __Q1 : whether and by how much this type of data augmentation might help in realistic large-scale classification scenarios.__ As we show in Table 3 ( Shape vs. Texture ) , our invariant classifier ensemble is able to trade off the different factors of variation on the full Imagenet classification task . Interestingly , we can strongly influence the classifier \u2019 s preferences while maintaining good performance on the ( large-scale ) classification task - even more than augmenting with Stylized Imagenet . Our intuition is that the quality of the synthetic images is not high enough yet to * improve * performance on ImageNet itself . On the one hand , the constraints we enforce on our model result in less realistic images , as evidenced by the lower IS . On the other hand , even state-of-the-art generative models ( with higher IS ) are not good enough yet to generate data for training competitive ImageNet classifiers , see [ 1 ] . __Q2 : Effect of the counterfactual data augmentation in terms of trade-off between performance on in-distribution and out-of-distribution samples.__ This question is indeed interesting , and we ran additional experiments both on MNIST and Imagenet . We did not observe a difference in our initial experiments when training on both OOD and IID data , or only on OOD data . We increase the OOD/IID ratio beyond 1 ( the initially used ratio ) in our new experiments . Interestingly , on MNIST , there seems to be no downgrade in performance when we increase the ratio of OOD/IID samples - more OOD data always improves the performance on the test set . This effect holds up to a ratio of 20 ; beyond that , we see stagnation . By increasing this ratio , we achieve a substantial improvement over our initial reported numbers ( Colored MNIST 86.4 % - > 95.1 % , Double-Colored MNIST 85.3 % - > 89.0 % , Wildlife MNIST 80.5 % - > 85.7 % ) . These results illustrate the potential of generated counterfactual data , especially in less complex domains . We add these new results to the paper . As for Imagenet , we settled on a ratio of 1 for all of our experiments , based on similar results by [ 1 ] . A ratio below 1 leads to inferior performance on both Imagenet and IN-9 . For a ratio above 1 , the training time for a single epoch increases . However , the network also learns faster on the counterfactual data so that these two effects keep a balance , but we also do not observe an improvement over a ratio of 1 . __Q3 : Lastly , it would be interesting to know whether the authors have any thought on how to generalize their architecture to other setting , i.e.how to isolate Independent Mechanisms in a general domain and what type of domain knowledge is necessary to do that effectively.__ We , too , are very interested in how to generalize our ideas to different settings . A straightforward way to extend our proposed CGN is to add several branches for object instances . This type of disentanglement might produce useful training data for semantic ( instance ) segmentation . Another possible avenue is to build on advances in 3d generative modeling [ 2 , 3 ] to generate data to train classifiers invariant to object pose . It is generally hard to state what domain knowledge is necessary and may depend heavily on the respective domain . With our focus on image classification , we incorporated general information about compositional image formation , i.e. , alpha blending . That being said , it would be interesting to explore ideas from causal discovery to isolate IMs in a domain-agnostic manner , e.g. , via meta-learning [ 4 ] . __References__ [ 1 ] Suman , Vinyals , \u201c Seeing is not necessarily believing : Limitations of biggans for data augmentation \u201d , 2019 , ICLR Workshop LLD , https : //openreview.net/forum ? id=rJMw747l_4 [ 2 ] Nguyen-Phuoc , Li , Theis , Richardt , Yang , \u201c Hologan : Unsupervised learning of 3d representations from natural images \u201d , 2019 , ICCV , https : //arxiv.org/abs/1904.01326 [ 3 ] Liao , Schwarz , Mescheder , Geiger , \u201c Towards unsupervised learning of generative models for 3d controllable image synthesis \u201d , 2020 , CVPR , https : //arxiv.org/abs/1912.05237 [ 4 ] Bengio , Deleu , Rahaman , Ke , Lachapelle , Bilaniuk , Goyal , Pal \u201c A meta-transfer objective for learning to disentangle causal mechanisms \u201d , 2019 , ICLR , https : //arxiv.org/abs/1901.10912"}}