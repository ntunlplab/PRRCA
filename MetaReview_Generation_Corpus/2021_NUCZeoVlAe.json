{"year": "2021", "forum": "NUCZeoVlAe", "title": "Empirical Studies on the Convergence of Feature Spaces in Deep Learning", "decision": "Reject", "meta_review": "This paper presents an intriguing empirical phenomenon in deep learning. They train a variety of architectures for different tasks using different datasets and study the relationship between the learned representations. In particular they collect the representations into a large matrix and take the top left singular vector and measure the cosine of the angle. They show that it is much smaller than one might expect, about 10 degrees or so, and has an approximate monotonicity property as the network is being trained although it does not seem to converge to zero. Moreover this measure also correlates with performance. \n\nThe reviewers had divided opinions on this paper. On the one hand, the range of experiments is impressive and truly demonstrates that this is a pervasive phenomenon. On the other hand, it is not so clear what it means. In particular, suppose we have a collection of graphs which have close to the same degree distributions. If we take the top left singular vectors of all the adjacency matrices, they would also have low angles between them. While this is a very different setting and there is no analogy between the experiments in this paper and this toy model, it does raise philosophical questions about whether the phenomenon is meaningful or is a byproduct of something else about the data. This may be a challenging question to answer, but one reviewer brought up a natural next step: One could measure the principal angle between the subspace of the top k left singular vectors across experiments for larger values of k. The authors do bring up the point that the spectrum decays very quickly, so it could be that beyond a certain point the singular vectors behave somewhat randomly. ", "reviews": [{"review_id": "NUCZeoVlAe-0", "review_text": "This paper studies the top singular vector of the feature space learned by supervised and unsupervised deep learning models on CIFAR datasets . The hypothesis of converging feature spaces is interesting ( converging both in terms of different models , and in terms of training epochs ) , but the conclusion from the current experiment results is overstretching . 1.While the authors emphasize the convergence of subspaces , the P-vector defined in the paper is actually the top singular vector of the feature space , so it 's actually about the convergence of the 1-dimensional principal subspace . A subspace refers to an arbitrary dimensional space in general . In the context of SVD , the literature often studies the top- $ k $ dimensional subspace , which is represented by the $ k $ top singular vectors , and the approximation error of the top- $ k $ dimensional subspace : $ E=\\|X - U_k \\Sigma_k V_k^T\\|_F^2 $ , where $ X $ would be the feature matrix in this paper , and $ U_k , V_k $ are the first $ k $ columns in the result of SVD . The authors did n't measure $ E $ , so the readers wo n't know how well the top-1 dimensional subspace represents the feature matrix . I recommend looking at $ E $ as a function of $ k $ , and use some criteria to determine how closely you want the subspace to approximate the feature matrix . For example , we can say we want to keep the top- $ k $ dimensional subspace such that $ E < 0.1 \\|X\\|_F^2 $ . This way , you can rule out the possibility that the P-vector is a trivial vector that every model will converge to . ( As an analogy for a trivial vector , we can consider the top-1 eigenvector of the similarity matrix defined in the classical spectral clustering method called Normalized Cut.No matter how the edge weights in a graph is defined , the similarity matrix used in Normalized Cut always has an all-one vector as the top-1 eigenvector . ) And to measure the angle between general subspaces , many methods are available including classical ones ( e.g.\u00c5ke Bj\u00f6rck and Gene H. Golub , Numerical Methods for Computing Angles Between Linear Subspaces , 1973 ) . 2.This paper tries to emphasize the P-vectors found in the features from different deep learning models are very close ( for example , `` no matter what type of DNN architectures or whether the labels have been used to train the models , the P-vectors of different models would converge to the same one '' ) . Actually it seems the angle typically converges to 10 to 20 degrees . It may be better to lower the tone , or quantify better ( compared to the angles obtained by ... , the angles between P-vectors are smaller ) . 3.The data in Fig.7 looks quite noisy , though p-value shows statistical significance of the correlation . p-value can guide our findings but is not always meaningful . For example , comparing Fig.7 ( e ) and Fig.7 ( l ) , we may argue the latter has a better correlation but the former has a much smaller p-value . It seems the very small p-value in Fig.7 ( e ) results from some outliers . Intuitively I do n't quite understand why the raw data and the features should have a correlated linear principal subspace , given that the neural network layers that generate the feature from the data are highly nonlinear . The only convincing data I found is in Table 1 , which shows P-vectors can serve as an indicator of the model performance . But overall the readers would need more evidence as explained in # 1 above .", "rating": "3: Clear rejection", "reply_text": "Even you conclude the P-vector is trivial upon the additional results above and figures in the appendix , we still make contribution in this work\u2013it might be the first evidence on the triviality of principal subspace of features learned by DNN or well-trained DNNs would converge to have a trivial principal subspace in their feature learning . Many thanks for mention the existing work in measuring the angle between principal subspaces . We will include these work in the discussion . We really appreciate your guidance here . We will significantly extend the methodology section to first analyze the singular value distribution of feature matrices , explained variances , and the reconstruction error of approximation using the top-k singular vectors , prior to introducing our observations ! Many thanks ! Explained Variances of top-k singular vectors : | k | Ratio | | - | - | | 1 | 0.5674805 | | 2 | 0.6489089 | |3 | 0.7067079| |4 | 0.75878125| |5 | 0.8063708| |6 | 0.8489084| |7 | 0.8891225| |8 | 0.92527133| |9 | 0.9577149| |10 | 0.98870885| |11 | 0.9896392| |12 | 0.9902976| Many thanks for your advices . We will lower our tone in the formal revised manuscript . Indeed , cosin ( 10 $ ^\\circ $ ) =98.5\\ % and cosin ( 20 $ ^\\circ $ ) = 94.0\\ % , are quite small when we treat the cosine measure in a correlation sense . All in all , in our formal revised manuscript , we will conclude the \u201c converging trends \u201d of the angle between P-vectors , and emphasized that it would not converge to zero , but to a much smaller angle . Many thanks for your question on the correlation between features and raw data in their linear principal subspace . First of all , this is just our empirical observation and we report it . Latter , to avoid possible affects due to the outliers , we tested the hypothesis using Spearman \u2019 s correlation to correlate the angles and training/testing accuracy , which considers the order of the data point in the samples rather than the value of them . In this way , few outliers would not dominate the correlation analysis . In the current modified manuscript , we take the log-log plots , where we can see the same phenomena . Thus , we don \u2019 t believe a few outliers would affect our conclusion here . Though the overall DNN is nonlinear , many previous work [ 1 ] , [ 2 ] also demonstrates the local linearity or piecewise linearity of DNN with certain activations . We believe it is the linear subcomponents of the DNN ( e.g. , a significant first-order term in the Taylor expansion ) that \u201c leaks \u201d certain information about the raw data through a weak linear transform . In this way , the correlation between features and raw data in the linear principal subspace characterizes \u201c how linear is the feature extractor of a DNN model \u201d which should relates to the ( generalization ) performance . Of-course , it is all of our intuition . We didn \u2019 t hope to claim it in the manuscript . To address your comment , we will discuss this issue in the formal revised manuscript to elaborate our intuition well . We really appreciate your comments . We hope to could get a chance to be shepherded and improve the manuscript . Please feel free to comment on the discussion thread , especially when we made anything wrong . Many thanks ! [ 1 ] Zhang X , Wu D. Empirical Studies on the Properties of Linear Regions in Deep Neural Networks [ C ] //International Conference on Learning Representations . 2019 . [ 2 ] Arora R , Basu A , Mianjy P , et al.Understanding Deep Neural Networks with Rectified Linear Units [ C ] //International Conference on Learning Representations . 2018 ."}, {"review_id": "NUCZeoVlAe-1", "review_text": "Summary : This paper has a closer look at the distributions of samples in the feature space by utilizing P-vector to analyze principal subspace . According to their empirical studies , the authors concluded that the feature spaces learned by different deep models with the same dataset would share common principal subspaces for the same dataset . It will not be affected by DNN architectures or the usage of labels in feature learning . Only the training procedure gradually shapes the feature subspace to the shared common subspace . -- Reasons for score : The paper explores a new question and gives some interesting conclusions . But my major concern is its empirical studies can not support the findings well . Besides , there are few discussions to provide the readers with some insights . I hope the authors carefully consider how to enhance this paper and make the conclusions more convincing . -- Pros : 1 . The paper explores a new question and gives some interesting conclusions . 2.The proposed metric is simple and easy to follow . The authors also attached the source code for reference . 3.The usage of the P-vector for predicting generalization achieves promising results . -- Cons : 1 . Why can the similarity of P-vectors be used to indicate the similarity of two distributions in the feature space ? 2.I would like to know how many trials it takes to plot similarity figures ( e.g. , Figure 1 ( a ) - ( b ) ) . It would be better to try many times and give the mean and variance to avoid coincidence . Besides , will other parameters such as the number of samples and dimensions of features affect Hypothesis I ? 3.The authors mentioned that the reference model used in Figure 4 ( a ) - ( c ) is Wide-ResNet28 trained with 200 epochs under suggest settings . But , the plot of Wide-ResNet28 in Figure 4 ( a ) is weird . It can not converge to zero . 4.I would like to know why most models ( such as Figure 4 ) can not converge to zero after about 200 epochs training , and the angles are approximately 10 degrees . In other words , is there exists a threshold after which we can think the compared two models have a common subspace ? 5.Why the P-vector can be used to predict the generalization ? -- Questions during the rebuttal period : Please address and clarify the cons above . -- Some typos : ( 1 ) Figure 5 , Figure 9 , the xaxis 's titile should be iterations rather than epochs .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Many thanks for your comments on the definition of \u201c convergence \u201d . We agree with you that the angles between P-vectors here usually can not converge to zero after 200 epochs training , and the angles remaining are around 10 $ ^\\circ $ degrees . Actually , we use top left singular vector of the feature matrix ( i.e. , # samples $ \\times $ # features ) as the P-vector . Thus , the number of dimensions of a P-vector is equivalent to the number of samples in a dataset . When we use CIFAR-10 or CIFAR-100 for experiments , the P-vectors should be with 50,000 dimensions . Please note that the high-dimensional random vectors are tending to orthogonal when they are not correlated . In our opinion , 10 $ ^\\circ $ degree is indeed quite small in such case , compared to the 80 $ ^\\circ $ to 90 $ ^\\circ $ in the begin of training procedure . Furthermore , cosine ( 10 $ ^\\circ $ ) =0.985 , which is quite significant in similarity comparison . Thus , we believe the remaining angles would not hurt our claims . To address your comments , we will include a discussion on the definition of \u201c convergence \u201d in the formal revised manuscript . Many thanks for your comments on the relation between P-vector and \u201c generalization \u201d . Indeed , we estimate the top left singular vector of the raw data matrix as the data P-vector and use the angle between the data P-vector and the P-vector ( obtained from the feature matrix of a model ) to predict performance of the model . A small angle between the data P-vector and the P-vector well demonstrates the correlation or divergence between the distribution of samples in the raw dataset and the distribution of samples in the feature space , through comparing their principal subspaces . Our intuition is that when the principal subspace of the raw dataset is close to the feature one , the CNN model would preserve more information about the data distribution ( even though the models are intensively parameterized and trained ) , demonstrate higher linearity ( as the principal subspaces of features should equivalent to the principal subspace of raw data when only linear transform applied to the data ) , are supposed to be with better generalization performance . We are inspired by some work on the local linearity of CNN , such as [ 1 ] and [ 2 ] . After all , we only hope to report our observations as a potential application of P-vector , while we don \u2019 t intend to over-claim the effectiveness of P-vector for model selection and/or generalization prediction . To address your comments , we will discuss these issues in the formal revised manuscript . Please check the appendix of current modified manuscript for additional examples , observations , and evidences . Again , many thanks for your review and encouraging comments . We will address all your concerns and fix language issues in the revised version . Please feel free to comment on the thread of discussion timely and shepherd us for improving the manuscript . [ 1 ] Zhang X , Wu D. Empirical Studies on the Properties of Linear Regions in Deep Neural Networks [ C ] //International Conference on Learning Representations . 2019 . [ 2 ] Arora R , Basu A , Mianjy P , et al.Understanding Deep Neural Networks with Rectified Linear Units [ C ] //International Conference on Learning Representations . 2018 ."}, {"review_id": "NUCZeoVlAe-2", "review_text": "The authors identify an interesting empirical phenomenon : across a range of network architectures and training approaches ( supervised , unsupervised , auto-encoders ) , the feature spaces identified by these networks are similar . The authors introduce a specific way to summarize the feature space of a network as a vector ( the top-left singular vector of the num_examples x num_features matrix ) and show that these vectors are highly correlated across networks . In addition , the authors show that the features spaces become more similar throughout training and are predictive of the generalization performance of a neural network . The paper presents purely experimental findings , but the experiments are sufficiently broad ( e.g. , covering different training approaches and datasets ) so that this is not a shortcoming . Investigating potential theoretical models or more models and datasets could be fruitful directions for future work . Aspects of the presentation in the paper could be improved ( see the comments below ) . Overall I still recommend accepting the paper . Additional comments : - Many plots have labels that are too small to read . I strongly encourage the authors to produce more readable plots . - Have the authors explored visualizations of the P-vectors ? For instance , what is the ordering of training examples induced by the P-vectors ? - What is the `` SupCon '' method ? Do the authors have a hypothesis for why it behaves different from the other methods w.r.t.P-vector angles ? - Have the authors experimented with training approaches that explicitly encourage small angles between model and data P-vectors ? - The paper would benefit from a thorough editing pass to fix typos and improve clarity . The structure of the paper is well-organized , just some sentences are hard to parse . - When abbreviations like `` AE '' or `` CNN '' are used for the first time , it is generally good to write them out . - Why does the paper sometimes use angle and sometimes use cosine of the angle ? It could be better to use one of the two consistently . - Should the x-axis labels in Figure 5 be `` training steps '' instead of `` epochs '' ? - There is too little vertical space separating the caption of Figure 7 from the text below . - Typos : * Introduction : `` To better euclid '' * Section 3 : `` various architectures amd different training paradigms '' * Section 5 : `` an data , We carry out '' ( capitalization ) * Section 5 : `` expect '' - > `` except ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "Many thanks for the review and encouraging comments . Actually , we have upload a modified version of manuscript , with a new appendix including additional figures/tables/evidences to address some of your concerns . We are working hard to revise the manuscript accordingly to address all your concerns and fix all language and presentation issues . For the formal revised manuscript , we will upload it before the end of rebuttal period . Actually , in P-vector computation , we use the index of every sample in the training/test datasets as the indices of dimensions in the P-vector . In appendix of the current modified manuscript , we have provided the visualization of the P-vector , including a figure on the P-vector values versus the indices of samples , figures on the frequency of the P-vector values ( counts versus the P-vector values ) , and a figure on the smoothed density of the P-vector values ( probability density versus the -vector values ) . The feature matrices are obtained through training a ResNet50 model using CIFAR-10 datasets , the feature matrices of 0th epoch ( obtained by random initial weights ) , 60th epoch , 120th epoch , and 200th epoch ( well-trained ) have been used for plots . In the formal revised manuscript , we will include all results based on the three datasets . SupCon refers to the supervised contrastive learning [ 1 ] . We include SupCon together with SimCLR [ 2 ] as two typical algorithms for self-supervised training . Generally , self-supervised learning , especially SimCLR [ 2 ] , aims at improving the training procedure of deep neural networks with self-supervised contrastive loss . While SimCLR [ 2 ] trains CNN feature extractors without the use of label information , SupCon [ 1 ] extended the self-supervised paradigm using supervised contrastive learning ( which is based on the labels ) . To make it clear , in the revised manuscript , we will address this issue when we introduce the self-supervised learning paradigms . We didn \u2019 t incorporate any algorithms , regularizers , or any treatments to make the angles between P-vectors smaller . All algorithms used for DNN training here were based on the open source implementations that are available online . The goal of our research is to investigate the similarity between P-vectors for DNN trained using the same dataset with various architectures/tasks . All experiments were carried out to follow the standard implementation and operations . Again , many thanks for your review and encouraging comments . We will address all your concerns and fix language issues in the revised version . Please feel free to comment on the thread of discussion and timely shepherd us for improving the manuscript . [ 1 ] Khosla P , Teterwak P , Wang C , et al.Supervised contrastive learning [ J ] . arXiv preprint arXiv:2004.11362 , 2020 . [ 2 ] Chen T , Kornblith S , Norouzi M , et al.A simple framework for contrastive learning of visual representations [ J ] . arXiv preprint arXiv:2002.05709 , 2020 . MLA"}], "0": {"review_id": "NUCZeoVlAe-0", "review_text": "This paper studies the top singular vector of the feature space learned by supervised and unsupervised deep learning models on CIFAR datasets . The hypothesis of converging feature spaces is interesting ( converging both in terms of different models , and in terms of training epochs ) , but the conclusion from the current experiment results is overstretching . 1.While the authors emphasize the convergence of subspaces , the P-vector defined in the paper is actually the top singular vector of the feature space , so it 's actually about the convergence of the 1-dimensional principal subspace . A subspace refers to an arbitrary dimensional space in general . In the context of SVD , the literature often studies the top- $ k $ dimensional subspace , which is represented by the $ k $ top singular vectors , and the approximation error of the top- $ k $ dimensional subspace : $ E=\\|X - U_k \\Sigma_k V_k^T\\|_F^2 $ , where $ X $ would be the feature matrix in this paper , and $ U_k , V_k $ are the first $ k $ columns in the result of SVD . The authors did n't measure $ E $ , so the readers wo n't know how well the top-1 dimensional subspace represents the feature matrix . I recommend looking at $ E $ as a function of $ k $ , and use some criteria to determine how closely you want the subspace to approximate the feature matrix . For example , we can say we want to keep the top- $ k $ dimensional subspace such that $ E < 0.1 \\|X\\|_F^2 $ . This way , you can rule out the possibility that the P-vector is a trivial vector that every model will converge to . ( As an analogy for a trivial vector , we can consider the top-1 eigenvector of the similarity matrix defined in the classical spectral clustering method called Normalized Cut.No matter how the edge weights in a graph is defined , the similarity matrix used in Normalized Cut always has an all-one vector as the top-1 eigenvector . ) And to measure the angle between general subspaces , many methods are available including classical ones ( e.g.\u00c5ke Bj\u00f6rck and Gene H. Golub , Numerical Methods for Computing Angles Between Linear Subspaces , 1973 ) . 2.This paper tries to emphasize the P-vectors found in the features from different deep learning models are very close ( for example , `` no matter what type of DNN architectures or whether the labels have been used to train the models , the P-vectors of different models would converge to the same one '' ) . Actually it seems the angle typically converges to 10 to 20 degrees . It may be better to lower the tone , or quantify better ( compared to the angles obtained by ... , the angles between P-vectors are smaller ) . 3.The data in Fig.7 looks quite noisy , though p-value shows statistical significance of the correlation . p-value can guide our findings but is not always meaningful . For example , comparing Fig.7 ( e ) and Fig.7 ( l ) , we may argue the latter has a better correlation but the former has a much smaller p-value . It seems the very small p-value in Fig.7 ( e ) results from some outliers . Intuitively I do n't quite understand why the raw data and the features should have a correlated linear principal subspace , given that the neural network layers that generate the feature from the data are highly nonlinear . The only convincing data I found is in Table 1 , which shows P-vectors can serve as an indicator of the model performance . But overall the readers would need more evidence as explained in # 1 above .", "rating": "3: Clear rejection", "reply_text": "Even you conclude the P-vector is trivial upon the additional results above and figures in the appendix , we still make contribution in this work\u2013it might be the first evidence on the triviality of principal subspace of features learned by DNN or well-trained DNNs would converge to have a trivial principal subspace in their feature learning . Many thanks for mention the existing work in measuring the angle between principal subspaces . We will include these work in the discussion . We really appreciate your guidance here . We will significantly extend the methodology section to first analyze the singular value distribution of feature matrices , explained variances , and the reconstruction error of approximation using the top-k singular vectors , prior to introducing our observations ! Many thanks ! Explained Variances of top-k singular vectors : | k | Ratio | | - | - | | 1 | 0.5674805 | | 2 | 0.6489089 | |3 | 0.7067079| |4 | 0.75878125| |5 | 0.8063708| |6 | 0.8489084| |7 | 0.8891225| |8 | 0.92527133| |9 | 0.9577149| |10 | 0.98870885| |11 | 0.9896392| |12 | 0.9902976| Many thanks for your advices . We will lower our tone in the formal revised manuscript . Indeed , cosin ( 10 $ ^\\circ $ ) =98.5\\ % and cosin ( 20 $ ^\\circ $ ) = 94.0\\ % , are quite small when we treat the cosine measure in a correlation sense . All in all , in our formal revised manuscript , we will conclude the \u201c converging trends \u201d of the angle between P-vectors , and emphasized that it would not converge to zero , but to a much smaller angle . Many thanks for your question on the correlation between features and raw data in their linear principal subspace . First of all , this is just our empirical observation and we report it . Latter , to avoid possible affects due to the outliers , we tested the hypothesis using Spearman \u2019 s correlation to correlate the angles and training/testing accuracy , which considers the order of the data point in the samples rather than the value of them . In this way , few outliers would not dominate the correlation analysis . In the current modified manuscript , we take the log-log plots , where we can see the same phenomena . Thus , we don \u2019 t believe a few outliers would affect our conclusion here . Though the overall DNN is nonlinear , many previous work [ 1 ] , [ 2 ] also demonstrates the local linearity or piecewise linearity of DNN with certain activations . We believe it is the linear subcomponents of the DNN ( e.g. , a significant first-order term in the Taylor expansion ) that \u201c leaks \u201d certain information about the raw data through a weak linear transform . In this way , the correlation between features and raw data in the linear principal subspace characterizes \u201c how linear is the feature extractor of a DNN model \u201d which should relates to the ( generalization ) performance . Of-course , it is all of our intuition . We didn \u2019 t hope to claim it in the manuscript . To address your comment , we will discuss this issue in the formal revised manuscript to elaborate our intuition well . We really appreciate your comments . We hope to could get a chance to be shepherded and improve the manuscript . Please feel free to comment on the discussion thread , especially when we made anything wrong . Many thanks ! [ 1 ] Zhang X , Wu D. Empirical Studies on the Properties of Linear Regions in Deep Neural Networks [ C ] //International Conference on Learning Representations . 2019 . [ 2 ] Arora R , Basu A , Mianjy P , et al.Understanding Deep Neural Networks with Rectified Linear Units [ C ] //International Conference on Learning Representations . 2018 ."}, "1": {"review_id": "NUCZeoVlAe-1", "review_text": "Summary : This paper has a closer look at the distributions of samples in the feature space by utilizing P-vector to analyze principal subspace . According to their empirical studies , the authors concluded that the feature spaces learned by different deep models with the same dataset would share common principal subspaces for the same dataset . It will not be affected by DNN architectures or the usage of labels in feature learning . Only the training procedure gradually shapes the feature subspace to the shared common subspace . -- Reasons for score : The paper explores a new question and gives some interesting conclusions . But my major concern is its empirical studies can not support the findings well . Besides , there are few discussions to provide the readers with some insights . I hope the authors carefully consider how to enhance this paper and make the conclusions more convincing . -- Pros : 1 . The paper explores a new question and gives some interesting conclusions . 2.The proposed metric is simple and easy to follow . The authors also attached the source code for reference . 3.The usage of the P-vector for predicting generalization achieves promising results . -- Cons : 1 . Why can the similarity of P-vectors be used to indicate the similarity of two distributions in the feature space ? 2.I would like to know how many trials it takes to plot similarity figures ( e.g. , Figure 1 ( a ) - ( b ) ) . It would be better to try many times and give the mean and variance to avoid coincidence . Besides , will other parameters such as the number of samples and dimensions of features affect Hypothesis I ? 3.The authors mentioned that the reference model used in Figure 4 ( a ) - ( c ) is Wide-ResNet28 trained with 200 epochs under suggest settings . But , the plot of Wide-ResNet28 in Figure 4 ( a ) is weird . It can not converge to zero . 4.I would like to know why most models ( such as Figure 4 ) can not converge to zero after about 200 epochs training , and the angles are approximately 10 degrees . In other words , is there exists a threshold after which we can think the compared two models have a common subspace ? 5.Why the P-vector can be used to predict the generalization ? -- Questions during the rebuttal period : Please address and clarify the cons above . -- Some typos : ( 1 ) Figure 5 , Figure 9 , the xaxis 's titile should be iterations rather than epochs .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Many thanks for your comments on the definition of \u201c convergence \u201d . We agree with you that the angles between P-vectors here usually can not converge to zero after 200 epochs training , and the angles remaining are around 10 $ ^\\circ $ degrees . Actually , we use top left singular vector of the feature matrix ( i.e. , # samples $ \\times $ # features ) as the P-vector . Thus , the number of dimensions of a P-vector is equivalent to the number of samples in a dataset . When we use CIFAR-10 or CIFAR-100 for experiments , the P-vectors should be with 50,000 dimensions . Please note that the high-dimensional random vectors are tending to orthogonal when they are not correlated . In our opinion , 10 $ ^\\circ $ degree is indeed quite small in such case , compared to the 80 $ ^\\circ $ to 90 $ ^\\circ $ in the begin of training procedure . Furthermore , cosine ( 10 $ ^\\circ $ ) =0.985 , which is quite significant in similarity comparison . Thus , we believe the remaining angles would not hurt our claims . To address your comments , we will include a discussion on the definition of \u201c convergence \u201d in the formal revised manuscript . Many thanks for your comments on the relation between P-vector and \u201c generalization \u201d . Indeed , we estimate the top left singular vector of the raw data matrix as the data P-vector and use the angle between the data P-vector and the P-vector ( obtained from the feature matrix of a model ) to predict performance of the model . A small angle between the data P-vector and the P-vector well demonstrates the correlation or divergence between the distribution of samples in the raw dataset and the distribution of samples in the feature space , through comparing their principal subspaces . Our intuition is that when the principal subspace of the raw dataset is close to the feature one , the CNN model would preserve more information about the data distribution ( even though the models are intensively parameterized and trained ) , demonstrate higher linearity ( as the principal subspaces of features should equivalent to the principal subspace of raw data when only linear transform applied to the data ) , are supposed to be with better generalization performance . We are inspired by some work on the local linearity of CNN , such as [ 1 ] and [ 2 ] . After all , we only hope to report our observations as a potential application of P-vector , while we don \u2019 t intend to over-claim the effectiveness of P-vector for model selection and/or generalization prediction . To address your comments , we will discuss these issues in the formal revised manuscript . Please check the appendix of current modified manuscript for additional examples , observations , and evidences . Again , many thanks for your review and encouraging comments . We will address all your concerns and fix language issues in the revised version . Please feel free to comment on the thread of discussion timely and shepherd us for improving the manuscript . [ 1 ] Zhang X , Wu D. Empirical Studies on the Properties of Linear Regions in Deep Neural Networks [ C ] //International Conference on Learning Representations . 2019 . [ 2 ] Arora R , Basu A , Mianjy P , et al.Understanding Deep Neural Networks with Rectified Linear Units [ C ] //International Conference on Learning Representations . 2018 ."}, "2": {"review_id": "NUCZeoVlAe-2", "review_text": "The authors identify an interesting empirical phenomenon : across a range of network architectures and training approaches ( supervised , unsupervised , auto-encoders ) , the feature spaces identified by these networks are similar . The authors introduce a specific way to summarize the feature space of a network as a vector ( the top-left singular vector of the num_examples x num_features matrix ) and show that these vectors are highly correlated across networks . In addition , the authors show that the features spaces become more similar throughout training and are predictive of the generalization performance of a neural network . The paper presents purely experimental findings , but the experiments are sufficiently broad ( e.g. , covering different training approaches and datasets ) so that this is not a shortcoming . Investigating potential theoretical models or more models and datasets could be fruitful directions for future work . Aspects of the presentation in the paper could be improved ( see the comments below ) . Overall I still recommend accepting the paper . Additional comments : - Many plots have labels that are too small to read . I strongly encourage the authors to produce more readable plots . - Have the authors explored visualizations of the P-vectors ? For instance , what is the ordering of training examples induced by the P-vectors ? - What is the `` SupCon '' method ? Do the authors have a hypothesis for why it behaves different from the other methods w.r.t.P-vector angles ? - Have the authors experimented with training approaches that explicitly encourage small angles between model and data P-vectors ? - The paper would benefit from a thorough editing pass to fix typos and improve clarity . The structure of the paper is well-organized , just some sentences are hard to parse . - When abbreviations like `` AE '' or `` CNN '' are used for the first time , it is generally good to write them out . - Why does the paper sometimes use angle and sometimes use cosine of the angle ? It could be better to use one of the two consistently . - Should the x-axis labels in Figure 5 be `` training steps '' instead of `` epochs '' ? - There is too little vertical space separating the caption of Figure 7 from the text below . - Typos : * Introduction : `` To better euclid '' * Section 3 : `` various architectures amd different training paradigms '' * Section 5 : `` an data , We carry out '' ( capitalization ) * Section 5 : `` expect '' - > `` except ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "Many thanks for the review and encouraging comments . Actually , we have upload a modified version of manuscript , with a new appendix including additional figures/tables/evidences to address some of your concerns . We are working hard to revise the manuscript accordingly to address all your concerns and fix all language and presentation issues . For the formal revised manuscript , we will upload it before the end of rebuttal period . Actually , in P-vector computation , we use the index of every sample in the training/test datasets as the indices of dimensions in the P-vector . In appendix of the current modified manuscript , we have provided the visualization of the P-vector , including a figure on the P-vector values versus the indices of samples , figures on the frequency of the P-vector values ( counts versus the P-vector values ) , and a figure on the smoothed density of the P-vector values ( probability density versus the -vector values ) . The feature matrices are obtained through training a ResNet50 model using CIFAR-10 datasets , the feature matrices of 0th epoch ( obtained by random initial weights ) , 60th epoch , 120th epoch , and 200th epoch ( well-trained ) have been used for plots . In the formal revised manuscript , we will include all results based on the three datasets . SupCon refers to the supervised contrastive learning [ 1 ] . We include SupCon together with SimCLR [ 2 ] as two typical algorithms for self-supervised training . Generally , self-supervised learning , especially SimCLR [ 2 ] , aims at improving the training procedure of deep neural networks with self-supervised contrastive loss . While SimCLR [ 2 ] trains CNN feature extractors without the use of label information , SupCon [ 1 ] extended the self-supervised paradigm using supervised contrastive learning ( which is based on the labels ) . To make it clear , in the revised manuscript , we will address this issue when we introduce the self-supervised learning paradigms . We didn \u2019 t incorporate any algorithms , regularizers , or any treatments to make the angles between P-vectors smaller . All algorithms used for DNN training here were based on the open source implementations that are available online . The goal of our research is to investigate the similarity between P-vectors for DNN trained using the same dataset with various architectures/tasks . All experiments were carried out to follow the standard implementation and operations . Again , many thanks for your review and encouraging comments . We will address all your concerns and fix language issues in the revised version . Please feel free to comment on the thread of discussion and timely shepherd us for improving the manuscript . [ 1 ] Khosla P , Teterwak P , Wang C , et al.Supervised contrastive learning [ J ] . arXiv preprint arXiv:2004.11362 , 2020 . [ 2 ] Chen T , Kornblith S , Norouzi M , et al.A simple framework for contrastive learning of visual representations [ J ] . arXiv preprint arXiv:2002.05709 , 2020 . MLA"}}