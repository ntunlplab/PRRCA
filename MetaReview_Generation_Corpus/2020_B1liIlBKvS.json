{"year": "2020", "forum": "B1liIlBKvS", "title": "Selfish Emergent Communication", "decision": "Reject", "meta_review": "There has been a long discussion on the paper, especially between the authors and the 2nd reviewer. While the authors' comments and paper modifications have improved the paper, the overall opinion on this paper is that it is below par in its current form. The main issue is that the significance of the results is insufficiently clear.  While the sender-receiver game introduced is interesting, a more thorough investigation would improve the paper a lot (for example, by looking if theoretical statements can be made).", "reviews": [{"review_id": "B1liIlBKvS-0", "review_text": "This paper looks at the question of emergent communication amongst self-interested learning agents. The paper finds that \"selfish\" (ie. self-interested) agents can learn to communicate using a cheap talk channel as long as the objective is partially cooperative. The paper makes states that this is is a novel finding that contradicts the previous understanding of emergent communication in the literature (side point: at least some of the papers referenced for this claim did not at all make the claim). I believe there is a major miss-understanding here: As noted in the paper, self-interested agents can learn to communicate in settings in which the reward function is cooperative. Furthermore, it is also known that in 2 player zero-sum there is no incentive to learn a communication protocol. This clearly shows that talking about whether or not \"selfish\" agents can learn to communicate only ever makes sense within the context of a specific game / reward structure. With this in mind, the main finding, agents learn to somewhat communicate with each other in a simple toy setting, with more communication happening when the payouts are more cooperative, is not very interesting. This doesn't mean that there isn't a good paper to be written here, in principle. Finding simple settings in which SOTA multi-agent learning \"fails\", ie. doesn't find Nash policies, understanding why it fails and then finding ways to mend things is generally a good research direction. However, this would require a few things which are currently lacking from the paper: (1) clear understanding of the Nash policies for the different reward settings (2) Implementation of SOTA methods for MARL which are appropriate for this setting (3) In depth analysis of learning successes and failures, ideally in settings which have previously been studied in literature (given how task-specific this analysis necessarily is). Regarding 2: General sum games will generally have mixed-strategies as Nash equilibria (just think 'rock-paper-scissors'). With this in mind, using a deterministic policy for the receiver is inappropriate for making any claims about learning in general sum games. Furthermore, it is well known that independent gradient descent (IGD) is not generally going to converge in general sum games (consider the loss functions X * Y and - X *Y or matching pennies). So looking at the outcome of IGD without checking for convergence means the results could be just about anything. Indeed, we don't have to go all the way to writing about emergent communication or complex \"sequential social dilemma\" to study this, those issues can easily be found in (iterated) matrix games. This gets us to the second major point of the paper. To the authors' credit, LOLA [1] has been shown to help with convergence in general sum settings and to lead to the emergence of cooperation and reciprocity in iterated games. However, the key point for the \u2018cooperation\u2019 part is iterated. In a single shot setting (which is explored in this paper), there is simply no way for the agents to reciprocate with each other. So in short, I do not believe the authors' interpretation that agents learn to cooperate with each other because of LOLA, but I do believe that LOLA can help with the learning of mixed strategies (at least for the sender, given that the receiver is deterministic) and with stabilizing convergence. Lastly, the part of the experimental section is dominated by large error bars and graphs that are difficult to interpret. Other points: -\"..but train agents to emerge their own.\" (and many other instances). AFAIK \"to emerge something\" is grammatically wrong (and also sounds really odd). -\"Since the loss is differentiable with respect to the receiver, it is trained directly with gradient descent, so we are training in the style of a stochastic computation graph (Schulman et al., 2015).\". This is a weird statement. You don't need SCGs for training a supervised objective. Also, note that the loss is also differentiable with respect to the action of the 1st agent. It is trivial in this setting to compute the true expected return, if that is what you are after. Note my point above about deterministic policies -\"We perform a hyperparameter search to over both agents\u2019\" -> spurious \"to\" -\"We investigate a similar scenario but concern ourselves with learning agents as opposed to fully-rational agents that have full knowledge of the structure of the game, and we do not assume that agents use an existing language, but train agents to emerge their own\" .This would be interesting, if the game was complex. - L_1 vs L - these symbols are used inconsistently, with the subscript _1 sometimes being applied and sometimes not. -\"we can look to extant results\" - s/extant/extent? -\"We use the L2 metric only on hyperparameter search and keep L1 as our game\u2019s loss to maintain a constant-sum game for the fully competitive case.\" - A few points: (a) the game is not in general constant sum (b) By doing this hyperparameter search the evaluation is strongly biased towards 'fair' attributions. This seems highly problematic. -\"We report our results in Figure ??\" -> Broken reference. -\"We do not test b = 180\u25e6 because the game is constant-sum and therefore trivially Ls1 + Lr1 = 180\u25e6.\" -> So? It would still be interesting to see what learning agents do in this setting. [1]: \"Learning with Opponent Learning Awareness\", Foerster et al. [update: I have updated the score based on the discussion with the authors]. While the paper lacks execution and conceptual clarity, I believe the game itself is interesting and could serve as a starting point for more thorough investigation. ", "rating": "3: Weak Reject", "reply_text": "Thank you for the in-depth comments , corrections , and suggestions . We \u2019 ve tried to address all your concerns point by point below and would be happy to discuss further and more in-depth . Papers Claiming Selfish Communication Doesn \u2019 t Work - After reviewing all the papers we agree that ( Foerster et al 2016 ) and ( Lazaridou et al 2018 ) are bad citations for previous literature making this claim , many thanks for bringing this to our attention . - We still believe that the view of emergent communication being possible only in cooperative settings is prevalent in the literature and believe this is an important misunderstanding to address . - For Cao et al ( 2018 ) , a main claim is that selfish agents can not learn to effectively emerge communication whereas agents that share a reward function do . - \u201c Selfish agents do not appear to ground cheap talk \u201d - They conjecture in section 3.2 that this is because the game is not iterated but we show this is not necessary ( more on this lower down in our comment ) - Instead , the game is likely too competitive and it is not necessary to share a reward function in order to communicate - Jaques et al ( 2019 ) reiterate the claim of Cao et al ( 2018 ) and claim that their learning rule allows for communication between competitive agents whereas regular methods do not , without explicitly quantifying the cooperative/competitive nature of their games . - \u201c The IC metrics demonstrate that baseline agents show almost no signs of coordinating behavior with communication , i.e.speakers saying A and listeners doing B consistently . This result is aligned with both theoretical results in cheap-talk literature ( Crawford & Sobel , 1982 ) , and recent empirical results in MARL ( e.g.Foerster et al . ( 2016 ) ; Lazaridou et al . ( 2018 ) ; Cao et al . ( 2018 ) ) . \u201d - We also found that Lanctot et al ( 2017 ) imply that emergent communication is a purely cooperative task ( in the sense that they take communication to be a paradigm of cooperation ) : - \u201c In MARL , several agents interact and learn in an environment simultaneously , either competitively such as in Go [ 92 ] and Poker [ 39,106,73 ] , cooperatively such as when learning to communicate [ 23 , 94 , 36 ] , or some mix of the two [ 59 , 96 , 35 ] . \u201d `` Reward Function Is Cooperative '' - We would like to make a small distinction between the reward function being cooperative and the game encouraging cooperation . - In previous work in emergent communication , there have been papers that have simply given the same reward to both agents or given a part of one agent \u2019 s reward to the other explicitly ( Lerer and Peysakovich 2018 ) which is a cooperative reward function . - The reward function for our agents is purely their own \u2014 selfish . They do not , a priori , have cooperative intentions . It is only through discovering the nature of the current game \u2019 s setup that they should realize cooperation is advantageous . - Cao et al ( 2018 ) study a cooperative \u201c prosocial \u201d reward function in a game that is quite competitive whereas we study purely selfish reward functions but in a game whose competitive nature can be tuned by the bias . - Regardless of the game \u201c prosocial \u201d agents are going to be cooperative - We take the view of Shoham et al ( 2003 ) that if agents are not being controlled by a central designer then the interesting scenario is when \u201c learning takes place by self-interested agents \u201d , as opposed to prosocially-interested agents Misunderstanding - We could not figure out what is the exact misunderstanding you are pointing to . Could you rephrase it perhaps ? `` Uninteresting '' - Though the result may seem uninteresting from the perspective of static analysis where Crawford and Sobel \u2019 s result is clear , we perform a dynamic analysis ( this is explained in more detail in our Crawford and Sobel discussion ) . - We believe it is , at minimum , interesting for the emergent communication community that seems to hold an opposing belief . - Since there are two well-cited publications from top conferences ( ICLR , ICML ) that clearly and unambiguously state that selfish agents do not learn to communicate and one implying emergent communication is purely cooperative , we believe that there is indeed a misconception of selfish emergent communication in the field that deserves to be clarified - We believe that our toy task is sufficient to show that selfish emergent communication should be feasible to achieve with modern deep RL methods , overturning that belief -- continued below --"}, {"review_id": "B1liIlBKvS-1", "review_text": "Summary: This paper introduces a new sender-receiver game to study emergent communication in partially-competitive scenarios. The authors find that communication can also emerge in partially-competitive scenarios and demonstrate how to encourage communication: 1) selfish communication is proportional to cooperation, and it naturally occurs for situations that are more cooperative than competitive, 2) stability and performance are improved by using LOLA, and 3) discrete protocols are better than continuous ones. Strengths: - This is an interesting paper that is well written and motivated. - They have justified a new sender-receiver game that can be tuned for various levels of competition which then allows them to analyze the effects of various levels of cooperation and competition. - They perform sufficient experimental analysis to show that LOLA outperforms standard methods like REINFORCE in these settings and that discrete communication lends to cooperative communication. - Evaluation is good in the sense that they repeat their experiments multiple times across different random seeds. Weaknesses: - Given that cheap talk is an extremely well-studied topic in economics, I feel that the authors should have devoted more time to explain the difference in setting between their work in classic pieces like those of Sobel and Crawford. The authors should properly define what they mean by learning agents versus fully rational agents, and the key differences between the two. Furthermore, nowhere do the authors in the cheap talk paper assert that agents use an existing language: the equilibrium itself assigns meaning to each sender\u2019s message; this is not part of the problem definition per se. In fact, the work of Sobel and Crawford does not even constrain the size of the vocabulary (as was done in this paper): one of its key contributions is to show that in strictly non-cooperative settings, all equilibria must be partition equilibria, with only a finite number of messages used. - Section 4.2: \u201cinitial random mapping of targets to messages.\u201d The authors made the assumption that this mapping has to be deterministic. Absent a proof or a citation, I find this difficult to accept. This is especially so since mixed strategies are a crucial component of games of imperfect information. - The introduction of the circular game is suspect. There already exist numerous games involving cheap talk, one of them from the Sobel and Crawford paper. Why is there a need for this new benchmark? - The description of the game is given as an algorithm in the appendix. This comes across as counterintuitive: why are the gradient steps being included as part of the game description? A game\u2019s specification and the algorithm which is being used to solve it are two different things. - It is difficult for me to assess the significance of these results since the authors have not presented real-world scenarios and experiments that demonstrate the importance of selfish communication. For cooperative communication we see it a lot in examples like grounded language learning, visual dialog, multi-agent communication etc. But I am concerned that the new setting proposed in this paper seems like a 'toy setting' to investigate if emergent communication would happen. - Are the communicated symbols (discrete or continuous) semantically meaningful? It was shown in Kottur et al. (2017) that for emergent communication to occur and generalize to unseen test instances, it was crucial that the communication protocol was grounded i.e. one symbol learning to represent the color, one representing the shape, one representing the size. What is the final communication protocol learned in this case, and it is useful/interpretable in a similar sense? - Typo: 'Figure ??' in line 3 of section 5.1", "rating": "1: Reject", "reply_text": "Thank you for your comments and corrections ! We \u2019 ve taken quite some time to mull everything over and address your concerns point by point below . We would be happy to discuss further and more in depth . Crawford and Sobel - We put a discussion about the differences between our paper and Crawford and Sobel ( 1986 ) in a post above . - On your point about our phrase \u201c an existing language \u201d , we agree and are also happy to revise the wording . Our point was more subtle that they look at specific equilibria where there exists a fixed mutual understanding as opposed to looking at learning dynamics where the language is emerged and in flux . Would it be more appropriate to write they study \u201c fixed languages at equilibria \u201d ? Section 4.2 Deterministic Mappings - In section 4.2 , we do not make an assumption of deterministic mappings and indeed , during training , the sender is stochastic , choosing a symbol based on categorical distribution over a vocabulary ( as is standard in emergent communication ) . - Our main point was that given a random initialization of a non-learning sender ( learning rate close to 0 ) and a learning receiver with a regular learning rate , it is highly likely that the learning agent would dominate . - This does not necessitate a deterministic sender since a stochastic sender \u2019 s mappings can be mostly learned ( and therefore dominated ) in nearly all cases . - The only case where a non-learning sender can not be dominated by a learning receiver would be a sender with a Nash policy ( e.g.all states are mapped to the same symbol and communication is uninformative ) . But initializing a sender to a Nash policy is very unlikely given the random initialization methods of neural networks . - So in the vast majority of cases , the learning agent would indeed do significantly better than its non-learning opponent . - We can revise the example to make it more clear that the situation is highly likely but not guaranteed . Justifying The Circular Game - We would like to clarify that this game is not a benchmark but closer to a diagnostic tool . - We wanted to run experiments on the full range of 2 player cooperative/competitive games and empirically show that selfish emergent communication can be feasibly achieved - We also wanted to demonstrate that it is indeed the bias of the game that influences the level of communication achieved and can explain why previous literature was mistaken . - We believe the game is really an extension of Crawford and Sobel , made to be smoothly tuneable from fully cooperative to fully competitive . - To our knowledge , there does not exist such a game in game theory literature ( Crawford and Sobel \u2019 s original game is not as easily made fully-competitive ) . - To our knowledge , no existing games in emergent communication literature have fine-grained control of the level of cooperation/competition in the game . Algorithmic Game Description - We added the extra line about updates to make it more clear when the episode ends and agents can update their weights . - This is indeed not explicitly part of the game and we can remove it if it seems superfluous to understanding how the game is played by our agents . Realism of Selfish Communication - We would like to stress that emergent communication is a \u201c how-possibly \u201d explanation ( Resnick , 1991 ) of language emergence . - In this way , we think that reward-sharing and full cooperation is not as realistic of a model as two selfish agents that emergent communication for selfish reasons given an environment that requires cooperation . - Current literature in emergent communication has usually assumed reward-sharing and therefore is less realistic than our setting . - On the topic of \u201c toy setting \u201d , this is indeed exactly a toy setting to see if communication emerges . - Since literature in the field of emergent communication has implied communication does not emerge , we created this toy task to see if it could and whether Crawford and Sobel \u2019 s equilibria could be feasibly achieved in the modern setting of emergent communication . - We do not make any presumptions about all games , nor do we think our game should be a benchmark . What we have shown is that research in emergent communication under competition should make use of strong baselines with selfish agents and take into account the quantifiable cooperation/competitive nature of the games studied , which it sometimes does not . -- - continued below -- -"}, {"review_id": "B1liIlBKvS-2", "review_text": "This ICLR submission deals with a problem of whether selfish agents can learn to use an emergent communication channel, using a sender-receiver game as a case study. It is found that communication can emerge in partially-competitive scenarios, and conditions in which this can happen are investigated. This review is delivered with the caveat that I am not an expert in this particulat field. The investigation seems relevant and the paper is well written and structured, being within the scope of the conference. Proofs in the appendix are sound to the best of my understanding. The literature review is up to date and seems overall relevant. This study should be understood as a proof of concept, given that the setting seems rather restrictive, so I am unsure that he results could be generalized. They seem anyhow promising and partially challenge the current understanding of the problem. Minor issues: All acronyms in the text should be defined the first time they appear in the text. LaTex problem with Fig. reference at the beginning of section 5.1.", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and comments , we appreciate your view and we hope that the paper was readable and enjoyable to someone who isn \u2019 t an expert in emergent communication . We will fix the figure reference and would like to know which acronyms you found unfamiliar that we can define them when they are first used . You make an important point about the generalizability of our results , our setup is indeed a simplified game that spans only the range of 2-player cooperative/competitive games and our learning is simplified by a simple state/action space . The reasoning for this is to be able to carefully control the learning dynamics : - tunable bias that perfectly reflects level of competition - no communication through a non-linguistic action space - clear way to differentiate between manipulation and communication - quantified optimal cooperative performance ( maximizing cooperative reward ) More complex 2-player environments should only differ in the complexity of learning the state , action , and game dynamics . But the feasibility of learning to communicate should still be relevant and our hope is to encourage strong baselines on selfish emergent communication . We also want researchers who don \u2019 t successfully achieve communication to investigate the underlying reasons , from competitiveness to issues with learning dynamics . For 3+ players , we expect game dynamics to be different and indeed it could be trivially reformulated to be zero-sum ( Balduzzi et al , 2019 ) so that concept is not meaningful . There are competitive situations not covered by 2 players , e.g.even if 3 agents are fully competitive with each other , two of them could be incentivized to cooperate with one another in order to defeat the third . Still , we think our fundamental claim is generalizable from the simple 2-player cooperative/competitive nature to the general idea that communication should emerge naturally if cooperation is always preferred to non-cooperation ( which we quantify ) . Our second two points should also be valid . LOLA should still be an essential tool to model opponents and allow communication even in cases where cooperation can be exploited . And discrete communication may still be preferable to continuous . Still , we would look forward to expanding on the research question of selfish communication to 3+ agents and more complex scenarios ( e.g.ad-hoc communication using meta-learning ) in future work"}], "0": {"review_id": "B1liIlBKvS-0", "review_text": "This paper looks at the question of emergent communication amongst self-interested learning agents. The paper finds that \"selfish\" (ie. self-interested) agents can learn to communicate using a cheap talk channel as long as the objective is partially cooperative. The paper makes states that this is is a novel finding that contradicts the previous understanding of emergent communication in the literature (side point: at least some of the papers referenced for this claim did not at all make the claim). I believe there is a major miss-understanding here: As noted in the paper, self-interested agents can learn to communicate in settings in which the reward function is cooperative. Furthermore, it is also known that in 2 player zero-sum there is no incentive to learn a communication protocol. This clearly shows that talking about whether or not \"selfish\" agents can learn to communicate only ever makes sense within the context of a specific game / reward structure. With this in mind, the main finding, agents learn to somewhat communicate with each other in a simple toy setting, with more communication happening when the payouts are more cooperative, is not very interesting. This doesn't mean that there isn't a good paper to be written here, in principle. Finding simple settings in which SOTA multi-agent learning \"fails\", ie. doesn't find Nash policies, understanding why it fails and then finding ways to mend things is generally a good research direction. However, this would require a few things which are currently lacking from the paper: (1) clear understanding of the Nash policies for the different reward settings (2) Implementation of SOTA methods for MARL which are appropriate for this setting (3) In depth analysis of learning successes and failures, ideally in settings which have previously been studied in literature (given how task-specific this analysis necessarily is). Regarding 2: General sum games will generally have mixed-strategies as Nash equilibria (just think 'rock-paper-scissors'). With this in mind, using a deterministic policy for the receiver is inappropriate for making any claims about learning in general sum games. Furthermore, it is well known that independent gradient descent (IGD) is not generally going to converge in general sum games (consider the loss functions X * Y and - X *Y or matching pennies). So looking at the outcome of IGD without checking for convergence means the results could be just about anything. Indeed, we don't have to go all the way to writing about emergent communication or complex \"sequential social dilemma\" to study this, those issues can easily be found in (iterated) matrix games. This gets us to the second major point of the paper. To the authors' credit, LOLA [1] has been shown to help with convergence in general sum settings and to lead to the emergence of cooperation and reciprocity in iterated games. However, the key point for the \u2018cooperation\u2019 part is iterated. In a single shot setting (which is explored in this paper), there is simply no way for the agents to reciprocate with each other. So in short, I do not believe the authors' interpretation that agents learn to cooperate with each other because of LOLA, but I do believe that LOLA can help with the learning of mixed strategies (at least for the sender, given that the receiver is deterministic) and with stabilizing convergence. Lastly, the part of the experimental section is dominated by large error bars and graphs that are difficult to interpret. Other points: -\"..but train agents to emerge their own.\" (and many other instances). AFAIK \"to emerge something\" is grammatically wrong (and also sounds really odd). -\"Since the loss is differentiable with respect to the receiver, it is trained directly with gradient descent, so we are training in the style of a stochastic computation graph (Schulman et al., 2015).\". This is a weird statement. You don't need SCGs for training a supervised objective. Also, note that the loss is also differentiable with respect to the action of the 1st agent. It is trivial in this setting to compute the true expected return, if that is what you are after. Note my point above about deterministic policies -\"We perform a hyperparameter search to over both agents\u2019\" -> spurious \"to\" -\"We investigate a similar scenario but concern ourselves with learning agents as opposed to fully-rational agents that have full knowledge of the structure of the game, and we do not assume that agents use an existing language, but train agents to emerge their own\" .This would be interesting, if the game was complex. - L_1 vs L - these symbols are used inconsistently, with the subscript _1 sometimes being applied and sometimes not. -\"we can look to extant results\" - s/extant/extent? -\"We use the L2 metric only on hyperparameter search and keep L1 as our game\u2019s loss to maintain a constant-sum game for the fully competitive case.\" - A few points: (a) the game is not in general constant sum (b) By doing this hyperparameter search the evaluation is strongly biased towards 'fair' attributions. This seems highly problematic. -\"We report our results in Figure ??\" -> Broken reference. -\"We do not test b = 180\u25e6 because the game is constant-sum and therefore trivially Ls1 + Lr1 = 180\u25e6.\" -> So? It would still be interesting to see what learning agents do in this setting. [1]: \"Learning with Opponent Learning Awareness\", Foerster et al. [update: I have updated the score based on the discussion with the authors]. While the paper lacks execution and conceptual clarity, I believe the game itself is interesting and could serve as a starting point for more thorough investigation. ", "rating": "3: Weak Reject", "reply_text": "Thank you for the in-depth comments , corrections , and suggestions . We \u2019 ve tried to address all your concerns point by point below and would be happy to discuss further and more in-depth . Papers Claiming Selfish Communication Doesn \u2019 t Work - After reviewing all the papers we agree that ( Foerster et al 2016 ) and ( Lazaridou et al 2018 ) are bad citations for previous literature making this claim , many thanks for bringing this to our attention . - We still believe that the view of emergent communication being possible only in cooperative settings is prevalent in the literature and believe this is an important misunderstanding to address . - For Cao et al ( 2018 ) , a main claim is that selfish agents can not learn to effectively emerge communication whereas agents that share a reward function do . - \u201c Selfish agents do not appear to ground cheap talk \u201d - They conjecture in section 3.2 that this is because the game is not iterated but we show this is not necessary ( more on this lower down in our comment ) - Instead , the game is likely too competitive and it is not necessary to share a reward function in order to communicate - Jaques et al ( 2019 ) reiterate the claim of Cao et al ( 2018 ) and claim that their learning rule allows for communication between competitive agents whereas regular methods do not , without explicitly quantifying the cooperative/competitive nature of their games . - \u201c The IC metrics demonstrate that baseline agents show almost no signs of coordinating behavior with communication , i.e.speakers saying A and listeners doing B consistently . This result is aligned with both theoretical results in cheap-talk literature ( Crawford & Sobel , 1982 ) , and recent empirical results in MARL ( e.g.Foerster et al . ( 2016 ) ; Lazaridou et al . ( 2018 ) ; Cao et al . ( 2018 ) ) . \u201d - We also found that Lanctot et al ( 2017 ) imply that emergent communication is a purely cooperative task ( in the sense that they take communication to be a paradigm of cooperation ) : - \u201c In MARL , several agents interact and learn in an environment simultaneously , either competitively such as in Go [ 92 ] and Poker [ 39,106,73 ] , cooperatively such as when learning to communicate [ 23 , 94 , 36 ] , or some mix of the two [ 59 , 96 , 35 ] . \u201d `` Reward Function Is Cooperative '' - We would like to make a small distinction between the reward function being cooperative and the game encouraging cooperation . - In previous work in emergent communication , there have been papers that have simply given the same reward to both agents or given a part of one agent \u2019 s reward to the other explicitly ( Lerer and Peysakovich 2018 ) which is a cooperative reward function . - The reward function for our agents is purely their own \u2014 selfish . They do not , a priori , have cooperative intentions . It is only through discovering the nature of the current game \u2019 s setup that they should realize cooperation is advantageous . - Cao et al ( 2018 ) study a cooperative \u201c prosocial \u201d reward function in a game that is quite competitive whereas we study purely selfish reward functions but in a game whose competitive nature can be tuned by the bias . - Regardless of the game \u201c prosocial \u201d agents are going to be cooperative - We take the view of Shoham et al ( 2003 ) that if agents are not being controlled by a central designer then the interesting scenario is when \u201c learning takes place by self-interested agents \u201d , as opposed to prosocially-interested agents Misunderstanding - We could not figure out what is the exact misunderstanding you are pointing to . Could you rephrase it perhaps ? `` Uninteresting '' - Though the result may seem uninteresting from the perspective of static analysis where Crawford and Sobel \u2019 s result is clear , we perform a dynamic analysis ( this is explained in more detail in our Crawford and Sobel discussion ) . - We believe it is , at minimum , interesting for the emergent communication community that seems to hold an opposing belief . - Since there are two well-cited publications from top conferences ( ICLR , ICML ) that clearly and unambiguously state that selfish agents do not learn to communicate and one implying emergent communication is purely cooperative , we believe that there is indeed a misconception of selfish emergent communication in the field that deserves to be clarified - We believe that our toy task is sufficient to show that selfish emergent communication should be feasible to achieve with modern deep RL methods , overturning that belief -- continued below --"}, "1": {"review_id": "B1liIlBKvS-1", "review_text": "Summary: This paper introduces a new sender-receiver game to study emergent communication in partially-competitive scenarios. The authors find that communication can also emerge in partially-competitive scenarios and demonstrate how to encourage communication: 1) selfish communication is proportional to cooperation, and it naturally occurs for situations that are more cooperative than competitive, 2) stability and performance are improved by using LOLA, and 3) discrete protocols are better than continuous ones. Strengths: - This is an interesting paper that is well written and motivated. - They have justified a new sender-receiver game that can be tuned for various levels of competition which then allows them to analyze the effects of various levels of cooperation and competition. - They perform sufficient experimental analysis to show that LOLA outperforms standard methods like REINFORCE in these settings and that discrete communication lends to cooperative communication. - Evaluation is good in the sense that they repeat their experiments multiple times across different random seeds. Weaknesses: - Given that cheap talk is an extremely well-studied topic in economics, I feel that the authors should have devoted more time to explain the difference in setting between their work in classic pieces like those of Sobel and Crawford. The authors should properly define what they mean by learning agents versus fully rational agents, and the key differences between the two. Furthermore, nowhere do the authors in the cheap talk paper assert that agents use an existing language: the equilibrium itself assigns meaning to each sender\u2019s message; this is not part of the problem definition per se. In fact, the work of Sobel and Crawford does not even constrain the size of the vocabulary (as was done in this paper): one of its key contributions is to show that in strictly non-cooperative settings, all equilibria must be partition equilibria, with only a finite number of messages used. - Section 4.2: \u201cinitial random mapping of targets to messages.\u201d The authors made the assumption that this mapping has to be deterministic. Absent a proof or a citation, I find this difficult to accept. This is especially so since mixed strategies are a crucial component of games of imperfect information. - The introduction of the circular game is suspect. There already exist numerous games involving cheap talk, one of them from the Sobel and Crawford paper. Why is there a need for this new benchmark? - The description of the game is given as an algorithm in the appendix. This comes across as counterintuitive: why are the gradient steps being included as part of the game description? A game\u2019s specification and the algorithm which is being used to solve it are two different things. - It is difficult for me to assess the significance of these results since the authors have not presented real-world scenarios and experiments that demonstrate the importance of selfish communication. For cooperative communication we see it a lot in examples like grounded language learning, visual dialog, multi-agent communication etc. But I am concerned that the new setting proposed in this paper seems like a 'toy setting' to investigate if emergent communication would happen. - Are the communicated symbols (discrete or continuous) semantically meaningful? It was shown in Kottur et al. (2017) that for emergent communication to occur and generalize to unseen test instances, it was crucial that the communication protocol was grounded i.e. one symbol learning to represent the color, one representing the shape, one representing the size. What is the final communication protocol learned in this case, and it is useful/interpretable in a similar sense? - Typo: 'Figure ??' in line 3 of section 5.1", "rating": "1: Reject", "reply_text": "Thank you for your comments and corrections ! We \u2019 ve taken quite some time to mull everything over and address your concerns point by point below . We would be happy to discuss further and more in depth . Crawford and Sobel - We put a discussion about the differences between our paper and Crawford and Sobel ( 1986 ) in a post above . - On your point about our phrase \u201c an existing language \u201d , we agree and are also happy to revise the wording . Our point was more subtle that they look at specific equilibria where there exists a fixed mutual understanding as opposed to looking at learning dynamics where the language is emerged and in flux . Would it be more appropriate to write they study \u201c fixed languages at equilibria \u201d ? Section 4.2 Deterministic Mappings - In section 4.2 , we do not make an assumption of deterministic mappings and indeed , during training , the sender is stochastic , choosing a symbol based on categorical distribution over a vocabulary ( as is standard in emergent communication ) . - Our main point was that given a random initialization of a non-learning sender ( learning rate close to 0 ) and a learning receiver with a regular learning rate , it is highly likely that the learning agent would dominate . - This does not necessitate a deterministic sender since a stochastic sender \u2019 s mappings can be mostly learned ( and therefore dominated ) in nearly all cases . - The only case where a non-learning sender can not be dominated by a learning receiver would be a sender with a Nash policy ( e.g.all states are mapped to the same symbol and communication is uninformative ) . But initializing a sender to a Nash policy is very unlikely given the random initialization methods of neural networks . - So in the vast majority of cases , the learning agent would indeed do significantly better than its non-learning opponent . - We can revise the example to make it more clear that the situation is highly likely but not guaranteed . Justifying The Circular Game - We would like to clarify that this game is not a benchmark but closer to a diagnostic tool . - We wanted to run experiments on the full range of 2 player cooperative/competitive games and empirically show that selfish emergent communication can be feasibly achieved - We also wanted to demonstrate that it is indeed the bias of the game that influences the level of communication achieved and can explain why previous literature was mistaken . - We believe the game is really an extension of Crawford and Sobel , made to be smoothly tuneable from fully cooperative to fully competitive . - To our knowledge , there does not exist such a game in game theory literature ( Crawford and Sobel \u2019 s original game is not as easily made fully-competitive ) . - To our knowledge , no existing games in emergent communication literature have fine-grained control of the level of cooperation/competition in the game . Algorithmic Game Description - We added the extra line about updates to make it more clear when the episode ends and agents can update their weights . - This is indeed not explicitly part of the game and we can remove it if it seems superfluous to understanding how the game is played by our agents . Realism of Selfish Communication - We would like to stress that emergent communication is a \u201c how-possibly \u201d explanation ( Resnick , 1991 ) of language emergence . - In this way , we think that reward-sharing and full cooperation is not as realistic of a model as two selfish agents that emergent communication for selfish reasons given an environment that requires cooperation . - Current literature in emergent communication has usually assumed reward-sharing and therefore is less realistic than our setting . - On the topic of \u201c toy setting \u201d , this is indeed exactly a toy setting to see if communication emerges . - Since literature in the field of emergent communication has implied communication does not emerge , we created this toy task to see if it could and whether Crawford and Sobel \u2019 s equilibria could be feasibly achieved in the modern setting of emergent communication . - We do not make any presumptions about all games , nor do we think our game should be a benchmark . What we have shown is that research in emergent communication under competition should make use of strong baselines with selfish agents and take into account the quantifiable cooperation/competitive nature of the games studied , which it sometimes does not . -- - continued below -- -"}, "2": {"review_id": "B1liIlBKvS-2", "review_text": "This ICLR submission deals with a problem of whether selfish agents can learn to use an emergent communication channel, using a sender-receiver game as a case study. It is found that communication can emerge in partially-competitive scenarios, and conditions in which this can happen are investigated. This review is delivered with the caveat that I am not an expert in this particulat field. The investigation seems relevant and the paper is well written and structured, being within the scope of the conference. Proofs in the appendix are sound to the best of my understanding. The literature review is up to date and seems overall relevant. This study should be understood as a proof of concept, given that the setting seems rather restrictive, so I am unsure that he results could be generalized. They seem anyhow promising and partially challenge the current understanding of the problem. Minor issues: All acronyms in the text should be defined the first time they appear in the text. LaTex problem with Fig. reference at the beginning of section 5.1.", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and comments , we appreciate your view and we hope that the paper was readable and enjoyable to someone who isn \u2019 t an expert in emergent communication . We will fix the figure reference and would like to know which acronyms you found unfamiliar that we can define them when they are first used . You make an important point about the generalizability of our results , our setup is indeed a simplified game that spans only the range of 2-player cooperative/competitive games and our learning is simplified by a simple state/action space . The reasoning for this is to be able to carefully control the learning dynamics : - tunable bias that perfectly reflects level of competition - no communication through a non-linguistic action space - clear way to differentiate between manipulation and communication - quantified optimal cooperative performance ( maximizing cooperative reward ) More complex 2-player environments should only differ in the complexity of learning the state , action , and game dynamics . But the feasibility of learning to communicate should still be relevant and our hope is to encourage strong baselines on selfish emergent communication . We also want researchers who don \u2019 t successfully achieve communication to investigate the underlying reasons , from competitiveness to issues with learning dynamics . For 3+ players , we expect game dynamics to be different and indeed it could be trivially reformulated to be zero-sum ( Balduzzi et al , 2019 ) so that concept is not meaningful . There are competitive situations not covered by 2 players , e.g.even if 3 agents are fully competitive with each other , two of them could be incentivized to cooperate with one another in order to defeat the third . Still , we think our fundamental claim is generalizable from the simple 2-player cooperative/competitive nature to the general idea that communication should emerge naturally if cooperation is always preferred to non-cooperation ( which we quantify ) . Our second two points should also be valid . LOLA should still be an essential tool to model opponents and allow communication even in cases where cooperation can be exploited . And discrete communication may still be preferable to continuous . Still , we would look forward to expanding on the research question of selfish communication to 3+ agents and more complex scenarios ( e.g.ad-hoc communication using meta-learning ) in future work"}}