{"year": "2017", "forum": "rJfMusFll", "title": "Batch Policy Gradient  Methods for  Improving Neural Conversation Models", "decision": "Accept (Poster)", "meta_review": "This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task, using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy. \n The paper is clear, well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way), this is still an interesting and informative read, and will probably be of interest to many people at ICLR.", "reviews": [{"review_id": "rJfMusFll-0", "review_text": "This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning \u2013 training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores. While the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. My main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available. References: Wen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. \"A Network-based End-to-End Trainable Task-oriented Dialogue System.\" arXiv preprint arXiv:1604.04562 (2016). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . We indeed agree that the size of the dataset is relatively small , which is why we use a small LSTM ( see page 8 ) . Also , while the responses are reasonable , they are not all amazing ( the maximum score is 0.9 out of 2 ) . We believe that the reason why a small model still works here is because this is a very narrow domain , tackling very few issues , and with a relatively lower variation ( as opposed to the Ubuntu Dialogue Corpus , for example ) . Indeed , seeing whether a similar approach works for larger datasets with larger model is an interesting question for future work . We will add the related references you mentioned , and add a small discussion regarding the size of the dataset . We will update the manuscript by the end of the month ."}, {"review_id": "rJfMusFll-1", "review_text": "The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots. The approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on \"clarification regarding batch vs. online setting\"). The artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review . We agree with you partially on the role of the VF estimate but still think it plays a critical role when you are in the backward view . See comment below ."}, {"review_id": "rJfMusFll-2", "review_text": "The paper discuss a \"batch\" method for RL setup to improve chat-bots. The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. I find the writing clear, and the algorithm a natural extension of the online version. Below are some constructive remarks: - Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option: - For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function. - section 2.2: sentence before last: s' is not defined. last sentence: missing \"... in the stochastic case.\" at the end. - Section 4.1 last paragraph: \"While Bot-1 is not significant ...\" => \"While Bot-1 is not significantly different from ML ...\" ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for the review . We updated only the top and softmax layers in all experiments - including the constant estimator - to keep the comparison fair ( paragarah RNN Design on page 7 ) . But yes , investigating when each estimator will work well is important . We suspect that when there is more data the GTD ( or a more sophisticated ) estimator would work better , but we would rather not make any strong statements or draw stronger conclusions from the current experimental setup . Thank you for the other suggestions and comments . We will update the manuscript by the end of the month ."}], "0": {"review_id": "rJfMusFll-0", "review_text": "This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning \u2013 training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores. While the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. My main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available. References: Wen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. \"A Network-based End-to-End Trainable Task-oriented Dialogue System.\" arXiv preprint arXiv:1604.04562 (2016). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . We indeed agree that the size of the dataset is relatively small , which is why we use a small LSTM ( see page 8 ) . Also , while the responses are reasonable , they are not all amazing ( the maximum score is 0.9 out of 2 ) . We believe that the reason why a small model still works here is because this is a very narrow domain , tackling very few issues , and with a relatively lower variation ( as opposed to the Ubuntu Dialogue Corpus , for example ) . Indeed , seeing whether a similar approach works for larger datasets with larger model is an interesting question for future work . We will add the related references you mentioned , and add a small discussion regarding the size of the dataset . We will update the manuscript by the end of the month ."}, "1": {"review_id": "rJfMusFll-1", "review_text": "The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots. The approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on \"clarification regarding batch vs. online setting\"). The artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review . We agree with you partially on the role of the VF estimate but still think it plays a critical role when you are in the backward view . See comment below ."}, "2": {"review_id": "rJfMusFll-2", "review_text": "The paper discuss a \"batch\" method for RL setup to improve chat-bots. The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. I find the writing clear, and the algorithm a natural extension of the online version. Below are some constructive remarks: - Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option: - For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function. - section 2.2: sentence before last: s' is not defined. last sentence: missing \"... in the stochastic case.\" at the end. - Section 4.1 last paragraph: \"While Bot-1 is not significant ...\" => \"While Bot-1 is not significantly different from ML ...\" ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for the review . We updated only the top and softmax layers in all experiments - including the constant estimator - to keep the comparison fair ( paragarah RNN Design on page 7 ) . But yes , investigating when each estimator will work well is important . We suspect that when there is more data the GTD ( or a more sophisticated ) estimator would work better , but we would rather not make any strong statements or draw stronger conclusions from the current experimental setup . Thank you for the other suggestions and comments . We will update the manuscript by the end of the month ."}}