{"year": "2019", "forum": "B1xhQhRcK7", "title": "Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures", "decision": "Accept (Poster)", "meta_review": "\n* Strengths\n\nThe paper addresses a timely topic, and reviewers generally agreed that the approach is reasonable and the experiments are convincing. Reviewers raised a number of specific concerns (which could be addressed in a revised version or future work), described below.\n\n* Weaknesses\n\nSome reviewers were concerned the baselines are weak. Several reviewers were concerned that relying on failures observed during training could create issues by narrowing the proposal distribution (Reviewer 3 characterizes this in a particularly precise manner). In addition, there was a general feeling that more steps are needed before the method can be used in practice (but this could be said of most research).\n\n* Recommendation\n\nAll reviewers agreed that the paper should be accepted, although there was also consensus that the paper would benefit from stronger baselines and more close attention to issues that could be caused by an overly narrow proposal distribution. The authors should consider addressing or commenting on these issues in the final version.", "reviews": [{"review_id": "B1xhQhRcK7-0", "review_text": "PAPER SUMMARY ------------- The paper proposes a method for evaluating the failure probability of a learned agent, which is important in safety critical domains. Using plain Monte Carlo for this evaluation can be too expensive, since discovering a failure probability of epsilon requires on the order of 1/epsilon samples. Therefore the authors propose an adversarial approach, which focuses on scenarios which are difficult for the agent, while still yielding unbiased estimates of failure probabilities. The key idea of the proposed approach is to learn a failure probability predictor (FPP). This function attempts to predict at which initial states the system will fail. This function is then used in an importance sampling scheme to sample the regions with higher failure probability more often, which leads to higher statistical efficiency. Finding the FPP is itself a problem which is just as hard as the original problem of estimating the overall failure probability. However, the FPP can be trained using data from different agents, not just the final agent to be evaluated (for instance the data from agent training, containing typically many failure cases). The approach hinges on the assumption that these agents tend to fail in the same states as the final agent, but with higher probability. The paper shows that the proposed method finds failure cases orders of magnitude faster than standard MC in simulated driving as well as a simulated humanoid task. Since the proposed approach uses data acquired during the training of the agent, it has more information at its disposal than standard MC. However, the paper shows that the proposed method is also orders of magnitudes more efficient than a naive approach using the failure cases during training. REVIEW SUMMARY -------------- I believe that this paper addresses an important problem in a novel manner (as far as I can tell) and the experiments are quite convincing. The main negative point is that I believe that the proposed method has some flaws which may actually decrease statistical efficiency in some cases (please see details below). DETAILED COMMENTS ----------------- - It seems to me that a weak point of the method is that it may also severly reduce the efficiency compared to a standard MC method. If the function f underestimates the probability of failure at certain x, it would take a very long time to correct itself because these points would hardly ever be evaluated. It seems that the paper heuristically addresses this to some extent using the exponent alpha of the function. However, I think there should be a more in-depth discussion of this issue. An upper-confidence-bound type of algorithm may be a principled way of addressing this problem. - The proposed method relies on the ability to initialize the system in any desired state. However, on a physical system, where finding failure cases is particularly important, this is usually not possible. It would be interesting if the paper would discuss how the proposed approach would be used on such real systems. - On page 6, in the first paragraph, the state is called s instead of x as before. Furthermore, the arguments of f are switched.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the specific feedback and helpful comments . We wanted to quickly clarify the correctness of Proposition 3.2 , since it seemed to be a major point in your review . > `` It seems to me that Proposition 3.2 is wrong . In the proof it is written E [ U^2 ] = E [ W^2 c ( X , Z ) ] , which is wrong since U^2 = W^2 c^2 ( X , Z ) . This means that the proposal distribution Q_f * is not in fact the optimal proposal distribution . This is problematic because the entire approach is justified using this argument . '' We believe the proof is correct , but this point is indeed subtle , and we \u2019 ll clarify it in the paper . In our case c ( X , Z ) is a Bernoulli random variable . So c^2 ( X , Z ) = c ( X , Z ) , as c ( \u00b7 , \u00b7 ) is either 0 or 1 and in both cases the square is the identity . This means E [ U^2 ] = E [ W^2 c^2 ( X , Z ) ] = E [ W^2 c ( X , Z ) ] . In the case where c represents an arbitrary distribution , the optimal proposal distribution is more difficult to compute and is a worthwhile question for future work . We also note that the standard analysis of the optimal proposal distribution under importance sampling does not account for unobserved stochasticity , which we model in Z . This is why the optimal proposal distribution we derive ( for Bernoulli random variables ) differs from the standard case . Please let us know if this addresses your concern ."}, {"review_id": "B1xhQhRcK7-1", "review_text": "Summary: Proposes an importance sampling approach to sampling failure cases for RL algorithms. The proposal distribution is based on a function learned via a neural network on failures that occur during agent training. The method is compared to random sampling on two problems where the \"true\" failure probability can be approximated through random sampling. The IS method requires substantially fewer samples to produce failure cases and to estimate the failure probability. Review: The overall approach is technically sound, and the experiments demonstrate a significant savings in sampling compared to naive random sampling. The specific novelty of the approach seems to be fitting the proposal distribution to failures observed during training. I think the method accomplishes what it sets out to do. However, as the paper notes, creating robust agents will require a combination of methodologies, of which this testing approach is only a part. I wonder if learning the proposal distribution based on failures observed during training presents a risk of narrowing the range of possible failures being considered. Of course identifying any failure is valuable, but by biasing the search toward failures that are similar to failures observed in training, might we be decreasing the likelihood of discovering failures that are substantially different from those seen during training? One could imagine that if the agent has not explored some regions of the state space, we would actually like to sample test examples from the unexplored states, which becomes less likely if we preferentially sample in states that were encountered in training. The paper is well-written with good coverage of related literature. I would suggest incorporating some of the descriptions of the models and methods in Appendix D into the main paper. Comments / Questions: * Sec 4.2: How are the confidence bounds for the results calculated? * What are the \"true\" failure probabilities in the experiments? * Sec 4.3: There is a reference to non-existant \"Appendix X\" Pros: * Overall approach is sound and achieves its objectives Cons: * Small amount of novelty; primarily an application of established techniques", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review and suggestions . We first address what we understand to be the main concerns in your review : We believe there are two sources of novelty . ( 1 ) A long-term goal is robust RL agents . Testing agents when rewards are highly sparse is on the critical path to this goal . To our knowledge , this problem has gone unaddressed . Thus , one novelty is considering a practical and important class of rare event estimation problems . ( 2 ) Our setting is fairly different from classical settings . By exploiting its structure , we provide an effective approach , whereas prior approaches simply would not work . > Small amount of novelty ; primarily an application of established techniques > The specific novelty of the approach seems to be fitting the proposal distribution to failures observed during training . We believe there are several novel ideas in our approach which are missing in this summary . These novelties aren \u2019 t just small changes - we don \u2019 t see how existing approaches could handle our setting ( failure search and risk estimation , with binary failure signals ) without them . Admittedly , we emphasized importance over novelty in writing the paper , and will edit for clarity . The main novelty in the continuation approach is to learn the proposal distribution from a family of related , but weaker , agents . Our method goes beyond simply fitting a function to data . Fitting a proposal distribution to failures observed for the final agent would not work well . For example , in Humanoid , the final agent fails once every 110k episodes , and was trained for 300k episodes . If we run existing methods like the cross-entropy method on the final agent , we would need significantly more than 300k episodes of data to get a good proposal distribution . Another novel aspect is our extension of the standard importance sampling setup to include stochasticity . While this seems very fundamental , we are not aware of this in prior work . To reflect the practicalities of RL tasks , we separate controllable randomness ( observed initial conditions ) from unobservable , uncontrollable randomness ( environment and agent randomness , or unobserved initial conditions ) . We show this changes the form of the minimum-variance proposal distribution ( Proposition 3.2 ) . Additionally , in our setup , the initial state distribution is arbitrary and unknown . > I wonder if learning the proposal distribution based on failures observed during training presents a risk of narrowing the range of possible failures being considered . This is a good observation . In our humanoid experiments , we safeguard against this using a differentiable neural dictionary ( Appendix D.1 , moved to E.1 in the latest revision ) . This encourages higher failure probabilities for initial conditions far from those seen during training . Also see our response to R3 regarding statistical efficiency ."}, {"review_id": "B1xhQhRcK7-2", "review_text": "This paper proposed an adversarial approach to identifying catastrophic failure cases in reinforcement learning. It is a timely topic and may have practical significance. The proposed approach is built on importance sampling for the failure search and function fitting for estimating the failure probabilities. Experiments on two simulated environments show significant gain of the proposed approaches over naive search. The reviewer is not familiar with this domain, but the baseline, naive search, seems like straightforward and very weak. Are there any other methods for the same problem in the literature? The authors may consider to contrast to them in the experiments. What is the certainty equivalence approach? A reference would be helpful and improve the presentation quality of the paper. What is exactly the $\\theta_t$ in Section 3.3? What is the dimension of this vector in the experiments? What quantities should be encoded in this vector in practice? I am still concerned about the fact that the FPP depends on the generalization of the binary classification neural network, although the authors tried to give intuitive examples and discussions. Nonetheless, I understand the difficulty. Could the authors give some conditions under which the approach would fail? Any alternative approaches to the binary neural network? What is a good principle to design the network architecture? Overall, this paper addresses a practically significant problem and has proposed reasonable approaches. While I still have concerns about the practical performance of the proposed methods, this work along the right track in my opinion. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "> Overall , this paper addresses a practically significant problem and has proposed reasonable approaches . While I still have concerns about the practical performance of the proposed methods , this work along the right track in my opinion . Thank you for the positive comments , and helpful feedback . Could you please explain what concerns you have about the practical performance of the proposed methods ? How can we address these ? We believe our approach is a large improvement over baselines , both in theory , and as supported by our experiments . > The reviewer is not familiar with this domain , but the baseline , naive search , seems like straightforward and very weak . Are there any other methods for the same problem in the literature ? We assume you are talking about failure search , and not failure rate estimation ? In our original paper , we did compare our method with an additional baseline : a prioritized replay baseline . This does significantly better than naive search , but significantly worse than our proposed method . We seem to be the first to tackle this problem . The setting is sufficiently different from classical settings , so classical baselines would not work , as we explain in our response to R2 . We \u2019 d be happy to compare to additional baselines though - are there are any other baselines you would suggest we include ? > I am still concerned about the fact that the FPP depends on the generalization of the binary classification neural network , although the authors tried to give intuitive examples and discussions . Nonetheless , I understand the difficulty . Could the authors give some conditions under which the approach would fail ? Any alternative approaches to the binary neural network ? What is a good principle to design the network architecture ? The main point we hope to convey is that approaches beyond VMC are crucial , and using an optimized adversary is a good idea in safety-critical settings . We can guarantee that we never do worse than VMC by over a small constant factor ( see the discussion on statistical efficiency in our response to R3 for details ) . However , as you point out , details can influence how much improvement we observe in practice . These details can be application specific , and is not the focus of our paper , but we expand on some of these details below . Our approach would not help if the neural network severely underestimates the failure probability of a large fraction of failure cases . This could occur for initial states that are very different from all the initial states we have seen during training . We could mitigate this issue : ( 1 ) In the humanoid domain , we use a differentiable neural dictionary . The DND outputs higher failure probabilities for points very far from those seen during training . ( 2 ) Since we train on weaker agents , we tend to overestimate the failure probabilities . In general , a guiding principle is to output higher failure probabilities for examples we are uncertain about . We included architectural details in Appendix D.1 , but will move the key ideas to the main paper in the next update . Does this address your concerns ? We are happy to provide more details if that helps ."}], "0": {"review_id": "B1xhQhRcK7-0", "review_text": "PAPER SUMMARY ------------- The paper proposes a method for evaluating the failure probability of a learned agent, which is important in safety critical domains. Using plain Monte Carlo for this evaluation can be too expensive, since discovering a failure probability of epsilon requires on the order of 1/epsilon samples. Therefore the authors propose an adversarial approach, which focuses on scenarios which are difficult for the agent, while still yielding unbiased estimates of failure probabilities. The key idea of the proposed approach is to learn a failure probability predictor (FPP). This function attempts to predict at which initial states the system will fail. This function is then used in an importance sampling scheme to sample the regions with higher failure probability more often, which leads to higher statistical efficiency. Finding the FPP is itself a problem which is just as hard as the original problem of estimating the overall failure probability. However, the FPP can be trained using data from different agents, not just the final agent to be evaluated (for instance the data from agent training, containing typically many failure cases). The approach hinges on the assumption that these agents tend to fail in the same states as the final agent, but with higher probability. The paper shows that the proposed method finds failure cases orders of magnitude faster than standard MC in simulated driving as well as a simulated humanoid task. Since the proposed approach uses data acquired during the training of the agent, it has more information at its disposal than standard MC. However, the paper shows that the proposed method is also orders of magnitudes more efficient than a naive approach using the failure cases during training. REVIEW SUMMARY -------------- I believe that this paper addresses an important problem in a novel manner (as far as I can tell) and the experiments are quite convincing. The main negative point is that I believe that the proposed method has some flaws which may actually decrease statistical efficiency in some cases (please see details below). DETAILED COMMENTS ----------------- - It seems to me that a weak point of the method is that it may also severly reduce the efficiency compared to a standard MC method. If the function f underestimates the probability of failure at certain x, it would take a very long time to correct itself because these points would hardly ever be evaluated. It seems that the paper heuristically addresses this to some extent using the exponent alpha of the function. However, I think there should be a more in-depth discussion of this issue. An upper-confidence-bound type of algorithm may be a principled way of addressing this problem. - The proposed method relies on the ability to initialize the system in any desired state. However, on a physical system, where finding failure cases is particularly important, this is usually not possible. It would be interesting if the paper would discuss how the proposed approach would be used on such real systems. - On page 6, in the first paragraph, the state is called s instead of x as before. Furthermore, the arguments of f are switched.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the specific feedback and helpful comments . We wanted to quickly clarify the correctness of Proposition 3.2 , since it seemed to be a major point in your review . > `` It seems to me that Proposition 3.2 is wrong . In the proof it is written E [ U^2 ] = E [ W^2 c ( X , Z ) ] , which is wrong since U^2 = W^2 c^2 ( X , Z ) . This means that the proposal distribution Q_f * is not in fact the optimal proposal distribution . This is problematic because the entire approach is justified using this argument . '' We believe the proof is correct , but this point is indeed subtle , and we \u2019 ll clarify it in the paper . In our case c ( X , Z ) is a Bernoulli random variable . So c^2 ( X , Z ) = c ( X , Z ) , as c ( \u00b7 , \u00b7 ) is either 0 or 1 and in both cases the square is the identity . This means E [ U^2 ] = E [ W^2 c^2 ( X , Z ) ] = E [ W^2 c ( X , Z ) ] . In the case where c represents an arbitrary distribution , the optimal proposal distribution is more difficult to compute and is a worthwhile question for future work . We also note that the standard analysis of the optimal proposal distribution under importance sampling does not account for unobserved stochasticity , which we model in Z . This is why the optimal proposal distribution we derive ( for Bernoulli random variables ) differs from the standard case . Please let us know if this addresses your concern ."}, "1": {"review_id": "B1xhQhRcK7-1", "review_text": "Summary: Proposes an importance sampling approach to sampling failure cases for RL algorithms. The proposal distribution is based on a function learned via a neural network on failures that occur during agent training. The method is compared to random sampling on two problems where the \"true\" failure probability can be approximated through random sampling. The IS method requires substantially fewer samples to produce failure cases and to estimate the failure probability. Review: The overall approach is technically sound, and the experiments demonstrate a significant savings in sampling compared to naive random sampling. The specific novelty of the approach seems to be fitting the proposal distribution to failures observed during training. I think the method accomplishes what it sets out to do. However, as the paper notes, creating robust agents will require a combination of methodologies, of which this testing approach is only a part. I wonder if learning the proposal distribution based on failures observed during training presents a risk of narrowing the range of possible failures being considered. Of course identifying any failure is valuable, but by biasing the search toward failures that are similar to failures observed in training, might we be decreasing the likelihood of discovering failures that are substantially different from those seen during training? One could imagine that if the agent has not explored some regions of the state space, we would actually like to sample test examples from the unexplored states, which becomes less likely if we preferentially sample in states that were encountered in training. The paper is well-written with good coverage of related literature. I would suggest incorporating some of the descriptions of the models and methods in Appendix D into the main paper. Comments / Questions: * Sec 4.2: How are the confidence bounds for the results calculated? * What are the \"true\" failure probabilities in the experiments? * Sec 4.3: There is a reference to non-existant \"Appendix X\" Pros: * Overall approach is sound and achieves its objectives Cons: * Small amount of novelty; primarily an application of established techniques", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review and suggestions . We first address what we understand to be the main concerns in your review : We believe there are two sources of novelty . ( 1 ) A long-term goal is robust RL agents . Testing agents when rewards are highly sparse is on the critical path to this goal . To our knowledge , this problem has gone unaddressed . Thus , one novelty is considering a practical and important class of rare event estimation problems . ( 2 ) Our setting is fairly different from classical settings . By exploiting its structure , we provide an effective approach , whereas prior approaches simply would not work . > Small amount of novelty ; primarily an application of established techniques > The specific novelty of the approach seems to be fitting the proposal distribution to failures observed during training . We believe there are several novel ideas in our approach which are missing in this summary . These novelties aren \u2019 t just small changes - we don \u2019 t see how existing approaches could handle our setting ( failure search and risk estimation , with binary failure signals ) without them . Admittedly , we emphasized importance over novelty in writing the paper , and will edit for clarity . The main novelty in the continuation approach is to learn the proposal distribution from a family of related , but weaker , agents . Our method goes beyond simply fitting a function to data . Fitting a proposal distribution to failures observed for the final agent would not work well . For example , in Humanoid , the final agent fails once every 110k episodes , and was trained for 300k episodes . If we run existing methods like the cross-entropy method on the final agent , we would need significantly more than 300k episodes of data to get a good proposal distribution . Another novel aspect is our extension of the standard importance sampling setup to include stochasticity . While this seems very fundamental , we are not aware of this in prior work . To reflect the practicalities of RL tasks , we separate controllable randomness ( observed initial conditions ) from unobservable , uncontrollable randomness ( environment and agent randomness , or unobserved initial conditions ) . We show this changes the form of the minimum-variance proposal distribution ( Proposition 3.2 ) . Additionally , in our setup , the initial state distribution is arbitrary and unknown . > I wonder if learning the proposal distribution based on failures observed during training presents a risk of narrowing the range of possible failures being considered . This is a good observation . In our humanoid experiments , we safeguard against this using a differentiable neural dictionary ( Appendix D.1 , moved to E.1 in the latest revision ) . This encourages higher failure probabilities for initial conditions far from those seen during training . Also see our response to R3 regarding statistical efficiency ."}, "2": {"review_id": "B1xhQhRcK7-2", "review_text": "This paper proposed an adversarial approach to identifying catastrophic failure cases in reinforcement learning. It is a timely topic and may have practical significance. The proposed approach is built on importance sampling for the failure search and function fitting for estimating the failure probabilities. Experiments on two simulated environments show significant gain of the proposed approaches over naive search. The reviewer is not familiar with this domain, but the baseline, naive search, seems like straightforward and very weak. Are there any other methods for the same problem in the literature? The authors may consider to contrast to them in the experiments. What is the certainty equivalence approach? A reference would be helpful and improve the presentation quality of the paper. What is exactly the $\\theta_t$ in Section 3.3? What is the dimension of this vector in the experiments? What quantities should be encoded in this vector in practice? I am still concerned about the fact that the FPP depends on the generalization of the binary classification neural network, although the authors tried to give intuitive examples and discussions. Nonetheless, I understand the difficulty. Could the authors give some conditions under which the approach would fail? Any alternative approaches to the binary neural network? What is a good principle to design the network architecture? Overall, this paper addresses a practically significant problem and has proposed reasonable approaches. While I still have concerns about the practical performance of the proposed methods, this work along the right track in my opinion. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "> Overall , this paper addresses a practically significant problem and has proposed reasonable approaches . While I still have concerns about the practical performance of the proposed methods , this work along the right track in my opinion . Thank you for the positive comments , and helpful feedback . Could you please explain what concerns you have about the practical performance of the proposed methods ? How can we address these ? We believe our approach is a large improvement over baselines , both in theory , and as supported by our experiments . > The reviewer is not familiar with this domain , but the baseline , naive search , seems like straightforward and very weak . Are there any other methods for the same problem in the literature ? We assume you are talking about failure search , and not failure rate estimation ? In our original paper , we did compare our method with an additional baseline : a prioritized replay baseline . This does significantly better than naive search , but significantly worse than our proposed method . We seem to be the first to tackle this problem . The setting is sufficiently different from classical settings , so classical baselines would not work , as we explain in our response to R2 . We \u2019 d be happy to compare to additional baselines though - are there are any other baselines you would suggest we include ? > I am still concerned about the fact that the FPP depends on the generalization of the binary classification neural network , although the authors tried to give intuitive examples and discussions . Nonetheless , I understand the difficulty . Could the authors give some conditions under which the approach would fail ? Any alternative approaches to the binary neural network ? What is a good principle to design the network architecture ? The main point we hope to convey is that approaches beyond VMC are crucial , and using an optimized adversary is a good idea in safety-critical settings . We can guarantee that we never do worse than VMC by over a small constant factor ( see the discussion on statistical efficiency in our response to R3 for details ) . However , as you point out , details can influence how much improvement we observe in practice . These details can be application specific , and is not the focus of our paper , but we expand on some of these details below . Our approach would not help if the neural network severely underestimates the failure probability of a large fraction of failure cases . This could occur for initial states that are very different from all the initial states we have seen during training . We could mitigate this issue : ( 1 ) In the humanoid domain , we use a differentiable neural dictionary . The DND outputs higher failure probabilities for points very far from those seen during training . ( 2 ) Since we train on weaker agents , we tend to overestimate the failure probabilities . In general , a guiding principle is to output higher failure probabilities for examples we are uncertain about . We included architectural details in Appendix D.1 , but will move the key ideas to the main paper in the next update . Does this address your concerns ? We are happy to provide more details if that helps ."}}