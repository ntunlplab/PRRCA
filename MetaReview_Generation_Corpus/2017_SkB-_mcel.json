{"year": "2017", "forum": "SkB-_mcel", "title": "Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning", "decision": "Accept (Poster)", "meta_review": "The proposed Central Moment Discrepancy criterion is well-described and supported in this paper. Its performance on domain adaptation tasks is good as well. The reviewers had several good comments and suggestions and the authors have taken most of these into account and improved the paper considerably. The paper thus makes a nice contribution to the distribution matching literature and toolbox.", "reviews": [{"review_id": "SkB-_mcel-0", "review_text": "The work introduces a new regularization for learning domain-invariant representations with neural networks. The regularization aims at matching the higher order central moments of the hidden activations of the NNs of the source and target domain. The authors compared the proposed method vs MMD and two state-of-art NN domain adaptation algorithms on the Amazon review and office datasets, and showed comparable performance. The idea proposed is simple and straightforward, and the empirical results suggest that it is quite effective. The biggest limitation I can see with the proposed method is the assumption that the hidden activations are independently distributed. For example, this assumption will clearly be violated for the hidden activations of convolutional layers, where neighboring activations are dependent. I guess this is why the authors start with the output of dense layers for the image dataset. Do the authors have insight on if it is beneficial to start adaptation from lower level? If so, do the authors have insight on how to relax the assumption? In these scenarios, if MMD has an advantage as it does not make this assumption? Figure 3 does not seems to clearly support the boost of performance shown in table 2. The only class where the new regularization brings the source and target domain closer seem to be the mouse class pointed by the authors. Is the performance improvement only coming from this single class? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Independent marginal distributions are assumed to guarantee that the CMD regularizer ( empirical estimate ) with infinite many terms has the property of matching the joint distributions . However , even without this assumption , a zero regularizer value implies equal marginal distributions and convergence in CMD implies convergence in distribution of the marginals . That is , we don \u2019 t need the independence assumption for the application of our method but to guarantee some metrical and convergence properties w.r.t.the joint distributions . From your review we have learned that it is not clear why the assumption is needed . In the new version of our paper , we added a detailed description of the properties that are obtained with the independence assumption and the properties that are obtained without the independence assumption . Recent findings reveal that deep features must eventually transition from general to specific along the network , and the transferability of features drops significantly in higher layers with increasing domain discrepancy ( Long et al. , 2015 ; Yosinski et al. , 2014 ) . Therefore , we try to force the network to train transferable features in the higher layers by adaptation . We compare the accuracy of our method with various state-of-the-art models on Office and most of them ( AdaBN , CORAL , Deep CORAL , DANN , DAN , DDC ) start adaptation from higher layers ( section 5.2 ) . Tzeng et al. , 2014 report higher accuracies based on MMD regularization when starting from the highest layer compared to lower ones . However , our method is also applicable to lower layers without any modification . We were just not able to produce competitive results with lower layer adaptation . Although not clearly visible in figure 3 of the last revision , 58 % of the individual classes are improved in this example . However , we removed the figure as it is hardly interpretable and it does not contain any additional information not contained elsewhere ."}, {"review_id": "SkB-_mcel-1", "review_text": "This paper proposed a new metric central moment discrepancy (CMD) for matching two distributions, with applications to domain adaptation. Compared to a more well-known variant, MMD, CMD has the benefit of not over penalizing the mean, and therefore can focus more on the shape of distribution around the center. In terms of discriminative power (the ability to tell two distributions apart), MMD and CMD should be equivalent, but in practice I can understand that CMD may be better as MMD tries to match the raw moments which may over penalize data that are not zero centered. In the paper CMD is used only up to Kth order, and not all the central moments are used, but rather only the diagonal entries are considered in the CMD objective, I think this is mostly motivated for computation efficiency. A natural comparison with MMD therefore can be made, by also explicitly include raw moments up to Kth order. Another thing to compare against is to include all moments, not just the diagonal terms, in the objective. This is computationally expensive, but can be done for e.g. 1st and 2nd orders. Since the experiments only compare CMD in the above form with kernelized MMD, the claim that explicit moment matching is helpful is not very well supported. To make this a solid claim CMD should be compared against MMD with explicit raw moments. The claim that the kernel parameter in MMD is hard to tune and CMD does not have such parameters only applies to kernel MMD, not explicit MMD. For kernel MMD, there are also studies on how to set these parameters, for example: Sriperumbudur et al. Kernel choice and classifiability for rkhs embeddings of probability distributions. Gretton et al. A kernel two-sample test. and also using multiple kernels (Li et al. 2015) which removes the need to tune them. Tuning the beta directly like done in this paper is usually not the way MMD is tuned. At least simple heuristics like dividing |x-y|^2 by dimensionality or mean pairwise distance first should be applied first before trying beta in the way done in this paper. Overall I think CMD could be better than MMD, and could have applications in many domains. But it also has the problem of not easily kernelizable (you can argue this both ways though). The experiments demonstrating that CMD is better could be done more convincinly. ", "rating": "7: Good paper, accept", "reply_text": "We try to approach your advice w.r.t.the experiments and your proposals for further improvement of our investigations . We would be happy to present additional experimental findings regarding explicit moment matching at the conference . We added the remarks about different MMD versions and kernel parameter choices to the Related Work section . We decided to use the reverse cross-validation procedure of Zhong et al. , 2010 with some modifications for neural networks ( Ganin et al. , 2016 ) . The method jointly utilizes the source inputs , source labels and target inputs . Each parameter of our models , including the domain regularizer weights , is tuned using this transfer learning specific cross-validation method . With this simple method a one-layer neural network with MMD domain-regularization shows higher accuracy than the state-of-the-art model VFAE on the Amazon reviews benchmark data set . There are also other papers that utilize cross-validation for the optimal kernel choice , see Sutherland et al. , 2016 subsection 2.2 for a comprehensive summary . We added your references and notes for studies on the unsupervised tuning of the parameter beta with the new revision to the Related Work section . In addition , we added your suggestions and your proposals for further improvements of the experiments to the Conclusion and Outlook section ."}, {"review_id": "SkB-_mcel-2", "review_text": "Variational auto-encoders, adversarial networks, and kernel scoring rules like MMD have recently gained popularity as methods for learning directed generative models and for other applications like domain adaptation. This paper gives an additional method along the scoring rules direction that uses the matching of central moments to match two probability distributions. The technique is simple, and in the case of domain adaptation, highly effective. CMD seems like a very nice and straightforward solution to the domain adaptation problem. The method is computationally straightforward to implement, and seems quite stable with respect to the tuning parameters when compared to MMD. I was skeptical reading through this, especially given the fact that you only use K=5 in your experiments, but the results seem quite good. The natural question that I have now is: how will this method do in training generative models? This is beyond the scope of this paper, but it\u2019s the lowest hanging fruit. Below I give more detailed feedback. One way to speed up MMD is to use a random Fourier basis as was done in \u201cFastmmd: Ensemble of circular discrepancy for efficient two-sample test\u201d by Zhao and Meng, 2015. There are also linear time estimators, e.g., in \u201cA Kernel Two-Sample Test\u201c by Gretton et al., 2012. I don\u2019t think you need to compare against these approaches since you compare to the full MMD, but they should be cited. The paper \u201cGenerative Models and Model Criticism via Optimized Maximum Mean Discrepancy\u201d by Sutherland et al. submitted to ICLR 2017 as well, discusses techniques for optimizing the kernel used in MMD and is worth citing in section 3. How limiting is the assumption that the distribution has independent marginals? The sample complexity of MMD depends heavily on the dimensionality of the input space - do you have any intuitions about the sample complexity of CMD? It seems like it's relatively insensitive based on the results in Figure 4, but I would be surprised if this were the case with 10,000 hidden units. I mainly ask this because with generative models, the output space can be quite high-dimensional. I\u2019m concerned that the central moments won\u2019t be numerically stable at higher orders when backpropagating. This doesn\u2019t seem to be a problem in the experimental results, but perhaps the authors could comment a bit on this? I\u2019m referring to the fact that ck(X) can be very large for k >= 3. Proposition 1 alleviates my concerns that the overall objective is unstable, I\u2019m referring specifically to the individual terms within. Figure 3 is rather cluttered, and aside from the mouse class it\u2019s not clear to me from the visualization that the CMD regularizer is actually helping. It would be useful to remove some of the classes for the purpose of visualization. I would like some clarification about the natural geometric interpretations of K=5. Do you mean that the moments up to K=5 have been well-studied? Do you have any references for this? Why does K >= 6 not have a natural geometric interpretation? Figure 4 should have a legend", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We added your remarks about fast MMD estimators and kernel parameter choices to the Related Work section . Independent marginal distributions are assumed in order to guarantee that the CMD regularizer ( empirical estimate ) still has the property of matching the joint distributions . However , even without this assumption , a zero regularizer value implies equal marginal distributions and convergence in CMD implies convergence in distribution of the marginals . That is , we don \u2019 t need the independence assumption for the application of our method but for guaranteeing some metrical and convergence properties w.r.t.the joint distributions . In the new version of our paper , we added a detailed description of the properties that are obtained with the independence assumption and the properties that are obtained without the independence assumption . The question about the higher numbers of hidden units ( > 10.000 ) is very interesting and we consider the answer as future work . The absolute value of a central moment ck ( X ) of a distribution is smaller than the absolute central moment ( proof of proposition 1 ) . For this absolute central moment there exists an upper bound ( Egozcue et al. , 2012 ) . For distributions on the interval [ 0,1 ] , this bound is decreasing with increasing moment order . That is , for random variables on [ 0,1 ] , the absolute values of the higher central moments have a small upper bound . In order to guarantee distributions on [ 0,1 ] we introduce the normalization factor 1/|b-a|^k in the CMD metric , where [ a , b ] is the interval of the activation functions . This is equivalent to dividing the activations by their maximum range . Please note that we restrict our method on bounded activation functions . Unbounded activations need to be clipped or normalized to be bounded . We removed figure 3 as it is hardly interpretable and it does not contain any additional information not contained elsewhere . Indeed , also the sixth and the seventh central moment are related to geometrical properties ( hyperskewness , hyperflatness ) . K=5 is heuristically set a priori large enough to capture rich geometrical information ( mean , variance , skewness , kurtosis ) , and small enough to be computationally efficient . However , the experiments in subsection Parameter Sensitivity show that similar results are obtained for K > =6 . Thanks to your review we changed our argumentation with the current revision . The previous figure 4 ( now figure 3 ) has a combined legend for all four experiments ."}], "0": {"review_id": "SkB-_mcel-0", "review_text": "The work introduces a new regularization for learning domain-invariant representations with neural networks. The regularization aims at matching the higher order central moments of the hidden activations of the NNs of the source and target domain. The authors compared the proposed method vs MMD and two state-of-art NN domain adaptation algorithms on the Amazon review and office datasets, and showed comparable performance. The idea proposed is simple and straightforward, and the empirical results suggest that it is quite effective. The biggest limitation I can see with the proposed method is the assumption that the hidden activations are independently distributed. For example, this assumption will clearly be violated for the hidden activations of convolutional layers, where neighboring activations are dependent. I guess this is why the authors start with the output of dense layers for the image dataset. Do the authors have insight on if it is beneficial to start adaptation from lower level? If so, do the authors have insight on how to relax the assumption? In these scenarios, if MMD has an advantage as it does not make this assumption? Figure 3 does not seems to clearly support the boost of performance shown in table 2. The only class where the new regularization brings the source and target domain closer seem to be the mouse class pointed by the authors. Is the performance improvement only coming from this single class? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Independent marginal distributions are assumed to guarantee that the CMD regularizer ( empirical estimate ) with infinite many terms has the property of matching the joint distributions . However , even without this assumption , a zero regularizer value implies equal marginal distributions and convergence in CMD implies convergence in distribution of the marginals . That is , we don \u2019 t need the independence assumption for the application of our method but to guarantee some metrical and convergence properties w.r.t.the joint distributions . From your review we have learned that it is not clear why the assumption is needed . In the new version of our paper , we added a detailed description of the properties that are obtained with the independence assumption and the properties that are obtained without the independence assumption . Recent findings reveal that deep features must eventually transition from general to specific along the network , and the transferability of features drops significantly in higher layers with increasing domain discrepancy ( Long et al. , 2015 ; Yosinski et al. , 2014 ) . Therefore , we try to force the network to train transferable features in the higher layers by adaptation . We compare the accuracy of our method with various state-of-the-art models on Office and most of them ( AdaBN , CORAL , Deep CORAL , DANN , DAN , DDC ) start adaptation from higher layers ( section 5.2 ) . Tzeng et al. , 2014 report higher accuracies based on MMD regularization when starting from the highest layer compared to lower ones . However , our method is also applicable to lower layers without any modification . We were just not able to produce competitive results with lower layer adaptation . Although not clearly visible in figure 3 of the last revision , 58 % of the individual classes are improved in this example . However , we removed the figure as it is hardly interpretable and it does not contain any additional information not contained elsewhere ."}, "1": {"review_id": "SkB-_mcel-1", "review_text": "This paper proposed a new metric central moment discrepancy (CMD) for matching two distributions, with applications to domain adaptation. Compared to a more well-known variant, MMD, CMD has the benefit of not over penalizing the mean, and therefore can focus more on the shape of distribution around the center. In terms of discriminative power (the ability to tell two distributions apart), MMD and CMD should be equivalent, but in practice I can understand that CMD may be better as MMD tries to match the raw moments which may over penalize data that are not zero centered. In the paper CMD is used only up to Kth order, and not all the central moments are used, but rather only the diagonal entries are considered in the CMD objective, I think this is mostly motivated for computation efficiency. A natural comparison with MMD therefore can be made, by also explicitly include raw moments up to Kth order. Another thing to compare against is to include all moments, not just the diagonal terms, in the objective. This is computationally expensive, but can be done for e.g. 1st and 2nd orders. Since the experiments only compare CMD in the above form with kernelized MMD, the claim that explicit moment matching is helpful is not very well supported. To make this a solid claim CMD should be compared against MMD with explicit raw moments. The claim that the kernel parameter in MMD is hard to tune and CMD does not have such parameters only applies to kernel MMD, not explicit MMD. For kernel MMD, there are also studies on how to set these parameters, for example: Sriperumbudur et al. Kernel choice and classifiability for rkhs embeddings of probability distributions. Gretton et al. A kernel two-sample test. and also using multiple kernels (Li et al. 2015) which removes the need to tune them. Tuning the beta directly like done in this paper is usually not the way MMD is tuned. At least simple heuristics like dividing |x-y|^2 by dimensionality or mean pairwise distance first should be applied first before trying beta in the way done in this paper. Overall I think CMD could be better than MMD, and could have applications in many domains. But it also has the problem of not easily kernelizable (you can argue this both ways though). The experiments demonstrating that CMD is better could be done more convincinly. ", "rating": "7: Good paper, accept", "reply_text": "We try to approach your advice w.r.t.the experiments and your proposals for further improvement of our investigations . We would be happy to present additional experimental findings regarding explicit moment matching at the conference . We added the remarks about different MMD versions and kernel parameter choices to the Related Work section . We decided to use the reverse cross-validation procedure of Zhong et al. , 2010 with some modifications for neural networks ( Ganin et al. , 2016 ) . The method jointly utilizes the source inputs , source labels and target inputs . Each parameter of our models , including the domain regularizer weights , is tuned using this transfer learning specific cross-validation method . With this simple method a one-layer neural network with MMD domain-regularization shows higher accuracy than the state-of-the-art model VFAE on the Amazon reviews benchmark data set . There are also other papers that utilize cross-validation for the optimal kernel choice , see Sutherland et al. , 2016 subsection 2.2 for a comprehensive summary . We added your references and notes for studies on the unsupervised tuning of the parameter beta with the new revision to the Related Work section . In addition , we added your suggestions and your proposals for further improvements of the experiments to the Conclusion and Outlook section ."}, "2": {"review_id": "SkB-_mcel-2", "review_text": "Variational auto-encoders, adversarial networks, and kernel scoring rules like MMD have recently gained popularity as methods for learning directed generative models and for other applications like domain adaptation. This paper gives an additional method along the scoring rules direction that uses the matching of central moments to match two probability distributions. The technique is simple, and in the case of domain adaptation, highly effective. CMD seems like a very nice and straightforward solution to the domain adaptation problem. The method is computationally straightforward to implement, and seems quite stable with respect to the tuning parameters when compared to MMD. I was skeptical reading through this, especially given the fact that you only use K=5 in your experiments, but the results seem quite good. The natural question that I have now is: how will this method do in training generative models? This is beyond the scope of this paper, but it\u2019s the lowest hanging fruit. Below I give more detailed feedback. One way to speed up MMD is to use a random Fourier basis as was done in \u201cFastmmd: Ensemble of circular discrepancy for efficient two-sample test\u201d by Zhao and Meng, 2015. There are also linear time estimators, e.g., in \u201cA Kernel Two-Sample Test\u201c by Gretton et al., 2012. I don\u2019t think you need to compare against these approaches since you compare to the full MMD, but they should be cited. The paper \u201cGenerative Models and Model Criticism via Optimized Maximum Mean Discrepancy\u201d by Sutherland et al. submitted to ICLR 2017 as well, discusses techniques for optimizing the kernel used in MMD and is worth citing in section 3. How limiting is the assumption that the distribution has independent marginals? The sample complexity of MMD depends heavily on the dimensionality of the input space - do you have any intuitions about the sample complexity of CMD? It seems like it's relatively insensitive based on the results in Figure 4, but I would be surprised if this were the case with 10,000 hidden units. I mainly ask this because with generative models, the output space can be quite high-dimensional. I\u2019m concerned that the central moments won\u2019t be numerically stable at higher orders when backpropagating. This doesn\u2019t seem to be a problem in the experimental results, but perhaps the authors could comment a bit on this? I\u2019m referring to the fact that ck(X) can be very large for k >= 3. Proposition 1 alleviates my concerns that the overall objective is unstable, I\u2019m referring specifically to the individual terms within. Figure 3 is rather cluttered, and aside from the mouse class it\u2019s not clear to me from the visualization that the CMD regularizer is actually helping. It would be useful to remove some of the classes for the purpose of visualization. I would like some clarification about the natural geometric interpretations of K=5. Do you mean that the moments up to K=5 have been well-studied? Do you have any references for this? Why does K >= 6 not have a natural geometric interpretation? Figure 4 should have a legend", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We added your remarks about fast MMD estimators and kernel parameter choices to the Related Work section . Independent marginal distributions are assumed in order to guarantee that the CMD regularizer ( empirical estimate ) still has the property of matching the joint distributions . However , even without this assumption , a zero regularizer value implies equal marginal distributions and convergence in CMD implies convergence in distribution of the marginals . That is , we don \u2019 t need the independence assumption for the application of our method but for guaranteeing some metrical and convergence properties w.r.t.the joint distributions . In the new version of our paper , we added a detailed description of the properties that are obtained with the independence assumption and the properties that are obtained without the independence assumption . The question about the higher numbers of hidden units ( > 10.000 ) is very interesting and we consider the answer as future work . The absolute value of a central moment ck ( X ) of a distribution is smaller than the absolute central moment ( proof of proposition 1 ) . For this absolute central moment there exists an upper bound ( Egozcue et al. , 2012 ) . For distributions on the interval [ 0,1 ] , this bound is decreasing with increasing moment order . That is , for random variables on [ 0,1 ] , the absolute values of the higher central moments have a small upper bound . In order to guarantee distributions on [ 0,1 ] we introduce the normalization factor 1/|b-a|^k in the CMD metric , where [ a , b ] is the interval of the activation functions . This is equivalent to dividing the activations by their maximum range . Please note that we restrict our method on bounded activation functions . Unbounded activations need to be clipped or normalized to be bounded . We removed figure 3 as it is hardly interpretable and it does not contain any additional information not contained elsewhere . Indeed , also the sixth and the seventh central moment are related to geometrical properties ( hyperskewness , hyperflatness ) . K=5 is heuristically set a priori large enough to capture rich geometrical information ( mean , variance , skewness , kurtosis ) , and small enough to be computationally efficient . However , the experiments in subsection Parameter Sensitivity show that similar results are obtained for K > =6 . Thanks to your review we changed our argumentation with the current revision . The previous figure 4 ( now figure 3 ) has a combined legend for all four experiments ."}}