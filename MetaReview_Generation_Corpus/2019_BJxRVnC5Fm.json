{"year": "2019", "forum": "BJxRVnC5Fm", "title": "Mean Replacement Pruning  ", "decision": "Reject", "meta_review": "This paper proposes an approach to pruning units in a deep neural network while training is in progress. The idea is to (1) use a specific \"scoring function\" (the absolute-valued Taylor expansion of the loss) to identify the best units to prune, (2) computing the mean activations of the units to be pruned on a small sample of training data, (3) adding the mean activations multiplied by the outgoing weights into the biases of the next layer's units, and (4) removing the pruned units from the network. Extensive experiments show that this approach to pruning does less immediate damage than the more common zero-replacement approach, that this advantage remains (but is much smaller) after fine-tuning, and that the importance of units tends not to change much during training. The reviewers liked the quality of the writing and the extensive experimentation, but even after discussion and revision had concerns about the limited novelty of the approach, the fact that the proposed approach is incompatible with batch normalization (which severely limits the range of architectures to which the method may be applied), and were concerned that the proposed method has limited impact after fine-tuning.", "reviews": [{"review_id": "BJxRVnC5Fm-0", "review_text": "This paper proposes a simple improvement to methods for unit pruning. After identifying a unit to remove (selected by the experimenter\u2019s pruning heuristic of choice), the activation of that unit is approximately incorporated into the subsequent unit by \u201cmean replacement\u201d. The mean unit activation (computed on a small subset of the training set) is multiplied by each outgoing weight (or convolutional filter) and added to each corresponding bias instead. Experiments show this method is generally better than the typical method of zero-replacement before fine-tuning, though the advantage is smaller after several epochs of fine-tuning. While I find this paper intriguing and applaud the extensive experimentation and documentation, I have some concerns as well: 1. There are unanswered questions about how this method relates to existing work. It is not clear from the paper how the \u201cmean replacement\u201d method differs from the two most related works (Ye, 2018) and (Morcos, 2018), which propose variations on replacing units with constant values or mean activations, respectively. Also, why does the method in this paper seem to yield good results, while the related method (Morcos, 2018) yields \u201cinferior performance\u201d? 2. The results are stated to only apply to networks \u201cwithout batch normalization\u201d. The reason seems intuitive: any change that can be merely rolled into the bias will be lost after normalization (depending perhaps on the ordering of normalization and the non-linearities). This leaves an annually decreasing fraction of networks to which this method is applicable, given the widespread use of batch norm. 3. Critically, it\u2019s difficult to compare this work against other pruning works given the lack of results reported in terms of final test error and the lack of the ubiquitous \u201cerror vs. %-pruned\u201d plot. Overall, this paper is lacking some clarity, may be limited in originality, may be helpful for some common networks and composable with other pruning methods (significance), but has a good quality evaluation (subject to the clarity issues). I\u2019m rating this paper below the threshold given the limitations, but I\u2019m willing to consider an upgrade to the score if these questions are addressed. Other notes: 4. What is your definition of a convolutional \u201cpruning unit\u201d? (From context, I\u2019d presume it corresponds to an output activation map.) 5. In Section 3.1: replace \u201cin practice, people \u2026\u201d with something like \u201cin practice, it is common to\u201d. 6. In Equation 3, is the absolute value of the pruning penalty used in the evaluation? 7. In the footnote in Section 3.2, how many training samples are needed for a good approximation? How many are used in the experiments? 8. There are a couple typos in Section 3.2: \u201creplacing -the- these units with zeroes\u201d and \u201ceach of these output*s*\u201d. 9. Presumably the \u201c\\Delta Loss after pruning\u201d in Figures 2-6 is validation or test loss, not training loss? Is this the cross-entropy loss? Also, it would be much easier to compare to other papers if test accuracy were reported instead or in addition. 10. In Figure 4, the cost to recover using fine-tuning seems to be only roughly 2% of the original training time. How much time is lost to the process of computing the average unit activation? UPDATE: I've raised the score slightly to 5 after the rebuttals and revisions.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your thorough comments and valuable suggestions . We hope the responses/clarifications below would help . ( 1 ) Ye et al . ( 2018 ) , proposes a pruning method that penalizes the variance of the output distribution of a unit . When units have low variance ( generating almost constant values ) , they remove the unit by propagating the mean values to the next layer . In our method we show that such a trick can be utilized in other pruning scenarios that don \u2019 t involve batch normalization or variance regularization . In particular , we show that mean replacement is the optimal update minimizing the reconstruction error on the next layer and empirically show that it does indeed reduce the pruning penalty in a variety of settings . Morcos et al . ( 2018 ) briefly mentions the possibility of using mean replacement in the ablation setting . In the single result shared in Section A1 , they prune/ablate the last layer of their convolutional network , since that would be the only layer without batch normalization . ( 2 ) We agree that there is a strong trend with using batch normalization in neural network training . However there is no guarantee that this trend will last or/and become a standard . There are also cases where batch normalization is not practical/preferred , like small batch training ( due to memory limitations of big networks ) and RNN training . We would also like to point out that our method is applicable in networks with instance or layer normalization . ( 3 ) We updated the table of scoring functions in Section 3.4 with related references . We hope that it will guide the reader to understand differences between our work and previous work . Following the common feedback we got from the reviewers , we performed some additional experiments . In our experiments on Cifar-10 , we did n't find any significant difference among different pruning methods despite the reported differences in others works . We share this surprising initial results in Section 6.6 along with a discussion about possible explanations . ( 4 ) We use the same notation as Morcos et al.2018 in our work . Given a convolutional kernel W with dimensions ( input channels , H , W , output channels ) , a unit i corresponds to the parameters W [ : , : , : ,i ] . In the case of a dense layer with parameters W of size ( input channels , output channels ) it corresponds to W [ : ,i ] . ( 5 ) Updated . ( 6 ) We used the average change in the loss to evaluate the performance . We expect the two measures ( absolute and average change in the loss ) to be very similar , especially when the pruning fraction is high . ( 7 ) We forgot to share these numbers , thanks for the note . It is 1000 and 10000 for cifar-10 and imagenet . We updated Section 3.4 to include these numbers . ( 8 ) Thanks for the feedback , we updated the part and had a second pass on the whole paper . ( 9 ) In Figures 2-6 we report pruning penalties ( cross entropy loss ) using a set of 1000 ( Cifar-10 ) and 10000 ( Imagenet2012 ) samples from the training set . Our motivation is that reducing the pruning penalty ( training loss ) would help the optimization by both/either reducing number of fine tuning steps needed and/or improving final performance . Our results support the first and undermine the second . ( 10 ) In practice we would suggest using a running average of the mean outputs , which would require constant number of FLOPS per sample ( linear in number of units in the network ) . Since our initial set of experiments do n't have end-2-end pruning experiments we have n't implemented such a feature and measure its effectiveness ."}, {"review_id": "BJxRVnC5Fm-1", "review_text": "This paper presents a mean-replacement pruning strategy and utilizes the absolute-valued Taylor expansion as the scoring function for the pruning. Some computer vision problems are used as test beds to empirically show the effect of the employment of bias-propagation and different scoring functions. The empirical results validates the effectiveness of bias-propagation and absolute-valued Taylor expansion scoring functions. The work is generally well-written and the results are promising, and the theoretical explanation in 3.3 is intriguing. However, I think the following issues need some further clarifications: 1. What's the exact difference and connection between the mean-replacement pruning technique, and the bias-propagation technique in Ye et al., (2018) and the mean activation technique in Morcos et al. (2018)? The authors only mention that mean replacement pruning extends the idea in Ye et al. (2018) to the non-constrained training setting, but it is very unclear what \"constraints\" are talked about. Some more detailed and formal comparisons should be added, together with potential empirical comparisons. 2. In the abstract, the authors claim that they \"adapt an existing score function ...\", but from the main text it seems that absolute-valued Taylor expansion score is exactly the same one in Molchanov et al. (2016). Is this a typo (or misleading claim) in the abstract? 3. There are no comparisons of the approach proposed in this paper with some existing state-of-the-art, apart from some simple comparisons between whether bias-propagation is adopted and some inner comparisons among different scoring functions. It would also be much better if some charts/tables with certain metrics for improvement apart from pruning penalties (e.g., compression rates, or inference speed, etc.) instead of simply illustrative figures are shown. ### some smaller suggestions/typos ### 1. The plot legends/labels are kind of inconsistent with the description before the figures. For example, in the main text the authors mainly use \"pruning penalty\", while in the figures the y-axes are typically labelled as \"\\Delta-loss after pruning\", and the plot tag at page 5 bottom is different from those used in the plots, which introduces some unnecessary confusion. 2. It is very unclear how the authors arrive at the conclusion \"This results suggests ... the training process\" from the \"winning ticket\" hypothesis. 3. Several typos that can be easily spell-checked (e.g., \"the effect or pruning\" -> \"the effect of pruning\", etc.). I hope the authors can address these issues. Thanks!", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and valuable comments . We would like to provide clarifications/updates about the questions raised . ( 1 ) Our work extends the setting in Ye et al , ( 2018 ) ( networks with batch normalization and units with very small variance ) to networks without batch-normalization , un-regularized training and shows that replacing a unit with its mean value is a better at reducing the immediate damage compared to mere removal . Morcos et al. , ( 2018 ) compare mere removal with mean replacement in the context of ablation studies using a network with batch normalization . Since they use layers with batch normalization they are able to mean replace the final layer of the network only ( Batch normalization should remove any constant signal coming from the previous layer and therefore we should n't see any difference between the two methods in their setting , except the output layer of the network . ) . Our experiments involve a much greater variety of settings and show that Mean Replacement indeed works better in general . ( 2 ) Our scoring function and Molchanov et al . ( 2016 ) are first order approximations of two different values . We updated the table of scoring functions in Section 3.4 with related references . We hope that it will guide the reader to understand differences between our work and previous work . ( 3 ) We intentionally avoided running end to end pruning experiments in our work . Our assumption was that reducing the change in the training loss should help any pruning strategy that employs any of the saliency scores used . However , we agree that this connection is not clear and needs further investigation . Therefore we ran additional experiments where we perform iterative pruning with fix training budget . In our experiments , surprisingly , we did n't find any significant difference among ` non-random ` methods despite the reported differences in others works . Even though the results do n't support our case , we like to share these initial results in the appendix ( Section 6.6 ) with a discussion on possible reasons . ( 3 ) And finally regarding minor suggestions , ( 1 ) we updated with 'Pruning Penalty ` following your suggestion ( 2 ) Detecting lottery ticket early in the training is important , since one could reduce the size of the network and the training time . This result , if general enough , promises a whole new direction for pruning research . We updated our introduction indicating this point . ( 3 ) spell-checked ."}, {"review_id": "BJxRVnC5Fm-2", "review_text": "1. Pruning neurons in pre-trained CNNs is a very important issue in deep learning, and there are a number of related works have been investigated in Section 2. However, it is very strange that, I did not see any comparison experiments to these related works in this paper. 2. The presentation of the experiment part is also wired, to report compression rates, speed-up rates, and accuracy might have a more explicit demonstration. 3. \"This is often done by replacing the these units with zeroes\". However, in previous works, we can directly establish a compact network with fewer neurons after pruning some unimportant neurons. Thus, some considerations and motivations in Section 3.2 seem wrong. 4. It seems that the neural network after using the proposed method has the same architecture as that of the original network, but some of it neurons are represented as mean replacement. Therefore, the compression and speed-up rates of the proposed method would be hard to implement in practice. 5. The paper should be further proofread. There are considerable grammar mistakes and unclear sentences.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . ( 1 ) We updated the table of scoring functions in Section 3.4 with related references . We hope that it will guide the reader to understand differences between our work and previous work . We also updated the last paragraph of Section 2 to highlight the difference between our work and Ye et al , ( 2018 ) , Morcos et al. , ( 2018 ) . ( 2 ) Even though pruning is widely used for model compression , recent work highlights a promising direction where pruning is used during training to reduce training time and improve final performance ( Han et al . ( 2016 ) , Frankle & Carbin ( 2018 ) ) . Therefore we focused on pruning experiments sampled from the entire length of a training job . Our motivation is that reducing the pruning penalty helps optimization by reducing the number of fine tuning steps needed for reaching the same level . However , following the common feedback we got from the reviewers , we performed some additional experiments . In our experimental setting , we did n't find any significant difference among different pruning methods . We share this surprising results in Section 6.6 along with a discussion about possible explanations . ( 3 ) Replacing a unit with zeros indeed corresponds to removing it along with all the outgoing weights ( it represents the same function ) . In our experiments we use masking to emulate this behaviour , however as you suggest , one can establish a smaller network in practice immediately if needed . ( 4 ) Our method replaces units with their mean output values . In practice , constant values are propagated to the next layer and units are removed as normal ( see Figure 1 ) . Mean output values can be aggregated during the training in an online fashion and bias propagation is a very cheap operation ( single matrix multiplication ) . Therefore , it is a practical method . ( 5 ) We are sorry about the mistakes slipped and we did further proofread the paper . Han , S. , Pool , J. , Narang , S. , Mao , H. , Gong , E. , Tang , S. , \u2026 Dally , W. J . ( 2016 ) .DSD : Dense-Sparse-Dense Training for Deep Neural Networks . Retrieved from http : //arxiv.org/abs/1607.04381"}], "0": {"review_id": "BJxRVnC5Fm-0", "review_text": "This paper proposes a simple improvement to methods for unit pruning. After identifying a unit to remove (selected by the experimenter\u2019s pruning heuristic of choice), the activation of that unit is approximately incorporated into the subsequent unit by \u201cmean replacement\u201d. The mean unit activation (computed on a small subset of the training set) is multiplied by each outgoing weight (or convolutional filter) and added to each corresponding bias instead. Experiments show this method is generally better than the typical method of zero-replacement before fine-tuning, though the advantage is smaller after several epochs of fine-tuning. While I find this paper intriguing and applaud the extensive experimentation and documentation, I have some concerns as well: 1. There are unanswered questions about how this method relates to existing work. It is not clear from the paper how the \u201cmean replacement\u201d method differs from the two most related works (Ye, 2018) and (Morcos, 2018), which propose variations on replacing units with constant values or mean activations, respectively. Also, why does the method in this paper seem to yield good results, while the related method (Morcos, 2018) yields \u201cinferior performance\u201d? 2. The results are stated to only apply to networks \u201cwithout batch normalization\u201d. The reason seems intuitive: any change that can be merely rolled into the bias will be lost after normalization (depending perhaps on the ordering of normalization and the non-linearities). This leaves an annually decreasing fraction of networks to which this method is applicable, given the widespread use of batch norm. 3. Critically, it\u2019s difficult to compare this work against other pruning works given the lack of results reported in terms of final test error and the lack of the ubiquitous \u201cerror vs. %-pruned\u201d plot. Overall, this paper is lacking some clarity, may be limited in originality, may be helpful for some common networks and composable with other pruning methods (significance), but has a good quality evaluation (subject to the clarity issues). I\u2019m rating this paper below the threshold given the limitations, but I\u2019m willing to consider an upgrade to the score if these questions are addressed. Other notes: 4. What is your definition of a convolutional \u201cpruning unit\u201d? (From context, I\u2019d presume it corresponds to an output activation map.) 5. In Section 3.1: replace \u201cin practice, people \u2026\u201d with something like \u201cin practice, it is common to\u201d. 6. In Equation 3, is the absolute value of the pruning penalty used in the evaluation? 7. In the footnote in Section 3.2, how many training samples are needed for a good approximation? How many are used in the experiments? 8. There are a couple typos in Section 3.2: \u201creplacing -the- these units with zeroes\u201d and \u201ceach of these output*s*\u201d. 9. Presumably the \u201c\\Delta Loss after pruning\u201d in Figures 2-6 is validation or test loss, not training loss? Is this the cross-entropy loss? Also, it would be much easier to compare to other papers if test accuracy were reported instead or in addition. 10. In Figure 4, the cost to recover using fine-tuning seems to be only roughly 2% of the original training time. How much time is lost to the process of computing the average unit activation? UPDATE: I've raised the score slightly to 5 after the rebuttals and revisions.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your thorough comments and valuable suggestions . We hope the responses/clarifications below would help . ( 1 ) Ye et al . ( 2018 ) , proposes a pruning method that penalizes the variance of the output distribution of a unit . When units have low variance ( generating almost constant values ) , they remove the unit by propagating the mean values to the next layer . In our method we show that such a trick can be utilized in other pruning scenarios that don \u2019 t involve batch normalization or variance regularization . In particular , we show that mean replacement is the optimal update minimizing the reconstruction error on the next layer and empirically show that it does indeed reduce the pruning penalty in a variety of settings . Morcos et al . ( 2018 ) briefly mentions the possibility of using mean replacement in the ablation setting . In the single result shared in Section A1 , they prune/ablate the last layer of their convolutional network , since that would be the only layer without batch normalization . ( 2 ) We agree that there is a strong trend with using batch normalization in neural network training . However there is no guarantee that this trend will last or/and become a standard . There are also cases where batch normalization is not practical/preferred , like small batch training ( due to memory limitations of big networks ) and RNN training . We would also like to point out that our method is applicable in networks with instance or layer normalization . ( 3 ) We updated the table of scoring functions in Section 3.4 with related references . We hope that it will guide the reader to understand differences between our work and previous work . Following the common feedback we got from the reviewers , we performed some additional experiments . In our experiments on Cifar-10 , we did n't find any significant difference among different pruning methods despite the reported differences in others works . We share this surprising initial results in Section 6.6 along with a discussion about possible explanations . ( 4 ) We use the same notation as Morcos et al.2018 in our work . Given a convolutional kernel W with dimensions ( input channels , H , W , output channels ) , a unit i corresponds to the parameters W [ : , : , : ,i ] . In the case of a dense layer with parameters W of size ( input channels , output channels ) it corresponds to W [ : ,i ] . ( 5 ) Updated . ( 6 ) We used the average change in the loss to evaluate the performance . We expect the two measures ( absolute and average change in the loss ) to be very similar , especially when the pruning fraction is high . ( 7 ) We forgot to share these numbers , thanks for the note . It is 1000 and 10000 for cifar-10 and imagenet . We updated Section 3.4 to include these numbers . ( 8 ) Thanks for the feedback , we updated the part and had a second pass on the whole paper . ( 9 ) In Figures 2-6 we report pruning penalties ( cross entropy loss ) using a set of 1000 ( Cifar-10 ) and 10000 ( Imagenet2012 ) samples from the training set . Our motivation is that reducing the pruning penalty ( training loss ) would help the optimization by both/either reducing number of fine tuning steps needed and/or improving final performance . Our results support the first and undermine the second . ( 10 ) In practice we would suggest using a running average of the mean outputs , which would require constant number of FLOPS per sample ( linear in number of units in the network ) . Since our initial set of experiments do n't have end-2-end pruning experiments we have n't implemented such a feature and measure its effectiveness ."}, "1": {"review_id": "BJxRVnC5Fm-1", "review_text": "This paper presents a mean-replacement pruning strategy and utilizes the absolute-valued Taylor expansion as the scoring function for the pruning. Some computer vision problems are used as test beds to empirically show the effect of the employment of bias-propagation and different scoring functions. The empirical results validates the effectiveness of bias-propagation and absolute-valued Taylor expansion scoring functions. The work is generally well-written and the results are promising, and the theoretical explanation in 3.3 is intriguing. However, I think the following issues need some further clarifications: 1. What's the exact difference and connection between the mean-replacement pruning technique, and the bias-propagation technique in Ye et al., (2018) and the mean activation technique in Morcos et al. (2018)? The authors only mention that mean replacement pruning extends the idea in Ye et al. (2018) to the non-constrained training setting, but it is very unclear what \"constraints\" are talked about. Some more detailed and formal comparisons should be added, together with potential empirical comparisons. 2. In the abstract, the authors claim that they \"adapt an existing score function ...\", but from the main text it seems that absolute-valued Taylor expansion score is exactly the same one in Molchanov et al. (2016). Is this a typo (or misleading claim) in the abstract? 3. There are no comparisons of the approach proposed in this paper with some existing state-of-the-art, apart from some simple comparisons between whether bias-propagation is adopted and some inner comparisons among different scoring functions. It would also be much better if some charts/tables with certain metrics for improvement apart from pruning penalties (e.g., compression rates, or inference speed, etc.) instead of simply illustrative figures are shown. ### some smaller suggestions/typos ### 1. The plot legends/labels are kind of inconsistent with the description before the figures. For example, in the main text the authors mainly use \"pruning penalty\", while in the figures the y-axes are typically labelled as \"\\Delta-loss after pruning\", and the plot tag at page 5 bottom is different from those used in the plots, which introduces some unnecessary confusion. 2. It is very unclear how the authors arrive at the conclusion \"This results suggests ... the training process\" from the \"winning ticket\" hypothesis. 3. Several typos that can be easily spell-checked (e.g., \"the effect or pruning\" -> \"the effect of pruning\", etc.). I hope the authors can address these issues. Thanks!", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and valuable comments . We would like to provide clarifications/updates about the questions raised . ( 1 ) Our work extends the setting in Ye et al , ( 2018 ) ( networks with batch normalization and units with very small variance ) to networks without batch-normalization , un-regularized training and shows that replacing a unit with its mean value is a better at reducing the immediate damage compared to mere removal . Morcos et al. , ( 2018 ) compare mere removal with mean replacement in the context of ablation studies using a network with batch normalization . Since they use layers with batch normalization they are able to mean replace the final layer of the network only ( Batch normalization should remove any constant signal coming from the previous layer and therefore we should n't see any difference between the two methods in their setting , except the output layer of the network . ) . Our experiments involve a much greater variety of settings and show that Mean Replacement indeed works better in general . ( 2 ) Our scoring function and Molchanov et al . ( 2016 ) are first order approximations of two different values . We updated the table of scoring functions in Section 3.4 with related references . We hope that it will guide the reader to understand differences between our work and previous work . ( 3 ) We intentionally avoided running end to end pruning experiments in our work . Our assumption was that reducing the change in the training loss should help any pruning strategy that employs any of the saliency scores used . However , we agree that this connection is not clear and needs further investigation . Therefore we ran additional experiments where we perform iterative pruning with fix training budget . In our experiments , surprisingly , we did n't find any significant difference among ` non-random ` methods despite the reported differences in others works . Even though the results do n't support our case , we like to share these initial results in the appendix ( Section 6.6 ) with a discussion on possible reasons . ( 3 ) And finally regarding minor suggestions , ( 1 ) we updated with 'Pruning Penalty ` following your suggestion ( 2 ) Detecting lottery ticket early in the training is important , since one could reduce the size of the network and the training time . This result , if general enough , promises a whole new direction for pruning research . We updated our introduction indicating this point . ( 3 ) spell-checked ."}, "2": {"review_id": "BJxRVnC5Fm-2", "review_text": "1. Pruning neurons in pre-trained CNNs is a very important issue in deep learning, and there are a number of related works have been investigated in Section 2. However, it is very strange that, I did not see any comparison experiments to these related works in this paper. 2. The presentation of the experiment part is also wired, to report compression rates, speed-up rates, and accuracy might have a more explicit demonstration. 3. \"This is often done by replacing the these units with zeroes\". However, in previous works, we can directly establish a compact network with fewer neurons after pruning some unimportant neurons. Thus, some considerations and motivations in Section 3.2 seem wrong. 4. It seems that the neural network after using the proposed method has the same architecture as that of the original network, but some of it neurons are represented as mean replacement. Therefore, the compression and speed-up rates of the proposed method would be hard to implement in practice. 5. The paper should be further proofread. There are considerable grammar mistakes and unclear sentences.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . ( 1 ) We updated the table of scoring functions in Section 3.4 with related references . We hope that it will guide the reader to understand differences between our work and previous work . We also updated the last paragraph of Section 2 to highlight the difference between our work and Ye et al , ( 2018 ) , Morcos et al. , ( 2018 ) . ( 2 ) Even though pruning is widely used for model compression , recent work highlights a promising direction where pruning is used during training to reduce training time and improve final performance ( Han et al . ( 2016 ) , Frankle & Carbin ( 2018 ) ) . Therefore we focused on pruning experiments sampled from the entire length of a training job . Our motivation is that reducing the pruning penalty helps optimization by reducing the number of fine tuning steps needed for reaching the same level . However , following the common feedback we got from the reviewers , we performed some additional experiments . In our experimental setting , we did n't find any significant difference among different pruning methods . We share this surprising results in Section 6.6 along with a discussion about possible explanations . ( 3 ) Replacing a unit with zeros indeed corresponds to removing it along with all the outgoing weights ( it represents the same function ) . In our experiments we use masking to emulate this behaviour , however as you suggest , one can establish a smaller network in practice immediately if needed . ( 4 ) Our method replaces units with their mean output values . In practice , constant values are propagated to the next layer and units are removed as normal ( see Figure 1 ) . Mean output values can be aggregated during the training in an online fashion and bias propagation is a very cheap operation ( single matrix multiplication ) . Therefore , it is a practical method . ( 5 ) We are sorry about the mistakes slipped and we did further proofread the paper . Han , S. , Pool , J. , Narang , S. , Mao , H. , Gong , E. , Tang , S. , \u2026 Dally , W. J . ( 2016 ) .DSD : Dense-Sparse-Dense Training for Deep Neural Networks . Retrieved from http : //arxiv.org/abs/1607.04381"}}