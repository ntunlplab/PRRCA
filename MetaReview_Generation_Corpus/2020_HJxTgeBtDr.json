{"year": "2020", "forum": "HJxTgeBtDr", "title": "Towards Interpretable Evaluations: A Case Study of Named Entity Recognition", "decision": "Reject", "meta_review": "The paper diligently setup and conducted multiple experiments to validate their approach - bucketizating attributions of data and analyze them accordingly to discover deeper insights eg biases. However, reviewers pointed out that such bucketing is tailored to tasks where attributions are easily observed, such as the one of the focus in this paper -NER. While manuscript proposes this approach as \u2018general\u2019, reviewers failed to seem this point. Another reviewer recommended this manuscript to become a journal item rather than conference, due to the length of the page in appendix (17). There were some confusions around writings as well, pointed out by some reviewers. We highly recommend authors to carefully reflect on reviewers both pros and cons of the paper to improve the paper for your future submission. \n", "reviews": [{"review_id": "HJxTgeBtDr-0", "review_text": " The manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets. The method considers a set of methods addressing the task of Named Entity Recognition (NER) as case study. In addition, it proposes a set of attribute-based criteria, i.e. bucketization strategies, under which the dataset can be divided and analyzed in order to highlight different properties of the evaluated methods. As said earlier, the manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets. The characteristic of being able to provided deeper insights on strength/weaknesses and relevant factors on the inner-workings of a given method is something very desirable for every evaluation. As such, in my opinion, the \"interpretable\" tag associate to the proposed method is somewhat out of place. Having said that, I would recommend removing the \"interpretable\" tag and stress the contribution of this manuscript as an evaluation protocol. In Section 4.2, for the R-Bucket strategy it is stated as having the requirement of discrete and finite attributes. Based on the equations of the other two strategies (R-bucket and F-bucket), it seems that they also have the requirement of having discrete attributes. Is this indeed the case? if so, it should be explicitly indicated. Having said that, this raises another question: Is this protocol exclusive to tasks/problems with explicit discrete attributes? The goal of this manuscript is to propose a general evaluation protocol for NLP tasks. However, it seems to be somewhat tailored to the NER task. My question is: How well the proposed method generalizes to other NLP tasks without attributes? Similarly, how well the proposed bucketization strategies generalize beyond the NER task? Perhaps the generalization characteristics and limitations of the proposed evaluation methodology should be explicitly discussed in the manuscript. Last paragraph of Section 4.2 summarizes ideas that were just presented. It feels somewhat redundant. I suggest removing in in favor of extending the existing discussions and analysis. I may consider upgrading my initial rating based on on the feedback given to my questions/doubts. ", "rating": "3: Weak Reject", "reply_text": "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Tasks Attributes Measures Related Ref . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Machine Translation sentence length Bleu [ 1 ] Machine Translation word ( or N-gram ) frequency Accuracy * [ 2 ] in the training set . Machine Translation word POS-tag in the training set Accuracy * [ 3 ] Machine Translation words in reference file Word likelihood [ 3 ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Summarization ( Ext.or Abs . ) sentence length Rouge - Summarization ( Ext.or Abs . ) compression of summary Rouge [ 6 ] Summarization ( Ext.or Abs . ) density of summary Rouge [ 6 ] Summarization ( Ext.or Abs . ) volume overlap Rouge [ 5 ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Summarization ( Ext . ) position of each sentence Rouge/Accuracy [ 4 ] [ 5 ] Summarization ( Ext . ) OOV rate of sentence Rouge - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Semantic Matching length of sent1 or sent2 Accuracy [ 7 ] Semantic Matching Func ( sent1 , sent2 ) Accuracy - Semantic Matching OOV Accuracy - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- QA answer length , type , position Matching F1 [ 8 ] QA document length Matching F1 [ 12 ] QA query length , type Matching F1 [ 9 ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Text Classification sentence/word length Accuracy [ 11 ] Text Classification OOV Accuracy [ 10 ] Text Classification sentence familiarity Accuracy - -- -- -- -- -- -- -- -- -- -- \u201c Sentence familiarity \u201d : we could quantify the degree to which the test sentence has been seen in the training set ( based on n-gram calculation ) . \u3010References\u3011 [ 1 ] Effective Approaches to Attention-based Neural Machine Translation , Minh-Thang Luong Hieu Pham Christopher D. Manning [ 2 ] Von misesfisher loss for training sequence to sequence , Sachin Kumar and Yulia Tsvetkov . [ 3 ] Compare-mt : A Tool for Holistic Comparison of Language Generation Systems , Graham Neubig , Zi-Yi Dou , Junjie Hu , Paul Michel , Danish Pruthi , Xinyi Wang , John Wieting [ 4 ] Text Summarization with Pretrained Encoders , Yang Liu , Mirella Lapata [ 5 ] Earlier Isn \u2019 t Always Better : Sub-aspect Analysis on Corpus and System Biases in Summarization , Taehee Jung , Dongyeop Kang , Lucas Mentch , Eduard Hovy [ 6 ] A Closer Look at Data Bias in Neural Extractive Summarization Models , Ming Zhong , Danqing Wang , Pengfei Liu , Xipeng Qiu , Xuanjing Huang [ 7 ] Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks , Kai Sheng Tai , Richard Socher , Christopher D. Manning [ 8 ] Bidirectional Attention Flow for Machine Comprehension , Minjoon Seo , Aniruddha Kembhavi , Ali Farhadi , Hannaneh Hajishirzi [ 9 ] A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task , Danqi Chen , Jason Bolton , Christopher D. Manning [ 10 ] Learning Semantic Representations of Users and Products for Document Level Sentiment Classification , Duyu Tang , Bing Qin , Ting Liu [ 11 ] The Relationship of Word Length and Sentence Length : The Inter-Textual Perspective , Peter Grzybek , Ernst Stadlober , Emmerich Kelih [ 12 ] TriviaQA : A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension , Mandar Joshi , Eunsol Choi , Daniel S. Weld , Luke Zettlemoyer"}, {"review_id": "HJxTgeBtDr-1", "review_text": "TOWARDS INTERPRETABLE EVALUATIONS A CASE STUDY OF NAMED ENTITY RECOGNITION The authors propose an evaluation methodology to study the relations between datasets and machine learning models. This methodology introduces the notion of attributes which describes different aspects of the samples and buckets which group samples according to the attributes. The goal is to give a better understanding of the strengths and weaknesses of an algorithm on a specific dataset according to the attributes, as shown on Fig4. The article is very dense and the author chose to present the method from an abstract and generic point of view which makes the reading of the article difficult. In the end, the proposition is a formalisation of the simple error analysis which is commonly done when trying to improve a machine learning system. The advantage of the method could be to introduce some metrics to make the error analysis more automatic. These metrics are given in section 4 but here again only from a formal point of view : it is very difficult for the reader to understand how to interpret them and how to use them for a practical case. The paper is 17 pages long with the annex : it would better fit a journal publication or the author should select some of the main results to present them in a conference paper. The aspects of the paper related to learning relations is no put forward enough. 2. Related work 2.1 : -supplementary exam : unclear 2.2 : - methodological perspective : a bit a repetition of introduction - task perspective : not very clear, is the main message \"it important to understand what in the dataset make the model work ?\" 3 Task Section is too small to be a level 1 title 4 Attributes figure 2 : where are the links to levels ? 4.2 : * familiarity : test/train distribution should be the same. Fk computer on train set because it is bigger ? it allows to study the impact of the number of occurrences in the training set. Is it more interesting than a learning curve ? * multi attribute familiarity : risk of metric explosion ? how to select the attributes ? * eq 3 : spearman not defined * 4.3 * metric are defined by formula but it is difficult to understand what is the rationale behind each of them and therefore figure out how to interpret them * \"Usually where a, b represent two different models and usually model a has a higher performance (by dataset-level metric)\" : unclear 5 Experimental setting Table3 : the encoding of the model name is not clear a metric on all the dataset for each model could be computed to decide which one is the best overall how did you choose the tested combinaisons ? 6\\.2 analysis of Fig 4 : R-eLen does not existe (R-Ele). what is eta ? table 4 : spearman\\**r*\\* ? 6\\.4 * CRF vs MLP : \"... a major factor for the choices of CRF and MLP: **if** a dataset with higher \u03b6MF\u2212et, in which longer entities can benefit more from CRF-based models.\" > missing words ? Writing : * \"Concretely\" isn't very natural at the beginning of sentences, same thing with \"Formally\", 'Intuitively' \u2026 * in 4.1 : \"We refer to E, P, K as the sets of entities (i.e. New York), entity attributes (i.e. entity length) and attributes values (i.e. 2).\" => \"We refer to the sets of entities (i.e. New York) as E, entity attributes (i.e. entity length) as P and attributes values (i.e. 2) as K\" would be better * same thing in 4.3 \"we refer to M = m1,\u00b7\u00b7\u00b7 ,m|M| as a set of **models** and P = p1,\u00b7\u00b7\u00b7 ,p|P| as a set of **attributes**\" doesn't really work, \"M = m1,\u00b7\u00b7\u00b7 ,m|M| is a set of **models** and P = p1,\u00b7\u00b7\u00b7 ,p|P| is a set of **attributes**\" maybe * in 4.2 page 5 : \"the familiarity Fk (p1 , p2 ) is a measure with intriguing explanation \u2026\" : not clear * 6.3 (3) \"Only using character-level CNN is apt to overfit the feature of capital letters.\" **apt** doesnt work here", "rating": "3: Weak Reject", "reply_text": "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Tasks Attributes Measures Bucket Strategy * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Machine Translation sentence length Bleu R-Buck Machine Translation word ( or N-gram ) frequency Accuracy * R-Buck in the training set . Machine Translation word POS-tag in the training set Accuracy * R-Buck Machine Translation words in reference file Word likelihood R-Buck -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Summarization ( Ext.or Abs . ) sentence length Rouge R-Buck Summarization ( Ext.or Abs . ) compression of summary Rouge R or F-Buck Summarization ( Ext.or Abs . ) density of summary Rouge R or F-Buck Summarization ( Ext.or Abs . ) volume overlap Rouge R-Buck -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Summarization ( Ext . ) position of each sentence Rouge/Accuracy F-Buck Summarization ( Ext . ) OOV rate of sentence Rouge R-Buck -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Semantic Matching length of sent1 or sent2 Accuracy R-Buck Semantic Matching Func ( sent1 , sent2 ) Accuracy R-Buck Semantic Matching OOV Accuracy R-Buck -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- QA answer length , type , position Matching F1 F-Buck QA document length Matching F1 R-Buck QA query type Matching F1 F-Buck -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Text Classification sentence/word length Accuracy R-Buck Text Classification OOV Accuracy R-Buck Text Classification sentence familiarity Accuracy F-Buck -- -- -- -- -- -- -- \u201c Attributes \u201d denotes the criterion that we use to divide the test set , and \u201c Measures \u201d represents the measure we use to evaluate each divided sub-set . \u201c Bucket Strategy \u201d shows which types of bucketization methods could be adopted ."}, {"review_id": "HJxTgeBtDr-2", "review_text": "This paper discusses a methodology to interpret models and model outputs for Named Entity Recognition (NER) based on assigned attributes. The key idea is to bucketize the test data based on characteristics of attributes and then comment on effect of the attribute on the model, the task itself or the dataset bias. The empirical evaluation is impressive. The authors have constructed a series of experiments to make their case. The paper is well-written and easy to understand, albeit some of the related work seems a little unrelated to the task at hand. While the authors have tried to state that the method is \"general\" and goes beyond NER, I am not sure if that is the case. The creation of attribute buckets is vital for any further analysis, its not clear how the method can be adapted to more general settings unless such attributes and buckets can be created easily (e.g. using domain knowledge). Furthermore, there is only one problem setting considered (i.e. NER), and for the paper is make claim to more general settings, I would expect evaluations on atleast one more problem setting. I would suggest the authors modify the claims accordingly. This is not to diminish from their contributions in the NER. The bucketization idea is not something out of the park novel. It is probably something already being used in practice. However, delineating the procedure and suggesting quantifiable statistics and designing experiments to illustrate how these can be used to draw qualitative conclusions is something that is very interesting and useful to the community as a whole. The strongest part of this paper is the empirical evaluation that allows drawing interesting conclusions, and suggests a methodology to reach that conclusion. While some of the claims made (e.g. regarding dataset biases) probably require further and deeper analysis, this is a good first step that should foster further research and discussion. ", "rating": "8: Accept", "reply_text": "Thank you for your encouraging review . We will continue to improve the draft in the revised version . Q1 : \u201c The bucketization idea is not something out of the park novel . It is probably something already being used in practice . However , delineating the procedure and suggesting quantifiable statistics and designing experiments to illustrate how these can be used to draw qualitative conclusions is something that is very interesting and useful to the community as a whole. \u201d A1 : We \u2019 re quite excited that you have pointed out the most challenging part of our work . Yes , when we would like to take multiple attributes , models , datasets all together , the most challenging thing is how to derive specific conclusions based on these tremendous results . In this paper , we overcome the difficulty by designing several meaningful measures , which can help us understand the relative merits between models quantitatively . This work also would like to show : when multiple datasets , models are ready , the time is ripe for us to shift the data-driven learning to data-driven analyzing ( conduct an analysis over plenty of experimental data with the help of meaning measures ) Q2 : \u201c While the authors have tried to state that the method is `` general '' and goes beyond NER , I am not sure if that is the case . The creation of attribute buckets is vital for any further analysis , its not clear how the method can be adapted to more general settings unless such attributes and buckets can be created easily ( e.g.using domain knowledge ) . \u201d A2 : We try to address your concern by presenting a detailed description of general settings on other tasks . You could refer to our first answer to R3 . Q3 : \u201c the paper is well-written and easy to understand , albeit some of the related work seems a little unrelated to the task at hand \u201d A3 : Thanks for your suggestion and we have made it revised in our new version ."}], "0": {"review_id": "HJxTgeBtDr-0", "review_text": " The manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets. The method considers a set of methods addressing the task of Named Entity Recognition (NER) as case study. In addition, it proposes a set of attribute-based criteria, i.e. bucketization strategies, under which the dataset can be divided and analyzed in order to highlight different properties of the evaluated methods. As said earlier, the manuscript proposes an evaluation methodology to obtain deeper insights regarding the strength and weaknesses of different methods on different datasets. The characteristic of being able to provided deeper insights on strength/weaknesses and relevant factors on the inner-workings of a given method is something very desirable for every evaluation. As such, in my opinion, the \"interpretable\" tag associate to the proposed method is somewhat out of place. Having said that, I would recommend removing the \"interpretable\" tag and stress the contribution of this manuscript as an evaluation protocol. In Section 4.2, for the R-Bucket strategy it is stated as having the requirement of discrete and finite attributes. Based on the equations of the other two strategies (R-bucket and F-bucket), it seems that they also have the requirement of having discrete attributes. Is this indeed the case? if so, it should be explicitly indicated. Having said that, this raises another question: Is this protocol exclusive to tasks/problems with explicit discrete attributes? The goal of this manuscript is to propose a general evaluation protocol for NLP tasks. However, it seems to be somewhat tailored to the NER task. My question is: How well the proposed method generalizes to other NLP tasks without attributes? Similarly, how well the proposed bucketization strategies generalize beyond the NER task? Perhaps the generalization characteristics and limitations of the proposed evaluation methodology should be explicitly discussed in the manuscript. Last paragraph of Section 4.2 summarizes ideas that were just presented. It feels somewhat redundant. I suggest removing in in favor of extending the existing discussions and analysis. I may consider upgrading my initial rating based on on the feedback given to my questions/doubts. ", "rating": "3: Weak Reject", "reply_text": "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Tasks Attributes Measures Related Ref . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Machine Translation sentence length Bleu [ 1 ] Machine Translation word ( or N-gram ) frequency Accuracy * [ 2 ] in the training set . Machine Translation word POS-tag in the training set Accuracy * [ 3 ] Machine Translation words in reference file Word likelihood [ 3 ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Summarization ( Ext.or Abs . ) sentence length Rouge - Summarization ( Ext.or Abs . ) compression of summary Rouge [ 6 ] Summarization ( Ext.or Abs . ) density of summary Rouge [ 6 ] Summarization ( Ext.or Abs . ) volume overlap Rouge [ 5 ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Summarization ( Ext . ) position of each sentence Rouge/Accuracy [ 4 ] [ 5 ] Summarization ( Ext . ) OOV rate of sentence Rouge - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Semantic Matching length of sent1 or sent2 Accuracy [ 7 ] Semantic Matching Func ( sent1 , sent2 ) Accuracy - Semantic Matching OOV Accuracy - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- QA answer length , type , position Matching F1 [ 8 ] QA document length Matching F1 [ 12 ] QA query length , type Matching F1 [ 9 ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Text Classification sentence/word length Accuracy [ 11 ] Text Classification OOV Accuracy [ 10 ] Text Classification sentence familiarity Accuracy - -- -- -- -- -- -- -- -- -- -- \u201c Sentence familiarity \u201d : we could quantify the degree to which the test sentence has been seen in the training set ( based on n-gram calculation ) . \u3010References\u3011 [ 1 ] Effective Approaches to Attention-based Neural Machine Translation , Minh-Thang Luong Hieu Pham Christopher D. Manning [ 2 ] Von misesfisher loss for training sequence to sequence , Sachin Kumar and Yulia Tsvetkov . [ 3 ] Compare-mt : A Tool for Holistic Comparison of Language Generation Systems , Graham Neubig , Zi-Yi Dou , Junjie Hu , Paul Michel , Danish Pruthi , Xinyi Wang , John Wieting [ 4 ] Text Summarization with Pretrained Encoders , Yang Liu , Mirella Lapata [ 5 ] Earlier Isn \u2019 t Always Better : Sub-aspect Analysis on Corpus and System Biases in Summarization , Taehee Jung , Dongyeop Kang , Lucas Mentch , Eduard Hovy [ 6 ] A Closer Look at Data Bias in Neural Extractive Summarization Models , Ming Zhong , Danqing Wang , Pengfei Liu , Xipeng Qiu , Xuanjing Huang [ 7 ] Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks , Kai Sheng Tai , Richard Socher , Christopher D. Manning [ 8 ] Bidirectional Attention Flow for Machine Comprehension , Minjoon Seo , Aniruddha Kembhavi , Ali Farhadi , Hannaneh Hajishirzi [ 9 ] A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task , Danqi Chen , Jason Bolton , Christopher D. Manning [ 10 ] Learning Semantic Representations of Users and Products for Document Level Sentiment Classification , Duyu Tang , Bing Qin , Ting Liu [ 11 ] The Relationship of Word Length and Sentence Length : The Inter-Textual Perspective , Peter Grzybek , Ernst Stadlober , Emmerich Kelih [ 12 ] TriviaQA : A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension , Mandar Joshi , Eunsol Choi , Daniel S. Weld , Luke Zettlemoyer"}, "1": {"review_id": "HJxTgeBtDr-1", "review_text": "TOWARDS INTERPRETABLE EVALUATIONS A CASE STUDY OF NAMED ENTITY RECOGNITION The authors propose an evaluation methodology to study the relations between datasets and machine learning models. This methodology introduces the notion of attributes which describes different aspects of the samples and buckets which group samples according to the attributes. The goal is to give a better understanding of the strengths and weaknesses of an algorithm on a specific dataset according to the attributes, as shown on Fig4. The article is very dense and the author chose to present the method from an abstract and generic point of view which makes the reading of the article difficult. In the end, the proposition is a formalisation of the simple error analysis which is commonly done when trying to improve a machine learning system. The advantage of the method could be to introduce some metrics to make the error analysis more automatic. These metrics are given in section 4 but here again only from a formal point of view : it is very difficult for the reader to understand how to interpret them and how to use them for a practical case. The paper is 17 pages long with the annex : it would better fit a journal publication or the author should select some of the main results to present them in a conference paper. The aspects of the paper related to learning relations is no put forward enough. 2. Related work 2.1 : -supplementary exam : unclear 2.2 : - methodological perspective : a bit a repetition of introduction - task perspective : not very clear, is the main message \"it important to understand what in the dataset make the model work ?\" 3 Task Section is too small to be a level 1 title 4 Attributes figure 2 : where are the links to levels ? 4.2 : * familiarity : test/train distribution should be the same. Fk computer on train set because it is bigger ? it allows to study the impact of the number of occurrences in the training set. Is it more interesting than a learning curve ? * multi attribute familiarity : risk of metric explosion ? how to select the attributes ? * eq 3 : spearman not defined * 4.3 * metric are defined by formula but it is difficult to understand what is the rationale behind each of them and therefore figure out how to interpret them * \"Usually where a, b represent two different models and usually model a has a higher performance (by dataset-level metric)\" : unclear 5 Experimental setting Table3 : the encoding of the model name is not clear a metric on all the dataset for each model could be computed to decide which one is the best overall how did you choose the tested combinaisons ? 6\\.2 analysis of Fig 4 : R-eLen does not existe (R-Ele). what is eta ? table 4 : spearman\\**r*\\* ? 6\\.4 * CRF vs MLP : \"... a major factor for the choices of CRF and MLP: **if** a dataset with higher \u03b6MF\u2212et, in which longer entities can benefit more from CRF-based models.\" > missing words ? Writing : * \"Concretely\" isn't very natural at the beginning of sentences, same thing with \"Formally\", 'Intuitively' \u2026 * in 4.1 : \"We refer to E, P, K as the sets of entities (i.e. New York), entity attributes (i.e. entity length) and attributes values (i.e. 2).\" => \"We refer to the sets of entities (i.e. New York) as E, entity attributes (i.e. entity length) as P and attributes values (i.e. 2) as K\" would be better * same thing in 4.3 \"we refer to M = m1,\u00b7\u00b7\u00b7 ,m|M| as a set of **models** and P = p1,\u00b7\u00b7\u00b7 ,p|P| as a set of **attributes**\" doesn't really work, \"M = m1,\u00b7\u00b7\u00b7 ,m|M| is a set of **models** and P = p1,\u00b7\u00b7\u00b7 ,p|P| is a set of **attributes**\" maybe * in 4.2 page 5 : \"the familiarity Fk (p1 , p2 ) is a measure with intriguing explanation \u2026\" : not clear * 6.3 (3) \"Only using character-level CNN is apt to overfit the feature of capital letters.\" **apt** doesnt work here", "rating": "3: Weak Reject", "reply_text": "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Tasks Attributes Measures Bucket Strategy * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Machine Translation sentence length Bleu R-Buck Machine Translation word ( or N-gram ) frequency Accuracy * R-Buck in the training set . Machine Translation word POS-tag in the training set Accuracy * R-Buck Machine Translation words in reference file Word likelihood R-Buck -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Summarization ( Ext.or Abs . ) sentence length Rouge R-Buck Summarization ( Ext.or Abs . ) compression of summary Rouge R or F-Buck Summarization ( Ext.or Abs . ) density of summary Rouge R or F-Buck Summarization ( Ext.or Abs . ) volume overlap Rouge R-Buck -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Summarization ( Ext . ) position of each sentence Rouge/Accuracy F-Buck Summarization ( Ext . ) OOV rate of sentence Rouge R-Buck -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Semantic Matching length of sent1 or sent2 Accuracy R-Buck Semantic Matching Func ( sent1 , sent2 ) Accuracy R-Buck Semantic Matching OOV Accuracy R-Buck -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- QA answer length , type , position Matching F1 F-Buck QA document length Matching F1 R-Buck QA query type Matching F1 F-Buck -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Text Classification sentence/word length Accuracy R-Buck Text Classification OOV Accuracy R-Buck Text Classification sentence familiarity Accuracy F-Buck -- -- -- -- -- -- -- \u201c Attributes \u201d denotes the criterion that we use to divide the test set , and \u201c Measures \u201d represents the measure we use to evaluate each divided sub-set . \u201c Bucket Strategy \u201d shows which types of bucketization methods could be adopted ."}, "2": {"review_id": "HJxTgeBtDr-2", "review_text": "This paper discusses a methodology to interpret models and model outputs for Named Entity Recognition (NER) based on assigned attributes. The key idea is to bucketize the test data based on characteristics of attributes and then comment on effect of the attribute on the model, the task itself or the dataset bias. The empirical evaluation is impressive. The authors have constructed a series of experiments to make their case. The paper is well-written and easy to understand, albeit some of the related work seems a little unrelated to the task at hand. While the authors have tried to state that the method is \"general\" and goes beyond NER, I am not sure if that is the case. The creation of attribute buckets is vital for any further analysis, its not clear how the method can be adapted to more general settings unless such attributes and buckets can be created easily (e.g. using domain knowledge). Furthermore, there is only one problem setting considered (i.e. NER), and for the paper is make claim to more general settings, I would expect evaluations on atleast one more problem setting. I would suggest the authors modify the claims accordingly. This is not to diminish from their contributions in the NER. The bucketization idea is not something out of the park novel. It is probably something already being used in practice. However, delineating the procedure and suggesting quantifiable statistics and designing experiments to illustrate how these can be used to draw qualitative conclusions is something that is very interesting and useful to the community as a whole. The strongest part of this paper is the empirical evaluation that allows drawing interesting conclusions, and suggests a methodology to reach that conclusion. While some of the claims made (e.g. regarding dataset biases) probably require further and deeper analysis, this is a good first step that should foster further research and discussion. ", "rating": "8: Accept", "reply_text": "Thank you for your encouraging review . We will continue to improve the draft in the revised version . Q1 : \u201c The bucketization idea is not something out of the park novel . It is probably something already being used in practice . However , delineating the procedure and suggesting quantifiable statistics and designing experiments to illustrate how these can be used to draw qualitative conclusions is something that is very interesting and useful to the community as a whole. \u201d A1 : We \u2019 re quite excited that you have pointed out the most challenging part of our work . Yes , when we would like to take multiple attributes , models , datasets all together , the most challenging thing is how to derive specific conclusions based on these tremendous results . In this paper , we overcome the difficulty by designing several meaningful measures , which can help us understand the relative merits between models quantitatively . This work also would like to show : when multiple datasets , models are ready , the time is ripe for us to shift the data-driven learning to data-driven analyzing ( conduct an analysis over plenty of experimental data with the help of meaning measures ) Q2 : \u201c While the authors have tried to state that the method is `` general '' and goes beyond NER , I am not sure if that is the case . The creation of attribute buckets is vital for any further analysis , its not clear how the method can be adapted to more general settings unless such attributes and buckets can be created easily ( e.g.using domain knowledge ) . \u201d A2 : We try to address your concern by presenting a detailed description of general settings on other tasks . You could refer to our first answer to R3 . Q3 : \u201c the paper is well-written and easy to understand , albeit some of the related work seems a little unrelated to the task at hand \u201d A3 : Thanks for your suggestion and we have made it revised in our new version ."}}