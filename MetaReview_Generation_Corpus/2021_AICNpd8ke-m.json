{"year": "2021", "forum": "AICNpd8ke-m", "title": "Multi-Class Uncertainty Calibration via Mutual Information Maximization-based Binning", "decision": "Accept (Poster)", "meta_review": "The paper proposes to maximizing the mutual information to optimize the bin for multiclass calibration. The idea, technique, and presentation are good. The paper solves some multiclass calibration  issues. The author should revise the paper according the reviewer's comments before publish.", "reviews": [{"review_id": "AICNpd8ke-m-0", "review_text": "Update : the authors went out of their way to address my concerns about the absence of the unbalanced class setting : they added a new datasets ( SVHN ) , new results ( table 4 ) and updated some of their explanations . All these additions seem satisfactory . I was also pleased with the feedback about computational cost ( R3 ) . I improved my rating . While I agree with the concerns of reviewer 4 ( those I could understand ) , they would apply to every publication I have read about calibration , and I think the authors addressed these concerns to the best of our current knowledge . This paper proposes an information maximization binning scheme for calibration . Starting with a good introduction , a clear progression leads to the core algorithm described by theorem 1 . Limits of previous histogram-based approaches , both in terms of performance or reliability of metric , are clearly demonstrated with clear figures and proper references . While using information measures to drive histogram binning has been done , I assume that the current classification setting where one maximizes the MI between the logit and the class is novel ( the authors do not give pointer to previous work here , only mentioning the Info Bottleneck without references ) . Theorem 1 leads to an alternative minimization algorithm with analytical steps . I did not check the convergence behavior proof but Figure 3 is convincing enough . I did not fully understand the information bottleneck limit . Experiments show first that the information binning strategy is far superior than equal-mass or size binning . Table 2 and 3 then shows how it improves on most scaling algorithms used for calibration . One detail I am not comfortable with : the ECE_ { 1/K } hack , as it looks like a last-minute addition to give even stronger gains to the I-MAX method . A more principled introduction would be better ( see below ) . This would be an excellent paper except for the following , which casts doubts whether all the steps of the method generalize to an unbalanced multiclass setting . It is probably possible to fix or explain before publication . This paper relies on a very unnatural and unfortunate state-of-affair in ML : classes are equally distributed on the test data . The phrasing does even consider any other possibility , and some of the algorithms seem to be quite specific to this setup , requiring significant changes in the \u201c real world \u201d case where test classes are not equally distributed . At the end of section 3.2 , the authors propose an algorithm to merge { S_k } across K classes based on the observation that they have similar distribution . Rather than a proof , they run a simulation on ImageNet ( Sec A.2 ) that shows it is better than binning each S_k separately . While the experiment is elegant , it probably strongly relies on the fact that each S_k has the same 1 : K-1 split . What would happen if the classes follow a more realistic Zipf law , as observed in real NLP classification tasks ? I would assume that the merging process could still be applicable , but applied to groups of { S_k } with similar class-0/class-1 distributions . In section 4.1 , the trick to remove from the measure of the ECE classes where the predicted probability is less than 1/K also depends on a uniform 1/K prior . It should also be adapted to a non-uniform prior .", "rating": "7: Good paper, accept", "reply_text": "* * R1-3 ) * * * Prior work of using information measures , only mentioning the Info . bottleneck without references * Thank you for pointing the missing reference for information bottleneck ( IB ) proposed by Tishby et . al.1999.It is added in the revision . From the perspective of using information measures to drive HB or , in a broader sense , quantization , our method falls into the framework of IB . Often , the design goal of quantization is to minimize the reconstruction loss or information loss before and after quantization . The IB framework instead considers a third variable which represents the relevant information carried by the variable before quantization . The goal then becomes to retain the information relevant to the third variable after quantization . Note that , IB as a theoretic framework is not application-oriented . In the context of multi-class calibration , the label is the third variable in addition to the logit and quantized logit . We care about how well the label can be predicted from the quantized logit rather than how well the logit can be reproduced after quantization . So , I-Max focuses on easing the difficulty of predicting the label from the quantized logits , rather than reconstructing the ( input ) logit from it . We have also identified that the bin edges of HB play the determinant role in preserving the label information . As two common choices in the literature for post-hoc calibration , Eq.size , and Eq.mass HB were shown to be suboptimal . Our contribution lies in identifying and remedying the issues of the binning schemes at preserving the label information . In the revision , we add a section , i.e. , Appendix A3.2 , to discuss the connection with the IB ."}, {"review_id": "AICNpd8ke-m-1", "review_text": "Update after the rebuttal : The authors have answered my concerns . I believe the paper should be accepted and would be a nice contribution to the current research . The paper proposes a novel approach for post-hoc calibration of outputs of the neural networks to estimate uncertainty of its prediction . The paper considers the histogram binning approach ( in contrast to scaling approaches existing in the literature ) and utilises the information theory in building bins . Strong points : * The work is very well placed in the context of the existing literature identifying the current gaps * Theoretically sound motivation of the approach * Extensive empirical evaluation Weak points : * Some discussion of the cost of the proposed method is lacking - i.e.how much in terms of computational time and memory this new calibration method is ? * The methods are compared with respect to accuracy and Expected Calibration Error ( ECE ) only . It has been shown that ECE is not a good metric for comparing different methods ( see , e.g.Ashukha , A. , Lyzhov , A. , Molchanov , D. and Vetrov , D. , 2020 . Pitfalls of in-domain uncertainty estimation and ensembling in deep learning . ICLR 2020 ) . I am recommending acceptance of the paper , though addressing the weak points above would largely improve the paper . The reasons for this decision is that strong points outweigh weak points : the proposed idea is interesting , it is shown that it is promising in practice ( subject to not very good metrics ) and the paper is mostly well written and easy to follow . Questions to authors : Could you please address raised weak points ? Additional feedback ( not necessarily important for evaluation , but could help to improve the paper ) : 1 . The part on shared class-wise binning is rather rushed in the main paper and it is not very clear . It is also rather independent contribution from the main I-Max calibration contribution . It would be better to somehow put them under one umbrella 2 . Section 2 . \u201c Bayesian DNNs , e.g . ( Blundell et al. , 2015 ) and their approximations ( Gal & Ghahramani , 2016 ) \u201d \u2013 a very arguable statement , I would suggest rephrasing it . Since Blundell et al.proposed variational inference which is also an approximation , and Gal & Ghahramani work is not an approximation of Blundell et al. \u2019 model 3 . Section 4.2 . \u201c Namely , matrix scaling w. L_2 \u201d dot after w is read as a full stop which is confusing 4 . After eq . ( 5 ) . `` So , we can solve the problem by iteratively and alternately updating $ { g_m } $ and $ { \\phi_m } $ based on ( A12 ) . '' - it seems eq.5 and A12 are the same and it would be more convenient to refer to eq.5 in the text right after it . 5.I am a little bit missing the overall procedure of the proposed calibration . I.e.all details are there ( especially if refer to appendix ) , but after reading the main paper , there is no feeling that I can now go and implement it for my problem . Maybe a pseudocode can help , or just step-by-step guidance Minor : Section 3 , first paragraph : \u201c Sec.3.2 ) \u201d \u2013 redundant bracket", "rating": "7: Good paper, accept", "reply_text": "* * R3-3 ) * * * Additional feedback on presentation * Thank you very much for this detailed feedback of corrections and suggestions . These points are integrated into the revision . In particular , we revised Sec.3.2 for a more detailed presentation of the class-wise calibration under the one-vs-rest strategy and the proposed sharing strategy , i.e. , sCW . We would also like to point out Appendix A2 where we discuss the sCW contribution in more detail and with empirical analysis . Yes , the sCW contribution is orthogonal to I-Max . As we have shown in Tab . 3 , it can also work with other methods based on one-vs-rest conversion , improving their calibration performance . We present both sCW and I-Max under the umbrella of class-wise calibration . sCW is a pragmatic technique for calibration set construction , addressing the class imbalance issue . As a binning scheme , I-Max is an outcome of maximizing the MI over the calibration set . sCW helps improve the sample efficiency at I-Max training ."}, {"review_id": "AICNpd8ke-m-2", "review_text": "This paper highlights the issues with the scaling method and histogram binning i.e. , underestimate calibration error in scaling methods and failing to preserve classification accuracy , and sample-inefficiency in HB . They use the I-Max concept for binning , which maximizes the mutual information between labels and quantized logits . They claim that their approach mitigates potential loss in ranking performance and allows simultaneous improvement of ranking and calibration performance by disentangling the optimization of bin edges and representatives . They also propose a shared class-wise ( sCW ) strategy that fits a single calibrator on the merged training sets of all K class-wise problems to improve the sample efficiency . The paper is well written and the authors provide enough motivation and intuition of why maximizing the mutual information between labels and quantized logits would help multi-class calibration . There are some concerns and issues that I think needs to be addressed . 1- One approach in estimating uncertainty in classification is to choose a model and a regularized loss function to inherently learn a good representation . For example using confidence as a term for regularization in neural networks is proposed in Regularizing neural networks by penalizing confident output distributions ( ICLR 2017 ) that penalizes low-entropy output distributions . I think it is worth comparing the results with such existing work and discussing the advantages and disadvantages since a similar concept has been used one while training the model and this paper as a post-hoc calibration . 2- It is interesting that a single calibrator on the merged training sets of all K class-wise problems ( sCW ) performs well . As it is mentioned in the paper , it introduces bias due to having samples drawn from the other classes . In HB , increasing its number of evaluation bins reduces the bias , but in ( sCW ) such bias can not be controlled . Moreover , Figure A2 shows it achieves smaller JSDs which is not expected . Is there any reason for that ? What would happen if the number of bins is increased ? 3- Based on the experimental results , it seems I-Max performs better than other binning approaches . However , compared to the scaling methods it seems GP ( Wenger et al.2020 ) performs better at NLL/Brier than the I-Max variants . 4- Even though the paper shows combining I-Max with GP improves the ECE , it is not clear how the issues of each approach will be handled . For example , the ECE might be underestimated .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * R4-3 ) * * * NLL/Brier Comparisons * Yes , we have observed that GP performs better than the I-Max binning variants in terms of NLL/Brier , and provided an explanation in Appendix A10 ( formerly Appendix A9 ) . As it may be too brief there , we elaborate more on this observation . GP is trained by directly optimizing the NLL as its loss . As a non-parametric Bayesian method , GP has a larger model expressive capacity than binning . While achieving better NLL/Brier , it costs significantly more computational complexity and memory . In contrast , I-Max only relies on logic comparisons at test time . Among the binning schemes , I-Max w. GP achieves the best NLL/Brier across the datasets and models . It is noted that I-Max w. GP remains to be a binning scheme . So , the combination does not change the model capacity of I-Max . GP is only exploited during training to improve the optimization of I-Max \u2019 s bin representatives . Besides the low complexity benefit , I-Max w. GP as a binning scheme does not suffer from the ECE underestimation issue of scaling methods such as GP . We further note that as a cross-entropy measure between two distributions , the NLL would be an ideal metric for calibration evaluation . However , `` empirical '' NLL and Brier favor high accuracy and high confident classifiers , as each sample only having one hard label essentially implies the maximum confidence on a single class . For this reason , during training , the empirical NLL loss will keep pushing the prediction probability to one even after reaching $ 100 $ % training set accuracy . As a result , the trained classifier showed poor calibration performance at test time ( Guo et al. , 2017 ) . In contrast to NLL/Brier , empirical ECEs use hard labels differently . The ground truth correctness associated with the prediction confidence $ p $ is estimated by averaging over the hard labels of the samples receiving the prediction probability $ p $ or close to $ p $ . Due to averaging , the empirical ground truth correctness is usually not a hard label . Lastly , we use a small example to show the difference between the NLL/Brier and ECE : for $ N $ predictions , all assigned a confidence of $ 1.0 $ and containing $ M $ mistakes , the calibrated confidence is $ M/N < 1 $ . Unlike ECE , the NLL/Brier loss is only non-zero only for the $ M $ wrong predictions , despite all $ N $ predictions being miscalibrated . This example shows that NLL/Brier penalizes miscalibration far less than ECE . * * * $ $ * * R4-4 ) * * * Combination of I-Max with GP and How the issues of each approach will be handled . * The proposed combination of I-Max and GP aims at exploiting their complementary benefits . GP is a sample efficient scaling method for post-hoc calibration . However , it suffers from the issue of high complexity and ECE underestimation . On the contrary , I-Max has low complexity and its ECE estimation can converge to the ground truth ECE . To accurately set the bin representatives of the binning schemes , we often need a sufficient number of samples per bin . I-Max w. GP first sets the bin edges to bin the raw logits , which maximally preserve the label information . Then , it sets the bin representatives by averaging the GP-scaled prediction probabilities of the samples in each bin , exploiting the sample efficiency of GP . After setting the bin representatives , GP is no longer needed . At test time , I-Max w. GP remains to be a binning scheme , enjoying its low complexity and reliable ECE estimation . As a result of exploiting their complementary benefits ( and in essence also addressing the underlying issues of each approach ) , we observe in Tab 2 that I-Max w. GP is the top-performing at ECE , being better than GP or I-Max alone ."}], "0": {"review_id": "AICNpd8ke-m-0", "review_text": "Update : the authors went out of their way to address my concerns about the absence of the unbalanced class setting : they added a new datasets ( SVHN ) , new results ( table 4 ) and updated some of their explanations . All these additions seem satisfactory . I was also pleased with the feedback about computational cost ( R3 ) . I improved my rating . While I agree with the concerns of reviewer 4 ( those I could understand ) , they would apply to every publication I have read about calibration , and I think the authors addressed these concerns to the best of our current knowledge . This paper proposes an information maximization binning scheme for calibration . Starting with a good introduction , a clear progression leads to the core algorithm described by theorem 1 . Limits of previous histogram-based approaches , both in terms of performance or reliability of metric , are clearly demonstrated with clear figures and proper references . While using information measures to drive histogram binning has been done , I assume that the current classification setting where one maximizes the MI between the logit and the class is novel ( the authors do not give pointer to previous work here , only mentioning the Info Bottleneck without references ) . Theorem 1 leads to an alternative minimization algorithm with analytical steps . I did not check the convergence behavior proof but Figure 3 is convincing enough . I did not fully understand the information bottleneck limit . Experiments show first that the information binning strategy is far superior than equal-mass or size binning . Table 2 and 3 then shows how it improves on most scaling algorithms used for calibration . One detail I am not comfortable with : the ECE_ { 1/K } hack , as it looks like a last-minute addition to give even stronger gains to the I-MAX method . A more principled introduction would be better ( see below ) . This would be an excellent paper except for the following , which casts doubts whether all the steps of the method generalize to an unbalanced multiclass setting . It is probably possible to fix or explain before publication . This paper relies on a very unnatural and unfortunate state-of-affair in ML : classes are equally distributed on the test data . The phrasing does even consider any other possibility , and some of the algorithms seem to be quite specific to this setup , requiring significant changes in the \u201c real world \u201d case where test classes are not equally distributed . At the end of section 3.2 , the authors propose an algorithm to merge { S_k } across K classes based on the observation that they have similar distribution . Rather than a proof , they run a simulation on ImageNet ( Sec A.2 ) that shows it is better than binning each S_k separately . While the experiment is elegant , it probably strongly relies on the fact that each S_k has the same 1 : K-1 split . What would happen if the classes follow a more realistic Zipf law , as observed in real NLP classification tasks ? I would assume that the merging process could still be applicable , but applied to groups of { S_k } with similar class-0/class-1 distributions . In section 4.1 , the trick to remove from the measure of the ECE classes where the predicted probability is less than 1/K also depends on a uniform 1/K prior . It should also be adapted to a non-uniform prior .", "rating": "7: Good paper, accept", "reply_text": "* * R1-3 ) * * * Prior work of using information measures , only mentioning the Info . bottleneck without references * Thank you for pointing the missing reference for information bottleneck ( IB ) proposed by Tishby et . al.1999.It is added in the revision . From the perspective of using information measures to drive HB or , in a broader sense , quantization , our method falls into the framework of IB . Often , the design goal of quantization is to minimize the reconstruction loss or information loss before and after quantization . The IB framework instead considers a third variable which represents the relevant information carried by the variable before quantization . The goal then becomes to retain the information relevant to the third variable after quantization . Note that , IB as a theoretic framework is not application-oriented . In the context of multi-class calibration , the label is the third variable in addition to the logit and quantized logit . We care about how well the label can be predicted from the quantized logit rather than how well the logit can be reproduced after quantization . So , I-Max focuses on easing the difficulty of predicting the label from the quantized logits , rather than reconstructing the ( input ) logit from it . We have also identified that the bin edges of HB play the determinant role in preserving the label information . As two common choices in the literature for post-hoc calibration , Eq.size , and Eq.mass HB were shown to be suboptimal . Our contribution lies in identifying and remedying the issues of the binning schemes at preserving the label information . In the revision , we add a section , i.e. , Appendix A3.2 , to discuss the connection with the IB ."}, "1": {"review_id": "AICNpd8ke-m-1", "review_text": "Update after the rebuttal : The authors have answered my concerns . I believe the paper should be accepted and would be a nice contribution to the current research . The paper proposes a novel approach for post-hoc calibration of outputs of the neural networks to estimate uncertainty of its prediction . The paper considers the histogram binning approach ( in contrast to scaling approaches existing in the literature ) and utilises the information theory in building bins . Strong points : * The work is very well placed in the context of the existing literature identifying the current gaps * Theoretically sound motivation of the approach * Extensive empirical evaluation Weak points : * Some discussion of the cost of the proposed method is lacking - i.e.how much in terms of computational time and memory this new calibration method is ? * The methods are compared with respect to accuracy and Expected Calibration Error ( ECE ) only . It has been shown that ECE is not a good metric for comparing different methods ( see , e.g.Ashukha , A. , Lyzhov , A. , Molchanov , D. and Vetrov , D. , 2020 . Pitfalls of in-domain uncertainty estimation and ensembling in deep learning . ICLR 2020 ) . I am recommending acceptance of the paper , though addressing the weak points above would largely improve the paper . The reasons for this decision is that strong points outweigh weak points : the proposed idea is interesting , it is shown that it is promising in practice ( subject to not very good metrics ) and the paper is mostly well written and easy to follow . Questions to authors : Could you please address raised weak points ? Additional feedback ( not necessarily important for evaluation , but could help to improve the paper ) : 1 . The part on shared class-wise binning is rather rushed in the main paper and it is not very clear . It is also rather independent contribution from the main I-Max calibration contribution . It would be better to somehow put them under one umbrella 2 . Section 2 . \u201c Bayesian DNNs , e.g . ( Blundell et al. , 2015 ) and their approximations ( Gal & Ghahramani , 2016 ) \u201d \u2013 a very arguable statement , I would suggest rephrasing it . Since Blundell et al.proposed variational inference which is also an approximation , and Gal & Ghahramani work is not an approximation of Blundell et al. \u2019 model 3 . Section 4.2 . \u201c Namely , matrix scaling w. L_2 \u201d dot after w is read as a full stop which is confusing 4 . After eq . ( 5 ) . `` So , we can solve the problem by iteratively and alternately updating $ { g_m } $ and $ { \\phi_m } $ based on ( A12 ) . '' - it seems eq.5 and A12 are the same and it would be more convenient to refer to eq.5 in the text right after it . 5.I am a little bit missing the overall procedure of the proposed calibration . I.e.all details are there ( especially if refer to appendix ) , but after reading the main paper , there is no feeling that I can now go and implement it for my problem . Maybe a pseudocode can help , or just step-by-step guidance Minor : Section 3 , first paragraph : \u201c Sec.3.2 ) \u201d \u2013 redundant bracket", "rating": "7: Good paper, accept", "reply_text": "* * R3-3 ) * * * Additional feedback on presentation * Thank you very much for this detailed feedback of corrections and suggestions . These points are integrated into the revision . In particular , we revised Sec.3.2 for a more detailed presentation of the class-wise calibration under the one-vs-rest strategy and the proposed sharing strategy , i.e. , sCW . We would also like to point out Appendix A2 where we discuss the sCW contribution in more detail and with empirical analysis . Yes , the sCW contribution is orthogonal to I-Max . As we have shown in Tab . 3 , it can also work with other methods based on one-vs-rest conversion , improving their calibration performance . We present both sCW and I-Max under the umbrella of class-wise calibration . sCW is a pragmatic technique for calibration set construction , addressing the class imbalance issue . As a binning scheme , I-Max is an outcome of maximizing the MI over the calibration set . sCW helps improve the sample efficiency at I-Max training ."}, "2": {"review_id": "AICNpd8ke-m-2", "review_text": "This paper highlights the issues with the scaling method and histogram binning i.e. , underestimate calibration error in scaling methods and failing to preserve classification accuracy , and sample-inefficiency in HB . They use the I-Max concept for binning , which maximizes the mutual information between labels and quantized logits . They claim that their approach mitigates potential loss in ranking performance and allows simultaneous improvement of ranking and calibration performance by disentangling the optimization of bin edges and representatives . They also propose a shared class-wise ( sCW ) strategy that fits a single calibrator on the merged training sets of all K class-wise problems to improve the sample efficiency . The paper is well written and the authors provide enough motivation and intuition of why maximizing the mutual information between labels and quantized logits would help multi-class calibration . There are some concerns and issues that I think needs to be addressed . 1- One approach in estimating uncertainty in classification is to choose a model and a regularized loss function to inherently learn a good representation . For example using confidence as a term for regularization in neural networks is proposed in Regularizing neural networks by penalizing confident output distributions ( ICLR 2017 ) that penalizes low-entropy output distributions . I think it is worth comparing the results with such existing work and discussing the advantages and disadvantages since a similar concept has been used one while training the model and this paper as a post-hoc calibration . 2- It is interesting that a single calibrator on the merged training sets of all K class-wise problems ( sCW ) performs well . As it is mentioned in the paper , it introduces bias due to having samples drawn from the other classes . In HB , increasing its number of evaluation bins reduces the bias , but in ( sCW ) such bias can not be controlled . Moreover , Figure A2 shows it achieves smaller JSDs which is not expected . Is there any reason for that ? What would happen if the number of bins is increased ? 3- Based on the experimental results , it seems I-Max performs better than other binning approaches . However , compared to the scaling methods it seems GP ( Wenger et al.2020 ) performs better at NLL/Brier than the I-Max variants . 4- Even though the paper shows combining I-Max with GP improves the ECE , it is not clear how the issues of each approach will be handled . For example , the ECE might be underestimated .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * R4-3 ) * * * NLL/Brier Comparisons * Yes , we have observed that GP performs better than the I-Max binning variants in terms of NLL/Brier , and provided an explanation in Appendix A10 ( formerly Appendix A9 ) . As it may be too brief there , we elaborate more on this observation . GP is trained by directly optimizing the NLL as its loss . As a non-parametric Bayesian method , GP has a larger model expressive capacity than binning . While achieving better NLL/Brier , it costs significantly more computational complexity and memory . In contrast , I-Max only relies on logic comparisons at test time . Among the binning schemes , I-Max w. GP achieves the best NLL/Brier across the datasets and models . It is noted that I-Max w. GP remains to be a binning scheme . So , the combination does not change the model capacity of I-Max . GP is only exploited during training to improve the optimization of I-Max \u2019 s bin representatives . Besides the low complexity benefit , I-Max w. GP as a binning scheme does not suffer from the ECE underestimation issue of scaling methods such as GP . We further note that as a cross-entropy measure between two distributions , the NLL would be an ideal metric for calibration evaluation . However , `` empirical '' NLL and Brier favor high accuracy and high confident classifiers , as each sample only having one hard label essentially implies the maximum confidence on a single class . For this reason , during training , the empirical NLL loss will keep pushing the prediction probability to one even after reaching $ 100 $ % training set accuracy . As a result , the trained classifier showed poor calibration performance at test time ( Guo et al. , 2017 ) . In contrast to NLL/Brier , empirical ECEs use hard labels differently . The ground truth correctness associated with the prediction confidence $ p $ is estimated by averaging over the hard labels of the samples receiving the prediction probability $ p $ or close to $ p $ . Due to averaging , the empirical ground truth correctness is usually not a hard label . Lastly , we use a small example to show the difference between the NLL/Brier and ECE : for $ N $ predictions , all assigned a confidence of $ 1.0 $ and containing $ M $ mistakes , the calibrated confidence is $ M/N < 1 $ . Unlike ECE , the NLL/Brier loss is only non-zero only for the $ M $ wrong predictions , despite all $ N $ predictions being miscalibrated . This example shows that NLL/Brier penalizes miscalibration far less than ECE . * * * $ $ * * R4-4 ) * * * Combination of I-Max with GP and How the issues of each approach will be handled . * The proposed combination of I-Max and GP aims at exploiting their complementary benefits . GP is a sample efficient scaling method for post-hoc calibration . However , it suffers from the issue of high complexity and ECE underestimation . On the contrary , I-Max has low complexity and its ECE estimation can converge to the ground truth ECE . To accurately set the bin representatives of the binning schemes , we often need a sufficient number of samples per bin . I-Max w. GP first sets the bin edges to bin the raw logits , which maximally preserve the label information . Then , it sets the bin representatives by averaging the GP-scaled prediction probabilities of the samples in each bin , exploiting the sample efficiency of GP . After setting the bin representatives , GP is no longer needed . At test time , I-Max w. GP remains to be a binning scheme , enjoying its low complexity and reliable ECE estimation . As a result of exploiting their complementary benefits ( and in essence also addressing the underlying issues of each approach ) , we observe in Tab 2 that I-Max w. GP is the top-performing at ECE , being better than GP or I-Max alone ."}}