{"year": "2019", "forum": "B1xJAsA5F7", "title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization", "decision": "Accept (Poster)", "meta_review": "The revisions made by the authors convinced the reviewers to all recommend accepting this paper. Therefore, I am recommending acceptance as well. I believe the revisions were important to make since I concur with several points in the initial reviews about additional baselines. It is all too easy to add confusion to the literature by not including enough experiments. ", "reviews": [{"review_id": "B1xJAsA5F7-0", "review_text": "Update: The score has been updated to reflect the authors' great efforts in improving the manuscript. This reviewer would suggest to accept the paper now. Old Review Below: The paper describes a graph-to-graph translation model for molecule optimization inspired from matched molecular pair analysis, which is an established approach for optimizing the properties of molecules. The model extends a chemistry-specific variational autoencoder architecture, and is assessed on a set of three benchmark tasks. While the idea of manuscript is interesting and promising for bioinformatics, there are several outstanding problems, which have to be addressed before it can be considered to be an acceptable submission. This referee is willing to adjust their rating if the raised points are addressed. Overall, the paper might also be more suited at a domain-specific bioinformatics conference. Most importantly, the paper makes several claims that are currently not backed up by experiments and/or data. First, the authors claim that MMPs \u201conly covers the most simple and common transformation patterns\u201d. This is not correct, since these MMP patterns can be as complex as desired. Also, it is claimed that the presented model is able to \u201clearn far more complex transformations than hard-coded rules\u201d. The authors will need to provide compelling evidence to back up these claims. At least, a comparison with a traditional MMPA method needs to be performed, and added as a baseline. Also, it has to be kept in mind that the reason MMPA was introduced was to provide an easily interpretable method, which performs only local transformations at one part of the molecule. \u201cFar more complex transformations\u201d may thus not be desirable in the context of MMPA. Can the authors comment on that? Second, the authors state that they \u201csidestep\u201d the problem of non-generalizing property predictors in reinforcement learning, by \u201cunifying graph generation and property estimation in one model\u201d. How does the authors\u2019 model not suffer from the same problem? Can they provide evidence that their model is better in property estimation than other models? In the first benchmark (logP) the GCPN baseline is shown, but in the second benchmark table, the GCPN baseline is missing. Why? The GCPN baseline will need to be added there. Can the authors also comment on how they ensure the comparison to the GPCN and VSeq2Seq is fair? Also, can the authors comment on why they think the penalized logP task is a good benchmark? Also, the authors write that Jin et al ICML 2018 (JTVAE) is a state of the model. However, also Liu et al NIPS 2018 (CGVAE) state that their model is state of the art. Unfortunately, both JTVAE and CGVAE were never compared against the strongest literature method so far, by Popova et al, which was evaluated on a much more challenging set of tasks than JT-VAE and CGVAE. The authors cite this paper but do not compare against it, which should to be rectified. This referee understands it is more compelling to invent new models, but currently, the literature of generative models for molecules is in a state of anarchy due to lack of solid comparison studies, which is not doing the community a great service. Furthermore, the training details are not described in enough detail. How exactly are the pairs selected? Where do the properties for the molecules come from? Were they calculated using the logP, QED and DRD2 models? How many molecules are used in total in each of these tasks? ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your insightful comments . Regarding your other comments and questions , our response is the following : 1 ) \u201c The authors claim that MMPs \u201c only covers the most simple and common transformation patterns \u201d . This is not correct , since these MMP patterns can be as complex as desired. \u201d We agree that MMP patterns can be as complex as desired . However , allowing the patterns to be arbitrarily complex will result in a huge number of transformation rules . For instance , we have extracted 12 million rules in total on the logP and QED tasks when no constraints are imposed . Therefore , we have updated this claim in the paper with the following statement : \u201c MMPA 's main drawback is that large numbers of rules have to be realized ( e.g.millions ) to cover all the complex transformation patterns. \u201d 2 ) \u201c the reason MMPA was introduced was to provide an easily interpretable method , which performs only local transformations at one part of the molecule . \u2018 Far more complex transformations \u2019 may thus not be desirable in the context of MMPA. \u201d Yes , we agree that there is always a trade-off between simple and understandable rules vs performance , and that the same trade-off is present in other machine learning applications ( e.g. , shallow decision trees vs neural networks ) . Our focus in this paper is on demonstrating the performance gains we can obtain by reformulating the task as a translation problem . Deriving interpretable explanations for the predictions is clearly an important future direction , but is orthogonal to our current effort . 3 ) \u201c The authors state that they \u201c sidestep \u201d the problem of non-generalizing property predictors in reinforcement learning \u2026 How does the authors \u2019 model not suffer from the same problem ? Can they provide evidence that their model is better in property estimation than other models ? \u201d We want to clarify that our model does not explicitly estimate the properties . As a result , we can only provide indirect evidence showing that our model can nevertheless outperform other models in mapping precursor molecules into the target set of molecules with better properties . 4 ) \u201c Can the authors also comment on how they ensure the comparison to the GCPN and VSeq2Seq is fair ? \u201d When comparing to VSeq2Seq , we ensure that all models have about the same number of parameters ( 3.8~3.9 million ) , trained on the same dataset with the same optimizer and the same number of epochs . Both models are evaluated with K=50 translation attempts for each test compound . Regarding GCPN , their exact setup is not provided . As described in their paper [ 4 ] , GCPN was trained in an environment whose initial state is one of the test set molecule of the logP task . They kept all the molecules generated during training and reported the molecule with the best logP improvement . We think this may bring more advantage to GCPN in our comparison , as our models do not have access to the test set . 5 ) \u201c Can the authors comment on why they think the penalized logP task is a good benchmark ? \u201d We evaluated on this task because some prior work ( e.g.JT-VAE , GCPN ) has been tested on this benchmark , and their results are readily available for comparison . Indeed , this benchmark itself is not comprehensive enough . We therefore tested on two more tasks ( QED and DRD2 ) aiming to provide a more thorough evaluation . 6 ) \u201c How exactly are the pairs selected ? Where do the properties for the molecules come from ? Were they calculated using the logP , QED and DRD2 models ? How many molecules are used \u2026 ? \u201d Those details have been discussed in the Appendix B . We updated the relevant paragraphs to make it more clear . To summarize , logP and QED scores are calculated with RDKit built-in functions . For DRD2 activity prediction , we directly used the pre-trained model in Olivecrona et al . [ 3 ] .On the QED and DRD2 tasks , a molecular pair ( X , Y ) is selected if the Tanimoto similarity sim ( X , Y ) > = 0.4 and both X and Y fall into the source and target property range . On the logP task , we select molecular pairs when similarity sim ( X , Y ) > = delta and property improvement is greater than 0.5 ( if delta=0.6 ) and 2.5 ( if delta=0.4 ) . In total 250K molecules are used for constructing the training pairs in the logP and QED tasks , and 350K molecules in the DRD2 task . References [ 1 ] A. Dalke , J. Hert , C. Kramer . mmpdb : An Open-Source Matched Molecular Pair Platform for Large Multiproperty Data Sets . J . Chem.Inf.Model. , 2018 , 58 ( 5 ) , pp 902\u2013910 . [ 2 ] M. Popova , O. Isayev , and A. Tropsha . Deep reinforcement learning for de novo drug design . Science advances , 4 ( 7 ) : eaap7885 , 2018 . [ 3 ] M. Olivecrona , T. Blaschke , O. Engkvist , and H. Chen . Molecular de-novo design through deep reinforcement learning . Journal of cheminformatics , 9 ( 1 ) :48 , 2017 . [ 4 ] J.You , B. Liu , R. Ying , V. Pande , and J. Leskovec . Graph convolutional policy network for goal-directed molecular graph generation . arXiv preprint arXiv:1806.02473 , 2018"}, {"review_id": "B1xJAsA5F7-1", "review_text": "This paper proposed an extension of JT-VAE [1] into the graph to graph translation scenario. To help make the translation model predicting diverse and valid outcomes, the author added the latent variable to capture the multi-modality, and an adversarial regularization in the latent space. Experiment on molecule translation tasks show significant improvement over existing methods. The paper is well written. The author explains the GNN, JT-VAE and GAN in a very organized way. The idea of modeling the molecule optimization as translation problem is interesting, and sounds more promising (and could be easier) than finding promising molecule from scratch. Technically I think it is reasonable to use latent variable model to handle the multi-modality. Using GAN to align the distribution is also a well adapted method recently. Thus overall the method is not too surprising to me, but the paper executes it nicely. Given the significant empirical improvement, I think this paper has made a valid contribution to the area. Regarding the results in Table 1, I\u2019m curious why the VSeq2Seq is better than JT-VAE and GCPN (given the latter two are the current state-of-the-art)? Another thing I\u2019m curious about is the \u2018stacking\u2019 of this translation model. Suppose we keep translating the molecule X1 -> X2 -> X3 ... using the learned translation model, would the model still gets improvement after X2? When would it get maxed out? Or if we train with \u2018path\u2019 translation (i.e., train with improvement path with variable length), instead of just the pair translation, would that be helpful? I\u2019m not asking for more experiments, but some discussion might be useful. [1] Jin et.al, Junction tree variational autoencoder for molecular graph generation, ICML 2018 ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your insightful comments . 1 ) Why VSeq2Seq is better than JT-VAE and GCPN ? The main reason is that VSeq2Seq is trained with direct translation pairs through supervised learning , while JT-VAE and GCPN have to learn to discover these pairs in a weakly supervised manner . For instance , GCPN iteratively modifies a given molecule to maximize the predicted property score , where the translation pairs are discovered through reinforcement learning . JT-VAE optimizes a molecule by first mapping it into its latent representation and then performing gradient ascent in the latent space . In this case , translation pairs are discovered through the gradient signal given by the property predictor , which is trained on molecules with labeled properties . As the models are evaluated by translation quality , training the model directly with translation pairs is advantageous . 2 ) Suppose we keep translating the molecule X1 - > X2 - > X3 ... using the learned translation model , would the model still get improvement after X2 ? When would it get maxed out ? On the logP task , the model may still get improvements after X2 , but we suspect this process will get maxed out after several steps because in general it is harder to optimize a molecule with high property scores . The QED and DRD2 tasks are different from logP task , as the target domain now becomes a closed set defined by the property range . As long as X2 belongs to the target domain ( e.g. , QED > = 0.9 , DRD2 > = 0.5 ) , this process will get maxed out since the model is trained only to improve molecules outside of the target domain . 3 ) If we train with \u2018 path \u2019 translation ( i.e. , train with improvement path with variable length ) , instead of just the pair translation , would that be helpful ? In general , it is harder to collect \u2018 path \u2019 translation data than translation pairs due to data sparsity . For instance , to find a translation path X1 - > X2 - > X3 , we need ( X1 , X2 ) and ( X2 , X3 ) to be valid translation pairs ( i.e. , both pairs satisfying property improvement and similarity constraints ) . Nonetheless , we believe that training the model with path translation will be helpful for global optimization -- finding molecules with the best property scores in the entire molecular space ."}, {"review_id": "B1xJAsA5F7-2", "review_text": "As a reviewer I am expert in learning in structured data domains. The paper proposes a quite complex system, involving many different choices and components, for obtaining chemical compounds with improved properties starting from a given corpora. Overall presentation is good, although some details/explanations/motivations are missing. I guess this was due to the need to keep the description of a quite complex system in the given space limit. Such details/explanations/motivations could, however, have been inserted in the appendix. As an example, let consider the description of the decoding of the junction tree. In that section, it is not explained when the decoding process stops. My understanding is that this is when, being in the root node, the choice is to go back to the parent (that does not exist). In the same section, it is not explicitly discussed that the probability to select between adding a node or going back to the parent should have a different distribution according to \"how many\" nodes have been generated before, i.e. we do not want to have a high probability to \"go back\" at the beginning of the decoding, while I guess it is desirable that such probability increases proportionally with the number of generated nodes. This leads to an issue that I personally think is important: the paper does lack an explicit probabilistic modelling of the different involved components, which may help for a better understanding of all the assumptions made in the construction of the proposed system. The complexity of the proposed system is actually an issue since the author(s) do not attempt (except for the presence or absence of the adversarial scaffold regularization and the number of trials in appendix) an analysis of the influence of the different components (and corresponding hyper-parameters). Reference to previous relevant work seems to be complete. I think the paper is relevant for ICLR (although there is no explicit analysis of the obtained hidden representations) and of interest for a good portion of attendees. Minor issues: - Tree and Graph Encoding: asynchronous update implies that T should be a multiple of the diameter of the input graph to guarantee a proper propagation of information across the graph. A discussion about that would be needed. - eq.(6): \\mathbb{u}^d is not defined. - Section 3.3: - first paragraph is not clear. An example and/or figure is needed to understand the argument, which is related to the presence of cycles. - the definition of f(G_i) involves \\mathbb{x}_u. I guess they should be \\mathbb{x}_u^G. - not clear how the log-likelihood of ground truth subgraphs is computed given that the predicted junction tree, especially at the beginning of training, may be way different from the correct one. Moreover, what is the assumed bias of this choice ? - Table I: please provide an explanation of why using a larger value for \\delta does provide worst performance than a smaller value. From an optimisation point of view it should provide at least an as good performance. This is a clear indication that the used procedure is suboptimal. - diversity could be influenced by the cardinality of the sample. Is this false ? please discuss why diversity is (not) biased versus larger sets.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your insightful comments . Our response to the issues you mentioned is the following : 1 ) Please provide an explanation of why using a larger value for delta gives worse performance than a smaller value . A larger delta implies a tighter similarity constraint . For instance , setting delta to 0.6 means the generated compounds Y have to be very similar to the input molecule X ( sim ( X , Y ) > 0.6 ) . When delta decreases to 0.4 , the generated structures are allowed to deviate more from the starting point X ( sim ( X , Y ) > 0.4 ) . Therefore , one would naturally expect the model to perform better ( find higher scoring molecules ) when delta is smaller since the structures can be chosen from a larger set . 2 ) Diversity could be influenced by the cardinality of the sample . Please discuss why diversity is ( not ) biased versus larger sets . We agree that the diversity depends on the sample size . Therefore , all the models are evaluated with the same sample size ( K=50 ) for fair comparison . That is , for each molecule in the test set , we randomly sample 50 times from each model to compute the resulting diversity score . 3 ) Tree and graph encoding : asynchronous update implies that T should be a multiple of the diameter of the input graph to guarantee a proper propagation of information across the graph . We agree that a number of iterations ( T ) is required for proper propagation of information across the input graph . However , T does not need to be larger than the diameter since we adopted an attention mechanism in the decoder . It can dynamically read the information across the input graph in different decoding steps . In fact , a large T ( e.g. , the diameter ) may potentially lead to overfitting . 4 ) Clarification of tree decoding step ( Section 3.2 ) First , the tree decoding process stops when it choose to backtrack at the root node . Second , we agree that this probability should depend on the number of nodes having been generated . This is implicitly captured by the neural message passing procedure . As noted in Eq . ( 4 ) , the model makes this decision ( expanding a new node or not ) based on all the incoming messages at the current node . The messages carry information about the current ( partial ) tree structure , including potentially the number of nodes generated so far though not explicitly . 5 ) Explanation of graph decoding step ( Section 3.3 ) We added Figure 2 to illustrate why the graph decoding step is not deterministic and how one junction tree can be decoded into different molecular graphs . Regarding the likelihood of ground truth subgraphs , we applied teacher forcing , i.e. , we feed the graph decoder with ground truth junction trees as input . Section 3.3 has been updated correspondingly ."}], "0": {"review_id": "B1xJAsA5F7-0", "review_text": "Update: The score has been updated to reflect the authors' great efforts in improving the manuscript. This reviewer would suggest to accept the paper now. Old Review Below: The paper describes a graph-to-graph translation model for molecule optimization inspired from matched molecular pair analysis, which is an established approach for optimizing the properties of molecules. The model extends a chemistry-specific variational autoencoder architecture, and is assessed on a set of three benchmark tasks. While the idea of manuscript is interesting and promising for bioinformatics, there are several outstanding problems, which have to be addressed before it can be considered to be an acceptable submission. This referee is willing to adjust their rating if the raised points are addressed. Overall, the paper might also be more suited at a domain-specific bioinformatics conference. Most importantly, the paper makes several claims that are currently not backed up by experiments and/or data. First, the authors claim that MMPs \u201conly covers the most simple and common transformation patterns\u201d. This is not correct, since these MMP patterns can be as complex as desired. Also, it is claimed that the presented model is able to \u201clearn far more complex transformations than hard-coded rules\u201d. The authors will need to provide compelling evidence to back up these claims. At least, a comparison with a traditional MMPA method needs to be performed, and added as a baseline. Also, it has to be kept in mind that the reason MMPA was introduced was to provide an easily interpretable method, which performs only local transformations at one part of the molecule. \u201cFar more complex transformations\u201d may thus not be desirable in the context of MMPA. Can the authors comment on that? Second, the authors state that they \u201csidestep\u201d the problem of non-generalizing property predictors in reinforcement learning, by \u201cunifying graph generation and property estimation in one model\u201d. How does the authors\u2019 model not suffer from the same problem? Can they provide evidence that their model is better in property estimation than other models? In the first benchmark (logP) the GCPN baseline is shown, but in the second benchmark table, the GCPN baseline is missing. Why? The GCPN baseline will need to be added there. Can the authors also comment on how they ensure the comparison to the GPCN and VSeq2Seq is fair? Also, can the authors comment on why they think the penalized logP task is a good benchmark? Also, the authors write that Jin et al ICML 2018 (JTVAE) is a state of the model. However, also Liu et al NIPS 2018 (CGVAE) state that their model is state of the art. Unfortunately, both JTVAE and CGVAE were never compared against the strongest literature method so far, by Popova et al, which was evaluated on a much more challenging set of tasks than JT-VAE and CGVAE. The authors cite this paper but do not compare against it, which should to be rectified. This referee understands it is more compelling to invent new models, but currently, the literature of generative models for molecules is in a state of anarchy due to lack of solid comparison studies, which is not doing the community a great service. Furthermore, the training details are not described in enough detail. How exactly are the pairs selected? Where do the properties for the molecules come from? Were they calculated using the logP, QED and DRD2 models? How many molecules are used in total in each of these tasks? ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your insightful comments . Regarding your other comments and questions , our response is the following : 1 ) \u201c The authors claim that MMPs \u201c only covers the most simple and common transformation patterns \u201d . This is not correct , since these MMP patterns can be as complex as desired. \u201d We agree that MMP patterns can be as complex as desired . However , allowing the patterns to be arbitrarily complex will result in a huge number of transformation rules . For instance , we have extracted 12 million rules in total on the logP and QED tasks when no constraints are imposed . Therefore , we have updated this claim in the paper with the following statement : \u201c MMPA 's main drawback is that large numbers of rules have to be realized ( e.g.millions ) to cover all the complex transformation patterns. \u201d 2 ) \u201c the reason MMPA was introduced was to provide an easily interpretable method , which performs only local transformations at one part of the molecule . \u2018 Far more complex transformations \u2019 may thus not be desirable in the context of MMPA. \u201d Yes , we agree that there is always a trade-off between simple and understandable rules vs performance , and that the same trade-off is present in other machine learning applications ( e.g. , shallow decision trees vs neural networks ) . Our focus in this paper is on demonstrating the performance gains we can obtain by reformulating the task as a translation problem . Deriving interpretable explanations for the predictions is clearly an important future direction , but is orthogonal to our current effort . 3 ) \u201c The authors state that they \u201c sidestep \u201d the problem of non-generalizing property predictors in reinforcement learning \u2026 How does the authors \u2019 model not suffer from the same problem ? Can they provide evidence that their model is better in property estimation than other models ? \u201d We want to clarify that our model does not explicitly estimate the properties . As a result , we can only provide indirect evidence showing that our model can nevertheless outperform other models in mapping precursor molecules into the target set of molecules with better properties . 4 ) \u201c Can the authors also comment on how they ensure the comparison to the GCPN and VSeq2Seq is fair ? \u201d When comparing to VSeq2Seq , we ensure that all models have about the same number of parameters ( 3.8~3.9 million ) , trained on the same dataset with the same optimizer and the same number of epochs . Both models are evaluated with K=50 translation attempts for each test compound . Regarding GCPN , their exact setup is not provided . As described in their paper [ 4 ] , GCPN was trained in an environment whose initial state is one of the test set molecule of the logP task . They kept all the molecules generated during training and reported the molecule with the best logP improvement . We think this may bring more advantage to GCPN in our comparison , as our models do not have access to the test set . 5 ) \u201c Can the authors comment on why they think the penalized logP task is a good benchmark ? \u201d We evaluated on this task because some prior work ( e.g.JT-VAE , GCPN ) has been tested on this benchmark , and their results are readily available for comparison . Indeed , this benchmark itself is not comprehensive enough . We therefore tested on two more tasks ( QED and DRD2 ) aiming to provide a more thorough evaluation . 6 ) \u201c How exactly are the pairs selected ? Where do the properties for the molecules come from ? Were they calculated using the logP , QED and DRD2 models ? How many molecules are used \u2026 ? \u201d Those details have been discussed in the Appendix B . We updated the relevant paragraphs to make it more clear . To summarize , logP and QED scores are calculated with RDKit built-in functions . For DRD2 activity prediction , we directly used the pre-trained model in Olivecrona et al . [ 3 ] .On the QED and DRD2 tasks , a molecular pair ( X , Y ) is selected if the Tanimoto similarity sim ( X , Y ) > = 0.4 and both X and Y fall into the source and target property range . On the logP task , we select molecular pairs when similarity sim ( X , Y ) > = delta and property improvement is greater than 0.5 ( if delta=0.6 ) and 2.5 ( if delta=0.4 ) . In total 250K molecules are used for constructing the training pairs in the logP and QED tasks , and 350K molecules in the DRD2 task . References [ 1 ] A. Dalke , J. Hert , C. Kramer . mmpdb : An Open-Source Matched Molecular Pair Platform for Large Multiproperty Data Sets . J . Chem.Inf.Model. , 2018 , 58 ( 5 ) , pp 902\u2013910 . [ 2 ] M. Popova , O. Isayev , and A. Tropsha . Deep reinforcement learning for de novo drug design . Science advances , 4 ( 7 ) : eaap7885 , 2018 . [ 3 ] M. Olivecrona , T. Blaschke , O. Engkvist , and H. Chen . Molecular de-novo design through deep reinforcement learning . Journal of cheminformatics , 9 ( 1 ) :48 , 2017 . [ 4 ] J.You , B. Liu , R. Ying , V. Pande , and J. Leskovec . Graph convolutional policy network for goal-directed molecular graph generation . arXiv preprint arXiv:1806.02473 , 2018"}, "1": {"review_id": "B1xJAsA5F7-1", "review_text": "This paper proposed an extension of JT-VAE [1] into the graph to graph translation scenario. To help make the translation model predicting diverse and valid outcomes, the author added the latent variable to capture the multi-modality, and an adversarial regularization in the latent space. Experiment on molecule translation tasks show significant improvement over existing methods. The paper is well written. The author explains the GNN, JT-VAE and GAN in a very organized way. The idea of modeling the molecule optimization as translation problem is interesting, and sounds more promising (and could be easier) than finding promising molecule from scratch. Technically I think it is reasonable to use latent variable model to handle the multi-modality. Using GAN to align the distribution is also a well adapted method recently. Thus overall the method is not too surprising to me, but the paper executes it nicely. Given the significant empirical improvement, I think this paper has made a valid contribution to the area. Regarding the results in Table 1, I\u2019m curious why the VSeq2Seq is better than JT-VAE and GCPN (given the latter two are the current state-of-the-art)? Another thing I\u2019m curious about is the \u2018stacking\u2019 of this translation model. Suppose we keep translating the molecule X1 -> X2 -> X3 ... using the learned translation model, would the model still gets improvement after X2? When would it get maxed out? Or if we train with \u2018path\u2019 translation (i.e., train with improvement path with variable length), instead of just the pair translation, would that be helpful? I\u2019m not asking for more experiments, but some discussion might be useful. [1] Jin et.al, Junction tree variational autoencoder for molecular graph generation, ICML 2018 ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your insightful comments . 1 ) Why VSeq2Seq is better than JT-VAE and GCPN ? The main reason is that VSeq2Seq is trained with direct translation pairs through supervised learning , while JT-VAE and GCPN have to learn to discover these pairs in a weakly supervised manner . For instance , GCPN iteratively modifies a given molecule to maximize the predicted property score , where the translation pairs are discovered through reinforcement learning . JT-VAE optimizes a molecule by first mapping it into its latent representation and then performing gradient ascent in the latent space . In this case , translation pairs are discovered through the gradient signal given by the property predictor , which is trained on molecules with labeled properties . As the models are evaluated by translation quality , training the model directly with translation pairs is advantageous . 2 ) Suppose we keep translating the molecule X1 - > X2 - > X3 ... using the learned translation model , would the model still get improvement after X2 ? When would it get maxed out ? On the logP task , the model may still get improvements after X2 , but we suspect this process will get maxed out after several steps because in general it is harder to optimize a molecule with high property scores . The QED and DRD2 tasks are different from logP task , as the target domain now becomes a closed set defined by the property range . As long as X2 belongs to the target domain ( e.g. , QED > = 0.9 , DRD2 > = 0.5 ) , this process will get maxed out since the model is trained only to improve molecules outside of the target domain . 3 ) If we train with \u2018 path \u2019 translation ( i.e. , train with improvement path with variable length ) , instead of just the pair translation , would that be helpful ? In general , it is harder to collect \u2018 path \u2019 translation data than translation pairs due to data sparsity . For instance , to find a translation path X1 - > X2 - > X3 , we need ( X1 , X2 ) and ( X2 , X3 ) to be valid translation pairs ( i.e. , both pairs satisfying property improvement and similarity constraints ) . Nonetheless , we believe that training the model with path translation will be helpful for global optimization -- finding molecules with the best property scores in the entire molecular space ."}, "2": {"review_id": "B1xJAsA5F7-2", "review_text": "As a reviewer I am expert in learning in structured data domains. The paper proposes a quite complex system, involving many different choices and components, for obtaining chemical compounds with improved properties starting from a given corpora. Overall presentation is good, although some details/explanations/motivations are missing. I guess this was due to the need to keep the description of a quite complex system in the given space limit. Such details/explanations/motivations could, however, have been inserted in the appendix. As an example, let consider the description of the decoding of the junction tree. In that section, it is not explained when the decoding process stops. My understanding is that this is when, being in the root node, the choice is to go back to the parent (that does not exist). In the same section, it is not explicitly discussed that the probability to select between adding a node or going back to the parent should have a different distribution according to \"how many\" nodes have been generated before, i.e. we do not want to have a high probability to \"go back\" at the beginning of the decoding, while I guess it is desirable that such probability increases proportionally with the number of generated nodes. This leads to an issue that I personally think is important: the paper does lack an explicit probabilistic modelling of the different involved components, which may help for a better understanding of all the assumptions made in the construction of the proposed system. The complexity of the proposed system is actually an issue since the author(s) do not attempt (except for the presence or absence of the adversarial scaffold regularization and the number of trials in appendix) an analysis of the influence of the different components (and corresponding hyper-parameters). Reference to previous relevant work seems to be complete. I think the paper is relevant for ICLR (although there is no explicit analysis of the obtained hidden representations) and of interest for a good portion of attendees. Minor issues: - Tree and Graph Encoding: asynchronous update implies that T should be a multiple of the diameter of the input graph to guarantee a proper propagation of information across the graph. A discussion about that would be needed. - eq.(6): \\mathbb{u}^d is not defined. - Section 3.3: - first paragraph is not clear. An example and/or figure is needed to understand the argument, which is related to the presence of cycles. - the definition of f(G_i) involves \\mathbb{x}_u. I guess they should be \\mathbb{x}_u^G. - not clear how the log-likelihood of ground truth subgraphs is computed given that the predicted junction tree, especially at the beginning of training, may be way different from the correct one. Moreover, what is the assumed bias of this choice ? - Table I: please provide an explanation of why using a larger value for \\delta does provide worst performance than a smaller value. From an optimisation point of view it should provide at least an as good performance. This is a clear indication that the used procedure is suboptimal. - diversity could be influenced by the cardinality of the sample. Is this false ? please discuss why diversity is (not) biased versus larger sets.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your insightful comments . Our response to the issues you mentioned is the following : 1 ) Please provide an explanation of why using a larger value for delta gives worse performance than a smaller value . A larger delta implies a tighter similarity constraint . For instance , setting delta to 0.6 means the generated compounds Y have to be very similar to the input molecule X ( sim ( X , Y ) > 0.6 ) . When delta decreases to 0.4 , the generated structures are allowed to deviate more from the starting point X ( sim ( X , Y ) > 0.4 ) . Therefore , one would naturally expect the model to perform better ( find higher scoring molecules ) when delta is smaller since the structures can be chosen from a larger set . 2 ) Diversity could be influenced by the cardinality of the sample . Please discuss why diversity is ( not ) biased versus larger sets . We agree that the diversity depends on the sample size . Therefore , all the models are evaluated with the same sample size ( K=50 ) for fair comparison . That is , for each molecule in the test set , we randomly sample 50 times from each model to compute the resulting diversity score . 3 ) Tree and graph encoding : asynchronous update implies that T should be a multiple of the diameter of the input graph to guarantee a proper propagation of information across the graph . We agree that a number of iterations ( T ) is required for proper propagation of information across the input graph . However , T does not need to be larger than the diameter since we adopted an attention mechanism in the decoder . It can dynamically read the information across the input graph in different decoding steps . In fact , a large T ( e.g. , the diameter ) may potentially lead to overfitting . 4 ) Clarification of tree decoding step ( Section 3.2 ) First , the tree decoding process stops when it choose to backtrack at the root node . Second , we agree that this probability should depend on the number of nodes having been generated . This is implicitly captured by the neural message passing procedure . As noted in Eq . ( 4 ) , the model makes this decision ( expanding a new node or not ) based on all the incoming messages at the current node . The messages carry information about the current ( partial ) tree structure , including potentially the number of nodes generated so far though not explicitly . 5 ) Explanation of graph decoding step ( Section 3.3 ) We added Figure 2 to illustrate why the graph decoding step is not deterministic and how one junction tree can be decoded into different molecular graphs . Regarding the likelihood of ground truth subgraphs , we applied teacher forcing , i.e. , we feed the graph decoder with ground truth junction trees as input . Section 3.3 has been updated correspondingly ."}}