{"year": "2020", "forum": "HyeJf1HKvS", "title": "Deep Graph Matching Consensus", "decision": "Accept (Poster)", "meta_review": "The paper proposed an end-to-end network architecture for graph matching problems, where first a GNN is applied to compute the initial soft correspondence, and then a message passing network is applied to attempt to resolve structural mismatch. The reviewers agree that the second component (message passing) is novel, and after the rebuttal period, additional experiments were provided by the authors to demonstrate the effectiveness of this. Overall this is an interesting network solution for graph-matching, and would be a worthwhile addition to the literature.", "reviews": [{"review_id": "HyeJf1HKvS-0", "review_text": "This paper proposes a two-stage GNN-based architecture to establish correspondences between two graphs. The first step is to learn node embeddings using a GNN to obtain soft node correspondences between two graphs. The second step is to iteratively refine them using the constraints of matching consensus in local neighborhoods between graphs. The overall refining process resembles the classic graph matching algorithm of graduated assignment (Gold & Rangarajan, 1996), but generalizes it using deep neural representation. Experiments show that the proposed algorithm performs well on real-world tasks of image matching and knowledge graph entity alignment. The paper is interesting and has some good potential but lacks some important evaluations and analyses. My main concerns are as follows. 1) The consensus in the second stage is crucial? As the title shows, the main technical contribution lies in the second stage of consensus inducing. But, for the real tasks, in the experiments, the gain by the second stage is not significant or often negligible (L=0 vs. L=10 or 20 in Table 1,2,3). The results of the first stage (L=0) already give better results than all the baselines in many cases, so that most gains appear to come from the usage of GNNs for representation. This makes the major contribution of this work less significant. I hope the authors justify this. And, I guess that's maybe because the consensus information may also be induced in the first stage by matching nodes with relational presentations learned using a GNN. To see this, the authors may run the second stage only without the first stage. 2) Comparison to the graduated assignment (GA) process As discussed in 3.3, the proposed neighborhood consensus can be viewed as a generalization of GA of Eq.6 with trainable neural modules. But, it's not actually shown what is the gain by this generalization. This needs to be shown experimentally by substituting the second stage by GA process. 3) Robustness to node addition or removal. All the experiments look assuming only edges are varied. Is this algorithm robust to node addition or removal, occurring in many practical graph matching problems? This needs to be also discussed. ====================================== The rebuttal succeeds in addressing most of my concerns so that I upgrade my initial rating to weak accept. I hope all the points in the rebuttal are included in the final manuscript. ", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer , thanks a lot for reviewing our paper and providing valuable comments . We are working on incorporating your feedback in a new revision of our paper . We would like to provide more explanations to address your concerns . Main Contribution ================ We emphasize that the main contribution of our work lies indeed in the second stage of our architecture , i.e. , refining initial soft correspondences using a trainable message passing scheme that reaches for neighborhood consensus . Our approach allows us to not only distribute local information , but to also distribute global information in the form of node indicator functions/node colorings using purely local operators . The distribution of global information is then used to resolve ambiguities/false matchings made in the first stage of our architecture . In addition , we proposed optimizations to make the consensus stage scale to large real-world instances . We agree that obtaining initial soft correspondences via similarity scores of node embeddings is not a novel contribution and should n't be viewed as one . We will make this more clear in a revised version . We performed additional experiments to emphasize the usefulness of our consensus stage ( see below ) . Impact of the Consensus Stage =========================== We argue that our consensus stage has a huge impact on the resulting performance of our model . For example , on the WILLOW-ObjectClass dataset , it at least reduces the error of the initial model ( $ L=0 $ ) by half across all categories . On the DBP15K dataset , it consistently improves the model 's performance by 4 percentage points on average which we claim to be highly significant . We nonetheless agree with Reviewer # 1 that those improvements should be more significant when using weaker baselines . To verify this , we conducted additional experiments as suggested where we replaced the first stage GNN module with a weaker MLP . The results are shown below which we will include in the final manuscript . Hits @ 1 on the WILLOW-ObjectClass dataset : -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + $ \\Psi_ { \\theta_1 } $ = MLP | Motorbike | Car | Duck | Winebottle | -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + $ L=0 $ | 56.85 \u00b1 2.65 | 73.44 \u00b1 2.65 | 71.93 \u00b1 2.10 | 86.10 \u00b1 1.25 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + isotropic $ L=10 $ | 80.34 \u00b1 2.34 | 81.31 \u00b1 2.48 | 81.16 \u00b1 2.55 | 93.53 \u00b1 1.38 | isotropic $ L=20 $ | 82.24 \u00b1 3.06 | 82.49 \u00b1 3.70 | 81.84 \u00b1 2.92 | 95.14 \u00b1 1.58 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- Here , the consensus stage improves the initial matchings significantly , with nearly up to 40 percentage points improvements on the Motorbike class . However , it is worth noting that good initial matchings do help the consensus stage to improve its performance further , which stresses the importance of our two-stage approach . Furthermore , starting from weak initial matchings takes significantly more refinement steps to converge ( as can be seen by the difference between $ L=10 $ and $ L=20 $ ) . It should be noted that the first stage can not infer any information about consensus . In order to check for consensus , an initial matching is needed that can be tested . The first stage siamese network has no information flow between both heads until the feature matching . The second stage can rerank those hypotheses based on additional information : matching agreement in neighborhoods . It can not be applied without an initial ranking of correspondences ."}, {"review_id": "HyeJf1HKvS-1", "review_text": "The authors proposed a message passing neural network-based graph matching methods. The overall framework can be viewed as a graph siamese network, where two set of points are passing through the same graph neural network, and then two new embeddings are generated. Using the two embedding the similarity between points can be computed and then the final matching can be generated. The overall structure of this paper is similar to [1] and [2], the authors should discuss the difference of the proposed with these two papers, if it is possible, the authors may try to compare with these two methods in experiments. Currently, I think the main contribution of the paper should be the new message-passing scheme (in Sec. 3.2). However, from the current experiment, I can not see if the performance improvement is from the new message-passing scheme. In fact, the message passing scheme is also related to the dual decomposition framework, which is previously used in the graph matching area. For example, in [3], a message-passing algorithm derived from dual decomposition is used to solve the graph matching problem. The authors may also consider add some discussion the difference between message-passing derived from dual decomposition and the message passing in the graph neural network. ==================================== After Revision ============================================== In the new experiment, the authors proved that the new message-passing scheme (i.e. the consensus stage) in Sec 3.2 can successfully improve the performance by refining original assignment. Thus I modify the score to weak accept. [1] Wang, Yue, and Justin M. Solomon. \"Deep Closest Point: Learning Representations for Point Cloud Registration.\", ICCV 2019, [2] Zhen Zhang, and Wee Sun Lee. \"Deep Graphical Feature Learning for the Feature Matching Problem.\", ICCV 2019 [3] Paul Swoboda et. al. \"A study of Lagrangean decompositions and dual ascent solvers for graph matching.\", CVPR 2017", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer , thanks a lot for reviewing our paper and providing valuable comments . We are working on incorporating your feedback in a new revision of our paper . We would like to provide more explanations to address your concerns . Main Contribution ================ We emphasize that the main contribution of our work lies indeed in the second stage of our architecture , i.e. , refining initial soft correspondences using a trainable message passing scheme that reaches for neighborhood consensus . Our approach allows us to not only distribute local information , but to also distribute global information in the form of node indicator functions/node colorings using purely local operators . The distribution of global information is then used to resolve ambiguities/false matchings made in the first stage of our architecture . In addition , we proposed optimizations to make the consensus stage scale to large real-world instances . We agree that obtaining initial soft correspondences via similarity scores of node embeddings is not a novel contribution and should n't be viewed as one . We will make this more clear in a revised version . In addition , we argue that our consensus stage has a huge impact on the resulting performance of our model . For example , on the WILLOW-ObjectClass dataset , it at least reduces the error of the initial model ( $ L=0 $ ) by half across all categories . On the DBP15K dataset , it consistently improves the model 's performance by 4 percentage points on average which we claim to be highly significant . We performed additional experiments as suggested by Reviewer # 1 to emphasize the usefulness of our consensus stage ( please see the response to Review # 1 for more details ) . Relation to [ 1 ] Wang and Solomon : Deep Closest Point : Learning Representations for Point Cloud Registration ( ICCV'19 ) ====================================================================================================== This work tackles the problem of finding an unknown rigid motion between point clouds by first matching points followed by a differentiable SVD module . We agree that this work tackles the feature matching procedure in a similar fashion as we do in our initial feature matching procedure based on inner product similarity scores . Additionally , this work leverages a Transformer module to let point clouds know about each other before feature matching takes place . Our work differs in that we introduce a consensus stage to resolve ambiguities in matchings after the initial matching procedure based on neighborhood consensus . Hence , our method could be used to improve the results of [ 1 ] further . In order to resolve ambiguities , [ 1 ] can only rely on the least squares optimization , inherentely utilizing the rigid embedding of the point cloud in $ \\mathbb { R } ^3 $ , which does not exist for general graphs and is not required for our approach . In addition , our approach is highly scalable due to only operating an local neighborhoods , while the Transformer module operates on the whole point cloud in a global fashion . Due to the different task and difference in assumptions , we do refrain from an in-depth experimental comparison . We will nonetheless discuss the similarities/differences to this work further in our related work . [ 1 ] Wang and Solomon : Deep Closest Point : Learning Representations for Point Cloud Registration ( ICCV'19 )"}, {"review_id": "HyeJf1HKvS-2", "review_text": "This paper suggests a framework for answering graph matching questions consisting of local node embeddings with a message passing refinement step. The paper has well written text, offers what appear to be nice experiments validating the method and discusses its own limitations. I am giving a weak accept. I the weak accept is my reflection of my inability to provide useful feedback. This is also out of domain for me and I am not a useful reviewer for this paper. As a general comment, I will say that the paper adopts a highly mathematical style that will be off putting to many readers. Many of the expressions are 'high entropy' For instance, the embedding network is given as $\\mathbf{\\Psi}_{\\theta_1}$ throughout. This is seven levels of typographical distinction for the main character of the story. This is a (1) bold (2) capital (3) Greek letter with a (4) subscript that is (5) greek with a (6) subscript that is (7) numeric. I understand that each of these levels of distinction has a purpose and a meaning, but it is also arguably a much richer designation that is necessary. Generally the paper feels like this, as if its being too unnecessarily specific. Personally I found the paper rather difficult to read and decompose, which I believe does count against the paper. The overly specific nature of paper will cut into its potential readership strongly.", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer , thanks a lot for reviewing our paper and for the positive feedback . To precisely describe our architecture with the theorems and their proofs , we opt for introducing a rigorous mathematical notation . In addition , we wanted to describe our procedure as general as possible . For example , $ \\Psi_ { \\theta } $ is not limited to be a particular GNN instance but can be any trainable architecture outputting node embeddings . We will strengthen the intuitive explanations and simplify notation in a revised version ."}], "0": {"review_id": "HyeJf1HKvS-0", "review_text": "This paper proposes a two-stage GNN-based architecture to establish correspondences between two graphs. The first step is to learn node embeddings using a GNN to obtain soft node correspondences between two graphs. The second step is to iteratively refine them using the constraints of matching consensus in local neighborhoods between graphs. The overall refining process resembles the classic graph matching algorithm of graduated assignment (Gold & Rangarajan, 1996), but generalizes it using deep neural representation. Experiments show that the proposed algorithm performs well on real-world tasks of image matching and knowledge graph entity alignment. The paper is interesting and has some good potential but lacks some important evaluations and analyses. My main concerns are as follows. 1) The consensus in the second stage is crucial? As the title shows, the main technical contribution lies in the second stage of consensus inducing. But, for the real tasks, in the experiments, the gain by the second stage is not significant or often negligible (L=0 vs. L=10 or 20 in Table 1,2,3). The results of the first stage (L=0) already give better results than all the baselines in many cases, so that most gains appear to come from the usage of GNNs for representation. This makes the major contribution of this work less significant. I hope the authors justify this. And, I guess that's maybe because the consensus information may also be induced in the first stage by matching nodes with relational presentations learned using a GNN. To see this, the authors may run the second stage only without the first stage. 2) Comparison to the graduated assignment (GA) process As discussed in 3.3, the proposed neighborhood consensus can be viewed as a generalization of GA of Eq.6 with trainable neural modules. But, it's not actually shown what is the gain by this generalization. This needs to be shown experimentally by substituting the second stage by GA process. 3) Robustness to node addition or removal. All the experiments look assuming only edges are varied. Is this algorithm robust to node addition or removal, occurring in many practical graph matching problems? This needs to be also discussed. ====================================== The rebuttal succeeds in addressing most of my concerns so that I upgrade my initial rating to weak accept. I hope all the points in the rebuttal are included in the final manuscript. ", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer , thanks a lot for reviewing our paper and providing valuable comments . We are working on incorporating your feedback in a new revision of our paper . We would like to provide more explanations to address your concerns . Main Contribution ================ We emphasize that the main contribution of our work lies indeed in the second stage of our architecture , i.e. , refining initial soft correspondences using a trainable message passing scheme that reaches for neighborhood consensus . Our approach allows us to not only distribute local information , but to also distribute global information in the form of node indicator functions/node colorings using purely local operators . The distribution of global information is then used to resolve ambiguities/false matchings made in the first stage of our architecture . In addition , we proposed optimizations to make the consensus stage scale to large real-world instances . We agree that obtaining initial soft correspondences via similarity scores of node embeddings is not a novel contribution and should n't be viewed as one . We will make this more clear in a revised version . We performed additional experiments to emphasize the usefulness of our consensus stage ( see below ) . Impact of the Consensus Stage =========================== We argue that our consensus stage has a huge impact on the resulting performance of our model . For example , on the WILLOW-ObjectClass dataset , it at least reduces the error of the initial model ( $ L=0 $ ) by half across all categories . On the DBP15K dataset , it consistently improves the model 's performance by 4 percentage points on average which we claim to be highly significant . We nonetheless agree with Reviewer # 1 that those improvements should be more significant when using weaker baselines . To verify this , we conducted additional experiments as suggested where we replaced the first stage GNN module with a weaker MLP . The results are shown below which we will include in the final manuscript . Hits @ 1 on the WILLOW-ObjectClass dataset : -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + $ \\Psi_ { \\theta_1 } $ = MLP | Motorbike | Car | Duck | Winebottle | -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + $ L=0 $ | 56.85 \u00b1 2.65 | 73.44 \u00b1 2.65 | 71.93 \u00b1 2.10 | 86.10 \u00b1 1.25 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + isotropic $ L=10 $ | 80.34 \u00b1 2.34 | 81.31 \u00b1 2.48 | 81.16 \u00b1 2.55 | 93.53 \u00b1 1.38 | isotropic $ L=20 $ | 82.24 \u00b1 3.06 | 82.49 \u00b1 3.70 | 81.84 \u00b1 2.92 | 95.14 \u00b1 1.58 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- Here , the consensus stage improves the initial matchings significantly , with nearly up to 40 percentage points improvements on the Motorbike class . However , it is worth noting that good initial matchings do help the consensus stage to improve its performance further , which stresses the importance of our two-stage approach . Furthermore , starting from weak initial matchings takes significantly more refinement steps to converge ( as can be seen by the difference between $ L=10 $ and $ L=20 $ ) . It should be noted that the first stage can not infer any information about consensus . In order to check for consensus , an initial matching is needed that can be tested . The first stage siamese network has no information flow between both heads until the feature matching . The second stage can rerank those hypotheses based on additional information : matching agreement in neighborhoods . It can not be applied without an initial ranking of correspondences ."}, "1": {"review_id": "HyeJf1HKvS-1", "review_text": "The authors proposed a message passing neural network-based graph matching methods. The overall framework can be viewed as a graph siamese network, where two set of points are passing through the same graph neural network, and then two new embeddings are generated. Using the two embedding the similarity between points can be computed and then the final matching can be generated. The overall structure of this paper is similar to [1] and [2], the authors should discuss the difference of the proposed with these two papers, if it is possible, the authors may try to compare with these two methods in experiments. Currently, I think the main contribution of the paper should be the new message-passing scheme (in Sec. 3.2). However, from the current experiment, I can not see if the performance improvement is from the new message-passing scheme. In fact, the message passing scheme is also related to the dual decomposition framework, which is previously used in the graph matching area. For example, in [3], a message-passing algorithm derived from dual decomposition is used to solve the graph matching problem. The authors may also consider add some discussion the difference between message-passing derived from dual decomposition and the message passing in the graph neural network. ==================================== After Revision ============================================== In the new experiment, the authors proved that the new message-passing scheme (i.e. the consensus stage) in Sec 3.2 can successfully improve the performance by refining original assignment. Thus I modify the score to weak accept. [1] Wang, Yue, and Justin M. Solomon. \"Deep Closest Point: Learning Representations for Point Cloud Registration.\", ICCV 2019, [2] Zhen Zhang, and Wee Sun Lee. \"Deep Graphical Feature Learning for the Feature Matching Problem.\", ICCV 2019 [3] Paul Swoboda et. al. \"A study of Lagrangean decompositions and dual ascent solvers for graph matching.\", CVPR 2017", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer , thanks a lot for reviewing our paper and providing valuable comments . We are working on incorporating your feedback in a new revision of our paper . We would like to provide more explanations to address your concerns . Main Contribution ================ We emphasize that the main contribution of our work lies indeed in the second stage of our architecture , i.e. , refining initial soft correspondences using a trainable message passing scheme that reaches for neighborhood consensus . Our approach allows us to not only distribute local information , but to also distribute global information in the form of node indicator functions/node colorings using purely local operators . The distribution of global information is then used to resolve ambiguities/false matchings made in the first stage of our architecture . In addition , we proposed optimizations to make the consensus stage scale to large real-world instances . We agree that obtaining initial soft correspondences via similarity scores of node embeddings is not a novel contribution and should n't be viewed as one . We will make this more clear in a revised version . In addition , we argue that our consensus stage has a huge impact on the resulting performance of our model . For example , on the WILLOW-ObjectClass dataset , it at least reduces the error of the initial model ( $ L=0 $ ) by half across all categories . On the DBP15K dataset , it consistently improves the model 's performance by 4 percentage points on average which we claim to be highly significant . We performed additional experiments as suggested by Reviewer # 1 to emphasize the usefulness of our consensus stage ( please see the response to Review # 1 for more details ) . Relation to [ 1 ] Wang and Solomon : Deep Closest Point : Learning Representations for Point Cloud Registration ( ICCV'19 ) ====================================================================================================== This work tackles the problem of finding an unknown rigid motion between point clouds by first matching points followed by a differentiable SVD module . We agree that this work tackles the feature matching procedure in a similar fashion as we do in our initial feature matching procedure based on inner product similarity scores . Additionally , this work leverages a Transformer module to let point clouds know about each other before feature matching takes place . Our work differs in that we introduce a consensus stage to resolve ambiguities in matchings after the initial matching procedure based on neighborhood consensus . Hence , our method could be used to improve the results of [ 1 ] further . In order to resolve ambiguities , [ 1 ] can only rely on the least squares optimization , inherentely utilizing the rigid embedding of the point cloud in $ \\mathbb { R } ^3 $ , which does not exist for general graphs and is not required for our approach . In addition , our approach is highly scalable due to only operating an local neighborhoods , while the Transformer module operates on the whole point cloud in a global fashion . Due to the different task and difference in assumptions , we do refrain from an in-depth experimental comparison . We will nonetheless discuss the similarities/differences to this work further in our related work . [ 1 ] Wang and Solomon : Deep Closest Point : Learning Representations for Point Cloud Registration ( ICCV'19 )"}, "2": {"review_id": "HyeJf1HKvS-2", "review_text": "This paper suggests a framework for answering graph matching questions consisting of local node embeddings with a message passing refinement step. The paper has well written text, offers what appear to be nice experiments validating the method and discusses its own limitations. I am giving a weak accept. I the weak accept is my reflection of my inability to provide useful feedback. This is also out of domain for me and I am not a useful reviewer for this paper. As a general comment, I will say that the paper adopts a highly mathematical style that will be off putting to many readers. Many of the expressions are 'high entropy' For instance, the embedding network is given as $\\mathbf{\\Psi}_{\\theta_1}$ throughout. This is seven levels of typographical distinction for the main character of the story. This is a (1) bold (2) capital (3) Greek letter with a (4) subscript that is (5) greek with a (6) subscript that is (7) numeric. I understand that each of these levels of distinction has a purpose and a meaning, but it is also arguably a much richer designation that is necessary. Generally the paper feels like this, as if its being too unnecessarily specific. Personally I found the paper rather difficult to read and decompose, which I believe does count against the paper. The overly specific nature of paper will cut into its potential readership strongly.", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer , thanks a lot for reviewing our paper and for the positive feedback . To precisely describe our architecture with the theorems and their proofs , we opt for introducing a rigorous mathematical notation . In addition , we wanted to describe our procedure as general as possible . For example , $ \\Psi_ { \\theta } $ is not limited to be a particular GNN instance but can be any trainable architecture outputting node embeddings . We will strengthen the intuitive explanations and simplify notation in a revised version ."}}