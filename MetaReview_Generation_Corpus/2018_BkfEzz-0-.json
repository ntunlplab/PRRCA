{"year": "2018", "forum": "BkfEzz-0-", "title": "Neuron as an Agent", "decision": "Invite to Workshop Track", "meta_review": "\nThe reviewers have significantly different views, with one strongly negative,\none strongly positive, and one borderline negative.  However, all three\nreviews seem to regard the NaaA framework as a very interesting and novel approach to training neural nets.  They also concur that the major issue with the paper is very confusing technical exposition regarding the motivation, math details, and how the idea works.  The authors indicate that they have significantly revised the manuscript to improve the exposition, but none of the reviewers have changed their scores.  One reviewer states that \"technical details are still too heavy to easily follow.\"  My own take regarding the current section 3 is that it is still very challenging to parse and follow. Given this analysis, the committee recommends this for workshop.\n\nPros:\n        Interesting and novel framework for training NNs\n        \"Adaptive DropConnect\" algorithm contribution\n        Good empirical results in image recognition and ViZDoom domains\n\nCons:\n        Technical exposition is very challenging to parse and follow\n        Some author rebuttals do not inspire confidence.  For example,\nmotivation of method due to \"$100 billion market cap of Bitcoin\" and in reply to unconvincing neuroscience motivation, saying \"throw away the typical image of auction.\"", "reviews": [{"review_id": "BkfEzz-0--0", "review_text": "This paper proposed a novel framework Neuron as an Agent (NaaA) for training neural networks to perform various machine learning tasks, including classification (supervised learning) and sequential decision making (reinforcement learning). The NaaA framework is based on the idea of treating all neural network units as self-interested agents and optimizes the neural network as a multi-agent RL problem. This paper also proposes adaptive dropconnect, which extends dropconnect (Wan et al., 2013) by using an adaptive algorithm for masking network topology. This work attempts to bring several fundamental principles in game theory to solve neural network optimization problems in deep learning. Although the ideas are interest and technically sound, and the proposed algorithms are demonstrated to outperform several baselines in various machine learning tasks, there several major problems with this paper, including lacking clarity of presentation, insights and substantiations of many claims. These issues may need a significant amount of effort to fix as I will elaborate more below. 1. Introduction There are several important concepts, such as reward distribution, credit assignment, which are used (from the very beginning of the paper) without explanation until the final part of the paper. The motivation of the work is not very clear. There seems to be a gap between the first paragraph and the second paragraph. The authors mentioned that \u201cFrom a micro perspective, the abstraction capability of each unit contribute to the return of the entire system. Therefore, we address the following questions. Will reinforcement learning work even if we consider each unit as an autonomous agent \u201d Is there any citation for the claim \u201cFrom a micro perspective, the abstraction capability of each unit contribute to the return of the entire system\u201d ? It seems to me this is a very general claim. Even RL methods with linear function approximations use abstractions. Also, it is unclear to me why this is an interest question. Does it have anything to do with existing issues in DRL? Moreover, The definition of autonomous agent is not clear, do you mean learning agent or policy execution agent? \u201cit uses \\epsilon-greedy as a policy, \u2026\u201d Do you mean exploration policy? I also have some concerns regarding the claim that \u201cWe confirm that optimization with the framework of NaaA leads to better performance of RL\u201d. Since there are only two baselines are compared to the proposed method, this claim seems too general to be true. It is not clear to why the authors mention that \u201cnegative result that the return decreases if we naively consider units as agents\u201d. What is the big picture behind this claim? \u201cThe counterfactual return is that by which we extend reward \u2026\u201d need to be rewritten. The last paragraph of introduction discussed the possible applications of the proposed methods without any substantiation, especially neither citations nor any related experiments of the authors are provided. 2 Related Work \u201cPOSG, a class of reinforcement learning with multiple ..\u201d -> reinforcement learning framework \u201cAnother one is credit assignment. Instead of reward.. \u201d Two sentences are disconnected and need to be rewritten. \u201cThis paper unifies both issues\u201d sounds very weird. Do you mean \u201csolves/considers both issues in a principled way\u201d? The introduction of GAN is very abrupt. Rather than starting from introducing those new concepts directly, it might be better to mention that the proposed method is related to many important concepts in game theory and GANs. \u201c,which we propose in a later part of this paper\u201d -> which we propose in this paper 3. Background \u201ca function from the state and the action of an agent to the real value\u201d -> a reward function Should provide a citation for DRQN There is a big gap between the last two paragraphs of section 3. 4. Neuro as an agent \u201cWe add the following assumption for characteristics of the v_i\u201d -> assumptions for characterizing v_i \u201cto maximize toward maximizing its own return\u201d -> to maximize its own return We construct the framework of NaaA from the assumptions -> from these assumptions \u201cindicates that the unit gives additional value to the obtained data. \u2026\u201d I am not sure what this sentence means, given that \\rho_ijt is not clearly defined. 5. Optimization \u201cNaaA assumes that all agents are not cooperative but selfish\u201d Why? Is there any justification for such a claim? What is the relation between \\rho_jit and q_it ? \u201cA buyer which cannot receive the activation approximates x_i with \u2026\u201d It is unclear why a buyer need to do so given that it cannot receive the activation anyway. \u201cQ_it maximizing the equation is designated as the optimal price.\u201d Which equation? e_j and 0 are not defined in equation 8 6 Experiment setare -> set are what is the std for CartPole in table 1 It is hard to judge the significance of the results on the left side of figure 2. It might be better to add errorbars to those curves More description should be provided to explain the reward visualization on the right side of figure 2. What reward? External/internal? \u201cSpecifically, it is applicable to various methods as described below \u2026\u201d Related papers should be cited.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for reading and commenting our paper . We really appreciate your detailed comments . Most of them were very helpful to brush up our paper . We are about to finalize the paper , and will upload a version which highly improved clarity at 5th Jan . So , please look forward it . Enjoy the holidays & Have a happy new year ."}, {"review_id": "BkfEzz-0--1", "review_text": "In this paper, the authors present a novel way to look at a neural network such that each neuron (node) in the network is an agent working to optimize its reward. The paper shows that by appropriately defining the neuron level reward function, the model can learn a better policy in different tasks. For example, if a classification task is formulated as reinforcement learning where the ultimate reward depends on the batch likelihood, the presented formulation (called Adaptive DropConnect in this context) does better on standard datasets when compared with a strong baseline. The idea proposed in the paper is quite interesting, but the presentation is severely lacking. In a work that relies heavily on precise mathematical formulation, there are several instances when the details are not addressed leading to ample confusion making it hard to fully comprehend how the idea works. For example, in section 5.1, notations are presented and defined much later or not at all (g_{jit} and d_{it}). Many equations were unclear to me for similar reasons to the point I decided to only skim those parts. Even the definition of external vs. internal environment (section 4) was unclear which is used a few times later. Like, what does it mean when we say, \u201cenvironment that the multi-agent system itself touches\u201d? Overall, I think the idea presented in the paper has merit, but without a thorough rewriting of the mathematical sections, it is difficult to fully comprehend its potential and applications.", "rating": "7: Good paper, accept", "reply_text": "Thank you for reading and commenting our paper . We will polish our mathematical formulation to improve your understanding during this period . > Even the definition of external vs. internal environment ( section 4 ) was unclear which is used a few times later . An external environment is the original environment such as Doom and Atari , and an internal environment is a set of units . From an agent 's perspective , other units are considered as an environment . The quick reference can also be helpful . Environment for a unit State for a unit Observation for a unit -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - External original environment original state original observation Internal other units activation of all the other units activation of allocated units Both - - be used to predict o_ { ijt } Reward per unit Total reward over units -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - External original reward total original reward ( designer 's objective ) Internal revenue from units - cost 0 Both units ' objective total original reward ( designer 's objective ) > \u201c environment that the multi-agent system itself touches \u201d Typically , there is boundary between an agent and an environment ( e.g. , a robot in a room ) . We wrote this situation that the agent with a NN ( as the multi-agent system ) touches the environment . > In section 5.1 , notations are presented and defined much later or not at all ( g_ { jit } and d_ { it } ) . > Many equations were unclear to me for similar reasons to the point > Without a thorough rewriting of the mathematical sections , it is difficult to fully comprehend its potential and applications . As we will reflect the comments to our paper , please wait for it ."}, {"review_id": "BkfEzz-0--2", "review_text": "The authors consider a Neural Network where the neurons are treated as rational agents. In this model, the neurons must pay to observe the activation of neurons upstream. Thus, each individual neuron seeks to maximize the sum of payments it receives from other neurons minus the cost for observing the activations of other neurons (plus an external reward for success at the task). While this is an interesting idea on its surface, the paper suffers from many problems in clarity, motivation, and technical presentation. It would require very major editing to be fit for publication. The major problem with this paper is its clarity. See detailed comments below for problems just in the introduction. More generally, the paper is riddled with non sequiturs. The related work section mentions Generative Adversarial Nets. As far as I can tell, this paper has nothing to do with GANs. The Background section introduces notation for POMDPs, never to be used again in the entirety of the paper, before launching into a paragraph about apoptosis in glial cells. There is also a general lack of attention to detail. For example, the entire network receives an external reward (R_t^{ex}), presumably for its performance on some task. This reward is dispersed to the the individual agents who receive individual external rewards (R_{it}^{ex}). It is never explained how this reward is allocated even in the authors\u2019 own experiments. The authors state that all units playing NOOP is an equilibrium. While this is certainly believable/expected, such a result would depend on the external rewards R_{it}^{ex}, the observation costs \\sigma_{jit}, and the network topology. None of this is discussed. The authors discuss Pareto optimality without ever formally describing what multi-objective function defines this supposed Pareto boundary. This is pervasive throughout the paper, and is detrimental to the reader\u2019s understanding. While this might be lost because of the clarity problems described above, the model itself is also never really motivated. Why is this an interesting problem? There are many ways to create rational incentives for neurons in a neural net. Why is paying to observe activations the one chosen here? The neuroscientific motivation is not very convincing to me, considering that ultimately these neurons have to hold an auction. Is there an economic motivation? Is it just a different way to train a NN? Detailed Comments: \u201cIn the of NaaA\u201d => remove \u201cof\u201d? \u201cpassing its activation to the unit as cost\u201d => Unclear. What does this mean? \u201cperformance decreases if we naively consider units as agents\u201d => Performance on what? \u201c.. we demonstrate that the agent obeys to maximize its counterfactual return as the Nash Equilibrium\u201c => Perhaps, this should be rewritten as \u201cAgents maximize their counterfactual return in equilibrium. \u201cSubsequently, we present that learning counterfactual return leads the model to learning optimal topology\u201d => Do you mean \u201cmaximizing\u201d instead of learning. Optimal with respect to what task? \u201cpure-randomly\u201d => \u201crandomly\u201d \u201cwith adaptive algorithm\u201d => \u201cwith an adaptive algorithm\u201d \u201cthe connection\u201d => \u201cconnections\u201d \u201cIn game theory, the outcome maximizing overall reward is named Pareto optimality.\u201d => This is simply incorrect. ", "rating": "3: Clear rejection", "reply_text": "Response to Paragraph 5 : > There are many ways to create rational incentives for neurons in a neural net . As I don \u2019 t think there are many methods for our problem setting , please provide a link . > The neuroscientific motivation is not very convincing to me , considering that ultimately these neurons have to hold an auction . Auction is more than auction as it used in mechanism design to orchestrate the actions of agents with mechanism . So , think out of the box , and throw away the typical image of auction . > Is there an economic motivation ? Is it just a different way to train a NN ? Yes , there is an economic motivation as well as to improve training a NN . Response to Paragraph 6 ( the detailed comments ) : > \u201c passing its activation to the unit as cost \u201d = > Unclear . What does this mean ? `` to observe their activation '' is correct . ( As it was a mistake in the native check process , we will change the native checker later ) > \u201c performance decreases if we naively consider units as agents \u201d = > Performance on what ? Performance on the total cumulative external reward . > \u201c Subsequently , we present that learning counterfactual return leads the model to learning optimal topology \u201d = > Do you mean \u201c maximizing \u201d instead of learning . Optimal with respect to what task ? Just like the above answer , it will be optimal with respect to the total cumulative external reward ."}], "0": {"review_id": "BkfEzz-0--0", "review_text": "This paper proposed a novel framework Neuron as an Agent (NaaA) for training neural networks to perform various machine learning tasks, including classification (supervised learning) and sequential decision making (reinforcement learning). The NaaA framework is based on the idea of treating all neural network units as self-interested agents and optimizes the neural network as a multi-agent RL problem. This paper also proposes adaptive dropconnect, which extends dropconnect (Wan et al., 2013) by using an adaptive algorithm for masking network topology. This work attempts to bring several fundamental principles in game theory to solve neural network optimization problems in deep learning. Although the ideas are interest and technically sound, and the proposed algorithms are demonstrated to outperform several baselines in various machine learning tasks, there several major problems with this paper, including lacking clarity of presentation, insights and substantiations of many claims. These issues may need a significant amount of effort to fix as I will elaborate more below. 1. Introduction There are several important concepts, such as reward distribution, credit assignment, which are used (from the very beginning of the paper) without explanation until the final part of the paper. The motivation of the work is not very clear. There seems to be a gap between the first paragraph and the second paragraph. The authors mentioned that \u201cFrom a micro perspective, the abstraction capability of each unit contribute to the return of the entire system. Therefore, we address the following questions. Will reinforcement learning work even if we consider each unit as an autonomous agent \u201d Is there any citation for the claim \u201cFrom a micro perspective, the abstraction capability of each unit contribute to the return of the entire system\u201d ? It seems to me this is a very general claim. Even RL methods with linear function approximations use abstractions. Also, it is unclear to me why this is an interest question. Does it have anything to do with existing issues in DRL? Moreover, The definition of autonomous agent is not clear, do you mean learning agent or policy execution agent? \u201cit uses \\epsilon-greedy as a policy, \u2026\u201d Do you mean exploration policy? I also have some concerns regarding the claim that \u201cWe confirm that optimization with the framework of NaaA leads to better performance of RL\u201d. Since there are only two baselines are compared to the proposed method, this claim seems too general to be true. It is not clear to why the authors mention that \u201cnegative result that the return decreases if we naively consider units as agents\u201d. What is the big picture behind this claim? \u201cThe counterfactual return is that by which we extend reward \u2026\u201d need to be rewritten. The last paragraph of introduction discussed the possible applications of the proposed methods without any substantiation, especially neither citations nor any related experiments of the authors are provided. 2 Related Work \u201cPOSG, a class of reinforcement learning with multiple ..\u201d -> reinforcement learning framework \u201cAnother one is credit assignment. Instead of reward.. \u201d Two sentences are disconnected and need to be rewritten. \u201cThis paper unifies both issues\u201d sounds very weird. Do you mean \u201csolves/considers both issues in a principled way\u201d? The introduction of GAN is very abrupt. Rather than starting from introducing those new concepts directly, it might be better to mention that the proposed method is related to many important concepts in game theory and GANs. \u201c,which we propose in a later part of this paper\u201d -> which we propose in this paper 3. Background \u201ca function from the state and the action of an agent to the real value\u201d -> a reward function Should provide a citation for DRQN There is a big gap between the last two paragraphs of section 3. 4. Neuro as an agent \u201cWe add the following assumption for characteristics of the v_i\u201d -> assumptions for characterizing v_i \u201cto maximize toward maximizing its own return\u201d -> to maximize its own return We construct the framework of NaaA from the assumptions -> from these assumptions \u201cindicates that the unit gives additional value to the obtained data. \u2026\u201d I am not sure what this sentence means, given that \\rho_ijt is not clearly defined. 5. Optimization \u201cNaaA assumes that all agents are not cooperative but selfish\u201d Why? Is there any justification for such a claim? What is the relation between \\rho_jit and q_it ? \u201cA buyer which cannot receive the activation approximates x_i with \u2026\u201d It is unclear why a buyer need to do so given that it cannot receive the activation anyway. \u201cQ_it maximizing the equation is designated as the optimal price.\u201d Which equation? e_j and 0 are not defined in equation 8 6 Experiment setare -> set are what is the std for CartPole in table 1 It is hard to judge the significance of the results on the left side of figure 2. It might be better to add errorbars to those curves More description should be provided to explain the reward visualization on the right side of figure 2. What reward? External/internal? \u201cSpecifically, it is applicable to various methods as described below \u2026\u201d Related papers should be cited.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for reading and commenting our paper . We really appreciate your detailed comments . Most of them were very helpful to brush up our paper . We are about to finalize the paper , and will upload a version which highly improved clarity at 5th Jan . So , please look forward it . Enjoy the holidays & Have a happy new year ."}, "1": {"review_id": "BkfEzz-0--1", "review_text": "In this paper, the authors present a novel way to look at a neural network such that each neuron (node) in the network is an agent working to optimize its reward. The paper shows that by appropriately defining the neuron level reward function, the model can learn a better policy in different tasks. For example, if a classification task is formulated as reinforcement learning where the ultimate reward depends on the batch likelihood, the presented formulation (called Adaptive DropConnect in this context) does better on standard datasets when compared with a strong baseline. The idea proposed in the paper is quite interesting, but the presentation is severely lacking. In a work that relies heavily on precise mathematical formulation, there are several instances when the details are not addressed leading to ample confusion making it hard to fully comprehend how the idea works. For example, in section 5.1, notations are presented and defined much later or not at all (g_{jit} and d_{it}). Many equations were unclear to me for similar reasons to the point I decided to only skim those parts. Even the definition of external vs. internal environment (section 4) was unclear which is used a few times later. Like, what does it mean when we say, \u201cenvironment that the multi-agent system itself touches\u201d? Overall, I think the idea presented in the paper has merit, but without a thorough rewriting of the mathematical sections, it is difficult to fully comprehend its potential and applications.", "rating": "7: Good paper, accept", "reply_text": "Thank you for reading and commenting our paper . We will polish our mathematical formulation to improve your understanding during this period . > Even the definition of external vs. internal environment ( section 4 ) was unclear which is used a few times later . An external environment is the original environment such as Doom and Atari , and an internal environment is a set of units . From an agent 's perspective , other units are considered as an environment . The quick reference can also be helpful . Environment for a unit State for a unit Observation for a unit -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - External original environment original state original observation Internal other units activation of all the other units activation of allocated units Both - - be used to predict o_ { ijt } Reward per unit Total reward over units -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - External original reward total original reward ( designer 's objective ) Internal revenue from units - cost 0 Both units ' objective total original reward ( designer 's objective ) > \u201c environment that the multi-agent system itself touches \u201d Typically , there is boundary between an agent and an environment ( e.g. , a robot in a room ) . We wrote this situation that the agent with a NN ( as the multi-agent system ) touches the environment . > In section 5.1 , notations are presented and defined much later or not at all ( g_ { jit } and d_ { it } ) . > Many equations were unclear to me for similar reasons to the point > Without a thorough rewriting of the mathematical sections , it is difficult to fully comprehend its potential and applications . As we will reflect the comments to our paper , please wait for it ."}, "2": {"review_id": "BkfEzz-0--2", "review_text": "The authors consider a Neural Network where the neurons are treated as rational agents. In this model, the neurons must pay to observe the activation of neurons upstream. Thus, each individual neuron seeks to maximize the sum of payments it receives from other neurons minus the cost for observing the activations of other neurons (plus an external reward for success at the task). While this is an interesting idea on its surface, the paper suffers from many problems in clarity, motivation, and technical presentation. It would require very major editing to be fit for publication. The major problem with this paper is its clarity. See detailed comments below for problems just in the introduction. More generally, the paper is riddled with non sequiturs. The related work section mentions Generative Adversarial Nets. As far as I can tell, this paper has nothing to do with GANs. The Background section introduces notation for POMDPs, never to be used again in the entirety of the paper, before launching into a paragraph about apoptosis in glial cells. There is also a general lack of attention to detail. For example, the entire network receives an external reward (R_t^{ex}), presumably for its performance on some task. This reward is dispersed to the the individual agents who receive individual external rewards (R_{it}^{ex}). It is never explained how this reward is allocated even in the authors\u2019 own experiments. The authors state that all units playing NOOP is an equilibrium. While this is certainly believable/expected, such a result would depend on the external rewards R_{it}^{ex}, the observation costs \\sigma_{jit}, and the network topology. None of this is discussed. The authors discuss Pareto optimality without ever formally describing what multi-objective function defines this supposed Pareto boundary. This is pervasive throughout the paper, and is detrimental to the reader\u2019s understanding. While this might be lost because of the clarity problems described above, the model itself is also never really motivated. Why is this an interesting problem? There are many ways to create rational incentives for neurons in a neural net. Why is paying to observe activations the one chosen here? The neuroscientific motivation is not very convincing to me, considering that ultimately these neurons have to hold an auction. Is there an economic motivation? Is it just a different way to train a NN? Detailed Comments: \u201cIn the of NaaA\u201d => remove \u201cof\u201d? \u201cpassing its activation to the unit as cost\u201d => Unclear. What does this mean? \u201cperformance decreases if we naively consider units as agents\u201d => Performance on what? \u201c.. we demonstrate that the agent obeys to maximize its counterfactual return as the Nash Equilibrium\u201c => Perhaps, this should be rewritten as \u201cAgents maximize their counterfactual return in equilibrium. \u201cSubsequently, we present that learning counterfactual return leads the model to learning optimal topology\u201d => Do you mean \u201cmaximizing\u201d instead of learning. Optimal with respect to what task? \u201cpure-randomly\u201d => \u201crandomly\u201d \u201cwith adaptive algorithm\u201d => \u201cwith an adaptive algorithm\u201d \u201cthe connection\u201d => \u201cconnections\u201d \u201cIn game theory, the outcome maximizing overall reward is named Pareto optimality.\u201d => This is simply incorrect. ", "rating": "3: Clear rejection", "reply_text": "Response to Paragraph 5 : > There are many ways to create rational incentives for neurons in a neural net . As I don \u2019 t think there are many methods for our problem setting , please provide a link . > The neuroscientific motivation is not very convincing to me , considering that ultimately these neurons have to hold an auction . Auction is more than auction as it used in mechanism design to orchestrate the actions of agents with mechanism . So , think out of the box , and throw away the typical image of auction . > Is there an economic motivation ? Is it just a different way to train a NN ? Yes , there is an economic motivation as well as to improve training a NN . Response to Paragraph 6 ( the detailed comments ) : > \u201c passing its activation to the unit as cost \u201d = > Unclear . What does this mean ? `` to observe their activation '' is correct . ( As it was a mistake in the native check process , we will change the native checker later ) > \u201c performance decreases if we naively consider units as agents \u201d = > Performance on what ? Performance on the total cumulative external reward . > \u201c Subsequently , we present that learning counterfactual return leads the model to learning optimal topology \u201d = > Do you mean \u201c maximizing \u201d instead of learning . Optimal with respect to what task ? Just like the above answer , it will be optimal with respect to the total cumulative external reward ."}}