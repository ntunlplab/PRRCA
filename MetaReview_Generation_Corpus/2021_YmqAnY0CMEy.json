{"year": "2021", "forum": "YmqAnY0CMEy", "title": "Mathematical Reasoning via Self-supervised Skip-tree Training", "decision": "Accept (Spotlight)", "meta_review": "All reviewers find the idea of self-supervised learning on mathematical reasoning with the proposed skip-tree training interesting and gave the firmly positive scores.  The paper is clearly written, and the experiments and the analysis are well-organized, particularly the ability of free-form conjecturing is quite thought-provoking.  Also, the reviewers' initial concerns have been properly addressed during the discussion phrase. \n\nI think this is a good paper from which people can learn a lot, and should be broadly presented at the conference either as an oral or a spotlight presentation.", "reviews": [{"review_id": "YmqAnY0CMEy-0", "review_text": "The authors propose a self-supervised learning task to enhance the reasoning capabilities of machine learning models on mathematical formulas and to perform conjecturing in higher order logic . The task consists in masking out specific portions of mathematical statements and predict them from the surrounding parts . The task can ( i ) be used during training , to provide supervisory signal to the machine learning model and to increase the effective size of the otherwise small training dataset , and ( ii ) be used during testing , to evaluate the reasoning capabilities of the learnt models by masking out the mathematical statements at different level of granularities . The authors perform an extensive experimental analysis and provide evidence on the utility of using self-supervised learning in the context of theorem proving . Overall , the paper is clearly written and the bibliography complete . Also , the experimental analysis is undoubtedly valuable for the machine learning community . In fact , it provides additional empirical evidence to the works of [ 1-2 ] , in the sense that self-supervised learning can be used not only as a pre-training stage for the machine learning models , but also as a task to perform conjecturing . There are major issues though . In particular , there are issues in terms of the originality of the proposed task and the reproducibility of the experiments , thus obscuring the positive aspects of the paper . Please , see below for more detailed comments . In lieu of this , I consider the paper marginally below the acceptance threshold and therefore recommend for an initial rejection . Nevertheless , I 'm willing to raise my score if the authors properly address the issues highlighted below . DETAILED COMMENTS Originality : The proposed task seems to be identical to the task in [ 3 ] , except for the fact that the task operates on mathematical statements rather than on natural language ones . Importantly , both kinds of sentences/statements share a tree-structured representation , which makes the task in [ 3 ] trivially applicable to the mathematical context . Can the authors highlight other differences or explain if this is not correct ? Reproducibility : First of all , it 's extremely important to add in the section about results and discussion detailed information about the computational resources and the time required for training , as well as the size of the machine learning model used in the experiments . In fact , it is well-known from other domains , like vision and natural language processing , that the performance of self-supervised learning models increase proportionally with the size of the model , at the expense of the training computational resources . Furthermore , it 's important to discuss about the methodology used to choose the hyperparameters . Appendix A only states that `` we explored encoders and decoders with up to 12 layers and various learning rates and intermediate sizes . '' Finally , do the authors plan to release the code ? FURTHER IMPORTANT COMMENTS Answering to this following comments could be helpful to clarify about the originality and the novelty of the proposed task . Distinguishing between training and evaluation tasks and saying that `` several tasks '' are proposed ( i.e.in abstract , introduction , section 5 and conclusions ) can be misleading for the reader , because he/she could think that they are different . Nevertheless , these tasks are all equivalent to the main skip-tree task , differing only in the way the terms are masked . Is that correct ? If so , I would suggest to rephrase by saying that the main skip-tree task is quite flexible at masking statements . In fact , depending on which portion of statement is masked , the model can be induced to perform reasoning at different level of abstraction . Based on this , experiments are categorised according to the different ways of masking . Furthermore , it is not completely clear when the authors says that `` most previous works ... have focused ... in supervised training settings . In contrast , we train language models on an unsupervised proxy task '' ( related work , but also introduction ) . Is the proposed training unsupervised ? First of all , the data used in training contains only valid theorems . This can be already considered as a supervised information , because collecting valid theorems requires human effort . Secondly , the evaluation tasks are essentially similar to the training task and therefore I would consider this more as a supervised learning strategy . Could you please elaborate more on this , also in the text ? MINOR COMMENTS In the experiments about conjecturing ( 2nd paragraph ) , can you provide more details on how you generated the free-form conjectures ? Furthermore , how is novelty of generated statements defined ? Consider the following examples for conjecturing on assumptions and free-form conjecturing . Conjecturing from assumptions Imagine you have a+b=0 implies a=-b and the task is given by < PREDICT > implies a=-b Is the statement ' a+b=c and c=0 ' considered new according to your definition ? Free-form conjecturing Imagine you have < THEOREM > for all x , x=x and the task is given by < THEOREM > < PREDICT > Is the theorem 'for all y , y=y ' considered new according to your definition ? [ 1 ] Li et al.Modelling High-Level Mathematical Reasoning in Mechanised Declarative Proofs . arXiv 2020 [ 2 ] Polu and Sutskever . Generative Language Modelling for Automated Theorem Provers . arXiv 2020 [ 3 ] Zhang et al.PEGASUS : Pre-training with Extracted Gap-sentences for Abstractive Summarization . ICML 2020 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # UPDATE The paper provides important and novel empirical observations about the use of machine learning to perform mathematical reasoning . The authors have addressed and clarified some doubts about the originality of their idea and the reproducibility of their experiments in the discussion phase . I believe that the paper is now ready for publication and I 'm happy to recommend for its acceptance .", "rating": "7: Good paper, accept", "reply_text": "Thanks for the detailed review ! Originality : Pegasus [ 2 ] is a task defined on natural language and relies on the notion of sentences , while our approach relies on the tree structure of expressions . Pegasus only considers \u201c whole sentences \u201d [ 2 , page 2 , paragraph 4 ] , so it does not consider tree structure . Additionally , our training task can create a much wider variety of training examples than PEGASUS as the number of subtrees is closer to the number of tokens . ( We exploit that when we sample many training examples from each mathematical statement . ) We actually think that the entire development of training tasks from MASS/UNILM ( predict subsequences ) to T5 ( predict multiple subsequences ) to PEGASUS ( predict sentences ) and finally to this work ( predict subtrees ) were all incremental steps when only examined for their technical merits . But their impact is undeniably large . The main novelty of this paper is perhaps more in the conceptual domain as it provides a new perspective on how to achieve mathematical reasoning abilities without supervised training data ( as also AnonReviewer1 notes ) . Reproducibility : We added a description of the computational resources , the size of the models , and our evaluation methodology . Larger models seem to perform better indeed . We have preliminary results that suggest this ( for a model with 110M parameters ) and are happy to add them to the paper , if the reviewers agree that this would improve the paper . Code : We will release the code for training data generation . Choice of hyperparameters : Because each training run is relatively costly , we did not perform a full grid search for the hyperparameters . Instead we consulted the literature and expert opinions and tested just a couple of hyperparameters . For the skip-sequence models we tried a more extensive list of hyperparameters , as we wanted to make sure that the negative results here are stable under different choices of hyperparameters . Difference between training task and evaluation tasks : The evaluation tasks and the training task indeed share the same format , i.e.predict a part of a larger expression . This is intentional as it allows to test the model without fine-tuning on reasoning tasks . The distributions of the training and evaluation tasks are , in fact , disjoint , because the evaluation tasks are derived from different source data , the validation/test theorems . We improved the description of the relationship between training and evaluation tasks in the beginning of Section 5 . Unsupervised vs self-supervised : We corrected the occurrence of \u201c unsupervised \u201d to \u201c self-supervised \u201d , which is a better term . On the question whether our technique qualifies as being unsupervised/self-supervised : It is a great question where the line between supervised and self-supervised techniques is exactly . We believe that our use of \u201c self-supervised \u201d is in line with other papers : Self-supervised translation techniques use monolingual corpora , which , with the same argument that reviewer 2 provided , could be seen as \u201c supervised information \u201d . We think that the use of arbitrary subexpressions as prediction targets is general enough to be applied to arbitrary sources . There is also nothing that keeps us from applying skip-tree training to statements that are not true . In one sense , in which we are clearly self-supervised is that the training data preparation does not require human labelers to produce labels . Generation of conjectures : We improved the description of the free-form conjecturing task . The procedure is quite simple : We simply ask the model to produce theorems by presenting the input sequence ( < theorem > < PREDICT > ) . So , the subexpression the models has to fill in must be an entire mathematical statement , and the < theorem > token ( hopefully ) makes the model produce a statement that could be a theorem ( as opposed to the < goal > tag , which indicates intermediate statements of proofs ) . Given only this single input sequence , we use a beam search with beam width 1024 to produce enough outputs for a meaningful evaluation . How we measure the novelty of terms : We consider strict syntactic ( string ) equality ."}, {"review_id": "YmqAnY0CMEy-1", "review_text": "Quality The paper is quite well written and the experiments seem well thought out . Please see specific comments below . Clarity The S-expression needs more explanation for the paper to be self-contained . What is ' A ' in ( v A x ) ? For those of us who might not be very familiar with the datasets , it might be helpful if the paper to demonstrate more examples as to what these proofs are translated to , in mathematical notation or textual explanations . There are a few snippets here and there but I do n't think I get a good grasp of what kinds of proofs we deal with . For example , the theorem examples in Appendix section D seem quite lengthy . With effort it should be decoded into normal text by readers , but it would be more convenient to demonstrate it directly . In addition , the free-form conjecturing evaluation can be more clear . In this section , it does n't entirely explain in details what makes the generated statements 'new ' . Is it only exact match with the training set ? If that 's the case , how robust is exact matching on measuring the novelty ? On the usefulness of the conjectures , there 's not much explanation on the RL experiments + DeepHOL theorem prover . What is it supposed to do / how robust does it measure usability ? Question : a beam search of 1024 seems rather large . How does the result look if no beam search is used ? Originality The introduced method 'skip-tree ' is not that different from the usual self-supervised training techniques used to train language models . This feature does not contribute to the novelty that much . However , this approach is a new take on learning mathematical reasoning with self-supervised learning , rather than supervised learning like in other previous work . Significance Deep learning for mathematical proofs is an interesting direction and is relatively unexplored , compared to other application areas . High-Level Pros & Cons Pros - The claim / conclusion for this work that self-supervised training can lead to mathematical reasoning is rather intriguing . - The proposed skip-tree technique seems to make a lot of difference for training . - Decent ablation study ( not using < MASK > token , skip-sequence instead of skip-tree ) . - The evaluation tasks introduced seem interesting . Cons - I am not 100 % convinced that the `` mathematical reasoning '' demonstrated is beyond pattern recognition . I understand that the evaluation tasks are evaluated on validation set theorems which are not seen during training . However , how can we be sure that this is mathematical reasoning versus the model recognizing similar patterns ( not necessarily exact match ) More explanations on this would be appreciated . - Low to moderate novelty for the skip-tree technique . Overall , I learned a lot from this paper and I believe others can benefit from it as well .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the insightful review . > S-expressions : What is ' A ' in ( v A x ) ? \u2018 A \u2019 is a generic type variable . The expression ( v A x ) is a variable named x that could still have any type . We expanded the description of S-expressions . > the theorem examples in Appendix section D seem quite lengthy . With effort it should be decoded into normal text by readers , but it would be more convenient to demonstrate it directly . For the missing assumptions and equality completion tasks , we had already provided human readable versions of the ground-truth , but somehow we forgot to add them for the type inference and hard type inference tasks . This is now fixed . > Is it only an exact match with the training set ? Yes , we only look for exact matches . We updated the formulation to make this clear . > What is it supposed to do / how robust does it measure [ usefulness ] ? We count a conjectured theorem as useful only if it is picked up as a premise by the DeepHOL theorem prover . This measurement only says that for some proof this premise was necessary ( DeepHOL includes a proof pruning step that , for each premise , tests if dropping the premise was still a valid proof ) . There may still be alternative proofs that do not require this particular premise , and even if a conjecture is not used as a premise , there might be theorems for which it would be very useful , but we may not have those in our list of tasks . So this should only be seen as a rough approximation of usefulness . We believe that the take-away is that this method managed to produce at least some new and useful theorems . > Question : a beam search of 1024 seems rather large . How does the result look if no beam search is used ? We used beam width 1024 only for the conjecturing experiments where we wanted to generate many different expressions . For the other evaluations we use a beam search with width 8 . If we only generate a single trace , our best models reach 25 % to 30 % on the missing assumptions and equality completion tasks . > I understand that the evaluation tasks are evaluated on validation set theorems which are not seen during training . However , how can we be sure that this is mathematical reasoning versus the model recognizing similar patterns ( not necessarily exact match ) We are not sure this is a \u201c con \u201d . We believe that our experiments show that , at the very least , \u201c pattern recognition \u201d can effectively simulate some forms of mathematical reasoning . We hope that this will contribute to the wider discussions in the community and help to understand the fundamental question of the relationship between reasoning and \u201c pattern recognition \u201d ."}, {"review_id": "YmqAnY0CMEy-2", "review_text": "-- Summary : This paper proposes a skip-tree training task . The authors show that self-supervised language models ( the Transformer architecture to be exact ) trained on the proposed skip-tree training task for mathematic theorem proving enable mathematical reasoning capabilities . Moreover , no fine-tuning is required to achieve the reported reasoning capabilities . They compare the mathematical reasoning abilities of the skip-tree training task with skip-sequence and show an impressive performance improvement . Another interesting result is studying whether any useful ( novel ) conjectures can be generated by the model . -- Overall assessment : I really enjoyed reading this paper . The authors work on an interesting problem that has been gaining popularity in the last few years . I believe that this problem domain is a suitable test bed to investigate the reasoning capabilities of language models . The authors propose a skip-tree training task and show that it significantly improves the performance of skip-sequence training . The idea is simple and nice : instead of masking out arbitrary sub-sequences ( which is the case for skip-sequence ) , the authors propose to mask out sub-trees . The authors provide extensive ablation studies and evaluations to justify several design choices and to show the reasoning abilities of their model . I do have some comments about presentation clarity ( look below ) , that I hope the authors can address during the rebuttal . Moreover , I am a bit skeptical about the almost 0 performance of the skip-sequence task under some scenarios . I will clarify this as well in the cons section below . -- Pros : ( 1 ) I like the idea of skip-tree training for structured problems . It seems to improve the performance of the model dramatically ( from almost not being able to predict AT ALL to near perfect prediction \u2014 in the Hard inference problem for example ) . I do have a comment about this in the cons section . ( 2 ) The authors design a set of mathematical reasoning evaluations that are used as down-stream tasks ( without fine-tuning ) to evaluate the self-supervised trained models . They show good performance on all the tasks compared to their baselines . ( 3 ) The authors provide a set of ablation studies to justify their design choices . ( 4 ) I liked the conjecture experiment that evaluated whether the model can generate novel provable conjectures . -- Cons : ( 1 ) The performance of skip-sequence training on three out of the four evaluation tasks in Table 2 ( Hard type inference , Assumptions , Equalities ) are extremely poor . It is my understanding that the only difference between these models and the proposed skip-tree model is in the held-out sub-expression ( I am not sure if the skip-sequence models have the < MASK > tokens in addition to < PREDICT > or not ) . If that understanding is correct , then this might indicate that almost all of the masked < PREDICT > sub-sequences are not \u201c proper \u201d trees . Is that correct ? If so , does enforcing a variable portion ( say from 0 % to 100 % ) of the held-out sub-sequences to be sub-trees smoothly move the success rates reported in Table 2 towards the skip-tree \u2019 s performance ? ( 1-1 ) Another possibility for the poor performance could be that the skip-sequence models were not trained with < MASK > tokens ( I am just judging this based on the large difference between the skip-tree and the skip-tree ( no mask ) model ) . If this is the case , a fair comparison would also include the skip-sequence training with the additional < MASK > tokens . This allows the reader to understand which parts of the model result in the reported gains . ( 2 ) In figure 1 the authors illustrate the portion of the data that they use in their experiments for train and validation . It seems like the gray areas ( specifically the test set ) are not considered in the paper . What is confusing is that in Section 3 they do mention a train/validation/test split . But all the evaluation tasks defined in Section 5 in the paper seem to only refer to the validation data . There seems to be no indication of the use of a test dataset for the reported results ( in Tables 2 and 3 ) , which I find troubling . Especially because the authors do mention that they hyper-parameter optimized their models in Appendix A ( not mentioned using which part of the data ) . Can you please clarify this ( both in the rebuttal response and in the paper ) ? ( 3 ) How is a correct prediction assessed ? It is mentioned that the model \u2019 s performance is evaluated for \u201c exact match \u201d . However , as mentioned in the Assumptions task , predicting y=x for the ground truth x=y should be counted as a \u201c correct prediction \u201d . If such predictions are not being counted as \u201c correct \u201d , I highly encourage the authors to add an evaluation metric that considers mathematical equivalence of the generated predictions and the ground-truth . A symbolic solver like Sympy or Mathematica might be able to at least verify the equivalence of simpler expressions ( I am not sure how complex these expressions are ) . ( 3-1 ) To follow up on point ( 3 ) above , the authors mention that \u201c to make a correct prediction our language models thus have to understand which statements are more general and also know about naming conventions \u201d . I strongly disagree that this is a good way of measuring mathematical reasoning abilities . On the contrary , I think a model that has truly learned to mathematically reason , is one that ignores irrelevant details such as naming conventions or \u201c generality \u201d of the statement ( generality is subjectively used by the authors and I think references to it should be removed from the paper ) . ( 4 ) It is not clearly stated what the authors mean by \u201c new \u201d in section 6.1 . Are variations of already seen statements considered new ? ( e.g.if x=y is in the training set , would y=x be a new statement ? ) . I recommend the authors clearly define this in the paper . ( 5 ) It is not clear to me what the takeaway is for the sampling strategy . In the main text it was implied that the weighted sampling will be better because it allows one to choose non-leafs more often . However , the results shown in Tables 2 and 3 show that some scenarios seem to be better with the uniform sampling and some with the weighted sampling . Do the authors have any comments/discussion on that ? -- Smaller details : ( 1 ) Please use the correct citation command for references that are at the beginning of the sentence ( e.g.section 2 , Paragraph 2 , line 4 : Zhang et al. ( 2019 ) ) . ( 2 ) The authors mention that Lample and Charton ( 2020 ) \u201c requires that the inverse of the prediction task can be computed \u201d . Can you explain what this means ? ( 3 ) The explanation of the s-expression given in Section 3 lacks enough details . I wasn \u2019 t able to fully understand the representation . Perhaps an illustration with step by step labels can make it easier to understand . ( 4 ) Section 3 , Last paragraph : What does \u201c the split is defined on the theorems \u201d mean ? In general this paragraph was somewhat hard to follow . ( 5 ) What portion of the training data is omitted as a result of what is described in Section 4 , Paragraph 2 . ( 6 ) I find the first argument about the reason for adding < MASK > tokens somewhat subjective . Why does making the task harder result in better performance ? ( 7 ) I did not understand the multiple sample per dataset generation at all . Do the authors mean that for each math statement they generate n=100 samples with the < PREDICT > and < MASK > tokens ? If so , this implies that there will be 360k * 100 examples in the larger training set . But this is not consistent with the data stats provided in Table 1 . ( 8 ) I think it would be very useful if the authors add human-readable equivalents of the s-expressions presented in Appendix D to that Appendix . ( 9 ) Section 6 : What does 1M \u201c steps \u201d refer to ? Is that the number of model updates ? ( 10 ) It would be great to add a few sentences about what the reinforcement learning experiments in the DeepHOL prover are to make the paper more self-contained . -- Typos : Sec 1 , Parag 3 : which are capable ( to ) of generating \u2026 Sec 2 , Parag 4 : models to logic ( s )", "rating": "7: Good paper, accept", "reply_text": "Thanks a lot for the detailed comments ! We first address the question in the `` Cons '' and then proceed with the minor remarks . > ( 1 ) ... almost all of the masked < PREDICT > sub-sequences are not \u201c proper \u201d trees . Is that correct ? Correct. > does enforcing a variable portion ( say from 0 % to 100 % ) of the held-out sub-sequences to be sub-trees smoothly move the success rates reported in Table 2 [ ... ] ? This sounds very likely . We started an experiment where we mixed skip-sequence and skip-tree training examples at a 1:1 ratio . We will report on the results in a couple of days . > ( 1-1 ) Another possibility for the poor performance could be that the skip-sequence models were not trained with < MASK > tokens The skip-sequence tasks indeed do not have the < MASK > token . This should not put them at a disadvantage for the type inference , missing assumptions , and equality completion tasks , as they don \u2019 t contain the < MASK > token either . For the hard type inference task we tried to make clear that these numbers are not comparable by graying them out in Table 2 . Addressing this issue by masking out further subsequences with a < MASK > token in the skip-sequence tasks might be possible , but we do not see a reason to expect that the performance would surpass the performance on the regular type inference task , i.e.it is bounded by 45 % . ( 2 ) train/validation/test split : The observation is correct , we did not use the test set . We had only tested a handful of hyperparameters , as each experiment is somewhat expensive . But we agree that this is not optimal . We now reran the evaluation on the test set for the main experiment ( skip-tree , weighted ) . There are no significant changes in the outcome . We will update the paper with the new numbers in the next few days when the experiments for the other models are complete . ( 3 ) We only count exact syntactic matches ; i.e. \u201c y=x \u201d is not the same as \u201c x=y \u201d . This may indeed lead to an underestimation of the performance of the models ( as also suggested by the experiments in Table 3 ) . It would be nice to have a more semantical test , but this is harder than it may seem at first . We see three main issues with more semantic checks : First , Sympy and Mathematica ( or some neural theorem prover ) could only ever prove a portion of unknown size of these equalities and so our measurement of the language models would depend on how good these external algorithms are . Second , Sympy and Mathematica have no support for higher-order logic and we do not expect them to work particularly well . At the very least , their performance would depend highly on the quality of the translation into a compatible format . Third and last , for each evaluation task , the exact semantical test would need to differ : for type inference and equality completion , the semantic equivalence of the prediction and the ground truth is not the right criterion . To keep the measurements interpretable and reproducible , we therefore prefer to restrict the measurement in Table 2 to syntactic equivalence . ( 3-1 ) This paragraph was supposed to shed light on other aspects of mathematics that the models have to learn besides the main reasoning task . We improved the wording . ( 4 ) Throughout the paper we consider syntactic equality . We slightly changed the final sentence in the paragraph \u201c How often are predictions true and new ? \u201d to emphasize that we consider syntactic matches . ( 5 ) There is not a clear winner here . We adapted the wording in Section 4 to avoid setting other expectations . Originally we expected this to make a difference , but it turned out not to be that significant ."}, {"review_id": "YmqAnY0CMEy-3", "review_text": "This paper extends the idea of language-model style self-supervised learning approach to training logical reasoning models from unlabeled mathematical expressions . The main idea is to develop a skip-tree proxy task ( self-supervision ) for training the encoder-decoder architecture . The skip-tree method masks out a complete sub-tree in the input and linearizes it into a sequence in the form of S-expression . The model is required to predict the masked subtree at the decoder end . The paper also proposes several new reasoning tasks for evaluating the model performance . Experimental results show that models learned from this task significantly outperform those trained on the skip-sequence task . Furthermore , the model also exhibits good conjecturing ability in generating quite reasonable amount of new theorems that are provable and useful , which is quite encouraging and impressive . I \u2019 m curious about why the method only masks out one sub-tree for prediction while treating other masked sub-trees as auxiliary part ( by increasing the difficulty of the task ) . This seems to be a waste of self-supervision signals . It might be more ( sample ) efficient to predict all the masked part in one sample just as what BERT did . It would be helpful to provide experimental justification for this specific design choice if deemed so . The current pretrained model is used on the newly created tasks without any finetuning because these tasks are similar to the mask prediction problem . This is interesting , but I \u2019 m wondering if the proposed method be used in various other downstream reasoning tasks ? For example , even with certain finetuning , could the pretrained reasoning model be used in other downstream tasks that are less similar to the pretraining objective ? Since it is claimed earlier that `` In contrast , we train language models on an unsupervised proxy task that does not require labeled data and can thus be applied to almost any source of mathematical expressions \u201d ( Section 2 ) , it would be necessary to further evaluate the pretrained models on other popular logic reasoning tasks . This would show how generalizable the skip-tree pretrained model is on various other downstream reasoning tasks , which would greatly enhance the strength of the work .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review and the great suggestions . > I \u2019 m curious about why the method only masks out one sub-tree for prediction [ ... ] BERT does not directly apply to encoder-decoder models , so the methodology has to be adapted . In principle , we could concatenate multiple masked-out subexpressions ( similar to T5 ) until we reach the context length of the decoder . This may indeed be an improvement to our current training method . > I \u2019 m wondering if the proposed method can be used in various other downstream reasoning tasks ? For example , > even with certain finetuning , could the pretrained reasoning model be used in other downstream tasks that are > less similar to the pretraining objective ? In this paper we wanted to focus on the finding that self-supervised training alone can lead to interpretable mathematical reasoning abilities , which , we believe , is an astonishing development by itself . It is very plausible that using skip-tree training as a pre-training and then fine-tune on other reasoning tasks improves the performance on those tasks . However , other papers on the same dataset do not use sequence-to-sequence models , which makes it non-trivial to compare them to our approach . Comparisons on datasets from entirely different sources come with technical problems , such as different tokenization . While it is probably possible to overcome these problems , those discussions would distract from the core insight presented in this paper ."}], "0": {"review_id": "YmqAnY0CMEy-0", "review_text": "The authors propose a self-supervised learning task to enhance the reasoning capabilities of machine learning models on mathematical formulas and to perform conjecturing in higher order logic . The task consists in masking out specific portions of mathematical statements and predict them from the surrounding parts . The task can ( i ) be used during training , to provide supervisory signal to the machine learning model and to increase the effective size of the otherwise small training dataset , and ( ii ) be used during testing , to evaluate the reasoning capabilities of the learnt models by masking out the mathematical statements at different level of granularities . The authors perform an extensive experimental analysis and provide evidence on the utility of using self-supervised learning in the context of theorem proving . Overall , the paper is clearly written and the bibliography complete . Also , the experimental analysis is undoubtedly valuable for the machine learning community . In fact , it provides additional empirical evidence to the works of [ 1-2 ] , in the sense that self-supervised learning can be used not only as a pre-training stage for the machine learning models , but also as a task to perform conjecturing . There are major issues though . In particular , there are issues in terms of the originality of the proposed task and the reproducibility of the experiments , thus obscuring the positive aspects of the paper . Please , see below for more detailed comments . In lieu of this , I consider the paper marginally below the acceptance threshold and therefore recommend for an initial rejection . Nevertheless , I 'm willing to raise my score if the authors properly address the issues highlighted below . DETAILED COMMENTS Originality : The proposed task seems to be identical to the task in [ 3 ] , except for the fact that the task operates on mathematical statements rather than on natural language ones . Importantly , both kinds of sentences/statements share a tree-structured representation , which makes the task in [ 3 ] trivially applicable to the mathematical context . Can the authors highlight other differences or explain if this is not correct ? Reproducibility : First of all , it 's extremely important to add in the section about results and discussion detailed information about the computational resources and the time required for training , as well as the size of the machine learning model used in the experiments . In fact , it is well-known from other domains , like vision and natural language processing , that the performance of self-supervised learning models increase proportionally with the size of the model , at the expense of the training computational resources . Furthermore , it 's important to discuss about the methodology used to choose the hyperparameters . Appendix A only states that `` we explored encoders and decoders with up to 12 layers and various learning rates and intermediate sizes . '' Finally , do the authors plan to release the code ? FURTHER IMPORTANT COMMENTS Answering to this following comments could be helpful to clarify about the originality and the novelty of the proposed task . Distinguishing between training and evaluation tasks and saying that `` several tasks '' are proposed ( i.e.in abstract , introduction , section 5 and conclusions ) can be misleading for the reader , because he/she could think that they are different . Nevertheless , these tasks are all equivalent to the main skip-tree task , differing only in the way the terms are masked . Is that correct ? If so , I would suggest to rephrase by saying that the main skip-tree task is quite flexible at masking statements . In fact , depending on which portion of statement is masked , the model can be induced to perform reasoning at different level of abstraction . Based on this , experiments are categorised according to the different ways of masking . Furthermore , it is not completely clear when the authors says that `` most previous works ... have focused ... in supervised training settings . In contrast , we train language models on an unsupervised proxy task '' ( related work , but also introduction ) . Is the proposed training unsupervised ? First of all , the data used in training contains only valid theorems . This can be already considered as a supervised information , because collecting valid theorems requires human effort . Secondly , the evaluation tasks are essentially similar to the training task and therefore I would consider this more as a supervised learning strategy . Could you please elaborate more on this , also in the text ? MINOR COMMENTS In the experiments about conjecturing ( 2nd paragraph ) , can you provide more details on how you generated the free-form conjectures ? Furthermore , how is novelty of generated statements defined ? Consider the following examples for conjecturing on assumptions and free-form conjecturing . Conjecturing from assumptions Imagine you have a+b=0 implies a=-b and the task is given by < PREDICT > implies a=-b Is the statement ' a+b=c and c=0 ' considered new according to your definition ? Free-form conjecturing Imagine you have < THEOREM > for all x , x=x and the task is given by < THEOREM > < PREDICT > Is the theorem 'for all y , y=y ' considered new according to your definition ? [ 1 ] Li et al.Modelling High-Level Mathematical Reasoning in Mechanised Declarative Proofs . arXiv 2020 [ 2 ] Polu and Sutskever . Generative Language Modelling for Automated Theorem Provers . arXiv 2020 [ 3 ] Zhang et al.PEGASUS : Pre-training with Extracted Gap-sentences for Abstractive Summarization . ICML 2020 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # UPDATE The paper provides important and novel empirical observations about the use of machine learning to perform mathematical reasoning . The authors have addressed and clarified some doubts about the originality of their idea and the reproducibility of their experiments in the discussion phase . I believe that the paper is now ready for publication and I 'm happy to recommend for its acceptance .", "rating": "7: Good paper, accept", "reply_text": "Thanks for the detailed review ! Originality : Pegasus [ 2 ] is a task defined on natural language and relies on the notion of sentences , while our approach relies on the tree structure of expressions . Pegasus only considers \u201c whole sentences \u201d [ 2 , page 2 , paragraph 4 ] , so it does not consider tree structure . Additionally , our training task can create a much wider variety of training examples than PEGASUS as the number of subtrees is closer to the number of tokens . ( We exploit that when we sample many training examples from each mathematical statement . ) We actually think that the entire development of training tasks from MASS/UNILM ( predict subsequences ) to T5 ( predict multiple subsequences ) to PEGASUS ( predict sentences ) and finally to this work ( predict subtrees ) were all incremental steps when only examined for their technical merits . But their impact is undeniably large . The main novelty of this paper is perhaps more in the conceptual domain as it provides a new perspective on how to achieve mathematical reasoning abilities without supervised training data ( as also AnonReviewer1 notes ) . Reproducibility : We added a description of the computational resources , the size of the models , and our evaluation methodology . Larger models seem to perform better indeed . We have preliminary results that suggest this ( for a model with 110M parameters ) and are happy to add them to the paper , if the reviewers agree that this would improve the paper . Code : We will release the code for training data generation . Choice of hyperparameters : Because each training run is relatively costly , we did not perform a full grid search for the hyperparameters . Instead we consulted the literature and expert opinions and tested just a couple of hyperparameters . For the skip-sequence models we tried a more extensive list of hyperparameters , as we wanted to make sure that the negative results here are stable under different choices of hyperparameters . Difference between training task and evaluation tasks : The evaluation tasks and the training task indeed share the same format , i.e.predict a part of a larger expression . This is intentional as it allows to test the model without fine-tuning on reasoning tasks . The distributions of the training and evaluation tasks are , in fact , disjoint , because the evaluation tasks are derived from different source data , the validation/test theorems . We improved the description of the relationship between training and evaluation tasks in the beginning of Section 5 . Unsupervised vs self-supervised : We corrected the occurrence of \u201c unsupervised \u201d to \u201c self-supervised \u201d , which is a better term . On the question whether our technique qualifies as being unsupervised/self-supervised : It is a great question where the line between supervised and self-supervised techniques is exactly . We believe that our use of \u201c self-supervised \u201d is in line with other papers : Self-supervised translation techniques use monolingual corpora , which , with the same argument that reviewer 2 provided , could be seen as \u201c supervised information \u201d . We think that the use of arbitrary subexpressions as prediction targets is general enough to be applied to arbitrary sources . There is also nothing that keeps us from applying skip-tree training to statements that are not true . In one sense , in which we are clearly self-supervised is that the training data preparation does not require human labelers to produce labels . Generation of conjectures : We improved the description of the free-form conjecturing task . The procedure is quite simple : We simply ask the model to produce theorems by presenting the input sequence ( < theorem > < PREDICT > ) . So , the subexpression the models has to fill in must be an entire mathematical statement , and the < theorem > token ( hopefully ) makes the model produce a statement that could be a theorem ( as opposed to the < goal > tag , which indicates intermediate statements of proofs ) . Given only this single input sequence , we use a beam search with beam width 1024 to produce enough outputs for a meaningful evaluation . How we measure the novelty of terms : We consider strict syntactic ( string ) equality ."}, "1": {"review_id": "YmqAnY0CMEy-1", "review_text": "Quality The paper is quite well written and the experiments seem well thought out . Please see specific comments below . Clarity The S-expression needs more explanation for the paper to be self-contained . What is ' A ' in ( v A x ) ? For those of us who might not be very familiar with the datasets , it might be helpful if the paper to demonstrate more examples as to what these proofs are translated to , in mathematical notation or textual explanations . There are a few snippets here and there but I do n't think I get a good grasp of what kinds of proofs we deal with . For example , the theorem examples in Appendix section D seem quite lengthy . With effort it should be decoded into normal text by readers , but it would be more convenient to demonstrate it directly . In addition , the free-form conjecturing evaluation can be more clear . In this section , it does n't entirely explain in details what makes the generated statements 'new ' . Is it only exact match with the training set ? If that 's the case , how robust is exact matching on measuring the novelty ? On the usefulness of the conjectures , there 's not much explanation on the RL experiments + DeepHOL theorem prover . What is it supposed to do / how robust does it measure usability ? Question : a beam search of 1024 seems rather large . How does the result look if no beam search is used ? Originality The introduced method 'skip-tree ' is not that different from the usual self-supervised training techniques used to train language models . This feature does not contribute to the novelty that much . However , this approach is a new take on learning mathematical reasoning with self-supervised learning , rather than supervised learning like in other previous work . Significance Deep learning for mathematical proofs is an interesting direction and is relatively unexplored , compared to other application areas . High-Level Pros & Cons Pros - The claim / conclusion for this work that self-supervised training can lead to mathematical reasoning is rather intriguing . - The proposed skip-tree technique seems to make a lot of difference for training . - Decent ablation study ( not using < MASK > token , skip-sequence instead of skip-tree ) . - The evaluation tasks introduced seem interesting . Cons - I am not 100 % convinced that the `` mathematical reasoning '' demonstrated is beyond pattern recognition . I understand that the evaluation tasks are evaluated on validation set theorems which are not seen during training . However , how can we be sure that this is mathematical reasoning versus the model recognizing similar patterns ( not necessarily exact match ) More explanations on this would be appreciated . - Low to moderate novelty for the skip-tree technique . Overall , I learned a lot from this paper and I believe others can benefit from it as well .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the insightful review . > S-expressions : What is ' A ' in ( v A x ) ? \u2018 A \u2019 is a generic type variable . The expression ( v A x ) is a variable named x that could still have any type . We expanded the description of S-expressions . > the theorem examples in Appendix section D seem quite lengthy . With effort it should be decoded into normal text by readers , but it would be more convenient to demonstrate it directly . For the missing assumptions and equality completion tasks , we had already provided human readable versions of the ground-truth , but somehow we forgot to add them for the type inference and hard type inference tasks . This is now fixed . > Is it only an exact match with the training set ? Yes , we only look for exact matches . We updated the formulation to make this clear . > What is it supposed to do / how robust does it measure [ usefulness ] ? We count a conjectured theorem as useful only if it is picked up as a premise by the DeepHOL theorem prover . This measurement only says that for some proof this premise was necessary ( DeepHOL includes a proof pruning step that , for each premise , tests if dropping the premise was still a valid proof ) . There may still be alternative proofs that do not require this particular premise , and even if a conjecture is not used as a premise , there might be theorems for which it would be very useful , but we may not have those in our list of tasks . So this should only be seen as a rough approximation of usefulness . We believe that the take-away is that this method managed to produce at least some new and useful theorems . > Question : a beam search of 1024 seems rather large . How does the result look if no beam search is used ? We used beam width 1024 only for the conjecturing experiments where we wanted to generate many different expressions . For the other evaluations we use a beam search with width 8 . If we only generate a single trace , our best models reach 25 % to 30 % on the missing assumptions and equality completion tasks . > I understand that the evaluation tasks are evaluated on validation set theorems which are not seen during training . However , how can we be sure that this is mathematical reasoning versus the model recognizing similar patterns ( not necessarily exact match ) We are not sure this is a \u201c con \u201d . We believe that our experiments show that , at the very least , \u201c pattern recognition \u201d can effectively simulate some forms of mathematical reasoning . We hope that this will contribute to the wider discussions in the community and help to understand the fundamental question of the relationship between reasoning and \u201c pattern recognition \u201d ."}, "2": {"review_id": "YmqAnY0CMEy-2", "review_text": "-- Summary : This paper proposes a skip-tree training task . The authors show that self-supervised language models ( the Transformer architecture to be exact ) trained on the proposed skip-tree training task for mathematic theorem proving enable mathematical reasoning capabilities . Moreover , no fine-tuning is required to achieve the reported reasoning capabilities . They compare the mathematical reasoning abilities of the skip-tree training task with skip-sequence and show an impressive performance improvement . Another interesting result is studying whether any useful ( novel ) conjectures can be generated by the model . -- Overall assessment : I really enjoyed reading this paper . The authors work on an interesting problem that has been gaining popularity in the last few years . I believe that this problem domain is a suitable test bed to investigate the reasoning capabilities of language models . The authors propose a skip-tree training task and show that it significantly improves the performance of skip-sequence training . The idea is simple and nice : instead of masking out arbitrary sub-sequences ( which is the case for skip-sequence ) , the authors propose to mask out sub-trees . The authors provide extensive ablation studies and evaluations to justify several design choices and to show the reasoning abilities of their model . I do have some comments about presentation clarity ( look below ) , that I hope the authors can address during the rebuttal . Moreover , I am a bit skeptical about the almost 0 performance of the skip-sequence task under some scenarios . I will clarify this as well in the cons section below . -- Pros : ( 1 ) I like the idea of skip-tree training for structured problems . It seems to improve the performance of the model dramatically ( from almost not being able to predict AT ALL to near perfect prediction \u2014 in the Hard inference problem for example ) . I do have a comment about this in the cons section . ( 2 ) The authors design a set of mathematical reasoning evaluations that are used as down-stream tasks ( without fine-tuning ) to evaluate the self-supervised trained models . They show good performance on all the tasks compared to their baselines . ( 3 ) The authors provide a set of ablation studies to justify their design choices . ( 4 ) I liked the conjecture experiment that evaluated whether the model can generate novel provable conjectures . -- Cons : ( 1 ) The performance of skip-sequence training on three out of the four evaluation tasks in Table 2 ( Hard type inference , Assumptions , Equalities ) are extremely poor . It is my understanding that the only difference between these models and the proposed skip-tree model is in the held-out sub-expression ( I am not sure if the skip-sequence models have the < MASK > tokens in addition to < PREDICT > or not ) . If that understanding is correct , then this might indicate that almost all of the masked < PREDICT > sub-sequences are not \u201c proper \u201d trees . Is that correct ? If so , does enforcing a variable portion ( say from 0 % to 100 % ) of the held-out sub-sequences to be sub-trees smoothly move the success rates reported in Table 2 towards the skip-tree \u2019 s performance ? ( 1-1 ) Another possibility for the poor performance could be that the skip-sequence models were not trained with < MASK > tokens ( I am just judging this based on the large difference between the skip-tree and the skip-tree ( no mask ) model ) . If this is the case , a fair comparison would also include the skip-sequence training with the additional < MASK > tokens . This allows the reader to understand which parts of the model result in the reported gains . ( 2 ) In figure 1 the authors illustrate the portion of the data that they use in their experiments for train and validation . It seems like the gray areas ( specifically the test set ) are not considered in the paper . What is confusing is that in Section 3 they do mention a train/validation/test split . But all the evaluation tasks defined in Section 5 in the paper seem to only refer to the validation data . There seems to be no indication of the use of a test dataset for the reported results ( in Tables 2 and 3 ) , which I find troubling . Especially because the authors do mention that they hyper-parameter optimized their models in Appendix A ( not mentioned using which part of the data ) . Can you please clarify this ( both in the rebuttal response and in the paper ) ? ( 3 ) How is a correct prediction assessed ? It is mentioned that the model \u2019 s performance is evaluated for \u201c exact match \u201d . However , as mentioned in the Assumptions task , predicting y=x for the ground truth x=y should be counted as a \u201c correct prediction \u201d . If such predictions are not being counted as \u201c correct \u201d , I highly encourage the authors to add an evaluation metric that considers mathematical equivalence of the generated predictions and the ground-truth . A symbolic solver like Sympy or Mathematica might be able to at least verify the equivalence of simpler expressions ( I am not sure how complex these expressions are ) . ( 3-1 ) To follow up on point ( 3 ) above , the authors mention that \u201c to make a correct prediction our language models thus have to understand which statements are more general and also know about naming conventions \u201d . I strongly disagree that this is a good way of measuring mathematical reasoning abilities . On the contrary , I think a model that has truly learned to mathematically reason , is one that ignores irrelevant details such as naming conventions or \u201c generality \u201d of the statement ( generality is subjectively used by the authors and I think references to it should be removed from the paper ) . ( 4 ) It is not clearly stated what the authors mean by \u201c new \u201d in section 6.1 . Are variations of already seen statements considered new ? ( e.g.if x=y is in the training set , would y=x be a new statement ? ) . I recommend the authors clearly define this in the paper . ( 5 ) It is not clear to me what the takeaway is for the sampling strategy . In the main text it was implied that the weighted sampling will be better because it allows one to choose non-leafs more often . However , the results shown in Tables 2 and 3 show that some scenarios seem to be better with the uniform sampling and some with the weighted sampling . Do the authors have any comments/discussion on that ? -- Smaller details : ( 1 ) Please use the correct citation command for references that are at the beginning of the sentence ( e.g.section 2 , Paragraph 2 , line 4 : Zhang et al. ( 2019 ) ) . ( 2 ) The authors mention that Lample and Charton ( 2020 ) \u201c requires that the inverse of the prediction task can be computed \u201d . Can you explain what this means ? ( 3 ) The explanation of the s-expression given in Section 3 lacks enough details . I wasn \u2019 t able to fully understand the representation . Perhaps an illustration with step by step labels can make it easier to understand . ( 4 ) Section 3 , Last paragraph : What does \u201c the split is defined on the theorems \u201d mean ? In general this paragraph was somewhat hard to follow . ( 5 ) What portion of the training data is omitted as a result of what is described in Section 4 , Paragraph 2 . ( 6 ) I find the first argument about the reason for adding < MASK > tokens somewhat subjective . Why does making the task harder result in better performance ? ( 7 ) I did not understand the multiple sample per dataset generation at all . Do the authors mean that for each math statement they generate n=100 samples with the < PREDICT > and < MASK > tokens ? If so , this implies that there will be 360k * 100 examples in the larger training set . But this is not consistent with the data stats provided in Table 1 . ( 8 ) I think it would be very useful if the authors add human-readable equivalents of the s-expressions presented in Appendix D to that Appendix . ( 9 ) Section 6 : What does 1M \u201c steps \u201d refer to ? Is that the number of model updates ? ( 10 ) It would be great to add a few sentences about what the reinforcement learning experiments in the DeepHOL prover are to make the paper more self-contained . -- Typos : Sec 1 , Parag 3 : which are capable ( to ) of generating \u2026 Sec 2 , Parag 4 : models to logic ( s )", "rating": "7: Good paper, accept", "reply_text": "Thanks a lot for the detailed comments ! We first address the question in the `` Cons '' and then proceed with the minor remarks . > ( 1 ) ... almost all of the masked < PREDICT > sub-sequences are not \u201c proper \u201d trees . Is that correct ? Correct. > does enforcing a variable portion ( say from 0 % to 100 % ) of the held-out sub-sequences to be sub-trees smoothly move the success rates reported in Table 2 [ ... ] ? This sounds very likely . We started an experiment where we mixed skip-sequence and skip-tree training examples at a 1:1 ratio . We will report on the results in a couple of days . > ( 1-1 ) Another possibility for the poor performance could be that the skip-sequence models were not trained with < MASK > tokens The skip-sequence tasks indeed do not have the < MASK > token . This should not put them at a disadvantage for the type inference , missing assumptions , and equality completion tasks , as they don \u2019 t contain the < MASK > token either . For the hard type inference task we tried to make clear that these numbers are not comparable by graying them out in Table 2 . Addressing this issue by masking out further subsequences with a < MASK > token in the skip-sequence tasks might be possible , but we do not see a reason to expect that the performance would surpass the performance on the regular type inference task , i.e.it is bounded by 45 % . ( 2 ) train/validation/test split : The observation is correct , we did not use the test set . We had only tested a handful of hyperparameters , as each experiment is somewhat expensive . But we agree that this is not optimal . We now reran the evaluation on the test set for the main experiment ( skip-tree , weighted ) . There are no significant changes in the outcome . We will update the paper with the new numbers in the next few days when the experiments for the other models are complete . ( 3 ) We only count exact syntactic matches ; i.e. \u201c y=x \u201d is not the same as \u201c x=y \u201d . This may indeed lead to an underestimation of the performance of the models ( as also suggested by the experiments in Table 3 ) . It would be nice to have a more semantical test , but this is harder than it may seem at first . We see three main issues with more semantic checks : First , Sympy and Mathematica ( or some neural theorem prover ) could only ever prove a portion of unknown size of these equalities and so our measurement of the language models would depend on how good these external algorithms are . Second , Sympy and Mathematica have no support for higher-order logic and we do not expect them to work particularly well . At the very least , their performance would depend highly on the quality of the translation into a compatible format . Third and last , for each evaluation task , the exact semantical test would need to differ : for type inference and equality completion , the semantic equivalence of the prediction and the ground truth is not the right criterion . To keep the measurements interpretable and reproducible , we therefore prefer to restrict the measurement in Table 2 to syntactic equivalence . ( 3-1 ) This paragraph was supposed to shed light on other aspects of mathematics that the models have to learn besides the main reasoning task . We improved the wording . ( 4 ) Throughout the paper we consider syntactic equality . We slightly changed the final sentence in the paragraph \u201c How often are predictions true and new ? \u201d to emphasize that we consider syntactic matches . ( 5 ) There is not a clear winner here . We adapted the wording in Section 4 to avoid setting other expectations . Originally we expected this to make a difference , but it turned out not to be that significant ."}, "3": {"review_id": "YmqAnY0CMEy-3", "review_text": "This paper extends the idea of language-model style self-supervised learning approach to training logical reasoning models from unlabeled mathematical expressions . The main idea is to develop a skip-tree proxy task ( self-supervision ) for training the encoder-decoder architecture . The skip-tree method masks out a complete sub-tree in the input and linearizes it into a sequence in the form of S-expression . The model is required to predict the masked subtree at the decoder end . The paper also proposes several new reasoning tasks for evaluating the model performance . Experimental results show that models learned from this task significantly outperform those trained on the skip-sequence task . Furthermore , the model also exhibits good conjecturing ability in generating quite reasonable amount of new theorems that are provable and useful , which is quite encouraging and impressive . I \u2019 m curious about why the method only masks out one sub-tree for prediction while treating other masked sub-trees as auxiliary part ( by increasing the difficulty of the task ) . This seems to be a waste of self-supervision signals . It might be more ( sample ) efficient to predict all the masked part in one sample just as what BERT did . It would be helpful to provide experimental justification for this specific design choice if deemed so . The current pretrained model is used on the newly created tasks without any finetuning because these tasks are similar to the mask prediction problem . This is interesting , but I \u2019 m wondering if the proposed method be used in various other downstream reasoning tasks ? For example , even with certain finetuning , could the pretrained reasoning model be used in other downstream tasks that are less similar to the pretraining objective ? Since it is claimed earlier that `` In contrast , we train language models on an unsupervised proxy task that does not require labeled data and can thus be applied to almost any source of mathematical expressions \u201d ( Section 2 ) , it would be necessary to further evaluate the pretrained models on other popular logic reasoning tasks . This would show how generalizable the skip-tree pretrained model is on various other downstream reasoning tasks , which would greatly enhance the strength of the work .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review and the great suggestions . > I \u2019 m curious about why the method only masks out one sub-tree for prediction [ ... ] BERT does not directly apply to encoder-decoder models , so the methodology has to be adapted . In principle , we could concatenate multiple masked-out subexpressions ( similar to T5 ) until we reach the context length of the decoder . This may indeed be an improvement to our current training method . > I \u2019 m wondering if the proposed method can be used in various other downstream reasoning tasks ? For example , > even with certain finetuning , could the pretrained reasoning model be used in other downstream tasks that are > less similar to the pretraining objective ? In this paper we wanted to focus on the finding that self-supervised training alone can lead to interpretable mathematical reasoning abilities , which , we believe , is an astonishing development by itself . It is very plausible that using skip-tree training as a pre-training and then fine-tune on other reasoning tasks improves the performance on those tasks . However , other papers on the same dataset do not use sequence-to-sequence models , which makes it non-trivial to compare them to our approach . Comparisons on datasets from entirely different sources come with technical problems , such as different tokenization . While it is probably possible to overcome these problems , those discussions would distract from the core insight presented in this paper ."}}