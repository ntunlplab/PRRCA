{"year": "2021", "forum": "TiXl51SCNw8", "title": "BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network Quantization", "decision": "Accept (Poster)", "meta_review": "The paper explores a solution for mixed precision quantization. The authors view the weights in their binary format, and suggest to prune the bits in a structured way. Namely, all weights in the same layer should have the same precision, and the bits should be pruned from the least significant to most significant. This point of view allows the authors to exploit techniques used for weight pruning, such as L1 and group lasso regularization.\n\nAlthough the field of quantization and model compression/acceleration is quite mature by now and has a large body of works, this paper is novel in its approach. Although the improvements provided over SoTA results are not very large, I believe that the novelty of the approach would make this paper a welcome addition to ICLR.\n\nThere are a few issues to be dealt with pointed out by the reviewers such as confusing terminology or required clarifications, but these are minor revisions that I trust the authors will be able to add to their paper.\n", "reviews": [{"review_id": "TiXl51SCNw8-0", "review_text": "This paper basically proposed to learn the quantization bits ( precision ) in each layer . Specially , weights are constructed with binary representation as $ W_s = \\ [ W_s^1 , ... , W_s^b\\ ] $ . During training , $ W_s^i $ is relaxed to $ \\in \\ [ 0 , 2\\ ] $ . And a group sparsity is imposed to all $ W_s^i $ for all weights in a layer , leading to certain $ W_s^i \\to 0 $ , thus cancelling the bit allocation in $ i $ -th . Experimental results is promising . Pros : 1.It is interesting to see that weights are represented in binary format , while each bit is trained in a full-precision scheme . Cons : 1.Training process is intricated : one has to tune the penalty in group sparsity . Also , training is separated in several steps : training and post-training finetuning . Questions : 1 . After determining the quantization bit in ( `` fake '' ) quantization training ( although $ W_q $ is quantized but $ W_s^i $ is not exactly binary , which is the exactly weight we want ) using Eq.5 . Author mention in `` Re-quantization and precision adjustment '' that $ W_q^ { ' } $ is converted to binary value . But how to deal with the precision loss here ? i.e.from $ W_s^i \\in \\ [ 0,2\\ ] $ to $ \\ { 0 , 1\\ } $ 2 . Author mentioned that DoReFa-Net is adopted to finetune the trained model . Since DoReFa-Net use tanh to constrain value to $ [ 0,1 ] $ . it seems there is no connection to the proposed quantization scheme ( Eq.6 ) . How to exactly finetune ? 3.Why is necessary for $ W_s $ to be separated into postive and negative part ( $ W_p $ , $ W_n $ ) in processing ? 4.Since $ W_s^i $ is float and trainable , is it necessary to incorporate a trainable $ s $ ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your constructive feedback on our paper ! To answer your questions : 1 . This question is similar to question 2 of reviewer 4 . As introduced at the top of page 6 , effective weight $ W=s W_q $ used in the forward pass STE remains unchanged before and after the re-quantization and precision adjustment . This is ensured by the use of STE in Equation ( 3 ) where the forward pass is always computed with the quantized weight , and the change in scaling factor $ s $ after precision adjustment , as illustrated in Equation ( 6 ) . Therefore , there will not be any change to the model performance and the final cross-entropy loss before and after the re-quantization and precision adjustment . Also , note that we allow the n-bit weight to be re-quantized to ( n+1 ) -bit after the re-quantization , so there won \u2019 t be any precision loss when converting from $ W^ { ( i ) } _ { s } \\in [ 0,2 ] $ to 0,1 2 . Thanks for pointing this out . It seems like we made a mistake here confusing the STE used in ( Polino et al. , 2018 ) with the one used in DoReFa . In this work we use the STE with a linear scaling function as proposed in ( Polino et al. , 2018 ) to quantize both weight and activation during the finetuning process . Specifically , weight and activation are first linearly scaled to the range of [ 0,1 ] before going through uniform quantization in the STE . We will correct this in the revised version . 3.The reason why we separate Wp and Wn is to assist to observe bit-level sparsity when both positive and negative elements are presented in a weight tensor , such that we can ignore the sign of the weight element and only focusing on the bit representation of the absolute values . This will simplify the implementation of re-quantization and precision adjustment . Actually , in the training process with floating-point variables , dividing Wp and Wn may not be required . As the gradient passed from the STE in Equation ( 3 ) onto the pair of Wp and Wn are always symmetric , training with separated variables is equivalent to training with Ws = Wp-Wn directly . 4.As discussed in Section 3.1 , although Ws is float and trainable , it will always be in the range of [ 0,2 ] for all the layers and will be quantized uniformly with the same step size , which may not be adequate to capture the difference of dynamic range across different layers , hurting the performance of some layers after quantization . In the meantime , using a scaling factor that always scale the largest element to 1 as done in ( Polino et al. , 2018 ) will make the dynamic precision reduction impossible . This is why we keep the scaling factor as a separate trainable variable , allow it to fit into the dynamic range requirement of each layer while not preventing the precision reduction of the bit representation . Reference : A. Polino , R. Pascanu , and D. Alistarh . Model compression via distillation and quantization . arXiv preprint arXiv:1802.05668 , 2018 ."}, {"review_id": "TiXl51SCNw8-1", "review_text": "This paper introduces a new method to quantize neural networks in a differentiable manner . Proposed method applies the group lasso on the bit-planes of the weight parameters to let certain LSBs in each layer to be zero-ed out . STE is used to train the binary representation of each bit-plane and the sign of weights during the training . Results demonstrate that the proposed method can achieve higher accuracy and compression ratio compared to previous studies . I think that the idea of introducing group lasso to prune the entire bit-plane is very interesting and the paper is well written , but some additional analysis will be helpful . 1.I think the result must be compared with more recent papers , such as LSQ ( Esser , Steven K. , et al. , 2020 ) . For example , LSQ demonstrates that it acheives 75.8 % top-1 and 92.7 % top-5 accuracy with 3/3-bit model ( weight/activation ) on ResNet-50 . However , according to the appendix C , proposed method seems to achieve only 92.16 % when the activation is quantized to 3-bit . 2.This is more of a question than suggestion : after the requantization , would the batch-normalization layers function correctly ? It seems like the parameters of batch normalization layers are kept the same after the requantization , while the requantization will impact the distribution of activations . Analysis on the difference between the distribution of the activation before the batch normalization layer before and afther the requantization will be helpful to see if the distribution of the activations really do not differ that much , or the batch-norm layer will just adapt to the occasion .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank your feedback on our paper and are glad that you find our paper interesting . For your questions : 1 . Thanks for bringing up the results in LSQ . The result mentioned in the review is evaluated with 3-bit ResNet-50 model on ImageNet , where LSQ achieves 75.8 % top-1 accuracy with a 10.67x compression rate comparing to the full-precision model . As reported in Table 3 , BSQ can achieve 75.3 % top-1 accuracy with a higher 11.90x compression rate on the ResNet-50 model , so the result is still competitive . We have added this comparison to Table 3 in the paper . As we consider the scale s as trainable variables in BSQ , it is similar to the training process proposed in LSQ . Although we haven \u2019 t made a dedicated analysis on the gradient w.r.t.s as done in LSQ . It would be interesting to further improve BSQ with the update rule proposed in LSQ during the training process in future work . 2.As introduced at the top of page 6 , the effective weight $ W=s W_q $ used in the forward pass STE remains unchanged before and after the re-quantization and precision adjustment . This is ensured by the use of bit representation STE in Equation ( 3 ) , where the forward pass is always computed with quantized weight , and the adjustment of scaling factor s after precision adjustment as illustrated in Equation ( 6 ) . Therefore , there will not be any change to the activation and the final cross-entropy loss before and after the re-quantization and precision adjustment , and will not affect the functionality of batch norm layers ."}, {"review_id": "TiXl51SCNw8-2", "review_text": "Quantization of weights in DNNs is a very effective way to reduce the computational and storage costs which can enable deployment of deep learning at the edge . However , determining suitable layer-wise bit-widths while training is a difficult task due to the discrete nature of the optimization problem . This paper proposes to utilize bit-level sparsity as a proxy for bit-width and employ regularization techniques to formulate the problem so that precision can be reduced while training the model . The method proposed by the authors is sound . It leverages insights that have been employed in a neighboring area ( pruning via regularization ) and re-purposes those to the problem of quantization . The empirical evaluation is robust as well . One issue I have with the proposed method is that the parameter space is expanded by a large amount . Since for every scalar weight , we end up with a collection of binary weights . Does n't this make training more difficult ? It would be nice to discuss this issue . And more importantly how does the extra effort compare to other approaches ( such as Dorefanet and others ) . Regarding the proposed regularization technique . Lasso ( least absolute shrinkage and selection operator ) is , as far as I am aware , an optimizer that regularizes the L_1 norm of the parameters . Why is the regularizer in eq . ( 4 ) using the L_2 norm ? Maybe I am missing something and/or there is an inconsistency is the notation/wording . The authors do a good job comparing with related works . However , one of the main early claims is that all works trying to find per-layer precision do so manually . This is not true , there have been some works that have done exactly that . One example is [ 1 ] which analytically determines precisions at all layers using a noise gain concept . It would be nice to contrast with such works as well . Minor issue : 'comp x ' is used in the results ( tables ) without being defined . It appears to indicate 'compression ratio ' . This has to be explicitly stated at least once ( maybe in the captions ) . references : [ 1 ] Sakr , Charbel , and Naresh R. Shanbhag . `` Per-tensor fixed-point quantization of the back-propagation algorithm . '' ICLR 2019 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank your feedback on our paper and are glad that you find our paper technically sound . * * For the issue on parameter space : * * As illustrated in Equation ( 3 ) and discussed below the equation , the gradient of the cross-entropy loss w.r.t.each bit of the scaled weight $ W_s $ is not independent . In fact , the gradient w.r.t.each $ W^ { ( b ) } _s $ has a linear relationship with the gradient w.r.t.the corresponding $ W_q $ . Thus , the proposed bit representation training only leads to minimal computational overhead comparing to the normal backpropagation procedure . * * For regularization : * * As introduced in Section 3.2 , here we use a form of group Lasso to induce a structural sparsity such that a certain bit of all elements in the weight tensor can become zero simultaneously . This is enabled by applying an L1 regularization ( i.e.sum ) across the L2 norm of the group of variables corresponding to each bit . Group Lasso is a well-known regularizer for inducing structural sparsity , and has been applied for DNN compression as in ( Wen et al. , 2016 ) . We have added a citation to the group Lasso in the revision to make it clearer Reference : Wei Wen , Chunpeng Wu , Yandan Wang , Yiran Chen , and Hai Li . Learning structured sparsity in deep neural networks . In Advances in neural information processing systems , pp . 2074\u20132082 , 2016 . * * For the related work : * * Thanks for bringing up this interesting work . We have added it to the related work discussion in the revision . [ 1 ] shares a similar method as the related work mentioned by reviewer 2 . The method is based on manually designed quantization criteria , which may not lead to the best tradeoff between model size and accuracy , especially after quantization-aware training . Consequently , the weight precision achieved in [ 1 ] is much higher than that of BSQ . On the other hand , BSQ induces the mixed-precision quantization scheme during the training process without any heuristic constraints . Therefore , BSQ can fully explore the design space of mixed-precision quantization and find better tradeoff points between model size and accuracy . Thanks for pointing out the issue of \u201c Comp \u201d . We add the description of it in Section 4.2 where it is first mentioned in the article ."}, {"review_id": "TiXl51SCNw8-3", "review_text": "The paper proposes a method to implement each layer with different precision ( mixed-precision quantization ) . The method employed is referred to as bit-level sparsity quantization whereby each bit of the parameter set is treated as a trainable parameter . A differential bit sparsity regularizer enables a smooth trade-off between accuracy and complexity/compression . 1 ) results are a slight improvement over SOTA methods . This is to be expected given the maturity of this topic . 2 ) typo in Table 2 ( `` wight precision '' ) 3 ) It will be good to relate this work to [ 1 ] that also studies mixed-precision quantization using a pre-trained floating point network . [ 1 ] Sakr et al. , An analytical method to determine minimum per-layer precision of deep neural networks . Overall a well-written paper with a solid reasoning behind the work . The results improve marginally over SOTA methods .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your positive feedback on our paper ! We are glad that you find the paper well-written , and have fixed the typo in the table . We thank the reviewer for bringing up the related work [ 1 ] , and have added it to the discussion of the related work . [ 1 ] proposes a pre-layer precision assignment framework to quantize pretrained DNN models , the precision assignment is done with manually designed criteria assuming that each layer should contribute equally to the overall noise gains . Although the assumption largely reduces the search space , it may not lead to the optimal tradeoff point between model size and accuracy in practice . Also , directly quantizing a pretrained floating-point model may not lead to the best accuracy as the model weight is not aware of the quantization . This can be seen as [ 1 ] requires a much larger average weight precision than BSQ as well as other works to maintain high accuracy . On the other hand , BSQ induces the mixed-precision quantization scheme during the training process without any heuristic constraints . Therefore , BSQ can fully explore the design space of mixed-precision quantization and find better tradeoff points between model size and accuracy . The training process with bit representation STE also ensures that the model weight is aware of the low-precision quantization , further improving the performance under ultra-low precision . This enables BSQ to achieve a much lower average precision with similar accuracy ."}], "0": {"review_id": "TiXl51SCNw8-0", "review_text": "This paper basically proposed to learn the quantization bits ( precision ) in each layer . Specially , weights are constructed with binary representation as $ W_s = \\ [ W_s^1 , ... , W_s^b\\ ] $ . During training , $ W_s^i $ is relaxed to $ \\in \\ [ 0 , 2\\ ] $ . And a group sparsity is imposed to all $ W_s^i $ for all weights in a layer , leading to certain $ W_s^i \\to 0 $ , thus cancelling the bit allocation in $ i $ -th . Experimental results is promising . Pros : 1.It is interesting to see that weights are represented in binary format , while each bit is trained in a full-precision scheme . Cons : 1.Training process is intricated : one has to tune the penalty in group sparsity . Also , training is separated in several steps : training and post-training finetuning . Questions : 1 . After determining the quantization bit in ( `` fake '' ) quantization training ( although $ W_q $ is quantized but $ W_s^i $ is not exactly binary , which is the exactly weight we want ) using Eq.5 . Author mention in `` Re-quantization and precision adjustment '' that $ W_q^ { ' } $ is converted to binary value . But how to deal with the precision loss here ? i.e.from $ W_s^i \\in \\ [ 0,2\\ ] $ to $ \\ { 0 , 1\\ } $ 2 . Author mentioned that DoReFa-Net is adopted to finetune the trained model . Since DoReFa-Net use tanh to constrain value to $ [ 0,1 ] $ . it seems there is no connection to the proposed quantization scheme ( Eq.6 ) . How to exactly finetune ? 3.Why is necessary for $ W_s $ to be separated into postive and negative part ( $ W_p $ , $ W_n $ ) in processing ? 4.Since $ W_s^i $ is float and trainable , is it necessary to incorporate a trainable $ s $ ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your constructive feedback on our paper ! To answer your questions : 1 . This question is similar to question 2 of reviewer 4 . As introduced at the top of page 6 , effective weight $ W=s W_q $ used in the forward pass STE remains unchanged before and after the re-quantization and precision adjustment . This is ensured by the use of STE in Equation ( 3 ) where the forward pass is always computed with the quantized weight , and the change in scaling factor $ s $ after precision adjustment , as illustrated in Equation ( 6 ) . Therefore , there will not be any change to the model performance and the final cross-entropy loss before and after the re-quantization and precision adjustment . Also , note that we allow the n-bit weight to be re-quantized to ( n+1 ) -bit after the re-quantization , so there won \u2019 t be any precision loss when converting from $ W^ { ( i ) } _ { s } \\in [ 0,2 ] $ to 0,1 2 . Thanks for pointing this out . It seems like we made a mistake here confusing the STE used in ( Polino et al. , 2018 ) with the one used in DoReFa . In this work we use the STE with a linear scaling function as proposed in ( Polino et al. , 2018 ) to quantize both weight and activation during the finetuning process . Specifically , weight and activation are first linearly scaled to the range of [ 0,1 ] before going through uniform quantization in the STE . We will correct this in the revised version . 3.The reason why we separate Wp and Wn is to assist to observe bit-level sparsity when both positive and negative elements are presented in a weight tensor , such that we can ignore the sign of the weight element and only focusing on the bit representation of the absolute values . This will simplify the implementation of re-quantization and precision adjustment . Actually , in the training process with floating-point variables , dividing Wp and Wn may not be required . As the gradient passed from the STE in Equation ( 3 ) onto the pair of Wp and Wn are always symmetric , training with separated variables is equivalent to training with Ws = Wp-Wn directly . 4.As discussed in Section 3.1 , although Ws is float and trainable , it will always be in the range of [ 0,2 ] for all the layers and will be quantized uniformly with the same step size , which may not be adequate to capture the difference of dynamic range across different layers , hurting the performance of some layers after quantization . In the meantime , using a scaling factor that always scale the largest element to 1 as done in ( Polino et al. , 2018 ) will make the dynamic precision reduction impossible . This is why we keep the scaling factor as a separate trainable variable , allow it to fit into the dynamic range requirement of each layer while not preventing the precision reduction of the bit representation . Reference : A. Polino , R. Pascanu , and D. Alistarh . Model compression via distillation and quantization . arXiv preprint arXiv:1802.05668 , 2018 ."}, "1": {"review_id": "TiXl51SCNw8-1", "review_text": "This paper introduces a new method to quantize neural networks in a differentiable manner . Proposed method applies the group lasso on the bit-planes of the weight parameters to let certain LSBs in each layer to be zero-ed out . STE is used to train the binary representation of each bit-plane and the sign of weights during the training . Results demonstrate that the proposed method can achieve higher accuracy and compression ratio compared to previous studies . I think that the idea of introducing group lasso to prune the entire bit-plane is very interesting and the paper is well written , but some additional analysis will be helpful . 1.I think the result must be compared with more recent papers , such as LSQ ( Esser , Steven K. , et al. , 2020 ) . For example , LSQ demonstrates that it acheives 75.8 % top-1 and 92.7 % top-5 accuracy with 3/3-bit model ( weight/activation ) on ResNet-50 . However , according to the appendix C , proposed method seems to achieve only 92.16 % when the activation is quantized to 3-bit . 2.This is more of a question than suggestion : after the requantization , would the batch-normalization layers function correctly ? It seems like the parameters of batch normalization layers are kept the same after the requantization , while the requantization will impact the distribution of activations . Analysis on the difference between the distribution of the activation before the batch normalization layer before and afther the requantization will be helpful to see if the distribution of the activations really do not differ that much , or the batch-norm layer will just adapt to the occasion .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank your feedback on our paper and are glad that you find our paper interesting . For your questions : 1 . Thanks for bringing up the results in LSQ . The result mentioned in the review is evaluated with 3-bit ResNet-50 model on ImageNet , where LSQ achieves 75.8 % top-1 accuracy with a 10.67x compression rate comparing to the full-precision model . As reported in Table 3 , BSQ can achieve 75.3 % top-1 accuracy with a higher 11.90x compression rate on the ResNet-50 model , so the result is still competitive . We have added this comparison to Table 3 in the paper . As we consider the scale s as trainable variables in BSQ , it is similar to the training process proposed in LSQ . Although we haven \u2019 t made a dedicated analysis on the gradient w.r.t.s as done in LSQ . It would be interesting to further improve BSQ with the update rule proposed in LSQ during the training process in future work . 2.As introduced at the top of page 6 , the effective weight $ W=s W_q $ used in the forward pass STE remains unchanged before and after the re-quantization and precision adjustment . This is ensured by the use of bit representation STE in Equation ( 3 ) , where the forward pass is always computed with quantized weight , and the adjustment of scaling factor s after precision adjustment as illustrated in Equation ( 6 ) . Therefore , there will not be any change to the activation and the final cross-entropy loss before and after the re-quantization and precision adjustment , and will not affect the functionality of batch norm layers ."}, "2": {"review_id": "TiXl51SCNw8-2", "review_text": "Quantization of weights in DNNs is a very effective way to reduce the computational and storage costs which can enable deployment of deep learning at the edge . However , determining suitable layer-wise bit-widths while training is a difficult task due to the discrete nature of the optimization problem . This paper proposes to utilize bit-level sparsity as a proxy for bit-width and employ regularization techniques to formulate the problem so that precision can be reduced while training the model . The method proposed by the authors is sound . It leverages insights that have been employed in a neighboring area ( pruning via regularization ) and re-purposes those to the problem of quantization . The empirical evaluation is robust as well . One issue I have with the proposed method is that the parameter space is expanded by a large amount . Since for every scalar weight , we end up with a collection of binary weights . Does n't this make training more difficult ? It would be nice to discuss this issue . And more importantly how does the extra effort compare to other approaches ( such as Dorefanet and others ) . Regarding the proposed regularization technique . Lasso ( least absolute shrinkage and selection operator ) is , as far as I am aware , an optimizer that regularizes the L_1 norm of the parameters . Why is the regularizer in eq . ( 4 ) using the L_2 norm ? Maybe I am missing something and/or there is an inconsistency is the notation/wording . The authors do a good job comparing with related works . However , one of the main early claims is that all works trying to find per-layer precision do so manually . This is not true , there have been some works that have done exactly that . One example is [ 1 ] which analytically determines precisions at all layers using a noise gain concept . It would be nice to contrast with such works as well . Minor issue : 'comp x ' is used in the results ( tables ) without being defined . It appears to indicate 'compression ratio ' . This has to be explicitly stated at least once ( maybe in the captions ) . references : [ 1 ] Sakr , Charbel , and Naresh R. Shanbhag . `` Per-tensor fixed-point quantization of the back-propagation algorithm . '' ICLR 2019 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank your feedback on our paper and are glad that you find our paper technically sound . * * For the issue on parameter space : * * As illustrated in Equation ( 3 ) and discussed below the equation , the gradient of the cross-entropy loss w.r.t.each bit of the scaled weight $ W_s $ is not independent . In fact , the gradient w.r.t.each $ W^ { ( b ) } _s $ has a linear relationship with the gradient w.r.t.the corresponding $ W_q $ . Thus , the proposed bit representation training only leads to minimal computational overhead comparing to the normal backpropagation procedure . * * For regularization : * * As introduced in Section 3.2 , here we use a form of group Lasso to induce a structural sparsity such that a certain bit of all elements in the weight tensor can become zero simultaneously . This is enabled by applying an L1 regularization ( i.e.sum ) across the L2 norm of the group of variables corresponding to each bit . Group Lasso is a well-known regularizer for inducing structural sparsity , and has been applied for DNN compression as in ( Wen et al. , 2016 ) . We have added a citation to the group Lasso in the revision to make it clearer Reference : Wei Wen , Chunpeng Wu , Yandan Wang , Yiran Chen , and Hai Li . Learning structured sparsity in deep neural networks . In Advances in neural information processing systems , pp . 2074\u20132082 , 2016 . * * For the related work : * * Thanks for bringing up this interesting work . We have added it to the related work discussion in the revision . [ 1 ] shares a similar method as the related work mentioned by reviewer 2 . The method is based on manually designed quantization criteria , which may not lead to the best tradeoff between model size and accuracy , especially after quantization-aware training . Consequently , the weight precision achieved in [ 1 ] is much higher than that of BSQ . On the other hand , BSQ induces the mixed-precision quantization scheme during the training process without any heuristic constraints . Therefore , BSQ can fully explore the design space of mixed-precision quantization and find better tradeoff points between model size and accuracy . Thanks for pointing out the issue of \u201c Comp \u201d . We add the description of it in Section 4.2 where it is first mentioned in the article ."}, "3": {"review_id": "TiXl51SCNw8-3", "review_text": "The paper proposes a method to implement each layer with different precision ( mixed-precision quantization ) . The method employed is referred to as bit-level sparsity quantization whereby each bit of the parameter set is treated as a trainable parameter . A differential bit sparsity regularizer enables a smooth trade-off between accuracy and complexity/compression . 1 ) results are a slight improvement over SOTA methods . This is to be expected given the maturity of this topic . 2 ) typo in Table 2 ( `` wight precision '' ) 3 ) It will be good to relate this work to [ 1 ] that also studies mixed-precision quantization using a pre-trained floating point network . [ 1 ] Sakr et al. , An analytical method to determine minimum per-layer precision of deep neural networks . Overall a well-written paper with a solid reasoning behind the work . The results improve marginally over SOTA methods .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your positive feedback on our paper ! We are glad that you find the paper well-written , and have fixed the typo in the table . We thank the reviewer for bringing up the related work [ 1 ] , and have added it to the discussion of the related work . [ 1 ] proposes a pre-layer precision assignment framework to quantize pretrained DNN models , the precision assignment is done with manually designed criteria assuming that each layer should contribute equally to the overall noise gains . Although the assumption largely reduces the search space , it may not lead to the optimal tradeoff point between model size and accuracy in practice . Also , directly quantizing a pretrained floating-point model may not lead to the best accuracy as the model weight is not aware of the quantization . This can be seen as [ 1 ] requires a much larger average weight precision than BSQ as well as other works to maintain high accuracy . On the other hand , BSQ induces the mixed-precision quantization scheme during the training process without any heuristic constraints . Therefore , BSQ can fully explore the design space of mixed-precision quantization and find better tradeoff points between model size and accuracy . The training process with bit representation STE also ensures that the model weight is aware of the low-precision quantization , further improving the performance under ultra-low precision . This enables BSQ to achieve a much lower average precision with similar accuracy ."}}