{"year": "2021", "forum": "zgMPc_48Zb", "title": "Differentially Private Generative Models Through Optimal Transport", "decision": "Reject", "meta_review": "The paper proposes a DP method for generative modelling based on optimal transport. The reviewers agree that the novelty is limited in relation to prior work, while the results are not especially compelling either. So, even though this is a valid approach, correctness is not sufficient for acceptance at ICLR. \n", "reviews": [{"review_id": "zgMPc_48Zb-0", "review_text": "This paper presents a differentially private method for training a generative model . The proposed method takes advantage of the Sinkhorn divergence to achieve robustness against the hyperparameters ' choices . The authors also introduce a cost function enabling the generative model to generate images associated with a specific class label . The experimental results show that the proposed method outperforms the existing methods for learning a generative model in a differentially private manner . Furthermore , such a high accuracy can be achieved without the use of publicity available data . The strong points of this paper are as follows : - The authors bring the Sinkhorn divergence-based learning of a generative model into the differentially private generative model learning problem . The weak points of this paper are as follows : - The proposed method is not clearly explained and thus is suspicious in the guarantee of differential privacy . - The presented method is a direct application of Wang et al . 's moment account technique with Zhu et al . 's Poisson sampling . The originality is considerably low . - This paper has no theoretical and experimental analysis about robustness against a hyperparameter choice , while the authors claim it as a contribution . - The proposed approach is not well motivated . We can use some differentially private classification algorithm if the objective is high accuracy in downstream classification . I recommend rejection of this paper because the proposed algorithm has low originality and is not well motivated . Also , the unclarity of the privacy guarantee is problematic . The authors do not give a clear explanation of the proposed method . In particular , it is unclear if the proposed algorithm guarantees differential privacy . I guess the authors employ either the composition theorem or moment account technique to prove the algorithm 's differential privacy ; however , there is no privacy proof of the proposed algorithm . I could not confirm that the proposed method ensures differential privacy . The proposed method is a straightforward application of the techniques from Wang et al.and Zhu et al.Also , defining the cost function as in Eq.4 is a straightforward way to combine the multidimensional real-valued data and discrete label . I can not find any original idea , except introducing the Sinkhorn divergence , in the proposed method . The authors claim that the proposed method is robust against the choice of its hyperparameters . However , there is no evidence to support the claim . A theoretical or experimental analysis of robustness is necessary to claim it . Why do n't we employ the differentially private classification algorithm , such as M. Adabi et al.Deep Learning with Differential Privacy . In CCS'16 . When the objective is high accuracy in the downstream classification , we can utilize such an algorithm directly . What is the benefit of employing the generative model-based privacy preservation ? The differentially private classification algorithm can achieve high classification accuracy ; for example , in Adabi et al . 's paper , the classification accuracy for MNIST with eps=10 is 97 % ; this value is significantly higher than that of the proposed method . # # # Minor comments - What is the definition of $ \\hat { S } $ ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the constructive feedback . Below is our response to some specific concerns raised in your review . * The guarantee of differential privacy Regarding the guarantee of privacy , we used poisson subsampling with moments accountant for composing Renyi Differential Privacy ( RDP ) over training iterations , as proposed in Zhu et al.2019 [ 1 ] .We used the implementation of this mechanism in Tensorflow-DP , with further details provided in appendix D.4 . As this moments accountant technique is well established and our implementation by working with the Tensorflow-DP library is fairly standard , we are certain that our method is differentially private in both theory and practice . We will release our source code once the paper is published . We added a proof of our method \u2019 s privacy at the end of section 3 . We chose to not include this in the original manuscript , since the technique is a standard application of privacy results for the Gaussian mechanism in RDP . The goal of our paper is indeed not to introduce a novel privacy mechanism , but rather to introduce optimal transport to the field of differential privacy as a promising method for learning differentially private generative models ( see below for motivation for that ) . * Originality of the proposed method Regarding originality , our work highlights that we can learn private generative models that are more useful than recent state-of-art methods ( GS-WGAN ) using an easy-to-train , non-adversarial loss formulation ( Sinkhorn Divergence ) and standard differential privacy tools . Our motivation is that as a non-adversarial optimal transport loss , Sinkhorn Divergence converges more stably than GANs , which makes it naturally suited for adapting to differential privacy . We are particularly interested in the usefulness of the synthesized data for downstream tasks , for which we choose classification of MNIST and FashionMNIST . This has emerged as the standard benchmark for differentially private generative models . A strong classifier that generalizes to real test data can only be trained if the data used for training is diverse . GANs , which are used by most competitive works , tend to suffer from instabilities during training due to their adversarial training scheme . These instabilities can manifest as mode dropping and imbalanced covering of the data distribution , thereby synthesizing imbalanced data for training the classifier . This would result in a sub-optimal classifier . The advantage of our optimal transport-based method is precisely that it does not require any adversarial objectives in its standard form and does not suffer from any noticeable mode-dropping problems . We indeed found our images to be diverse , as seen by samples ( see variance of shape and style of digits and grayscale color of outfit pieces in additional samples in Appendix E ) and supported by the strong classification results . Our results are still worse in FID than the state-of-the-art due to slight blur . However , for downstream tasks diversity seems to trump sharpness . Generally , training instabilities and mode dropping problems in GANs may be exacerbated by the gradient noise applied during training for ensuring privacy . We think that it is unclear whether GANs will be as successful for training differentially private generative models as they are for non-private generative modeling . We believe that optimal transport provides a promising and very simple and robust framework for differentially private generative modeling , which we try to demonstrate with our results . We see our method \u2019 s simplicity as its strength . More generally , in non-private settings , a growing number of works have studied the application of optimal transport in learning deep generative models due to their desirable convergence properties , yet previous works on private generative learning have mostly focused on adversarial training methods . Our method fills the knowledge gap by bridging an important branch of generative models with the differentially private setting . Our work is conceptually most related to DP-MERF [ 2 ] , which uses maximum mean discrepancy , another type of integral probability metric . Compared to DP-MERF , samples generated by our method are much more representative of the training data at a weaker privacy requirement of epsilon=10 , whereas DP-MERF excels at the strong privacy regime of epsilon < 1 , but fails to scale in utility with weaker privacy requirements ."}, {"review_id": "zgMPc_48Zb-1", "review_text": "The paper proposes a method for training OT GANs using differentially private sinkhorn algorithm . The idea is very simple - train GANs with sinkhorn divergences and add Gaussian noise to the gradient of output wrt generated samples . So , the novelty by itself is minimal as sinkhorn GANs have previously been proposed . The method merely adds Gaussian noise to gradients . The DP analysis is also not very different . I am generally ok with incremental improvements in method if experimental results are strong . This does n't seem to be the case in this paper . Two datasets are considered - MNIST and CelebA . In MNIST , we observe that the algorithm performs poorly compared to GS-WGAN both in inception score and FID . The method gets better accuracy though . This suggests that the method does not produce diverse samples . For example , consider a case of a mode collapse where a GAN generates only one sample per class . In this case , FID scores will be poor . However , if the generated sample correcly corresponds to the class , classification score will be high . My guess is similar thing is happening here . This is evident even in Fig 2 where the method has much poor sample diversity compared to GS-WGAN . Hence , the model itself is not that great . In CelebA , despite using BigGAN style architecture , there is significant blur . The real challenge in differential private GANs is to generate samples with good quality while still satisfying privacy constraints . This doesnt seem to be happening here . Also , comparison with GS-WGAN or other differentially private GANs are missing in CelebA experiment . For conditional generation , is concatenation of data and labels in cost function a good strategy ? It looks like both data and label space is very different , and I am wondering if such type of concatenation might be weak . Can you comment on this .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the constructive feedback . Below is our response to some specific concerns raised in your review . * \u201c Two datasets are considered\u2026 [ DP-Sinkhorn ] performs poorly compared to GS-WGAN both in inception score and FID \u201d We would like to first highlight that we conducted experiments on three datasets : MNIST , Fashion-MNIST , and CelebA . MNIST and Fashion-MNIST are the de-facto benchmarks for differentially private generative learning in the image domain , as they have been used by many related works . The improvement in downstream classifier accuracy with DP-Sinkhorn is most pronounced on the Fashion-MNIST dataset , with an absolute improvement of 5.4 % ( 8 % relative ) over GS-WGAN under respective best case scenarios . The metrics we evaluated on were classification accuracies and FID ; we chose not to evaluate Inception score since we couldn \u2019 t determine how inception score was calculated in previous works when evaluating on Fashion-MNIST . * Regarding FID and sample diversity . Regarding FID , DP-Sinkhorn with L2 loss indeed produced images with higher FID than GS-WGAN , while comparing favorably against all other methods . We found that if we suppress pixels with brightness less than 0.5 in images generated by DP-Sinkhorn , we obtain images with worse visual quality . Clearly , this modification could not improve sample diversity either . Yet , the modified images score better FID ( 54.16 ) than GS-WGAN ( 61.3 ) . This indicates that our method indeed produces diverse samples , but these images are slightly blurry and hence score poorly on FID which emphasizes texture . For fair comparison , we didn \u2019 t do any such tricks for the results presented in the paper , though . This also suggests that FID is not a reliable metric for this low resolution grayscale data and we deem the classification task more important . Importantly , we believe that DP-Sinkhorn performs better on classification despite the blur precisely because our images are more diverse and hence provide better generalization for downstream classifiers . The importance of diversity for downstream accuracy is even more pronounced in our newly added ablation experiments where we add noise on parameter gradients instead of image gradients . DP-Sinkhorn with parameter gradients produced even noisier images , yet the downstream classifier accuracy is still better than GS-WGAN . We hypothesize that this actually suggests that GS-WGAN and the other GAN-based approaches suffer from a certain amount of mode-dropping , while our approach does not . We attribute this to our robust optimal transport-based training approach that does avoid any potentially unstable adversarial objectives ( in fact , we don \u2019 t consider our method a type of GAN , as it doesn \u2019 t train in an adversarial fashion.Rather , it directly optimizes the primal formulation of the optimal transport distance between the target and generated distribution via the Sinkhorn divergence - also see expanded explanation in section 2.2 ) . We also added additional figures with more samples drawn from our model in Appendix E to demonstrate that it indeed produces diverse samples ( see , for example , variations in shape and orientation of digits and grayscale color in outfit pieces ) . * Regarding the CelebA experiments We emphasize that our goal is to produce \u201c useful \u201d synthetic data for downstream tasks . As Anonreviewer 2 has also pointed out , it is impossible to perfectly learn a synthetic distribution ( in the language of statistics , match a target distribution up to arbitrary order statistics ) while also enforcing differential privacy . DP-Sinkhorn shows that it is indeed possible to synthesize useful data when training private generators on RGB images . To our best knowledge , no previous works have attempted training differentially private generative models on RGB images without additional public data . We want to explore this important yet missing application with our proposed method . Although we find the generated data useful for downstream tasks , there is clearly still room for improvement . Future works might explore metrics ( replacing pixel space L2 ) that algorithmically encode our priors about natural images . We couldn \u2019 t evaluate GS-WGAN on CelebA as code for GS-WGAN was not available at the time of submission . We are currently trying to train GS-WGAN on CelebA , but we expect experiments to run past the end of the discussion period ."}, {"review_id": "zgMPc_48Zb-2", "review_text": "This paper proposes a novel architecture and training process for learning differentially private synthetic data generators . Rather than taking an explicit adversarial GAN approach , the paper looks at minimizing a regularized Wasserstein distance ( called the Sinkhorn loss ) between the empirical distribution of the data and the synthetic distribution . Their approach brings two benefits : first , the Sinkhorn loss is more straightforward to optimize than traditional GANs . The authors don \u2019 t really discuss why this is , but my understanding is that the \u201c adversary \u201d is contained in the dual formulation of Wasserstein distance ( as the minimum over Lipschitz functions of the difference in expectation between two distributions ) . Considering such a Lipschitz adversary simplifies optimization . Second , the paper uses an idea first published by Chen et al ( NeurIPS 2020 ) but described as independent work here : Instead of measuring the gradient of the full parametric distribution of the generator , the authors measure the gradient \u201c at the generated image level \u201d . This gives a lower-dimensional gradient ( requiring less noise ) . Overall , the paper achieves a moderate improvement over the work of Chen et al with respect to a few simple measures of accuracy ( namely , how well two DNN models do at classifying real data when they are trained on synthetic data ) . It does considerably worse than Chen et al \u2019 s method with respect to the FID score , a measure of visual similarity . I generally like the idea of exploring alternate training strategies . Nevertheless , I have several reservations about the paper : 1 . The relationship to Chen et al.It seems like much of the gain relative to previous work comes from the way gradients are compressed . But this idea appears already in the NeurIPS paper of Chen et al.It is unclear to me how to handle the relative priority of the papers . 2.Overly simplistic accuracy measures : Wasserstein ( or Sinkhorn ) distance is a natural loss function , but it is only likely to be a really good measure of accuracy when it is very small . The DP literature , since the pioneering work of Blum , Ligett , and Roth , has focused on using as a loss function the minimum over a ( potentially enormous but ) task-specific set of queries as the ultimate loss . We understand as a field that general-purpose synthetic data isn \u2019 t really possible ( it preserves \u201c too many statistics , too accurately \u201d to avoid attacks ) . How can the framework here be adapted to such accuracy measures ? How well does DP Sinkhorn do when measured against such task-specific accuracy ? Can DP Sinkhorn be applied to anything except images ( and if so , how well does it do ) ? Overall , I find the paper interesting but perhaps borderline .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the constructive feedback . Below is our response to some specific points raised in your review . * Why Sinkhorn divergence is easier to optimize than GANs . When compared to WGAN , learning with the Sinkhorn divergence has distinct differences . First , the Sinkhorn divergence is computed under the primal formulation of OT , whereas WGAN 's loss is computed under the dual formulation . While both are approximations to the exact Wasserstein distance , the source of the approximation error differs . The Sinkhorn divergence uses entropic regularization to ensure linear convergence when finding the optimal transport plan $ \\pi $ [ 1 ] . Its two sources of error are the suboptimality of the transport plan and bias introduced by entropic regularization . With the guarantee of linear convergence , $ \\pi $ can converge to optimality by using enough iterations , thereby allowing control of the first error . The second source of error can be controlled by using small values of $ \\epsilon $ , which we found to work well in practice . In contrast , WGAN 's source of error lies in the sub-optimality of the dual potential function . Since this potential function is parameterized by an adversarially trained deep neural network , it enjoys neither convergence guarantees nor feasibility guarantees . Furthermore , the adversarial training scheme can produce oscillatory behavior , where the discriminator and generator change abruptly every iteration to counter the strategy of the other player from the previous iteration [ 2 ] . These shortcomings contribute to WGAN 's problems of non-convergence , which in turn can lead to mode dropping . In contrast , training with the Sinkhorn divergence does not involve any adversarial training at all , converges more stably , and reaps the benefits of OT metrics at covering modes . This stability is our key motivation for using the Sinkhorn approach for learning differentially private generative models , where stability in the adversarial GAN setting may be even harder to achieve due to the additional perturbations for guaranteeing differential privacy . We \u2019 ve expanded our discussion on differences between Sinkhorn divergence and GANs in section 2.2 of the manuscript . * Impact of gradient perturbation on generated images We have added an ablation study to our MNIST and FashionMNIST experiments in section 4.2 . Our results indicate that gradient perturbation on generated images is still superior to doing so on the parameters , but DP-Sinkhorn with parameter gradient perturbation still slightly outperforms all baselines on the classification task . Qualitatively , the images generated by parameter gradient perturbation appear worse ( see Appendix E ) , yet the downstream classification accuracy is close to the one achieved by image gradient perturbation . We think this is due to the fact that downstream classifiers are dependent on sample diversity to achieve generalization , and images generated by DP-Sinkhorn with parameter gradients are diverse enough to train ( relatively ) good classifiers despite having a noisier appearance . We hypothesize that the baselines , mostly being advarially trained GAN-based methods , suffer from some amount of mode dropping compared to our method . We attribute this to our robust optimal transport-based training approach ( see previous bullet point ) ."}], "0": {"review_id": "zgMPc_48Zb-0", "review_text": "This paper presents a differentially private method for training a generative model . The proposed method takes advantage of the Sinkhorn divergence to achieve robustness against the hyperparameters ' choices . The authors also introduce a cost function enabling the generative model to generate images associated with a specific class label . The experimental results show that the proposed method outperforms the existing methods for learning a generative model in a differentially private manner . Furthermore , such a high accuracy can be achieved without the use of publicity available data . The strong points of this paper are as follows : - The authors bring the Sinkhorn divergence-based learning of a generative model into the differentially private generative model learning problem . The weak points of this paper are as follows : - The proposed method is not clearly explained and thus is suspicious in the guarantee of differential privacy . - The presented method is a direct application of Wang et al . 's moment account technique with Zhu et al . 's Poisson sampling . The originality is considerably low . - This paper has no theoretical and experimental analysis about robustness against a hyperparameter choice , while the authors claim it as a contribution . - The proposed approach is not well motivated . We can use some differentially private classification algorithm if the objective is high accuracy in downstream classification . I recommend rejection of this paper because the proposed algorithm has low originality and is not well motivated . Also , the unclarity of the privacy guarantee is problematic . The authors do not give a clear explanation of the proposed method . In particular , it is unclear if the proposed algorithm guarantees differential privacy . I guess the authors employ either the composition theorem or moment account technique to prove the algorithm 's differential privacy ; however , there is no privacy proof of the proposed algorithm . I could not confirm that the proposed method ensures differential privacy . The proposed method is a straightforward application of the techniques from Wang et al.and Zhu et al.Also , defining the cost function as in Eq.4 is a straightforward way to combine the multidimensional real-valued data and discrete label . I can not find any original idea , except introducing the Sinkhorn divergence , in the proposed method . The authors claim that the proposed method is robust against the choice of its hyperparameters . However , there is no evidence to support the claim . A theoretical or experimental analysis of robustness is necessary to claim it . Why do n't we employ the differentially private classification algorithm , such as M. Adabi et al.Deep Learning with Differential Privacy . In CCS'16 . When the objective is high accuracy in the downstream classification , we can utilize such an algorithm directly . What is the benefit of employing the generative model-based privacy preservation ? The differentially private classification algorithm can achieve high classification accuracy ; for example , in Adabi et al . 's paper , the classification accuracy for MNIST with eps=10 is 97 % ; this value is significantly higher than that of the proposed method . # # # Minor comments - What is the definition of $ \\hat { S } $ ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the constructive feedback . Below is our response to some specific concerns raised in your review . * The guarantee of differential privacy Regarding the guarantee of privacy , we used poisson subsampling with moments accountant for composing Renyi Differential Privacy ( RDP ) over training iterations , as proposed in Zhu et al.2019 [ 1 ] .We used the implementation of this mechanism in Tensorflow-DP , with further details provided in appendix D.4 . As this moments accountant technique is well established and our implementation by working with the Tensorflow-DP library is fairly standard , we are certain that our method is differentially private in both theory and practice . We will release our source code once the paper is published . We added a proof of our method \u2019 s privacy at the end of section 3 . We chose to not include this in the original manuscript , since the technique is a standard application of privacy results for the Gaussian mechanism in RDP . The goal of our paper is indeed not to introduce a novel privacy mechanism , but rather to introduce optimal transport to the field of differential privacy as a promising method for learning differentially private generative models ( see below for motivation for that ) . * Originality of the proposed method Regarding originality , our work highlights that we can learn private generative models that are more useful than recent state-of-art methods ( GS-WGAN ) using an easy-to-train , non-adversarial loss formulation ( Sinkhorn Divergence ) and standard differential privacy tools . Our motivation is that as a non-adversarial optimal transport loss , Sinkhorn Divergence converges more stably than GANs , which makes it naturally suited for adapting to differential privacy . We are particularly interested in the usefulness of the synthesized data for downstream tasks , for which we choose classification of MNIST and FashionMNIST . This has emerged as the standard benchmark for differentially private generative models . A strong classifier that generalizes to real test data can only be trained if the data used for training is diverse . GANs , which are used by most competitive works , tend to suffer from instabilities during training due to their adversarial training scheme . These instabilities can manifest as mode dropping and imbalanced covering of the data distribution , thereby synthesizing imbalanced data for training the classifier . This would result in a sub-optimal classifier . The advantage of our optimal transport-based method is precisely that it does not require any adversarial objectives in its standard form and does not suffer from any noticeable mode-dropping problems . We indeed found our images to be diverse , as seen by samples ( see variance of shape and style of digits and grayscale color of outfit pieces in additional samples in Appendix E ) and supported by the strong classification results . Our results are still worse in FID than the state-of-the-art due to slight blur . However , for downstream tasks diversity seems to trump sharpness . Generally , training instabilities and mode dropping problems in GANs may be exacerbated by the gradient noise applied during training for ensuring privacy . We think that it is unclear whether GANs will be as successful for training differentially private generative models as they are for non-private generative modeling . We believe that optimal transport provides a promising and very simple and robust framework for differentially private generative modeling , which we try to demonstrate with our results . We see our method \u2019 s simplicity as its strength . More generally , in non-private settings , a growing number of works have studied the application of optimal transport in learning deep generative models due to their desirable convergence properties , yet previous works on private generative learning have mostly focused on adversarial training methods . Our method fills the knowledge gap by bridging an important branch of generative models with the differentially private setting . Our work is conceptually most related to DP-MERF [ 2 ] , which uses maximum mean discrepancy , another type of integral probability metric . Compared to DP-MERF , samples generated by our method are much more representative of the training data at a weaker privacy requirement of epsilon=10 , whereas DP-MERF excels at the strong privacy regime of epsilon < 1 , but fails to scale in utility with weaker privacy requirements ."}, "1": {"review_id": "zgMPc_48Zb-1", "review_text": "The paper proposes a method for training OT GANs using differentially private sinkhorn algorithm . The idea is very simple - train GANs with sinkhorn divergences and add Gaussian noise to the gradient of output wrt generated samples . So , the novelty by itself is minimal as sinkhorn GANs have previously been proposed . The method merely adds Gaussian noise to gradients . The DP analysis is also not very different . I am generally ok with incremental improvements in method if experimental results are strong . This does n't seem to be the case in this paper . Two datasets are considered - MNIST and CelebA . In MNIST , we observe that the algorithm performs poorly compared to GS-WGAN both in inception score and FID . The method gets better accuracy though . This suggests that the method does not produce diverse samples . For example , consider a case of a mode collapse where a GAN generates only one sample per class . In this case , FID scores will be poor . However , if the generated sample correcly corresponds to the class , classification score will be high . My guess is similar thing is happening here . This is evident even in Fig 2 where the method has much poor sample diversity compared to GS-WGAN . Hence , the model itself is not that great . In CelebA , despite using BigGAN style architecture , there is significant blur . The real challenge in differential private GANs is to generate samples with good quality while still satisfying privacy constraints . This doesnt seem to be happening here . Also , comparison with GS-WGAN or other differentially private GANs are missing in CelebA experiment . For conditional generation , is concatenation of data and labels in cost function a good strategy ? It looks like both data and label space is very different , and I am wondering if such type of concatenation might be weak . Can you comment on this .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the constructive feedback . Below is our response to some specific concerns raised in your review . * \u201c Two datasets are considered\u2026 [ DP-Sinkhorn ] performs poorly compared to GS-WGAN both in inception score and FID \u201d We would like to first highlight that we conducted experiments on three datasets : MNIST , Fashion-MNIST , and CelebA . MNIST and Fashion-MNIST are the de-facto benchmarks for differentially private generative learning in the image domain , as they have been used by many related works . The improvement in downstream classifier accuracy with DP-Sinkhorn is most pronounced on the Fashion-MNIST dataset , with an absolute improvement of 5.4 % ( 8 % relative ) over GS-WGAN under respective best case scenarios . The metrics we evaluated on were classification accuracies and FID ; we chose not to evaluate Inception score since we couldn \u2019 t determine how inception score was calculated in previous works when evaluating on Fashion-MNIST . * Regarding FID and sample diversity . Regarding FID , DP-Sinkhorn with L2 loss indeed produced images with higher FID than GS-WGAN , while comparing favorably against all other methods . We found that if we suppress pixels with brightness less than 0.5 in images generated by DP-Sinkhorn , we obtain images with worse visual quality . Clearly , this modification could not improve sample diversity either . Yet , the modified images score better FID ( 54.16 ) than GS-WGAN ( 61.3 ) . This indicates that our method indeed produces diverse samples , but these images are slightly blurry and hence score poorly on FID which emphasizes texture . For fair comparison , we didn \u2019 t do any such tricks for the results presented in the paper , though . This also suggests that FID is not a reliable metric for this low resolution grayscale data and we deem the classification task more important . Importantly , we believe that DP-Sinkhorn performs better on classification despite the blur precisely because our images are more diverse and hence provide better generalization for downstream classifiers . The importance of diversity for downstream accuracy is even more pronounced in our newly added ablation experiments where we add noise on parameter gradients instead of image gradients . DP-Sinkhorn with parameter gradients produced even noisier images , yet the downstream classifier accuracy is still better than GS-WGAN . We hypothesize that this actually suggests that GS-WGAN and the other GAN-based approaches suffer from a certain amount of mode-dropping , while our approach does not . We attribute this to our robust optimal transport-based training approach that does avoid any potentially unstable adversarial objectives ( in fact , we don \u2019 t consider our method a type of GAN , as it doesn \u2019 t train in an adversarial fashion.Rather , it directly optimizes the primal formulation of the optimal transport distance between the target and generated distribution via the Sinkhorn divergence - also see expanded explanation in section 2.2 ) . We also added additional figures with more samples drawn from our model in Appendix E to demonstrate that it indeed produces diverse samples ( see , for example , variations in shape and orientation of digits and grayscale color in outfit pieces ) . * Regarding the CelebA experiments We emphasize that our goal is to produce \u201c useful \u201d synthetic data for downstream tasks . As Anonreviewer 2 has also pointed out , it is impossible to perfectly learn a synthetic distribution ( in the language of statistics , match a target distribution up to arbitrary order statistics ) while also enforcing differential privacy . DP-Sinkhorn shows that it is indeed possible to synthesize useful data when training private generators on RGB images . To our best knowledge , no previous works have attempted training differentially private generative models on RGB images without additional public data . We want to explore this important yet missing application with our proposed method . Although we find the generated data useful for downstream tasks , there is clearly still room for improvement . Future works might explore metrics ( replacing pixel space L2 ) that algorithmically encode our priors about natural images . We couldn \u2019 t evaluate GS-WGAN on CelebA as code for GS-WGAN was not available at the time of submission . We are currently trying to train GS-WGAN on CelebA , but we expect experiments to run past the end of the discussion period ."}, "2": {"review_id": "zgMPc_48Zb-2", "review_text": "This paper proposes a novel architecture and training process for learning differentially private synthetic data generators . Rather than taking an explicit adversarial GAN approach , the paper looks at minimizing a regularized Wasserstein distance ( called the Sinkhorn loss ) between the empirical distribution of the data and the synthetic distribution . Their approach brings two benefits : first , the Sinkhorn loss is more straightforward to optimize than traditional GANs . The authors don \u2019 t really discuss why this is , but my understanding is that the \u201c adversary \u201d is contained in the dual formulation of Wasserstein distance ( as the minimum over Lipschitz functions of the difference in expectation between two distributions ) . Considering such a Lipschitz adversary simplifies optimization . Second , the paper uses an idea first published by Chen et al ( NeurIPS 2020 ) but described as independent work here : Instead of measuring the gradient of the full parametric distribution of the generator , the authors measure the gradient \u201c at the generated image level \u201d . This gives a lower-dimensional gradient ( requiring less noise ) . Overall , the paper achieves a moderate improvement over the work of Chen et al with respect to a few simple measures of accuracy ( namely , how well two DNN models do at classifying real data when they are trained on synthetic data ) . It does considerably worse than Chen et al \u2019 s method with respect to the FID score , a measure of visual similarity . I generally like the idea of exploring alternate training strategies . Nevertheless , I have several reservations about the paper : 1 . The relationship to Chen et al.It seems like much of the gain relative to previous work comes from the way gradients are compressed . But this idea appears already in the NeurIPS paper of Chen et al.It is unclear to me how to handle the relative priority of the papers . 2.Overly simplistic accuracy measures : Wasserstein ( or Sinkhorn ) distance is a natural loss function , but it is only likely to be a really good measure of accuracy when it is very small . The DP literature , since the pioneering work of Blum , Ligett , and Roth , has focused on using as a loss function the minimum over a ( potentially enormous but ) task-specific set of queries as the ultimate loss . We understand as a field that general-purpose synthetic data isn \u2019 t really possible ( it preserves \u201c too many statistics , too accurately \u201d to avoid attacks ) . How can the framework here be adapted to such accuracy measures ? How well does DP Sinkhorn do when measured against such task-specific accuracy ? Can DP Sinkhorn be applied to anything except images ( and if so , how well does it do ) ? Overall , I find the paper interesting but perhaps borderline .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the constructive feedback . Below is our response to some specific points raised in your review . * Why Sinkhorn divergence is easier to optimize than GANs . When compared to WGAN , learning with the Sinkhorn divergence has distinct differences . First , the Sinkhorn divergence is computed under the primal formulation of OT , whereas WGAN 's loss is computed under the dual formulation . While both are approximations to the exact Wasserstein distance , the source of the approximation error differs . The Sinkhorn divergence uses entropic regularization to ensure linear convergence when finding the optimal transport plan $ \\pi $ [ 1 ] . Its two sources of error are the suboptimality of the transport plan and bias introduced by entropic regularization . With the guarantee of linear convergence , $ \\pi $ can converge to optimality by using enough iterations , thereby allowing control of the first error . The second source of error can be controlled by using small values of $ \\epsilon $ , which we found to work well in practice . In contrast , WGAN 's source of error lies in the sub-optimality of the dual potential function . Since this potential function is parameterized by an adversarially trained deep neural network , it enjoys neither convergence guarantees nor feasibility guarantees . Furthermore , the adversarial training scheme can produce oscillatory behavior , where the discriminator and generator change abruptly every iteration to counter the strategy of the other player from the previous iteration [ 2 ] . These shortcomings contribute to WGAN 's problems of non-convergence , which in turn can lead to mode dropping . In contrast , training with the Sinkhorn divergence does not involve any adversarial training at all , converges more stably , and reaps the benefits of OT metrics at covering modes . This stability is our key motivation for using the Sinkhorn approach for learning differentially private generative models , where stability in the adversarial GAN setting may be even harder to achieve due to the additional perturbations for guaranteeing differential privacy . We \u2019 ve expanded our discussion on differences between Sinkhorn divergence and GANs in section 2.2 of the manuscript . * Impact of gradient perturbation on generated images We have added an ablation study to our MNIST and FashionMNIST experiments in section 4.2 . Our results indicate that gradient perturbation on generated images is still superior to doing so on the parameters , but DP-Sinkhorn with parameter gradient perturbation still slightly outperforms all baselines on the classification task . Qualitatively , the images generated by parameter gradient perturbation appear worse ( see Appendix E ) , yet the downstream classification accuracy is close to the one achieved by image gradient perturbation . We think this is due to the fact that downstream classifiers are dependent on sample diversity to achieve generalization , and images generated by DP-Sinkhorn with parameter gradients are diverse enough to train ( relatively ) good classifiers despite having a noisier appearance . We hypothesize that the baselines , mostly being advarially trained GAN-based methods , suffer from some amount of mode dropping compared to our method . We attribute this to our robust optimal transport-based training approach ( see previous bullet point ) ."}}