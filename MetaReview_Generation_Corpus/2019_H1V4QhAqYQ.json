{"year": "2019", "forum": "H1V4QhAqYQ", "title": "Augment your batch: better training with larger batches", "decision": "Reject", "meta_review": "The authors propose to use large batch training of neural networks, where each batch contains multiple augmentations of each sample. The experiments demonstrate that this leads to better performance compared to training with small batches. However, as noted by Reviewers 2 and 3, the experiments do not convincingly show where the improvement comes from. Considering that the described technique is very simplistic, having an extensive ablation study and comparison to the strong baselines is essential. The rebuttal didn\u2019t address the reviewers' concerns, and they argue for rejection.", "reviews": [{"review_id": "H1V4QhAqYQ-0", "review_text": "The paper shows that training with large batch size (e.g., with MxB samples) serves as an effective regularization method for deep networks, thus improving the convergence and generalization accuracy of the models. The enlarged batch of MxB consists of multiple (i.e., B) transforms of each of the M samples from the given batch; the transform is executed by a data augmentation method such as Cutout or Dropout. The authors also provide a theoretical explanation for the working of the method, suggesting that the enlarged batch training decreases the gradient variance during the training of the networks. The paper is well written and easy to follow. Also, some interesting results are experimentally obtained such as the figures presented in Figure4. Nevertheless, the experimental studies are not very satisfactory in its current form. Major remarks: 1. In terms of regularization with transformed data in a given batch, the proposed method is related to MixUp (Zhang et al., mixup: Beyond empirical risk minimization), AdaMixUp (Guo et al., MixUp as Locally Linear Out-Of-Manifold Regularization), Manifold Mixup (Verma et al., Manifold Mixup: Learning Better Representations by Interpolating Hidden States), and AgrLearn (Guo et al. Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks). It would be useful for the authors to discuss how the proposed strategy differs from them or empirically show how the proposed regularization method compares to them in terms of regularization effect. For example, in MixUp, AdaMixup and Manifold Mixup, the samples in a given batch will be linearly interpolated with randomly reshuffled samples of the same batch. In these sense, using them as baselines would make the contribution of the proposed method much significant. 2. In the experiments, it seems the authors use different data augmentation methods for different datasets (except for Cifar10 and Cifar100), it would be useful to stick with a particular data augmentation method for all the datasets, for example, it would be interesting to see the performance of also using Cutout for the MobileNet and ResNet50 on the ImageNet data set. 3. Regarding the experimental study, I wonder if it would be beneficial to include three variations of the proposed method. First, use baseline with the same batch size, namely BxM, but with sampling with replacement. That is, using the same batchsize as that in Batch Augmentation but with repeated samples. In this way, the contribution of the data augmentation in the proposed method would be much clearer. Second, as suggested from the results in the PTB data in Table1, using only Dropout obtains very minor improvement over the baseline method. In this sense, using other data augmentation methods instead of Cutout for the image tasks would make the contribution of the paper much clear. Third, training the networks with the batchsize of BxM, but excluding the original data samples in the given batch would be another interesting experiment. That is, all samples of the batch in the batch augmentation are synthetic samples. Minor remarks: 1. Is the regularized model robust to adversarial attacks as suggested in Mixup and Manifold Mixup? 2. Would it be beneficial to include various data augmentation methods for the same batch? That is, each transformed sample may come from a different data augmentation strategy. ==========after rebuttal=========== My main concern is that the paper did not clearly show where the performance improvement comes from. It may simply come from the larger batch size instead of the added augmented samples as claimed by the paper. I think the current comparison baseline in the paper is insufficient. I did propose three comparison baselines in my initial review, but I am not satisfied with the authors' rebuttal on that. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "1.Regarding mixup : mixup requires a mixed input from two separate labels as well as a mixed target by same amount . It does not deal with data augmentations as BA ( multiple instances of same sample ) . Therefore , Mixup approach is orthogonal to ours and both can be combined . 2.We used different augmentation techniques to emphasize the improvement of batch-augment upon them all . We commonly used augmentation for each network ( for example , cutout is common for modern cifar-10 based models , but not for ImageNet ) . We agree that applying BA with other augmentation techniques would make an interesting experiments , that can further improve accuracy , but we argue that this is not the essence of our work . 3.We 've added an additional experiment regarding training for longer with M * B batch size ( accounting for same number of examples ) in Appendix Figure 5 . We wish to clarify that in each experiment we 've performed ( including baseline ) the same augmentation technique was used ( according to original paper , or explicitly stated as in the case of cutout ) . 4.We stress that in this work , we do not suggest a new type of augmentation technique but rather a method that utilize any type of augmentation . Thus , we argue that BA should be as robust to adversarial attacks as the augmentation technique it utilize ( e.g. , cutout , random cropping , flipping , etc . ) . Nonetheless , we thank the reviewer for his suggestions and encourage researchers to use BA with different augmentation strategies ."}, {"review_id": "H1V4QhAqYQ-1", "review_text": "This paper tested a very simple idea: when we do large batch training, instead of sampling more training data for each minibatch, we use data augmentation techniques to generate training data from a small minibatch. The authors claim the proposed method has better generalization performance. I think it is an interesting idea, but the current draft does not provide sufficient support. 1. The proposed method is very simple. In this case, I would expect the authors provide more intuitive explanations. It looks to me the better generalization comes from more complicated data augmentation, not from the proposed large batch training. 2. It is unclear to me what is the benefit of the proposed method. Even provided more computing resources, the proposed method is not faster than small batch training. The improvement on test errors does not look significant. If given more computing resources, and under same timing constraint, we have many other methods to improve performance. For example, a simple thing to do is t0 separately train networks with standard setting and then ensemble trained networks. Or apply distributed knowledge distillation like in (Anil 2018 Large scale distributed neural network training through online distillation) 3. The experiments are not strong. The largest batch considered is 64*32, which is relatively small. In figure 1 (b), the results of M=4,8,16,32 are very similar, and it looks unstable. It is unclear what is the default batchsize for Imagenet. In Table 1, the proposed method tuned M as a hyperparameter. The baselines are fairly weak, the authors did not compare with any other method. I would expect at least the following baselines: i) use normal large batch training and complicated data augmentation, train the model for same number of epochs ii) use normal large batch training and complicated data augmentation, train the model for same number of iterations ii) use normal large batch training and complicated data augmentation, scale the learning rate up as in Goyal et al. 2017 4. For theorem 1, it is hard to say how much the theoretical analysis based on linear approximation near global minimizer would help understand the behavior of SGD. I fail to understand the the authors\u2019 augmentation. Following the author\u2019s logic, normal large batch training decrease the variability of <H>_k and \\lambda_max, which converges to \u2018\u2019flat\" minima. It contradicts with the authors\u2019 other explanation. 5. In section 4.2, I fail to understand why the proposed method can affect the norm of gradient. 6. Related works: Smith et al. 2018 Don't Decay the Learning Rate, Increase the Batch Size. =============== after rebuttal ==================== I appreciate the authors' response, but I do not think the rebuttal addressed my concerns. I will keep my score and argue for the rejection of this paper. My main concern is that the benefit of this method is unclear. The main baseline that has been compared is the standard small-batch training. However, the proposed method use a N times larger batch and same number of iterations, and hence N times more computation resources. Moreover, the proposed method also use N times more augmented samples. Like the authors said, they did not propose new data augmentation method, and their contribution is how to combine data augmentation with large-batch training. However, I am not convinced by the experiments that the good performance is from the proposed method, not from the N times more augmented samples. I have suggested the authors to compare with stronger baselines to demonstrate the benefits. However, the authors quote a previous paper that use different data augmentation and (potentially) other experimental settings. The proposed method looks unstable. Moreover, instead of showing the consistent benefits of large batch, the authors tune the batchsize as a hyperparameter for different experiments. Regarding the theoretical part, I still do not follow the authors' explanation. I think it could at least be improved for clarity. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "1.There may have been a misunderstanding : we compared our method to the baseline , and both had the same type of data augmentation ( e.g.in ResNet we did this comparison for both normal augmentation and cutout separately ) . We are therefore certain that the generalization improvements stem from batch-augment method , as it appears for all augmentation schemes we 've tried . 2.Both ensemble methods and `` Distributed knowledge distillation '' result in models larger then the original model , and therefore requires additional resources at run time ( after training ) . In contrast , our method does not have this issue as the final trained method is identical to the original . Moreover , our method is much simpler and does not require any change of settings - as we used the original training regime without modifications . Lastly , in many cases it is possible to increase the batch size without affecting the wall clock time , due to surplus compute power ( e.g. , Table 2 ) . In those cases , our BA method can be easily used to take advantage of this surplus large batch size , and improve the final model accuracy , as we demonstrated . 3.We kindly disagree , as we feel that results on various models and datasets show a consistent and ( mostly ) non-trivial improvement on baseline results . As others have shown before , a batch size of 64 * 32=2048 is not small , and often yields noticeably decreased in accuracy when training regime is not adapted [ 1 ] . We note that similar experiments to the ones the reviewer asked for were previously done in [ 1 , Table 1 & 2 ] for several datasets and models . For example , for a baseline of 93.07 % ( Resnet44 on cifar10 dataset ) , we improved to 93.65 % . However : ( i ) On large batch without adapting the training regime for the same number of steps : accuracy drops to 86.10 % [ 1 ] . ( ii+iii ) When using large batch for the same number of steps and learning rate is increased , accuracy returns to 93.07 % . For experiment ( ii ) we note that accuracy is marginally worse . We 've added this experiment to the paper along with convergence graphs ( Figure 5 , Appendix ) . 4 . `` For theorem 1 , it is hard to say how much the theoretical analysis based on linear approximation near global minimizer would help understand the behavior of SGD . '' Our theoretical analysis is focused on how SGD selects stationary points , using stability analysis . Such stability analysis requires linearization . `` I fail to understand the the authors \u2019 augmentation . Following the author \u2019 s logic , normal large batch training decrease the variability of < H > _k and \\lambda_max , which converges to \u2018 \u2019 flat \u2019 \u2019 minima . It contradicts with the authors \u2019 other explanation . '' There may have been a misunderstanding : increasing batch size will not decrease flatness , i.e.the maximal eigenvalue of the Hessian ( as defined in Keskar et al . ) , which is different from \\lambda_max . To clarify , we suggested in section 4 that BA works well since enables the model to observe more augmentations , with only a small effect on the variance ( since most of the samples in the mini-batch are highly correlated ) . This is in contrast to standard large-batch training , which works less well since it has a larger effect on the variance . 5.As we explained in section 4.2 , batch-augmentation causes each batch to have correlated samples ( different instances of the same image ) . When computing gradients on this batch we accumulate multiple gradient instances -- leading to smaller variance , and hence , smaller norm . This reduction is less than the reduction in large-batch training , since the batch instances are much more highly correlated . We 've added an additional figure ( Figure 6 , Appendix ) that demonstrates this point . [ 1 ] `` Train Longer Generalize Better '' - Hoffer et al ( NIPS 2017 ) ."}, {"review_id": "H1V4QhAqYQ-2", "review_text": "This paper describes a new method for data augmentation which is called batch augmentation. The idea is very simple -- include in your batch M augmentations of the each training sample, effectively this will increase the size of the batch by M. I have not seen a similar idea to this proposed before. As the authors show this simple technique has the potential to increase training convergence and final accuracy. Several experiments support the paper's claims illustrating the effectiveness of the technique on a variety of datasets (e.g. CIFAR, ImageNet, PTB) and architectures (ResNet, Wide-ResNet, DenseNet, MobileNets). Following that there's a more theoretical section which provides some analysis on why the method works, and seems also reasonable. Overall simple idea, well written-paper with clear practical application and of potential great interest to many researchers", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for his remarks and positive assessment of our work ."}], "0": {"review_id": "H1V4QhAqYQ-0", "review_text": "The paper shows that training with large batch size (e.g., with MxB samples) serves as an effective regularization method for deep networks, thus improving the convergence and generalization accuracy of the models. The enlarged batch of MxB consists of multiple (i.e., B) transforms of each of the M samples from the given batch; the transform is executed by a data augmentation method such as Cutout or Dropout. The authors also provide a theoretical explanation for the working of the method, suggesting that the enlarged batch training decreases the gradient variance during the training of the networks. The paper is well written and easy to follow. Also, some interesting results are experimentally obtained such as the figures presented in Figure4. Nevertheless, the experimental studies are not very satisfactory in its current form. Major remarks: 1. In terms of regularization with transformed data in a given batch, the proposed method is related to MixUp (Zhang et al., mixup: Beyond empirical risk minimization), AdaMixUp (Guo et al., MixUp as Locally Linear Out-Of-Manifold Regularization), Manifold Mixup (Verma et al., Manifold Mixup: Learning Better Representations by Interpolating Hidden States), and AgrLearn (Guo et al. Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks). It would be useful for the authors to discuss how the proposed strategy differs from them or empirically show how the proposed regularization method compares to them in terms of regularization effect. For example, in MixUp, AdaMixup and Manifold Mixup, the samples in a given batch will be linearly interpolated with randomly reshuffled samples of the same batch. In these sense, using them as baselines would make the contribution of the proposed method much significant. 2. In the experiments, it seems the authors use different data augmentation methods for different datasets (except for Cifar10 and Cifar100), it would be useful to stick with a particular data augmentation method for all the datasets, for example, it would be interesting to see the performance of also using Cutout for the MobileNet and ResNet50 on the ImageNet data set. 3. Regarding the experimental study, I wonder if it would be beneficial to include three variations of the proposed method. First, use baseline with the same batch size, namely BxM, but with sampling with replacement. That is, using the same batchsize as that in Batch Augmentation but with repeated samples. In this way, the contribution of the data augmentation in the proposed method would be much clearer. Second, as suggested from the results in the PTB data in Table1, using only Dropout obtains very minor improvement over the baseline method. In this sense, using other data augmentation methods instead of Cutout for the image tasks would make the contribution of the paper much clear. Third, training the networks with the batchsize of BxM, but excluding the original data samples in the given batch would be another interesting experiment. That is, all samples of the batch in the batch augmentation are synthetic samples. Minor remarks: 1. Is the regularized model robust to adversarial attacks as suggested in Mixup and Manifold Mixup? 2. Would it be beneficial to include various data augmentation methods for the same batch? That is, each transformed sample may come from a different data augmentation strategy. ==========after rebuttal=========== My main concern is that the paper did not clearly show where the performance improvement comes from. It may simply come from the larger batch size instead of the added augmented samples as claimed by the paper. I think the current comparison baseline in the paper is insufficient. I did propose three comparison baselines in my initial review, but I am not satisfied with the authors' rebuttal on that. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "1.Regarding mixup : mixup requires a mixed input from two separate labels as well as a mixed target by same amount . It does not deal with data augmentations as BA ( multiple instances of same sample ) . Therefore , Mixup approach is orthogonal to ours and both can be combined . 2.We used different augmentation techniques to emphasize the improvement of batch-augment upon them all . We commonly used augmentation for each network ( for example , cutout is common for modern cifar-10 based models , but not for ImageNet ) . We agree that applying BA with other augmentation techniques would make an interesting experiments , that can further improve accuracy , but we argue that this is not the essence of our work . 3.We 've added an additional experiment regarding training for longer with M * B batch size ( accounting for same number of examples ) in Appendix Figure 5 . We wish to clarify that in each experiment we 've performed ( including baseline ) the same augmentation technique was used ( according to original paper , or explicitly stated as in the case of cutout ) . 4.We stress that in this work , we do not suggest a new type of augmentation technique but rather a method that utilize any type of augmentation . Thus , we argue that BA should be as robust to adversarial attacks as the augmentation technique it utilize ( e.g. , cutout , random cropping , flipping , etc . ) . Nonetheless , we thank the reviewer for his suggestions and encourage researchers to use BA with different augmentation strategies ."}, "1": {"review_id": "H1V4QhAqYQ-1", "review_text": "This paper tested a very simple idea: when we do large batch training, instead of sampling more training data for each minibatch, we use data augmentation techniques to generate training data from a small minibatch. The authors claim the proposed method has better generalization performance. I think it is an interesting idea, but the current draft does not provide sufficient support. 1. The proposed method is very simple. In this case, I would expect the authors provide more intuitive explanations. It looks to me the better generalization comes from more complicated data augmentation, not from the proposed large batch training. 2. It is unclear to me what is the benefit of the proposed method. Even provided more computing resources, the proposed method is not faster than small batch training. The improvement on test errors does not look significant. If given more computing resources, and under same timing constraint, we have many other methods to improve performance. For example, a simple thing to do is t0 separately train networks with standard setting and then ensemble trained networks. Or apply distributed knowledge distillation like in (Anil 2018 Large scale distributed neural network training through online distillation) 3. The experiments are not strong. The largest batch considered is 64*32, which is relatively small. In figure 1 (b), the results of M=4,8,16,32 are very similar, and it looks unstable. It is unclear what is the default batchsize for Imagenet. In Table 1, the proposed method tuned M as a hyperparameter. The baselines are fairly weak, the authors did not compare with any other method. I would expect at least the following baselines: i) use normal large batch training and complicated data augmentation, train the model for same number of epochs ii) use normal large batch training and complicated data augmentation, train the model for same number of iterations ii) use normal large batch training and complicated data augmentation, scale the learning rate up as in Goyal et al. 2017 4. For theorem 1, it is hard to say how much the theoretical analysis based on linear approximation near global minimizer would help understand the behavior of SGD. I fail to understand the the authors\u2019 augmentation. Following the author\u2019s logic, normal large batch training decrease the variability of <H>_k and \\lambda_max, which converges to \u2018\u2019flat\" minima. It contradicts with the authors\u2019 other explanation. 5. In section 4.2, I fail to understand why the proposed method can affect the norm of gradient. 6. Related works: Smith et al. 2018 Don't Decay the Learning Rate, Increase the Batch Size. =============== after rebuttal ==================== I appreciate the authors' response, but I do not think the rebuttal addressed my concerns. I will keep my score and argue for the rejection of this paper. My main concern is that the benefit of this method is unclear. The main baseline that has been compared is the standard small-batch training. However, the proposed method use a N times larger batch and same number of iterations, and hence N times more computation resources. Moreover, the proposed method also use N times more augmented samples. Like the authors said, they did not propose new data augmentation method, and their contribution is how to combine data augmentation with large-batch training. However, I am not convinced by the experiments that the good performance is from the proposed method, not from the N times more augmented samples. I have suggested the authors to compare with stronger baselines to demonstrate the benefits. However, the authors quote a previous paper that use different data augmentation and (potentially) other experimental settings. The proposed method looks unstable. Moreover, instead of showing the consistent benefits of large batch, the authors tune the batchsize as a hyperparameter for different experiments. Regarding the theoretical part, I still do not follow the authors' explanation. I think it could at least be improved for clarity. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "1.There may have been a misunderstanding : we compared our method to the baseline , and both had the same type of data augmentation ( e.g.in ResNet we did this comparison for both normal augmentation and cutout separately ) . We are therefore certain that the generalization improvements stem from batch-augment method , as it appears for all augmentation schemes we 've tried . 2.Both ensemble methods and `` Distributed knowledge distillation '' result in models larger then the original model , and therefore requires additional resources at run time ( after training ) . In contrast , our method does not have this issue as the final trained method is identical to the original . Moreover , our method is much simpler and does not require any change of settings - as we used the original training regime without modifications . Lastly , in many cases it is possible to increase the batch size without affecting the wall clock time , due to surplus compute power ( e.g. , Table 2 ) . In those cases , our BA method can be easily used to take advantage of this surplus large batch size , and improve the final model accuracy , as we demonstrated . 3.We kindly disagree , as we feel that results on various models and datasets show a consistent and ( mostly ) non-trivial improvement on baseline results . As others have shown before , a batch size of 64 * 32=2048 is not small , and often yields noticeably decreased in accuracy when training regime is not adapted [ 1 ] . We note that similar experiments to the ones the reviewer asked for were previously done in [ 1 , Table 1 & 2 ] for several datasets and models . For example , for a baseline of 93.07 % ( Resnet44 on cifar10 dataset ) , we improved to 93.65 % . However : ( i ) On large batch without adapting the training regime for the same number of steps : accuracy drops to 86.10 % [ 1 ] . ( ii+iii ) When using large batch for the same number of steps and learning rate is increased , accuracy returns to 93.07 % . For experiment ( ii ) we note that accuracy is marginally worse . We 've added this experiment to the paper along with convergence graphs ( Figure 5 , Appendix ) . 4 . `` For theorem 1 , it is hard to say how much the theoretical analysis based on linear approximation near global minimizer would help understand the behavior of SGD . '' Our theoretical analysis is focused on how SGD selects stationary points , using stability analysis . Such stability analysis requires linearization . `` I fail to understand the the authors \u2019 augmentation . Following the author \u2019 s logic , normal large batch training decrease the variability of < H > _k and \\lambda_max , which converges to \u2018 \u2019 flat \u2019 \u2019 minima . It contradicts with the authors \u2019 other explanation . '' There may have been a misunderstanding : increasing batch size will not decrease flatness , i.e.the maximal eigenvalue of the Hessian ( as defined in Keskar et al . ) , which is different from \\lambda_max . To clarify , we suggested in section 4 that BA works well since enables the model to observe more augmentations , with only a small effect on the variance ( since most of the samples in the mini-batch are highly correlated ) . This is in contrast to standard large-batch training , which works less well since it has a larger effect on the variance . 5.As we explained in section 4.2 , batch-augmentation causes each batch to have correlated samples ( different instances of the same image ) . When computing gradients on this batch we accumulate multiple gradient instances -- leading to smaller variance , and hence , smaller norm . This reduction is less than the reduction in large-batch training , since the batch instances are much more highly correlated . We 've added an additional figure ( Figure 6 , Appendix ) that demonstrates this point . [ 1 ] `` Train Longer Generalize Better '' - Hoffer et al ( NIPS 2017 ) ."}, "2": {"review_id": "H1V4QhAqYQ-2", "review_text": "This paper describes a new method for data augmentation which is called batch augmentation. The idea is very simple -- include in your batch M augmentations of the each training sample, effectively this will increase the size of the batch by M. I have not seen a similar idea to this proposed before. As the authors show this simple technique has the potential to increase training convergence and final accuracy. Several experiments support the paper's claims illustrating the effectiveness of the technique on a variety of datasets (e.g. CIFAR, ImageNet, PTB) and architectures (ResNet, Wide-ResNet, DenseNet, MobileNets). Following that there's a more theoretical section which provides some analysis on why the method works, and seems also reasonable. Overall simple idea, well written-paper with clear practical application and of potential great interest to many researchers", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for his remarks and positive assessment of our work ."}}