{"year": "2021", "forum": "WqXAKcwfZtI", "title": "f-Domain-Adversarial Learning: Theory and Algorithms for Unsupervised Domain Adaptation with Neural Networks", "decision": "Reject", "meta_review": "This paper presents a novel theoretical analysis for unsupervised domain adaptation based on f-divergences. The reviews unanimously pointed out the interest and the quality of the theoretical part. However, some limitations in the experiments, presentation and the significance of the result have been raised. The authors provided a rebuttal that addresses some concerns.\nHowever, the reviewers agree that the experimental part still requires some extension to fully support the claim of the paper, as well as some writing improvement.\nThe paper was evaluated to be not ready for ICLR, thus I recommend rejection.\n", "reviews": [{"review_id": "WqXAKcwfZtI-0", "review_text": "Further comments after the rebuttal First , I would thank the authors for the great efforts in trying to address my comments . I should say that many of my previous concerns have been clarified . Again , I like the theoretical part . Nonetheless , I agreed with some other reviewers that the experiments seemed not to fully convince me . Particularly to myself , the authors may want to show some analysis on how their method could truly stand out by even a toy example , which may further enhance the paper . I tend to keep my original rating after reading all the rebuttal messages in the whole thread . == This paper proposed a f-domain adversarial learning framework ( f-DAL ) using the complete family of f-divergences as domain discrepancy measurements . The proposed method extended the seminal works of Ben-David et al . ( 2007 ; 2010a ; b ) ; Mansour et al . ( 2009 ) that provided generalization bounds for UDA based on a special type of f-divergence . To enable the complete family of f-divergences to measure the domain distribution discrepancy , the variational characterization of f-divergences is leveraged to estimates f-divergences from samples by turning the estimation problem into variational optimization . Furthermore , a new type of discrepancy was used to compare two marginal distribution and its corresponding generalization bound was tailored for a general class of f-divergence , which can mitigate two limitations of the seminal works . In addition , the optimal solution of f-DAL is a Stackelberg equilibrium , which allows f-DAL to incorporate the latest optimizers from the game-optimization literature , such as Aggressive Extra-Gradient ( AExG ) . The key strength of this paper is , it enables a generalized version of f-divergences that can be used for adversarial domain adaptation . This is valuable for UDA algorithms in many application areas . As verified in the experiments , the proposed method achieved comparable results in Office-31 and Office Home dataset . The paper was well organized and written . However , there are some major concerns as follows : ( 1 ) Although the authors provided theoretical insight for their method , the performance improvements are not very significant . ( 2 ) The ablation study needs to be further conducted . It is necessary to eliminate the Aggressive Extra-Gradient ( AExG ) and other learning strategies , such as spectral normalization ( SN ) and GRL warm-up strategy when compared with state-of-the-art methods . More specifically , the authors employed an integrated model that makes the comparison experiments seemingly unfair especially in the case of using MDD as the baseline . Although achieving the Stackelberg equilibria is a good characteristic of f-DAL , AExG may also increase the performance . Furthermore , the spectral normalization ( SN ) and GRL warm-up strategy may also potentially improve the performance . However , these techniques were seemingly not applied in other comparison methods in Table 2 and Table 3 . ( 3 ) The experiments may not answer the general question \u2018 ( 2 ) Is there a better universal notion of f-divergence that achieves significant performance gains across different datasets ? \u2019 . In Figure 3 , it is hard to say that there is a consistent increment tendency for a specific f-divergence across different transfer tasks . More importantly , it is better to demonstrate if choosing different f-divergence can lead to consistent performance increase on more diverse datasets or applications . ( 4 ) For a typical UDA theory method , it is better to show results on the VisDA-2017 dataset . ( 5 ) The authors offered some hints to generalize the proposed method to a multi-class scenario . It is better to provide theoretical insight/details . ( 6 ) It may enhance the paper if more illustrative or even toy examples can be conducted to further show clearly the advantages of the proposed method . Again , the improvement may not be very obvious as observed in the experiments though it is theoretically interesting .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the feedback . We address the questions below . R-1 ) \u201c Although the authors provided theoretical insight for their method , the performance improvements are not very significant. \u201d We believe it is significant that by connecting theory and algorithms ( and thus correcting previous adversarial domain adaptation methods that follows DANN [ 1 ] ) , we can achieve performance that previous methods can only achieve through adversarial learning + additional techniques ( i.e regularization , conditioning , and tweaking of extra hyperparams per dataset ) . In practical terms , the connection between theory and algorithms means replacing a domain classifier with a per-category domain classifier ( as the theory suggests ) and using a particular divergence from the family ( also inspired by the theory ) . Thus , we respectfully disagree with the view that the performance of our model is not very significant . To be more precise , our method requires one hyper-parameter less than MDD . This hyperparameter needs to be determined per dataset ( i.e Table 4 in [ 1 ] and section 5.1 therein ) , we do not have that limitation . We additionally emphasize that MDD is part of our framework itself ( it is a gamma-weight JS divergence Appendix D.3 ) , and that the gamma extra hyperparameter can be motivated for any divergence ( Sect 4 ) . We decided not to use it as it requires additional tuning . In comparison with CDAN ( the other SoTA algorithm ) , our method does not use a conditional discriminator or entropy minimization ( which requires additional hyperparameters as well ) and still manages to outperform it . As already mentioned , our framework is general in the sense that it includes a larger family of divergences ( i.e.the family of f-divergences ) and the general formulation of adversarial adversarial training as a zero-sum game leads to better optimization . Notice that the additional techniques can be applied on top of our framework to improve the performance further ( i.e the per-category domain classifier can be conditioned , and regularization terms can be added to the loss function ) . Overall , we believe that the aim for simplicity should not be confused with the performance strength of our framework . Our goal was to propose a unifying framework for different works in the literature , our results can be further improved with additional tuning . In 2b ) , we address questions related to fairness in the experiment set-up . R-2 ) \u201c The ablation study needs to be further conducted . It is necessary to eliminate the Aggressive Extra-Gradient ( AExG ) and other learning strategies \u201d a ) The ablation study that is asked in order to eliminate the effects of the Aggressive Extra-Gradient ( AExG ) vs the conventional setting ( SGD ) was shown in Figure 4 ( top ) ( Figure 5 left side in the updated revision ) . Notice that this was performed for every divergence , for every pair of datasets and using the same training condition that our experiments in Table 2 . This is all explained in the section \u201c What do we get by the \u201c extra \u201d gradient ? \u201d . Inspection of that figure shows that for the Pearson X^2 ( i.e the result shown in Table 2 vs SoTA baselines ) , the AExG led to a relative improvement of less than 0.5 in AVG ( Specifically the improvement is +0.35 ) . Notice that AExG is not the reason for our method outperforming the baselines . The use of the AExG in f-DAL is motivated in our framework after noticing and proving that optimality for the general framework is a Stackelberg Equilibrium ( Sect 4 and Appendix C ) . As mentioned , AExG indeed improves performance and faster convergence , which is inline with the insights from theoretical results presented in Sect 4 and Appendix C but it is not the reason for our method outperforming baselines . Due to the 8 page format , we believe that our experimental section was too compressed in the first submitted version , we extended and clarified it in the revised 9 page version ."}, {"review_id": "WqXAKcwfZtI-1", "review_text": "-- After rebuttal ( updated ) Thanks for your detailed response . I would like to apologize for my late feedback . Since your rebuttal is long without proper organization ( 5 pages without properly using markdown ) , I may miss several points . I directly commented my feedback on my original review , ` by the marked text ` Several of my concerns have been surely fixed and the understanding of the proposed approach is much more clear . However , it is still difficult to change my score because of the following reasons : 1 . * Current paper still requires careful polishing for facilitating reading . * e.g.The authors claimed several times they will update the paper with pseudo-code , it is still missing after discussion . The paper and the rebuttal are still dense , which make the reader difficult to understand . 2 . * The diverse datasets ( such as NLP , digits experiment in the original DANN paper ) and additional in-depth empirical analysis ( not only accuracy ) are indeed necessary . * I think you do not need to claim a significantly better performance with SOTA . The detailed empirical analysis is more important . I hope my additional comments and feedback can help you improve your paper . Summary : This paper proposed a hypothesis based f-divergence domain adaptation theory and algorithm . They found previous popular divergences such as H-divergence and MDD can be viewed as the special cases of f-divergence . Finally , they validated their practical benefits on office-31 and office-home dataset . Overall review Pros : [ 1 ] New DA theory on f-divergence . [ 2 ] The proof is technically sound . Cons : [ 1 ] The significance of the paper ( theoretical and practical ) is rather unclear . [ 2 ] The experimental results are weak . [ 3 ] Some details need better justifications and discussions . Based on these , I recommend a rejection but encourage a major revision for resubmission . -- Detailed explanations [ A ] Significance . ( theoretical and practical aspects ) Theoretical aspects [ 1 ] Using f-divergence in DA is not new . For instance , [ 1 ] [ 2 ] [ 3 ] [ 4 ] have already discussed DA by using $ \\beta $ ( R\u00e9nyi ) -divergence , $ \\chi $ divergence . I do not understand why such a * * unified * * f-divergence does not include these divergences . I guess because the theory requires Lipschitz f-divergence , but this is too limited . A theory on general f-divergence ( not only Lipschitz $ \\phi^ { \\star } $ ) is highly expected . ` Partially fixed . Missing point : it is possible to prove f-divergence only with f ( 1 ) =0 and f is convex ? ( the original definition of f-divergence ) ` [ 2 ] The theoretical assumptions are too restrictive for practice . I just list some of them : f-divergence is Lipschitz , $ \\ell\\in [ 0,1 ] $ and strong triangle inequality , deterministic label function setting . In contrast , previous work [ 2,3 ] has proposed strong theoretical guarantees for the cross-entropy loss and * * stochastic data generation process * * $ P ( y|x ) $ instead of $ f ( x ) $ . Given this paper is in the same nature as f-divergence . More general settings are * * highly * * expected . ` Not fixed . My concern is that the previous approach has proved stochastic settings in some f-divergence . This should be properly discussed and addressed. ` [ 3 ] The contribution of the optimization part is unclear . If this part is the theoretical contribution , an optimization convergence bound should be provided . ` Partially fixed . The term * rigorous * is confusing . You did not provide a bound how to define it is rigorous. ` [ 4 ] The theoretical assumptions on representation learning are strong . I am not sure how these are realistic in the office-31 and office-home dataset . The conditional distribution is not clearly defined . More discussions on this part are highly expected . ` Not fixed . I am still not sure how these are realistic . The conditional distribution is referred as a labeling function on $ x $ f ( x ) ? or $ z $ f ( z ) , or probability distribution P ( y|x ) or P ( y|z ) . This is quite important in the representation learning approach. ` [ 5 ] Theorem 3 seems too coarse in deep learning . ( Since this paper claims they have a strong theoretical contribution in deep learning.I think this ought to be addressed ) . ` Fixed ` [ 6 ] The KL-divergence is * * not tight * * if we use dual terms of f-divergence . A Donsker-Varadhan Theorem based theoretical analysis on KL divergence is expected . ` Partially fixed . The Wasserstein distance makes me more confused about introducing f-divergence . Since JS , TV has such problems , why not Wasserstein distance . It is always better. ` Practical aspects . [ 7 ] From a practical aspect , f-divergence adversarial training is not new . Based on the f-gans paper , we can practically easily replace JS divergence with any-other f-divergence without any technical difficulty . For example , [ 5 ] derives some DA algorithms on general f-divergence . From this perspective , the new empirical insights are rather limited . ` Fixed.Thanks for your additional figures . I understand your new practice. ` [ 8 ] The extra gradient approach presented in the experiments is rather unclear and inspired by existing optimization papers . I think a clear and complete discussion is expected . The current version seems like a plug-in approach . ` Not fixed . Maybe the rebuttal is too dense . I can not get your point ` [ 9 ] The whole empirical parts are presented in a dense mode , I suggest some parts can be safely moved to the appendix . ` Partially fixed . It is better but the dense wrap figure makes it still hard to read. ` [ 10 ] It is Ok not to provide the code , but a detailed algorithm description or protocol ought to be provided . This is particularly important when this paper aims at proposing several new ideas . ` Not fixed . This is really important . Author claimed they will write a description but i have not seen even in the appendix ` [ B ] Empirical results and analysis [ 1 ] From all the results . The empirical gain is too limited . Besides , the compared baselines are limited or not recent SOTA . ( The newest results only come from ICML 2019 ) . ` Not fixed . If you claim your approach is SOTA , at least a statistical test should be added to show it is indeed significant. ` [ 2 ] In office-home . I do not know why the std values are not reported in this case . The other 7 tasks are missing in the paper and appendix . The number of baselines is significantly fewer than office-31 . ` Partially fixed . STD still not reported and still fewer baselines . The author claimed previous papers did not report these .. But do you think it is a good thing not reporting variance ? Particularly in office-31 you reported std , which is really odd. ` [ 3 ] The standard digits datasets are not evaluated . Since the Digits dataset is trained from scratch and different from the pre-training approach . Testing on these is expected . ` It seems to be ignored .. I think this is essential to validate the theory. ` [ 4 ] When testing KL-divergence , the dual term of KL-divergence is not tight . I think a Donsker-Varadhan Theorem based practice should be tested . ` Fixed. ` [ C ] Other details The only analysis of this paper is the numerical accuracy of office-home/31 and toy data . A deeper analysis of why the proposed f-divergence is better than the previous is quite lacking . I can not feel the strong motivation of why preferring f-divergence . I suggest to put additional analysis such as T-SNE , the optimal value on * * real-data * * ( such as p_t ( z ) /p_s ( z ) ) , the convergence behavior , the evolution of f-divergence , ablation study . ` Not fixed . The only indicator in this paper is numerical accuracy . I still do not know what makes f-divergence better. ` -- Suggestions I suggest a major revision on the improved theory and empirical analysis ( not simply accuracy ) on the benefits of f-divergence . Ref : [ 1 ] Multiple source adaptation and the R\u00e9nyi divergence . UAI 2009 [ 2 ] A new PAC-Bayesian perspective on domain adaptation . ICML 2016 [ 3 ] Algorithms and theory for multiple-source adaptation . NeurIPS 2018 [ 4 ] Revisiting ( \\epsilon , \\gamma , \\tau ) -similarity learning for domain adaptation . NeurIPS 2018 [ 5 ] Domain adaptation with asymmetrically-relaxed distribution alignment . ICML 2019", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the detailed explanation provided . We also thank the reviewer for taking the time to verify our proofs and finding them technically sound . R-1.Using f-divergence in DA is not new . For instance , [ 1 ] [ 2 ] [ 3 ] [ 4 ] have already discussed DA by using ( R\u00e9nyi ) -divergence and \\chi^2 divergence , \u2026 \u201d Our novelty is in providing a unifying framework that connects theory and practical algorithms through its variational characterization . We can explain previous theoretical results and previous practical algorithms ( based on JS ) under the same framework . This follows from our analysis . Notice that methods that follow [ 6 ] ( which constitutes most of SoTA in Unsupervised Domain Adaptation ( UDA ) ) are explained with insights from the theory of Ben-David et al 2010 or similar reductions of the TV . In practice , the divergence that they minimize is JS ( Appendix D ) . We claim novelty for resolving this disconnect , and additionally for showing how a proper adversarial framework ( different from those that follow [ 6 ] ) results from resolving such a disconnect . Regarding \\chi^2 divergences , we did use Pearson \\chi^2 and Neyman chi^2 ( Table 1 and Table 4 ) . Indeed , we found the Pearson \\chi^2 works best across all datasets and scenarios in our experimental results . We do not consider ( R\u00e9nyi ) -divergences because these measures are not exactly f-divergences , they are obtained as monotone transformation of an appropriate f-divergence , \u201c as stated in 7.12 paragraph 1 second sentence in [ 9 ] \u201d . Our focus is on f-divergences which are already very general . That said , the \\beta_q divergence in [ 2 ] is compatible with our framework because according to [ 2 ] , eq . ( 7 ) : \\beta_q ( S||T ) ^q = \\int p_T ( p_S/p_T ) ^q dx . This is a f-divergence with \\phi ( x ) = x^q with q > 0 and phi * is C^1 in [ a , b ] then Lipschitz . For additional details and quantitative results about the theoretical and algorithmic significance please check \u201c Theoretical and Algorithmic Significance \u201c in the General Response . R-2 \u201c A theory on general f-divergence ( not only Lipschitz ) is highly expected\u2026 .. The theoretical assumptions are too restrictive for practice We do not completely understand why the restriction of \\phi^ * to be Lipschitz in a compact domain ( i.e [ a , b ] in the paper ) is too restrictive for practical applications . If \\phi * is continuously differentiable in [ a , b ] then the Lipschitz assumption holds . This assumption holds for most f-divergences which to the best of our knowledge are used in most practical applications . Notice that , the domain of \\phi^ * can be shifted by taking phi ( x ) to \\phi ( x ) + c x , or scaled with Lemma 4 , which changes the f-divergence Def . 1 only up to a shift of constant or up to a rescaling . In this way , \\phi * can always include the domain [ 0 , 1 ] as required by Theorem 2 and 3 . Regarding \\chi^2 divergences , we did use Pearson \\chi^2 and Neyman chi^2 ( Table 1 and Table 4 ) . Please let us know what other proper f-divergences ( beyond the ones stated in Table 1 and Table 4 ) are used in practical applications for domain adaptation with neural networks and do not satisfy these properties . R-3 - > \u201c deterministic label function setting. \u201d ... and \u201c stochastic data generation process \u201d We agree with this but we believe this should be left for further work as it does not constitute the essence of the current project . In this work , we aim to connect the previous theoretical works used to explain DANN [ 6 ] and similar SoTA algorithms , and the algorithms themselves . This results in a new domain adversarial framework as explained above . R-4 The contribution of the optimization part is unclear . If this part is the theoretical contribution , an optimization convergence bound should be provided . The optimization part shows that the desired solution of f-DAL is a Stackelberg Equilibrium . To the best of our knowledge , we are the first to rigorously show this . This key insight allows us to motivate the use of more suitable optimizers for this problem . We believe that providing a convergence bound for the optimization objective of f-DAL should be left for further work . This is a highly non-convex minimax optimization problem and it might be impossible to prove some results unless strong assumptions are stated . We emphasize the key idea of this section is to show that the desired solution of the proposed algorithmic framework is under mild assumptions a Stackelberg equilibrium . This is stated in the introductory paragraph of 4.1 . We have updated the section name to Optimality in f-DAL to make this clearer ."}, {"review_id": "WqXAKcwfZtI-2", "review_text": "Update after reading authors ' response . The authors did n't address my question on the statistical significance of their results compared with baselines . The authors can address this question by `` perform a double-sided student-t test to verify whether the performance difference between your method and the baselines are statistically significant . '' I suggest the authors to perform experiments to compare with SOTA baselines in the entire field of domain adaptation , instead of just comparing with a subset of algorithms that are based on adversarial training . After all , you want to see how your work stands in the entire field , rather than limited to a sub-field . This paper studies unsupervised domain adaptation . The authors derive a generalization bound that utilizes a new measure of discrepancy between distributions based on a variational characterization of f-divergences . They develop an algorithm for domain-adversarial learning for the family of f-divergences . Overall , I recommend to reject this paper , due to the following major concerns : 1 ) writing is not self-contained ; 2 ) important baselines are missing ; 3 ) insufficient justification of the advantage of the proposed method ; 4 ) experimental results are not strong . My major concerns of this paper include : 1 . The writing needs to be significantly improved , especially the experiment section . For example , it is very difficult to find out what those baselines in Table 2 refer to . The paper needs to be self-contained . Directing the readers to a third paper for important details such as experimental settings and results is not proper . 2.Some state-of-the-art baselines are not compared with . For example , Contrastive Adaptation Network for Unsupervised Domain Adaptation . Guoliang Kang , Lu Jiang , Yi Yang , Alexander G Hauptmann . CVPR 2019 . 3.In the generalization bound in Theorem 2 , the authors utilize a new discrepancy to compare the two marginal distributions . The authors did n't articulate the advantage of this new discrepancy over existing ones . Why is it significant so that we need to study it ? 4.In table 5 , are the results significantly different ? It would be nice to perform a double-sided student-t test to verify whether the performance difference between your method and the baselines are statistically significant . 5.Values of hyperparameters are missing , making this paper difficult to reproduce . If space is an issue , the authors can put the hyperparameters in the appendix . 6.The related works section in the appendix needs to significantly improved . There are a lot of works in the space of adversarial domain adaptation and in the broader space of general domain adaptation methods . The authors need to give a more comprehensive review . Again , the review needs to be self-contained in this paper instead of directing the readers to read other review papers . And the review needs to summarize the high-level key ideas , contributions , limitations of those papers instead of focusing on detailed math . However , this paper does have a few strong points . 1.The theoretical analysis in Section 3 is sound . I read the proofs , which appear to be correct . 2.Section 3 and 4 are well-organized and easy to follow . Other comments 1 . What does ADAA in Table 2 refer to ? 2.It 's better to use an algorithm box to outline the optimization algorithm in Section 4.1 . 3.The paragraph title `` Experimental results '' in Section 5 is confusing . This paragraph is more like an introduction of this section instead of one presenting results .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the detailed explanation provided . We also thank the reviewer for finding the theoretical analysis of Section 3 sound . R-1 1 ) Writing is not self-contained .... directing the readers to a third paper for important details such as experimental settings and results is not proper . This was an oversight . We have addressed this in the revision . We will also provide source code . R-2 Insufficient justification for the advantage of the proposed method\u2026 .. In the generalization bound in Theorem 2 , the authors utilize a new discrepancy to compare the two marginal distributions . The authors did n't articulate the advantage of this new discrepancy over existing ones . Why is it significant so that we need to study it ? We respectfully disagree . The motivation of the new discrepancy is stated clear from the abstract . The goal of the new discrepancy measure is to bridge the gap between theory and the state-of-the-art methods ( those that rely on adversarial learning ) while providing a unified framework . Our theoretical framework provides a unifying view that connects theory and practical algorithms . Importantly , it can explain previous theoretical results and previous algorithms under the same framework . This was not possible before . Most importantly , the insights from our theory allow us to derive a new adversarial algorithm that is both conceptually and practically different from previously analyzed adversarial approaches ( i.e those that follow [ 1 ] ) . Our resulting algorithmic approach ( f-DAL ) is different from other adversarial adaptation frameworks because our theory shows that we should use a per-category domain classifier instead of a \u201c global \u201d domain-classifier or \u201c discriminator \u201d ( please notice eq 4.3 \\hat H is the same ) . ( page 7 middle paragraphs ) . This is explained in the abstract , introduction ( paragraphs : 3 , 4 and 5 ) . First paragraph of 3.1 and last paragraph of 3.2 . It is also restated in the conclusions . R-3 In table 5 , are the results significantly different ? We do not have Table 5 in the paper , we assume the reviewer refers to Table 3 . We respectfully disagree with the view that the performance of our model is not very significant . Indeed , we believe it is significant that by connecting theory and algorithms ( and thus correcting previous adversarial domain adaptation methods that follows DANN [ 1 ] ) , we can achieve performances that previous methods can only achieve through adversarial learning + additional techniques ( i.e regularization , conditioning , and tweaking of extra hyperparams per dataset ) . Our goal was only to show that fixing this key ingredient ( inspired by the theory ) is powerful enough to achieve better results than other SoTA ."}, {"review_id": "WqXAKcwfZtI-3", "review_text": "# # # # # # # # # # # # # # # Summary : This paper proposes a new generalization bound for domain adaptation based on f-divergences . Accordingly , a new algorithmic framework is also derived . # # # # # # # # # # # # # # # Pros : Some theoretical results are new \u2013 to the best of my knowledge , there are no existing works establishing learning bounds for DA with f-divergences . One benefit of f-divergence is that it can offer a very general framework that accommodates various discrepancy measures for DA . The authors empirically evaluate different choices of divergences over several benchmarks . The paper is well-written and easy to follow . Cons : My main concern is the theoretical contribution and motivation of this work . Given a large variety of divergence measures ( e.g. , H-divergence , JS-divergence , Wasserstein distance , MDD\u2026 ) , this paper does not give me any new theoretical insights compared to previous results . From an algorithmic perspective , extending JS divergence to more general f-divergences has already studied in GAN training [ 1 ] . In addition , most of the theoretical analysis follows standard steps of existing works . For example , the high-level idea of Definition 3 follows the notion of Disparity Discrepancy in [ 2 ] and is also similar to the notion of source-guided discrepancy in [ 3 ] . The proofs of Lemma 1 , Lemma 2 , Theorem 2 , and Theorem 3 are either straightforward or follow standard techniques in DA ( e.g. , [ 2,4,5 ] ) . The empirical results in Table 3 indicate that the improvement of f-DAL is marginal compared to other algorithms ( e.g. , MDD ) . Furthermore , the improvement may even come from AExG , rather than f-DAL itself . I would speculate that f-DAL can even be outperformed by other baseline algorithms without AExG . # # # # # # # # # # # # # # # # Overall : While I appreciate the f-divergences generalization bounds derived in this paper , I didn \u2019 t get any new perspective after reading the paper . Every point of the theoretical results in this paper seems familiar to me . [ 1 ] Sebastian Nowozin , Botond Cseke , and Ryota Tomioka . f-gan : Training generative neural samplers using variational divergence minimization . In D. D. Lee , M. Sugiyama , U. V. Luxburg , I. Guyon , and R. Garnett ( eds . ) , Advances in Neural Information Processing Systems 29 , pp . 271\u2013279.Curran Associates , Inc. , 2016 . [ 2 ] Yuchen Zhang , Tianle Liu , Mingsheng Long , and Michael Jordan . Bridging theory and algorithm for domain adaptation . In Kamalika Chaudhuri and Ruslan Salakhutdinov ( eds . ) , Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pp . 7404\u20137413 , Long Beach , California , USA , 09\u201315 Jun 2019 . [ 3 ] Kuroki , S. , Charonenphakdee , N. , Bao , H. , Honda , J. , Sato , I. , and Sugiyama , M. Unsupervised domain adaptation based on source-guided discrepancy . In AAAI Conference on Artificial Intelligence ( AAAI ) , 2019 . [ 4 ] Shai Ben-David , John Blitzer , Koby Crammer , Alex Kulesza , Fernando Pereira , and Jennifer Wortman Vaughan . A theory of learning from different domains . Machine learning , 79 ( 1-2 ) :151\u2013175 , 2010 . [ 5 ] Han Zhao , Remi Tachet Des Combes , Kun Zhang , and Geoffrey Gordon . On learning invariant representations for domain adaptation . In International Conference on Machine Learning , pp . 7523-7532 , 2019 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the feedback and acknowledging the non-existence of works establishing learning bounds for DA with f-divergences . R-1 - \u201c My main concern is the theoretical contribution and motivation of this work . Given a large variety of divergence measures ( e.g. , H-divergence , JS-divergence , Wasserstein distance , MDD\u2026 ) , this paper does not give me any new theoretical insights compared to previous results. \u201d a ) The large variety of divergence measures used in the theoretical analysis of DA and the disconnect between these and the ones used in practice for adversarial learning ( mostly JS ) is one of the strong motivations of our work . Our theoretical framework provides a unifying view that connects theory and practical algorithms . Importantly , our approach explains previous theoretical results and previous algorithms under the same framework ( as the reviewer pointed ) . This allows us to explain how some optimization algorithms used in practice are less appropriate than others . Most importantly , the insights from our theory allows us to derive a new adversarial algorithm that is both conceptually and practically different from previously analyzed adversarial approaches ( i.e those that follow [ 1 ] ) . Our resulting algorithmic approach is different from others ' adversarial adaptation frameworks because our theory shows that we should use a per-category domain classifier instead of a \u201c global \u201d domain-classifier or \u201c discriminator \u201d ( please notice eq 4.3 \\hat H is the same ) . This was also explained in words on page 6 first paragraph ( middle of page 7 in updated revision and marked in red ) . Please notice that this key insight makes the algorithm both conceptually and practically different from previously analyzed adversarial approaches ( i.e those that follow [ 1 ] ) . We have further emphasized this in the revised manuscript , and provided a diagram that illustrates this important and key contribution . Quantitative Significance of this key contribution . Please check \u201c Theoretical and Algorithmic Significance \u201d in the general response . R-2 \u201c From an algorithmic perspective , extending JS divergence to more general f-divergences has already studied in GAN training [ 1 ] \u201d We respectfully disagree with this view . Our goal is conceptually and practically different to the one presented in f-GAN , and so it is the training algorithm . In addition to having a discrepancy term , our objective also incorporates a supervised loss ( unlike f-GAN ) . The intuition of a discriminator is also different ( as explained above ) . Conceptually , we aim to learn features that are indistinguishable from the point of view of the domains but discriminative enough for a classifier to make accurate predictions . GANs are generative models . Thus , while in both cases the variational characterization of f-divergences is exploited , the goal is different . From an algorithmic point of view , we also show optimality in our framework is at the Stackelberg Equilibrium ( Sect 4 and Appendix C ) . This motivates the use of more appropriate optimizers ( i.e ExtraGradient algorithms ) . Please check \u201c Theoretical and Algorithmic Significance \u201d in the general response for more details in the difference vs our method and traditional domain adversarial approaches . R-3 \u201c Most of the theoretical analysis follows standard steps of existing works. \u201d We do not see this as a limitation . Indeed , this shows the need for a framework that can summarize and explain all those works and beyond ( with a focus on what works in practice ) . The key observation is that most of them belong to the same \u201c f-divergence \u201d family and thus can be generalized through a reduction of its variational characterization . Thus , we believe the simplicity of the analysis has nothing to do with its significance and is actually one of its advantages . R-4 \u201c The empirical results in Table 3 indicate that the improvement of f-DAL is marginal compared to other algorithms ( e.g. , MDD ) \u201d We respectfully disagree with this view . We believe it is significant that by connecting theory and algorithm and fixing traditional adversarial approaches as explained above ( R-1 a ) ) , we can achieve performances that previous methods can only achieve through additional techniques ( i.e regularization , conditioning , and tweaking of extra hyperparams per dataset ) . We argue that a fair comparison would be vs DANN [ 1 ] using JS as the example provided above ( please check general response `` Theoretical and Algorithmic Significance '' ) . Most importantly , concerning the \u201c marginal improvement \u201d , MDD is part of our framework ( gamma JS in Appendix D ) but requires additional hyperpameter tuning per dataset ( see Table 4 in [ 2 ] ) . MDD is competitive with our approach only after this additional hyperparameter tuning that our approach does not use ( nor require ) . The goal of the result tables was to show that we can obtain state-of-the-art results with less tuning ."}], "0": {"review_id": "WqXAKcwfZtI-0", "review_text": "Further comments after the rebuttal First , I would thank the authors for the great efforts in trying to address my comments . I should say that many of my previous concerns have been clarified . Again , I like the theoretical part . Nonetheless , I agreed with some other reviewers that the experiments seemed not to fully convince me . Particularly to myself , the authors may want to show some analysis on how their method could truly stand out by even a toy example , which may further enhance the paper . I tend to keep my original rating after reading all the rebuttal messages in the whole thread . == This paper proposed a f-domain adversarial learning framework ( f-DAL ) using the complete family of f-divergences as domain discrepancy measurements . The proposed method extended the seminal works of Ben-David et al . ( 2007 ; 2010a ; b ) ; Mansour et al . ( 2009 ) that provided generalization bounds for UDA based on a special type of f-divergence . To enable the complete family of f-divergences to measure the domain distribution discrepancy , the variational characterization of f-divergences is leveraged to estimates f-divergences from samples by turning the estimation problem into variational optimization . Furthermore , a new type of discrepancy was used to compare two marginal distribution and its corresponding generalization bound was tailored for a general class of f-divergence , which can mitigate two limitations of the seminal works . In addition , the optimal solution of f-DAL is a Stackelberg equilibrium , which allows f-DAL to incorporate the latest optimizers from the game-optimization literature , such as Aggressive Extra-Gradient ( AExG ) . The key strength of this paper is , it enables a generalized version of f-divergences that can be used for adversarial domain adaptation . This is valuable for UDA algorithms in many application areas . As verified in the experiments , the proposed method achieved comparable results in Office-31 and Office Home dataset . The paper was well organized and written . However , there are some major concerns as follows : ( 1 ) Although the authors provided theoretical insight for their method , the performance improvements are not very significant . ( 2 ) The ablation study needs to be further conducted . It is necessary to eliminate the Aggressive Extra-Gradient ( AExG ) and other learning strategies , such as spectral normalization ( SN ) and GRL warm-up strategy when compared with state-of-the-art methods . More specifically , the authors employed an integrated model that makes the comparison experiments seemingly unfair especially in the case of using MDD as the baseline . Although achieving the Stackelberg equilibria is a good characteristic of f-DAL , AExG may also increase the performance . Furthermore , the spectral normalization ( SN ) and GRL warm-up strategy may also potentially improve the performance . However , these techniques were seemingly not applied in other comparison methods in Table 2 and Table 3 . ( 3 ) The experiments may not answer the general question \u2018 ( 2 ) Is there a better universal notion of f-divergence that achieves significant performance gains across different datasets ? \u2019 . In Figure 3 , it is hard to say that there is a consistent increment tendency for a specific f-divergence across different transfer tasks . More importantly , it is better to demonstrate if choosing different f-divergence can lead to consistent performance increase on more diverse datasets or applications . ( 4 ) For a typical UDA theory method , it is better to show results on the VisDA-2017 dataset . ( 5 ) The authors offered some hints to generalize the proposed method to a multi-class scenario . It is better to provide theoretical insight/details . ( 6 ) It may enhance the paper if more illustrative or even toy examples can be conducted to further show clearly the advantages of the proposed method . Again , the improvement may not be very obvious as observed in the experiments though it is theoretically interesting .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the feedback . We address the questions below . R-1 ) \u201c Although the authors provided theoretical insight for their method , the performance improvements are not very significant. \u201d We believe it is significant that by connecting theory and algorithms ( and thus correcting previous adversarial domain adaptation methods that follows DANN [ 1 ] ) , we can achieve performance that previous methods can only achieve through adversarial learning + additional techniques ( i.e regularization , conditioning , and tweaking of extra hyperparams per dataset ) . In practical terms , the connection between theory and algorithms means replacing a domain classifier with a per-category domain classifier ( as the theory suggests ) and using a particular divergence from the family ( also inspired by the theory ) . Thus , we respectfully disagree with the view that the performance of our model is not very significant . To be more precise , our method requires one hyper-parameter less than MDD . This hyperparameter needs to be determined per dataset ( i.e Table 4 in [ 1 ] and section 5.1 therein ) , we do not have that limitation . We additionally emphasize that MDD is part of our framework itself ( it is a gamma-weight JS divergence Appendix D.3 ) , and that the gamma extra hyperparameter can be motivated for any divergence ( Sect 4 ) . We decided not to use it as it requires additional tuning . In comparison with CDAN ( the other SoTA algorithm ) , our method does not use a conditional discriminator or entropy minimization ( which requires additional hyperparameters as well ) and still manages to outperform it . As already mentioned , our framework is general in the sense that it includes a larger family of divergences ( i.e.the family of f-divergences ) and the general formulation of adversarial adversarial training as a zero-sum game leads to better optimization . Notice that the additional techniques can be applied on top of our framework to improve the performance further ( i.e the per-category domain classifier can be conditioned , and regularization terms can be added to the loss function ) . Overall , we believe that the aim for simplicity should not be confused with the performance strength of our framework . Our goal was to propose a unifying framework for different works in the literature , our results can be further improved with additional tuning . In 2b ) , we address questions related to fairness in the experiment set-up . R-2 ) \u201c The ablation study needs to be further conducted . It is necessary to eliminate the Aggressive Extra-Gradient ( AExG ) and other learning strategies \u201d a ) The ablation study that is asked in order to eliminate the effects of the Aggressive Extra-Gradient ( AExG ) vs the conventional setting ( SGD ) was shown in Figure 4 ( top ) ( Figure 5 left side in the updated revision ) . Notice that this was performed for every divergence , for every pair of datasets and using the same training condition that our experiments in Table 2 . This is all explained in the section \u201c What do we get by the \u201c extra \u201d gradient ? \u201d . Inspection of that figure shows that for the Pearson X^2 ( i.e the result shown in Table 2 vs SoTA baselines ) , the AExG led to a relative improvement of less than 0.5 in AVG ( Specifically the improvement is +0.35 ) . Notice that AExG is not the reason for our method outperforming the baselines . The use of the AExG in f-DAL is motivated in our framework after noticing and proving that optimality for the general framework is a Stackelberg Equilibrium ( Sect 4 and Appendix C ) . As mentioned , AExG indeed improves performance and faster convergence , which is inline with the insights from theoretical results presented in Sect 4 and Appendix C but it is not the reason for our method outperforming baselines . Due to the 8 page format , we believe that our experimental section was too compressed in the first submitted version , we extended and clarified it in the revised 9 page version ."}, "1": {"review_id": "WqXAKcwfZtI-1", "review_text": "-- After rebuttal ( updated ) Thanks for your detailed response . I would like to apologize for my late feedback . Since your rebuttal is long without proper organization ( 5 pages without properly using markdown ) , I may miss several points . I directly commented my feedback on my original review , ` by the marked text ` Several of my concerns have been surely fixed and the understanding of the proposed approach is much more clear . However , it is still difficult to change my score because of the following reasons : 1 . * Current paper still requires careful polishing for facilitating reading . * e.g.The authors claimed several times they will update the paper with pseudo-code , it is still missing after discussion . The paper and the rebuttal are still dense , which make the reader difficult to understand . 2 . * The diverse datasets ( such as NLP , digits experiment in the original DANN paper ) and additional in-depth empirical analysis ( not only accuracy ) are indeed necessary . * I think you do not need to claim a significantly better performance with SOTA . The detailed empirical analysis is more important . I hope my additional comments and feedback can help you improve your paper . Summary : This paper proposed a hypothesis based f-divergence domain adaptation theory and algorithm . They found previous popular divergences such as H-divergence and MDD can be viewed as the special cases of f-divergence . Finally , they validated their practical benefits on office-31 and office-home dataset . Overall review Pros : [ 1 ] New DA theory on f-divergence . [ 2 ] The proof is technically sound . Cons : [ 1 ] The significance of the paper ( theoretical and practical ) is rather unclear . [ 2 ] The experimental results are weak . [ 3 ] Some details need better justifications and discussions . Based on these , I recommend a rejection but encourage a major revision for resubmission . -- Detailed explanations [ A ] Significance . ( theoretical and practical aspects ) Theoretical aspects [ 1 ] Using f-divergence in DA is not new . For instance , [ 1 ] [ 2 ] [ 3 ] [ 4 ] have already discussed DA by using $ \\beta $ ( R\u00e9nyi ) -divergence , $ \\chi $ divergence . I do not understand why such a * * unified * * f-divergence does not include these divergences . I guess because the theory requires Lipschitz f-divergence , but this is too limited . A theory on general f-divergence ( not only Lipschitz $ \\phi^ { \\star } $ ) is highly expected . ` Partially fixed . Missing point : it is possible to prove f-divergence only with f ( 1 ) =0 and f is convex ? ( the original definition of f-divergence ) ` [ 2 ] The theoretical assumptions are too restrictive for practice . I just list some of them : f-divergence is Lipschitz , $ \\ell\\in [ 0,1 ] $ and strong triangle inequality , deterministic label function setting . In contrast , previous work [ 2,3 ] has proposed strong theoretical guarantees for the cross-entropy loss and * * stochastic data generation process * * $ P ( y|x ) $ instead of $ f ( x ) $ . Given this paper is in the same nature as f-divergence . More general settings are * * highly * * expected . ` Not fixed . My concern is that the previous approach has proved stochastic settings in some f-divergence . This should be properly discussed and addressed. ` [ 3 ] The contribution of the optimization part is unclear . If this part is the theoretical contribution , an optimization convergence bound should be provided . ` Partially fixed . The term * rigorous * is confusing . You did not provide a bound how to define it is rigorous. ` [ 4 ] The theoretical assumptions on representation learning are strong . I am not sure how these are realistic in the office-31 and office-home dataset . The conditional distribution is not clearly defined . More discussions on this part are highly expected . ` Not fixed . I am still not sure how these are realistic . The conditional distribution is referred as a labeling function on $ x $ f ( x ) ? or $ z $ f ( z ) , or probability distribution P ( y|x ) or P ( y|z ) . This is quite important in the representation learning approach. ` [ 5 ] Theorem 3 seems too coarse in deep learning . ( Since this paper claims they have a strong theoretical contribution in deep learning.I think this ought to be addressed ) . ` Fixed ` [ 6 ] The KL-divergence is * * not tight * * if we use dual terms of f-divergence . A Donsker-Varadhan Theorem based theoretical analysis on KL divergence is expected . ` Partially fixed . The Wasserstein distance makes me more confused about introducing f-divergence . Since JS , TV has such problems , why not Wasserstein distance . It is always better. ` Practical aspects . [ 7 ] From a practical aspect , f-divergence adversarial training is not new . Based on the f-gans paper , we can practically easily replace JS divergence with any-other f-divergence without any technical difficulty . For example , [ 5 ] derives some DA algorithms on general f-divergence . From this perspective , the new empirical insights are rather limited . ` Fixed.Thanks for your additional figures . I understand your new practice. ` [ 8 ] The extra gradient approach presented in the experiments is rather unclear and inspired by existing optimization papers . I think a clear and complete discussion is expected . The current version seems like a plug-in approach . ` Not fixed . Maybe the rebuttal is too dense . I can not get your point ` [ 9 ] The whole empirical parts are presented in a dense mode , I suggest some parts can be safely moved to the appendix . ` Partially fixed . It is better but the dense wrap figure makes it still hard to read. ` [ 10 ] It is Ok not to provide the code , but a detailed algorithm description or protocol ought to be provided . This is particularly important when this paper aims at proposing several new ideas . ` Not fixed . This is really important . Author claimed they will write a description but i have not seen even in the appendix ` [ B ] Empirical results and analysis [ 1 ] From all the results . The empirical gain is too limited . Besides , the compared baselines are limited or not recent SOTA . ( The newest results only come from ICML 2019 ) . ` Not fixed . If you claim your approach is SOTA , at least a statistical test should be added to show it is indeed significant. ` [ 2 ] In office-home . I do not know why the std values are not reported in this case . The other 7 tasks are missing in the paper and appendix . The number of baselines is significantly fewer than office-31 . ` Partially fixed . STD still not reported and still fewer baselines . The author claimed previous papers did not report these .. But do you think it is a good thing not reporting variance ? Particularly in office-31 you reported std , which is really odd. ` [ 3 ] The standard digits datasets are not evaluated . Since the Digits dataset is trained from scratch and different from the pre-training approach . Testing on these is expected . ` It seems to be ignored .. I think this is essential to validate the theory. ` [ 4 ] When testing KL-divergence , the dual term of KL-divergence is not tight . I think a Donsker-Varadhan Theorem based practice should be tested . ` Fixed. ` [ C ] Other details The only analysis of this paper is the numerical accuracy of office-home/31 and toy data . A deeper analysis of why the proposed f-divergence is better than the previous is quite lacking . I can not feel the strong motivation of why preferring f-divergence . I suggest to put additional analysis such as T-SNE , the optimal value on * * real-data * * ( such as p_t ( z ) /p_s ( z ) ) , the convergence behavior , the evolution of f-divergence , ablation study . ` Not fixed . The only indicator in this paper is numerical accuracy . I still do not know what makes f-divergence better. ` -- Suggestions I suggest a major revision on the improved theory and empirical analysis ( not simply accuracy ) on the benefits of f-divergence . Ref : [ 1 ] Multiple source adaptation and the R\u00e9nyi divergence . UAI 2009 [ 2 ] A new PAC-Bayesian perspective on domain adaptation . ICML 2016 [ 3 ] Algorithms and theory for multiple-source adaptation . NeurIPS 2018 [ 4 ] Revisiting ( \\epsilon , \\gamma , \\tau ) -similarity learning for domain adaptation . NeurIPS 2018 [ 5 ] Domain adaptation with asymmetrically-relaxed distribution alignment . ICML 2019", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the detailed explanation provided . We also thank the reviewer for taking the time to verify our proofs and finding them technically sound . R-1.Using f-divergence in DA is not new . For instance , [ 1 ] [ 2 ] [ 3 ] [ 4 ] have already discussed DA by using ( R\u00e9nyi ) -divergence and \\chi^2 divergence , \u2026 \u201d Our novelty is in providing a unifying framework that connects theory and practical algorithms through its variational characterization . We can explain previous theoretical results and previous practical algorithms ( based on JS ) under the same framework . This follows from our analysis . Notice that methods that follow [ 6 ] ( which constitutes most of SoTA in Unsupervised Domain Adaptation ( UDA ) ) are explained with insights from the theory of Ben-David et al 2010 or similar reductions of the TV . In practice , the divergence that they minimize is JS ( Appendix D ) . We claim novelty for resolving this disconnect , and additionally for showing how a proper adversarial framework ( different from those that follow [ 6 ] ) results from resolving such a disconnect . Regarding \\chi^2 divergences , we did use Pearson \\chi^2 and Neyman chi^2 ( Table 1 and Table 4 ) . Indeed , we found the Pearson \\chi^2 works best across all datasets and scenarios in our experimental results . We do not consider ( R\u00e9nyi ) -divergences because these measures are not exactly f-divergences , they are obtained as monotone transformation of an appropriate f-divergence , \u201c as stated in 7.12 paragraph 1 second sentence in [ 9 ] \u201d . Our focus is on f-divergences which are already very general . That said , the \\beta_q divergence in [ 2 ] is compatible with our framework because according to [ 2 ] , eq . ( 7 ) : \\beta_q ( S||T ) ^q = \\int p_T ( p_S/p_T ) ^q dx . This is a f-divergence with \\phi ( x ) = x^q with q > 0 and phi * is C^1 in [ a , b ] then Lipschitz . For additional details and quantitative results about the theoretical and algorithmic significance please check \u201c Theoretical and Algorithmic Significance \u201c in the General Response . R-2 \u201c A theory on general f-divergence ( not only Lipschitz ) is highly expected\u2026 .. The theoretical assumptions are too restrictive for practice We do not completely understand why the restriction of \\phi^ * to be Lipschitz in a compact domain ( i.e [ a , b ] in the paper ) is too restrictive for practical applications . If \\phi * is continuously differentiable in [ a , b ] then the Lipschitz assumption holds . This assumption holds for most f-divergences which to the best of our knowledge are used in most practical applications . Notice that , the domain of \\phi^ * can be shifted by taking phi ( x ) to \\phi ( x ) + c x , or scaled with Lemma 4 , which changes the f-divergence Def . 1 only up to a shift of constant or up to a rescaling . In this way , \\phi * can always include the domain [ 0 , 1 ] as required by Theorem 2 and 3 . Regarding \\chi^2 divergences , we did use Pearson \\chi^2 and Neyman chi^2 ( Table 1 and Table 4 ) . Please let us know what other proper f-divergences ( beyond the ones stated in Table 1 and Table 4 ) are used in practical applications for domain adaptation with neural networks and do not satisfy these properties . R-3 - > \u201c deterministic label function setting. \u201d ... and \u201c stochastic data generation process \u201d We agree with this but we believe this should be left for further work as it does not constitute the essence of the current project . In this work , we aim to connect the previous theoretical works used to explain DANN [ 6 ] and similar SoTA algorithms , and the algorithms themselves . This results in a new domain adversarial framework as explained above . R-4 The contribution of the optimization part is unclear . If this part is the theoretical contribution , an optimization convergence bound should be provided . The optimization part shows that the desired solution of f-DAL is a Stackelberg Equilibrium . To the best of our knowledge , we are the first to rigorously show this . This key insight allows us to motivate the use of more suitable optimizers for this problem . We believe that providing a convergence bound for the optimization objective of f-DAL should be left for further work . This is a highly non-convex minimax optimization problem and it might be impossible to prove some results unless strong assumptions are stated . We emphasize the key idea of this section is to show that the desired solution of the proposed algorithmic framework is under mild assumptions a Stackelberg equilibrium . This is stated in the introductory paragraph of 4.1 . We have updated the section name to Optimality in f-DAL to make this clearer ."}, "2": {"review_id": "WqXAKcwfZtI-2", "review_text": "Update after reading authors ' response . The authors did n't address my question on the statistical significance of their results compared with baselines . The authors can address this question by `` perform a double-sided student-t test to verify whether the performance difference between your method and the baselines are statistically significant . '' I suggest the authors to perform experiments to compare with SOTA baselines in the entire field of domain adaptation , instead of just comparing with a subset of algorithms that are based on adversarial training . After all , you want to see how your work stands in the entire field , rather than limited to a sub-field . This paper studies unsupervised domain adaptation . The authors derive a generalization bound that utilizes a new measure of discrepancy between distributions based on a variational characterization of f-divergences . They develop an algorithm for domain-adversarial learning for the family of f-divergences . Overall , I recommend to reject this paper , due to the following major concerns : 1 ) writing is not self-contained ; 2 ) important baselines are missing ; 3 ) insufficient justification of the advantage of the proposed method ; 4 ) experimental results are not strong . My major concerns of this paper include : 1 . The writing needs to be significantly improved , especially the experiment section . For example , it is very difficult to find out what those baselines in Table 2 refer to . The paper needs to be self-contained . Directing the readers to a third paper for important details such as experimental settings and results is not proper . 2.Some state-of-the-art baselines are not compared with . For example , Contrastive Adaptation Network for Unsupervised Domain Adaptation . Guoliang Kang , Lu Jiang , Yi Yang , Alexander G Hauptmann . CVPR 2019 . 3.In the generalization bound in Theorem 2 , the authors utilize a new discrepancy to compare the two marginal distributions . The authors did n't articulate the advantage of this new discrepancy over existing ones . Why is it significant so that we need to study it ? 4.In table 5 , are the results significantly different ? It would be nice to perform a double-sided student-t test to verify whether the performance difference between your method and the baselines are statistically significant . 5.Values of hyperparameters are missing , making this paper difficult to reproduce . If space is an issue , the authors can put the hyperparameters in the appendix . 6.The related works section in the appendix needs to significantly improved . There are a lot of works in the space of adversarial domain adaptation and in the broader space of general domain adaptation methods . The authors need to give a more comprehensive review . Again , the review needs to be self-contained in this paper instead of directing the readers to read other review papers . And the review needs to summarize the high-level key ideas , contributions , limitations of those papers instead of focusing on detailed math . However , this paper does have a few strong points . 1.The theoretical analysis in Section 3 is sound . I read the proofs , which appear to be correct . 2.Section 3 and 4 are well-organized and easy to follow . Other comments 1 . What does ADAA in Table 2 refer to ? 2.It 's better to use an algorithm box to outline the optimization algorithm in Section 4.1 . 3.The paragraph title `` Experimental results '' in Section 5 is confusing . This paragraph is more like an introduction of this section instead of one presenting results .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the detailed explanation provided . We also thank the reviewer for finding the theoretical analysis of Section 3 sound . R-1 1 ) Writing is not self-contained .... directing the readers to a third paper for important details such as experimental settings and results is not proper . This was an oversight . We have addressed this in the revision . We will also provide source code . R-2 Insufficient justification for the advantage of the proposed method\u2026 .. In the generalization bound in Theorem 2 , the authors utilize a new discrepancy to compare the two marginal distributions . The authors did n't articulate the advantage of this new discrepancy over existing ones . Why is it significant so that we need to study it ? We respectfully disagree . The motivation of the new discrepancy is stated clear from the abstract . The goal of the new discrepancy measure is to bridge the gap between theory and the state-of-the-art methods ( those that rely on adversarial learning ) while providing a unified framework . Our theoretical framework provides a unifying view that connects theory and practical algorithms . Importantly , it can explain previous theoretical results and previous algorithms under the same framework . This was not possible before . Most importantly , the insights from our theory allow us to derive a new adversarial algorithm that is both conceptually and practically different from previously analyzed adversarial approaches ( i.e those that follow [ 1 ] ) . Our resulting algorithmic approach ( f-DAL ) is different from other adversarial adaptation frameworks because our theory shows that we should use a per-category domain classifier instead of a \u201c global \u201d domain-classifier or \u201c discriminator \u201d ( please notice eq 4.3 \\hat H is the same ) . ( page 7 middle paragraphs ) . This is explained in the abstract , introduction ( paragraphs : 3 , 4 and 5 ) . First paragraph of 3.1 and last paragraph of 3.2 . It is also restated in the conclusions . R-3 In table 5 , are the results significantly different ? We do not have Table 5 in the paper , we assume the reviewer refers to Table 3 . We respectfully disagree with the view that the performance of our model is not very significant . Indeed , we believe it is significant that by connecting theory and algorithms ( and thus correcting previous adversarial domain adaptation methods that follows DANN [ 1 ] ) , we can achieve performances that previous methods can only achieve through adversarial learning + additional techniques ( i.e regularization , conditioning , and tweaking of extra hyperparams per dataset ) . Our goal was only to show that fixing this key ingredient ( inspired by the theory ) is powerful enough to achieve better results than other SoTA ."}, "3": {"review_id": "WqXAKcwfZtI-3", "review_text": "# # # # # # # # # # # # # # # Summary : This paper proposes a new generalization bound for domain adaptation based on f-divergences . Accordingly , a new algorithmic framework is also derived . # # # # # # # # # # # # # # # Pros : Some theoretical results are new \u2013 to the best of my knowledge , there are no existing works establishing learning bounds for DA with f-divergences . One benefit of f-divergence is that it can offer a very general framework that accommodates various discrepancy measures for DA . The authors empirically evaluate different choices of divergences over several benchmarks . The paper is well-written and easy to follow . Cons : My main concern is the theoretical contribution and motivation of this work . Given a large variety of divergence measures ( e.g. , H-divergence , JS-divergence , Wasserstein distance , MDD\u2026 ) , this paper does not give me any new theoretical insights compared to previous results . From an algorithmic perspective , extending JS divergence to more general f-divergences has already studied in GAN training [ 1 ] . In addition , most of the theoretical analysis follows standard steps of existing works . For example , the high-level idea of Definition 3 follows the notion of Disparity Discrepancy in [ 2 ] and is also similar to the notion of source-guided discrepancy in [ 3 ] . The proofs of Lemma 1 , Lemma 2 , Theorem 2 , and Theorem 3 are either straightforward or follow standard techniques in DA ( e.g. , [ 2,4,5 ] ) . The empirical results in Table 3 indicate that the improvement of f-DAL is marginal compared to other algorithms ( e.g. , MDD ) . Furthermore , the improvement may even come from AExG , rather than f-DAL itself . I would speculate that f-DAL can even be outperformed by other baseline algorithms without AExG . # # # # # # # # # # # # # # # # Overall : While I appreciate the f-divergences generalization bounds derived in this paper , I didn \u2019 t get any new perspective after reading the paper . Every point of the theoretical results in this paper seems familiar to me . [ 1 ] Sebastian Nowozin , Botond Cseke , and Ryota Tomioka . f-gan : Training generative neural samplers using variational divergence minimization . In D. D. Lee , M. Sugiyama , U. V. Luxburg , I. Guyon , and R. Garnett ( eds . ) , Advances in Neural Information Processing Systems 29 , pp . 271\u2013279.Curran Associates , Inc. , 2016 . [ 2 ] Yuchen Zhang , Tianle Liu , Mingsheng Long , and Michael Jordan . Bridging theory and algorithm for domain adaptation . In Kamalika Chaudhuri and Ruslan Salakhutdinov ( eds . ) , Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pp . 7404\u20137413 , Long Beach , California , USA , 09\u201315 Jun 2019 . [ 3 ] Kuroki , S. , Charonenphakdee , N. , Bao , H. , Honda , J. , Sato , I. , and Sugiyama , M. Unsupervised domain adaptation based on source-guided discrepancy . In AAAI Conference on Artificial Intelligence ( AAAI ) , 2019 . [ 4 ] Shai Ben-David , John Blitzer , Koby Crammer , Alex Kulesza , Fernando Pereira , and Jennifer Wortman Vaughan . A theory of learning from different domains . Machine learning , 79 ( 1-2 ) :151\u2013175 , 2010 . [ 5 ] Han Zhao , Remi Tachet Des Combes , Kun Zhang , and Geoffrey Gordon . On learning invariant representations for domain adaptation . In International Conference on Machine Learning , pp . 7523-7532 , 2019 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the feedback and acknowledging the non-existence of works establishing learning bounds for DA with f-divergences . R-1 - \u201c My main concern is the theoretical contribution and motivation of this work . Given a large variety of divergence measures ( e.g. , H-divergence , JS-divergence , Wasserstein distance , MDD\u2026 ) , this paper does not give me any new theoretical insights compared to previous results. \u201d a ) The large variety of divergence measures used in the theoretical analysis of DA and the disconnect between these and the ones used in practice for adversarial learning ( mostly JS ) is one of the strong motivations of our work . Our theoretical framework provides a unifying view that connects theory and practical algorithms . Importantly , our approach explains previous theoretical results and previous algorithms under the same framework ( as the reviewer pointed ) . This allows us to explain how some optimization algorithms used in practice are less appropriate than others . Most importantly , the insights from our theory allows us to derive a new adversarial algorithm that is both conceptually and practically different from previously analyzed adversarial approaches ( i.e those that follow [ 1 ] ) . Our resulting algorithmic approach is different from others ' adversarial adaptation frameworks because our theory shows that we should use a per-category domain classifier instead of a \u201c global \u201d domain-classifier or \u201c discriminator \u201d ( please notice eq 4.3 \\hat H is the same ) . This was also explained in words on page 6 first paragraph ( middle of page 7 in updated revision and marked in red ) . Please notice that this key insight makes the algorithm both conceptually and practically different from previously analyzed adversarial approaches ( i.e those that follow [ 1 ] ) . We have further emphasized this in the revised manuscript , and provided a diagram that illustrates this important and key contribution . Quantitative Significance of this key contribution . Please check \u201c Theoretical and Algorithmic Significance \u201d in the general response . R-2 \u201c From an algorithmic perspective , extending JS divergence to more general f-divergences has already studied in GAN training [ 1 ] \u201d We respectfully disagree with this view . Our goal is conceptually and practically different to the one presented in f-GAN , and so it is the training algorithm . In addition to having a discrepancy term , our objective also incorporates a supervised loss ( unlike f-GAN ) . The intuition of a discriminator is also different ( as explained above ) . Conceptually , we aim to learn features that are indistinguishable from the point of view of the domains but discriminative enough for a classifier to make accurate predictions . GANs are generative models . Thus , while in both cases the variational characterization of f-divergences is exploited , the goal is different . From an algorithmic point of view , we also show optimality in our framework is at the Stackelberg Equilibrium ( Sect 4 and Appendix C ) . This motivates the use of more appropriate optimizers ( i.e ExtraGradient algorithms ) . Please check \u201c Theoretical and Algorithmic Significance \u201d in the general response for more details in the difference vs our method and traditional domain adversarial approaches . R-3 \u201c Most of the theoretical analysis follows standard steps of existing works. \u201d We do not see this as a limitation . Indeed , this shows the need for a framework that can summarize and explain all those works and beyond ( with a focus on what works in practice ) . The key observation is that most of them belong to the same \u201c f-divergence \u201d family and thus can be generalized through a reduction of its variational characterization . Thus , we believe the simplicity of the analysis has nothing to do with its significance and is actually one of its advantages . R-4 \u201c The empirical results in Table 3 indicate that the improvement of f-DAL is marginal compared to other algorithms ( e.g. , MDD ) \u201d We respectfully disagree with this view . We believe it is significant that by connecting theory and algorithm and fixing traditional adversarial approaches as explained above ( R-1 a ) ) , we can achieve performances that previous methods can only achieve through additional techniques ( i.e regularization , conditioning , and tweaking of extra hyperparams per dataset ) . We argue that a fair comparison would be vs DANN [ 1 ] using JS as the example provided above ( please check general response `` Theoretical and Algorithmic Significance '' ) . Most importantly , concerning the \u201c marginal improvement \u201d , MDD is part of our framework ( gamma JS in Appendix D ) but requires additional hyperpameter tuning per dataset ( see Table 4 in [ 2 ] ) . MDD is competitive with our approach only after this additional hyperparameter tuning that our approach does not use ( nor require ) . The goal of the result tables was to show that we can obtain state-of-the-art results with less tuning ."}}