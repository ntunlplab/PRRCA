{"year": "2019", "forum": "H1GLm2R9Km", "title": "Learning Backpropagation-Free Deep Architectures with Kernels", "decision": "Reject", "meta_review": "The reviewers mostly raised two concerns regarding the paper: a) why this algorithm is more interpretability than BP (which is just gradient descent); b) the exposition of the paper is somewhat confusing at various places; c) the lack of large-scale experiment results to show this is practically relevant. In the AC's opinion, a principled kernel-based approach can be counted as interpretable, and there the AC would support the paper if a) is the only concern. However, c) seems to be a serious concern since the paper doesn't seem to have experiments beyond fashion MNIST (e.g., CIFAR is pretty easy to train these days) and doesn't have experiments with convolutional models. Based on c), the AC decided that the paper is not quite ready for acceptance. ", "reviews": [{"review_id": "H1GLm2R9Km-0", "review_text": "****Reply to authors' rebuttal**** Dear Authors, Thank you very much for all the effort you have put into the rebuttal. Based on the improved theoretical and experimental results, I have decided to increase my score from 5 to 6. Best wishes, Rev 1 ****Original review**** This paper explores integration of kernel machines with neural networks based on replacing the non-linear function represented by each neuron with a function living in some pre-defined RKHS. From the theoretical standpoint, this work is a clear improvement upon the work of Zhang et al. (2017). Authors further propose a layer-wise training algorithm based on optimisation of a particular similarity measure between embeddings based on their class assignments at each layer, which eliminates necessity of gradient-based training. However, the experimental performance of the proposed algorithm is somewhat lacking in comparison, perhaps because the authors focus on kernelised equivalents of MLPs instead of CNNs as Zhang et al. My rating of the paper is mainly due to the lack of experimental evidence for usefulness of the layer-wise training, and absence of experimental comparison with several baselines (see details below). It is also unclear whether the structure of KNs is significantly better than that of NNs in terms of interpretability. Apart from the comments below, I would like to ask the authors to discuss relation to the following related papers: 1) Kulkarni & Karande, 2017: \"Layer-wise training of deep networks using kernel similarity\" https://arxiv.org/pdf/1703.07115.pdf 2) Scardapanea et al., 2017: \"Kafnets: kernel-based non-parametric activation functions for neural networks\" https://arxiv.org/pdf/1707.04035.pdf Detailed comments: Theory - (Sec 4.1) Backpropagation (BP) is being criticised: BP is only a particular implementation of gradient calculation. It seems to me that your criticisms are thus more related to use of iterative gradient-based optimisation algorithms, rather than to obtaining gradients through BP?! Regarding the criticism that BP forces intermediate layers to correct for \"mistakes\" made by layers higher up: it seems your layer-wise algorithm attempts to learn the best possible representation in first layer, and then progresses to the next layer where it tries to correct for the potential error of the first layer and so on. In other words it seems that the errors of layers are propagated from first to last, instead of last to first as in BP, but are still being propagated in a sense. I do not immediately see why propagation forward should be preferable. Can you please further explain this point? - It is proven in the appendix (Lemma B.3) that under certain conditions stacking additional layers never leads to degradation of training loss. Can you please clarify whether additional layers can be helpful even in the case where previous layers already succeeded in learning the optimal representation? - (Sec 4.1) Layer-wise vs. network-wise optimality: I find the claim that BP-based learner is not aware of the network-wise optimality confusing. BP explicitly optimises for network-wise optimality and the relative contribution to the network-wise error of each weight is propagated accordingly. I suppose my confusion stems from lack of a clear description of what defines a learner \"aware\" or \"blind\" to network-wise optimality. In general, I am not convinced layer-wise optimality is a useful criterion when what we want to achieve is network-wise optimality. As you show in the appendix, if layer-wise optimality is achieved then it implies network-wise optimality; however, layer-wise optimality is only a sufficient condition and likely not a necessary one (except for the simplified scenario studied in B.3). It is thus not clear to me why layer-wise training would always be preferable to network-wise training (e.g. using BP) especially because its greedy nature might intuitively prevent learning of hierarchical representations which are commonly claimed to be key to the success of neural networks. Can you please clarify? - (Sec 4.2) I think it would be beneficial to state in the introduction that the \"risk\" is with respect to the hinge loss which is common in the SVM/kernel literature but much less in the deep learning literature and thus could surprise a few people when they reach this point. Futher questions: - From Lemma 4.3, it seems that the derived representation is only optimal with respect to the **upper bound** on the empirical risk (which for \\tau >= 2 will be an upper bound on the population risk). I got slightly confused at this point as my interpretation of the previous text was that the representation is optimal with respect to the population risk itself. Does the upper bound have the same set of optima? Please clarify. - (p.5) There are two assumptions that I find somewhat restrictive. Just before Lemma 4.3 you assume that the number of points in each class must be the same. Can you comment on whether you expect the same representation to be optimal for classification problems with significantly imbalanced number of samples per class? The second assumption is after Lemma 4.4 where you state that the stationary kernel k^{l-1} should attain its infinum for all x, y s.t. || x - y || greater than some threshold. This does not hold for many of the popular kernels like RBF, Matern, or inverse multiquadric. Do you think this assumption can be relaxed? - (p.5) Choice of the dissimilarity measure for G: Can you provide more intuition about why you selected L^1 distance and whether you would expect different results with L^2 or other common metrics? - (Sec 4.3) Can you please provide more detaild about the relation of the proposed objective (\\hat(R)(F) + \\tau max_j ||f_j||_H) to Lemmas 4.3 and 4.5 where the optimal representation was derived for functions that optimise an upper bound in terms of Gaussian complexity (e.g. is the representation that minimises risk w.r.t. the Gaussian bound also optimal with respect to functions that optimise this objective)? Experiments - I would appreciate addition of some standard baselines, like MLP combined with dropout or batch normalisation, and optimised with RMSProp (or similar). These would greatly help with assessing competitiveness with current SOTA results. - It would be nice to see the relative contribution of the two main components of the paper. Specifically, an experiment which would evaluate empirical performance of KNs optimised by some form of gradient descent vs. by your layer-wise training rule would be very insightful. Other - (p.2, 1st par in Sec 2) [minor] You state \"a kernel machine is a universal function approximator\". I suppose that might be true for a certain class of kernels but not in general?! Please clarify. - (p.2, 3rd par in Sec 2) [minor] Are you using a particular version of the representer theorem in the representation of f_j^{(i)} as linear combination of feature maps? Please clarify. - (p.2, end of 1st par in Sec 3) L^{(i)} is defined as sup over X_i. It is not clear to me that this constant is necessarily finite and I suspect it will not be in general (it will for the RBF kernel (and most stationary kernels) used in experiments though). Finiteness of L^{(i)} is necessary for the bound in Eq. (2) to be non-vacuous. Please clarify. - (p.3, after 1st display in Sec 4.2.1) [minor] Missing dot after \"that we wish to minimise\". Next sentence states \"**the** optimal F\" (emphasis mine) -- I am sorry if I overlooked it, but I did not notice a proof that a solution exists and is unique, and am not familiar enough with the literature to immediately see the answer. Perhaps a footnote clarifying the statement would help. - (p.4, 1st par in Sec 4) You say \"A generalisation to regression is reserved for future work\". I did not expect that based on the first few pages. On high-level, it seems that generalisation to regression need not be trivial as, for example, the optimal representation derived in Lemma 4.3 and Lemma 4.5 explicitly relies on the classification nature of the problem. Can you comment on expected difficulty of extension to regression? Possibly state in the introduction that only classification is considered in this paper. - (p.7, 1st par in Sec 6) [related] \"However they did not extend the idea to any **arbitrary** NN\" (emphasis mine). Can you please be more specific here? - (p.5-6) [minor] Last sentence in Lemmas 4.3 and 4.5 is slightly confusing. Can you rephrase please? - (p.6) [minor] You say \"the learned decision boundary would generalise better to unseen data\". Can you please clarify the last sentence (e.g. being more precise about the meaning of the word \"simple\" in the same sentence) and provide reference for why this is necessarily the case?", "rating": "6: Marginally above acceptance threshold", "reply_text": "First , we thank Reviewer 1 for the very insightful comments . We can see that Reviewer 1 has read through our paper thoroughly and we are truly thankful for that . We will try our best to address the concerns and answer the questions from the reviewer and we hope that the reviewer finds our reply satisfying . Comments from the reviewer are listed first with each preceded by a dash . Our replies are put in brackets . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - GENERAL COMMENTS : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - ... which eliminates necessity of gradient-based training . [ Just to clarify , our layer-wise learning algorithm only eliminates the need of obtaining gradients using BP . It is still a gradient-based optimization per se . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - My rating of the paper is mainly due to the lack of experimental evidence for usefulness of the layer-wise training , and absence of experimental comparison with several baselines ( see details below ) . [ The objective of the current paper was to provide a comprehensive solution to the theoretical problem and therefore , majority of the time was spent on trying to achieve this goal . Nevertheless , we completely agree with the reviewer that more empirical results would complement the theory and henceforth , we are working hard to produce more results as suggested . We shall notify all reviewers once we have new results and have updated our manuscript accordingly . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - It is also unclear whether the structure of KNs is significantly better than that of NNs in terms of interpretability . [ We are thankful that the reviewer brought up this important issue . Please see our reply to all reviewers for our response . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- TWO RELATED PAPERS : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- We were not aware of these two papers and we thank Reviewer 1 for bringing them into our attention . Below are our comments on these two works , which we have added to the newest manuscript as well . 1 ) Kulkarni & Karande , 2017 : `` Layer-wise training of deep networks using kernel similarity '' https : //arxiv.org/pdf/1703.07115.pdf [ This work used the idea of an ideal kernel matrix to train NNs layer-by-layer . The activation of each layer , together with a kernel function that is separate from the architecture , is used to compute a kernel matrix . And the training of each layer amounts to aligning that kernel matrix to an ideal one . The ideal kernel matrix used therein is a special case of the ideal kernel matrix characterized by our Lemma 4.3 and Lemma 4.5 . However , this work did not discuss or prove the optimality of the underlying hidden representations for NNs . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - 2 ) Scardapanea et al. , 2017 : `` Kafnets : kernel-based non-parametric activation functions for neural networks '' https : //arxiv.org/pdf/1707.04035.pdf [ This work explored the possibility of substituting the nonlinearities of NNs with kernel expansions . While the resulting networks are similar to our KNs , the authors did not further study specially-tailored training methods as we did in our work . Instead , the resulting models are simply optimized with gradient-based optimization together with BP . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- DETAILED COMMENTS : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - ( Sec 4.1 ) Backpropagation ( BP ) is being criticised : BP is only a particular implementation of gradient calculation . It seems to me that your criticisms are thus more related to use of iterative gradient-based optimisation algorithms , rather than to obtaining gradients through BP ? ! [ Our criticism for BP in Section 1 is on the scheme of obtaining gradients via BP instead of gradient-based optimization algorithms . Our layer-wise learning algorithm is also gradient-based , as pointed out earlier in our reply . To further clarify , we now provide more details backing up our comments on BP in Section 1 : 1 ) BP can be computationally intensive and memory inefficient . This is because in standard BP , gradients for all layers have to be computed at each update . And one can either save these gradients while updating each layer ( memory inefficient ) , or compute gradient for each layer on the fly ( requires a lot of redundant computations since one has to differentiate through all layers between the output and the layer being updated ) . Clearly , a layer-wise learning approach mitigates this issue . 2 ) Obtaining gradients through BP can cause the vanishing gradient problem when the model contains a composition of many nonlinear layers . And it is clear that a layer-wise , gradient-based optimization approach is less subject to this issue since one no longer needs to differentiate through multiple layers for each gradient computation ."}, {"review_id": "H1GLm2R9Km-1", "review_text": "This paper attempts to learn layers of NNs greedily one at a time by using kernel machines as nodes instead of standard nonlinearities. The paper is well-written and was an interesting read, despite being notation heavy. I think the interpretability claims have some merits but are over-stated. Furthermore, the expressive power of universal approximation through kernels holds only asymptotically. So I am not sure if the authors can claim equivalence in expressive powers to more traditional NNs theoretically. I have some additional questions about the paper, and I am reserving my recommendation on this paper till the authors answer them. 1) Since individual node is simply a hyperplane in the induced kernel space, why not just specify the cost function as the risk + \\tau * norm(weights) ? What is the benefit of explicitly talking about gaussian complexities and delineating Theorem 4.2 when the same can be achieved by writing a much simpler form? Lemmas 4.4 and 4.5 should be straightforward extensions too if just used in this form since Lemma C.1 follows easily, and again could be simplified a lot by just using the regularized cost function. Am I missing something here? 2) Lemma 4.3 assumes separability (since c should be > a for \\tau to be positive) of classes, and also balanced classes (since number of positives = number of negatives). Why are these assumptions reasonable ? I understand that the empirical evaluation presented do justify the methodology, but I am wondering if based on these assumptions the theoretical results are of any use in the way they are currently presented. Minor : Below Def 4.1 \"to a standard normal distribution \" should be \"according to P\". Some typos, please proof read e.g. spelling error \"represnetation \". ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Firstly , we would like to thank Reviewer 2 for the insightful review . We have found the comments and questions really helpful and we now address them in details . We do hope that Reviewer 2 finds our response satisfying . Comments from the reviewer are listed first with each preceded by a dash . Our replies are put in brackets . -- -- -- -- -- -- -- -- - COMMENTS : -- -- -- -- -- -- -- -- - - I think the interpretability claims have some merits but are over-stated . [ We are thankful that the reviewer brought up this important issue . Please see our reply to all reviewers for our response . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - Furthermore , the expressive power of universal approximation through kernels holds only asymptotically . So I am not sure if the authors can claim equivalence in expressive powers to more traditional NNs theoretically . [ Generally , we think that the issue of expressive power is a highly abstract one and it is usually difficult to argue which model possesses stronger expressive power in very concrete terms . Although , we do agree with the reviewer that the universal approximation property of kernel machines holds only when we do not limit the number and the positions of its centroids [ 1 ] . Nevertheless , to the best of our knowledge , all classical universal approximation results for NNs are also asymptotical results [ 2 ] [ 3 ] [ 4 ] . Moreover , intuitively , a single node in a KN ( a kernel machine ) is already ( asymptotically ) a universal function approximator . In contrast , it takes at least two layers of NN to be ( asymptotically ) a universal function approximator . Further , in terms of some complexity measures such as Gaussian complexity , Lemma B.2 in our paper seems to show that the model complexity of kMLP is comparable to that of MLP [ 5 ] . And they also scale in a similar way in the depth and width of the network . Combining the arguments above , we expect KN to be at least comparable to NN in terms of expressive power , which is corroborated by our experimental results in the paper . ]"}, {"review_id": "H1GLm2R9Km-2", "review_text": "Summary: The paper considers so-called kernel neural networks where the non-linear activation function at each neuron is replaced by a kernelized linear operation, and analyses a layer-wise training scheme to train such networks. The theoretical claims are that (i) the optimal representation at each hidden layer can be determined by getting the similarity between two kernel matrices and (ii) this procedure gives a more interpretable training procedure and can avoid the vanishing gradient problems. Some small-scale experiments are provided. Evaluation: I have a mixed feeling about this paper: the theoretical contributions seem interesting but its interpretation and practicality are somewhat non-intuitive and philosophically troubling, in my opinion. I did not check the proofs in the appendix so I might have missed some critical info or have not fully understood the experimental set-up. - interpretability: it's not clear to me if this training scheme is any more interpretable than backprop training (not to mention it's not clear to me how to define interpretability for neural networks). Whether BP or any layer-wise training schemes is used, isn't the goal is to get S_{l-1} to the state where S_{l-1}s for examples of different classes are far away from each other as this is easier for the classifier? - function representation: in section 2, fj^i(x) is parameterized as a sum of kernel values evaluated at x and the training points. It's unclear to me what is x here -- input to the network or output of the previous layer? This also has a sum over all training points, so is training kMLPs in a layer-wise fashion more efficient than traditional kernel methods? - training scheme: what is the order of layers being trained? input to output or output to input? I'm slightly hazy on how to obtain F^{(l-1)}(S) to compute G_{l-1}. - the intuition of layer-wise optimality: on page 4, the paper states that \"the global min of R_l wrt S_{l-1} can be explicitly identified prior to any training\" but intuitively this must condition on some known function/function class F^(l). Could you please enlighten me on this? - the experiments are of small-scale and, as the paper pointed out, only demonstrating the concepts. What are the main practical difficulties preventing this from being applied to bigger networks/bigger datasets? - vanishing gradients: I'm not clear how layer-wise training can avoid this issue - could you please explain this? - some typos: p1 emplying -> employing, p4 supress -> suppress, p5 represnetation -> representation", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to first thank Reviewer 3 for the helpful comments and questions . Our detailed reply is provided below . And we hope that it answers the questions from Reviewer 3 . Comments from the reviewer are listed first with each preceded by a dash . Our replies are put in brackets . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - the theoretical contributions seem interesting but its interpretation and practicality are somewhat non-intuitive and philosophically troubling [ In terms of the interpretation of the theoretical results , our work showed that , thanks to the use of kernel functions as nonlinearities in a connectionist architecture , one can concretely answer the question : What is the best representation for each hidden layer ? This is the fundamental question behind training deep connectionist models [ 1 ] . Before , the most widely-accepted answer to this problem was an indirect one : Use BP . This is indirect since , despite that BP does the job of training the model well , it does not provide any interpretable or generalizable knowledge as to what defines a good hidden representation . In contrast , our Lemma 4.3 and Lemma 4.5 provided explicit and general conditions characterizing such optimal hidden representations . In terms of practicality , these theoretical results removed the need for BP and directly made possible a layer-wise learning algorithm with optimality guarantee equivalent to that offered by BP . Among works that try to replace BP , to the best of our knowledge , ours is the first to provide such an optimality guarantee , thanks to the theoretical results in this paper . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - interpretability : it 's not clear to me if this training scheme is any more interpretable than backprop training ( not to mention it 's not clear to me how to define interpretability for neural networks ) . [ We are thankful that the reviewer brought up this important issue . Please see our reply to all reviewers for our response . ] - Whether BP or any layer-wise training schemes is used , is n't the goal is to get S_ { l-1 } to the state where S_ { l-1 } s for examples of different classes are far away from each other as this is easier for the classifier ? [ For the hidden layers in NN , it is difficult to even talk about the notion of examples being `` far away from each other '' since there is no natural metric space in which the training can be geometrically interpreted or discussed . Of course , one may use the Euclidean space in which the hidden activation vectors live , but it is not entirely trivial ( at least to us ) how to prove that what backpropagation does is to push examples from distinct classes as far apart as possible in the metric of that Euclidean space . For KN trained with the proposed layer-wise algorithm , on the other hand , such an interpretation can be readily applied , as we have shown in Sections 4.2.2 and 4.2.3 . And this makes its learning dynamics more transparent and straightforward than NN . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - function representation : in section 2 , f_j^i ( x ) is parameterized as a sum of kernel values evaluated at x and the training points . It 's unclear to me what is x here -- input to the network or output of the previous layer ? [ Edit ( Nov. 26 ) : We have updated this section to clarify . Please refer to the newest manuscript for details . ] - This also has a sum over all training points , so is training kMLPs in a layer-wise fashion more efficient than traditional kernel methods ? [ In terms of computational complexity , a kMLP is more demanding than a traditional kernel machine since the latter corresponds to a single node in the former . Section 4.4 provided a natural accelerating approach to mitigate this issue in practice . Nevertheless , our results in Appendix B.5.1 suggest that kMLP performs much better than traditional kernel machine and kernel machine enhanced by state-of-the-art multiple kernel learning algorithms . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - training scheme : what is the order of layers being trained ? input to output or output to input ? I 'm slightly hazy on how to obtain F^ ( l-1 ) ( S ) to compute G_ { l-1 } . [ The training proceeds from input to output . Each layer is trained and frozen afterward . For example , one first train F^ ( 1 ) to minimize some dissimilarity measure between the ideal kernel matrix G^\\star and the actual kernel matrix G_1 defined as ( G_1 ) _ { mn } = k^ ( 2 ) ( F^ ( 1 ) ( x_m ) , F^ ( 1 ) ( x_n ) ) . After the training of F^ ( 1 ) , freeze it and call the frozen state F^ { ( 1 ) * } . Now start training F^ ( 2 ) to minimize some dissimilarity measure between G^\\star and kernel matrix G_2 defined as ( G_2 ) _ { mn } = k^ ( 3 ) ( F^ ( 2 ) \\circ F^ { ( 1 ) * } ( x_m ) , F^ ( 2 ) \\circ F^ { ( 1 ) * } ( x_n ) ) . And so on . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -"}], "0": {"review_id": "H1GLm2R9Km-0", "review_text": "****Reply to authors' rebuttal**** Dear Authors, Thank you very much for all the effort you have put into the rebuttal. Based on the improved theoretical and experimental results, I have decided to increase my score from 5 to 6. Best wishes, Rev 1 ****Original review**** This paper explores integration of kernel machines with neural networks based on replacing the non-linear function represented by each neuron with a function living in some pre-defined RKHS. From the theoretical standpoint, this work is a clear improvement upon the work of Zhang et al. (2017). Authors further propose a layer-wise training algorithm based on optimisation of a particular similarity measure between embeddings based on their class assignments at each layer, which eliminates necessity of gradient-based training. However, the experimental performance of the proposed algorithm is somewhat lacking in comparison, perhaps because the authors focus on kernelised equivalents of MLPs instead of CNNs as Zhang et al. My rating of the paper is mainly due to the lack of experimental evidence for usefulness of the layer-wise training, and absence of experimental comparison with several baselines (see details below). It is also unclear whether the structure of KNs is significantly better than that of NNs in terms of interpretability. Apart from the comments below, I would like to ask the authors to discuss relation to the following related papers: 1) Kulkarni & Karande, 2017: \"Layer-wise training of deep networks using kernel similarity\" https://arxiv.org/pdf/1703.07115.pdf 2) Scardapanea et al., 2017: \"Kafnets: kernel-based non-parametric activation functions for neural networks\" https://arxiv.org/pdf/1707.04035.pdf Detailed comments: Theory - (Sec 4.1) Backpropagation (BP) is being criticised: BP is only a particular implementation of gradient calculation. It seems to me that your criticisms are thus more related to use of iterative gradient-based optimisation algorithms, rather than to obtaining gradients through BP?! Regarding the criticism that BP forces intermediate layers to correct for \"mistakes\" made by layers higher up: it seems your layer-wise algorithm attempts to learn the best possible representation in first layer, and then progresses to the next layer where it tries to correct for the potential error of the first layer and so on. In other words it seems that the errors of layers are propagated from first to last, instead of last to first as in BP, but are still being propagated in a sense. I do not immediately see why propagation forward should be preferable. Can you please further explain this point? - It is proven in the appendix (Lemma B.3) that under certain conditions stacking additional layers never leads to degradation of training loss. Can you please clarify whether additional layers can be helpful even in the case where previous layers already succeeded in learning the optimal representation? - (Sec 4.1) Layer-wise vs. network-wise optimality: I find the claim that BP-based learner is not aware of the network-wise optimality confusing. BP explicitly optimises for network-wise optimality and the relative contribution to the network-wise error of each weight is propagated accordingly. I suppose my confusion stems from lack of a clear description of what defines a learner \"aware\" or \"blind\" to network-wise optimality. In general, I am not convinced layer-wise optimality is a useful criterion when what we want to achieve is network-wise optimality. As you show in the appendix, if layer-wise optimality is achieved then it implies network-wise optimality; however, layer-wise optimality is only a sufficient condition and likely not a necessary one (except for the simplified scenario studied in B.3). It is thus not clear to me why layer-wise training would always be preferable to network-wise training (e.g. using BP) especially because its greedy nature might intuitively prevent learning of hierarchical representations which are commonly claimed to be key to the success of neural networks. Can you please clarify? - (Sec 4.2) I think it would be beneficial to state in the introduction that the \"risk\" is with respect to the hinge loss which is common in the SVM/kernel literature but much less in the deep learning literature and thus could surprise a few people when they reach this point. Futher questions: - From Lemma 4.3, it seems that the derived representation is only optimal with respect to the **upper bound** on the empirical risk (which for \\tau >= 2 will be an upper bound on the population risk). I got slightly confused at this point as my interpretation of the previous text was that the representation is optimal with respect to the population risk itself. Does the upper bound have the same set of optima? Please clarify. - (p.5) There are two assumptions that I find somewhat restrictive. Just before Lemma 4.3 you assume that the number of points in each class must be the same. Can you comment on whether you expect the same representation to be optimal for classification problems with significantly imbalanced number of samples per class? The second assumption is after Lemma 4.4 where you state that the stationary kernel k^{l-1} should attain its infinum for all x, y s.t. || x - y || greater than some threshold. This does not hold for many of the popular kernels like RBF, Matern, or inverse multiquadric. Do you think this assumption can be relaxed? - (p.5) Choice of the dissimilarity measure for G: Can you provide more intuition about why you selected L^1 distance and whether you would expect different results with L^2 or other common metrics? - (Sec 4.3) Can you please provide more detaild about the relation of the proposed objective (\\hat(R)(F) + \\tau max_j ||f_j||_H) to Lemmas 4.3 and 4.5 where the optimal representation was derived for functions that optimise an upper bound in terms of Gaussian complexity (e.g. is the representation that minimises risk w.r.t. the Gaussian bound also optimal with respect to functions that optimise this objective)? Experiments - I would appreciate addition of some standard baselines, like MLP combined with dropout or batch normalisation, and optimised with RMSProp (or similar). These would greatly help with assessing competitiveness with current SOTA results. - It would be nice to see the relative contribution of the two main components of the paper. Specifically, an experiment which would evaluate empirical performance of KNs optimised by some form of gradient descent vs. by your layer-wise training rule would be very insightful. Other - (p.2, 1st par in Sec 2) [minor] You state \"a kernel machine is a universal function approximator\". I suppose that might be true for a certain class of kernels but not in general?! Please clarify. - (p.2, 3rd par in Sec 2) [minor] Are you using a particular version of the representer theorem in the representation of f_j^{(i)} as linear combination of feature maps? Please clarify. - (p.2, end of 1st par in Sec 3) L^{(i)} is defined as sup over X_i. It is not clear to me that this constant is necessarily finite and I suspect it will not be in general (it will for the RBF kernel (and most stationary kernels) used in experiments though). Finiteness of L^{(i)} is necessary for the bound in Eq. (2) to be non-vacuous. Please clarify. - (p.3, after 1st display in Sec 4.2.1) [minor] Missing dot after \"that we wish to minimise\". Next sentence states \"**the** optimal F\" (emphasis mine) -- I am sorry if I overlooked it, but I did not notice a proof that a solution exists and is unique, and am not familiar enough with the literature to immediately see the answer. Perhaps a footnote clarifying the statement would help. - (p.4, 1st par in Sec 4) You say \"A generalisation to regression is reserved for future work\". I did not expect that based on the first few pages. On high-level, it seems that generalisation to regression need not be trivial as, for example, the optimal representation derived in Lemma 4.3 and Lemma 4.5 explicitly relies on the classification nature of the problem. Can you comment on expected difficulty of extension to regression? Possibly state in the introduction that only classification is considered in this paper. - (p.7, 1st par in Sec 6) [related] \"However they did not extend the idea to any **arbitrary** NN\" (emphasis mine). Can you please be more specific here? - (p.5-6) [minor] Last sentence in Lemmas 4.3 and 4.5 is slightly confusing. Can you rephrase please? - (p.6) [minor] You say \"the learned decision boundary would generalise better to unseen data\". Can you please clarify the last sentence (e.g. being more precise about the meaning of the word \"simple\" in the same sentence) and provide reference for why this is necessarily the case?", "rating": "6: Marginally above acceptance threshold", "reply_text": "First , we thank Reviewer 1 for the very insightful comments . We can see that Reviewer 1 has read through our paper thoroughly and we are truly thankful for that . We will try our best to address the concerns and answer the questions from the reviewer and we hope that the reviewer finds our reply satisfying . Comments from the reviewer are listed first with each preceded by a dash . Our replies are put in brackets . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - GENERAL COMMENTS : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - ... which eliminates necessity of gradient-based training . [ Just to clarify , our layer-wise learning algorithm only eliminates the need of obtaining gradients using BP . It is still a gradient-based optimization per se . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - My rating of the paper is mainly due to the lack of experimental evidence for usefulness of the layer-wise training , and absence of experimental comparison with several baselines ( see details below ) . [ The objective of the current paper was to provide a comprehensive solution to the theoretical problem and therefore , majority of the time was spent on trying to achieve this goal . Nevertheless , we completely agree with the reviewer that more empirical results would complement the theory and henceforth , we are working hard to produce more results as suggested . We shall notify all reviewers once we have new results and have updated our manuscript accordingly . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - It is also unclear whether the structure of KNs is significantly better than that of NNs in terms of interpretability . [ We are thankful that the reviewer brought up this important issue . Please see our reply to all reviewers for our response . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- TWO RELATED PAPERS : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- We were not aware of these two papers and we thank Reviewer 1 for bringing them into our attention . Below are our comments on these two works , which we have added to the newest manuscript as well . 1 ) Kulkarni & Karande , 2017 : `` Layer-wise training of deep networks using kernel similarity '' https : //arxiv.org/pdf/1703.07115.pdf [ This work used the idea of an ideal kernel matrix to train NNs layer-by-layer . The activation of each layer , together with a kernel function that is separate from the architecture , is used to compute a kernel matrix . And the training of each layer amounts to aligning that kernel matrix to an ideal one . The ideal kernel matrix used therein is a special case of the ideal kernel matrix characterized by our Lemma 4.3 and Lemma 4.5 . However , this work did not discuss or prove the optimality of the underlying hidden representations for NNs . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - 2 ) Scardapanea et al. , 2017 : `` Kafnets : kernel-based non-parametric activation functions for neural networks '' https : //arxiv.org/pdf/1707.04035.pdf [ This work explored the possibility of substituting the nonlinearities of NNs with kernel expansions . While the resulting networks are similar to our KNs , the authors did not further study specially-tailored training methods as we did in our work . Instead , the resulting models are simply optimized with gradient-based optimization together with BP . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- DETAILED COMMENTS : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - ( Sec 4.1 ) Backpropagation ( BP ) is being criticised : BP is only a particular implementation of gradient calculation . It seems to me that your criticisms are thus more related to use of iterative gradient-based optimisation algorithms , rather than to obtaining gradients through BP ? ! [ Our criticism for BP in Section 1 is on the scheme of obtaining gradients via BP instead of gradient-based optimization algorithms . Our layer-wise learning algorithm is also gradient-based , as pointed out earlier in our reply . To further clarify , we now provide more details backing up our comments on BP in Section 1 : 1 ) BP can be computationally intensive and memory inefficient . This is because in standard BP , gradients for all layers have to be computed at each update . And one can either save these gradients while updating each layer ( memory inefficient ) , or compute gradient for each layer on the fly ( requires a lot of redundant computations since one has to differentiate through all layers between the output and the layer being updated ) . Clearly , a layer-wise learning approach mitigates this issue . 2 ) Obtaining gradients through BP can cause the vanishing gradient problem when the model contains a composition of many nonlinear layers . And it is clear that a layer-wise , gradient-based optimization approach is less subject to this issue since one no longer needs to differentiate through multiple layers for each gradient computation ."}, "1": {"review_id": "H1GLm2R9Km-1", "review_text": "This paper attempts to learn layers of NNs greedily one at a time by using kernel machines as nodes instead of standard nonlinearities. The paper is well-written and was an interesting read, despite being notation heavy. I think the interpretability claims have some merits but are over-stated. Furthermore, the expressive power of universal approximation through kernels holds only asymptotically. So I am not sure if the authors can claim equivalence in expressive powers to more traditional NNs theoretically. I have some additional questions about the paper, and I am reserving my recommendation on this paper till the authors answer them. 1) Since individual node is simply a hyperplane in the induced kernel space, why not just specify the cost function as the risk + \\tau * norm(weights) ? What is the benefit of explicitly talking about gaussian complexities and delineating Theorem 4.2 when the same can be achieved by writing a much simpler form? Lemmas 4.4 and 4.5 should be straightforward extensions too if just used in this form since Lemma C.1 follows easily, and again could be simplified a lot by just using the regularized cost function. Am I missing something here? 2) Lemma 4.3 assumes separability (since c should be > a for \\tau to be positive) of classes, and also balanced classes (since number of positives = number of negatives). Why are these assumptions reasonable ? I understand that the empirical evaluation presented do justify the methodology, but I am wondering if based on these assumptions the theoretical results are of any use in the way they are currently presented. Minor : Below Def 4.1 \"to a standard normal distribution \" should be \"according to P\". Some typos, please proof read e.g. spelling error \"represnetation \". ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Firstly , we would like to thank Reviewer 2 for the insightful review . We have found the comments and questions really helpful and we now address them in details . We do hope that Reviewer 2 finds our response satisfying . Comments from the reviewer are listed first with each preceded by a dash . Our replies are put in brackets . -- -- -- -- -- -- -- -- - COMMENTS : -- -- -- -- -- -- -- -- - - I think the interpretability claims have some merits but are over-stated . [ We are thankful that the reviewer brought up this important issue . Please see our reply to all reviewers for our response . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - Furthermore , the expressive power of universal approximation through kernels holds only asymptotically . So I am not sure if the authors can claim equivalence in expressive powers to more traditional NNs theoretically . [ Generally , we think that the issue of expressive power is a highly abstract one and it is usually difficult to argue which model possesses stronger expressive power in very concrete terms . Although , we do agree with the reviewer that the universal approximation property of kernel machines holds only when we do not limit the number and the positions of its centroids [ 1 ] . Nevertheless , to the best of our knowledge , all classical universal approximation results for NNs are also asymptotical results [ 2 ] [ 3 ] [ 4 ] . Moreover , intuitively , a single node in a KN ( a kernel machine ) is already ( asymptotically ) a universal function approximator . In contrast , it takes at least two layers of NN to be ( asymptotically ) a universal function approximator . Further , in terms of some complexity measures such as Gaussian complexity , Lemma B.2 in our paper seems to show that the model complexity of kMLP is comparable to that of MLP [ 5 ] . And they also scale in a similar way in the depth and width of the network . Combining the arguments above , we expect KN to be at least comparable to NN in terms of expressive power , which is corroborated by our experimental results in the paper . ]"}, "2": {"review_id": "H1GLm2R9Km-2", "review_text": "Summary: The paper considers so-called kernel neural networks where the non-linear activation function at each neuron is replaced by a kernelized linear operation, and analyses a layer-wise training scheme to train such networks. The theoretical claims are that (i) the optimal representation at each hidden layer can be determined by getting the similarity between two kernel matrices and (ii) this procedure gives a more interpretable training procedure and can avoid the vanishing gradient problems. Some small-scale experiments are provided. Evaluation: I have a mixed feeling about this paper: the theoretical contributions seem interesting but its interpretation and practicality are somewhat non-intuitive and philosophically troubling, in my opinion. I did not check the proofs in the appendix so I might have missed some critical info or have not fully understood the experimental set-up. - interpretability: it's not clear to me if this training scheme is any more interpretable than backprop training (not to mention it's not clear to me how to define interpretability for neural networks). Whether BP or any layer-wise training schemes is used, isn't the goal is to get S_{l-1} to the state where S_{l-1}s for examples of different classes are far away from each other as this is easier for the classifier? - function representation: in section 2, fj^i(x) is parameterized as a sum of kernel values evaluated at x and the training points. It's unclear to me what is x here -- input to the network or output of the previous layer? This also has a sum over all training points, so is training kMLPs in a layer-wise fashion more efficient than traditional kernel methods? - training scheme: what is the order of layers being trained? input to output or output to input? I'm slightly hazy on how to obtain F^{(l-1)}(S) to compute G_{l-1}. - the intuition of layer-wise optimality: on page 4, the paper states that \"the global min of R_l wrt S_{l-1} can be explicitly identified prior to any training\" but intuitively this must condition on some known function/function class F^(l). Could you please enlighten me on this? - the experiments are of small-scale and, as the paper pointed out, only demonstrating the concepts. What are the main practical difficulties preventing this from being applied to bigger networks/bigger datasets? - vanishing gradients: I'm not clear how layer-wise training can avoid this issue - could you please explain this? - some typos: p1 emplying -> employing, p4 supress -> suppress, p5 represnetation -> representation", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to first thank Reviewer 3 for the helpful comments and questions . Our detailed reply is provided below . And we hope that it answers the questions from Reviewer 3 . Comments from the reviewer are listed first with each preceded by a dash . Our replies are put in brackets . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - the theoretical contributions seem interesting but its interpretation and practicality are somewhat non-intuitive and philosophically troubling [ In terms of the interpretation of the theoretical results , our work showed that , thanks to the use of kernel functions as nonlinearities in a connectionist architecture , one can concretely answer the question : What is the best representation for each hidden layer ? This is the fundamental question behind training deep connectionist models [ 1 ] . Before , the most widely-accepted answer to this problem was an indirect one : Use BP . This is indirect since , despite that BP does the job of training the model well , it does not provide any interpretable or generalizable knowledge as to what defines a good hidden representation . In contrast , our Lemma 4.3 and Lemma 4.5 provided explicit and general conditions characterizing such optimal hidden representations . In terms of practicality , these theoretical results removed the need for BP and directly made possible a layer-wise learning algorithm with optimality guarantee equivalent to that offered by BP . Among works that try to replace BP , to the best of our knowledge , ours is the first to provide such an optimality guarantee , thanks to the theoretical results in this paper . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - interpretability : it 's not clear to me if this training scheme is any more interpretable than backprop training ( not to mention it 's not clear to me how to define interpretability for neural networks ) . [ We are thankful that the reviewer brought up this important issue . Please see our reply to all reviewers for our response . ] - Whether BP or any layer-wise training schemes is used , is n't the goal is to get S_ { l-1 } to the state where S_ { l-1 } s for examples of different classes are far away from each other as this is easier for the classifier ? [ For the hidden layers in NN , it is difficult to even talk about the notion of examples being `` far away from each other '' since there is no natural metric space in which the training can be geometrically interpreted or discussed . Of course , one may use the Euclidean space in which the hidden activation vectors live , but it is not entirely trivial ( at least to us ) how to prove that what backpropagation does is to push examples from distinct classes as far apart as possible in the metric of that Euclidean space . For KN trained with the proposed layer-wise algorithm , on the other hand , such an interpretation can be readily applied , as we have shown in Sections 4.2.2 and 4.2.3 . And this makes its learning dynamics more transparent and straightforward than NN . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - function representation : in section 2 , f_j^i ( x ) is parameterized as a sum of kernel values evaluated at x and the training points . It 's unclear to me what is x here -- input to the network or output of the previous layer ? [ Edit ( Nov. 26 ) : We have updated this section to clarify . Please refer to the newest manuscript for details . ] - This also has a sum over all training points , so is training kMLPs in a layer-wise fashion more efficient than traditional kernel methods ? [ In terms of computational complexity , a kMLP is more demanding than a traditional kernel machine since the latter corresponds to a single node in the former . Section 4.4 provided a natural accelerating approach to mitigate this issue in practice . Nevertheless , our results in Appendix B.5.1 suggest that kMLP performs much better than traditional kernel machine and kernel machine enhanced by state-of-the-art multiple kernel learning algorithms . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - - training scheme : what is the order of layers being trained ? input to output or output to input ? I 'm slightly hazy on how to obtain F^ ( l-1 ) ( S ) to compute G_ { l-1 } . [ The training proceeds from input to output . Each layer is trained and frozen afterward . For example , one first train F^ ( 1 ) to minimize some dissimilarity measure between the ideal kernel matrix G^\\star and the actual kernel matrix G_1 defined as ( G_1 ) _ { mn } = k^ ( 2 ) ( F^ ( 1 ) ( x_m ) , F^ ( 1 ) ( x_n ) ) . After the training of F^ ( 1 ) , freeze it and call the frozen state F^ { ( 1 ) * } . Now start training F^ ( 2 ) to minimize some dissimilarity measure between G^\\star and kernel matrix G_2 defined as ( G_2 ) _ { mn } = k^ ( 3 ) ( F^ ( 2 ) \\circ F^ { ( 1 ) * } ( x_m ) , F^ ( 2 ) \\circ F^ { ( 1 ) * } ( x_n ) ) . And so on . ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -"}}