{"year": "2019", "forum": "rJg8yhAqKm", "title": "InfoBot: Transfer and Exploration via the Information Bottleneck", "decision": "Accept (Poster)", "meta_review": "The paper presents the use of information bottlenecks as a way to identify key \"decision states\" in exploration, in a goal-conditioned model. The concept of \"decision states\" is actually common in RL, states where exploring can lead to very diverse/new states. The implementation of the \"information bottleneck\" is done by adding a regularizing term, the conditional mutual information I(A;G|S).\n\nThe main weaknesses of the paper were its lack of clarity and the experimental section. It seems to me that the rebuttals, and the additional experiments and details, made the paper worthy of publication. The authors cleared enough of the gray areas and showcased the relative merits of the methods.", "reviews": [{"review_id": "rJg8yhAqKm-0", "review_text": "The authors propose a new regularizer for policy search in a multi-goal RL setting. The objective promotes a more efficient exploration strategy by encouraging the agent to learn policies that depend as little as possible on the target goal. This is achieved by regularizing standard RL losses with the negative conditional mutual information I(A;G|S). Although this regularizer cannot be optimize, the authors propose a tractable bound. The net effect of this regularizer is to promote more effective exploration by encouraging the agent to visit decision states, in which goal-depend decisions play a more important role. The idea of using this particular regularizer is inspired by an existing line of work on the information bottleneck. I find the idea proposed by the authors to be interesting. However, I have the following concerns, and overall I think this paper is borderline. 1. The quality of the experimental validation provided by the authors is in my opinion borderline acceptable. Although the method performs better on toy settings, it seems barely better on more challenging ones. Experiments in section 4.5 lack detail and context. 2. The clarity of the presentation is also not great. 2.1. The two-stage nature of the method was confusing to me. I didn\u2019t understand the role of the second stage. Most focus is on the first stage, and only very little on the second stage. For example, I was confused about why the sign of the regularizer was flipped. 2.2. I was confused by how exactly the bounds (3) and (4) we applied and in what order. 2.3. I think the intuition of the method could be better explained and better validated by experiments. I also have the following additional comments: * How is the regularizer applied with other policy search algorithms besides Reinforce? Was it done in the paper? I can\u2019t say for sure. Specifically, when comparing to PPO, was the algorithm compared to a version of PPO augmented with this regularizer? Why yes or why no? * More generally, experiments where more modern policy search algorithms are combined with the regularizer would be helpful. In particular, does it matter which policy search algorithm we use with this method? * Experimental plots in section 4.4 are missing error bars, and I can\u2019t tell if the results are significant without them. * I thought the motivation for choosing this regularizer was lacking. The authors cite the information bottleneck literature, but we shouldn\u2019t need to read all these papers, the main ideas should be summarized here. * The argument for how the regularizer improves exploration seemed to me very hand-wavy and not well substantiated by experiments. * I would love to see a better discussion of how the method is useful when he RL setting is not truly multi-goal. * The second part of the algorithm needs to be explained much more clearly. * What is the effect of the approximation on Q? --- I have read the response of the authors, and they have addressed a significant numbers of concerns that I had. I am upgrading my rating to a 7.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the positive and constructive feedback . We appreciate that the reviewer finds that our method interesting . `` The quality of the experimental validation provided by the authors is in my opinion borderline acceptable . Although the method performs better on toy settings , it seems barely better on more challenging ones . Experiments in section 4.5 lack detail and context . '' In order to make our experiments more rigorous , we conducted more experiments to answer reviewers concern . 1 ) More challenging Navigation setup . - We ask the reviewer to refer to heading `` More challenging navigation Environment '' . Also for more details refer to Section 4.6 in the main paper . We compared to several strong baselines like hierarchical RL methods , strong exploration methods as well as goal based methods . We would be happy to add more comparisons which reviewer has in mind . 2 ) Comparison to State of the art Off policy Methods ( SAC ) in sparse rewards . We ask the reviewer to refer to heading `` InfoBot Comparison to State of the art off policy methods ( Soft Actor Critic ) '' . Also , for more details refer to Section F in the appendix . 3 ) Application of the proposed method in multi-agent communication , such that the goal in the proposed method corresponds to the information obtained because of communication with another agent in multi-agent communication channel . Here , we want to show that by training agents to develop \u201c default behaviours \u201d as well as the knowledge of when to break those behaviours , using an information bottleneck can also help in other scenarios like multi-agent communication . Consider multiagent communication , where in order to solve a task , agents require communicating with another agents . Ideally , an agent would would like to communicate with other agent , only when its essential to communicate , i.e the agents would like to minimize the communication with another agents . Here we show that selectively deciding when to communicate with another agent can result in faster learning . We follow the same experimental setup as in the paper ( Mordatch and Abbeel , 2018 ) . Method Train Reward TestReward No Communication -0.919 -0.920 Communication -0.36 -0.472 Communication ( with KL cost ) -0.293 -0.38 ( Lower is better ) . More details about this experimental setup can be found in Section D ( Appendix ) . I. Mordatch and P. Abbeel . Emergence of grounded compositional language in multi-agent pop- ulations . We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places . We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer 's suggestions ."}, {"review_id": "rJg8yhAqKm-1", "review_text": "This paper proposes the concept of decision state, which is the state where decision is made \u201cmore\u201d dependent to a particular goal. The authors propose a KL divergence regularization to learn the structure of the tasks, and then use this information to encourage the policy to visit the decision states. The method is tested on several different experiment setups. In general the paper is well-written and easy to follow. Learning a more general policy is not new (as also discussed in the paper), but using the learned structure to further guide the exploration of the policy is novel and interesting. I have a couple questions about the experimental part though, mostly about the baselines. 1. What is the reasoning behind the selection of the baselines, e.g. A2C as the baseline for the miniGrid experiments? 2. What are the performances of the methods in Table 2, in direct policy generalization? Or is there any reason not reporting them here? 3. What is the reasoning of picking \u201cCount-base baseline\u201d for Figure 4, rather than the method of curiosity-based exploration? 4. For the Mujoco tasks, there are couple ones outperforming PPO, e.g. TD3, SAC etc.. [1,2] The authors should include their results too. 5. As an ablation study, it would be interesting to see how the bonus reward of visiting decision states can help the exploration on the training tasks, compared to the policy learned from equation (1), and the policies learned without information of other tasks. 6. Lastly, the idea of decision states can also be used in other RL algorithms. It would be also interesting to see if this idea can further improve their performances. Other comments: 1. Equation (3) should be \\le. 2. Why would Equation (5) hold? 3. Right before section 2.2, incomplete sentence. Disclaimer: The reviewer is not familiar with multitask reinforcement learning, and the miniGrid environment in the paper. Other reviewers should have better judgement on the significance of the experimental results. [1] Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint arXiv:1801.01290 (2018). [2] Fujimoto, Scott, Herke van Hoof, and Dave Meger. \"Addressing Function Approximation Error in Actor-Critic Methods.\" arXiv preprint arXiv:1802.09477 (2018).", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their time and feedback . We hope to address concerns the reviewer has here . `` What is the reasoning behind the selection of the baselines , e.g.A2C as the baseline for the miniGrid experiments ? `` We build from A2C with using goal-conditioned policies and the KL regularization . Hence , The A2C with no kl-regularization is the immediate baseline to consider . Since for maze experiments , the env is of the nature of mini-grid POMDP environments with sparse rewards , as well as discrete action , a2c worked out of the box and hence was the most straightforward baseline for comparison . We would be happy to add other comparisons which reviewer has in mind . > > What are the performances of the methods in Table 2 , in direct policy generalization ? Or is there any reason not reporting them here ? The setup in direct policy generalization is different as to what we evaluate in Table 2 . Direct policy generalization refers to first training an agent with a goal bottleneck on one set of environments ( MultiRoomN2S6 ) , and then evaluate the trained agent ( without fine tuning on new set of environment ( ( MultiRoomN3S4 , MultiRoomN4S4 , and MultiRoomN5S4 ) ) . What we evaluate in Table 2 , is how can we transfer the knowledge in form of decision states . Basically the intuition is , what we would like from any unsupervised/supervised transferrable exploration technique is to build a policy that is somehow good for adapting to or solving new problems . Here we try to generalize to new mazes the knowledge acquired by the encoder in the form of the KL estimator . Basically , the intuition is that high KL = interesting state , even before the agent has discovered a single path to the goal . So if we can use egocentric observations and generalize effectively , we can predict which points have high KL before we have even learned to traverse the maze , and then we can use these high-KL regions as rewards without the need to have solved that particular maze in advance , using knowledge transferred from other mazes . To do this , we first train agents with a goal bottleneck on one set of environments ( MultiRoomN2S6 ) where they learn the sensory cues that correspond to decision states . Then , we use this knowledge to guide exploration on another set of environments ( MultiRoomN3S4 , MultiRoomN4S4 , and MultiRoomN5S4 ) . And hence in this new environments , we are training another policy from scratch , And using the KL from D_KL ( p ( z|s , g ) | N ( 0,1 ) ) as an exploration bonus to guide exploration ."}, {"review_id": "rJg8yhAqKm-2", "review_text": "The paper proposes a method of regularising goal-conditioned policies with a mutual information term. While this is potentially useful, I found the motivation for the approach and the experimental results insufficient. On top of that the presentation could also use some improvements. I do not recommend acceptance at this time. The introduction is vague and involves undefined terms such as \"useful habits\". It is not clear what problems the authors have in mind and why exactly they propose their specific method. The presentation of the method itself is not self-contained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper. Some symbols are not defined, for example what is Z and why is it discrete? The experiments are rather weak, they are missing comparison to strong exploration baselines and goal-oriented baselines.", "rating": "3: Clear rejection", "reply_text": "Thanks for the feedback . We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below . We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have . `` While this is potentially useful , I found the motivation for the approach It is not clear what problems the authors have in mind and why exactly they propose their specific method . '' We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places . We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer 's suggestions . We focus on multi-goal environments and goal-conditioned policies . The problem statement is quite simple : we aim to propose an algorithm whereby we incentive agents to learn task structure by training policies that perform well under a variety of goals , while not overfitting to any individual goal . We achieve this by training agents that , in addition to maximizing reward , minimize the policy dependence on the individual goal , quantified by the conditional mutual information I ( A ; G | S ) . In order to minimize this quantity , we formulate it using ideas from variational information bottleneck . To make the paper self-explanatory , we added the mathematical description of the proposed method in the appendix ( Section A ) . `` The presentation of the method itself is not self-contained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper. `` We again acknowledge that the paper was missing certain parts which made the paper difficult to read . We have added another section in the appendix which gives a more mathematical description of the proposed approach . We realized because of the way we have explained things there could be some fundamental misunderstanding about the proposed method . Thus , we would like to clarify this misunderstanding , not only with the intent of convincing you of the idea behind the proposed method but also with the intent of making amends to the method description where necessary so that readers may not arrive at the same conclusions as you . We added the mathematical description of the proposed framework in the appendix ( Section A ) ."}], "0": {"review_id": "rJg8yhAqKm-0", "review_text": "The authors propose a new regularizer for policy search in a multi-goal RL setting. The objective promotes a more efficient exploration strategy by encouraging the agent to learn policies that depend as little as possible on the target goal. This is achieved by regularizing standard RL losses with the negative conditional mutual information I(A;G|S). Although this regularizer cannot be optimize, the authors propose a tractable bound. The net effect of this regularizer is to promote more effective exploration by encouraging the agent to visit decision states, in which goal-depend decisions play a more important role. The idea of using this particular regularizer is inspired by an existing line of work on the information bottleneck. I find the idea proposed by the authors to be interesting. However, I have the following concerns, and overall I think this paper is borderline. 1. The quality of the experimental validation provided by the authors is in my opinion borderline acceptable. Although the method performs better on toy settings, it seems barely better on more challenging ones. Experiments in section 4.5 lack detail and context. 2. The clarity of the presentation is also not great. 2.1. The two-stage nature of the method was confusing to me. I didn\u2019t understand the role of the second stage. Most focus is on the first stage, and only very little on the second stage. For example, I was confused about why the sign of the regularizer was flipped. 2.2. I was confused by how exactly the bounds (3) and (4) we applied and in what order. 2.3. I think the intuition of the method could be better explained and better validated by experiments. I also have the following additional comments: * How is the regularizer applied with other policy search algorithms besides Reinforce? Was it done in the paper? I can\u2019t say for sure. Specifically, when comparing to PPO, was the algorithm compared to a version of PPO augmented with this regularizer? Why yes or why no? * More generally, experiments where more modern policy search algorithms are combined with the regularizer would be helpful. In particular, does it matter which policy search algorithm we use with this method? * Experimental plots in section 4.4 are missing error bars, and I can\u2019t tell if the results are significant without them. * I thought the motivation for choosing this regularizer was lacking. The authors cite the information bottleneck literature, but we shouldn\u2019t need to read all these papers, the main ideas should be summarized here. * The argument for how the regularizer improves exploration seemed to me very hand-wavy and not well substantiated by experiments. * I would love to see a better discussion of how the method is useful when he RL setting is not truly multi-goal. * The second part of the algorithm needs to be explained much more clearly. * What is the effect of the approximation on Q? --- I have read the response of the authors, and they have addressed a significant numbers of concerns that I had. I am upgrading my rating to a 7.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the positive and constructive feedback . We appreciate that the reviewer finds that our method interesting . `` The quality of the experimental validation provided by the authors is in my opinion borderline acceptable . Although the method performs better on toy settings , it seems barely better on more challenging ones . Experiments in section 4.5 lack detail and context . '' In order to make our experiments more rigorous , we conducted more experiments to answer reviewers concern . 1 ) More challenging Navigation setup . - We ask the reviewer to refer to heading `` More challenging navigation Environment '' . Also for more details refer to Section 4.6 in the main paper . We compared to several strong baselines like hierarchical RL methods , strong exploration methods as well as goal based methods . We would be happy to add more comparisons which reviewer has in mind . 2 ) Comparison to State of the art Off policy Methods ( SAC ) in sparse rewards . We ask the reviewer to refer to heading `` InfoBot Comparison to State of the art off policy methods ( Soft Actor Critic ) '' . Also , for more details refer to Section F in the appendix . 3 ) Application of the proposed method in multi-agent communication , such that the goal in the proposed method corresponds to the information obtained because of communication with another agent in multi-agent communication channel . Here , we want to show that by training agents to develop \u201c default behaviours \u201d as well as the knowledge of when to break those behaviours , using an information bottleneck can also help in other scenarios like multi-agent communication . Consider multiagent communication , where in order to solve a task , agents require communicating with another agents . Ideally , an agent would would like to communicate with other agent , only when its essential to communicate , i.e the agents would like to minimize the communication with another agents . Here we show that selectively deciding when to communicate with another agent can result in faster learning . We follow the same experimental setup as in the paper ( Mordatch and Abbeel , 2018 ) . Method Train Reward TestReward No Communication -0.919 -0.920 Communication -0.36 -0.472 Communication ( with KL cost ) -0.293 -0.38 ( Lower is better ) . More details about this experimental setup can be found in Section D ( Appendix ) . I. Mordatch and P. Abbeel . Emergence of grounded compositional language in multi-agent pop- ulations . We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places . We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer 's suggestions ."}, "1": {"review_id": "rJg8yhAqKm-1", "review_text": "This paper proposes the concept of decision state, which is the state where decision is made \u201cmore\u201d dependent to a particular goal. The authors propose a KL divergence regularization to learn the structure of the tasks, and then use this information to encourage the policy to visit the decision states. The method is tested on several different experiment setups. In general the paper is well-written and easy to follow. Learning a more general policy is not new (as also discussed in the paper), but using the learned structure to further guide the exploration of the policy is novel and interesting. I have a couple questions about the experimental part though, mostly about the baselines. 1. What is the reasoning behind the selection of the baselines, e.g. A2C as the baseline for the miniGrid experiments? 2. What are the performances of the methods in Table 2, in direct policy generalization? Or is there any reason not reporting them here? 3. What is the reasoning of picking \u201cCount-base baseline\u201d for Figure 4, rather than the method of curiosity-based exploration? 4. For the Mujoco tasks, there are couple ones outperforming PPO, e.g. TD3, SAC etc.. [1,2] The authors should include their results too. 5. As an ablation study, it would be interesting to see how the bonus reward of visiting decision states can help the exploration on the training tasks, compared to the policy learned from equation (1), and the policies learned without information of other tasks. 6. Lastly, the idea of decision states can also be used in other RL algorithms. It would be also interesting to see if this idea can further improve their performances. Other comments: 1. Equation (3) should be \\le. 2. Why would Equation (5) hold? 3. Right before section 2.2, incomplete sentence. Disclaimer: The reviewer is not familiar with multitask reinforcement learning, and the miniGrid environment in the paper. Other reviewers should have better judgement on the significance of the experimental results. [1] Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint arXiv:1801.01290 (2018). [2] Fujimoto, Scott, Herke van Hoof, and Dave Meger. \"Addressing Function Approximation Error in Actor-Critic Methods.\" arXiv preprint arXiv:1802.09477 (2018).", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their time and feedback . We hope to address concerns the reviewer has here . `` What is the reasoning behind the selection of the baselines , e.g.A2C as the baseline for the miniGrid experiments ? `` We build from A2C with using goal-conditioned policies and the KL regularization . Hence , The A2C with no kl-regularization is the immediate baseline to consider . Since for maze experiments , the env is of the nature of mini-grid POMDP environments with sparse rewards , as well as discrete action , a2c worked out of the box and hence was the most straightforward baseline for comparison . We would be happy to add other comparisons which reviewer has in mind . > > What are the performances of the methods in Table 2 , in direct policy generalization ? Or is there any reason not reporting them here ? The setup in direct policy generalization is different as to what we evaluate in Table 2 . Direct policy generalization refers to first training an agent with a goal bottleneck on one set of environments ( MultiRoomN2S6 ) , and then evaluate the trained agent ( without fine tuning on new set of environment ( ( MultiRoomN3S4 , MultiRoomN4S4 , and MultiRoomN5S4 ) ) . What we evaluate in Table 2 , is how can we transfer the knowledge in form of decision states . Basically the intuition is , what we would like from any unsupervised/supervised transferrable exploration technique is to build a policy that is somehow good for adapting to or solving new problems . Here we try to generalize to new mazes the knowledge acquired by the encoder in the form of the KL estimator . Basically , the intuition is that high KL = interesting state , even before the agent has discovered a single path to the goal . So if we can use egocentric observations and generalize effectively , we can predict which points have high KL before we have even learned to traverse the maze , and then we can use these high-KL regions as rewards without the need to have solved that particular maze in advance , using knowledge transferred from other mazes . To do this , we first train agents with a goal bottleneck on one set of environments ( MultiRoomN2S6 ) where they learn the sensory cues that correspond to decision states . Then , we use this knowledge to guide exploration on another set of environments ( MultiRoomN3S4 , MultiRoomN4S4 , and MultiRoomN5S4 ) . And hence in this new environments , we are training another policy from scratch , And using the KL from D_KL ( p ( z|s , g ) | N ( 0,1 ) ) as an exploration bonus to guide exploration ."}, "2": {"review_id": "rJg8yhAqKm-2", "review_text": "The paper proposes a method of regularising goal-conditioned policies with a mutual information term. While this is potentially useful, I found the motivation for the approach and the experimental results insufficient. On top of that the presentation could also use some improvements. I do not recommend acceptance at this time. The introduction is vague and involves undefined terms such as \"useful habits\". It is not clear what problems the authors have in mind and why exactly they propose their specific method. The presentation of the method itself is not self-contained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper. Some symbols are not defined, for example what is Z and why is it discrete? The experiments are rather weak, they are missing comparison to strong exploration baselines and goal-oriented baselines.", "rating": "3: Clear rejection", "reply_text": "Thanks for the feedback . We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below . We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have . `` While this is potentially useful , I found the motivation for the approach It is not clear what problems the authors have in mind and why exactly they propose their specific method . '' We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places . We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer 's suggestions . We focus on multi-goal environments and goal-conditioned policies . The problem statement is quite simple : we aim to propose an algorithm whereby we incentive agents to learn task structure by training policies that perform well under a variety of goals , while not overfitting to any individual goal . We achieve this by training agents that , in addition to maximizing reward , minimize the policy dependence on the individual goal , quantified by the conditional mutual information I ( A ; G | S ) . In order to minimize this quantity , we formulate it using ideas from variational information bottleneck . To make the paper self-explanatory , we added the mathematical description of the proposed method in the appendix ( Section A ) . `` The presentation of the method itself is not self-contained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper. `` We again acknowledge that the paper was missing certain parts which made the paper difficult to read . We have added another section in the appendix which gives a more mathematical description of the proposed approach . We realized because of the way we have explained things there could be some fundamental misunderstanding about the proposed method . Thus , we would like to clarify this misunderstanding , not only with the intent of convincing you of the idea behind the proposed method but also with the intent of making amends to the method description where necessary so that readers may not arrive at the same conclusions as you . We added the mathematical description of the proposed framework in the appendix ( Section A ) ."}}