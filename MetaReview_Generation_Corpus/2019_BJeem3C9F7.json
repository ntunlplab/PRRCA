{"year": "2019", "forum": "BJeem3C9F7", "title": "Pix2Scene: Learning Implicit 3D Representations from Images", "decision": "Reject", "meta_review": "This paper proposes an approach for learning to generate 3D views, using a surfel-based representation, trained entirely from 2D images.  After the discussion phase, reviewers rate the paper close to the acceptance threshold.\n\nAnonReviewer3, who initially stated \"My second concern is the results are all on synthetic data, and most shapes are very simple\", remains concerned after the rebuttal, stating \"all results are on synthetic, simple scenes. In particular, these synthetic scenes don't have lighting, material, and texture variations, making them considerably easier than any types of real images.\"\n\nThe AC agrees with the concerns raised by AnonReviewer3, and believes that more extensive experimentation, either on more complex synthetic scenes or on real images, is needed to back the claims of the paper.  Particularly relevant is the criticism that \"While the paper is called \u2018pix2scene\u2019, it\u2019s really about \u2018pix2object\u2019 or \u2018pix2shape\u2019.\"\n", "reviews": [{"review_id": "BJeem3C9F7-0", "review_text": "The paper deals with creating 3D representations or depth maps from 2D image data using adversarial training methods. The flow makes the paper readable. One main concern is that most of the experiments seem to have results as visual inspections of figures provided. It is really hard to judge the correctness or how well the algorithms do. It would be useful to provide references of equation 1 if used from previous text. In the experiments, it is usually not clear how many training images were used, how many test. How different were the objects used in the training data vs test? Were all the test objects novel? How useful were the GAN techniques? Which part of the GAN did the most work i.e. the usefulness and accuracy of the different parts of the net? Even in 4.2, though it mentions use of 6 object types for both training and testing, using the figures is hard to estimate how well the model does compared to a reference baseline. In 4.4.1, the discussion on how much improvement there is due to use of unlabeled images is missing? Do they even help? It is not quite clear from table 1. How many unlabeled images were used? How many iterations in total are used of the unlabeled ones (given there is 1 in 100 update of labeled ones). Missing reference: http://www.cs.cornell.edu/~asaxena/reconstruction3d/saxena_make3d_learning3dstructure.pdf ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear reviewer , thank you for your time and effort . TL ; DR : We added more quantitative results . Our paper already includes examples generalizing to viewpoints that weren \u2019 t part of the training data , but we included additional samples . And we added significantly more details about the methods . Here are our responses in more detail : == RE : Most evaluations are qualitative . There is no standard protocol to evaluate 3D reconstruction and generation . Most of the state-of-the-art methods ( fully unsupervised methods learned on single images ) just show qualitative results in their papers . We did quantitatively evaluate the surfel reconstruction against the ground truth via Hausdorff distance ( HD ) as described in Appendix B and the reconstructions of our model via mean squared error ( MSE ) on the depth map . We achieved near-zero MSE and reasonably lower HD ( when reconstructed from the same view ) . We included a table showing the difference in these values for different , unseen camera views : https : //ibb.co/nj73qL . On top of these metrics , we also created the 3D IQ test task ( 3D-IQTT ) which is exclusively quantitative . We compared our method with two CNN baselines and we now also included human evaluation . The CNN baselines demonstrate that the task can only be solved with an understanding of the 3D geometry . A preview of the updated comparison table can be found here : ( https : //ibb.co/nhHUS0 ) . == RE : Add reference for the rendering equation . Sorry for the oversight . We have added the reference for the rendering equation : it is an approximation of Kajiya \u2019 s rendering equation [ Kajiya 1986 ] . [ Kajiya 1986 ] \u201c The Rendering Equation \u201d == RE : More details on experimental setup . We have added more details on the experimental setup ( camera , lights , and material properties used ) in the appendix . All our images are of resolutions 128x128 . Except for the 3D-IQTT , we didn \u2019 t store a fixed dataset but rather created the dataset on the fly . For example in the existing Figure 4 , during the data generation process the rotation , translation , and object categories were randomized . The probability of seeing the same configuration from two different views is near zero . == RE : Which parts of the GAN are more important . Our Pix2Scene architecture is a bidirectional adversarial model . It consists of an encoder , decoder , renderer , and discriminator . The encoder translates the input image into a latent representation . The decoder transforms a similar latent vector , sampled from noise , into our surfel representation , which is converted into a 2D image by the renderer . The discriminator \u2019 s purpose is to make sure the output images become the same distribution as the input images , and ascertain that the encoded latent representation corresponds to the latent input to the decoder . See our existing Figure 3 for an overview . The decoder-rendering part is important for generating new viewpoints for a given latent code and the encoder-decoder pipeline allows us to infer the 3D structure of a 2D image . Without the encoder , the model would be purely generative . == RE : Novelty of the generated images . GAN-based models usually suffer from mode-collapse . We demonstrated in Figure 8 that our model overcame this issue and was able to interpolate between two given scenes . We \u2019 ve added another figure to further emphasize the interpolation capabilities of our model . == RE : 3D-IQTT semisupervised learning . Thanks for this feedback . We agree that this section wasn \u2019 t sufficiently clear . We \u2019 ve rewritten a part of this section and added the details on the interleaved training . It 's similar to algorithm 2 from [ Kingma et al.2014 ] , except instead of a randomized minibatch , we train a few iterations of unsupervised data followed by a few iterations of supervised data . We also extended Table 1 to include an entirely unsupervised case as well as human performance on the same task . A preview of the table can be found here : https : //ibb.co/nhHUS0 . In all cases , the model was trained with an unsupervised dataset of 100,000 lines of data , where each line contained the reference image , the 3 possible answers , but no information on which one was the correct answer . [ Kingma et al.2014 ] `` Semi-supervised Learning with Deep Generative Models '' , 2014 . == RE : Missing reference Make-3D . Sorry for the oversight . We will add the reference in our introduction ."}, {"review_id": "BJeem3C9F7-1", "review_text": "This paper explored explaining scenes with surfels in a neural recognition model. The authors demonstrated results on image reconstruction, synthesis, and mental shape rotation. The paper has many strengths. The model is clearly presented, the implementation is neat, the results on synthetic images are good. In particular, the results on the mental rotation task are interesting and new; I feel we should include more studies like these for scene and object representation learning. A few concerns remain. First, the motivation of the paper is unclear. The main advantage of the proposed representation, according to the intro, is its `implicitness\u2019, which enables viewpoint extrapolation. I\u2019d like to see more explanation on why \u2018explicit\u2019 representations don\u2019t support that. A lot of the intro is currently talking about related work, which can be moved to later sections or to the supp material. The paper then moves on to discuss surfels. While it\u2019s new combine surfels with deep nets, I\u2019m not sure how much benefits it brings over voxels, point clouds, or primitives. It\u2019d be good to compare with these scene representations. My second concern is the results are all on synthetic data, and most shapes are very simple. While the paper is called \u2018pix2scene\u2019, it\u2019s really about \u2018pix2object\u2019 or \u2018pix2shape\u2019. I\u2019d like to see results on more realistic scenes, where the number of objects as well as their shape and material varies. For the mental rotation task, the authors should cite and discuss the classic work from Shepard and Metzler and include human performance for calibration. I\u2019m on the border for this paper. Happy to adjust my rating based on the discussion and revision. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear AnonReviewer3 , Thank you so much for your review . We appreciate the time you put into this and your feedback . Before we respond in full to all of your points , we have a quick question : You mentioned , `` It \u2019 d be good to compare with these [ voxel/point clouds/primitives ] scene representations '' . We 'd love to implement this but we 're having some issues finding suitable code . Do you know of any code for methods that implicitly ( i.e.without supervised training ) learns object reconstruction using meshes/voxels/point clouds ? Thanks , the Pix2Scene team"}, {"review_id": "BJeem3C9F7-2", "review_text": "This paper introduces a method to create a 3D scene model given a 2D image and a camera pose. The method is: (1) an \"encoder\" network maps the image to some latent code vector, (2) a \"decoder\" network uses the code and the camera pose to create a depthmap, (3) surface normals are computed from the depthmap, and (4) these outputs are fed to a differentiable renderer which reconstructs the input image. At training time, a discriminator provides feedback to (and simultaneously trains on) the latent code and the reconstructions. The model is self-supervised by the reconstruction error and the GAN setup. Experiments show compelling results in 3D scene generation for simple monochromatic synthetic scenes composed of an empty room corner and floating ShapeNet shapes. This is a nice problem, and if the approach ever works in the real world, it will be useful. On synthetic environments, the results are impressive. The paper seems to claim more ground than it actually covers. The abstract says \"Our method learns the depth and orientation of scene points visible in images\", but really only the depth is learned, and the \"orientation\" is an automatically-computed surface normal, which is a free byproduct of any depth estimate. The \"surfel\" description includes a reflectance vector, but this is never estimated or further described in the paper, so my guess is that it is simply treated as a scalar (which equals 1). Taking this reflectance issue together with the orientation issue, the model is not really estimating surfels at all, but rather just a depthmap, which makes the method seem considerably less novel. Furthermore, the differentiable rendering (eq. 1) appears to assume that all light sources are known exactly -- this is not a trivial assumption, and yet it is never mentioned in the paper. The text suggests that only an image is required to run the model, but Figure 3 shows that the networks are conditioned on the camera pose -- exact knowledge of the camera pose is difficult to obtain precisely in real settings, so this again is not an assumption to ignore. To rewrite the paper more plainly, one might say that it receives a monochrome image as input, estimates a depthmap, and then shades this depthmap using perfect knowledge of lighting and camera pose, which reconstructs the input. This may sound less appealing, but it also seems more accurate. The paper is also missing some details of the method and evaluation, which I hope can be cleared up easily. - What is happening with the light source? This is critical in the shading equation (eq. 1), and yet no information is given on it -- we need the color and the position of every light in the scene. - How is the camera pose represented? Section 3.3.3 says conditional normalization is used, but what exactly is fed to the network that estimates these conditional normalization parameters? - What is the exact form of the reconstruction error? An equation would be great. - How is the class-conditioning done in 4.2? - In Eq. 4, the first usage of D_\\theta should use only the object part of the vectors, and the second usage should use only the geometric part, right? Maybe this can be cleared up with a second D_subscript. - I do not understand the \"interleaved\" training setup in 4.4.1. Please explain that more. - It is not clear to me why the task in 4.4.2 needs any supervised training at all, if the classification is just done by computing L2 distances in the latent space. What happens with \"0 sampled labels\"? Overall, I like the paper, and I can imagine others in my group liking it. I hope it gets in, assuming the technical details get cleaned up and the language gets softer.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks so much for your time . TL ; DR : We \u2019 re completely rewriting the intro to focus on our actual contribution and not the long-term plan . We \u2019 re also working towards removing all the assumptions like camera position and light position knowledge . And we \u2019 ve added a lot more details about the 3D-IQTT methods . == RE : Providing clarification on the claims . We agree that the extent of the contributions claimed in the introduction was unclear . We set out to present our long-term goal , to \u201c learn the 3D structure of the real world just from single images \u201d ; however , in this paper , we have just made the first steps towards the goal and that was not apparent from the original introduction . We have reworked our claims accordingly in our new paper version ( will be uploaded in the next few days ) to reflect that our method \u201c receives a monochrome image as input , estimates a depth map , and then shades this depth map using perfect knowledge of lighting and camera pose , which reconstructs the input \u201d - as you suggested . Our model makes several assumptions : ( a ) the camera pose is known , ( b ) the material properties are constant , ( c ) the light positions are known , and ( d ) the world is piece-wise smooth . In order to achieve our long-term goal , we have to eventually get rid of these . Therefore we have made some first steps to address each one : - ( Camera pose is known ) : In the 3D-IQTT experiments , we estimated the camera position in our latent representation while we kept the camera looking at the center of the object . We used the estimated camera parameters in the generator for the rendering process . - ( Material properties are constant ) : We used diffuse materials with uniform reflectance for all our experiments . The reflectance values were chosen arbitrarily but kept fixed for input-output pairs . In other words , we use fixed material which can be chromatic ( reflects different wavelengths by different amount ) or monochromatic ( reflects all wavelengths the same amount ) . This is not the same as using `` monochromatic image '' , it is just that material is constant and does n't need to be inferred . We 've added the details to the appendix . Learning the reflectance and color/texture properties ( in addition to the surface depth and orientation ) is significantly more challenging , but we are currently working towards that . - ( Lighting assumptions ) : In our work presented in the last version of the paper , we used multiple point light sources that were placed randomly on the surface of a spherical sector around the scene and colored randomly . For each pair of rendered input image and model-reconstructed output image , these light conditions were identical . We added more details about how we handled the lighting to the appendix . - ( The world is piece-wise smooth ) : This might not be perfectly accurate , but it \u2019 s a common assumption in 3D reconstruction . In an extreme case like when capturing cactus spikes , this might not work , but for example , when we were reconstructing a chair with a thin stretcher ( see our video , https : //bit.ly/2zADuqG ) , the reconstruction worked well . We agree that we are currently mainly recovering a depth map , but the surfel representation was picked with our long-term goal in mind , since gives us several advantages : ( a ) surfel representation allow us to represent only the visible surface of a complicated scene instead of explicitly representing the complete scene . Given an image we can infer its implicit 3D representation and then recreate novel surfel representations of the underlying scene from unobserved viewpoints . Moreover this representation fits well with current convolutional architectures ( b ) with our existing normal estimation and additional material estimation this allows for realistic shading ."}], "0": {"review_id": "BJeem3C9F7-0", "review_text": "The paper deals with creating 3D representations or depth maps from 2D image data using adversarial training methods. The flow makes the paper readable. One main concern is that most of the experiments seem to have results as visual inspections of figures provided. It is really hard to judge the correctness or how well the algorithms do. It would be useful to provide references of equation 1 if used from previous text. In the experiments, it is usually not clear how many training images were used, how many test. How different were the objects used in the training data vs test? Were all the test objects novel? How useful were the GAN techniques? Which part of the GAN did the most work i.e. the usefulness and accuracy of the different parts of the net? Even in 4.2, though it mentions use of 6 object types for both training and testing, using the figures is hard to estimate how well the model does compared to a reference baseline. In 4.4.1, the discussion on how much improvement there is due to use of unlabeled images is missing? Do they even help? It is not quite clear from table 1. How many unlabeled images were used? How many iterations in total are used of the unlabeled ones (given there is 1 in 100 update of labeled ones). Missing reference: http://www.cs.cornell.edu/~asaxena/reconstruction3d/saxena_make3d_learning3dstructure.pdf ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear reviewer , thank you for your time and effort . TL ; DR : We added more quantitative results . Our paper already includes examples generalizing to viewpoints that weren \u2019 t part of the training data , but we included additional samples . And we added significantly more details about the methods . Here are our responses in more detail : == RE : Most evaluations are qualitative . There is no standard protocol to evaluate 3D reconstruction and generation . Most of the state-of-the-art methods ( fully unsupervised methods learned on single images ) just show qualitative results in their papers . We did quantitatively evaluate the surfel reconstruction against the ground truth via Hausdorff distance ( HD ) as described in Appendix B and the reconstructions of our model via mean squared error ( MSE ) on the depth map . We achieved near-zero MSE and reasonably lower HD ( when reconstructed from the same view ) . We included a table showing the difference in these values for different , unseen camera views : https : //ibb.co/nj73qL . On top of these metrics , we also created the 3D IQ test task ( 3D-IQTT ) which is exclusively quantitative . We compared our method with two CNN baselines and we now also included human evaluation . The CNN baselines demonstrate that the task can only be solved with an understanding of the 3D geometry . A preview of the updated comparison table can be found here : ( https : //ibb.co/nhHUS0 ) . == RE : Add reference for the rendering equation . Sorry for the oversight . We have added the reference for the rendering equation : it is an approximation of Kajiya \u2019 s rendering equation [ Kajiya 1986 ] . [ Kajiya 1986 ] \u201c The Rendering Equation \u201d == RE : More details on experimental setup . We have added more details on the experimental setup ( camera , lights , and material properties used ) in the appendix . All our images are of resolutions 128x128 . Except for the 3D-IQTT , we didn \u2019 t store a fixed dataset but rather created the dataset on the fly . For example in the existing Figure 4 , during the data generation process the rotation , translation , and object categories were randomized . The probability of seeing the same configuration from two different views is near zero . == RE : Which parts of the GAN are more important . Our Pix2Scene architecture is a bidirectional adversarial model . It consists of an encoder , decoder , renderer , and discriminator . The encoder translates the input image into a latent representation . The decoder transforms a similar latent vector , sampled from noise , into our surfel representation , which is converted into a 2D image by the renderer . The discriminator \u2019 s purpose is to make sure the output images become the same distribution as the input images , and ascertain that the encoded latent representation corresponds to the latent input to the decoder . See our existing Figure 3 for an overview . The decoder-rendering part is important for generating new viewpoints for a given latent code and the encoder-decoder pipeline allows us to infer the 3D structure of a 2D image . Without the encoder , the model would be purely generative . == RE : Novelty of the generated images . GAN-based models usually suffer from mode-collapse . We demonstrated in Figure 8 that our model overcame this issue and was able to interpolate between two given scenes . We \u2019 ve added another figure to further emphasize the interpolation capabilities of our model . == RE : 3D-IQTT semisupervised learning . Thanks for this feedback . We agree that this section wasn \u2019 t sufficiently clear . We \u2019 ve rewritten a part of this section and added the details on the interleaved training . It 's similar to algorithm 2 from [ Kingma et al.2014 ] , except instead of a randomized minibatch , we train a few iterations of unsupervised data followed by a few iterations of supervised data . We also extended Table 1 to include an entirely unsupervised case as well as human performance on the same task . A preview of the table can be found here : https : //ibb.co/nhHUS0 . In all cases , the model was trained with an unsupervised dataset of 100,000 lines of data , where each line contained the reference image , the 3 possible answers , but no information on which one was the correct answer . [ Kingma et al.2014 ] `` Semi-supervised Learning with Deep Generative Models '' , 2014 . == RE : Missing reference Make-3D . Sorry for the oversight . We will add the reference in our introduction ."}, "1": {"review_id": "BJeem3C9F7-1", "review_text": "This paper explored explaining scenes with surfels in a neural recognition model. The authors demonstrated results on image reconstruction, synthesis, and mental shape rotation. The paper has many strengths. The model is clearly presented, the implementation is neat, the results on synthetic images are good. In particular, the results on the mental rotation task are interesting and new; I feel we should include more studies like these for scene and object representation learning. A few concerns remain. First, the motivation of the paper is unclear. The main advantage of the proposed representation, according to the intro, is its `implicitness\u2019, which enables viewpoint extrapolation. I\u2019d like to see more explanation on why \u2018explicit\u2019 representations don\u2019t support that. A lot of the intro is currently talking about related work, which can be moved to later sections or to the supp material. The paper then moves on to discuss surfels. While it\u2019s new combine surfels with deep nets, I\u2019m not sure how much benefits it brings over voxels, point clouds, or primitives. It\u2019d be good to compare with these scene representations. My second concern is the results are all on synthetic data, and most shapes are very simple. While the paper is called \u2018pix2scene\u2019, it\u2019s really about \u2018pix2object\u2019 or \u2018pix2shape\u2019. I\u2019d like to see results on more realistic scenes, where the number of objects as well as their shape and material varies. For the mental rotation task, the authors should cite and discuss the classic work from Shepard and Metzler and include human performance for calibration. I\u2019m on the border for this paper. Happy to adjust my rating based on the discussion and revision. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear AnonReviewer3 , Thank you so much for your review . We appreciate the time you put into this and your feedback . Before we respond in full to all of your points , we have a quick question : You mentioned , `` It \u2019 d be good to compare with these [ voxel/point clouds/primitives ] scene representations '' . We 'd love to implement this but we 're having some issues finding suitable code . Do you know of any code for methods that implicitly ( i.e.without supervised training ) learns object reconstruction using meshes/voxels/point clouds ? Thanks , the Pix2Scene team"}, "2": {"review_id": "BJeem3C9F7-2", "review_text": "This paper introduces a method to create a 3D scene model given a 2D image and a camera pose. The method is: (1) an \"encoder\" network maps the image to some latent code vector, (2) a \"decoder\" network uses the code and the camera pose to create a depthmap, (3) surface normals are computed from the depthmap, and (4) these outputs are fed to a differentiable renderer which reconstructs the input image. At training time, a discriminator provides feedback to (and simultaneously trains on) the latent code and the reconstructions. The model is self-supervised by the reconstruction error and the GAN setup. Experiments show compelling results in 3D scene generation for simple monochromatic synthetic scenes composed of an empty room corner and floating ShapeNet shapes. This is a nice problem, and if the approach ever works in the real world, it will be useful. On synthetic environments, the results are impressive. The paper seems to claim more ground than it actually covers. The abstract says \"Our method learns the depth and orientation of scene points visible in images\", but really only the depth is learned, and the \"orientation\" is an automatically-computed surface normal, which is a free byproduct of any depth estimate. The \"surfel\" description includes a reflectance vector, but this is never estimated or further described in the paper, so my guess is that it is simply treated as a scalar (which equals 1). Taking this reflectance issue together with the orientation issue, the model is not really estimating surfels at all, but rather just a depthmap, which makes the method seem considerably less novel. Furthermore, the differentiable rendering (eq. 1) appears to assume that all light sources are known exactly -- this is not a trivial assumption, and yet it is never mentioned in the paper. The text suggests that only an image is required to run the model, but Figure 3 shows that the networks are conditioned on the camera pose -- exact knowledge of the camera pose is difficult to obtain precisely in real settings, so this again is not an assumption to ignore. To rewrite the paper more plainly, one might say that it receives a monochrome image as input, estimates a depthmap, and then shades this depthmap using perfect knowledge of lighting and camera pose, which reconstructs the input. This may sound less appealing, but it also seems more accurate. The paper is also missing some details of the method and evaluation, which I hope can be cleared up easily. - What is happening with the light source? This is critical in the shading equation (eq. 1), and yet no information is given on it -- we need the color and the position of every light in the scene. - How is the camera pose represented? Section 3.3.3 says conditional normalization is used, but what exactly is fed to the network that estimates these conditional normalization parameters? - What is the exact form of the reconstruction error? An equation would be great. - How is the class-conditioning done in 4.2? - In Eq. 4, the first usage of D_\\theta should use only the object part of the vectors, and the second usage should use only the geometric part, right? Maybe this can be cleared up with a second D_subscript. - I do not understand the \"interleaved\" training setup in 4.4.1. Please explain that more. - It is not clear to me why the task in 4.4.2 needs any supervised training at all, if the classification is just done by computing L2 distances in the latent space. What happens with \"0 sampled labels\"? Overall, I like the paper, and I can imagine others in my group liking it. I hope it gets in, assuming the technical details get cleaned up and the language gets softer.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks so much for your time . TL ; DR : We \u2019 re completely rewriting the intro to focus on our actual contribution and not the long-term plan . We \u2019 re also working towards removing all the assumptions like camera position and light position knowledge . And we \u2019 ve added a lot more details about the 3D-IQTT methods . == RE : Providing clarification on the claims . We agree that the extent of the contributions claimed in the introduction was unclear . We set out to present our long-term goal , to \u201c learn the 3D structure of the real world just from single images \u201d ; however , in this paper , we have just made the first steps towards the goal and that was not apparent from the original introduction . We have reworked our claims accordingly in our new paper version ( will be uploaded in the next few days ) to reflect that our method \u201c receives a monochrome image as input , estimates a depth map , and then shades this depth map using perfect knowledge of lighting and camera pose , which reconstructs the input \u201d - as you suggested . Our model makes several assumptions : ( a ) the camera pose is known , ( b ) the material properties are constant , ( c ) the light positions are known , and ( d ) the world is piece-wise smooth . In order to achieve our long-term goal , we have to eventually get rid of these . Therefore we have made some first steps to address each one : - ( Camera pose is known ) : In the 3D-IQTT experiments , we estimated the camera position in our latent representation while we kept the camera looking at the center of the object . We used the estimated camera parameters in the generator for the rendering process . - ( Material properties are constant ) : We used diffuse materials with uniform reflectance for all our experiments . The reflectance values were chosen arbitrarily but kept fixed for input-output pairs . In other words , we use fixed material which can be chromatic ( reflects different wavelengths by different amount ) or monochromatic ( reflects all wavelengths the same amount ) . This is not the same as using `` monochromatic image '' , it is just that material is constant and does n't need to be inferred . We 've added the details to the appendix . Learning the reflectance and color/texture properties ( in addition to the surface depth and orientation ) is significantly more challenging , but we are currently working towards that . - ( Lighting assumptions ) : In our work presented in the last version of the paper , we used multiple point light sources that were placed randomly on the surface of a spherical sector around the scene and colored randomly . For each pair of rendered input image and model-reconstructed output image , these light conditions were identical . We added more details about how we handled the lighting to the appendix . - ( The world is piece-wise smooth ) : This might not be perfectly accurate , but it \u2019 s a common assumption in 3D reconstruction . In an extreme case like when capturing cactus spikes , this might not work , but for example , when we were reconstructing a chair with a thin stretcher ( see our video , https : //bit.ly/2zADuqG ) , the reconstruction worked well . We agree that we are currently mainly recovering a depth map , but the surfel representation was picked with our long-term goal in mind , since gives us several advantages : ( a ) surfel representation allow us to represent only the visible surface of a complicated scene instead of explicitly representing the complete scene . Given an image we can infer its implicit 3D representation and then recreate novel surfel representations of the underlying scene from unobserved viewpoints . Moreover this representation fits well with current convolutional architectures ( b ) with our existing normal estimation and additional material estimation this allows for realistic shading ."}}