{"year": "2019", "forum": "rJl-b3RcF7", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "decision": "Accept (Oral)", "meta_review": "The authors posit and investigate a hypothesis -- the \u201clottery ticket hypothesis\u201d -- which aims to explain why overparameterized neural networks are easier to train than their sparse counterparts. Under this hypothesis, randomly initialized dense networks are easier to train because they contain a larger number of \u201cwinning tickets\u201d.\nThis paper received very favorable reviews, though there were some notable points of concern. The reviewers and the AC appreciated the detailed and careful experimentation and analysis. However, there were a couple of points of concern raised by the reviewers: 1) the lack of experiments conducted on large-scale tasks and models, and 2) the lack of a clear application of the idea beyond what has been proposed previously. \n\nOverall, this is a very interesting paper with convincing experimental validation and as such the AC is happy to accept the work.", "reviews": [{"review_id": "rJl-b3RcF7-0", "review_text": "It was believed that sparse architectures generated by pruning are difficult to train from scratch. The authors show that there exist sparse subnetworks that can be trained from scratch with good generalization performance. To explain the difficulty of training pruned networks from scratch or why training needs the overparameterized networks that make pruning necessary, the authors propose a lottery ticket hypothesis: unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy. They also present an algorithm to identify the winning tickets. The conjecture is interesting and it is still a open question for whether a pruned network can reach the same accuracy when trained from scratch. It may helps to explain why bigger networks are easier to train due to \u201chaving more possible subnetworks from which training can recover a winning ticket\u201d. It also shows the importance of both the pruned architecture and the initialization value. Actually another submission (https://openreview.net/forum?id=rJlnB3C5Ym) made the opposite conclusions. The limitations of this paper are several folds: - The paper seems a bit preliminary and unfinished. A lot of notations seems confusing, such as \u201cwhen pruned to 21%\u201d. The author defines a winning lottery ticket as a sparse subnetwork that can reaching the same performance of the original network when trained from scratch with the \u201coriginal initialization\u201d. It is quite confusing as there is no definition anywhere about the \u201coriginal initialization\u201d. It would be clearer if the author can use some math notations. - As identified by the authors themself, lacking of supporting experiments on large-scale dataset and real-world models. Only MNIST/CIFAR-10 and toy networks like LeNet, Conv2/Conv4/Conv6 are used. The author has done experiments on resnet, I would be better to move it to the main paper. - There is no explanation about why the \u201clottery ticket\u201d can perform well when trained with the \u201coriginal initialization\u201d but not with random initialization. Is it because the original initialization is not far from the pruned solution? Then this is a kind of overting to the obtained solution. - The other problem is that the implications are not clearly useful without showing any applications. The paper could be stronger if the authors can provide more results to support the applications of this conjecture. - The authors only explore the sparse networks. Model compression by sparsification has good compression rate, especially for networks with large FC layers. However, the acceleration relies on specific hardware/libraries. It would be more complete if the author can provide experiments on structurally pruned networks, especially for CNNs. - The x-axis of pruning ratios in Figure 1/4/5 could be uniformly sampled and make the figure easier to read. Questions: - Does the winning tickets always exist? - What is the size of winning tickets for a very thin network? Would it also be less than 10%? ------update---------- I appreciate the author\u2019s efforts on providing detailed response and more experiments. After reading the rebuttal and the revised version, though the paper has been improved, my concerns are not fully addressed to safely accept it. It can be summarized that there exists a sparse network that can be trained well only provided with certain weight initialization.The winning tickets can only be found via iterative pruning of the trained network. This is a chicken-egg problem and I failed to see how it can improve the network design. It still feels incomplete to me by just providing a hypothesis with limited sets of experiments. The implications are actually the most valuable/attractive part, such as \u201cImprove our theoretical understanding of neural networks\u201d, however, they are very vague with no clear instructions even after accepting this hypothesis. I would expect analysis of the reason behind failure and success. I understand that it could be left for another paper, but the observations/experiments only are not strong enough for confirming the the hypothesis. Specifically, the experiments are conducted on relatively wide and shallow CNNs. Note that VGG-16/19 and ResNet-18 are designed for ImageNet but not CIFAR-10, which are much wider than normal CIFAR-10 networks, such as ResNet-56. Even \u201cresnet18 has 16x fewer parameters than conv2 and 75x fewer than VGG19\u201d, it is mainly due to the removal of FC layers with average pooling and cannot be claimed as \u201cmuch thinner\u201d networks. As increasing the wideness usually ease the optimization, and the pruned sparse network still enjoy this property unless significantly pruned. Thus, I still doubt whether the conclusion can hold for much thinner network, i.e., \u201cwinning tickets near or below 10-20%, depending on the level of overparameterization of the original network.\u201d The observation of \u201cwinning ticket weights tend to change by a larger amount then weights in the rest of the network\u201d in Figure 19 seems natural and the conjecture of the reason \u201cmagnitude-pruning biases the winning tickets we find toward those containing weights that change in the direction of higher magnitude\u201d sounds reasonable. It would be great if the authors can dig into this and make more comparison with the distribution of random weights initialization. The figures could also be improved and simplified as the lines are hard to read and compare. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "> The authors only explore the sparse networks . Model compression by sparsification has good compression rate , especially for networks with large FC layers . However , the acceleration relies on specific hardware/libraries . It would be more complete if the author can provide experiments on structurally pruned networks , especially for CNNs . This is a great observation . We agree that structured pruning techniques produce pruned networks that are more amenable to existing software/hardware acceleration techniques . In the limitations section of the updated version ( Section 7 ) , we have explicitly noted structured pruning as an opportunity to connect our empirical observations of winning tickets to concrete practice . -- - > The x-axis of pruning ratios in Figure 1/4/5 could be uniformly sampled and make the figure easier to read . Done - thank you for the suggestion ! -- - > Does the winning tickets always exist ? Our experiments indicate that winning tickets do seem to exist for the variety of network architectures considered in this paper ( and as explicitly scoped by our stated limitations in Section 7 - we acknowledge that we only consider a limited subset of neural network tasks in this paper ) . However , in the most literal sense , no : winning tickets do not always exist for all datasets and networks . Take , as an example , a minimal dense network for two-way XOR which has two hidden units . If the parameters of the network are initialized to values that give the correct outputs from the very start , then removing any one parameter makes it impossible to reach the same accuracy as the unpruned network . -- - > What is the size of winning tickets for a very thin network ? Would it also be less than 10 % ? In the updated version of the paper ( Section 5 ) , we have studied several networks that are much thinner than those described in the original version of the paper : VGG16 , VGG19 , and resnet18 . For VGG16 and VGG19 , we continue to find winning tickets that are at or less than 10 % of the original size of the network . For resnet18 ( which has 16x fewer parameters than conv2 and 75x fewer than VGG19 ) , we find winning tickets that are about 15 % of the size of the original network . Our results suggest that , for several exemplary thin networks , we still find winning tickets near or below 10-20 % , depending on the level of overparameterization of the original network ."}, {"review_id": "rJl-b3RcF7-1", "review_text": "==== Summary ==== It is widely known that large neural networks can typically be compressed into smaller networks that perform as well as the original network while directly training small networks can be complicated. This paper proposes a conjecture to explain this phenomenon that the authors call \u201cThe Lottery Ticket Hypothesis\u201d: large networks that can be trained successfully contain at initialization time small sub-networks \u2014 which are defined by both connectivity and the initial weights that the authors call \u201cwinning tickets\u201d \u2014 that if trained separately for similar number of iterations could reach the same performance as the large network. The paper follows by proposing a method to find these winning tickets by pruning methods, which are typically used for compressing networks, and then proceed to test this hypothesis on several architectures and tasks. The paper also conjectures that the reason large networks are more straightforward to train is that when randomly initialized large networks have more combinations for subnetworks which makes have a winning ticket more likely. ==== Detailed Review ==== I have found the hypothesis that the paper puts forth to be very appealing, as it articulates the essence of many ideas that have been floating around for quite a while. For example, the notion that having a large network makes it more probable for some of the initialized weights to be in the \u201cright\u201d direction for the beginning of the training, as mentioned in [1] that was cited in this submission. Given our lack of understanding of the optimization and generalization properties of neural networks, as well as how these two interact, then any insight into this process, like this paper suggests, could have a significant impact on both theory and practice. To that effect, I generally found the experiments in support of the hypothesis to be pretty convincing, or at the very least that there is some truth to it. Most importantly, the hypothesis and experiments presented in this paper gave me a new perspective on both the generalization and optimization problem, which as a theoretician gave me new ideas on how to approach analyzing them rigorously \u2014 and that is why I strongly vote for the acceptance of this paper. Though I have very much enjoyed reading this submission, which for the most part is very well written, it does have some issues: 1. Though this is an empirical paper about an observed phenomenon, it should contain a bit more background and discussion on the theoretical implications of its subject. For example, see [2] which is also an empirical work about a theoretical hypothesis, but still includes the right theoretical context that helps the reader judge the meaning of their results. The same should be done here. For instance, there is a growing interest in the link between compression and generalization that is relevant to this work [3,4], and the effect of winning ticket leading to better generalization could be explained via other works which link structure to inductive bias [5,6]. 2. The lottery ticket hypothesis is described in the paper as being both about optimization (faster \u201cconvergence\u201d) and about generalization (better \u201cgeneralization accuracy\u201d). However, there is a slight issue with how these terms are treated in the paper. First, \u201cconvergence\u201d is defined as the point at which the test accuracy reaches to a minimum and before it begins to rise again, but it does not mean (and most likely not) that it is the point at which the optimization algorithm converged to its minimum \u2014 it is better to write that early stopping regularization was used in this case. Second, the convergence point is chosen according to the test set which is bad methodology, because the test set cannot be used for choosing the final model (only the training and validation sets). Third, the training accuracies are not reported in the paper, and without them, it is difficult to judge if a given model fails to generalize is simply fails to converge to 100% accuracy on the training set. As a minor note, \u201cgeneralization accuracy\u201d as a term is not that common and might be a bit confusing, so it is better to write \u201ctest accuracy\u201d. To conclude, even though I urge the authors to address the above issues, which could significantly improve its quality and clarity, I think that this article thought-provoking and highly deserving of being accepted to ICLR. [1] Bengio et al. Convex neural networks. NIPS 2006. [2] Zhang et al. Understanding deep learning requires rethinking generalization. ICLR 2017. [3] Arora et al. Stronger generalization bounds for deep nets via a compression approach. ICML 2018. [4] Zhou et al. Compressibility and Generalization in Large-Scale Deep Learning. Arxiv preprint 2018. [5] Cohen et al. Inductive Bias of Deep Convolutional Networks through Pooling Geometry. ICLR 2017. [6] Levine et al. Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design. ICLR 2018. ==== Updated Review Following Rebuttal ==== The authors have addressed all of the concerns that I have mentioned above, and so I have updated my score accordingly. The additional background on related works, as well as the additional experiments in response to the other reviews will help readers appreciate the observations that are raised by the authors. The new revision is a very strong submission, and I highly recommend accepting it to ICLR. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you so much for your thoughtful review . Below , you will find our responses to your questions and comments . We have modified the paper to reflect your feedback , and we are very interested in any further feedback you have about the new version of the paper . We have summarized the changes in the new version of the paper in a top-level comment called `` Summary of Changes in the New Version . '' -- - > 1.Though this is an empirical paper about an observed phenomenon , it should contain a bit more background and discussion on the theoretical implications of its subject . For example , see [ 2 ] which is also an empirical work about a theoretical hypothesis , but still includes the right theoretical context that helps the reader judge the meaning of their results . The same should be done here . For instance , there is a growing interest in the link between compression and generalization that is relevant to this work [ 3,4 ] , and the effect of winning ticket leading to better generalization could be explained via other works which link structure to inductive bias [ 5,6 ] . We have rewritten our discussion section ( Section 6 ) to connect with contemporary understanding of inductive bias , generalization ( and its relation to compressibility ) , and optimization of overparameterized networks . We hope that this section provides appropriate context for interpreting these results , however we are open to additional suggestions . -- - > 2.The lottery ticket hypothesis is described in the paper as being both about optimization ( faster \u201c convergence \u201d ) and about generalization ( better \u201c generalization accuracy \u201d ) . However , there is a slight issue with how these terms are treated in the paper . First , \u201c convergence \u201d is defined as the point at which the test accuracy reaches to a minimum and before it begins to rise again , but it does not mean ( and most likely not ) that it is the point at which the optimization algorithm converged to its minimum \u2014 it is better to write that early stopping regularization was used in this case . Thank you for this very helpful suggestion . We have updated our language throughout the paper to ensure that we are using this terminology properly . -- - > Second , the convergence point is chosen according to the test set which is bad methodology , because the test set can not be used for choosing the final model ( only the training and validation sets ) . We have updated all of our experiments in the main body of the paper to report the iteration of early-stopping based on validation loss and to report the accuracy at that iteration based on test loss . The conclusions from our results remain the same . -- - > Third , the training accuracies are not reported in the paper , and without them , it is difficult to judge if a given model fails to generalize is simply fails to converge to 100 % accuracy on the training set . We have updated the paper to include graphs of the training accuracies at early-stopping time for lenet and conv2/4/6 . In general , training accuracy at early-stopping time rises with test accuracy . However , at the end of the training process , training accuracy generally reaches 100 % for all but the most heavily pruned networks ( see the new Appendix B ) ; this is true for both winning tickets and randomly reinitialized networks ( although winning tickets generally still reach 100 % training accuracy when pruned slightly further ( e.g. , 3.6 % vs. 1.9 % for MNIST ) ) . Even so , the accuracy patterns witnessed at early-stopping time remain in place at the end of training : winning tickets see test accuracy improvements and reach higher test accuracy than when randomly reinitialized , indicating that winning tickets indeed generalize better . -- - > As a minor note , \u201c generalization accuracy \u201d as a term is not that common and might be a bit confusing , so it is better to write \u201c test accuracy \u201d . We have updated our language to reflect this suggestion ."}, {"review_id": "rJl-b3RcF7-2", "review_text": "(Score raised from 8 to 9 after rebuttal) The paper examines the hypothesis that randomly initialized (feed-forward) neural networks contain sub-networks that train well in the sense that they converge equally fast or faster and reach the same or better classification accuracy. Interestingly, such sub-networks can be identified by simple, magnitude-based pruning. It is crucial that these sub-networks are initialized with their original initialization values, otherwise they typically fail to be trained, implying that it is not purely the structure of the sub-networks that matters. The paper thoroughly investigates the existence of such \u201cwinning-tickets\u201d on MNIST and CIFAR-10 on both, fully connected but also convolutional neural networks. Winning-tickets are found across networks, various optimizers, at different pruning-levels and across various other hyper-parameters. The experiments also show that iterative pruning (with re-starts) is more effective at finding winning-tickets. The paper adds a novel and interesting angle to the question of why neural networks apparently need to be heavily over-parameterized for training. This question is intriguing and of high importance to further the understanding of how neural networks train. Additionally, the findings might have practical relevance as they might help avoid unnecessary over-parameterization which, in turn, might save use of computational resources and energy. The main idea is simple (which is good) and can be tested with relatively simple experiments (also good). The experiments conducted in the paper are clean (averaging over multiple runs, controlling for a lot of factors) and should allow for easy reproduction but also for clean comparison against future experiments. The experimental section is well executed, the writing is clear and good and related work is taken into account to a sufficient degree. The paper touches upon a very intriguing \u201cfeature\u201d of neural networks and, in my opinion, should be relevant to theorists and practitioners across many sub-fields of deep learning research. I therefore vote and argue for accepting the paper for presentation at the conference. The following comments are suggestions to the authors on how to further improve the paper. I do not expect all issues to be addressed in the camera-ready version. 1) The main \u201cweakness\u201d of the paper might be that, while the amount of experiments and controls is impressive, the generality of the lottery ticket hypothesis remains somewhat open. Even when restricting the statement to feed-forward networks only, the networks investigated in the paper are relatively \u201csmall\u201d and MNIST and CIFAR-10 bear the risk of finding patterns that do not hold when scaling to larger-scale networks and tasks. I acknowledge and support the author\u2019s decision to have thorough and clean experiments on these small models and tasks, rather than having half-baked results on ImageNet, etc. The downside of this is that the experiments are thus not sufficient to claim (with reasonable certainty) that the lottery ticket hypothesis holds \u201cin general\u201d. The paper would be stronger, if the existence of winning tickets on larger-scale experiments or tasks other than classification were shown - even if these experiments did not have a large number of control experiments/ablation studies. 2) While the paper shows the existence of winning tickets robustly and convincingly on the networks/tasks investigated, the next important question would be how to systematically and reliably \u201cbreak\u201d the existence of lottery tickets. Can they be attributed to a few fundamental factors? Are they a consequence of batch-wise, gradient-based optimization, or an inherent feature of neural networks, or is it the loss functions commonly used, \u2026? On page 2, second paragraph, the paper states: \u201dWhen randomly reinitialized, our winning tickets no longer match the performance of the original network, explaining the difficulty of training pruned networks from scratch\u201d. I don\u2019t fully agree - the paper certainly sheds some light on the issue, but an actual explanation would result in a testable hypothesis. My comment here is intended to be constructive criticism, I think that the paper has enough \u201cjuice\u201d and novelty for being accepted - I am merely pointing out that the overall story is not yet conclusive (and I am aware that it might need several more publications to find these answers). 3) Do the winning tickets generalize across hyper-parameters or even tasks. I.e. if a winning ticket is found with one set of hyper-parameters, but then Optimizer/learning-rate/etc. are changed, does the winning-ticket still lead to improved convergence and accuracy? Same question for data-sets: do winning-tickets found on CIFAR-100 also work for CIFAR-10 and vice versa? If winning-tickets turn out to generalize well, in the extreme this could allow \u201cshipping\u201d each network architecture with a few good winning-tickets, thus making it unnecessary to apply expensive iterative pruning every time. I would not expect generalization across data-sets, but it would be highly interesting to see if winning tickets generalize in any way (after all I am still surprised by how well adversarial examples generalize and transfer). 4) Some things that would be interesting to try: 4a) Is there anything special about the pruned/non-pruned weights at the time of initialization? Did they start out with very small values already or are they all \u201cbehind\u201d some (dead) downstream neuron? Is there anything that might essentially block gradient signal from updating the pruned neurons? This could perhaps be checked by recording weights\u2019 \u201ctrajectories\u201d during training to see if there is a correlation between the \u201cdistance weights traveled\u201d and whether or not they end up in the winning ticket. 4b) Do ARD-style/Bayesian approaches or second-order methods to pruning identify (roughly) the same neurons for pruning? 5) Typo (should be through): \u201cwe find winning tickets though a principled search process\u201d 6) For the standard ConvNets I assume you did not use batchnorm. Does batchnorm interfere in any way with the existence of winning tickets? (at least on ResNet they seem to exist with batchnorm as well) ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "> 4.Some things that would be interesting to try : 4a ) Is there anything special about the pruned/non-pruned weights at the time of initialization ? Did they start out with very small values already or are they all \u201c behind \u201d some ( dead ) downstream neuron ? Is there anything that might essentially block gradient signal from updating the pruned neurons ? This could perhaps be checked by recording weights \u2019 \u201c trajectories \u201d during training to see if there is a correlation between the \u201c distance weights traveled \u201d and whether or not they end up in the winning ticket . In the new Appendix D , we study the pruned and non-pruned weights at the time of initialization . We find that winning ticket initializations tend to come from the extremes of the truncated normal distribution from which the unpruned networks are initialized . We are interested in studying the other questions you mention in future work . We also look at the distance weights travel in the unpruned network , finding that weights that are part of the eventual winning tickets tend to move more than weights that are not part of the winning ticket . -- - > 4b ) Do ARD-style/Bayesian approaches or second-order methods to pruning identify ( roughly ) the same neurons for pruning ? These are great questions that we are interested in understanding as well . In order to keep our experiments as simple and tractable as possible , we opted to focus on a single , simple , widely-accepted pruning method . However , we have updated our limitations section ( Section 7 ) to reflect that we only use a single identification technique and that other techniques may produce winning tickets with different properties ( e.g. , fewer weights , improved training times , better generalization , or better performance on hardware ) . -- - > 5.Typo ( should be through ) : \u201c we find winning tickets though a principled search process \u201d Nice catch - it should now be corrected ! -- - > For the standard ConvNets I assume you did not use batchnorm . Does batchnorm interfere in any way with the existence of winning tickets ? ( at least on ResNet they seem to exist with batchnorm as well ) The new networks ( resnet18 and vgg16/19 ) all use batchnorm . You 're correct that lenet and conv2/4/6 do not use batchnorm . As you note , since we still find winning tickets on these larger networks , it does not appear that batchnorm interferes with the existence of winning tickets ."}], "0": {"review_id": "rJl-b3RcF7-0", "review_text": "It was believed that sparse architectures generated by pruning are difficult to train from scratch. The authors show that there exist sparse subnetworks that can be trained from scratch with good generalization performance. To explain the difficulty of training pruned networks from scratch or why training needs the overparameterized networks that make pruning necessary, the authors propose a lottery ticket hypothesis: unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy. They also present an algorithm to identify the winning tickets. The conjecture is interesting and it is still a open question for whether a pruned network can reach the same accuracy when trained from scratch. It may helps to explain why bigger networks are easier to train due to \u201chaving more possible subnetworks from which training can recover a winning ticket\u201d. It also shows the importance of both the pruned architecture and the initialization value. Actually another submission (https://openreview.net/forum?id=rJlnB3C5Ym) made the opposite conclusions. The limitations of this paper are several folds: - The paper seems a bit preliminary and unfinished. A lot of notations seems confusing, such as \u201cwhen pruned to 21%\u201d. The author defines a winning lottery ticket as a sparse subnetwork that can reaching the same performance of the original network when trained from scratch with the \u201coriginal initialization\u201d. It is quite confusing as there is no definition anywhere about the \u201coriginal initialization\u201d. It would be clearer if the author can use some math notations. - As identified by the authors themself, lacking of supporting experiments on large-scale dataset and real-world models. Only MNIST/CIFAR-10 and toy networks like LeNet, Conv2/Conv4/Conv6 are used. The author has done experiments on resnet, I would be better to move it to the main paper. - There is no explanation about why the \u201clottery ticket\u201d can perform well when trained with the \u201coriginal initialization\u201d but not with random initialization. Is it because the original initialization is not far from the pruned solution? Then this is a kind of overting to the obtained solution. - The other problem is that the implications are not clearly useful without showing any applications. The paper could be stronger if the authors can provide more results to support the applications of this conjecture. - The authors only explore the sparse networks. Model compression by sparsification has good compression rate, especially for networks with large FC layers. However, the acceleration relies on specific hardware/libraries. It would be more complete if the author can provide experiments on structurally pruned networks, especially for CNNs. - The x-axis of pruning ratios in Figure 1/4/5 could be uniformly sampled and make the figure easier to read. Questions: - Does the winning tickets always exist? - What is the size of winning tickets for a very thin network? Would it also be less than 10%? ------update---------- I appreciate the author\u2019s efforts on providing detailed response and more experiments. After reading the rebuttal and the revised version, though the paper has been improved, my concerns are not fully addressed to safely accept it. It can be summarized that there exists a sparse network that can be trained well only provided with certain weight initialization.The winning tickets can only be found via iterative pruning of the trained network. This is a chicken-egg problem and I failed to see how it can improve the network design. It still feels incomplete to me by just providing a hypothesis with limited sets of experiments. The implications are actually the most valuable/attractive part, such as \u201cImprove our theoretical understanding of neural networks\u201d, however, they are very vague with no clear instructions even after accepting this hypothesis. I would expect analysis of the reason behind failure and success. I understand that it could be left for another paper, but the observations/experiments only are not strong enough for confirming the the hypothesis. Specifically, the experiments are conducted on relatively wide and shallow CNNs. Note that VGG-16/19 and ResNet-18 are designed for ImageNet but not CIFAR-10, which are much wider than normal CIFAR-10 networks, such as ResNet-56. Even \u201cresnet18 has 16x fewer parameters than conv2 and 75x fewer than VGG19\u201d, it is mainly due to the removal of FC layers with average pooling and cannot be claimed as \u201cmuch thinner\u201d networks. As increasing the wideness usually ease the optimization, and the pruned sparse network still enjoy this property unless significantly pruned. Thus, I still doubt whether the conclusion can hold for much thinner network, i.e., \u201cwinning tickets near or below 10-20%, depending on the level of overparameterization of the original network.\u201d The observation of \u201cwinning ticket weights tend to change by a larger amount then weights in the rest of the network\u201d in Figure 19 seems natural and the conjecture of the reason \u201cmagnitude-pruning biases the winning tickets we find toward those containing weights that change in the direction of higher magnitude\u201d sounds reasonable. It would be great if the authors can dig into this and make more comparison with the distribution of random weights initialization. The figures could also be improved and simplified as the lines are hard to read and compare. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "> The authors only explore the sparse networks . Model compression by sparsification has good compression rate , especially for networks with large FC layers . However , the acceleration relies on specific hardware/libraries . It would be more complete if the author can provide experiments on structurally pruned networks , especially for CNNs . This is a great observation . We agree that structured pruning techniques produce pruned networks that are more amenable to existing software/hardware acceleration techniques . In the limitations section of the updated version ( Section 7 ) , we have explicitly noted structured pruning as an opportunity to connect our empirical observations of winning tickets to concrete practice . -- - > The x-axis of pruning ratios in Figure 1/4/5 could be uniformly sampled and make the figure easier to read . Done - thank you for the suggestion ! -- - > Does the winning tickets always exist ? Our experiments indicate that winning tickets do seem to exist for the variety of network architectures considered in this paper ( and as explicitly scoped by our stated limitations in Section 7 - we acknowledge that we only consider a limited subset of neural network tasks in this paper ) . However , in the most literal sense , no : winning tickets do not always exist for all datasets and networks . Take , as an example , a minimal dense network for two-way XOR which has two hidden units . If the parameters of the network are initialized to values that give the correct outputs from the very start , then removing any one parameter makes it impossible to reach the same accuracy as the unpruned network . -- - > What is the size of winning tickets for a very thin network ? Would it also be less than 10 % ? In the updated version of the paper ( Section 5 ) , we have studied several networks that are much thinner than those described in the original version of the paper : VGG16 , VGG19 , and resnet18 . For VGG16 and VGG19 , we continue to find winning tickets that are at or less than 10 % of the original size of the network . For resnet18 ( which has 16x fewer parameters than conv2 and 75x fewer than VGG19 ) , we find winning tickets that are about 15 % of the size of the original network . Our results suggest that , for several exemplary thin networks , we still find winning tickets near or below 10-20 % , depending on the level of overparameterization of the original network ."}, "1": {"review_id": "rJl-b3RcF7-1", "review_text": "==== Summary ==== It is widely known that large neural networks can typically be compressed into smaller networks that perform as well as the original network while directly training small networks can be complicated. This paper proposes a conjecture to explain this phenomenon that the authors call \u201cThe Lottery Ticket Hypothesis\u201d: large networks that can be trained successfully contain at initialization time small sub-networks \u2014 which are defined by both connectivity and the initial weights that the authors call \u201cwinning tickets\u201d \u2014 that if trained separately for similar number of iterations could reach the same performance as the large network. The paper follows by proposing a method to find these winning tickets by pruning methods, which are typically used for compressing networks, and then proceed to test this hypothesis on several architectures and tasks. The paper also conjectures that the reason large networks are more straightforward to train is that when randomly initialized large networks have more combinations for subnetworks which makes have a winning ticket more likely. ==== Detailed Review ==== I have found the hypothesis that the paper puts forth to be very appealing, as it articulates the essence of many ideas that have been floating around for quite a while. For example, the notion that having a large network makes it more probable for some of the initialized weights to be in the \u201cright\u201d direction for the beginning of the training, as mentioned in [1] that was cited in this submission. Given our lack of understanding of the optimization and generalization properties of neural networks, as well as how these two interact, then any insight into this process, like this paper suggests, could have a significant impact on both theory and practice. To that effect, I generally found the experiments in support of the hypothesis to be pretty convincing, or at the very least that there is some truth to it. Most importantly, the hypothesis and experiments presented in this paper gave me a new perspective on both the generalization and optimization problem, which as a theoretician gave me new ideas on how to approach analyzing them rigorously \u2014 and that is why I strongly vote for the acceptance of this paper. Though I have very much enjoyed reading this submission, which for the most part is very well written, it does have some issues: 1. Though this is an empirical paper about an observed phenomenon, it should contain a bit more background and discussion on the theoretical implications of its subject. For example, see [2] which is also an empirical work about a theoretical hypothesis, but still includes the right theoretical context that helps the reader judge the meaning of their results. The same should be done here. For instance, there is a growing interest in the link between compression and generalization that is relevant to this work [3,4], and the effect of winning ticket leading to better generalization could be explained via other works which link structure to inductive bias [5,6]. 2. The lottery ticket hypothesis is described in the paper as being both about optimization (faster \u201cconvergence\u201d) and about generalization (better \u201cgeneralization accuracy\u201d). However, there is a slight issue with how these terms are treated in the paper. First, \u201cconvergence\u201d is defined as the point at which the test accuracy reaches to a minimum and before it begins to rise again, but it does not mean (and most likely not) that it is the point at which the optimization algorithm converged to its minimum \u2014 it is better to write that early stopping regularization was used in this case. Second, the convergence point is chosen according to the test set which is bad methodology, because the test set cannot be used for choosing the final model (only the training and validation sets). Third, the training accuracies are not reported in the paper, and without them, it is difficult to judge if a given model fails to generalize is simply fails to converge to 100% accuracy on the training set. As a minor note, \u201cgeneralization accuracy\u201d as a term is not that common and might be a bit confusing, so it is better to write \u201ctest accuracy\u201d. To conclude, even though I urge the authors to address the above issues, which could significantly improve its quality and clarity, I think that this article thought-provoking and highly deserving of being accepted to ICLR. [1] Bengio et al. Convex neural networks. NIPS 2006. [2] Zhang et al. Understanding deep learning requires rethinking generalization. ICLR 2017. [3] Arora et al. Stronger generalization bounds for deep nets via a compression approach. ICML 2018. [4] Zhou et al. Compressibility and Generalization in Large-Scale Deep Learning. Arxiv preprint 2018. [5] Cohen et al. Inductive Bias of Deep Convolutional Networks through Pooling Geometry. ICLR 2017. [6] Levine et al. Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design. ICLR 2018. ==== Updated Review Following Rebuttal ==== The authors have addressed all of the concerns that I have mentioned above, and so I have updated my score accordingly. The additional background on related works, as well as the additional experiments in response to the other reviews will help readers appreciate the observations that are raised by the authors. The new revision is a very strong submission, and I highly recommend accepting it to ICLR. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you so much for your thoughtful review . Below , you will find our responses to your questions and comments . We have modified the paper to reflect your feedback , and we are very interested in any further feedback you have about the new version of the paper . We have summarized the changes in the new version of the paper in a top-level comment called `` Summary of Changes in the New Version . '' -- - > 1.Though this is an empirical paper about an observed phenomenon , it should contain a bit more background and discussion on the theoretical implications of its subject . For example , see [ 2 ] which is also an empirical work about a theoretical hypothesis , but still includes the right theoretical context that helps the reader judge the meaning of their results . The same should be done here . For instance , there is a growing interest in the link between compression and generalization that is relevant to this work [ 3,4 ] , and the effect of winning ticket leading to better generalization could be explained via other works which link structure to inductive bias [ 5,6 ] . We have rewritten our discussion section ( Section 6 ) to connect with contemporary understanding of inductive bias , generalization ( and its relation to compressibility ) , and optimization of overparameterized networks . We hope that this section provides appropriate context for interpreting these results , however we are open to additional suggestions . -- - > 2.The lottery ticket hypothesis is described in the paper as being both about optimization ( faster \u201c convergence \u201d ) and about generalization ( better \u201c generalization accuracy \u201d ) . However , there is a slight issue with how these terms are treated in the paper . First , \u201c convergence \u201d is defined as the point at which the test accuracy reaches to a minimum and before it begins to rise again , but it does not mean ( and most likely not ) that it is the point at which the optimization algorithm converged to its minimum \u2014 it is better to write that early stopping regularization was used in this case . Thank you for this very helpful suggestion . We have updated our language throughout the paper to ensure that we are using this terminology properly . -- - > Second , the convergence point is chosen according to the test set which is bad methodology , because the test set can not be used for choosing the final model ( only the training and validation sets ) . We have updated all of our experiments in the main body of the paper to report the iteration of early-stopping based on validation loss and to report the accuracy at that iteration based on test loss . The conclusions from our results remain the same . -- - > Third , the training accuracies are not reported in the paper , and without them , it is difficult to judge if a given model fails to generalize is simply fails to converge to 100 % accuracy on the training set . We have updated the paper to include graphs of the training accuracies at early-stopping time for lenet and conv2/4/6 . In general , training accuracy at early-stopping time rises with test accuracy . However , at the end of the training process , training accuracy generally reaches 100 % for all but the most heavily pruned networks ( see the new Appendix B ) ; this is true for both winning tickets and randomly reinitialized networks ( although winning tickets generally still reach 100 % training accuracy when pruned slightly further ( e.g. , 3.6 % vs. 1.9 % for MNIST ) ) . Even so , the accuracy patterns witnessed at early-stopping time remain in place at the end of training : winning tickets see test accuracy improvements and reach higher test accuracy than when randomly reinitialized , indicating that winning tickets indeed generalize better . -- - > As a minor note , \u201c generalization accuracy \u201d as a term is not that common and might be a bit confusing , so it is better to write \u201c test accuracy \u201d . We have updated our language to reflect this suggestion ."}, "2": {"review_id": "rJl-b3RcF7-2", "review_text": "(Score raised from 8 to 9 after rebuttal) The paper examines the hypothesis that randomly initialized (feed-forward) neural networks contain sub-networks that train well in the sense that they converge equally fast or faster and reach the same or better classification accuracy. Interestingly, such sub-networks can be identified by simple, magnitude-based pruning. It is crucial that these sub-networks are initialized with their original initialization values, otherwise they typically fail to be trained, implying that it is not purely the structure of the sub-networks that matters. The paper thoroughly investigates the existence of such \u201cwinning-tickets\u201d on MNIST and CIFAR-10 on both, fully connected but also convolutional neural networks. Winning-tickets are found across networks, various optimizers, at different pruning-levels and across various other hyper-parameters. The experiments also show that iterative pruning (with re-starts) is more effective at finding winning-tickets. The paper adds a novel and interesting angle to the question of why neural networks apparently need to be heavily over-parameterized for training. This question is intriguing and of high importance to further the understanding of how neural networks train. Additionally, the findings might have practical relevance as they might help avoid unnecessary over-parameterization which, in turn, might save use of computational resources and energy. The main idea is simple (which is good) and can be tested with relatively simple experiments (also good). The experiments conducted in the paper are clean (averaging over multiple runs, controlling for a lot of factors) and should allow for easy reproduction but also for clean comparison against future experiments. The experimental section is well executed, the writing is clear and good and related work is taken into account to a sufficient degree. The paper touches upon a very intriguing \u201cfeature\u201d of neural networks and, in my opinion, should be relevant to theorists and practitioners across many sub-fields of deep learning research. I therefore vote and argue for accepting the paper for presentation at the conference. The following comments are suggestions to the authors on how to further improve the paper. I do not expect all issues to be addressed in the camera-ready version. 1) The main \u201cweakness\u201d of the paper might be that, while the amount of experiments and controls is impressive, the generality of the lottery ticket hypothesis remains somewhat open. Even when restricting the statement to feed-forward networks only, the networks investigated in the paper are relatively \u201csmall\u201d and MNIST and CIFAR-10 bear the risk of finding patterns that do not hold when scaling to larger-scale networks and tasks. I acknowledge and support the author\u2019s decision to have thorough and clean experiments on these small models and tasks, rather than having half-baked results on ImageNet, etc. The downside of this is that the experiments are thus not sufficient to claim (with reasonable certainty) that the lottery ticket hypothesis holds \u201cin general\u201d. The paper would be stronger, if the existence of winning tickets on larger-scale experiments or tasks other than classification were shown - even if these experiments did not have a large number of control experiments/ablation studies. 2) While the paper shows the existence of winning tickets robustly and convincingly on the networks/tasks investigated, the next important question would be how to systematically and reliably \u201cbreak\u201d the existence of lottery tickets. Can they be attributed to a few fundamental factors? Are they a consequence of batch-wise, gradient-based optimization, or an inherent feature of neural networks, or is it the loss functions commonly used, \u2026? On page 2, second paragraph, the paper states: \u201dWhen randomly reinitialized, our winning tickets no longer match the performance of the original network, explaining the difficulty of training pruned networks from scratch\u201d. I don\u2019t fully agree - the paper certainly sheds some light on the issue, but an actual explanation would result in a testable hypothesis. My comment here is intended to be constructive criticism, I think that the paper has enough \u201cjuice\u201d and novelty for being accepted - I am merely pointing out that the overall story is not yet conclusive (and I am aware that it might need several more publications to find these answers). 3) Do the winning tickets generalize across hyper-parameters or even tasks. I.e. if a winning ticket is found with one set of hyper-parameters, but then Optimizer/learning-rate/etc. are changed, does the winning-ticket still lead to improved convergence and accuracy? Same question for data-sets: do winning-tickets found on CIFAR-100 also work for CIFAR-10 and vice versa? If winning-tickets turn out to generalize well, in the extreme this could allow \u201cshipping\u201d each network architecture with a few good winning-tickets, thus making it unnecessary to apply expensive iterative pruning every time. I would not expect generalization across data-sets, but it would be highly interesting to see if winning tickets generalize in any way (after all I am still surprised by how well adversarial examples generalize and transfer). 4) Some things that would be interesting to try: 4a) Is there anything special about the pruned/non-pruned weights at the time of initialization? Did they start out with very small values already or are they all \u201cbehind\u201d some (dead) downstream neuron? Is there anything that might essentially block gradient signal from updating the pruned neurons? This could perhaps be checked by recording weights\u2019 \u201ctrajectories\u201d during training to see if there is a correlation between the \u201cdistance weights traveled\u201d and whether or not they end up in the winning ticket. 4b) Do ARD-style/Bayesian approaches or second-order methods to pruning identify (roughly) the same neurons for pruning? 5) Typo (should be through): \u201cwe find winning tickets though a principled search process\u201d 6) For the standard ConvNets I assume you did not use batchnorm. Does batchnorm interfere in any way with the existence of winning tickets? (at least on ResNet they seem to exist with batchnorm as well) ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "> 4.Some things that would be interesting to try : 4a ) Is there anything special about the pruned/non-pruned weights at the time of initialization ? Did they start out with very small values already or are they all \u201c behind \u201d some ( dead ) downstream neuron ? Is there anything that might essentially block gradient signal from updating the pruned neurons ? This could perhaps be checked by recording weights \u2019 \u201c trajectories \u201d during training to see if there is a correlation between the \u201c distance weights traveled \u201d and whether or not they end up in the winning ticket . In the new Appendix D , we study the pruned and non-pruned weights at the time of initialization . We find that winning ticket initializations tend to come from the extremes of the truncated normal distribution from which the unpruned networks are initialized . We are interested in studying the other questions you mention in future work . We also look at the distance weights travel in the unpruned network , finding that weights that are part of the eventual winning tickets tend to move more than weights that are not part of the winning ticket . -- - > 4b ) Do ARD-style/Bayesian approaches or second-order methods to pruning identify ( roughly ) the same neurons for pruning ? These are great questions that we are interested in understanding as well . In order to keep our experiments as simple and tractable as possible , we opted to focus on a single , simple , widely-accepted pruning method . However , we have updated our limitations section ( Section 7 ) to reflect that we only use a single identification technique and that other techniques may produce winning tickets with different properties ( e.g. , fewer weights , improved training times , better generalization , or better performance on hardware ) . -- - > 5.Typo ( should be through ) : \u201c we find winning tickets though a principled search process \u201d Nice catch - it should now be corrected ! -- - > For the standard ConvNets I assume you did not use batchnorm . Does batchnorm interfere in any way with the existence of winning tickets ? ( at least on ResNet they seem to exist with batchnorm as well ) The new networks ( resnet18 and vgg16/19 ) all use batchnorm . You 're correct that lenet and conv2/4/6 do not use batchnorm . As you note , since we still find winning tickets on these larger networks , it does not appear that batchnorm interferes with the existence of winning tickets ."}}