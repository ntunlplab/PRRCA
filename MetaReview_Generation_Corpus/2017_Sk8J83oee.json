{"year": "2017", "forum": "Sk8J83oee", "title": "Generative Adversarial Parallelization", "decision": "Reject", "meta_review": "This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (problems with the use of GAM metric, lack of convincing results) and unanimously recommend rejection. I do not see a reason to overturn their recommendation.", "reviews": [{"review_id": "Sk8J83oee-0", "review_text": "This paper proposes an extension of the GAN framework known as GAP whereby multiple generators and discriminators are trained in parallel. The generator/discriminator pairing is shuffled according to a periodic schedule. Pros: + The proposed approach is simple and easy to replicate. Cons: - The paper is confusing to read. - The results are suggestive but do not conclusively show a performance win for GAP. The main argument of the paper is that GAP leads to improved convergence and improved coverage of modes. The coverage visualizations are suggestive but there still is not enough evidence to conclude that GAP is in fact improving coverage. And for convergence it is difficult to assess the effect of GAP on the basis of learning curves. The proposed GAM-II metric is circular in that model performance depends on the collection of baselines the model is being compared with. Estimating likelihood via AIS seems to be a promising way to evaluate, as does using the Inception score. Perhaps a more systematic way to determine GAP's effect would be to set up a grid search of hyperparameters and train an equal number of GANs and GAP-GANs for each setting. Then a histogram over final Inception scores or likelihood estimates of the trained models would help to show whether GAP tended to produce better models. Overall the approach seems promising but there are too many open questions regarding the paper in its current form. * Section 2: \"Remark that when...\" => seems like a to-do. * Section A.1: The proposed metric is not described in adequate detail.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your thoughtful review . We have updated the paper and the results have improved ( in terms of performance and consistency ) since the initial submission . The key improvements are the following : - We have updated the GAM-II results in Table 2 and 3 . - We evaluated GAP ( DCGAN ) based on maximum likelihood using AIS ( Wu et al 2016 ) . - We also have looked at the distribution over class prediction based on samples generated by GAP and we included the inception score . - Lastly , we have improved the clarity of the paper . Regarding to the coverage of visualization , we agree with your comment and had acknowledged that the results are only suggestive . Grid search over the hyper-parameter is a good suggestion . We will definitely try it !"}, {"review_id": "Sk8J83oee-1", "review_text": "This paper proposes to address the mode collapsing problem of GANs by training a large set of generators and discriminators, pairing them each up with different ones at different times throughout training. The idea here is that no one generator-discriminator pair can be too locked together since they are all being swapped. This idea is nice and is addressing an important issue with GAN training. However, I think the paper is lacking in experimental results. In particular: - The authors need to do more work to motivate the GAM metric. It is not intuitively obvious to me that the GAM metric is a good way of evaluating the generator networks since it relies on the prediction of the discriminator networks which can fixate on artifacts. Perhaps the authors could explore if the GAM metric correlates with inception scores or human evaluations. Currently the only quantitative evaluation uses this criterion and it really isn't clear it's a relevant quantity to be measuring. - Related to the above comment, the authors need to compare more to other methods. Why not evaluate inception scores and compare with previous methods. Similarly, generation quality is not compared with previous methods. It's not obvious that the sample quality is any better with this method. And now just repeating questions from pre-review section: - If, instead of swapping, you were to simply train K GANs on K splits of the data, or K GANs with differing initial conditions (but without swapping) do you see any improvement in results? Similarly, how about if you train larger capacity models with dropout in G and D? Since dropout essentially averages many models it would be interesting to see if the effects are the same. - In figure 6 it appears that the validation costs remain the same as parallelization increase, but the training cost goes up and that is why the gap is shrinking. Does this really imply better generalization? In summary, interesting paper that addresses an important issue with GAN training, but compelling results are missing. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your thoughtful review . We have summarized the key improvements with respect to results and clarity in our response to Reviewer 3 . To avoid duplicating text , we kindly refer you to that summary . Both Reviewer 2 and 3 expressed some concern with the GAM-II metric \u2019 s reliance on discriminators , which can fixate on artifacts . This is an excellent point and we agree that GAM-II is flawed in this respect . But we believe that it is an improvement over GAM ( Im et al.2016 ) which also relies on discriminators . Despite its flaws , GAM-II has some other benefits compared to evaluation schemes like AIS and Inception scores : it can be performed online , it does not need any extra trained models nor data , etc . That being said , the focus of our paper was n't on the GAM-II metric , nor do we think that there is any single best metric for quantitatively evaluating generative models . We applied GAM-II as one of the evaluations in order to understand the performance of GAP . In response to the reviewer \u2019 s suggestions , we also measured negative log-likelihood of DCGANs based on the technique proposed by ( Wu et al.2016 ) -- see Table 4 . We also reported the Inception score as suggested by all of the reviewers -- see Table 5 ."}, {"review_id": "Sk8J83oee-2", "review_text": "This paper proposes Generative Adversarial Parallelization (GAP), one schedule to train N Generative Adversarial Networks (GANs) in parallel. GAP proceeds by shuffling the assignments between the N generators and the N discriminators at play every few epochs. Therefore, GAP forces each generator to compete with multiple discriminators at random. The authors claim that such randomization reduces undesired \"mode collapsing behaviour\", typical of GANs. I have three concerns with this submission. 1) After training the N GANs for a sufficient amount of time, the authors propose to choose the best generator using the GAM metric. I oppose to this because of two reasons. First, a single GAN will most likely be unable to express the full richness of the true data begin modeled. Said differently, a single generator with limited power will either describe a mode well, or describe many modes poorly. Second, GAM relies on the scores given by the discriminators, which can be ill-posed (focus on artifacts). Since there is There is nothing wrong with mode collapsing when this happens under control. Thus, I believe that a better strategy would be to not choose and combine all generators into a mixture. Of course, this would require a way to decide on mixture weights. This can be done, for instance, using rejection sampling based on discriminator scores. 2) The authors should provide a theoretical (or at least conceptual) comparison to dropout. In essence, this paper has a very similar flavour: every generator is competing against all N discriminators, but at each epoch we drop N-1 for every generator. Related to the previous point, after training dropout keeps all the neurons, effectively approximating a large ensemble of neural networks. 3) The qualitative results are not convincing. Most of the figures show only results about GAP. How do the baseline samples look like? The GAN and LAPGAN papers show very similar samples. On the other hand, I do not find Figures 3 and 4 convincing: for instance, the generator in Figure 3 was most likely under-parametrized. As a minor comment, I would remove Figure 2. This is because of three reasons: it may be protected by copyright, it occupies a lot of space, and it does not add much value to the explanation. Also, the indices (i_t) are undefined in Algorithm 1. Overall, this paper shows good ideas, but it needs further work in terms of conceptual development and experimental evaluation.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your review and suggestions on how to improve our paper . Please see the summary of improvements with respect to results and clarity in our response to Reviewer 3 . We now respond to your specific concerns . Concern 1 : Choosing the single best generator using the [ GAM-II ] metric . In this work we viewed GAP as a training strategy and attempted to compare , \u201c apples-to-apples \u201d synthesis among single generators ( not ensembles ) . However , we agree with the reviewer that ensembling all GANs ( e.g.as a mixture ) has a richer modeling capacity and will likely improve quality of generated samples . While we have not explored such a strategy in this work , we are interested in this extension and intend to pursue it . Moreover , we recently found a paper that has a similar flavour to what the author suggested : AdaGAN : Boosting Generative Models ( arXiv:1701.02386 ) . It may be interesting to explore the use of GAP for training AdaGAN models . Please also see our response to Reviewer 2 for a discussion on the weakness of the GAM-II metric . Concern 2 : We should provide a theoretical ( or at least a conceptual ) comparison to dropout . Thank you for pointing out the connection between GAP and dropout . We have added a conceptual comparison to Dropout in Section 3.1 . Concern 3a : The qualitative results are not convincing . Most of the [ figures of generated samples ] show only GAP results . We agree that other papers often use this scheme to show samples of one GAN variant vs. another . In our experience , visually distinguishing between the samples is difficult since our improvement mainly lies on mode coverage and stability . We \u2019 re not arguing that the visual quality of individual samples are better than the baseline training schemes . Concern 3b : Figures 3 and 4 are not convincing ; the generator in Figure 3 was most likely under-parametrized . The architecture of the generator and discriminator for both individually-trained and GAP-trained models are exactly the same . Not only that , the hyper-parameters were also exactly the same . The only additional parameter tuned for GAP was swapping rate . Figure 3 and 4 are merely visualizations , and we noted that one should not draw strong conclusions from them , but we do feel that it is a fair comparison . The GAP-trained models appear to utilize the fixed number of parameters more effectively ."}], "0": {"review_id": "Sk8J83oee-0", "review_text": "This paper proposes an extension of the GAN framework known as GAP whereby multiple generators and discriminators are trained in parallel. The generator/discriminator pairing is shuffled according to a periodic schedule. Pros: + The proposed approach is simple and easy to replicate. Cons: - The paper is confusing to read. - The results are suggestive but do not conclusively show a performance win for GAP. The main argument of the paper is that GAP leads to improved convergence and improved coverage of modes. The coverage visualizations are suggestive but there still is not enough evidence to conclude that GAP is in fact improving coverage. And for convergence it is difficult to assess the effect of GAP on the basis of learning curves. The proposed GAM-II metric is circular in that model performance depends on the collection of baselines the model is being compared with. Estimating likelihood via AIS seems to be a promising way to evaluate, as does using the Inception score. Perhaps a more systematic way to determine GAP's effect would be to set up a grid search of hyperparameters and train an equal number of GANs and GAP-GANs for each setting. Then a histogram over final Inception scores or likelihood estimates of the trained models would help to show whether GAP tended to produce better models. Overall the approach seems promising but there are too many open questions regarding the paper in its current form. * Section 2: \"Remark that when...\" => seems like a to-do. * Section A.1: The proposed metric is not described in adequate detail.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your thoughtful review . We have updated the paper and the results have improved ( in terms of performance and consistency ) since the initial submission . The key improvements are the following : - We have updated the GAM-II results in Table 2 and 3 . - We evaluated GAP ( DCGAN ) based on maximum likelihood using AIS ( Wu et al 2016 ) . - We also have looked at the distribution over class prediction based on samples generated by GAP and we included the inception score . - Lastly , we have improved the clarity of the paper . Regarding to the coverage of visualization , we agree with your comment and had acknowledged that the results are only suggestive . Grid search over the hyper-parameter is a good suggestion . We will definitely try it !"}, "1": {"review_id": "Sk8J83oee-1", "review_text": "This paper proposes to address the mode collapsing problem of GANs by training a large set of generators and discriminators, pairing them each up with different ones at different times throughout training. The idea here is that no one generator-discriminator pair can be too locked together since they are all being swapped. This idea is nice and is addressing an important issue with GAN training. However, I think the paper is lacking in experimental results. In particular: - The authors need to do more work to motivate the GAM metric. It is not intuitively obvious to me that the GAM metric is a good way of evaluating the generator networks since it relies on the prediction of the discriminator networks which can fixate on artifacts. Perhaps the authors could explore if the GAM metric correlates with inception scores or human evaluations. Currently the only quantitative evaluation uses this criterion and it really isn't clear it's a relevant quantity to be measuring. - Related to the above comment, the authors need to compare more to other methods. Why not evaluate inception scores and compare with previous methods. Similarly, generation quality is not compared with previous methods. It's not obvious that the sample quality is any better with this method. And now just repeating questions from pre-review section: - If, instead of swapping, you were to simply train K GANs on K splits of the data, or K GANs with differing initial conditions (but without swapping) do you see any improvement in results? Similarly, how about if you train larger capacity models with dropout in G and D? Since dropout essentially averages many models it would be interesting to see if the effects are the same. - In figure 6 it appears that the validation costs remain the same as parallelization increase, but the training cost goes up and that is why the gap is shrinking. Does this really imply better generalization? In summary, interesting paper that addresses an important issue with GAN training, but compelling results are missing. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your thoughtful review . We have summarized the key improvements with respect to results and clarity in our response to Reviewer 3 . To avoid duplicating text , we kindly refer you to that summary . Both Reviewer 2 and 3 expressed some concern with the GAM-II metric \u2019 s reliance on discriminators , which can fixate on artifacts . This is an excellent point and we agree that GAM-II is flawed in this respect . But we believe that it is an improvement over GAM ( Im et al.2016 ) which also relies on discriminators . Despite its flaws , GAM-II has some other benefits compared to evaluation schemes like AIS and Inception scores : it can be performed online , it does not need any extra trained models nor data , etc . That being said , the focus of our paper was n't on the GAM-II metric , nor do we think that there is any single best metric for quantitatively evaluating generative models . We applied GAM-II as one of the evaluations in order to understand the performance of GAP . In response to the reviewer \u2019 s suggestions , we also measured negative log-likelihood of DCGANs based on the technique proposed by ( Wu et al.2016 ) -- see Table 4 . We also reported the Inception score as suggested by all of the reviewers -- see Table 5 ."}, "2": {"review_id": "Sk8J83oee-2", "review_text": "This paper proposes Generative Adversarial Parallelization (GAP), one schedule to train N Generative Adversarial Networks (GANs) in parallel. GAP proceeds by shuffling the assignments between the N generators and the N discriminators at play every few epochs. Therefore, GAP forces each generator to compete with multiple discriminators at random. The authors claim that such randomization reduces undesired \"mode collapsing behaviour\", typical of GANs. I have three concerns with this submission. 1) After training the N GANs for a sufficient amount of time, the authors propose to choose the best generator using the GAM metric. I oppose to this because of two reasons. First, a single GAN will most likely be unable to express the full richness of the true data begin modeled. Said differently, a single generator with limited power will either describe a mode well, or describe many modes poorly. Second, GAM relies on the scores given by the discriminators, which can be ill-posed (focus on artifacts). Since there is There is nothing wrong with mode collapsing when this happens under control. Thus, I believe that a better strategy would be to not choose and combine all generators into a mixture. Of course, this would require a way to decide on mixture weights. This can be done, for instance, using rejection sampling based on discriminator scores. 2) The authors should provide a theoretical (or at least conceptual) comparison to dropout. In essence, this paper has a very similar flavour: every generator is competing against all N discriminators, but at each epoch we drop N-1 for every generator. Related to the previous point, after training dropout keeps all the neurons, effectively approximating a large ensemble of neural networks. 3) The qualitative results are not convincing. Most of the figures show only results about GAP. How do the baseline samples look like? The GAN and LAPGAN papers show very similar samples. On the other hand, I do not find Figures 3 and 4 convincing: for instance, the generator in Figure 3 was most likely under-parametrized. As a minor comment, I would remove Figure 2. This is because of three reasons: it may be protected by copyright, it occupies a lot of space, and it does not add much value to the explanation. Also, the indices (i_t) are undefined in Algorithm 1. Overall, this paper shows good ideas, but it needs further work in terms of conceptual development and experimental evaluation.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your review and suggestions on how to improve our paper . Please see the summary of improvements with respect to results and clarity in our response to Reviewer 3 . We now respond to your specific concerns . Concern 1 : Choosing the single best generator using the [ GAM-II ] metric . In this work we viewed GAP as a training strategy and attempted to compare , \u201c apples-to-apples \u201d synthesis among single generators ( not ensembles ) . However , we agree with the reviewer that ensembling all GANs ( e.g.as a mixture ) has a richer modeling capacity and will likely improve quality of generated samples . While we have not explored such a strategy in this work , we are interested in this extension and intend to pursue it . Moreover , we recently found a paper that has a similar flavour to what the author suggested : AdaGAN : Boosting Generative Models ( arXiv:1701.02386 ) . It may be interesting to explore the use of GAP for training AdaGAN models . Please also see our response to Reviewer 2 for a discussion on the weakness of the GAM-II metric . Concern 2 : We should provide a theoretical ( or at least a conceptual ) comparison to dropout . Thank you for pointing out the connection between GAP and dropout . We have added a conceptual comparison to Dropout in Section 3.1 . Concern 3a : The qualitative results are not convincing . Most of the [ figures of generated samples ] show only GAP results . We agree that other papers often use this scheme to show samples of one GAN variant vs. another . In our experience , visually distinguishing between the samples is difficult since our improvement mainly lies on mode coverage and stability . We \u2019 re not arguing that the visual quality of individual samples are better than the baseline training schemes . Concern 3b : Figures 3 and 4 are not convincing ; the generator in Figure 3 was most likely under-parametrized . The architecture of the generator and discriminator for both individually-trained and GAP-trained models are exactly the same . Not only that , the hyper-parameters were also exactly the same . The only additional parameter tuned for GAP was swapping rate . Figure 3 and 4 are merely visualizations , and we noted that one should not draw strong conclusions from them , but we do feel that it is a fair comparison . The GAP-trained models appear to utilize the fixed number of parameters more effectively ."}}