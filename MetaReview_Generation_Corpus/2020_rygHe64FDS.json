{"year": "2020", "forum": "rygHe64FDS", "title": "Zeno++: Robust Fully Asynchronous SGD", "decision": "Reject", "meta_review": "Main content:\n\nBlind review #2 summarizes it well:\n\nThis paper investigates the security of distributed asynchronous SGD. Authors propose Zeno++, worker-server asynchronous implementation of SGD which is robust to Byzantine failures. To ensure that the gradients sent by the workers are correct, Zeno++ server scores each worker gradients using a \u201creference\u201d gradient computed on a \u201csecret\u201d validation set.  If the score is under a given threshold, then the worker gradient is discarded. \n\nAuthors provide convergence guarantee for the Zeno++ optimizer for non-convex function. In addition, they provide an empirical evaluation of Zeno++ on the CIFAR10 datasets and compare with various baselines.\n\n--\n\nDiscussion:\n\nReviews are generally weak on the limited novelty of the approach compared with Zeno, but the rebuttal of the authors on Nov 15 is fair (too long to summarize here).\n\n--\n\nRecommendation and justification:\n\nI do not feel strongly enough to override the weak reviews (but if there is room in the program I would support a weak accept).", "reviews": [{"review_id": "rygHe64FDS-0", "review_text": "This paper addresses security of distributed optimization algorithm under Byzantine failures. These failures usually prevent convergence of training neural network models. Focusing on the asynchronous SGD algorithm implemented with a parameter-server, the authors propose to use stochastic line search ideas to detect whether the gradients are good descent directions or not. It applies to a general scenario including repeated and unbounded Byzantine failures. Theoretical results of this paper seem to me to have some flaw. In the proof of Theorem 1, line 6, it is not clear to me why the gradient norm ||g||^2 is replaced by || grad f_s (x_tau) ||^2. Clearly the g comes from any worker which can be very different to grad f_s (x_tau). Therefore, I recommend the authors check the proof of both theorem 1 and 2 more carefully. Numerical results show that the proposed algorithm Zeno++ works well with sign-flipping attacks. However, this scenario is very limited to validate all imaginable Byzantine failures that this paper would like to address. For example, one can use random gradients instead of sign-flipped gradients as a Byzantine attack, would the algorithm still work? ", "rating": "3: Weak Reject", "reply_text": "1.Question : \u201c In the proof of Theorem 1 , line 6 , it is not clear to me why the gradient norm ||g||^2 is replaced by || grad f_s ( x_tau ) ||^2. \u201d Answer : This is because of ( re- ) normalization . In Zeno++ , Line 4-6 of Algorithm 2 ( Server ) , the server receives $ \\tilde { g } $ from worker , and then normalize it , which results in $ g $ , where $ \\|g\\|^2 = \\| \\nabla f_s ( x_\\tau ) \\|^2 $ . It is true that $ g $ is very different from $ \\nabla f_s ( x_\\tau ) $ , but our algorithm makes them to have the same 2-norm by normalization . The purpose/intuition of doing so is that the attackers can also rescale the candidate gradient $ \\tilde { g } $ to have very large 2-norm , which will be very harmful to the convergence . By normalization , $ g $ will have similar 2-norm as ordinary gradients , so that even if a Byzantine $ g $ passes the test of Zeno+ or Zeno++ , the harm will be limited . We have added some experiments according to the reviewer 's requirement , including experiments with random attack . We have also double-checked the proof , and we think the current version is correct . We hope that our answers and clarifications resolve the reviewers ' concern ."}, {"review_id": "rygHe64FDS-1", "review_text": "Summary: This paper investigates the security of distributed asynchronous SGD. Authors propose Zeno++, worker-server asynchronous implementation of SGD which is robust to Byzantine failures. To ensure that the gradients sent by the workers are correct, Zeno++ server scores each worker gradients using a \u201creference\u201d gradient computed on a \u201csecret\u201d validation set. If the score is under a given threshold, then the worker gradient is discarded. Authors provide convergence guarantee for the Zeno++ optimizer for non-convex function. In addition, they provide an empirical evaluation of Zeno++ on the CIFAR10 datasets and compare with various baselines. Originality: I would argue that the paper novelty is limited. Paper builds upon the Zeno algorithm. From the paper, the main changes with respect to Zeno algorithm is the use of a hard-threshold instead of a ranking to adapt Zeno to the asynchronous case, and the use of a first-order Tayler approximation of the score to speed up its computations. Clarity: Paper is clear and easy to follow. Quality: My main concerns are related to the experimental section. Authors only report results for one model on one dataset. It is unclear how those results would transfer to different tasks and architectures. In addition, the experiments are relatively small scale (10 workers), how does the system scale as you increase the number of workers? The top-1 reported on CIFAR10 seems pretty low with respect to the current state-of-art. It would be nice to use a more competitive model such as a Resnet-18 to verify that one can achieve similar performance with Zeno++ compared to AR-SGD (without attack). Authors introduced Zeno++ to reduce the computation overhead over Zeno+. Did you empirically quantify this speed-up, and do you achieve similar accuracy than Zeno+? Significance: Authors only compare with asynchronous baseline. It would be nice to demonstrate the advantage of asynchronous methods over synchronous one (such as Zeno and All-Reduce SGD without attack). Can you show a speed benefit of asynchronous Zeno++ and show similar accuracy than synchronous approaches? Minor: - It would be to reports use a log scale in Fig 1 b), c) and Fig 2. b), d). ", "rating": "3: Weak Reject", "reply_text": "1.For novelty , we have to argue that asynchronous training is very different from synchronous training . Although the main idea is based on Zeno , applying such idea to asynchronous training requires a lot of non-trivial practical and theoretical work , which is our contribution . Furthermore , as we have shown in the experiments , since the performance of the baseline ( Kardam ) is very bad , our proposed algorithm could be the first algorithm that actually tolerates Byzantine failures and makes reasonable progress in convergence for asynchronous training . 2.Asynchronous training and synchronous training are designed for different scenarios . When there are stragglers in the workers , asynchronous training is preferred . In this paper , we only answer the question \u201c if we use asynchronous training , can we develop an algorithm to tolerate Byzantine workers ? \u201d , and assume that there are stragglers in the workers , so that asynchronous training is preferred . Even if all the workers are assumed to be homogeneous , \u201c which one of synchronous training and asynchronous training is faster \u201d is a controversial topic in distributed machine learning . There are tricks ( e.g. , rescaling learning rate w.r.t.batch sizes , and warmup ) to improve the performance of synchronous SGD to match the performance with single-threaded SGD with small batch sizes . There are also tricks [ 2 ] to improve asynchronous SGD to match the performance of synchronous SGD . We believe that the research and discussion about the competition between synchronous training and asynchronous training will continue for a long time , which is out of the scope of this paper . 3.It is perhaps worth emphasizing that Zeno+ is not a previously existing algorithm i.e.not a previous baseline . Zeno++ is our main contribution ( which we evaluate theoretically and empirically ) . As stated , this novel approach is inspired by Zeno+ , which we chose to include for completeness . We did not empirically compare Zeno+ and Zeno++ , but we can theoretically compare the computation overhead on the server side . Assume that model size is $ d $ . Assume that the overhead of simple element-wise vector operations is $ c_1 d $ , the overhead of a single forward step is $ c_2 d $ , the overhead of a single backward step is $ c_3 d $ . When using Zeno+ or Zeno++ , the server uses $ n_s $ samples to validate any received gradient candidate . Zeno++ updates the validation vector $ v $ after receiving every $ k $ gradient candidates . Typically , $ c_1 \\leq c_2 \\leq c_3 $ For Zeno+ , the overhead of validation is $ 2 c_1 d + n_s c_2 d $ ( $ 2 c_1 $ for computing $ x-\\gamma g $ and $ \\|g\\|^2 $ , evaluating $ f_S ( x-\\gamma g ) $ takes $ n_s $ forward steps , ignoring the overhead of computing $ f_s ( x ) $ ) . Furthermore , note that the validation can only be started after the gradient candidate $ g $ is received . For Zeno++ , the overhead of validation on average is $ 2 c_1 d + n_s ( c_2 + c_3 ) d / k $ ( $ 2 c_1 $ for computing $ < v , g > $ and $ \\|g\\|^2 $ , computing $ v $ takes $ n_s $ backward and forward steps for every $ k $ iterations ) . Furthermore , note that the computation of $ v $ does not need to wait until $ g $ is received . Thus , if we assume $ c_2 \\approx c_3 $ , then taking $ k > 2 $ will make the overhead of Zeno++ less than that of Zeno+ . Note that in our experiments , we take $ k=10 $ . Furthermore , since for Zeno++ , the computation of $ v $ is non-blocking , the overhead $ n_s ( c_2 + c_3 ) d / k $ could be hidden . In the ideal cases , the overhead of Zeno++ could be very close to $ 2 c_1 d $ . 4.We are adding some experiments according to the reviewer 's requirement , including experiments on Resnet-20 , and experiments on language models . We will revise the manuscript and append the new results after the additional experiments are done . References [ 1 ] You , Yang , et al . `` Imagenet training in minutes . '' Proceedings of the 47th International Conference on Parallel Processing . ACM , 2018 . [ 2 ] Aji , Alham Fikri , and Kenneth Heafield . `` Making Asynchronous Stochastic Gradient Descent Work for Transformers . '' arXiv preprint arXiv:1906.03496 ( 2019 ) ."}, {"review_id": "rygHe64FDS-2", "review_text": " This paper proposes an approach to Byzantine fault tolerance in asynchronous distributed SGD. The approach appears to be novel. Theoretical convergence guarantees are provided, and an empirical evaluation illustrates very promising results. Overall, the paper is well-written and the results appear to be novel and interesting. Although I have a few questions, listed below, I generally lean towards accepting this paper. The assumption of a lower bound on validation gradients is somewhat troubling, especially for over-parameterized problems where so-called \"interpolation\" may be possible. I realize that validation samples are never used explicitly for stochastic gradient updates, but the algorithm does ensure that the stochastic gradients used are similar to gradients of validation samples. If one is converging to a (local) minimizer, one wants the gradient to vanish. How do we reconcile these points? Also, to properly set $\\rho$, for the theory to be valid, one needs to know this bound (or a lower bound on $V_2$). Is this practical? The paper claims that the computational overhead of Zeno+ is too great to evaluate for comparison with Zeno++. From my reading of the two methods, it isn't immediately obvious to me why this is the case. Including experiments which at least compare the per-iteration runtime (even if not running Zeno+ for training to completion) would make the paper more compelling. After all, Zeno++ still involves periodically evaluating the gradient at a validation sample. The paper makes the reasonable point that it is not reasonable to assume a bounded number of adversaries in the asynchronous setting, and the theorem statements make no assumption about the number of adversaries or rate at which received gradients are from a Byzantine worker. However, there are also no guarantees about whether the algorithm will ever make progress (i.e., will line 8 ever be reached?). This should be stated more transparently in the paper. Also, I was wondering, given that a gradient has been computed on the parameter server's validation set, which is assumed to be \"clean\", why not take a step using this gradient when the test in line 7 fails? Finally, the paper titles includes SGD, but the description in Def 1 doesn't appear to involve stochastic gradients. Typical parameter server implementations have workers compute mini-batch stochastic gradients, not full gradients on their shard of the training set. Does Zeno++ need to be modified to run in this setting? Does the theory still hold? Minor: - Is there a typo in line 5 of Zeno++? Should this be $\\nabla f_s$ instead of $f_s$? Otherwise, what does it mean to take the inner product of $v$ and $g$ in line 7? ", "rating": "6: Weak Accept", "reply_text": "1. \u201c If one is converging to a ( local ) minimizer , one wants the gradient to vanish . How do we reconcile these points ? \u201d Note that we assume that the training data ( for $ F ( x ) $ ) and the validation data ( for $ f_s ( x ) $ ) are different , although they could be similar to each other . Thus , a reasonable implication is that $ F ( x ) $ and $ E [ f_s ( x ) ] $ has different minimizers . Thus , when the model converges to the training data , i.e. $ \\| \\nabla F ( x_ * ) \\|^2 = 0 $ where $ x_ * $ is the minimizer , we should have $ \\| E [ \\nabla f_s ( x_ * ) ] \\|^2 \\neq 0 $ since $ x_ * $ is not a minimizer of $ E [ f_s ( x ) ] $ . Furthermore , even if the expectation $ E [ \\nabla f_s ( x_ * ) ] = 0 $ $ \\| E [ \\nabla f_s ( x_ * ) ] \\|^2 = 0 $ ) , the stochastic gradient $ \\nabla f_s ( x_ * ) $ won \u2019 t converge to 0 , because $ E [ \\| \\nabla f_s ( x_ * ) \\|^2 ] = E [ \\| \\nabla f_s ( x_ * ) - E [ \\nabla f_s ( x_ * ) ] \\|^2 ] + \\| E [ \\nabla f_s ( x_ * ) ] \\|^2 = E [ \\| \\nabla f_s ( x_ * ) - E [ \\nabla f_s ( x_ * ) ] \\|^2 ] $ converges to a non-zero variance . Thus , such assumption does not conflict with the convergence/vanishing gradient on the training data . 2.For computation overhead : It is perhaps worth emphasizing that Zeno+ is not a previously existing algorithm i.e.not a previous baseline . Zeno++ is our main contribution ( which we evaluate theoretically and empirically ) . As stated , this novel approach is inspired by Zeno+ , which we chose to include for completeness . We did not empirically compare Zeno+ and Zeno++ , but we can theoretically compare their computation overhead . Assume that model size is $ d $ . Assume that the overhead of simple element-wise vector operations is $ c_1 d $ , the overhead of a single forward step is $ c_2 d $ , the overhead of a single backward step is $ c_3 d $ . When using Zeno+ or Zeno++ , the server uses $ n_s $ samples to validate any received gradient candidate . Zeno++ updates the validation vector $ v $ after receiving every $ k $ gradient candidates . Typically , $ c_1 \\leq c_2 \\leq c_3 $ For Zeno+ , the overhead of validation is $ 2 c_1 d + n_s c_2 d $ ( $ 2 c_1 $ for computing $ x-\\gamma g $ and $ \\|g\\|^2 $ , evaluating $ f_S ( x-\\gamma g ) $ takes $ n_s $ forward steps , ignoring the overhead of computing $ f_s ( x ) $ ) . Furthermore , note that the validation can only be started after the gradient candidate $ g $ is received . For Zeno++ , the overhead of validation on average is $ 2 c_1 d + n_s ( c_2 + c_3 ) d / k $ ( $ 2 c_1 $ for computing $ < v , g > $ and $ \\|g\\|^2 $ , computing $ v $ takes $ n_s $ backward and forward steps for every $ k $ iterations ) .Furthermore , note that the computation of $ v $ does not need to wait until $ g $ is received . Thus , if we assume $ c_2 \\approx c_3 $ , then taking $ k > 2 $ will make the overhead of Zeno++ less than that of Zeno+ . Note that in our experiments , we take $ k=10 $ . Furthermore , since for Zeno++ , the computation of $ v $ is non-blocking , the overhead $ n_s ( c_2 + c_3 ) d / k $ could be hidden . In the idea cases , the overhead of Zeno++ could be very close to $ 2 c_1 d $ . 3.As mentioned by the reviewer , it is true that line 8 of Algorithm 2 may never be reached , if bad hyperparamers ( $ \\rho $ and $ \\epsilon $ ) are taken . In our experiments , we found that typically , we can take $ \\epsilon $ close to 0 , and $ \\rho \\in [ \\gamma \\times 10^ { -2 } , \\gamma \\times 10^ { -1 } ] $ , where $ \\gamma $ is the learning rate . 4. \u201c why not take a step using this gradient when the test in line 7 fails ? \u201d According to our objective function , we want to train a model on the training data . We assume that the validation dataset for the testing gradient is similar but different from the training data in expectation ( or , they have similar but different distributions ) . It is possible to use these gradients , but that will end up with training a model on a different objective function . Thus , in general , we do not want to directly use these gradients for training . Furthermore , in our experiments , we show that if we train the model only on the validation data ( no training data is used ) , the performance will be very bad ( see the baseline \u201c Server-only \u201d in all the figures ) . 5.In Definition 1 , $ \\nabla f ( x_\\tau ; z_ { i , j } ) $ is a stochastic gradient . $ n $ is the mini-batch size of the stochastic gradient . $ z_ { i , j } $ is a random sample from the local dataset on the $ i $ th worker . Also , in Algorithm 2 , line 4 of Worker , we randomly draw $ n $ samples from the local dataset $ D_i $ , and compute the stochastic gradient $ \\tilde { g } $ . All the gradients in the algorithms are stochastic , no full gradients are used . The theorems are already for stochastic gradients . We will clearly state that they are all stochastic gradients in a revised version . 6.Yes , line 5 of Algorithm 2 is a typo , it should be $ \\nabla f_s ( x_\\tau ) $ instead of $ f_s ( x_\\tau ) $ . 7.Yes , $ < x , y > $ means the inner-product between vector $ x $ and $ y $ ."}], "0": {"review_id": "rygHe64FDS-0", "review_text": "This paper addresses security of distributed optimization algorithm under Byzantine failures. These failures usually prevent convergence of training neural network models. Focusing on the asynchronous SGD algorithm implemented with a parameter-server, the authors propose to use stochastic line search ideas to detect whether the gradients are good descent directions or not. It applies to a general scenario including repeated and unbounded Byzantine failures. Theoretical results of this paper seem to me to have some flaw. In the proof of Theorem 1, line 6, it is not clear to me why the gradient norm ||g||^2 is replaced by || grad f_s (x_tau) ||^2. Clearly the g comes from any worker which can be very different to grad f_s (x_tau). Therefore, I recommend the authors check the proof of both theorem 1 and 2 more carefully. Numerical results show that the proposed algorithm Zeno++ works well with sign-flipping attacks. However, this scenario is very limited to validate all imaginable Byzantine failures that this paper would like to address. For example, one can use random gradients instead of sign-flipped gradients as a Byzantine attack, would the algorithm still work? ", "rating": "3: Weak Reject", "reply_text": "1.Question : \u201c In the proof of Theorem 1 , line 6 , it is not clear to me why the gradient norm ||g||^2 is replaced by || grad f_s ( x_tau ) ||^2. \u201d Answer : This is because of ( re- ) normalization . In Zeno++ , Line 4-6 of Algorithm 2 ( Server ) , the server receives $ \\tilde { g } $ from worker , and then normalize it , which results in $ g $ , where $ \\|g\\|^2 = \\| \\nabla f_s ( x_\\tau ) \\|^2 $ . It is true that $ g $ is very different from $ \\nabla f_s ( x_\\tau ) $ , but our algorithm makes them to have the same 2-norm by normalization . The purpose/intuition of doing so is that the attackers can also rescale the candidate gradient $ \\tilde { g } $ to have very large 2-norm , which will be very harmful to the convergence . By normalization , $ g $ will have similar 2-norm as ordinary gradients , so that even if a Byzantine $ g $ passes the test of Zeno+ or Zeno++ , the harm will be limited . We have added some experiments according to the reviewer 's requirement , including experiments with random attack . We have also double-checked the proof , and we think the current version is correct . We hope that our answers and clarifications resolve the reviewers ' concern ."}, "1": {"review_id": "rygHe64FDS-1", "review_text": "Summary: This paper investigates the security of distributed asynchronous SGD. Authors propose Zeno++, worker-server asynchronous implementation of SGD which is robust to Byzantine failures. To ensure that the gradients sent by the workers are correct, Zeno++ server scores each worker gradients using a \u201creference\u201d gradient computed on a \u201csecret\u201d validation set. If the score is under a given threshold, then the worker gradient is discarded. Authors provide convergence guarantee for the Zeno++ optimizer for non-convex function. In addition, they provide an empirical evaluation of Zeno++ on the CIFAR10 datasets and compare with various baselines. Originality: I would argue that the paper novelty is limited. Paper builds upon the Zeno algorithm. From the paper, the main changes with respect to Zeno algorithm is the use of a hard-threshold instead of a ranking to adapt Zeno to the asynchronous case, and the use of a first-order Tayler approximation of the score to speed up its computations. Clarity: Paper is clear and easy to follow. Quality: My main concerns are related to the experimental section. Authors only report results for one model on one dataset. It is unclear how those results would transfer to different tasks and architectures. In addition, the experiments are relatively small scale (10 workers), how does the system scale as you increase the number of workers? The top-1 reported on CIFAR10 seems pretty low with respect to the current state-of-art. It would be nice to use a more competitive model such as a Resnet-18 to verify that one can achieve similar performance with Zeno++ compared to AR-SGD (without attack). Authors introduced Zeno++ to reduce the computation overhead over Zeno+. Did you empirically quantify this speed-up, and do you achieve similar accuracy than Zeno+? Significance: Authors only compare with asynchronous baseline. It would be nice to demonstrate the advantage of asynchronous methods over synchronous one (such as Zeno and All-Reduce SGD without attack). Can you show a speed benefit of asynchronous Zeno++ and show similar accuracy than synchronous approaches? Minor: - It would be to reports use a log scale in Fig 1 b), c) and Fig 2. b), d). ", "rating": "3: Weak Reject", "reply_text": "1.For novelty , we have to argue that asynchronous training is very different from synchronous training . Although the main idea is based on Zeno , applying such idea to asynchronous training requires a lot of non-trivial practical and theoretical work , which is our contribution . Furthermore , as we have shown in the experiments , since the performance of the baseline ( Kardam ) is very bad , our proposed algorithm could be the first algorithm that actually tolerates Byzantine failures and makes reasonable progress in convergence for asynchronous training . 2.Asynchronous training and synchronous training are designed for different scenarios . When there are stragglers in the workers , asynchronous training is preferred . In this paper , we only answer the question \u201c if we use asynchronous training , can we develop an algorithm to tolerate Byzantine workers ? \u201d , and assume that there are stragglers in the workers , so that asynchronous training is preferred . Even if all the workers are assumed to be homogeneous , \u201c which one of synchronous training and asynchronous training is faster \u201d is a controversial topic in distributed machine learning . There are tricks ( e.g. , rescaling learning rate w.r.t.batch sizes , and warmup ) to improve the performance of synchronous SGD to match the performance with single-threaded SGD with small batch sizes . There are also tricks [ 2 ] to improve asynchronous SGD to match the performance of synchronous SGD . We believe that the research and discussion about the competition between synchronous training and asynchronous training will continue for a long time , which is out of the scope of this paper . 3.It is perhaps worth emphasizing that Zeno+ is not a previously existing algorithm i.e.not a previous baseline . Zeno++ is our main contribution ( which we evaluate theoretically and empirically ) . As stated , this novel approach is inspired by Zeno+ , which we chose to include for completeness . We did not empirically compare Zeno+ and Zeno++ , but we can theoretically compare the computation overhead on the server side . Assume that model size is $ d $ . Assume that the overhead of simple element-wise vector operations is $ c_1 d $ , the overhead of a single forward step is $ c_2 d $ , the overhead of a single backward step is $ c_3 d $ . When using Zeno+ or Zeno++ , the server uses $ n_s $ samples to validate any received gradient candidate . Zeno++ updates the validation vector $ v $ after receiving every $ k $ gradient candidates . Typically , $ c_1 \\leq c_2 \\leq c_3 $ For Zeno+ , the overhead of validation is $ 2 c_1 d + n_s c_2 d $ ( $ 2 c_1 $ for computing $ x-\\gamma g $ and $ \\|g\\|^2 $ , evaluating $ f_S ( x-\\gamma g ) $ takes $ n_s $ forward steps , ignoring the overhead of computing $ f_s ( x ) $ ) . Furthermore , note that the validation can only be started after the gradient candidate $ g $ is received . For Zeno++ , the overhead of validation on average is $ 2 c_1 d + n_s ( c_2 + c_3 ) d / k $ ( $ 2 c_1 $ for computing $ < v , g > $ and $ \\|g\\|^2 $ , computing $ v $ takes $ n_s $ backward and forward steps for every $ k $ iterations ) . Furthermore , note that the computation of $ v $ does not need to wait until $ g $ is received . Thus , if we assume $ c_2 \\approx c_3 $ , then taking $ k > 2 $ will make the overhead of Zeno++ less than that of Zeno+ . Note that in our experiments , we take $ k=10 $ . Furthermore , since for Zeno++ , the computation of $ v $ is non-blocking , the overhead $ n_s ( c_2 + c_3 ) d / k $ could be hidden . In the ideal cases , the overhead of Zeno++ could be very close to $ 2 c_1 d $ . 4.We are adding some experiments according to the reviewer 's requirement , including experiments on Resnet-20 , and experiments on language models . We will revise the manuscript and append the new results after the additional experiments are done . References [ 1 ] You , Yang , et al . `` Imagenet training in minutes . '' Proceedings of the 47th International Conference on Parallel Processing . ACM , 2018 . [ 2 ] Aji , Alham Fikri , and Kenneth Heafield . `` Making Asynchronous Stochastic Gradient Descent Work for Transformers . '' arXiv preprint arXiv:1906.03496 ( 2019 ) ."}, "2": {"review_id": "rygHe64FDS-2", "review_text": " This paper proposes an approach to Byzantine fault tolerance in asynchronous distributed SGD. The approach appears to be novel. Theoretical convergence guarantees are provided, and an empirical evaluation illustrates very promising results. Overall, the paper is well-written and the results appear to be novel and interesting. Although I have a few questions, listed below, I generally lean towards accepting this paper. The assumption of a lower bound on validation gradients is somewhat troubling, especially for over-parameterized problems where so-called \"interpolation\" may be possible. I realize that validation samples are never used explicitly for stochastic gradient updates, but the algorithm does ensure that the stochastic gradients used are similar to gradients of validation samples. If one is converging to a (local) minimizer, one wants the gradient to vanish. How do we reconcile these points? Also, to properly set $\\rho$, for the theory to be valid, one needs to know this bound (or a lower bound on $V_2$). Is this practical? The paper claims that the computational overhead of Zeno+ is too great to evaluate for comparison with Zeno++. From my reading of the two methods, it isn't immediately obvious to me why this is the case. Including experiments which at least compare the per-iteration runtime (even if not running Zeno+ for training to completion) would make the paper more compelling. After all, Zeno++ still involves periodically evaluating the gradient at a validation sample. The paper makes the reasonable point that it is not reasonable to assume a bounded number of adversaries in the asynchronous setting, and the theorem statements make no assumption about the number of adversaries or rate at which received gradients are from a Byzantine worker. However, there are also no guarantees about whether the algorithm will ever make progress (i.e., will line 8 ever be reached?). This should be stated more transparently in the paper. Also, I was wondering, given that a gradient has been computed on the parameter server's validation set, which is assumed to be \"clean\", why not take a step using this gradient when the test in line 7 fails? Finally, the paper titles includes SGD, but the description in Def 1 doesn't appear to involve stochastic gradients. Typical parameter server implementations have workers compute mini-batch stochastic gradients, not full gradients on their shard of the training set. Does Zeno++ need to be modified to run in this setting? Does the theory still hold? Minor: - Is there a typo in line 5 of Zeno++? Should this be $\\nabla f_s$ instead of $f_s$? Otherwise, what does it mean to take the inner product of $v$ and $g$ in line 7? ", "rating": "6: Weak Accept", "reply_text": "1. \u201c If one is converging to a ( local ) minimizer , one wants the gradient to vanish . How do we reconcile these points ? \u201d Note that we assume that the training data ( for $ F ( x ) $ ) and the validation data ( for $ f_s ( x ) $ ) are different , although they could be similar to each other . Thus , a reasonable implication is that $ F ( x ) $ and $ E [ f_s ( x ) ] $ has different minimizers . Thus , when the model converges to the training data , i.e. $ \\| \\nabla F ( x_ * ) \\|^2 = 0 $ where $ x_ * $ is the minimizer , we should have $ \\| E [ \\nabla f_s ( x_ * ) ] \\|^2 \\neq 0 $ since $ x_ * $ is not a minimizer of $ E [ f_s ( x ) ] $ . Furthermore , even if the expectation $ E [ \\nabla f_s ( x_ * ) ] = 0 $ $ \\| E [ \\nabla f_s ( x_ * ) ] \\|^2 = 0 $ ) , the stochastic gradient $ \\nabla f_s ( x_ * ) $ won \u2019 t converge to 0 , because $ E [ \\| \\nabla f_s ( x_ * ) \\|^2 ] = E [ \\| \\nabla f_s ( x_ * ) - E [ \\nabla f_s ( x_ * ) ] \\|^2 ] + \\| E [ \\nabla f_s ( x_ * ) ] \\|^2 = E [ \\| \\nabla f_s ( x_ * ) - E [ \\nabla f_s ( x_ * ) ] \\|^2 ] $ converges to a non-zero variance . Thus , such assumption does not conflict with the convergence/vanishing gradient on the training data . 2.For computation overhead : It is perhaps worth emphasizing that Zeno+ is not a previously existing algorithm i.e.not a previous baseline . Zeno++ is our main contribution ( which we evaluate theoretically and empirically ) . As stated , this novel approach is inspired by Zeno+ , which we chose to include for completeness . We did not empirically compare Zeno+ and Zeno++ , but we can theoretically compare their computation overhead . Assume that model size is $ d $ . Assume that the overhead of simple element-wise vector operations is $ c_1 d $ , the overhead of a single forward step is $ c_2 d $ , the overhead of a single backward step is $ c_3 d $ . When using Zeno+ or Zeno++ , the server uses $ n_s $ samples to validate any received gradient candidate . Zeno++ updates the validation vector $ v $ after receiving every $ k $ gradient candidates . Typically , $ c_1 \\leq c_2 \\leq c_3 $ For Zeno+ , the overhead of validation is $ 2 c_1 d + n_s c_2 d $ ( $ 2 c_1 $ for computing $ x-\\gamma g $ and $ \\|g\\|^2 $ , evaluating $ f_S ( x-\\gamma g ) $ takes $ n_s $ forward steps , ignoring the overhead of computing $ f_s ( x ) $ ) . Furthermore , note that the validation can only be started after the gradient candidate $ g $ is received . For Zeno++ , the overhead of validation on average is $ 2 c_1 d + n_s ( c_2 + c_3 ) d / k $ ( $ 2 c_1 $ for computing $ < v , g > $ and $ \\|g\\|^2 $ , computing $ v $ takes $ n_s $ backward and forward steps for every $ k $ iterations ) .Furthermore , note that the computation of $ v $ does not need to wait until $ g $ is received . Thus , if we assume $ c_2 \\approx c_3 $ , then taking $ k > 2 $ will make the overhead of Zeno++ less than that of Zeno+ . Note that in our experiments , we take $ k=10 $ . Furthermore , since for Zeno++ , the computation of $ v $ is non-blocking , the overhead $ n_s ( c_2 + c_3 ) d / k $ could be hidden . In the idea cases , the overhead of Zeno++ could be very close to $ 2 c_1 d $ . 3.As mentioned by the reviewer , it is true that line 8 of Algorithm 2 may never be reached , if bad hyperparamers ( $ \\rho $ and $ \\epsilon $ ) are taken . In our experiments , we found that typically , we can take $ \\epsilon $ close to 0 , and $ \\rho \\in [ \\gamma \\times 10^ { -2 } , \\gamma \\times 10^ { -1 } ] $ , where $ \\gamma $ is the learning rate . 4. \u201c why not take a step using this gradient when the test in line 7 fails ? \u201d According to our objective function , we want to train a model on the training data . We assume that the validation dataset for the testing gradient is similar but different from the training data in expectation ( or , they have similar but different distributions ) . It is possible to use these gradients , but that will end up with training a model on a different objective function . Thus , in general , we do not want to directly use these gradients for training . Furthermore , in our experiments , we show that if we train the model only on the validation data ( no training data is used ) , the performance will be very bad ( see the baseline \u201c Server-only \u201d in all the figures ) . 5.In Definition 1 , $ \\nabla f ( x_\\tau ; z_ { i , j } ) $ is a stochastic gradient . $ n $ is the mini-batch size of the stochastic gradient . $ z_ { i , j } $ is a random sample from the local dataset on the $ i $ th worker . Also , in Algorithm 2 , line 4 of Worker , we randomly draw $ n $ samples from the local dataset $ D_i $ , and compute the stochastic gradient $ \\tilde { g } $ . All the gradients in the algorithms are stochastic , no full gradients are used . The theorems are already for stochastic gradients . We will clearly state that they are all stochastic gradients in a revised version . 6.Yes , line 5 of Algorithm 2 is a typo , it should be $ \\nabla f_s ( x_\\tau ) $ instead of $ f_s ( x_\\tau ) $ . 7.Yes , $ < x , y > $ means the inner-product between vector $ x $ and $ y $ ."}}