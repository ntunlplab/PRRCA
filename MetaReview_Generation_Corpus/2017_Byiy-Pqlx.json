{"year": "2017", "forum": "Byiy-Pqlx", "title": "Lie-Access Neural Turing Machines", "decision": "Accept (Poster)", "meta_review": "The paper presents a Lie-(group) access neural turing machine (LANTM) architecture, and demonstrates it's utility on several problems. \n \n Pros:\n Reviewers agree that this is an interesting and clearly-presented idea.\n Overall, the paper is clearly written and presents original ideas.\n It is likely to inspire further work into more effective generalizations of NTMs.\n \n Cons:\n The true impact and capabilities of these architectures are not yet clear, although it is argued that the same can be said for NTMs. \n \n The paper has been revised to address some NTM features (sharpening) that were not included in the original version.\n The purpose and precise definition of the invNorm have also been fixed.", "reviews": [{"review_id": "Byiy-Pqlx-0", "review_text": "The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups. This allows to place memory elements as points on a manifold, while still allowing training with backpropagation. It's a more general version of the NTM memory, and possibly allows for training a more efficient addressing schemes. Pros: - novel and interesting idea for memory access - nicely written Cons: - need to manually specify the Lie group to use (it would be better if network could learn the best way of accessing memory) - not clear if this really works better than standard NTM (compared only to simplified version) - not clear if this is useful in practice (no comparison on real tasks) ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for taking the time to read and review our paper . > need to manually specify the Lie group to use ( it would be better if network could learn the best way of accessing memory ) We see the choice of Lie Group as analogous to the choice of memory layout in discrete models . In a perfect world that could be learned , but we think something like an R^n key space is a fine general-layout . > not clear if this really works better than standard NTM ( compared only to simplified version ) We will implement some of the tricks in a standard NTM such as sharpening . However , we see it as a benefit that these are not necessary in Lie-access . > not clear if this is useful in practice ( no comparison on real tasks ) We realize that the tasks used are mainly synthetic , but this is true for most work studying NTMs . Our tasks capture essential aspects of algorithmic learning , like for loops ( repeat copy ) , sorting ( priority sort ) , and arithmetic . They are nontrivial , as demonstrated by the gap between baseline performances and LANTM performances ."}, {"review_id": "Byiy-Pqlx-1", "review_text": "The Neural Turing Machine and related \u201cexternal memory models\u201d have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position. The NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which \u201csoftly\u201d shifts the head, allowing the machine to read and write sequences. Since this soft shift typically \u201csmears\u201d the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution. The premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z. This is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere. Overall, the paper is well communicated and a novel idea. The primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified. The baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming \u2018smeared\u2019). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses. Minor issues: Footnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix. Figures on page 8 are difficult to follow. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for reading and reviewing our paper . > The primary limitation of this paper is its limited impact . [ ... I ] t is not clear that this significantly contributes to building models capable of more general program learning . Instead , it is likely to make an already complex and slow model such as the NTM even slower . What leads you to think that Lie-Access is complex or slower ? We feel that this model is simpler than NTM/DNC and contributes to program learning by providing a more natural interpretation of memory than soft versions of discrete memory . Our model requires only simple lookups in R^2 and could certainly be optimized to run fast . For example , all the advances made in the recent paper `` Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes '' by Rae et al.could be applied to Lie-Access Memory . > In general , it would seem memory topology is problem specific and should therefore be learned rather than specified . We feel this criticism should apply as well to NTM and DNC . They specify a rigid memory system modelled after discrete memory arrays . By trying different memories topologies we were simply experimenting with types of access and not trying to advocate problem specific topology . Likely an R^n key space would be sufficient for most problems . > The baseline used for comparison is a very simple model , which does not even having the sharpening ( the NTM approach to solving the problem of head distributions becoming \u2018 smeared \u2019 ) . There is also no comparison with the successor to the NTM , the DNC , which provides a more general approach to linking memories based on prior memory accesses . We will add sharpening to the simplified NTM in our experiments . We had hoped to compare to DNC , but were not able to reimplement it . There are now reimplementations though that we will compare to ."}, {"review_id": "Byiy-Pqlx-2", "review_text": "The paper proposes a new memory access scheme based on Lie group actions for NTMs. Pros: * Well written * Novel addressing scheme as an extension to NTM. * Seems to work slightly better than normal NTMs. * Some interesting theory about the novel addressing scheme based on Lie groups. Cons: * In the results, the LANTM only seems to be slightly better than the normal NTM. * The result tables are a bit confusing. * No source code available. * The difference to the properties of normal NTM doesn't become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme. * It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn't become clear what is meant here. * No tests on real-world tasks, only some toy tasks. * No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) (https://arxiv.org/abs/1610.09027). Although the motivations of other NTM extensions might be different, such comparisons still would have been interesting. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for reviewing our paper . > In the results , the LANTM only seems to be slightly better than the normal NTM . We disagree with this interpretation of the results . In most tasks the best LANTM gets almost 100 % coarse on generalization , but the corresponding simplified NTM never gets more than 50 % coarse on generalization . > The result tables are a bit confusing . Can you clarify this comment ? We closely followed past work in this area when presenting the results . > No source code available . We plan on open sourcing the code and will include a link in the next version . > The difference to the properties of normal NTM does n't become too clear . Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme . Let us reiterate the key motivation of the paper : the set of NTM \u2019 s head movement does * not * compose a group . In particular it is not true that every head movement has an inverse movement . This is briefly explained in footnote 1 . In short , NTM \u2019 s head movement is a Markov kernel , which mixes the probabilities encoded in the NTM \u2019 s head . Intuitively , a mixture can not be \u201c unmixed \u201d by mixing further ; formally , it can be shown that a Markov kernel does not have an inverse that is a Markov kernel if it is not the delta kernel ( all values concentrated at one point ) . This problem manifests as the loss of concentration in NTM \u2019 s head after shifting repeatedly . This was fixed in an ad hoc way in the NTM paper via \u201c sharpening coefficients \u201d that boosts concentration , but they have to be learned . > It is said that the head is discrete in NTM but actually it is in space R^n , i.e.it is already continuous . It does n't become clear what is meant here . We agree that NTM is continuous . Our point is that it does not form a group . > No tests on real-world tasks , only some toy tasks . We agree that the tasks used are mainly synthetic , but this is true on most work studying NTMs . Our tasks capture essential aspects of algorithmic learning , like for loops ( repeat copy ) , sorting ( priority sort ) , and arithmetic . They are nontrivial , as demonstrated by the gap between baseline performances and LANTM performances . > No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory ( SAM ) ( https : //arxiv.org/abs/1610.09027 ) . Although the motivations of other NTM extensions might be different , such comparisons still would have been interesting . These are very interesting papers but they target aspects of the NTM that are orthogonal to the topology of addressing schemes . For example , the contributions of the D-NTM paper were mainly Dynamic Least Recently Used ( LRU ) addressing , regularization methods , and hard addressing using REINFORCE . In our paper under review here , we argue for a principled way to purpose different manifolds as memory storage and different Lie groups as relative head movement . In contrast , D-NTM \u2019 s memory is still modeled after a traditional memory array and it does not have relative head movement . A priori , there is no reason that ideas from both papers can not be combined in a coherent way ."}, {"review_id": "Byiy-Pqlx-3", "review_text": "*** Paper Summary *** This paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation. It then proposes a framework in which any Lie group as the addressing space. Experiments on algorithmic tasks are reported. *** Review Summary *** This paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. Its proposal provide a generic scheme to build addressing mechanisms. When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. *** Detailed Review *** The paper reads well, has appropriate relevance to related work. The unified presentation of memory augmented networks is clear and brings unity to the field. The proposed approach is introduced clearly, is powerful and gives a tool that can be reused after reading the article. I do not appreciate that the growing memory is not mentioned as a drawback. It should be stressed and a discussion on the impact it has on efficiency/scalability is needed. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for reading and reviewing our paper . > I do not appreciate that the growing memory is not mentioned as a drawback . It should be stressed and a discussion on the impact it has on efficiency/scalability is needed . Thanks for the comment . We agree this is an issue with key/value type approaches . We will discuss this more explicitly and mention possible future corrections ."}], "0": {"review_id": "Byiy-Pqlx-0", "review_text": "The paper introduces a novel memory mechanism for NTMs based on differentiable Lie groups. This allows to place memory elements as points on a manifold, while still allowing training with backpropagation. It's a more general version of the NTM memory, and possibly allows for training a more efficient addressing schemes. Pros: - novel and interesting idea for memory access - nicely written Cons: - need to manually specify the Lie group to use (it would be better if network could learn the best way of accessing memory) - not clear if this really works better than standard NTM (compared only to simplified version) - not clear if this is useful in practice (no comparison on real tasks) ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for taking the time to read and review our paper . > need to manually specify the Lie group to use ( it would be better if network could learn the best way of accessing memory ) We see the choice of Lie Group as analogous to the choice of memory layout in discrete models . In a perfect world that could be learned , but we think something like an R^n key space is a fine general-layout . > not clear if this really works better than standard NTM ( compared only to simplified version ) We will implement some of the tricks in a standard NTM such as sharpening . However , we see it as a benefit that these are not necessary in Lie-access . > not clear if this is useful in practice ( no comparison on real tasks ) We realize that the tasks used are mainly synthetic , but this is true for most work studying NTMs . Our tasks capture essential aspects of algorithmic learning , like for loops ( repeat copy ) , sorting ( priority sort ) , and arithmetic . They are nontrivial , as demonstrated by the gap between baseline performances and LANTM performances ."}, "1": {"review_id": "Byiy-Pqlx-1", "review_text": "The Neural Turing Machine and related \u201cexternal memory models\u201d have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position. The NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which \u201csoftly\u201d shifts the head, allowing the machine to read and write sequences. Since this soft shift typically \u201csmears\u201d the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution. The premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z. This is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere. Overall, the paper is well communicated and a novel idea. The primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified. The baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming \u2018smeared\u2019). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses. Minor issues: Footnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix. Figures on page 8 are difficult to follow. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for reading and reviewing our paper . > The primary limitation of this paper is its limited impact . [ ... I ] t is not clear that this significantly contributes to building models capable of more general program learning . Instead , it is likely to make an already complex and slow model such as the NTM even slower . What leads you to think that Lie-Access is complex or slower ? We feel that this model is simpler than NTM/DNC and contributes to program learning by providing a more natural interpretation of memory than soft versions of discrete memory . Our model requires only simple lookups in R^2 and could certainly be optimized to run fast . For example , all the advances made in the recent paper `` Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes '' by Rae et al.could be applied to Lie-Access Memory . > In general , it would seem memory topology is problem specific and should therefore be learned rather than specified . We feel this criticism should apply as well to NTM and DNC . They specify a rigid memory system modelled after discrete memory arrays . By trying different memories topologies we were simply experimenting with types of access and not trying to advocate problem specific topology . Likely an R^n key space would be sufficient for most problems . > The baseline used for comparison is a very simple model , which does not even having the sharpening ( the NTM approach to solving the problem of head distributions becoming \u2018 smeared \u2019 ) . There is also no comparison with the successor to the NTM , the DNC , which provides a more general approach to linking memories based on prior memory accesses . We will add sharpening to the simplified NTM in our experiments . We had hoped to compare to DNC , but were not able to reimplement it . There are now reimplementations though that we will compare to ."}, "2": {"review_id": "Byiy-Pqlx-2", "review_text": "The paper proposes a new memory access scheme based on Lie group actions for NTMs. Pros: * Well written * Novel addressing scheme as an extension to NTM. * Seems to work slightly better than normal NTMs. * Some interesting theory about the novel addressing scheme based on Lie groups. Cons: * In the results, the LANTM only seems to be slightly better than the normal NTM. * The result tables are a bit confusing. * No source code available. * The difference to the properties of normal NTM doesn't become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme. * It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn't become clear what is meant here. * No tests on real-world tasks, only some toy tasks. * No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) (https://arxiv.org/abs/1610.09027). Although the motivations of other NTM extensions might be different, such comparisons still would have been interesting. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for reviewing our paper . > In the results , the LANTM only seems to be slightly better than the normal NTM . We disagree with this interpretation of the results . In most tasks the best LANTM gets almost 100 % coarse on generalization , but the corresponding simplified NTM never gets more than 50 % coarse on generalization . > The result tables are a bit confusing . Can you clarify this comment ? We closely followed past work in this area when presenting the results . > No source code available . We plan on open sourcing the code and will include a link in the next version . > The difference to the properties of normal NTM does n't become too clear . Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme . Let us reiterate the key motivation of the paper : the set of NTM \u2019 s head movement does * not * compose a group . In particular it is not true that every head movement has an inverse movement . This is briefly explained in footnote 1 . In short , NTM \u2019 s head movement is a Markov kernel , which mixes the probabilities encoded in the NTM \u2019 s head . Intuitively , a mixture can not be \u201c unmixed \u201d by mixing further ; formally , it can be shown that a Markov kernel does not have an inverse that is a Markov kernel if it is not the delta kernel ( all values concentrated at one point ) . This problem manifests as the loss of concentration in NTM \u2019 s head after shifting repeatedly . This was fixed in an ad hoc way in the NTM paper via \u201c sharpening coefficients \u201d that boosts concentration , but they have to be learned . > It is said that the head is discrete in NTM but actually it is in space R^n , i.e.it is already continuous . It does n't become clear what is meant here . We agree that NTM is continuous . Our point is that it does not form a group . > No tests on real-world tasks , only some toy tasks . We agree that the tasks used are mainly synthetic , but this is true on most work studying NTMs . Our tasks capture essential aspects of algorithmic learning , like for loops ( repeat copy ) , sorting ( priority sort ) , and arithmetic . They are nontrivial , as demonstrated by the gap between baseline performances and LANTM performances . > No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory ( SAM ) ( https : //arxiv.org/abs/1610.09027 ) . Although the motivations of other NTM extensions might be different , such comparisons still would have been interesting . These are very interesting papers but they target aspects of the NTM that are orthogonal to the topology of addressing schemes . For example , the contributions of the D-NTM paper were mainly Dynamic Least Recently Used ( LRU ) addressing , regularization methods , and hard addressing using REINFORCE . In our paper under review here , we argue for a principled way to purpose different manifolds as memory storage and different Lie groups as relative head movement . In contrast , D-NTM \u2019 s memory is still modeled after a traditional memory array and it does not have relative head movement . A priori , there is no reason that ideas from both papers can not be combined in a coherent way ."}, "3": {"review_id": "Byiy-Pqlx-3", "review_text": "*** Paper Summary *** This paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation. It then proposes a framework in which any Lie group as the addressing space. Experiments on algorithmic tasks are reported. *** Review Summary *** This paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. Its proposal provide a generic scheme to build addressing mechanisms. When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. *** Detailed Review *** The paper reads well, has appropriate relevance to related work. The unified presentation of memory augmented networks is clear and brings unity to the field. The proposed approach is introduced clearly, is powerful and gives a tool that can be reused after reading the article. I do not appreciate that the growing memory is not mentioned as a drawback. It should be stressed and a discussion on the impact it has on efficiency/scalability is needed. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for reading and reviewing our paper . > I do not appreciate that the growing memory is not mentioned as a drawback . It should be stressed and a discussion on the impact it has on efficiency/scalability is needed . Thanks for the comment . We agree this is an issue with key/value type approaches . We will discuss this more explicitly and mention possible future corrections ."}}