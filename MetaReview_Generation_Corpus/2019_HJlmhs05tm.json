{"year": "2019", "forum": "HJlmhs05tm", "title": "EnGAN: Latent Space MCMC and Maximum Entropy Generators for Energy-based Models", "decision": "Reject", "meta_review": "The proposed method is an extension of Kim & Bengio (2016)'s energy-based GAN. The novel contributions are to approximate the entropy regularizer using a mutual information estimator, and to try to clean up the model samples using some Langevin steps. Experiments include mode dropping experiments on toy data, samples from the model on CelebA, and measures of inception score and FID.\n\nThe paper is well-written, and the proposal seems sensible. But as various reviewers point out, the work is a fairly incremental extension of Kim and Bengio (2016). Most of the new elements, such as Langevin sampling and the gradient penalty, have also been well-explored in the deep generative modeling literature. It's not clear there is a particular contribution here that really stands out.\n\nThe experimental evidence for improvement is also fairly limited. Generated samples, inception scores, and FID are pretty weak measures for generative models, though I'm willing to go with them since they seem to be standard in the field. But even by these measures, there doesn't seem to be much improvement. I wouldn't expect SOTA results because of computational limitations, but the generated samples and quantitative evaluations seem worse than the WGAN-GP, even though the proposed method includes the gradient penalty and hence should be able to at least match WGAN-GP. The MCMC sampling doesn't appear to have helped, as far as I can tell.\n\nOverall, the proposal seems promising, but I don't think this paper is ready for publication at ICLR.\n", "reviews": [{"review_id": "HJlmhs05tm-0", "review_text": " In this paper, the authors extend the framework proposed by Kim&Bengio 2016 and Dai et.al. 2017, which introduce an extra step to fit a generator to approximate the current model for estimating the deep energy model. Specifically, the generator is fitted by reverse KL divergence. To bypass the difficulty in handling the entropy term, the authors exploit the Deep INFOMAX formulation, which introduces one more discriminator. Finally, to obtain better samples, the authors inject the Metropolis-adjusted Langevin algorithm within the learned generator to generate samples in latent space. They demonstrate the better performances of the proposed algorithms in both synthetic and real-world datasets, and apply the learned model for anomaly detection task. The paper is well-written and does a quite good job in combining several existing algorithms to obtain the ultimate algorithm. The algorithm achieves quite good empirical performances. However, the major problem of this paper is the novelty. The algorithm is basically an extension of the Kim&Bengio 2016 and Dai et.al. 2017, with other existing learning technique. Maybe the only novel part is combining the MCMC with the learned generator for generating samples. However, the benefits of such combination is not well justified empirically. Based the figure 4, it seems the MCMC does not provide better samples, comparing to directly generate samples from G_z. It will be better if the authors can justify the motivation of using MCMC step. Secondly, it is reasonable that the authors introduce the gradient norm as the regularization to the objective for training stability. However, it will be better if the effect of the regularization for the energy model estimation can be discussed. Minor: The loss function for potential in Eq(3) is incorrect and inconsistent with the Algorithm 1. I think the formulation in the Algorithm box is correct. In sum, I personally like the paper as a nice combination of recently developed techniques to improve the algorithm for solving the remaining problem in statistics. The paper can be better if the above mentioned issues can be addressed ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their time and feedback . We hope to address concerns the reviewer has here . * \u201c However , the major problem of this paper is the novelty . The algorithm is basically an extension of the Kim & Bengio 2016 and Dai et.al . 2017 , with other existing learning technique \u201d We strongly believe this paper goes well beyond Kim & Bengio 2016 . First , a major issue of Kim & Bengio 2016 is that it used covariance to maximize entropy . When we tried reproducing the results in that paper , even with the help of the authors , we could not get stable results . Entropy maximization using a mutual information estimator is much more robust compared to covariance maximization . But that alone was not enough and we got strong improvements by using the gradient norm regularizer ( see ( 3 ) below ) which helped stabilize the training as well . Finally , we show a successful form of MCMC exploiting the generator latent space composed with the energy function and we show new and successful empirical results on anomaly detection and sharp image generation , something which had not been done earlier for an energy-based model ( and certainly not by Kim & Bengio ) . We also direct the reviewer towards our strong empirical results on discrete mode collapse where we show our model naturally covers all the modes in that data ( in the expanded , 10^4 mode StackedMNIST dataset ) and also better matches the mode count distribution as evidenced by the very low KL divergence scores . * \u201c Maybe the only novel part is combining the MCMC with the learned generator for generating samples . However , the benefits of such combination is not well justified empirically . Based the figure 4 , it seems the MCMC does not provide better samples , comparing to directly generate samples from G_z . It will be better if the authors can justify the motivation of using MCMC step. \u201d Our contribution is also to enforce entropy maximization on the discriminator and using a regularizer on the energy-function to stabilize training . This specific combination was instrumental in obtaining our empirical result : ( 1 ) Covering all modes in our discrete mode collapse experiment where our model matches the mode count distribution of the data significantly better than WGAN-GP as pointed out in Section 5.2 and Table 1 . ( 2 ) Using the learned energy function to perform anomaly detection , beating the previous SOTA energy-based model ( DSEBM ) by a large margin ( as mentioned in Section 5.4 Table 3 ) and comparable to the SOTA anomaly detection method ( DAGMM ) which is purely designed for anomaly detection and not generative modeling ( 3 ) Natural image generation , where our energy-based method performs comparable to a strong WGAN-GP baseline in perceptual quality and doesn \u2019 t exhibit the common blurriness issue in standard maximum-likelihood trained EBMs ( Section 5.3 Table 2 ) . Regarding the justification of latent space MCMC : Note that the MCMC on the energy function in data space did not give good results , while doing it in the latent space worked . ( Refer Figure 5 for data-space MCMC samples ) . We hypothesize that the reason for this is ( a ) walking on the data manifold is much easier in the latent space , as shown earlier by Bengio et al 2013 and ( b ) composing the generator with the energy function gets rid of spurious modes of the energy which the generator can not represent ( if it did , then the negative samples from the generator would have made the energy function learn to get rid of these spurious modes ) . * \u201c Secondly , it is reasonable that the authors introduce the gradient norm as the regularization to the objective for training stability . However , it will be better if the effect of the regularization for the energy model estimation can be discussed. \u201d Effect of the regularization of the energy function : the regularizer ||dEnergy ( x ) /dx||^2 is not just a smoothness regularizer but it also makes data points x energy minima ( because ||dEnergy ( x ) /dx|| should be 0 at data points ) . This thus helps to learn a better energy function . Note that this is similar in spirit to score matching , which also carves the energy function so that it has local minima at the training points ( i.e it is helping to make data points as an energy minima ) . The regularizer also stabilizes the temperature ( scale ) of the energy function , making training stable . * \u201c The loss function for potential in Eq ( 3 ) is incorrect and inconsistent with the Algorithm 1 . I think the formulation in the Algorithm box is correct. \u201d Indeed there was a typo in eqn 3 . The LHS should have been the gradient of L_E wrt theta , and Omega on the RHS should have been dOmega/dtheta ."}, {"review_id": "HJlmhs05tm-1", "review_text": "It is well known that energy-based model training requires sampling from the current model. This paper aims to develop an energy-based generative model with a generator that produces approximate samples. For this purpose, this paper combines a number of existing techniques, including sampling in latent space, using a GAN-like technique to maximize the entropy of the generator distribution. Evaluation experiments are conducted on toy 2D data, unsupervised anomaly detection, image generation. The proposed method is interesting, but there are some unclear issues, which hurts the quality of this paper. 1. Correctness The justification of adding a gradient norm regularizer in Eq. (3) for turning a GAN discriminator into an energy function is not clear. Sampling in latent space and then converting to data space samples to approximate the sampling from p_theta is operationally possible. There are three distributions - the generator distribution p_G, the distribution p_comp implicitly defined by the latent-space energy obtained by composing the generator and the data-space energy, and the energy-based model p_E. p_G is trained to approximate p_E, since we minimize KL(p_G||p_E). Does latent space sampling necessarily imply that p_comp leads to be closer to p_E ? 2. Significance In my view, the paper is an extension of Kim&Bengio 2016. Two extensions - providing a new manner to calculate the entropy term, and using sampling in latent space. In this regard, Section 3 is unnecessarily obscure. The results of image generation in Table 2 on CIFAR-10 are worse than WGAN-GP, which is now in fact only moderately performed GANs. In a concurrent ICLR submission - \"Learning Neural Random Fields with Inclusive Auxiliary Generators\", energy-based models trained with their method are shown to significantly outperform WGAN-GP. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their time and feedback . We hope to address concerns the reviewer has here . * \u201c The justification of adding a gradient norm regularizer in Eq . ( 3 ) for turning a GAN discriminator into an energy function is not clear. \u201d Gradient norm regularizer in Eq . ( 3 ) : the regularizer ||dEnergy ( x ) /dx||^2 is not just a smoothness regularizer but it also makes data points x energy minima ( because ||dEnergy ( x ) /dx|| should be 0 at data points ) . This thus helps to learn a better energy function . Note that this is similar in spirit to score matching , which also carves the energy function so that it has local minima at the training points . The regularizer also stabilizes the temperature ( scale ) of the energy function , making training stable . * \u201c In my view , the paper is an extension of Kim & Bengio 2016 \u201d We strongly believe this paper goes well beyond Kim & Bengio 2016 . First , a major issue of Kim & Bengio 2016 is that it used covariance to maximize entropy . When we tried reproducing the results in that paper , even with the help of the authors , we could not get stable results . Entropy maximization using a mutual information estimator is much more robust compared to covariance maximization . But that alone was not enough and we got strong improvements by using the gradient norm regularizer ( see ( 3 ) below ) which helped stabilize the training as well . Finally , we show a successful form of MCMC exploiting the generator latent space composed with the energy function and we show new and successful empirical results on anomaly detection and sharp image generation , something which had not been done earlier for an energy-based model ( and certainly not by Kim & Bengio ) . We also direct the reviewer towards our empirical results on discrete mode collapse where we show our model naturally covers all the modes in that data ( in the expanded , 10^4 mode StackedMNIST dataset ) and also better matches the mode count distribution as evidenced by the very low KL divergence scores . * \u201c Sampling in latent space and then converting to data space samples to approximate the sampling from p_theta is operationally possible . There are three distributions - the generator distribution p_G , the distribution p_comp implicitly defined by the latent-space energy obtained by composing the generator and the data-space energy , and the energy-based model p_E. , p_G is trained to approximate p_E , since we minimize KL ( p_G||p_E ) . Does latent space sampling necessarily imply that p_comp leads to be closer to p_E ? \u201d MCMC on the energy function p_E in data space did not give good results , while doing it in the latent space worked . We hypothesize that the reason for this is ( a ) walking on the data manifold is much easier in the latent space , as shown earlier by Bengio et al 2013 ( because the data manifold has been somewhat flattened when represented in the latent space ) and ( b ) composing the generator with the energy function gets rid of spurious modes of the energy which the generator can not represent ( if it did , then the negative samples from the generator would have made the energy function learn to get rid of these spurious modes , via the 2nd term of eqn 3 when training the energy function ) . * \u201c The results of image generation in Table 2 on CIFAR-10 are worse than WGAN-GP , which is now in fact only moderately performed GANs . In a concurrent ICLR submission - `` Learning Neural Random Fields with Inclusive Auxiliary Generators '' , energy-based models trained with their method are shown to significantly outperform WGAN-GP \u201d The objective was not to beat the best GANs ( which do not provide an energy function ) but to show that it was possible to have both an energy function and good samples by appropriately fixing issues with the Kim & Bengio setup ( and we clearly did not know about the concurrent ICLR submissions on energy-based models ) . Please let us know if anything is unclear here or if there is any other comparison that would be helpful in clarifying things more ."}, {"review_id": "HJlmhs05tm-2", "review_text": "Thank you for an interesting read. The paper proposes an approximate training technique for energy-based models (EBMs). More specifically, the samples used negative phase gradient in EBM training is approximated by samples from another generator. This \"approximate generator\" is a composition of a decoder (which, with a Gaussian prior on latent variable z, is trained to approximate the data distribution) and another EBM in latent space. The authors show connections to WGAN training, thus the name EnGAN. Experiments on natural image generation and anomaly detection show promising improvements, although not very significant. From my understanding of the paper, the main contribution of the paper comes from section 4, which proposes a latent-space MCMC scheme to improve sample quality. I have seen several papers fusing EBMs and GAN training together and to the best of my knowledge section 4 is novel (but with problems, see below). Section 3's recipe is quite standard, e.g. as seen in Kim and Bengio (2017), and in principle contrastive divergence also uses the same idea. The idea of estimating of the entropy term for the implicit distribution p_G with adversarial mutual information estimation is something new, although quite straight-forward. Although I do agree that MCMC mixing in x space can be much harder than MCMC mixing in z space, since I don't think the proposed latent-space MCMC scheme is exact (apart from finite-time simulation, rejection...), I don't see theoretically why the method works. 1. The MCMC method essentially samples z from another EBM, where that EBM(z) has energy function -E_{\\theta}(G(z)), and then generate x = G(z). Note here EBM(z) != p(z). The key issue is, even when p_G(x) = p_{\\theta}(x), there is no guarantee that the proposed latent-space MCMC method would return x samples according to distribution p_{\\theta}(x). You can easily work out a counter example by considering G is an invertible transformation. Therefore I don't understand why doing MCMC on this latent-space EBM can help improve sample quality in x space. 2. Continuing point 1, with Algorithm 1 that only fits p_G(x) towards p_{\\theta}(x), I am confident that the negative phase gradient is still quite biased. Why not just use the latent-space MCMC sampler composited with G as the generator, and use these MCMC samples to train both the decoder G and the mutual information estimator? 3. I am not exactly sure why the gradient norm regulariser in (3) make sense here? True that it would be helpful to correct the bias of the negative phase, but why this particular form? We are not doing WGAN here and in general we don't usually put a Lipschitz constraint on the energy function. I've seem several GAN papers arguing that gradient penalty helps in cases beyond WGAN, but most of them are just empirical observations... Also the Omega regulariser is computed on which x? On data? Do you know whether the energy is guaranteed to be minimized at data locations? In this is that appropriate to call Omega a regulariser? The presentation is overall clear, although I think there are a few typos and confusing equations: 1. There should be a negative sign on the LHS of equation 2. 2. Equation 3 is inconsistent with the energy update equation in Algorithm 1. The latter one makes more sense. 3. Where is the ratio between the transition kernels in the acceptance ratio equation? In general for Langevin dynamics the transition kernel is not symmetric. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the positive and constructive feedback . We appreciate that the reviewer finds that our method is clearly explained . * \u201c 1.The MCMC method essentially samples z from another EBM , where that EBM ( z ) has energy function -E_ { \\theta } ( G ( z ) ) , and then generate x = G ( z ) . Note here EBM ( z ) ! = p ( z ) . The key issue is , even when p_G ( x ) = p_ { \\theta } ( x ) , there is no guarantee that the proposed latent-space MCMC method would return x samples according to distribution p_ { \\theta } ( x ) . You can easily work out a counter example by considering G is an invertible transformation . Therefore I do n't understand why doing MCMC on this latent-space EBM can help improve sample quality in x space. \u201d Our hypothesis is the following : composing the generator with the energy function gets rid of spurious modes of the energy which the generator can not represent . If the generator did sample from these spurious modes , then the negative samples from the generator would have made the energy function learn to get rid of these spurious modes , via the 2nd term of eqn 3 when training the energy function . Hence we get a cleaned-up version of the energy function . Spurious modes of the energy function which have not been eliminated via training through eqn 3 are thus erased by this composition of G with E. Now there may be a price to pay for this , i.e. , G may also be missing some modes ( as usual with GANs ) . However , because we have the entropy maximization term ( eqn 4 ) , we at least train in a way that attempts to minimize this problem . We agree that the composed energy function is different from E. The other good thing about MCMC in the composed energy function is that it seems to also be easier , following the observations of Bengio et al 2013 , because the data manifold has been somewhat flattened in the latent space of the generator . * \u201c 2.Continuing point 1 , with Algorithm 1 that only fits p_G ( x ) towards p_ { \\theta } ( x ) , I am confident that the negative phase gradient is still quite biased . Why not just use the latent-space MCMC sampler composited with G as the generator , and use these MCMC samples to train both the decoder G and the mutual information estimator ? \u201d This is a good idea , which we did not execute yet because it would slow down training 10-fold , but it is an interesting direction to follow-up with . * \u201c 3.I am not exactly sure why the gradient norm regularizer in ( 3 ) make sense here ? True that it would be helpful to correct the bias of the negative phase , but why this particular form ? We are not doing WGAN here and in general we do n't usually put a Lipschitz constraint on the energy function . I 've seem several GAN papers arguing that gradient penalty helps in cases beyond WGAN , but most of them are just empirical observations ... Also the Omega regularizer is computed on which x ? On data ? Do you know whether the energy is guaranteed to be minimized at data locations ? In this is that appropriate to call Omega a regularizer ? \u201d The regularizer ||dEnergy ( x ) /dx||^2 is not just a smoothness regularizer but it also makes data points x energy minima ( because ||dEnergy ( x ) /dx|| should be 0 at data points ) . This thus helps to learn a better energy function . Note that this is similar in spirit to score matching , which also carves the energy function so that it has local minima at the training points . The regularizer also stabilizes the temperature ( scale ) of the energy function , making training stable ( avoiding continued growth of precision , inverse temperature , as training continues ) . * \u201c 2.Equation 3 is inconsistent with the energy update equation in Algorithm 1 . The latter one makes more sense. \u201d Sorry for the typo in eqn 3 . The LHS should have been the gradient of L_E wrt theta , and Omega on the RHS should have been dOmega/dtheta . * \u201c 3.Where is the ratio between the transition kernels in the acceptance ratio equation ? In general for Langevin dynamics the transition kernel is not symmetric. \u201d The correction term to be added to -E ( G ( z ' ) ) +E ( G ( z ) ) ( where z ' = new z , and z = old z ) would be : log ( q ( z|z ' ) /q ( z|z ' ) ) = 0.5 ( ||eps||^2 - ||eps - sqrt ( alpha/2 ) ( E ' ( z ) - E ' ( z ' ) ) ||^2 ) where q is the proposal distribution producing z ' from z and E ' = gradient of E. We tried using the full formula but that it did not seem to make a discernible difference . Please let us know if anything is unclear here or if there is any other comparison that would be helpful in clarifying things more ."}], "0": {"review_id": "HJlmhs05tm-0", "review_text": " In this paper, the authors extend the framework proposed by Kim&Bengio 2016 and Dai et.al. 2017, which introduce an extra step to fit a generator to approximate the current model for estimating the deep energy model. Specifically, the generator is fitted by reverse KL divergence. To bypass the difficulty in handling the entropy term, the authors exploit the Deep INFOMAX formulation, which introduces one more discriminator. Finally, to obtain better samples, the authors inject the Metropolis-adjusted Langevin algorithm within the learned generator to generate samples in latent space. They demonstrate the better performances of the proposed algorithms in both synthetic and real-world datasets, and apply the learned model for anomaly detection task. The paper is well-written and does a quite good job in combining several existing algorithms to obtain the ultimate algorithm. The algorithm achieves quite good empirical performances. However, the major problem of this paper is the novelty. The algorithm is basically an extension of the Kim&Bengio 2016 and Dai et.al. 2017, with other existing learning technique. Maybe the only novel part is combining the MCMC with the learned generator for generating samples. However, the benefits of such combination is not well justified empirically. Based the figure 4, it seems the MCMC does not provide better samples, comparing to directly generate samples from G_z. It will be better if the authors can justify the motivation of using MCMC step. Secondly, it is reasonable that the authors introduce the gradient norm as the regularization to the objective for training stability. However, it will be better if the effect of the regularization for the energy model estimation can be discussed. Minor: The loss function for potential in Eq(3) is incorrect and inconsistent with the Algorithm 1. I think the formulation in the Algorithm box is correct. In sum, I personally like the paper as a nice combination of recently developed techniques to improve the algorithm for solving the remaining problem in statistics. The paper can be better if the above mentioned issues can be addressed ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their time and feedback . We hope to address concerns the reviewer has here . * \u201c However , the major problem of this paper is the novelty . The algorithm is basically an extension of the Kim & Bengio 2016 and Dai et.al . 2017 , with other existing learning technique \u201d We strongly believe this paper goes well beyond Kim & Bengio 2016 . First , a major issue of Kim & Bengio 2016 is that it used covariance to maximize entropy . When we tried reproducing the results in that paper , even with the help of the authors , we could not get stable results . Entropy maximization using a mutual information estimator is much more robust compared to covariance maximization . But that alone was not enough and we got strong improvements by using the gradient norm regularizer ( see ( 3 ) below ) which helped stabilize the training as well . Finally , we show a successful form of MCMC exploiting the generator latent space composed with the energy function and we show new and successful empirical results on anomaly detection and sharp image generation , something which had not been done earlier for an energy-based model ( and certainly not by Kim & Bengio ) . We also direct the reviewer towards our strong empirical results on discrete mode collapse where we show our model naturally covers all the modes in that data ( in the expanded , 10^4 mode StackedMNIST dataset ) and also better matches the mode count distribution as evidenced by the very low KL divergence scores . * \u201c Maybe the only novel part is combining the MCMC with the learned generator for generating samples . However , the benefits of such combination is not well justified empirically . Based the figure 4 , it seems the MCMC does not provide better samples , comparing to directly generate samples from G_z . It will be better if the authors can justify the motivation of using MCMC step. \u201d Our contribution is also to enforce entropy maximization on the discriminator and using a regularizer on the energy-function to stabilize training . This specific combination was instrumental in obtaining our empirical result : ( 1 ) Covering all modes in our discrete mode collapse experiment where our model matches the mode count distribution of the data significantly better than WGAN-GP as pointed out in Section 5.2 and Table 1 . ( 2 ) Using the learned energy function to perform anomaly detection , beating the previous SOTA energy-based model ( DSEBM ) by a large margin ( as mentioned in Section 5.4 Table 3 ) and comparable to the SOTA anomaly detection method ( DAGMM ) which is purely designed for anomaly detection and not generative modeling ( 3 ) Natural image generation , where our energy-based method performs comparable to a strong WGAN-GP baseline in perceptual quality and doesn \u2019 t exhibit the common blurriness issue in standard maximum-likelihood trained EBMs ( Section 5.3 Table 2 ) . Regarding the justification of latent space MCMC : Note that the MCMC on the energy function in data space did not give good results , while doing it in the latent space worked . ( Refer Figure 5 for data-space MCMC samples ) . We hypothesize that the reason for this is ( a ) walking on the data manifold is much easier in the latent space , as shown earlier by Bengio et al 2013 and ( b ) composing the generator with the energy function gets rid of spurious modes of the energy which the generator can not represent ( if it did , then the negative samples from the generator would have made the energy function learn to get rid of these spurious modes ) . * \u201c Secondly , it is reasonable that the authors introduce the gradient norm as the regularization to the objective for training stability . However , it will be better if the effect of the regularization for the energy model estimation can be discussed. \u201d Effect of the regularization of the energy function : the regularizer ||dEnergy ( x ) /dx||^2 is not just a smoothness regularizer but it also makes data points x energy minima ( because ||dEnergy ( x ) /dx|| should be 0 at data points ) . This thus helps to learn a better energy function . Note that this is similar in spirit to score matching , which also carves the energy function so that it has local minima at the training points ( i.e it is helping to make data points as an energy minima ) . The regularizer also stabilizes the temperature ( scale ) of the energy function , making training stable . * \u201c The loss function for potential in Eq ( 3 ) is incorrect and inconsistent with the Algorithm 1 . I think the formulation in the Algorithm box is correct. \u201d Indeed there was a typo in eqn 3 . The LHS should have been the gradient of L_E wrt theta , and Omega on the RHS should have been dOmega/dtheta ."}, "1": {"review_id": "HJlmhs05tm-1", "review_text": "It is well known that energy-based model training requires sampling from the current model. This paper aims to develop an energy-based generative model with a generator that produces approximate samples. For this purpose, this paper combines a number of existing techniques, including sampling in latent space, using a GAN-like technique to maximize the entropy of the generator distribution. Evaluation experiments are conducted on toy 2D data, unsupervised anomaly detection, image generation. The proposed method is interesting, but there are some unclear issues, which hurts the quality of this paper. 1. Correctness The justification of adding a gradient norm regularizer in Eq. (3) for turning a GAN discriminator into an energy function is not clear. Sampling in latent space and then converting to data space samples to approximate the sampling from p_theta is operationally possible. There are three distributions - the generator distribution p_G, the distribution p_comp implicitly defined by the latent-space energy obtained by composing the generator and the data-space energy, and the energy-based model p_E. p_G is trained to approximate p_E, since we minimize KL(p_G||p_E). Does latent space sampling necessarily imply that p_comp leads to be closer to p_E ? 2. Significance In my view, the paper is an extension of Kim&Bengio 2016. Two extensions - providing a new manner to calculate the entropy term, and using sampling in latent space. In this regard, Section 3 is unnecessarily obscure. The results of image generation in Table 2 on CIFAR-10 are worse than WGAN-GP, which is now in fact only moderately performed GANs. In a concurrent ICLR submission - \"Learning Neural Random Fields with Inclusive Auxiliary Generators\", energy-based models trained with their method are shown to significantly outperform WGAN-GP. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their time and feedback . We hope to address concerns the reviewer has here . * \u201c The justification of adding a gradient norm regularizer in Eq . ( 3 ) for turning a GAN discriminator into an energy function is not clear. \u201d Gradient norm regularizer in Eq . ( 3 ) : the regularizer ||dEnergy ( x ) /dx||^2 is not just a smoothness regularizer but it also makes data points x energy minima ( because ||dEnergy ( x ) /dx|| should be 0 at data points ) . This thus helps to learn a better energy function . Note that this is similar in spirit to score matching , which also carves the energy function so that it has local minima at the training points . The regularizer also stabilizes the temperature ( scale ) of the energy function , making training stable . * \u201c In my view , the paper is an extension of Kim & Bengio 2016 \u201d We strongly believe this paper goes well beyond Kim & Bengio 2016 . First , a major issue of Kim & Bengio 2016 is that it used covariance to maximize entropy . When we tried reproducing the results in that paper , even with the help of the authors , we could not get stable results . Entropy maximization using a mutual information estimator is much more robust compared to covariance maximization . But that alone was not enough and we got strong improvements by using the gradient norm regularizer ( see ( 3 ) below ) which helped stabilize the training as well . Finally , we show a successful form of MCMC exploiting the generator latent space composed with the energy function and we show new and successful empirical results on anomaly detection and sharp image generation , something which had not been done earlier for an energy-based model ( and certainly not by Kim & Bengio ) . We also direct the reviewer towards our empirical results on discrete mode collapse where we show our model naturally covers all the modes in that data ( in the expanded , 10^4 mode StackedMNIST dataset ) and also better matches the mode count distribution as evidenced by the very low KL divergence scores . * \u201c Sampling in latent space and then converting to data space samples to approximate the sampling from p_theta is operationally possible . There are three distributions - the generator distribution p_G , the distribution p_comp implicitly defined by the latent-space energy obtained by composing the generator and the data-space energy , and the energy-based model p_E. , p_G is trained to approximate p_E , since we minimize KL ( p_G||p_E ) . Does latent space sampling necessarily imply that p_comp leads to be closer to p_E ? \u201d MCMC on the energy function p_E in data space did not give good results , while doing it in the latent space worked . We hypothesize that the reason for this is ( a ) walking on the data manifold is much easier in the latent space , as shown earlier by Bengio et al 2013 ( because the data manifold has been somewhat flattened when represented in the latent space ) and ( b ) composing the generator with the energy function gets rid of spurious modes of the energy which the generator can not represent ( if it did , then the negative samples from the generator would have made the energy function learn to get rid of these spurious modes , via the 2nd term of eqn 3 when training the energy function ) . * \u201c The results of image generation in Table 2 on CIFAR-10 are worse than WGAN-GP , which is now in fact only moderately performed GANs . In a concurrent ICLR submission - `` Learning Neural Random Fields with Inclusive Auxiliary Generators '' , energy-based models trained with their method are shown to significantly outperform WGAN-GP \u201d The objective was not to beat the best GANs ( which do not provide an energy function ) but to show that it was possible to have both an energy function and good samples by appropriately fixing issues with the Kim & Bengio setup ( and we clearly did not know about the concurrent ICLR submissions on energy-based models ) . Please let us know if anything is unclear here or if there is any other comparison that would be helpful in clarifying things more ."}, "2": {"review_id": "HJlmhs05tm-2", "review_text": "Thank you for an interesting read. The paper proposes an approximate training technique for energy-based models (EBMs). More specifically, the samples used negative phase gradient in EBM training is approximated by samples from another generator. This \"approximate generator\" is a composition of a decoder (which, with a Gaussian prior on latent variable z, is trained to approximate the data distribution) and another EBM in latent space. The authors show connections to WGAN training, thus the name EnGAN. Experiments on natural image generation and anomaly detection show promising improvements, although not very significant. From my understanding of the paper, the main contribution of the paper comes from section 4, which proposes a latent-space MCMC scheme to improve sample quality. I have seen several papers fusing EBMs and GAN training together and to the best of my knowledge section 4 is novel (but with problems, see below). Section 3's recipe is quite standard, e.g. as seen in Kim and Bengio (2017), and in principle contrastive divergence also uses the same idea. The idea of estimating of the entropy term for the implicit distribution p_G with adversarial mutual information estimation is something new, although quite straight-forward. Although I do agree that MCMC mixing in x space can be much harder than MCMC mixing in z space, since I don't think the proposed latent-space MCMC scheme is exact (apart from finite-time simulation, rejection...), I don't see theoretically why the method works. 1. The MCMC method essentially samples z from another EBM, where that EBM(z) has energy function -E_{\\theta}(G(z)), and then generate x = G(z). Note here EBM(z) != p(z). The key issue is, even when p_G(x) = p_{\\theta}(x), there is no guarantee that the proposed latent-space MCMC method would return x samples according to distribution p_{\\theta}(x). You can easily work out a counter example by considering G is an invertible transformation. Therefore I don't understand why doing MCMC on this latent-space EBM can help improve sample quality in x space. 2. Continuing point 1, with Algorithm 1 that only fits p_G(x) towards p_{\\theta}(x), I am confident that the negative phase gradient is still quite biased. Why not just use the latent-space MCMC sampler composited with G as the generator, and use these MCMC samples to train both the decoder G and the mutual information estimator? 3. I am not exactly sure why the gradient norm regulariser in (3) make sense here? True that it would be helpful to correct the bias of the negative phase, but why this particular form? We are not doing WGAN here and in general we don't usually put a Lipschitz constraint on the energy function. I've seem several GAN papers arguing that gradient penalty helps in cases beyond WGAN, but most of them are just empirical observations... Also the Omega regulariser is computed on which x? On data? Do you know whether the energy is guaranteed to be minimized at data locations? In this is that appropriate to call Omega a regulariser? The presentation is overall clear, although I think there are a few typos and confusing equations: 1. There should be a negative sign on the LHS of equation 2. 2. Equation 3 is inconsistent with the energy update equation in Algorithm 1. The latter one makes more sense. 3. Where is the ratio between the transition kernels in the acceptance ratio equation? In general for Langevin dynamics the transition kernel is not symmetric. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the positive and constructive feedback . We appreciate that the reviewer finds that our method is clearly explained . * \u201c 1.The MCMC method essentially samples z from another EBM , where that EBM ( z ) has energy function -E_ { \\theta } ( G ( z ) ) , and then generate x = G ( z ) . Note here EBM ( z ) ! = p ( z ) . The key issue is , even when p_G ( x ) = p_ { \\theta } ( x ) , there is no guarantee that the proposed latent-space MCMC method would return x samples according to distribution p_ { \\theta } ( x ) . You can easily work out a counter example by considering G is an invertible transformation . Therefore I do n't understand why doing MCMC on this latent-space EBM can help improve sample quality in x space. \u201d Our hypothesis is the following : composing the generator with the energy function gets rid of spurious modes of the energy which the generator can not represent . If the generator did sample from these spurious modes , then the negative samples from the generator would have made the energy function learn to get rid of these spurious modes , via the 2nd term of eqn 3 when training the energy function . Hence we get a cleaned-up version of the energy function . Spurious modes of the energy function which have not been eliminated via training through eqn 3 are thus erased by this composition of G with E. Now there may be a price to pay for this , i.e. , G may also be missing some modes ( as usual with GANs ) . However , because we have the entropy maximization term ( eqn 4 ) , we at least train in a way that attempts to minimize this problem . We agree that the composed energy function is different from E. The other good thing about MCMC in the composed energy function is that it seems to also be easier , following the observations of Bengio et al 2013 , because the data manifold has been somewhat flattened in the latent space of the generator . * \u201c 2.Continuing point 1 , with Algorithm 1 that only fits p_G ( x ) towards p_ { \\theta } ( x ) , I am confident that the negative phase gradient is still quite biased . Why not just use the latent-space MCMC sampler composited with G as the generator , and use these MCMC samples to train both the decoder G and the mutual information estimator ? \u201d This is a good idea , which we did not execute yet because it would slow down training 10-fold , but it is an interesting direction to follow-up with . * \u201c 3.I am not exactly sure why the gradient norm regularizer in ( 3 ) make sense here ? True that it would be helpful to correct the bias of the negative phase , but why this particular form ? We are not doing WGAN here and in general we do n't usually put a Lipschitz constraint on the energy function . I 've seem several GAN papers arguing that gradient penalty helps in cases beyond WGAN , but most of them are just empirical observations ... Also the Omega regularizer is computed on which x ? On data ? Do you know whether the energy is guaranteed to be minimized at data locations ? In this is that appropriate to call Omega a regularizer ? \u201d The regularizer ||dEnergy ( x ) /dx||^2 is not just a smoothness regularizer but it also makes data points x energy minima ( because ||dEnergy ( x ) /dx|| should be 0 at data points ) . This thus helps to learn a better energy function . Note that this is similar in spirit to score matching , which also carves the energy function so that it has local minima at the training points . The regularizer also stabilizes the temperature ( scale ) of the energy function , making training stable ( avoiding continued growth of precision , inverse temperature , as training continues ) . * \u201c 2.Equation 3 is inconsistent with the energy update equation in Algorithm 1 . The latter one makes more sense. \u201d Sorry for the typo in eqn 3 . The LHS should have been the gradient of L_E wrt theta , and Omega on the RHS should have been dOmega/dtheta . * \u201c 3.Where is the ratio between the transition kernels in the acceptance ratio equation ? In general for Langevin dynamics the transition kernel is not symmetric. \u201d The correction term to be added to -E ( G ( z ' ) ) +E ( G ( z ) ) ( where z ' = new z , and z = old z ) would be : log ( q ( z|z ' ) /q ( z|z ' ) ) = 0.5 ( ||eps||^2 - ||eps - sqrt ( alpha/2 ) ( E ' ( z ) - E ' ( z ' ) ) ||^2 ) where q is the proposal distribution producing z ' from z and E ' = gradient of E. We tried using the full formula but that it did not seem to make a discernible difference . Please let us know if anything is unclear here or if there is any other comparison that would be helpful in clarifying things more ."}}