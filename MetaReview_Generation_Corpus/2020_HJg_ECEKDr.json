{"year": "2020", "forum": "HJg_ECEKDr", "title": "Generative Teaching Networks: Accelerating Neural Architecture Search by Learning  to Generate Synthetic Training Data", "decision": "Reject", "meta_review": "Overview:\nThis paper introduces a method to distill a large dataset into a smaller one that allows for faster training. The main application of this technique being studied is neural architecture search, which can be sped up by quickly evaluating architectures on the generated data rather than slowly evaluating them on the original data.\n\nSummary of discussion:\nDuring the discussion period, the authors appear to have updated the paper quite a bit, leading to the reviewers feeling more positive about it now than in the beginning. In particular, in the beginning, it appears to have been unclear that the distillation is merely used as a speedup trick, not to generate additional information out of thin air. The reviewers' scores left the paper below the decision boundary, but closely enough so that I read it myself. \n\nMy own judgement:\nI like the idea, which I find very novel. However, I have to push back on the authors' claims about their good performance in NAS. This has several reasons:\n\n1. In contrast to what is claimed by the authors, the comparison to graph hypernetworks (Zhang et al) is not fair, since the authors used a different protocol: Zhang et al sampled 800 networks and reported the performance (mean +/- std) of the 10 judged to be best by the hypernetwork. In contrast, the authors of the current paper sampled 1000 networks and reported the performance of the single one judged to be best. They repeated this procedure 5 times to get mean +/- std. The best architecture of 1000 is of course more likely to be strong than the average of the top 10 of 800.\n\n2. The comparison to random search with weight sharing (here: 3.92% error) does not appear fair. The cited paper in Table 1 is *not* the paper introducing random search + weight sharing, but the neural architecture optimization paper. The original one reported an error of 2.85% +/- 0.08% with 4.3M params. That paper also has the full source code available, so the authors could have performed a true apples-to-apples comparison. \n\n3. The authors' method requires an additional (one-time) cost for actually creating the 'fake' training data, so their runtimes should be increased by the 8h required for that.\n\n4. The fact that the authors achieve 2.42% error doesn't mean much; that result is just based on scaling the network up to 100M params. (The network obtained by random search also achieves 2.51%.)\n\nAs it stands, I cannot judge whether the authors' approach yields strong performance for NAS. In order to allow that conclusion, the authors would have to compare to another method based on the same underlying code base and experimental protocol. Also, the authors do not make code available at this time. Their method has a lot of bells and whistles, and I do not expect that I could reproduce it. They promise code, but it is unclear what this would include: the generated training data, code for training the networks, code for the meta-approach, etc? This would have been much easier to judge had the authors made the code available in anonymized fashion during the review.\n\nBecause of these reasons, in terms of making progress on NAS, the paper does not quite clear the bar for me. The authors also evaluated their method in several other scenarios, including reinforcement learning. These results appear to be very promising, but largely preliminary due to lack of time in the rebuttal phase.  \n\nRecommendation:\nThe paper is very novel and the results appear very promising, but they are also somewhat preliminary. The reviewers' scores leave the paper just below the acceptance threshold and my own borderline judgement is not positive enough to overrule this. I believe that some more time, and one more iteration of reorganization and review, would allow this paper to ripen into a very strong paper. For a resubmission to the next venue, I would recommend to either perform an apples-to-apples comparison for NAS or reorganize and just use NAS as one of several equally-weighted possible applications. In the current form, I believe the paper is not using its full potential.", "reviews": [{"review_id": "HJg_ECEKDr-0", "review_text": "This paper proposes a meta-learning algorithm Generative Teaching Networks (GTN) to generate fake training data for models to learn more accurate models. In the inner loop, a generator produces training data and the learner takes gradient steps on this data. In the outer loop, the parameters of the generator are updated by evaluating the learner on real data and differentiating through the gradient steps of the inner loop. The main claim is this method is shown to give improvements in performance on supervised learning for MNIST and CIFAR10. They also suggest weight normalization patches up instability issues with meta-learning and evaluate this in the supervised learning setting, and curriculum learning for GTNs. To me, the main claim is very surprising and counter-intuitive - it is not clear where the extra juice is coming from, as the algorithm does not assume any extra information. The actual results I believe do not bear out this claim because the actual results on MNIST and CIFAR10 are significantly below state of the art. On MNIST, GTN achieves about 98% accuracy and the baseline \u201cReal Data\u201d achieves <97% accuracy, while the state of the art is about 99.7% and well-tuned convnets without any pre-processing or fancy extras achieve about 99% according to Yann LeCunn\u2019s website. The disparity on CIFAR seems to be less egregious but the state of the art stands at 99% while the best GTN model (without cutout) achieves about 96.2% which matches good convnets and is slightly worse than neural architecture search according to https://paperswithcode.com/sota/image-classification-on-cifar-10. This does not negate the potential of GTNs which I feel are an interesting approach, but I believe the paper should be more straightforward with the presentation of these results. The current results basically show that GTNs improve the performance of learners with bad hyper-parameters. On problems that are not as well-studied as MNIST or CIFAR10 this could still be very valuable (as we do not know what performance is good or bad in advance). Based on the results, GTN does seem to be a significant step forward in synthetic data generation for learning compared to prior work (Zhang 2018, Luo 2018). The paper proposes two other contributions: using weight normalization for meta-learning and curriculum learning for GTNs. Weight normalization is shown to stabilize GTNs on MNIST. I think the paper oversteps in the relevant method section, hypothesizing it may stabilize meta-learning more broadly. The paper should present a wider set of experiments to make this claim convincing. But the point for GTNs on MNIST nevertheless stands. For curriculum learning: the description of the method is done across section 2 and section 3.2 and does not really describe it completely. How exactly are the samples chosen in GTN - All Shuffled? How does GTN - Full Curriculum and Shuffled Batch parametrize the order of the samples so that it can be learned? I suggest that this information is all included as a subsection in the method (section 2). The results seem to show the learned curriculum is superior to no curriculum. At a high level it would be very surprising to me if the way forward for better discriminative models was to learn good generative models and use them again for training discriminative models, simply because discriminative models have proved thus far significantly easier to train. If this work does eventually show this result, it would be a very interesting result. At the moment, I believe it does not, but I would be happy to change my mind if the authors provide convincing evidence. Alternatively, I feel that the paper could be a valuable contribution to the community if the writing is toned down to focus on the contributions, presents the results comparing to well-tuned hyperparameters and not over-claim. More comments: What is the outer loop loss function? Is it assumed to be the same as the inner one (but using real data instead of training data)? I think this should be made explicit in the method section. There are some additional experiments in other settings such as RL and unsupervised learning. Both seem like quite interesting directions but seem like preliminary experiments that don\u2019t work convincingly yet. The RL experiment shows that using GTN does not change performance much. There is a claim about optimizing randomly initialized networks at each step, but the baseline which uses randomly initialized networks at each step with A2C is missing. The GAN experiments shows the GAN loss makes GTN realistic (as expected) but there are no quantitative results on mode collapse. (Another interesting experiment would be to show how adding a GAN loss for generating data affects the test performance of the method.) Perhaps it would benefit the paper to narrow in on supervised learning? Given that these final experiments are not polished, the claim in the abstract that the method is \u201ca general approach that is applicable to supervised, unsupervised, and reinforcement learning\u201d seems to be over-claiming. I understand it can be applicable but the paper has not really done the work to show this outside the supervised learning setting. Minor comments: Pg. 4: comperable -> comparable", "rating": "3: Weak Reject", "reply_text": "> The GAN experiments shows the GAN loss makes GTN realistic ( as expected ) but there are no quantitative results on mode collapse . That is true , which is why we described them merely \u201c encouraging initial results \u201d and put them in the supplementary material instead of the main text . We do not think the GAN results are the main focus of the paper , but we could try to expand them ( or cut them entirely ) if you feel that is necessary and a better solution than what we do currently , which is simply letting the reader know about an intriguing possibility for future work . ( Another interesting experiment would be to show how adding a GAN loss for generating data affects the test performance of the method . ) Nice idea ! We actually tried this at various times over the course of the research project . In general it hurts results . We think the reason is because GANs are incentivized to produce images that look like * real * images . Adding that realism constraint prevents the GTN from producing images that teach many concepts at once ( e.g.about many different forms a 7 can take ) , but look unrealistic . Of course , that is just a hypothesis for why performance tended to be worse . We did not do systematic experiments , and GANs can be finicky ( and there are many forms of them at this point ) which is why we did not include the results in the paper . However , if you would like us to add them , we would be happy to . Please let us know . Perhaps it would benefit the paper to narrow in on supervised learning ? Given that these final experiments are not polished , the claim in the abstract that the method is \u201c a general approach that is applicable to supervised , unsupervised , and reinforcement learning \u201d seems to be over-claiming . I understand it can be applicable but the paper has not really done the work to show this outside the supervised learning setting . We made the following change . Does this change address your concern ? We are trying to balance between letting people know about the generality of the idea , but also making it clear that here we have only focused on the supervised case . Old : a general approach that is applicable to supervised , unsupervised , and reinforcement learning . New : a general approach that is , in theory , applicable to supervised , unsupervised , and reinforcement learning , although our experiments only focus on the supervised case . > Pg.4 : comperable - > comparable Fixed Thank you again for the constructive feedback ! Please let us know if there are any other concerns you would like us to address . We hope you will consider increasing your score ."}, {"review_id": "HJg_ECEKDr-1", "review_text": "Summary: The paper proposes Generative Teaching Networks, which aims to generate synthetic training data for a given prediction problem. The authors demonstrate its use in an MNIST prediction task and a neural architecture search task on Cifar10. I do not find the idea compelling nor the empirical idea convincing enough to warrant acceptance at ICLR. Detailed Comments: At a high level, the motivation for data generation in order to improve a given prediction problem is not clear. From a statistical perspective, one can only do so well given a certain amount of training data, and being able to generate new data would suggest that one can do arbitrarily better by simply creating more data -- this is not true. While data augmentation techniques have improved accuracy in many cases, they have also relied heavily on domain knowledge about the problem, such as mirroring, cropping for images. The proposed GTN model does not seem to incorporate such priors and I would be surprised that one can do better with such synthetically generated data. Indeed, the proposed approach does not do better than the best performing models on MNIST. The authors use GTNs in a NAS problem where they use the accuracy on the generated images as a proxy for the validation accuracy. As figure 4c illustrates there actually does not seem to be much correlation between the accuracies on the synthetic and real datasets. While Table 1 indicates that they outperform some baselines, I do not find them compelling. This could simply be because random search is a coarse optimization method (and hence the proposed metric may not do well on more sophisticated search techniques). - On a side note, why is evaluating on the synthetic images cheaper than evaluating on the original images? - What is the rank-correlation metric used? Did you try more standard correlation metrics such as Pearson's coefficient? ================= Post rebuttal Having read the rebuttal, the comments from other reviewers, and the updated manuscript, I am more positive about the paper now. I agree that with reviewer 2 that the proposed approach is interesting and could be a method to speed up NAS in new domains. I have upgraded my score to reflect this. My only remaining issue is that the authors should have demonstrated this on new datasets (by running other methods on these datasets) instead of sticking to the same old datasets. However, this is the standard practice in the NAS literature today.", "rating": "6: Weak Accept", "reply_text": "> On a side note , why is evaluating on the synthetic images cheaper than evaluating on the original images ? Image for image , they are equally fast . What our results show , however , is that because the GTN-produced data is trained to lead to fast learning , we need to train on far less of it to achieve the same performance as real training . Specifically , we can achieve equal performance with ~4x fewer SGD steps by using GTN-data vs. real data ( Figure 4a ) . More importantly , our approach is trying to , as quickly as possible , estimate the asymptotic performance of an architecture when trained on real data . To do so , we train on GTN-data for a very low number of SGD steps ( 128 ) . We show that GTNs produces an equally accurate rank-correlation in architectures ( i.e.using the proxy to estimate their asymptotic-performance ordering from best to worst ) just as well as when training with real data , but using 9x fewer SGD steps . That provides significant compute savings . On our development computer , for example , it takes about 30 seconds per architecture for our approach vs. 270 seconds with real data . > What is the rank-correlation metric used ? Did you try more standard correlation metrics such as Pearson 's coefficient ? We use Spearman \u2019 s rank-correlation because NAS needs only to know the ranks of the networks , not their absolute performance . Spearman \u2019 s rank-correlation is in effect the rank-based version of Pearson \u2019 s coefficient ."}, {"review_id": "HJg_ECEKDr-2", "review_text": "This paper proposes an algorithm for generating training data to help other machine learning agents learn faster. The proposed Generative Teaching Networks (GTNs) are networks that are trained to generate training data for other networks and are trained jointly with these other networks by back propagating through the entire learning problems via meta-gradients. They also show how weight normalization can help stabilize the training of GTNs. The paper is well-written overall and easy to follow, except for a few typos that can be fixed for the camera ready. The main idea of the paper is quite simple and it\u2019s nice to see it works well. I\u2019m actually surprised it has not been proposed before, but I am also not very familiar with this research area. For these reasons, I lean towards accepting this paper, although I have a few comments that I would like to see addressed for the camera ready version. The authors present experiments where they apply GTNs on image classification and neural architecture search. GTN does indeed seem to do better than the baselines for these problems. However, for MNIST and CIFAR it looks like the models being used may not be that good, as it\u2019s quite easy to obtain better performance than the results shown in the paper. I would be curious to know why the authors did not use a better architecture and also what the architecture they used actually is. I am unfortunately not familiar with neural architecture search to be able to evaluate their experimental setup in that case. Regarding the curriculum used in Section 3.2, my understanding is that the full curriculum approach has an additional computational cost vs the no curriculum approach, as you have to learn that curriculum. Thus, even though the result the authors present is interesting and verifies that GTNs learn a useful curriculum, I would also like to see curves of how accuracy improves per computational unit (e.g., the horizontal axis could be CPU training time). This would allow us to see whether learning a curriculum this way is in fact practically useful. It may just as well be that it is too expensive and training without it is faster. The authors show example images generated by GTNs and, as they also mention, these images do not look very realistic. It would be good to have some explanation/analysis around this. Could it be that these are images that are \u201chard\u201d for the classifier? (e.g., thinking in terms of support vector machines, do these images lie in or close to the margin of the classifier?). I would love to seem an analysis around this and a couple of proposed explanations. I would also like to see more details on the actual architecture used for the experiments as I feel that the paper does not provide enough information to reproduce the results.", "rating": "6: Weak Accept", "reply_text": "Thank you for your valuable and constructive feedback . We are glad you think our paper is well written . We will make all suggested changes to improve it . > The authors present experiments where they apply GTNs on image classification and neural architecture search . GTN does indeed seem to do better than the baselines for these problems . However , for MNIST and CIFAR it looks like the models being used may not be that good , as it \u2019 s quite easy to obtain better performance than the results shown in the paper . I would be curious to know why the authors did not use a better architecture and also what the architecture they used actually is . I am unfortunately not familiar with neural architecture search to be able to evaluate their experimental setup in that case . Please see our # mainIssue response in the reply to all reviewers . > Regarding the curriculum used in Section 3.2 , my understanding is that the full curriculum approach has an additional computational cost vs the no curriculum approach , as you have to learn that curriculum . Thus , even though the result the authors present is interesting and verifies that GTNs learn a useful curriculum , I would also like to see curves of how accuracy improves per computational unit ( e.g. , the horizontal axis could be CPU training time ) . This would allow us to see whether learning a curriculum this way is in fact practically useful . It may just as well be that it is too expensive and training without it is faster . The curriculum version does not have any additional cost because the curriculum is learned concurrently with the rest of the system ( in fact , it \u2019 s slightly cheaper to train , as described next ) . The only difference ( during meta-training and meta-testing ) is that in the no-curriculum version we sample a Z code from a Gaussian and pass that as input to the GTN and , in the curriculum version , we directly train a series of Z codes that are sequentially passed to the GTN over the N inner-loop steps . The extra compute to update that Z-code block is negligible . We will clarify in the paper that a single meta-iteration with GTN costs virtually the same amount of computation regardless of the curriculum-type ( as does inference ) . > The authors show example images generated by GTNs and , as they also mention , these images do not look very realistic . It would be good to have some explanation/analysis around this . Could it be that these are images that are \u201c hard \u201d for the classifier ? ( e.g. , thinking in terms of support vector machines , do these images lie in or close to the margin of the classifier ? ) . I would love to seem an analysis around this and a couple of proposed explanations . You raise an interesting hypothesis that we have considered ( amongst many ) . We have some ideas for experiments we can do to try to shed light on this issue broadly . We will actively begin them and report back within the discussion window . One possibility ( if none of our experiments are conclusive ) is that in the discussion we could list all of the different hypotheses we have come up with that might explain this phenomenon and say that investigating them is an interesting area for future work . Would you like that approach ? > I would also like to see more details on the actual architecture used for the experiments as I feel that the paper does not provide enough information to reproduce the results . All architectures used are described in SI Appendix A , B , and C. We will update the main text to better emphasize where these details are located . We will also release our code and trained models to ensure reproducible results . Thank you again for your valuable and constructive feedback . Please let us know if there are any additional changes you would like to see made ."}], "0": {"review_id": "HJg_ECEKDr-0", "review_text": "This paper proposes a meta-learning algorithm Generative Teaching Networks (GTN) to generate fake training data for models to learn more accurate models. In the inner loop, a generator produces training data and the learner takes gradient steps on this data. In the outer loop, the parameters of the generator are updated by evaluating the learner on real data and differentiating through the gradient steps of the inner loop. The main claim is this method is shown to give improvements in performance on supervised learning for MNIST and CIFAR10. They also suggest weight normalization patches up instability issues with meta-learning and evaluate this in the supervised learning setting, and curriculum learning for GTNs. To me, the main claim is very surprising and counter-intuitive - it is not clear where the extra juice is coming from, as the algorithm does not assume any extra information. The actual results I believe do not bear out this claim because the actual results on MNIST and CIFAR10 are significantly below state of the art. On MNIST, GTN achieves about 98% accuracy and the baseline \u201cReal Data\u201d achieves <97% accuracy, while the state of the art is about 99.7% and well-tuned convnets without any pre-processing or fancy extras achieve about 99% according to Yann LeCunn\u2019s website. The disparity on CIFAR seems to be less egregious but the state of the art stands at 99% while the best GTN model (without cutout) achieves about 96.2% which matches good convnets and is slightly worse than neural architecture search according to https://paperswithcode.com/sota/image-classification-on-cifar-10. This does not negate the potential of GTNs which I feel are an interesting approach, but I believe the paper should be more straightforward with the presentation of these results. The current results basically show that GTNs improve the performance of learners with bad hyper-parameters. On problems that are not as well-studied as MNIST or CIFAR10 this could still be very valuable (as we do not know what performance is good or bad in advance). Based on the results, GTN does seem to be a significant step forward in synthetic data generation for learning compared to prior work (Zhang 2018, Luo 2018). The paper proposes two other contributions: using weight normalization for meta-learning and curriculum learning for GTNs. Weight normalization is shown to stabilize GTNs on MNIST. I think the paper oversteps in the relevant method section, hypothesizing it may stabilize meta-learning more broadly. The paper should present a wider set of experiments to make this claim convincing. But the point for GTNs on MNIST nevertheless stands. For curriculum learning: the description of the method is done across section 2 and section 3.2 and does not really describe it completely. How exactly are the samples chosen in GTN - All Shuffled? How does GTN - Full Curriculum and Shuffled Batch parametrize the order of the samples so that it can be learned? I suggest that this information is all included as a subsection in the method (section 2). The results seem to show the learned curriculum is superior to no curriculum. At a high level it would be very surprising to me if the way forward for better discriminative models was to learn good generative models and use them again for training discriminative models, simply because discriminative models have proved thus far significantly easier to train. If this work does eventually show this result, it would be a very interesting result. At the moment, I believe it does not, but I would be happy to change my mind if the authors provide convincing evidence. Alternatively, I feel that the paper could be a valuable contribution to the community if the writing is toned down to focus on the contributions, presents the results comparing to well-tuned hyperparameters and not over-claim. More comments: What is the outer loop loss function? Is it assumed to be the same as the inner one (but using real data instead of training data)? I think this should be made explicit in the method section. There are some additional experiments in other settings such as RL and unsupervised learning. Both seem like quite interesting directions but seem like preliminary experiments that don\u2019t work convincingly yet. The RL experiment shows that using GTN does not change performance much. There is a claim about optimizing randomly initialized networks at each step, but the baseline which uses randomly initialized networks at each step with A2C is missing. The GAN experiments shows the GAN loss makes GTN realistic (as expected) but there are no quantitative results on mode collapse. (Another interesting experiment would be to show how adding a GAN loss for generating data affects the test performance of the method.) Perhaps it would benefit the paper to narrow in on supervised learning? Given that these final experiments are not polished, the claim in the abstract that the method is \u201ca general approach that is applicable to supervised, unsupervised, and reinforcement learning\u201d seems to be over-claiming. I understand it can be applicable but the paper has not really done the work to show this outside the supervised learning setting. Minor comments: Pg. 4: comperable -> comparable", "rating": "3: Weak Reject", "reply_text": "> The GAN experiments shows the GAN loss makes GTN realistic ( as expected ) but there are no quantitative results on mode collapse . That is true , which is why we described them merely \u201c encouraging initial results \u201d and put them in the supplementary material instead of the main text . We do not think the GAN results are the main focus of the paper , but we could try to expand them ( or cut them entirely ) if you feel that is necessary and a better solution than what we do currently , which is simply letting the reader know about an intriguing possibility for future work . ( Another interesting experiment would be to show how adding a GAN loss for generating data affects the test performance of the method . ) Nice idea ! We actually tried this at various times over the course of the research project . In general it hurts results . We think the reason is because GANs are incentivized to produce images that look like * real * images . Adding that realism constraint prevents the GTN from producing images that teach many concepts at once ( e.g.about many different forms a 7 can take ) , but look unrealistic . Of course , that is just a hypothesis for why performance tended to be worse . We did not do systematic experiments , and GANs can be finicky ( and there are many forms of them at this point ) which is why we did not include the results in the paper . However , if you would like us to add them , we would be happy to . Please let us know . Perhaps it would benefit the paper to narrow in on supervised learning ? Given that these final experiments are not polished , the claim in the abstract that the method is \u201c a general approach that is applicable to supervised , unsupervised , and reinforcement learning \u201d seems to be over-claiming . I understand it can be applicable but the paper has not really done the work to show this outside the supervised learning setting . We made the following change . Does this change address your concern ? We are trying to balance between letting people know about the generality of the idea , but also making it clear that here we have only focused on the supervised case . Old : a general approach that is applicable to supervised , unsupervised , and reinforcement learning . New : a general approach that is , in theory , applicable to supervised , unsupervised , and reinforcement learning , although our experiments only focus on the supervised case . > Pg.4 : comperable - > comparable Fixed Thank you again for the constructive feedback ! Please let us know if there are any other concerns you would like us to address . We hope you will consider increasing your score ."}, "1": {"review_id": "HJg_ECEKDr-1", "review_text": "Summary: The paper proposes Generative Teaching Networks, which aims to generate synthetic training data for a given prediction problem. The authors demonstrate its use in an MNIST prediction task and a neural architecture search task on Cifar10. I do not find the idea compelling nor the empirical idea convincing enough to warrant acceptance at ICLR. Detailed Comments: At a high level, the motivation for data generation in order to improve a given prediction problem is not clear. From a statistical perspective, one can only do so well given a certain amount of training data, and being able to generate new data would suggest that one can do arbitrarily better by simply creating more data -- this is not true. While data augmentation techniques have improved accuracy in many cases, they have also relied heavily on domain knowledge about the problem, such as mirroring, cropping for images. The proposed GTN model does not seem to incorporate such priors and I would be surprised that one can do better with such synthetically generated data. Indeed, the proposed approach does not do better than the best performing models on MNIST. The authors use GTNs in a NAS problem where they use the accuracy on the generated images as a proxy for the validation accuracy. As figure 4c illustrates there actually does not seem to be much correlation between the accuracies on the synthetic and real datasets. While Table 1 indicates that they outperform some baselines, I do not find them compelling. This could simply be because random search is a coarse optimization method (and hence the proposed metric may not do well on more sophisticated search techniques). - On a side note, why is evaluating on the synthetic images cheaper than evaluating on the original images? - What is the rank-correlation metric used? Did you try more standard correlation metrics such as Pearson's coefficient? ================= Post rebuttal Having read the rebuttal, the comments from other reviewers, and the updated manuscript, I am more positive about the paper now. I agree that with reviewer 2 that the proposed approach is interesting and could be a method to speed up NAS in new domains. I have upgraded my score to reflect this. My only remaining issue is that the authors should have demonstrated this on new datasets (by running other methods on these datasets) instead of sticking to the same old datasets. However, this is the standard practice in the NAS literature today.", "rating": "6: Weak Accept", "reply_text": "> On a side note , why is evaluating on the synthetic images cheaper than evaluating on the original images ? Image for image , they are equally fast . What our results show , however , is that because the GTN-produced data is trained to lead to fast learning , we need to train on far less of it to achieve the same performance as real training . Specifically , we can achieve equal performance with ~4x fewer SGD steps by using GTN-data vs. real data ( Figure 4a ) . More importantly , our approach is trying to , as quickly as possible , estimate the asymptotic performance of an architecture when trained on real data . To do so , we train on GTN-data for a very low number of SGD steps ( 128 ) . We show that GTNs produces an equally accurate rank-correlation in architectures ( i.e.using the proxy to estimate their asymptotic-performance ordering from best to worst ) just as well as when training with real data , but using 9x fewer SGD steps . That provides significant compute savings . On our development computer , for example , it takes about 30 seconds per architecture for our approach vs. 270 seconds with real data . > What is the rank-correlation metric used ? Did you try more standard correlation metrics such as Pearson 's coefficient ? We use Spearman \u2019 s rank-correlation because NAS needs only to know the ranks of the networks , not their absolute performance . Spearman \u2019 s rank-correlation is in effect the rank-based version of Pearson \u2019 s coefficient ."}, "2": {"review_id": "HJg_ECEKDr-2", "review_text": "This paper proposes an algorithm for generating training data to help other machine learning agents learn faster. The proposed Generative Teaching Networks (GTNs) are networks that are trained to generate training data for other networks and are trained jointly with these other networks by back propagating through the entire learning problems via meta-gradients. They also show how weight normalization can help stabilize the training of GTNs. The paper is well-written overall and easy to follow, except for a few typos that can be fixed for the camera ready. The main idea of the paper is quite simple and it\u2019s nice to see it works well. I\u2019m actually surprised it has not been proposed before, but I am also not very familiar with this research area. For these reasons, I lean towards accepting this paper, although I have a few comments that I would like to see addressed for the camera ready version. The authors present experiments where they apply GTNs on image classification and neural architecture search. GTN does indeed seem to do better than the baselines for these problems. However, for MNIST and CIFAR it looks like the models being used may not be that good, as it\u2019s quite easy to obtain better performance than the results shown in the paper. I would be curious to know why the authors did not use a better architecture and also what the architecture they used actually is. I am unfortunately not familiar with neural architecture search to be able to evaluate their experimental setup in that case. Regarding the curriculum used in Section 3.2, my understanding is that the full curriculum approach has an additional computational cost vs the no curriculum approach, as you have to learn that curriculum. Thus, even though the result the authors present is interesting and verifies that GTNs learn a useful curriculum, I would also like to see curves of how accuracy improves per computational unit (e.g., the horizontal axis could be CPU training time). This would allow us to see whether learning a curriculum this way is in fact practically useful. It may just as well be that it is too expensive and training without it is faster. The authors show example images generated by GTNs and, as they also mention, these images do not look very realistic. It would be good to have some explanation/analysis around this. Could it be that these are images that are \u201chard\u201d for the classifier? (e.g., thinking in terms of support vector machines, do these images lie in or close to the margin of the classifier?). I would love to seem an analysis around this and a couple of proposed explanations. I would also like to see more details on the actual architecture used for the experiments as I feel that the paper does not provide enough information to reproduce the results.", "rating": "6: Weak Accept", "reply_text": "Thank you for your valuable and constructive feedback . We are glad you think our paper is well written . We will make all suggested changes to improve it . > The authors present experiments where they apply GTNs on image classification and neural architecture search . GTN does indeed seem to do better than the baselines for these problems . However , for MNIST and CIFAR it looks like the models being used may not be that good , as it \u2019 s quite easy to obtain better performance than the results shown in the paper . I would be curious to know why the authors did not use a better architecture and also what the architecture they used actually is . I am unfortunately not familiar with neural architecture search to be able to evaluate their experimental setup in that case . Please see our # mainIssue response in the reply to all reviewers . > Regarding the curriculum used in Section 3.2 , my understanding is that the full curriculum approach has an additional computational cost vs the no curriculum approach , as you have to learn that curriculum . Thus , even though the result the authors present is interesting and verifies that GTNs learn a useful curriculum , I would also like to see curves of how accuracy improves per computational unit ( e.g. , the horizontal axis could be CPU training time ) . This would allow us to see whether learning a curriculum this way is in fact practically useful . It may just as well be that it is too expensive and training without it is faster . The curriculum version does not have any additional cost because the curriculum is learned concurrently with the rest of the system ( in fact , it \u2019 s slightly cheaper to train , as described next ) . The only difference ( during meta-training and meta-testing ) is that in the no-curriculum version we sample a Z code from a Gaussian and pass that as input to the GTN and , in the curriculum version , we directly train a series of Z codes that are sequentially passed to the GTN over the N inner-loop steps . The extra compute to update that Z-code block is negligible . We will clarify in the paper that a single meta-iteration with GTN costs virtually the same amount of computation regardless of the curriculum-type ( as does inference ) . > The authors show example images generated by GTNs and , as they also mention , these images do not look very realistic . It would be good to have some explanation/analysis around this . Could it be that these are images that are \u201c hard \u201d for the classifier ? ( e.g. , thinking in terms of support vector machines , do these images lie in or close to the margin of the classifier ? ) . I would love to seem an analysis around this and a couple of proposed explanations . You raise an interesting hypothesis that we have considered ( amongst many ) . We have some ideas for experiments we can do to try to shed light on this issue broadly . We will actively begin them and report back within the discussion window . One possibility ( if none of our experiments are conclusive ) is that in the discussion we could list all of the different hypotheses we have come up with that might explain this phenomenon and say that investigating them is an interesting area for future work . Would you like that approach ? > I would also like to see more details on the actual architecture used for the experiments as I feel that the paper does not provide enough information to reproduce the results . All architectures used are described in SI Appendix A , B , and C. We will update the main text to better emphasize where these details are located . We will also release our code and trained models to ensure reproducible results . Thank you again for your valuable and constructive feedback . Please let us know if there are any additional changes you would like to see made ."}}