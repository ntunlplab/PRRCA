{"year": "2017", "forum": "ry4Vrt5gl", "title": "Learning to Optimize", "decision": "Accept (Poster)", "meta_review": "The authors propose an approach to learning optimization algorithms by framing the problem as a policy search task, then using the guided policy search algorithm. The method is a nice contribution to the \"learning to learn\" framework, and actually was developed simultaneously to a few papers that have since already been published. It's definitely a useful addition to this space.\n \n The biggest issue with this paper is that the results simply aren't that compelling. The methodology and proposed approach is nice, and the text is improved upon a previous version posted to Arxiv, but it seems that for most problems the results aren't that much better than some of the more common off-the-shelf optimization approaches that _don't_ require learning anything. Furthermore, in the one domain where the method does seem to (marginally) outperform the other methods, the neural net domain, it's unclear why we'd want to use the proposed approaches instead of SGD-based methods and their like (which of course everyone actually does for optimization).\n \n Pros:\n + Nice contribution to the learning to learn framework\n + Takes a different approach from past (concurrent) work, namely one based upon policy search\n \n Cons:\n - Experiments are not particularly compelling\n \n Overall, this work is a little borderline. Still, the PCs have determined that it was deserving of appearing at the conference. We hope the authors can strengthen the empirical validation for the camera ready version.", "reviews": [{"review_id": "ry4Vrt5gl-0", "review_text": "The current version of the paper is improved w.r.t. the original arXiv version from June. While the results are exactly the same, the text does not oversell them as much as before. You may also consider to avoid words like \"mantra\", etc. I believe that my criticism given in my comment from 3 Dec 2016 about \"randomly generated task\" is valid and you answer is not.", "rating": "7: Good paper, accept", "reply_text": "We are not sure what your criticism is exactly , but the section it pertains to is not the main point we wanted to make in our paper . Your point might be valid ; hopefully we 'll get a chance to discuss it further in person sometime ."}, {"review_id": "ry4Vrt5gl-1", "review_text": " This papers adds to the literature on learning optimizers/algorithms that has gained popularity recently. The authors choose to use the framework of guided policy search at the meta-level to train the optimizers. They also opt to train on random objectives and assess transfer to a few simple tasks. As pointed below, this is a useful addition. However, the argument of using RL vs gradients at the meta-level that appears below is not clear or convincing. I urge the authors to run an experiment comparing the two approaches and to present comparative results. This is a very important question, and the scalability of this approach could very well hinge on this fact. Indeed, demonstrating both scaling to large domains and transfer to those domains is the key challenge in this domain. In summary, the idea is a good one, but the experiments are weak. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We note that our comment on the possible advantages of RL over supervised learning was in response to your question asking us to speculate on this subject . Accordingly , the comment conveys our opinion , which is based on the RL literature , e.g . [ 1-4 ] , and was not part of the paper . Of course , to settle this question definitively in this domain , empirical evidence may be needed , which would be an interesting topic for exploration in future work . [ 1 ] Ross , St\u00e9phane , and Drew Bagnell . `` Efficient Reductions for Imitation Learning . '' AISTATS.2010 . [ 2 ] Pomerleau , Dean A . `` ALVINN : An Autonomous Land Vehicle in a Neural Network . '' NIPS.1989 . [ 3 ] Levine , Sergey , and Vladlen Koltun . `` Learning Complex Neural Network Policies with Trajectory Optimization . '' ICML.2014 . [ 4 ] Venkatraman , Arun , Martial Hebert , and J. Andrew Bagnell . `` Improving Multi-Step Prediction of Learned Time Series Models . '' AAAI.2015 ."}, {"review_id": "ry4Vrt5gl-2", "review_text": "This paper proposes an approach to learning a custom optimizer for a given class optimization problems. I think in the case of training machine learning algorithms, a class would represent a model like \u201clogistic regression\u201d. The authors cleverly cast this as a reinforcement learning problem and use guided policy search to train a neural network to map the current location and history onto a step direction/magnitude. Overall I think this is a great idea and a very nice contribution to the fast growing meta-learning literature. However, I think that there are some aspects that could be touched on to make this a stronger paper. My first thought is that the authors claim to train the method to learn the regularities of an entire class of optimization problems, rather than learning to exploit regularities in a given set of tasks. The distinction here is not terribly clear to me. For example, in learning an optimizer for logistic regression, the authors seem to claim that learning on a randomly sampled set of logistic regression problems will allow the model to learn about logistic regression itself. I am not convinced of this, because there is bias in the randomly sampled data itself. From the paper in this case, \u201cThe instances are drawn randomly from two multivariate Gaussians with random means and covariances, with half drawn from each.\u201d It seems the optimizer is then trained to optimize instances of logistic regression *given this specific family of training inputs* and not logistic regression problems in general. A simple experiment to prove the method works more generally would be to repeat the existing experiments, but where the test instances are drawn from a completely different distribution. It would be even more interesting to see how this changes as the test distribution deviates further from the training distribution. Can the authors comment on the choice of architecture used here? Why one layer with 50 hidden units and softplus activations specifically? Why not e.g., 100 units, 2 layers and ReLUs? Presumably this is to prevent overfitting, but given the limited capacity of the network, how do these results look when the dimensionality of the input space increases beyond 2 or 3? I would love to see what kind of policy the network learns on e.g., a 2D function using a contour plot. What do the steps look like on a random problem instance when compared to other hand-engineered optimizers? Overall I think this a really interesting paper with a great methodological contribution. My main concern is that the results may be oversold as the problems are still relatively simple and constrained. However, if the authors can demonstrate that this approach produces robust policies for a very general set of problems then that would be truly spectacular. Minor notes below. Section 3.1 should you be using \\pi_T^* to denote the optimal policy? You use \\pi_t^* and \\pi^* currently. Are the problems here considered noiseless? That is, is the state transition given an action deterministic? It would be very interesting to see this on noisy problems. ", "rating": "7: Good paper, accept", "reply_text": "As per your suggestion , we tested the learned neural net optimizer on problems with different data distributions . Specifically , we considered classification problems on datasets drawn from mixtures of different numbers of random Gaussians ( so that as the number of mixture components increases , the data distributions at test time deviate more and more from those used for training ) . We found the learned optimizer to be fairly robust and appeared to work well for various numbers of mixture components we tested . We have updated the paper to include these results ( see the appendix in the updated paper ) . We tried increasing the number of layers and increasing the number of units in a layer . We found that the former resulted in overfitting and the latter resulted in similar performance . For higher dimensional problems , we found that a similar number of hidden units also suffices . We conjecture that the number of hidden units depends more on how complex the geometry of the objective function is rather than how high the dimensionality of the input is . Indeed , if one were to consider the update rules of most existing optimization algorithms , the complexity of the update rule does not change with the dimensionality . Thanks for the suggestion regarding contour plots . We have added them for random 2D logistic regression problems ( see section 6.4 of the updated paper ) . Good catch \u2013 \\pi_t^ * should have been \\pi^ * . The state transition given an action is not deterministic because the next state contains the gradient and the objective value at the next iterate , which can not be determined solely from the current state and action ."}], "0": {"review_id": "ry4Vrt5gl-0", "review_text": "The current version of the paper is improved w.r.t. the original arXiv version from June. While the results are exactly the same, the text does not oversell them as much as before. You may also consider to avoid words like \"mantra\", etc. I believe that my criticism given in my comment from 3 Dec 2016 about \"randomly generated task\" is valid and you answer is not.", "rating": "7: Good paper, accept", "reply_text": "We are not sure what your criticism is exactly , but the section it pertains to is not the main point we wanted to make in our paper . Your point might be valid ; hopefully we 'll get a chance to discuss it further in person sometime ."}, "1": {"review_id": "ry4Vrt5gl-1", "review_text": " This papers adds to the literature on learning optimizers/algorithms that has gained popularity recently. The authors choose to use the framework of guided policy search at the meta-level to train the optimizers. They also opt to train on random objectives and assess transfer to a few simple tasks. As pointed below, this is a useful addition. However, the argument of using RL vs gradients at the meta-level that appears below is not clear or convincing. I urge the authors to run an experiment comparing the two approaches and to present comparative results. This is a very important question, and the scalability of this approach could very well hinge on this fact. Indeed, demonstrating both scaling to large domains and transfer to those domains is the key challenge in this domain. In summary, the idea is a good one, but the experiments are weak. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We note that our comment on the possible advantages of RL over supervised learning was in response to your question asking us to speculate on this subject . Accordingly , the comment conveys our opinion , which is based on the RL literature , e.g . [ 1-4 ] , and was not part of the paper . Of course , to settle this question definitively in this domain , empirical evidence may be needed , which would be an interesting topic for exploration in future work . [ 1 ] Ross , St\u00e9phane , and Drew Bagnell . `` Efficient Reductions for Imitation Learning . '' AISTATS.2010 . [ 2 ] Pomerleau , Dean A . `` ALVINN : An Autonomous Land Vehicle in a Neural Network . '' NIPS.1989 . [ 3 ] Levine , Sergey , and Vladlen Koltun . `` Learning Complex Neural Network Policies with Trajectory Optimization . '' ICML.2014 . [ 4 ] Venkatraman , Arun , Martial Hebert , and J. Andrew Bagnell . `` Improving Multi-Step Prediction of Learned Time Series Models . '' AAAI.2015 ."}, "2": {"review_id": "ry4Vrt5gl-2", "review_text": "This paper proposes an approach to learning a custom optimizer for a given class optimization problems. I think in the case of training machine learning algorithms, a class would represent a model like \u201clogistic regression\u201d. The authors cleverly cast this as a reinforcement learning problem and use guided policy search to train a neural network to map the current location and history onto a step direction/magnitude. Overall I think this is a great idea and a very nice contribution to the fast growing meta-learning literature. However, I think that there are some aspects that could be touched on to make this a stronger paper. My first thought is that the authors claim to train the method to learn the regularities of an entire class of optimization problems, rather than learning to exploit regularities in a given set of tasks. The distinction here is not terribly clear to me. For example, in learning an optimizer for logistic regression, the authors seem to claim that learning on a randomly sampled set of logistic regression problems will allow the model to learn about logistic regression itself. I am not convinced of this, because there is bias in the randomly sampled data itself. From the paper in this case, \u201cThe instances are drawn randomly from two multivariate Gaussians with random means and covariances, with half drawn from each.\u201d It seems the optimizer is then trained to optimize instances of logistic regression *given this specific family of training inputs* and not logistic regression problems in general. A simple experiment to prove the method works more generally would be to repeat the existing experiments, but where the test instances are drawn from a completely different distribution. It would be even more interesting to see how this changes as the test distribution deviates further from the training distribution. Can the authors comment on the choice of architecture used here? Why one layer with 50 hidden units and softplus activations specifically? Why not e.g., 100 units, 2 layers and ReLUs? Presumably this is to prevent overfitting, but given the limited capacity of the network, how do these results look when the dimensionality of the input space increases beyond 2 or 3? I would love to see what kind of policy the network learns on e.g., a 2D function using a contour plot. What do the steps look like on a random problem instance when compared to other hand-engineered optimizers? Overall I think this a really interesting paper with a great methodological contribution. My main concern is that the results may be oversold as the problems are still relatively simple and constrained. However, if the authors can demonstrate that this approach produces robust policies for a very general set of problems then that would be truly spectacular. Minor notes below. Section 3.1 should you be using \\pi_T^* to denote the optimal policy? You use \\pi_t^* and \\pi^* currently. Are the problems here considered noiseless? That is, is the state transition given an action deterministic? It would be very interesting to see this on noisy problems. ", "rating": "7: Good paper, accept", "reply_text": "As per your suggestion , we tested the learned neural net optimizer on problems with different data distributions . Specifically , we considered classification problems on datasets drawn from mixtures of different numbers of random Gaussians ( so that as the number of mixture components increases , the data distributions at test time deviate more and more from those used for training ) . We found the learned optimizer to be fairly robust and appeared to work well for various numbers of mixture components we tested . We have updated the paper to include these results ( see the appendix in the updated paper ) . We tried increasing the number of layers and increasing the number of units in a layer . We found that the former resulted in overfitting and the latter resulted in similar performance . For higher dimensional problems , we found that a similar number of hidden units also suffices . We conjecture that the number of hidden units depends more on how complex the geometry of the objective function is rather than how high the dimensionality of the input is . Indeed , if one were to consider the update rules of most existing optimization algorithms , the complexity of the update rule does not change with the dimensionality . Thanks for the suggestion regarding contour plots . We have added them for random 2D logistic regression problems ( see section 6.4 of the updated paper ) . Good catch \u2013 \\pi_t^ * should have been \\pi^ * . The state transition given an action is not deterministic because the next state contains the gradient and the objective value at the next iterate , which can not be determined solely from the current state and action ."}}