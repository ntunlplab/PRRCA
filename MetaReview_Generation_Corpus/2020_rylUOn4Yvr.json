{"year": "2020", "forum": "rylUOn4Yvr", "title": "ROBUST DISCRIMINATIVE REPRESENTATION LEARNING VIA GRADIENT RESCALING: AN EMPHASIS REGULARISATION PERSPECTIVE", "decision": "Reject", "meta_review": "The paper proposes a gradient rescaling method to make deep neural network training more robust to label noise. The intuition of focusing more on easier examples is not particularly new, but empirical results are promising. On the weak side, no theoretical justification is provided, and the method introduces extra hyperparameters that need to be tuned. Finally, more discussions on recent SOTA methods (e.g., Lee et al. 2019) as well as further comprehensive evaluations on various cases, such as asymmetric label noise, semantic label noise, and open-set label noise, would be needed to justify and demonstrate the effectiveness of the proposed method. ", "reviews": [{"review_id": "rylUOn4Yvr-0", "review_text": "Summary: The paper proposes a method for noise robustness based on scaling gradients of examples. By choosing the proper scaling parameters (alpha and beta), the method recovers standard losses such as CCE, MAE, and GCE, while also recovering other losses. The method is strongly related to reweighting training examples, where alpha and beta define the shape of this weighting as a function of the model's prediction (i.e., p_i). Experiments show that the proposed method achieves competitive results on several standard benchmarks for noisy-labelled data. Comments: - The main drawback of the proposed approach is that there is no clear way of choosing alpha and beta, other than a hyper-parameter search, which is not very practical and can lead to overfitting the test set. - The paper in general is easy to follow, but the paper is not very rigorous or clear on some important concepts. For example: * No clear mathematical definition of emphasis focus and spread * The term \"semantically abnormal examples\" should be defined in the main text. * It is not so clear what it means to \"babyset\" emphasis focus and spread. * I don't understand what Eq. 6 is supposed to tell. * What are the \\dots in equation - The experiments are very thorough and the results are very good, but I have few clarifying questions: * The procedure for choosing beta/gamma is not clear, and I see that for every experiment those values change. * It would be nice if the CIFAR-10 Table 4 results are performed using the exact same setting to prior work to make sure the comparison is fair. For example, GCE results in (Zhang & Sabuncu 2018) are much that the reported ones. While it seems that you're using GoogLeNet V1 architecture similar to Jiang et al. 2018, it's not clear which experimental setting you are comparing against. * Can you be more specific what do you mean by \"with a little effort for optimizing beta and gamma\" in caption of Table 5? Minor: * Grammer mistake: \"what training examples...focused *on*...\" * Citations should be done with parentheses ", "rating": "6: Weak Accept", "reply_text": "1.The main drawback of the proposed approach is that there is no clear way of choosing alpha and beta , other than a hyper-parameter search , which is not very practical and can lead to overfitting the test set . 2.The experiments are very thorough and the results are very good , but I have few clarifying questions : 2.1 . The procedure for choosing beta/gamma is not clear , and I see that for every experiment those values change . 2.2.Can you be more specific what do you mean by `` with a little effort for optimizing beta and gamma '' in caption of Table 5 ? We reply to questions 1 and 2 together as they are related . - Reply to 1 & 2.1 on \u201c the procedure for choosing beta/lambda \u201d : We have summarised the procedure and principle for choosing $ \\lambda , \\beta $ in the empirical analysis part , i.e. , Section 4.2.1 . Details are as follows : ( 1 ) Emphasis focus : When noise rate is higher , we can improve a model \u2019 s robustness by moving emphasis focus towards relatively less difficult examples with a larger $ \\lambda $ , which is informative in practice . ( 2 ) When $ \\lambda $ is larger , $ \\beta $ should be larger as shown in Figure 1c . This is also demonstrated in Table 3 , Table 5 , and Table 10 . - Reply to 2.1 on \u201c for every experiment those values change \u201d : In Table 4 and Table 5 , we have displayed the results of GR with fixed parameters $ ( \\beta=8 , \\lambda=0.5 ) $ . Those results are better than the state-of-the-art . - Reply to 2.2 : Those empirical results above make it easy to optimise $ \\lambda , \\beta $ in practice . Therefore , we wrote \u201c with a little effort for optimizing $ \\lambda , \\beta $ \u201d . 3.It would be nice if the CIFAR-10 Table 4 results are performed using the exact same setting to prior work to make sure the comparison is fair . For example , GCE results in ( Zhang & Sabuncu 2018 ) are much that the reported ones . While it seems that you 're using GoogLeNet V1 architecture similar to Jiang et al.2018 , it 's not clear which experimental setting you are comparing against . - The research problem we work on is important and popular . Therefore , on different datasets , different baselines along with different net architectures have been evaluated in the literature . For example , some prior work reimplemented baselines with their custom-designed net architectures . The experimental settings , e.g. , choice of net architectures , are not consistent and rigid in different prior work . On the one hand , it is nice since it provides diverse evaluation settings . On the other hand , it presents a challenge to keep the compared baselines consistent when you aim to compare fairly with different types of baselines on different datasets . Especially some prior work designed their own net architectures . - In our experimental setup , we aim to test on different benchmarks using different publicly available net architectures for more comprehensive evaluation : ( 1 ) We choose GoogLeNet V1 on CIFAR-10 following MentorNet ( Jiang et al. , 2018 ) , where most baselines are example reweighting algorithms as shown in Table 4 . ( 2 ) We use ResNet-44 on CIFAR-100 following D2L ( Ma et al. , 2018 ) , where most competitors are specifically designed for addressing label noise as displayed in Table 5 . ( 3 ) The experiments on Clothing 1M using ResNet-50 are consistent in most prior work . We have displayed most related baselines in Table 6 . - We remark that on each dataset , we only report the results of those baselines using the same net architecture for a fair comparison . 4.The paper in general is easy to follow , but the paper is not very rigorous or clear on some important concepts . For example : 4.1 It is not so clear what it means to `` babysit '' emphasis focus and spread . It means that we need to choose emphasis focus and spread properly instead of using the built-in default settings in loss functions . 4.2 I do n't understand what Eq.6 is supposed to tell . We use Eq . ( 6 ) to tell that GR can be independent of empirical loss formulations . Since gradient computation is independent of loss computation , different losses only indicate how far we are away from different minimisation objectives . The supervision information of GR can be controlled independently and straightforwardly . 4.3 & 4.4 No clear mathematical definition of emphasis focus and spread . What are the \\dots in equation ? We have provided mathematical definitions of emphasis focus and spread in our revised version . We have removed those \\cdot in our revised version . 5.Minor : * Grammer mistake : `` what training examples ... focused * on * ... '' * Citations should be done with parentheses Thanks so much for your helpful and careful check . We have revised them in our revised version ."}, {"review_id": "rylUOn4Yvr-1", "review_text": "Summary This paper presents Gradient Rescaling (GR) for robust learning to combat label noise. They propose to treat each data sample with different significance scores: some samples are important to learning, and some examples are insignificant (or even detrimental) to learning. So they desire to weight each samples according to their significance. They propose the notion of emphasis focus (When learning, whether we should put emphasis on learning \u201chard\u201d examples or \u201ceasy\u201d examples) and emphasis spread (the variance of these significance weights). The authors propose that this \u201cdifficulty\u201d of samples are proportional to their network output logit values. The authors examine the analytical forms of the gradients of popular loss functions such as Categorical Cross Entropy, Mean Absolute Error and Generalized Cross Entropy. They find that the formulas for the gradient are of similar family with varying hyperparameters. Authors claim that tweaking these hyperparameters result in tuning the emphasis focus and spread. The authors conduct Experiments on CIFAR10, CIFAR100 with simulated symmetric noise. Also, they conduct experiments on real-world noisy datasets: Clothing 1M dataset and MARS video dataset. The authors claim that the performance of GR exceeds various baselines. Significance/Novelty/Clarity Significance: Low-Medium. The performance increase exhibited in the experiments are a bit underwhelming (when considering the fact that benchmarks of most recent noise-robustness algos such as <Lee et al. 2019 ICML> are missing). Novelty: Medium. The paper is interesting in the sense that the authors integrated (and allegedly generalized) the gradient formulas for several losses into one family, and tried to integrate and tweak their postulation of \u201cEmphasis focus\u201d and \u201cemphasis spread\u201d into the framework. However, the theoretical ground and convincing reasoning for their claim seems a bit lackluster. Clarity: Low. The overall flow of the paper is a bit fuzzy - exhibiting a stream-of-consciousness style flow. Pros and Cons in Detail Pros: 1.The authors try to unify the analytical forms of the gradients of various loss functions into a single family equipped with hyperparameters that control emphasis focus and spread. 2.Conducted experiments show that GR achieved increased performance when compared to the baselines. Cons: My major concern is about tuning newly introduced hyperparameters in practical settings. How can we guarantee to have intact validation set? Can we get any improvement via GR even with corrupted validation set for tuning hyperparameters? 1. The arguments of the authors are grounded in the premise that \u201cdifficult\u201d samples will exhibit small logit values, and \u201ceasy\u201d samples high logit values. 2. No justifications (both theoretical and experimental) are provided on the claim that controlling emphasis focus/spread will result in more robust learning. 3. This algorithm introduces 2 additional hyperparameters that are correlated with each other. This introduces additional labor. 4. By changing the loss function, the outputs of the network might lose its interpretation as a probability distribution. 5. No confidence intervals are shown except for the CIFAR-100 experiment. 6. Experiments are only conducted on vision tasks. 7. The baseline menagerie also changes when the authors change the target dataset. 8. Additional benchmarks of most recent noise-robustness algos such as <Lee et al. 2019 ICML> are required. Questions 1. Is it always the case that \u201cdifficult\u201d samples exhibit small logit values, and \u201ceasy\u201d samples high logit values? 2. If not, GR\u2019s emphasis manipulation might result in neglecting samples containing valuable information. 3. Can GR be used simultaneously with other noise-robust learning methods to further boost the performance? 4. Technically, GR aims to rescale the gradients of the logits. How will it interact with optimizers other than SGD such as Adam? 5. Does GR still work well on small datasets(#points < 5000)? Misc. Comments Page 3-> inside L1 norm, no differentiation sign in the denominator. Around eq 2 and 4: missing derivative symbol w.r.t. z", "rating": "3: Weak Reject", "reply_text": "Thank you for your questions . 1 & 2 : Is it always the case that \u201c difficult \u201d samples exhibit small logit values , and \u201c easy \u201d samples high logit values ? If not , ..... Generally , the answer is yes . As training goes , the premise that semantic anomalies have small classification confidences while normal examples tend to have large classification confidences is indeed our reasonable assumption . Actually , a lot of prior work has demonstrated the reasonability of this premise . In addition , many algorithms have been proposed based on similar premises . Our empirical analysis also supports this premise well . More details are as follows : ( 1 ) Technically , we have also remarked that \u201c we do not design the weighting scheme heuristically from scratch . Instead , it is naturally motivated by the gradient analysis of several loss functions \u201d , which makes GR principally and technically sound . More discussion is provided in Section 3.3 . ( 2 ) In terms of prior work , self-paced learning , e.g. , Self-paced ( Kumar et al. , 2010 ) , and curriculum learning , e.g. , MentorNet ( Jiang et al. , 2018 ) are practical algorithms based on this premise . In addition , as demonstrated in ( Krueger et al. , 2017 ; Arpit et al. , 2017 ) , when severe noise exists , DNNs learn simple meaningful patterns first before memorising abnormal examples . ( 3 ) Finally , in the \u201c emphasis focus \u201d paragraph of Section 1 , we discussed that \u201c It is a common practice to focus on harder instances when training DNNs ( Shrivastava et al. , 2016 ; Lin et al. , 2017 ) . \u201d \u201c When a dataset is clean , it achieves faster convergence and better performance to emphasise on harder examples because they own larger gradient magnitude , which means more information and a larger update step for model \u2019 s parameters. \u201d \u201c However , when severe noise exists , as demonstrated in ( Krueger et al. , 2017 ; Arpit et al. , 2017 ) , DNNs learn simple meaningful patterns first before memorising abnormal ones. \u201d \u201c In other words , anomalies are harder to fit and own larger gradient magnitude in the later stage . Consequently , if we use the default sample weighting in categorical cross entropy ( CCE ) where harder samples obtain higher weights , anomalies tend to be fitted well especially when a network has large enough capacity . That is why we need to move the emphasis focus towards relatively easier ones , which serves as emphasis regularisation. \u201d 3 . Can GR be used simultaneously with other noise-robust learning methods to further boost the performance ? We have tried combining GR with other standard regularisers in Section 4.5 , please see Table 8 . For example , $ \\mathit { Dropout \\ is \\ demonstrated \\ to \\ be \\ a \\ great \\ regulariser \\ against \\ label \\ noise \\ in \\ Arpit \\ et \\ al. , \\ 2017 } $ , \u201c A closer look at memorization in deep networks \u201d . This is also demonstrated in our Table 8 , e.g. , Dropout > Baseline , and Dropout+L2 > L2 . Case 1 : GR can help other regularisation techniques . GR consistently improves the generalisation performance after it is added : GR > Baseline ; GR+L2 > L2 ; GR+Dropout > Dropout ; GR+L2+Dropout > Dropout+L2 . Case 2 : Other regularisation techniques may not help GR . When GR is already applied , adding another regulariser may not lead to better regularisation effect and generalisation performance . For example : Adding Dropout hurts the performance : GR+Dropout < GR ; GR+L2+Dropout < GR+L2 . Adding L2 decay improves the performance : GR+L2 > GR ; GR+L2+Dropout > GR+Dropout . Therefore , we are sorry that there is no deterministic answer for your question . The search and studying space is large when considering the diverse combination options of different regularisers . However , it is worth noting that the interaction over multiple regularisers may not improve the generalisation performance in practice . 5.Does GR still work well on small datasets ( # points < 5000 ) ? To address your concern on this question , we add a Section $ C $ in our supplementary material in the new revised version . Please have a check . Thanks . ( 1 ) .The problem of label noise we study on CIFAR-10 and CIFAR-100 in Section 4.2 is of similar scale . ( 2 ) .We compare GR with other standard regularisers on a small-scale fine-grained visual categorisation problem in Table 9 . The number of training data points is 5,000 in total . This new experiment demonstrates that GR works on small datasets as well ."}, {"review_id": "rylUOn4Yvr-2", "review_text": "Summary: The authors first analyze and answer the question: What training examples should be focused and how large the emphasis spread should be? Then, they proposed the gradient rescaling framework serving as emphasis regularization. Strengths: 1. The paper is well organized except the reference citation (read difficultly) 2. The proposed method is very simple and effective. 3. Experiments show the improvements over SOTA. Weakness: 1. The experiments lack the recent important baseline \"symmetric cross entropy for robust learning with noisy labels, ICCV2019\", which are the current SOTA. Maybe the author should check the above paper and show the results. 2. The experiments are only conducted on symmetric noise. Actually, asymmetric noise is also important. The author should conduct at least some experiments on asymmetric noise. ", "rating": "3: Weak Reject", "reply_text": "Thanks so much for your helpful review . We are glad that you like our simple , effective and principled method . Regarding the weakness points you mentioned , we clarify them as follows . We look forward to further discussion with you . 1.Regarding the recent baseline \u201c symmetric cross entropy for robust learning with noisy labels , ICCV2019 \u201d , we read it before submission but did not compare with it because it was not officially published at that time . Now , we add their results in our revised version . Please check our Tables 5 and 6 , and you will find our method outperforms this recent baseline . Beyond , please check our Section 2.1 , where we discuss and present some remarks on robustness theorems conditioned on symmetric losses and label noise . Our work challenges those robustness theorems , which can promote new thinking . 2.We explain our two reasons for without testing on asymmetric label noise : 1 ) . As we mentioned in Section 4.2 , we follow the prior work \u201c Ma et al.Dimensionality-Driven Learning with Noisy Labels , ICML 2019 \u201d to test only on symmetric label noise as it has been demonstrated in Vahdat ( 2017 ) that it is more challenging than asymmetric noisy labels . Please check \u201c Arash Vahdat . Toward robustness against label noise in training deep discriminative neural networks . NeurIPS , 2017. \u201d 2 ) . We spend more effort and space on experimental analysis and more complex and valuable real-world applications , e.g. , image classification on Clothing 1M and video retrieval on MARS . $ \\mathit { Those \\ problems \\ are \\ challenging \\ and \\ contain \\ diverse \\ semantic \\ anomalies \\ instead \\ of \\ only \\ \\ noisy \\ labels . } $ For example , in Figure 3 in the supplementary material : $ \\mathit { 1 ) \\ Out-of-distribution \\ anomalies } $ : An image may contain only background or an object which does not belong to any training class ; $ \\mathit { 2 ) \\ In-distribution \\ anomalies } $ : An image of class $ a $ may be annotated to class $ b $ or an image may contain more than one semantic object . $ \\ \\ $ Those experimental results prove that GR can achieve state-of-the-art performance on different domain tasks . Finally , we add the results of asymmetric label noise in Section $ D $ of the supplementary material in our new revised version . The results are displayed in Table 10 . When GR is used , the performance is better than its counterpart without GR . 3.The reference citation causes read difficulty . Thanks so much for pointing it out . We have put the citations into parentheses in the revised version ."}], "0": {"review_id": "rylUOn4Yvr-0", "review_text": "Summary: The paper proposes a method for noise robustness based on scaling gradients of examples. By choosing the proper scaling parameters (alpha and beta), the method recovers standard losses such as CCE, MAE, and GCE, while also recovering other losses. The method is strongly related to reweighting training examples, where alpha and beta define the shape of this weighting as a function of the model's prediction (i.e., p_i). Experiments show that the proposed method achieves competitive results on several standard benchmarks for noisy-labelled data. Comments: - The main drawback of the proposed approach is that there is no clear way of choosing alpha and beta, other than a hyper-parameter search, which is not very practical and can lead to overfitting the test set. - The paper in general is easy to follow, but the paper is not very rigorous or clear on some important concepts. For example: * No clear mathematical definition of emphasis focus and spread * The term \"semantically abnormal examples\" should be defined in the main text. * It is not so clear what it means to \"babyset\" emphasis focus and spread. * I don't understand what Eq. 6 is supposed to tell. * What are the \\dots in equation - The experiments are very thorough and the results are very good, but I have few clarifying questions: * The procedure for choosing beta/gamma is not clear, and I see that for every experiment those values change. * It would be nice if the CIFAR-10 Table 4 results are performed using the exact same setting to prior work to make sure the comparison is fair. For example, GCE results in (Zhang & Sabuncu 2018) are much that the reported ones. While it seems that you're using GoogLeNet V1 architecture similar to Jiang et al. 2018, it's not clear which experimental setting you are comparing against. * Can you be more specific what do you mean by \"with a little effort for optimizing beta and gamma\" in caption of Table 5? Minor: * Grammer mistake: \"what training examples...focused *on*...\" * Citations should be done with parentheses ", "rating": "6: Weak Accept", "reply_text": "1.The main drawback of the proposed approach is that there is no clear way of choosing alpha and beta , other than a hyper-parameter search , which is not very practical and can lead to overfitting the test set . 2.The experiments are very thorough and the results are very good , but I have few clarifying questions : 2.1 . The procedure for choosing beta/gamma is not clear , and I see that for every experiment those values change . 2.2.Can you be more specific what do you mean by `` with a little effort for optimizing beta and gamma '' in caption of Table 5 ? We reply to questions 1 and 2 together as they are related . - Reply to 1 & 2.1 on \u201c the procedure for choosing beta/lambda \u201d : We have summarised the procedure and principle for choosing $ \\lambda , \\beta $ in the empirical analysis part , i.e. , Section 4.2.1 . Details are as follows : ( 1 ) Emphasis focus : When noise rate is higher , we can improve a model \u2019 s robustness by moving emphasis focus towards relatively less difficult examples with a larger $ \\lambda $ , which is informative in practice . ( 2 ) When $ \\lambda $ is larger , $ \\beta $ should be larger as shown in Figure 1c . This is also demonstrated in Table 3 , Table 5 , and Table 10 . - Reply to 2.1 on \u201c for every experiment those values change \u201d : In Table 4 and Table 5 , we have displayed the results of GR with fixed parameters $ ( \\beta=8 , \\lambda=0.5 ) $ . Those results are better than the state-of-the-art . - Reply to 2.2 : Those empirical results above make it easy to optimise $ \\lambda , \\beta $ in practice . Therefore , we wrote \u201c with a little effort for optimizing $ \\lambda , \\beta $ \u201d . 3.It would be nice if the CIFAR-10 Table 4 results are performed using the exact same setting to prior work to make sure the comparison is fair . For example , GCE results in ( Zhang & Sabuncu 2018 ) are much that the reported ones . While it seems that you 're using GoogLeNet V1 architecture similar to Jiang et al.2018 , it 's not clear which experimental setting you are comparing against . - The research problem we work on is important and popular . Therefore , on different datasets , different baselines along with different net architectures have been evaluated in the literature . For example , some prior work reimplemented baselines with their custom-designed net architectures . The experimental settings , e.g. , choice of net architectures , are not consistent and rigid in different prior work . On the one hand , it is nice since it provides diverse evaluation settings . On the other hand , it presents a challenge to keep the compared baselines consistent when you aim to compare fairly with different types of baselines on different datasets . Especially some prior work designed their own net architectures . - In our experimental setup , we aim to test on different benchmarks using different publicly available net architectures for more comprehensive evaluation : ( 1 ) We choose GoogLeNet V1 on CIFAR-10 following MentorNet ( Jiang et al. , 2018 ) , where most baselines are example reweighting algorithms as shown in Table 4 . ( 2 ) We use ResNet-44 on CIFAR-100 following D2L ( Ma et al. , 2018 ) , where most competitors are specifically designed for addressing label noise as displayed in Table 5 . ( 3 ) The experiments on Clothing 1M using ResNet-50 are consistent in most prior work . We have displayed most related baselines in Table 6 . - We remark that on each dataset , we only report the results of those baselines using the same net architecture for a fair comparison . 4.The paper in general is easy to follow , but the paper is not very rigorous or clear on some important concepts . For example : 4.1 It is not so clear what it means to `` babysit '' emphasis focus and spread . It means that we need to choose emphasis focus and spread properly instead of using the built-in default settings in loss functions . 4.2 I do n't understand what Eq.6 is supposed to tell . We use Eq . ( 6 ) to tell that GR can be independent of empirical loss formulations . Since gradient computation is independent of loss computation , different losses only indicate how far we are away from different minimisation objectives . The supervision information of GR can be controlled independently and straightforwardly . 4.3 & 4.4 No clear mathematical definition of emphasis focus and spread . What are the \\dots in equation ? We have provided mathematical definitions of emphasis focus and spread in our revised version . We have removed those \\cdot in our revised version . 5.Minor : * Grammer mistake : `` what training examples ... focused * on * ... '' * Citations should be done with parentheses Thanks so much for your helpful and careful check . We have revised them in our revised version ."}, "1": {"review_id": "rylUOn4Yvr-1", "review_text": "Summary This paper presents Gradient Rescaling (GR) for robust learning to combat label noise. They propose to treat each data sample with different significance scores: some samples are important to learning, and some examples are insignificant (or even detrimental) to learning. So they desire to weight each samples according to their significance. They propose the notion of emphasis focus (When learning, whether we should put emphasis on learning \u201chard\u201d examples or \u201ceasy\u201d examples) and emphasis spread (the variance of these significance weights). The authors propose that this \u201cdifficulty\u201d of samples are proportional to their network output logit values. The authors examine the analytical forms of the gradients of popular loss functions such as Categorical Cross Entropy, Mean Absolute Error and Generalized Cross Entropy. They find that the formulas for the gradient are of similar family with varying hyperparameters. Authors claim that tweaking these hyperparameters result in tuning the emphasis focus and spread. The authors conduct Experiments on CIFAR10, CIFAR100 with simulated symmetric noise. Also, they conduct experiments on real-world noisy datasets: Clothing 1M dataset and MARS video dataset. The authors claim that the performance of GR exceeds various baselines. Significance/Novelty/Clarity Significance: Low-Medium. The performance increase exhibited in the experiments are a bit underwhelming (when considering the fact that benchmarks of most recent noise-robustness algos such as <Lee et al. 2019 ICML> are missing). Novelty: Medium. The paper is interesting in the sense that the authors integrated (and allegedly generalized) the gradient formulas for several losses into one family, and tried to integrate and tweak their postulation of \u201cEmphasis focus\u201d and \u201cemphasis spread\u201d into the framework. However, the theoretical ground and convincing reasoning for their claim seems a bit lackluster. Clarity: Low. The overall flow of the paper is a bit fuzzy - exhibiting a stream-of-consciousness style flow. Pros and Cons in Detail Pros: 1.The authors try to unify the analytical forms of the gradients of various loss functions into a single family equipped with hyperparameters that control emphasis focus and spread. 2.Conducted experiments show that GR achieved increased performance when compared to the baselines. Cons: My major concern is about tuning newly introduced hyperparameters in practical settings. How can we guarantee to have intact validation set? Can we get any improvement via GR even with corrupted validation set for tuning hyperparameters? 1. The arguments of the authors are grounded in the premise that \u201cdifficult\u201d samples will exhibit small logit values, and \u201ceasy\u201d samples high logit values. 2. No justifications (both theoretical and experimental) are provided on the claim that controlling emphasis focus/spread will result in more robust learning. 3. This algorithm introduces 2 additional hyperparameters that are correlated with each other. This introduces additional labor. 4. By changing the loss function, the outputs of the network might lose its interpretation as a probability distribution. 5. No confidence intervals are shown except for the CIFAR-100 experiment. 6. Experiments are only conducted on vision tasks. 7. The baseline menagerie also changes when the authors change the target dataset. 8. Additional benchmarks of most recent noise-robustness algos such as <Lee et al. 2019 ICML> are required. Questions 1. Is it always the case that \u201cdifficult\u201d samples exhibit small logit values, and \u201ceasy\u201d samples high logit values? 2. If not, GR\u2019s emphasis manipulation might result in neglecting samples containing valuable information. 3. Can GR be used simultaneously with other noise-robust learning methods to further boost the performance? 4. Technically, GR aims to rescale the gradients of the logits. How will it interact with optimizers other than SGD such as Adam? 5. Does GR still work well on small datasets(#points < 5000)? Misc. Comments Page 3-> inside L1 norm, no differentiation sign in the denominator. Around eq 2 and 4: missing derivative symbol w.r.t. z", "rating": "3: Weak Reject", "reply_text": "Thank you for your questions . 1 & 2 : Is it always the case that \u201c difficult \u201d samples exhibit small logit values , and \u201c easy \u201d samples high logit values ? If not , ..... Generally , the answer is yes . As training goes , the premise that semantic anomalies have small classification confidences while normal examples tend to have large classification confidences is indeed our reasonable assumption . Actually , a lot of prior work has demonstrated the reasonability of this premise . In addition , many algorithms have been proposed based on similar premises . Our empirical analysis also supports this premise well . More details are as follows : ( 1 ) Technically , we have also remarked that \u201c we do not design the weighting scheme heuristically from scratch . Instead , it is naturally motivated by the gradient analysis of several loss functions \u201d , which makes GR principally and technically sound . More discussion is provided in Section 3.3 . ( 2 ) In terms of prior work , self-paced learning , e.g. , Self-paced ( Kumar et al. , 2010 ) , and curriculum learning , e.g. , MentorNet ( Jiang et al. , 2018 ) are practical algorithms based on this premise . In addition , as demonstrated in ( Krueger et al. , 2017 ; Arpit et al. , 2017 ) , when severe noise exists , DNNs learn simple meaningful patterns first before memorising abnormal examples . ( 3 ) Finally , in the \u201c emphasis focus \u201d paragraph of Section 1 , we discussed that \u201c It is a common practice to focus on harder instances when training DNNs ( Shrivastava et al. , 2016 ; Lin et al. , 2017 ) . \u201d \u201c When a dataset is clean , it achieves faster convergence and better performance to emphasise on harder examples because they own larger gradient magnitude , which means more information and a larger update step for model \u2019 s parameters. \u201d \u201c However , when severe noise exists , as demonstrated in ( Krueger et al. , 2017 ; Arpit et al. , 2017 ) , DNNs learn simple meaningful patterns first before memorising abnormal ones. \u201d \u201c In other words , anomalies are harder to fit and own larger gradient magnitude in the later stage . Consequently , if we use the default sample weighting in categorical cross entropy ( CCE ) where harder samples obtain higher weights , anomalies tend to be fitted well especially when a network has large enough capacity . That is why we need to move the emphasis focus towards relatively easier ones , which serves as emphasis regularisation. \u201d 3 . Can GR be used simultaneously with other noise-robust learning methods to further boost the performance ? We have tried combining GR with other standard regularisers in Section 4.5 , please see Table 8 . For example , $ \\mathit { Dropout \\ is \\ demonstrated \\ to \\ be \\ a \\ great \\ regulariser \\ against \\ label \\ noise \\ in \\ Arpit \\ et \\ al. , \\ 2017 } $ , \u201c A closer look at memorization in deep networks \u201d . This is also demonstrated in our Table 8 , e.g. , Dropout > Baseline , and Dropout+L2 > L2 . Case 1 : GR can help other regularisation techniques . GR consistently improves the generalisation performance after it is added : GR > Baseline ; GR+L2 > L2 ; GR+Dropout > Dropout ; GR+L2+Dropout > Dropout+L2 . Case 2 : Other regularisation techniques may not help GR . When GR is already applied , adding another regulariser may not lead to better regularisation effect and generalisation performance . For example : Adding Dropout hurts the performance : GR+Dropout < GR ; GR+L2+Dropout < GR+L2 . Adding L2 decay improves the performance : GR+L2 > GR ; GR+L2+Dropout > GR+Dropout . Therefore , we are sorry that there is no deterministic answer for your question . The search and studying space is large when considering the diverse combination options of different regularisers . However , it is worth noting that the interaction over multiple regularisers may not improve the generalisation performance in practice . 5.Does GR still work well on small datasets ( # points < 5000 ) ? To address your concern on this question , we add a Section $ C $ in our supplementary material in the new revised version . Please have a check . Thanks . ( 1 ) .The problem of label noise we study on CIFAR-10 and CIFAR-100 in Section 4.2 is of similar scale . ( 2 ) .We compare GR with other standard regularisers on a small-scale fine-grained visual categorisation problem in Table 9 . The number of training data points is 5,000 in total . This new experiment demonstrates that GR works on small datasets as well ."}, "2": {"review_id": "rylUOn4Yvr-2", "review_text": "Summary: The authors first analyze and answer the question: What training examples should be focused and how large the emphasis spread should be? Then, they proposed the gradient rescaling framework serving as emphasis regularization. Strengths: 1. The paper is well organized except the reference citation (read difficultly) 2. The proposed method is very simple and effective. 3. Experiments show the improvements over SOTA. Weakness: 1. The experiments lack the recent important baseline \"symmetric cross entropy for robust learning with noisy labels, ICCV2019\", which are the current SOTA. Maybe the author should check the above paper and show the results. 2. The experiments are only conducted on symmetric noise. Actually, asymmetric noise is also important. The author should conduct at least some experiments on asymmetric noise. ", "rating": "3: Weak Reject", "reply_text": "Thanks so much for your helpful review . We are glad that you like our simple , effective and principled method . Regarding the weakness points you mentioned , we clarify them as follows . We look forward to further discussion with you . 1.Regarding the recent baseline \u201c symmetric cross entropy for robust learning with noisy labels , ICCV2019 \u201d , we read it before submission but did not compare with it because it was not officially published at that time . Now , we add their results in our revised version . Please check our Tables 5 and 6 , and you will find our method outperforms this recent baseline . Beyond , please check our Section 2.1 , where we discuss and present some remarks on robustness theorems conditioned on symmetric losses and label noise . Our work challenges those robustness theorems , which can promote new thinking . 2.We explain our two reasons for without testing on asymmetric label noise : 1 ) . As we mentioned in Section 4.2 , we follow the prior work \u201c Ma et al.Dimensionality-Driven Learning with Noisy Labels , ICML 2019 \u201d to test only on symmetric label noise as it has been demonstrated in Vahdat ( 2017 ) that it is more challenging than asymmetric noisy labels . Please check \u201c Arash Vahdat . Toward robustness against label noise in training deep discriminative neural networks . NeurIPS , 2017. \u201d 2 ) . We spend more effort and space on experimental analysis and more complex and valuable real-world applications , e.g. , image classification on Clothing 1M and video retrieval on MARS . $ \\mathit { Those \\ problems \\ are \\ challenging \\ and \\ contain \\ diverse \\ semantic \\ anomalies \\ instead \\ of \\ only \\ \\ noisy \\ labels . } $ For example , in Figure 3 in the supplementary material : $ \\mathit { 1 ) \\ Out-of-distribution \\ anomalies } $ : An image may contain only background or an object which does not belong to any training class ; $ \\mathit { 2 ) \\ In-distribution \\ anomalies } $ : An image of class $ a $ may be annotated to class $ b $ or an image may contain more than one semantic object . $ \\ \\ $ Those experimental results prove that GR can achieve state-of-the-art performance on different domain tasks . Finally , we add the results of asymmetric label noise in Section $ D $ of the supplementary material in our new revised version . The results are displayed in Table 10 . When GR is used , the performance is better than its counterpart without GR . 3.The reference citation causes read difficulty . Thanks so much for pointing it out . We have put the citations into parentheses in the revised version ."}}