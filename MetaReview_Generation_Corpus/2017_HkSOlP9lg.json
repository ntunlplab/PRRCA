{"year": "2017", "forum": "HkSOlP9lg", "title": "Recurrent Inference Machines for Solving Inverse Problems", "decision": "Reject", "meta_review": "This paper presents an approach for learning both a model and inference procedure at the same time using RNNs. The reviewers agree that the idea is interesting, but discussion and considering the responses to the reviews, still felt that more is needed to motivate and contextualise the work, and to make the methods presented more convincing. For this reason, the paper is unfortunately not yet ready for inclusion in this year's proceedings.", "reviews": [{"review_id": "HkSOlP9lg-0", "review_text": "This paper proposes the RIMs that unrolls variational inference procedure. The author claims that the novelty lies in the separation of the model and inference procedure, making the MAP inference as an end-to-end approach. The effectiveness is shown in image restoration experiments. While unrolling the inference is not new, the author does raise an interesting perspective towards the `model-free' configuration, where model and inference are not separable and can be learnt jointly. However I do not quite agree the authors' argument regarding [1] and [2]. Although both [1] and [2] have pre-defined MAP inference problem. It is not necessarily that a separate step is required. In fact, both do not have either a pre-defined prior model or an explicit prior evaluation step as shown in Fig. 1(a). I believe that the implementation of both follows the same procedure as the proposed, that could be explained through Fig. 1(c). That is to say, the whole inference procedure eventually becomes a learnable neural network and the energy is implicitly defined through learning the parameters. Moreover, the RNN block architecture (GRU) and non-linearity (tanh) restrict the flexibility and implicitly form the inherent family of variational energy and inference algorithm. This is also similar with [1] and [2]. Based on that fact, I have the similar feeling with R1 that the novelty is somewhat limited. Also some discussions should be added in terms of the architecture and nonlinearity that you have chosen. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "R3 : \u201c This paper proposes the RIMs that unrolls variational inference procedure. \u201d A : To be clear , we do not claim to unroll a variational inference procedure . In no place in the paper do we define a lower bound on an objective function , neither do we write down an approximation of a probability distribution . We motivate our approach from MAP inference , and we emphasise that the resulting model needs three input components : 1 ) the current estimate ( external state ) , 2 ) an error gradient from the likelihood under the current estimate , and 3 ) an internal state . This insight is summarised in figure 1 . Although we motivate the approach through MAP inference , after training the resulting model can step away from this scheme . There is no explicit objective function to be optimised during inference , in fact the objective function becomes implicit in the iterative process . This acknowledges the fact that objective functions in the context of inverse problems are often only used as a utility to define an inference procedure rather than as a measure of goodness-of-fit . Under these conditions , it is simply unnecessary to explicitly define an objective function for inference . R3 : \u201c However I do not quite agree the authors ' argument regarding [ 1 ] and [ 2 ] . Although both [ 1 ] and [ 2 ] have pre-defined MAP inference problem . It is not necessarily that a separate step is required . In fact , both do not have either a pre-defined prior model or an explicit prior evaluation step as shown in Fig.1 ( a ) .I believe that the implementation of both follows the same procedure as the proposed , that could be explained through Fig.1 ( c ) .That is to say , the whole inference procedure eventually becomes a learnable neural network and the energy is implicitly defined through learning the parameters. \u201c A : We do not disagree with the reviewer on this point . Yes , both approaches ( [ 1 ] and [ 2 ] ) can be described through Fig.1 ( c ) .This goes in hand with our argument that RIMs generalise over these classes of models . The difference lies in the way model architectures are chosen . In [ 1 ] and [ 2 ] , both the prior and inference method are chosen by hand to form the model . The update equations for the respective inference procedures can then be interpreted as a single step in an RNN . This follows a bottom-up approach : First define the implementation level to then form an algorithm . However , due to the choice of prior and inference methods , the interaction between both components remains fixed through a predefined set of update equations . In RIMs we motivate iterative inference in a top-down fashion : define a computational model ( the RIM ) , then choose an implementation ( a specific model architecture ) . The use of neural networks in this context generally allows RIMs to implement a broad class of prior and inference methods , more so it relaxes any possible interactions between prior components and inference components , all of which is done through learning . R3 : \u201c Moreover , the RNN block architecture ( GRU ) and non-linearity ( tanh ) restrict the flexibility and implicitly form the inherent family of variational energy and inference algorithm . This is also similar with [ 1 ] and [ 2 ] . \u201d A : The reviewer appears to have an insight into our chosen model architecture which is not apparent to us . It is not clear to us why our chosen architecture would restrict the implicit objective function . We would very much appreciate further insights from the reviewer on this comment . R3 : \u201c Based on that fact , I have the similar feeling with R1 that the novelty is somewhat limited . Also some discussions should be added in terms of the architecture and nonlinearity that you have chosen. \u201d A : The choice of architecture is uncommon in the context of inverse models , but more common in the context deep learning . GRUs often show similar performance to LSTMs while having less parameters . We chose this architecture to emphasise that with the RIM perspective these inverse problems can be addressed solely from a deep learning perspective without any particular inspiration from model architectures in the inverse modeling context ."}, {"review_id": "HkSOlP9lg-1", "review_text": "Unfortunately, even after reading the authors' response to my pre-review question, I feel this paper in its current form lacks sufficient novelty to be accepted to ICLR. Fundamentally, the paper suggests that traditional iterative algorithms for specific class of problems (ill-posed image inverse problems) can be replaced by discriminatively trained recurrent networks. As R3 also notes, un-rolled networks for iterative inference aren't new: they've been used to replace CRF-type inference, and _also_ to solve image inverse problems (my refs [1-3]). Therefore, I'd argue that the fundamental idea proposed by the paper isn't new---it is just that the paper seeks to 'formalize' it as an approach for inverse problems (although, there is nothing specific about the analysis that ties it to inverse problems: the paper only shows that the RIM can express gradient descent over prior + likelihood objective). I also did not find the claims about benefits over prior approaches very compelling. The comment about parameter sharing works both ways---it is possible that untying the parameters leads to better performance over a fewer number of 'iterations', and given that the 'training set' is synthetically generated, learning a larger number of parameters doesn't seem to be an issue. Also, I'd argue that sharing the parameters is the 'obvious' approach, and the prior methods choose to not tie the parameters to get better accuracy. The same holds for being able to handle different noise levels / scale sizes. A single model can always be trained to handle multiple forms of degradation---its just that its likely to do better when it's trained for specific degradation model/level. But more importantly, there is no evidence in the current set of experiments that shows that this is a property of the RIM architecture. (Moreover, this claim goes against one of the motivations of the paper of not training a single prior for different observation models ... but to train the entire inference architecture end-to-end). It is possible that the proposed method does offer practical benefits beyond prior work---but these benefits don't come from the idea of simply unrolling iterations, which is not novel. I would strongly recommend that the authors consider a significant re-write of the paper---with a detailed discussion of prior work mentioned in the comments that highlights, with experiments, the specific aspects of their recurrent architecture that enables better recovery for inverse problems. I would also suggest that to claim the mantle of 'solving inverse problems', the paper consider a broader set of inverse tasks---in-painting, deconvolution, different noise models, and possibly working with multiple observations (like for HDR).", "rating": "4: Ok but not good enough - rejection", "reply_text": "The reviewer makes some claims about our paper which to us appear not to be a reflection of the actual paper but a result of the communication between Reviewer 1 and Reviewer 3 . We therefore chose to answer each paragraph separately : R1 : \u201c Fundamentally , the paper suggests that traditional iterative algorithms for specific class of problems ( ill-posed image inverse problems ) can be replaced by discriminatively trained recurrent networks . As R3 also notes , un-rolled networks for iterative inference are n't new : they 've been used to replace CRF-type inference , and _also_ to solve image inverse problems ( my refs [ 1-3 ] ) . Therefore , I 'd argue that the fundamental idea proposed by the paper is n't new -- -it is just that the paper seeks to 'formalize ' it as an approach for inverse problems ( although , there is nothing specific about the analysis that ties it to inverse problems : the paper only shows that the RIM can express gradient descent over prior + likelihood objective ) . \u201d A : In no place in the paper do we claim that \u201c unrolling inference \u201d is the novelty of our approach . We do claim however , that we identify 3 minimal components that we deem necessary for well formed iterative inference methods : 1 ) the current estimate ( external state ) , 2 ) an error gradient from the likelihood under the current estimate , and 3 ) an internal state . We define RIMs in a top-down fashion with these 3 components . The implementation is then up to the practitioner ( see our argument about generalisation ) . In contrast [ 1-3 ] , define their models in a bottom up fashion : First start with an architecture ( graphical model , prior model , likelihood model ) , then choose an inference method and finally construct the algorithm . We claim that with the 3 components , we could choose any architecture ( on an implementation level ) that we find to work best for a given problem . This perspective can significantly speed up the workflow in finding new models . R1 : \u201c I also did not find the claims about benefits over prior approaches very compelling . The comment about parameter sharing works both ways -- -it is possible that untying the parameters leads to better performance over a fewer number of 'iterations ' , and given that the 'training set ' is synthetically generated , learning a larger number of parameters does n't seem to be an issue . Also , I 'd argue that sharing the parameters is the 'obvious ' approach , and the prior methods choose to not tie the parameters to get better accuracy. \u201d A : It is surprising the reviewer raises this point . In the paper we clearly show that although we use parameter sharing , our approach ( marginally ) outperforms Regression Tree Fields ( RTF-5 ) which is a discriminatively trained reconstruction method with non-shared parameters per time step ( trained for each noise level separately ) . Given the reviewers claims , we put the presented RIM at a disadvantage in comparison to RTF-5 , while still outperforming said method . Overall , we find the discussion about parameter sharing tedious because it is not part of the central claims of our paper . Yes , we could have chosen to not share parameters over time , but this would yet have been another architectural choice . In our last response to the reviewer we tried to explain the benefits that parameter sharing can bring ( e.g.figure 2 can only be done with a model that has shared parameters ) , unfortunately the reviewer chose to ignore our argument in their review . R1 : \u201c The same holds for being able to handle different noise levels / scale sizes . A single model can always be trained to handle multiple forms of degradation -- -its just that its likely to do better when it 's trained for specific degradation model/level . But more importantly , there is no evidence in the current set of experiments that shows that this is a property of the RIM architecture . ( Moreover , this claim goes against one of the motivations of the paper of not training a single prior for different observation models ... but to train the entire inference architecture end-to-end ) . \u201d A : The reviewer makes it sound very trivial that a model can handle different noise levels/scales ( or other corruption processes , see section 4.4 Multi-task learning ) . In fact , if this was the case there would be few arguments for using RIMs or other inference approaches that motivate through Bayes \u2019 theorem , whatsoever . However , the reviewer does not give any evidence for their claims . It is , in fact , the most important result of our paper , that we have a discriminatively trained model that can handle different corruption processes at the same time . None of the papers that are presented by Reviewer 2 and Reviewer 3 have this feature . Either the presented methods trained a new model for every noise level , or results were only presented for a single noise level . In the super-resolution task we compare the RIM with SRCNN which are discriminatively trained CNNs for super-resolution . SRCNNs do not explicitly take into account the corruption model , but instead a new model is trained for each of the 3 scales . In our paper we show that one RIM which is trained for all 3 scales can outperform the SRCNN on all scales although the RIM has less parameters and it uses parameter sharing over time . To us this clearly shows that model definition of RIMs can benefit performance . Further the argument of the reviewer that training a new model for every degradation model/level would be always beneficial does not take into account any practical applications . In practice , it is simply impossible to have different models for every possible noise level ( noise levels are discrete in experiments but not in practical applications ) . And even if there were only 10 possible noise levels , it would imply that it was still necessary to have 10 different models on each device just for the task of denoising . In our paper we show that RIMs can handle different corruption processes under the hood of one model . It \u2019 s quite surprising that the reviewer tries to turn this positive feature of the model into a flaw ."}, {"review_id": "HkSOlP9lg-2", "review_text": "This paper presents a method to learn both a model and inference procedure at the same time with recurrent neural networks in the context of inverse problems. The proposed method is interesting and results are quite good. The paper is also nicely presented. I would be happy to see some discussion about what the network learns in practice about natural images in the case of denoising. What are the filters like? Is it particularly sensitive to different structures in images? edges? Also, what is the state in the recurrent unit used for? when are the gates open etc. Nevertheless, I think this is nice work which should be accepted.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the positive review . The reviewer raises an interesting perspective on further analysis of the chosen model architecture . We have not done this type of analysis so far . We believe , however , that this approach would allow us to make better informed architectural choices in the future . Here , we wanted to put less focus on the chosen architecture from an implementation level but more of a focus on the top-down viewpoint . In applications , we see this type of analysis as part of an iterative workflow for finding a good model architecture ."}], "0": {"review_id": "HkSOlP9lg-0", "review_text": "This paper proposes the RIMs that unrolls variational inference procedure. The author claims that the novelty lies in the separation of the model and inference procedure, making the MAP inference as an end-to-end approach. The effectiveness is shown in image restoration experiments. While unrolling the inference is not new, the author does raise an interesting perspective towards the `model-free' configuration, where model and inference are not separable and can be learnt jointly. However I do not quite agree the authors' argument regarding [1] and [2]. Although both [1] and [2] have pre-defined MAP inference problem. It is not necessarily that a separate step is required. In fact, both do not have either a pre-defined prior model or an explicit prior evaluation step as shown in Fig. 1(a). I believe that the implementation of both follows the same procedure as the proposed, that could be explained through Fig. 1(c). That is to say, the whole inference procedure eventually becomes a learnable neural network and the energy is implicitly defined through learning the parameters. Moreover, the RNN block architecture (GRU) and non-linearity (tanh) restrict the flexibility and implicitly form the inherent family of variational energy and inference algorithm. This is also similar with [1] and [2]. Based on that fact, I have the similar feeling with R1 that the novelty is somewhat limited. Also some discussions should be added in terms of the architecture and nonlinearity that you have chosen. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "R3 : \u201c This paper proposes the RIMs that unrolls variational inference procedure. \u201d A : To be clear , we do not claim to unroll a variational inference procedure . In no place in the paper do we define a lower bound on an objective function , neither do we write down an approximation of a probability distribution . We motivate our approach from MAP inference , and we emphasise that the resulting model needs three input components : 1 ) the current estimate ( external state ) , 2 ) an error gradient from the likelihood under the current estimate , and 3 ) an internal state . This insight is summarised in figure 1 . Although we motivate the approach through MAP inference , after training the resulting model can step away from this scheme . There is no explicit objective function to be optimised during inference , in fact the objective function becomes implicit in the iterative process . This acknowledges the fact that objective functions in the context of inverse problems are often only used as a utility to define an inference procedure rather than as a measure of goodness-of-fit . Under these conditions , it is simply unnecessary to explicitly define an objective function for inference . R3 : \u201c However I do not quite agree the authors ' argument regarding [ 1 ] and [ 2 ] . Although both [ 1 ] and [ 2 ] have pre-defined MAP inference problem . It is not necessarily that a separate step is required . In fact , both do not have either a pre-defined prior model or an explicit prior evaluation step as shown in Fig.1 ( a ) .I believe that the implementation of both follows the same procedure as the proposed , that could be explained through Fig.1 ( c ) .That is to say , the whole inference procedure eventually becomes a learnable neural network and the energy is implicitly defined through learning the parameters. \u201c A : We do not disagree with the reviewer on this point . Yes , both approaches ( [ 1 ] and [ 2 ] ) can be described through Fig.1 ( c ) .This goes in hand with our argument that RIMs generalise over these classes of models . The difference lies in the way model architectures are chosen . In [ 1 ] and [ 2 ] , both the prior and inference method are chosen by hand to form the model . The update equations for the respective inference procedures can then be interpreted as a single step in an RNN . This follows a bottom-up approach : First define the implementation level to then form an algorithm . However , due to the choice of prior and inference methods , the interaction between both components remains fixed through a predefined set of update equations . In RIMs we motivate iterative inference in a top-down fashion : define a computational model ( the RIM ) , then choose an implementation ( a specific model architecture ) . The use of neural networks in this context generally allows RIMs to implement a broad class of prior and inference methods , more so it relaxes any possible interactions between prior components and inference components , all of which is done through learning . R3 : \u201c Moreover , the RNN block architecture ( GRU ) and non-linearity ( tanh ) restrict the flexibility and implicitly form the inherent family of variational energy and inference algorithm . This is also similar with [ 1 ] and [ 2 ] . \u201d A : The reviewer appears to have an insight into our chosen model architecture which is not apparent to us . It is not clear to us why our chosen architecture would restrict the implicit objective function . We would very much appreciate further insights from the reviewer on this comment . R3 : \u201c Based on that fact , I have the similar feeling with R1 that the novelty is somewhat limited . Also some discussions should be added in terms of the architecture and nonlinearity that you have chosen. \u201d A : The choice of architecture is uncommon in the context of inverse models , but more common in the context deep learning . GRUs often show similar performance to LSTMs while having less parameters . We chose this architecture to emphasise that with the RIM perspective these inverse problems can be addressed solely from a deep learning perspective without any particular inspiration from model architectures in the inverse modeling context ."}, "1": {"review_id": "HkSOlP9lg-1", "review_text": "Unfortunately, even after reading the authors' response to my pre-review question, I feel this paper in its current form lacks sufficient novelty to be accepted to ICLR. Fundamentally, the paper suggests that traditional iterative algorithms for specific class of problems (ill-posed image inverse problems) can be replaced by discriminatively trained recurrent networks. As R3 also notes, un-rolled networks for iterative inference aren't new: they've been used to replace CRF-type inference, and _also_ to solve image inverse problems (my refs [1-3]). Therefore, I'd argue that the fundamental idea proposed by the paper isn't new---it is just that the paper seeks to 'formalize' it as an approach for inverse problems (although, there is nothing specific about the analysis that ties it to inverse problems: the paper only shows that the RIM can express gradient descent over prior + likelihood objective). I also did not find the claims about benefits over prior approaches very compelling. The comment about parameter sharing works both ways---it is possible that untying the parameters leads to better performance over a fewer number of 'iterations', and given that the 'training set' is synthetically generated, learning a larger number of parameters doesn't seem to be an issue. Also, I'd argue that sharing the parameters is the 'obvious' approach, and the prior methods choose to not tie the parameters to get better accuracy. The same holds for being able to handle different noise levels / scale sizes. A single model can always be trained to handle multiple forms of degradation---its just that its likely to do better when it's trained for specific degradation model/level. But more importantly, there is no evidence in the current set of experiments that shows that this is a property of the RIM architecture. (Moreover, this claim goes against one of the motivations of the paper of not training a single prior for different observation models ... but to train the entire inference architecture end-to-end). It is possible that the proposed method does offer practical benefits beyond prior work---but these benefits don't come from the idea of simply unrolling iterations, which is not novel. I would strongly recommend that the authors consider a significant re-write of the paper---with a detailed discussion of prior work mentioned in the comments that highlights, with experiments, the specific aspects of their recurrent architecture that enables better recovery for inverse problems. I would also suggest that to claim the mantle of 'solving inverse problems', the paper consider a broader set of inverse tasks---in-painting, deconvolution, different noise models, and possibly working with multiple observations (like for HDR).", "rating": "4: Ok but not good enough - rejection", "reply_text": "The reviewer makes some claims about our paper which to us appear not to be a reflection of the actual paper but a result of the communication between Reviewer 1 and Reviewer 3 . We therefore chose to answer each paragraph separately : R1 : \u201c Fundamentally , the paper suggests that traditional iterative algorithms for specific class of problems ( ill-posed image inverse problems ) can be replaced by discriminatively trained recurrent networks . As R3 also notes , un-rolled networks for iterative inference are n't new : they 've been used to replace CRF-type inference , and _also_ to solve image inverse problems ( my refs [ 1-3 ] ) . Therefore , I 'd argue that the fundamental idea proposed by the paper is n't new -- -it is just that the paper seeks to 'formalize ' it as an approach for inverse problems ( although , there is nothing specific about the analysis that ties it to inverse problems : the paper only shows that the RIM can express gradient descent over prior + likelihood objective ) . \u201d A : In no place in the paper do we claim that \u201c unrolling inference \u201d is the novelty of our approach . We do claim however , that we identify 3 minimal components that we deem necessary for well formed iterative inference methods : 1 ) the current estimate ( external state ) , 2 ) an error gradient from the likelihood under the current estimate , and 3 ) an internal state . We define RIMs in a top-down fashion with these 3 components . The implementation is then up to the practitioner ( see our argument about generalisation ) . In contrast [ 1-3 ] , define their models in a bottom up fashion : First start with an architecture ( graphical model , prior model , likelihood model ) , then choose an inference method and finally construct the algorithm . We claim that with the 3 components , we could choose any architecture ( on an implementation level ) that we find to work best for a given problem . This perspective can significantly speed up the workflow in finding new models . R1 : \u201c I also did not find the claims about benefits over prior approaches very compelling . The comment about parameter sharing works both ways -- -it is possible that untying the parameters leads to better performance over a fewer number of 'iterations ' , and given that the 'training set ' is synthetically generated , learning a larger number of parameters does n't seem to be an issue . Also , I 'd argue that sharing the parameters is the 'obvious ' approach , and the prior methods choose to not tie the parameters to get better accuracy. \u201d A : It is surprising the reviewer raises this point . In the paper we clearly show that although we use parameter sharing , our approach ( marginally ) outperforms Regression Tree Fields ( RTF-5 ) which is a discriminatively trained reconstruction method with non-shared parameters per time step ( trained for each noise level separately ) . Given the reviewers claims , we put the presented RIM at a disadvantage in comparison to RTF-5 , while still outperforming said method . Overall , we find the discussion about parameter sharing tedious because it is not part of the central claims of our paper . Yes , we could have chosen to not share parameters over time , but this would yet have been another architectural choice . In our last response to the reviewer we tried to explain the benefits that parameter sharing can bring ( e.g.figure 2 can only be done with a model that has shared parameters ) , unfortunately the reviewer chose to ignore our argument in their review . R1 : \u201c The same holds for being able to handle different noise levels / scale sizes . A single model can always be trained to handle multiple forms of degradation -- -its just that its likely to do better when it 's trained for specific degradation model/level . But more importantly , there is no evidence in the current set of experiments that shows that this is a property of the RIM architecture . ( Moreover , this claim goes against one of the motivations of the paper of not training a single prior for different observation models ... but to train the entire inference architecture end-to-end ) . \u201d A : The reviewer makes it sound very trivial that a model can handle different noise levels/scales ( or other corruption processes , see section 4.4 Multi-task learning ) . In fact , if this was the case there would be few arguments for using RIMs or other inference approaches that motivate through Bayes \u2019 theorem , whatsoever . However , the reviewer does not give any evidence for their claims . It is , in fact , the most important result of our paper , that we have a discriminatively trained model that can handle different corruption processes at the same time . None of the papers that are presented by Reviewer 2 and Reviewer 3 have this feature . Either the presented methods trained a new model for every noise level , or results were only presented for a single noise level . In the super-resolution task we compare the RIM with SRCNN which are discriminatively trained CNNs for super-resolution . SRCNNs do not explicitly take into account the corruption model , but instead a new model is trained for each of the 3 scales . In our paper we show that one RIM which is trained for all 3 scales can outperform the SRCNN on all scales although the RIM has less parameters and it uses parameter sharing over time . To us this clearly shows that model definition of RIMs can benefit performance . Further the argument of the reviewer that training a new model for every degradation model/level would be always beneficial does not take into account any practical applications . In practice , it is simply impossible to have different models for every possible noise level ( noise levels are discrete in experiments but not in practical applications ) . And even if there were only 10 possible noise levels , it would imply that it was still necessary to have 10 different models on each device just for the task of denoising . In our paper we show that RIMs can handle different corruption processes under the hood of one model . It \u2019 s quite surprising that the reviewer tries to turn this positive feature of the model into a flaw ."}, "2": {"review_id": "HkSOlP9lg-2", "review_text": "This paper presents a method to learn both a model and inference procedure at the same time with recurrent neural networks in the context of inverse problems. The proposed method is interesting and results are quite good. The paper is also nicely presented. I would be happy to see some discussion about what the network learns in practice about natural images in the case of denoising. What are the filters like? Is it particularly sensitive to different structures in images? edges? Also, what is the state in the recurrent unit used for? when are the gates open etc. Nevertheless, I think this is nice work which should be accepted.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the positive review . The reviewer raises an interesting perspective on further analysis of the chosen model architecture . We have not done this type of analysis so far . We believe , however , that this approach would allow us to make better informed architectural choices in the future . Here , we wanted to put less focus on the chosen architecture from an implementation level but more of a focus on the top-down viewpoint . In applications , we see this type of analysis as part of an iterative workflow for finding a good model architecture ."}}