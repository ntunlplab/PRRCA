{"year": "2021", "forum": "ATp1nW2FuZL", "title": "Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces", "decision": "Accept (Poster)", "meta_review": "The reviewers are enthusiastic about this work, and the few comments that they had were appropriately addressed by the reviewers.", "reviews": [{"review_id": "ATp1nW2FuZL-0", "review_text": "The quality and clarity of the paper is good overall . In my opinion the presentation is clear , the goal of the work , and the proposed solutions are presented cleanly . Authors give a few examples of the issues raised by learning from many correct solutions , which is appreciated . In terms of significance . I believe the theory is not surprising at all , it is straightforward to see that eq . ( 1 ) will not be a consistent loss function for generalization . Now , in Lemma 2 , given the definition of eq . ( 2 ) , it is also not surprising to see that it is a consistent estimator , and in fact , the proofs are rather trivial . Thus , from the theoretical viewpoint these issues undermine the paper . From the practical viewpoint , authors show that their proposed method SelectRL is better than other baselines , and my main concern in the practicality of the algorithm is that I do n't see a strong case where SelectRL is significantly better than MinLoss , at least not statistically . Thus , my question here is , can authors claim that SelectRL is better than MinLoss from the experiments ? If so , the gain seems small , and computationally speaking training an RL agent to select a solution seems an overkill . Another question is : in the Random baseline , was the solution being picked uniformly at random ? If so , would n't a distribution that is concentrated around a solution be better ? I would 've liked to see this in the experiments . If authors can comment on this question would be appreciated .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your crisp review . Below we make an attempt to assuage all your concerns . Please let us know if more clarification would help and we would be happy to provide it . * * Theory is not surprising at all * * We would like to note that in this work , we are defining a new problem and one of the primary contributions of our work is in laying out the framework for dealing with solution multiplicity , formally introducing all the definitions , fundamental theorems and their proofs , even if some of them are natural and not surprising . Once the concepts are crisply and clearly defined , the proofs may appear trivial to expert readers . To the best of our knowledge , the problem of solution multiplicity has not been hitherto addressed in the literature . Our major contributions in this work can be summarized as follows : 1 . Define a novel problem . 2.Propose an intuitive , yet strong baseline along with its underlying theory and mathematical foundation ( MinLoss ) , and , 3 . Propose an RL based method to overcome the shortcomings of the greedy baseline via exploration ( SelectR ) . In addition , we have also presented an extensive set of experiments showing the efficacy of our proposed model over the baseline approaches which ignore solution multiplicity altogether . * * Can authors claim that SelectRL is better than MinLoss from the experiments ? * * Based on our experiments , we do believe that our SelectR based approach performs better than MinLoss . Table 3 ( appendix ) gives detailed numbers along with the standard error . We would like to note that the standard errors reported here are over variations in the choice of different random seeds in the experiments . Since it is difficult to do a large number of such experiments ( with varying seeds ) due to high computational complexity , we have also reported the comparisons using the ( max ) model based on a validation set for each of the algorithms . In both the settings of ( mean ) and ( max ) , our SelectR based approach does better than MinLoss . In fact , SelectR performs better than MinLoss for each of the three seeds independently in all the experiments . We note that starting with the same seed in our implementation means that the initialization of the prediction network is the same . We have added a table ( Table 4 ) in the appendix for seed-wise comparison of the gains of SelectR over MinLoss . In addition , we would like to note that the standard error arising out of the variation in the test set , is very very small ( of the order of less than 0.005 ) , and improvement of SelectR over all other algorithms is statistically significant with p values ( computed using McNemar \u2019 s test for comparing MinLoss and SelectR using best models selected based on devset ) of 1e-16 , 0.03 , and 1.7e-18 for NQueens , Futoshiki and Sudoku respectively . We also refer to the discussion about using a simpler exploration strategy in response to AnonReviewer4 ; please see the comment on \u201c * Regarding a separate module for exploration and alternative of exploring based on model probability * \u201d . * * Computationally speaking training an RL agent to select a solution seems an overkill . * * Computationally , we note that training time for SelectR is about 1.5 times more than MinLoss on average . Inference time is identical in both cases since it is only a forward propagation through the trained $ M_\\Theta $ network . * * In the Random baseline , was the solution being picked uniformly at random ? * * Yes , we did pick the solution to be trained on uniformly at random among the set of possible solutions for each input ( $ x $ ) . Once this was picked , we trained the entire algorithm using this solution ( for a given input ( $ x $ ) ) . Choosing a uniform distribution seemed like a natural choice for a baseline since we did not have any natural preference for one solution over the others at the start of the training . We could possibly pick the solution to train on based on a distribution centered around a solution , but it is not clear what solution to centre the distribution on , and what this distribution should be . A more sophisticated strategy would pick a solution based on current model parameters in every iteration , but that is exactly what MinLoss and SelectR do ."}, {"review_id": "ATp1nW2FuZL-1", "review_text": "Summary : The authors work in the domain of applying neural networks to combinatorial problems with structured output space , such as sudoku and n-queens . They notice how models currently performing well at this task encounter difficulties when there are multiple possible solutions . They formalize the task of learning any of multiple given ( and possibly quite different ) labels and propose an RL based approach to solve that task . They show improvements over selected baselines . Great : * The discussion of why the task is different from other instances of multiple labels is well argued and clear . * Numerical examples are very helpful in following the description of the algorithms . * The evaluation setting is well thought of : utilizing the state of the art model and comparing it to reasonable baselines ( one of which is indeed current SoTA ) . The experiments seem reproducible , given the detailed descriptions in the appendix . Could be improved : * Table 2 is perhaps misleading . Table 3 with the same results with standard deviation gives a less clear answer on whether SelectR is actually always better than MinLoss ( careful with significant digits , the standard deviation can \u2019 t have more than one ! ) . * The experiment depicted in figure 2 isn \u2019 t discussed , it doesn \u2019 t report confidence intervals , the distribution of training samples is not discussed ( are there significantly less training examples with 50+ solutions , that could justify the drop in performance ? A sudoku with this many solutions is likely to be quite sparse , does that affect performance ? ) . In summary , the paper introduces and formalizes an interesting novel task in the context of combinatorial problems with structured output space . While aspects in the evaluation could be clarified , the paper is clear and interesting . I recommend an accept , and would be willing to increase the score if my concerns are addressed .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your thoughtful review . Below we hope to address the concerns raised by you . Please let us know if there are still any doubts and we would be happy to address them . * * Table 2 is perhaps misleading . Is SelectR actually better than MinLoss * * Thanks for the careful read of the appendix . We believe your concern arises out of the seemingly \u201c high \u201d standard error numbers reported in Table 3 . We would like to note here that the standard errors reported here are over variations in the choice of different random seeds in the experiments . Since it is difficult to do a large number of such experiments ( with varying seeds ) due to high computational complexity , we have also reported the comparisons using the ( max ) model based on a validation set for each of the algorithms . In both the settings of ( mean ) and ( max ) , our SelectR based approach does better than MinLoss . In fact , SelectR performs better than MinLoss for each of the three seeds independently in all the experiments . We note that starting with the same seed in our implementation means that the initialization of the prediction network is the same . We have added a table ( Table 4 ) in the appendix for seed-wise comparison of the gains of SelectR over MinLoss . In addition , we would like to note that the standard error arising out of the variation in the test set , is very very small ( of the order of less than 0.005 ) , and improvement of SelectR over all other algorithms is statistically significant with p values ( computed using McNemar \u2019 s test for comparing MinLoss and SelectR using best models selected based on devset ) of 1e-16 , 0.03 , and 1.7e-18 for NQueens , Futoshiki and Sudoku respectively . We have made this explicit in the newly uploaded version of the paper . If there are any additional concerns , please let us know and we will be happy to address them . * * Significant digits in std dev * * We have reported both mean and std error up to 2 decimal places . We are not sure why the number of significant digits in std error should be less than that in the mean . We would appreciate it if you could elaborate , or share a reference . * * Discussion on experiment depicted in figure 2 * * First , we would like to point out that due to an oversight , the curves depicted in Figure 2 correspond to the models trained using a randomly picked seed value for each of the algorithms and not for the best model ( max ) chosen based on the validation set as reported in Table 2 . We apologize for this mistake which has been corrected in the newly uploaded version of the paper . Trends remain the same in the updated graph , but the effects are more pronounced . Below , we give further details as requested in the review : 1 . The trained models are fixed across all the points in the Figure for each of the algorithms . These models exactly correspond to those used for obtaining the results reported in Table 2 ( max models selected using a validation set ) . The test data is also the same as used for computing Table 2 results . The only difference is that now we have divided the test data into different bins based on the number of possible solutions for each test input ( x ) for further analysis and insights . 2.By construction , the number of test points with a unique solution is equal to the total number of test points with more than one solution [ Section 4.1 # Sudoku ] . Further , while creating the puzzles with more than one solution , we ensured uniform distribution of the number of filled cells from 17 to 34 , as is done in Palm et al.2018 , for creating puzzles with unique solutions in their paper . Hence , the number of points across different bins ( representing solution count ) may not be the same in Figure 2 . In the appendix , we have added a plot showing the average size of each bin and the average number of filled cells for queries in a bin . 3.As correctly pointed by the reviewer , as we move to the right in the graph ( i.e. , increase the number of solutions for a given problem ) , the number of filled cells in the corresponding Sudoku puzzles decreases , resulting in harder problems . This is also demonstrated by the corresponding decrease in the performance of all the models in Figure 2 . SelectR is most robust to this decrease in performance . 4.We have added 95 % confidence intervals for the points in Figure 2 in the newly uploaded version of the paper . All our differences are statistically significant ( comparison is across max models for each of the algorithms ) . We have also added the above details in the main paper and appendix . We will be happy to answer additional questions if any ."}, {"review_id": "ATp1nW2FuZL-2", "review_text": "1.Summary & contributions This paper formalizes the 'one-of-many-learning ' problem and proposes a method for solving this problem . The paper first defines the one-of-many-learning problem , where a model must learn to map an input x to one of many possible targets y , where all possible y or only a subset may be given , in particular in a combinatorial setting . The paper explains failures of na\u00efve approaches such as summing the loss over all possible y or only taking into account the y which has the lowest loss for the current model . The paper then introduces the 'SelectR ' framework where a separate neural network model ( the selection module ) is trained to predict which of the targets should be used for training the main model . The models are trained jointly using RL , where the selection module is trained to match the prediction of the main model and the main model minimizes the loss on the target selected by the selection module . The authors demonstrate improved results compared to different ( relatively simple ) baselines in three different constraint satisfaction problems : N-Queens , Futoshiki and Sudoku . 2.Strengths & weaknesses This paper addresses an interesting problem which is well motivated , well defined and clearly and extensively related to other problem settings from the literature . The paper explains the problem clearly and gives good examples of failure modes of na\u00efve approaches , motivating an alternate approach . The paper shows that indeed performance improvements can be achieved by using such an alternate approach which explicitly considers the one-of-many-learning problem , by training a selection module to select the target used for training dynamically , in a joint fashion using RL . The main weakness of the paper is that the specific proposed solution/framework ( training a selection module using RL ) is in my opinion not well motivated . The motivating problem ( with the na\u00efve MinLoss strategy ) seems to be ( lack of ) exploration : ca n't this be addressed simply by adding some randomness ( e.g.sampling target proportional to loss or model probability ) . Why is a separate neural network module needed ? Also , I do not understand the reward structure ( # predicted variables equal to main model ) . It seems that the selection module is trained to basically match the prediction of the main model , and the paper states that this is 'since we do not know a-priori which y is optimal for defining the loss ' . How does training a separate model to match the main model prediction help overcome this problem ? While the results do show the benefit of the 'SelectR ' framework , I would like to see them compared to a simple ( informed ) strategy such as sampling a target according to model probability / loss / MinLoss with some epsilon probability of using a random target . The results may help in answering above questions . Although I like the writing in general , I think the paper uses too much math notation and formalism . For example , the concepts of MinLoss and SelectR are relatively simple , but the notation in terms of one-hot w_ij makes things unnecessarily hard ( er ) to read and complicated . I think also the Lemma 's and examples would be better explained with words than with heavy math , to help understanding . As a bonus , removing a lot of the math would allow for very helpful Algorithm 1 and maybe Figure 3 of the Appendix to be included in the main text . 3.Recommendation My current assessment is that the paper is marginally below the acceptance threshold . 4.Arguments for recommendation The paper addresses a well motivated and well explained problem and the results obtained using SelectR improve over the baselines , but it does not convince ( enough ) that such a ( relatively complicated ) approach is actually beneficial from a practical point of view as simpler alternatives are underexplored ( see strengths & weaknesses ) . Additionally , I think the paper should use less formal notation as that will make the paper easier to read without losing the content . 5.Questions to authors See also strengths & weaknesses - Do I understand correctly ( from the formula for the cross entropy loss function ) that Example 1 assumes a model which predicts y_1 and y_2 independently ? I can imagine an autoregressive ( structured ) model which has p ( y_1 = 1 ) = 0.5 and p ( y_2 = 1|y1=1 ) = 0 and p ( y_2 = 1|y_1 = 0 ) = 1 so p ( 1,0 ) =p ( 0,1 ) =0.5 , which is optimal . It seems to me that the problem arises because of the independence structure of the variables in the model combined with the loss function ( i.e.summing the log-probs for all targets ) . - Why does Lemma 1 , which is posed as a formalization of the problem arising in Example 1 , consider a zero-one loss whereas Example 1 is based on a cross-entropy loss ? This is confusing to me . - The paper repeatedly mentions a 'prediction y^hat ' as output of the main model . How is this defined ? The model outputs a ( structured ) probability distribution but y^hat seems a vector . Is this vector a sample/argmax solution ? 6.Additional feedback Minor comments/suggestions : - The paper claims that compared to Neural Program Synthesis ( NPS ) , where a generated program can be verified , in the setting considered in the paper there is no such additional signal available . However , the experiments all consist of problems where a solution can be verified easily , even if it is outside the target set , so this does seem similar to NPS . - It seems that forcing all probability mass to concentrate on one target can be helpful for some models ( i.e.Example 1 if we assume independence of the variables ) , but may also be ( unnecessarily ) restrictive for other models which could more easily divide the probability mass . Maybe this would be interesting to discuss/investigate . - The paper notes that 'one could also backpropagate the gradient of the expected loss given Pr ( y_ij ) ' . This seems preferred to sampling , so a bit more discussion on why this was or was not used would be interesting . - I would not consider the parameters of the main model as 'input ' for the selection module . I would just say it takes as input the prediction from the main model .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the detailed review . Below , we have tried to address all of your concerns . Please let us know if we have left out anything , or something needs additional clarification . * * Regarding a separate module for exploration and alternative of exploring based on model probability * * We agree with the reviewer that the primary issue with MinLoss based approach is lack of exploration . And a simple strategy to incorporate exploration could be either based on using the probabilities of each of the possible target labels ( $ y $ \u2019 s ) or use an epsilon greedy RL approach . Such a simple strategy , though partially effective , may not extract the full information from the underlying problem structure and the associated solution landscape . Particularly , such a strategy would be local in nature , since it makes an independent decision for each data point . On the other hand , we argue that optimal exploration strategy may depend in complex ways on all the data points , viewed collectively , and could turn out to be significantly more complex , depending on the nature of the problem . In order to validate this thesis experimentally , based on the reviewer \u2019 s suggestion , we experimented with a simpler strategy that decides exploration based on the probability assigned by the prediction network to the targets in the set $ Y_x $ , referred to as I-ExplR ( Informed Exploration Strategy ) . We find that , in line with our thesis , I-ExplR performs better than MinLoss on all the datasets . Further , the performance of I-ExplR is worse than our strategy of using a neural network ( $ G_\\phi $ ) for exploration . As a meta-point , we would like to stress that , in our work , SelectR refers to the strategy of selecting a training label for each input , using an RL based framework . I-ExplR can be seen as a degenerate RL strategy which has an in-built exploration strategy but no learning . We have added relevant details in the paper including additional experiments . Besides contributing SelectR , our paper \u2019 s contributions include defining the novel ML setting , and providing a solution framework for the problem of solution multiplicity . Also , we will acknowledge the anonymous reviewer for suggesting I-ExplR . * * Reward structure * * Our reward structure is straightforward : for a given true label $ \\bar { y } $ , which is one of the possible solutions for input $ x $ , the reward for selecting this label for further training the model is the Hamming distance of this label from the prediction $ \\hat { y } $ , given the current model . Therefore , labels which are closer to the current prediction of the model will incur more reward , which makes intuitive sense . Note that If we were given an oracle to tell us which $ \\bar { y } $ is most suited for training $ M_\\Theta $ , we wouldn \u2019 t have to use this proxy of Hamming distance . Instead , we would have trained the selection module to match the oracle . We will be happy to add more details to clarify this in the paper . * * Math notation and formalism ; Algorithm 1 and figure 3 in the main paper * * Thank you for your feedback . In the next revision , we will be happy to add more explanations and intuitions in plain language to explain the concepts introduced in the paper . If there are any specific places , where you would like to see this happen , it will be great to get this feedback . However , we feel that since we are defining a new problem , it is important to lay down the foundations with proper formalism so that anyone working on it in the future has access to crisp definitions and proofs along with a clear understanding of the underlying concepts . We agree regarding Algorithm 1 and Figure 3 , that they add clarity to the paper : we plan to use the extra page to move them to the main paper from the appendix . * * Autoregressive ( structured ) model and independence of predictions * * You correctly point out that the issue of solution multiplicity primarily arises due to the non-autoregressive model , where variables in the structured output space are decoded simultaneously ( and therefore independently ) from the underlying embeddings . Notably , most of the current state-of-the-art neural models for solving combinatorial problems work with non autoregressive models , e.g.SATNET [ 1 ] , NLM [ 2 ] , RRN [ 3 ] , CNN for solving Sudoku [ 4 ] . We believe this might be because of their high efficiency of training and inference , since they do not have to decode the solution sequentially . We know of one variant of autoregressive models for our task [ 4 ] , which predicts one output label at a time , but for each such prediction , it solves the full non-autoregressive decoding task . So , solution multiplicity problem will likely be a factor there too . Examining the value of 1oML for auto-regressive models will be a direction for future work after strong auto-regressive models for this task get developed . We will clarify in the paper that , for now , the scope of this work is within non-autoregressive models . * continued .. *"}, {"review_id": "ATp1nW2FuZL-3", "review_text": "This paper aims at devising a targeted approach that takes into account the specific structure of combinatorial problems with multiple solutions . The proposed approach leverages RL to select the best targets among the solution sets at each iteration . SelectR convincingly outperforms both the naive and a cleverer baseline , showcasing the applicability of the method . Originality The problem of interest is relevant to a number of machine learning applications but has largely been ignored by the community up until now , as is made clear in the very complete related works section of the paper . Notably , the question of selecting the best target for learning is , as far as I am aware , novel . Consequently , so is both the approach and the baselines it is compared against . Significance As ML practitioners try their hand at more and more complex problems , this approach will become more and more relevant . Further , since this paper is the first to define the one-of-many problem , sets out to define the general framework , and defines reasonable baseline , it is very relevant . The effort made to link the problem of interest with existing other problems mean that it 's easier for readers to draw parallels , and helps bolster the paper 's significance . For instance , the experimental results tend to show that naively using multiple possible solutions is worse than ignoring these data points . This is in direct contradiction with the general consensus for tasks such as machine translation , where multi-solution datasets are not available , but are longed for . Clarity Overall , the paper is well-written and easy to follow . A couple of things might be made clearer , though , including : - the description of the pretraining regimen , which is a bit convoluted . It would probably help to refer to the internal M for the selection module as a target network , which it seems to be . - the description of the reward for the selection module is a bit complicated too , and the the fact that solutions can be split into r components could be reminded here . It would also be helpful to give more insight into why this reward was chosen ( I imagine this partial-reward makes it easier to 'see ' some reward than a reward for exact matching , but I 'm speculating here ) , and what the consequences of this choice are ( aside from improved performance ) . Does the selection module opt for the 'easiest ' targets to predict ? Do the targets chance as training goes along ? Could the selection module be trained at a meta-level rather than at the transition-level ? Overall , this is a nicely-written paper offering a novel approach to a significant problem , and showcasing its performance improvements . It would make a nice addition to this year 's ICLR .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for a very encouraging feedback and review . Below we try to address the few concerns raised by the reviewer regarding clarity in some of the aspects . * * Description of the pre-training regimen * * Here are the two steps of pre-training : ( a ) $ M_\\Theta $ is pre-trained using either MinLoss on all the data or using MinLoss over data points with unique solutions , which is decided using a validation set . ( b ) $ \\Theta $ parameters learned in step ( a ) are copied into $ M_ { \\Theta_- } $ , the copy of the prediction network inside the selection module . Keeping $ \\Theta $ and $ M_ { \\Theta_- } $ fixed , $ G_\\phi $ is trained . In this sense , $ G_\\phi $ is being trained to mimic ( maximize ) the reward obtained using the prediction from the $ M_\\Theta $ network . If you meant to say that the copy of $ M $ inside the selection module may be called \u2018 target network \u2019 since $ G_\\phi $ is trying to mimic it , this may not be entirely correct . Since the reward to train $ G_\\phi $ is coming externally based on the similarity of prediction with the chosen target , i.e. , based on Hamming distance between the two , which is not identical to the probabilities used in computing the cross-entropy loss . We will add more details to clarify this in the paper . * * Description of the reward * * The job of the selection module is to pick one target , $ \\bar { y } $ , among the possible targets for each input ( $ x $ ) for training the prediction network $ M_\\Theta $ . Intuitively , $ \\bar { y } $ would be a good choice for training $ M_\\Theta $ , if it is \u201c easier \u201d for the model to predict $ \\bar { y } $ . In our reward design , we measure this degree of ease using hamming distance between $ \\bar { y } $ and $ M_\\Theta $ prediction $ \\hat { y } $ . Specifically , the reward associated with a certain choice of $ \\bar { y } $ is the similarity of the current prediction of the model , $ \\hat { y } $ , with $ \\bar { y } $ where the similarity is measured in terms of the number of matching grid cells . This detail is described in the paragraph preceding Equation 6 in the paper . We will make it more prominent in the updated version of the paper to have additional clarity . We note that if we had an oracle to tell us which $ \\bar { y } $ to train on for a given input ( $ x $ ) , to get the optimal model , the selection module can be trained using that oracle . Since we do not have such an oracle , a reward based scheme as described above is used . In general , the selection network would pick the \u201c easiest \u201d target for input ( $ x $ ) ( with the notion of easiness as defined above ) . Since there is an exploration and exploitation trade-off in an RL setting , often it might pick those targets which are not \u201c easy \u201d ( i.e. , with greater hamming distance from the current model prediction ) as a way of exploration . In order to examine the number of target switchings during training , we performed the experiment described below . * * Do the targets change as training goes along ? * * Based on your question , we performed some additional experiments where we calculated the fraction of data points with more than one solution for which the MinLoss target ( closest to the current prediction ) switched as learning proceeds . We note that the change in the MinLoss based target is an important indicator since this would tell us that the closest point to current model prediction has moved , presumably due to RL based exploration . We observed an interesting trend for all the datasets : In the beginning , this fraction increases as the learning proceeds , indicating that a lot of switching of targets was happening . The switching fraction being small in the beginning can be explained by the fact that pre-trained model parameters ( $ \\Theta $ ) have just started to move . This fraction increases as learning proceeds due to RL based exploration in SelectR and simultaneous changing of the parameters $ \\Theta $ of the prediction network $ M $ . Towards the end of training , this fraction again comes down showing that learning had stabilized ; in our experiments , the value remained high for Sudoku , which we believe is due to the fact that we only had a subset of all possible solutions for training . * * Training selection module at a meta-level * * We did not quite understand what you mean by the comment \u201c Could the selection module be trained at a meta-level rather than at the transition-level ? \u201d We request you to please elaborate , and we will be happy to respond ."}], "0": {"review_id": "ATp1nW2FuZL-0", "review_text": "The quality and clarity of the paper is good overall . In my opinion the presentation is clear , the goal of the work , and the proposed solutions are presented cleanly . Authors give a few examples of the issues raised by learning from many correct solutions , which is appreciated . In terms of significance . I believe the theory is not surprising at all , it is straightforward to see that eq . ( 1 ) will not be a consistent loss function for generalization . Now , in Lemma 2 , given the definition of eq . ( 2 ) , it is also not surprising to see that it is a consistent estimator , and in fact , the proofs are rather trivial . Thus , from the theoretical viewpoint these issues undermine the paper . From the practical viewpoint , authors show that their proposed method SelectRL is better than other baselines , and my main concern in the practicality of the algorithm is that I do n't see a strong case where SelectRL is significantly better than MinLoss , at least not statistically . Thus , my question here is , can authors claim that SelectRL is better than MinLoss from the experiments ? If so , the gain seems small , and computationally speaking training an RL agent to select a solution seems an overkill . Another question is : in the Random baseline , was the solution being picked uniformly at random ? If so , would n't a distribution that is concentrated around a solution be better ? I would 've liked to see this in the experiments . If authors can comment on this question would be appreciated .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your crisp review . Below we make an attempt to assuage all your concerns . Please let us know if more clarification would help and we would be happy to provide it . * * Theory is not surprising at all * * We would like to note that in this work , we are defining a new problem and one of the primary contributions of our work is in laying out the framework for dealing with solution multiplicity , formally introducing all the definitions , fundamental theorems and their proofs , even if some of them are natural and not surprising . Once the concepts are crisply and clearly defined , the proofs may appear trivial to expert readers . To the best of our knowledge , the problem of solution multiplicity has not been hitherto addressed in the literature . Our major contributions in this work can be summarized as follows : 1 . Define a novel problem . 2.Propose an intuitive , yet strong baseline along with its underlying theory and mathematical foundation ( MinLoss ) , and , 3 . Propose an RL based method to overcome the shortcomings of the greedy baseline via exploration ( SelectR ) . In addition , we have also presented an extensive set of experiments showing the efficacy of our proposed model over the baseline approaches which ignore solution multiplicity altogether . * * Can authors claim that SelectRL is better than MinLoss from the experiments ? * * Based on our experiments , we do believe that our SelectR based approach performs better than MinLoss . Table 3 ( appendix ) gives detailed numbers along with the standard error . We would like to note that the standard errors reported here are over variations in the choice of different random seeds in the experiments . Since it is difficult to do a large number of such experiments ( with varying seeds ) due to high computational complexity , we have also reported the comparisons using the ( max ) model based on a validation set for each of the algorithms . In both the settings of ( mean ) and ( max ) , our SelectR based approach does better than MinLoss . In fact , SelectR performs better than MinLoss for each of the three seeds independently in all the experiments . We note that starting with the same seed in our implementation means that the initialization of the prediction network is the same . We have added a table ( Table 4 ) in the appendix for seed-wise comparison of the gains of SelectR over MinLoss . In addition , we would like to note that the standard error arising out of the variation in the test set , is very very small ( of the order of less than 0.005 ) , and improvement of SelectR over all other algorithms is statistically significant with p values ( computed using McNemar \u2019 s test for comparing MinLoss and SelectR using best models selected based on devset ) of 1e-16 , 0.03 , and 1.7e-18 for NQueens , Futoshiki and Sudoku respectively . We also refer to the discussion about using a simpler exploration strategy in response to AnonReviewer4 ; please see the comment on \u201c * Regarding a separate module for exploration and alternative of exploring based on model probability * \u201d . * * Computationally speaking training an RL agent to select a solution seems an overkill . * * Computationally , we note that training time for SelectR is about 1.5 times more than MinLoss on average . Inference time is identical in both cases since it is only a forward propagation through the trained $ M_\\Theta $ network . * * In the Random baseline , was the solution being picked uniformly at random ? * * Yes , we did pick the solution to be trained on uniformly at random among the set of possible solutions for each input ( $ x $ ) . Once this was picked , we trained the entire algorithm using this solution ( for a given input ( $ x $ ) ) . Choosing a uniform distribution seemed like a natural choice for a baseline since we did not have any natural preference for one solution over the others at the start of the training . We could possibly pick the solution to train on based on a distribution centered around a solution , but it is not clear what solution to centre the distribution on , and what this distribution should be . A more sophisticated strategy would pick a solution based on current model parameters in every iteration , but that is exactly what MinLoss and SelectR do ."}, "1": {"review_id": "ATp1nW2FuZL-1", "review_text": "Summary : The authors work in the domain of applying neural networks to combinatorial problems with structured output space , such as sudoku and n-queens . They notice how models currently performing well at this task encounter difficulties when there are multiple possible solutions . They formalize the task of learning any of multiple given ( and possibly quite different ) labels and propose an RL based approach to solve that task . They show improvements over selected baselines . Great : * The discussion of why the task is different from other instances of multiple labels is well argued and clear . * Numerical examples are very helpful in following the description of the algorithms . * The evaluation setting is well thought of : utilizing the state of the art model and comparing it to reasonable baselines ( one of which is indeed current SoTA ) . The experiments seem reproducible , given the detailed descriptions in the appendix . Could be improved : * Table 2 is perhaps misleading . Table 3 with the same results with standard deviation gives a less clear answer on whether SelectR is actually always better than MinLoss ( careful with significant digits , the standard deviation can \u2019 t have more than one ! ) . * The experiment depicted in figure 2 isn \u2019 t discussed , it doesn \u2019 t report confidence intervals , the distribution of training samples is not discussed ( are there significantly less training examples with 50+ solutions , that could justify the drop in performance ? A sudoku with this many solutions is likely to be quite sparse , does that affect performance ? ) . In summary , the paper introduces and formalizes an interesting novel task in the context of combinatorial problems with structured output space . While aspects in the evaluation could be clarified , the paper is clear and interesting . I recommend an accept , and would be willing to increase the score if my concerns are addressed .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your thoughtful review . Below we hope to address the concerns raised by you . Please let us know if there are still any doubts and we would be happy to address them . * * Table 2 is perhaps misleading . Is SelectR actually better than MinLoss * * Thanks for the careful read of the appendix . We believe your concern arises out of the seemingly \u201c high \u201d standard error numbers reported in Table 3 . We would like to note here that the standard errors reported here are over variations in the choice of different random seeds in the experiments . Since it is difficult to do a large number of such experiments ( with varying seeds ) due to high computational complexity , we have also reported the comparisons using the ( max ) model based on a validation set for each of the algorithms . In both the settings of ( mean ) and ( max ) , our SelectR based approach does better than MinLoss . In fact , SelectR performs better than MinLoss for each of the three seeds independently in all the experiments . We note that starting with the same seed in our implementation means that the initialization of the prediction network is the same . We have added a table ( Table 4 ) in the appendix for seed-wise comparison of the gains of SelectR over MinLoss . In addition , we would like to note that the standard error arising out of the variation in the test set , is very very small ( of the order of less than 0.005 ) , and improvement of SelectR over all other algorithms is statistically significant with p values ( computed using McNemar \u2019 s test for comparing MinLoss and SelectR using best models selected based on devset ) of 1e-16 , 0.03 , and 1.7e-18 for NQueens , Futoshiki and Sudoku respectively . We have made this explicit in the newly uploaded version of the paper . If there are any additional concerns , please let us know and we will be happy to address them . * * Significant digits in std dev * * We have reported both mean and std error up to 2 decimal places . We are not sure why the number of significant digits in std error should be less than that in the mean . We would appreciate it if you could elaborate , or share a reference . * * Discussion on experiment depicted in figure 2 * * First , we would like to point out that due to an oversight , the curves depicted in Figure 2 correspond to the models trained using a randomly picked seed value for each of the algorithms and not for the best model ( max ) chosen based on the validation set as reported in Table 2 . We apologize for this mistake which has been corrected in the newly uploaded version of the paper . Trends remain the same in the updated graph , but the effects are more pronounced . Below , we give further details as requested in the review : 1 . The trained models are fixed across all the points in the Figure for each of the algorithms . These models exactly correspond to those used for obtaining the results reported in Table 2 ( max models selected using a validation set ) . The test data is also the same as used for computing Table 2 results . The only difference is that now we have divided the test data into different bins based on the number of possible solutions for each test input ( x ) for further analysis and insights . 2.By construction , the number of test points with a unique solution is equal to the total number of test points with more than one solution [ Section 4.1 # Sudoku ] . Further , while creating the puzzles with more than one solution , we ensured uniform distribution of the number of filled cells from 17 to 34 , as is done in Palm et al.2018 , for creating puzzles with unique solutions in their paper . Hence , the number of points across different bins ( representing solution count ) may not be the same in Figure 2 . In the appendix , we have added a plot showing the average size of each bin and the average number of filled cells for queries in a bin . 3.As correctly pointed by the reviewer , as we move to the right in the graph ( i.e. , increase the number of solutions for a given problem ) , the number of filled cells in the corresponding Sudoku puzzles decreases , resulting in harder problems . This is also demonstrated by the corresponding decrease in the performance of all the models in Figure 2 . SelectR is most robust to this decrease in performance . 4.We have added 95 % confidence intervals for the points in Figure 2 in the newly uploaded version of the paper . All our differences are statistically significant ( comparison is across max models for each of the algorithms ) . We have also added the above details in the main paper and appendix . We will be happy to answer additional questions if any ."}, "2": {"review_id": "ATp1nW2FuZL-2", "review_text": "1.Summary & contributions This paper formalizes the 'one-of-many-learning ' problem and proposes a method for solving this problem . The paper first defines the one-of-many-learning problem , where a model must learn to map an input x to one of many possible targets y , where all possible y or only a subset may be given , in particular in a combinatorial setting . The paper explains failures of na\u00efve approaches such as summing the loss over all possible y or only taking into account the y which has the lowest loss for the current model . The paper then introduces the 'SelectR ' framework where a separate neural network model ( the selection module ) is trained to predict which of the targets should be used for training the main model . The models are trained jointly using RL , where the selection module is trained to match the prediction of the main model and the main model minimizes the loss on the target selected by the selection module . The authors demonstrate improved results compared to different ( relatively simple ) baselines in three different constraint satisfaction problems : N-Queens , Futoshiki and Sudoku . 2.Strengths & weaknesses This paper addresses an interesting problem which is well motivated , well defined and clearly and extensively related to other problem settings from the literature . The paper explains the problem clearly and gives good examples of failure modes of na\u00efve approaches , motivating an alternate approach . The paper shows that indeed performance improvements can be achieved by using such an alternate approach which explicitly considers the one-of-many-learning problem , by training a selection module to select the target used for training dynamically , in a joint fashion using RL . The main weakness of the paper is that the specific proposed solution/framework ( training a selection module using RL ) is in my opinion not well motivated . The motivating problem ( with the na\u00efve MinLoss strategy ) seems to be ( lack of ) exploration : ca n't this be addressed simply by adding some randomness ( e.g.sampling target proportional to loss or model probability ) . Why is a separate neural network module needed ? Also , I do not understand the reward structure ( # predicted variables equal to main model ) . It seems that the selection module is trained to basically match the prediction of the main model , and the paper states that this is 'since we do not know a-priori which y is optimal for defining the loss ' . How does training a separate model to match the main model prediction help overcome this problem ? While the results do show the benefit of the 'SelectR ' framework , I would like to see them compared to a simple ( informed ) strategy such as sampling a target according to model probability / loss / MinLoss with some epsilon probability of using a random target . The results may help in answering above questions . Although I like the writing in general , I think the paper uses too much math notation and formalism . For example , the concepts of MinLoss and SelectR are relatively simple , but the notation in terms of one-hot w_ij makes things unnecessarily hard ( er ) to read and complicated . I think also the Lemma 's and examples would be better explained with words than with heavy math , to help understanding . As a bonus , removing a lot of the math would allow for very helpful Algorithm 1 and maybe Figure 3 of the Appendix to be included in the main text . 3.Recommendation My current assessment is that the paper is marginally below the acceptance threshold . 4.Arguments for recommendation The paper addresses a well motivated and well explained problem and the results obtained using SelectR improve over the baselines , but it does not convince ( enough ) that such a ( relatively complicated ) approach is actually beneficial from a practical point of view as simpler alternatives are underexplored ( see strengths & weaknesses ) . Additionally , I think the paper should use less formal notation as that will make the paper easier to read without losing the content . 5.Questions to authors See also strengths & weaknesses - Do I understand correctly ( from the formula for the cross entropy loss function ) that Example 1 assumes a model which predicts y_1 and y_2 independently ? I can imagine an autoregressive ( structured ) model which has p ( y_1 = 1 ) = 0.5 and p ( y_2 = 1|y1=1 ) = 0 and p ( y_2 = 1|y_1 = 0 ) = 1 so p ( 1,0 ) =p ( 0,1 ) =0.5 , which is optimal . It seems to me that the problem arises because of the independence structure of the variables in the model combined with the loss function ( i.e.summing the log-probs for all targets ) . - Why does Lemma 1 , which is posed as a formalization of the problem arising in Example 1 , consider a zero-one loss whereas Example 1 is based on a cross-entropy loss ? This is confusing to me . - The paper repeatedly mentions a 'prediction y^hat ' as output of the main model . How is this defined ? The model outputs a ( structured ) probability distribution but y^hat seems a vector . Is this vector a sample/argmax solution ? 6.Additional feedback Minor comments/suggestions : - The paper claims that compared to Neural Program Synthesis ( NPS ) , where a generated program can be verified , in the setting considered in the paper there is no such additional signal available . However , the experiments all consist of problems where a solution can be verified easily , even if it is outside the target set , so this does seem similar to NPS . - It seems that forcing all probability mass to concentrate on one target can be helpful for some models ( i.e.Example 1 if we assume independence of the variables ) , but may also be ( unnecessarily ) restrictive for other models which could more easily divide the probability mass . Maybe this would be interesting to discuss/investigate . - The paper notes that 'one could also backpropagate the gradient of the expected loss given Pr ( y_ij ) ' . This seems preferred to sampling , so a bit more discussion on why this was or was not used would be interesting . - I would not consider the parameters of the main model as 'input ' for the selection module . I would just say it takes as input the prediction from the main model .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the detailed review . Below , we have tried to address all of your concerns . Please let us know if we have left out anything , or something needs additional clarification . * * Regarding a separate module for exploration and alternative of exploring based on model probability * * We agree with the reviewer that the primary issue with MinLoss based approach is lack of exploration . And a simple strategy to incorporate exploration could be either based on using the probabilities of each of the possible target labels ( $ y $ \u2019 s ) or use an epsilon greedy RL approach . Such a simple strategy , though partially effective , may not extract the full information from the underlying problem structure and the associated solution landscape . Particularly , such a strategy would be local in nature , since it makes an independent decision for each data point . On the other hand , we argue that optimal exploration strategy may depend in complex ways on all the data points , viewed collectively , and could turn out to be significantly more complex , depending on the nature of the problem . In order to validate this thesis experimentally , based on the reviewer \u2019 s suggestion , we experimented with a simpler strategy that decides exploration based on the probability assigned by the prediction network to the targets in the set $ Y_x $ , referred to as I-ExplR ( Informed Exploration Strategy ) . We find that , in line with our thesis , I-ExplR performs better than MinLoss on all the datasets . Further , the performance of I-ExplR is worse than our strategy of using a neural network ( $ G_\\phi $ ) for exploration . As a meta-point , we would like to stress that , in our work , SelectR refers to the strategy of selecting a training label for each input , using an RL based framework . I-ExplR can be seen as a degenerate RL strategy which has an in-built exploration strategy but no learning . We have added relevant details in the paper including additional experiments . Besides contributing SelectR , our paper \u2019 s contributions include defining the novel ML setting , and providing a solution framework for the problem of solution multiplicity . Also , we will acknowledge the anonymous reviewer for suggesting I-ExplR . * * Reward structure * * Our reward structure is straightforward : for a given true label $ \\bar { y } $ , which is one of the possible solutions for input $ x $ , the reward for selecting this label for further training the model is the Hamming distance of this label from the prediction $ \\hat { y } $ , given the current model . Therefore , labels which are closer to the current prediction of the model will incur more reward , which makes intuitive sense . Note that If we were given an oracle to tell us which $ \\bar { y } $ is most suited for training $ M_\\Theta $ , we wouldn \u2019 t have to use this proxy of Hamming distance . Instead , we would have trained the selection module to match the oracle . We will be happy to add more details to clarify this in the paper . * * Math notation and formalism ; Algorithm 1 and figure 3 in the main paper * * Thank you for your feedback . In the next revision , we will be happy to add more explanations and intuitions in plain language to explain the concepts introduced in the paper . If there are any specific places , where you would like to see this happen , it will be great to get this feedback . However , we feel that since we are defining a new problem , it is important to lay down the foundations with proper formalism so that anyone working on it in the future has access to crisp definitions and proofs along with a clear understanding of the underlying concepts . We agree regarding Algorithm 1 and Figure 3 , that they add clarity to the paper : we plan to use the extra page to move them to the main paper from the appendix . * * Autoregressive ( structured ) model and independence of predictions * * You correctly point out that the issue of solution multiplicity primarily arises due to the non-autoregressive model , where variables in the structured output space are decoded simultaneously ( and therefore independently ) from the underlying embeddings . Notably , most of the current state-of-the-art neural models for solving combinatorial problems work with non autoregressive models , e.g.SATNET [ 1 ] , NLM [ 2 ] , RRN [ 3 ] , CNN for solving Sudoku [ 4 ] . We believe this might be because of their high efficiency of training and inference , since they do not have to decode the solution sequentially . We know of one variant of autoregressive models for our task [ 4 ] , which predicts one output label at a time , but for each such prediction , it solves the full non-autoregressive decoding task . So , solution multiplicity problem will likely be a factor there too . Examining the value of 1oML for auto-regressive models will be a direction for future work after strong auto-regressive models for this task get developed . We will clarify in the paper that , for now , the scope of this work is within non-autoregressive models . * continued .. *"}, "3": {"review_id": "ATp1nW2FuZL-3", "review_text": "This paper aims at devising a targeted approach that takes into account the specific structure of combinatorial problems with multiple solutions . The proposed approach leverages RL to select the best targets among the solution sets at each iteration . SelectR convincingly outperforms both the naive and a cleverer baseline , showcasing the applicability of the method . Originality The problem of interest is relevant to a number of machine learning applications but has largely been ignored by the community up until now , as is made clear in the very complete related works section of the paper . Notably , the question of selecting the best target for learning is , as far as I am aware , novel . Consequently , so is both the approach and the baselines it is compared against . Significance As ML practitioners try their hand at more and more complex problems , this approach will become more and more relevant . Further , since this paper is the first to define the one-of-many problem , sets out to define the general framework , and defines reasonable baseline , it is very relevant . The effort made to link the problem of interest with existing other problems mean that it 's easier for readers to draw parallels , and helps bolster the paper 's significance . For instance , the experimental results tend to show that naively using multiple possible solutions is worse than ignoring these data points . This is in direct contradiction with the general consensus for tasks such as machine translation , where multi-solution datasets are not available , but are longed for . Clarity Overall , the paper is well-written and easy to follow . A couple of things might be made clearer , though , including : - the description of the pretraining regimen , which is a bit convoluted . It would probably help to refer to the internal M for the selection module as a target network , which it seems to be . - the description of the reward for the selection module is a bit complicated too , and the the fact that solutions can be split into r components could be reminded here . It would also be helpful to give more insight into why this reward was chosen ( I imagine this partial-reward makes it easier to 'see ' some reward than a reward for exact matching , but I 'm speculating here ) , and what the consequences of this choice are ( aside from improved performance ) . Does the selection module opt for the 'easiest ' targets to predict ? Do the targets chance as training goes along ? Could the selection module be trained at a meta-level rather than at the transition-level ? Overall , this is a nicely-written paper offering a novel approach to a significant problem , and showcasing its performance improvements . It would make a nice addition to this year 's ICLR .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for a very encouraging feedback and review . Below we try to address the few concerns raised by the reviewer regarding clarity in some of the aspects . * * Description of the pre-training regimen * * Here are the two steps of pre-training : ( a ) $ M_\\Theta $ is pre-trained using either MinLoss on all the data or using MinLoss over data points with unique solutions , which is decided using a validation set . ( b ) $ \\Theta $ parameters learned in step ( a ) are copied into $ M_ { \\Theta_- } $ , the copy of the prediction network inside the selection module . Keeping $ \\Theta $ and $ M_ { \\Theta_- } $ fixed , $ G_\\phi $ is trained . In this sense , $ G_\\phi $ is being trained to mimic ( maximize ) the reward obtained using the prediction from the $ M_\\Theta $ network . If you meant to say that the copy of $ M $ inside the selection module may be called \u2018 target network \u2019 since $ G_\\phi $ is trying to mimic it , this may not be entirely correct . Since the reward to train $ G_\\phi $ is coming externally based on the similarity of prediction with the chosen target , i.e. , based on Hamming distance between the two , which is not identical to the probabilities used in computing the cross-entropy loss . We will add more details to clarify this in the paper . * * Description of the reward * * The job of the selection module is to pick one target , $ \\bar { y } $ , among the possible targets for each input ( $ x $ ) for training the prediction network $ M_\\Theta $ . Intuitively , $ \\bar { y } $ would be a good choice for training $ M_\\Theta $ , if it is \u201c easier \u201d for the model to predict $ \\bar { y } $ . In our reward design , we measure this degree of ease using hamming distance between $ \\bar { y } $ and $ M_\\Theta $ prediction $ \\hat { y } $ . Specifically , the reward associated with a certain choice of $ \\bar { y } $ is the similarity of the current prediction of the model , $ \\hat { y } $ , with $ \\bar { y } $ where the similarity is measured in terms of the number of matching grid cells . This detail is described in the paragraph preceding Equation 6 in the paper . We will make it more prominent in the updated version of the paper to have additional clarity . We note that if we had an oracle to tell us which $ \\bar { y } $ to train on for a given input ( $ x $ ) , to get the optimal model , the selection module can be trained using that oracle . Since we do not have such an oracle , a reward based scheme as described above is used . In general , the selection network would pick the \u201c easiest \u201d target for input ( $ x $ ) ( with the notion of easiness as defined above ) . Since there is an exploration and exploitation trade-off in an RL setting , often it might pick those targets which are not \u201c easy \u201d ( i.e. , with greater hamming distance from the current model prediction ) as a way of exploration . In order to examine the number of target switchings during training , we performed the experiment described below . * * Do the targets change as training goes along ? * * Based on your question , we performed some additional experiments where we calculated the fraction of data points with more than one solution for which the MinLoss target ( closest to the current prediction ) switched as learning proceeds . We note that the change in the MinLoss based target is an important indicator since this would tell us that the closest point to current model prediction has moved , presumably due to RL based exploration . We observed an interesting trend for all the datasets : In the beginning , this fraction increases as the learning proceeds , indicating that a lot of switching of targets was happening . The switching fraction being small in the beginning can be explained by the fact that pre-trained model parameters ( $ \\Theta $ ) have just started to move . This fraction increases as learning proceeds due to RL based exploration in SelectR and simultaneous changing of the parameters $ \\Theta $ of the prediction network $ M $ . Towards the end of training , this fraction again comes down showing that learning had stabilized ; in our experiments , the value remained high for Sudoku , which we believe is due to the fact that we only had a subset of all possible solutions for training . * * Training selection module at a meta-level * * We did not quite understand what you mean by the comment \u201c Could the selection module be trained at a meta-level rather than at the transition-level ? \u201d We request you to please elaborate , and we will be happy to respond ."}}