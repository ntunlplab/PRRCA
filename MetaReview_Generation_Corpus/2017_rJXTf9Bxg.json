{"year": "2017", "forum": "rJXTf9Bxg", "title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "decision": "Reject", "meta_review": "Ratings summary:\n 3: Clear rejection\n 6: Marginally above acceptance threshold\n 6: Marginally above acceptance threshold\n \n Clear easy to read paper focusing on generating higher quality higher resolution (128x128) pixel imagery with GANs. There were broad concerns however across reviewers that the work is lacking in clearly identifiable novelty. The author\u00d5s point to a list of novel elements of the work in their rebuttal. However, the most negative reviewer also has issues with the evaluation metrics used.\n \n Thus, unfortunately, the PCs believe that this work isn't ready to appear at the conference.", "reviews": [{"review_id": "rJXTf9Bxg-0", "review_text": "Apologies for the late review. This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample. Figure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs. Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised). The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples. It is found - not surprisingly - that higher resolution improves discriminability (because more information is present). Discriminability: Figure 3 doesn\u2019t have legends so it is a bit hard to understand what is going on. Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place. Diversity: The authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class. I think this method is significantly flawed and limited, for reasons mentioned in (Theis et al, 2015, A note on the evaluation\u2026). In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1. Evaluating diversity/entropy of samples in high dimensions is very hard, especially if the distributions involved are non-trivial for example concentrated around manifolds. Consider for example a generative model which randomly samples just two images. Assuming that the MSSSIM between these two images is -1, this generative model can easily achieve an average MSSSIM score of 0, implying a conclusion that this model has more diversity than the training data itself. Conversely, SSIM is designed not to be sensitive to contrast and average pixel intensity, so if a model is diverse in this sense, that will be ignored by this measure. Overall, the paper proposes a new way to incorporate class labels into training GAN-type models. As far as I know the particular algorithm is novel, but I consider it incremental compared to what has been done before. I think the proposed evaluation metrics are flawed, especially when evaluating the diversity of the samples for the aforementioned reasons.", "rating": "3: Clear rejection", "reply_text": "Thank you for the review . We have used `` > '' for quotes in the below response . As mentioned in the global response , we believe that the review misstates several key points in our paper . We have revised our paper for clarity and we respond to each point in detail below . ====================== VARIABILITY AND MS-SSIM ====================== 1 . \u201c The authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class . I think this method is significantly flawed and limited , for reasons mentioned in ( Theis et al , 2015 , A note on the evaluation\u2026 ) . In its behaviour , MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1. \u201d The main claim of Theis et al ( 2015 ) is that log-likelihood might not correspond to sample quality in a generative model . Furthermore , training a model based on one objective will not guarantee good performance under another objective . We view these points as orthogonal to our evaluation framework . If the reviewer views this differently , we would be interested to hear their perspective . That said , Theis et al ( 2015 ) provide an explicit motivation for us to build an evaluation method that is independent of the training objective . 2.In regards to the statement that \u2018 MS-SSIM is not that dissimilar from Euclidean distance \u2019 , we strongly disagree . We believe that this statement is not supported by the literature . To start with , SSIM is a highly nonlinear metric that has been constructed to reflect human perceptual judgements [ * ] . The original SSIM paper as well as follow up works on MS-SSIM have demonstrated that SSIM-based metrics provide substantially improved quantitative estimates of human perceptual judgements compared to simple Euclidean distance measures ( and many other quantitative measures of perceptual similarity ) [ 1 , 2 , 3 ; and references therein ] . A simple intuition for why SSIM is perceptually superior to Euclidean distance can be gained through looking at examples . The SSIM web page ( http : //www.cns.nyu.edu/~lcv/ssim/ ) shows that images which are drastically different perceptually can have the same MSE and quite different SSIM scores . [ 1 ] Wang , Zhou , Eero P. Simoncelli , and Alan C. Bovik . `` Multiscale structural similarity for image quality assessment . '' [ 2 ] Z. Wang , A. C. Bovik , H. R. Sheikh and E. P. Simoncelli , `` Image quality assessment : From error visibility to structural similarity '' [ 3 ] Kede Ma , Qingbo Wu , Zhou Wang , Zhengfang Duanmu , Hongwei Yong , Hongliang Li , and Lei Zhang . Group mad competition - a new methodology to compare objective image quality models . [ * ] Note that our MS-SSIM implementation ranges from [ 0 , 1 ] and not from [ -1 , 1 ] . All values on our graphs should be interpreted accordingly . 3. \u201c Evaluating diversity/entropy of samples in high dimensions is very hard , especially if the distributions involved are non-trivial for example concentrated around manifolds . Consider for example a generative model which randomly samples just two images . Assuming that the MSSSIM between these two images is -1 , this generative model can easily achieve an average MSSSIM score of 0 , implying a conclusion that this model has more diversity than the training data itself. \u201d We claim that this example is seriously flawed , for the following reasons : a . The range of MS-SSIM is [ 0 , 1 ] ( see above ) , so the average would be 0.5 , which corresponds to half as much diversity as in the least diverse ImageNet class . b.This type of memorization would manifest as high variance of the mean MS-SSIM , but we report lower variance for samples than for training data ( In other words , we explicitly account for this in the metric itself ) . One could also compute higher moments or look at a histogram of pairwise MS-SSIM values , etc . c. We have also ruled out memorization explicitly by examining both latent space interpolations and nearest neighbors ( both by L1 and MS-SSIM ) . d. We have provided 10,000 sample images , which do not show this type of memorization . e. It \u2019 s not even clear how well other existing methods of measuring diversity would perform in this case . 4.The reviewer suggests that in an ideal world we would compute the entropy of the generator distribution . We argue that -- even if this were feasible -- we would still want to use perceptually calibrated metrics such as MS-SSIM . Namely , we might want to ignore variability due to contrast or pixel intensity in favor of diversity of image content . 5. \u201c Conversely , SSIM is designed not to be sensitive to contrast and average pixel intensity , so if a model is diverse in this sense , that will be ignored by this measure. \u201d We think that this is a good thing ( see above ) . This is a sense in which Euclidean distance measures are very different than MS-SSIM , which directly contradicts the reviewer \u2019 s earlier point . ============= DISCRIMINABILITY =============== 1 . \u201c Discriminability : Figure 3 doesn \u2019 t have legends so it is a bit hard to understand what is going on . Furthermore , my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image , which is much more like a blurring , very different from retraining all the models to work on low resolution in the first place. \u201d We feel that there was a large misunderstanding about our methods , so we present a quick summary of what was done to measure discriminability : Our goal in this analysis was to measure how much of the output resolution is actually used by the image synthesis model . In other words , does a 128x128 model just produce 32x32 images that are naively resized to 128x128 ? This is the goal of the \u2018 blurring \u2019 analysis and a question that has not been addressed in the literature . If a sample were a naive resizing , a blurring would not reduce its discriminability . For each image analyzed , and for each resolution in [ 16 , 32 , 64 , 128 , 256 ] , we iteratively resized the sample from its original size ( using bilinear interpolation ) to the resolution in question . We then passed the image to a pretrained Inception model , which resizes all inputs to 299x299 before processing . We took note of whether Inception correctly classified the input , and then we reported these results in the lower left hand corner of figure 3 . We have revised the manuscript to better describe these methods . 2. \u201c It is found - not surprisingly - that higher resolution improves discriminability ( because more information is present ) . \u201d If the reviewer means that simply increasing the resolution should result in higher discriminability : We did explicitly test for this by upsampling images to confirm that simply upsampling would not increase discriminability . See Figure 3 and Section 4.1 . If the reviewer means that it is unsurprising that the model is successfully making use of its available output resolution : Naive resizing is not an idle concern . We tested another model that actually failed to meaningfully increase discriminability score from 64x64 to 128x128 . 3 . `` [ the authors do not retrain ] all the models to work on low resolution in the first place '' . We did retrain models at a lower output resolution . We found that samples from these models are about half as discriminable at the 128x128 resolution as samples from the 128x128 model . The results of this experiment correspond to the blue curve in the lower left of Figure 3 . This procedure was also described in section 4.1 ."}, {"review_id": "rJXTf9Bxg-1", "review_text": "This is a clear, easy to read, highly relevant paper that improves GAN training for images and explores evaluation criterion on GANs. The main contributions are as follows: - Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to the real/fake objective improves performance. Generator is conditioned on 1-hot encoding of class and is trained to generate the specified class. - Training different models on different subsets of imagenet classes improves performance. - They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples (and detect mode collapse) - They show this metric correlates with a discriminability metric (classification accuracy of pre-trained imagenet model on generated samples) . The overall novelty of this approach is somewhat lacking in that previous methods have proposed training a classifier head on the discriminator and the discriminability metric proposed is simply the inception score of [1] except with class information. However, I think there is still a contribution to be made my putting these tricks together and successfully demonstrating image synthesis gains. Questions for the authors: (1) Why do you think splitting the imagenet training into 100 different models improves performance? Is the issue with the representation of the class? In other words, if an encoding more meaningful that 1-hot vector was used do you still think 100 models would be needed. Ideally we should hope that a generative model can leverage information from different classes to help with the generation of a particular class and also text-image synthesis models [2] have been quite successful when trained on diverse datasets (and these are conditioned on a semantically meaningful text encoding) which suggests to be that the issue is with the representation. (2) In section 3 the AC-GAN classification objective (omitting expectation for brevity) is given as L_S = log P(C=c|X_real) + log P(C=c|X_fake) and you say that both the discriminator and generator are trained to maximize this quantity. Obviously the generator would want to maximize log P(C=c|X_fake) for its given conditioning class c. But can you explain why you would want the discriminator to also maximize the classification accuracy of generated samples? Why not do something similar to the CatGAN paper [3] and train the discriminator to be as uncertain as possible about the generated examples. Seems counterintuitive to me to have both the generator and discriminator trying to optimize the same classification objective rather than not be adversarial wrt to this loss as well as the real/fake loss. Overall, this paper makes a clear contribution to GAN research both in terms of image quality and evaluation metrics and I would recommend it for acceptance. [1] Salimans et al. Improved Techniques for Training GANs (https://arxiv.org/abs/1606.03498) [2] Reed et al. Generative Adversarial Text to Image Synthesis (https://arxiv.org/abs/1605.05396) [3] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks (https://arxiv.org/abs/1511.06390) ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . We have used `` > '' for quotes in the below response . > The overall novelty of this approach is somewhat lacking in that previous methods have > proposed training a classifier head on the discriminator and the discriminability metric > proposed is simply the inception score of [ 1 ] except with class information . The novelty of our discriminability analysis is mostly that we measure the extent to which the model is making use of its given output resolution . Our work is the first to attempt this measurement . Note that this also allows us to single out classes for which high-frequency information is important ( e.g.zebra - see the green dot in Figure 3 ) . > Why do you think splitting the imagenet training into 100 different models improves > performance ? Is the issue with the representation of the class ? We stumbled upon this result when debugging previous GAN models . We do not have an explanation for this behavior beyond our empirical results . We also claim that this work highlights that the challenge of ImageNet is mainly the large number of classes . This result is consistent with [ 1 ] in which the image synthesis model is restricted to visually similar classes ( i.e.birds ) .We do not know whether this is an issue of representation or learning , but we think it is a fascinating question and one which we hope to address in future work . We have added some comments to the discussion accordingly . > But can you explain why you would want the discriminator to also maximize the classification > accuracy of generated samples ? The short answer is that it makes the results better . Our suspicion is that this is due to a sort of feedback loop : more training data for the discriminator makes the discriminator better , which is then reflected in the generator . [ 1 ] Reed et al.Generative Adversarial Text to Image Synthesis ( https : //arxiv.org/abs/1605.05396 )"}, {"review_id": "rJXTf9Bxg-2", "review_text": "This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets. Pros: + The paper is clear and well-written. + Experiments performed in the relatively under-explored 128 x 128 ImageNet setting. + The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models. Cons: - AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class. - Diversity metric is of limited use for training non class-conditional GANs. - No experimental comparison of AC-GAN to other class-conditional models. To my knowledge training GANs on large, diverse images such as 128 x 128 ImageNet images is under-explored ([1] contains just a few samples in this setting). Though the model is not very novel and a comparison to other class-conditional models is lacking, I feel the community will find the diagnostic tools and the thorough exploration of the ImageNet-trained model to be of interest. * Section 4.2: MS-SSIM is traditionally defined for grayscale images only. How do you extend MS-SSIM to color images in your work? Were they computed channel-wise across R,G, and B? * Section 4.4: It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or one for each group. If single, why were the samples split into groups for computing Inception Score? And if multiple, the comparison to Salimans et al. is not a direct one. Also it would be helpful to include the real data Inception score as a point of comparison. * Appendix D: The caption of Figure 9 states that the same number of training steps was taken for each model. From this it seems possible that the models with more classes simply did not converge yet. [1] Salimans, Tim, et al. \"Improved techniques for training GANs.\" Advances in Neural Information Processing Systems. 2016.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . We have used `` > '' for quotes in the below response . > - Diversity metric is of limited use for training non class-conditional GANs . We note that the MS-SSIM metric may be used for measuring \u201c collapsing behavior \u201d in GANs regardless of whether the GAN is class-conditional . In regards to employing MS-SSIM for measuring diversity * across * classes , we would like to clarify that this should be doable ; it would just require re-fitting the MS-SSIM parameters . > - No experimental comparison of AC-GAN to other class-conditional models . We have not been able make the original conditional GAN [ 1 ] train successfully on the full ImageNet dataset - we claim that a primary contribution of this paper is to provide the first image synthesis model to do so . From a model performance perspective , we would argue that the relevant architectural choice is whether label information is used . We do compare to the most favorable quantitative assessment of a GAN that has used label information [ 2 ] and show substantially improved results . > Section 4.2 : MS-SSIM is traditionally defined for grayscale images only . How do you > extend MS-SSIM to color images in your work ? Were they computed channel-wise across > R , G , and B ? We calculate the mean across all color channels using an open-source implementation : https : //github.com/tensorflow/models/blob/master/compression/msssim.py # L120 > Section 4.4 : It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or > one for each group\u2026 To the best of our knowledge , we are performing an analysis identical to that in [ 2 ] . ( Note that they also perform the splitting of the samples in order to compute score variance . ) We have added information to Table 2 of Appendix A to highlight this . > * Appendix D : The caption of Figure 9 states that the same number of training steps was taken for each model . From this it seems possible that the models with more classes simply did not converge yet . Our empirical observation ( as some others have seen ) is that once mode-collapse has occurred , the GAN can not recover . [ 1 ] Mehdi Mirza and Simon Osindero . Conditional generative adversarial nets . CoRR , abs/1411.1784 , 2014 [ 2 ] Salimans , Tim , et al . `` Improved techniques for training GANs . '' Advances in Neural Information Processing Systems . 2016 ."}], "0": {"review_id": "rJXTf9Bxg-0", "review_text": "Apologies for the late review. This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample. Figure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs. Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised). The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples. It is found - not surprisingly - that higher resolution improves discriminability (because more information is present). Discriminability: Figure 3 doesn\u2019t have legends so it is a bit hard to understand what is going on. Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place. Diversity: The authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class. I think this method is significantly flawed and limited, for reasons mentioned in (Theis et al, 2015, A note on the evaluation\u2026). In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1. Evaluating diversity/entropy of samples in high dimensions is very hard, especially if the distributions involved are non-trivial for example concentrated around manifolds. Consider for example a generative model which randomly samples just two images. Assuming that the MSSSIM between these two images is -1, this generative model can easily achieve an average MSSSIM score of 0, implying a conclusion that this model has more diversity than the training data itself. Conversely, SSIM is designed not to be sensitive to contrast and average pixel intensity, so if a model is diverse in this sense, that will be ignored by this measure. Overall, the paper proposes a new way to incorporate class labels into training GAN-type models. As far as I know the particular algorithm is novel, but I consider it incremental compared to what has been done before. I think the proposed evaluation metrics are flawed, especially when evaluating the diversity of the samples for the aforementioned reasons.", "rating": "3: Clear rejection", "reply_text": "Thank you for the review . We have used `` > '' for quotes in the below response . As mentioned in the global response , we believe that the review misstates several key points in our paper . We have revised our paper for clarity and we respond to each point in detail below . ====================== VARIABILITY AND MS-SSIM ====================== 1 . \u201c The authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class . I think this method is significantly flawed and limited , for reasons mentioned in ( Theis et al , 2015 , A note on the evaluation\u2026 ) . In its behaviour , MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1. \u201d The main claim of Theis et al ( 2015 ) is that log-likelihood might not correspond to sample quality in a generative model . Furthermore , training a model based on one objective will not guarantee good performance under another objective . We view these points as orthogonal to our evaluation framework . If the reviewer views this differently , we would be interested to hear their perspective . That said , Theis et al ( 2015 ) provide an explicit motivation for us to build an evaluation method that is independent of the training objective . 2.In regards to the statement that \u2018 MS-SSIM is not that dissimilar from Euclidean distance \u2019 , we strongly disagree . We believe that this statement is not supported by the literature . To start with , SSIM is a highly nonlinear metric that has been constructed to reflect human perceptual judgements [ * ] . The original SSIM paper as well as follow up works on MS-SSIM have demonstrated that SSIM-based metrics provide substantially improved quantitative estimates of human perceptual judgements compared to simple Euclidean distance measures ( and many other quantitative measures of perceptual similarity ) [ 1 , 2 , 3 ; and references therein ] . A simple intuition for why SSIM is perceptually superior to Euclidean distance can be gained through looking at examples . The SSIM web page ( http : //www.cns.nyu.edu/~lcv/ssim/ ) shows that images which are drastically different perceptually can have the same MSE and quite different SSIM scores . [ 1 ] Wang , Zhou , Eero P. Simoncelli , and Alan C. Bovik . `` Multiscale structural similarity for image quality assessment . '' [ 2 ] Z. Wang , A. C. Bovik , H. R. Sheikh and E. P. Simoncelli , `` Image quality assessment : From error visibility to structural similarity '' [ 3 ] Kede Ma , Qingbo Wu , Zhou Wang , Zhengfang Duanmu , Hongwei Yong , Hongliang Li , and Lei Zhang . Group mad competition - a new methodology to compare objective image quality models . [ * ] Note that our MS-SSIM implementation ranges from [ 0 , 1 ] and not from [ -1 , 1 ] . All values on our graphs should be interpreted accordingly . 3. \u201c Evaluating diversity/entropy of samples in high dimensions is very hard , especially if the distributions involved are non-trivial for example concentrated around manifolds . Consider for example a generative model which randomly samples just two images . Assuming that the MSSSIM between these two images is -1 , this generative model can easily achieve an average MSSSIM score of 0 , implying a conclusion that this model has more diversity than the training data itself. \u201d We claim that this example is seriously flawed , for the following reasons : a . The range of MS-SSIM is [ 0 , 1 ] ( see above ) , so the average would be 0.5 , which corresponds to half as much diversity as in the least diverse ImageNet class . b.This type of memorization would manifest as high variance of the mean MS-SSIM , but we report lower variance for samples than for training data ( In other words , we explicitly account for this in the metric itself ) . One could also compute higher moments or look at a histogram of pairwise MS-SSIM values , etc . c. We have also ruled out memorization explicitly by examining both latent space interpolations and nearest neighbors ( both by L1 and MS-SSIM ) . d. We have provided 10,000 sample images , which do not show this type of memorization . e. It \u2019 s not even clear how well other existing methods of measuring diversity would perform in this case . 4.The reviewer suggests that in an ideal world we would compute the entropy of the generator distribution . We argue that -- even if this were feasible -- we would still want to use perceptually calibrated metrics such as MS-SSIM . Namely , we might want to ignore variability due to contrast or pixel intensity in favor of diversity of image content . 5. \u201c Conversely , SSIM is designed not to be sensitive to contrast and average pixel intensity , so if a model is diverse in this sense , that will be ignored by this measure. \u201d We think that this is a good thing ( see above ) . This is a sense in which Euclidean distance measures are very different than MS-SSIM , which directly contradicts the reviewer \u2019 s earlier point . ============= DISCRIMINABILITY =============== 1 . \u201c Discriminability : Figure 3 doesn \u2019 t have legends so it is a bit hard to understand what is going on . Furthermore , my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image , which is much more like a blurring , very different from retraining all the models to work on low resolution in the first place. \u201d We feel that there was a large misunderstanding about our methods , so we present a quick summary of what was done to measure discriminability : Our goal in this analysis was to measure how much of the output resolution is actually used by the image synthesis model . In other words , does a 128x128 model just produce 32x32 images that are naively resized to 128x128 ? This is the goal of the \u2018 blurring \u2019 analysis and a question that has not been addressed in the literature . If a sample were a naive resizing , a blurring would not reduce its discriminability . For each image analyzed , and for each resolution in [ 16 , 32 , 64 , 128 , 256 ] , we iteratively resized the sample from its original size ( using bilinear interpolation ) to the resolution in question . We then passed the image to a pretrained Inception model , which resizes all inputs to 299x299 before processing . We took note of whether Inception correctly classified the input , and then we reported these results in the lower left hand corner of figure 3 . We have revised the manuscript to better describe these methods . 2. \u201c It is found - not surprisingly - that higher resolution improves discriminability ( because more information is present ) . \u201d If the reviewer means that simply increasing the resolution should result in higher discriminability : We did explicitly test for this by upsampling images to confirm that simply upsampling would not increase discriminability . See Figure 3 and Section 4.1 . If the reviewer means that it is unsurprising that the model is successfully making use of its available output resolution : Naive resizing is not an idle concern . We tested another model that actually failed to meaningfully increase discriminability score from 64x64 to 128x128 . 3 . `` [ the authors do not retrain ] all the models to work on low resolution in the first place '' . We did retrain models at a lower output resolution . We found that samples from these models are about half as discriminable at the 128x128 resolution as samples from the 128x128 model . The results of this experiment correspond to the blue curve in the lower left of Figure 3 . This procedure was also described in section 4.1 ."}, "1": {"review_id": "rJXTf9Bxg-1", "review_text": "This is a clear, easy to read, highly relevant paper that improves GAN training for images and explores evaluation criterion on GANs. The main contributions are as follows: - Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to the real/fake objective improves performance. Generator is conditioned on 1-hot encoding of class and is trained to generate the specified class. - Training different models on different subsets of imagenet classes improves performance. - They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples (and detect mode collapse) - They show this metric correlates with a discriminability metric (classification accuracy of pre-trained imagenet model on generated samples) . The overall novelty of this approach is somewhat lacking in that previous methods have proposed training a classifier head on the discriminator and the discriminability metric proposed is simply the inception score of [1] except with class information. However, I think there is still a contribution to be made my putting these tricks together and successfully demonstrating image synthesis gains. Questions for the authors: (1) Why do you think splitting the imagenet training into 100 different models improves performance? Is the issue with the representation of the class? In other words, if an encoding more meaningful that 1-hot vector was used do you still think 100 models would be needed. Ideally we should hope that a generative model can leverage information from different classes to help with the generation of a particular class and also text-image synthesis models [2] have been quite successful when trained on diverse datasets (and these are conditioned on a semantically meaningful text encoding) which suggests to be that the issue is with the representation. (2) In section 3 the AC-GAN classification objective (omitting expectation for brevity) is given as L_S = log P(C=c|X_real) + log P(C=c|X_fake) and you say that both the discriminator and generator are trained to maximize this quantity. Obviously the generator would want to maximize log P(C=c|X_fake) for its given conditioning class c. But can you explain why you would want the discriminator to also maximize the classification accuracy of generated samples? Why not do something similar to the CatGAN paper [3] and train the discriminator to be as uncertain as possible about the generated examples. Seems counterintuitive to me to have both the generator and discriminator trying to optimize the same classification objective rather than not be adversarial wrt to this loss as well as the real/fake loss. Overall, this paper makes a clear contribution to GAN research both in terms of image quality and evaluation metrics and I would recommend it for acceptance. [1] Salimans et al. Improved Techniques for Training GANs (https://arxiv.org/abs/1606.03498) [2] Reed et al. Generative Adversarial Text to Image Synthesis (https://arxiv.org/abs/1605.05396) [3] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks (https://arxiv.org/abs/1511.06390) ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . We have used `` > '' for quotes in the below response . > The overall novelty of this approach is somewhat lacking in that previous methods have > proposed training a classifier head on the discriminator and the discriminability metric > proposed is simply the inception score of [ 1 ] except with class information . The novelty of our discriminability analysis is mostly that we measure the extent to which the model is making use of its given output resolution . Our work is the first to attempt this measurement . Note that this also allows us to single out classes for which high-frequency information is important ( e.g.zebra - see the green dot in Figure 3 ) . > Why do you think splitting the imagenet training into 100 different models improves > performance ? Is the issue with the representation of the class ? We stumbled upon this result when debugging previous GAN models . We do not have an explanation for this behavior beyond our empirical results . We also claim that this work highlights that the challenge of ImageNet is mainly the large number of classes . This result is consistent with [ 1 ] in which the image synthesis model is restricted to visually similar classes ( i.e.birds ) .We do not know whether this is an issue of representation or learning , but we think it is a fascinating question and one which we hope to address in future work . We have added some comments to the discussion accordingly . > But can you explain why you would want the discriminator to also maximize the classification > accuracy of generated samples ? The short answer is that it makes the results better . Our suspicion is that this is due to a sort of feedback loop : more training data for the discriminator makes the discriminator better , which is then reflected in the generator . [ 1 ] Reed et al.Generative Adversarial Text to Image Synthesis ( https : //arxiv.org/abs/1605.05396 )"}, "2": {"review_id": "rJXTf9Bxg-2", "review_text": "This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets. Pros: + The paper is clear and well-written. + Experiments performed in the relatively under-explored 128 x 128 ImageNet setting. + The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models. Cons: - AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class. - Diversity metric is of limited use for training non class-conditional GANs. - No experimental comparison of AC-GAN to other class-conditional models. To my knowledge training GANs on large, diverse images such as 128 x 128 ImageNet images is under-explored ([1] contains just a few samples in this setting). Though the model is not very novel and a comparison to other class-conditional models is lacking, I feel the community will find the diagnostic tools and the thorough exploration of the ImageNet-trained model to be of interest. * Section 4.2: MS-SSIM is traditionally defined for grayscale images only. How do you extend MS-SSIM to color images in your work? Were they computed channel-wise across R,G, and B? * Section 4.4: It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or one for each group. If single, why were the samples split into groups for computing Inception Score? And if multiple, the comparison to Salimans et al. is not a direct one. Also it would be helpful to include the real data Inception score as a point of comparison. * Appendix D: The caption of Figure 9 states that the same number of training steps was taken for each model. From this it seems possible that the models with more classes simply did not converge yet. [1] Salimans, Tim, et al. \"Improved techniques for training GANs.\" Advances in Neural Information Processing Systems. 2016.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . We have used `` > '' for quotes in the below response . > - Diversity metric is of limited use for training non class-conditional GANs . We note that the MS-SSIM metric may be used for measuring \u201c collapsing behavior \u201d in GANs regardless of whether the GAN is class-conditional . In regards to employing MS-SSIM for measuring diversity * across * classes , we would like to clarify that this should be doable ; it would just require re-fitting the MS-SSIM parameters . > - No experimental comparison of AC-GAN to other class-conditional models . We have not been able make the original conditional GAN [ 1 ] train successfully on the full ImageNet dataset - we claim that a primary contribution of this paper is to provide the first image synthesis model to do so . From a model performance perspective , we would argue that the relevant architectural choice is whether label information is used . We do compare to the most favorable quantitative assessment of a GAN that has used label information [ 2 ] and show substantially improved results . > Section 4.2 : MS-SSIM is traditionally defined for grayscale images only . How do you > extend MS-SSIM to color images in your work ? Were they computed channel-wise across > R , G , and B ? We calculate the mean across all color channels using an open-source implementation : https : //github.com/tensorflow/models/blob/master/compression/msssim.py # L120 > Section 4.4 : It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or > one for each group\u2026 To the best of our knowledge , we are performing an analysis identical to that in [ 2 ] . ( Note that they also perform the splitting of the samples in order to compute score variance . ) We have added information to Table 2 of Appendix A to highlight this . > * Appendix D : The caption of Figure 9 states that the same number of training steps was taken for each model . From this it seems possible that the models with more classes simply did not converge yet . Our empirical observation ( as some others have seen ) is that once mode-collapse has occurred , the GAN can not recover . [ 1 ] Mehdi Mirza and Simon Osindero . Conditional generative adversarial nets . CoRR , abs/1411.1784 , 2014 [ 2 ] Salimans , Tim , et al . `` Improved techniques for training GANs . '' Advances in Neural Information Processing Systems . 2016 ."}}