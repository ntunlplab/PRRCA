{"year": "2021", "forum": "CMsvjAnW1zE", "title": "Spherical Motion Dynamics: Learning Dynamics of Neural Network with Normalization, Weight Decay, and SGD", "decision": "Reject", "meta_review": "This paper studies gradient descent with weight decay and momentum for scale-invariant networks. While this is an interesting research direction, the clarity of the paper suffers from poor writing. In its current form, I doubt this paper would have a large impact in the community. In addition, some reviewers pointed out that the analysis is not rigorous (some steps are missing, the authors re-use some unjustified steps from previous papers, ...). The authors claim these steps are obvious, I certainly don't think that's the case and at the very least, one would expect that detailed and rigorous derivations in the appendix if the lack of space is the real issue in the main paper.\n\nI think the feedback provided by the reviewers is valuable and I strongly encourage the authors to take advantage of this feedback to improve their work and submit to another venue.\n\n", "reviews": [{"review_id": "CMsvjAnW1zE-0", "review_text": "The main goal of the paper is to establish theoretically some previous known results that for scale invariant networks the weight norm has a fixed point with ||w||^4=eta/lambda ||\\tilde { g } || . They also discuss the angular update , which because of scale invariance is basically equivalent to arccos ( 1-eta lambda ) |w_t|^2/|w_t+1|^2 and it thus comes mainly from the gradient . They have some experiments which they compare with the predicted equilibrium values for the angular update/ weight norm . The theorems are the main results and I do n't think they are very powerful given what is known in the literature ( the van Laarhoven paper but also the more recent Li-Arora and Lewkowycz-Gurari where the relevant eta lambda time scale has been shown to be characterize the convergence rate ) . Also theorem 3 assumes that the normalized gradient norm is constant through training , but it actually changes a lot . The experiments do not scale invariant networks ( even if they have batch-norm , the last layer for example is clearly not scale invariant ) so it is not that clear why the theory applies to that case , and the authors should probably discuss this . In the learning rate decay setup , the experiments do n't include the equilibrium value for the weights and it would be good to include them , and it does not seem like the equilibrium analysis is valid there as the authors discuss . The authors propose to rescale the weights together with decaying the learning rate , it would be good if they could mention how this changes the model validation accuracy . I find it really intriguing that in figures 2ef , the weight norm tracks so well the equilibrium one ( which by definition has \\Delta |w|=0 ) but at the same time it , the weight norm changes considerably ? Could the authors comment on how this is possible ? I suspect that this is because the predicted one is actually really noisy due to batch noise ( as shown in figures 2ad ) and thus the system is never at equilibrium .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your comments . We will response your concerns one by one . 1. \u201c The theorems are the main results and I do n't think they are very powerful given what is known in the literature ( the van Laarhoven paper but also the more recent Li-Arora and Lewkowycz-Gurari where the relevant eta lambda time scale has been shown to be characterize the convergence rate ) \u201d As we emphasize our novelty and contribution in the introduction part , what we really focus on is how equilibrium condition can be reached during the training . However , all the previous published works before submission due ( Oct 2 2020 ) assume the equilibrium condition has been reached and this strong assumption makes their result impossible to connect with observations in practice . For example , van Laarhoven , 2017 derived the theoretical value of weight norm in equilibrium condition , but his interpretation about equilibrium condition is ambiguous and contradictory , as we discussed in in Sec 1 ( the paragraph next to figure 1 ) . We solve the contradiction by clarifying the role of unit gradient norm during training . We also derive the theoretical value of weight norm/AU in SGDM case , while results of van Laarhoven 2017 does not include these . As for [ 1 ] and [ 2 ] ( we think reviewer may refer to these two papers ) \uff0c [ 1 ] studied the rate of SGD to achieve the best performance of DNN , their analysis has no connection to equilibrium condition of normalized DNN with weight decay . DNN can reach equilibrium condition long before DNN is well trained ( get best performance ) , so [ 1 ] is basically irrelevant to our submission . [ 2 ] indeed explores the cause of equilibrium condition by SDE , but its analysis is limited on pure SGD case and full of conjectures ( the discussion about momentum case is only conjecture presented in appendix ) , and their theory is only qualitative results , unable to derive theoretical value of weight norm/AU . Compared with [ 2 ] , our work depicts the dynamics of weight norm as an iterative sequence , with mild assumptions that are highly correlated with observations in practice . Our theory on momentum case is the novel result which is rigorously proved , and able to quantify the rate to reach equilibrium condition . We also derive the theoretical value of weight norm/AU in SGDM case . We emphasize that the expansion from SGD case to SGDM case is not trivial . Besides , [ 2 ] is a neurlps2020 submission , the paper is revealed to public on Oct 6 2020 , 4 days after submission due of ICLR2021 , so it is a parallel work . In summary , we humbly suggest reviewer can read our paper over again and re-evaluate the novelty/contribution of our work . We have added the comparison with [ 1 ] and [ 2 ] in revised version . 2. \u201c Also theorem 3 assumes that the normalized gradient norm is constant through training , but it actually changes a lot. \u201d We have modified the statement of theorem 3 to take variance case into account in revised version . 3. \u201c The experiments do not scale invariant networks ( even if they have batch-norm , the last layer for example is clearly not scale invariant ) so it is not that clear why the theory applies to that case , and the authors should probably discuss this. \u201d We need to emphasize that our theorem focus on scale invariant weights , it is applicable for all scale-invariant weight even they are sub-sets of the whole weights , that is the reason we use partial gradient symbol\uff08 $ \\partial $ \uff09not full gradient symbol $ \\nabla $ in mathematical formulation . The equilibrium condition of each sub-group scale-invariant weights is not affected by other weights \u2019 behavior or performance of networks , including those who are not scale-invariant . This independent property yields the most impressive results : all scale invariant weights from different convolution layers have the same expectation of AU , only determined by hyper-parameters ( See figure 3 ( a , b , d , e ) ) . [ 1 ] Lewkowycz , Aitor , and Guy Gur-Ari . `` On the training dynamics of deep networks with $ L_2 $ regularization . '' Advances in Neural Information Processing Systems 33 ( 2020 ) . [ 2 ] Li , Zhiyuan , Kaifeng Lyu , and Sanjeev Arora . `` Reconciling Modern Deep Learning with Traditional Optimization Analyses : The Intrinsic Learning Rate . '' Advances in Neural Information Processing Systems 33 ( 2020 ) ."}, {"review_id": "CMsvjAnW1zE-1", "review_text": "This paper tried to analyze gradient descent with weight decay and momentum for scale-invariant networks . Convergence rate regarding the norm of the iterate is provided . I think this paper has several weaknesses and hence do not recommend accepting the paper at the current point . 1.The paper is poorly written . The analysis and arguments are very poorly explained . Page 2 : `` If $ w_t \\approxeq w_ { t+1 } $ , then the weight norm can be approximated as $ \\|w_t \\|_2 \\approxeq $ ... '' Why is that ? `` while the gradient component provided by WD always tends to reduce weight norm . '' Why ? Is there any proof to support this argument ? The authors say `` one can obtain '' equality ( 4 ) . But why ( 4 ) holds ? it is very unclear and more explanation is necessary The authors say `` one can easily speculate from eq ( 5 ) the magnitude of update in SGDM cases should be ... '' but the reasoning is unclear . Page 3 : `` Eq . ( 7 ) implies gradient norm is influenced by weight norm , but weight norm does not affect the output of DNN '' Why ? How to derive ( 8 ) ? `` If \\| w_t \\| \\approxeq \\| w_ { t+1 } \\|_2 , one can obtain ( 10 ) '' What do you exactly mean when you write `` \\| w_t \\| \\approxeq \\| w_ { t+1 } \\|_2 '' ? and why the condition leads to ( 10 ) ? The authors say `` one can derive '' ( 12 ) . But again this is unclear . `` From Eq . ( 7 ) we can infer that increasing weight norm can lead to smaller gradient norm if unit gradient norm is unchanged . '' This needs more explanation . `` Arora et al . ( 2019 ) proves that full gradient descent can avoid such problem and converge to a stationary point ... '' To avoid which problem ? `` Besides , practical implementation suggests training DNN without WD always suffers from poor generalization '' This needs references . Page 4 : `` We can easily derive ( 15 ) '' How ? Also , what does $ \\approxeq $ on ( 15 ) mean ? Can you give a precise statement ? `` Notice that the core assumption mentioned above is unit gradient norm is unchanged . In fact this assumption can solve the contradiction we present in Section 1 : the convergence of weight norm is not equivalent to convergence of weight , steady unit gradient norm can also make weight norm converge '' I found the paragraph hard to parse . What do you mean by `` unit gradient norm is unchanged '' ? Of course , convergence of weight norm is not equivalent to convergence of weight . So why is it a `` contradiction '' ? 2.The proof is not rigorous and the writing of the proof is very sloppy . I believe the proof in the appendix is not rigorous and I feel unease . The authors use the following phrases ( not an exhaustive list ) to skip a lot of detailed proof , which does not meet the standard of mathematical writing . `` we can easily prove '' ( page 11 ) `` we can prove '' ( page 13 ) `` can be formulated as '' ( page 14 ) `` it 's easy to prove '' ( page 14 ) `` can be explicitly expressed '' ( page 14 ) `` which it 's easy to prove '' ( page 15 ) `` it 's easy to obtain '' ( page 16 ) `` therefore it is easy to derive '' ( page 17 ) 3 . The theorems are not very insightful . What insight does the convergence of weight norm provide ? for example , it does n't rule out the case of converging to a bad local min . What is the meaning of the value $ w^ * = ( L \\eta / 2 \\lambda ) ^ { 1/4 } $ ? Can you explain why w^ * should have the dependency on each parameter ? Does the theorem say that it will converge to the value $ w^ * $ regardless of any initialized point ? What does the condition $ \\lambda \\eta < < 1 $ exactly mean ? What does the condition $ \\| \\tilde { g } _t \\|^2 > l $ mean ? Why does it hold ? When the iterate reaches at a stationary point , is n't it zero ? 4.Comparison to the related works are not clear . Remark 1 and 2 state the results of this paper are `` consistent '' with some related works but did not provide any elaboration . What results exactly are being compared with ? If the results are consistent , then what is new here ? I hope the authors can explain more . 5.Experiment does not really support the theorems . It seems to me that the authors did n't really show that the weight norm converges to $ w^ * $ of Theorem 1 or 2 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your detailed concerns , we will answer it one by one . 1 . `` 'If $ w_t \\approx w_ { t+1 } $ , then the weight norm can be approximated as\u2026 ' Why is that ? '' It \u2019 s not our claim , it \u2019 s from [ 1 ] . we don \u2019 t think it is reasonable either , so we have discussed its reasonableness in the last paragraph of Sec 3 . 2 . `` 'while the gradient component provided by WD always tends to reduce weight norm . ' Why ? Is there any proof to support this argument ? '' Please see Eqn ( 14 ) , ( 15 ) , if you are unfamiliar with weight decay , please refer to [ 2 , 3 ] for the background of weight decay ; 3 . \u201c The authors say 'one can obtain ' equality ( 4 ) . But why ( 4 ) holds ? it is very unclear and more explanation is necessary \u201d Eqn ( 4 ) is from [ 4 ] , you can refer to [ 4 ] for more details . Due to the limited length of submission , we do not have enough space to analyze related work with full details . 4. \u201c The authors say 'one can easily speculate from eq ( 5 ) the magnitude of update in SGDM cases should be ... ' but the reasoning is unclear. \u201d Eqn ( 5 ) is from [ 5 ] , you can refer to [ 5 ] . Again , we do not have enough space to analyze related work with full details . 5. \u201c \u2018 Eq . ( 7 ) implies gradient norm is influenced by weight norm , but weight norm does not affect the output of DNN \u2019 Why ? '' Eqn ( 7 ) shows when weight norm is multiplied by k , gradient norm will be divided by k. Weight norm does not affect the output of DNN because of scale-invariant property . Please see the definition of scale-invariant property in the second paragraph in Introduction , or refer to related work [ 1 , 4 , 5 , 6 ] . 6 . `` How to derive ( 8 ) ? \u201c By setting $ k =1/||w_t|| $ in Eqn ( 7 ) , Eqn ( 8 ) is obtained . 7. \u201c \u2018 If | w_t | \\approxeq | w_ { t+1 } |_2 , one can obtain ( 10 ) \u2019 What do you exactly mean when you write `` | w_t | \\approxeq | w_ { t+1 } |_2 '' ? and why the condition leads to ( 10 ) ? \u201d `` | w_t | \\approxeq | w_ { t+1 } |_2 '' means $ ||w_t||_2 $ and $ ||w_ { t+1 } ||_2 $ are very close . Eqn ( 10 ) is obtained via dividing Eqn ( 9 ) by $ ||w_t||_2 $ . This statement comes from [ 7 ] , you can refer to [ 7 ] for more details . We modify the statement in revised version for clarification . 8. \u201c The authors say `` one can derive '' ( 12 ) . But again this is unclear. \u201d When $ \\Delta_t $ is very close to 0 , $ \\tan ( \\Delta_t ) \\approx \\Delta $ ( First order Taylor series expansion ) . Then combining Eqn . ( 6 ) , ( 9 ) , ( 11 ) , we have Eqn ( 12 ) . 9. \u201c \u2018 From Eq . ( 7 ) we can infer that increasing weight norm can lead to smaller gradient norm if unit gradient norm is unchanged. \u2019 This needs more explanation. \u201d You can see Eqn . ( 8 ) : if unit gradient norm is unchanged , gradient norm is inversely proportional to weight norm . \u201c Inverse proportion \u201d means if weight norm increase , gradient norm will decrease . 10. \u201c \u2018 Arora et al . ( 2019 ) proves that full gradient descent can avoid such problem and converge to a stationary point ... \u2019 To avoid which problem ? \u201c We present the problem in the sentence exactly before the one you mentioned : \u201c Zhang et al . ( 2019 ) states the potential risk that GD/SGD without WD but BN will converge to a stationary point not by reducing loss but by reducing effective learning rate due to increasing weight norm. \u201d We change \u201c problem \u201d to \u201c risk \u201d for clarification in revised version . [ 1 ] Twan van Laarhoven . L2 regularization versus batch and weight normalization . In Advances in Neural Information Processing Systems . 2017 . [ 2 ] Alex Krizhevsky and Hinton Geoffrey . Learning multiple layers of features from tiny images . 2009 . [ 3 ] Guodong Zhang , Chaoqi Wang , Bowen Xu , and Roger Grosse . Three mechanisms of weight decay regularization . In International Conference on Learning Representations , 2019 . [ 4 ] Vitaliy Chiley , Ilya Sharapov , Atli Kosson , Urs Koster , Ryan Reece , Sofia Samaniego de la Fuente , Vishal Subbiah , and Michael James . Online normalization for training neural networks . In Ad- vances in Neural Information Processing Systems 32 , pp . 8433\u20138443 . Curran Associates , Inc. , 2019 . [ 5 ] Zhiyuan Li and Sanjeev Arora . An exponential learning rate schedule for deep learning . In Interna- tional Conference on Learning Representations , 2020 . [ 6 ] Sanjeev Arora , Zhiyuan Li , and Kaifeng Lyu . Theoretical analysis of auto rate-tuning by batch normalization . In International Conference on Learning Representations , 2019 . [ 7 ] Elad Hoffer , Ron Banner , Itay Golan , and Daniel Soudry . Norm matters : efficient and accurate normalization schemes in deep networks . In S. Bengio , H. Wallach , H. Larochelle , K. Grauman , N. Cesa-Bianchi , and R. Garnett ( eds . ) , Advances in Neural Information Processing Systems 31 , pp . 2160\u20132170 . Curran Associates , Inc. , 2018 ."}, {"review_id": "CMsvjAnW1zE-2", "review_text": "The equilibrium condition is important during the theoretical analysis of spherical motion dynamics . This paper focuses on the reasonableness of the equilibrium condition . Concretely , the authors first show that the weight norm can converge to its theoretical result under some assumptions ( with SGD , SGDM settings ) . Finally , the experiments verify their conclusions . # # # CONTRIBUTIONS a ) The analysis of the equilibrium condition is important . The authors analyzed the equilibrium conditions under different settings , including SGD , SGDM , angular . b ) The problem is motivated well . The authors gave a detailed introduction to the question of why we need to analyze the equilibrium conditions . c ) The authors conducted several experiments to show the reasonableness of the theorems/assumptions . And the theoretical findings agree well with empirical observations . # # # MAJOR CONCERNS I am still concerned with the explanations in Eqn.17 . ( Of course , the same concern in Eqn.20 . ) In Eqn.17 , the formula ( left ) is the MSE form , which is the combination of the bias and the variance . The authors explained it like \u201c the square of weight norm can linearly converge to its theoretical value in equilibrium , and its variance is bounded by the variance of the square of unit gradient norm multiplying the square of learning rate \u201d . Could the authors tell us why the first term ( vanish as t \\to \\infty ) on the right hand is actually the bias ? In my view , one can not say the estimator * converge * to its theoretical value if the right-hand does not converge to zero . I think the most important point in the paper is the convergence of the estimator to the theoretical value , which is also the thing the authors tried to verify in Figure2 ( b , e ) . To reach the conclusion , one needs to at least claim \u201c V\\eta^2/l \u201d is small . As shown in Figure2 ( a ) , I do not think V is such small . Also , the claims in the main text that \u201c the learning rate \\eta is small \u201d is not satisfying . I am not sure if I have skipped some important claims in the main text . I would like to raise my score if the authors can explain it better . # # # OTHER CONCERNS a ) Eqn.12 ( as well as its statements ) is not clear . It is still unclear to me why one needs to use an angular update . b ) There are several typos ( and also something like log- > \\log , min- > \\min ) in the appendix . I believe the conclusions are mostly correct ( although I did not check the details ) , but I still spend a hard time reading the appendix . c ) In Theorem3 , the assumption \u201c when V=0 \u201d is a little bit strong and unrealistic ( shown in Figure2 ( a ) ) . The importance of the theorem will be larger if the authors could use a mild assumption .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate your valuable comments . We will response your concerns one by one . Major concerns : 1 . `` \u2026one can not say the estimator converge to its theoretical value if the right-hand does not converge to zero . '' We want show that MSE between $ ||w_t||_2^2 $ and $ ( w^ * ) ^2 $ can approach to a small value $ V\\eta^2/l $ in a linear rate regime , when the vanishing term $ ( 1-4\\lambda\\eta ) ^TB $ is larger than the variance term $ V\\eta^2/l $ . We will correct the misleading statements in the revised version . 2 . `` \u2026I do not think V is such small . Also , the claims in the main text that \u201c the learning rate \\eta is small \u201d is not satisfying\u2026 \u201d We do not state $ V\\eta^2/l $ is small due to small value of $ V $ , we need to clarify that the denominator $ l $ ( $ l $ denotes the lower bounder of square of unit gradient norm ) is not small either , and $ \\eta $ is set as 0.1 in our experiment . For example , in Figure 2 ( a ) ( semi-transparent line does not represent standard deviation bar , but exact experiment records . ) , at epoch 40 , $ V < 4 , l > 6 , \\eta=0.2 $ , then $ V\\eta^2/l < 0.027 $ , . According to Figure 2 ( b ) , $ ||w_t||_2^2 > 64 $ , therefore $ V\\eta^2/l < < ||w_t||_2^2 $ , our experiment results strongly verify our claim that $ V\\eta^2/l $ is small . We will refine the statement to emphasize this point in revised version . Other concerns : a ) Eqn.12 wants to show the connection between angular update ( AU ) and ELF defined in [ 1 ] ( Eqn . ( 3 ) , we will refine the statement for clarification . We propose AU for two reasons : first , we intend to replace the ambiguous definition \u201c effective learning rate ( ELF ) \u201d with AU to depict the change of scale-invariant weight . Previous important literatures ( [ 1 ] , [ 2 ] , [ 3 ] ) give different meanings to ELF , because they choose different forms of gradient norm to multiply ; Second , AU is the intrinsic meaning of relative update $ ||w_ { t+1 } \u2013 w_t||/||w_t|| $ within equilibrium condition , which has been mentioned in previous works ( [ 1 ] , [ 2 ] , [ 3 ] ) . $ w_ { t+1 } \u2013 w_t $ and $ w_t $ are both vectors , and $ w_t $ is a scale-invariant weight , so $ ||w_ { t+1 } \u2013 w_t||/||w_t|| $ can not truly indicate the extent of changes if the equilibrium condition has not been reached yet , while AU can represent the change of $ w_t $ well at any time during training process . Moreover , AU is highly correlated to our most impressive results ( see Figure 3 ( a , b , d , e ) ) : all scale-invariant ( sub ) weights in convolutional layers has the same ( expected ) AU in equilibrium condition , no matter which layer they belong to , what size ( number of parameters ) they have , what shape they have , or how large their gradient/unit gradient norm is . Both our theory and experiment results imply expected AU of all scale invariant weight is only determined by hyper-parameters . This property of AU is amazing , but it is caused by standard SGD/SGDM , we believe it has great potential to explore the behavior of AU when analyzing learning dynamic of normalized network . b ) We will check the proof over again and correct the typos , the proof of our theory is not easy and straightforward , we will try our best to make it more readable . c ) .We have modified the statement of theorem 3 to take variance case into account in revised version . [ 1 ] Twan van Laarhoven . L2 regularization versus batch and weight normalization . In Advances in Neural Information Processing Systems . 2017 ."}, {"review_id": "CMsvjAnW1zE-3", "review_text": "Unfortunately , I am not qualified enough in the literature of learning dynamics of the neural network to comment on the novelty and contribution of this paper . Therefore I may take the author \u2019 s word and would like to rely on the comment on other reviewers . In general , the motivation of this paper is clear , since the theoretical result in literature mainly relies on the assumption of equilibrium condition but does not discuss why this equilibrium can be reached . The main contribution comes from Theorem 1 and Theorem 2 . This paper proves that weight norm can converge at the linear rate under mild assumption . It also defines another index called angular update to measure the change of normalized neural network in a single iteration and prove its convergence with linear rate . Overall the theoretical result looks novel and the experimental result matches the theory.Hence , I lean toward accepting . My only concern is whether this theory can help us to train the deep neural network or give us some insights to design certain layers of the neural network . But it maybe that the overall novelty/contribution outweights this concerns . Can the author further explain that ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your recognition on our work . Due to the limited length of submission , we can not discuss too much on the significance of our theory in the paper . Our theory is highly correlated with large batch training issue . A typical strategy to deal with large batch training issue is Linear Scaling Rule [ 1 ] ( see further discussion in appendix B.2 ( C.3 in revised version ) ) , our theory can show what will happen if we apply Linear Scaling Rule , and can explain why Linear Scaling Rule will fail with extremely large batch size . Therefore , our theory has the potential to inspire new and effective learning rate tuning schedule ; Another popular large batch training method LARS [ 2 ] , is also highly correlated with our theory : the intuition of LARS is to make the relative update of weights in each layer , which happens to coincide with the performance of angular update in SGD/SGDM . But LARS is very sensitive , and its performance is very hard to reproduce , which prevents it from being widely used as SGD or Adam . We believe our theory has the potential to help fix its weakness and make it as stable as SGDM/Adam in large batch training issue . [ 1 ] Priya Goyal , Piotr Dolla \u0301r , Ross Girshick , Pieter Noordhuis , Lukasz Wesolowski , Aapo Kyrola , An- drew Tulloch , Yangqing Jia , and Kaiming He . Accurate , large minibatch sgd : Training imagenet in 1 hour . arXiv preprint arXiv:1706.02677 , 2017 . [ 2 ] You , Yang , et al . `` Imagenet training in minutes . '' Proceedings of the 47th International Conference on Parallel Processing . 2018 ."}], "0": {"review_id": "CMsvjAnW1zE-0", "review_text": "The main goal of the paper is to establish theoretically some previous known results that for scale invariant networks the weight norm has a fixed point with ||w||^4=eta/lambda ||\\tilde { g } || . They also discuss the angular update , which because of scale invariance is basically equivalent to arccos ( 1-eta lambda ) |w_t|^2/|w_t+1|^2 and it thus comes mainly from the gradient . They have some experiments which they compare with the predicted equilibrium values for the angular update/ weight norm . The theorems are the main results and I do n't think they are very powerful given what is known in the literature ( the van Laarhoven paper but also the more recent Li-Arora and Lewkowycz-Gurari where the relevant eta lambda time scale has been shown to be characterize the convergence rate ) . Also theorem 3 assumes that the normalized gradient norm is constant through training , but it actually changes a lot . The experiments do not scale invariant networks ( even if they have batch-norm , the last layer for example is clearly not scale invariant ) so it is not that clear why the theory applies to that case , and the authors should probably discuss this . In the learning rate decay setup , the experiments do n't include the equilibrium value for the weights and it would be good to include them , and it does not seem like the equilibrium analysis is valid there as the authors discuss . The authors propose to rescale the weights together with decaying the learning rate , it would be good if they could mention how this changes the model validation accuracy . I find it really intriguing that in figures 2ef , the weight norm tracks so well the equilibrium one ( which by definition has \\Delta |w|=0 ) but at the same time it , the weight norm changes considerably ? Could the authors comment on how this is possible ? I suspect that this is because the predicted one is actually really noisy due to batch noise ( as shown in figures 2ad ) and thus the system is never at equilibrium .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your comments . We will response your concerns one by one . 1. \u201c The theorems are the main results and I do n't think they are very powerful given what is known in the literature ( the van Laarhoven paper but also the more recent Li-Arora and Lewkowycz-Gurari where the relevant eta lambda time scale has been shown to be characterize the convergence rate ) \u201d As we emphasize our novelty and contribution in the introduction part , what we really focus on is how equilibrium condition can be reached during the training . However , all the previous published works before submission due ( Oct 2 2020 ) assume the equilibrium condition has been reached and this strong assumption makes their result impossible to connect with observations in practice . For example , van Laarhoven , 2017 derived the theoretical value of weight norm in equilibrium condition , but his interpretation about equilibrium condition is ambiguous and contradictory , as we discussed in in Sec 1 ( the paragraph next to figure 1 ) . We solve the contradiction by clarifying the role of unit gradient norm during training . We also derive the theoretical value of weight norm/AU in SGDM case , while results of van Laarhoven 2017 does not include these . As for [ 1 ] and [ 2 ] ( we think reviewer may refer to these two papers ) \uff0c [ 1 ] studied the rate of SGD to achieve the best performance of DNN , their analysis has no connection to equilibrium condition of normalized DNN with weight decay . DNN can reach equilibrium condition long before DNN is well trained ( get best performance ) , so [ 1 ] is basically irrelevant to our submission . [ 2 ] indeed explores the cause of equilibrium condition by SDE , but its analysis is limited on pure SGD case and full of conjectures ( the discussion about momentum case is only conjecture presented in appendix ) , and their theory is only qualitative results , unable to derive theoretical value of weight norm/AU . Compared with [ 2 ] , our work depicts the dynamics of weight norm as an iterative sequence , with mild assumptions that are highly correlated with observations in practice . Our theory on momentum case is the novel result which is rigorously proved , and able to quantify the rate to reach equilibrium condition . We also derive the theoretical value of weight norm/AU in SGDM case . We emphasize that the expansion from SGD case to SGDM case is not trivial . Besides , [ 2 ] is a neurlps2020 submission , the paper is revealed to public on Oct 6 2020 , 4 days after submission due of ICLR2021 , so it is a parallel work . In summary , we humbly suggest reviewer can read our paper over again and re-evaluate the novelty/contribution of our work . We have added the comparison with [ 1 ] and [ 2 ] in revised version . 2. \u201c Also theorem 3 assumes that the normalized gradient norm is constant through training , but it actually changes a lot. \u201d We have modified the statement of theorem 3 to take variance case into account in revised version . 3. \u201c The experiments do not scale invariant networks ( even if they have batch-norm , the last layer for example is clearly not scale invariant ) so it is not that clear why the theory applies to that case , and the authors should probably discuss this. \u201d We need to emphasize that our theorem focus on scale invariant weights , it is applicable for all scale-invariant weight even they are sub-sets of the whole weights , that is the reason we use partial gradient symbol\uff08 $ \\partial $ \uff09not full gradient symbol $ \\nabla $ in mathematical formulation . The equilibrium condition of each sub-group scale-invariant weights is not affected by other weights \u2019 behavior or performance of networks , including those who are not scale-invariant . This independent property yields the most impressive results : all scale invariant weights from different convolution layers have the same expectation of AU , only determined by hyper-parameters ( See figure 3 ( a , b , d , e ) ) . [ 1 ] Lewkowycz , Aitor , and Guy Gur-Ari . `` On the training dynamics of deep networks with $ L_2 $ regularization . '' Advances in Neural Information Processing Systems 33 ( 2020 ) . [ 2 ] Li , Zhiyuan , Kaifeng Lyu , and Sanjeev Arora . `` Reconciling Modern Deep Learning with Traditional Optimization Analyses : The Intrinsic Learning Rate . '' Advances in Neural Information Processing Systems 33 ( 2020 ) ."}, "1": {"review_id": "CMsvjAnW1zE-1", "review_text": "This paper tried to analyze gradient descent with weight decay and momentum for scale-invariant networks . Convergence rate regarding the norm of the iterate is provided . I think this paper has several weaknesses and hence do not recommend accepting the paper at the current point . 1.The paper is poorly written . The analysis and arguments are very poorly explained . Page 2 : `` If $ w_t \\approxeq w_ { t+1 } $ , then the weight norm can be approximated as $ \\|w_t \\|_2 \\approxeq $ ... '' Why is that ? `` while the gradient component provided by WD always tends to reduce weight norm . '' Why ? Is there any proof to support this argument ? The authors say `` one can obtain '' equality ( 4 ) . But why ( 4 ) holds ? it is very unclear and more explanation is necessary The authors say `` one can easily speculate from eq ( 5 ) the magnitude of update in SGDM cases should be ... '' but the reasoning is unclear . Page 3 : `` Eq . ( 7 ) implies gradient norm is influenced by weight norm , but weight norm does not affect the output of DNN '' Why ? How to derive ( 8 ) ? `` If \\| w_t \\| \\approxeq \\| w_ { t+1 } \\|_2 , one can obtain ( 10 ) '' What do you exactly mean when you write `` \\| w_t \\| \\approxeq \\| w_ { t+1 } \\|_2 '' ? and why the condition leads to ( 10 ) ? The authors say `` one can derive '' ( 12 ) . But again this is unclear . `` From Eq . ( 7 ) we can infer that increasing weight norm can lead to smaller gradient norm if unit gradient norm is unchanged . '' This needs more explanation . `` Arora et al . ( 2019 ) proves that full gradient descent can avoid such problem and converge to a stationary point ... '' To avoid which problem ? `` Besides , practical implementation suggests training DNN without WD always suffers from poor generalization '' This needs references . Page 4 : `` We can easily derive ( 15 ) '' How ? Also , what does $ \\approxeq $ on ( 15 ) mean ? Can you give a precise statement ? `` Notice that the core assumption mentioned above is unit gradient norm is unchanged . In fact this assumption can solve the contradiction we present in Section 1 : the convergence of weight norm is not equivalent to convergence of weight , steady unit gradient norm can also make weight norm converge '' I found the paragraph hard to parse . What do you mean by `` unit gradient norm is unchanged '' ? Of course , convergence of weight norm is not equivalent to convergence of weight . So why is it a `` contradiction '' ? 2.The proof is not rigorous and the writing of the proof is very sloppy . I believe the proof in the appendix is not rigorous and I feel unease . The authors use the following phrases ( not an exhaustive list ) to skip a lot of detailed proof , which does not meet the standard of mathematical writing . `` we can easily prove '' ( page 11 ) `` we can prove '' ( page 13 ) `` can be formulated as '' ( page 14 ) `` it 's easy to prove '' ( page 14 ) `` can be explicitly expressed '' ( page 14 ) `` which it 's easy to prove '' ( page 15 ) `` it 's easy to obtain '' ( page 16 ) `` therefore it is easy to derive '' ( page 17 ) 3 . The theorems are not very insightful . What insight does the convergence of weight norm provide ? for example , it does n't rule out the case of converging to a bad local min . What is the meaning of the value $ w^ * = ( L \\eta / 2 \\lambda ) ^ { 1/4 } $ ? Can you explain why w^ * should have the dependency on each parameter ? Does the theorem say that it will converge to the value $ w^ * $ regardless of any initialized point ? What does the condition $ \\lambda \\eta < < 1 $ exactly mean ? What does the condition $ \\| \\tilde { g } _t \\|^2 > l $ mean ? Why does it hold ? When the iterate reaches at a stationary point , is n't it zero ? 4.Comparison to the related works are not clear . Remark 1 and 2 state the results of this paper are `` consistent '' with some related works but did not provide any elaboration . What results exactly are being compared with ? If the results are consistent , then what is new here ? I hope the authors can explain more . 5.Experiment does not really support the theorems . It seems to me that the authors did n't really show that the weight norm converges to $ w^ * $ of Theorem 1 or 2 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your detailed concerns , we will answer it one by one . 1 . `` 'If $ w_t \\approx w_ { t+1 } $ , then the weight norm can be approximated as\u2026 ' Why is that ? '' It \u2019 s not our claim , it \u2019 s from [ 1 ] . we don \u2019 t think it is reasonable either , so we have discussed its reasonableness in the last paragraph of Sec 3 . 2 . `` 'while the gradient component provided by WD always tends to reduce weight norm . ' Why ? Is there any proof to support this argument ? '' Please see Eqn ( 14 ) , ( 15 ) , if you are unfamiliar with weight decay , please refer to [ 2 , 3 ] for the background of weight decay ; 3 . \u201c The authors say 'one can obtain ' equality ( 4 ) . But why ( 4 ) holds ? it is very unclear and more explanation is necessary \u201d Eqn ( 4 ) is from [ 4 ] , you can refer to [ 4 ] for more details . Due to the limited length of submission , we do not have enough space to analyze related work with full details . 4. \u201c The authors say 'one can easily speculate from eq ( 5 ) the magnitude of update in SGDM cases should be ... ' but the reasoning is unclear. \u201d Eqn ( 5 ) is from [ 5 ] , you can refer to [ 5 ] . Again , we do not have enough space to analyze related work with full details . 5. \u201c \u2018 Eq . ( 7 ) implies gradient norm is influenced by weight norm , but weight norm does not affect the output of DNN \u2019 Why ? '' Eqn ( 7 ) shows when weight norm is multiplied by k , gradient norm will be divided by k. Weight norm does not affect the output of DNN because of scale-invariant property . Please see the definition of scale-invariant property in the second paragraph in Introduction , or refer to related work [ 1 , 4 , 5 , 6 ] . 6 . `` How to derive ( 8 ) ? \u201c By setting $ k =1/||w_t|| $ in Eqn ( 7 ) , Eqn ( 8 ) is obtained . 7. \u201c \u2018 If | w_t | \\approxeq | w_ { t+1 } |_2 , one can obtain ( 10 ) \u2019 What do you exactly mean when you write `` | w_t | \\approxeq | w_ { t+1 } |_2 '' ? and why the condition leads to ( 10 ) ? \u201d `` | w_t | \\approxeq | w_ { t+1 } |_2 '' means $ ||w_t||_2 $ and $ ||w_ { t+1 } ||_2 $ are very close . Eqn ( 10 ) is obtained via dividing Eqn ( 9 ) by $ ||w_t||_2 $ . This statement comes from [ 7 ] , you can refer to [ 7 ] for more details . We modify the statement in revised version for clarification . 8. \u201c The authors say `` one can derive '' ( 12 ) . But again this is unclear. \u201d When $ \\Delta_t $ is very close to 0 , $ \\tan ( \\Delta_t ) \\approx \\Delta $ ( First order Taylor series expansion ) . Then combining Eqn . ( 6 ) , ( 9 ) , ( 11 ) , we have Eqn ( 12 ) . 9. \u201c \u2018 From Eq . ( 7 ) we can infer that increasing weight norm can lead to smaller gradient norm if unit gradient norm is unchanged. \u2019 This needs more explanation. \u201d You can see Eqn . ( 8 ) : if unit gradient norm is unchanged , gradient norm is inversely proportional to weight norm . \u201c Inverse proportion \u201d means if weight norm increase , gradient norm will decrease . 10. \u201c \u2018 Arora et al . ( 2019 ) proves that full gradient descent can avoid such problem and converge to a stationary point ... \u2019 To avoid which problem ? \u201c We present the problem in the sentence exactly before the one you mentioned : \u201c Zhang et al . ( 2019 ) states the potential risk that GD/SGD without WD but BN will converge to a stationary point not by reducing loss but by reducing effective learning rate due to increasing weight norm. \u201d We change \u201c problem \u201d to \u201c risk \u201d for clarification in revised version . [ 1 ] Twan van Laarhoven . L2 regularization versus batch and weight normalization . In Advances in Neural Information Processing Systems . 2017 . [ 2 ] Alex Krizhevsky and Hinton Geoffrey . Learning multiple layers of features from tiny images . 2009 . [ 3 ] Guodong Zhang , Chaoqi Wang , Bowen Xu , and Roger Grosse . Three mechanisms of weight decay regularization . In International Conference on Learning Representations , 2019 . [ 4 ] Vitaliy Chiley , Ilya Sharapov , Atli Kosson , Urs Koster , Ryan Reece , Sofia Samaniego de la Fuente , Vishal Subbiah , and Michael James . Online normalization for training neural networks . In Ad- vances in Neural Information Processing Systems 32 , pp . 8433\u20138443 . Curran Associates , Inc. , 2019 . [ 5 ] Zhiyuan Li and Sanjeev Arora . An exponential learning rate schedule for deep learning . In Interna- tional Conference on Learning Representations , 2020 . [ 6 ] Sanjeev Arora , Zhiyuan Li , and Kaifeng Lyu . Theoretical analysis of auto rate-tuning by batch normalization . In International Conference on Learning Representations , 2019 . [ 7 ] Elad Hoffer , Ron Banner , Itay Golan , and Daniel Soudry . Norm matters : efficient and accurate normalization schemes in deep networks . In S. Bengio , H. Wallach , H. Larochelle , K. Grauman , N. Cesa-Bianchi , and R. Garnett ( eds . ) , Advances in Neural Information Processing Systems 31 , pp . 2160\u20132170 . Curran Associates , Inc. , 2018 ."}, "2": {"review_id": "CMsvjAnW1zE-2", "review_text": "The equilibrium condition is important during the theoretical analysis of spherical motion dynamics . This paper focuses on the reasonableness of the equilibrium condition . Concretely , the authors first show that the weight norm can converge to its theoretical result under some assumptions ( with SGD , SGDM settings ) . Finally , the experiments verify their conclusions . # # # CONTRIBUTIONS a ) The analysis of the equilibrium condition is important . The authors analyzed the equilibrium conditions under different settings , including SGD , SGDM , angular . b ) The problem is motivated well . The authors gave a detailed introduction to the question of why we need to analyze the equilibrium conditions . c ) The authors conducted several experiments to show the reasonableness of the theorems/assumptions . And the theoretical findings agree well with empirical observations . # # # MAJOR CONCERNS I am still concerned with the explanations in Eqn.17 . ( Of course , the same concern in Eqn.20 . ) In Eqn.17 , the formula ( left ) is the MSE form , which is the combination of the bias and the variance . The authors explained it like \u201c the square of weight norm can linearly converge to its theoretical value in equilibrium , and its variance is bounded by the variance of the square of unit gradient norm multiplying the square of learning rate \u201d . Could the authors tell us why the first term ( vanish as t \\to \\infty ) on the right hand is actually the bias ? In my view , one can not say the estimator * converge * to its theoretical value if the right-hand does not converge to zero . I think the most important point in the paper is the convergence of the estimator to the theoretical value , which is also the thing the authors tried to verify in Figure2 ( b , e ) . To reach the conclusion , one needs to at least claim \u201c V\\eta^2/l \u201d is small . As shown in Figure2 ( a ) , I do not think V is such small . Also , the claims in the main text that \u201c the learning rate \\eta is small \u201d is not satisfying . I am not sure if I have skipped some important claims in the main text . I would like to raise my score if the authors can explain it better . # # # OTHER CONCERNS a ) Eqn.12 ( as well as its statements ) is not clear . It is still unclear to me why one needs to use an angular update . b ) There are several typos ( and also something like log- > \\log , min- > \\min ) in the appendix . I believe the conclusions are mostly correct ( although I did not check the details ) , but I still spend a hard time reading the appendix . c ) In Theorem3 , the assumption \u201c when V=0 \u201d is a little bit strong and unrealistic ( shown in Figure2 ( a ) ) . The importance of the theorem will be larger if the authors could use a mild assumption .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We appreciate your valuable comments . We will response your concerns one by one . Major concerns : 1 . `` \u2026one can not say the estimator converge to its theoretical value if the right-hand does not converge to zero . '' We want show that MSE between $ ||w_t||_2^2 $ and $ ( w^ * ) ^2 $ can approach to a small value $ V\\eta^2/l $ in a linear rate regime , when the vanishing term $ ( 1-4\\lambda\\eta ) ^TB $ is larger than the variance term $ V\\eta^2/l $ . We will correct the misleading statements in the revised version . 2 . `` \u2026I do not think V is such small . Also , the claims in the main text that \u201c the learning rate \\eta is small \u201d is not satisfying\u2026 \u201d We do not state $ V\\eta^2/l $ is small due to small value of $ V $ , we need to clarify that the denominator $ l $ ( $ l $ denotes the lower bounder of square of unit gradient norm ) is not small either , and $ \\eta $ is set as 0.1 in our experiment . For example , in Figure 2 ( a ) ( semi-transparent line does not represent standard deviation bar , but exact experiment records . ) , at epoch 40 , $ V < 4 , l > 6 , \\eta=0.2 $ , then $ V\\eta^2/l < 0.027 $ , . According to Figure 2 ( b ) , $ ||w_t||_2^2 > 64 $ , therefore $ V\\eta^2/l < < ||w_t||_2^2 $ , our experiment results strongly verify our claim that $ V\\eta^2/l $ is small . We will refine the statement to emphasize this point in revised version . Other concerns : a ) Eqn.12 wants to show the connection between angular update ( AU ) and ELF defined in [ 1 ] ( Eqn . ( 3 ) , we will refine the statement for clarification . We propose AU for two reasons : first , we intend to replace the ambiguous definition \u201c effective learning rate ( ELF ) \u201d with AU to depict the change of scale-invariant weight . Previous important literatures ( [ 1 ] , [ 2 ] , [ 3 ] ) give different meanings to ELF , because they choose different forms of gradient norm to multiply ; Second , AU is the intrinsic meaning of relative update $ ||w_ { t+1 } \u2013 w_t||/||w_t|| $ within equilibrium condition , which has been mentioned in previous works ( [ 1 ] , [ 2 ] , [ 3 ] ) . $ w_ { t+1 } \u2013 w_t $ and $ w_t $ are both vectors , and $ w_t $ is a scale-invariant weight , so $ ||w_ { t+1 } \u2013 w_t||/||w_t|| $ can not truly indicate the extent of changes if the equilibrium condition has not been reached yet , while AU can represent the change of $ w_t $ well at any time during training process . Moreover , AU is highly correlated to our most impressive results ( see Figure 3 ( a , b , d , e ) ) : all scale-invariant ( sub ) weights in convolutional layers has the same ( expected ) AU in equilibrium condition , no matter which layer they belong to , what size ( number of parameters ) they have , what shape they have , or how large their gradient/unit gradient norm is . Both our theory and experiment results imply expected AU of all scale invariant weight is only determined by hyper-parameters . This property of AU is amazing , but it is caused by standard SGD/SGDM , we believe it has great potential to explore the behavior of AU when analyzing learning dynamic of normalized network . b ) We will check the proof over again and correct the typos , the proof of our theory is not easy and straightforward , we will try our best to make it more readable . c ) .We have modified the statement of theorem 3 to take variance case into account in revised version . [ 1 ] Twan van Laarhoven . L2 regularization versus batch and weight normalization . In Advances in Neural Information Processing Systems . 2017 ."}, "3": {"review_id": "CMsvjAnW1zE-3", "review_text": "Unfortunately , I am not qualified enough in the literature of learning dynamics of the neural network to comment on the novelty and contribution of this paper . Therefore I may take the author \u2019 s word and would like to rely on the comment on other reviewers . In general , the motivation of this paper is clear , since the theoretical result in literature mainly relies on the assumption of equilibrium condition but does not discuss why this equilibrium can be reached . The main contribution comes from Theorem 1 and Theorem 2 . This paper proves that weight norm can converge at the linear rate under mild assumption . It also defines another index called angular update to measure the change of normalized neural network in a single iteration and prove its convergence with linear rate . Overall the theoretical result looks novel and the experimental result matches the theory.Hence , I lean toward accepting . My only concern is whether this theory can help us to train the deep neural network or give us some insights to design certain layers of the neural network . But it maybe that the overall novelty/contribution outweights this concerns . Can the author further explain that ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your recognition on our work . Due to the limited length of submission , we can not discuss too much on the significance of our theory in the paper . Our theory is highly correlated with large batch training issue . A typical strategy to deal with large batch training issue is Linear Scaling Rule [ 1 ] ( see further discussion in appendix B.2 ( C.3 in revised version ) ) , our theory can show what will happen if we apply Linear Scaling Rule , and can explain why Linear Scaling Rule will fail with extremely large batch size . Therefore , our theory has the potential to inspire new and effective learning rate tuning schedule ; Another popular large batch training method LARS [ 2 ] , is also highly correlated with our theory : the intuition of LARS is to make the relative update of weights in each layer , which happens to coincide with the performance of angular update in SGD/SGDM . But LARS is very sensitive , and its performance is very hard to reproduce , which prevents it from being widely used as SGD or Adam . We believe our theory has the potential to help fix its weakness and make it as stable as SGDM/Adam in large batch training issue . [ 1 ] Priya Goyal , Piotr Dolla \u0301r , Ross Girshick , Pieter Noordhuis , Lukasz Wesolowski , Aapo Kyrola , An- drew Tulloch , Yangqing Jia , and Kaiming He . Accurate , large minibatch sgd : Training imagenet in 1 hour . arXiv preprint arXiv:1706.02677 , 2017 . [ 2 ] You , Yang , et al . `` Imagenet training in minutes . '' Proceedings of the 47th International Conference on Parallel Processing . 2018 ."}}