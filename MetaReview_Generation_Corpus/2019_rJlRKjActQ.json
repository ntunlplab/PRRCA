{"year": "2019", "forum": "rJlRKjActQ", "title": "Manifold Mixup: Learning Better Representations by Interpolating Hidden States", "decision": "Reject", "meta_review": "The paper contains useful information and shows relative improvements compared to mixup. However, some of the main claims are not substantiated enough to be fully convincing. For example, the claims that manifold mixup can prevent can manifold collision issue where the interpolation between two samples collides with a sample from other class is incorrect. The authors are encouraged to incorporate remarks of the reviewers.", "reviews": [{"review_id": "rJlRKjActQ-0", "review_text": "TL;DR. a generalization of the mixup algorithm to any layer, improving generalization abilities. * Summary The manuscript generalizes the mixup algorithm (Zhang et al., 2017) which proposed to interpolate between inputs to yield better generalization. The present manuscript addresses a fairly more general setting as the mixup may occur at *any* layer of the network, not just the input layer. Once a layer is chosen, mixup occurs with a random proportion $\\lambda\\in (0,1)$ (sampled from a $\\mathrm{Beta}(\\alpha,\\alpha)$ distribution). A salient asset of the manuscript is that it avoids a pitfall of the original mixup algorithm: interpolating between inputs may result in underfitting (if inputs are far from each others: the interpolation may overlap with existing inputs). Interpolating deep layers of the networks makes it less prone to this phenomenon. A sufficient condition for Manifold Mixup to avoid this underfitting phenomenon is that the dimension of the hidden layer exceeds the number of classes. I found no flaw in the (two) proofs. Literature is well acknowledged. In my opinion, a clear accept. * Major remarks - There is little discussion in the manuscript about which layers should be eligible to mixup and how such layers get picked up by the algorithm. I would suggest elaborating on this. - References: several preprints cited in the manuscript are in fact long-published. I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations. - I find the manifold mixup idea to be closely related to several lines of work for generalization abilities in machine learning (not just for deep neural networks). In particular, I would like to read the authors' opinion on possible connection to the vicinal risk minimization (VRM) framework, in which training data is perturbed before learning, to improve generalization (see, among other references, Chapelle et al., 2000). I feel it would help improve supporting the case of the manuscript and reach a broader community. * Minor issues - Tables 1 and 3: no confidence interval / standard deviation provided, diminishing the usefulness of those tables. - Footnote, page 4: I would suggest to add a reference to the consistency theorem, to improve readability.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "\u201c - There is little discussion in the manuscript about which layers should be eligible to mixup and how such layers get picked up by the algorithm . I would suggest elaborating on this. \u201d We performed a new experiment to directly study this . Because the theory in section 3 assumes that the part of the network after mixing is a universal approximator , there is a sensible case to be made for not mixing in the very last layer . For this experiment , we evaluated PreActResNet18 models on CIFAR-10 and considered mixing in a subset of the layers , we ran for fewer epochs than in the paper ( making the accuracies slightly lower across the board ) , and we decided to fix the alpha to 2.0 as we did in the paper for manifold mixup . We considered different subsets of layers to mix in , with 0 referring to the input , 1/2/3 referring to the output of the 1st/2nd/3rd resblocks respectively . For example { 0,2 } refers to mixing in the input layer and the output of the 2nd resblock . { } refers to no mixing . Layers : Test Accuracy { 0,1,2 } : 96.73 % { 0,1 } : 96.40 % { 0,1,2,3 } : 96.23 % { 1,2 } : 96.14 % { 0 } : 95.83 % { 1,2,3 } : 95.66 % { 1 } : 95.59 % { 2,3 } : 94.63 % { 2 } : 94.31 % { 3 } : 93.96 % { } : 93.21 % Essentially , it helps to mix in more layers , except for the later layers which hurts to some extent - which we believe is consistent with our theory . \u201c - References : several preprints cited in the manuscript are in fact long-published . I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations. \u201d We \u2019 ve updated all of the references to the conference/journal citations . See the new version of the paper uploaded . In the future it would be nice if arXiv could also list the bibtex for a conference/journal version , because these are often not easy to look up ( for example , for older ICLR conferences it was hard to find the bibtex ) . Google scholar does not help because it often only lists the first instance of the paper , which is usually arXiv . \u201c I find the manifold mixup idea to be closely related to several lines of work for generalization abilities in machine learning ( not just for deep neural networks ) . In particular , I would like to read the authors ' opinion on possible connection to the vicinal risk minimization ( VRM ) framework , in which training data is perturbed before learning , to improve generalization ( see , among other references , Chapelle et al. , 2000 ) . I feel it would help improve supporting the case of the manuscript and reach a broader community. \u201c The fundamental question of interest to us here is how deep networks behave when evaluated on points which are off of the data manifold . Vicinal risk minimization ( Chapelle 2000 ) , which you refer to , definitely seems like an improvement over ERM , but it seems like it \u2019 s very dependent on our ability to select the right \u201c vicinity \u201d . Our intuition is that our models should still be able to classify well off of the data manifold ( just meaning points x where p_data ( x ) =0 ) , by identifying factors and structural elements that are shared with the training distribution . VRM can deal with this if the vicinity covers points which are off of the manifold but doesn \u2019 t include points which change the class identity . In practice selecting this can be quite difficult . Defining the vicinity as a spherical-Gaussian around the data points is unlikely to capture much of the space that exists off of the data manifold ( or at least , reach these points with reasonable probability ) while avoiding class overlap . The \u201c AutoAugment \u201d paper ( Cubuk 2018 ) proposed to learn such augmentations with a neural architecture search procedure ( i.e.manually training submodels with different augmentation schemes and selecting those which lead to better generalization ) , although this is quite expensive and may be difficult to scale beyond a sequence of fixed augmentations ."}, {"review_id": "rJlRKjActQ-1", "review_text": "The tone of the paper is notably scientific, as the authors clearly state the assumptions and all observations, whether positive or negative. That said, the approach itself can be seen as a direct extension of the earlier advanced 'mixup' scheme. In addition to performing data augmentation solely in the input space, their method proposes to train the networks on the convex combinations of the hidden state representations by learning to map them to the convex combinations of their one-hot ground truth encodings. The results are competitive, in most cases exceeding the current state-of-art. However, the scheme has only been tested on low-res datasets such as MNIST, CIFAR and SVHN while the predecessor (plain 'mixup') also demonstrated improvement over the much larger and high-res ImageNet dataset. Although their work is not extremely novel, the experiments and observations could serve as a useful extension to this line of research. Suggestions: 1. The results on ImageNet would be a useful add-on to really drive home the benefit of their method when we talk of real-world large-scale datasets. 2. The associated functions represented by 'f', 'g' and 'h' change meaning between sec. 2 and sec. 3. It would be more smooth if some consistency in notations was maintained.", "rating": "6: Marginally above acceptance threshold", "reply_text": "R3 : \u201c Although their work is not extremely novel , the experiments and observations could serve as a useful extension to this line of research . \u201c Although novelty is subjective , there is a case that the work is actually quite novel : 1 ) We present a novel analysis of how manifold mixup changes representations ( section 3 ) which is totally different from the motivation of mixup ( and indeed deals with a completely different problem , as the inputs in input mixup are fixed and can not be changed by training ) . 2 ) The way that the representations are changed by manifold mixup is to our knowledge fairly unique , not just relative to mixup , but compared to other regularizers as well . For example if you look at Figure 1 and Figure 6 in appendix B , you \u2019 ll see that the way the representations are changed by manifold mixup is not accomplished by four common regularizers : weight decay , batch normalization , dropout , and adding noise to the hidden states . The representations look completely different , even though all of the methods succeed ( to some extent ) as regularizers . More concretely , manifold mixup has the fairly unique effect of concentrating the hidden states of the points from each class and encouraging the hidden state to have broad areas of low confidence between those regions . This is not accomplished to any appreciable degree by the other regularizers . This is some evidence that the method by which manifold mixup achieves regularization is fairly unique and worthy of further study . \u201c The associated functions represented by ' f ' , ' g ' and ' h ' change meaning between sec.2 and sec.3.It would be more smooth if some consistency in notations was maintained. \u201d Thanks , that \u2019 s a good catch . Our intent was for g to refer to the earlier part of the network and for f to refer to the later part of the network . We \u2019 ve fixed the notation and uploaded an updated version of the paper ."}, {"review_id": "rJlRKjActQ-2", "review_text": "The paper proposes a novel method called Manifold Mixup, which linearly interpolating (with a careful selected mixing ratio) two feature maps in latent space as well as their labels during training, aiming at regularizing deep neural networks for better generalization and robust to adversarial attacks. The authors experimentally show that networks with Manifold Mixup as regularizer can improve accuracy for both supervised and semi-supervised learning, are robust to adversarial attacks, and obtain promising results on Negative Log-Likelihood on held out samples. The paper is well written and easy to follow. Various experiments are conducted to support the contributions of the paper. Nevertheless, the technical novelty seems a bit weak to me. The method basically moves the interpolating process from input space as in MixUp to randomly selected hidden states. More importantly, some of the paper\u2019s claims are not very convincing to me in its current form. Major remarks: 1. The authors suggest that Mixup can suffer from interpolations intersecting with a real sample, but how Manifold Mixup can avoid this issue is not very clear to me. The authors theoretically prove that with the proposed training cost in Manifold Mixup, the representation for each class will lie on a subspace of dimension dim (h) \u2013d +1 (h and d are the hidden dimension and number of classes, respectively). I did not get the idea of how such dimension reduction relates to the \u2018\u2019flattening\" of the manifold and in particular how such representations (representations for each class \u201cconcentrating into local regions\u201d) can avoid the class collision issues as that in Mixup. Experimentally, from Figures 3 and 4, it seems the class collision issue could be worse than that of Mixup. For example, for mixing ratio of 0.6 (meaning the created image has almost half labels from the two original images), MixUp clearly shows, for instance in the second row, that there are two overlapped images (Horse and Plane), but Manifold Mixup seems to have only the Plane in the mixed image with a soft label. 2. The observations of mixing in the hidden space is better than mixing in the input space seem to contradictive to the observations by Mixup, it would be very useful if the paper can make that much clear to the readers. I would suggest that the authors fully compare with MixUp in the supervised learning tasks, namely using all the datasets (including ImageNet) and networks architectures used in MixUp for supervised learning. In this way, the paper would be much more convincing because the proposed method is so close to MixUp and the observation here is contradictive. 3. I wonder how sensitive is the parameter Alpha in Manifold Mixup. For example, how the mixing rate Alpha impacts the results for NLL and Semi-supervised learning in section 5.2? 4. It would be useful to also present the results for SVHN for supervised learning since the Cifar10 and Cifar100 datasets are similar, and the authors have already used SVHN for other task in the paper. Minor remarks: 1. In Table2, the result from AdaMix seems missed. 2. Why not using Cifar100, but with a new dataset SVHN for the semi-supervised learning in section 5.2? 3. In related work, regarding regularizing deep networks by perturbing the hidden states, the proposed method may relate to AgrLearn (Guo et al., Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks) as well. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . We will post a more detailed response with new experimental results soon , but I want to quickly address issues related to the motivation for why manifold mixup works . We also updated the paper with a new appendix section H ( page 20 ) which discusses this in more detail and gives an illustration . \u201c Mixup can suffer from interpolations intersecting with a real sample , but how Manifold Mixup can avoid this issue is not very clear to me \u2026 The observations of mixing in the hidden space is better than mixing in the input space seem to contradictive to the observations by Mixup , it would be very useful if the paper can make that much clear to the readers \u201d You are correct that manifold mixup works through a mechanism which is very different from input mixup , which I think is actually what makes it interesting . With input mixup , if the interpolations between two points of the same class intersect with points from a different class ( or interpolations are inconsistent ) , this leads to underfitting and poor performance . You can see this in the center column of figure 1 . However with manifold mixup , the hidden states of the network are learned , such that these inconsistent interpolations are avoided . To illustrate , let \u2019 s imagine that you have a binary classification problem with 2 examples from class A and 2 examples from class B. Let \u2019 s suppose that we perform manifold mixup in a single 1-dimensional hidden layer . Let \u2019 s say that the points from A are both at h=0 . Where can the points from B be located for the interpolations to all return the same label ? If the points from class B have different h values , then the interpolations must be inconsistent . For example if one point from class B is at h=1 and one point from B is at h=2 , then the point h=1 will either be labeled as 100 % class B or it will be labeled as 50 % class B / 50 % class A . This will cause manifold mixup to have error , and the only way for it to avoid this is to learn the hidden states such that all examples from each class maps to the same point . This is what needs to happen if we have a 1D hidden space and 2 classes . For higher dimensional hidden spaces , a similar phenomenon occurs but it is much less restrictive . Section 3 provides exact conditions for these inconsistent interpolations to be completely avoided . Essentially , the representations for each class need to \u201c flatten \u201d so that they don \u2019 t have any variation in directions which point towards other classes ( you can imagine that this would lead to inconsistent interpolations because some points of the same class would have different distances to points from the other classes ) . Figure 1c/1f shows exactly how this happens in a toy problem . Moreover in section 5.1 we presented an experiment where we train with manifold mixup , but don \u2019 t pass gradient to layers before the layer where we mix ( however all layers are still trained , as the layer to mix in is randomly selected on each update ) - and this made accuracy much worse . This is strong evidence that it is important for manifold mixup to learn to change the representations to make interpolations consistent . Why is it desirable for manifold mixup to change the representations to avoid inconsistent interpolations ? The first reason is that it can help to avoid underfitting , but another reason is that the way to make interpolations consistent is to make the representations for each class more concentrated , which can only be accomplished by forcing the network to learn more discriminative features in earlier layers . Please let me know if anything is unclear here , if you \u2019 re uncertain about part of the argument , or if there is any other type of illustration/figure that would be helpful ."}], "0": {"review_id": "rJlRKjActQ-0", "review_text": "TL;DR. a generalization of the mixup algorithm to any layer, improving generalization abilities. * Summary The manuscript generalizes the mixup algorithm (Zhang et al., 2017) which proposed to interpolate between inputs to yield better generalization. The present manuscript addresses a fairly more general setting as the mixup may occur at *any* layer of the network, not just the input layer. Once a layer is chosen, mixup occurs with a random proportion $\\lambda\\in (0,1)$ (sampled from a $\\mathrm{Beta}(\\alpha,\\alpha)$ distribution). A salient asset of the manuscript is that it avoids a pitfall of the original mixup algorithm: interpolating between inputs may result in underfitting (if inputs are far from each others: the interpolation may overlap with existing inputs). Interpolating deep layers of the networks makes it less prone to this phenomenon. A sufficient condition for Manifold Mixup to avoid this underfitting phenomenon is that the dimension of the hidden layer exceeds the number of classes. I found no flaw in the (two) proofs. Literature is well acknowledged. In my opinion, a clear accept. * Major remarks - There is little discussion in the manuscript about which layers should be eligible to mixup and how such layers get picked up by the algorithm. I would suggest elaborating on this. - References: several preprints cited in the manuscript are in fact long-published. I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations. - I find the manifold mixup idea to be closely related to several lines of work for generalization abilities in machine learning (not just for deep neural networks). In particular, I would like to read the authors' opinion on possible connection to the vicinal risk minimization (VRM) framework, in which training data is perturbed before learning, to improve generalization (see, among other references, Chapelle et al., 2000). I feel it would help improve supporting the case of the manuscript and reach a broader community. * Minor issues - Tables 1 and 3: no confidence interval / standard deviation provided, diminishing the usefulness of those tables. - Footnote, page 4: I would suggest to add a reference to the consistency theorem, to improve readability.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "\u201c - There is little discussion in the manuscript about which layers should be eligible to mixup and how such layers get picked up by the algorithm . I would suggest elaborating on this. \u201d We performed a new experiment to directly study this . Because the theory in section 3 assumes that the part of the network after mixing is a universal approximator , there is a sensible case to be made for not mixing in the very last layer . For this experiment , we evaluated PreActResNet18 models on CIFAR-10 and considered mixing in a subset of the layers , we ran for fewer epochs than in the paper ( making the accuracies slightly lower across the board ) , and we decided to fix the alpha to 2.0 as we did in the paper for manifold mixup . We considered different subsets of layers to mix in , with 0 referring to the input , 1/2/3 referring to the output of the 1st/2nd/3rd resblocks respectively . For example { 0,2 } refers to mixing in the input layer and the output of the 2nd resblock . { } refers to no mixing . Layers : Test Accuracy { 0,1,2 } : 96.73 % { 0,1 } : 96.40 % { 0,1,2,3 } : 96.23 % { 1,2 } : 96.14 % { 0 } : 95.83 % { 1,2,3 } : 95.66 % { 1 } : 95.59 % { 2,3 } : 94.63 % { 2 } : 94.31 % { 3 } : 93.96 % { } : 93.21 % Essentially , it helps to mix in more layers , except for the later layers which hurts to some extent - which we believe is consistent with our theory . \u201c - References : several preprints cited in the manuscript are in fact long-published . I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations. \u201d We \u2019 ve updated all of the references to the conference/journal citations . See the new version of the paper uploaded . In the future it would be nice if arXiv could also list the bibtex for a conference/journal version , because these are often not easy to look up ( for example , for older ICLR conferences it was hard to find the bibtex ) . Google scholar does not help because it often only lists the first instance of the paper , which is usually arXiv . \u201c I find the manifold mixup idea to be closely related to several lines of work for generalization abilities in machine learning ( not just for deep neural networks ) . In particular , I would like to read the authors ' opinion on possible connection to the vicinal risk minimization ( VRM ) framework , in which training data is perturbed before learning , to improve generalization ( see , among other references , Chapelle et al. , 2000 ) . I feel it would help improve supporting the case of the manuscript and reach a broader community. \u201c The fundamental question of interest to us here is how deep networks behave when evaluated on points which are off of the data manifold . Vicinal risk minimization ( Chapelle 2000 ) , which you refer to , definitely seems like an improvement over ERM , but it seems like it \u2019 s very dependent on our ability to select the right \u201c vicinity \u201d . Our intuition is that our models should still be able to classify well off of the data manifold ( just meaning points x where p_data ( x ) =0 ) , by identifying factors and structural elements that are shared with the training distribution . VRM can deal with this if the vicinity covers points which are off of the manifold but doesn \u2019 t include points which change the class identity . In practice selecting this can be quite difficult . Defining the vicinity as a spherical-Gaussian around the data points is unlikely to capture much of the space that exists off of the data manifold ( or at least , reach these points with reasonable probability ) while avoiding class overlap . The \u201c AutoAugment \u201d paper ( Cubuk 2018 ) proposed to learn such augmentations with a neural architecture search procedure ( i.e.manually training submodels with different augmentation schemes and selecting those which lead to better generalization ) , although this is quite expensive and may be difficult to scale beyond a sequence of fixed augmentations ."}, "1": {"review_id": "rJlRKjActQ-1", "review_text": "The tone of the paper is notably scientific, as the authors clearly state the assumptions and all observations, whether positive or negative. That said, the approach itself can be seen as a direct extension of the earlier advanced 'mixup' scheme. In addition to performing data augmentation solely in the input space, their method proposes to train the networks on the convex combinations of the hidden state representations by learning to map them to the convex combinations of their one-hot ground truth encodings. The results are competitive, in most cases exceeding the current state-of-art. However, the scheme has only been tested on low-res datasets such as MNIST, CIFAR and SVHN while the predecessor (plain 'mixup') also demonstrated improvement over the much larger and high-res ImageNet dataset. Although their work is not extremely novel, the experiments and observations could serve as a useful extension to this line of research. Suggestions: 1. The results on ImageNet would be a useful add-on to really drive home the benefit of their method when we talk of real-world large-scale datasets. 2. The associated functions represented by 'f', 'g' and 'h' change meaning between sec. 2 and sec. 3. It would be more smooth if some consistency in notations was maintained.", "rating": "6: Marginally above acceptance threshold", "reply_text": "R3 : \u201c Although their work is not extremely novel , the experiments and observations could serve as a useful extension to this line of research . \u201c Although novelty is subjective , there is a case that the work is actually quite novel : 1 ) We present a novel analysis of how manifold mixup changes representations ( section 3 ) which is totally different from the motivation of mixup ( and indeed deals with a completely different problem , as the inputs in input mixup are fixed and can not be changed by training ) . 2 ) The way that the representations are changed by manifold mixup is to our knowledge fairly unique , not just relative to mixup , but compared to other regularizers as well . For example if you look at Figure 1 and Figure 6 in appendix B , you \u2019 ll see that the way the representations are changed by manifold mixup is not accomplished by four common regularizers : weight decay , batch normalization , dropout , and adding noise to the hidden states . The representations look completely different , even though all of the methods succeed ( to some extent ) as regularizers . More concretely , manifold mixup has the fairly unique effect of concentrating the hidden states of the points from each class and encouraging the hidden state to have broad areas of low confidence between those regions . This is not accomplished to any appreciable degree by the other regularizers . This is some evidence that the method by which manifold mixup achieves regularization is fairly unique and worthy of further study . \u201c The associated functions represented by ' f ' , ' g ' and ' h ' change meaning between sec.2 and sec.3.It would be more smooth if some consistency in notations was maintained. \u201d Thanks , that \u2019 s a good catch . Our intent was for g to refer to the earlier part of the network and for f to refer to the later part of the network . We \u2019 ve fixed the notation and uploaded an updated version of the paper ."}, "2": {"review_id": "rJlRKjActQ-2", "review_text": "The paper proposes a novel method called Manifold Mixup, which linearly interpolating (with a careful selected mixing ratio) two feature maps in latent space as well as their labels during training, aiming at regularizing deep neural networks for better generalization and robust to adversarial attacks. The authors experimentally show that networks with Manifold Mixup as regularizer can improve accuracy for both supervised and semi-supervised learning, are robust to adversarial attacks, and obtain promising results on Negative Log-Likelihood on held out samples. The paper is well written and easy to follow. Various experiments are conducted to support the contributions of the paper. Nevertheless, the technical novelty seems a bit weak to me. The method basically moves the interpolating process from input space as in MixUp to randomly selected hidden states. More importantly, some of the paper\u2019s claims are not very convincing to me in its current form. Major remarks: 1. The authors suggest that Mixup can suffer from interpolations intersecting with a real sample, but how Manifold Mixup can avoid this issue is not very clear to me. The authors theoretically prove that with the proposed training cost in Manifold Mixup, the representation for each class will lie on a subspace of dimension dim (h) \u2013d +1 (h and d are the hidden dimension and number of classes, respectively). I did not get the idea of how such dimension reduction relates to the \u2018\u2019flattening\" of the manifold and in particular how such representations (representations for each class \u201cconcentrating into local regions\u201d) can avoid the class collision issues as that in Mixup. Experimentally, from Figures 3 and 4, it seems the class collision issue could be worse than that of Mixup. For example, for mixing ratio of 0.6 (meaning the created image has almost half labels from the two original images), MixUp clearly shows, for instance in the second row, that there are two overlapped images (Horse and Plane), but Manifold Mixup seems to have only the Plane in the mixed image with a soft label. 2. The observations of mixing in the hidden space is better than mixing in the input space seem to contradictive to the observations by Mixup, it would be very useful if the paper can make that much clear to the readers. I would suggest that the authors fully compare with MixUp in the supervised learning tasks, namely using all the datasets (including ImageNet) and networks architectures used in MixUp for supervised learning. In this way, the paper would be much more convincing because the proposed method is so close to MixUp and the observation here is contradictive. 3. I wonder how sensitive is the parameter Alpha in Manifold Mixup. For example, how the mixing rate Alpha impacts the results for NLL and Semi-supervised learning in section 5.2? 4. It would be useful to also present the results for SVHN for supervised learning since the Cifar10 and Cifar100 datasets are similar, and the authors have already used SVHN for other task in the paper. Minor remarks: 1. In Table2, the result from AdaMix seems missed. 2. Why not using Cifar100, but with a new dataset SVHN for the semi-supervised learning in section 5.2? 3. In related work, regarding regularizing deep networks by perturbing the hidden states, the proposed method may relate to AgrLearn (Guo et al., Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks) as well. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . We will post a more detailed response with new experimental results soon , but I want to quickly address issues related to the motivation for why manifold mixup works . We also updated the paper with a new appendix section H ( page 20 ) which discusses this in more detail and gives an illustration . \u201c Mixup can suffer from interpolations intersecting with a real sample , but how Manifold Mixup can avoid this issue is not very clear to me \u2026 The observations of mixing in the hidden space is better than mixing in the input space seem to contradictive to the observations by Mixup , it would be very useful if the paper can make that much clear to the readers \u201d You are correct that manifold mixup works through a mechanism which is very different from input mixup , which I think is actually what makes it interesting . With input mixup , if the interpolations between two points of the same class intersect with points from a different class ( or interpolations are inconsistent ) , this leads to underfitting and poor performance . You can see this in the center column of figure 1 . However with manifold mixup , the hidden states of the network are learned , such that these inconsistent interpolations are avoided . To illustrate , let \u2019 s imagine that you have a binary classification problem with 2 examples from class A and 2 examples from class B. Let \u2019 s suppose that we perform manifold mixup in a single 1-dimensional hidden layer . Let \u2019 s say that the points from A are both at h=0 . Where can the points from B be located for the interpolations to all return the same label ? If the points from class B have different h values , then the interpolations must be inconsistent . For example if one point from class B is at h=1 and one point from B is at h=2 , then the point h=1 will either be labeled as 100 % class B or it will be labeled as 50 % class B / 50 % class A . This will cause manifold mixup to have error , and the only way for it to avoid this is to learn the hidden states such that all examples from each class maps to the same point . This is what needs to happen if we have a 1D hidden space and 2 classes . For higher dimensional hidden spaces , a similar phenomenon occurs but it is much less restrictive . Section 3 provides exact conditions for these inconsistent interpolations to be completely avoided . Essentially , the representations for each class need to \u201c flatten \u201d so that they don \u2019 t have any variation in directions which point towards other classes ( you can imagine that this would lead to inconsistent interpolations because some points of the same class would have different distances to points from the other classes ) . Figure 1c/1f shows exactly how this happens in a toy problem . Moreover in section 5.1 we presented an experiment where we train with manifold mixup , but don \u2019 t pass gradient to layers before the layer where we mix ( however all layers are still trained , as the layer to mix in is randomly selected on each update ) - and this made accuracy much worse . This is strong evidence that it is important for manifold mixup to learn to change the representations to make interpolations consistent . Why is it desirable for manifold mixup to change the representations to avoid inconsistent interpolations ? The first reason is that it can help to avoid underfitting , but another reason is that the way to make interpolations consistent is to make the representations for each class more concentrated , which can only be accomplished by forcing the network to learn more discriminative features in earlier layers . Please let me know if anything is unclear here , if you \u2019 re uncertain about part of the argument , or if there is any other type of illustration/figure that would be helpful ."}}