{"year": "2021", "forum": "QpT9Q_NNfQL", "title": "NeurWIN: Neural Whittle Index Network for Restless Bandits via Deep RL", "decision": "Reject", "meta_review": "This paper approximates the Whittle index in restless bandits using a neural network. Finding the Whittle index is a difficult problem and all reviewers agreed on this. Nevertheless, the scores of this paper are split between 2x 4 and 2x 7, essentially along the line of whether this paper is too preliminary to be accepted. Therefore, I read the paper and propose a rejection.\n\nThe reason is that the paper lacks rigor, which was brought up by the two reviewers who suggested rejections. For instance, in the last line of Algorithm 1, it is not clear what kind of a gradient is computed. The reason is that \\bar{G}_b is not a proper baseline, as it depends on the future actions of the bandit policy in any given round. I suggest that the authors look at recent papers on meta-learning of bandit policies by policy gradients,\n\nhttps://papers.nips.cc/paper/2020/hash/171ae1bbb81475eb96287dd78565b38b-Abstract.html\n\nhttps://arxiv.org/abs/2006.16507\n\nThis is the level of rigor that I would expect from this paper, to make sure that the gradients are correct.", "reviews": [{"review_id": "QpT9Q_NNfQL-0", "review_text": "This paper proposes the use of Deep Learning , namely a multi-layer perceptron , for approximating the Whittle index in restless bandits . The introduction and the related works are well written . The problem and the background on restless bandits are clearly exposed . However , there are a lot of issues in the presentation of the proposed algorithm , NeurWin , in the analysis of the proposed algorithm , and in the experimentations . The statement of Corollary 1 is not correct . In the general case , the Whittle index is not optimal , it is a heuristic . So if the neural controller produces the Whittle index , it does not produce the optimal discounted reward . As the authors suggest the statement of Corollary 1 should be rewritten as a necessary condition for a neural network to be Whittle-accurate . The proof of Theorem 2 is wrong . The statement of Theorem 2 is for any states s_0 , s_1 , for any \\lambda \\in [ f_\\theta ( s_0 ) -\\delta , f_\\theta ( s_0 ) + \\delta ] \u2026 then the neural network is \\gamma-Whittle-accurate . In the proof the authors only show that the neural network is \\gamma-Whittle-accurate , when s_0=s_1 and when \\lambda = , f_\\theta ( s_0 ) + \\delta . So they can not conclude that Theorem 2 holds for any states s_0 , s_1 , and for any \\lambda \\in [ f_\\theta ( s_0 ) -\\delta , f_\\theta ( s_0 ) + \\delta ] . In the section 4.2 where the training procedure is described the authors write that the choice of s_1 depends on the MAB problems , and hence it has be chosen knowing the MAB problem . The hearth of MAB problem is precisely that you do not know if some states or some arms have to be less-frequently-visited that others . This is the exploration / exploitation dilemma . So if for tuning the proposed algorithm you need to know the MAB problem , the proposed algorithm does not work at all . In the experimental section the experiment on recovering bandits is not convincing . The authors write that the algorithmic complexity of the algorithms proposed in recovering bandits is ( \\binom ( N , M ) ) ^d . It is not correct . In recovering bandits the arms are the recovering functions . In your experiment it should be N , and d is the number of times the arms are sampled in a round , so in your experiment it should be M. The algorithmic complexity is N^M , that it is still very large . However in the cited paper the authors propose the use of optimistic planning with a given budget B . So the algorithmic complexity of their algorithms is B , which is a parameter . So the choice of d=1 in the experiment is unfair . Moreover , in the experiment the choice of only four different recovering functions for 100 arms is a little bit strange . In recovering bandits the arms are the recovering functions . Finally , the choice of training offline the neural network is not realistic in a bandit setting , where the environment is not known at the beginning . To make a fair comparison with other bandit algorithms the authors should train the neural network online and compare the accumulated rewards taking into account the convergence time to a good solution . Approximating the Whittle index by a multi-layer perceptron is a good idea , but the submitted paper is not ready for publication . __________________________________________________________ After rebuttal , the authors reformulate Theorem 2 , and then I think it is right . I raised my score . I am still not convinced by their experiments on recovering bandits .", "rating": "4: Ok but not good enough - rejection", "reply_text": "It appears that the reviewer has some misunderstandings about the paper . We would like to clarify the issues and better explain them in the paper . 1.Regarding Corollary 1 : When one considers that there is only one arm with an activation cost , then Whittle index IS the optimal control policy as long as the arm is indexable . This is the direct result from Theorem 1 . It seems that the `` general case '' under which the reviewer claims that Whittle index policy may not be optimal is the case when there are multiple arms , and the controller chooses to activate the arms with the highest indices . In Section 4 , we are focused on training the Whittle index for one single arm , as shown in Fig.1.Hence , Theorem 1 and Corollary 1 hold . We will revise the paper to make it clear that we consider one single arm at the beginning of Section 4 . 2.Regarding the proof of Theorem 2 : We would like to explain the proof procedure . Theorem 2 states that `` If the condition holds for any ( s_0 , s_1 , \\lambda ) , then the neural network is \\gamma-Whittle-accurate . '' In the proof , we prove the equivalent statement : `` If the neural network is NOT \\gamma-Whittle-accurate , then there exists at least one ( s_0 , s_1 , \\lambda ) that violates the condition . '' Indeed , the proof explains how to find the ( s_0 , s_1 , \\lambda ) that violates the condition , and hence the theorem holds . We will revise the paper so that it is clear that we are proving the equivalent statement at the beginning of the proof . 3.Regarding the complexity of the recovering bandits : It seems the issue is that the reviewer misunderstands the d-lookahead algorithm since it is not clearly described in our paper . The variable d is NOT `` the number of times the arms are sampled in a round '' , as the reviewer claims . It is the number of steps that the d-lookahead algorithm enumerates . Each step consists of choosing M arms out of N arms , with a total of ( N \\choose M ) possible choices . Therefore , in d steps , there are a total of ( N \\choose M ) ^d different choices . In the recovering bandit paper , it considers the case with N = K and M = 1 . In the beginning of Section 6 of the recovering bandit paper , it indeed states that `` Algorithm 1 ... searches K^d leaves '' . Note that K^d = ( K \\choose 1 ) ^d . This is consistent with our complexity analysis . We would also like to point out that the trick used in the recovering bandit paper to improve computational efficiency is not sufficient for our case . In Section 7 , the recovering bandit paper states that the trick still requires to evaluate ~0.1 % of all possible choices . For the case N = 100 , M = 25 , d = 2 , there are more than 5 * 10^46 possible choices . Even evaluating only 0.1 % of them is intractable . Thus , we are only able to present the result for d = 1 . Please note that , when d =1 , the d-lookahead algorithm still activates M arms in each step . Hence , the comparison is fair . Seeing the confusion arisen , we will update the paper to include a short description of the d-lookahead algorithm . 4.Regarding offline training : We would like to stress that this paper studies * restless * multi-armed bandit problem . For restless bandits , the optimal control policy is difficult to find even when the environment is known . Indeed , there are many problems where the environments are well-defined , but the optimal control is not known , such as the wireless scheduling problem described in Section 5.3 . We believe that our work makes an important contribution to these problems . For example , we show in Section 5.3 that NeurWIN achieves better reward than the best-known policy . For problems where the environments are not known a priori , we believe that our work nicely compliments existing studies that aim to learn the environments but fail to find the optimal control policy . For example , the recovering bandit paper only uses the heuristic d-lookahead after learning the environment . Our work would compliment the recovering bandit paper by finding the near-optimal control policy after it learns the environment . We will add some discussions to highlight the usefulness of this work ."}, {"review_id": "QpT9Q_NNfQL-1", "review_text": "The paper `` NeurWIN : Neural Whittle Index Network for Restless Bandits via Deep RL '' proposes a new RL approach for estimating the Whittle index in restless bandit problems , which allows to define effective strategies for selecting arms to activate at each round of the bandit process . I found the paper very interesting . Maybe this was because I was not familiar with the Whittle index and such kind of policies for restless bandits . But the paper is very didactic , I liked to discover these concepts . More importantly , I found the proposed approach very well presented , with relevant theoretical justification provided . The approach is elegant and looks useful from the results . I still have some concerns however . First , one limitation of the approach is that it requires to have access to a simulator in the training phase , that strictly follows the dynamics of the real world it is designed for . It would have been nice to consider the impact of discrepancies between simulator and real-world dynamics for the setting considered . What happens if dynamics are not stationary ? Second , it would have be very useful to consider more baselines in the experiments . Authors argue that they only could applied a classical Reinforce strategy on the ( 4,1 ) setting , since the combinatorial aspect of the action and state space is problematic beyond that point . They also claim that if this approach is not effective even in that simple setting , it is mostly due to the combinatorial dimension of the state space . Ok , but other instances of this could have been considered . At least , it would have been important to consider a policy trained with Reinforce on individual states , and then select the M best outputs at each round , as it is done for the proposed approach ! Also , multi-agent RL approaches , such as MADDPG or Q-Mix , could have been considered to cope with the combinatorial aspect of the problem , and assess if cooperation would help improve the results . At last , authors give conditions for the applicability of their method . It would have been useful to help the reader with some intuitions about what imply these indexability and strong indexability conditions . Are they very constraining ? What kind of restless bandit settings are excluded due to these two necessary conditions ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for the helpful and detailed review . Below , we answer the concerns raised . 1 . `` First , one limitation of the approach is that it requires to have access to a simulator in the training phase ... '' We thank the reviewer for raising this issue . To address the concern , we conduct additional experiments where the simulator is a noisy one . Specifically , the simulator has an average 5 % error is its reward estimation . Due to the time constraint , we were only able to complete the experiments for the recovering bandits and the deadline scheduling . Our results show that NeurWIN still performs well despite that the simulators are noisy . 2 . `` Second , it would have be very useful to consider more baselines in the experiments ... '' We thank the reviewer for the suggestion . We implemented and tested ( Fu et al.2019 ) , a Q-learning Whittle index algorithm , in our experiments . Results show that NeurWIN achieves superior performance . 3 . `` At last , authors give conditions for the applicability of their method . It would have been useful to help the reader with some intuitions about what imply these indexability and strong indexability conditions ... '' Both the indexability condition and the strong indexability condition basically states the following : `` The net reward of activating an arm in a given state ( strictly ) decreases as the activation cost increases , '' which seems to be intuitively true for most practical problems . Indeed , many practical problems have been proven to be indexable in the literature ( see below for an incomplete list ) . However , proving whether a problem is indexable or not is usually done on a case-by-case basis . As such , we are unable to provide a simple answer to whether a problem is strongly indexable . Borkar , Vivek S. , and Sarath Pattathil . `` Whittle indexability in egalitarian processor sharing systems . '' Annals of Operations Research , 2017. https : //arxiv.org/pdf/1707.02440.pdf Mate , Aditya , Jackson Killian , Haifeng Xu , Andrew Perrault , and Milind Tambe . `` Collapsing Bandits and Their Application to Public Health Intervention . '' Advances in Neural Information Processing Systems ( NeurIPS ) , 2020. https : //proceedings.neurips.cc/paper/2020/file/b460cf6b09878b00a3e1ad4c72344ccd-Paper.pdf Liu , Keqin , and Qing Zhao . `` Indexability of restless bandit problems and optimality of whittle index for dynamic multichannel access . '' IEEE Transactions on Information Theory , 2010. https : //arxiv.org/abs/0810.4658 Hsu , Yu-Pin , Eytan Modiano , and Lingjie Duan . `` Scheduling algorithms for minimizing age of information in wireless broadcast networks with random arrivals . '' IEEE Transactions on Mobile Computing , 2019. https : //arxiv.org/abs/1712.07419 Xu , Yizhen , Peng Cheng , Zhuo Chen , Ming Ding , Yonghui Li , and Branka Vucetic . `` Real-Time Task Offloading for Large-Scale Mobile Edge Computing . '' IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , 2020. https : //ieeexplore.ieee.org/abstract/document/9054413 Anand , Arjun , and Gustavo de Veciana . `` A Whittle 's index based approach for QoE optimization in wireless networks . '' Proceedings of the ACM on Measurement and Analysis of Computing Systems , 2018. http : //users.ece.utexas.edu/~gustavo/papers/AnD18.pdf"}, {"review_id": "QpT9Q_NNfQL-2", "review_text": "The paper proves a simple but important result which essentially states the following : It is possible to construct an RL environment for a one arm restless bandit system , such that any neural network which achieves a certain discounted net reward has to approximate the whittle index of that arm well . The paper then uses this observation to form exactly such an environment and train a neural-network using REINFORCE to maximize discounted reward for the environment , thus in principle achieving a good approximation of the Whittle index of the arm . The network can then be used to approximate a Whittle index based policy for restless bandit problems . I think the idea is interesting and it seems to work well in practice on three simulated but well-studied environments . In particular in the deadline scheduling problem where the optimal strategy is known , it can be observed that this policy can match the performance of the optimal policy after several hundred training epochs . I recommend the paper for acceptance . I would encourage the authors to provide confidence bars for their algorithm as the results are presented over 10 independent runs only . It is vital to see the variance in rewards obtained by the strategy .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for time taken in assessing our work . We also thank the reviewer for the great advice on confidence interval . We rerun our experiments for 200 independent runs and provide confidence intervals in the figures for wireless scheduling and deadline scheduling . For recovering bandit , the problem itself is not stochastic , and hence there is only one experiment run ."}, {"review_id": "QpT9Q_NNfQL-3", "review_text": "This paper considers the problem of learning how to control restless bandits . When all parameters of the system are known , Whittle index policy usually offers a good performance . The main contribution of the paper is to propose an algorithm , NeurWIN , that uses a neural network architecture to learn the Whittle indices . Most of the paper is devoted to the description and the derivation of the algorithm . At the end of the paper , the authors present four illustrations of how the algorithm works . The algorithm is compared to an index-based policy and an ( old ? ) RL algorithm REINFORCE for the small systems . The learning behavior of NeurWIN is very good in all tested cases . The paper does not contain theoretical result . It is mostly about presenting an algorithm and performing numerical experiments to demonstrate that it works . Yet , there is no comparison with ( reasonable ) related work . In the numerical part , the only algorithm to which the authors compare their algorithm is REINFORCE algororithm of Williams 92 , that is clearly non-adapted to learning whittle index ( because of the large state-space ) . As acknowledged by the authors , there are a number of recent work on learning Whittle index ( last paragraph of Section 2 ) : I do not understand why these solutions are not implemented . Also , I am not convinced that the algorithm of ( Avrachenkov , Borkar,2019 ) is not applicable to the current context . The paper is a bit sloppy and not very precise . For instance : - The authors write several times `` Finding Whittle index is typically intractable '' without proof or precise reference . I doubt that this statement is true . - page 4 , top : the definition of strong indexability needs some clarifications : how strong is the assumption ? Do the presented example satisfy this assumption ? And if not , is it important ? - page 4 : is it really the problem statement ? Then why are the simulation results not showing this ? - page 7 ( end ) : `` The size of the state-space is 10^ { 12 } : given the appendix , this seems largely exaggerated ( the order seems closer to 10^6 if one discretize byte per byte ) . Moreover , discretizing at the kB level would lead to about 10^3 states in which case Whittle index would be applicable . - page 8 ( beginning ) : `` REINFORCE [ ... ] improves a litle '' - > this seems wrong . - page 8 , Figure 4 ( a ) : how can NeurWIN perform better than Whittle index ? I thought that NeurWIN was learning those indices . The paper would be much stronger , if it would contain either some theoretical guarantee , or a more thourough comparison of performance with related work .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank reviewer # 3 for the constructive and detailed feedback . We discuss each point the reviewer gave below . 1.Regarding the comparison with recent work : We thank the reviewer for raising this issue . We will implement ( Fu et al.2019 ) , a Q-learning Whittle index algorithm , and compare its performance with NeurWIN . We would like to point out that ( Fu et al.2019 ) is a tabular method that may not scale well . Further , the paper itself shows that its algorithm does not converge to the Whittle index . Still , as it is the closest recent work to our NeurWIN , we will implement and test it . For ( Avrachenkov , Borkar 2019 ) , the proposed algorithm is only applicable when the state of an arm is a scalar . In this paper , we consider that the state of an arm can be a vector . Indeed , the applications in Section 5.3 and 5.4 both have vector states . Hence , ( Avrachenkov , Borkar 2019 ) is not applicable . 2.Regarding the hardness of finding the Whittle index : To the best of our knowledge , there is no tractable solutions for finding Whittle indices . The standard approach , as described in Section III.B in ( Yu et al. , 2018 ) , finds Whittle indices by solving uncountably infinitely many linear programming problems , one for each activation cost \\lambda in ( -\\infty , +\\infty ) . This approach is clearly not tractable . While we are not aware of any hardness result of finding the Whittle index , we would like the reviewer to consider the following two points : A . Most existing work on Whittle index is only able to find the Whittle index for some special cases . B.In ( Aalto et al. , 2015 ) , which we discussed in Section 5.3 , the authors were only able to find the Whittle index under some relaxations . If finding Whittle indices were tractable , researchers would n't have resorted to studying special cases and/or using relaxations . 3 . `` page 4 , top : the definition of strong indexability needs some clarifications : how strong is the assumption ? Do the presented example satisfy this assumption ? And if not , is it important ? '' We need the strong indexability condition to prove theorem 2 . However , NeurWIN can still be applied if this condition is not met . We would also like to point out that the strong indexability condition basically states the following : `` The net reward of activating an arm in a given state strictly decreases as the activation cost increases , '' which seems to be intuitively true for most practical problems . In the literature , proving whether a problem is indexable or not is usually done on a case-by-case basis . Proving whether a problem is strongly indexable is beyond the scope of this work . 4 . `` page 4 : is it really the problem statement ? Then why are the simulation results not showing this ? '' The problem statement is to find neural networks that are \\gamma-Whittle-accurate . In the first two applications that we evaluate ( Sections 5.2 and 5.3 ) , there are no known ground truths about the Whittle indices . Hence , we are unable to evaluate whether NeurWIN is Whittle-accurate . We can only evaluate NeurWIN by its resulting rewards . As the rewards are higher than state-of-the-art policies , it appears that NeurWIN must produce accurate predictions of Whittle indices . For the application in Section 5.4 , we do have the ground truths and we will include the comparison with ground truths in the paper . What we observe is that NeurWIN is accurate when T is small , but is less accurate when T is large . This is reasonable . In Section 5.4 , T is the time to deadline . When the time to deadline is large , the actions have little impact on the resulting performance . However , when T is small , making the wrong action can result in big penalty . The fact that NeurWIN is more accurate when T is small suggests that NeurWIN focuses more on learning the Whittle indices for states with higher importance . 5 . `` page 7 ( end ) : `` The size of the state-space is 10^ { 12 } : given the appendix , this seems largely exaggerated ... '' We thank the reviewer for spotting the typo . The size of the state-space is 2 * 10^6 . The size of the file ranges from 1 Byte to 10^6 Bytes . The channel can be either good or bad . We will correct the typo . 6 . `` page 8 ( beginning ) : `` REINFORCE [ ... ] improves a litle '' - > this seems wrong . '' We thank the reviewer for spotting the typo . We meant to say `` REINFORCE ... improves little , '' meaning that it does not improve . We will correct the typo . 7 . `` page 8 , Figure 4 ( a ) : how can NeurWIN perform better than Whittle index ? `` As mentioned in page 1 end of paragraph 2 , Whittle index policy is asymptotically optimal in many settings . Here , `` asymptotically optimal '' means that it converges to the optimal policy when the number of arms becomes large . For a small number of arms ( e.g.N = 4 ) , there may exist a family of control policies that perform better than the Whittle index-based policy . For NeurWIN with a small set of arms N , we observed that the learned index representations converged to one such control hypothesis ."}], "0": {"review_id": "QpT9Q_NNfQL-0", "review_text": "This paper proposes the use of Deep Learning , namely a multi-layer perceptron , for approximating the Whittle index in restless bandits . The introduction and the related works are well written . The problem and the background on restless bandits are clearly exposed . However , there are a lot of issues in the presentation of the proposed algorithm , NeurWin , in the analysis of the proposed algorithm , and in the experimentations . The statement of Corollary 1 is not correct . In the general case , the Whittle index is not optimal , it is a heuristic . So if the neural controller produces the Whittle index , it does not produce the optimal discounted reward . As the authors suggest the statement of Corollary 1 should be rewritten as a necessary condition for a neural network to be Whittle-accurate . The proof of Theorem 2 is wrong . The statement of Theorem 2 is for any states s_0 , s_1 , for any \\lambda \\in [ f_\\theta ( s_0 ) -\\delta , f_\\theta ( s_0 ) + \\delta ] \u2026 then the neural network is \\gamma-Whittle-accurate . In the proof the authors only show that the neural network is \\gamma-Whittle-accurate , when s_0=s_1 and when \\lambda = , f_\\theta ( s_0 ) + \\delta . So they can not conclude that Theorem 2 holds for any states s_0 , s_1 , and for any \\lambda \\in [ f_\\theta ( s_0 ) -\\delta , f_\\theta ( s_0 ) + \\delta ] . In the section 4.2 where the training procedure is described the authors write that the choice of s_1 depends on the MAB problems , and hence it has be chosen knowing the MAB problem . The hearth of MAB problem is precisely that you do not know if some states or some arms have to be less-frequently-visited that others . This is the exploration / exploitation dilemma . So if for tuning the proposed algorithm you need to know the MAB problem , the proposed algorithm does not work at all . In the experimental section the experiment on recovering bandits is not convincing . The authors write that the algorithmic complexity of the algorithms proposed in recovering bandits is ( \\binom ( N , M ) ) ^d . It is not correct . In recovering bandits the arms are the recovering functions . In your experiment it should be N , and d is the number of times the arms are sampled in a round , so in your experiment it should be M. The algorithmic complexity is N^M , that it is still very large . However in the cited paper the authors propose the use of optimistic planning with a given budget B . So the algorithmic complexity of their algorithms is B , which is a parameter . So the choice of d=1 in the experiment is unfair . Moreover , in the experiment the choice of only four different recovering functions for 100 arms is a little bit strange . In recovering bandits the arms are the recovering functions . Finally , the choice of training offline the neural network is not realistic in a bandit setting , where the environment is not known at the beginning . To make a fair comparison with other bandit algorithms the authors should train the neural network online and compare the accumulated rewards taking into account the convergence time to a good solution . Approximating the Whittle index by a multi-layer perceptron is a good idea , but the submitted paper is not ready for publication . __________________________________________________________ After rebuttal , the authors reformulate Theorem 2 , and then I think it is right . I raised my score . I am still not convinced by their experiments on recovering bandits .", "rating": "4: Ok but not good enough - rejection", "reply_text": "It appears that the reviewer has some misunderstandings about the paper . We would like to clarify the issues and better explain them in the paper . 1.Regarding Corollary 1 : When one considers that there is only one arm with an activation cost , then Whittle index IS the optimal control policy as long as the arm is indexable . This is the direct result from Theorem 1 . It seems that the `` general case '' under which the reviewer claims that Whittle index policy may not be optimal is the case when there are multiple arms , and the controller chooses to activate the arms with the highest indices . In Section 4 , we are focused on training the Whittle index for one single arm , as shown in Fig.1.Hence , Theorem 1 and Corollary 1 hold . We will revise the paper to make it clear that we consider one single arm at the beginning of Section 4 . 2.Regarding the proof of Theorem 2 : We would like to explain the proof procedure . Theorem 2 states that `` If the condition holds for any ( s_0 , s_1 , \\lambda ) , then the neural network is \\gamma-Whittle-accurate . '' In the proof , we prove the equivalent statement : `` If the neural network is NOT \\gamma-Whittle-accurate , then there exists at least one ( s_0 , s_1 , \\lambda ) that violates the condition . '' Indeed , the proof explains how to find the ( s_0 , s_1 , \\lambda ) that violates the condition , and hence the theorem holds . We will revise the paper so that it is clear that we are proving the equivalent statement at the beginning of the proof . 3.Regarding the complexity of the recovering bandits : It seems the issue is that the reviewer misunderstands the d-lookahead algorithm since it is not clearly described in our paper . The variable d is NOT `` the number of times the arms are sampled in a round '' , as the reviewer claims . It is the number of steps that the d-lookahead algorithm enumerates . Each step consists of choosing M arms out of N arms , with a total of ( N \\choose M ) possible choices . Therefore , in d steps , there are a total of ( N \\choose M ) ^d different choices . In the recovering bandit paper , it considers the case with N = K and M = 1 . In the beginning of Section 6 of the recovering bandit paper , it indeed states that `` Algorithm 1 ... searches K^d leaves '' . Note that K^d = ( K \\choose 1 ) ^d . This is consistent with our complexity analysis . We would also like to point out that the trick used in the recovering bandit paper to improve computational efficiency is not sufficient for our case . In Section 7 , the recovering bandit paper states that the trick still requires to evaluate ~0.1 % of all possible choices . For the case N = 100 , M = 25 , d = 2 , there are more than 5 * 10^46 possible choices . Even evaluating only 0.1 % of them is intractable . Thus , we are only able to present the result for d = 1 . Please note that , when d =1 , the d-lookahead algorithm still activates M arms in each step . Hence , the comparison is fair . Seeing the confusion arisen , we will update the paper to include a short description of the d-lookahead algorithm . 4.Regarding offline training : We would like to stress that this paper studies * restless * multi-armed bandit problem . For restless bandits , the optimal control policy is difficult to find even when the environment is known . Indeed , there are many problems where the environments are well-defined , but the optimal control is not known , such as the wireless scheduling problem described in Section 5.3 . We believe that our work makes an important contribution to these problems . For example , we show in Section 5.3 that NeurWIN achieves better reward than the best-known policy . For problems where the environments are not known a priori , we believe that our work nicely compliments existing studies that aim to learn the environments but fail to find the optimal control policy . For example , the recovering bandit paper only uses the heuristic d-lookahead after learning the environment . Our work would compliment the recovering bandit paper by finding the near-optimal control policy after it learns the environment . We will add some discussions to highlight the usefulness of this work ."}, "1": {"review_id": "QpT9Q_NNfQL-1", "review_text": "The paper `` NeurWIN : Neural Whittle Index Network for Restless Bandits via Deep RL '' proposes a new RL approach for estimating the Whittle index in restless bandit problems , which allows to define effective strategies for selecting arms to activate at each round of the bandit process . I found the paper very interesting . Maybe this was because I was not familiar with the Whittle index and such kind of policies for restless bandits . But the paper is very didactic , I liked to discover these concepts . More importantly , I found the proposed approach very well presented , with relevant theoretical justification provided . The approach is elegant and looks useful from the results . I still have some concerns however . First , one limitation of the approach is that it requires to have access to a simulator in the training phase , that strictly follows the dynamics of the real world it is designed for . It would have been nice to consider the impact of discrepancies between simulator and real-world dynamics for the setting considered . What happens if dynamics are not stationary ? Second , it would have be very useful to consider more baselines in the experiments . Authors argue that they only could applied a classical Reinforce strategy on the ( 4,1 ) setting , since the combinatorial aspect of the action and state space is problematic beyond that point . They also claim that if this approach is not effective even in that simple setting , it is mostly due to the combinatorial dimension of the state space . Ok , but other instances of this could have been considered . At least , it would have been important to consider a policy trained with Reinforce on individual states , and then select the M best outputs at each round , as it is done for the proposed approach ! Also , multi-agent RL approaches , such as MADDPG or Q-Mix , could have been considered to cope with the combinatorial aspect of the problem , and assess if cooperation would help improve the results . At last , authors give conditions for the applicability of their method . It would have been useful to help the reader with some intuitions about what imply these indexability and strong indexability conditions . Are they very constraining ? What kind of restless bandit settings are excluded due to these two necessary conditions ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for the helpful and detailed review . Below , we answer the concerns raised . 1 . `` First , one limitation of the approach is that it requires to have access to a simulator in the training phase ... '' We thank the reviewer for raising this issue . To address the concern , we conduct additional experiments where the simulator is a noisy one . Specifically , the simulator has an average 5 % error is its reward estimation . Due to the time constraint , we were only able to complete the experiments for the recovering bandits and the deadline scheduling . Our results show that NeurWIN still performs well despite that the simulators are noisy . 2 . `` Second , it would have be very useful to consider more baselines in the experiments ... '' We thank the reviewer for the suggestion . We implemented and tested ( Fu et al.2019 ) , a Q-learning Whittle index algorithm , in our experiments . Results show that NeurWIN achieves superior performance . 3 . `` At last , authors give conditions for the applicability of their method . It would have been useful to help the reader with some intuitions about what imply these indexability and strong indexability conditions ... '' Both the indexability condition and the strong indexability condition basically states the following : `` The net reward of activating an arm in a given state ( strictly ) decreases as the activation cost increases , '' which seems to be intuitively true for most practical problems . Indeed , many practical problems have been proven to be indexable in the literature ( see below for an incomplete list ) . However , proving whether a problem is indexable or not is usually done on a case-by-case basis . As such , we are unable to provide a simple answer to whether a problem is strongly indexable . Borkar , Vivek S. , and Sarath Pattathil . `` Whittle indexability in egalitarian processor sharing systems . '' Annals of Operations Research , 2017. https : //arxiv.org/pdf/1707.02440.pdf Mate , Aditya , Jackson Killian , Haifeng Xu , Andrew Perrault , and Milind Tambe . `` Collapsing Bandits and Their Application to Public Health Intervention . '' Advances in Neural Information Processing Systems ( NeurIPS ) , 2020. https : //proceedings.neurips.cc/paper/2020/file/b460cf6b09878b00a3e1ad4c72344ccd-Paper.pdf Liu , Keqin , and Qing Zhao . `` Indexability of restless bandit problems and optimality of whittle index for dynamic multichannel access . '' IEEE Transactions on Information Theory , 2010. https : //arxiv.org/abs/0810.4658 Hsu , Yu-Pin , Eytan Modiano , and Lingjie Duan . `` Scheduling algorithms for minimizing age of information in wireless broadcast networks with random arrivals . '' IEEE Transactions on Mobile Computing , 2019. https : //arxiv.org/abs/1712.07419 Xu , Yizhen , Peng Cheng , Zhuo Chen , Ming Ding , Yonghui Li , and Branka Vucetic . `` Real-Time Task Offloading for Large-Scale Mobile Edge Computing . '' IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , 2020. https : //ieeexplore.ieee.org/abstract/document/9054413 Anand , Arjun , and Gustavo de Veciana . `` A Whittle 's index based approach for QoE optimization in wireless networks . '' Proceedings of the ACM on Measurement and Analysis of Computing Systems , 2018. http : //users.ece.utexas.edu/~gustavo/papers/AnD18.pdf"}, "2": {"review_id": "QpT9Q_NNfQL-2", "review_text": "The paper proves a simple but important result which essentially states the following : It is possible to construct an RL environment for a one arm restless bandit system , such that any neural network which achieves a certain discounted net reward has to approximate the whittle index of that arm well . The paper then uses this observation to form exactly such an environment and train a neural-network using REINFORCE to maximize discounted reward for the environment , thus in principle achieving a good approximation of the Whittle index of the arm . The network can then be used to approximate a Whittle index based policy for restless bandit problems . I think the idea is interesting and it seems to work well in practice on three simulated but well-studied environments . In particular in the deadline scheduling problem where the optimal strategy is known , it can be observed that this policy can match the performance of the optimal policy after several hundred training epochs . I recommend the paper for acceptance . I would encourage the authors to provide confidence bars for their algorithm as the results are presented over 10 independent runs only . It is vital to see the variance in rewards obtained by the strategy .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for time taken in assessing our work . We also thank the reviewer for the great advice on confidence interval . We rerun our experiments for 200 independent runs and provide confidence intervals in the figures for wireless scheduling and deadline scheduling . For recovering bandit , the problem itself is not stochastic , and hence there is only one experiment run ."}, "3": {"review_id": "QpT9Q_NNfQL-3", "review_text": "This paper considers the problem of learning how to control restless bandits . When all parameters of the system are known , Whittle index policy usually offers a good performance . The main contribution of the paper is to propose an algorithm , NeurWIN , that uses a neural network architecture to learn the Whittle indices . Most of the paper is devoted to the description and the derivation of the algorithm . At the end of the paper , the authors present four illustrations of how the algorithm works . The algorithm is compared to an index-based policy and an ( old ? ) RL algorithm REINFORCE for the small systems . The learning behavior of NeurWIN is very good in all tested cases . The paper does not contain theoretical result . It is mostly about presenting an algorithm and performing numerical experiments to demonstrate that it works . Yet , there is no comparison with ( reasonable ) related work . In the numerical part , the only algorithm to which the authors compare their algorithm is REINFORCE algororithm of Williams 92 , that is clearly non-adapted to learning whittle index ( because of the large state-space ) . As acknowledged by the authors , there are a number of recent work on learning Whittle index ( last paragraph of Section 2 ) : I do not understand why these solutions are not implemented . Also , I am not convinced that the algorithm of ( Avrachenkov , Borkar,2019 ) is not applicable to the current context . The paper is a bit sloppy and not very precise . For instance : - The authors write several times `` Finding Whittle index is typically intractable '' without proof or precise reference . I doubt that this statement is true . - page 4 , top : the definition of strong indexability needs some clarifications : how strong is the assumption ? Do the presented example satisfy this assumption ? And if not , is it important ? - page 4 : is it really the problem statement ? Then why are the simulation results not showing this ? - page 7 ( end ) : `` The size of the state-space is 10^ { 12 } : given the appendix , this seems largely exaggerated ( the order seems closer to 10^6 if one discretize byte per byte ) . Moreover , discretizing at the kB level would lead to about 10^3 states in which case Whittle index would be applicable . - page 8 ( beginning ) : `` REINFORCE [ ... ] improves a litle '' - > this seems wrong . - page 8 , Figure 4 ( a ) : how can NeurWIN perform better than Whittle index ? I thought that NeurWIN was learning those indices . The paper would be much stronger , if it would contain either some theoretical guarantee , or a more thourough comparison of performance with related work .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank reviewer # 3 for the constructive and detailed feedback . We discuss each point the reviewer gave below . 1.Regarding the comparison with recent work : We thank the reviewer for raising this issue . We will implement ( Fu et al.2019 ) , a Q-learning Whittle index algorithm , and compare its performance with NeurWIN . We would like to point out that ( Fu et al.2019 ) is a tabular method that may not scale well . Further , the paper itself shows that its algorithm does not converge to the Whittle index . Still , as it is the closest recent work to our NeurWIN , we will implement and test it . For ( Avrachenkov , Borkar 2019 ) , the proposed algorithm is only applicable when the state of an arm is a scalar . In this paper , we consider that the state of an arm can be a vector . Indeed , the applications in Section 5.3 and 5.4 both have vector states . Hence , ( Avrachenkov , Borkar 2019 ) is not applicable . 2.Regarding the hardness of finding the Whittle index : To the best of our knowledge , there is no tractable solutions for finding Whittle indices . The standard approach , as described in Section III.B in ( Yu et al. , 2018 ) , finds Whittle indices by solving uncountably infinitely many linear programming problems , one for each activation cost \\lambda in ( -\\infty , +\\infty ) . This approach is clearly not tractable . While we are not aware of any hardness result of finding the Whittle index , we would like the reviewer to consider the following two points : A . Most existing work on Whittle index is only able to find the Whittle index for some special cases . B.In ( Aalto et al. , 2015 ) , which we discussed in Section 5.3 , the authors were only able to find the Whittle index under some relaxations . If finding Whittle indices were tractable , researchers would n't have resorted to studying special cases and/or using relaxations . 3 . `` page 4 , top : the definition of strong indexability needs some clarifications : how strong is the assumption ? Do the presented example satisfy this assumption ? And if not , is it important ? '' We need the strong indexability condition to prove theorem 2 . However , NeurWIN can still be applied if this condition is not met . We would also like to point out that the strong indexability condition basically states the following : `` The net reward of activating an arm in a given state strictly decreases as the activation cost increases , '' which seems to be intuitively true for most practical problems . In the literature , proving whether a problem is indexable or not is usually done on a case-by-case basis . Proving whether a problem is strongly indexable is beyond the scope of this work . 4 . `` page 4 : is it really the problem statement ? Then why are the simulation results not showing this ? '' The problem statement is to find neural networks that are \\gamma-Whittle-accurate . In the first two applications that we evaluate ( Sections 5.2 and 5.3 ) , there are no known ground truths about the Whittle indices . Hence , we are unable to evaluate whether NeurWIN is Whittle-accurate . We can only evaluate NeurWIN by its resulting rewards . As the rewards are higher than state-of-the-art policies , it appears that NeurWIN must produce accurate predictions of Whittle indices . For the application in Section 5.4 , we do have the ground truths and we will include the comparison with ground truths in the paper . What we observe is that NeurWIN is accurate when T is small , but is less accurate when T is large . This is reasonable . In Section 5.4 , T is the time to deadline . When the time to deadline is large , the actions have little impact on the resulting performance . However , when T is small , making the wrong action can result in big penalty . The fact that NeurWIN is more accurate when T is small suggests that NeurWIN focuses more on learning the Whittle indices for states with higher importance . 5 . `` page 7 ( end ) : `` The size of the state-space is 10^ { 12 } : given the appendix , this seems largely exaggerated ... '' We thank the reviewer for spotting the typo . The size of the state-space is 2 * 10^6 . The size of the file ranges from 1 Byte to 10^6 Bytes . The channel can be either good or bad . We will correct the typo . 6 . `` page 8 ( beginning ) : `` REINFORCE [ ... ] improves a litle '' - > this seems wrong . '' We thank the reviewer for spotting the typo . We meant to say `` REINFORCE ... improves little , '' meaning that it does not improve . We will correct the typo . 7 . `` page 8 , Figure 4 ( a ) : how can NeurWIN perform better than Whittle index ? `` As mentioned in page 1 end of paragraph 2 , Whittle index policy is asymptotically optimal in many settings . Here , `` asymptotically optimal '' means that it converges to the optimal policy when the number of arms becomes large . For a small number of arms ( e.g.N = 4 ) , there may exist a family of control policies that perform better than the Whittle index-based policy . For NeurWIN with a small set of arms N , we observed that the learned index representations converged to one such control hypothesis ."}}