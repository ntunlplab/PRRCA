{"year": "2018", "forum": "ryserbZR-", "title": "Classification and Disease Localization in Histopathology Using Only Global Labels: A Weakly-Supervised Approach", "decision": "Reject", "meta_review": "Authors present a method for disease classification and localization in histopathology images. Standard image processing techniques are used to extract and normalize tiles of tissue, after which features are extracted from pertained networks. A 1-D convolutional filter is applied to the bag of features from the tiles (along the tile dimension, kernel filter size equal to dimensionality of feature vector). The max R and min R values are kept as input into a neural network for classification, and thresholding of these values provides localization for disease / non-disease.\n\nPro:\n - Potential to reduce annotation complexity of datasets while producing predictions and localization\n\nCon:\n- Results are not great. If anything, results re-affirm why strong annotations are necessary.\n- Several reviewer concerns regarding novelty of proposed method. While authors have made clear the distinctions from prior art, the significance of those changes are debated.\n\nGiven the current pros/cons, the committee feels the paper is not ready for acceptance in its current form.", "reviews": [{"review_id": "ryserbZR--0", "review_text": "This paper describes a semi-supervised method to classify and segment WSI histological images that are only labeled at the whole image level. Images are tiled and tiles are sampled and encoded into a feature vector via a ResNET-50 pretrained on ImageNET. A 1D convolutional layer followed by a min-max layer and 2 fully connected layer compose the network. The conv layer produces a single value per tile. The min-max layer selects the R min and max values, which then enter the FC layers. A multi-instance (MIL) approach is used to train the model by backpropagating only instances that generate min and max values at the min-max layer. Experiments are run on 2 public datasets achieving potentially top performance. Potentially, because all other methods supposedly make use of segmentation labels of tumor, while this method only uses the whole image label. Previous publications have used MIL training on tiles with only top-level labels [1,2] and this is essentially an incremental improvement on the MIL approach by using several instances (both min-negative and max-positive) instead of a single instance for backprop, as described in [3]. So, the main contribution here, is to adapt min-max MIL to the histology domain. Although the result are good and the method interesting, I think that the technical contribution is a bit thin for a ML conference and this paper may be a better fit for a medical imaging conference. The paper is well written and easy to understand. [1] Hou, L., Samaras, D., Kurc, T. M., Gao, Y., Davis, J. E., & Saltz, J. H. (2016). Patch-based convolutional neural network for whole slide tissue image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2424-2433). [2] Cosatto, E., Laquerre, P. F., Malon, C., Graf, H. P., Saito, A., Kiyuna, T., ... (2013). Automated gastric cancer diagnosis on H&E-stained sections; training a classifier on a large scale with multiple instance machine learning. Medical Imaging, 2. 2013. [3] Durand, T., Thome, N., & Cord, M. (2016). Weldon: Weakly supervised learning of deep convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4743-4752).", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the referee for their time and effort in assessing our work . We hope that the discussion we provide in our general comments provides further justification for our work 's presence at ICLR . Specifically , we note the significance of our architectural contributions , as well as our modifications to the training regime . We also detail how our system provides human-pathologist-level performance without being guided by detailed expert instruction on what structures lead to disease diagnoses . We believe that this significant advance in machine learning as applied to medical imaging , and to this gold standard oncology diagnostic in particular , will be of great interest to the general ICLR audience . > Previous publications have used MIL training on tiles with only top-level labels [ 1,2 ] > and this is essentially an incremental improvement on the MIL approach by using > several instances ( both min-negative and max-positive ) instead of a single instance > for backprop , as described in [ 3 ] . So , the main contribution here , is to adapt min-max > MIL to the histology domain . Although the result are good and the method interesting , > I think that the technical contribution is a bit thin for a ML conference and this paper > may be a better fit for a medical imaging conference . We thank the referee for their positive view of our method and results . We agree that the work we present is , as Reviewer1 noted , a `` down-to-earth practical application , '' however we do make novel architectural , process , and implementation contributions . While we do not provide a theory of MIL in the context of HIA , we note that many successful advances in our field have been made from an empirical , rather than theoretical , perspective . While there is no newly proposed loss , neuron non-linearity , or adaptive momentum scheme in our work , we do demonstrate the steps necessary to provide state-of-the-art performance for diagnosis prediction and disease localization without expert assistance beyond diagnosis labels . While these results would indeed be incredibly pertinent at a more medically focused venue , it is our strong belief that the audience of ICLR would greatly benefit both from our demonstration , as well as their introduction to a budding application area in great need of their technical expertise . > The paper is well written and easy to understand . We thank the referee for their comments and positive feedback on our presentation of our work ."}, {"review_id": "ryserbZR--1", "review_text": "This paper proposes a deep learning (DL) approach (pre-trained CNNs) to the analysis of histopathological images for disease localization. It correctly identifies the problem that DL usually requires large image databases to provide competitive results, while annotated histopathological data repositories are costly to produce and not on that size scale. It also correctly identifies that this is a daunting task for human medical experts and therefore one that could surely benefit from the use of automated methods like the ones proposed. The study seems sound from a technical viewpoint to me and its contribution is incremental, as it builds on existing research, which is correctly identified. Results are not always too impressive, but authors seem intent on making them useful for pathogists in practice (an intention that is always worth the effort). I think the paper would benefit from a more explicit statement of its original contributions (against contextual published research) Minor issues: Revise typos (e.g. title of section 2) Please revise list of references (right now a mess in terms of format, typos, incompleteness", "rating": "6: Marginally above acceptance threshold", "reply_text": "> The study seems sound from a technical viewpoint to me and its contribution is incremental , > as it builds on existing research , which is correctly identified . > Results are not always too impressive , but authors seem intent on making them useful for > pathogists in practice ( an intention that is always worth the effort ) . > I think the paper would benefit from a more explicit statement of its original contributions > ( against contextual published research ) We thank the referee for their comments and their effort in assessing our work . With respect to our specific contributions , we have added further clarifications to the text ( as noted in the paper modifications ) to identify our contribution with respect to prior art . Additionally , with respect to the significance of the results we present , we note that the performance reported in Table 1 represents the state-of-the-art for HIA classification using only WSI-wide labels . For further justifications on the significance of our work , we refer to our general comments on this subject . > Minor issues : > Revise typos ( e.g.title of section 2 ) Thank you for pointing out this ( rather embarrassing ) typo ! We have corrected this mistake along with others throughout the text . > Please revise list of references ( right now a mess in terms of format , typos , incompleteness As noted in the general comments , we have revised the references to fit a common standard and have attempted to include all relevant citation details.We thank you for your attentiveness ."}, {"review_id": "ryserbZR--2", "review_text": "The authors approach the task of labeling histology images with just a single global label, with promising results on two different data sets. This is of high relevance given the difficulty in obtaining expert annotated data. At the same time the key elements of the presented approach remain identical to those in a previous study, the main novelty is to replace the final step of the previous architecture (that averages across a vector) with a multiplayer perceptron. As such I feel that this would be interesting to present if there is interest in the overall application (and results of the 2016 CVPR paper), but not necessarily as a novel contribution to MIL and histology image classification. Comments to the authors: * The intro starts from a very high clinical level. A introduction that points out specifics of the technical aspects of this application, the remaining technical challenges, and the contribution of this work might be appreciated by some of your readers. * There is preprocessing that includes feature extraction, and part of the algorithm that includes the same feature extraction. This is somewhat confusing to me and maybe you want to review the structure of the sections. You are telling us you are using the first layer (P=1) of the ResNet50 in the method description, and you mention that you are using the pre-final layer in the preprocessing section. I assume you are using the latter, or is P=1 identical to the prefinal layer in your notation? Tell us. Moreover, not having read Durand 2016, I would appreciate a few more technical details or formal description here and there. Can you detail about the ranking method in Durand 2016, for example? * Would it make sense to discuss Durand 2016 in the base line methods section? * To some degree this paper evaluates WELDON (Durand 2016) on new data, and compares it against and an extended WELDON algorithm called CHOWDER that features the final MLP step. Results in table 1 suggest that this leads to some 2-5% performance increase which is a nice result. I would assume that experimental conditions (training data, preprocessing, optimization, size of ensemble) are kept constant in between those two comparisons? Or is there anything of relevance that also changed (like size of the ensemble, size of training data) because the WELDON results are essentially previously generated results? Please comment in case there are differences. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "> * Would it make sense to discuss Durand 2016 in the base line methods section ? Considering the adaptations we make to the approach of Durand et al . ( 2016 ) , we felt it more appropriate to cite their work within Sec.2.3 in order to show the line of development . In Sec.2.3 , we cite Durand et al . ( 2016 ) extensively , pointing out the notable contributions of this work , and how the originally proposed approach must be adapted in order to provide an effective architecture for the setting of WSI classification without local annotations . In Sec.2.2 , when we introduce baseline techniques , we truly mean baseline . Aggregation via feature pooling is one of the most direct ways one can attempt to approach the task of WSI classification sans annotations . Indeed , this approach is very attractive , as compared to MIL approaches , when tackling large-scale datasets from a purely computational standpoint . For this reason , we denote these approaches as our `` baseline , '' whereby we demonstrate that either technique ( WELDON or CHOWDER ) can provide improvements in both detection and localization which are significant enough , as compared to feature pooling , to justify their complexity . By not including Durand et al . ( 2016 ) within Sec 2.2 , we do not imply that we should not compare ( we do ) , rather we simply make a semantic distinction between feature pooling and MIL . > * To some degree this paper evaluates WELDON ( Durand 2016 ) on new data , and > compares it against and an extended WELDON algorithm called CHOWDER that features > the final MLP step . Results in table 1 suggest that this leads to some 2-5 % performance > increase which is a nice result . I would assume that experimental conditions ( training data , > preprocessing , optimization , size of ensemble ) are kept constant in between those two > comparisons ? Or is there anything of relevance that also changed ( like size of the ensemble , > size of training data ) We do , in fact , evaluate the WELDON architecture of Durand et al.on new data , as reported in Table 1 . We also compare WELDON against our proposed modifications . In all cases , experimental settings remain consistent between all tested methods . In the case of WELDON and CHOWDER , we have ensured that the ensemble size remains consistent between the two ( $ E = 10 $ as described in Sec.3.1 ) .For both WELDON and CHOWDER , we use best-case hyper-parameter settings . Additionally , the improvement in AUC demonstrated by the CHOWDER architecture is more significant than the referee reports . In Table 1 , we report a percent change in AUC of 12.15 % and 8.53 % over the WELDON architecture for the competition and cross-validation splits , respectively . This corresponds to a 12.59 % and 23.66 % percent change in AUC as compared to the best-performing baseline methods for the same splits . In the case of TCGA-Lung , we demonstrate a 1.32 % percent change in AUC , but we point this out specifically in the text . This dataset is well suited to feature pooling due to the balanced instance classes present in the TCGA-Lung dataset . The diseased regions in these slides are much more diffuse over the entire tissue sample , as opposed to the highly-localized metastases present in Camelyon-16 . Therefore , the excellent performance of the baseline feature-pooling methods is expected for this dataset , as the disease signal is not lost in the pooled representation . > because the WELDON results are essentially previously generated results ? > Please comment in case there are differences . It is not clear to us what is meant by previously generated results . In the case of WELDON , in Durand et al . ( 2016 ) , the method was proposed only for object region detection in natural images . To the best of our knowledge , there has been no other application of a WELDON-inspired architecture to HIA , or to the TCGA-Lung and Camelyon-16 datasets in particular ."}], "0": {"review_id": "ryserbZR--0", "review_text": "This paper describes a semi-supervised method to classify and segment WSI histological images that are only labeled at the whole image level. Images are tiled and tiles are sampled and encoded into a feature vector via a ResNET-50 pretrained on ImageNET. A 1D convolutional layer followed by a min-max layer and 2 fully connected layer compose the network. The conv layer produces a single value per tile. The min-max layer selects the R min and max values, which then enter the FC layers. A multi-instance (MIL) approach is used to train the model by backpropagating only instances that generate min and max values at the min-max layer. Experiments are run on 2 public datasets achieving potentially top performance. Potentially, because all other methods supposedly make use of segmentation labels of tumor, while this method only uses the whole image label. Previous publications have used MIL training on tiles with only top-level labels [1,2] and this is essentially an incremental improvement on the MIL approach by using several instances (both min-negative and max-positive) instead of a single instance for backprop, as described in [3]. So, the main contribution here, is to adapt min-max MIL to the histology domain. Although the result are good and the method interesting, I think that the technical contribution is a bit thin for a ML conference and this paper may be a better fit for a medical imaging conference. The paper is well written and easy to understand. [1] Hou, L., Samaras, D., Kurc, T. M., Gao, Y., Davis, J. E., & Saltz, J. H. (2016). Patch-based convolutional neural network for whole slide tissue image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2424-2433). [2] Cosatto, E., Laquerre, P. F., Malon, C., Graf, H. P., Saito, A., Kiyuna, T., ... (2013). Automated gastric cancer diagnosis on H&E-stained sections; training a classifier on a large scale with multiple instance machine learning. Medical Imaging, 2. 2013. [3] Durand, T., Thome, N., & Cord, M. (2016). Weldon: Weakly supervised learning of deep convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4743-4752).", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the referee for their time and effort in assessing our work . We hope that the discussion we provide in our general comments provides further justification for our work 's presence at ICLR . Specifically , we note the significance of our architectural contributions , as well as our modifications to the training regime . We also detail how our system provides human-pathologist-level performance without being guided by detailed expert instruction on what structures lead to disease diagnoses . We believe that this significant advance in machine learning as applied to medical imaging , and to this gold standard oncology diagnostic in particular , will be of great interest to the general ICLR audience . > Previous publications have used MIL training on tiles with only top-level labels [ 1,2 ] > and this is essentially an incremental improvement on the MIL approach by using > several instances ( both min-negative and max-positive ) instead of a single instance > for backprop , as described in [ 3 ] . So , the main contribution here , is to adapt min-max > MIL to the histology domain . Although the result are good and the method interesting , > I think that the technical contribution is a bit thin for a ML conference and this paper > may be a better fit for a medical imaging conference . We thank the referee for their positive view of our method and results . We agree that the work we present is , as Reviewer1 noted , a `` down-to-earth practical application , '' however we do make novel architectural , process , and implementation contributions . While we do not provide a theory of MIL in the context of HIA , we note that many successful advances in our field have been made from an empirical , rather than theoretical , perspective . While there is no newly proposed loss , neuron non-linearity , or adaptive momentum scheme in our work , we do demonstrate the steps necessary to provide state-of-the-art performance for diagnosis prediction and disease localization without expert assistance beyond diagnosis labels . While these results would indeed be incredibly pertinent at a more medically focused venue , it is our strong belief that the audience of ICLR would greatly benefit both from our demonstration , as well as their introduction to a budding application area in great need of their technical expertise . > The paper is well written and easy to understand . We thank the referee for their comments and positive feedback on our presentation of our work ."}, "1": {"review_id": "ryserbZR--1", "review_text": "This paper proposes a deep learning (DL) approach (pre-trained CNNs) to the analysis of histopathological images for disease localization. It correctly identifies the problem that DL usually requires large image databases to provide competitive results, while annotated histopathological data repositories are costly to produce and not on that size scale. It also correctly identifies that this is a daunting task for human medical experts and therefore one that could surely benefit from the use of automated methods like the ones proposed. The study seems sound from a technical viewpoint to me and its contribution is incremental, as it builds on existing research, which is correctly identified. Results are not always too impressive, but authors seem intent on making them useful for pathogists in practice (an intention that is always worth the effort). I think the paper would benefit from a more explicit statement of its original contributions (against contextual published research) Minor issues: Revise typos (e.g. title of section 2) Please revise list of references (right now a mess in terms of format, typos, incompleteness", "rating": "6: Marginally above acceptance threshold", "reply_text": "> The study seems sound from a technical viewpoint to me and its contribution is incremental , > as it builds on existing research , which is correctly identified . > Results are not always too impressive , but authors seem intent on making them useful for > pathogists in practice ( an intention that is always worth the effort ) . > I think the paper would benefit from a more explicit statement of its original contributions > ( against contextual published research ) We thank the referee for their comments and their effort in assessing our work . With respect to our specific contributions , we have added further clarifications to the text ( as noted in the paper modifications ) to identify our contribution with respect to prior art . Additionally , with respect to the significance of the results we present , we note that the performance reported in Table 1 represents the state-of-the-art for HIA classification using only WSI-wide labels . For further justifications on the significance of our work , we refer to our general comments on this subject . > Minor issues : > Revise typos ( e.g.title of section 2 ) Thank you for pointing out this ( rather embarrassing ) typo ! We have corrected this mistake along with others throughout the text . > Please revise list of references ( right now a mess in terms of format , typos , incompleteness As noted in the general comments , we have revised the references to fit a common standard and have attempted to include all relevant citation details.We thank you for your attentiveness ."}, "2": {"review_id": "ryserbZR--2", "review_text": "The authors approach the task of labeling histology images with just a single global label, with promising results on two different data sets. This is of high relevance given the difficulty in obtaining expert annotated data. At the same time the key elements of the presented approach remain identical to those in a previous study, the main novelty is to replace the final step of the previous architecture (that averages across a vector) with a multiplayer perceptron. As such I feel that this would be interesting to present if there is interest in the overall application (and results of the 2016 CVPR paper), but not necessarily as a novel contribution to MIL and histology image classification. Comments to the authors: * The intro starts from a very high clinical level. A introduction that points out specifics of the technical aspects of this application, the remaining technical challenges, and the contribution of this work might be appreciated by some of your readers. * There is preprocessing that includes feature extraction, and part of the algorithm that includes the same feature extraction. This is somewhat confusing to me and maybe you want to review the structure of the sections. You are telling us you are using the first layer (P=1) of the ResNet50 in the method description, and you mention that you are using the pre-final layer in the preprocessing section. I assume you are using the latter, or is P=1 identical to the prefinal layer in your notation? Tell us. Moreover, not having read Durand 2016, I would appreciate a few more technical details or formal description here and there. Can you detail about the ranking method in Durand 2016, for example? * Would it make sense to discuss Durand 2016 in the base line methods section? * To some degree this paper evaluates WELDON (Durand 2016) on new data, and compares it against and an extended WELDON algorithm called CHOWDER that features the final MLP step. Results in table 1 suggest that this leads to some 2-5% performance increase which is a nice result. I would assume that experimental conditions (training data, preprocessing, optimization, size of ensemble) are kept constant in between those two comparisons? Or is there anything of relevance that also changed (like size of the ensemble, size of training data) because the WELDON results are essentially previously generated results? Please comment in case there are differences. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "> * Would it make sense to discuss Durand 2016 in the base line methods section ? Considering the adaptations we make to the approach of Durand et al . ( 2016 ) , we felt it more appropriate to cite their work within Sec.2.3 in order to show the line of development . In Sec.2.3 , we cite Durand et al . ( 2016 ) extensively , pointing out the notable contributions of this work , and how the originally proposed approach must be adapted in order to provide an effective architecture for the setting of WSI classification without local annotations . In Sec.2.2 , when we introduce baseline techniques , we truly mean baseline . Aggregation via feature pooling is one of the most direct ways one can attempt to approach the task of WSI classification sans annotations . Indeed , this approach is very attractive , as compared to MIL approaches , when tackling large-scale datasets from a purely computational standpoint . For this reason , we denote these approaches as our `` baseline , '' whereby we demonstrate that either technique ( WELDON or CHOWDER ) can provide improvements in both detection and localization which are significant enough , as compared to feature pooling , to justify their complexity . By not including Durand et al . ( 2016 ) within Sec 2.2 , we do not imply that we should not compare ( we do ) , rather we simply make a semantic distinction between feature pooling and MIL . > * To some degree this paper evaluates WELDON ( Durand 2016 ) on new data , and > compares it against and an extended WELDON algorithm called CHOWDER that features > the final MLP step . Results in table 1 suggest that this leads to some 2-5 % performance > increase which is a nice result . I would assume that experimental conditions ( training data , > preprocessing , optimization , size of ensemble ) are kept constant in between those two > comparisons ? Or is there anything of relevance that also changed ( like size of the ensemble , > size of training data ) We do , in fact , evaluate the WELDON architecture of Durand et al.on new data , as reported in Table 1 . We also compare WELDON against our proposed modifications . In all cases , experimental settings remain consistent between all tested methods . In the case of WELDON and CHOWDER , we have ensured that the ensemble size remains consistent between the two ( $ E = 10 $ as described in Sec.3.1 ) .For both WELDON and CHOWDER , we use best-case hyper-parameter settings . Additionally , the improvement in AUC demonstrated by the CHOWDER architecture is more significant than the referee reports . In Table 1 , we report a percent change in AUC of 12.15 % and 8.53 % over the WELDON architecture for the competition and cross-validation splits , respectively . This corresponds to a 12.59 % and 23.66 % percent change in AUC as compared to the best-performing baseline methods for the same splits . In the case of TCGA-Lung , we demonstrate a 1.32 % percent change in AUC , but we point this out specifically in the text . This dataset is well suited to feature pooling due to the balanced instance classes present in the TCGA-Lung dataset . The diseased regions in these slides are much more diffuse over the entire tissue sample , as opposed to the highly-localized metastases present in Camelyon-16 . Therefore , the excellent performance of the baseline feature-pooling methods is expected for this dataset , as the disease signal is not lost in the pooled representation . > because the WELDON results are essentially previously generated results ? > Please comment in case there are differences . It is not clear to us what is meant by previously generated results . In the case of WELDON , in Durand et al . ( 2016 ) , the method was proposed only for object region detection in natural images . To the best of our knowledge , there has been no other application of a WELDON-inspired architecture to HIA , or to the TCGA-Lung and Camelyon-16 datasets in particular ."}}