{"year": "2019", "forum": "r1lWUoA9FQ", "title": "Are adversarial examples inevitable?", "decision": "Accept (Poster)", "meta_review": "There's precious little work asking existential questions about adversarial examples, and so this work is most welcome. The work connects with deep results in probability to make simple and transparent claims about the inevitability of adversarial examples under some assumptions. The authors have addressed the key criticisms of the authors around clarity.", "reviews": [{"review_id": "r1lWUoA9FQ-0", "review_text": "This paper explores the inevitability of adversarial examples with concentration inequalities. It is motivated by the difficulties of achieving adversarial robustness in literature. It derives isoperimetric inequalities on a cube, and then discuss the adversarial robustness of data distributed inside the cube, with the assumption that the data has bounded density. These inequalities are established on different norms. The authors then discuss limitation of the proposed bounds when analyzing practical data distribution and discussed the influence of dimensionality on adversarial robustness. Novelty of the idea: The idea of using concentration inequalities to explain vulnerability is novel in the field of adversarial examples and is a relevant/meaningful angle on understanding this phenomenon. (Although there are concurrent works also relating concentration inequalities to adversarial robustness, they don't diminish the novelty of this work.) On technical contributions: In summary, this paper applies / adapts previous results in concentration inequalities to develop bounds related to adversarial examples. The bounds in Lemma 3 are on any p>0, this seems to be new to my knowledge, but the technical contribution in the proof is limited. Here are some detailed comments. The authors claim that \"This question is complicated by the fact that simple, geometric isoperimetric inequalities fail to exist for the cube, and the shapes that achieve minimal \\eps-expansion (if they exist) depend on the volume they enclose and the choice of \\eps.\" This statement is at least misleading, if not wrong. It is well known that geometric isoperimetric inequality does exist for cube for the L2 case (see Ledoux, M., 2001. Proposition 2.8.), and the proof procedure the author used is also very similar to the proofs in Ledoux, M., 2001. Theorem 5's proof is confusing, if not wrong. This is my brief recap on the first part of Thm 5, If there exists eps and p such that, for all classifiers on MNIST, a random image has eps-adv with probability at least p, then for all classifiers on b-MNIST, a random image has b*eps-adv with probability at least p. The proof in Appendix E says b-MNIST images can be classified by first downsampling. These downsampled classifiers do not cover \"all classifiers on b-MNIST\", so I don't see how the proof stands. Likewise, the proof of the second part has the similar problem. Therefore, I'm not yet convinced that Thm 5 is correct. Also I suggest the authors use more rigorous language to present Theorem 5, in a similar fashion to previous theorems. Re: Lemma 4, my understanding is that it is from previous literature. The authors should point out exactly where is it from (with section# and theorem#), so that readers and reviewers can more easily check the correctness of it. The authors mention that \"Intuitively, the concentration limit Uc can be interpreted as a measure of image complexity.\" I think this statement is problematic. It is, at best, oversimplifying the the problem. If we assume the data lies in low-dimensional space, the volume of the support will be 0, no matter how complex the shape of the manifold is. This lead to unbounded density in the ambient dimension. Even when considering \"expanded dataset\" like the authors discussed in Section 7, it is not obvious that Uc can be interpreted as image complexity. To make such a claim, more assumptions need to made and more analyses need to be done. Similar comments applies to the \"correlations between pixels\" and concentration. On the significance: As the author themselves have already mentioned, the bounds described in the paper all depends on the bounded density of the data distribution. In practice, the density of data distribution is difficult to understand, if not impossible. Therefore it is still inconclusive whether the \"inevitability\" exists. But to be fair, I believe this is mostly due to the difficulty of the problem being studied. Clarity and writing: The skeleton of the paper is well written and easy to follow. I've pointed out some problems in my previous comments. I also appreciate that the authors made efforts to not overclaim. here are a few more comments: - I personally feel Section 3 as an \"warm-up\" section is redundant, and the authors can consider move them to the appendix. - In Section 6 and 7, the authors talk about when is the bound \"meaningful\" and \"active\". This part is confusing/misleading. eps=sqrt(n) is actually the maximum possible perturbation and not falls into the common \"adversarial perturbation\" where the perturbation does not change the semantic meaning of the image. There should be a least an additional numerical examples on small eps, so the readers have better ideas on the tightness/looseness of the bound. References: Ledoux, M., 2001. The concentration of measure phenomenon (No. 89). American Mathematical Soc.. ========================== I change my rating on this paper to be 6, after the authors' response. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for considering our paper and for giving it a careful read . We agree with the reviewer that this paper does not make any groundbreaking contributions to the field of isoperimetric inequalities . Indeed , that was not our goal . This is a paper on adversarial examples ; we are trying to show how known results from the isoperimetry literature can be adapted to study and explain adversarial behavior in complex classifiers . Furthermore , we made great efforts to give proper credit by citing the mathematicians who developed the isoperimetric inequalities that we rely on , and we dug into the literature to cite the original authors when possible , rather than review articles . We also took efforts to cite the authors who developed proof techniques that we use . We have revised the statement `` This question is complicated by the fact ... '' so that it is not misinterpreted . We want to clarify what we meant by the statement . On the sphere , there are \u201c geometric \u201d isoperimetric inequalities because we know the shapes that produce minimal epsilon-expansions ( i.e. , semi-spheres ) , and so we can directly ( and exactly ) calculate the size of a minimal epsilon expansion . On the cube , there are no known \u201c geometric \u201d results - the shapes that produce minimal expansion are unknown . This is a widely discussed open problem ( see , e.g. , the top of page 11 in the journal article http : //www.ugr.es/~aros/isoper.pdf ) . Fortunately , there are \u201c algebraic \u201d bounds on the size of an expansion that do not rely on the geometry of such sets . It was not our intention to imply that we are the first to study such \u201c algebraic \u201d bounds , or that work has not been done in this area , but rather we were trying to explain what makes isoperimetric results on the cube less intuitive and more challenging than on the sphere . Thanks for pointing out the result by Ledoux . While we were aware of this book on isoperimetric inequalities , we were not aware that it contained a result on the cube . We have updated the paper to make the origin of the result clear ( 2nd paragraph , Section 4 ) . We have decided to continue to include our version of the proof because Ledoux \u2019 s version produces much weaker constants than the fairly tight constants that we produce ( this is not because our proof is superior in any way , but rather because Ledoux chose not to keep tight constants ) . The paper contains an acknowledgement that our proof uses methods that appear in Ledoux 's paper and earlier . We maintain that the proof of Theorem 5 is correct , but after looking back at the layout of the proof we understand the source of the reviewer \u2019 s confusion . Theorem 5 states a bound on the robustness of high-res classifiers , and then states a bound on low-res classifiers . The original proof proved the statements in the opposite order ( it proved the bound on low res classifiers and then high res ) . We ask the reviewer to have a look at the revised version of the proof which has been re-ordered and clarified . The proof of Theorem 5 is quite trivial ( although we think non-obvious ) . We think this theorem is valuable though , given that a number of papers now claim that high-res classifiers are inherently less robust than low-res classifiers . Theorem 5 exhibits a simple class of imaging problems for which this is provably not so . Regarding the attribution of Lemma 4 : While we already included citations to the Milman , McDiarmid , and Talagrand in the original submission , we \u2019 ve updated the paper to include section numbers for these citations . Regarding image \u201c complexity \u201d : We don \u2019 t think that further analysis can lend more strength to the interpretation of \u201c complexity \u201d here because it is just an interpretation and not a mathematical concept . Our goal is just to give some intuition for what kinds of image sets have large/small U . We have made modifications to make clear to the reviewer that our use of the term `` complexity '' is informal and non-rigorous . Also , see our comments about density estimation to the review above , which we think lends some strength to this interpretation . Regarding \u201c meaningful vs active \u201d : we have revised this statement to make clear that the active bound may be quite large , and possibly not of interest ( end of Section 6 ) ."}, {"review_id": "r1lWUoA9FQ-1", "review_text": "This paper uses several lemmas in geometry to prove that adversarial examples are hard to avoid under the assumption that there is no \"don't know\" class and the distribution of each class is not too concentrated. The paper first starts with a simple case where the data points are distributed on a sphere, and then extends the results to the realistic case where data points are inside a cube [0,1]^n. The paper uses epsilon expansion of a set as a mathematical tool, and borrows some important lemmas from geometry to the case of adversarial learning. In the sphere case, the results come from a fact that high dimensional half-spheres can almost cover all points in the sphere after an epsilon expansion, and the results depend on dimension n. For the unit cube case, the authors borrow a result from Talagrand, to show that the epsilon expansion of a set can cover a large portion of the cube as long as the set distribution is not very concentrated. In this case, the results (for l_2 norm) do not depend on dimension n. Experimentally, the authors show that inputs with higher dimension can actually get better robustness, aligning with the provided analysis. The primary reason that current adversarial defense does not work well on CIFAR is due to the fact that dataset is more spread out in high dimensional space. This is a good insight for understanding adversarial examples. The paper is overall well written and easy to follow. The interpretation of each lemma and proposition is clear. Although the paper mostly depend on well-known results in geometry and the ideas used are simple, it does provide good insight on explaining the prevalence of adversarial examples. I recommend to accept this paper. Question: Is there any good method to estimate U_c for a dataset? Although it is intuitive that CIFAR may have a smaller U_c than MNIST, is it possible to numerically estimate this quantity? This is necessary to fully support the conclusions made in experiments. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for taking the time to carefully read our paper and provide feedback . To answer the question : Yes , there are methods for quantifying the density of CIFAR and MNIST , although the accuracy of these methods is disputed . Classical density estimation methods ( like Parzan windows and GMMs ) fail on complex high-dimensional distributions . However , neural-network-based methods can attack this problem by training a GAN on the dataset , and then using a formula that predicts the likelihood of samples produced by the GAN . This formula involves the Jacobian of the generator , and the density of the latent `` z '' that produced the image . This was the approach taken in ( https : //arxiv.org/pdf/1705.08868.pdf ) . The authors of that work use several different methods for training generative models , and find that the estimated densities are * highly * dependent on how the model is trained , although for each specific training method the predicted MNIST densities are much higher than CIFAR densities ( see , e.g. , Fig2 in the referenced arxiv paper ) . This observation is compatible with the claims made in our paper . The issue of accurate density estimation on images is still an active area of research . We have been collaborating with another lab to develop new methods for high-dimensional density estimation with the goal of getting more consistent results than previous methods . We have omitted a discussion of these density estimates to remain anonymous ( our work on density estimation is under review ) , but we will include a citation and a brief discussion in the camera ready . To be transparent about our results , we find that typical CIFAR-10 images have log-densities roughly 40 orders of magnitude smaller than typical big-MNIST images , and these observed differences in density are compatible with the differences in adversarial robustness we observe for MNIST and CIFAR in Section 8 ."}, {"review_id": "r1lWUoA9FQ-2", "review_text": "The paper considers the problem of adversarial examples in (mostly high-dimensional) multi-class classification problems. Although the results are not specific necessarily to very high dimensional data or two images, the paper mostly uses images as a running example, and so will I in the review. Assume that the data all lies in the unit box in R^n ([0, 1]^n). A multiclass classifier with K classes partitions the unit cube into K parts, each part corresponding to a given class. There are distributions \\rho_c associated with each class and there is a bound on their density given by U_c and the fraction of examples of class c is f_c. And (eps, p) adversarial point y for some point x is such that |x - y|_p <= \\eps and the classifier classifies x & y differently. The paper shows that under this modeling assumption adversarial examples are inevitable. The results mostly use standard (but deep) results from probability theory. The technical proofs themselves are not particular difficult (provided one has the right background). I think the overall implications are interesting, and I will recommend the paper be accepted. However, I also feel that this is a missed opportunity. To some extent the authors do try to have some high-level discussion about adversarial examples, but I think this could be expanded on more. For instance, why should it be assumed that an example that is \\eps far should automatically have the same class label? Surely, being \"eps\"-far away is an equivalence relation, thus this would mean that all the hypercube would have to be labeled by the same class. This is clearly not the case. One plausible explanation is that if you take two points that are in two different classes, then any sequence of points that take one to the other with the property that each adjacent pair is at most \\eps far away, must have the property that some intermediate mass have negligible chance of being a \"natural\" image. On the other hand, doesn't the fact that humans are not susceptible to most adversarial examples, imply that adversarial-example resistant classifiers exist? My own feeling is the assumption that U_c is bounded is the strongest assumption that may not hold true with real data. In any case, the paper has enough technical content to merit acceptance and I hope the open review forum will lead to a fruitful discussion about some of these questions. -- Minor comments: Page 6 (just after Thm 2). Isn't the bound in Eqn. (5) true for all \\ell_p norms for p \\geq 2? (not just \\ell_2 as the sentence says) Paras on Page 6 (just below Thm 2). It would be more pleasant if equation x could be replaced by Eq. (x) or Equation (x). Para in Sec 7 on Unbounded density: Clarify what norm you mean when you talk about \\eps/2 perturbations. Thm 5: Seems odd to have a theorem about MNIST. Surely the result is a lot more general!!!", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Regarding the epsilon-walk argument you discuss : Your observation about passing through a region that does not contain \u201c natural \u201d images is correct . One interesting thing about our theoretical framework is that a \u201c class \u201d is a distribution on the cube ( the support of which might cover only a tiny fraction of the cube ) , while the \u201c classifier \u201d is a function that maps all points in the cube ( not just the points that lie in the class distribution support ) onto a label . In our theory , it could be that two classes have distributions with supports that are separated by more than epsilon units . However , it could still be easy to fool the classifier with an epsilon perturbation because the classifier assigns a label to all points , including things that don \u2019 t look natural . There \u2019 s an argument to be made that this is what many classifiers do in real life ; adversarial examples might not lie on the \u201c natural \u201d image manifold because they contain \u201c fuzz \u201d and other artifacts that natural images don \u2019 t , and yet they get assigned a label by the classifier . One way to avoid this problem ( at least in theory ) , which we discuss in the paper , is having a \u201c don \u2019 t know \u201d class . In this case , one could degrade classifier performance by perturbing images into the \u201c don \u2019 t know \u201d class , but it might be difficult for an adversary to change the label to another defined class . We have seen from the adversarial examples literature , though , that producing classifiers that do n't assign strong labels to adversarially perturbed images might be easier said than done . Finally , we \u2019 ll say a few things about the reviewers comments on whether humans are subject to adversarial examples . There seems to be some debate about this . It \u2019 s certainly true that , most of the time , attacks on neural nets don \u2019 t transfer to humans . However , our experience has been that attacks on neural nets usually don \u2019 t transfer to other ( black-box ) neural nets either ( although they sometimes do for certain pairs/ensembles of target/victim networks ) , and so we don \u2019 t think this observation conclusively resolves the issue of whether it \u2019 s possible to make adversarial attacks on humans . To complicate things further , some authors claim to observe cases in which adversarial examples for neural nets do transfer to humans in certain contexts ( https : //arxiv.org/abs/1802.08195 ) . For what it \u2019 s worth , several neuroscientists and psychologist we have spoken to about this issue believe quite strongly that humans are susceptible to adversarial examples , just maybe not ones crafted using a neural net as a model for the human brain . We remain agnostic on this issue because it \u2019 s outside the scope of our expertise . This question seems to lie in the realm of philosophy and psychology , and we \u2019 ve avoided it in our paper in favor of sticking to mathematical issues . Finally , thanks for pointing out a number of minor errors . We have fixed them in the revision . We agree with the reviewer that Eqn ( 5 ) is more clear than Eq 5 , but unfortunately the non-parenthetical version seems to be the standard style chosen by the ICLR editors ( they chose this unusual definition for the \\eqref command ) ."}], "0": {"review_id": "r1lWUoA9FQ-0", "review_text": "This paper explores the inevitability of adversarial examples with concentration inequalities. It is motivated by the difficulties of achieving adversarial robustness in literature. It derives isoperimetric inequalities on a cube, and then discuss the adversarial robustness of data distributed inside the cube, with the assumption that the data has bounded density. These inequalities are established on different norms. The authors then discuss limitation of the proposed bounds when analyzing practical data distribution and discussed the influence of dimensionality on adversarial robustness. Novelty of the idea: The idea of using concentration inequalities to explain vulnerability is novel in the field of adversarial examples and is a relevant/meaningful angle on understanding this phenomenon. (Although there are concurrent works also relating concentration inequalities to adversarial robustness, they don't diminish the novelty of this work.) On technical contributions: In summary, this paper applies / adapts previous results in concentration inequalities to develop bounds related to adversarial examples. The bounds in Lemma 3 are on any p>0, this seems to be new to my knowledge, but the technical contribution in the proof is limited. Here are some detailed comments. The authors claim that \"This question is complicated by the fact that simple, geometric isoperimetric inequalities fail to exist for the cube, and the shapes that achieve minimal \\eps-expansion (if they exist) depend on the volume they enclose and the choice of \\eps.\" This statement is at least misleading, if not wrong. It is well known that geometric isoperimetric inequality does exist for cube for the L2 case (see Ledoux, M., 2001. Proposition 2.8.), and the proof procedure the author used is also very similar to the proofs in Ledoux, M., 2001. Theorem 5's proof is confusing, if not wrong. This is my brief recap on the first part of Thm 5, If there exists eps and p such that, for all classifiers on MNIST, a random image has eps-adv with probability at least p, then for all classifiers on b-MNIST, a random image has b*eps-adv with probability at least p. The proof in Appendix E says b-MNIST images can be classified by first downsampling. These downsampled classifiers do not cover \"all classifiers on b-MNIST\", so I don't see how the proof stands. Likewise, the proof of the second part has the similar problem. Therefore, I'm not yet convinced that Thm 5 is correct. Also I suggest the authors use more rigorous language to present Theorem 5, in a similar fashion to previous theorems. Re: Lemma 4, my understanding is that it is from previous literature. The authors should point out exactly where is it from (with section# and theorem#), so that readers and reviewers can more easily check the correctness of it. The authors mention that \"Intuitively, the concentration limit Uc can be interpreted as a measure of image complexity.\" I think this statement is problematic. It is, at best, oversimplifying the the problem. If we assume the data lies in low-dimensional space, the volume of the support will be 0, no matter how complex the shape of the manifold is. This lead to unbounded density in the ambient dimension. Even when considering \"expanded dataset\" like the authors discussed in Section 7, it is not obvious that Uc can be interpreted as image complexity. To make such a claim, more assumptions need to made and more analyses need to be done. Similar comments applies to the \"correlations between pixels\" and concentration. On the significance: As the author themselves have already mentioned, the bounds described in the paper all depends on the bounded density of the data distribution. In practice, the density of data distribution is difficult to understand, if not impossible. Therefore it is still inconclusive whether the \"inevitability\" exists. But to be fair, I believe this is mostly due to the difficulty of the problem being studied. Clarity and writing: The skeleton of the paper is well written and easy to follow. I've pointed out some problems in my previous comments. I also appreciate that the authors made efforts to not overclaim. here are a few more comments: - I personally feel Section 3 as an \"warm-up\" section is redundant, and the authors can consider move them to the appendix. - In Section 6 and 7, the authors talk about when is the bound \"meaningful\" and \"active\". This part is confusing/misleading. eps=sqrt(n) is actually the maximum possible perturbation and not falls into the common \"adversarial perturbation\" where the perturbation does not change the semantic meaning of the image. There should be a least an additional numerical examples on small eps, so the readers have better ideas on the tightness/looseness of the bound. References: Ledoux, M., 2001. The concentration of measure phenomenon (No. 89). American Mathematical Soc.. ========================== I change my rating on this paper to be 6, after the authors' response. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for considering our paper and for giving it a careful read . We agree with the reviewer that this paper does not make any groundbreaking contributions to the field of isoperimetric inequalities . Indeed , that was not our goal . This is a paper on adversarial examples ; we are trying to show how known results from the isoperimetry literature can be adapted to study and explain adversarial behavior in complex classifiers . Furthermore , we made great efforts to give proper credit by citing the mathematicians who developed the isoperimetric inequalities that we rely on , and we dug into the literature to cite the original authors when possible , rather than review articles . We also took efforts to cite the authors who developed proof techniques that we use . We have revised the statement `` This question is complicated by the fact ... '' so that it is not misinterpreted . We want to clarify what we meant by the statement . On the sphere , there are \u201c geometric \u201d isoperimetric inequalities because we know the shapes that produce minimal epsilon-expansions ( i.e. , semi-spheres ) , and so we can directly ( and exactly ) calculate the size of a minimal epsilon expansion . On the cube , there are no known \u201c geometric \u201d results - the shapes that produce minimal expansion are unknown . This is a widely discussed open problem ( see , e.g. , the top of page 11 in the journal article http : //www.ugr.es/~aros/isoper.pdf ) . Fortunately , there are \u201c algebraic \u201d bounds on the size of an expansion that do not rely on the geometry of such sets . It was not our intention to imply that we are the first to study such \u201c algebraic \u201d bounds , or that work has not been done in this area , but rather we were trying to explain what makes isoperimetric results on the cube less intuitive and more challenging than on the sphere . Thanks for pointing out the result by Ledoux . While we were aware of this book on isoperimetric inequalities , we were not aware that it contained a result on the cube . We have updated the paper to make the origin of the result clear ( 2nd paragraph , Section 4 ) . We have decided to continue to include our version of the proof because Ledoux \u2019 s version produces much weaker constants than the fairly tight constants that we produce ( this is not because our proof is superior in any way , but rather because Ledoux chose not to keep tight constants ) . The paper contains an acknowledgement that our proof uses methods that appear in Ledoux 's paper and earlier . We maintain that the proof of Theorem 5 is correct , but after looking back at the layout of the proof we understand the source of the reviewer \u2019 s confusion . Theorem 5 states a bound on the robustness of high-res classifiers , and then states a bound on low-res classifiers . The original proof proved the statements in the opposite order ( it proved the bound on low res classifiers and then high res ) . We ask the reviewer to have a look at the revised version of the proof which has been re-ordered and clarified . The proof of Theorem 5 is quite trivial ( although we think non-obvious ) . We think this theorem is valuable though , given that a number of papers now claim that high-res classifiers are inherently less robust than low-res classifiers . Theorem 5 exhibits a simple class of imaging problems for which this is provably not so . Regarding the attribution of Lemma 4 : While we already included citations to the Milman , McDiarmid , and Talagrand in the original submission , we \u2019 ve updated the paper to include section numbers for these citations . Regarding image \u201c complexity \u201d : We don \u2019 t think that further analysis can lend more strength to the interpretation of \u201c complexity \u201d here because it is just an interpretation and not a mathematical concept . Our goal is just to give some intuition for what kinds of image sets have large/small U . We have made modifications to make clear to the reviewer that our use of the term `` complexity '' is informal and non-rigorous . Also , see our comments about density estimation to the review above , which we think lends some strength to this interpretation . Regarding \u201c meaningful vs active \u201d : we have revised this statement to make clear that the active bound may be quite large , and possibly not of interest ( end of Section 6 ) ."}, "1": {"review_id": "r1lWUoA9FQ-1", "review_text": "This paper uses several lemmas in geometry to prove that adversarial examples are hard to avoid under the assumption that there is no \"don't know\" class and the distribution of each class is not too concentrated. The paper first starts with a simple case where the data points are distributed on a sphere, and then extends the results to the realistic case where data points are inside a cube [0,1]^n. The paper uses epsilon expansion of a set as a mathematical tool, and borrows some important lemmas from geometry to the case of adversarial learning. In the sphere case, the results come from a fact that high dimensional half-spheres can almost cover all points in the sphere after an epsilon expansion, and the results depend on dimension n. For the unit cube case, the authors borrow a result from Talagrand, to show that the epsilon expansion of a set can cover a large portion of the cube as long as the set distribution is not very concentrated. In this case, the results (for l_2 norm) do not depend on dimension n. Experimentally, the authors show that inputs with higher dimension can actually get better robustness, aligning with the provided analysis. The primary reason that current adversarial defense does not work well on CIFAR is due to the fact that dataset is more spread out in high dimensional space. This is a good insight for understanding adversarial examples. The paper is overall well written and easy to follow. The interpretation of each lemma and proposition is clear. Although the paper mostly depend on well-known results in geometry and the ideas used are simple, it does provide good insight on explaining the prevalence of adversarial examples. I recommend to accept this paper. Question: Is there any good method to estimate U_c for a dataset? Although it is intuitive that CIFAR may have a smaller U_c than MNIST, is it possible to numerically estimate this quantity? This is necessary to fully support the conclusions made in experiments. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for taking the time to carefully read our paper and provide feedback . To answer the question : Yes , there are methods for quantifying the density of CIFAR and MNIST , although the accuracy of these methods is disputed . Classical density estimation methods ( like Parzan windows and GMMs ) fail on complex high-dimensional distributions . However , neural-network-based methods can attack this problem by training a GAN on the dataset , and then using a formula that predicts the likelihood of samples produced by the GAN . This formula involves the Jacobian of the generator , and the density of the latent `` z '' that produced the image . This was the approach taken in ( https : //arxiv.org/pdf/1705.08868.pdf ) . The authors of that work use several different methods for training generative models , and find that the estimated densities are * highly * dependent on how the model is trained , although for each specific training method the predicted MNIST densities are much higher than CIFAR densities ( see , e.g. , Fig2 in the referenced arxiv paper ) . This observation is compatible with the claims made in our paper . The issue of accurate density estimation on images is still an active area of research . We have been collaborating with another lab to develop new methods for high-dimensional density estimation with the goal of getting more consistent results than previous methods . We have omitted a discussion of these density estimates to remain anonymous ( our work on density estimation is under review ) , but we will include a citation and a brief discussion in the camera ready . To be transparent about our results , we find that typical CIFAR-10 images have log-densities roughly 40 orders of magnitude smaller than typical big-MNIST images , and these observed differences in density are compatible with the differences in adversarial robustness we observe for MNIST and CIFAR in Section 8 ."}, "2": {"review_id": "r1lWUoA9FQ-2", "review_text": "The paper considers the problem of adversarial examples in (mostly high-dimensional) multi-class classification problems. Although the results are not specific necessarily to very high dimensional data or two images, the paper mostly uses images as a running example, and so will I in the review. Assume that the data all lies in the unit box in R^n ([0, 1]^n). A multiclass classifier with K classes partitions the unit cube into K parts, each part corresponding to a given class. There are distributions \\rho_c associated with each class and there is a bound on their density given by U_c and the fraction of examples of class c is f_c. And (eps, p) adversarial point y for some point x is such that |x - y|_p <= \\eps and the classifier classifies x & y differently. The paper shows that under this modeling assumption adversarial examples are inevitable. The results mostly use standard (but deep) results from probability theory. The technical proofs themselves are not particular difficult (provided one has the right background). I think the overall implications are interesting, and I will recommend the paper be accepted. However, I also feel that this is a missed opportunity. To some extent the authors do try to have some high-level discussion about adversarial examples, but I think this could be expanded on more. For instance, why should it be assumed that an example that is \\eps far should automatically have the same class label? Surely, being \"eps\"-far away is an equivalence relation, thus this would mean that all the hypercube would have to be labeled by the same class. This is clearly not the case. One plausible explanation is that if you take two points that are in two different classes, then any sequence of points that take one to the other with the property that each adjacent pair is at most \\eps far away, must have the property that some intermediate mass have negligible chance of being a \"natural\" image. On the other hand, doesn't the fact that humans are not susceptible to most adversarial examples, imply that adversarial-example resistant classifiers exist? My own feeling is the assumption that U_c is bounded is the strongest assumption that may not hold true with real data. In any case, the paper has enough technical content to merit acceptance and I hope the open review forum will lead to a fruitful discussion about some of these questions. -- Minor comments: Page 6 (just after Thm 2). Isn't the bound in Eqn. (5) true for all \\ell_p norms for p \\geq 2? (not just \\ell_2 as the sentence says) Paras on Page 6 (just below Thm 2). It would be more pleasant if equation x could be replaced by Eq. (x) or Equation (x). Para in Sec 7 on Unbounded density: Clarify what norm you mean when you talk about \\eps/2 perturbations. Thm 5: Seems odd to have a theorem about MNIST. Surely the result is a lot more general!!!", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Regarding the epsilon-walk argument you discuss : Your observation about passing through a region that does not contain \u201c natural \u201d images is correct . One interesting thing about our theoretical framework is that a \u201c class \u201d is a distribution on the cube ( the support of which might cover only a tiny fraction of the cube ) , while the \u201c classifier \u201d is a function that maps all points in the cube ( not just the points that lie in the class distribution support ) onto a label . In our theory , it could be that two classes have distributions with supports that are separated by more than epsilon units . However , it could still be easy to fool the classifier with an epsilon perturbation because the classifier assigns a label to all points , including things that don \u2019 t look natural . There \u2019 s an argument to be made that this is what many classifiers do in real life ; adversarial examples might not lie on the \u201c natural \u201d image manifold because they contain \u201c fuzz \u201d and other artifacts that natural images don \u2019 t , and yet they get assigned a label by the classifier . One way to avoid this problem ( at least in theory ) , which we discuss in the paper , is having a \u201c don \u2019 t know \u201d class . In this case , one could degrade classifier performance by perturbing images into the \u201c don \u2019 t know \u201d class , but it might be difficult for an adversary to change the label to another defined class . We have seen from the adversarial examples literature , though , that producing classifiers that do n't assign strong labels to adversarially perturbed images might be easier said than done . Finally , we \u2019 ll say a few things about the reviewers comments on whether humans are subject to adversarial examples . There seems to be some debate about this . It \u2019 s certainly true that , most of the time , attacks on neural nets don \u2019 t transfer to humans . However , our experience has been that attacks on neural nets usually don \u2019 t transfer to other ( black-box ) neural nets either ( although they sometimes do for certain pairs/ensembles of target/victim networks ) , and so we don \u2019 t think this observation conclusively resolves the issue of whether it \u2019 s possible to make adversarial attacks on humans . To complicate things further , some authors claim to observe cases in which adversarial examples for neural nets do transfer to humans in certain contexts ( https : //arxiv.org/abs/1802.08195 ) . For what it \u2019 s worth , several neuroscientists and psychologist we have spoken to about this issue believe quite strongly that humans are susceptible to adversarial examples , just maybe not ones crafted using a neural net as a model for the human brain . We remain agnostic on this issue because it \u2019 s outside the scope of our expertise . This question seems to lie in the realm of philosophy and psychology , and we \u2019 ve avoided it in our paper in favor of sticking to mathematical issues . Finally , thanks for pointing out a number of minor errors . We have fixed them in the revision . We agree with the reviewer that Eqn ( 5 ) is more clear than Eq 5 , but unfortunately the non-parenthetical version seems to be the standard style chosen by the ICLR editors ( they chose this unusual definition for the \\eqref command ) ."}}