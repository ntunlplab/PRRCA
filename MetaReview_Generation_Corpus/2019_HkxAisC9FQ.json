{"year": "2019", "forum": "HkxAisC9FQ", "title": "Improved robustness to adversarial examples using Lipschitz regularization of the loss", "decision": "Reject", "meta_review": "This paper suggests augmenting adversarial training with a Lipschitz regularization of the loss, and suggests that this improves the adversarial robustness of deep neural networks. The idea of using such regularization seems novel. However, several reviewers were seriously concerned with the quality of the writing. In particular, the paper contains claims that not only are not needed but also are incorrect. Also, the Reviewer 2 in particular was also concerned with the presentation of prior work on Lipschitz regularization. \n\nSuch poor quality of the presentation makes it impossible to properly evaluate the actual paper contribution. ", "reviews": [{"review_id": "HkxAisC9FQ-0", "review_text": "This paper explores augmenting the training loss with an additional gradient regularization term to improve the robustness of models against adversarial examples. The authors show that this training loss can be interpreted as a form of adversarial training against optimal L2 and L_infinity adversarial perturbations. This augmented training effectively reduces the Lipschitz constant of the network, leading to improved robustness against a wide variety of attack algorithms. While I believe the results are correct and possibly significant, the paper is poorly written (especially for a 10 page submission) and comparison with prior work on reducing the Lipschitz constant of the network is lacking. The authors also made little to no effort in writing to ensure the clarity of their paper. I would like to see a completely reworked draft before opening to the idea of recommending acceptance. Pros: - Theoretically intuitive method for improving the model's robustness. - Evaluation against a wide variety of attacks. - Empirically demonstrated improvement over traditional adversarial training. Cons: - Lack of comparison to prior work. The authors are aware of numerous techniques for controlling the Lipschitz constant of the network for improved robustness, but did not compare to them at all. - Poorly written. The paper contains multiple missing figure references, has a duplicated table (Tables 1 and 3), and the method is not explained well. I am confused as to how the 2-Lip loss is minimized. Also, the paper organization seems very chaotic and incoherent, e.g., the introduction section contains many technical details that would better belong in related works or methods sections. -------------------------------------------- Revision: I thank the authors for incorporating my suggestions and reworking the draft, and I have updated my rating in response to the revision. While I believe the organization is much cleaner and easier to follow, there is still much room for improvement. In particular, the paper does not introduce concepts in a logical order for a non-expert to follow (e.g. Reviewer 1) and leaps into the paper's core idea too quickly. I am strongly in favor of exceeding the suggested page limit of 8 pages and using that space to address these concerns. A more pressing concern is the evaluation of prior work. The authors added a short section (Section 5.4) comparing their method to that of (Qian and Wegman, 2018). This is certainly a reasonable comparison and the results seem promising, the evaluation lacks an important dimension -- varying the value of epsilon and observing the change in robustness. This is an important aspect for defenses against adversarial examples as certain defense may be less robust but are insensitive to the adversary's strength. Showing the robustness across different adversary strengths gives a more informative view of the authors' proposed method in comparison to others. The evaluation is also lacking in breadth, ignoring other similar defenses such as (Cisse et al., 2017) and (Gouk et al., 2018).", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments . Following your suggestion , we have completely reworked our submission , with an eye towards clarity and the page limit . In fact we have reduced the page count to just over seven pages . We hope that you find the paper better organized and easier to read . Regarding comparison to other results , we have re-analyzed our experimental results and have now included a direct comparison with state-of-the-art results . We find that when attacks are measured in the 2-norm , our method is state-of-the-art , improving on the previous state-of-the-art by 11 % . When measured in the max-norm , our results are comparable to the state-of-the-art ( Madry et al ( 2017 ) ) , however we use only one-step adversarial training , whereas in Madry et al seven step adversarial training is used . We have also now included a section explicitly comparing our methods with prior methods , which can be summarized as follows . Prior work has focused on controlling the estimate of the Lipschitz constant using the product of norms of weight matrices . We argue that for deep networks this estimate is inaccurate , since its error grows exponentially in the number of layers . In our work we propose an alternative method for estimating the Lipschitz constant , which is an underestimate , and is estimated from the training data . This is a novel approach . Please also see the general reply to all reviewers , above ."}, {"review_id": "HkxAisC9FQ-1", "review_text": "The authors propose a novel method of training neural networks for robustness of adversarial attacks based on 2-norm and Lipschitz regularization. Unfortunately I'm not at all familiar with the literature on adversarial attacks so it is difficult for me to judge the quality and significance of this work. The theoretical results look plausible and clearly stated. The experiments show improvements over existing methods but I can't tell whether the right baselines were used. Overall the writing is reasonably clear but not very accessible for someone not already familiar with the area.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We have posted a new version of our paper . We have re-written the paper to be as accessible as possible to someone not directly familiar with adversarial robustness . We have also pushed the heavier math to the appendix , and reinterpreted our Lipschitz regularization as worst-case adversarial training , which is an interpretation of a more familiar idea in the area . We would greatly appreciate your comments on the new draft . We hope that you will find it easier to read . Please also see our general reply to all reviewers above ."}, {"review_id": "HkxAisC9FQ-2", "review_text": "Summary: this paper uses a trick to simplify the adversarial loss by one in which the adversarial perturbation appears in closed form. pros: - interesting idea - experiments are interesting cons: - formal results are either trivial or could be improved in their statements - experimental guarantees only, up to what is hidden in the Big-Oh notations of Theorem 2.2, 2.3. details: * In Theorem 2.2, you need to remove the $O(epsilon^2)$, unless you point to the Taylor theorem that guarantees that for the identity you claim before (5). The closest one I see is that the O(||a||^2) is in fact $||a|| u(||a||)$ with $\\lim u(x) = 0$ as $x \\rightarrow 0$, which does not guarantee the $O$ notation for any $a$. * In Theorem 2.2, how do you pass from the solution of (5) (which is indeed a vector) to the solution of the following equation, which, without constraint, gives a dim > 1 subspace in the general case ? * In all cases, you do not get Theorem 2.3 in its form as the $O$ notation just guarantees you an upperbound. You need to rephrase. * Figure ?? (twice) before Section 3 * Define the \u201cgroup norm\u201d notation appearing with the max in (8) (isn\u2019t one redundant ?) * Section 3.4 is interesting. Have you looked at generalising your observation in the last identity to more losses = f-divergences (hence, proper losses modulo assumptions) ? * Section 4: many Figure ??", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We have reworked our draft , and we hope that our new version addresses your points . We would like to make a comment regarding Big-O notation . In the context used in the paper , which corresponds to https : //en.wikipedia.org/wiki/Big_O_notation , Big-O notation is a rigorous result , not experimental . In many areas of scientific computing , engineering and statistics it is accepted that results need only be shown up to Big-O of some error term . For example in polynomial interpolation it typically it suffices to show a particular method has error epsilon^ ( n+1 ) with a n-th degree polynomial . We have shown that adversarial training is equivalent to Total Variation minimization , up to order epsilon^2 , where epsilon is the size of the adversarial perturbation . This means that when epsilon is small , as is typical ( our epsilon is 0.01 ) , the two methods are nearly equivalent . By equivalent , lossely speaking , we mean that replacing one term with another should lead to results which are very close . However the Big-O notation has a rigorous meaning in the limit as \\epsilon goes to zero . Please also see our general reply to all reviewers , above ."}], "0": {"review_id": "HkxAisC9FQ-0", "review_text": "This paper explores augmenting the training loss with an additional gradient regularization term to improve the robustness of models against adversarial examples. The authors show that this training loss can be interpreted as a form of adversarial training against optimal L2 and L_infinity adversarial perturbations. This augmented training effectively reduces the Lipschitz constant of the network, leading to improved robustness against a wide variety of attack algorithms. While I believe the results are correct and possibly significant, the paper is poorly written (especially for a 10 page submission) and comparison with prior work on reducing the Lipschitz constant of the network is lacking. The authors also made little to no effort in writing to ensure the clarity of their paper. I would like to see a completely reworked draft before opening to the idea of recommending acceptance. Pros: - Theoretically intuitive method for improving the model's robustness. - Evaluation against a wide variety of attacks. - Empirically demonstrated improvement over traditional adversarial training. Cons: - Lack of comparison to prior work. The authors are aware of numerous techniques for controlling the Lipschitz constant of the network for improved robustness, but did not compare to them at all. - Poorly written. The paper contains multiple missing figure references, has a duplicated table (Tables 1 and 3), and the method is not explained well. I am confused as to how the 2-Lip loss is minimized. Also, the paper organization seems very chaotic and incoherent, e.g., the introduction section contains many technical details that would better belong in related works or methods sections. -------------------------------------------- Revision: I thank the authors for incorporating my suggestions and reworking the draft, and I have updated my rating in response to the revision. While I believe the organization is much cleaner and easier to follow, there is still much room for improvement. In particular, the paper does not introduce concepts in a logical order for a non-expert to follow (e.g. Reviewer 1) and leaps into the paper's core idea too quickly. I am strongly in favor of exceeding the suggested page limit of 8 pages and using that space to address these concerns. A more pressing concern is the evaluation of prior work. The authors added a short section (Section 5.4) comparing their method to that of (Qian and Wegman, 2018). This is certainly a reasonable comparison and the results seem promising, the evaluation lacks an important dimension -- varying the value of epsilon and observing the change in robustness. This is an important aspect for defenses against adversarial examples as certain defense may be less robust but are insensitive to the adversary's strength. Showing the robustness across different adversary strengths gives a more informative view of the authors' proposed method in comparison to others. The evaluation is also lacking in breadth, ignoring other similar defenses such as (Cisse et al., 2017) and (Gouk et al., 2018).", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments . Following your suggestion , we have completely reworked our submission , with an eye towards clarity and the page limit . In fact we have reduced the page count to just over seven pages . We hope that you find the paper better organized and easier to read . Regarding comparison to other results , we have re-analyzed our experimental results and have now included a direct comparison with state-of-the-art results . We find that when attacks are measured in the 2-norm , our method is state-of-the-art , improving on the previous state-of-the-art by 11 % . When measured in the max-norm , our results are comparable to the state-of-the-art ( Madry et al ( 2017 ) ) , however we use only one-step adversarial training , whereas in Madry et al seven step adversarial training is used . We have also now included a section explicitly comparing our methods with prior methods , which can be summarized as follows . Prior work has focused on controlling the estimate of the Lipschitz constant using the product of norms of weight matrices . We argue that for deep networks this estimate is inaccurate , since its error grows exponentially in the number of layers . In our work we propose an alternative method for estimating the Lipschitz constant , which is an underestimate , and is estimated from the training data . This is a novel approach . Please also see the general reply to all reviewers , above ."}, "1": {"review_id": "HkxAisC9FQ-1", "review_text": "The authors propose a novel method of training neural networks for robustness of adversarial attacks based on 2-norm and Lipschitz regularization. Unfortunately I'm not at all familiar with the literature on adversarial attacks so it is difficult for me to judge the quality and significance of this work. The theoretical results look plausible and clearly stated. The experiments show improvements over existing methods but I can't tell whether the right baselines were used. Overall the writing is reasonably clear but not very accessible for someone not already familiar with the area.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We have posted a new version of our paper . We have re-written the paper to be as accessible as possible to someone not directly familiar with adversarial robustness . We have also pushed the heavier math to the appendix , and reinterpreted our Lipschitz regularization as worst-case adversarial training , which is an interpretation of a more familiar idea in the area . We would greatly appreciate your comments on the new draft . We hope that you will find it easier to read . Please also see our general reply to all reviewers above ."}, "2": {"review_id": "HkxAisC9FQ-2", "review_text": "Summary: this paper uses a trick to simplify the adversarial loss by one in which the adversarial perturbation appears in closed form. pros: - interesting idea - experiments are interesting cons: - formal results are either trivial or could be improved in their statements - experimental guarantees only, up to what is hidden in the Big-Oh notations of Theorem 2.2, 2.3. details: * In Theorem 2.2, you need to remove the $O(epsilon^2)$, unless you point to the Taylor theorem that guarantees that for the identity you claim before (5). The closest one I see is that the O(||a||^2) is in fact $||a|| u(||a||)$ with $\\lim u(x) = 0$ as $x \\rightarrow 0$, which does not guarantee the $O$ notation for any $a$. * In Theorem 2.2, how do you pass from the solution of (5) (which is indeed a vector) to the solution of the following equation, which, without constraint, gives a dim > 1 subspace in the general case ? * In all cases, you do not get Theorem 2.3 in its form as the $O$ notation just guarantees you an upperbound. You need to rephrase. * Figure ?? (twice) before Section 3 * Define the \u201cgroup norm\u201d notation appearing with the max in (8) (isn\u2019t one redundant ?) * Section 3.4 is interesting. Have you looked at generalising your observation in the last identity to more losses = f-divergences (hence, proper losses modulo assumptions) ? * Section 4: many Figure ??", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We have reworked our draft , and we hope that our new version addresses your points . We would like to make a comment regarding Big-O notation . In the context used in the paper , which corresponds to https : //en.wikipedia.org/wiki/Big_O_notation , Big-O notation is a rigorous result , not experimental . In many areas of scientific computing , engineering and statistics it is accepted that results need only be shown up to Big-O of some error term . For example in polynomial interpolation it typically it suffices to show a particular method has error epsilon^ ( n+1 ) with a n-th degree polynomial . We have shown that adversarial training is equivalent to Total Variation minimization , up to order epsilon^2 , where epsilon is the size of the adversarial perturbation . This means that when epsilon is small , as is typical ( our epsilon is 0.01 ) , the two methods are nearly equivalent . By equivalent , lossely speaking , we mean that replacing one term with another should lead to results which are very close . However the Big-O notation has a rigorous meaning in the limit as \\epsilon goes to zero . Please also see our general reply to all reviewers , above ."}}