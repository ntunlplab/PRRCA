{"year": "2019", "forum": "rkgKBhA5Y7", "title": "There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average", "decision": "Accept (Poster)", "meta_review": "All reviewers appreciate the empirical analysis and insights provided in the paper. The paper also reports impressive results on SSL. It will be a good addition to the ICLR program. ", "reviews": [{"review_id": "rkgKBhA5Y7-0", "review_text": "This paper proposes to apply Stochastic Weight Averaging to the semi-supervised learning context. It makes an interesting argument that the semi-supervised MT/Pi models are especially amenable to SWA since they are empirically observed to traverse a large flat region of the weight space during the later stages of training. To speed up training, the authors propose fast-SWA. Secition 3.2 is a little confusing. - If a random direction is, with high probability, not penalized, then why is it so flat along a random direction? Or is this simply an argument for why it is not guaranteed to be penalized, and therefore adversarial rays exist? I think the claim needs to be more precise (though it remains unclear how accurate the claim would be). - I also think that there is maybe something special about measuring the SGD-SGD ray at epochs 170/180. It coincides with the regime of training where the signal is dominated by the consistency loss. Is it possible this somehow induces a near-linear path in the parameter space? I would be interested in seeing projections of other epoch\u2019s SGD-SGD (e.g. 170/17x) vectors onto the 170/180 SGD-SGD ray and the extend to which they are co-linear. - It is also striking that traversing the SGD-SGD ray causes an error rate so similar to the adversarial ray for the supervised model; can the authors explain this phenomenon? - All this being said, I find the diversity argument compelling---though what would happen if we train the model even longer? Does it keep exploring? - Overall, I am not sure how comfortable we should be with interpreting the SGD-SGD ray results. It is important that the authors provide a convincing argument for the interpretability of the SGD-SGD ray results, as this appears to be the key to the \u201clarge flat region\u201d claim. I think Mandt\u2019s paper should be cited in-text, since this is what motivates Figure 2d. Is the benefit of Fast-SWA\u2019s fast convergence (to a competitive/better solution than SWA) unique to semi-supervised learning? Or can it be demonstrated by fully-supervised learning too? Given the focus on the semi-supervised regime, I would prefer if what the authors are proposing is, in some sense, special to the semi-supervised regime. Table 1 is confusing to read. I just want to see a comparison between with and without using fast-SWA, *with all else kept equal*. Is the intention to compare \u201cPrevious Best CNN\u201d and \u201cOurs CNN\u201d? Is this a fair comparison? Pros: + Interesting story + Good empirical performance Cons: - Unclear whether the story is entirely correct If the authors can provide a convincing case for the interpretability of the SGD-SGD results, I am happy to raise my score.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your thoughtful and supportive feedback . We address your questions below . 1.Regarding the argument in section 3.2 : In section 3.2 we discuss the behavior of train and test error along different types of rays : SGD-SGD , random , and adversarial rays . We analyze the error along SGD-SGD rays for two reasons . Firstly , in fast-SWA we are averaging solutions traversed by SGD so the rays connecting SGD iterates serve as a proxy for the space we average over . Secondly , we are interested in evaluating the width of the solutions that we explore during training which we expect will be improved by the consistency training as discussed in section 3.1 and A.6 . We do not expect width along random rays to be very meaningful because there are many directions in the parameter space that do not change the network outputs ( see e.g . [ 2 , 4 , 5 ] ) . However , by evaluating SGD-SGD rays , we can expect that these directions corresponds to meaningful changes to our model because individual SGD updates correspond to directions that change the predictions on the training set . Furthermore , we observe that different SGD iterates produce significantly different predictions on the test data . In section 3.2 we observe that along SGD-SGD directions the Pi and MT solutions are much wider than supervised solutions . On the other hand , we observe that along random and adversarial directions the difference in flatness is less pronounced . Neural networks in general are known to be resilient to noise , explaining why both MT / Pi and Supervised models are flat along random directions [ 1 ] . At the same time neural networks are susceptible to targeted perturbations ( such as adversarial attacks ) . We hypothesize that we do not observe improved flatness for semi-supervised methods along adversarial rays because we do not choose our input or weight perturbations adversarially , but rather they are sampled from a predefined set of transformations . 2.Regarding the choice of epochs 170 , 180 for SGD-SGD ray analysis : We consider epochs 170 and 180 ( the last 10 epochs of training ) for the SGD-SGD rays , as we are interested in the regime when the training has converged to the neighbourhood of the optimum , rather than the behavior during early iterations . We argue that in this regime SGD explores the set of possible solutions instead of converging to a single solution . Based on your suggestion , we computed the cosine similarity between the SGD-SGD rays for epoch pairs 170 & 175 and 175 & 180 using the Pi model . We measured a value of -0.065 , which corresponds to an angle of 93 degrees . Thus , the path traversed by SGD late in training is rather far from linear as the weight updates between epochs 170 and 175 and between epochs 175 and 180 are almost orthogonal . 3.Regarding the similarity between SGD-SGD rays and adversarial rays for supervised training : SGD-SGD directions and adversarial rays are indeed related . The adversarial ray for train loss at the given point is aligned with the gradient of the train loss at this point . The directions between SGD solutions from different epochs are also obtained by combining multiple gradient steps . In particular , if we use the full dataset as our mini-batch , the ray connecting SGD solutions at epochs 170 and 171 would be aligned with the adversarial ray computed at the SGD solution for epoch 170 ( but pointing in the opposite direction ) . Since the adversarial ray is constructed using only the derivative of the train loss at a given point -- this local derivative information says that if we perturb the weights with an infinitesimal step , the error goes up the fastest along this adversarial direction -- it is not guaranteed that along the adversarial ray , for any given distance , the error would be as large as possible . In Figure 2 ( d ) we observe that locally the train and test error go up more sharply along adversarial rays , but for larger distances SGD-SGD rays exhibit similar behavior . 4.Regarding training longer : Yes , the model continues to explore even if we train longer , if we don \u2019 t anneal the learning rate to zero . In particular note that for the results in section 3.3 we extend the training time . We run training for a total of 330 epochs using a cyclical learning rate schedule ( see section A9 for the details ) . Further , note that in combination with SWA or fast-SWA running longer consistently leads to improved performance . For example on CIFAR-10 with 4k labeled data using MT+fast-SWA we get 10.7 % test error after 180 epochs , 10.34 % after 240 epochs , 9.86 % after 480 epochs , and 9.05 % after 1200 epochs ( see Tables 2-5 in the appendix for detailed results ) . The fact that running fast-SWA longer improves the results suggests that SGD continues to explore diverse solutions and is demonstrated by the diversity plots in figures 2 and 7 ."}, {"review_id": "rkgKBhA5Y7-1", "review_text": "OVERVIEW: The paper looks at the problem of self-supervised learning using consistency-enforcing approaches. Their main contributions are two-fold: 1. Analysis to understand current state-of-the-art methods for self-supervised learning, namely the Mean Teacher model (MT) by Tarvainen and Valpola (2017) and the \\Pi model (Laine and Aila, 2017). They show a theoretical analysis (Sec.3.1) of a simplified version of the \\Pi model and show that it reaches flatter minima leading to good generalization. They show an analysis of the SDG trajectories (Sec. 3.2) that shows how these self-supervised models achieve flatter and lower minima compared to a fully supervised approach. They also provide an intuitive explanation to explore more solutions along the SGD trajectory. Finally, in Sec.3.3, they also discuss how ensembling and weight averaging help get better solutions. 2. Fast-SWA, which is a tweak to the SWA procedure (Izmailov et al, 2018) that averages models in the weight space along the SGD trajectory with a cyclical learning rate. They show good performance on CIFAR-10 and CIFAR-100 with their proposed Fast-SWA. PROS: 1. The paper contains a lot of empirical analysis explaining the behavior of these models and providing intuition about the optimization leading to their proposed solution. The problem and experiments are very organized and explained very well. 2. Exhaustive experiments, plots and tables showing very good performance on the standardized benchmark. CONS: 1. The novel contribution (as I see it) is in the theoretical analysis of Sec. 3.1 & A.5 and the Fast-SWA procedure. The Fast-SWA is a minor tweak to the regular SWA. The theoretical analysis is the main novelty and it is hidden away in the appendix ! Also, the results seems to be derived on the basis of Avron and Toledo and the authors' contribution relative to that is not clear. Also, what is the difference between the regular \\Pi model and simplified \\Pi model and how big a difference does this make in your theory ? 2. Can the Fast SWA be used directly say while supervised training of ImageNet ? Or is it applicable only to self-supervised problems ? Comments on the generalizability of this contribution might help increase novelty. OVERALL: I like the thorough analysis and good results of the paper. The novelty being a little weak results in the final rating of 7.5 (rounded up to 8, subject to change depending on other reviewers).", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We appreciate your supportive and thoughtful review . Our responses are below : 1 . Novelty can have many forms , and in this paper the main novelty , which we believe to be very significant , and also rare , is a thorough conceptual exploration of how loss geometry interacts with training procedures , particularly for semi-supervised learning , leading to several meaningful insights . Since many settings of neural network weights lead to essentially no loss , it is of foundational importance to understand how the geometric properties of a solution affect generalization . While conceptual papers can be more difficult to assess , they are often highly impactful , such as the ICLR paper by Zhang et . al ( 2016 ) on rethinking generalization [ 1 ] . In addition to these conceptual advances , we do also propose simple algorithms , combined with very strong results on many thorough experiments . In this context , we view simplicity as a strength . There is sometimes a temptation to propose complicated approaches that can appear highly novel , but are not adopted because similar results can be achieved by simpler alternatives . It is our contention that the extensive strong results in our paper combined with a simple algorithm , and a novel conceptual understanding ( which is rare ) , are a real service to the community . As you note we also make novel theoretical contributions , some of which are in the appendix . We will highlight some of this material more in the main text . In addition to the novel theoretical and methodological contributions , there is also a novel empirical analyses in sections 3.2-3.3 . In the simplified \\Pi model , we consider small additive input perturbations to the inputs whereas in the full \\Pi model we use random translations and horizontal flips of the inputs , and dropout perturbations on the weights . Tarvainen & Valpola ( 2017 ) showed that dropout could be removed without much degradation in performance . We view the random translations to be more targeted perturbations that lie along directions of the image manifold . This case is referred to in a footnote of the appendix section A.5 . We mentioned the main results from the theoretical analysis in the main text but keep the proof details in the appendix due to space limitation . We will bring forward key parts to the main text for clarity . 2.Yes , ( fast- ) SWA can be used on purely supervised problems . SWA was used for supervised problems on CIFAR-10 and ImageNet ( Izmailov , 2018 ) and achieved improved performance over SGD . Our paper , however , shows that the gains from weight averaging in consistency-based models are much larger than in semi-supervised learning than in supervised learning due to the geometric properties of the training trajectories and solutions discussed in Section 3 . We really appreciate your strong support , and we hope that you can consider our comments on overall novelty -- across methods , experiments , theory , and conceptual understanding -- combined with strong results , in your final assessment . We are happy to answer any further questions . [ 1 ] : Zhang et . al.Understanding deep learning requires rethinking generalization . ICLR 2017 ."}, {"review_id": "rkgKBhA5Y7-2", "review_text": "The paper is nice thread, easy to follow. The paper proposed to apply SWA (Stochastic Weight Averaging) Izmailov et al. 2018 to the semi-supervised approached based on consistency regularization. The paper first describes the related work nicely and offers a succinct explanation of two semi-supervised approaches they study. The paper then present an analysis on SGD trajectories of these 2 approaches, drawing comparisons with the supervised training and then building a case of why SWA is a valid idea to apply. The analysis section is very well described, the theoretical explanations are easy to follow and Figure 1, Figure 2 are really helpful to understand this analysis. Overall, the paper offers a useful insight into semi-supervised model trainings and offers recipe of converging to supervised results which is a valid contribution. I have following questions to the authors: 1. Did the authors do the analysis and apply SWA on ImageNet training besides Cifar-10 and Cifar-100 2. The accuracy number reported in abstract (5.0% error) is top-1 error or top-5 error? I think it's top-5 but explicit mention would be great. 3. In section 3.2, authors offer an analysis by chosing epoch 170, 180. How are these epochs chosen? 4. In section 3.1, authors consider a simple model version where only small additive perturbations to student inputs are applied. Is this a practical setup i.e. is this ever the case in actual model training? 5. In section 3.3, pg 6, do authors have intuition into why weight averaging has better improvement (1.18) vs ensembling (0.94)? 6. In section 5.2, page 8 , can authors provide their intuition behind the results: \"We found that the improvement on VAT is not drastic \u2013 our base implementation obtains 11.26% error where fast-SWA reduces it to 10.97%\" - why did fast-SWA not improve much?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your support and thoughtful questions . Below we address your questions : 1 . We have not yet been able to reproduce the baseline ( without SWA ) results for the MT model . We have been in touch with the authors of [ 1 ] to replicate these results . ImageNet experiments have also been difficult to run due to limited computational resources . Semi-supervised learning is more computationally intense than standard supervised training , which amplifies the computational difficulties and expense in running ImageNet experiments . 2.All the errors reported in the paper are top-1 errors . We will make this more explicit in the updated draft . 3.The epochs 170-180 are the last 10 epochs of training . We select these epochs as we are interested in the regime when the training has converged to the neighbourhood of the optimum , rather than the behavior during early iterations . We argue that in this regime SGD explores the set of possible solutions instead of converging to a single solution . 4.The typical setup uses perturbations from the data augmentations ( random translation and flipping ) and from dropout . The space of images is highly structured , and as such we believe that the more targeted perturbations of translation and flipping are more efficient at enforcing meaningful consistencies between teacher and student . In the 3072-dimensional input space , random perturbations will have a low projection onto these more meaningful directions . 5.The difference between the results of ensembling and averaging weights is sufficiently minor that the ordering could be different had we used a different dataset or architecture . We focus on weight averaging since ensembling N models results in N-fold increase in the number of computations at test time . Note that Izmailov et al . ( Averaging Weights Leads to Wider Optima and Better Generalization , 2018 , section 3.5 ) provides an argument that weight averaging approximates ensembling given that the averaged models are close in the weight space . 6.In Figure 2b we found that different methods can achieve different gains from averaging . Our empirical analysis in section 3 is focused on the Pi model and Mean Teacher , as the training trajectories for VAT are different , due to adversarial perturbations . Since random perturbations in Pi and MT lead to heterogeneous solution spaces , the gains from averaging could be greater in these model classes due to capturing a greater diversity of models . [ 1 ] Tarvainen and Valpola . Mean teachers are better role models : Weight-averaged consistency targets improve semi-supervised deep learning results . NIPS , 2017"}], "0": {"review_id": "rkgKBhA5Y7-0", "review_text": "This paper proposes to apply Stochastic Weight Averaging to the semi-supervised learning context. It makes an interesting argument that the semi-supervised MT/Pi models are especially amenable to SWA since they are empirically observed to traverse a large flat region of the weight space during the later stages of training. To speed up training, the authors propose fast-SWA. Secition 3.2 is a little confusing. - If a random direction is, with high probability, not penalized, then why is it so flat along a random direction? Or is this simply an argument for why it is not guaranteed to be penalized, and therefore adversarial rays exist? I think the claim needs to be more precise (though it remains unclear how accurate the claim would be). - I also think that there is maybe something special about measuring the SGD-SGD ray at epochs 170/180. It coincides with the regime of training where the signal is dominated by the consistency loss. Is it possible this somehow induces a near-linear path in the parameter space? I would be interested in seeing projections of other epoch\u2019s SGD-SGD (e.g. 170/17x) vectors onto the 170/180 SGD-SGD ray and the extend to which they are co-linear. - It is also striking that traversing the SGD-SGD ray causes an error rate so similar to the adversarial ray for the supervised model; can the authors explain this phenomenon? - All this being said, I find the diversity argument compelling---though what would happen if we train the model even longer? Does it keep exploring? - Overall, I am not sure how comfortable we should be with interpreting the SGD-SGD ray results. It is important that the authors provide a convincing argument for the interpretability of the SGD-SGD ray results, as this appears to be the key to the \u201clarge flat region\u201d claim. I think Mandt\u2019s paper should be cited in-text, since this is what motivates Figure 2d. Is the benefit of Fast-SWA\u2019s fast convergence (to a competitive/better solution than SWA) unique to semi-supervised learning? Or can it be demonstrated by fully-supervised learning too? Given the focus on the semi-supervised regime, I would prefer if what the authors are proposing is, in some sense, special to the semi-supervised regime. Table 1 is confusing to read. I just want to see a comparison between with and without using fast-SWA, *with all else kept equal*. Is the intention to compare \u201cPrevious Best CNN\u201d and \u201cOurs CNN\u201d? Is this a fair comparison? Pros: + Interesting story + Good empirical performance Cons: - Unclear whether the story is entirely correct If the authors can provide a convincing case for the interpretability of the SGD-SGD results, I am happy to raise my score.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your thoughtful and supportive feedback . We address your questions below . 1.Regarding the argument in section 3.2 : In section 3.2 we discuss the behavior of train and test error along different types of rays : SGD-SGD , random , and adversarial rays . We analyze the error along SGD-SGD rays for two reasons . Firstly , in fast-SWA we are averaging solutions traversed by SGD so the rays connecting SGD iterates serve as a proxy for the space we average over . Secondly , we are interested in evaluating the width of the solutions that we explore during training which we expect will be improved by the consistency training as discussed in section 3.1 and A.6 . We do not expect width along random rays to be very meaningful because there are many directions in the parameter space that do not change the network outputs ( see e.g . [ 2 , 4 , 5 ] ) . However , by evaluating SGD-SGD rays , we can expect that these directions corresponds to meaningful changes to our model because individual SGD updates correspond to directions that change the predictions on the training set . Furthermore , we observe that different SGD iterates produce significantly different predictions on the test data . In section 3.2 we observe that along SGD-SGD directions the Pi and MT solutions are much wider than supervised solutions . On the other hand , we observe that along random and adversarial directions the difference in flatness is less pronounced . Neural networks in general are known to be resilient to noise , explaining why both MT / Pi and Supervised models are flat along random directions [ 1 ] . At the same time neural networks are susceptible to targeted perturbations ( such as adversarial attacks ) . We hypothesize that we do not observe improved flatness for semi-supervised methods along adversarial rays because we do not choose our input or weight perturbations adversarially , but rather they are sampled from a predefined set of transformations . 2.Regarding the choice of epochs 170 , 180 for SGD-SGD ray analysis : We consider epochs 170 and 180 ( the last 10 epochs of training ) for the SGD-SGD rays , as we are interested in the regime when the training has converged to the neighbourhood of the optimum , rather than the behavior during early iterations . We argue that in this regime SGD explores the set of possible solutions instead of converging to a single solution . Based on your suggestion , we computed the cosine similarity between the SGD-SGD rays for epoch pairs 170 & 175 and 175 & 180 using the Pi model . We measured a value of -0.065 , which corresponds to an angle of 93 degrees . Thus , the path traversed by SGD late in training is rather far from linear as the weight updates between epochs 170 and 175 and between epochs 175 and 180 are almost orthogonal . 3.Regarding the similarity between SGD-SGD rays and adversarial rays for supervised training : SGD-SGD directions and adversarial rays are indeed related . The adversarial ray for train loss at the given point is aligned with the gradient of the train loss at this point . The directions between SGD solutions from different epochs are also obtained by combining multiple gradient steps . In particular , if we use the full dataset as our mini-batch , the ray connecting SGD solutions at epochs 170 and 171 would be aligned with the adversarial ray computed at the SGD solution for epoch 170 ( but pointing in the opposite direction ) . Since the adversarial ray is constructed using only the derivative of the train loss at a given point -- this local derivative information says that if we perturb the weights with an infinitesimal step , the error goes up the fastest along this adversarial direction -- it is not guaranteed that along the adversarial ray , for any given distance , the error would be as large as possible . In Figure 2 ( d ) we observe that locally the train and test error go up more sharply along adversarial rays , but for larger distances SGD-SGD rays exhibit similar behavior . 4.Regarding training longer : Yes , the model continues to explore even if we train longer , if we don \u2019 t anneal the learning rate to zero . In particular note that for the results in section 3.3 we extend the training time . We run training for a total of 330 epochs using a cyclical learning rate schedule ( see section A9 for the details ) . Further , note that in combination with SWA or fast-SWA running longer consistently leads to improved performance . For example on CIFAR-10 with 4k labeled data using MT+fast-SWA we get 10.7 % test error after 180 epochs , 10.34 % after 240 epochs , 9.86 % after 480 epochs , and 9.05 % after 1200 epochs ( see Tables 2-5 in the appendix for detailed results ) . The fact that running fast-SWA longer improves the results suggests that SGD continues to explore diverse solutions and is demonstrated by the diversity plots in figures 2 and 7 ."}, "1": {"review_id": "rkgKBhA5Y7-1", "review_text": "OVERVIEW: The paper looks at the problem of self-supervised learning using consistency-enforcing approaches. Their main contributions are two-fold: 1. Analysis to understand current state-of-the-art methods for self-supervised learning, namely the Mean Teacher model (MT) by Tarvainen and Valpola (2017) and the \\Pi model (Laine and Aila, 2017). They show a theoretical analysis (Sec.3.1) of a simplified version of the \\Pi model and show that it reaches flatter minima leading to good generalization. They show an analysis of the SDG trajectories (Sec. 3.2) that shows how these self-supervised models achieve flatter and lower minima compared to a fully supervised approach. They also provide an intuitive explanation to explore more solutions along the SGD trajectory. Finally, in Sec.3.3, they also discuss how ensembling and weight averaging help get better solutions. 2. Fast-SWA, which is a tweak to the SWA procedure (Izmailov et al, 2018) that averages models in the weight space along the SGD trajectory with a cyclical learning rate. They show good performance on CIFAR-10 and CIFAR-100 with their proposed Fast-SWA. PROS: 1. The paper contains a lot of empirical analysis explaining the behavior of these models and providing intuition about the optimization leading to their proposed solution. The problem and experiments are very organized and explained very well. 2. Exhaustive experiments, plots and tables showing very good performance on the standardized benchmark. CONS: 1. The novel contribution (as I see it) is in the theoretical analysis of Sec. 3.1 & A.5 and the Fast-SWA procedure. The Fast-SWA is a minor tweak to the regular SWA. The theoretical analysis is the main novelty and it is hidden away in the appendix ! Also, the results seems to be derived on the basis of Avron and Toledo and the authors' contribution relative to that is not clear. Also, what is the difference between the regular \\Pi model and simplified \\Pi model and how big a difference does this make in your theory ? 2. Can the Fast SWA be used directly say while supervised training of ImageNet ? Or is it applicable only to self-supervised problems ? Comments on the generalizability of this contribution might help increase novelty. OVERALL: I like the thorough analysis and good results of the paper. The novelty being a little weak results in the final rating of 7.5 (rounded up to 8, subject to change depending on other reviewers).", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We appreciate your supportive and thoughtful review . Our responses are below : 1 . Novelty can have many forms , and in this paper the main novelty , which we believe to be very significant , and also rare , is a thorough conceptual exploration of how loss geometry interacts with training procedures , particularly for semi-supervised learning , leading to several meaningful insights . Since many settings of neural network weights lead to essentially no loss , it is of foundational importance to understand how the geometric properties of a solution affect generalization . While conceptual papers can be more difficult to assess , they are often highly impactful , such as the ICLR paper by Zhang et . al ( 2016 ) on rethinking generalization [ 1 ] . In addition to these conceptual advances , we do also propose simple algorithms , combined with very strong results on many thorough experiments . In this context , we view simplicity as a strength . There is sometimes a temptation to propose complicated approaches that can appear highly novel , but are not adopted because similar results can be achieved by simpler alternatives . It is our contention that the extensive strong results in our paper combined with a simple algorithm , and a novel conceptual understanding ( which is rare ) , are a real service to the community . As you note we also make novel theoretical contributions , some of which are in the appendix . We will highlight some of this material more in the main text . In addition to the novel theoretical and methodological contributions , there is also a novel empirical analyses in sections 3.2-3.3 . In the simplified \\Pi model , we consider small additive input perturbations to the inputs whereas in the full \\Pi model we use random translations and horizontal flips of the inputs , and dropout perturbations on the weights . Tarvainen & Valpola ( 2017 ) showed that dropout could be removed without much degradation in performance . We view the random translations to be more targeted perturbations that lie along directions of the image manifold . This case is referred to in a footnote of the appendix section A.5 . We mentioned the main results from the theoretical analysis in the main text but keep the proof details in the appendix due to space limitation . We will bring forward key parts to the main text for clarity . 2.Yes , ( fast- ) SWA can be used on purely supervised problems . SWA was used for supervised problems on CIFAR-10 and ImageNet ( Izmailov , 2018 ) and achieved improved performance over SGD . Our paper , however , shows that the gains from weight averaging in consistency-based models are much larger than in semi-supervised learning than in supervised learning due to the geometric properties of the training trajectories and solutions discussed in Section 3 . We really appreciate your strong support , and we hope that you can consider our comments on overall novelty -- across methods , experiments , theory , and conceptual understanding -- combined with strong results , in your final assessment . We are happy to answer any further questions . [ 1 ] : Zhang et . al.Understanding deep learning requires rethinking generalization . ICLR 2017 ."}, "2": {"review_id": "rkgKBhA5Y7-2", "review_text": "The paper is nice thread, easy to follow. The paper proposed to apply SWA (Stochastic Weight Averaging) Izmailov et al. 2018 to the semi-supervised approached based on consistency regularization. The paper first describes the related work nicely and offers a succinct explanation of two semi-supervised approaches they study. The paper then present an analysis on SGD trajectories of these 2 approaches, drawing comparisons with the supervised training and then building a case of why SWA is a valid idea to apply. The analysis section is very well described, the theoretical explanations are easy to follow and Figure 1, Figure 2 are really helpful to understand this analysis. Overall, the paper offers a useful insight into semi-supervised model trainings and offers recipe of converging to supervised results which is a valid contribution. I have following questions to the authors: 1. Did the authors do the analysis and apply SWA on ImageNet training besides Cifar-10 and Cifar-100 2. The accuracy number reported in abstract (5.0% error) is top-1 error or top-5 error? I think it's top-5 but explicit mention would be great. 3. In section 3.2, authors offer an analysis by chosing epoch 170, 180. How are these epochs chosen? 4. In section 3.1, authors consider a simple model version where only small additive perturbations to student inputs are applied. Is this a practical setup i.e. is this ever the case in actual model training? 5. In section 3.3, pg 6, do authors have intuition into why weight averaging has better improvement (1.18) vs ensembling (0.94)? 6. In section 5.2, page 8 , can authors provide their intuition behind the results: \"We found that the improvement on VAT is not drastic \u2013 our base implementation obtains 11.26% error where fast-SWA reduces it to 10.97%\" - why did fast-SWA not improve much?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your support and thoughtful questions . Below we address your questions : 1 . We have not yet been able to reproduce the baseline ( without SWA ) results for the MT model . We have been in touch with the authors of [ 1 ] to replicate these results . ImageNet experiments have also been difficult to run due to limited computational resources . Semi-supervised learning is more computationally intense than standard supervised training , which amplifies the computational difficulties and expense in running ImageNet experiments . 2.All the errors reported in the paper are top-1 errors . We will make this more explicit in the updated draft . 3.The epochs 170-180 are the last 10 epochs of training . We select these epochs as we are interested in the regime when the training has converged to the neighbourhood of the optimum , rather than the behavior during early iterations . We argue that in this regime SGD explores the set of possible solutions instead of converging to a single solution . 4.The typical setup uses perturbations from the data augmentations ( random translation and flipping ) and from dropout . The space of images is highly structured , and as such we believe that the more targeted perturbations of translation and flipping are more efficient at enforcing meaningful consistencies between teacher and student . In the 3072-dimensional input space , random perturbations will have a low projection onto these more meaningful directions . 5.The difference between the results of ensembling and averaging weights is sufficiently minor that the ordering could be different had we used a different dataset or architecture . We focus on weight averaging since ensembling N models results in N-fold increase in the number of computations at test time . Note that Izmailov et al . ( Averaging Weights Leads to Wider Optima and Better Generalization , 2018 , section 3.5 ) provides an argument that weight averaging approximates ensembling given that the averaged models are close in the weight space . 6.In Figure 2b we found that different methods can achieve different gains from averaging . Our empirical analysis in section 3 is focused on the Pi model and Mean Teacher , as the training trajectories for VAT are different , due to adversarial perturbations . Since random perturbations in Pi and MT lead to heterogeneous solution spaces , the gains from averaging could be greater in these model classes due to capturing a greater diversity of models . [ 1 ] Tarvainen and Valpola . Mean teachers are better role models : Weight-averaged consistency targets improve semi-supervised deep learning results . NIPS , 2017"}}