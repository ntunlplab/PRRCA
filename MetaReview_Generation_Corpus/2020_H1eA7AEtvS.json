{"year": "2020", "forum": "H1eA7AEtvS", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "decision": "Accept (Spotlight)", "meta_review": "This paper proposes three modifications of BERT type models two of which is concerned with parameter sharing and one with a new auxiliary loss. New SOTA on downstream tasks are demonstrated. \n\nAll reviewers liked the paper and so did a lot of comments. \n\nAcceptance is recommended.", "reviews": [{"review_id": "H1eA7AEtvS-0", "review_text": "The authors present ALBERT, a modification of the BERT architecture with substantially fewer parameters. They show that despite being much smaller, the performance is very strong and achieves state of the art on a variety of different tasks. There are several ideas proposed here: embedding factorization, sharing layers, and sentence ordering as a training objective. 1. The point that naively increasing the size of the BERT architecture does not work is a good one, but the authors don't acknowledge that this is tied up in the effect of regularization. Cross layer parameter sharing has a regularization effect that simply scaling up BERT large to x-large or such sizes does not have. This is also an issue with the authors making the statement that they are the first to show that dropout is harmful for Transformers. This is a large generalization that seems to be a special case of not only the regularized architecture they propose but also the large quantity of data that the model still underfits to. 2. The authors propose embedding factorization to reduce the number of parameters in the embedding dimension. This is very intuitive, but the authors do not cite or compare to related approaches. I understand these models are computationally intensive and thus do not expect large quantities of detailed ablations. However, this kind of dimensionality reduction has been explored with other techniques, for example for knowledge distillation, quantization, or even adaptive input/softmax (and with subword as well, not just whole word modeling). These techniques have also been applied to machine translation models, which do not use them to learn rare words. I believe a better discussion of these methods should be added to the paper, as this is not a novel proposition. 3. A large takeaway I have from this paper is that parameter size is not a good metric. While ALBERT is substantially smaller, the authors do not make it clear that this model is very slow at inference time due to the large size. This raises several questions: is it better to have models that are deeper or more wide? Can the authors actually report the latency in a comparative table next to BERT? Can the authors provide a sense of how large this model is in MB - e.g. presumably a goal of less parameters would be to have a model with less memory, but then the decision between memory and latency that different models make should be made more clear. 4. Section 4.8 is not clear. Exactly how much data, in terms of GB of uncompressed text, is used here? Is it the data of XLNet and RoBERTa, so larger than both of those settings individually? Further, the authors train for 1 million steps. This is larger than both XLNet and RoBERTa, is that correct? Or there is some detail about the size of the batch that actually makes it comparable? The many small tables where the changes are not clearly delineated makes it difficult to compare results. ", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer # 3 , Thank you so much for going through the paper carefully and providing positive and useful feedback to our work ! Please see our responses below : We do mention that \u201c The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization. \u201d However , we did not want to interpret along this dimension too much , as we did not have rigorous proof that regularization can solve the model degradation problem . One of the major difficulties in doing experiments with the BERT-xlarge setup is that it needs to train on 1024 TPUs v3 units , which is an expensive resource . For the overgeneralization problem in the sentence \u201c dropout can hurt performance in large Transformer-based models \u201d , we will make it clear by adding this sentence : \u201c However , the underlying network structure of ALBERT is a special case of the Transformer , and further experimentation is needed to see if this phenomenon appears with other Transformer-based variants. \u201d We agree that a better discussion of related work would help people to better understand our work . Currently , we have a public comment from Sachin Mehta asking to compare related works such as \u2018 Adaptive input representations for neural language modeling. \u2019 , \u2018 transformer-xl \u2019 , and \u2018 Efficient softmax approximation for GPUs \u2019 . Please let us know if you have any specific additional work that you want us to include in the comparison of our embedding factorization methods , and we will incorporate it in the final version of the paper . The reason we did not report the inference time ( latency ) is because it is a platform-specific metric . We would need different strategies to optimize for TPUs and CPUs . However , we do have some TPU-based metrics for BERT-base and ALBERT-base . By looking at these numbers , we see that ALBERT-base is about 3x faster than BERT-base at inference time . Other people have also converted a Chinese version of ALBERT-tiny ( we would like to thank brightmart for implementing this project and he got an amazing number ( 1.4k ) of stars in such a short amount of time ! ) into tf-lite format and measured the inference time on mobile devices . Here is the quote from his website ( https : //github.com/brightmart/albert_zh ) : \u201c On an Android phone w/ Qualcomm 's SD845 SoC , via the above benchmark tool , as of 2019/11/01 , the inference latency is ~120ms w/ this converted TFLite model using 4 threads on CPU , and the memory usage is ~60MB for the model during inference . Note the performance will improve further with future TFLite implementation optimizations. \u201d That being said , we would say ALBERT could have a huge impact on inference speed because inference speed is usually memory-bandwidth bound and memory bandwidth is limited by memory capacity . For example , if you can keep the model weights used in matmuls in a smaller high-bandwidth memory , you can go 10x - 100x faster than if you need to read it out of a larger lower-bandwidth memory . In terms of model size in MB , they are roughly 4x as large as the parameter size as the weights are mostly in float format . We use all the XLNet data ( 126G ) as well as the stories data ( 31G ) of raw data . Our 1M step training went though the same number of iterations over the data as Roberta 500K , as they use 2x as large a batch size as ours . Thanks for bringing up these points , they are certainly valid and relevant . We will incorporate more info along the discussion above in the next version of our paper ."}, {"review_id": "H1eA7AEtvS-1", "review_text": "Summary: This paper investigates improving upon BERT by reducing complexity in terms of free parameters and memory footprint as well as computation steps. They propose 2 strategies for doing this: 1) Splitting the embedding matrix into two smaller matrices (going from V x A to V x B + B x A where B <<<< A); 2) layer-wise parameter sharing. They also utilize sentence order prediction to help with training. These coupled with a bunch of other choices such as using the lamb optimizer, certain hyperparameters etc help show dramatic empirical gains across the board on a wide variety of NLP/NLU tasks. Positives: This paper has a dramatic, seemingly statistically significant reduction in error across a wide-variety of tasks. It provides a thorough experimental plan and approaches the few addendums to training (splitting the embedding matrix, the layer-wise parameter sharing, and the sentence order prediction). Concerns & Questions: There's a lot of experimentation here and a lot of seemingly deliberate choices after seeing empirical results during the research phase. How crucial are the choices of optimizer and other specific hyperparameters? Were there ones you observed that were more brittle than others? Any specific 'reasonable' configurations/settings that caused degenerate solutions?", "rating": "8: Accept", "reply_text": "Dear Reviewer # 1 Thank you so much for going through the paper carefully and providing such a positive feedback about our work ! Please see our answers below : For hyper-parameter tuning , we only explore those hyper-parameters that are related to model size . This is done for the following two reasons : 1 ) To keep the comparison as meaningful as possible ( so we fixed all other parameters as in BERT , and always used LAMB optimizer ) 2 ) To keep under control the number of experiments we need to run ; we already have a lot of experiments to report on ; if we were to tune other hyper-parameters like learning rate and optimizer , the number of experiments needed can easily run out of control . For optimizer , we choose LAMB because it allows us to use large batch sizes . We haven \u2019 t tested other optimizers yet . Because large models that can cause degenerate solutions are extremely expensive to run , we did not explore that area very much . For example , in order to run with a batch size of 4096 , BERT-xlarge requires 1024 TPUs v3 units , which is an expensive resource . However , we are working on this and hopefully can give a reasonable explanation/solution to this problem soon ."}, {"review_id": "H1eA7AEtvS-2", "review_text": "This paper proposes a new pre-trained BERT-like model called ALBERT. The contributions are mainly 3-fold: factorized embedding parameterization, cross-layer parameter sharing, and intern-sentence coherence loss. The first two address the issue of model size and memory consumption in BERT; the third corresponds to a new auxiliary task in pre-train, sentence-order prediction (SOP), replacing the next sentence prediction (NSP) task in BERT. These modifications lead to a much leaner model and improved performance. As a result, ALBERT pushes the state of the art on GLUE, RACE, and SQuAD while having fewer parameters than BERT-large. This is a well-written paper which is easy to follow even for readers without deep background knowledge. The proposed method is meaningful and effective. Its empirical results are impressive. Other comments: - Section 4.9. Why use the all-share condition for state-of-the-art ALBERT results (as indicated in Table 2)? Judging from Table 4 and 5, shouldn't the non-shared condition give better results? The number of parameters would be larger, of course. - I like the justification/motivation given for replacing NSP with SOP. I wonder if the authors have tried other objectives (but didn't work out). Such negative results are valuable to practitioners. - Typo in Sec. 4.1: x1,1, x1,2 should be x2,1, x2,2. ", "rating": "8: Accept", "reply_text": "Dear Reviewer # 2 , Thank you so much for going through the paper carefully and providing such a positive feedback . Please see below our response to your comments : About all-sharing vs non-shared : yes , non-shared gives better results , but the number of parameters is increased dramatically . We tried other strategies of sharing the parameters across layers . For example , we divided the L layers into N groups of size M ( L=N * M ) , and each size-M group shares parameters . Overall , our experimental results show that the smaller the group size M is , the better the performance we get . However , decreasing group size M also dramatically increase the number of overall parameters . We chose the all-shared strategy to maximize our parameter reduction . We are glad that you like our SOP objective . We did try other changes to the objectives , such as multiword masking , but papers proposing these ideas were posted before our work , so for simplicity we adopted and cited the previous works . Thank you so much for helping us to correct the typo , we will fix them in our next version ."}], "0": {"review_id": "H1eA7AEtvS-0", "review_text": "The authors present ALBERT, a modification of the BERT architecture with substantially fewer parameters. They show that despite being much smaller, the performance is very strong and achieves state of the art on a variety of different tasks. There are several ideas proposed here: embedding factorization, sharing layers, and sentence ordering as a training objective. 1. The point that naively increasing the size of the BERT architecture does not work is a good one, but the authors don't acknowledge that this is tied up in the effect of regularization. Cross layer parameter sharing has a regularization effect that simply scaling up BERT large to x-large or such sizes does not have. This is also an issue with the authors making the statement that they are the first to show that dropout is harmful for Transformers. This is a large generalization that seems to be a special case of not only the regularized architecture they propose but also the large quantity of data that the model still underfits to. 2. The authors propose embedding factorization to reduce the number of parameters in the embedding dimension. This is very intuitive, but the authors do not cite or compare to related approaches. I understand these models are computationally intensive and thus do not expect large quantities of detailed ablations. However, this kind of dimensionality reduction has been explored with other techniques, for example for knowledge distillation, quantization, or even adaptive input/softmax (and with subword as well, not just whole word modeling). These techniques have also been applied to machine translation models, which do not use them to learn rare words. I believe a better discussion of these methods should be added to the paper, as this is not a novel proposition. 3. A large takeaway I have from this paper is that parameter size is not a good metric. While ALBERT is substantially smaller, the authors do not make it clear that this model is very slow at inference time due to the large size. This raises several questions: is it better to have models that are deeper or more wide? Can the authors actually report the latency in a comparative table next to BERT? Can the authors provide a sense of how large this model is in MB - e.g. presumably a goal of less parameters would be to have a model with less memory, but then the decision between memory and latency that different models make should be made more clear. 4. Section 4.8 is not clear. Exactly how much data, in terms of GB of uncompressed text, is used here? Is it the data of XLNet and RoBERTa, so larger than both of those settings individually? Further, the authors train for 1 million steps. This is larger than both XLNet and RoBERTa, is that correct? Or there is some detail about the size of the batch that actually makes it comparable? The many small tables where the changes are not clearly delineated makes it difficult to compare results. ", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer # 3 , Thank you so much for going through the paper carefully and providing positive and useful feedback to our work ! Please see our responses below : We do mention that \u201c The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization. \u201d However , we did not want to interpret along this dimension too much , as we did not have rigorous proof that regularization can solve the model degradation problem . One of the major difficulties in doing experiments with the BERT-xlarge setup is that it needs to train on 1024 TPUs v3 units , which is an expensive resource . For the overgeneralization problem in the sentence \u201c dropout can hurt performance in large Transformer-based models \u201d , we will make it clear by adding this sentence : \u201c However , the underlying network structure of ALBERT is a special case of the Transformer , and further experimentation is needed to see if this phenomenon appears with other Transformer-based variants. \u201d We agree that a better discussion of related work would help people to better understand our work . Currently , we have a public comment from Sachin Mehta asking to compare related works such as \u2018 Adaptive input representations for neural language modeling. \u2019 , \u2018 transformer-xl \u2019 , and \u2018 Efficient softmax approximation for GPUs \u2019 . Please let us know if you have any specific additional work that you want us to include in the comparison of our embedding factorization methods , and we will incorporate it in the final version of the paper . The reason we did not report the inference time ( latency ) is because it is a platform-specific metric . We would need different strategies to optimize for TPUs and CPUs . However , we do have some TPU-based metrics for BERT-base and ALBERT-base . By looking at these numbers , we see that ALBERT-base is about 3x faster than BERT-base at inference time . Other people have also converted a Chinese version of ALBERT-tiny ( we would like to thank brightmart for implementing this project and he got an amazing number ( 1.4k ) of stars in such a short amount of time ! ) into tf-lite format and measured the inference time on mobile devices . Here is the quote from his website ( https : //github.com/brightmart/albert_zh ) : \u201c On an Android phone w/ Qualcomm 's SD845 SoC , via the above benchmark tool , as of 2019/11/01 , the inference latency is ~120ms w/ this converted TFLite model using 4 threads on CPU , and the memory usage is ~60MB for the model during inference . Note the performance will improve further with future TFLite implementation optimizations. \u201d That being said , we would say ALBERT could have a huge impact on inference speed because inference speed is usually memory-bandwidth bound and memory bandwidth is limited by memory capacity . For example , if you can keep the model weights used in matmuls in a smaller high-bandwidth memory , you can go 10x - 100x faster than if you need to read it out of a larger lower-bandwidth memory . In terms of model size in MB , they are roughly 4x as large as the parameter size as the weights are mostly in float format . We use all the XLNet data ( 126G ) as well as the stories data ( 31G ) of raw data . Our 1M step training went though the same number of iterations over the data as Roberta 500K , as they use 2x as large a batch size as ours . Thanks for bringing up these points , they are certainly valid and relevant . We will incorporate more info along the discussion above in the next version of our paper ."}, "1": {"review_id": "H1eA7AEtvS-1", "review_text": "Summary: This paper investigates improving upon BERT by reducing complexity in terms of free parameters and memory footprint as well as computation steps. They propose 2 strategies for doing this: 1) Splitting the embedding matrix into two smaller matrices (going from V x A to V x B + B x A where B <<<< A); 2) layer-wise parameter sharing. They also utilize sentence order prediction to help with training. These coupled with a bunch of other choices such as using the lamb optimizer, certain hyperparameters etc help show dramatic empirical gains across the board on a wide variety of NLP/NLU tasks. Positives: This paper has a dramatic, seemingly statistically significant reduction in error across a wide-variety of tasks. It provides a thorough experimental plan and approaches the few addendums to training (splitting the embedding matrix, the layer-wise parameter sharing, and the sentence order prediction). Concerns & Questions: There's a lot of experimentation here and a lot of seemingly deliberate choices after seeing empirical results during the research phase. How crucial are the choices of optimizer and other specific hyperparameters? Were there ones you observed that were more brittle than others? Any specific 'reasonable' configurations/settings that caused degenerate solutions?", "rating": "8: Accept", "reply_text": "Dear Reviewer # 1 Thank you so much for going through the paper carefully and providing such a positive feedback about our work ! Please see our answers below : For hyper-parameter tuning , we only explore those hyper-parameters that are related to model size . This is done for the following two reasons : 1 ) To keep the comparison as meaningful as possible ( so we fixed all other parameters as in BERT , and always used LAMB optimizer ) 2 ) To keep under control the number of experiments we need to run ; we already have a lot of experiments to report on ; if we were to tune other hyper-parameters like learning rate and optimizer , the number of experiments needed can easily run out of control . For optimizer , we choose LAMB because it allows us to use large batch sizes . We haven \u2019 t tested other optimizers yet . Because large models that can cause degenerate solutions are extremely expensive to run , we did not explore that area very much . For example , in order to run with a batch size of 4096 , BERT-xlarge requires 1024 TPUs v3 units , which is an expensive resource . However , we are working on this and hopefully can give a reasonable explanation/solution to this problem soon ."}, "2": {"review_id": "H1eA7AEtvS-2", "review_text": "This paper proposes a new pre-trained BERT-like model called ALBERT. The contributions are mainly 3-fold: factorized embedding parameterization, cross-layer parameter sharing, and intern-sentence coherence loss. The first two address the issue of model size and memory consumption in BERT; the third corresponds to a new auxiliary task in pre-train, sentence-order prediction (SOP), replacing the next sentence prediction (NSP) task in BERT. These modifications lead to a much leaner model and improved performance. As a result, ALBERT pushes the state of the art on GLUE, RACE, and SQuAD while having fewer parameters than BERT-large. This is a well-written paper which is easy to follow even for readers without deep background knowledge. The proposed method is meaningful and effective. Its empirical results are impressive. Other comments: - Section 4.9. Why use the all-share condition for state-of-the-art ALBERT results (as indicated in Table 2)? Judging from Table 4 and 5, shouldn't the non-shared condition give better results? The number of parameters would be larger, of course. - I like the justification/motivation given for replacing NSP with SOP. I wonder if the authors have tried other objectives (but didn't work out). Such negative results are valuable to practitioners. - Typo in Sec. 4.1: x1,1, x1,2 should be x2,1, x2,2. ", "rating": "8: Accept", "reply_text": "Dear Reviewer # 2 , Thank you so much for going through the paper carefully and providing such a positive feedback . Please see below our response to your comments : About all-sharing vs non-shared : yes , non-shared gives better results , but the number of parameters is increased dramatically . We tried other strategies of sharing the parameters across layers . For example , we divided the L layers into N groups of size M ( L=N * M ) , and each size-M group shares parameters . Overall , our experimental results show that the smaller the group size M is , the better the performance we get . However , decreasing group size M also dramatically increase the number of overall parameters . We chose the all-shared strategy to maximize our parameter reduction . We are glad that you like our SOP objective . We did try other changes to the objectives , such as multiword masking , but papers proposing these ideas were posted before our work , so for simplicity we adopted and cited the previous works . Thank you so much for helping us to correct the typo , we will fix them in our next version ."}}