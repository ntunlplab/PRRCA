{"year": "2019", "forum": "BJzuKiC9KX", "title": "Revisiting Reweighted Wake-Sleep", "decision": "Reject", "meta_review": "The paper presents a well conducted empirical study of the Reweighted Wake Sleep (RWS) algorithm (Bornschein and Bengio, 2015). It shows that it performs consistently better than alternatives such as Importance Weighted Autoencoder (IWAE) for the hard problem of learning deep generative models with discrete latent variables acting as a stochastic control flow. \nThe work is well-written and extracts valuable insights supported by empirical observations: in particular the fact that increasing the number of particles improves learning in RWS but hurts in IWAE, and the fact that RWS can also be successfully applied to continuous variables.\nThe reviewers and AC note the following weaknesses of the work as it currently stands:  a) it is almost exclusively empirical and while reasonable explanations are argued, it does not provide a formal theoretical analysis justifying the observed behaviour b) experiments are limited to MNIST and synthetic data, confirmation of the findings on larger-scale real-world data and model would provide a more complete and convincing evidence. \nThe paper should be made stronger on at least one (and ideally both) of these accounts.\n\n", "reviews": [{"review_id": "BJzuKiC9KX-0", "review_text": "Main idea: This paper studies a problem of the importance weighted autoencoder (IWAE) pointed out by Rainforth 18, that is, tighter lower bounds arising from increasing the number of particles improve the learning of the generative model, but worsen the learning of the inference network. The authors show that the reweighted wake-sleep algorithm (RWS) doesn't suffer from this issue. Moreover, as an alternative to control variate scheme and reparameterization trick, RWS doesn't suffer from high variance gradients, thus it is particularly useful for discrete latent variable models. To support the claim, they conduct three experiments: 1) on ATTEND, INFER, REPEAT, a generative model with both discrete and continuous latent variables; 2) on MNIST with a continuous latent variable model; 3) on a synthetic GMM. Clarity issues: 1. \"branching\" has been used many times, but AFAIK, this seems not a standard terminology. What do \"branching on the samples\", \"conditional branching\", \"branching paths\" mean? 2. zero-forcing failure mode and delta-WW: I find this part difficult to follow. For example, the following sentence \"the inference network q(z|x) becomes the posterior for this model which, in this model, also has support at most {0, . . . , 9} for all x\". However, this failure mode seems an interesting finding, and since delta-WW outperforms other methods, it deserves a better introduction. Questions: 1. In Fig 1 (right), how do you estimate KL(q(z|x) || p(z|x))? 2. In Sec 4.2, why do you say IWAE learns a better model only up to a point (K = 128) and suffers from diminishing returns afterwards? 3. In Fig 4, why WS doesn't achieve a better performance when K increasing? Experiments: 1. Since the motivating story is about discrete latent variable models, better baselines should be compared, e.g. RBM, DVAE, DVAE++, VQ-VAE etc. 2. All experiments were on either on MNIST or synthetic data, at least one large scale experiment on discrete data should be made to verify the performance of RWS. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Clarity issues : By stochastic branching we refer to the evaluation of generative models where discrete latent variables are used to select which part of the model is going to be evaluated next . For example , in AIR , this decides when the program halts , in GMM the cluster index decides which likelihood function is evaluated . Another example of this are probabilistic context-free grammars ( PCFGs ) where discrete variables are used to describe which production rule is used ( for example https : //arxiv.org/abs/1806.07832 ) . This is in contrast with modeling approaches where the discrete latent variable is merely an input to a neural network that doesn \u2019 t distinguish it from a non-discrete latent variable since it does not explicitly use the discreteness to model distinct modes of the data . ( also see our general comment ) We will clarify the \u201c zero-forcing \u201d failure mode and delta-WW in the updated manuscript . Questions : To estimate KL ( q ( z|x ) || p ( z|x ) ) , we take the difference of the log likelihood estimated by a 5000-particle IWAE bound and the ELBO estimated by 5000 Monte Carlo samples . The statement that \u201c IWAE learns a better model only up to a point \u201d is justified by the IWAE curve in the middle of Figure 2 : the decreasing slope indicates that improvements in marginal log probability decrease with increasing numbers of particles . This is even more pronounced in Figure 1 , where IWAE performance decreases for k > 10 . In Fig 4 , WS actually does achieve better performance as K increases - the final value of the learning curve goes down , although only very slightly . Experiments : Regarding experiments , RBM/DVAE/++/ # and VQ-VAE allow learning models with discrete latent variables in general ; however , the resulting discreteness can not be used for directing the control flow of a generative model ( see also response to AnonReviewer3 and our general comment ) . - In the DVAE family of algorithms , learning in discrete latent variable models is achieved by a continuous relaxation . This prevents using these variables as hard branching conditions . - In the VQ-VAE algorithm , the discrete latent variable is explicitly designed to be used to select an embedding and it is deterministic . This limits the use of a discrete latent variable ( can not be used to model a cluster identity or stopping of a while loop ) . Even though we do not have experiments on large-scale real-world datasets , AIR is a non-trivial model , and using it can be seen as a large-scale experiment - taking several days ( and several GPUs ) to obtain results summarized in Figure 1 . Similarly , to the best of our knowledge , ours is the first reported result of an MNIST model trained with IWAE with 512 particles ."}, {"review_id": "BJzuKiC9KX-1", "review_text": "This manuscript investigates the performance of Reweighted Wake-Sleep (RWS) framework for learning deep generative models with discrete latent variables. It gives a clear introduction to variational autoencoder based models for scenarios with discrete latent variables, including IWAE and also models based on continuous relaxations of discrete variables. The paper performs several experiments, which suggest that RWS is more appropriate for discrete latent variables than other methods such as IWAE. Especially, increasing the number of particles, unlike IWAE, always enhances the performance of RWS. While this paper investigates an important problem, and also offers interesting observations, it lacks a rigorous analysis of why the RWS performance is consistently better than IWAE. More precisely, the propositions should be stated in more formal language and they should be accompanied with a minimal rigorous justification.", "rating": "6: Marginally above acceptance threshold", "reply_text": "The key formal justification is relatively straightforward : RWS , unlike IWAE , does not suffer from the \u201c tighter bounds \u201d problem . On the contrary , RWS uses self-normalized importance sampling to estimate the gradient with respect to \\phi . Both the asymptotic bias and variance of a self-normalized importance sampling estimator decrease linearly in number of particles . This means that increasing number of particles improves our gradient estimator and thus the optimization procedure . We will explain this in more detail in the updated manuscript ."}, {"review_id": "BJzuKiC9KX-2", "review_text": "This paper conducts an extensive set of experiments on RWS and compares it against a set of benchmarks such as GMM and IWAE. The main contribution of the paper is the fact revealed by these experiments, that RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent variable models as well. The performance of RWS will increase significantly if we increase the number of particles. The experimental part is written in an inspiring way, and I enjoyed reading it. However, there should be stronger baselines incorporated. for example, https://arxiv.org/abs/1805.07445. Also, I think the authors could try to emphasize more on the shortcomings of RWS discovered by the GMM experiments, and how defensive importance sampling fixes it. There are several other parts in the paper that indicates interesting facts, diving deeper into it could possibly lead to more interesting findings. In all, I would consider these comparison results important to be somewhere in the literature, but because its lack of rigorous analysis and explanation for the observations, I personally think these observations alone are not novel enough to be an ICLR paper. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Baselines : As set out in our overall response , we aim to show that RWS is a better choice for inference in models that have stochastic control flow , where the choice from the discrete latent variables matters explicitly . In our GMM example , the cluster identity is such a choice , and in AIR , the stopping condition for the loop is another such choice . The work done in DVAE++ , DVAE # , and other such approaches do not really handle this general class of problems well -- -by typically requiring enumeration of all possible branches and choices . GMM : We will include a more detailed description of how defensive sampling ameliorates issues discovered in the GMM experiments in the updated manuscript . Theoretical Rigour : We will include a more comprehensive discussion of the theoretical basis of why RWS is better than IWAE in the updated manuscript . Briefly , the justification for why RWS does not suffer from the `` tighter bounds '' problem is due to RWS 's use of self-normalised importance sampling to compute the gradient of proposal parameters -- -resulting in both the asymptotic bias and variance decreasing linearly with number of samples . Empirical Rigour : Our experiments strongly support our hypotheses : a . Unlike IWAE , RWS performs better with more particles , both in terms of the generative model and inference network , and b . It allows for effective and easy application to models where the choice from the discrete random variables affects model expansion or computation -- -something that requires expensive enumeration with continuous relaxations , or extremely finicky and unreliable construction with control-variate methods ."}], "0": {"review_id": "BJzuKiC9KX-0", "review_text": "Main idea: This paper studies a problem of the importance weighted autoencoder (IWAE) pointed out by Rainforth 18, that is, tighter lower bounds arising from increasing the number of particles improve the learning of the generative model, but worsen the learning of the inference network. The authors show that the reweighted wake-sleep algorithm (RWS) doesn't suffer from this issue. Moreover, as an alternative to control variate scheme and reparameterization trick, RWS doesn't suffer from high variance gradients, thus it is particularly useful for discrete latent variable models. To support the claim, they conduct three experiments: 1) on ATTEND, INFER, REPEAT, a generative model with both discrete and continuous latent variables; 2) on MNIST with a continuous latent variable model; 3) on a synthetic GMM. Clarity issues: 1. \"branching\" has been used many times, but AFAIK, this seems not a standard terminology. What do \"branching on the samples\", \"conditional branching\", \"branching paths\" mean? 2. zero-forcing failure mode and delta-WW: I find this part difficult to follow. For example, the following sentence \"the inference network q(z|x) becomes the posterior for this model which, in this model, also has support at most {0, . . . , 9} for all x\". However, this failure mode seems an interesting finding, and since delta-WW outperforms other methods, it deserves a better introduction. Questions: 1. In Fig 1 (right), how do you estimate KL(q(z|x) || p(z|x))? 2. In Sec 4.2, why do you say IWAE learns a better model only up to a point (K = 128) and suffers from diminishing returns afterwards? 3. In Fig 4, why WS doesn't achieve a better performance when K increasing? Experiments: 1. Since the motivating story is about discrete latent variable models, better baselines should be compared, e.g. RBM, DVAE, DVAE++, VQ-VAE etc. 2. All experiments were on either on MNIST or synthetic data, at least one large scale experiment on discrete data should be made to verify the performance of RWS. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Clarity issues : By stochastic branching we refer to the evaluation of generative models where discrete latent variables are used to select which part of the model is going to be evaluated next . For example , in AIR , this decides when the program halts , in GMM the cluster index decides which likelihood function is evaluated . Another example of this are probabilistic context-free grammars ( PCFGs ) where discrete variables are used to describe which production rule is used ( for example https : //arxiv.org/abs/1806.07832 ) . This is in contrast with modeling approaches where the discrete latent variable is merely an input to a neural network that doesn \u2019 t distinguish it from a non-discrete latent variable since it does not explicitly use the discreteness to model distinct modes of the data . ( also see our general comment ) We will clarify the \u201c zero-forcing \u201d failure mode and delta-WW in the updated manuscript . Questions : To estimate KL ( q ( z|x ) || p ( z|x ) ) , we take the difference of the log likelihood estimated by a 5000-particle IWAE bound and the ELBO estimated by 5000 Monte Carlo samples . The statement that \u201c IWAE learns a better model only up to a point \u201d is justified by the IWAE curve in the middle of Figure 2 : the decreasing slope indicates that improvements in marginal log probability decrease with increasing numbers of particles . This is even more pronounced in Figure 1 , where IWAE performance decreases for k > 10 . In Fig 4 , WS actually does achieve better performance as K increases - the final value of the learning curve goes down , although only very slightly . Experiments : Regarding experiments , RBM/DVAE/++/ # and VQ-VAE allow learning models with discrete latent variables in general ; however , the resulting discreteness can not be used for directing the control flow of a generative model ( see also response to AnonReviewer3 and our general comment ) . - In the DVAE family of algorithms , learning in discrete latent variable models is achieved by a continuous relaxation . This prevents using these variables as hard branching conditions . - In the VQ-VAE algorithm , the discrete latent variable is explicitly designed to be used to select an embedding and it is deterministic . This limits the use of a discrete latent variable ( can not be used to model a cluster identity or stopping of a while loop ) . Even though we do not have experiments on large-scale real-world datasets , AIR is a non-trivial model , and using it can be seen as a large-scale experiment - taking several days ( and several GPUs ) to obtain results summarized in Figure 1 . Similarly , to the best of our knowledge , ours is the first reported result of an MNIST model trained with IWAE with 512 particles ."}, "1": {"review_id": "BJzuKiC9KX-1", "review_text": "This manuscript investigates the performance of Reweighted Wake-Sleep (RWS) framework for learning deep generative models with discrete latent variables. It gives a clear introduction to variational autoencoder based models for scenarios with discrete latent variables, including IWAE and also models based on continuous relaxations of discrete variables. The paper performs several experiments, which suggest that RWS is more appropriate for discrete latent variables than other methods such as IWAE. Especially, increasing the number of particles, unlike IWAE, always enhances the performance of RWS. While this paper investigates an important problem, and also offers interesting observations, it lacks a rigorous analysis of why the RWS performance is consistently better than IWAE. More precisely, the propositions should be stated in more formal language and they should be accompanied with a minimal rigorous justification.", "rating": "6: Marginally above acceptance threshold", "reply_text": "The key formal justification is relatively straightforward : RWS , unlike IWAE , does not suffer from the \u201c tighter bounds \u201d problem . On the contrary , RWS uses self-normalized importance sampling to estimate the gradient with respect to \\phi . Both the asymptotic bias and variance of a self-normalized importance sampling estimator decrease linearly in number of particles . This means that increasing number of particles improves our gradient estimator and thus the optimization procedure . We will explain this in more detail in the updated manuscript ."}, "2": {"review_id": "BJzuKiC9KX-2", "review_text": "This paper conducts an extensive set of experiments on RWS and compares it against a set of benchmarks such as GMM and IWAE. The main contribution of the paper is the fact revealed by these experiments, that RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent variable models as well. The performance of RWS will increase significantly if we increase the number of particles. The experimental part is written in an inspiring way, and I enjoyed reading it. However, there should be stronger baselines incorporated. for example, https://arxiv.org/abs/1805.07445. Also, I think the authors could try to emphasize more on the shortcomings of RWS discovered by the GMM experiments, and how defensive importance sampling fixes it. There are several other parts in the paper that indicates interesting facts, diving deeper into it could possibly lead to more interesting findings. In all, I would consider these comparison results important to be somewhere in the literature, but because its lack of rigorous analysis and explanation for the observations, I personally think these observations alone are not novel enough to be an ICLR paper. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Baselines : As set out in our overall response , we aim to show that RWS is a better choice for inference in models that have stochastic control flow , where the choice from the discrete latent variables matters explicitly . In our GMM example , the cluster identity is such a choice , and in AIR , the stopping condition for the loop is another such choice . The work done in DVAE++ , DVAE # , and other such approaches do not really handle this general class of problems well -- -by typically requiring enumeration of all possible branches and choices . GMM : We will include a more detailed description of how defensive sampling ameliorates issues discovered in the GMM experiments in the updated manuscript . Theoretical Rigour : We will include a more comprehensive discussion of the theoretical basis of why RWS is better than IWAE in the updated manuscript . Briefly , the justification for why RWS does not suffer from the `` tighter bounds '' problem is due to RWS 's use of self-normalised importance sampling to compute the gradient of proposal parameters -- -resulting in both the asymptotic bias and variance decreasing linearly with number of samples . Empirical Rigour : Our experiments strongly support our hypotheses : a . Unlike IWAE , RWS performs better with more particles , both in terms of the generative model and inference network , and b . It allows for effective and easy application to models where the choice from the discrete random variables affects model expansion or computation -- -something that requires expensive enumeration with continuous relaxations , or extremely finicky and unreliable construction with control-variate methods ."}}