{"year": "2017", "forum": "rJEgeXFex", "title": "Predicting Medications from Diagnostic Codes with Recurrent Neural Networks", "decision": "Accept (Poster)", "meta_review": "This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our \"expert\" stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound.", "reviews": [{"review_id": "rJEgeXFex-0", "review_text": "This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings. The authors also did address the questions of the reviewers. My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We agree that this work would be relevant in a data science or computational medicine venue . We submitted it to ICLR after seeing applications listed as a relevant subject area in the call for papers , and reasoning that there was no better place to have discussions about our application than among the researchers who developed the methods it uses ."}, {"review_id": "rJEgeXFex-1", "review_text": "In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees. ----- This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and a random forest, as well as a common sense baseline. The differences in performance between the recurrent models and the MLP appear to be large enough to be significant, given the size of the test set. Strengths: - Very important problem. As the authors point out, two the value propositions of EHRs -- which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government -- included more accurate records and fewer medication mistakes. These two benefits have largely failed to materialize. This seems like a major opportunity for data mining and machine learning. - Paper is well-written with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results. - Empirical results are solid with a strong win for RNNs over convincing baselines. This is in contrast to some recent related papers, including Lipton & Kale et al, ICLR 2016, where the gap between the RNN and MLP was relatively small, and Choi et al, MLHC 2016, which omitted many obvious baselines. - Discussion is thorough and thoughtful. The authors are right about the kidney code embedding results: this is a very promising result. Weaknesses: - The authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point. This does not necessarily invalidate their results, but it is somewhat unnatural and the explanation is difficult to follow, reducing the paper's potential impact. It is also reduces the RNN's potential advantage. - The chosen metrics seem appropriate, but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial, e.g., RNN score 0.01 more than NN!). The authors should invest some space in explaining (1) what level of performance -- for each metric -- would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are \"significant\" (even in an informal sense). - The paper proposes nothing novel in terms of methods, which is a serious weakness for a methods conference like ICLR. I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive. For example, one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets. Is there some way to investigate this, perhaps by looking at the kinds of errors each model makes? I have a final comment: as a piece of clinical work, the paper has a huge weakness: the lack of ground truth labels for missing medications. Models are both trained and tested on data with noisy labels. For training, the authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional isn't too big of a problem). For testing, though, this seems like it could skew metrics. Further, the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians. The cases shown in Appendix C lend some credence to this assertion: for Case 1, 7/26 actual medications received probabilities < 0.5. My hunch is that clinical reviewers would view the paper with great skepticism. The authors will need to get creative about evaluation -- or invest a lot of time/money in labeling data -- to really prove that this works. For what it is worth, I hope that this paper is accepted as I think it will be of great interest to the ICLR community. However, I am borderline about whether I'd be willing to fight for its acceptance. If the authors can address the reviewers' critiques -- and in particular, dive into the question of overfitting the imperfect labels and provide some insights -- I might be willing to raise my score and lobby for acceptance.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "> Would have been more natural to use full sequences rather than partial sequences limited by a maximum number of elements . The description of sampling was hard to follow . The sequence limitation may have reduced the RNN 's advantage . We understand this concern , and we agree that it would have been more natural and intuitive to use the full sequence for each patient . We also agree that its original description was hard to follow . We explained in the question/answer round the reasons for our choice ( mainly training time and stratifying input samples by medication ) , and the efforts we made to investigate its impact on the results ( probably a small impact , if any ) . We re-wrote the description to be clearer in this version ( Section 3.1 , paragraph 3 ) . We believe our design is a reasonable compromise between the ideal and the practical . > Please explain what performance of each measure is needed to be clinically useful . Multi-label classification is pretty new to medicine , so there is not much documented experience to draw from . But we 've added a two short paragraphs to Section 4 to give a couple rules of thumb that will help readers evaluate the clinical utility of the results . > Deeper analysis , such as investigating the effect of noisy targets and the kinds of errors made by each model , may make the paper more competitive , given that it is an applications paper that does n't present any new theory or methods . While we did not have the time or resources to go further in this suggested direction ( which we agree would be useful ) , we did add further analysis on the efficacy of the embedding , which we believe will be of interest to the ICLR community . > Clinicians will view the results with skepticism because of the lack of ground truth due to noisy labels . Yes , we agree , and we try to succinctly make this point in the paper . We intended this work to present initial experiments in this direction , with more thorough and expensive validation to come later , depending on how well this proof of concept worked . Of course , the necessary depth of that validation depends on the way in which the suggestions will be used . If we want to use the suggestions as some kind of prior probability for further downstream analysis , then we 're probably well on our way to having acceptable performance . For use in clinical quality assurance , or use at the bedside , especially in a modal context in which the clinician would have to explicitly accept or reject the suggestions , we would have to demonstrate their correctness with much more rigor , probably culminating in a prospective trial ( and the suggestions themselves would need to be improved from what we achieved here ) . The reviewer also points out the example of Case 1 in the appendix , where `` 7/26 actual medications received probabilities < 0.5 . '' Note that we make no claims about the calibration of the RNN 's predictions . We have found by eyeball that a cutoff of 0.2 gives a reasonable balance between sensitivity and specificity , and we state this in the paper ."}, {"review_id": "rJEgeXFex-2", "review_text": "This is a well written, organized, and presented paper that I enjoyed reading. I commend the authors on their attention to the narrative and the explanations. While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes. The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning. That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper. Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture. A few points of criticism: -The numerical results are in my view too brief. Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1. I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances. To be clear I don't think this is disqualifying or deeply concerning; I simply found it a bit underwhelming. - To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance. I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure. - There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader. I saw that another reviewer suggested perhaps ICLR is not the right venue for this work. While I certainly see the reviewer's point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye). Overall, a nice paper.", "rating": "7: Good paper, accept", "reply_text": "We appreciate the positive comments of all reviewers and the questions/suggestions that have improved the paper to this point . Below we summarize and address the concerns of each reviewer . > More analysis would be useful , especially of the embedding , which appears to be a negative result . The embedding is not actually a negative result , but from this comment we now understand that the way we presented Figure 5 gives the impression of one . In the figure , we colored the elements by the ICD chapter ( roughly corresponding to organ system ) , but we did not mean to imply that an ideal embedding would separate the elements into groups of the same color . This was an obvious mistake on our part , because usually such figures are indeed colored that way . In fact , the reason we thought an embedding might be useful is because of the strong relationships that exist between elements of different chapters , and we expected/wanted to see much mixture of the colors in among some general areas of color sameness . To address this issue and provide further analysis of the embedding , we have changed the coloring of the figure to better indicate when the embedding aligns with clinical judgment . Each element is now colored according to the semantic clinical relatedness of the element 's nearest neighbor in the embedding space ( not the t-SNE space ) , indicating whether the neighbor represents a condition that is strongly related , loosely related , or unrelated to that of the given element . The determination was made by hand for all 2000 elements , using the clinical judgement of a licensed physician . Nearest neighbors were computed under the manhattan distance , which we found to be the most useful of several Minkowski distance metrics tried , with p ranging above and below 1.0 . We found that about half of the nearest neighbors were strongly related , whereas a sampling of 100 random pairs produced 0.08 ( 95 % CI [ 0.028 , 0.125 ] ) fraction of nearest neighbors that were strongly related . We also found the surprising result that the probability of a strongly related neighbor actually increases with longer distances to that neighbor , instead of what we had expected , that closer neighbors would indicate a stronger relationship . While it was obviously true by inspection that closer distances reflect stronger clinical relationships , an increasing distance to the nearest neighbor indicates a sparser populated portion of the embedding space , and there is a clear , approximately linear relationship between the nearest neighbor distance and the probability of a strong semantic relationship that extends over the full range of nearest distances . Further work in this direction to discover the causes of the relationship will be very interesting , but time and space limit us to simply reporting the result of this analysis . We have updated the paper to include the re-colored figure , the new numeric results ( in the text of Section 4 ) , a paragraph of description and one of discussion on the analysis ( Sections 3.4.2 and 4 ) , and we have added Figure 6 displaying the conditional probability of relatedness given distance to the nearest neighbor . We believe that this is an intriguing addition to the paper , and we appreciate the suggestion of the reviewer to go further in this direction . It does make the paper a little long , however . We could move Figure 6 to an appendix if desired . > Reduce medical jargon . We agree that jargon is undesirable , and we have reviewed the paper and did our best to minimize medical jargon and explain medical terms , although we only found a few cases where we could improve on what we had . We replaced the term RxNorm with a simple description of it ( but kept the footnote to its URL ) , added concise descriptions of the example conditions we mention , and spelled out Intensive Care Unit . We 'll be happy to address further specific instances if someone points them out , or if we have misunderstood what the reviewer is referring to ."}], "0": {"review_id": "rJEgeXFex-0", "review_text": "This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings. The authors also did address the questions of the reviewers. My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We agree that this work would be relevant in a data science or computational medicine venue . We submitted it to ICLR after seeing applications listed as a relevant subject area in the call for papers , and reasoning that there was no better place to have discussions about our application than among the researchers who developed the methods it uses ."}, "1": {"review_id": "rJEgeXFex-1", "review_text": "In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees. ----- This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and a random forest, as well as a common sense baseline. The differences in performance between the recurrent models and the MLP appear to be large enough to be significant, given the size of the test set. Strengths: - Very important problem. As the authors point out, two the value propositions of EHRs -- which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government -- included more accurate records and fewer medication mistakes. These two benefits have largely failed to materialize. This seems like a major opportunity for data mining and machine learning. - Paper is well-written with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results. - Empirical results are solid with a strong win for RNNs over convincing baselines. This is in contrast to some recent related papers, including Lipton & Kale et al, ICLR 2016, where the gap between the RNN and MLP was relatively small, and Choi et al, MLHC 2016, which omitted many obvious baselines. - Discussion is thorough and thoughtful. The authors are right about the kidney code embedding results: this is a very promising result. Weaknesses: - The authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point. This does not necessarily invalidate their results, but it is somewhat unnatural and the explanation is difficult to follow, reducing the paper's potential impact. It is also reduces the RNN's potential advantage. - The chosen metrics seem appropriate, but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial, e.g., RNN score 0.01 more than NN!). The authors should invest some space in explaining (1) what level of performance -- for each metric -- would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are \"significant\" (even in an informal sense). - The paper proposes nothing novel in terms of methods, which is a serious weakness for a methods conference like ICLR. I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive. For example, one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets. Is there some way to investigate this, perhaps by looking at the kinds of errors each model makes? I have a final comment: as a piece of clinical work, the paper has a huge weakness: the lack of ground truth labels for missing medications. Models are both trained and tested on data with noisy labels. For training, the authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional isn't too big of a problem). For testing, though, this seems like it could skew metrics. Further, the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians. The cases shown in Appendix C lend some credence to this assertion: for Case 1, 7/26 actual medications received probabilities < 0.5. My hunch is that clinical reviewers would view the paper with great skepticism. The authors will need to get creative about evaluation -- or invest a lot of time/money in labeling data -- to really prove that this works. For what it is worth, I hope that this paper is accepted as I think it will be of great interest to the ICLR community. However, I am borderline about whether I'd be willing to fight for its acceptance. If the authors can address the reviewers' critiques -- and in particular, dive into the question of overfitting the imperfect labels and provide some insights -- I might be willing to raise my score and lobby for acceptance.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "> Would have been more natural to use full sequences rather than partial sequences limited by a maximum number of elements . The description of sampling was hard to follow . The sequence limitation may have reduced the RNN 's advantage . We understand this concern , and we agree that it would have been more natural and intuitive to use the full sequence for each patient . We also agree that its original description was hard to follow . We explained in the question/answer round the reasons for our choice ( mainly training time and stratifying input samples by medication ) , and the efforts we made to investigate its impact on the results ( probably a small impact , if any ) . We re-wrote the description to be clearer in this version ( Section 3.1 , paragraph 3 ) . We believe our design is a reasonable compromise between the ideal and the practical . > Please explain what performance of each measure is needed to be clinically useful . Multi-label classification is pretty new to medicine , so there is not much documented experience to draw from . But we 've added a two short paragraphs to Section 4 to give a couple rules of thumb that will help readers evaluate the clinical utility of the results . > Deeper analysis , such as investigating the effect of noisy targets and the kinds of errors made by each model , may make the paper more competitive , given that it is an applications paper that does n't present any new theory or methods . While we did not have the time or resources to go further in this suggested direction ( which we agree would be useful ) , we did add further analysis on the efficacy of the embedding , which we believe will be of interest to the ICLR community . > Clinicians will view the results with skepticism because of the lack of ground truth due to noisy labels . Yes , we agree , and we try to succinctly make this point in the paper . We intended this work to present initial experiments in this direction , with more thorough and expensive validation to come later , depending on how well this proof of concept worked . Of course , the necessary depth of that validation depends on the way in which the suggestions will be used . If we want to use the suggestions as some kind of prior probability for further downstream analysis , then we 're probably well on our way to having acceptable performance . For use in clinical quality assurance , or use at the bedside , especially in a modal context in which the clinician would have to explicitly accept or reject the suggestions , we would have to demonstrate their correctness with much more rigor , probably culminating in a prospective trial ( and the suggestions themselves would need to be improved from what we achieved here ) . The reviewer also points out the example of Case 1 in the appendix , where `` 7/26 actual medications received probabilities < 0.5 . '' Note that we make no claims about the calibration of the RNN 's predictions . We have found by eyeball that a cutoff of 0.2 gives a reasonable balance between sensitivity and specificity , and we state this in the paper ."}, "2": {"review_id": "rJEgeXFex-2", "review_text": "This is a well written, organized, and presented paper that I enjoyed reading. I commend the authors on their attention to the narrative and the explanations. While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes. The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning. That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper. Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture. A few points of criticism: -The numerical results are in my view too brief. Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1. I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances. To be clear I don't think this is disqualifying or deeply concerning; I simply found it a bit underwhelming. - To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance. I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure. - There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader. I saw that another reviewer suggested perhaps ICLR is not the right venue for this work. While I certainly see the reviewer's point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye). Overall, a nice paper.", "rating": "7: Good paper, accept", "reply_text": "We appreciate the positive comments of all reviewers and the questions/suggestions that have improved the paper to this point . Below we summarize and address the concerns of each reviewer . > More analysis would be useful , especially of the embedding , which appears to be a negative result . The embedding is not actually a negative result , but from this comment we now understand that the way we presented Figure 5 gives the impression of one . In the figure , we colored the elements by the ICD chapter ( roughly corresponding to organ system ) , but we did not mean to imply that an ideal embedding would separate the elements into groups of the same color . This was an obvious mistake on our part , because usually such figures are indeed colored that way . In fact , the reason we thought an embedding might be useful is because of the strong relationships that exist between elements of different chapters , and we expected/wanted to see much mixture of the colors in among some general areas of color sameness . To address this issue and provide further analysis of the embedding , we have changed the coloring of the figure to better indicate when the embedding aligns with clinical judgment . Each element is now colored according to the semantic clinical relatedness of the element 's nearest neighbor in the embedding space ( not the t-SNE space ) , indicating whether the neighbor represents a condition that is strongly related , loosely related , or unrelated to that of the given element . The determination was made by hand for all 2000 elements , using the clinical judgement of a licensed physician . Nearest neighbors were computed under the manhattan distance , which we found to be the most useful of several Minkowski distance metrics tried , with p ranging above and below 1.0 . We found that about half of the nearest neighbors were strongly related , whereas a sampling of 100 random pairs produced 0.08 ( 95 % CI [ 0.028 , 0.125 ] ) fraction of nearest neighbors that were strongly related . We also found the surprising result that the probability of a strongly related neighbor actually increases with longer distances to that neighbor , instead of what we had expected , that closer neighbors would indicate a stronger relationship . While it was obviously true by inspection that closer distances reflect stronger clinical relationships , an increasing distance to the nearest neighbor indicates a sparser populated portion of the embedding space , and there is a clear , approximately linear relationship between the nearest neighbor distance and the probability of a strong semantic relationship that extends over the full range of nearest distances . Further work in this direction to discover the causes of the relationship will be very interesting , but time and space limit us to simply reporting the result of this analysis . We have updated the paper to include the re-colored figure , the new numeric results ( in the text of Section 4 ) , a paragraph of description and one of discussion on the analysis ( Sections 3.4.2 and 4 ) , and we have added Figure 6 displaying the conditional probability of relatedness given distance to the nearest neighbor . We believe that this is an intriguing addition to the paper , and we appreciate the suggestion of the reviewer to go further in this direction . It does make the paper a little long , however . We could move Figure 6 to an appendix if desired . > Reduce medical jargon . We agree that jargon is undesirable , and we have reviewed the paper and did our best to minimize medical jargon and explain medical terms , although we only found a few cases where we could improve on what we had . We replaced the term RxNorm with a simple description of it ( but kept the footnote to its URL ) , added concise descriptions of the example conditions we mention , and spelled out Intensive Care Unit . We 'll be happy to address further specific instances if someone points them out , or if we have misunderstood what the reviewer is referring to ."}}