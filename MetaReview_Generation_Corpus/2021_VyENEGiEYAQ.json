{"year": "2021", "forum": "VyENEGiEYAQ", "title": "Cluster-Former: Clustering-based Sparse Transformer for Question Answering", "decision": "Reject", "meta_review": "The paper attempts to make transformers more scalable for longer sequences. In this regards, authors propose a clustering-based attention mechanism, where only tokens attends to other tokens in the same cluster. This design reduces memory requirements and allows more information mixing than simple local windows. Using the proposed approach, new state-of-the-art performance is obtained on Natural Questions long answer, although marginal. However, reviewers raised numerous concerns. First, the novelty of the paper compared to prior work like reformer or routing transformer which also conceptually does clustering is not resolved. Second, the claim that k-means yields a more balanced/stable clustering than LSH is not well established. Finally, why clustering, i.e. attention between similar vectors is better than dissimilar or randomly chosen vectors or does is it even as expressive is not clear. Thus, unfortunately I cannot recommend an acceptance of the paper in its current form to ICLR.", "reviews": [{"review_id": "VyENEGiEYAQ-0", "review_text": "Summary : Cluster-former is the latest proposal for enabling transformers to deal with long input sequences . Such sequences are particularly problematic for problems like question answering , QA , ( or summarization ) , where the context can be arbitrarily long , and effectively open-ended when the setup includes a context retrieval component ( e.g. , as in OpenQA ) . Cluster-Former combines local information by encoding sequence chunks separately with a sliding window , then injects clustering layers , that use k-means to compute centroids to cluster hidden states and capture global information . The approach yields state-of-the-art , and top-of-leaderboard , results on Natural Questions ( long answers ) . This is great solid work , showing how clustering can be designed , implemented and used successfully , to capture long distance dependencies in sparsified self-attention models . This is a concrete and useful contribution in itself for the large community working on this type of architecture and related problems . At the same time the approach involves quite a bit of complexity which makes one wonder if the baselines could be more competitive given a comparable amount of fine tuning . At the same time , competitive solutions of different nature ( generative ) are being proposed that pose a concrete challenge to this type of architecture , which are not evaluated , but should be at least discussed . Pros - Solid proof of concept and reference to successfully implementing clustering in sparse attention . - Strong empirical results , particularly the Natural Questions \u2019 leaderboard for long answers . - Impressive amount of technical work , also with respect to reproducing results with other systems . - Notwithstanding the amount of work in this area , literature review and comparison seems adequate but I might have missed something . - Some qualitative analysis : which could be extended and articulated , in particular it would be interesting to understand where the long distance information helps ; e.g. , vs the sliding window approach and particularly vs LSH . Cons - One of the arguments for the paper is that it is not clear if related methods , like Reformer , can generalize to long sequences . However , in the evaluated implementation ( Table 2 ) LSH is not that much worse than k-means . In fact , even just the sliding window alone seems surprisingly competitive on all QA tasks . While being much simpler . I find the authors \u2019 effort to compare with all these related methods truly commendable . It seems natural to wonder how much more fine-tuning has gone into Cluster-Former compared to the simpler baselines , given its additional complexity . It would be important to discuss this aspect in more detail . - Given the recent work of generative readers : https : //arxiv.org/abs/2005.11401 , and particularly Izacard & Grave , ( FID , https : //arxiv.org/pdf/2007.01282.pdf ) it seems unclear that direct encoding is the only , or the best , option for dealing with long sequences , at least for QA . In particular , FID seems attractive due to its simplicity and capacity ( about twice as much as Cluster-Former it seems ) . The authors should discuss this work . It would be ideal , at some point , to compare directly by evaluating on OpenQA-NQ or by other means . Detailed feedback - Pleas define x , from x\\times d , right below Eq ( 1 ) . Num tokens in context ? - Scaler value/scalar value ? - It would be great to explain Eq ( 2 ) step by step for clarity . - What is the effect of the overlapping content size m-l ? And in general of parameters l and m. In particular , could this affect positively the performance of the simpler sliding window model ? - Why using cluster layers at only 2 fixed depths ? How does this parameter affect results ? - The max length is constrained to 5k ( 10k test ) due to memory constraints , can this be improved , how ? - How long did it take to train the leaderboard ( NQ ) entry system ? - Unclear what table 2 evaluates on , e.g. , for NQ , is this on the dev set ? Or a fraction of it ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your encouraging and insightful comments . We have updated the draft with the modifications in blue . Below , we provide detailed responses to your questions . Q1 : About comparison to baselines such as LSH , and how much more fine-tuning has gone into Cluster-Former compared to the simpler baselines . A1 : We really appreciate your acknowledgment of our efforts on baselines . Most of the baselines are either not open-sourced or can not be easily transferred to our tasks . To make a fair comparison , we re-implement LSH and Sparse Attention , and integrate them into our framework . We fix the sliding window layers for all the methods . To select which layers should be used for sliding windows , we only have a wide exploration of it on Quasar-T dataset based on Cluster-Former . Then we fix sliding windows layers for the other datasets , and only tune dropout from { 0.1 , 0.15 , 0.2 } for all the methods including baselines and report the best result . We have updated it in the draft . Q2 : Comparison to generative readers and FID A2 : Yes , we agree that generative reader is a good solution to overcome reading long sequences . However , it still depends on how well the retrievers are . As pointed out by FID , \u201c we show that the performance of our method significantly improves when the number of retrieved passages increases \u201d , it is still important to have a better reader to read longer context . The main reason we didn \u2019 t test our models on OpenQA-NQ is that the retrieved passages are not the same in different methods . Thus , to make a fair comparison with previous readers , we focus more on the datasets with fixed context . We will try to evaluate our model on OpenQA-NQ during the discussion or in the final version . Q3 : Pleas define x , from x\\times d , right below Eq ( 1 ) . Num tokens in context ? Scaler value/scalar value ? It would be great to explain Eq ( 2 ) step by step for clarity . A3 : Thank you for your suggestions ! We have modified them and rewritten the explanation for Eq ( 2 ) step by step in the updated draft . Q4 : What is the effect of the overlapping content size l-m ? A4 : We had experiments on ( l=256 , m=224 ) and ( l=256 , m=230 ) for sliding-window-only baseline on Quasar-T . There is no significant difference , and we select ( l=256 , m=224 ) for all our experiments . Q5 : Why using cluster layers at only 2 fixed depths ? How does this parameter affect results ? A5 : In Table 4 , we have a hyper-parameter search for using cluster layers . It also includes experiments with 3/4/5/6 fixed depths . And we select the best hyper-parameter for all the other datasets . Q6 : The max length is constrained to 5k ( 10k test ) due to memory constraints , can this be improved , how ? A6 : Yes , as our method doesn \u2019 t have the quadratic issue on sequence length , one solution is to map 24 layers to multi-GPUs , and another solution is to call checkpoint function , such as \u201c torch.utils.checkpoint.checkpoint \u201d from Pytorch , which does not save intermediate activations , and instead recomputes them in backward pass . However , both methods will make the encoding slower and we will try it to encode longer sequences in future work . Q7 : How long did it take to train the leaderboard ( NQ ) entry system ? A7 : It takes one day by using 8 V100 GPU . Q8 : Unclear what table 2 evaluates on , e.g. , for NQ , is this on the dev set ? Or a fraction of it ? A8 : For NQ , all the results including baselines are on the full dev set . And the other datasets are on the test set ."}, {"review_id": "VyENEGiEYAQ-1", "review_text": "The paper describes a method to handle long documents for question answering . Most existing approaches use a sliding window approach , without communication between different sliding windows . Instead , they propose an approach that clusters individual vectors , and allows communication ( attention ) among the locations in the same cluster . I am not sure about the intuition behind this -- why would communicate between similar vectors more efficient than dissimilar or randomly chosen vectors ? Would the performance improve if you use a better clustering algorithm ? The authors do not provide much intuition on this either . I have a concern about comparison with locality sensitive hashing . The number of buckets used in locality sensitive hashing was 64 . And it 's clear that having more clusters help . And the comparison between # C=64 Cluster Former and Locality Sensitive Hashing is marginal -- less than one point on all measures . I am not sure the results are strong enough to support that clustering is better than random assignments . For a valid comparison , they should report the results with locality-sensitive hashing and 512 buckets . The paper evaluates on three QA datasets , as long as experiments on perplexity for language modeling and shows promising performances . Some clarifying questions : 1 ) could you specify a bit more on how do `` classify the mean values of the first hidden state of all the chunked sequences to identify whether the question has short / long answers or not '' ? 2 ) I 'm a bit confused with the experimental set up . For NQ , what 's the numbers in Table 2 ? Is it on the dev set , and the numbers on Table 3 are on the test set ? Please make it clear . 3 ) would this work on a really lengthy QA dataset such as narrativeQA ? 4 ) From Table 2 , it seems the more the number of clusters , the better the performance . Why do you stop at 512 ? Is this have something to do with computational efficiency ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your insightful comments . We have updated the draft with the modifications in blue . Below , we provide detailed responses to your questions . Q1 : Why would communicate between similar vectors more efficient than dissimilar or randomly chosen vectors ? Would the performance improve if you use a better clustering algorithm ? A1 : According to LSH in Reformer ( Kitaev et al. , 2020 ) , \u201c the queries are sorted by bucket number \u201d . In this way , if using randomly chosen vectors , one extreme case would be that most of the vectors are assigned to one or two buckets . In this case , sorting by bucket number will fail , as most vectors share a same number . By using K-Means , it automatically computes the bucket centroids which will follow the distribution of hidden states . It can make the model more robust and less likely to fail the sorting . Thus , we do agree that a better clustering algorithm will make the model better and more robust . While considering the speed of clustering , we mainly rely on K-Means from Nvidia . Q2 : Experiments of LSH with more buckets . A2 : The results of LSH with 512 buckets are also worse than our method after tuning the dropout rate . The following is about LSH best results on Quasar-T , SearchQA and NQ respectively : 53.4/63.5 , 67.2/74.6 , 76.0/56.7 , compared to our best results 54.0/63.9 , 68.0/75.1 , 76.5/57.1 . Q3 : How to `` classify the mean values of the first hidden state of all the chunked sequences to identify whether the question has short / long answers or not '' . A3 : For Natural Question , we first identify whether the question has short/long answers or not based on the mean values of the first hidden state of all the chunked sequences , $ \\frac { 1 } { K } \\sum_ { k=1 } ^ { K } \\mathbf { H } ^ { N } _k [ 0 ] $ , where $ K $ is the number of chunks and $ N $ is the number of layers . We have modified the description in the updated draft . Q4 : The experimental set up of NQ on Table 2 and 3 . A4 : All the results of NQ ( including baselines ) on Table 2 are based on dev set and Table 3 based on test set . We have modified the captions of the tables in the updated draft . Q5 : Would this work on a really lengthy QA dataset such as narrativeQA ? A5 : The problem of NarrativeQA is quite close to OpenQA setting ( like Quarsar-T and SearchQA ) . If we can retrieve the question related context from the book by an information retriever , our model ( a reader ) is able to extract the answer from the context . In this work , we mainly focus on improving the reader part instead of information retriever , similar to BigBird and ETC works . Q6 : From Table 2 , it seems the more the number of clusters , the better the performance . Why do you stop at 512 ? Is this have something to do with computational efficiency ? A6 : We had experiments on 1024 clusters and it will not further boost the performance . As shown in Table 1 , the median number of tokens in context for different datasets are 2.8k , 2.5k , 6.3k which are not too long . Too many clusters can not significantly boost performance ."}, {"review_id": "VyENEGiEYAQ-2", "review_text": "* * Summary : * * This paper introduces the ClusterFormer , a transformer architecture that scales gracefully to long sequences by restricting pairwise attention by the cluster assignments of the hidden states of input tokens . The paper presents strong empirical results on question answering benchmark datasets outperform state-of-the-art approaches as well as strong baselines introduced by the authors . Summary of review : Strong empirical results on question answering datasets ; interesting data-driven efficient transformer model ; further clarification on relationship to related work needed ; experimental results would be stronger with more analysis of the proposed method . * * Strengths : * * The all pairs self attention component of transformers limits their scalability to long sequences . This paper presents a model that is reduces the complexity by grouping related tokens into clusters , such that self-attention is applied only within each cluster . In particular , a long sequence is first encoded using a sliding window style approach , then these sliding window representations are clustered and the resulting cluster memberships determine the sparsity for the remaining layers of the transformer . The approach appears to work quite well on question answering datasets for which the approach achieves state-of-the-art results on three datasets . The paper is well written and the presentation is very clear . * * Weaknesses : * * * * Relationship to related work : * * The proposed approach appears to share many similarities to the Routing Transformer ( Roy et al , 2020 ) . While both approaches from this year , I think that it would be important to present the similarities and differences of the two approaches ( i.e.sliding windows , way k-means centers are updated , etc ) clearly in this paper . Other related , though more distinct , ideas are used in the inducing point based variant of Set Transformers ( Lee et al , 2019 ) . * * Empirical Analysis of Scaling to Long Sequences : * * I think the presentation of the paper would be improved if the authors demonstrated just how much computation is saved by using these sparse , cluster-based attention layers . It would also improve the presentation to compare the efficiency of the proposed approach to other methods at varying input length sizes . Similarly , it would be interesting to show the performance of the proposed approach compared to baselines for varying maximum sequence lengths . It would further be interesting to investigate the cluster centers discovered by the method , what they represent , and how they change over time . This would be particularly important to analyze how the model picks up information across long sequences ( i.e. , showing that clusters are not made up of tokens from the same sliding window ) . * * Details of k-means * * : Apologies if I 've missed this , but is anything done to ensure that the cluster sizes produced by k-means are relatively balanced ? The skew of these sizes will directly impact the scalability of the method ? Further , while it is implied by the method/text , it would be nice to describe how the gradient is calculated for this hard cluster assignment . Aurko Roy , Mohammad Saffar , Ashish Vaswani , David Grangier . Efficient Content-Based Sparse Attention with Routing Transformers . First posted March 2020. https : //arxiv.org/abs/2003.05997 Juho Lee , Yoonho Lee , Jungtaek Kim , Adam R. Kosiorek , Seungjin Choi , Yee Whye The . Set Transformer : A Framework for Attention-based Permutation-Invariant Neural Networks . ICML 2019. http : //proceedings.mlr.press/v97/lee19d/lee19d.pdf * * Questions for the authors : * * \u2022 Please see questions in the details of k-means section .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your encouraging and insightful comments . We have updated the draft with the modifications in blue . Below , we provide detailed responses to your questions . Q1 : Relationship to related work . A1 : Thanks a lot for your valuable suggestions ! For the comparison to Routing Transformer , please refer to the answer for Reviewer 1 . And for Set Transformers , it is more like a memory and low-rank based method by projecting the key and value vectors from attention mechanism into low dimension space . We also think it is different from our work . Besides , one major problem of these frameworks is that they can not fully make use of existing pretraining models , such as BERT , RoBERTa , ALBERT . While our model is more general and can be easily initialized by general Transformer frameworks to target on SOTA of different tasks . We have added further analysis of the related works in the updated draft . Q2 : Empirical Analysis of Scaling to Long Sequences . A2 : 1 ) As the classic Transformer can not encode long sequences with 5K tokens , we can not make a fair empirical comparison of encoding time . And we would like to say sliding windows is a necessary step to encode long sequences by reducing complexity from O ( $ n^2 $ ) to O ( nl ) , where n is the sequence length and l is the window size . The complexity of our method is also O ( nl ) . 2 ) For the experiments of running on varying input length sizes , as we focus more on the question answering tasks and the answer may not appear in the context with short sequence . It is difficult to say whether the performance changes come from lower answer recall or the ability of long dependency detection . The more context we have , the better performance we can achieve for OpenQA ( like Quasar-T and SearchQA ) has been proved by other works , such as Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering . 3 ) For the clustering details , as shown in the last row of Table 6 , we find that states in long distance , such as the 50-th and 6060-th states ( over 6000 tokens apart ) , can be in one cluster , which demonstrates the ability of Cluster-Former in detecting long-range dependencies . Q3 : Details of k-means A3 : Our K-Means is more like first sorting the hidden states and then chunking them , so that it will not have the unbalance issue . As shown in Algorithm 1 , we also greedily sort the cluster centers , so that the hidden states with closer cluster ids are also similar to each other ."}, {"review_id": "VyENEGiEYAQ-3", "review_text": "The paper proposes ClusterFormer to address the problem of quadratic compute requirements of the attention mechanism in a Transformer model . To this end this paper proposes to combine local attention to promote local consistency and proposes KMeans clustering to gather global information for every token . The paper establishes strong results on the long form question answering task of Natural Questions in an extractive setup , with it getting the leaderboard position ahead of ETC-large . While the idea in the paper is natural and the results on NQ are strong , unfortunately the idea in the paper is not new and has already been introduced in the work `` Efficient Content-based Sparse Attention with Routing Transformers '' [ 1 , 2 ] which the authors fail to cite or credit . Therefore , I recommend rejection . References : [ 1 ] https : //openreview.net/forum ? id=B1gjs6EtDr [ 2 ] https : //arxiv.org/abs/2003.05997", "rating": "2: Strong rejection", "reply_text": "Thank you for your insightful comments . We are sorry that we missed to cite Routing Transformer , which has been added in the revision . Below , we provide detailed comparison to Routing Transformer . Q : Comparison to Routing Transformer A : Although Routing Transformer also uses clustering-based method to build sparse attention , we are different regarding the following aspects : 1 ) Frameworks are different . Routing Transformer is based on the combination of local attention and routing attention , and focuses on Auto-regressive Sequence Modeling tasks . Our Cluster-Former combines sliding window and cluster-former layers , and we focus on Question Answering tasks . Our framework needs to take question into consideration whenever encoding sequences in sliding windows . 2 ) Initializations are different . Routing Transformer is trained from scratch and not clear how well it works by initializing with pre-trained models , especially the limitation of 512 position embeddings from pre-trained models , such as BERT , RoBERTa , ALBERT . We use sliding window layers to overcome this issue , and our Cluster-Former can be easily initialized by pretrained models . All of our experiments are based on RoBERTa . 3 ) Cluster centroids are updated in different ways . Routing Transformer updates each cluster centroid \u00b5 by an exponentially moving average of all the keys and queries assigned to it , $ \\mu \\leftarrow \\lambda \\mu + ( 1-\\lambda ) Q/2+ ( 1-\\lambda ) K/2 $ . Our Cluster-Former maintains a memory queue to save hidden states which are used to update cluster centroids by running KMeans periodically . As Routing Transformer and Cluster-Former work on different tasks , we haven \u2019 t found an easy way to run Routing Transformer on QA for fair comparison . We have added analysis of Routing Transformer in the updated draft and will try to re-implement Routing Transformer for comparison in the future work . Please do let us know if you think the above comparison is not reasonable ! Thanks a lot !"}], "0": {"review_id": "VyENEGiEYAQ-0", "review_text": "Summary : Cluster-former is the latest proposal for enabling transformers to deal with long input sequences . Such sequences are particularly problematic for problems like question answering , QA , ( or summarization ) , where the context can be arbitrarily long , and effectively open-ended when the setup includes a context retrieval component ( e.g. , as in OpenQA ) . Cluster-Former combines local information by encoding sequence chunks separately with a sliding window , then injects clustering layers , that use k-means to compute centroids to cluster hidden states and capture global information . The approach yields state-of-the-art , and top-of-leaderboard , results on Natural Questions ( long answers ) . This is great solid work , showing how clustering can be designed , implemented and used successfully , to capture long distance dependencies in sparsified self-attention models . This is a concrete and useful contribution in itself for the large community working on this type of architecture and related problems . At the same time the approach involves quite a bit of complexity which makes one wonder if the baselines could be more competitive given a comparable amount of fine tuning . At the same time , competitive solutions of different nature ( generative ) are being proposed that pose a concrete challenge to this type of architecture , which are not evaluated , but should be at least discussed . Pros - Solid proof of concept and reference to successfully implementing clustering in sparse attention . - Strong empirical results , particularly the Natural Questions \u2019 leaderboard for long answers . - Impressive amount of technical work , also with respect to reproducing results with other systems . - Notwithstanding the amount of work in this area , literature review and comparison seems adequate but I might have missed something . - Some qualitative analysis : which could be extended and articulated , in particular it would be interesting to understand where the long distance information helps ; e.g. , vs the sliding window approach and particularly vs LSH . Cons - One of the arguments for the paper is that it is not clear if related methods , like Reformer , can generalize to long sequences . However , in the evaluated implementation ( Table 2 ) LSH is not that much worse than k-means . In fact , even just the sliding window alone seems surprisingly competitive on all QA tasks . While being much simpler . I find the authors \u2019 effort to compare with all these related methods truly commendable . It seems natural to wonder how much more fine-tuning has gone into Cluster-Former compared to the simpler baselines , given its additional complexity . It would be important to discuss this aspect in more detail . - Given the recent work of generative readers : https : //arxiv.org/abs/2005.11401 , and particularly Izacard & Grave , ( FID , https : //arxiv.org/pdf/2007.01282.pdf ) it seems unclear that direct encoding is the only , or the best , option for dealing with long sequences , at least for QA . In particular , FID seems attractive due to its simplicity and capacity ( about twice as much as Cluster-Former it seems ) . The authors should discuss this work . It would be ideal , at some point , to compare directly by evaluating on OpenQA-NQ or by other means . Detailed feedback - Pleas define x , from x\\times d , right below Eq ( 1 ) . Num tokens in context ? - Scaler value/scalar value ? - It would be great to explain Eq ( 2 ) step by step for clarity . - What is the effect of the overlapping content size m-l ? And in general of parameters l and m. In particular , could this affect positively the performance of the simpler sliding window model ? - Why using cluster layers at only 2 fixed depths ? How does this parameter affect results ? - The max length is constrained to 5k ( 10k test ) due to memory constraints , can this be improved , how ? - How long did it take to train the leaderboard ( NQ ) entry system ? - Unclear what table 2 evaluates on , e.g. , for NQ , is this on the dev set ? Or a fraction of it ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your encouraging and insightful comments . We have updated the draft with the modifications in blue . Below , we provide detailed responses to your questions . Q1 : About comparison to baselines such as LSH , and how much more fine-tuning has gone into Cluster-Former compared to the simpler baselines . A1 : We really appreciate your acknowledgment of our efforts on baselines . Most of the baselines are either not open-sourced or can not be easily transferred to our tasks . To make a fair comparison , we re-implement LSH and Sparse Attention , and integrate them into our framework . We fix the sliding window layers for all the methods . To select which layers should be used for sliding windows , we only have a wide exploration of it on Quasar-T dataset based on Cluster-Former . Then we fix sliding windows layers for the other datasets , and only tune dropout from { 0.1 , 0.15 , 0.2 } for all the methods including baselines and report the best result . We have updated it in the draft . Q2 : Comparison to generative readers and FID A2 : Yes , we agree that generative reader is a good solution to overcome reading long sequences . However , it still depends on how well the retrievers are . As pointed out by FID , \u201c we show that the performance of our method significantly improves when the number of retrieved passages increases \u201d , it is still important to have a better reader to read longer context . The main reason we didn \u2019 t test our models on OpenQA-NQ is that the retrieved passages are not the same in different methods . Thus , to make a fair comparison with previous readers , we focus more on the datasets with fixed context . We will try to evaluate our model on OpenQA-NQ during the discussion or in the final version . Q3 : Pleas define x , from x\\times d , right below Eq ( 1 ) . Num tokens in context ? Scaler value/scalar value ? It would be great to explain Eq ( 2 ) step by step for clarity . A3 : Thank you for your suggestions ! We have modified them and rewritten the explanation for Eq ( 2 ) step by step in the updated draft . Q4 : What is the effect of the overlapping content size l-m ? A4 : We had experiments on ( l=256 , m=224 ) and ( l=256 , m=230 ) for sliding-window-only baseline on Quasar-T . There is no significant difference , and we select ( l=256 , m=224 ) for all our experiments . Q5 : Why using cluster layers at only 2 fixed depths ? How does this parameter affect results ? A5 : In Table 4 , we have a hyper-parameter search for using cluster layers . It also includes experiments with 3/4/5/6 fixed depths . And we select the best hyper-parameter for all the other datasets . Q6 : The max length is constrained to 5k ( 10k test ) due to memory constraints , can this be improved , how ? A6 : Yes , as our method doesn \u2019 t have the quadratic issue on sequence length , one solution is to map 24 layers to multi-GPUs , and another solution is to call checkpoint function , such as \u201c torch.utils.checkpoint.checkpoint \u201d from Pytorch , which does not save intermediate activations , and instead recomputes them in backward pass . However , both methods will make the encoding slower and we will try it to encode longer sequences in future work . Q7 : How long did it take to train the leaderboard ( NQ ) entry system ? A7 : It takes one day by using 8 V100 GPU . Q8 : Unclear what table 2 evaluates on , e.g. , for NQ , is this on the dev set ? Or a fraction of it ? A8 : For NQ , all the results including baselines are on the full dev set . And the other datasets are on the test set ."}, "1": {"review_id": "VyENEGiEYAQ-1", "review_text": "The paper describes a method to handle long documents for question answering . Most existing approaches use a sliding window approach , without communication between different sliding windows . Instead , they propose an approach that clusters individual vectors , and allows communication ( attention ) among the locations in the same cluster . I am not sure about the intuition behind this -- why would communicate between similar vectors more efficient than dissimilar or randomly chosen vectors ? Would the performance improve if you use a better clustering algorithm ? The authors do not provide much intuition on this either . I have a concern about comparison with locality sensitive hashing . The number of buckets used in locality sensitive hashing was 64 . And it 's clear that having more clusters help . And the comparison between # C=64 Cluster Former and Locality Sensitive Hashing is marginal -- less than one point on all measures . I am not sure the results are strong enough to support that clustering is better than random assignments . For a valid comparison , they should report the results with locality-sensitive hashing and 512 buckets . The paper evaluates on three QA datasets , as long as experiments on perplexity for language modeling and shows promising performances . Some clarifying questions : 1 ) could you specify a bit more on how do `` classify the mean values of the first hidden state of all the chunked sequences to identify whether the question has short / long answers or not '' ? 2 ) I 'm a bit confused with the experimental set up . For NQ , what 's the numbers in Table 2 ? Is it on the dev set , and the numbers on Table 3 are on the test set ? Please make it clear . 3 ) would this work on a really lengthy QA dataset such as narrativeQA ? 4 ) From Table 2 , it seems the more the number of clusters , the better the performance . Why do you stop at 512 ? Is this have something to do with computational efficiency ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your insightful comments . We have updated the draft with the modifications in blue . Below , we provide detailed responses to your questions . Q1 : Why would communicate between similar vectors more efficient than dissimilar or randomly chosen vectors ? Would the performance improve if you use a better clustering algorithm ? A1 : According to LSH in Reformer ( Kitaev et al. , 2020 ) , \u201c the queries are sorted by bucket number \u201d . In this way , if using randomly chosen vectors , one extreme case would be that most of the vectors are assigned to one or two buckets . In this case , sorting by bucket number will fail , as most vectors share a same number . By using K-Means , it automatically computes the bucket centroids which will follow the distribution of hidden states . It can make the model more robust and less likely to fail the sorting . Thus , we do agree that a better clustering algorithm will make the model better and more robust . While considering the speed of clustering , we mainly rely on K-Means from Nvidia . Q2 : Experiments of LSH with more buckets . A2 : The results of LSH with 512 buckets are also worse than our method after tuning the dropout rate . The following is about LSH best results on Quasar-T , SearchQA and NQ respectively : 53.4/63.5 , 67.2/74.6 , 76.0/56.7 , compared to our best results 54.0/63.9 , 68.0/75.1 , 76.5/57.1 . Q3 : How to `` classify the mean values of the first hidden state of all the chunked sequences to identify whether the question has short / long answers or not '' . A3 : For Natural Question , we first identify whether the question has short/long answers or not based on the mean values of the first hidden state of all the chunked sequences , $ \\frac { 1 } { K } \\sum_ { k=1 } ^ { K } \\mathbf { H } ^ { N } _k [ 0 ] $ , where $ K $ is the number of chunks and $ N $ is the number of layers . We have modified the description in the updated draft . Q4 : The experimental set up of NQ on Table 2 and 3 . A4 : All the results of NQ ( including baselines ) on Table 2 are based on dev set and Table 3 based on test set . We have modified the captions of the tables in the updated draft . Q5 : Would this work on a really lengthy QA dataset such as narrativeQA ? A5 : The problem of NarrativeQA is quite close to OpenQA setting ( like Quarsar-T and SearchQA ) . If we can retrieve the question related context from the book by an information retriever , our model ( a reader ) is able to extract the answer from the context . In this work , we mainly focus on improving the reader part instead of information retriever , similar to BigBird and ETC works . Q6 : From Table 2 , it seems the more the number of clusters , the better the performance . Why do you stop at 512 ? Is this have something to do with computational efficiency ? A6 : We had experiments on 1024 clusters and it will not further boost the performance . As shown in Table 1 , the median number of tokens in context for different datasets are 2.8k , 2.5k , 6.3k which are not too long . Too many clusters can not significantly boost performance ."}, "2": {"review_id": "VyENEGiEYAQ-2", "review_text": "* * Summary : * * This paper introduces the ClusterFormer , a transformer architecture that scales gracefully to long sequences by restricting pairwise attention by the cluster assignments of the hidden states of input tokens . The paper presents strong empirical results on question answering benchmark datasets outperform state-of-the-art approaches as well as strong baselines introduced by the authors . Summary of review : Strong empirical results on question answering datasets ; interesting data-driven efficient transformer model ; further clarification on relationship to related work needed ; experimental results would be stronger with more analysis of the proposed method . * * Strengths : * * The all pairs self attention component of transformers limits their scalability to long sequences . This paper presents a model that is reduces the complexity by grouping related tokens into clusters , such that self-attention is applied only within each cluster . In particular , a long sequence is first encoded using a sliding window style approach , then these sliding window representations are clustered and the resulting cluster memberships determine the sparsity for the remaining layers of the transformer . The approach appears to work quite well on question answering datasets for which the approach achieves state-of-the-art results on three datasets . The paper is well written and the presentation is very clear . * * Weaknesses : * * * * Relationship to related work : * * The proposed approach appears to share many similarities to the Routing Transformer ( Roy et al , 2020 ) . While both approaches from this year , I think that it would be important to present the similarities and differences of the two approaches ( i.e.sliding windows , way k-means centers are updated , etc ) clearly in this paper . Other related , though more distinct , ideas are used in the inducing point based variant of Set Transformers ( Lee et al , 2019 ) . * * Empirical Analysis of Scaling to Long Sequences : * * I think the presentation of the paper would be improved if the authors demonstrated just how much computation is saved by using these sparse , cluster-based attention layers . It would also improve the presentation to compare the efficiency of the proposed approach to other methods at varying input length sizes . Similarly , it would be interesting to show the performance of the proposed approach compared to baselines for varying maximum sequence lengths . It would further be interesting to investigate the cluster centers discovered by the method , what they represent , and how they change over time . This would be particularly important to analyze how the model picks up information across long sequences ( i.e. , showing that clusters are not made up of tokens from the same sliding window ) . * * Details of k-means * * : Apologies if I 've missed this , but is anything done to ensure that the cluster sizes produced by k-means are relatively balanced ? The skew of these sizes will directly impact the scalability of the method ? Further , while it is implied by the method/text , it would be nice to describe how the gradient is calculated for this hard cluster assignment . Aurko Roy , Mohammad Saffar , Ashish Vaswani , David Grangier . Efficient Content-Based Sparse Attention with Routing Transformers . First posted March 2020. https : //arxiv.org/abs/2003.05997 Juho Lee , Yoonho Lee , Jungtaek Kim , Adam R. Kosiorek , Seungjin Choi , Yee Whye The . Set Transformer : A Framework for Attention-based Permutation-Invariant Neural Networks . ICML 2019. http : //proceedings.mlr.press/v97/lee19d/lee19d.pdf * * Questions for the authors : * * \u2022 Please see questions in the details of k-means section .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your encouraging and insightful comments . We have updated the draft with the modifications in blue . Below , we provide detailed responses to your questions . Q1 : Relationship to related work . A1 : Thanks a lot for your valuable suggestions ! For the comparison to Routing Transformer , please refer to the answer for Reviewer 1 . And for Set Transformers , it is more like a memory and low-rank based method by projecting the key and value vectors from attention mechanism into low dimension space . We also think it is different from our work . Besides , one major problem of these frameworks is that they can not fully make use of existing pretraining models , such as BERT , RoBERTa , ALBERT . While our model is more general and can be easily initialized by general Transformer frameworks to target on SOTA of different tasks . We have added further analysis of the related works in the updated draft . Q2 : Empirical Analysis of Scaling to Long Sequences . A2 : 1 ) As the classic Transformer can not encode long sequences with 5K tokens , we can not make a fair empirical comparison of encoding time . And we would like to say sliding windows is a necessary step to encode long sequences by reducing complexity from O ( $ n^2 $ ) to O ( nl ) , where n is the sequence length and l is the window size . The complexity of our method is also O ( nl ) . 2 ) For the experiments of running on varying input length sizes , as we focus more on the question answering tasks and the answer may not appear in the context with short sequence . It is difficult to say whether the performance changes come from lower answer recall or the ability of long dependency detection . The more context we have , the better performance we can achieve for OpenQA ( like Quasar-T and SearchQA ) has been proved by other works , such as Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering . 3 ) For the clustering details , as shown in the last row of Table 6 , we find that states in long distance , such as the 50-th and 6060-th states ( over 6000 tokens apart ) , can be in one cluster , which demonstrates the ability of Cluster-Former in detecting long-range dependencies . Q3 : Details of k-means A3 : Our K-Means is more like first sorting the hidden states and then chunking them , so that it will not have the unbalance issue . As shown in Algorithm 1 , we also greedily sort the cluster centers , so that the hidden states with closer cluster ids are also similar to each other ."}, "3": {"review_id": "VyENEGiEYAQ-3", "review_text": "The paper proposes ClusterFormer to address the problem of quadratic compute requirements of the attention mechanism in a Transformer model . To this end this paper proposes to combine local attention to promote local consistency and proposes KMeans clustering to gather global information for every token . The paper establishes strong results on the long form question answering task of Natural Questions in an extractive setup , with it getting the leaderboard position ahead of ETC-large . While the idea in the paper is natural and the results on NQ are strong , unfortunately the idea in the paper is not new and has already been introduced in the work `` Efficient Content-based Sparse Attention with Routing Transformers '' [ 1 , 2 ] which the authors fail to cite or credit . Therefore , I recommend rejection . References : [ 1 ] https : //openreview.net/forum ? id=B1gjs6EtDr [ 2 ] https : //arxiv.org/abs/2003.05997", "rating": "2: Strong rejection", "reply_text": "Thank you for your insightful comments . We are sorry that we missed to cite Routing Transformer , which has been added in the revision . Below , we provide detailed comparison to Routing Transformer . Q : Comparison to Routing Transformer A : Although Routing Transformer also uses clustering-based method to build sparse attention , we are different regarding the following aspects : 1 ) Frameworks are different . Routing Transformer is based on the combination of local attention and routing attention , and focuses on Auto-regressive Sequence Modeling tasks . Our Cluster-Former combines sliding window and cluster-former layers , and we focus on Question Answering tasks . Our framework needs to take question into consideration whenever encoding sequences in sliding windows . 2 ) Initializations are different . Routing Transformer is trained from scratch and not clear how well it works by initializing with pre-trained models , especially the limitation of 512 position embeddings from pre-trained models , such as BERT , RoBERTa , ALBERT . We use sliding window layers to overcome this issue , and our Cluster-Former can be easily initialized by pretrained models . All of our experiments are based on RoBERTa . 3 ) Cluster centroids are updated in different ways . Routing Transformer updates each cluster centroid \u00b5 by an exponentially moving average of all the keys and queries assigned to it , $ \\mu \\leftarrow \\lambda \\mu + ( 1-\\lambda ) Q/2+ ( 1-\\lambda ) K/2 $ . Our Cluster-Former maintains a memory queue to save hidden states which are used to update cluster centroids by running KMeans periodically . As Routing Transformer and Cluster-Former work on different tasks , we haven \u2019 t found an easy way to run Routing Transformer on QA for fair comparison . We have added analysis of Routing Transformer in the updated draft and will try to re-implement Routing Transformer for comparison in the future work . Please do let us know if you think the above comparison is not reasonable ! Thanks a lot !"}}