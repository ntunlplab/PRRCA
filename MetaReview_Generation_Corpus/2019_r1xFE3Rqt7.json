{"year": "2019", "forum": "r1xFE3Rqt7", "title": "Adaptive Mixture of Low-Rank Factorizations for Compact Neural Modeling", "decision": "Reject", "meta_review": "The paper is clearly written and well motivated, but there are remaining concerns on contributions and comparisons.\n\nThe paper received mixed initial reviews. After extensive discussions, while the authors successfully clarified several important issues (such as computation efficiency w.r.t splitting) pointed out by Reviewer 4 (an expert in the field), they were not able to convince him/her about the significance of the proposed network compression method. \n\nReviewer 4 has the following remaining concerns:\n\n1) This is a typical paper showing only FLOPs reduction but with an intent of real-time acceleration. However, wall-clock speedup is different from FLOPs reduction. It may not be beneficial to change the current computing flow optimized in modern software/hardware. This is one of major reasons why the reported wall-clock time even slows down. The problem may be alleviated with optimization efforts on software or hardware, then it is unclear how good/worse will it be when compared with fine-grain pruning solutions (Han et al. 2015b, Han et al. 2016 & Han et al. 2017), which achieved a higher FLOP reduction and a great wall-clock speedup with hardware optimized (using ASIC and FPGA);\n\n2) If it is OK to target on FLOPs reduction (without comparison with fine-grain pruning solutions), \n  2.1) In LSTM experiments, the major producer of FLOPs -- the output layer, is excluded and this exclusion was hidden in the first version. Although the author(s) claimed that an output layer could be compressed, it is not shown in the paper. Compressing output layer will reduce model capacity, making other layers more difficult being compressed. \n  2.2) In CNN experiments, the improvements of CIFAR-10 is within a random range and not statistically significant. In table 2, \"Regular low-rank MobileNet\" improves the original MobileNet, showing that the original MobileNet (an arXiv paper) is not well designed. \"Adaptive Low-rank MobileNet\" improves accuracy upon \"Regular low-rank MobileNet\", but using 0.3M more parameters. The trade-off is unclear.\n\nIn addition to these remaining concerns of Reviewer 4, the AC feels that the paper essentially modifies the original network structure in a very specific way: adding a particular nonlinear layer between two adjacent layers. Thus it seems a little bit unfair to mainly use low-rank factorization (which can be considered as a compression technique that barely changes the network architecture) for comparison. Adding comparisons with fine-grain pruning solutions (Han et al. 2015b, Han et al. 2016 & Han et al. 2017) and a large number of more recent related references inspired by the low-rank baseline (M. Jaderberg et al 2014) , as listed by Reviewer 4, will make the proposed method much more convincing. ", "reviews": [{"review_id": "r1xFE3Rqt7-0", "review_text": "In this paper, the authors propose a compression technique to reduce the number of parameters to learn in a neural network without losing expressiveness. The paper nicely introduces the problem of lack in espressiveness with low-rank factorizations, a well-known technique to reduce the number of parameters in a network. The authors propose to use a linear combination of low-rank factorizations with coefficients adaptively computed on data input. Through a nice toy example based on XNOR data, they provide a good proof of concept showing that the accuracy of the proposed technique outperforms the classical low-rank approach. I enjoyed reading the paper, which gives an intuitive line of reasoning providing also extensive experimental results on multilayer perceptron, convolutional neural networks and recurrent neural networks as well. The proposal is based on an intuitive line of reasoning with no strong theoretical founding. However, they provide a quick theoretical result in the appendix (Proposition 1) but, I couldn\u2019t understand very well its implications on the expressiveness of proposed method against classical low-rank approach. ", "rating": "7: Good paper, accept", "reply_text": "We would like to thank the reviewer for the time and valuable feedback . We also like to add on the implication of proposition 1 , which demonstrates that the mixture of low-rank factorizations are actually learned non-linear transformation , which is more expressive than the linear one of regular low-rank factorization , i.e.the former can not be approximated by the latter ."}, {"review_id": "r1xFE3Rqt7-1", "review_text": "The paper proposes an input-dependent low rank approximations of weight matrices in neural nets with a goal of accelerating inference. Instead of decomposing a matrix (a layer) to a low rank space like previous work did, the paper proposes to use a linear combination/mixture of multiple low rank spaces. The linear combination is dynamic and data dependent, making it some sense of non-linear combination. The paper is interesting, however, I doubt its significance at three aspects: (1) computation efficiency: the primary motivation of this paper is to accelerate inference stage; however, it might not be wise to break computation in a single low-rank space to segments in multiple low-rank spaces. In the original low-rank approximation, only two matrix-vector multiplications are needed, but this paper increases it to 2*K plus some additional overheads. Although the theoretical FLOPs can be cut, but modern hardware runs much faster when a whole bulk of data are computed all together. Because of this, the primary motivation in this paper wasn't successfully supported by wall-clock time; (2) low-rank approximation: low-rank approximation only makes sense when matrices are known and redundant, otherwise, no approximation target exists (i.e., what matrix is it approximating?). Because of this, low-rank neural nets [1][2] start from trained models, approximate it and fine-tune it, while this method trains from scratch without an approximation target. Although, we can fit the method to approximate trained matrices, then decomposing a matrix to a mixture of low-rank spaces is equivalent to decomposing to one single low-rank space (the only difference is the combination is data dependent). Therefore, I view this paper more in a research line of designing compact neural nets, which brings me to a concern in (3). (3) efficient architecture design: essentially, the paper proposes a class of compact neural nets, at each layer of which there are K \"low-rank\" branches with a gating mechanism to select those branches. However, branching and gating is not new [3][4]. I like the results in Table 2, but, to show the efficiency, it should have been compared with more SOTA compact models like CondenseNet. Clarity: How FLOPs reduction are exactly calculated? I am not convinced by FLOPs reduction in the LSTM experiments, since in LSTM (especially in language modeling), FLOPs in the output layer are large because of a large vocabulary size (650x10000 in the experiments). However, output layer is not explicitly mentioned in the paper. Improvement: (1) Accuracy improvement in Table 1 is not statistically significant, but used more parameters. For example, an improvement of 93.01% over 92.92% is within an effect of training noise; (2) It is a little hacking to conclude that a random matrix P_random has a small storage size because we can storage a seed for recovery. When we deploy models across platforms, we cannot guarantee they use the same random generator and has a consistent implementation; (3) sparse gating of low-rank branches may make this method more computation efficient. [1] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in Neural Information Processing Systems (NIPS). 2014. [2] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference (BMVC), 2014. [3] Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. \"Going deeper with convolutions.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9. 2015. [4] Huang, Gao, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q. Weinberger. \"Multi-scale dense networks for resource efficient image classification.\" arXiv preprint arXiv:1703.09844 (2017).", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the time and detailed and valuable comments . Please find our response below . - computation efficiency : in the original low-rank approximation , only two matrix-vector multiplications are needed , but this paper increases it to 2 * K plus some additional overheads . Although the theoretical FLOPs can be cut , but modern hardware runs much faster when a whole bulk of data are computed all together . Thanks for the detailed analysis . We would like to point out that the reviewer \u2019 s analysis is based on a specific type of implementation of our method , where K matrix-vector multiplications are conducted . However , this implementation can be easily optimized ( especially when we set K to rank where the rank is small ) : we can still use two matrix-vector multiplications , with a smaller matrix-vector multiplications to compute the mixing weights , and a vector-vector multiplication for weighting bottleneck . This avoids 2 * K matrix multiplications . The proposed implementation supports massive parallel computing and also has good data locality , thus it is very efficient to compute . We willingly admit that the current paper mainly focus the FLOPs , as the actual inference time depends on implementation , and also include runtime by other factors ( such as final softmax layer in RNN language modeling ) . Nevertheless , we conducted some preliminary experiments with non-optimized implementation on RNN ( without final softmax ) , and measure the actual inference time with CPU as shown in the table below . We observed that the proposed method still provides similar speedup as regular low-rank ( while getting significantly better accuracy/perplexity ) . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Method | low-rank ratio ; time in ms -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - | 1 | 1/2 | 1/4 | 1/8 | 1/16 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Full Rank | 10.8 | N/A | N/A | N/A | N/A regular LR | N/A | 13.1 | 6.6 | 3.3 | 2.0 adaptive LR | N/A | 16.5 | 8.6 | 4.5 | 2.8 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - We would like to improve our implementation of the adaptive low-rank to further speed up the actual inference time , and conduct more run-time comparisons in the future . - low-rank approximation : decomposing a matrix to a mixture of low-rank spaces is equivalent to decomposing to one single low-rank space ( the only difference is the combination is data dependent ) . While we appreciate that the reviewer \u2019 s careful observation , especially on that our method can also be applied to the scenarios where the weight matrices are pre-trained . But we would like to point out a inaccurate assertion that our method is equivalent to regular low-rank factorization in [ 1 ] [ 2 ] . Essentially , as shown in proposition 1 , our method is non-linear transformation while the regular low-rank is linear . They are not equivalent theoretically , and in practice , as explicitly demonstrated in the toy example ( Figure 1 ) , the proposed method is much more expressive than regular low-rank , meaning it can approximate better with around the same amount of parameters and computation . Our experiments on RNN and CNN also show that adaptive low-rank enjoys better expressiveness than conventional low-rank decomposition . - efficient architecture design : branching and gating is not new [ 3 ] [ 4 ] . I like the results in Table 2 , but , to show the efficiency , it should have been compared with more SOTA compact models like CondenseNet . Thanks for the multi-angular analysis . In our understanding , [ 3 ] proposes GoogleNet using static branching , and [ 4 ] proposes a dynamic network based on input samples . These two papers are orthogonal to our method . We aim to propose an simple yet expressive low-rank decomposition method to speed up the inference of matrix multiplication , which is the fundamental operation in modern neural networks . Therefore , we can apply the method to any existing network architectures . Regarding empirical evaluation , we aim to show our adaptive low-rank is better than regular low-rank method in our experiments , so we used a very similar yet powerful architecture MobileNet to exclude the interference of other factors , making sure it is a fair comparison . We aim to improve low-rank decomposition itself that is general but not to design a specific compact network architecture ."}, {"review_id": "r1xFE3Rqt7-2", "review_text": "Some suggested improvements follow below 1. It is claimed (page 2, last paragraph) that the proposed method leads to a 3.5% and 2.5% improvement in top-1 accuracy over the mobilenet v1 and v2 models. However the results in table 2 indicate 2.5% and 1.4% improvement. This should be corrected. 2. The authors should include the performance of the full rank CNN for the toy example in Figure 1. A Neural Net with 2 neurons in the hidden layer can not learn the XOR/XNOR efficiently . So its rank-1 factorization can only perform as good as the original CNN. 3. In (1), the dimensions of U^k and V^k should be mentioned explicitly. 4. The choice of \u201ck\u201d in (1) should be discussed. How does it relate to the overall accuracy / compression of the CNN? 5. The paper addresses low rank factorization for \u201cMLP\u201d, RNN/LSTM and \u201cpointwise\u201d convolutions. All of these have weights in the form of matrices (mode 2 tensors). The extension to mode-3 and and mode-4 tensors which are more common in CNNs is not straightforward. 6. In the imagenet experiment, the number of mixtures (k) is set to the rank (d). How is the rank computed for every layer? 7. In Fig 7, row 0 and row 8 look identical. Is this indicative of something?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the time and valuable comments . Please find our response below . - The results in table 2 indicate 2.5 % and 1.4 % improvement . This should be corrected . Thanks for pointing out our typos on the improvement rates . The correct ones should be ( 1 ) ( 70.5-68.8 ) /68.8=2.5 % and ( 2 ) ( 73.1-71.7 ) /71.7=1.95 % . We will update them in the revision . - The authors should include the performance of the full rank CNN for the toy example in Figure 1 . A Neural Net with 2 neurons in the hidden layer can not learn the XOR/XNOR efficiently . So its rank-1 factorization can only perform as good as the original CNN . We \u2019 d like to clarify the toy example in Figure 1 , the input data point is 2-dimensional , and a MLP ( not CNN ) is used as function to classify the labels , i.e.P ( y|x ) = softmax ( W \u2019 \u03c3 ( Wx ) ) , where W \u2208 R^ { 2\u00d72 } . This is the original full rank model , which is able to effectively learn the synthetic XOR/XNOR task . However , when we factorize W using two 2\u00d71 matrices , i.e.W = UV^T , the induced linear bottleneck largely degenerates the performance ( Figure 1b ) . After applying the proposed method , the performance can be largely improved ( Figure 1c ) . - In ( 1 ) , the dimensions of U^k and V^k should be mentioned explicitly . Thanks for the suggestion , we will mention it in the revision . - The choice of \u201c k \u201d in ( 1 ) should be discussed . How does it relate to the overall accuracy / compression of the CNN ? The discussion of k is presented in the experiments . We tested different K , and found that using more mixtures generally leads to better results , although the performance starts to plateau when the number of mixtures is large enough . However , to obtain a larger compression rate and speedup , the rank-d we use in the low-rank factorization can be already small , thus the extras of using different number of mixtures may not differ too much . For this reason , we can just set K as rank-d. - The extension to mode-3 and and mode-4 tensors which are more common in CNNs is not straightforward . Thanks for the questions . We willingly acknowledge that we only considered low-rank factorization of 2d matrices in the scope of this work , which has already found applications in many deep neural network scenarios . For CNNs ( which was targeted in this work ) , we apply our method with widely-used compact depth-separable convolution layers such that it does not require a direct mode-3 tensor factorization . We also believe it could be straightforward to extend this framework to high-order tensor factorization with minor adjustments . For example , we could apply our method to CP decomposition ( https : //en.wikipedia.org/wiki/Tensor_rank_decomposition ) , simply by extending each mixture from two low-rank vector products to three low-rank vector products . - In the imagenet experiment , the number of mixtures ( k ) is set to the rank ( d ) . How is the rank computed for every layer ? To ensure a fair comparisons with MobielNets , we simply followed the setting in the original MobileNetV2 paper by setting the number of channel of bottleneck to be \u2159 of the output channels . - In Fig 7 , row 0 and row 8 look identical . Is this indicative of something ? In Fig 7 , the labels for each row are in the same order as in original CIFAR-10 dataset , namely `` airplane , automobile , bird , cat , deer , dog , frog , horse , ship , truck '' . The row 0 and the row 8 correspond to the class of airplane and the class of ship respectively , which suggests the learned mixtures are class discriminative ."}], "0": {"review_id": "r1xFE3Rqt7-0", "review_text": "In this paper, the authors propose a compression technique to reduce the number of parameters to learn in a neural network without losing expressiveness. The paper nicely introduces the problem of lack in espressiveness with low-rank factorizations, a well-known technique to reduce the number of parameters in a network. The authors propose to use a linear combination of low-rank factorizations with coefficients adaptively computed on data input. Through a nice toy example based on XNOR data, they provide a good proof of concept showing that the accuracy of the proposed technique outperforms the classical low-rank approach. I enjoyed reading the paper, which gives an intuitive line of reasoning providing also extensive experimental results on multilayer perceptron, convolutional neural networks and recurrent neural networks as well. The proposal is based on an intuitive line of reasoning with no strong theoretical founding. However, they provide a quick theoretical result in the appendix (Proposition 1) but, I couldn\u2019t understand very well its implications on the expressiveness of proposed method against classical low-rank approach. ", "rating": "7: Good paper, accept", "reply_text": "We would like to thank the reviewer for the time and valuable feedback . We also like to add on the implication of proposition 1 , which demonstrates that the mixture of low-rank factorizations are actually learned non-linear transformation , which is more expressive than the linear one of regular low-rank factorization , i.e.the former can not be approximated by the latter ."}, "1": {"review_id": "r1xFE3Rqt7-1", "review_text": "The paper proposes an input-dependent low rank approximations of weight matrices in neural nets with a goal of accelerating inference. Instead of decomposing a matrix (a layer) to a low rank space like previous work did, the paper proposes to use a linear combination/mixture of multiple low rank spaces. The linear combination is dynamic and data dependent, making it some sense of non-linear combination. The paper is interesting, however, I doubt its significance at three aspects: (1) computation efficiency: the primary motivation of this paper is to accelerate inference stage; however, it might not be wise to break computation in a single low-rank space to segments in multiple low-rank spaces. In the original low-rank approximation, only two matrix-vector multiplications are needed, but this paper increases it to 2*K plus some additional overheads. Although the theoretical FLOPs can be cut, but modern hardware runs much faster when a whole bulk of data are computed all together. Because of this, the primary motivation in this paper wasn't successfully supported by wall-clock time; (2) low-rank approximation: low-rank approximation only makes sense when matrices are known and redundant, otherwise, no approximation target exists (i.e., what matrix is it approximating?). Because of this, low-rank neural nets [1][2] start from trained models, approximate it and fine-tune it, while this method trains from scratch without an approximation target. Although, we can fit the method to approximate trained matrices, then decomposing a matrix to a mixture of low-rank spaces is equivalent to decomposing to one single low-rank space (the only difference is the combination is data dependent). Therefore, I view this paper more in a research line of designing compact neural nets, which brings me to a concern in (3). (3) efficient architecture design: essentially, the paper proposes a class of compact neural nets, at each layer of which there are K \"low-rank\" branches with a gating mechanism to select those branches. However, branching and gating is not new [3][4]. I like the results in Table 2, but, to show the efficiency, it should have been compared with more SOTA compact models like CondenseNet. Clarity: How FLOPs reduction are exactly calculated? I am not convinced by FLOPs reduction in the LSTM experiments, since in LSTM (especially in language modeling), FLOPs in the output layer are large because of a large vocabulary size (650x10000 in the experiments). However, output layer is not explicitly mentioned in the paper. Improvement: (1) Accuracy improvement in Table 1 is not statistically significant, but used more parameters. For example, an improvement of 93.01% over 92.92% is within an effect of training noise; (2) It is a little hacking to conclude that a random matrix P_random has a small storage size because we can storage a seed for recovery. When we deploy models across platforms, we cannot guarantee they use the same random generator and has a consistent implementation; (3) sparse gating of low-rank branches may make this method more computation efficient. [1] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in Neural Information Processing Systems (NIPS). 2014. [2] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference (BMVC), 2014. [3] Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. \"Going deeper with convolutions.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9. 2015. [4] Huang, Gao, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q. Weinberger. \"Multi-scale dense networks for resource efficient image classification.\" arXiv preprint arXiv:1703.09844 (2017).", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the time and detailed and valuable comments . Please find our response below . - computation efficiency : in the original low-rank approximation , only two matrix-vector multiplications are needed , but this paper increases it to 2 * K plus some additional overheads . Although the theoretical FLOPs can be cut , but modern hardware runs much faster when a whole bulk of data are computed all together . Thanks for the detailed analysis . We would like to point out that the reviewer \u2019 s analysis is based on a specific type of implementation of our method , where K matrix-vector multiplications are conducted . However , this implementation can be easily optimized ( especially when we set K to rank where the rank is small ) : we can still use two matrix-vector multiplications , with a smaller matrix-vector multiplications to compute the mixing weights , and a vector-vector multiplication for weighting bottleneck . This avoids 2 * K matrix multiplications . The proposed implementation supports massive parallel computing and also has good data locality , thus it is very efficient to compute . We willingly admit that the current paper mainly focus the FLOPs , as the actual inference time depends on implementation , and also include runtime by other factors ( such as final softmax layer in RNN language modeling ) . Nevertheless , we conducted some preliminary experiments with non-optimized implementation on RNN ( without final softmax ) , and measure the actual inference time with CPU as shown in the table below . We observed that the proposed method still provides similar speedup as regular low-rank ( while getting significantly better accuracy/perplexity ) . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Method | low-rank ratio ; time in ms -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - | 1 | 1/2 | 1/4 | 1/8 | 1/16 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Full Rank | 10.8 | N/A | N/A | N/A | N/A regular LR | N/A | 13.1 | 6.6 | 3.3 | 2.0 adaptive LR | N/A | 16.5 | 8.6 | 4.5 | 2.8 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - We would like to improve our implementation of the adaptive low-rank to further speed up the actual inference time , and conduct more run-time comparisons in the future . - low-rank approximation : decomposing a matrix to a mixture of low-rank spaces is equivalent to decomposing to one single low-rank space ( the only difference is the combination is data dependent ) . While we appreciate that the reviewer \u2019 s careful observation , especially on that our method can also be applied to the scenarios where the weight matrices are pre-trained . But we would like to point out a inaccurate assertion that our method is equivalent to regular low-rank factorization in [ 1 ] [ 2 ] . Essentially , as shown in proposition 1 , our method is non-linear transformation while the regular low-rank is linear . They are not equivalent theoretically , and in practice , as explicitly demonstrated in the toy example ( Figure 1 ) , the proposed method is much more expressive than regular low-rank , meaning it can approximate better with around the same amount of parameters and computation . Our experiments on RNN and CNN also show that adaptive low-rank enjoys better expressiveness than conventional low-rank decomposition . - efficient architecture design : branching and gating is not new [ 3 ] [ 4 ] . I like the results in Table 2 , but , to show the efficiency , it should have been compared with more SOTA compact models like CondenseNet . Thanks for the multi-angular analysis . In our understanding , [ 3 ] proposes GoogleNet using static branching , and [ 4 ] proposes a dynamic network based on input samples . These two papers are orthogonal to our method . We aim to propose an simple yet expressive low-rank decomposition method to speed up the inference of matrix multiplication , which is the fundamental operation in modern neural networks . Therefore , we can apply the method to any existing network architectures . Regarding empirical evaluation , we aim to show our adaptive low-rank is better than regular low-rank method in our experiments , so we used a very similar yet powerful architecture MobileNet to exclude the interference of other factors , making sure it is a fair comparison . We aim to improve low-rank decomposition itself that is general but not to design a specific compact network architecture ."}, "2": {"review_id": "r1xFE3Rqt7-2", "review_text": "Some suggested improvements follow below 1. It is claimed (page 2, last paragraph) that the proposed method leads to a 3.5% and 2.5% improvement in top-1 accuracy over the mobilenet v1 and v2 models. However the results in table 2 indicate 2.5% and 1.4% improvement. This should be corrected. 2. The authors should include the performance of the full rank CNN for the toy example in Figure 1. A Neural Net with 2 neurons in the hidden layer can not learn the XOR/XNOR efficiently . So its rank-1 factorization can only perform as good as the original CNN. 3. In (1), the dimensions of U^k and V^k should be mentioned explicitly. 4. The choice of \u201ck\u201d in (1) should be discussed. How does it relate to the overall accuracy / compression of the CNN? 5. The paper addresses low rank factorization for \u201cMLP\u201d, RNN/LSTM and \u201cpointwise\u201d convolutions. All of these have weights in the form of matrices (mode 2 tensors). The extension to mode-3 and and mode-4 tensors which are more common in CNNs is not straightforward. 6. In the imagenet experiment, the number of mixtures (k) is set to the rank (d). How is the rank computed for every layer? 7. In Fig 7, row 0 and row 8 look identical. Is this indicative of something?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the time and valuable comments . Please find our response below . - The results in table 2 indicate 2.5 % and 1.4 % improvement . This should be corrected . Thanks for pointing out our typos on the improvement rates . The correct ones should be ( 1 ) ( 70.5-68.8 ) /68.8=2.5 % and ( 2 ) ( 73.1-71.7 ) /71.7=1.95 % . We will update them in the revision . - The authors should include the performance of the full rank CNN for the toy example in Figure 1 . A Neural Net with 2 neurons in the hidden layer can not learn the XOR/XNOR efficiently . So its rank-1 factorization can only perform as good as the original CNN . We \u2019 d like to clarify the toy example in Figure 1 , the input data point is 2-dimensional , and a MLP ( not CNN ) is used as function to classify the labels , i.e.P ( y|x ) = softmax ( W \u2019 \u03c3 ( Wx ) ) , where W \u2208 R^ { 2\u00d72 } . This is the original full rank model , which is able to effectively learn the synthetic XOR/XNOR task . However , when we factorize W using two 2\u00d71 matrices , i.e.W = UV^T , the induced linear bottleneck largely degenerates the performance ( Figure 1b ) . After applying the proposed method , the performance can be largely improved ( Figure 1c ) . - In ( 1 ) , the dimensions of U^k and V^k should be mentioned explicitly . Thanks for the suggestion , we will mention it in the revision . - The choice of \u201c k \u201d in ( 1 ) should be discussed . How does it relate to the overall accuracy / compression of the CNN ? The discussion of k is presented in the experiments . We tested different K , and found that using more mixtures generally leads to better results , although the performance starts to plateau when the number of mixtures is large enough . However , to obtain a larger compression rate and speedup , the rank-d we use in the low-rank factorization can be already small , thus the extras of using different number of mixtures may not differ too much . For this reason , we can just set K as rank-d. - The extension to mode-3 and and mode-4 tensors which are more common in CNNs is not straightforward . Thanks for the questions . We willingly acknowledge that we only considered low-rank factorization of 2d matrices in the scope of this work , which has already found applications in many deep neural network scenarios . For CNNs ( which was targeted in this work ) , we apply our method with widely-used compact depth-separable convolution layers such that it does not require a direct mode-3 tensor factorization . We also believe it could be straightforward to extend this framework to high-order tensor factorization with minor adjustments . For example , we could apply our method to CP decomposition ( https : //en.wikipedia.org/wiki/Tensor_rank_decomposition ) , simply by extending each mixture from two low-rank vector products to three low-rank vector products . - In the imagenet experiment , the number of mixtures ( k ) is set to the rank ( d ) . How is the rank computed for every layer ? To ensure a fair comparisons with MobielNets , we simply followed the setting in the original MobileNetV2 paper by setting the number of channel of bottleneck to be \u2159 of the output channels . - In Fig 7 , row 0 and row 8 look identical . Is this indicative of something ? In Fig 7 , the labels for each row are in the same order as in original CIFAR-10 dataset , namely `` airplane , automobile , bird , cat , deer , dog , frog , horse , ship , truck '' . The row 0 and the row 8 correspond to the class of airplane and the class of ship respectively , which suggests the learned mixtures are class discriminative ."}}