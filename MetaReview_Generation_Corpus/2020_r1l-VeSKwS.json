{"year": "2020", "forum": "r1l-VeSKwS", "title": "SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing", "decision": "Reject", "meta_review": "I had a little bit of difficulty with my recommendation here, but in the end I don't feel confident in recommending this paper for acceptance, with my concerns largely boiling down to the lack of clear description of the overall motivation.\n\nStandard adversarial attacks are meant to be *imperceptible* changes that do not change the underlying semantics of the input to the human eye. In other words, the goal of the current work, generating \"semantically meaningful\" perturbations goes against the standard definition of adversarial attacks. This left me with two questions:\n\n1. Under the definition of semantic adversarial attacks, what is to prevent someone from swapping out the current image with an entirely different image? From what I saw in the evaluation measures utilized in the paper, such a method would be judged as having performed a successful attack, and given no constraints there is nothing stopping this.\n\n2. In what situation would such an attack method would be practically useful?\n\nEven the reviewers who reviewed the paper favorably were not able to provide answers to these questions, and I was not able to resolve this from my reading of the paper as well. I do understand that there is a challenge on this by Google. In my opinion, even this contest is somewhat ill-defined, but it also features extensive human evaluation to evaluate the validity of the perturbations, which is not featured in the experimental evaluation here.\n\nWhile I think this work is potentially interesting, it seems that there are too many open questions that are not resolved yet to recommend acceptance at this time, but I would encourage the authors to tighten up the argumentation/evaluation in this regard and revise the paper to be better accordingly!", "reviews": [{"review_id": "r1l-VeSKwS-0", "review_text": "The authors describe a method for adversarially modifying a given (test) example that 1) still retains the correct label on the example, but 2) causes a model to make an incorrect prediction on it. The novelty of their proposed method is that their adversarial modifications are along a provided semantic axis (e.g., changing the color of someone's skin in a face recognition task) instead of the standard $L_p$ perturbations that the existing literature has focused on (e.g., making a very small change to each individual pixel). The adversarial examples that the authors construct, experimentally, are impressive and striking. I'd especially like to acknowledge the work that the authors put in to construct an anonymous link where they showcase results from their experiments. Thank you! Overall, I think that this is interesting work that can help to broaden the study of adversarial examples and make them more applicable even in non-adversarial settings (e.g., by making models more robust to the changes in semantic attributes that the authors consider). There has been quite a bit of interest in the community in adversarial examples that are not just $L_p$ perturbations, and I believe that the authors' approach will encourage a good deal of follow-up research. However, my main concern with the paper is that in my opinion, it does not sufficiently address why it is important to generate adversarial examples in the way they do. For example: 1) Is the argument that this is a more powerful attack surface, so adversaries should take note (and defenders should figure out how to defend against this)? If that is the case, what is the attack model under which these attacks are realistic? For example, the original $L_\\infty$ attacks are motivated in the sense that the adversarial examples are visually imperceptible, so they might not be noticed by the end-user. What is the equivalent argument for these semantic attacks? 2) Is the argument that these semantic attacks somehow capture a more realistic part of the data distribution over all natural images, and therefore it is good to have models that perform well on these semantic adversarial examples even if we're not concerned about an adversary (e.g., because the model might generalize better to other tasks or be more causally correct)? If that's the case, then I think this needs to be explored more. For example, what about the following straw man baseline: use a controllable semantic-attribute-based generator to generate semantically different images without any notion of an adversarial attack, and then do standard $L_p$ attacks on that generated image? How would that be better or worse than the proposed method? 3) Or is the argument that it is just good to be able to generate examples that models get wrong? If so, why, and why is this method better than other methods? I think the paper would be significantly stronger if the importance and implications of their work were explicated along the above lines. For this reason, my current assessment is a weak reject, though I'd be open to changing this assessment. === Less critical comments, no need to respond or fix right away === While the overall concept and approach was clear, I generally found the notation and mathematical exposition difficult to follow. Please be more precise. Here is a non-exhaustive list of examples from section 3: a) I'm not sure what's the difference between $x^\\text{tgt}$ and $x^\\text{adv}$, or between $x^\\text{new}$ and $x^*$. These seem to be used somewhat interchangeably? b) Equation 3 is the central optimization problem in the paper, and should be written out explicitly using $\\alpha$ as the optimization variable, instead of referring to equations 1 and 2 (in which $x^*$ doesn't even appear). c) I didn't understand equation 4. What does assuming $M(x^\\text{tgt}) = y^\\text{tgt}$ mean? What happens when that is not true? d) Equation 5: Why is $y$ in the right hand side by not in the left? e) Equation 6: $L_\\text{smooth}$ is missing an argument. ", "rating": "3: Weak Reject", "reply_text": "We appreciate the reviewer \u2019 s precious comments and suggestions . We thank the reviewer for recognizing our work as helpful to broaden the study of adversarial examples and encourage a good deal of follow-up research . We will first provide the high-level motivation of why we need to generate adversarial examples and then answer the individual questions . We have revised the notations and equations in our updated manuscript . Q1.Why it is important to generate adversarial examples in the way they do ? A1 : Thanks for the question , the reasons/motivations are described below . Deep Neural Networks ( DNNs ) have achieved great success in a variety of applications . However , various security threats are emerging with the deployment of machine learning models . Without a deep understanding of how neural networks fail under attacks , it would be concerning to apply them in security-critical systems such as face verification and autonomous driving systems . Additionally , learning systems are usually required to be immune to * reasonable variations * of the input . So far , such * variations * have been focused on imperceptible perturbation added to the given inputs whose magnitude is bounded by pixel-space $ L_p $ -norm . Some works have discussed the limitations of only measuring and evaluating the $ L_p $ bounded perturbation [ a1 , a3 , a4 ] . Therefore , it is important to explore other non- $ L_p $ bounded perturbation , especially semantically meaningful perturbation , and more detailed reasons are listed below . First , the semantic based perturbation is new and interesting , which contains different intrinsic properties compared with the traditional $ L_p $ bounded attacks . For instance , the semantic perturbation could be very large to cover the other side of $ L_p $ bounded perturbation . Second , in our proposed semantic based adversarial examples , we can explicitly control the desired editing attribute ( e.g.hair color ) , and successfully preserve the high perceptual quality of the generated images as shown in Figure 4 . This would help to explore the vulnerability/sensitivity of different semantic attributes . Third , various methods have been proposed to defend against adversarial attacks . Adversarial training based methods are currently the most efficient . Currently most adversarial training methods are only effective against a small set of seen attacks [ a1 ] , and researchers ( e.g. , Kang , et.al . [ a2 ] ) have shown that generating diverse attacks can help improve adversarial training performance against unseen attacks . Therefore , we believe that our semantic adversarial examples can potentially benefit adversarial training to improve model robustness by providing diverse unseen adversarial examples . In addition , partially based on the reasons above , Brown , et . al . [ a4 ] proposed the unrestricted adversarial example challenge to encourage the community to explore the adversarial space beyond $ L_p $ , which would potentially benefit the adversarial learning research , and we do hope SemanticAdv can contribute as well . ( To be continued . )"}, {"review_id": "r1l-VeSKwS-1", "review_text": "Summary: This paper proposes to generate \"unrestricted adversarial examples\" via attribute-conditional image editing. Their method, SemanticAdv, leverages disentangled semantic factors and interpolates feature-map with higher freedom than attribute-space. Their adversarial optimization objectives combine both attack effectiveness and interpolation smoothness. They conduct extensive experiments for several tasks compared with CW-attack, showing broad applicability of the proposed method. The paper is well written and technically sound with concrete experimental results. I'm glad to suggest accepting the paper. With the help of attribute-conditional StarGAN, SemanticAdv generates adversarial examples by interpolating feature-maps conditioned on attributes. They design adversarial optimization objectives with specific attack objectives for identity verification and structured prediction tasks. They provide experiments showing the effectiveness of SemanticAdv; analysis on attributes, attack transferability, black-box attack, and robustness against defenses; as well as user study with subjective. The qualitative results also look nice and the code base is open-sourced. A question out of curiosity, the last conv layer in the generator is used as the feature-map. How is the attack effectiveness of using other layers?", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the constructive suggestions and comments , and we have conducted additional experiments based on the comments . Q1.Effectiveness of using other layers ? We have tested another two feature maps ( $ f_ { 1 } $ , $ f_ { 2 } $ ) after the first/second up-sampling operations as shown in Table E ( see Section D in our appendix ) in the submitted paper ; and we also conducted additional experiments on two extra feature maps ( $ f_ { -2 } $ , $ f_ { -1 } $ ) based on the suggestions . $ f_ { -2 } $ indicates the first feature map after the last down-sampling operations and $ f_ { -1 } $ represents the feature map after $ f_ { -2 } $ . The full results are shown in the revision Table E and F. We also present the results as below . The result shows that samples generated by interpolating on our selected layer ( $ f_0 $ ) achieve the highest attack success rate . +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+ | T-FPR ( G-FPR ) | $ 10^ { \u22123 } ( 10^ { \u22123 } ) $ | $ 3\\times10^ { \u22123 } ( 3\\times10^ { \u22123 } ) $ | $ 10^ { \u22124 } ( 10^ { \u22124 } ) $ | +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+ | Layer ( f ) | $ f_ { -2 } $ | $ f_ { -1 } $ | $ f_0 $ | $ f_ { -2 } $ | $ f_ { -1 } $ | $ f_ { 0 } $ | $ f_ { -2 } $ | $ f_ { -1 } $ | $ f_ { 0 } $ | +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+ | Attack Success Rate | 49.4 | 92.09 | 99.29 | 30.44 | 81.87 | 97.35 | 6.66 | 45.46 | 76.64 | +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+"}, {"review_id": "r1l-VeSKwS-2", "review_text": "This paper proposes adversarial attacks by modifying semantic properties of the image. Rather than modifying low-level pixels, it modifies mid-level attributes. The authors show that the proposed method is effective and achieves stronger results than the pixel-level attack method (CW) in terms of attacking capability transferring to other architectures. Importantly, the authors show results on a variety of tasks, e.g. landmark detection and segmentation in addition to classification/identification. The most related work is Joshi 2019 and the authors show that the method used in that work (modification in attribute space) is inferior to modification in feature space still via attributes, as the authors proposed. However, I have a few comments and concerns: 1) The authors mention on page 3 they assume M is an oracle-- what is the impact of this? 2) The results in Table C don't look good-- the proposed method can *at best* (in a generous setup) equal the results of CW-- maybe I missed something but more discussion would be helpful. 3) Is there a way to evaluate the merits of semantic modification (beyond attack success) in addition to \"does it look reasonable\"? The authors mention attribute-based modifications are more practical, how can this be evaluated? If attribute-based attacks are better, is there a cost to this? How easy is it to make attribute-based attacks compared to low-level ones? 4) The authors mention that for their transferrability results, they \"select the successfully attacked...\" (page 7). What is the impact of this, as opposed to selecting non-successfully attacked samples? 5) Re: behavior with defense methods, is the advantage of the proposed method a matter of training the defense methods in a tailored way, so they're aware of attribute-based attacks?", "rating": "6: Weak Accept", "reply_text": "We really appreciate the reviewer \u2019 s precious comments . Sorry for the potential confusion . We would like to answer your questions as follows and we have added them in our revision . Q1 : assume M is an oracle -- what is the impact of this ? A1 : Thanks for pointing this out and we will remove this notation in the revision to avoid confusion . Basically , M here is used to obtain the corresponding label related to data x , and we actually do not need to use this assumption in our experiments ( we can assume the ground-truth label is given ) . But we see this assumption introducing the confusion and we will remove this statement by using the ground truth label y directly . Q2 : \u201c The results in Table C do n't look good. \u201d A2 : We believe the \u201c Table C don \u2019 t look good \u201d refers to the results with \u201c worst \u201d and \u201c average \u201d metrics . In Table C , the \u201c best \u201d metric of SemanticAdv should be served as a fair comparison to CW , where both methods achieve 100 % attack success rate . Therefore , our result is good . The detailed reasons are as follows . For each victim image , our SemanticAdv generates a total of 17 adversarial images by augmenting one semantic attribute each time ( e.g. , we have 17 attributes to manipulate ) . However , CW generates a single adversarial example regardless of attributes , which can be viewed as instance-level generation . Therefore , we compare CW with our SemanticAdv on the instance-level which corresponds to the \u201c best \u201d metric . In addition , we report the performance using the \u201c average \u201d and \u201c worst \u201d metric , which actually provides additional insights into the robustness of face verification models across different attributes . Combining the results from Table C in our appendix and Figure 3 , we understand that the face verification models used in our experiments have different levels of robustness across attributes . For example , face verification models are more robust against local shape variations than color variations , e.g. , pale skin has higher attack success rate than mouth open . We believe these discoveries will help the community further understand the properties of face verification models . To summarize , our * semantic * adversarial examples not only achieves attack success rate comparable to traditional $ L_p $ -norm bounded CW attacks , but also enables us to investigate the model robustness under different semantic attributes . We will make the description of Table C clearer in the revised manuscript . ( To be continued . )"}], "0": {"review_id": "r1l-VeSKwS-0", "review_text": "The authors describe a method for adversarially modifying a given (test) example that 1) still retains the correct label on the example, but 2) causes a model to make an incorrect prediction on it. The novelty of their proposed method is that their adversarial modifications are along a provided semantic axis (e.g., changing the color of someone's skin in a face recognition task) instead of the standard $L_p$ perturbations that the existing literature has focused on (e.g., making a very small change to each individual pixel). The adversarial examples that the authors construct, experimentally, are impressive and striking. I'd especially like to acknowledge the work that the authors put in to construct an anonymous link where they showcase results from their experiments. Thank you! Overall, I think that this is interesting work that can help to broaden the study of adversarial examples and make them more applicable even in non-adversarial settings (e.g., by making models more robust to the changes in semantic attributes that the authors consider). There has been quite a bit of interest in the community in adversarial examples that are not just $L_p$ perturbations, and I believe that the authors' approach will encourage a good deal of follow-up research. However, my main concern with the paper is that in my opinion, it does not sufficiently address why it is important to generate adversarial examples in the way they do. For example: 1) Is the argument that this is a more powerful attack surface, so adversaries should take note (and defenders should figure out how to defend against this)? If that is the case, what is the attack model under which these attacks are realistic? For example, the original $L_\\infty$ attacks are motivated in the sense that the adversarial examples are visually imperceptible, so they might not be noticed by the end-user. What is the equivalent argument for these semantic attacks? 2) Is the argument that these semantic attacks somehow capture a more realistic part of the data distribution over all natural images, and therefore it is good to have models that perform well on these semantic adversarial examples even if we're not concerned about an adversary (e.g., because the model might generalize better to other tasks or be more causally correct)? If that's the case, then I think this needs to be explored more. For example, what about the following straw man baseline: use a controllable semantic-attribute-based generator to generate semantically different images without any notion of an adversarial attack, and then do standard $L_p$ attacks on that generated image? How would that be better or worse than the proposed method? 3) Or is the argument that it is just good to be able to generate examples that models get wrong? If so, why, and why is this method better than other methods? I think the paper would be significantly stronger if the importance and implications of their work were explicated along the above lines. For this reason, my current assessment is a weak reject, though I'd be open to changing this assessment. === Less critical comments, no need to respond or fix right away === While the overall concept and approach was clear, I generally found the notation and mathematical exposition difficult to follow. Please be more precise. Here is a non-exhaustive list of examples from section 3: a) I'm not sure what's the difference between $x^\\text{tgt}$ and $x^\\text{adv}$, or between $x^\\text{new}$ and $x^*$. These seem to be used somewhat interchangeably? b) Equation 3 is the central optimization problem in the paper, and should be written out explicitly using $\\alpha$ as the optimization variable, instead of referring to equations 1 and 2 (in which $x^*$ doesn't even appear). c) I didn't understand equation 4. What does assuming $M(x^\\text{tgt}) = y^\\text{tgt}$ mean? What happens when that is not true? d) Equation 5: Why is $y$ in the right hand side by not in the left? e) Equation 6: $L_\\text{smooth}$ is missing an argument. ", "rating": "3: Weak Reject", "reply_text": "We appreciate the reviewer \u2019 s precious comments and suggestions . We thank the reviewer for recognizing our work as helpful to broaden the study of adversarial examples and encourage a good deal of follow-up research . We will first provide the high-level motivation of why we need to generate adversarial examples and then answer the individual questions . We have revised the notations and equations in our updated manuscript . Q1.Why it is important to generate adversarial examples in the way they do ? A1 : Thanks for the question , the reasons/motivations are described below . Deep Neural Networks ( DNNs ) have achieved great success in a variety of applications . However , various security threats are emerging with the deployment of machine learning models . Without a deep understanding of how neural networks fail under attacks , it would be concerning to apply them in security-critical systems such as face verification and autonomous driving systems . Additionally , learning systems are usually required to be immune to * reasonable variations * of the input . So far , such * variations * have been focused on imperceptible perturbation added to the given inputs whose magnitude is bounded by pixel-space $ L_p $ -norm . Some works have discussed the limitations of only measuring and evaluating the $ L_p $ bounded perturbation [ a1 , a3 , a4 ] . Therefore , it is important to explore other non- $ L_p $ bounded perturbation , especially semantically meaningful perturbation , and more detailed reasons are listed below . First , the semantic based perturbation is new and interesting , which contains different intrinsic properties compared with the traditional $ L_p $ bounded attacks . For instance , the semantic perturbation could be very large to cover the other side of $ L_p $ bounded perturbation . Second , in our proposed semantic based adversarial examples , we can explicitly control the desired editing attribute ( e.g.hair color ) , and successfully preserve the high perceptual quality of the generated images as shown in Figure 4 . This would help to explore the vulnerability/sensitivity of different semantic attributes . Third , various methods have been proposed to defend against adversarial attacks . Adversarial training based methods are currently the most efficient . Currently most adversarial training methods are only effective against a small set of seen attacks [ a1 ] , and researchers ( e.g. , Kang , et.al . [ a2 ] ) have shown that generating diverse attacks can help improve adversarial training performance against unseen attacks . Therefore , we believe that our semantic adversarial examples can potentially benefit adversarial training to improve model robustness by providing diverse unseen adversarial examples . In addition , partially based on the reasons above , Brown , et . al . [ a4 ] proposed the unrestricted adversarial example challenge to encourage the community to explore the adversarial space beyond $ L_p $ , which would potentially benefit the adversarial learning research , and we do hope SemanticAdv can contribute as well . ( To be continued . )"}, "1": {"review_id": "r1l-VeSKwS-1", "review_text": "Summary: This paper proposes to generate \"unrestricted adversarial examples\" via attribute-conditional image editing. Their method, SemanticAdv, leverages disentangled semantic factors and interpolates feature-map with higher freedom than attribute-space. Their adversarial optimization objectives combine both attack effectiveness and interpolation smoothness. They conduct extensive experiments for several tasks compared with CW-attack, showing broad applicability of the proposed method. The paper is well written and technically sound with concrete experimental results. I'm glad to suggest accepting the paper. With the help of attribute-conditional StarGAN, SemanticAdv generates adversarial examples by interpolating feature-maps conditioned on attributes. They design adversarial optimization objectives with specific attack objectives for identity verification and structured prediction tasks. They provide experiments showing the effectiveness of SemanticAdv; analysis on attributes, attack transferability, black-box attack, and robustness against defenses; as well as user study with subjective. The qualitative results also look nice and the code base is open-sourced. A question out of curiosity, the last conv layer in the generator is used as the feature-map. How is the attack effectiveness of using other layers?", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the constructive suggestions and comments , and we have conducted additional experiments based on the comments . Q1.Effectiveness of using other layers ? We have tested another two feature maps ( $ f_ { 1 } $ , $ f_ { 2 } $ ) after the first/second up-sampling operations as shown in Table E ( see Section D in our appendix ) in the submitted paper ; and we also conducted additional experiments on two extra feature maps ( $ f_ { -2 } $ , $ f_ { -1 } $ ) based on the suggestions . $ f_ { -2 } $ indicates the first feature map after the last down-sampling operations and $ f_ { -1 } $ represents the feature map after $ f_ { -2 } $ . The full results are shown in the revision Table E and F. We also present the results as below . The result shows that samples generated by interpolating on our selected layer ( $ f_0 $ ) achieve the highest attack success rate . +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+ | T-FPR ( G-FPR ) | $ 10^ { \u22123 } ( 10^ { \u22123 } ) $ | $ 3\\times10^ { \u22123 } ( 3\\times10^ { \u22123 } ) $ | $ 10^ { \u22124 } ( 10^ { \u22124 } ) $ | +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+ | Layer ( f ) | $ f_ { -2 } $ | $ f_ { -1 } $ | $ f_0 $ | $ f_ { -2 } $ | $ f_ { -1 } $ | $ f_ { 0 } $ | $ f_ { -2 } $ | $ f_ { -1 } $ | $ f_ { 0 } $ | +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+ | Attack Success Rate | 49.4 | 92.09 | 99.29 | 30.44 | 81.87 | 97.35 | 6.66 | 45.46 | 76.64 | +\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+\u2500\u2500\u2500+"}, "2": {"review_id": "r1l-VeSKwS-2", "review_text": "This paper proposes adversarial attacks by modifying semantic properties of the image. Rather than modifying low-level pixels, it modifies mid-level attributes. The authors show that the proposed method is effective and achieves stronger results than the pixel-level attack method (CW) in terms of attacking capability transferring to other architectures. Importantly, the authors show results on a variety of tasks, e.g. landmark detection and segmentation in addition to classification/identification. The most related work is Joshi 2019 and the authors show that the method used in that work (modification in attribute space) is inferior to modification in feature space still via attributes, as the authors proposed. However, I have a few comments and concerns: 1) The authors mention on page 3 they assume M is an oracle-- what is the impact of this? 2) The results in Table C don't look good-- the proposed method can *at best* (in a generous setup) equal the results of CW-- maybe I missed something but more discussion would be helpful. 3) Is there a way to evaluate the merits of semantic modification (beyond attack success) in addition to \"does it look reasonable\"? The authors mention attribute-based modifications are more practical, how can this be evaluated? If attribute-based attacks are better, is there a cost to this? How easy is it to make attribute-based attacks compared to low-level ones? 4) The authors mention that for their transferrability results, they \"select the successfully attacked...\" (page 7). What is the impact of this, as opposed to selecting non-successfully attacked samples? 5) Re: behavior with defense methods, is the advantage of the proposed method a matter of training the defense methods in a tailored way, so they're aware of attribute-based attacks?", "rating": "6: Weak Accept", "reply_text": "We really appreciate the reviewer \u2019 s precious comments . Sorry for the potential confusion . We would like to answer your questions as follows and we have added them in our revision . Q1 : assume M is an oracle -- what is the impact of this ? A1 : Thanks for pointing this out and we will remove this notation in the revision to avoid confusion . Basically , M here is used to obtain the corresponding label related to data x , and we actually do not need to use this assumption in our experiments ( we can assume the ground-truth label is given ) . But we see this assumption introducing the confusion and we will remove this statement by using the ground truth label y directly . Q2 : \u201c The results in Table C do n't look good. \u201d A2 : We believe the \u201c Table C don \u2019 t look good \u201d refers to the results with \u201c worst \u201d and \u201c average \u201d metrics . In Table C , the \u201c best \u201d metric of SemanticAdv should be served as a fair comparison to CW , where both methods achieve 100 % attack success rate . Therefore , our result is good . The detailed reasons are as follows . For each victim image , our SemanticAdv generates a total of 17 adversarial images by augmenting one semantic attribute each time ( e.g. , we have 17 attributes to manipulate ) . However , CW generates a single adversarial example regardless of attributes , which can be viewed as instance-level generation . Therefore , we compare CW with our SemanticAdv on the instance-level which corresponds to the \u201c best \u201d metric . In addition , we report the performance using the \u201c average \u201d and \u201c worst \u201d metric , which actually provides additional insights into the robustness of face verification models across different attributes . Combining the results from Table C in our appendix and Figure 3 , we understand that the face verification models used in our experiments have different levels of robustness across attributes . For example , face verification models are more robust against local shape variations than color variations , e.g. , pale skin has higher attack success rate than mouth open . We believe these discoveries will help the community further understand the properties of face verification models . To summarize , our * semantic * adversarial examples not only achieves attack success rate comparable to traditional $ L_p $ -norm bounded CW attacks , but also enables us to investigate the model robustness under different semantic attributes . We will make the description of Table C clearer in the revised manuscript . ( To be continued . )"}}