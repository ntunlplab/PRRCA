{"year": "2021", "forum": "6fb4mex_pUT", "title": "An Algorithm for Out-Of-Distribution Attack to Neural Network Encoder ", "decision": "Reject", "meta_review": "**Problem significance** This paper proposes an attack mechanism in the latent space of a neural network f(x), which produces out-of-distribution examples. The AC agrees reviewers on the significance of the OOD detection problem, particularly addressing the vulnerability aspect is relevant and of great interest to the community. \n\n**Technical contribution** The AC shares the concern with several reviewers on the limited technical novelty as well as the problem formulation. While the authors have clarified the difference between adversarial attack vs. OOD attack, the underlying attack mechanism is not new to the community (except for allowing for a larger degree of search space without constrained by the visual imperceptibility). In some sense, the search is made easier than the standard adversarial attack by removing the similarity constraint. Given the unrealisticness of the created OOD examples (largely noisy patches), the AC thinks perhaps a more interesting problem is to look at naturally occurring OOD examples that would lead to the similar latent encoding w.r.t in-distribution data, or adversarial robustness w.r.t the OOD detector.  This to me, would steer the community in the right direction. \n\nFrom a problem formulation perspective, the AC thinks it's useful to differentiate three highly related attacks (that are distinct but can cause confusions):\n\n- adversarial attack w.r.t the classifier\n- OOD attack w.r.t the classifier \n- adversarial attack w.r.t the OOD detector (see recent works [1][2][3] which considered the robustness aspect of OOD detector)\n\n\n**Rebuttal feedback** The AC recognizes the effort made by the authors to address the concerns and comments raised by reviewers. The AC agrees with R1/R2/R3 that the additional experiments are valuable, however, the changes to the manuscript are substantial enough to deem another round of review in the future venue. The paper can improve with better organization and presentation, moving the results in the appendix to the main paper. \n\n**Recommendation** The AC recommends rejection. \n\nReferences\n\n[1] Sehwag et al. Analyzing the robustness of open-world machine learning. 2019\n\n[2] Hein et al. Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem. 2019\n\n[3] Chen et al. Informative Outlier Matters: Robustifying Out-of-distribution Detection Using Outlier Mining. arXiv:2006.15207\n\n\n\n", "reviews": [{"review_id": "6fb4mex_pUT-0", "review_text": "* * UPDATE * * I acknowledge that I have read the author responses as well as the other reviews . Overall , I appreciate the clarifications and added experiments given by the authors . My concerns about the low novelty of the presented algorithm and findings remain , however , as I find the OOD attack to be only a slight modification of existing adversarial attacks . I also appreciate that the defense solution claim has been weakened and moved to the appendix , yet these promises are still left to be validated . Lastly , I find all the many added experiments positive , but these have significantly changed the content of the initial submission at this point , which is somewhat out of scope of the ICLR rebuttal phase ( see Q4 in the FAQ of the Reviewer Guide : https : //iclr.cc/Conferences/2021/ReviewerGuide ) . For these reasons , I would keep my recommendation to reject this work for ICLR 2021 , but I encourage the authors to further improve and re-submit the now extended work to some future venue . # # # # # * * Summary * * This paper presents an algorithm for out-of-distribution ( OOD ) attacks on neural networks . Given a target network and an initial OOD input , the proposed algorithm performs projected gradient descent ( PGD ) w.r.t.the input to obtain an output that is close to the embedding of some desired in-distribution sample . The paper makes the case that dimensionality reduction is one key reason that makes standard deep networks vulnerable to OOD attacks , due to the non-bijective nature of such mappings . An experimental evaluation on two ImageNet- pretrained classifiers ( ResNet-18 and DenseNet-121 ) is presented , where a subset of ImageNet serves as in-distribution and chest x-ray , lung-CT , and noise images serve as OOD samples , that demonstrates empirically that these networks can be attacked with the proposed algorithm . Another experiment on the likelihood-based normalizing flow model Glow is carried out which demonstrates that such bijective , dimensionality-preserving deep generative models are also breakable by the proposed attack . Finally , a theoretical sketch for a solution of the problem is described . * * Pros * * + The paper presents a simple OOD attack algorithm and demonstrates empirically that OOD samples can be perturbed such that they map to the embedding of some arbitrary in-distribution example . + The paper makes a plausible argument that dimensionality reduction enables OOD attacks . Inversely , it is argued that reconstruction methods , which map back to the original space , are favorable for OOD detection for which the proposed attack is also ineffective . + Attacking the dimensionality-preserving , likelihood-based Glow model is an interesting experiment to consider in the context of the presented dimensionality reduction argument . + The paper is structured well and overall well-placed into existing literature . * * Cons * * - I find the novelty of the presented algorithm and experimental findings to be rather low . - The experimental evaluation of the proposed attack is limited to standard classifiers and does not include deep networks that have been trained to increase OOD robustness [ 1 , 2 , 4 , 3 ] . - The defense solution is only a sketch and makes promises that are left to be validated . - There are many errors in language and grammar , e.g.wrong use of tense , missing articles , etc . ( see minor comments below ) * * Recommendation * * I think the current paper is ok but not good enough ( score : 4 ) due to ( i ) low novelty , ( ii ) a limited experimental evaluation , and ( iii ) solution claims that are left to be validated . ( i ) Though I see and agree that the OOD attack setting is slightly different to adversarial attacks , I find the brittleness of standard classifiers in this regard not surprising . In particular , the OOD attack has a greater degree of freedom since any arbitrary OOD input can be used as a starting point for perturbation ( pure noise is also OOD , as remarked in the paper ) , i.e.there is no similarity constraint on the input as there is for adversarial attacks . Moreover , I find the algorithmic novelty to be low as well , since the proposed algorithm essentially is a slight adaptation of previously introduced projected gradient descent ( PGD ) attacks . ( ii ) Following ( i ) , I think the current experimental evaluation is limited and the findings are not surprising for the two standard , pre-trained classifiers ( ResNet-18 and DenseNet-121 ) . There exist many approaches that have shown to improve OOD robustness [ 1 , 2 , 4 , 3 ] , which should be included in the analysis . It would be interesting to see how these approaches perform and compare , which could be insightful for improving OOD robustness . ( iii ) Proposing an attack begs the question what possible defenses could be . Currently , the main paper is only phenomenological , i.e.demonstrates that OOD attacks are an open issue , but the description of a possible defense at the end of the paper and in the appendix makes only a solution claim which is left to be validated . I don \u2019 t say or think such a solution would be necessary for an interesting and valid contribution , as OOD detection poses a hard problem which likely lacks a simple solution , but the current solution is a mere sketch making promises with questions left open . For example , how can a sufficient space saturation be achieved with a finite sample in practice ? Which measure to use ? * * Additional feedback and ideas for improvement * * I think the OOD detection problem , both from an attack and defense perspective is relevant and of great interest to the community , which is why I encourage the authors to build upon and extend the current manuscript . Some ideas : - Including methods that have shown to improve OOD robustness [ 1 , 2 , 4 , 3 ] would greatly improve the value of the analysis . This could be insightful to further improve OOD robustness and see pros/cons of existing approaches . - Does adversarial training , as mentioned in Section 2.2 , also help OOD robustness ? - Implementing and validating the presented OOD detection solution would be interesting . - Further investigate the reason why the dimensionality-preserving Glow model can be attacked as well , as this suggests dimensionality reduction is only a part of the issue . - \u2018 Could we design an evaluation method ( experimental or analytical ) that does not rely on OOD samples ? \u2019 I think this is a worthwhile question to ask and research . - Add missing related work [ 5 ] . * * Minor Comments * * 1 . \u2018 The algorithm needs a weak assumption that $ f ( x ) $ is differentiable \u2019 I think sub-differentiable would be sufficient , correct ? 2. \u2018 If the neural network only uses ReLU activation , then the input-output relationship can be exactly expressed as a linear mapping \u2019 Only piecewise linear mapping , right ? 3. \u2018 [ ... ] , the above mentioned classification-based OOD detection is theoretically almost ineffective [ ... ] \u2019 ; \u2018 [ ... ] , then it is highly possible that the entire latent space is crawling with the shadows of OOD samples. \u2019 ; \u2018 [ ... ] , there are a huge number of \u201c holes \u201d in the space , [ ... ] \u2019 Rather avoid such vague formulations . 4.Published works should be included as such in the references , not by their preprint reference ( e.g. , [ 2 ] appeared at ICLR ) . 5.A list of citations in a sentence should be concatenated within single parentheses/brackets , separated by commas ( not individual parentheses/brackets per citation ) . 6.When referring to a specific section , section should be capitalized , e.g.Section 3 . 7.Abstract : \u2018 Deep Neural * Networks ( DNNs ) * , especially convolutional neural * networks ( CNNs ) * , * have * become ... \u2019 It is a class of models , hence plural . 8.Algorithm 1 : \u2018 [ ... ] not similar to * any sample * in the dataset. \u2019 ; \u2018 $ \\alpha $ the learning rate of * the * optimizer \u2019 ; $ h $ used in the Algorithm is not defined ; I would use $ \\nabla_x J $ instead of $ J ' $ for the gradient to stress the derivative is w.r.t.the input . 9.Many figure labels are tiny and hard to read . 10.The captions of Figure 3 + 4 are ( almost ) identical . Add a label indicating the network . 11.Section 1 : \u2018 It was shown by * Nguyen et al . ( 2015 ) * [ ... ] , and * an * evolutionary algorithm was used [ ... ] \u2019 12 . Section 1 : \u2018 Since then , many methods * have been * proposed for OOD detection [ ... ] \u2019 13 . Section 1 : \u2018 For instance , * Hendrycks et al . ( 2016 ) show * that a classifier \u2019 s prediction * probabilities * of OOD examples tend to be * more uniform * [ ... ] \u2019 \u2018 lower \u2019 is ambiguous as the softmax probabilites always add up to 1 , right ? 14.Section 1 : \u2018 For * the evaluation of OOD detection methods * , an OOD detector is * usually * trained [ ... ] \u2019 15 . Section 1 : \u2018 [ ... ] pre-trained on * the * ImageNet dataset. \u2019 16 . Section 1 : \u2018 [ ... ] which could be any kind of * image * ( even random * noise * ) [ ... ] \u2019 . 17.Section 1 : \u2018 [ ... ] , which is the input to the last fully-connected linear layer before * the * softmax operation. \u2019 18 . Section 1 : \u2018 [ ... ] , and * one * fix to the problem could be using likelihood ratio [ ... ] \u2019 19 . Section 1 : \u2018 [ ... ] , we will show that * the * OOD sample \u2019 s likelihood score from the Glow model [ ... ] \u2019 20 . Section 2 : \u2018 [ ... ] projected gradient descent ( PGD ) which is used for adversarial * attacks * . \u2019 21 . Section 2 : \u2018 In practice , * Algorithm 1 can be repeated * many times [ ... ] \u2019 22 . Section 2 : \u2018 [ ... ] , the algorithm will have a better chance to avoid * a local minimum * [ ... ] \u2019 23 . Section 2 : \u2018 This * is * simply because the vectors in a lower-dimensional space [ ... ] \u2019 24 . Section 2 : \u2018 [ ... ] , which is the Pigeonhole Principle. \u2019 Citation ? 25.Section 2 : \u2018 Usually , * the * training set is only a subset \u2019 26 . \u2018 3 EXPERIMENT \u2019 Do not capitalize and \u2018 3 Experimental Evaluation \u2019 ? 27.Section 3 : \u2018 * A * Nvidia Titan V GPU was used [ ... ] \u2019 28 . Section 4 : \u2018 We would like to point out that it is * difficult * to evaluate [ ... ] \u2019 # # # # # * * References * * [ 1 ] B. Lakshminarayanan , A. Pritzel , and C. Blundell . Simple and scalable predictive uncertainty estimation using deep ensembles . In NIPS , pages 6402\u20136413 , 2017 . [ 2 ] K. Lee , H. Lee , K. Lee , and J. Shin . Training confidence-calibrated classifiers for detecting out-of-distribution samples . In ICLR , 2018 . [ 3 ] K. Lee , K. Lee , H. Lee , and J. Shin . A simple unified framework for detecting out-of-distribution samples and adversarial attacks . In NeurIPS , pages 7167\u20137177 , 2018 . [ 4 ] S. Liang , Y. Li , and R. Srikant . Enhancing the reliability of out-of-distribution image detection in neural networks . In ICLR , 2018 . [ 5 ] A. Meinke and M. Hein . Towards neural networks that provably know when they don \u2019 t know . In ICLR , 2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "First , we thank the reviewer for thorough reading and reviewing our manuscript . Next , we answer the specific questions from the reviewer . ( 1 ) `` I find the novelty of the presented algorithm and experimental findings to be rather low . '' Reply : we have provided new experimental findings . About the novelty , please read New Discussion ( 6 ) and ( 7 ) ( 2 ) `` The experimental evaluation of the proposed attack is limited to standard classifiers and does not include deep networks that have been trained to increase OOD robustness [ 1 , 2 , 4 , 3 ] . '' Reply : we evaluated the four methods in the new experiments ( 3 ) `` The defense solution is only a sketch and makes promises that are left to be validated . '' Reply : Please read New Discussion ( 8 ) ( 4 ) `` Though I see and agree that the OOD attack setting is slightly different to adversarial attacks , I find the brittleness of standard classifiers in this regard not surprising . In particular , the OOD attack has a greater degree of freedom since any arbitrary OOD input can be used as a starting point for perturbation ( pure noise is also OOD , as remarked in the paper ) , i.e.there is no similarity constraint on the input as there is for adversarial attacks . '' Reply : our new experiments show that eight classification-based OOD detection methods do not work well under the OOD attack , which is new information . ( 5 ) `` Moreover , I find the algorithmic novelty to be low as well , since the proposed algorithm essentially is a slight adaptation of previously introduced projected gradient descent ( PGD ) attacks . '' Reply : about the novelty , please read New Discussion ( 6 ) and ( 7 ) . ( 6 ) `` I think the current experimental evaluation is limited and the findings are not surprising for the two standard , pre-trained classifiers ( ResNet-18 and DenseNet-121 ) . There exist many approaches that have shown to improve OOD robustness [ 1 , 2 , 4 , 3 ] , which should be included in the analysis . It would be interesting to see how these approaches perform and compare , which could be insightful for improving OOD robustness . '' Reply : we evaluated the four methods in the new experiments . ( 7 ) `` Proposing an attack begs the question what possible defenses could be . Currently , the main paper is only phenomenological , i.e.demonstrates that OOD attacks are an open issue , but the description of a possible defense at the end of the paper and in the appendix makes only a solution claim which is left to be validated . I don \u2019 t say or think such a solution would be necessary for an interesting and valid contribution , as OOD detection poses a hard problem which likely lacks a simple solution , but the current solution is a mere sketch making promises with questions left open . For example , how can a sufficient space saturation be achieved with a finite sample in practice ? Which measure to use ? '' Reply : please read New Discussion ( 8 ) ( 8 ) `` I think the OOD detection problem , both from an attack and defense perspective is relevant and of great interest to the community , which is why I encourage the authors to build upon and extend the current manuscript . Some ideas : Including methods that have shown to improve OOD robustness [ 1 , 2 , 4 , 3 ] would greatly improve the value of the analysis . This could be insightful to further improve OOD robustness and see pros/cons of existing approaches . Does adversarial training , as mentioned in Section 2.2 , also help OOD robustness ? '' Reply : we have evaluated the four methods in new experiments . The Deep Ensembles ( NeurIPS 2017 , https : //arxiv.org/abs/1612.01474 ) used adversarial training , and the evaluation results show that it did not work . This is not surprising because noisy images with adversarial noises are still in-distribution ( e.g.a noisy image of a panda still is an image of a panda ) . Adversarial samples and OOD samples are completely different . ( 9 ) `` Implementing and validating the presented OOD detection solution would be interesting . '' Reply : please read New Discussion ( 8 ) ( 10 ) Further investigate the reason why the dimensionality-preserving Glow model can be attacked as well , as this suggests dimensionality reduction is only a part of the issue . '' Reply : please read New Discussion ( 2 ) ( 11 ) `` Could we design an evaluation method ( experimental or analytical ) that does not rely on OOD samples ? \u2019 I think this is a worthwhile question to ask and research . '' Reply : we thank the reviewer for this comment . ( 12 ) `` Add missing related work [ 5 ] '' Reply : we will add it to the revised manuscript . ( 13 ) `` Minor Comments\u2026 '' Reply : we thank the reviewer for the comments , and we will incorporate those into the revised manuscript ."}, {"review_id": "6fb4mex_pUT-1", "review_text": "Summary of paper : this work shows that adversarial perturbations can make any OOD image , map to the same latent code as an in-distribution - creating an attack on confidence-based or flow-based ODD detection methods . Results are shown on a few datasets with some attempts at evaluation . Novelty : Unfortunately , I do n't think there is much new here . Adversarial attacks are of course well known - I am not sure that attacks on intermediate latent codes present novelty either . Adversarial attacks against anomaly detection methods have also been investigated before ( e.g . [ 1 ] [ 2 ] , although their setting is a little different ) and there is nothing in the proposed method that is particularly tailored to OOD . Evaluation : the evaluation is not extensive - only one adversarial attack is investigated and no reasonable baselines have been selected . I am not sure that MAPE is an appropriate metric - it really depends on the allowed perturbation . Ss the allowed perturbation small enough ? do the perturbed images look realistic - if I understood Fig.12 , they do n't - but the caption there is not clear . Clarity - the paper is not particularly clearly written - although the idea is simple enough . E.g.I do n't see the scatter plots clearly explained , the analysis in Sec.2.2 is very dense for a fairly simple idea . Overall : ultimately , this is conceptually repeating the same thing as any other adversarial examples work , perturbations can make a network confident that any image has the label of another image - and this obviously would overcome confidence based OOD detection methods . I therefore do not see a strong contribution by this work . As there is also very limited methodological novelty , I do not think it should be accepted . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # I understand the distinction the authors are trying to draw between adversarial examples for anomaly detection and fooling OOD to think that images are in distribution where in fact they are OOD . I still do n't think that technically or conceptually , there is much difference . The authors presented many fresh results during the rebuttal ( which might have been better presented just as a table in the manuscript , rather than on this thread ) . The experiments can form a part of a resubmission of this work , that will incorporate the extensive comments presented by the current reviews . [ 1 ] Rigaki , Maria . `` Adversarial deep learning against intrusion detection classifiers . '' ( 2017 ) . [ 2 ] Bergman and Hoshen , Classification-based anomaly detection for general data , ICLR'20", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for thorough reading and reviewing our manuscript . Here , we answer the specific questions from the reviewer . ( 1 ) '' Unfortunately , I do n't think there is much new here . Adversarial attacks are of course well known - I am not sure that attacks on intermediate latent codes present novelty either . Adversarial attacks against anomaly detection methods have also been investigated before ( e.g . [ 1 ] [ 2 ] , although their setting is a little different ) and there is nothing in the proposed method that is particularly tailored to OOD . '' Reply : we presented an approach to generate OOD samples and evaluate OOD detection methods , which is not done in the eight OOD detection papers published at ICLR and NeurIPS . We note that adversarial attack and OOD attack are doing completely different things to neural networks . [ 1 ] Rigaki , Maria . `` Adversarial deep learning against intrusion detection classifiers . '' ( 2017 ) .In this paper , FSGM and JSMA methods are used to generate adversarial samples . This is a study of adversarial robustness , NOT OOD Attack issue presented in our manuscript , NOT the OOD detection issue defined in the baseline method ( ICLR 2017 ) in https : //arxiv.org/abs/1610.02136 and investigated in the other seven papers at ICLR and NeurIPS . [ 2 ] Bergman and Hoshen , Classification-based anomaly detection for general data , ICLR'20 . For image related applications , this paper investigated a special case of OOD detection , also known as one class classification , which is very different from the OOD detection tasks on dataset level : e.g.MNIST to be in-distribution and Omniglot to be OOD as demonstrated in the paper of the baseline method . Our study focused on OOD detection issues on the dataset level . ( 2 ) `` Evaluation : the evaluation is not extensive - only one adversarial attack is investigated and no reasonable baselines have been selected . I am not sure that MAPE is an appropriate metric - it really depends on the allowed perturbation . Ss the allowed perturbation small enough ? do the perturbed images look realistic - if I understood Fig.12 , they do n't - but the caption there is not clear . '' Reply : adversarial attack and OOD attack are doing completely different things . For an adversarial attack , the perturbation must not be too large , and the perturbed images should be human-recognizable . For an OOD attack , the generated OOD samples can be arbitrary , such as random noises or wired images . By the definition of OOD , an OOD sample does not and should not look similar to the in-distribution samples . ( 3 ) `` Clarity - the paper is not particularly clearly written - although the idea is simple enough . E.g.I do n't see the scatter plots clearly explained '' Reply : As explained by the figure captions , the scatter plots show the feature vectors ( z ) . ( 4 ) `` the analysis in Sec.2.2 is very dense for a fairly simple idea. `` Reply : OOD is a complex issue , and it should be thoroughly analyzed . ( 5 ) `` Overall : ultimately , this is conceptually repeating the same thing as any other adversarial examples work , perturbations can make a network confident that any image has the label of another image - and this obviously would overcome confidence based OOD detection methods . I therefore do not see a strong contribution by this work . As there is also very limited methodological novelty , I do not think it should be accepted . '' Reply : Adversarial attack and OOD attack are doing completely different things to neural networks . Please see our new experiments and discussion . We note that we have evaluated eight dataset-level OOD detection methods , and we believe our work has made a considerable contribution to the field ."}, {"review_id": "6fb4mex_pUT-2", "review_text": "Summary : -- The paper presents a method that attacks existing out-of-distribution ( OOD ) detection methods . Most of the existing OOD detection methods perform detection using a latent representation . Main motivation of the paper is that the size of the latent representation is much smaller than the input images which results mapping both OOD and in-distribution images to the same place in the latent space and diminishing OOD detection performance . With this motivation , the proposed method perturbs input images to obtain an image whose latent representation is similar to the latent representation of an in-distribution image . Since such perturbations can be obtained for any OOD image , existing OOD detection algorithms fails distinguishing such OOD samples . The paper contains experiments on multiple dataset to demonstrate that the proposed method obtains a latent representation similar to the representation of an in-distribution image . Comments : - 1 - From the abstract , I infer that the paper has roughly 3 contributions : 1 ) the paper shows existing OOD detection methods are practically breakable , 2 ) Glow likelihood-based OOD detection is ineffective , and 3 ) present a simple theoretical solution with guaranteed performance for OOD detection . However , the 3rd contribution is never mentioned/introduced in the paper until Appendix A where they briefly presents 2 different implementations of an OOD detection method idea . I would not consider this as a contribution since there is no experimental evaluation showing the OOD detection performance of the idea . Also , since this contribution is mentioned in the abstract , I would expect seeing its description and the results in the main paper rather than the Appendix . 2 - Until the end of the Introduction , it is not very clear that the main contribution of the paper is a method that attacks OOD detection methods . I liked the motivating example in Figure 1 , but the contribution can be given in a more clear way . 3 - In the last paragraph of page 2 , it is mentioned that the clip operator can ensure x_out to be OOD after a small modification to x'_out . I found this statement quite vague . How this operation `` ensures '' that x_out to be OOD after the modification ? 4 - In the last paragraph of Section 2.1 , it is mentioned that adding initial random noise helps to avoid local minimum caused by a bad initialization . Is this something that you observed empirically ? The contribution of the noise is not very clear to me and I think showing results with and without the noise would be very useful to demonstrate how the noise helps to avoid local minima . Also , it would be interesting to show how this loss evolves during the optimization . 5 - In Sec 2.2 - Each pixel in an 8-bit image can take 256 different values . So , for an image with size 224x224x3 , there are 256^ ( 224x224x3 ) possible images that can be generated ; not 8^ ( 224x224x3 ) . 6 - I did n't quite understand the message in the last paragraph of Sec.2.2.Why \\Omega_in is split into \\Omega_ { in_clean } and \\Omega_ { in_noisy } ? How this is used in the proposed method ? 7 - I think that experimental evaluations are not comprehensive enough , 7.1 . It is mentioned that implementing OOD detection methods are not necessary since Alg.1 already produces z_out ~= z_in . I do n't agree with this since most of the SoTA methods do not rely on only a single representation but multiple representations at different layers [ ref1 , ref2 , ref3 ] . Therefore , it is crucial to show the performance loss of these methods due to the proposed method . [ ref1 ] Erdil et al. , Unsupervised out-of-distribution detection using kernel density estimation [ ref2 ] Lee et al. , A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks [ ref3 ] Sastry et al. , Detecting Out-of-Distribution Examples with Gram Matrices 7.2 . Benchmark datasets that have been used in OOD detection paper are usually different than the ones used in the paper . Please see the references above . I wonder why the datasets choice are different than the standard benchmarks ? 7.3.As mention in paragraph 3 of Sec.4. , there are other generative models that show promising results . However , I did n't quite understand comparison with these methods are `` out of the scope '' while Glow is relevant . 8.The parameters used in different experiments are different than each other . How are these parameters determined ? How sensitive is the proposed method to parameter choice ? Minor comments : - 1 - In Introduction paragraph 2 , there is a typo : laten - > latent 2 - In Sec 2.1 , paragraph 3 : the algorithm is referred as `` above algorithm '' but it appears `` below '' in the paper . 3 - In Sec 2.2 , paragraph 3 , there is a typo : state-of-art - > state-of-the-art 4 - In Sec 2.2 , paragraph 3 , there is a typo : laten - > latent Overall : -- I found the idea of attacking existing OOD detection methods interesting . However , I believe , this paper needs improvement in terms of clarity of the presentation , experimental evaluations and the presentation of the contributions . Therefore , my initial rating is reject for this paper .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for thorough reading and reviewing our manuscript . Next , we answer the specific questions from the reviewer . ( 1 ) `` From the abstract , I infer that the paper has roughly 3 contributions : 1 ) the paper shows existing OOD detection methods are practically breakable , 2 ) Glow likelihood-based OOD detection is ineffective , and 3 ) present a simple theoretical solution with guaranteed performance for OOD detection . However , the 3rd contribution is never mentioned/introduced in the paper until Appendix A where they briefly presents 2 different implementations of an OOD detection method idea . I would not consider this as a contribution since there is no experimental evaluation showing the OOD detection performance of the idea . Also , since this contribution is mentioned in the abstract , I would expect seeing its description and the results in the main paper rather than the Appendix '' . Reply : In the new experiments , we have shown that eight OOD detection methods are breakable under the OOD Attack . Please see New Discussion ( 8 ) for the theoretical solution . ( 2 ) `` Until the end of the Introduction , it is not very clear that the main contribution of the paper is a method that attacks OOD detection methods . I liked the motivating example in Figure 1 , but the contribution can be given in a more clear way . '' Reply : We will explicitly state the contribution in the revised manuscript . ( 3 ) `` In the last paragraph of page 2 , it is mentioned that the clip operator can ensure x_out to be OOD after a small modification to x'_out . I found this statement quite vague . How this operation `` ensures '' that x_out to be OOD after the modification ? '' Reply : we thank the reviewer for pointing out this issue . `` ensure '' is not the right word , and we will change the description to `` the clip operator will limit the difference between x'_out and x_out so that x_out may be OOD '' . ( 4 ) `` In the last paragraph of Section 2.1 , it is mentioned that adding initial random noise helps to avoid local minimum caused by a bad initialization . Is this something that you observed empirically ? The contribution of the noise is not very clear to me and I think showing results with and without the noise would be very useful to demonstrate how the noise helps to avoid local minima . Also , it would be interesting to show how this loss evolves during the optimization . '' Reply : adding initial random noise is the standard procedure in the PGD optimization technique , and it is theoretically useful . Since in most of our experiments , we use random noise images as the initial samples , this step could be skipped . We will add a figure of loss curves in the Appendix of the revised manuscript , which is basically a curve going down and becoming flat . ( 5 ) `` In Sec 2.2 - Each pixel in an 8-bit image can take 256 different values . So , for an image with size 224x224x3 , there are 256^ ( 224x224x3 ) possible images that can be generated ; not 8^ ( 224x224x3 ) . '' Reply : we thank the reviewer for pointing out this bug . ( 6 ) `` I did n't quite understand the message in the last paragraph of Sec.2.2.Why \\Omega_in is split into \\Omega_ { in_clean } and \\Omega_ { in_noisy } ? How this is used in the proposed method ? '' Reply : this is used to explain the feasibility of adversarial training using noisy images generated by adversarial attacks , NOT used in OOD Attack method . An adversarial attack will add a small amount of noise to the input , and the noisy image is still human-recognizable , e.g. , a noisy image of a panda still is an image of a panda . The noisy image of the panda is in \\Omega_ { in_noisy } and the clean image of the panda is in \\Omega_ { in_clean } ."}, {"review_id": "6fb4mex_pUT-3", "review_text": "Summary : The paper defines an out-of-distribution attack , a process which drives an out-of-distribution ( OOD ) input to have the same latent representation to an inlier . The paper also analyzes that an encoder is inevitably vulnerable to out-of-distribution attack when its latent dimensionality is smaller than the dimensionality of the input . Decision : Reject Strength : The paper addresses an important vulnerability of classifier-based OOD detection . As classifier-based method is one of the currently dominating approaches for OOD detection , investigating its weakness is a significant contribution to the research community . Weakness : The proposed attack algorithm is significantly similar to the previously known adversarial attack algorithms and therefore seems trivial . The main quantitative result , Table 1 , is not very convincing . I suggest the authors provide AUC scores computed before and after OOD attack , so that the difference clearly shows that the proposed attack causes a decrease in OOD detection performance . The paper should benchmark the proposed attack algorithm against state-of-the-art OOD detection methods . Currently , only a relatively simple method of Hendrycks and Gimpel , 2016 is used . The method should include at least [ 1,2,3 ] to show the effectiveness of the proposed attack . At least some of these OOD detection methods may be able to resist the proposed attack . For example , multiple hidden layer representations from a classifier are used in [ 1 ] , and therefore it can still detect OOD even if a specific latent representation is under attack . The organization of the paper needs to be improved . In the last sentence of the abstract , `` a simple theoretical solution '' is mentioned but is only addressed in Appendix . If it is a contribution that is important enough to be mentioned in the abstract , it should be covered in depth in the main manuscript instead of Appendix . Minor comments : - The visibility of figures are poor . The axis titles and MAPE values in Figure 2 , 3 , 4 , 5 should be larger . - In Section 2.2 , 8^ { 224 * 224 * 3 } should 256^ { 224 * 224 * 3 } . An 8-bit integer can represent 256 values . - The captions of Figure 3 and Figure 4 are the same . - The captions of Figure 5 and Figure 6 are too close . Typos : laten \u2192 latent ( Section 1 paragraph 2 line 10 ) Dicussion \u2192 Discussion ( Section 4 title ) Difficulty \u2192 difficult ( Section 4 paragraph 4 first line ) [ 1 ] Lee , Kimin , et al . `` A simple unified framework for detecting out-of-distribution samples and adversarial attacks . '' Advances in Neural Information Processing Systems . 2018 . [ 2 ] Liang , Shiyu , Yixuan Li , and Rayadurgam Srikant . `` Enhancing the reliability of out-of-distribution image detection in neural networks . '' arXiv preprint arXiv:1706.02690 ( 2017 ) . [ 3 ] Grathwohl , Will , et al . `` Your classifier is secretly an energy based model and you should treat it like one . '' arXiv preprint arXiv:1912.03263 ( 2019 ) .", "rating": "3: Clear rejection", "reply_text": "First , we thank the reviewer for thorough reading and reviewing our manuscript . Next , we answer the specific questions from the reviewer . ( 1 ) `` The proposed attack algorithm is significantly similar to the previously known adversarial attack algorithms and therefore seems trivial . '' Reply : about the novelty , please read New Discussion ( 6 ) and ( 7 ) . We note that adversarial attack and OOD attack are doing completely different things to neural networks . Please read the Clarification . ( 2 ) `` The main quantitative result , Table 1 , is not very convincing . I suggest the authors provide AUC scores computed before and after OOD attack , so that the difference clearly shows that the proposed attack causes a decrease in OOD detection performance . '' Reply : we evaluated eight OOD detection methods in the new experiments . What are `` AUC scores computed before OOD attack '' ? the AUC scores in the papers of the OOD detection methods ? ( 3 ) `` The paper should benchmark the proposed attack algorithm against state-of-the-art OOD detection methods . Currently , only a relatively simple method of Hendrycks and Gimpel , 2016 is used . The method should include at least [ 1,2,3 ] to show the effectiveness of the proposed attack . At least some of these OOD detection methods may be able to resist the proposed attack . For example , multiple hidden layer representations from a classifier are used in [ 1 ] , and therefore it can still detect OOD even if a specific latent representation is under attack . '' Reply : we evaluated the three OOD detection methods in the new experiments . ( 4 ) `` The organization of the paper needs to be improved . In the last sentence of the abstract , `` a simple theoretical solution '' is mentioned but is only addressed in Appendix . If it is a contribution that is important enough to be mentioned in the abstract , it should be covered in depth in the main manuscript instead of Appendix . '' Reply : please read New Discussion ( 8 ) ( 5 ) `` Minor comments\u2026 '' Reply : we thank the reviewer for the comments , and we will revise the manuscript accordingly . The figures are in high-resolution ; please zoom in on the computer screen ."}], "0": {"review_id": "6fb4mex_pUT-0", "review_text": "* * UPDATE * * I acknowledge that I have read the author responses as well as the other reviews . Overall , I appreciate the clarifications and added experiments given by the authors . My concerns about the low novelty of the presented algorithm and findings remain , however , as I find the OOD attack to be only a slight modification of existing adversarial attacks . I also appreciate that the defense solution claim has been weakened and moved to the appendix , yet these promises are still left to be validated . Lastly , I find all the many added experiments positive , but these have significantly changed the content of the initial submission at this point , which is somewhat out of scope of the ICLR rebuttal phase ( see Q4 in the FAQ of the Reviewer Guide : https : //iclr.cc/Conferences/2021/ReviewerGuide ) . For these reasons , I would keep my recommendation to reject this work for ICLR 2021 , but I encourage the authors to further improve and re-submit the now extended work to some future venue . # # # # # * * Summary * * This paper presents an algorithm for out-of-distribution ( OOD ) attacks on neural networks . Given a target network and an initial OOD input , the proposed algorithm performs projected gradient descent ( PGD ) w.r.t.the input to obtain an output that is close to the embedding of some desired in-distribution sample . The paper makes the case that dimensionality reduction is one key reason that makes standard deep networks vulnerable to OOD attacks , due to the non-bijective nature of such mappings . An experimental evaluation on two ImageNet- pretrained classifiers ( ResNet-18 and DenseNet-121 ) is presented , where a subset of ImageNet serves as in-distribution and chest x-ray , lung-CT , and noise images serve as OOD samples , that demonstrates empirically that these networks can be attacked with the proposed algorithm . Another experiment on the likelihood-based normalizing flow model Glow is carried out which demonstrates that such bijective , dimensionality-preserving deep generative models are also breakable by the proposed attack . Finally , a theoretical sketch for a solution of the problem is described . * * Pros * * + The paper presents a simple OOD attack algorithm and demonstrates empirically that OOD samples can be perturbed such that they map to the embedding of some arbitrary in-distribution example . + The paper makes a plausible argument that dimensionality reduction enables OOD attacks . Inversely , it is argued that reconstruction methods , which map back to the original space , are favorable for OOD detection for which the proposed attack is also ineffective . + Attacking the dimensionality-preserving , likelihood-based Glow model is an interesting experiment to consider in the context of the presented dimensionality reduction argument . + The paper is structured well and overall well-placed into existing literature . * * Cons * * - I find the novelty of the presented algorithm and experimental findings to be rather low . - The experimental evaluation of the proposed attack is limited to standard classifiers and does not include deep networks that have been trained to increase OOD robustness [ 1 , 2 , 4 , 3 ] . - The defense solution is only a sketch and makes promises that are left to be validated . - There are many errors in language and grammar , e.g.wrong use of tense , missing articles , etc . ( see minor comments below ) * * Recommendation * * I think the current paper is ok but not good enough ( score : 4 ) due to ( i ) low novelty , ( ii ) a limited experimental evaluation , and ( iii ) solution claims that are left to be validated . ( i ) Though I see and agree that the OOD attack setting is slightly different to adversarial attacks , I find the brittleness of standard classifiers in this regard not surprising . In particular , the OOD attack has a greater degree of freedom since any arbitrary OOD input can be used as a starting point for perturbation ( pure noise is also OOD , as remarked in the paper ) , i.e.there is no similarity constraint on the input as there is for adversarial attacks . Moreover , I find the algorithmic novelty to be low as well , since the proposed algorithm essentially is a slight adaptation of previously introduced projected gradient descent ( PGD ) attacks . ( ii ) Following ( i ) , I think the current experimental evaluation is limited and the findings are not surprising for the two standard , pre-trained classifiers ( ResNet-18 and DenseNet-121 ) . There exist many approaches that have shown to improve OOD robustness [ 1 , 2 , 4 , 3 ] , which should be included in the analysis . It would be interesting to see how these approaches perform and compare , which could be insightful for improving OOD robustness . ( iii ) Proposing an attack begs the question what possible defenses could be . Currently , the main paper is only phenomenological , i.e.demonstrates that OOD attacks are an open issue , but the description of a possible defense at the end of the paper and in the appendix makes only a solution claim which is left to be validated . I don \u2019 t say or think such a solution would be necessary for an interesting and valid contribution , as OOD detection poses a hard problem which likely lacks a simple solution , but the current solution is a mere sketch making promises with questions left open . For example , how can a sufficient space saturation be achieved with a finite sample in practice ? Which measure to use ? * * Additional feedback and ideas for improvement * * I think the OOD detection problem , both from an attack and defense perspective is relevant and of great interest to the community , which is why I encourage the authors to build upon and extend the current manuscript . Some ideas : - Including methods that have shown to improve OOD robustness [ 1 , 2 , 4 , 3 ] would greatly improve the value of the analysis . This could be insightful to further improve OOD robustness and see pros/cons of existing approaches . - Does adversarial training , as mentioned in Section 2.2 , also help OOD robustness ? - Implementing and validating the presented OOD detection solution would be interesting . - Further investigate the reason why the dimensionality-preserving Glow model can be attacked as well , as this suggests dimensionality reduction is only a part of the issue . - \u2018 Could we design an evaluation method ( experimental or analytical ) that does not rely on OOD samples ? \u2019 I think this is a worthwhile question to ask and research . - Add missing related work [ 5 ] . * * Minor Comments * * 1 . \u2018 The algorithm needs a weak assumption that $ f ( x ) $ is differentiable \u2019 I think sub-differentiable would be sufficient , correct ? 2. \u2018 If the neural network only uses ReLU activation , then the input-output relationship can be exactly expressed as a linear mapping \u2019 Only piecewise linear mapping , right ? 3. \u2018 [ ... ] , the above mentioned classification-based OOD detection is theoretically almost ineffective [ ... ] \u2019 ; \u2018 [ ... ] , then it is highly possible that the entire latent space is crawling with the shadows of OOD samples. \u2019 ; \u2018 [ ... ] , there are a huge number of \u201c holes \u201d in the space , [ ... ] \u2019 Rather avoid such vague formulations . 4.Published works should be included as such in the references , not by their preprint reference ( e.g. , [ 2 ] appeared at ICLR ) . 5.A list of citations in a sentence should be concatenated within single parentheses/brackets , separated by commas ( not individual parentheses/brackets per citation ) . 6.When referring to a specific section , section should be capitalized , e.g.Section 3 . 7.Abstract : \u2018 Deep Neural * Networks ( DNNs ) * , especially convolutional neural * networks ( CNNs ) * , * have * become ... \u2019 It is a class of models , hence plural . 8.Algorithm 1 : \u2018 [ ... ] not similar to * any sample * in the dataset. \u2019 ; \u2018 $ \\alpha $ the learning rate of * the * optimizer \u2019 ; $ h $ used in the Algorithm is not defined ; I would use $ \\nabla_x J $ instead of $ J ' $ for the gradient to stress the derivative is w.r.t.the input . 9.Many figure labels are tiny and hard to read . 10.The captions of Figure 3 + 4 are ( almost ) identical . Add a label indicating the network . 11.Section 1 : \u2018 It was shown by * Nguyen et al . ( 2015 ) * [ ... ] , and * an * evolutionary algorithm was used [ ... ] \u2019 12 . Section 1 : \u2018 Since then , many methods * have been * proposed for OOD detection [ ... ] \u2019 13 . Section 1 : \u2018 For instance , * Hendrycks et al . ( 2016 ) show * that a classifier \u2019 s prediction * probabilities * of OOD examples tend to be * more uniform * [ ... ] \u2019 \u2018 lower \u2019 is ambiguous as the softmax probabilites always add up to 1 , right ? 14.Section 1 : \u2018 For * the evaluation of OOD detection methods * , an OOD detector is * usually * trained [ ... ] \u2019 15 . Section 1 : \u2018 [ ... ] pre-trained on * the * ImageNet dataset. \u2019 16 . Section 1 : \u2018 [ ... ] which could be any kind of * image * ( even random * noise * ) [ ... ] \u2019 . 17.Section 1 : \u2018 [ ... ] , which is the input to the last fully-connected linear layer before * the * softmax operation. \u2019 18 . Section 1 : \u2018 [ ... ] , and * one * fix to the problem could be using likelihood ratio [ ... ] \u2019 19 . Section 1 : \u2018 [ ... ] , we will show that * the * OOD sample \u2019 s likelihood score from the Glow model [ ... ] \u2019 20 . Section 2 : \u2018 [ ... ] projected gradient descent ( PGD ) which is used for adversarial * attacks * . \u2019 21 . Section 2 : \u2018 In practice , * Algorithm 1 can be repeated * many times [ ... ] \u2019 22 . Section 2 : \u2018 [ ... ] , the algorithm will have a better chance to avoid * a local minimum * [ ... ] \u2019 23 . Section 2 : \u2018 This * is * simply because the vectors in a lower-dimensional space [ ... ] \u2019 24 . Section 2 : \u2018 [ ... ] , which is the Pigeonhole Principle. \u2019 Citation ? 25.Section 2 : \u2018 Usually , * the * training set is only a subset \u2019 26 . \u2018 3 EXPERIMENT \u2019 Do not capitalize and \u2018 3 Experimental Evaluation \u2019 ? 27.Section 3 : \u2018 * A * Nvidia Titan V GPU was used [ ... ] \u2019 28 . Section 4 : \u2018 We would like to point out that it is * difficult * to evaluate [ ... ] \u2019 # # # # # * * References * * [ 1 ] B. Lakshminarayanan , A. Pritzel , and C. Blundell . Simple and scalable predictive uncertainty estimation using deep ensembles . In NIPS , pages 6402\u20136413 , 2017 . [ 2 ] K. Lee , H. Lee , K. Lee , and J. Shin . Training confidence-calibrated classifiers for detecting out-of-distribution samples . In ICLR , 2018 . [ 3 ] K. Lee , K. Lee , H. Lee , and J. Shin . A simple unified framework for detecting out-of-distribution samples and adversarial attacks . In NeurIPS , pages 7167\u20137177 , 2018 . [ 4 ] S. Liang , Y. Li , and R. Srikant . Enhancing the reliability of out-of-distribution image detection in neural networks . In ICLR , 2018 . [ 5 ] A. Meinke and M. Hein . Towards neural networks that provably know when they don \u2019 t know . In ICLR , 2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "First , we thank the reviewer for thorough reading and reviewing our manuscript . Next , we answer the specific questions from the reviewer . ( 1 ) `` I find the novelty of the presented algorithm and experimental findings to be rather low . '' Reply : we have provided new experimental findings . About the novelty , please read New Discussion ( 6 ) and ( 7 ) ( 2 ) `` The experimental evaluation of the proposed attack is limited to standard classifiers and does not include deep networks that have been trained to increase OOD robustness [ 1 , 2 , 4 , 3 ] . '' Reply : we evaluated the four methods in the new experiments ( 3 ) `` The defense solution is only a sketch and makes promises that are left to be validated . '' Reply : Please read New Discussion ( 8 ) ( 4 ) `` Though I see and agree that the OOD attack setting is slightly different to adversarial attacks , I find the brittleness of standard classifiers in this regard not surprising . In particular , the OOD attack has a greater degree of freedom since any arbitrary OOD input can be used as a starting point for perturbation ( pure noise is also OOD , as remarked in the paper ) , i.e.there is no similarity constraint on the input as there is for adversarial attacks . '' Reply : our new experiments show that eight classification-based OOD detection methods do not work well under the OOD attack , which is new information . ( 5 ) `` Moreover , I find the algorithmic novelty to be low as well , since the proposed algorithm essentially is a slight adaptation of previously introduced projected gradient descent ( PGD ) attacks . '' Reply : about the novelty , please read New Discussion ( 6 ) and ( 7 ) . ( 6 ) `` I think the current experimental evaluation is limited and the findings are not surprising for the two standard , pre-trained classifiers ( ResNet-18 and DenseNet-121 ) . There exist many approaches that have shown to improve OOD robustness [ 1 , 2 , 4 , 3 ] , which should be included in the analysis . It would be interesting to see how these approaches perform and compare , which could be insightful for improving OOD robustness . '' Reply : we evaluated the four methods in the new experiments . ( 7 ) `` Proposing an attack begs the question what possible defenses could be . Currently , the main paper is only phenomenological , i.e.demonstrates that OOD attacks are an open issue , but the description of a possible defense at the end of the paper and in the appendix makes only a solution claim which is left to be validated . I don \u2019 t say or think such a solution would be necessary for an interesting and valid contribution , as OOD detection poses a hard problem which likely lacks a simple solution , but the current solution is a mere sketch making promises with questions left open . For example , how can a sufficient space saturation be achieved with a finite sample in practice ? Which measure to use ? '' Reply : please read New Discussion ( 8 ) ( 8 ) `` I think the OOD detection problem , both from an attack and defense perspective is relevant and of great interest to the community , which is why I encourage the authors to build upon and extend the current manuscript . Some ideas : Including methods that have shown to improve OOD robustness [ 1 , 2 , 4 , 3 ] would greatly improve the value of the analysis . This could be insightful to further improve OOD robustness and see pros/cons of existing approaches . Does adversarial training , as mentioned in Section 2.2 , also help OOD robustness ? '' Reply : we have evaluated the four methods in new experiments . The Deep Ensembles ( NeurIPS 2017 , https : //arxiv.org/abs/1612.01474 ) used adversarial training , and the evaluation results show that it did not work . This is not surprising because noisy images with adversarial noises are still in-distribution ( e.g.a noisy image of a panda still is an image of a panda ) . Adversarial samples and OOD samples are completely different . ( 9 ) `` Implementing and validating the presented OOD detection solution would be interesting . '' Reply : please read New Discussion ( 8 ) ( 10 ) Further investigate the reason why the dimensionality-preserving Glow model can be attacked as well , as this suggests dimensionality reduction is only a part of the issue . '' Reply : please read New Discussion ( 2 ) ( 11 ) `` Could we design an evaluation method ( experimental or analytical ) that does not rely on OOD samples ? \u2019 I think this is a worthwhile question to ask and research . '' Reply : we thank the reviewer for this comment . ( 12 ) `` Add missing related work [ 5 ] '' Reply : we will add it to the revised manuscript . ( 13 ) `` Minor Comments\u2026 '' Reply : we thank the reviewer for the comments , and we will incorporate those into the revised manuscript ."}, "1": {"review_id": "6fb4mex_pUT-1", "review_text": "Summary of paper : this work shows that adversarial perturbations can make any OOD image , map to the same latent code as an in-distribution - creating an attack on confidence-based or flow-based ODD detection methods . Results are shown on a few datasets with some attempts at evaluation . Novelty : Unfortunately , I do n't think there is much new here . Adversarial attacks are of course well known - I am not sure that attacks on intermediate latent codes present novelty either . Adversarial attacks against anomaly detection methods have also been investigated before ( e.g . [ 1 ] [ 2 ] , although their setting is a little different ) and there is nothing in the proposed method that is particularly tailored to OOD . Evaluation : the evaluation is not extensive - only one adversarial attack is investigated and no reasonable baselines have been selected . I am not sure that MAPE is an appropriate metric - it really depends on the allowed perturbation . Ss the allowed perturbation small enough ? do the perturbed images look realistic - if I understood Fig.12 , they do n't - but the caption there is not clear . Clarity - the paper is not particularly clearly written - although the idea is simple enough . E.g.I do n't see the scatter plots clearly explained , the analysis in Sec.2.2 is very dense for a fairly simple idea . Overall : ultimately , this is conceptually repeating the same thing as any other adversarial examples work , perturbations can make a network confident that any image has the label of another image - and this obviously would overcome confidence based OOD detection methods . I therefore do not see a strong contribution by this work . As there is also very limited methodological novelty , I do not think it should be accepted . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # I understand the distinction the authors are trying to draw between adversarial examples for anomaly detection and fooling OOD to think that images are in distribution where in fact they are OOD . I still do n't think that technically or conceptually , there is much difference . The authors presented many fresh results during the rebuttal ( which might have been better presented just as a table in the manuscript , rather than on this thread ) . The experiments can form a part of a resubmission of this work , that will incorporate the extensive comments presented by the current reviews . [ 1 ] Rigaki , Maria . `` Adversarial deep learning against intrusion detection classifiers . '' ( 2017 ) . [ 2 ] Bergman and Hoshen , Classification-based anomaly detection for general data , ICLR'20", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for thorough reading and reviewing our manuscript . Here , we answer the specific questions from the reviewer . ( 1 ) '' Unfortunately , I do n't think there is much new here . Adversarial attacks are of course well known - I am not sure that attacks on intermediate latent codes present novelty either . Adversarial attacks against anomaly detection methods have also been investigated before ( e.g . [ 1 ] [ 2 ] , although their setting is a little different ) and there is nothing in the proposed method that is particularly tailored to OOD . '' Reply : we presented an approach to generate OOD samples and evaluate OOD detection methods , which is not done in the eight OOD detection papers published at ICLR and NeurIPS . We note that adversarial attack and OOD attack are doing completely different things to neural networks . [ 1 ] Rigaki , Maria . `` Adversarial deep learning against intrusion detection classifiers . '' ( 2017 ) .In this paper , FSGM and JSMA methods are used to generate adversarial samples . This is a study of adversarial robustness , NOT OOD Attack issue presented in our manuscript , NOT the OOD detection issue defined in the baseline method ( ICLR 2017 ) in https : //arxiv.org/abs/1610.02136 and investigated in the other seven papers at ICLR and NeurIPS . [ 2 ] Bergman and Hoshen , Classification-based anomaly detection for general data , ICLR'20 . For image related applications , this paper investigated a special case of OOD detection , also known as one class classification , which is very different from the OOD detection tasks on dataset level : e.g.MNIST to be in-distribution and Omniglot to be OOD as demonstrated in the paper of the baseline method . Our study focused on OOD detection issues on the dataset level . ( 2 ) `` Evaluation : the evaluation is not extensive - only one adversarial attack is investigated and no reasonable baselines have been selected . I am not sure that MAPE is an appropriate metric - it really depends on the allowed perturbation . Ss the allowed perturbation small enough ? do the perturbed images look realistic - if I understood Fig.12 , they do n't - but the caption there is not clear . '' Reply : adversarial attack and OOD attack are doing completely different things . For an adversarial attack , the perturbation must not be too large , and the perturbed images should be human-recognizable . For an OOD attack , the generated OOD samples can be arbitrary , such as random noises or wired images . By the definition of OOD , an OOD sample does not and should not look similar to the in-distribution samples . ( 3 ) `` Clarity - the paper is not particularly clearly written - although the idea is simple enough . E.g.I do n't see the scatter plots clearly explained '' Reply : As explained by the figure captions , the scatter plots show the feature vectors ( z ) . ( 4 ) `` the analysis in Sec.2.2 is very dense for a fairly simple idea. `` Reply : OOD is a complex issue , and it should be thoroughly analyzed . ( 5 ) `` Overall : ultimately , this is conceptually repeating the same thing as any other adversarial examples work , perturbations can make a network confident that any image has the label of another image - and this obviously would overcome confidence based OOD detection methods . I therefore do not see a strong contribution by this work . As there is also very limited methodological novelty , I do not think it should be accepted . '' Reply : Adversarial attack and OOD attack are doing completely different things to neural networks . Please see our new experiments and discussion . We note that we have evaluated eight dataset-level OOD detection methods , and we believe our work has made a considerable contribution to the field ."}, "2": {"review_id": "6fb4mex_pUT-2", "review_text": "Summary : -- The paper presents a method that attacks existing out-of-distribution ( OOD ) detection methods . Most of the existing OOD detection methods perform detection using a latent representation . Main motivation of the paper is that the size of the latent representation is much smaller than the input images which results mapping both OOD and in-distribution images to the same place in the latent space and diminishing OOD detection performance . With this motivation , the proposed method perturbs input images to obtain an image whose latent representation is similar to the latent representation of an in-distribution image . Since such perturbations can be obtained for any OOD image , existing OOD detection algorithms fails distinguishing such OOD samples . The paper contains experiments on multiple dataset to demonstrate that the proposed method obtains a latent representation similar to the representation of an in-distribution image . Comments : - 1 - From the abstract , I infer that the paper has roughly 3 contributions : 1 ) the paper shows existing OOD detection methods are practically breakable , 2 ) Glow likelihood-based OOD detection is ineffective , and 3 ) present a simple theoretical solution with guaranteed performance for OOD detection . However , the 3rd contribution is never mentioned/introduced in the paper until Appendix A where they briefly presents 2 different implementations of an OOD detection method idea . I would not consider this as a contribution since there is no experimental evaluation showing the OOD detection performance of the idea . Also , since this contribution is mentioned in the abstract , I would expect seeing its description and the results in the main paper rather than the Appendix . 2 - Until the end of the Introduction , it is not very clear that the main contribution of the paper is a method that attacks OOD detection methods . I liked the motivating example in Figure 1 , but the contribution can be given in a more clear way . 3 - In the last paragraph of page 2 , it is mentioned that the clip operator can ensure x_out to be OOD after a small modification to x'_out . I found this statement quite vague . How this operation `` ensures '' that x_out to be OOD after the modification ? 4 - In the last paragraph of Section 2.1 , it is mentioned that adding initial random noise helps to avoid local minimum caused by a bad initialization . Is this something that you observed empirically ? The contribution of the noise is not very clear to me and I think showing results with and without the noise would be very useful to demonstrate how the noise helps to avoid local minima . Also , it would be interesting to show how this loss evolves during the optimization . 5 - In Sec 2.2 - Each pixel in an 8-bit image can take 256 different values . So , for an image with size 224x224x3 , there are 256^ ( 224x224x3 ) possible images that can be generated ; not 8^ ( 224x224x3 ) . 6 - I did n't quite understand the message in the last paragraph of Sec.2.2.Why \\Omega_in is split into \\Omega_ { in_clean } and \\Omega_ { in_noisy } ? How this is used in the proposed method ? 7 - I think that experimental evaluations are not comprehensive enough , 7.1 . It is mentioned that implementing OOD detection methods are not necessary since Alg.1 already produces z_out ~= z_in . I do n't agree with this since most of the SoTA methods do not rely on only a single representation but multiple representations at different layers [ ref1 , ref2 , ref3 ] . Therefore , it is crucial to show the performance loss of these methods due to the proposed method . [ ref1 ] Erdil et al. , Unsupervised out-of-distribution detection using kernel density estimation [ ref2 ] Lee et al. , A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks [ ref3 ] Sastry et al. , Detecting Out-of-Distribution Examples with Gram Matrices 7.2 . Benchmark datasets that have been used in OOD detection paper are usually different than the ones used in the paper . Please see the references above . I wonder why the datasets choice are different than the standard benchmarks ? 7.3.As mention in paragraph 3 of Sec.4. , there are other generative models that show promising results . However , I did n't quite understand comparison with these methods are `` out of the scope '' while Glow is relevant . 8.The parameters used in different experiments are different than each other . How are these parameters determined ? How sensitive is the proposed method to parameter choice ? Minor comments : - 1 - In Introduction paragraph 2 , there is a typo : laten - > latent 2 - In Sec 2.1 , paragraph 3 : the algorithm is referred as `` above algorithm '' but it appears `` below '' in the paper . 3 - In Sec 2.2 , paragraph 3 , there is a typo : state-of-art - > state-of-the-art 4 - In Sec 2.2 , paragraph 3 , there is a typo : laten - > latent Overall : -- I found the idea of attacking existing OOD detection methods interesting . However , I believe , this paper needs improvement in terms of clarity of the presentation , experimental evaluations and the presentation of the contributions . Therefore , my initial rating is reject for this paper .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for thorough reading and reviewing our manuscript . Next , we answer the specific questions from the reviewer . ( 1 ) `` From the abstract , I infer that the paper has roughly 3 contributions : 1 ) the paper shows existing OOD detection methods are practically breakable , 2 ) Glow likelihood-based OOD detection is ineffective , and 3 ) present a simple theoretical solution with guaranteed performance for OOD detection . However , the 3rd contribution is never mentioned/introduced in the paper until Appendix A where they briefly presents 2 different implementations of an OOD detection method idea . I would not consider this as a contribution since there is no experimental evaluation showing the OOD detection performance of the idea . Also , since this contribution is mentioned in the abstract , I would expect seeing its description and the results in the main paper rather than the Appendix '' . Reply : In the new experiments , we have shown that eight OOD detection methods are breakable under the OOD Attack . Please see New Discussion ( 8 ) for the theoretical solution . ( 2 ) `` Until the end of the Introduction , it is not very clear that the main contribution of the paper is a method that attacks OOD detection methods . I liked the motivating example in Figure 1 , but the contribution can be given in a more clear way . '' Reply : We will explicitly state the contribution in the revised manuscript . ( 3 ) `` In the last paragraph of page 2 , it is mentioned that the clip operator can ensure x_out to be OOD after a small modification to x'_out . I found this statement quite vague . How this operation `` ensures '' that x_out to be OOD after the modification ? '' Reply : we thank the reviewer for pointing out this issue . `` ensure '' is not the right word , and we will change the description to `` the clip operator will limit the difference between x'_out and x_out so that x_out may be OOD '' . ( 4 ) `` In the last paragraph of Section 2.1 , it is mentioned that adding initial random noise helps to avoid local minimum caused by a bad initialization . Is this something that you observed empirically ? The contribution of the noise is not very clear to me and I think showing results with and without the noise would be very useful to demonstrate how the noise helps to avoid local minima . Also , it would be interesting to show how this loss evolves during the optimization . '' Reply : adding initial random noise is the standard procedure in the PGD optimization technique , and it is theoretically useful . Since in most of our experiments , we use random noise images as the initial samples , this step could be skipped . We will add a figure of loss curves in the Appendix of the revised manuscript , which is basically a curve going down and becoming flat . ( 5 ) `` In Sec 2.2 - Each pixel in an 8-bit image can take 256 different values . So , for an image with size 224x224x3 , there are 256^ ( 224x224x3 ) possible images that can be generated ; not 8^ ( 224x224x3 ) . '' Reply : we thank the reviewer for pointing out this bug . ( 6 ) `` I did n't quite understand the message in the last paragraph of Sec.2.2.Why \\Omega_in is split into \\Omega_ { in_clean } and \\Omega_ { in_noisy } ? How this is used in the proposed method ? '' Reply : this is used to explain the feasibility of adversarial training using noisy images generated by adversarial attacks , NOT used in OOD Attack method . An adversarial attack will add a small amount of noise to the input , and the noisy image is still human-recognizable , e.g. , a noisy image of a panda still is an image of a panda . The noisy image of the panda is in \\Omega_ { in_noisy } and the clean image of the panda is in \\Omega_ { in_clean } ."}, "3": {"review_id": "6fb4mex_pUT-3", "review_text": "Summary : The paper defines an out-of-distribution attack , a process which drives an out-of-distribution ( OOD ) input to have the same latent representation to an inlier . The paper also analyzes that an encoder is inevitably vulnerable to out-of-distribution attack when its latent dimensionality is smaller than the dimensionality of the input . Decision : Reject Strength : The paper addresses an important vulnerability of classifier-based OOD detection . As classifier-based method is one of the currently dominating approaches for OOD detection , investigating its weakness is a significant contribution to the research community . Weakness : The proposed attack algorithm is significantly similar to the previously known adversarial attack algorithms and therefore seems trivial . The main quantitative result , Table 1 , is not very convincing . I suggest the authors provide AUC scores computed before and after OOD attack , so that the difference clearly shows that the proposed attack causes a decrease in OOD detection performance . The paper should benchmark the proposed attack algorithm against state-of-the-art OOD detection methods . Currently , only a relatively simple method of Hendrycks and Gimpel , 2016 is used . The method should include at least [ 1,2,3 ] to show the effectiveness of the proposed attack . At least some of these OOD detection methods may be able to resist the proposed attack . For example , multiple hidden layer representations from a classifier are used in [ 1 ] , and therefore it can still detect OOD even if a specific latent representation is under attack . The organization of the paper needs to be improved . In the last sentence of the abstract , `` a simple theoretical solution '' is mentioned but is only addressed in Appendix . If it is a contribution that is important enough to be mentioned in the abstract , it should be covered in depth in the main manuscript instead of Appendix . Minor comments : - The visibility of figures are poor . The axis titles and MAPE values in Figure 2 , 3 , 4 , 5 should be larger . - In Section 2.2 , 8^ { 224 * 224 * 3 } should 256^ { 224 * 224 * 3 } . An 8-bit integer can represent 256 values . - The captions of Figure 3 and Figure 4 are the same . - The captions of Figure 5 and Figure 6 are too close . Typos : laten \u2192 latent ( Section 1 paragraph 2 line 10 ) Dicussion \u2192 Discussion ( Section 4 title ) Difficulty \u2192 difficult ( Section 4 paragraph 4 first line ) [ 1 ] Lee , Kimin , et al . `` A simple unified framework for detecting out-of-distribution samples and adversarial attacks . '' Advances in Neural Information Processing Systems . 2018 . [ 2 ] Liang , Shiyu , Yixuan Li , and Rayadurgam Srikant . `` Enhancing the reliability of out-of-distribution image detection in neural networks . '' arXiv preprint arXiv:1706.02690 ( 2017 ) . [ 3 ] Grathwohl , Will , et al . `` Your classifier is secretly an energy based model and you should treat it like one . '' arXiv preprint arXiv:1912.03263 ( 2019 ) .", "rating": "3: Clear rejection", "reply_text": "First , we thank the reviewer for thorough reading and reviewing our manuscript . Next , we answer the specific questions from the reviewer . ( 1 ) `` The proposed attack algorithm is significantly similar to the previously known adversarial attack algorithms and therefore seems trivial . '' Reply : about the novelty , please read New Discussion ( 6 ) and ( 7 ) . We note that adversarial attack and OOD attack are doing completely different things to neural networks . Please read the Clarification . ( 2 ) `` The main quantitative result , Table 1 , is not very convincing . I suggest the authors provide AUC scores computed before and after OOD attack , so that the difference clearly shows that the proposed attack causes a decrease in OOD detection performance . '' Reply : we evaluated eight OOD detection methods in the new experiments . What are `` AUC scores computed before OOD attack '' ? the AUC scores in the papers of the OOD detection methods ? ( 3 ) `` The paper should benchmark the proposed attack algorithm against state-of-the-art OOD detection methods . Currently , only a relatively simple method of Hendrycks and Gimpel , 2016 is used . The method should include at least [ 1,2,3 ] to show the effectiveness of the proposed attack . At least some of these OOD detection methods may be able to resist the proposed attack . For example , multiple hidden layer representations from a classifier are used in [ 1 ] , and therefore it can still detect OOD even if a specific latent representation is under attack . '' Reply : we evaluated the three OOD detection methods in the new experiments . ( 4 ) `` The organization of the paper needs to be improved . In the last sentence of the abstract , `` a simple theoretical solution '' is mentioned but is only addressed in Appendix . If it is a contribution that is important enough to be mentioned in the abstract , it should be covered in depth in the main manuscript instead of Appendix . '' Reply : please read New Discussion ( 8 ) ( 5 ) `` Minor comments\u2026 '' Reply : we thank the reviewer for the comments , and we will revise the manuscript accordingly . The figures are in high-resolution ; please zoom in on the computer screen ."}}