{"year": "2021", "forum": "yvQKLaqNE6M", "title": "You Only Need Adversarial Supervision for Semantic Image Synthesis", "decision": "Accept (Poster)", "meta_review": "The paper received 3 reviews with positive ratings: 7,6,7. The reviewers appreciated overall quality of the manuscript, thoroughness of the evaluation, and practical importance of this work (mentioning though that the technical novelty is still not high). They also acknowledged impressive empirical performance. The authors provided detailed responses to each of the reviews separately, which seemed to have resolved the remaining concerns.\nAs a result, the final recommendation is to accept this work for presentation at ICLR as a poster.", "reviews": [{"review_id": "yvQKLaqNE6M-0", "review_text": "# Post Rebuttal : Thank you for the detailed rebuttal . The comment made about being able to use this in a semi-supervised setting is an exciting direction and I encourage the authors to pursue it on larger less-labeled datasets mentioned in the review in a future work/final submission . I am glad that removing VGG improved the results on COCO . Ultimately , I am keeping my score at 7 , accept . # Pre-Rebuttal : Summary = This paper provides an improvement to the SPADE+ architecture that removes the need for a perceptual loss mechanism while also quantitatively improving the outputs with respect to FID/mmIOU . The paper achieves this while reducing the number of params and removing the ImageNet bias of the original perceptual VGG architecture . The authors evaluate their architecture on 3 datasets : ADE20K , Cityscapes and COCO-stuff . They do while using only adversarial supervision . They achieve this by proposing label mix to train the network by having the discriminator output class labels including an additional `` fake '' label in addition to a variety of other optimizations . Overall , I vote for accepting this paper . It significantly improves on the SPADE+ architecture by removing a pre-trained component and reducing the number of params . The technique seems to improve upon prior work qualitatively , quantitatively , and performance-wise . Pros == 1 . The paper is very well written . 1.The paper does provide a very thorough comparison with prior works . 1.This paper improves on the results of prior methods while reducing the # parameters and the need for imagenet pretraining . Cons : == 1 . Pixel level semantic segmentations are difficult and expensive to generate . This limits the number of datasets that can be used for these work . In this sense , the data is highly supervised . 1.That being said , I would love to the results on a more challenging dataset like LVIS and/or Open Images . For Open Images , it would be interesting to see if the performance scales with 350+ classes . For the former , it would be interesting to see how well it does on objects that appear very rarely . ( I do n't expect the results on LVIS to be good , but I would expect them to be better than SPADE+ ) . 1.Architectural ablations only cover the first two datasets , any reasons why there are none for COCO-stuff ? One of the main claims of the paper , ( that this technique does better without VGG ) , is not evaluated on COCO-stuff .", "rating": "7: Good paper, accept", "reply_text": "* * R4-3 : * * * \u201c Architectural ablations only cover the first two datasets , any reasons why there are none for COCO-stuff ? One of the main claims of the paper , ( that this technique does better without VGG ) , is not evaluated on COCO-stuff. \u201d * We conducted our ablations on ADE20k and Cityscapes due to their shorter training time . Training models on COCO-stuff can take up to 4 weeks , while on ADE20K and Cityscapes it takes circa 10 days . Therefore , we only executed for a few ablations on COCO-stuff . In particular , we conducted experiments where OASIS is trained in the absence of 3D noise , with and without VGG . The results are as follows : With VGG , FID and mIoU are 18.0 and 44.2 . Without VGG , FID and mIoU are 16.7 and 45.5 . Thus , OASIS without VGG performs better . For comparison , our final OASIS model ( trained without VGG but with noise ) achieves FID of 17.0 and mIoU of 44.1 , with the added benefit of generating multi-modal images via the 3D noise sampling . Lastly , training SPADE+ without VGG leads to collapse , which once more validated the dependence of SPADE+ on VGG . The above results are aligned with our findings on ADE20K and Cityscapes . We present these additional results on COCO-stuff in Appendix A8 and Table J ."}, {"review_id": "yvQKLaqNE6M-1", "review_text": "Post-rebuttal : I acknowledge reading the rebuttal as well as other reviewers comments . I 'm satisfied with the rebuttal , I think that the authors have addressed many of my initial comments and I 'm happy to increase the score of the paper to 7 . If the paper gets accepted to the conference I would encourage the authors to include and expand a discussion about method limitations in the main body of the paper . - Pre-rebuttal : The paper deals with semantic image synthesis and proposes a model that can generate images from segmentation masks . The main methodological contribution of the paper is a segmentation-based discriminator . By using such discriminator , the model is able to learn high quality image syntheses . The proposed pipeline is evaluated on multiple datasets ( ADE20K , Cityscapes and COCO-stuff ) . The qualitative and quantitative results highlight good image quality of the proposed pipeline . Pros : Solid model evaluation including large number of ablations . Simple and intuitive pipeline design that is shown to produce high quality image syntheses . Well written paper and well structured appendix . Cons : Although the synthesized image quality is improved it seems to come at the expense of image diversity . The terms used in the papers are sometimes confusing and could be improved . Although the paper uses standard evaluation metrics for image generation ( such as FID ) , their use for conditional image synthesis is is not fully justified . Detailed review and comments : Abstract : - > `` semantic image synthesis GAN '' - I 'm not convinced with this wording used in the paper . It mixes task ( semantic image synthesis ) with a type of generative model ( GAN ) . This introduces some confusions in the text , e.g . `` semantic image synthesis GAN models still greatly suffer from poor image quality when trained with only adversarial supervision '' that might be a bit unclear since GANs are always trained with only adversarial supervision . I would recommend to simply use `` semantic image synthesis '' . Introduction : - > Conditional GANs references : Goodfellow et al 2014 is a good reference for unconditional GAN , not conditional GAN setup . Currently , this reference is linked with other references to conditional reconstruction . - > Since this is an application oriented paper I think that the introduction would benefit from motivating a bit the task that is under study in this paper . e.g.why semantic image synthesis is of interest to ICLR community ? - > `` Although the perceptual loss substantially improves the accuracy of previous methods , it comes with the computational overhead introduced by utilizing an extra network for training . Moreover , it usually dominates over the adversarial loss during training , introducing a bias towards ImageNet , which can have a negative impact on the diversity and quality of generated images , as we show in our experiments . '' - I 'm not convinced if the diversity reduction when using the perceptual loss is due to the bias towards ImageNet . The reduction of diversity is rather an effect of perceptual loss overall . Could the authors comment on this ? Prior work references : - > This reference ( https : //arxiv.org/pdf/2004.03590.pdf ) seems relevant . Could the authors comment on this reference in the paper ? Adding comparison to this work would make the paper even stronger . Methodology : - > I find the wording of `` 3D noise sampling '' a bit unclear . Is it just a resized version of 1D tensor ? Is there any importance on how the 3D noise sampling is structured ? Could the authors clarify the idea of 3D noise sampling in the main body of the paper ? Experiments : - > Could the authors comment why FID is a good metric for semantic image synthesis ? Justifying the choice of metrics would make the evaluation section stronger . From the discussion of the results , ( diversity quality tradeoffs ) it seems that the paper would benefit from reporting precision-recall like metrics for GANs ( e.g.https : //arxiv.org/abs/1807.09499 , https : //arxiv.org/abs/1905.10887 , https : //arxiv.org/abs/1904.06991 ) . Adding additional metrics would further improve the paper . - > Table 2 . It is unclear to me what is the meaning of red color numbers . Could the authors add clarifications about it to the caption ? - > Table 2 . It is bit surprising to see that the model trained only with adversarial loss has relatively low diversity . I would expect that the lack of perceptual loss would lead also to increased diversity in synthesized images . Could the authors comment on what is the limiting factor to increase sample diversity ? Could the diversity be affected by the proportion of mixing ( real vs. fake ) in LabelMix ? - > Would it be possible to add qualitative results for SPADE+ ? This model seems to have high diversity scores and it would be interesting to see model samples . - > The paper lacks discussion on the limitations of the method . Could the authors comment on the potential limitations of the suggested approach ?", "rating": "7: Good paper, accept", "reply_text": "* * R1-10 : * * * \u201c This reference ( https : //arxiv.org/pdf/2004.03590.pdf ) seems relevant . Could the authors comment on this reference in the paper ? Adding comparison to this work would make the paper even stronger. \u201d * Thanks for pointing towards that reference . The referenced paper proposes an interesting approach to semantic synthesis , based on the relatively novel principle of implicit maximum likelihood estimation . We now comment on this reference in the revision in the related work section on page 3 . However , direct quantitative comparison to OASIS and preceding work such as SPADE and CC-FPSE is difficult for several reasons : ( 1 ) The paper uses a different evaluation protocol . Instead of employing FID and mIoU for evaluation , it uses LPIPS and crowd-sourced human judgment . ( 2 ) This work does not also evaluate on the standard label-to-image benchmarks , such as ADE20K , Cityscapes and COCO , but instead uses the GTA5 dataset and the BDD100K dataset . ( 3 ) The proposed model is not quantitatively compared to state-of-the-art GAN models for semantic image synthesis . The closest possible comparison is to visually compare their BDD100K samples to the Cityscapes samples of OASIS , since both datasets consist of real urban street scenes . From visually inspecting Fig.14 in the given reference , it is evident that the synthesized street scenes have blurry textures and lack detail . In contrast , OASIS synthesizes less blurry images with more realistic colors and a lot more detail , such as windows on house facades and driving lanes on the streets ( see Appendix Fig.D ) . $ $ * * R1-11 : * * * \u201c I find the wording of `` 3D noise sampling '' a bit unclear . Is it just a resized version of 1D tensor ? Is there any importance on how the 3D noise sampling is structured ? Could the authors clarify the idea of 3D noise sampling in the main body of the paper ? \u201d * During training , the 3D noise tensor is sampled globally , i.e.per-channel , where we sample each channel value and copy it along the height ( H ) and width ( W ) dimension of the tensor . In other words , globally sampled 3D noise can be viewed as a spatially replicated version of a 1D tensor . We have also investigated other ways of structuring the 3D noise tensor during training in Appendix A7 . Based on our analysis we chose global sampling for training , due to its simplicity and competitive performance with other considered strategies . We added a clarification about the 3D noise sampling in section 3.3 on page 6 . Note that during inference , the 3D noise can be sampled in various ways : ( 1 ) globally as during the training , i.e.sampling per-channel and replicating each channel value spatially to the whole tensor ; ( 2 ) locally , sampling different channel values area-wise . The option ( 1 ) entirely changes the image . The option ( 2 ) allows us to only change parts of the image . For example , based on the label map , we can extract the area of sky . Through the option ( 2 ) , different noise realizations in the area corresponding to the sky segment lead to different skies in the generated images without changing the rest of the scene ( see Fig.F in the Appendix or Fig.2 ) . $ $ * * R1-12 : * * * \u201c Table 2 . It is unclear to me what is the meaning of red color numbers . Could the authors add clarifications about it to the caption ? \u201d * Table 2 reports the results of the ablation on the use of VGG and 3D noise , showing their impacts on the diversity and quality performance of SPADE+ and OASIS . As we observe a trade-off between diversity and quality , it is important to jointly assess a model from both aspects . Therefore , we make not only the best numbers bold , but also mark the worst numbers in red . For instance , SPADE+ with 3D noise without VGG has the best diversity performance ( bold ) , but FID and mIoU are strongly compromised ( red ) . This indicates that the diversity gain is spurious , being a side effect of a poor quality of synthesized images that are naturally more different from each other . OASIS with 3D noise and VGG , has the best mIoU performance ( bold ) , but the diversity is worse ( red ) than the same version w/o VGG . This indicates that the model sacrifices variety to generate easy-to-segment images . We added a clarification in the caption . $ $ * * R1-13 : * * * \u201c Would it be possible to add qualitative results for SPADE+ ? This model seems to have high diversity scores and it would be interesting to see model samples. \u201d * Thanks for the suggestion . We added qualitative results for SPADE+ ( with VGG ) in Appendix Figure E. Please note that despite its improved quality over original SPADE , this model still depends on the VGG loss and has very low diversity ( The MS-SSIM for SPADE+ in Table 2 is 0.85 , while OASIS has 0.65 , which indicates that SPADE+ has lower diversity ) ."}, {"review_id": "yvQKLaqNE6M-2", "review_text": "In this paper , the authors approach the problem of conditional image generation via generative adversarial networks . To this end , they propose an approach that utilizes only semantic segmentation annotations and adversarial loss . No perceptual loss is required . Their discriminator leverages semantic labels to improve the image generations . They evaluate their approach on a variety of datasets including ADE20K , COCO , and CityScapes . They demonstrate substantial quantitative and qualitative performance over baselines and perform an ablation analysis . Pros : 1.The problem of mask conditioned image generation has immediate applications in computational photography and computer graphics . 2.The paper is well written and easy to understand . 3.Performance is impressive and experimental evaluation is thorough . The authors perform an ablation analysis . 4.The proposed techniques ( Segment based discriminator , 3d noise vector ) are useful insights into tuning GANs to perform well . Cons : 1.Technical novelty is limited . The contributions ( segment discriminator and 3D noise ) are useful engineering increments but not particularly large insights into the workings of image generation . 2.While the need for VGG based perceptual loss may not be needed here , it seems like the perceptual loss has just been shifted to the spatial realm . There is still a dependence on strong semantic constraints . Instead of using image class labels to train features , the per pixel class labels are used . In summary I believe the approach shows promise and important tweaks for image generation networks . Performance is good . However , the technical novelty is limited .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the positive feedback ! We would like to take the opportunity to share our perspective on the two cons you have raised : $ $ * * R3-1 : * * * \u201c Technical novelty is limited . The contributions ( segment discriminator and 3D noise ) are useful engineering increments but not particularly large insights into the workings of image generation. \u201d * Current GAN models for semantic image synthesis rely on external networks ( VGG ) for training stability and synthesis quality . While these methods achieve good results , the external supervision imposes model bias and dataset bias . Our contribution is to remove the need for such external supervision . By removing the bias , we improve synthesis quality and diversity . Thus , through our technical contributions , GAN models for semantic image synthesis can now behave like other cGANs : The model is sensitive to noise and no external supervision is needed . Lastly , our technical contributions enable new features such as ( 1 ) local editing of generated images , ( 2 ) segmenting and re-synthesizing real images ( see R4-1 , App.B4 and Figure K ) as well as ( 3 ) local class-balancing , which helps training on imbalanced datasets . $ $ * * R3-2 : * * * \u201c While the need for VGG based perceptual loss may not be needed here , it seems like the perceptual loss has just been shifted to the spatial realm . There is still a dependence on strong semantic constraints . Instead of using image class labels to train features , the per pixel class labels are used. \u201d * Thanks a lot for your perspective ! We would like to provide an argument for why our method reduces supervision , and why the perceptual loss and the adversarial segmentation loss are not equivalent : By design semantic image synthesis is a task where pairs of label maps and real images are given . Note that all previous works also provide the image and the label map to the discriminator . However , in our work , we make more effective use of the label maps , through the proposed adversarial segmentation loss . This enables the generation of high-quality images without the need for extra supervision from an external perceptual network ( VGG ) . Since the use of VGG was crucial for previous works , but not for OASIS , we reduce the amount of supervision needed . Further , the use of the perceptual loss and adversarial segmentation loss is not equivalent : the perceptual loss matches features based on what VGG learned from ImageNet , independent of the ground truth semantic labels . In contrast , the adversarial segmentation loss explicitly takes the ground truth semantic labels into account . This also has the immediate benefit of better alignment with the ground truth label maps , as shown by the improvement in mIoU ."}], "0": {"review_id": "yvQKLaqNE6M-0", "review_text": "# Post Rebuttal : Thank you for the detailed rebuttal . The comment made about being able to use this in a semi-supervised setting is an exciting direction and I encourage the authors to pursue it on larger less-labeled datasets mentioned in the review in a future work/final submission . I am glad that removing VGG improved the results on COCO . Ultimately , I am keeping my score at 7 , accept . # Pre-Rebuttal : Summary = This paper provides an improvement to the SPADE+ architecture that removes the need for a perceptual loss mechanism while also quantitatively improving the outputs with respect to FID/mmIOU . The paper achieves this while reducing the number of params and removing the ImageNet bias of the original perceptual VGG architecture . The authors evaluate their architecture on 3 datasets : ADE20K , Cityscapes and COCO-stuff . They do while using only adversarial supervision . They achieve this by proposing label mix to train the network by having the discriminator output class labels including an additional `` fake '' label in addition to a variety of other optimizations . Overall , I vote for accepting this paper . It significantly improves on the SPADE+ architecture by removing a pre-trained component and reducing the number of params . The technique seems to improve upon prior work qualitatively , quantitatively , and performance-wise . Pros == 1 . The paper is very well written . 1.The paper does provide a very thorough comparison with prior works . 1.This paper improves on the results of prior methods while reducing the # parameters and the need for imagenet pretraining . Cons : == 1 . Pixel level semantic segmentations are difficult and expensive to generate . This limits the number of datasets that can be used for these work . In this sense , the data is highly supervised . 1.That being said , I would love to the results on a more challenging dataset like LVIS and/or Open Images . For Open Images , it would be interesting to see if the performance scales with 350+ classes . For the former , it would be interesting to see how well it does on objects that appear very rarely . ( I do n't expect the results on LVIS to be good , but I would expect them to be better than SPADE+ ) . 1.Architectural ablations only cover the first two datasets , any reasons why there are none for COCO-stuff ? One of the main claims of the paper , ( that this technique does better without VGG ) , is not evaluated on COCO-stuff .", "rating": "7: Good paper, accept", "reply_text": "* * R4-3 : * * * \u201c Architectural ablations only cover the first two datasets , any reasons why there are none for COCO-stuff ? One of the main claims of the paper , ( that this technique does better without VGG ) , is not evaluated on COCO-stuff. \u201d * We conducted our ablations on ADE20k and Cityscapes due to their shorter training time . Training models on COCO-stuff can take up to 4 weeks , while on ADE20K and Cityscapes it takes circa 10 days . Therefore , we only executed for a few ablations on COCO-stuff . In particular , we conducted experiments where OASIS is trained in the absence of 3D noise , with and without VGG . The results are as follows : With VGG , FID and mIoU are 18.0 and 44.2 . Without VGG , FID and mIoU are 16.7 and 45.5 . Thus , OASIS without VGG performs better . For comparison , our final OASIS model ( trained without VGG but with noise ) achieves FID of 17.0 and mIoU of 44.1 , with the added benefit of generating multi-modal images via the 3D noise sampling . Lastly , training SPADE+ without VGG leads to collapse , which once more validated the dependence of SPADE+ on VGG . The above results are aligned with our findings on ADE20K and Cityscapes . We present these additional results on COCO-stuff in Appendix A8 and Table J ."}, "1": {"review_id": "yvQKLaqNE6M-1", "review_text": "Post-rebuttal : I acknowledge reading the rebuttal as well as other reviewers comments . I 'm satisfied with the rebuttal , I think that the authors have addressed many of my initial comments and I 'm happy to increase the score of the paper to 7 . If the paper gets accepted to the conference I would encourage the authors to include and expand a discussion about method limitations in the main body of the paper . - Pre-rebuttal : The paper deals with semantic image synthesis and proposes a model that can generate images from segmentation masks . The main methodological contribution of the paper is a segmentation-based discriminator . By using such discriminator , the model is able to learn high quality image syntheses . The proposed pipeline is evaluated on multiple datasets ( ADE20K , Cityscapes and COCO-stuff ) . The qualitative and quantitative results highlight good image quality of the proposed pipeline . Pros : Solid model evaluation including large number of ablations . Simple and intuitive pipeline design that is shown to produce high quality image syntheses . Well written paper and well structured appendix . Cons : Although the synthesized image quality is improved it seems to come at the expense of image diversity . The terms used in the papers are sometimes confusing and could be improved . Although the paper uses standard evaluation metrics for image generation ( such as FID ) , their use for conditional image synthesis is is not fully justified . Detailed review and comments : Abstract : - > `` semantic image synthesis GAN '' - I 'm not convinced with this wording used in the paper . It mixes task ( semantic image synthesis ) with a type of generative model ( GAN ) . This introduces some confusions in the text , e.g . `` semantic image synthesis GAN models still greatly suffer from poor image quality when trained with only adversarial supervision '' that might be a bit unclear since GANs are always trained with only adversarial supervision . I would recommend to simply use `` semantic image synthesis '' . Introduction : - > Conditional GANs references : Goodfellow et al 2014 is a good reference for unconditional GAN , not conditional GAN setup . Currently , this reference is linked with other references to conditional reconstruction . - > Since this is an application oriented paper I think that the introduction would benefit from motivating a bit the task that is under study in this paper . e.g.why semantic image synthesis is of interest to ICLR community ? - > `` Although the perceptual loss substantially improves the accuracy of previous methods , it comes with the computational overhead introduced by utilizing an extra network for training . Moreover , it usually dominates over the adversarial loss during training , introducing a bias towards ImageNet , which can have a negative impact on the diversity and quality of generated images , as we show in our experiments . '' - I 'm not convinced if the diversity reduction when using the perceptual loss is due to the bias towards ImageNet . The reduction of diversity is rather an effect of perceptual loss overall . Could the authors comment on this ? Prior work references : - > This reference ( https : //arxiv.org/pdf/2004.03590.pdf ) seems relevant . Could the authors comment on this reference in the paper ? Adding comparison to this work would make the paper even stronger . Methodology : - > I find the wording of `` 3D noise sampling '' a bit unclear . Is it just a resized version of 1D tensor ? Is there any importance on how the 3D noise sampling is structured ? Could the authors clarify the idea of 3D noise sampling in the main body of the paper ? Experiments : - > Could the authors comment why FID is a good metric for semantic image synthesis ? Justifying the choice of metrics would make the evaluation section stronger . From the discussion of the results , ( diversity quality tradeoffs ) it seems that the paper would benefit from reporting precision-recall like metrics for GANs ( e.g.https : //arxiv.org/abs/1807.09499 , https : //arxiv.org/abs/1905.10887 , https : //arxiv.org/abs/1904.06991 ) . Adding additional metrics would further improve the paper . - > Table 2 . It is unclear to me what is the meaning of red color numbers . Could the authors add clarifications about it to the caption ? - > Table 2 . It is bit surprising to see that the model trained only with adversarial loss has relatively low diversity . I would expect that the lack of perceptual loss would lead also to increased diversity in synthesized images . Could the authors comment on what is the limiting factor to increase sample diversity ? Could the diversity be affected by the proportion of mixing ( real vs. fake ) in LabelMix ? - > Would it be possible to add qualitative results for SPADE+ ? This model seems to have high diversity scores and it would be interesting to see model samples . - > The paper lacks discussion on the limitations of the method . Could the authors comment on the potential limitations of the suggested approach ?", "rating": "7: Good paper, accept", "reply_text": "* * R1-10 : * * * \u201c This reference ( https : //arxiv.org/pdf/2004.03590.pdf ) seems relevant . Could the authors comment on this reference in the paper ? Adding comparison to this work would make the paper even stronger. \u201d * Thanks for pointing towards that reference . The referenced paper proposes an interesting approach to semantic synthesis , based on the relatively novel principle of implicit maximum likelihood estimation . We now comment on this reference in the revision in the related work section on page 3 . However , direct quantitative comparison to OASIS and preceding work such as SPADE and CC-FPSE is difficult for several reasons : ( 1 ) The paper uses a different evaluation protocol . Instead of employing FID and mIoU for evaluation , it uses LPIPS and crowd-sourced human judgment . ( 2 ) This work does not also evaluate on the standard label-to-image benchmarks , such as ADE20K , Cityscapes and COCO , but instead uses the GTA5 dataset and the BDD100K dataset . ( 3 ) The proposed model is not quantitatively compared to state-of-the-art GAN models for semantic image synthesis . The closest possible comparison is to visually compare their BDD100K samples to the Cityscapes samples of OASIS , since both datasets consist of real urban street scenes . From visually inspecting Fig.14 in the given reference , it is evident that the synthesized street scenes have blurry textures and lack detail . In contrast , OASIS synthesizes less blurry images with more realistic colors and a lot more detail , such as windows on house facades and driving lanes on the streets ( see Appendix Fig.D ) . $ $ * * R1-11 : * * * \u201c I find the wording of `` 3D noise sampling '' a bit unclear . Is it just a resized version of 1D tensor ? Is there any importance on how the 3D noise sampling is structured ? Could the authors clarify the idea of 3D noise sampling in the main body of the paper ? \u201d * During training , the 3D noise tensor is sampled globally , i.e.per-channel , where we sample each channel value and copy it along the height ( H ) and width ( W ) dimension of the tensor . In other words , globally sampled 3D noise can be viewed as a spatially replicated version of a 1D tensor . We have also investigated other ways of structuring the 3D noise tensor during training in Appendix A7 . Based on our analysis we chose global sampling for training , due to its simplicity and competitive performance with other considered strategies . We added a clarification about the 3D noise sampling in section 3.3 on page 6 . Note that during inference , the 3D noise can be sampled in various ways : ( 1 ) globally as during the training , i.e.sampling per-channel and replicating each channel value spatially to the whole tensor ; ( 2 ) locally , sampling different channel values area-wise . The option ( 1 ) entirely changes the image . The option ( 2 ) allows us to only change parts of the image . For example , based on the label map , we can extract the area of sky . Through the option ( 2 ) , different noise realizations in the area corresponding to the sky segment lead to different skies in the generated images without changing the rest of the scene ( see Fig.F in the Appendix or Fig.2 ) . $ $ * * R1-12 : * * * \u201c Table 2 . It is unclear to me what is the meaning of red color numbers . Could the authors add clarifications about it to the caption ? \u201d * Table 2 reports the results of the ablation on the use of VGG and 3D noise , showing their impacts on the diversity and quality performance of SPADE+ and OASIS . As we observe a trade-off between diversity and quality , it is important to jointly assess a model from both aspects . Therefore , we make not only the best numbers bold , but also mark the worst numbers in red . For instance , SPADE+ with 3D noise without VGG has the best diversity performance ( bold ) , but FID and mIoU are strongly compromised ( red ) . This indicates that the diversity gain is spurious , being a side effect of a poor quality of synthesized images that are naturally more different from each other . OASIS with 3D noise and VGG , has the best mIoU performance ( bold ) , but the diversity is worse ( red ) than the same version w/o VGG . This indicates that the model sacrifices variety to generate easy-to-segment images . We added a clarification in the caption . $ $ * * R1-13 : * * * \u201c Would it be possible to add qualitative results for SPADE+ ? This model seems to have high diversity scores and it would be interesting to see model samples. \u201d * Thanks for the suggestion . We added qualitative results for SPADE+ ( with VGG ) in Appendix Figure E. Please note that despite its improved quality over original SPADE , this model still depends on the VGG loss and has very low diversity ( The MS-SSIM for SPADE+ in Table 2 is 0.85 , while OASIS has 0.65 , which indicates that SPADE+ has lower diversity ) ."}, "2": {"review_id": "yvQKLaqNE6M-2", "review_text": "In this paper , the authors approach the problem of conditional image generation via generative adversarial networks . To this end , they propose an approach that utilizes only semantic segmentation annotations and adversarial loss . No perceptual loss is required . Their discriminator leverages semantic labels to improve the image generations . They evaluate their approach on a variety of datasets including ADE20K , COCO , and CityScapes . They demonstrate substantial quantitative and qualitative performance over baselines and perform an ablation analysis . Pros : 1.The problem of mask conditioned image generation has immediate applications in computational photography and computer graphics . 2.The paper is well written and easy to understand . 3.Performance is impressive and experimental evaluation is thorough . The authors perform an ablation analysis . 4.The proposed techniques ( Segment based discriminator , 3d noise vector ) are useful insights into tuning GANs to perform well . Cons : 1.Technical novelty is limited . The contributions ( segment discriminator and 3D noise ) are useful engineering increments but not particularly large insights into the workings of image generation . 2.While the need for VGG based perceptual loss may not be needed here , it seems like the perceptual loss has just been shifted to the spatial realm . There is still a dependence on strong semantic constraints . Instead of using image class labels to train features , the per pixel class labels are used . In summary I believe the approach shows promise and important tweaks for image generation networks . Performance is good . However , the technical novelty is limited .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the positive feedback ! We would like to take the opportunity to share our perspective on the two cons you have raised : $ $ * * R3-1 : * * * \u201c Technical novelty is limited . The contributions ( segment discriminator and 3D noise ) are useful engineering increments but not particularly large insights into the workings of image generation. \u201d * Current GAN models for semantic image synthesis rely on external networks ( VGG ) for training stability and synthesis quality . While these methods achieve good results , the external supervision imposes model bias and dataset bias . Our contribution is to remove the need for such external supervision . By removing the bias , we improve synthesis quality and diversity . Thus , through our technical contributions , GAN models for semantic image synthesis can now behave like other cGANs : The model is sensitive to noise and no external supervision is needed . Lastly , our technical contributions enable new features such as ( 1 ) local editing of generated images , ( 2 ) segmenting and re-synthesizing real images ( see R4-1 , App.B4 and Figure K ) as well as ( 3 ) local class-balancing , which helps training on imbalanced datasets . $ $ * * R3-2 : * * * \u201c While the need for VGG based perceptual loss may not be needed here , it seems like the perceptual loss has just been shifted to the spatial realm . There is still a dependence on strong semantic constraints . Instead of using image class labels to train features , the per pixel class labels are used. \u201d * Thanks a lot for your perspective ! We would like to provide an argument for why our method reduces supervision , and why the perceptual loss and the adversarial segmentation loss are not equivalent : By design semantic image synthesis is a task where pairs of label maps and real images are given . Note that all previous works also provide the image and the label map to the discriminator . However , in our work , we make more effective use of the label maps , through the proposed adversarial segmentation loss . This enables the generation of high-quality images without the need for extra supervision from an external perceptual network ( VGG ) . Since the use of VGG was crucial for previous works , but not for OASIS , we reduce the amount of supervision needed . Further , the use of the perceptual loss and adversarial segmentation loss is not equivalent : the perceptual loss matches features based on what VGG learned from ImageNet , independent of the ground truth semantic labels . In contrast , the adversarial segmentation loss explicitly takes the ground truth semantic labels into account . This also has the immediate benefit of better alignment with the ground truth label maps , as shown by the improvement in mIoU ."}}