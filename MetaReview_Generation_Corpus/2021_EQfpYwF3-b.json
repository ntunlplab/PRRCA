{"year": "2021", "forum": "EQfpYwF3-b", "title": "Deep Learning meets Projective Clustering", "decision": "Accept (Poster)", "meta_review": "The paper proposes to use projective clustering to compress the embedding layers of DNN. This is a novel interesting idea which can  impact the area of Knowledge distillation. There were some concerns about the empirical study which was addressed to some extent  by the authors during the rebuttal.", "reviews": [{"review_id": "EQfpYwF3-b-0", "review_text": "This work proposes a new approach , based on projective clustering , for compressing the embedding layers of DNNs for natural language modeling tasks . The authors show that the trade-off between compression and model accuracy can be improved by considering a set of k subspaces rather than just a single subspace . Methods for compressing DNNs is an active area of research and this paper presents a promising approach to do so as well as interesting results . Rating : The paper presents interesting ideas for compressing embedding layers . However , since this is an empirical paper , I would expect a more comprehensive set of empirical results and a better comparison with other related methods . Overall , the paper seems not very mature in its current form , hence my rating is 'Ok but not good enough - rejection ' . Pros - * The proposed method is appealing due to its simplicity and the idea of considering multiple subspaces for embedding is plausible in the context of compressing embedding matrices of NLP models . * The results show improvements as compared to using just a single subspace . * The framework provides several ideas for future works . Cons -- * Typically , the SVD takes the form A = UDV , where U and V are the left and right singular vectors and the diagonal entries of D are the singular values . From the discussion it is not clear whether you factor the singular values into U , or whether you simply ignore the singular values ? Also , how do you enforce the orthogonality constraints on U and V during the fine tuning stage ? Have you considered a simpler low-rank factorization A = EF in your experiments , where no orthogonality constraints on E and F are imposed ? * It would be good to see the progression for k= { 2,3,4,5 } in Figure 5 and 6 . Further , the ensemble approach in Figure 6 has n't been discussed in detail anywhere in the paper . It is not exactly clear to me how you are computing the ensemble . * It would be very helpful to see some Tables that shows the total number of weights , accuracy , k , j , etc. , in order to better understand the performance . * How do you determine k and j in practice ? Are you using some heuristic or are you simply doing a grid search ? * I would like to see how your method compares to ALBERT and whether a modified ALBERT ( as you suggest in your future work section ) is doing better . * I would be interesting to see if you approach is also useful for compressing a fully connected layer in different settings . This should be easy to test and could be reported in the Appendix . Minor comments : -- * It is nice to see that you have many generalizations an extensions in mind , but this section appears very lengthy to me . * compression rater - > compression rates", "rating": "5: Marginally below acceptance threshold", "reply_text": "First , we thank the reviewer for the detailed review and very helpful suggestions . Q : The paper presents interesting ideas for compressing embedding layers . A : We thank the reviewer for appreciating the ideas in our paper . -- Q. I would expect a more comprehensive set of empirical results A : Following this reviewer 's request , many experiments were added . Including 1. compressing fully connected layers using the suggested approach . 2. checking how another clustering method can fit in our pipeline . 3. more results on other $ k $ -value as requested . Also , we are coding and running more experiments now -- hoping to finish before the deadline . -- Q : Typically , the SVD takes the form A = UDV ... from the discussion , it is not clear whether you factor the singular values into U , or whether you simply ignore the singular values ? A : As in previous papers , we use the more general factorization ( as e.g.in NNMF ) A=EF which corresponds to a pair of layers . This means that we did not ignore the singular values , and they can be assigned to either the left or right matrix . See next question . -- Q : how do you enforce the orthogonality constraints on U and V during the fine-tuning stage ? A : We did not . This is a very good point that was added to the text . The orthogonalization is used to obtain a low rank approximation A~EF using SVD . From this point , we did not see an advantage to keep this property in the network . -- Q : Have you considered a simpler low-rank factorization A = EF in your experiments , where no orthogonality constraints on E and F are imposed ? A : This is exactly what we do after computing SVD , as explained in the previous answers . In fact , any non-orthogonal base to the span of the reduced matrix will do . It is an interesting idea to try other non-orthogonal basis , as done e.g.in Dictionary learning . -- Q : It would be good to see the progression for k= { 2,3,4,5 } in Figure 5 and 6 . A : Added.We thank the reviewer for this good suggestion . -- Q : The ensemble approach in Figure 6 has n't been discussed in detail anywhere in the paper . A : Added.Due to space constraints , the graphs and explanations can be found in the appendix . -- Q : It would be very helpful to see some Tables that show the total number of weights , accuracy , k , j , etc. , in order to better understand the performance . A : We thank the reviewer for the suggestion , and expect to finish this task before the end of the rebuttal . -- Q : How do you determine k and j in practice ? Are you using some heuristic or are you simply doing a grid search ? A : For a given compression rate $ x $ , we try multiple values of $ k $ via binary search on $ k $ . , For every such $ k $ value we compute the implied value $ j = ( 1-x ) dn/ ( n+kd ) $ , and then check the compression result on those values . -- Q : Apply your technique on ALBERT . A : We are implementing and running these experiments and expect to finish before the end of the rebuttal . -- Q : Compress a fully connected layer in different settings using your approach . A : Added.See Appendix C for LENET_300_100 and vgg19 models . The results are strong and better than the competitors as in the other experiments . -- Q : It is nice to see that you have many generalizations and extensions in mind , but this section appears very lengthy to me . A : Fixed.We were indeed excited by our results and have many future ideas . However , most of this section was moved to the appendix . -- Q : compression rater - > compression rates A : Fixed . We thank the reviewer for the careful reading ."}, {"review_id": "EQfpYwF3-b-1", "review_text": "This paper extends the idea of using subspace clustering to compress the neural nets by considering multiple subspaces and projecting each point to its closest subspace . The paper needs more investigation on the related works . Basically , the idea and the technique is not novel . See the related literature below : [ 1 ] Trittenbach , Holger , and Klemens B\u00f6hm . `` One-Class Active Learning for Outlier Detection with Multiple Subspaces . '' Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 2019 . [ 2 ] Liu , Risheng , et al . `` Fixed-rank representation for unsupervised visual learning . '' 2012 IEEE Conference on Computer Vision and Pattern Recognition . IEEE , 2012 . [ 3 ] Xu , Dong , et al . `` Concurrent subspaces analysis . '' 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ( CVPR'05 ) . Vol.2.IEEE , 2005 . [ 4 ] Feng , Jianzhou , et al . `` Learning dictionary via subspace segmentation for sparse representation . '' 2011 18th IEEE International Conference on Image Processing . IEEE , 2011 . Pros : \u2022 Smoothly readable . \u2022 The contribution section is described thoroughly and properly . \u2022 Providing the codes for reproducing results . Cons : Abstract : \u2022 The abbreviations like NLP or SVD should be defined first , then used . \u2022 Assuming that the reader already has corresponding field knowledge about systems such as GLUE , DistilBERT , or RoBERTa and mentioning them in the abstract may be bold . \u2022 Details of the methods such as the use of Aj matrix or k > 1 subspace should not be mentioned in the abstract but rather in the contribution or introduction section accordingly . \u2022 The last sentence \u201c Open Code for \u2026. \u201d Should not be mentioned in the abstract but in the code description section . \u2022 The figures 1-3 in the paper look not well organized , which makes the proposed simple idea to be extremely complex . Results : \u2022 It would be better to discuss the comparable results more thoroughly . \u2022 Model compression literature should be reviewed and the typical methods should be compared with in the experiments . Discussion and Conclusion : \u2022 Only discussion of the results is provided in this section and the conclusion is not provided explicitly . Future Work : \u2022 Better not to start the section with numbered items right away . Better to have a starting sentence first . Appendix B \u2022 Titled results before fine-tuning and includes figures with no explanation . Provide proper description and discussion for each subfigure .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the suggestions to improve the writing , and for the detailed review . Q : the idea and the technique is not novel . See the related literature below A : Of course , projective clustering is a classic old technique . We gave citations to all of the reviewer 's suggestions . Nevertheless , as the title implies , our paper is the first that forges a link between projective clustering and Deep Learning . It also answers the question of how to apply this classic technique to deep learning , by presenting a new network architecture . The experimental results show that this new meeting of fields improves the results significantly in practice . -- abstract : Q : The abbreviations like NLP or SVD should be defined first , then used . A : Fixed.Q : The last sentence \u201c Open Code for \u2026. \u201d Should not be mentioned in the abstract but in the code description section . A.Fixed.As for the other comments about the abstract : All the questions of the reviewers regarding the abstract are answered in the introduction . We try to add more hints but due to the space constraints , we can not answer all the questions already in the abstract . -- Results : Q It would be better to discuss the comparable results more thoroughly . A : Due to space limitations , such discussions appear in the appendix . More discussions will be added ( before the rebuttal period ends ) following the reviewer 's request . Q : The conclusion is not provided explicitly . A : Fixed , thanks to the reviewer for pointing this out . Q : Better not to start the section with numbered items right away . Better to have a starting sentence first . Appendix B A : Fixed . Q : Titled results before fine-tuning and includes figures with no explanation . Provide proper description and discussion for each subfigure . A : Fixed ."}, {"review_id": "EQfpYwF3-b-2", "review_text": "Summary : This paper applies projective clustering to the embedding layer of deep networks with large model sizes such as RoBERTa . The idea of finding more than one subspaces to factorize the embedding weight matrix has nice intuition and insights . I vote for accepting . Strengths : 1 . The paper has convincing evidence showing the reduction in percent of accuracy drop when applying projective clustering to the embedding weight vectors . 2.The paper has illustration figures that clearly show the intuition of the approach as well as how the compression is achieved . Weaknesses : 1 . It would be better if more baselines can be included in the experiment comparisons . In particular , Since Step 2-3 of the proposed MESSI pipeline ( page 4 ) is partitioning of all the input neurons and computing SVD for each partition , I would be really interested in seeing the comparison of projective clustering vs simpler clustering methods such as k-means to partition the input neurons , in the evaluation . 2.The authors discussed extensions such as using $ L_1 $ error and $ L_1 $ distance , but no experiments were performed for the extensions . Some experiment results will be better to establish the flexibility of the framework of projective clustering in model compression tasks . Questions during rebuttal period : 1 . Please provide some results regarding the weaknesses above , especially the result of more baseline methods . 2.Is projective clustering the only way to find clusters in multiple subspaces ? What are some alternatives ? For example , in subspace clustering , all the data points can be projected to the same subspace and form clusters ; we may run subspace clustering for multiple times to get clustering results in different subspaces .", "rating": "7: Good paper, accept", "reply_text": "First , we thank the reviewer for appreciating our result , for the high scoring , and for the very helpful suggestion . Q. I would be really interested in seeing the comparison of projective clustering vs simpler clustering methods such as k-means A : Added . This was a very good idea that we believe that significantly improved the paper . See graphs in section E. The new graphs show that projective clustering can guarantee a better initialization , i.e. , better accuracy before fine-tuning . This implies a fewer number of epochs . -- Q : Is projective clustering the only way to find clusters in multiple subspaces ? A : It is a very good question and the answer depends on the definition of `` clusters in multiple subspaces '' , i.e. , the generative model . Projective Clustering aims to compute subspaces that maximize the likelihood , assuming that every point was generated by adding some noise to a point on a single subspace . As the reviewer suggests , we may assume soft clustering , e.g. , that every point is a linear combination of points on a pair of subspaces . Such versions may be defined e.g.via Dictionary Learning . We expect that this paper will inspire many such generalizations in future works via our suggested network architecture or its variants . -- Q.The authors discussed extensions such as using error and distance , but no experiments were performed for the extensions . Some experiment results will be better to establish the flexibility of the framework of projective clustering in model compression tasks . A.We are doing our best to add such experiments , we believe we can provide some before the rebuttal period ."}], "0": {"review_id": "EQfpYwF3-b-0", "review_text": "This work proposes a new approach , based on projective clustering , for compressing the embedding layers of DNNs for natural language modeling tasks . The authors show that the trade-off between compression and model accuracy can be improved by considering a set of k subspaces rather than just a single subspace . Methods for compressing DNNs is an active area of research and this paper presents a promising approach to do so as well as interesting results . Rating : The paper presents interesting ideas for compressing embedding layers . However , since this is an empirical paper , I would expect a more comprehensive set of empirical results and a better comparison with other related methods . Overall , the paper seems not very mature in its current form , hence my rating is 'Ok but not good enough - rejection ' . Pros - * The proposed method is appealing due to its simplicity and the idea of considering multiple subspaces for embedding is plausible in the context of compressing embedding matrices of NLP models . * The results show improvements as compared to using just a single subspace . * The framework provides several ideas for future works . Cons -- * Typically , the SVD takes the form A = UDV , where U and V are the left and right singular vectors and the diagonal entries of D are the singular values . From the discussion it is not clear whether you factor the singular values into U , or whether you simply ignore the singular values ? Also , how do you enforce the orthogonality constraints on U and V during the fine tuning stage ? Have you considered a simpler low-rank factorization A = EF in your experiments , where no orthogonality constraints on E and F are imposed ? * It would be good to see the progression for k= { 2,3,4,5 } in Figure 5 and 6 . Further , the ensemble approach in Figure 6 has n't been discussed in detail anywhere in the paper . It is not exactly clear to me how you are computing the ensemble . * It would be very helpful to see some Tables that shows the total number of weights , accuracy , k , j , etc. , in order to better understand the performance . * How do you determine k and j in practice ? Are you using some heuristic or are you simply doing a grid search ? * I would like to see how your method compares to ALBERT and whether a modified ALBERT ( as you suggest in your future work section ) is doing better . * I would be interesting to see if you approach is also useful for compressing a fully connected layer in different settings . This should be easy to test and could be reported in the Appendix . Minor comments : -- * It is nice to see that you have many generalizations an extensions in mind , but this section appears very lengthy to me . * compression rater - > compression rates", "rating": "5: Marginally below acceptance threshold", "reply_text": "First , we thank the reviewer for the detailed review and very helpful suggestions . Q : The paper presents interesting ideas for compressing embedding layers . A : We thank the reviewer for appreciating the ideas in our paper . -- Q. I would expect a more comprehensive set of empirical results A : Following this reviewer 's request , many experiments were added . Including 1. compressing fully connected layers using the suggested approach . 2. checking how another clustering method can fit in our pipeline . 3. more results on other $ k $ -value as requested . Also , we are coding and running more experiments now -- hoping to finish before the deadline . -- Q : Typically , the SVD takes the form A = UDV ... from the discussion , it is not clear whether you factor the singular values into U , or whether you simply ignore the singular values ? A : As in previous papers , we use the more general factorization ( as e.g.in NNMF ) A=EF which corresponds to a pair of layers . This means that we did not ignore the singular values , and they can be assigned to either the left or right matrix . See next question . -- Q : how do you enforce the orthogonality constraints on U and V during the fine-tuning stage ? A : We did not . This is a very good point that was added to the text . The orthogonalization is used to obtain a low rank approximation A~EF using SVD . From this point , we did not see an advantage to keep this property in the network . -- Q : Have you considered a simpler low-rank factorization A = EF in your experiments , where no orthogonality constraints on E and F are imposed ? A : This is exactly what we do after computing SVD , as explained in the previous answers . In fact , any non-orthogonal base to the span of the reduced matrix will do . It is an interesting idea to try other non-orthogonal basis , as done e.g.in Dictionary learning . -- Q : It would be good to see the progression for k= { 2,3,4,5 } in Figure 5 and 6 . A : Added.We thank the reviewer for this good suggestion . -- Q : The ensemble approach in Figure 6 has n't been discussed in detail anywhere in the paper . A : Added.Due to space constraints , the graphs and explanations can be found in the appendix . -- Q : It would be very helpful to see some Tables that show the total number of weights , accuracy , k , j , etc. , in order to better understand the performance . A : We thank the reviewer for the suggestion , and expect to finish this task before the end of the rebuttal . -- Q : How do you determine k and j in practice ? Are you using some heuristic or are you simply doing a grid search ? A : For a given compression rate $ x $ , we try multiple values of $ k $ via binary search on $ k $ . , For every such $ k $ value we compute the implied value $ j = ( 1-x ) dn/ ( n+kd ) $ , and then check the compression result on those values . -- Q : Apply your technique on ALBERT . A : We are implementing and running these experiments and expect to finish before the end of the rebuttal . -- Q : Compress a fully connected layer in different settings using your approach . A : Added.See Appendix C for LENET_300_100 and vgg19 models . The results are strong and better than the competitors as in the other experiments . -- Q : It is nice to see that you have many generalizations and extensions in mind , but this section appears very lengthy to me . A : Fixed.We were indeed excited by our results and have many future ideas . However , most of this section was moved to the appendix . -- Q : compression rater - > compression rates A : Fixed . We thank the reviewer for the careful reading ."}, "1": {"review_id": "EQfpYwF3-b-1", "review_text": "This paper extends the idea of using subspace clustering to compress the neural nets by considering multiple subspaces and projecting each point to its closest subspace . The paper needs more investigation on the related works . Basically , the idea and the technique is not novel . See the related literature below : [ 1 ] Trittenbach , Holger , and Klemens B\u00f6hm . `` One-Class Active Learning for Outlier Detection with Multiple Subspaces . '' Proceedings of the 28th ACM International Conference on Information and Knowledge Management . 2019 . [ 2 ] Liu , Risheng , et al . `` Fixed-rank representation for unsupervised visual learning . '' 2012 IEEE Conference on Computer Vision and Pattern Recognition . IEEE , 2012 . [ 3 ] Xu , Dong , et al . `` Concurrent subspaces analysis . '' 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ( CVPR'05 ) . Vol.2.IEEE , 2005 . [ 4 ] Feng , Jianzhou , et al . `` Learning dictionary via subspace segmentation for sparse representation . '' 2011 18th IEEE International Conference on Image Processing . IEEE , 2011 . Pros : \u2022 Smoothly readable . \u2022 The contribution section is described thoroughly and properly . \u2022 Providing the codes for reproducing results . Cons : Abstract : \u2022 The abbreviations like NLP or SVD should be defined first , then used . \u2022 Assuming that the reader already has corresponding field knowledge about systems such as GLUE , DistilBERT , or RoBERTa and mentioning them in the abstract may be bold . \u2022 Details of the methods such as the use of Aj matrix or k > 1 subspace should not be mentioned in the abstract but rather in the contribution or introduction section accordingly . \u2022 The last sentence \u201c Open Code for \u2026. \u201d Should not be mentioned in the abstract but in the code description section . \u2022 The figures 1-3 in the paper look not well organized , which makes the proposed simple idea to be extremely complex . Results : \u2022 It would be better to discuss the comparable results more thoroughly . \u2022 Model compression literature should be reviewed and the typical methods should be compared with in the experiments . Discussion and Conclusion : \u2022 Only discussion of the results is provided in this section and the conclusion is not provided explicitly . Future Work : \u2022 Better not to start the section with numbered items right away . Better to have a starting sentence first . Appendix B \u2022 Titled results before fine-tuning and includes figures with no explanation . Provide proper description and discussion for each subfigure .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the suggestions to improve the writing , and for the detailed review . Q : the idea and the technique is not novel . See the related literature below A : Of course , projective clustering is a classic old technique . We gave citations to all of the reviewer 's suggestions . Nevertheless , as the title implies , our paper is the first that forges a link between projective clustering and Deep Learning . It also answers the question of how to apply this classic technique to deep learning , by presenting a new network architecture . The experimental results show that this new meeting of fields improves the results significantly in practice . -- abstract : Q : The abbreviations like NLP or SVD should be defined first , then used . A : Fixed.Q : The last sentence \u201c Open Code for \u2026. \u201d Should not be mentioned in the abstract but in the code description section . A.Fixed.As for the other comments about the abstract : All the questions of the reviewers regarding the abstract are answered in the introduction . We try to add more hints but due to the space constraints , we can not answer all the questions already in the abstract . -- Results : Q It would be better to discuss the comparable results more thoroughly . A : Due to space limitations , such discussions appear in the appendix . More discussions will be added ( before the rebuttal period ends ) following the reviewer 's request . Q : The conclusion is not provided explicitly . A : Fixed , thanks to the reviewer for pointing this out . Q : Better not to start the section with numbered items right away . Better to have a starting sentence first . Appendix B A : Fixed . Q : Titled results before fine-tuning and includes figures with no explanation . Provide proper description and discussion for each subfigure . A : Fixed ."}, "2": {"review_id": "EQfpYwF3-b-2", "review_text": "Summary : This paper applies projective clustering to the embedding layer of deep networks with large model sizes such as RoBERTa . The idea of finding more than one subspaces to factorize the embedding weight matrix has nice intuition and insights . I vote for accepting . Strengths : 1 . The paper has convincing evidence showing the reduction in percent of accuracy drop when applying projective clustering to the embedding weight vectors . 2.The paper has illustration figures that clearly show the intuition of the approach as well as how the compression is achieved . Weaknesses : 1 . It would be better if more baselines can be included in the experiment comparisons . In particular , Since Step 2-3 of the proposed MESSI pipeline ( page 4 ) is partitioning of all the input neurons and computing SVD for each partition , I would be really interested in seeing the comparison of projective clustering vs simpler clustering methods such as k-means to partition the input neurons , in the evaluation . 2.The authors discussed extensions such as using $ L_1 $ error and $ L_1 $ distance , but no experiments were performed for the extensions . Some experiment results will be better to establish the flexibility of the framework of projective clustering in model compression tasks . Questions during rebuttal period : 1 . Please provide some results regarding the weaknesses above , especially the result of more baseline methods . 2.Is projective clustering the only way to find clusters in multiple subspaces ? What are some alternatives ? For example , in subspace clustering , all the data points can be projected to the same subspace and form clusters ; we may run subspace clustering for multiple times to get clustering results in different subspaces .", "rating": "7: Good paper, accept", "reply_text": "First , we thank the reviewer for appreciating our result , for the high scoring , and for the very helpful suggestion . Q. I would be really interested in seeing the comparison of projective clustering vs simpler clustering methods such as k-means A : Added . This was a very good idea that we believe that significantly improved the paper . See graphs in section E. The new graphs show that projective clustering can guarantee a better initialization , i.e. , better accuracy before fine-tuning . This implies a fewer number of epochs . -- Q : Is projective clustering the only way to find clusters in multiple subspaces ? A : It is a very good question and the answer depends on the definition of `` clusters in multiple subspaces '' , i.e. , the generative model . Projective Clustering aims to compute subspaces that maximize the likelihood , assuming that every point was generated by adding some noise to a point on a single subspace . As the reviewer suggests , we may assume soft clustering , e.g. , that every point is a linear combination of points on a pair of subspaces . Such versions may be defined e.g.via Dictionary Learning . We expect that this paper will inspire many such generalizations in future works via our suggested network architecture or its variants . -- Q.The authors discussed extensions such as using error and distance , but no experiments were performed for the extensions . Some experiment results will be better to establish the flexibility of the framework of projective clustering in model compression tasks . A.We are doing our best to add such experiments , we believe we can provide some before the rebuttal period ."}}