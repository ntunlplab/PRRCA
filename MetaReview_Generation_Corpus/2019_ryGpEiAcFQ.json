{"year": "2019", "forum": "ryGpEiAcFQ", "title": "A Synaptic Neural Network and Synapse Learning", "decision": "Reject", "meta_review": "In this paper, neural networks are taken a step further by increasing their biological likeliness.  In particular, a model of the membranes of biological cells are used computationally to train a neural network.  The results are validated on MNIST.\n\nThe paper argumentation is not easy to follow, and all reviewers agree that the text needs to be improved.  \u02dcThe neuroscience sources that the models are based on are possibly outdated.  Finally, the results are too meagre and, in the end, not well compared with competing approaches.\n\nAll in all, the merit of this approach is not fully demonstrated, and further work seems to be needed to clarify this.", "reviews": [{"review_id": "ryGpEiAcFQ-0", "review_text": "Thanks for submitting your paper. It takes a lot of effort and courage to put your ideas out into the world. Sometimes the hardest work for researchers is conveying their thoughts to others in a manner in which those ideas can be understood. With that in mind, I had an extremely difficult time following your arguments. I noticed several things: - There are numerous places in the text that lack proper citation, or are cited improperly. - Why was there not a related methods section? I find it hard to believe that all of your ideas have no precursor. - When there are citations, there is usually only one text and it is quite old. For example, all of your neuroscience citations reference a work that is almost 40 years old. There have been quite a few improvements in our biological understanding as well as theoretical understanding since then. ( I make this point as a common justification used in the manuscript is that the method describes how synapses function in biology. ) - There is a claim regarding how this can be used in fintech. This statement doesn't belong in this work. - There are many different equations given throughout the text. Some of these equations come from areas like physics or information theory, and others seem to be of your own design. Regarding the latter, there is no justification or explanation for the origin of the equations. Regarding the former, if you are using equations from lots of different fields, or even field you think part of your audience might not be familiar with, you should, at the very least, include a some description of the algorithm or intuition as to why it is being leveraged. - It wasn't clear from your diagrams or your descriptions what the difference between a synapse and a neuron was in your architecture. It seemed like the name was used interchangeably in some areas, but then had a strict definition in others. - I was also not able to understand how the excitatory and the inhibitory connections that were to enter each neuron were connected to the previous layer of the network. Is a link between neurons in Figure 2 actually two links? If this is the case, then it is a direct violation of Dale's law. Again, I only mention this because most arguments seem to be of the form \"this is correct because it is how it is done biologically\". - There were a few claims made in the paper that were completely unsubstantiated. A good example of this was in the conclusion section part ii) where it was stated that \"using a large number of synapses and neurons SynaNN can solve the complex problems in the real world.\" - Also, the last sentence of the conclusion was not discussed anywhere in the rest of the paper. Nor was the statement itself supported except with a single citation and no description. Regarding the empirical testing of your algorithm, I was very dissapointed to see that the only dataset it was tested against was MNIST. Furthermore there was absolutely no benchmarking against other comparative algorithms. At the very least I would have expected a comparison to the perceptron algorithm that you use as inspiration, but that would also still not have been enough. This paper needs heavy amounts of work to make it understandable. Once it is understandable an attempt to evaluate the merit of the scientific contribution would then be possible.", "rating": "2: Strong rejection", "reply_text": "Thanks for your detail review . With that in mind , I had an extremely difficult time following your arguments . I noticed several things : - There are numerous places in the text that lack proper citation , or are cited improperly . We are going to check it . We are in the procedure to reorganize and revise the paper . - Why was there not a related methods section ? I find it hard to believe that all of your ideas have no precursor . We are going to explain some of our methods . For example , dual space analysis is a widely applied method . In our case , it is probability space and surprisal space . It is hard to study the non-linear function in the probability space , but our function becomes linear in surprisal space . Many results can be easily concluded from surprisal space . Entropy , surprisal , surprisal function have been well known and defined . But we need to consider everything of synapse after converting to surprisal , therefore we defined the surprisal space . We do not know somebody defined and use it so far . Let us know if find some references . Log space has been widely used . Surprisal space different from log space in a negative sign and the domain . The domain of log space is the field of the positive real number . The domain of surprisal space is the real interval of ( 0,1 ) . Surprisal space does make sense as the self-information bit space . - When there are citations , there is usually only one text and it is quite old . For example , all of your neuroscience citations reference a work that is almost 40 years old . There have been quite a few improvements in our biological understanding as well as theoretical understanding since then . ( I make this point as a common justification used in the manuscript is that the method describes how synapses function in biology . ) You are right . We have no intention to ignore the latest remark work in neuroscience . Most synaptic models are built on the differential equations of electronic property . Our biological model is very simple but we believe that it works in the right direction . The opening property of ion channels with different types is the key in our analysis . Fortunately , there are two types of channels in a synapse : excitatory and inhibitory channels . The randomness of the channels is the basis for us to apply probability theory . It is very difficult to figure out a complete synapse model . We tried to build a simple model to approach the property of the synapse . - There is a claim regarding how this can be used in fintech . This statement does n't belong in this work . This paragraph is wrong . It will be removed . - There are many different equations given throughout the text . Some of these equations come from areas like physics or information theory , and others seem to be of your own design . Regarding the latter , there is no justification or explanation for the origin of the equations . Regarding the former , if you are using equations from lots of different fields , or even field you think part of your audience might not be familiar with , you should , at the very least , include some description of the algorithm or intuition as to why it is being leveraged . One confusion is that we put some standard terms such as entropy in our definition . We are going to make all exact definitions and theorems from us . One case is to prove the gradient of the synapse function in surprisal space has the expression Bose-Einstein distribution . That is dlog ( 1-e^-x ) /dx = 1/ ( e^x-1 ) . Unfortunately , we did not found any references to mentation this . All of them came from the computing in statistical physics . In our context , it has an obvious meaning that is the gradient over the parameter of our synapse function in surprisal space . As far as we know this is the first time we figured out this equation , at least a rediscovery . We 'd like to know any claims for their discovery . Yes , we are going to add some explanations ."}, {"review_id": "ryGpEiAcFQ-1", "review_text": "The authors propose a hybrid neural network, composed of a synapse graph that can be embedded into and a standard neural network, such that the entire architecture can be trained in a way that is compatible with the gradient descent and backpropagation of. As a proof of concept, the hybrid architecture is trained to classify MNIST. I am not convinced by the way this work is motivated. What problem are the authors actually addressing? Just because biological neurons use synapses does not mean we should try hard to put a certain instance of them into deep neural networks. Clearly this is not an attempt to add to neuroscience, as beyond the inspiration of neurons having synapses, there is little attempt to biologically plausible. As an attempt to add to machine learning research, the neuroscience motivation is unconvincing. Provided that the math works out (and I admit that I did not attempt to follow the detailed derivations), this looks like an interesting intellectual exercise, but it also seems a bit like a discovery of a hammer that is in need for nails to be applied to. And it\u2019s not even clear to me how practical the hammer would actually be, even if we had a convincing problem setting at hand. How scalable is it beyond toy-settings? The final sentence makes a tantalizing claim, but at this stage the work has to resort to promising potential, rather than being able to demonstrate that it is practically useful. Moreover, this work is not presented right for the venue and audience, and would need substantial rewriting and restructuring to make the central claims and contributions sufficiently clear. ", "rating": "3: Clear rejection", "reply_text": "Recall the history of the neural network , current neural networks come from the simulation of biological neurons and their systems . An artificial neural network is the simplified mathematical model of the biological neural network . The deep neural network is far more simple in topology than the human brain . History has proved the method to study simplified neuron model and expand it to bring rich fruits . In reverse , we know more brain from model studying . The motivation of this work is based on the research and analysis of the biological neural system . First , synapse plays an important role in learning and memory . In neuroscience , it is synaptic plasticity . The excitation and inhibition are observed in the synapse network . In advanced , the random opening of ion channels is also observed . Second , synapses can connect to other synapses to form a synapse network . Third , synapse makes a non-linear transform . Synapse in the artificial neural network is supposed as a simple linear amplifier , it is the multiplication of a parameter and an input variable . There are no synapses connecting to synapses . But one thing is the same , the learning and memory are related to synapses , the change of synaptic parameters . Current artificial neural network ignored the existence of synapses but simply consider them as weights . All the focus are on neurons . Back to our motivation . Why synapse act as a transform ? Why is it non-linear ? What is the distribution of the synaptic matrix ? The same question for an artificial neural network is what is the distribution of its weight matrix ? Make sense ? OK. We found a reasonable synapse function . The reasons were explained in the paper . The function is not a simulation of a synapse but an abstraction of the probability . It is a non-linear function with parameters . From their connection , we can form a synapse network . What is our `` interesting intellectual exercise '' bring in our paper ? 1.Probability Space and Surprisal Space Logarithmic space has been studied in artificial neural network for a long history . But fruitless in ANN . Because the data field of ANN is the real number field from negative to positive . The surprisal has been defined in information theory and natural language processing and it is related to a random variable . Direct select variable from probability space , the surprisal is the negative log function . Moreover , we found that it is useful to define the surprisal space . Two difference between logarithmic space and surprisal space : 1 ) different in a negative sign 2 ) real space vs ( 0,1 ) space . surprisal represents a self-information bit . The non-linear product in probability space is the linear addition in surprisal space . There may have a lot of new things need to be studied in the surprisal space . Surprisal Space opens a door ."}, {"review_id": "ryGpEiAcFQ-2", "review_text": "The authors present a biologically-inspired neural network model based on the excitatory and inhibitory ion channels in the membranes of real cells. Unfortunately, the paper is structured incoherently, making it nearly impossible to appreciate the authors' contribution. The introduction references neuroscience alongside ResNets, FinTech, surprisal spaces, Bose-Einstein statistics, and topological conjugacy without adequately motivating or defining any of the above. The fundamental definition of the model synapse as a conditional probability (Eq. 1) is not guaranteed to be non-negative, casting serious doubt on any of the subsequent conclusions. Figure 1 conveys no further information about the proposed model. There is no explicit related work or background section. The single experiment offers no comparison to alternative methods. I suggest the authors invest serious effort into rewriting the paper to clarify the presentation and explicitly state their contributions in the context of existing work on biologically-inspired learning models. This is indeed a subfield of machine learning worthy of more investigation. ", "rating": "2: Strong rejection", "reply_text": "Thanks for your review . Q1 : `` Unfortunately , the paper is structured incoherently , making it nearly impossible to appreciate the authors ' contribution . '' This paper was constructed in the structure of a math paper . We first defined a basic formula and gave an explanation of the formula that was based on an abstraction of the biological neural network . Then we gave the definition of the model , related concepts , and theorems with proofs . In this way , we explored and concluded many features of the model , the target was to figure out the learning rules be applied with backpropagation . Finally , we showed an experiment to prove the concept . Authors ' contributions : ( A lot of ) 1 . The neural network model is the authors ' creation . The biologically-inspired synapse equation S ( x , y ; a , b ) =ax ( 1-by ) is defined by the product of the opening probability of excitatory channels of a synapse and the opening probability of inhibitory channels of a synapse . Unlike the synapse in spike neural network , we consider the ion channels as the basis to build our synapse model . We ignored the spike feature of the biological neural network because it is the feature of neurons not synapses . The flow of ions and the random opening of the channels are the foundation of our synapse analysis . From a probability perspective , we abstract the synapse equation . In contrast to classical neural network with weights , its synapse is simply a product of the input variable and the weight . Our synapse is a non-linear unit . The practical biological synapse is much complex , But we present a simple model that can be analyzed in math . 2.We defined the surprisal space to connect Information Theory with our model . Although entropy is widely used in machine learning but surprisal is the more fundamental concept . When we apply surprisal on synapse equation , we have a linear combination in log space which has been used in machine learning analysis . With a negative in front of the log function , we can convert data to the surprisal space . In surprisal space , we can explain the negative log probability as the bits of self-information . That does make sense of surprisal space . It can be a new representation of the neural network . If somebody finds papers to applying surprisal space to explain neural network please let us know , we are going to list them as our references . By defining surprisal space , we build the mapping between a probability space and the surprisal space . It is a real positive space in our definition . The bits addition is the basic operation of a neural network . 3.Synapse with topological conjugacy is our discovery It is very exciting to find that the surprisal of the inhibitory probability is a topological conjugation in our model . That means the dynamical behavior in probability space can be bijected into surprisal space and both have the same dynamics . In advance , we discovered that this topological conjugation is a commutative diagram in category theory . That opened the door to apply new mathematic tools to study neural network . The discovery between the connection of the neural network and category theory is unexpected . That is one of our exciting contributions . 4.We discovered gradient updating in synapse learning followed Bose-Einstein distribution This is a direct conclusion from synapse equation in surprisal space without any statistical hypothesis . That solved the famous black box problem in our model . So we can expect some kind of BE distribution in the parametric matrix . It is a new representation of a neural network . 5.We constructed a fully-connected synaptic neural network as synapse tensor We successfully convert fully-connected non-linear synapse network into a matrix ( tensor ) computing . This synapse tensor is a special connection of synapses . Other network topologies are possible . Synapse tensor can be basic blocks to construct a large-scale neural network . 6.We discovered that synaptic neural network has a similar block to ResNet block in surprisal space That is why we mentioned ResNet . Except we apply surprisal non-linear function but still computing identity mapping . So we expect some features of ResNet such as protect gradient from vanishing in the very deep synaptic neural network . 7.We proved the gradient rule with loss function that mapped in surprisal space That is proof that we can apply the backpropagation algorithm on the fully-connected synaptic neural network . The proof is in very details because we want to verify that the new gradient computing is correct . In conclusion , our synaptic neural network is compatible with backpropagation , however , spike neural network is not ."}, {"review_id": "ryGpEiAcFQ-3", "review_text": "Quality - poor The highly complicated work is evaluated only on the simplest of benchmarks with no significant results. Clarity - poor The paper seems to amount to gobbledygook, many disparate terminology strung together. Originality No idea. Significance None. cons: the paper to me seems a hashing of citations to the main works in neuroscience and deep learning for which only the simplest network is demonstrated (single hidden layer MLP on MNIST) with results that do not exceed that of a standard MLP. pros: the only pro I can think of for this work is that synaptic computing imo deserves more consideration, as real synapses are very complicated beasts, the functioning of which relatively little is known about. ", "rating": "2: Strong rejection", "reply_text": "`` the paper to me seems a hashing of citations to the main works in neuroscience and deep learning '' Please show us what main works in neuroscience and deep learning we have been hashing ?"}], "0": {"review_id": "ryGpEiAcFQ-0", "review_text": "Thanks for submitting your paper. It takes a lot of effort and courage to put your ideas out into the world. Sometimes the hardest work for researchers is conveying their thoughts to others in a manner in which those ideas can be understood. With that in mind, I had an extremely difficult time following your arguments. I noticed several things: - There are numerous places in the text that lack proper citation, or are cited improperly. - Why was there not a related methods section? I find it hard to believe that all of your ideas have no precursor. - When there are citations, there is usually only one text and it is quite old. For example, all of your neuroscience citations reference a work that is almost 40 years old. There have been quite a few improvements in our biological understanding as well as theoretical understanding since then. ( I make this point as a common justification used in the manuscript is that the method describes how synapses function in biology. ) - There is a claim regarding how this can be used in fintech. This statement doesn't belong in this work. - There are many different equations given throughout the text. Some of these equations come from areas like physics or information theory, and others seem to be of your own design. Regarding the latter, there is no justification or explanation for the origin of the equations. Regarding the former, if you are using equations from lots of different fields, or even field you think part of your audience might not be familiar with, you should, at the very least, include a some description of the algorithm or intuition as to why it is being leveraged. - It wasn't clear from your diagrams or your descriptions what the difference between a synapse and a neuron was in your architecture. It seemed like the name was used interchangeably in some areas, but then had a strict definition in others. - I was also not able to understand how the excitatory and the inhibitory connections that were to enter each neuron were connected to the previous layer of the network. Is a link between neurons in Figure 2 actually two links? If this is the case, then it is a direct violation of Dale's law. Again, I only mention this because most arguments seem to be of the form \"this is correct because it is how it is done biologically\". - There were a few claims made in the paper that were completely unsubstantiated. A good example of this was in the conclusion section part ii) where it was stated that \"using a large number of synapses and neurons SynaNN can solve the complex problems in the real world.\" - Also, the last sentence of the conclusion was not discussed anywhere in the rest of the paper. Nor was the statement itself supported except with a single citation and no description. Regarding the empirical testing of your algorithm, I was very dissapointed to see that the only dataset it was tested against was MNIST. Furthermore there was absolutely no benchmarking against other comparative algorithms. At the very least I would have expected a comparison to the perceptron algorithm that you use as inspiration, but that would also still not have been enough. This paper needs heavy amounts of work to make it understandable. Once it is understandable an attempt to evaluate the merit of the scientific contribution would then be possible.", "rating": "2: Strong rejection", "reply_text": "Thanks for your detail review . With that in mind , I had an extremely difficult time following your arguments . I noticed several things : - There are numerous places in the text that lack proper citation , or are cited improperly . We are going to check it . We are in the procedure to reorganize and revise the paper . - Why was there not a related methods section ? I find it hard to believe that all of your ideas have no precursor . We are going to explain some of our methods . For example , dual space analysis is a widely applied method . In our case , it is probability space and surprisal space . It is hard to study the non-linear function in the probability space , but our function becomes linear in surprisal space . Many results can be easily concluded from surprisal space . Entropy , surprisal , surprisal function have been well known and defined . But we need to consider everything of synapse after converting to surprisal , therefore we defined the surprisal space . We do not know somebody defined and use it so far . Let us know if find some references . Log space has been widely used . Surprisal space different from log space in a negative sign and the domain . The domain of log space is the field of the positive real number . The domain of surprisal space is the real interval of ( 0,1 ) . Surprisal space does make sense as the self-information bit space . - When there are citations , there is usually only one text and it is quite old . For example , all of your neuroscience citations reference a work that is almost 40 years old . There have been quite a few improvements in our biological understanding as well as theoretical understanding since then . ( I make this point as a common justification used in the manuscript is that the method describes how synapses function in biology . ) You are right . We have no intention to ignore the latest remark work in neuroscience . Most synaptic models are built on the differential equations of electronic property . Our biological model is very simple but we believe that it works in the right direction . The opening property of ion channels with different types is the key in our analysis . Fortunately , there are two types of channels in a synapse : excitatory and inhibitory channels . The randomness of the channels is the basis for us to apply probability theory . It is very difficult to figure out a complete synapse model . We tried to build a simple model to approach the property of the synapse . - There is a claim regarding how this can be used in fintech . This statement does n't belong in this work . This paragraph is wrong . It will be removed . - There are many different equations given throughout the text . Some of these equations come from areas like physics or information theory , and others seem to be of your own design . Regarding the latter , there is no justification or explanation for the origin of the equations . Regarding the former , if you are using equations from lots of different fields , or even field you think part of your audience might not be familiar with , you should , at the very least , include some description of the algorithm or intuition as to why it is being leveraged . One confusion is that we put some standard terms such as entropy in our definition . We are going to make all exact definitions and theorems from us . One case is to prove the gradient of the synapse function in surprisal space has the expression Bose-Einstein distribution . That is dlog ( 1-e^-x ) /dx = 1/ ( e^x-1 ) . Unfortunately , we did not found any references to mentation this . All of them came from the computing in statistical physics . In our context , it has an obvious meaning that is the gradient over the parameter of our synapse function in surprisal space . As far as we know this is the first time we figured out this equation , at least a rediscovery . We 'd like to know any claims for their discovery . Yes , we are going to add some explanations ."}, "1": {"review_id": "ryGpEiAcFQ-1", "review_text": "The authors propose a hybrid neural network, composed of a synapse graph that can be embedded into and a standard neural network, such that the entire architecture can be trained in a way that is compatible with the gradient descent and backpropagation of. As a proof of concept, the hybrid architecture is trained to classify MNIST. I am not convinced by the way this work is motivated. What problem are the authors actually addressing? Just because biological neurons use synapses does not mean we should try hard to put a certain instance of them into deep neural networks. Clearly this is not an attempt to add to neuroscience, as beyond the inspiration of neurons having synapses, there is little attempt to biologically plausible. As an attempt to add to machine learning research, the neuroscience motivation is unconvincing. Provided that the math works out (and I admit that I did not attempt to follow the detailed derivations), this looks like an interesting intellectual exercise, but it also seems a bit like a discovery of a hammer that is in need for nails to be applied to. And it\u2019s not even clear to me how practical the hammer would actually be, even if we had a convincing problem setting at hand. How scalable is it beyond toy-settings? The final sentence makes a tantalizing claim, but at this stage the work has to resort to promising potential, rather than being able to demonstrate that it is practically useful. Moreover, this work is not presented right for the venue and audience, and would need substantial rewriting and restructuring to make the central claims and contributions sufficiently clear. ", "rating": "3: Clear rejection", "reply_text": "Recall the history of the neural network , current neural networks come from the simulation of biological neurons and their systems . An artificial neural network is the simplified mathematical model of the biological neural network . The deep neural network is far more simple in topology than the human brain . History has proved the method to study simplified neuron model and expand it to bring rich fruits . In reverse , we know more brain from model studying . The motivation of this work is based on the research and analysis of the biological neural system . First , synapse plays an important role in learning and memory . In neuroscience , it is synaptic plasticity . The excitation and inhibition are observed in the synapse network . In advanced , the random opening of ion channels is also observed . Second , synapses can connect to other synapses to form a synapse network . Third , synapse makes a non-linear transform . Synapse in the artificial neural network is supposed as a simple linear amplifier , it is the multiplication of a parameter and an input variable . There are no synapses connecting to synapses . But one thing is the same , the learning and memory are related to synapses , the change of synaptic parameters . Current artificial neural network ignored the existence of synapses but simply consider them as weights . All the focus are on neurons . Back to our motivation . Why synapse act as a transform ? Why is it non-linear ? What is the distribution of the synaptic matrix ? The same question for an artificial neural network is what is the distribution of its weight matrix ? Make sense ? OK. We found a reasonable synapse function . The reasons were explained in the paper . The function is not a simulation of a synapse but an abstraction of the probability . It is a non-linear function with parameters . From their connection , we can form a synapse network . What is our `` interesting intellectual exercise '' bring in our paper ? 1.Probability Space and Surprisal Space Logarithmic space has been studied in artificial neural network for a long history . But fruitless in ANN . Because the data field of ANN is the real number field from negative to positive . The surprisal has been defined in information theory and natural language processing and it is related to a random variable . Direct select variable from probability space , the surprisal is the negative log function . Moreover , we found that it is useful to define the surprisal space . Two difference between logarithmic space and surprisal space : 1 ) different in a negative sign 2 ) real space vs ( 0,1 ) space . surprisal represents a self-information bit . The non-linear product in probability space is the linear addition in surprisal space . There may have a lot of new things need to be studied in the surprisal space . Surprisal Space opens a door ."}, "2": {"review_id": "ryGpEiAcFQ-2", "review_text": "The authors present a biologically-inspired neural network model based on the excitatory and inhibitory ion channels in the membranes of real cells. Unfortunately, the paper is structured incoherently, making it nearly impossible to appreciate the authors' contribution. The introduction references neuroscience alongside ResNets, FinTech, surprisal spaces, Bose-Einstein statistics, and topological conjugacy without adequately motivating or defining any of the above. The fundamental definition of the model synapse as a conditional probability (Eq. 1) is not guaranteed to be non-negative, casting serious doubt on any of the subsequent conclusions. Figure 1 conveys no further information about the proposed model. There is no explicit related work or background section. The single experiment offers no comparison to alternative methods. I suggest the authors invest serious effort into rewriting the paper to clarify the presentation and explicitly state their contributions in the context of existing work on biologically-inspired learning models. This is indeed a subfield of machine learning worthy of more investigation. ", "rating": "2: Strong rejection", "reply_text": "Thanks for your review . Q1 : `` Unfortunately , the paper is structured incoherently , making it nearly impossible to appreciate the authors ' contribution . '' This paper was constructed in the structure of a math paper . We first defined a basic formula and gave an explanation of the formula that was based on an abstraction of the biological neural network . Then we gave the definition of the model , related concepts , and theorems with proofs . In this way , we explored and concluded many features of the model , the target was to figure out the learning rules be applied with backpropagation . Finally , we showed an experiment to prove the concept . Authors ' contributions : ( A lot of ) 1 . The neural network model is the authors ' creation . The biologically-inspired synapse equation S ( x , y ; a , b ) =ax ( 1-by ) is defined by the product of the opening probability of excitatory channels of a synapse and the opening probability of inhibitory channels of a synapse . Unlike the synapse in spike neural network , we consider the ion channels as the basis to build our synapse model . We ignored the spike feature of the biological neural network because it is the feature of neurons not synapses . The flow of ions and the random opening of the channels are the foundation of our synapse analysis . From a probability perspective , we abstract the synapse equation . In contrast to classical neural network with weights , its synapse is simply a product of the input variable and the weight . Our synapse is a non-linear unit . The practical biological synapse is much complex , But we present a simple model that can be analyzed in math . 2.We defined the surprisal space to connect Information Theory with our model . Although entropy is widely used in machine learning but surprisal is the more fundamental concept . When we apply surprisal on synapse equation , we have a linear combination in log space which has been used in machine learning analysis . With a negative in front of the log function , we can convert data to the surprisal space . In surprisal space , we can explain the negative log probability as the bits of self-information . That does make sense of surprisal space . It can be a new representation of the neural network . If somebody finds papers to applying surprisal space to explain neural network please let us know , we are going to list them as our references . By defining surprisal space , we build the mapping between a probability space and the surprisal space . It is a real positive space in our definition . The bits addition is the basic operation of a neural network . 3.Synapse with topological conjugacy is our discovery It is very exciting to find that the surprisal of the inhibitory probability is a topological conjugation in our model . That means the dynamical behavior in probability space can be bijected into surprisal space and both have the same dynamics . In advance , we discovered that this topological conjugation is a commutative diagram in category theory . That opened the door to apply new mathematic tools to study neural network . The discovery between the connection of the neural network and category theory is unexpected . That is one of our exciting contributions . 4.We discovered gradient updating in synapse learning followed Bose-Einstein distribution This is a direct conclusion from synapse equation in surprisal space without any statistical hypothesis . That solved the famous black box problem in our model . So we can expect some kind of BE distribution in the parametric matrix . It is a new representation of a neural network . 5.We constructed a fully-connected synaptic neural network as synapse tensor We successfully convert fully-connected non-linear synapse network into a matrix ( tensor ) computing . This synapse tensor is a special connection of synapses . Other network topologies are possible . Synapse tensor can be basic blocks to construct a large-scale neural network . 6.We discovered that synaptic neural network has a similar block to ResNet block in surprisal space That is why we mentioned ResNet . Except we apply surprisal non-linear function but still computing identity mapping . So we expect some features of ResNet such as protect gradient from vanishing in the very deep synaptic neural network . 7.We proved the gradient rule with loss function that mapped in surprisal space That is proof that we can apply the backpropagation algorithm on the fully-connected synaptic neural network . The proof is in very details because we want to verify that the new gradient computing is correct . In conclusion , our synaptic neural network is compatible with backpropagation , however , spike neural network is not ."}, "3": {"review_id": "ryGpEiAcFQ-3", "review_text": "Quality - poor The highly complicated work is evaluated only on the simplest of benchmarks with no significant results. Clarity - poor The paper seems to amount to gobbledygook, many disparate terminology strung together. Originality No idea. Significance None. cons: the paper to me seems a hashing of citations to the main works in neuroscience and deep learning for which only the simplest network is demonstrated (single hidden layer MLP on MNIST) with results that do not exceed that of a standard MLP. pros: the only pro I can think of for this work is that synaptic computing imo deserves more consideration, as real synapses are very complicated beasts, the functioning of which relatively little is known about. ", "rating": "2: Strong rejection", "reply_text": "`` the paper to me seems a hashing of citations to the main works in neuroscience and deep learning '' Please show us what main works in neuroscience and deep learning we have been hashing ?"}}