{"year": "2021", "forum": "wZ4yWvQ_g2y", "title": "Task-Agnostic and Adaptive-Size BERT Compression", "decision": "Reject", "meta_review": "Compressing BERT is a practically important research direction. Our main concern on this submission is on its practical value. Comparing with MobileBERT in the literature, NAS-BERT does not show advantages on any aspect: latency, prediction performance, or model size (less important), while being much more costly to build because of NAS. MobileBERT just simply narrowed the original BERT models (8x narrower than BERT large). So it is hard to convince the readers that adaptive-size or NAS is interesting or matters. On the research side, this paper have some interesting points on designing the search space, but overall the novelty of this paper is limited, as all of the reviewers pointed out. It is also worth noticing that the claim of \"task agonistic\" in this paper does not fully hold: in the downstream tasks, the soft labels of the teacher model are required to train the compressed model. To be fully \"task agonistic\", the results on downstream tasks should be solely based on training with the ground truth labels, as in the MobileBERT paper. Once following the exact task agnostic experimental protocol, the reported performance in this paper may be significantly lower. ", "reviews": [{"review_id": "wZ4yWvQ_g2y-0", "review_text": "Summary : This paper proposes to search architectures of BERT model under various memory and latency contraints . The search algorithm is conducted by pretraining a big supernet that contains the all the sub-network structures , where the optimal models for different requirements are selected from it . Once an architecture is found , it is re-trained through pretraining-finetuning or two-stage distillation for each specific task . Several approaches ( block-wise training and search , progressive shrinking , performance approximation ) are proposed to improve the search efficiency . Experiments on GLUE benchmark shows the models found by proposed methods can achieve better accuracy than some of the previous compressed BERT models . The paper ( together with the appendix ) is clearly presented , and the idea is new and interesting to me . The experiments are detailed and comprehensive . Pros : The paper is well presented . The architecture of the superent and the candidate operations are carefully designed and selected . It seems that the SpeConv operation is particularly effective when the model size is small . The search algorithm including the block-wise training , progressive shrinking can remove less-optimal structures quickly and significantly reduce the search space . The performance of NAS-BERT models are generally better than those of the compressed BERT models with similar model size , although the comparisons may not be completely fair . Concerns : 1 . The organization of the paper can be further improved . The paper may not be easy to follow if the appendix is skipped , especially for the readers who are not familiar with NAS or related work . Many of the important information can only be found in appendix . 2.The novelty of the paper is unclear to me . Although this work may be new on search BERT-like language model , it seems many of the ideas such as block-wise search and distillation are borrowed from existing work . Please the author clarify the main novelties and technical contribution of this work , especially to the field of neural architecture search or more broadly , AutoML . Moreover , some of the proposed techniques such as progressive shrinking are merely empirical practices and are lack of theory or insight showing how accurate the approximation would be . 3.It is usually more illustrative ( and also space saving ) to plot accuracy versus latency/ # parameters of different models in the same figure . Some of the well noted models such as MobileBERT and TinyBERT are not included in comparison . For DynaBERT , there are multiple configurations but only one is included . AdaBERT , which adopts NAS for each specific task , should also be included if possible . Again , since there are of many models with different size and latency , it may be better to have a plot for clear comparison . 4.HAT ( Wang et al.HAT : Hardware-Aware Transformers for Efficient Natural Language Processing . ACL 2020 . ) is not mentioned in the paper , which share similarities ( training supernet ) and differences ( search algorithm ) with this work from technical point of view . It will be better if the author can explain and compare the proposed search algorithm to evolutionary search .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your helpful review comments . Below are our responses to your concerns . * * [ Organization of the paper ] * * Thanks for your advice . We have re-organized in the new version as follows : 1 ) We move the detailed description of search space design from the Appendix to Section 3.1 . 2 ) To enable better comparison of accuracy versus latency/ # parameters of different models , we add more information in the Table 2 in the revised paper . 3 ) We move the experiment description of progreive shrinking from the Appendix to Section 4.2 . 4 ) We add the discussions with other works including HAT in the related work part . * * [ About the novelty of NAS-BERT ] * * - To achieve task-agnostic and adaptive-size BERT compression , we conduct neural architecture search by training the big supernet on the pre-training stage , which is extremely costly . Our proposed techniques including block-wise search , progressive shrinking , and performance approximation all target for alleviating the pain of heavy training cost and improving the search efficiency . - Different from block-wise search in [ 1 ] , we add identity operation to automatically search models with adaptive depths in each block . [ 1 ] achieves this by manually training different depths to get several models , which is acceptable in lightweight image tasks , but is extremely costly in BERT pre-training task and can not support devices with different kinds of memory and latency limitations . - Besides block-wise search , our newly proposed progressive shrinking and performance approximation are critical for efficiency and effectiveness . Specifically , for progressive shrinking , we split the models into different bins to prevent the smaller models from being pruned and guarantee that we can get various architectures to meet different constraints . - We think our work is not simply using existing techniques , but developing the existing techniques ( block-wise search ) and designing new techniques ( progressive shrinking , performance approximation , novel search space for BERT ) to solve the unique challenges in task-agnostic and adaptive-size BERT compression . - From the field of neural architecture search , most algorithms [ 2,3,4 ] are proposed to improve the performance on the computer vision tasks and light-weight NLP tasks ( e.g. , PTB ) . We extend NAS to the much heavy BERT pre-training task and propose a series of techniques to improve the efficiency , which may inspire a lot of following works on NAS for heavy tasks ."}, {"review_id": "wZ4yWvQ_g2y-1", "review_text": "This paper presents an effective NAS method for pre-trained language models at the pre-training stage , so the selected models can be applied to various downstream tasks with fine-tuning . To achieve better performances the widely used two-stage distillation and data augmentation are applied to the selected models from super-net . The main contribution of this work lies in the designed search space and the proposed three strategies ( block-wise search , progressive shrinking and performance approximation ) for improving search efficiency and accuracy . Although the novelty of this work is quite limited , training a big supernet for BERT at the pre-training stage is not trivial , which is useful for industry applications . The authors evaluate their approach on GLUE datasets and compare it to other state-of-the-art models . The paper is well-written and organized , the experiments are thorough . However , I have several concerns : 1 ) In the Table 1 under the KD setting , \u201c two-stage distillation \u201d is conducted on the selected models from supernet to further improve the performances , it would be interesting to add another two settings : a ) only conducting the distillation at the pre-training stage , b ) continuing to pre-training on large scale unlabeled data to finally obtain better task-agnostic models . 2 ) The models are evaluated on the GLUE dataset , more experiments on challenging QA tasks should be added . 3 ) In the Table 1 , the comparison to MobileBERT and TinyBERT should be added , and the FLOPs or the inference time on CPU/GPU can be provided . 4 ) Some important related work should be included , [ 1 ] HAT : Hardware-Aware Transformers for Efficient Natural Language Processing ( although this work focuses on the machine translation task ) [ 2 ] Finding Fast Transformers : One-Shot Neural Architecture Search by Component Composition .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your constructive review comments . Below are our responses to your concerns . * * [ Ablation study of two-stage distillation ] * * To study the performance of distillation at the pre-training or fine-tuning stage , we conduct experiments on two settings : 1 ) only using the distillation on the pre-training stage ; 2 ) only using the distillation on the fine-tuning stage . The results are shown as follows : | Setting |\\|| PD |\\|| KD |\\|| MNLI |\\|| QQP |\\|| QNLI |\\|| CoLA |\\|| SST-2 |\\|| STS-B |\\|| RTE |\\|| MRPC |\\|| AVG | || : - : | : -- : | : - : | : -- : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : -- : | : - : | : -- : | : - : | : - : | : - : | : - : | : - : | : - : | | $ \\text { BERT } _ { 60 } $ |\\|| \u221a |\\|| \u221a |\\|| 83.2 |\\|| 90.5 |\\|| 90.2 |\\|| 56.3 |\\|| 91.8 |\\|| 88.8 |\\|| 78.5 |\\|| 88.5 |\\|| 83.5 | | $ \\text { NAS-BERT } _ { 60 } $ |\\|| \u221a |\\|| \u221a |\\|| 84.1 |\\|| 91.0 |\\|| 91.3 |\\|| 58.1 |\\|| 92.1 |\\|| 89.4 |\\|| 79.2 |\\|| 88.5 |\\|| * * 84.2 * * | | $ \\text { BERT } _ { 60 } $ |\\|| \u221a |\\|| |\\|| 83.2 |\\|| 90.3 |\\|| 89.5 |\\|| 55.0 |\\|| 91.6 |\\|| 88.6 |\\|| 77.8 |\\|| 87.3 |\\|| 82.9 | | $ \\text { NAS-BERT } _ { 60 } $ |\\|| \u221a |\\|| |\\|| 83.3 |\\|| 90.9 |\\|| 91.3 |\\|| 55.6 |\\|| 92.0 |\\|| We have two observations from the results : 1 ) NAS-BERT outperforms the BERT baseline at different settings ; 2 ) distillation on either pre-training or fine-tuning stage can improve the scores , and two-stage distillation can further get better performance . * * [ Continuing to pre-training on large scale unlabeled data ] * * We are now continuing to pre-train on the large-scale dataset ( 160GB ) to obtain a better model . Due to the huge training cost , we will update the results when it is finished . * * [ More results on the challenging QA tasks ] * * We follow your suggestions and are conducting experiments on SQuAD 1.1 and SQuAD 2.0 . We will update the results when it is finished ."}, {"review_id": "wZ4yWvQ_g2y-2", "review_text": "Summary The paper develops a new method to compress the BERT model with varying model sizes depending on the underlying usage . They use block-wise neural architecture search to choose the best set of submodules for each of the blocks . To reduce the size of the exponential search space they progressively remove the architectural configurations that yields high loss . Over-all the paper is well written and nicely presented . +ve - The NAS-BERT can produce pertained models with varying model sizes which is better than DistilBERT and BERT-PKD that requires pre-training every time the number of layers are varied . - The paper conducts rigorous experiments to demonstrate the effectiveness of NAS-BERT . The baseline methods used for comparison covers most of the state-of-the-art methods used for model compression for BERT . Concerns : - Can the authors clarify if they save the model parameters corresponding to all the possible architectural choices ? Or they find out the best configuration matching the model size and latency requirements and then do the pre-training again with those architectural choices . - In appendix A.7 the paper demonstrates some of the architectures used by NAS-BERT . For lower model size ( < 20M ) it can be observed that the NAS-BERT ends up choosing SepConv layers most of the times . Do the authors do any analysis on why SepConv layer works better than the self-attention layer and the feed forward network . How does the network perform if it is composed of all SepConv layers ? Have the authors tried to use only SepConv layers and see if that itself gives good accuracy rather than doing the architecture search . - In terms of original ideas , although the concepts of block-wise architecture search , using SepConv layer for NLP tasks and using block-wise knowledge distillation are not novel by themselves but this paper has efficiently made use of the available techniques ( along with efficient engineering work like progressively reducing the search space ) to develop a method that gives good performance on NLP tasks .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your constructive review comments . Below are our responses to your concerns . * * [ clarify if we save the model parameters corresponding to all the possible architectural choices ] * * NAS-BERT can deliver various architectures with different model sizes and latency , which can save model parameters in different degrees . According to the given constraints ( params , latency ) , we can select an architecture to meet these requirements . Then the selected model is pre-trained from scratch independently without inheriting the parameters from the supernet . * * [ Analysis on why SepConv is preferred in small models ] * * We have conducted the analysis before the search space design . To demonstrate why SepConv is preferred in small models , we show the parameters and latency of MHA , FFN and SepConv as follows : | Operation ( hidden size 384 ) |\\|| Params |\\|| Latency ( s ) |\\|| Latency/Params | | -- | : -- : | : - : | : -- : | : -- : | : -- : | : -- : | | Multi-Head Attention |\\|| 592128 |\\|| 0.006146 |\\|| 1.00E-08 | | Feed-Forward Network |\\|| 1182336 |\\|| 0.005555 |\\|| 4.70E-09 | | Separable Conv 3 |\\|| 298760 |\\|| 0.002272 |\\|| 7.60E-09 | | Separable Conv 5 |\\|| 300300 |\\|| 0.002285 |\\|| 7.60E-09 | | Separable Conv 7 |\\|| 301840 |\\|| 0.002297 |\\|| 7.60E-09 | Compared with MHA , we can find that Latency/Params ratio of SepConv is smaller . Thus , the latency of SepConv is low when using the same number of parameters . Compared with FFN , given constraints of the same number of parameters , we can stack more layers without adding much latency ( usually better representation ability ) . For small models with strict latency and parameter constraints , SepConv can be stacked with more layers than FFN and MHA , and do not add too much latency . Thus SepConv is preferred . However , the network completely composed of SepConv layers can not get good results as explained in the next point below . * * [ Can the network completely composed of SepConv layers get better results than the searched model ? ] * * In our preliminary study , we build a network completely composed of SepConv layers , which can not get good performance . Specifically , we build two 10M models with a full stack of SepConv layers : 1 ) SepConv with kernel size 3 and hidden size 384 ; 2 ) SepConv with kernel size 7 and hidden size 384 to compare with $ \\text { NAS-BERT } _ { 10 } $ . The results are as follows : | Setting |\\|| MNLI |\\|| QQP |\\|| QNLI |\\|| CoLA |\\|| SST-2 |\\|| STS-B |\\|| RTE |\\|| MRPC |\\|| AVG | | -- | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : -- : | : - : | : -- : | : - : | : - : | : - : | : - : | : - : | : - : | | $ \\text { NAS-BERT } _ { 10 } $ |\\|| 76.4 |\\|| 88.5 |\\|| 86.3 |\\|| 34.0 |\\|| 88.6 |\\|| 84.8 |\\|| 66.6 |\\|| 79.1 |\\|| * * 75.5 * * | | $ \\text { BERT } _ { 10 } $ |\\|| 74.4 |\\|| 87.8 |\\|| 85.7 |\\|| 32.5 |\\|| 86.6 |\\|| 85.2 |\\|| 66.9 |\\|| 77.9 |\\|| 74.6 | | SepConv ( Kernel 3 ) |\\|| 58.1 |\\|| 80.9 |\\|| 63.0 |\\|| 38.4 |\\|| 83.3 |\\|| 45.2 |\\|| 53.0 |\\|| 70.7 |\\|| 61.6 | | SepConv ( Kernel 7 ) |\\|| 64.7 |\\|| 82.8 |\\|| 67.8 |\\|| 40.7 |\\|| 85.4 |\\|| 51.2 |\\|| 54.7 |\\|| 73.8 |\\|| 65.1 | We can observe that SepConv network gets much worse performance on the paired sentence tasks ( MNLI , QQP , QNLI , STS-B , RTE and MRPC ) compared with NAS-BERT and BERT We analyze that the paired sentence tasks typically need a large reception field to efficiently extract the information between two sentences . However , network composed of only SepConv has limited reception field given normal kernel size and limited layers , which can not efficiently model the global information . We can find that SepConv network with kernel size 7 gets better scores than that with kernel size 3 but still has a large gap with BERT baseline and NAS-BERT . We hypothesis that the advantages of NAS-BERT rely on the novel stacking between MHA , SepConv and FFN . MHA can extract the global information and SepConv or FFN can refine the information between adjacent positions ( feed-forward network can be viewed as a convolutional network with kernel size 1 ) ."}, {"review_id": "wZ4yWvQ_g2y-3", "review_text": "First of all , I believe the paper is looking into a very important question that attracts lots of attention recently . The set of techniques proposed in the work are also reasonable and practical , where the proposed progressive space pruning seems to work very well . Empirically , the obtained models do perform better compared to standard Transformer baselines . As for the comparison with previous methods , since there are too many implementation details that can affect the fairness of the comparison ( e.g.length of pretraining , batch size , teacher performances , etc ) , it 's hard to judge the actual scale of the gain . There are also a few concerns . - Firstly , when the block-wise search is used , it feels like the NAS-BERT is trained in a way that is more similar to a variant of distillation that additionally utilizes intermediate hidden states . As this signal is not used in the standard BERT baseline , some improvement could actually come from this factor besides a better model ( architecture+param ) . A better baseline could be a Transformer trained in a similar way . - Secondly , in terms of novelty , this work is more like the combination of existing ideas , namely `` once-for-all '' and `` block-wise search '' . One general issue with `` once-for-all '' is after the search , although you obtain multiple models of different sizes , the excellence these models are tied to the ( 1 ) specific set of shared parameters obtained and ( 2 ) the pre-training task . So , whether this is really a good way to obtain the desired task-agnostic compressed models is still questionable .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your constructive review comments . Below are our responses to your comments . * * [ Whether block-wise search is also used in student training and thus lead to unfair comparison with baselines ] * * Our pipeline consists of search stage and evaluation stage . The block-wise distillation is only conducted in the search stage to reduce the search space and improve the search efficiency . In the evaluation stage , the NAS-BERT model searched by our algorithm is pre-trained from scratch ( without inheriting weights from the supernet ) and then fine-tuned for a fair comparison with baseline . Thus , the improvement of our NAS-BERT comes from the discovered architectures . We also conduct experiments on NAS-BERT and the baseline using block-wise distillation in the pre-training in evaluation stage . We split the student model into different blocks following Fig.2 in the paper . Each block is trained under the supervision of the corresponding teacher block . Since block-wise distillation does not jointly optimize different blocks , we also use standard distillation as used in NAS-BERT in the second half of training steps . The results are shown as follows : | Model |\\|| MNLI |\\|| QQP |\\|| QNLI |\\|| CoLA |\\|| SST-2 |\\|| STS-B |\\|| RTE |\\|| MRPC |\\|| AVG | |-| : - : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : -- : | : - : | : -- : | : - : | : - : | : - : | : - : | : - : | : - : | | $ \\text { BERT } _ { 60 } $ |\\|| 83.2 |\\|| 90.5 |\\|| 90.2 |\\|| 56.3 |\\|| 91.8 |\\|| 88.8 |\\|| 78.5 |\\|| 88.5 |\\|| 83.5 | | $ \\text { NAS-BERT } _ { 60 } $ |\\|| 84.1 |\\|| 91.0 |\\|| 91.3 |\\|| 58.1 |\\|| 92.1 |\\|| 89.4 |\\|| 79.2 |\\|| 88.5 |\\|| * * 84.2 * * | | $ \\text { BERT } _ { 60 } +\\text { Block } $ |\\|| 82.2 |\\|| 90.4 |\\|| 90.0 |\\|| 54.4 |\\|| 92.0 |\\|| 88.7 |\\|| 74.9 |\\|| 87.7 |\\|| 82.5 | | $ \\text { NAS-BERT } _ { 60 } +\\text { Block } $ |\\|| 83.1 |\\|| 90.8 |\\|| 90.3 |\\|| 54.8 |\\|| 92.2 |\\|| 89.2 |\\|| 77.0 |\\|| 87.0 |\\|| * * 83.1 * * | * * Title * * : Comparison between block-wise distillation and two-stage distillation `` +Block '' means that the student model is trained with half of the training steps and continues to be trained with distillation as used in NAS-BERT with the remaining training steps . We find that block-wise distillation performs worse than the normal distillation in both BERT baseline and NAS-BERT . In the block-wise distillation setting , NAS-BERT still outperforms BERT baseline ."}], "0": {"review_id": "wZ4yWvQ_g2y-0", "review_text": "Summary : This paper proposes to search architectures of BERT model under various memory and latency contraints . The search algorithm is conducted by pretraining a big supernet that contains the all the sub-network structures , where the optimal models for different requirements are selected from it . Once an architecture is found , it is re-trained through pretraining-finetuning or two-stage distillation for each specific task . Several approaches ( block-wise training and search , progressive shrinking , performance approximation ) are proposed to improve the search efficiency . Experiments on GLUE benchmark shows the models found by proposed methods can achieve better accuracy than some of the previous compressed BERT models . The paper ( together with the appendix ) is clearly presented , and the idea is new and interesting to me . The experiments are detailed and comprehensive . Pros : The paper is well presented . The architecture of the superent and the candidate operations are carefully designed and selected . It seems that the SpeConv operation is particularly effective when the model size is small . The search algorithm including the block-wise training , progressive shrinking can remove less-optimal structures quickly and significantly reduce the search space . The performance of NAS-BERT models are generally better than those of the compressed BERT models with similar model size , although the comparisons may not be completely fair . Concerns : 1 . The organization of the paper can be further improved . The paper may not be easy to follow if the appendix is skipped , especially for the readers who are not familiar with NAS or related work . Many of the important information can only be found in appendix . 2.The novelty of the paper is unclear to me . Although this work may be new on search BERT-like language model , it seems many of the ideas such as block-wise search and distillation are borrowed from existing work . Please the author clarify the main novelties and technical contribution of this work , especially to the field of neural architecture search or more broadly , AutoML . Moreover , some of the proposed techniques such as progressive shrinking are merely empirical practices and are lack of theory or insight showing how accurate the approximation would be . 3.It is usually more illustrative ( and also space saving ) to plot accuracy versus latency/ # parameters of different models in the same figure . Some of the well noted models such as MobileBERT and TinyBERT are not included in comparison . For DynaBERT , there are multiple configurations but only one is included . AdaBERT , which adopts NAS for each specific task , should also be included if possible . Again , since there are of many models with different size and latency , it may be better to have a plot for clear comparison . 4.HAT ( Wang et al.HAT : Hardware-Aware Transformers for Efficient Natural Language Processing . ACL 2020 . ) is not mentioned in the paper , which share similarities ( training supernet ) and differences ( search algorithm ) with this work from technical point of view . It will be better if the author can explain and compare the proposed search algorithm to evolutionary search .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your helpful review comments . Below are our responses to your concerns . * * [ Organization of the paper ] * * Thanks for your advice . We have re-organized in the new version as follows : 1 ) We move the detailed description of search space design from the Appendix to Section 3.1 . 2 ) To enable better comparison of accuracy versus latency/ # parameters of different models , we add more information in the Table 2 in the revised paper . 3 ) We move the experiment description of progreive shrinking from the Appendix to Section 4.2 . 4 ) We add the discussions with other works including HAT in the related work part . * * [ About the novelty of NAS-BERT ] * * - To achieve task-agnostic and adaptive-size BERT compression , we conduct neural architecture search by training the big supernet on the pre-training stage , which is extremely costly . Our proposed techniques including block-wise search , progressive shrinking , and performance approximation all target for alleviating the pain of heavy training cost and improving the search efficiency . - Different from block-wise search in [ 1 ] , we add identity operation to automatically search models with adaptive depths in each block . [ 1 ] achieves this by manually training different depths to get several models , which is acceptable in lightweight image tasks , but is extremely costly in BERT pre-training task and can not support devices with different kinds of memory and latency limitations . - Besides block-wise search , our newly proposed progressive shrinking and performance approximation are critical for efficiency and effectiveness . Specifically , for progressive shrinking , we split the models into different bins to prevent the smaller models from being pruned and guarantee that we can get various architectures to meet different constraints . - We think our work is not simply using existing techniques , but developing the existing techniques ( block-wise search ) and designing new techniques ( progressive shrinking , performance approximation , novel search space for BERT ) to solve the unique challenges in task-agnostic and adaptive-size BERT compression . - From the field of neural architecture search , most algorithms [ 2,3,4 ] are proposed to improve the performance on the computer vision tasks and light-weight NLP tasks ( e.g. , PTB ) . We extend NAS to the much heavy BERT pre-training task and propose a series of techniques to improve the efficiency , which may inspire a lot of following works on NAS for heavy tasks ."}, "1": {"review_id": "wZ4yWvQ_g2y-1", "review_text": "This paper presents an effective NAS method for pre-trained language models at the pre-training stage , so the selected models can be applied to various downstream tasks with fine-tuning . To achieve better performances the widely used two-stage distillation and data augmentation are applied to the selected models from super-net . The main contribution of this work lies in the designed search space and the proposed three strategies ( block-wise search , progressive shrinking and performance approximation ) for improving search efficiency and accuracy . Although the novelty of this work is quite limited , training a big supernet for BERT at the pre-training stage is not trivial , which is useful for industry applications . The authors evaluate their approach on GLUE datasets and compare it to other state-of-the-art models . The paper is well-written and organized , the experiments are thorough . However , I have several concerns : 1 ) In the Table 1 under the KD setting , \u201c two-stage distillation \u201d is conducted on the selected models from supernet to further improve the performances , it would be interesting to add another two settings : a ) only conducting the distillation at the pre-training stage , b ) continuing to pre-training on large scale unlabeled data to finally obtain better task-agnostic models . 2 ) The models are evaluated on the GLUE dataset , more experiments on challenging QA tasks should be added . 3 ) In the Table 1 , the comparison to MobileBERT and TinyBERT should be added , and the FLOPs or the inference time on CPU/GPU can be provided . 4 ) Some important related work should be included , [ 1 ] HAT : Hardware-Aware Transformers for Efficient Natural Language Processing ( although this work focuses on the machine translation task ) [ 2 ] Finding Fast Transformers : One-Shot Neural Architecture Search by Component Composition .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your constructive review comments . Below are our responses to your concerns . * * [ Ablation study of two-stage distillation ] * * To study the performance of distillation at the pre-training or fine-tuning stage , we conduct experiments on two settings : 1 ) only using the distillation on the pre-training stage ; 2 ) only using the distillation on the fine-tuning stage . The results are shown as follows : | Setting |\\|| PD |\\|| KD |\\|| MNLI |\\|| QQP |\\|| QNLI |\\|| CoLA |\\|| SST-2 |\\|| STS-B |\\|| RTE |\\|| MRPC |\\|| AVG | || : - : | : -- : | : - : | : -- : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : -- : | : - : | : -- : | : - : | : - : | : - : | : - : | : - : | : - : | | $ \\text { BERT } _ { 60 } $ |\\|| \u221a |\\|| \u221a |\\|| 83.2 |\\|| 90.5 |\\|| 90.2 |\\|| 56.3 |\\|| 91.8 |\\|| 88.8 |\\|| 78.5 |\\|| 88.5 |\\|| 83.5 | | $ \\text { NAS-BERT } _ { 60 } $ |\\|| \u221a |\\|| \u221a |\\|| 84.1 |\\|| 91.0 |\\|| 91.3 |\\|| 58.1 |\\|| 92.1 |\\|| 89.4 |\\|| 79.2 |\\|| 88.5 |\\|| * * 84.2 * * | | $ \\text { BERT } _ { 60 } $ |\\|| \u221a |\\|| |\\|| 83.2 |\\|| 90.3 |\\|| 89.5 |\\|| 55.0 |\\|| 91.6 |\\|| 88.6 |\\|| 77.8 |\\|| 87.3 |\\|| 82.9 | | $ \\text { NAS-BERT } _ { 60 } $ |\\|| \u221a |\\|| |\\|| 83.3 |\\|| 90.9 |\\|| 91.3 |\\|| 55.6 |\\|| 92.0 |\\|| We have two observations from the results : 1 ) NAS-BERT outperforms the BERT baseline at different settings ; 2 ) distillation on either pre-training or fine-tuning stage can improve the scores , and two-stage distillation can further get better performance . * * [ Continuing to pre-training on large scale unlabeled data ] * * We are now continuing to pre-train on the large-scale dataset ( 160GB ) to obtain a better model . Due to the huge training cost , we will update the results when it is finished . * * [ More results on the challenging QA tasks ] * * We follow your suggestions and are conducting experiments on SQuAD 1.1 and SQuAD 2.0 . We will update the results when it is finished ."}, "2": {"review_id": "wZ4yWvQ_g2y-2", "review_text": "Summary The paper develops a new method to compress the BERT model with varying model sizes depending on the underlying usage . They use block-wise neural architecture search to choose the best set of submodules for each of the blocks . To reduce the size of the exponential search space they progressively remove the architectural configurations that yields high loss . Over-all the paper is well written and nicely presented . +ve - The NAS-BERT can produce pertained models with varying model sizes which is better than DistilBERT and BERT-PKD that requires pre-training every time the number of layers are varied . - The paper conducts rigorous experiments to demonstrate the effectiveness of NAS-BERT . The baseline methods used for comparison covers most of the state-of-the-art methods used for model compression for BERT . Concerns : - Can the authors clarify if they save the model parameters corresponding to all the possible architectural choices ? Or they find out the best configuration matching the model size and latency requirements and then do the pre-training again with those architectural choices . - In appendix A.7 the paper demonstrates some of the architectures used by NAS-BERT . For lower model size ( < 20M ) it can be observed that the NAS-BERT ends up choosing SepConv layers most of the times . Do the authors do any analysis on why SepConv layer works better than the self-attention layer and the feed forward network . How does the network perform if it is composed of all SepConv layers ? Have the authors tried to use only SepConv layers and see if that itself gives good accuracy rather than doing the architecture search . - In terms of original ideas , although the concepts of block-wise architecture search , using SepConv layer for NLP tasks and using block-wise knowledge distillation are not novel by themselves but this paper has efficiently made use of the available techniques ( along with efficient engineering work like progressively reducing the search space ) to develop a method that gives good performance on NLP tasks .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your constructive review comments . Below are our responses to your concerns . * * [ clarify if we save the model parameters corresponding to all the possible architectural choices ] * * NAS-BERT can deliver various architectures with different model sizes and latency , which can save model parameters in different degrees . According to the given constraints ( params , latency ) , we can select an architecture to meet these requirements . Then the selected model is pre-trained from scratch independently without inheriting the parameters from the supernet . * * [ Analysis on why SepConv is preferred in small models ] * * We have conducted the analysis before the search space design . To demonstrate why SepConv is preferred in small models , we show the parameters and latency of MHA , FFN and SepConv as follows : | Operation ( hidden size 384 ) |\\|| Params |\\|| Latency ( s ) |\\|| Latency/Params | | -- | : -- : | : - : | : -- : | : -- : | : -- : | : -- : | | Multi-Head Attention |\\|| 592128 |\\|| 0.006146 |\\|| 1.00E-08 | | Feed-Forward Network |\\|| 1182336 |\\|| 0.005555 |\\|| 4.70E-09 | | Separable Conv 3 |\\|| 298760 |\\|| 0.002272 |\\|| 7.60E-09 | | Separable Conv 5 |\\|| 300300 |\\|| 0.002285 |\\|| 7.60E-09 | | Separable Conv 7 |\\|| 301840 |\\|| 0.002297 |\\|| 7.60E-09 | Compared with MHA , we can find that Latency/Params ratio of SepConv is smaller . Thus , the latency of SepConv is low when using the same number of parameters . Compared with FFN , given constraints of the same number of parameters , we can stack more layers without adding much latency ( usually better representation ability ) . For small models with strict latency and parameter constraints , SepConv can be stacked with more layers than FFN and MHA , and do not add too much latency . Thus SepConv is preferred . However , the network completely composed of SepConv layers can not get good results as explained in the next point below . * * [ Can the network completely composed of SepConv layers get better results than the searched model ? ] * * In our preliminary study , we build a network completely composed of SepConv layers , which can not get good performance . Specifically , we build two 10M models with a full stack of SepConv layers : 1 ) SepConv with kernel size 3 and hidden size 384 ; 2 ) SepConv with kernel size 7 and hidden size 384 to compare with $ \\text { NAS-BERT } _ { 10 } $ . The results are as follows : | Setting |\\|| MNLI |\\|| QQP |\\|| QNLI |\\|| CoLA |\\|| SST-2 |\\|| STS-B |\\|| RTE |\\|| MRPC |\\|| AVG | | -- | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : -- : | : - : | : -- : | : - : | : - : | : - : | : - : | : - : | : - : | | $ \\text { NAS-BERT } _ { 10 } $ |\\|| 76.4 |\\|| 88.5 |\\|| 86.3 |\\|| 34.0 |\\|| 88.6 |\\|| 84.8 |\\|| 66.6 |\\|| 79.1 |\\|| * * 75.5 * * | | $ \\text { BERT } _ { 10 } $ |\\|| 74.4 |\\|| 87.8 |\\|| 85.7 |\\|| 32.5 |\\|| 86.6 |\\|| 85.2 |\\|| 66.9 |\\|| 77.9 |\\|| 74.6 | | SepConv ( Kernel 3 ) |\\|| 58.1 |\\|| 80.9 |\\|| 63.0 |\\|| 38.4 |\\|| 83.3 |\\|| 45.2 |\\|| 53.0 |\\|| 70.7 |\\|| 61.6 | | SepConv ( Kernel 7 ) |\\|| 64.7 |\\|| 82.8 |\\|| 67.8 |\\|| 40.7 |\\|| 85.4 |\\|| 51.2 |\\|| 54.7 |\\|| 73.8 |\\|| 65.1 | We can observe that SepConv network gets much worse performance on the paired sentence tasks ( MNLI , QQP , QNLI , STS-B , RTE and MRPC ) compared with NAS-BERT and BERT We analyze that the paired sentence tasks typically need a large reception field to efficiently extract the information between two sentences . However , network composed of only SepConv has limited reception field given normal kernel size and limited layers , which can not efficiently model the global information . We can find that SepConv network with kernel size 7 gets better scores than that with kernel size 3 but still has a large gap with BERT baseline and NAS-BERT . We hypothesis that the advantages of NAS-BERT rely on the novel stacking between MHA , SepConv and FFN . MHA can extract the global information and SepConv or FFN can refine the information between adjacent positions ( feed-forward network can be viewed as a convolutional network with kernel size 1 ) ."}, "3": {"review_id": "wZ4yWvQ_g2y-3", "review_text": "First of all , I believe the paper is looking into a very important question that attracts lots of attention recently . The set of techniques proposed in the work are also reasonable and practical , where the proposed progressive space pruning seems to work very well . Empirically , the obtained models do perform better compared to standard Transformer baselines . As for the comparison with previous methods , since there are too many implementation details that can affect the fairness of the comparison ( e.g.length of pretraining , batch size , teacher performances , etc ) , it 's hard to judge the actual scale of the gain . There are also a few concerns . - Firstly , when the block-wise search is used , it feels like the NAS-BERT is trained in a way that is more similar to a variant of distillation that additionally utilizes intermediate hidden states . As this signal is not used in the standard BERT baseline , some improvement could actually come from this factor besides a better model ( architecture+param ) . A better baseline could be a Transformer trained in a similar way . - Secondly , in terms of novelty , this work is more like the combination of existing ideas , namely `` once-for-all '' and `` block-wise search '' . One general issue with `` once-for-all '' is after the search , although you obtain multiple models of different sizes , the excellence these models are tied to the ( 1 ) specific set of shared parameters obtained and ( 2 ) the pre-training task . So , whether this is really a good way to obtain the desired task-agnostic compressed models is still questionable .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your constructive review comments . Below are our responses to your comments . * * [ Whether block-wise search is also used in student training and thus lead to unfair comparison with baselines ] * * Our pipeline consists of search stage and evaluation stage . The block-wise distillation is only conducted in the search stage to reduce the search space and improve the search efficiency . In the evaluation stage , the NAS-BERT model searched by our algorithm is pre-trained from scratch ( without inheriting weights from the supernet ) and then fine-tuned for a fair comparison with baseline . Thus , the improvement of our NAS-BERT comes from the discovered architectures . We also conduct experiments on NAS-BERT and the baseline using block-wise distillation in the pre-training in evaluation stage . We split the student model into different blocks following Fig.2 in the paper . Each block is trained under the supervision of the corresponding teacher block . Since block-wise distillation does not jointly optimize different blocks , we also use standard distillation as used in NAS-BERT in the second half of training steps . The results are shown as follows : | Model |\\|| MNLI |\\|| QQP |\\|| QNLI |\\|| CoLA |\\|| SST-2 |\\|| STS-B |\\|| RTE |\\|| MRPC |\\|| AVG | |-| : - : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : - : | : -- : | : - : | : -- : | : - : | : - : | : - : | : - : | : - : | : - : | | $ \\text { BERT } _ { 60 } $ |\\|| 83.2 |\\|| 90.5 |\\|| 90.2 |\\|| 56.3 |\\|| 91.8 |\\|| 88.8 |\\|| 78.5 |\\|| 88.5 |\\|| 83.5 | | $ \\text { NAS-BERT } _ { 60 } $ |\\|| 84.1 |\\|| 91.0 |\\|| 91.3 |\\|| 58.1 |\\|| 92.1 |\\|| 89.4 |\\|| 79.2 |\\|| 88.5 |\\|| * * 84.2 * * | | $ \\text { BERT } _ { 60 } +\\text { Block } $ |\\|| 82.2 |\\|| 90.4 |\\|| 90.0 |\\|| 54.4 |\\|| 92.0 |\\|| 88.7 |\\|| 74.9 |\\|| 87.7 |\\|| 82.5 | | $ \\text { NAS-BERT } _ { 60 } +\\text { Block } $ |\\|| 83.1 |\\|| 90.8 |\\|| 90.3 |\\|| 54.8 |\\|| 92.2 |\\|| 89.2 |\\|| 77.0 |\\|| 87.0 |\\|| * * 83.1 * * | * * Title * * : Comparison between block-wise distillation and two-stage distillation `` +Block '' means that the student model is trained with half of the training steps and continues to be trained with distillation as used in NAS-BERT with the remaining training steps . We find that block-wise distillation performs worse than the normal distillation in both BERT baseline and NAS-BERT . In the block-wise distillation setting , NAS-BERT still outperforms BERT baseline ."}}