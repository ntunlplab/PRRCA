{"year": "2020", "forum": "HyxjOyrKvr", "title": "Neural Epitome Search for Architecture-Agnostic Network Compression", "decision": "Accept (Poster)", "meta_review": "The paper proposed a novel way to compress arbitrary networks by learning epitiomes and corresponding transformations of them to reconstruct the original weight tensors. The idea is very interesting and the paper presented good experimental validations of the proposed method on state-of-the-art models and showed good MAdd reduction. The authors also put a lot of efforts addressing the concerns of all the reviewers by improving the presentation of the paper, which although can still be further improved, and adding more explanations and validations on the proposed method. Although there's still concerns on whether the reduction of MAdd really transforms to computation reduction, all the reviewers agreed the paper is interesting and useful and further development of such work would be useful too. ", "reviews": [{"review_id": "HyxjOyrKvr-0", "review_text": "In this work, the authors describe a technique for compressing neural networks by learning a so-called Epitome (E), and a transformation function (\\theta) such that the weights for each layer can be constructed using \\theta(E). The epitome and the transformation function can be learnt jointly while optimizing the network for the task specific loss. The main idea of this paper is really interesting, and the experimental results which compare against other recent techniques also validate the proposed technique. However, while I think the main idea is relatively clear, I personally found the description of the proposed techniques -- particularly 3.2 and 3.4 -- to be somewhat hard to follow, particularly given that some details only appear in the Appendix. I would suggest that the authors try to revise this section by trying to move Figures 4 and 5 from the appendix into the main text. Some additional suggestions also appear below. Overall, I would while I like the ideas in this paper, based on the current presentation I am inclined to rate the paper as a \u201cweak reject\u201d, though I would raise my rating if the paper was revised to improve the presentation. Main comments: 1. The authors mention that \u201cDuring inference, (the) routing map enables the model to reuse computations when the expanded weight tensors are formed based on the same set of elements in the epitomes and therefore effectively reduces the computation cost.\u201d It would be nice to include some results which indicate what the savings are with the proposed routing map. 2. Section 3.2: There were a few aspects of section 3.2 that I think could be improved for clarity. A.) Personally, I found Figure 2 somewhat tricky to follow. I would suggest removing the 3x2 \u201cEpitome\u201d and \u201cGenerated Kernel\u201d figures on the extreme right of the image, since I\u2019m assuming the only goal of these is to indicate that \u201corange\u201d and \u201cblue\u201d correspond to \u201cEpitome\u201d and \u201cKernel\u201d respectively. I would also suggest mentioning the correspondence of the colors as the first sentence in the caption. Finally, if possible, I would suggest adding a small description alongside the (a), (b), (c) subcaptions: e.g., (a) straightforward but non-differentiable mapping, \u2026 , (c) Generated Weigh Kernel. B.) I believe that the authors use a separate E for each layer, and that Epitomes are not shared across layers. I may have missed this in the text, but it would be useful to clarify this explicitly again in the section. C.) The \u201cparameterized transformation layer\u201d is mentioned before Equation 3. I think it would be useful to mention that this is implemented using neural networks in your work for clarity. E.g.: \u201cTo handle the above two obstacles, ... three parts: (1) a parameterized transformation learner \u03b7 (implemented using a neural network in this work) used to learn a set of starting indices for patches in epitome (i.e. all elements in the same epitome patch share identical starting indices); \u2026 and an interpolation based generator (Eqn. 3).\u201d or \u201cTo handle the above two obstacles, ... three parts: (1) a parameterized transformation learner \u03b7 (See Section 3.3) used to learn a set of starting indices for patches in epitome (i.e. all elements in the same epitome patch share identical starting indices); \u2026 and an interpolation based generator (Eqn. 3).\u201d D.) The exact structure of the \u201cparameterized transformation layer\u201d wasn\u2019t exactly clear to me. In 3.3, the authors mention that it consists of \u201cof two convolutional layer, followed by a sigmoid function \u2026 takes the feature map of the convolutional layer as input\u201d. Please clarify exactly what is fed in as the input to this network e.g., (input feature map: F, and the indices i,j). 3. I personally also found Section 3.4 which discusses the Computation reduction was also somewhat hard to follow. Some clarification questions: Is the memory/computation cost of the storing/creating the routing map included in the compression calculations? I think it is important to factor these costs when computing the savings achieved by the model. Also, I was unclear on what R_{cin} and R_{cout} are, and why they appear in Equation 5. Could the authors please clarify. Minor Comments: 1. Abstract: \u201cTraditional compression methods \u2026 all assume that network architectures and parameters should be hardwired.\u201d What does it mean for them to be \u201chardwired\u201d in this context? 2. Abstract: \u201cExperiments demonstrate that, \u2026 with 25% MAdd reduction and AutoML for Model Compression (AMC) by 2.5% with nearly the same compression ratio.\u201d --> \u201cExperiments demonstrate that, \u2026 with 25% MAdd reduction, and a 2.5% Madd reduction for AutoML for Model Compression (AMC) with nearly the same compression ratio.\u201d ------- Update after Author Response -------- I would like to thank the authors for their responses and for the updates which strengthen the paper in my view. I have updated my score accordingly. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your detailed and valuable review ! Following your suggestions , we have updated the paper with improved presentation and we 're happy to address your concerns . A summary of the modifications is shown as below : Reply to major comments : Computation savings with routing map : \u2022 The benefits brought by the routing map is that the index learner can be discarded during the inference phase , and the outputs of the index learner is recorded in the routing map and saved as a look-up table . With the implementation of the routing map , the computation overhead due to the index learner is removed . We have revised section 3.2 to state this benefit clearer in a separate paragraph named as \u201c Routing map \u201d Redesign of the weight tensor generation process ( figure 2 in original version ) : \u2022 We agree that previous figure 2 is not clear on the generation process . We have modified figure 2 in section 3.2 to improve the presentation . We combined figure 4 and figure 2 in the original version to give an example of the transformation process along the spatial dimension with the structure of the index learner . We also added figure 3 to illustrate the transformation along the channel dimensions . Based on figure 3 , we put in more details regarding the meaning of R_ { cin } . The meaning of R_ { cin } is the number of samplings applied along the input channel dimension . The transformation along the filter dimension is same as the transformation along the input channel dimension . The difference is that the computation reuse along the input channel dimension needs special techniques of channel wrapping . We illustrate the channel wrapping process in Figure 4 of the updated version . Yes , each layer has an epitome and the epitome is not shared among layers . We have revised the paper to emphasize this point in section 3.2 paragraph \u201c Indexing function \u201d . Design of the transformation function : \u2022 We use separate transformation learners for each layer . The learner is implemented by two convolution layers . It takes in the input feature tensor of the corresponding layer and outputs the mapped starting index of the sub-tensor in the epitome . The sub-tensor at the new position will be used to construct the weight tensor for convolution . We also show the structure of the learner in the revised figure 2 . We rename this transformation learner as index learner in the revised version in section 3.2 . Memory and computation compression calculation of the routing map : \u2022 The memory cost of the routing map is 3 x R_ { cin } + R_ { cout } . The routing map is used to record the index mapping between the sub-tensor in the epitome and the weight tensor . This is implemented as a look-up table manner . Because we take a patch of the tensor in the epitome , only the starting indices are recorded . The position of the rest of the elements can be calculated as starting index + offset where offset is the relative position for those elements in the sub-tensor . Explanation of R_ { cin } and R_ { cout } : \u2022 For example , with a learned routing map M , the sub-tensor in the epitome that will be used to fill up the sub-tensor at position ( i , j , m ) in the weight tensor is E [ p : p+w , q : q+h , c \u2019 _ { in } : c \u2019 _ { in } + \\beta_1 , : ] , where ( p , q , c \u2019 _ { in } ) = M ( i , j , m ) . The selected sub-tensors in the epitome are concatenated together to form a larger tensor . Hence , the number of such indices is calculated as C \u2019 _ { in } / \\beta_1 and this is noted as R_ { cin } . Since each recorded index has three values , the size of the routing map is 3 x R_ { cin } . Similarly , along the filter dimension , the size is R_ { cout } where R_ { cout } = C ' _ { out } / \\beta_2 . Here , \\beta_1 and \\beta_2 denotes the length of the sub-tensor along the input channel dimension and the filter dimension that we select from the epitome . We have put in more details regarding the meaning of R_ { cin } and R_ { cout } in section 3.2 . Since the routing map is implemented as a look-up table , the computation is O ( 1 ) . The formulation for the computation of the compression ratio has taken the routing map into consideration . Reply to minor comments : \u2022 Meaning of \u201c hardwired \u201d : this term is supposed to describe the relationship between the neural network architecture and the corresponding parameters . In this work , we consider the network architecture and the parameters as separate components . The word \u201c hardwired \u201d is used to describe the one-to-one correspondence between the architecture and the parameters . We have updated the abstract to make this clearer . \u2022 We have modified the experiments summary part in the abstract as pointed out ."}, {"review_id": "HyxjOyrKvr-1", "review_text": "In this paper, the authors learn epitomes, which are small weight tensors which can be used with a learnt transform to produce tensors of an appropriate size (e.g. the sizes used in MobileNet v2). This gives a reduction in the number of parameters required, and the number of MAdds in theory. This paper is badly written, and could do with a rewrite: - Citations are used incorrectly (\\cite should be used when the citation is meant to be read as part of the sentence). - \"less elements\" --> \"fewer elements\" - \"misuse the notion\" --> \"abuse the notation\"? - \"for fair comparisons\" --> \"for a fair comparison\" The method is poorly explained; I have read Section 3.2 several times, and I'm still not entirely certain of what's going on. Figure 1 is helpful, but Figure 2 is not, and could be redesigned. 2(b) makes it look like you are going from a 3x3 epitome to a 2x2 kernel, which is clearly not what is happening. I think it would be helpful to give a detailed pictoral example of an epitome mapping to a weight tensor, with arrows between relevant indices changing. On initial reading, I thought the method allowed dynamic allocation of your epitomes to lots of different tensor sizes. From what I can tell, the network has to be trained from scratch for each possible size, so it isn't flexible in that respect. From what I can gather, the paper is presenting an alternate approach to *downscale* networks, as opposed to say, reducing width or depth. The comparisons to different widths of MobileNet v2 make more sense under this scenario. The main sell of the methods appears to be on the basis of MAdd reduction. This makes me nervous, as it doesn't necessarily correspond to actual speed-up or a reduction in energy (see https://arxiv.org/abs/1801.04326). EfficientNet was mainly about Madds too, but they provided some inference times. Would it be possible to add these? The method used with the integral image sounds expensive. The results look good, but error bars,on the CIFAR experiments at the very least, would be appreciated. Pros: ------- - Good results - Method appears largely novel (although bears some resemblance to https://arxiv.org/abs/1906.04309) Cons: -------- - Badly written - The method is poorly explained - Uncertainty re: MAdds as a primary comparator I propose a weak reject for this paper for two primary reasons: 1) The standard of writing, and the explanation of the all-important method are not up to scratch for a top tier conference 2) I have concerns regarding the MAdd calculations. Perhaps you could provide some pseudo-code in the author response? I am happy to upgrade my score if the authors deal with these issues sufficiently.", "rating": "3: Weak Reject", "reply_text": "The pseudo code for the madd calculation is shown as below : Pseudo code for Madd calculation : def conv_flops_counter_hook ( conv_module , input , output ) : # calculation based on equation 21 in the paper batch_size = input.shape [ 0 ] output_dims = list ( output.shape [ 2 : ] ) kernel_dims = list ( conv_module.kernel_size ) in_channels = conv_module.in_channels out_channels = conv_module.out_channels epi_in = self.weight.shape [ 1 ] epi_out = self.weight.shape [ 0 ] epi_w , epi_h = self.weight.shape [ 2 ] , self.weight.shape [ 3 ] # we choose \\beta_1 = epi_in and \\beta_2 = epi_out empirically rep_dim1 = out_channels // epi_in rep_dim0 = in_channels // epi_out filters_per_channel = out_channels // groups product_map_cost = ( 2 * in_channels * epi_w * epi_h -1 ) * np.prod ( output_dims ) * epi_out integral_map_cost = np.prod ( output_dims ) * epi_w * epi_h * epi_out channel_trans_cost = 2 * rep_dim1 * np.prod ( output_dims ) filter_trans_cost = 2 * rep_dim0 conv_per_position_flops = product_map_cost + integral_map_cost + channel_trans_cost + filter_trans_cost"}, {"review_id": "HyxjOyrKvr-2", "review_text": "This paper focuses on the problem of neural network compression, and proposes a new scheme, the neural epitome search. It learns to find compact yet expressive epitomes for weight parameters of a specified network architecture. The learned weight tensors are independent of the architecture design. It can be encapsulated as a drop in replacement to the current convolutional operator. It can incur less performance drop. Experiments are conducted to show the effectiveness of the proposed method. However, there are some concerns to be addressed. -It is not too clear how to learn the epitomes and transformation functions. -Authors stated that the proposed method is independent of the architecture design. From the current statements, it is not explained clearly.", "rating": "6: Weak Accept", "reply_text": "Thank you for your review ! Your questions are valuable , and we are happy to address your concerns . In summary , we explain below how the epitome parameters are updated and why the method is agnostic to the model architecture . Q1 : It is not too clear how to learn the epitome and transformation functions . \u2022 Epitome parameter updates : the epitome is defined as a parameter tensor with smaller dimensions . The parameters inside the epitome are updated with standard back-propagation to minimize the training loss , after being transformed to instantiate model parameters . The gradient of each parameter in the epitome is the summation of the gradients of all the elements in the convolution weight tensor where the corresponding epitome element is used . The transformation function is end-to-end differentiable and hence the epitome parameters are updated in an end-to-end manner . The learning procedure for epitome is introduced in detail in Section 3.3 . We have revised the section to make it clearer . \u2022 Transformation function : We updated the description in Section 3.2 on transformation functions in the paper . In summary , the transformation function includes three parts : ( 1 ) an index learner , ( 2 ) a routing map and ( 3 ) an interpolation based sampler . The transformation function works as follows : It uses the index learner to learn a group of indices that map the sub-tensor inside the epitome to the convolution weight tensor . The learned indices and the epitome are then fed into an interpolation based sampler . Each pair of the learned starting index will have a corresponding output from the interpolation based sampler . Those outputs are then concatenated together to constitute the weight tensor . The index learner is implemented by a two-layer module . We use separate index learners for each layer and the learner takes the input feature of the corresponding layer as its input and outputs the learned starting indices . Thus , the learner can be optimized via standard back propagation . Q2 : Independence between the epitome and the architecture \u2022 In section 3 , we show that the model size and the computation are mainly related to the size of the epitome . With our proposed transformation function , the shape of the epitome is independent of the model architecture . This is because the transformation function is able to transform any shape of the epitome to a tensor matching the shape of weight tensor for the architecture ."}], "0": {"review_id": "HyxjOyrKvr-0", "review_text": "In this work, the authors describe a technique for compressing neural networks by learning a so-called Epitome (E), and a transformation function (\\theta) such that the weights for each layer can be constructed using \\theta(E). The epitome and the transformation function can be learnt jointly while optimizing the network for the task specific loss. The main idea of this paper is really interesting, and the experimental results which compare against other recent techniques also validate the proposed technique. However, while I think the main idea is relatively clear, I personally found the description of the proposed techniques -- particularly 3.2 and 3.4 -- to be somewhat hard to follow, particularly given that some details only appear in the Appendix. I would suggest that the authors try to revise this section by trying to move Figures 4 and 5 from the appendix into the main text. Some additional suggestions also appear below. Overall, I would while I like the ideas in this paper, based on the current presentation I am inclined to rate the paper as a \u201cweak reject\u201d, though I would raise my rating if the paper was revised to improve the presentation. Main comments: 1. The authors mention that \u201cDuring inference, (the) routing map enables the model to reuse computations when the expanded weight tensors are formed based on the same set of elements in the epitomes and therefore effectively reduces the computation cost.\u201d It would be nice to include some results which indicate what the savings are with the proposed routing map. 2. Section 3.2: There were a few aspects of section 3.2 that I think could be improved for clarity. A.) Personally, I found Figure 2 somewhat tricky to follow. I would suggest removing the 3x2 \u201cEpitome\u201d and \u201cGenerated Kernel\u201d figures on the extreme right of the image, since I\u2019m assuming the only goal of these is to indicate that \u201corange\u201d and \u201cblue\u201d correspond to \u201cEpitome\u201d and \u201cKernel\u201d respectively. I would also suggest mentioning the correspondence of the colors as the first sentence in the caption. Finally, if possible, I would suggest adding a small description alongside the (a), (b), (c) subcaptions: e.g., (a) straightforward but non-differentiable mapping, \u2026 , (c) Generated Weigh Kernel. B.) I believe that the authors use a separate E for each layer, and that Epitomes are not shared across layers. I may have missed this in the text, but it would be useful to clarify this explicitly again in the section. C.) The \u201cparameterized transformation layer\u201d is mentioned before Equation 3. I think it would be useful to mention that this is implemented using neural networks in your work for clarity. E.g.: \u201cTo handle the above two obstacles, ... three parts: (1) a parameterized transformation learner \u03b7 (implemented using a neural network in this work) used to learn a set of starting indices for patches in epitome (i.e. all elements in the same epitome patch share identical starting indices); \u2026 and an interpolation based generator (Eqn. 3).\u201d or \u201cTo handle the above two obstacles, ... three parts: (1) a parameterized transformation learner \u03b7 (See Section 3.3) used to learn a set of starting indices for patches in epitome (i.e. all elements in the same epitome patch share identical starting indices); \u2026 and an interpolation based generator (Eqn. 3).\u201d D.) The exact structure of the \u201cparameterized transformation layer\u201d wasn\u2019t exactly clear to me. In 3.3, the authors mention that it consists of \u201cof two convolutional layer, followed by a sigmoid function \u2026 takes the feature map of the convolutional layer as input\u201d. Please clarify exactly what is fed in as the input to this network e.g., (input feature map: F, and the indices i,j). 3. I personally also found Section 3.4 which discusses the Computation reduction was also somewhat hard to follow. Some clarification questions: Is the memory/computation cost of the storing/creating the routing map included in the compression calculations? I think it is important to factor these costs when computing the savings achieved by the model. Also, I was unclear on what R_{cin} and R_{cout} are, and why they appear in Equation 5. Could the authors please clarify. Minor Comments: 1. Abstract: \u201cTraditional compression methods \u2026 all assume that network architectures and parameters should be hardwired.\u201d What does it mean for them to be \u201chardwired\u201d in this context? 2. Abstract: \u201cExperiments demonstrate that, \u2026 with 25% MAdd reduction and AutoML for Model Compression (AMC) by 2.5% with nearly the same compression ratio.\u201d --> \u201cExperiments demonstrate that, \u2026 with 25% MAdd reduction, and a 2.5% Madd reduction for AutoML for Model Compression (AMC) with nearly the same compression ratio.\u201d ------- Update after Author Response -------- I would like to thank the authors for their responses and for the updates which strengthen the paper in my view. I have updated my score accordingly. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your detailed and valuable review ! Following your suggestions , we have updated the paper with improved presentation and we 're happy to address your concerns . A summary of the modifications is shown as below : Reply to major comments : Computation savings with routing map : \u2022 The benefits brought by the routing map is that the index learner can be discarded during the inference phase , and the outputs of the index learner is recorded in the routing map and saved as a look-up table . With the implementation of the routing map , the computation overhead due to the index learner is removed . We have revised section 3.2 to state this benefit clearer in a separate paragraph named as \u201c Routing map \u201d Redesign of the weight tensor generation process ( figure 2 in original version ) : \u2022 We agree that previous figure 2 is not clear on the generation process . We have modified figure 2 in section 3.2 to improve the presentation . We combined figure 4 and figure 2 in the original version to give an example of the transformation process along the spatial dimension with the structure of the index learner . We also added figure 3 to illustrate the transformation along the channel dimensions . Based on figure 3 , we put in more details regarding the meaning of R_ { cin } . The meaning of R_ { cin } is the number of samplings applied along the input channel dimension . The transformation along the filter dimension is same as the transformation along the input channel dimension . The difference is that the computation reuse along the input channel dimension needs special techniques of channel wrapping . We illustrate the channel wrapping process in Figure 4 of the updated version . Yes , each layer has an epitome and the epitome is not shared among layers . We have revised the paper to emphasize this point in section 3.2 paragraph \u201c Indexing function \u201d . Design of the transformation function : \u2022 We use separate transformation learners for each layer . The learner is implemented by two convolution layers . It takes in the input feature tensor of the corresponding layer and outputs the mapped starting index of the sub-tensor in the epitome . The sub-tensor at the new position will be used to construct the weight tensor for convolution . We also show the structure of the learner in the revised figure 2 . We rename this transformation learner as index learner in the revised version in section 3.2 . Memory and computation compression calculation of the routing map : \u2022 The memory cost of the routing map is 3 x R_ { cin } + R_ { cout } . The routing map is used to record the index mapping between the sub-tensor in the epitome and the weight tensor . This is implemented as a look-up table manner . Because we take a patch of the tensor in the epitome , only the starting indices are recorded . The position of the rest of the elements can be calculated as starting index + offset where offset is the relative position for those elements in the sub-tensor . Explanation of R_ { cin } and R_ { cout } : \u2022 For example , with a learned routing map M , the sub-tensor in the epitome that will be used to fill up the sub-tensor at position ( i , j , m ) in the weight tensor is E [ p : p+w , q : q+h , c \u2019 _ { in } : c \u2019 _ { in } + \\beta_1 , : ] , where ( p , q , c \u2019 _ { in } ) = M ( i , j , m ) . The selected sub-tensors in the epitome are concatenated together to form a larger tensor . Hence , the number of such indices is calculated as C \u2019 _ { in } / \\beta_1 and this is noted as R_ { cin } . Since each recorded index has three values , the size of the routing map is 3 x R_ { cin } . Similarly , along the filter dimension , the size is R_ { cout } where R_ { cout } = C ' _ { out } / \\beta_2 . Here , \\beta_1 and \\beta_2 denotes the length of the sub-tensor along the input channel dimension and the filter dimension that we select from the epitome . We have put in more details regarding the meaning of R_ { cin } and R_ { cout } in section 3.2 . Since the routing map is implemented as a look-up table , the computation is O ( 1 ) . The formulation for the computation of the compression ratio has taken the routing map into consideration . Reply to minor comments : \u2022 Meaning of \u201c hardwired \u201d : this term is supposed to describe the relationship between the neural network architecture and the corresponding parameters . In this work , we consider the network architecture and the parameters as separate components . The word \u201c hardwired \u201d is used to describe the one-to-one correspondence between the architecture and the parameters . We have updated the abstract to make this clearer . \u2022 We have modified the experiments summary part in the abstract as pointed out ."}, "1": {"review_id": "HyxjOyrKvr-1", "review_text": "In this paper, the authors learn epitomes, which are small weight tensors which can be used with a learnt transform to produce tensors of an appropriate size (e.g. the sizes used in MobileNet v2). This gives a reduction in the number of parameters required, and the number of MAdds in theory. This paper is badly written, and could do with a rewrite: - Citations are used incorrectly (\\cite should be used when the citation is meant to be read as part of the sentence). - \"less elements\" --> \"fewer elements\" - \"misuse the notion\" --> \"abuse the notation\"? - \"for fair comparisons\" --> \"for a fair comparison\" The method is poorly explained; I have read Section 3.2 several times, and I'm still not entirely certain of what's going on. Figure 1 is helpful, but Figure 2 is not, and could be redesigned. 2(b) makes it look like you are going from a 3x3 epitome to a 2x2 kernel, which is clearly not what is happening. I think it would be helpful to give a detailed pictoral example of an epitome mapping to a weight tensor, with arrows between relevant indices changing. On initial reading, I thought the method allowed dynamic allocation of your epitomes to lots of different tensor sizes. From what I can tell, the network has to be trained from scratch for each possible size, so it isn't flexible in that respect. From what I can gather, the paper is presenting an alternate approach to *downscale* networks, as opposed to say, reducing width or depth. The comparisons to different widths of MobileNet v2 make more sense under this scenario. The main sell of the methods appears to be on the basis of MAdd reduction. This makes me nervous, as it doesn't necessarily correspond to actual speed-up or a reduction in energy (see https://arxiv.org/abs/1801.04326). EfficientNet was mainly about Madds too, but they provided some inference times. Would it be possible to add these? The method used with the integral image sounds expensive. The results look good, but error bars,on the CIFAR experiments at the very least, would be appreciated. Pros: ------- - Good results - Method appears largely novel (although bears some resemblance to https://arxiv.org/abs/1906.04309) Cons: -------- - Badly written - The method is poorly explained - Uncertainty re: MAdds as a primary comparator I propose a weak reject for this paper for two primary reasons: 1) The standard of writing, and the explanation of the all-important method are not up to scratch for a top tier conference 2) I have concerns regarding the MAdd calculations. Perhaps you could provide some pseudo-code in the author response? I am happy to upgrade my score if the authors deal with these issues sufficiently.", "rating": "3: Weak Reject", "reply_text": "The pseudo code for the madd calculation is shown as below : Pseudo code for Madd calculation : def conv_flops_counter_hook ( conv_module , input , output ) : # calculation based on equation 21 in the paper batch_size = input.shape [ 0 ] output_dims = list ( output.shape [ 2 : ] ) kernel_dims = list ( conv_module.kernel_size ) in_channels = conv_module.in_channels out_channels = conv_module.out_channels epi_in = self.weight.shape [ 1 ] epi_out = self.weight.shape [ 0 ] epi_w , epi_h = self.weight.shape [ 2 ] , self.weight.shape [ 3 ] # we choose \\beta_1 = epi_in and \\beta_2 = epi_out empirically rep_dim1 = out_channels // epi_in rep_dim0 = in_channels // epi_out filters_per_channel = out_channels // groups product_map_cost = ( 2 * in_channels * epi_w * epi_h -1 ) * np.prod ( output_dims ) * epi_out integral_map_cost = np.prod ( output_dims ) * epi_w * epi_h * epi_out channel_trans_cost = 2 * rep_dim1 * np.prod ( output_dims ) filter_trans_cost = 2 * rep_dim0 conv_per_position_flops = product_map_cost + integral_map_cost + channel_trans_cost + filter_trans_cost"}, "2": {"review_id": "HyxjOyrKvr-2", "review_text": "This paper focuses on the problem of neural network compression, and proposes a new scheme, the neural epitome search. It learns to find compact yet expressive epitomes for weight parameters of a specified network architecture. The learned weight tensors are independent of the architecture design. It can be encapsulated as a drop in replacement to the current convolutional operator. It can incur less performance drop. Experiments are conducted to show the effectiveness of the proposed method. However, there are some concerns to be addressed. -It is not too clear how to learn the epitomes and transformation functions. -Authors stated that the proposed method is independent of the architecture design. From the current statements, it is not explained clearly.", "rating": "6: Weak Accept", "reply_text": "Thank you for your review ! Your questions are valuable , and we are happy to address your concerns . In summary , we explain below how the epitome parameters are updated and why the method is agnostic to the model architecture . Q1 : It is not too clear how to learn the epitome and transformation functions . \u2022 Epitome parameter updates : the epitome is defined as a parameter tensor with smaller dimensions . The parameters inside the epitome are updated with standard back-propagation to minimize the training loss , after being transformed to instantiate model parameters . The gradient of each parameter in the epitome is the summation of the gradients of all the elements in the convolution weight tensor where the corresponding epitome element is used . The transformation function is end-to-end differentiable and hence the epitome parameters are updated in an end-to-end manner . The learning procedure for epitome is introduced in detail in Section 3.3 . We have revised the section to make it clearer . \u2022 Transformation function : We updated the description in Section 3.2 on transformation functions in the paper . In summary , the transformation function includes three parts : ( 1 ) an index learner , ( 2 ) a routing map and ( 3 ) an interpolation based sampler . The transformation function works as follows : It uses the index learner to learn a group of indices that map the sub-tensor inside the epitome to the convolution weight tensor . The learned indices and the epitome are then fed into an interpolation based sampler . Each pair of the learned starting index will have a corresponding output from the interpolation based sampler . Those outputs are then concatenated together to constitute the weight tensor . The index learner is implemented by a two-layer module . We use separate index learners for each layer and the learner takes the input feature of the corresponding layer as its input and outputs the learned starting indices . Thus , the learner can be optimized via standard back propagation . Q2 : Independence between the epitome and the architecture \u2022 In section 3 , we show that the model size and the computation are mainly related to the size of the epitome . With our proposed transformation function , the shape of the epitome is independent of the model architecture . This is because the transformation function is able to transform any shape of the epitome to a tensor matching the shape of weight tensor for the architecture ."}}