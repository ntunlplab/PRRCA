{"year": "2021", "forum": "-757TnNDwIn", "title": "Generative Adversarial Neural Architecture Search with Importance Sampling", "decision": "Reject", "meta_review": "This paper proposes a method for neural architecture search (NAS) based on adversarial methods. It uses a discriminator trained to distinguish between random vs. good architectures, letting the discriminator's scores serve as a reward signal for an autoregressive generator. I agree with AR1: this is a nice and clever idea. Reviewers generally agreed that the method was interesting, e.g. it's quite flexible in that it's able to incorporate constraints, and that the evaluation is rather extensive and shows that the method performs well across the board. Many minor criticisms were raised and addressed well by the authors in their responses and manuscript updates.\n\nThe major criticism shared by most reviewers was the high methodological complexity of the proposed approach, and the proportionally small gains shown over much simpler baselines. This criticism remained despite the authors' responses. The method is indeed complex: the same method without any adversarial component already performs well, and many important details of the model are relegated to Appendix A.2. (I would recommend, for example, moving Fig. 2 to the main text if at all possible. Also, the Appendix can/should be included in the main PDF for ICLR, rather than in supplementary material, as AR1 mentions.) It was not clear to reviewers that the adversarial component of the approach has a significant benefit. The authors respond by pointing to Table 7 showing that the discriminator reduces the number of queries and points out that in reality these queries correspond to expensive evaluations. If this is a major selling point of the method (it sounds like it could be), it should be highlighted and analyzed far more -- at least moved to the main text rather than an Appendix -- ideally with a real-world evaluation showing a practical large improvement in overall wall-clock time, rather than a benchmark where these evaluations are free. Perhaps the exclusive reliance on these benchmarks, though undoubtedly useful for quick experimentation, in the end holds back the paper and prevents the method's benefits from becoming apparent to the readers.\n\nAs a minor point (also raised by AR1), the paper is formatted incorrectly for ICLR: the font color is off, and more importantly the PDF is unsearchable (text cannot be selected, ctrl-F does not work), which makes it very difficult to quickly reference and review. Please try not to stray from the conference-provided style file for future submissions.\n\nI appreciate the cleverness of the method, the extent of the evaluation, and the thorough responses to the reviews. However, unfortunately with the current presentation, it is too difficult to discern the benefit of the proposed approach from the manuscript. The approach is nonetheless intuitively appealing and seems quite promising, and I hope the authors will take the reviewers' good feedback into account and resubmit the paper in the future.", "reviews": [{"review_id": "-757TnNDwIn-0", "review_text": "This paper proposes a Neural Architecture Search algorithm ( GA-NAS ) based on adversarial learning . The generator constructs architectures auto-regressively , which receives feedback from a GNN discriminator . Reinforcement learning ( PPO ) is used for training , to solve non-differentiability . GA-NAS \u2019 s effectiveness is demonstrated on several architecture search benchmarks for CIFAR-10 and 100 , and is shown to improve EfficientNet for ImageNet . Disclosure : I \u2019 m only vaguely familiar with the neural architecture search literature . Pros : 1.GA-NAS appears to consistently find high ranking architectures compared to the baselines , often at the cost of fewer queries . 2.A fairly extensive set of experiments are included in the Experiments section and Appendix . 3.GA-NAS is able to incorporate constraints into search . The authors demonstrate that GA-NAS is able to find a variation that slightly outperforms EfficientNet-B0 . Cons : 1.As the name implies , the goal of NAS is search . GANs have proven excellent over the years at interpolation , but not extrapolation , which is what search/exploration requires . I \u2019 m concerned that a GAN-based approach is therefore limited in its search ability . How would GA-NAS compare with a random search policy constrained to be close to known \u201c good \u201d architectures ? 2.The GA-NAS generator and discriminator are initialized with an initial set of good architectures X_0 . In the experiments , X_0 takes on the value of 50 and 100 . The assumption that such a large number of \u201c good \u201d architectures are available ahead of time seems rather strong . 3.The authors state that previous work has determined that other architecture search methods are not any better than random search . The gains in accuracies of the architectures produced by GA-NAS over the baseline seem moderate at best . Questions : 1 . Where in the f-GAN paper is it stated that the JS divergence is more robust than the assymetric KL ? Can you demonstrate this claimed advantage in GA-NAS by comparing objectives ? 2.Is X_0 counted in the number of queries reported ? How were these initial architectures chosen ? What rank are they ? Miscellaneous : 1 . This paper doesn \u2019 t adhere to the ICLR citation guidelines ; citations in the text should include the first author \u2019 s last name and year . Additionally , the entire author list should appear in the references , not just \u201c [ First author ] et al. \u201d Please correct this during the rebuttal phase . 2.Table 1 : Typically , values in a table are bolded to represent the best values . BANANAS has the same accuracy value as GA-NAS . 3.Why is the variance of GA-NAS \u2019 s accuracy so small in Table 1 ? 4.Table 2 caption could be more descriptive . Besides the columns , how is this table differ from Table 1 ? 5.There \u2019 s an extra space between \u201c ImageNet \u201d and footnote 2 . Rating : While the results seem good , I \u2019 m not entirely convinced that a generative adversarial approach can effectively explore a neural architecture space , as the discriminator will inherently disincentivize deviating from previously seen architectures . I \u2019 m also concerned about the validity of the assumption that an initial set of known \u201c good \u201d architectures would be available to a search algorithm ; the authors should clarify how these were selected . I lean toward rejection for now , but would be willing to raise my score if the authors could address my concerns . == Post-rebuttal == I thank the authors for answering my questions . While I am satisfied with the authors response to my concern about the `` initial good architectures '' assumption , I still remain unconvinced that adversarial learning can help search find new good architectures . I keep my score . I also encourage the authors to carefully re-read the f-GAN paper , which explains exactly how any f-divergence ( including the KL ) can be implemented for adversarial learning . Switching to any f-divergence requires only a simple change to the loss function . It also appears that the authors significantly misunderstand VAEs . The difference between GANs and VAEs is not JS-divergence vs KL-divergence . Using a KL loss for adversarial learning does not require switching to a VAE . Given the central role they play in this paper 's motivation , a better understanding of these subjects is important .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for mentioning the citation to f-gan paper . Regarding the JS-Divergence versus KL-Divergence , we need to cite two other papers . In fact , the differences between GANs ( relying on JS-divergence ) and Variational Auto Encoders ( VAEs , which rely on KL divergence ) , in terms of comparing objectives , are comprehensively discussed on page 14 of \u201c NIPS 2016 Tutorial : Generative Adversarial Networks \u201d by Ian Goodfellow as well as page 2 of \u201c TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS \u201d by Martin Arjovsky and L\u00b4eon Bottou . According to the discussion in the second article , asymmetric KL-divergence fails to work properly in two extreme cases . In the case $ P_ { real } ( x ) > P_ { gen } ( x ) $ , $ P_ { real } ( x ) > 0 $ , and $ P_ { gen } ( x ) $ goes to zero , the generator \u201c does not cover parts of data \u201d , and in the case $ P_ { gen } ( x ) > P_ { real } ( x ) $ , $ P_ { gen } ( x ) > 0 $ , and $ P_ { real } ( x ) $ goes to zero , the generator generates \u201c fake looking samples \u201d . Therefore , it is a general belief in the literature that the symmetry of JS-divergence with respect to $ P_ { real } ( x ) $ and $ P_ { gen } ( x ) $ causes GANs to generate samples of better quality than VAEs ( based on KL-divergence ) . This is one of the first few statements in GAN literature explaining the benefit of GAN , i.e. , the adoption of JS-divergence , as compared to VAEs . However , we are afraid that we can not easily switch to KL divergence objective for comparison . In fact , the GAN framework is minimizing the JS divergence . Switching to KL Divergence objective means that we need to replace the whole Generator-Discriminator GAN framework by VAE , which implements another completely different AutoEncoder framework ."}, {"review_id": "-757TnNDwIn-1", "review_text": "A flexible but complex and expensive NAS method . Summary : The authors introduce a method for NAS that repeatedly trains a generator to sample candidate architectures . The method is evaluated on three NAS oracle benchmarks as well as constrained NAS settings . While there are some promising experimental results , I lean slightly against acceptance due to poor presentation , limited comparisons on most evaluations , and what seems like fairly limited benefits of the approach given its complexity and cost . Strengths : 1 . The method can easily incorporate constraints on computation and memory . 2.The method outperforms existing non-weight-sharing methods on several benchmarks . Weaknesses : 1 . The method introduces a lot of complexity such as a graph NN , a recurrent NN , and a full round of generative adversarial training in each search iteration . The computational cost of the latter is not discussed . 2.As with most non-weight-sharing methods , GA-NAS requires several hundred queries on each benchmark , which translates to GPU-weeks of search time . It is not clear that the benefits over weight-sharing methods , which are not quantified for most cases , outweigh this large search cost . 3.The results section for unconstrained search is confusing and it is hard to make comparisons ( see notes 4-7 below ) . 4.From a look at the NAS-Bench-301 paper , it seems that BANANAS was the best non-weight-sharing method evaluated , but the authors compare only to EA and RS . 5.In the constrained search section , there are no comparisons to any other NAS methods . For example , random search is just as easy to apply to constrained problems as GA-NAS and should be used as a baseline . 6.There is no code in the supplementary materials . Will code be released ? Notes : 1. \u201c Remain hard to be assessed \u201d - > \u201c Remain hard to assess \u201d 2 . The citation style does not follow ICLR guidelines . 3. \u201c architectures are sampled , which are discretized graphs \u201d - > \u201c architectures , which are discretized graphs , are sampled \u201d 4 . What does it mean to \u201c discover the Nth best architecture in Q queries \u201d ? Is \u201c best \u201d here according to test or validation ? Why is Q a good metric for speed given that the algorithm doesn \u2019 t know to stop after query Q since in practice it won \u2019 t know the rank N of the current architecture ? 5.Many numbers in paragraph 4 for Section 4.1 do not correspond to any number in any table . 6.Why isn \u2019 t the BANANAS performance bolded in Table 1 ? 7.Table 4 should include at least one weight-sharing method such as GDAS ( Dong & Yang , 2019 ) , which is much faster and performs reasonably well . # Post-response update Thank you to the authors for answering some of my questions and clarifying the search and evaluation of GA-NAS . I believe my original assessment that the contributed method was complex remains accurate ; while the authors note that other methods like ENAS also use an RNN controller , in my view those methods are also complex . This paper increases this complexity with a GNN and an adversarial training setup . Use of such additions require showing significant improvements over baselines like random search , which I do not believe is achieved . I thus stand by my initial rating .", "rating": "5: Marginally below acceptance threshold", "reply_text": "1.Regarding questions on complexity and computational cost , we would like to add that the search cost of GA-NAS is about 8 GPU hours on NAS-Bench-101 if we do not use weight-sharing , which is shorter compared to the time that would be spent on evaluations if architectures are not pre-labeled . With weight-sharing GA-NAS takes about 1 GPU day for search and 1 GPU day for supernet training on NAS-Bench-101 search space , making a total cost of 2 GPU days . We \u2019 ll add this information to the paper . In terms of network complexity , our GraphRNN generator has about the same complexity as some of the existing NAS works that use a popular RNN controller ( ENAS , NAO ) . The only additional component here is an additional GNN encoder , which does not increase the training or inference time significantly and is able to better capture the useful features of a candidate architecture . 2.Regarding your concerns on comparing to BANANAS on NAS-Bench-301 , when we conducted the NAS-Bench-301 experiment , we could only find a figure in the NAS-Bench-301 paper that reports the performance of BANANAS , since that figure has a y-axis with varying scales it is difficult to compare quantitatively . And we are not able to locate the official code that tests BANANAS on NAS-Bench-301 , so it is not included as a baseline . However , we are able to locate the official BANANAS code for NAS-Bench-101 and hence we reported its performance there and believed that it would be enough . 3.Regarding your concerns on adding more baselines for constrained search , we agree with your point and are working on adding a RS baseline . 4.Regarding the code publication , it needs to be approved internally and we are looking into that . The code will be made public after all the paperwork is done . 5.Regarding your comments on adding GDAS in Table 4 , we would like to clarify that in Table 4 , we only test and compare with results obtained using non-weight-sharing as reported in NAS-Bench-201 paper , i.e. , querying the benchmark for accuracy . We didn \u2019 t train a supernet for NAS-Bench-201 and didn \u2019 t include other methods reported in NB201 that relied on weight sharing . Since GDAS results in NAS-Bench-201 paper were obtained using weight-sharing methods , adding GDAS to Table 4 wo n't be a fair comparison to the listed methods . For weight sharing results , in addition to Table 3 on NAS-Bench101 and EfficientNet , we have an unconstrained-search experiment running GA-NAS on ProxylessNas search space with weight sharing , which achieves 75.52 % final evaluation accuracy on full ImageNet , outperforming the original ProxylessNAS ( 75.1 % ) . We did not add that result to the paper due to page limit , but we could add it to the manuscript , as more results on weight sharing . 6.Last but not least , we would like to thank you again for spotting the typos , formatting and style issues , these will be addressed ."}, {"review_id": "-757TnNDwIn-2", "review_text": "Thanks for your informative response addressing my comments . After the revision , the description of the method is clearer ( Sec 3.2 ) , and the experimental results are clearer ( Sec 4 ) . I 'll stay with my original accept-score . Summary : The paper provides interesting results for neural architecture search . In particular , this paper proposes a search strategy for NAS problems , Generative Adversarial NAS ( GA-NAS ) , using importance sampling , which can be applied to micro/macro , constrained/unconstrained search problems . GA-NAS beats the state-of-the-art search algorithms proposed for NAS on public benchmarks , including NAS-Bench-101 , NAS-Bench-201 , and NAS-Bench-301 . Also , on the EfficientNet macro search space , GA-NAS finds a new architecture with higher ImageNet accuracy and a lower number of parameters than EfficientNet-B0 . Pros : 1.The proposed method achieves higher performance to compare to previous methods with better robustness , reproducibility , and efficiency . 2.The idea of NAS based on importance sampling for rare event simulation in the method seems interesting . The proposed method at the same time could be broadly applied to micro/macro , constrained/unconstrained search problems . 3.This paper provides comprehensive experiments , including various ablation studies , to show the effectiveness of the proposed framework . Cons : 1.I suggest the authors conduct further ablation studies to enhance the understanding of the approach and readability of the paper : ( 1 ) Comparison of computational resources ( e.g.wall clock inference time ) required for each query in Table 1 and Table 3 . To be a fair comparison , it would be better to compare [ number of queries * resource consumed per query ] . ( 2 ) For the update algorithm of the generator , the proposed method uses JS-divergence minimization referring to [ 29 ] . Section 3.1 mentions that JS-divergence is more robust than KL-divergence , but I think a further analysis could strengthen the point . ( 3 ) Adding FLOPs or inference speed to Table 5 and Table 6 would be helpful in explaining the performance of the new architecture found by GA-NAS . 2.Section 3 , 4 need to be polished for better readability . For example , an explanation about the method and the concept of evaluation metric `` rank '' should be improved for the readers . Some typos : ( 1 ) In the section 3.2 \u201c \\tau=\\ { C_0 , C_1 , \\ldotsC_ { N-1 } \\ } \u201d - > \u201c \\tau=\\ { C_0 , C_1 , \\ldots , C_ { N-1 } \\ } \u201d ( 2 ) In the equation ( 10 ) of appendix , \u201c - ( 1-\\rho ) + P ( X\\leq \\zeta^ * ) \\geq 0. \u201d - > \u201c - ( 1-\\rho ) + P ( X\\leq \\zeta^ * ) \\geq 0 , \u201d Some suggestions : `` \\cdots '' s are used in the expression such as `` X_1 , \\cdots , X_N '' and `` S ( X_1 ) , \\cdots , S ( X_N ) '' in the proof of the theorem 6.2 in the appendix . I think `` \\ldots '' is more syntactically typical here .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the constructive feedback on improving the writing of the paper . 1.Computational cost : Regarding your questions on computational cost , we would like to note that the time needed for querying an architecture on NAS-Bench-101 or 201 does not vary between different search algorithms and is cheap . If we directly query the benchmark for performance then the computational cost is just the search time needed by the algorithm . The total search time of GA-NAS is about 8 GPU hours on NAS-Bench-101 , including all the training of Discriminator and PPO , and querying the datasets . Other baselines do not report their wall-clock time on NAS-Bench-101 . So it is difficult to compare this in a table . We believe # of queries is a fair metric for the resource consumption because in a real-world setting , if we do not use a weight-sharing supernet , then full-train evaluation is dominating . The number of queries made to the benchmark can directly represent the number of evaluations needed , thus approximating the total cost . Even if weight-sharing is used , it will still take some major time to train the supernet and evaluate each architecture using weights inherited from the supernet . In both cases , the total cost is mainly bottlenecked by evaluation cost . In the case of weight-sharing-based search in Table 3 , GA-NAS takes about 1 GPU day for search and 1GPU day for supernet training on NAS-Bench-101 , making a total cost of 2 GPU days . We \u2019 ll be adding this information to the paper . 2.Regarding your suggestion of showing the FLOPS of the found architectures in Table 5,6 : We are looking into whether that is possible now . But we would like to clarify that for ResNet-constrained search , we used both the number of trainable weights and the training time as constraints , and for EfficientNet-constrained search we used the number of trainable weights as the constraint . Since our constrained search experiments did not incorporate FLOPS or inference latency as a constraint during search , it is unpredictable how they would compare to the original example model , i.e. , the constraints we used may not correlate positively with FLOPS . The idea of Table 5 , 6 is to show that GA-NAS is able to incorporate any ( hard ) constraints into search ( including FLOPS/latency although not demonstrated here ) , and beat the original model . So if one is interested in finding better architectures with lower FLOPS , he/she can easily include this as a constraint in the particular use of GA-NAS . 3.For JS Divergence vs KL Divergence : See the response to AnonReviewer3 titled \u201c Explaining advantage of the symmetric JS Divergence \u201d ."}, {"review_id": "-757TnNDwIn-3", "review_text": "In this paper , the authors introduce a NAS technique with an adversarial component . The discriminator learns to tell the difference between a set of good networks and randomly generated ones . This is quite a nice idea . A few comments . I don \u2019 t think Section 3.1 adds anything and would be better off in an appendix . The connection between Algorithm 1 and Algorithms 2 seems fairly tenuous to me ( although I could be wrong ) . The bibliography is unacceptable . All papers with more than two authors are written as \u201c et al. \u201d , and there are glaring inconsistencies . arXiv is mentioned in multiple different ways and different fonts are used for different entries . On a formatting issue , the text in the paper doesn \u2019 t look right compared to other ICLR submissions ( it \u2019 s too pale ) . It would be worth looking in to this . The paper is otherwise fairly well written . Referring to EfficientNet as \u201c Google \u2019 s EfficientNet \u201d is quite odd . ResNet is not written as \u201c Microsoft \u2019 s ResNet \u201d . I would recommend crediting the authors and not the institution . Table 2 appears above Table 1 . The details regarding training and the generator architecture are relegated to the appendix . These are very elaborate , which makes it very difficult to tell what is exactly contributing to the algorithm working . Table 7 seems to indicate that the discriminator itself can be removed for quite a small change in mean accuracy ( 94.2ish to 94.1ish ) . Without the discriminator the algorithm appears to be REINFORCE but with a more complicated generator network . On a related note , the comparison to REINFORCE is missing in table 1 which makes me suspect that this algorithm is basically the same thing in outcome , if not in effect . Table 9 in the appendix ( minor note \u2014 it \u2019 s not really an appendix when it \u2019 s in a separate file ! ) seems to show that different means of varying the size of the pool of networks over training has very little effect . ( Having 94.227 in bold above 94.22 doesn \u2019 t change the fact that we are talking about 0.007 % ! ) The evaluation in Table 1 is very odd , as the authors are reporting the best acc instead of mean+-std as is common practice . This is unreliable , as when deploying these algorithms in the wild we are far more interested in how they do in expectation ( particularly if they fail completely some of the time ) . Although the idea of using an adversarial framework to tell apart architectures in a search space is nice , the implementation has many moving parts , and doesn \u2019 t appear noticeably different to the standard REINFORCE NAS approach ( which just has a generator as an RNN ) . The presence of a discriminator has a very minimal effect , which is a shame . Some evaluation choices are very questionable . I am inclined towards rejection .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Algorithm 1 in Section 3.1 presents the algorithmic steps of a general importance sampling framework for optimizing a black-box function , which is key to the proposed GA-NAS . In Section 3.2 , we instantiate Algorithm 1 for our specific NAS problem with GAN and RL components . Step 3 of Algorithm 1 minimizes the JS-divergence between the two distributions in each iteration . It is well known that GAN solves a minimax problem in an iterative fashion and is equivalent to minimizing the JS-divergence between true data and generated data distributions . A particular implementation of the JS-minimizer ( step 3 ) of Algorithm 1 is based on adversarial training and is described in Algorithm 2 , where a discriminator and an RL-based generator are trained alternately in each iteration . Furthermore , the relationship between the $ \\rho_t $ parameter in Algorithm 1 and the top-k parameter in Algorithm 2 is explained in Section 3.2 ."}], "0": {"review_id": "-757TnNDwIn-0", "review_text": "This paper proposes a Neural Architecture Search algorithm ( GA-NAS ) based on adversarial learning . The generator constructs architectures auto-regressively , which receives feedback from a GNN discriminator . Reinforcement learning ( PPO ) is used for training , to solve non-differentiability . GA-NAS \u2019 s effectiveness is demonstrated on several architecture search benchmarks for CIFAR-10 and 100 , and is shown to improve EfficientNet for ImageNet . Disclosure : I \u2019 m only vaguely familiar with the neural architecture search literature . Pros : 1.GA-NAS appears to consistently find high ranking architectures compared to the baselines , often at the cost of fewer queries . 2.A fairly extensive set of experiments are included in the Experiments section and Appendix . 3.GA-NAS is able to incorporate constraints into search . The authors demonstrate that GA-NAS is able to find a variation that slightly outperforms EfficientNet-B0 . Cons : 1.As the name implies , the goal of NAS is search . GANs have proven excellent over the years at interpolation , but not extrapolation , which is what search/exploration requires . I \u2019 m concerned that a GAN-based approach is therefore limited in its search ability . How would GA-NAS compare with a random search policy constrained to be close to known \u201c good \u201d architectures ? 2.The GA-NAS generator and discriminator are initialized with an initial set of good architectures X_0 . In the experiments , X_0 takes on the value of 50 and 100 . The assumption that such a large number of \u201c good \u201d architectures are available ahead of time seems rather strong . 3.The authors state that previous work has determined that other architecture search methods are not any better than random search . The gains in accuracies of the architectures produced by GA-NAS over the baseline seem moderate at best . Questions : 1 . Where in the f-GAN paper is it stated that the JS divergence is more robust than the assymetric KL ? Can you demonstrate this claimed advantage in GA-NAS by comparing objectives ? 2.Is X_0 counted in the number of queries reported ? How were these initial architectures chosen ? What rank are they ? Miscellaneous : 1 . This paper doesn \u2019 t adhere to the ICLR citation guidelines ; citations in the text should include the first author \u2019 s last name and year . Additionally , the entire author list should appear in the references , not just \u201c [ First author ] et al. \u201d Please correct this during the rebuttal phase . 2.Table 1 : Typically , values in a table are bolded to represent the best values . BANANAS has the same accuracy value as GA-NAS . 3.Why is the variance of GA-NAS \u2019 s accuracy so small in Table 1 ? 4.Table 2 caption could be more descriptive . Besides the columns , how is this table differ from Table 1 ? 5.There \u2019 s an extra space between \u201c ImageNet \u201d and footnote 2 . Rating : While the results seem good , I \u2019 m not entirely convinced that a generative adversarial approach can effectively explore a neural architecture space , as the discriminator will inherently disincentivize deviating from previously seen architectures . I \u2019 m also concerned about the validity of the assumption that an initial set of known \u201c good \u201d architectures would be available to a search algorithm ; the authors should clarify how these were selected . I lean toward rejection for now , but would be willing to raise my score if the authors could address my concerns . == Post-rebuttal == I thank the authors for answering my questions . While I am satisfied with the authors response to my concern about the `` initial good architectures '' assumption , I still remain unconvinced that adversarial learning can help search find new good architectures . I keep my score . I also encourage the authors to carefully re-read the f-GAN paper , which explains exactly how any f-divergence ( including the KL ) can be implemented for adversarial learning . Switching to any f-divergence requires only a simple change to the loss function . It also appears that the authors significantly misunderstand VAEs . The difference between GANs and VAEs is not JS-divergence vs KL-divergence . Using a KL loss for adversarial learning does not require switching to a VAE . Given the central role they play in this paper 's motivation , a better understanding of these subjects is important .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for mentioning the citation to f-gan paper . Regarding the JS-Divergence versus KL-Divergence , we need to cite two other papers . In fact , the differences between GANs ( relying on JS-divergence ) and Variational Auto Encoders ( VAEs , which rely on KL divergence ) , in terms of comparing objectives , are comprehensively discussed on page 14 of \u201c NIPS 2016 Tutorial : Generative Adversarial Networks \u201d by Ian Goodfellow as well as page 2 of \u201c TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS \u201d by Martin Arjovsky and L\u00b4eon Bottou . According to the discussion in the second article , asymmetric KL-divergence fails to work properly in two extreme cases . In the case $ P_ { real } ( x ) > P_ { gen } ( x ) $ , $ P_ { real } ( x ) > 0 $ , and $ P_ { gen } ( x ) $ goes to zero , the generator \u201c does not cover parts of data \u201d , and in the case $ P_ { gen } ( x ) > P_ { real } ( x ) $ , $ P_ { gen } ( x ) > 0 $ , and $ P_ { real } ( x ) $ goes to zero , the generator generates \u201c fake looking samples \u201d . Therefore , it is a general belief in the literature that the symmetry of JS-divergence with respect to $ P_ { real } ( x ) $ and $ P_ { gen } ( x ) $ causes GANs to generate samples of better quality than VAEs ( based on KL-divergence ) . This is one of the first few statements in GAN literature explaining the benefit of GAN , i.e. , the adoption of JS-divergence , as compared to VAEs . However , we are afraid that we can not easily switch to KL divergence objective for comparison . In fact , the GAN framework is minimizing the JS divergence . Switching to KL Divergence objective means that we need to replace the whole Generator-Discriminator GAN framework by VAE , which implements another completely different AutoEncoder framework ."}, "1": {"review_id": "-757TnNDwIn-1", "review_text": "A flexible but complex and expensive NAS method . Summary : The authors introduce a method for NAS that repeatedly trains a generator to sample candidate architectures . The method is evaluated on three NAS oracle benchmarks as well as constrained NAS settings . While there are some promising experimental results , I lean slightly against acceptance due to poor presentation , limited comparisons on most evaluations , and what seems like fairly limited benefits of the approach given its complexity and cost . Strengths : 1 . The method can easily incorporate constraints on computation and memory . 2.The method outperforms existing non-weight-sharing methods on several benchmarks . Weaknesses : 1 . The method introduces a lot of complexity such as a graph NN , a recurrent NN , and a full round of generative adversarial training in each search iteration . The computational cost of the latter is not discussed . 2.As with most non-weight-sharing methods , GA-NAS requires several hundred queries on each benchmark , which translates to GPU-weeks of search time . It is not clear that the benefits over weight-sharing methods , which are not quantified for most cases , outweigh this large search cost . 3.The results section for unconstrained search is confusing and it is hard to make comparisons ( see notes 4-7 below ) . 4.From a look at the NAS-Bench-301 paper , it seems that BANANAS was the best non-weight-sharing method evaluated , but the authors compare only to EA and RS . 5.In the constrained search section , there are no comparisons to any other NAS methods . For example , random search is just as easy to apply to constrained problems as GA-NAS and should be used as a baseline . 6.There is no code in the supplementary materials . Will code be released ? Notes : 1. \u201c Remain hard to be assessed \u201d - > \u201c Remain hard to assess \u201d 2 . The citation style does not follow ICLR guidelines . 3. \u201c architectures are sampled , which are discretized graphs \u201d - > \u201c architectures , which are discretized graphs , are sampled \u201d 4 . What does it mean to \u201c discover the Nth best architecture in Q queries \u201d ? Is \u201c best \u201d here according to test or validation ? Why is Q a good metric for speed given that the algorithm doesn \u2019 t know to stop after query Q since in practice it won \u2019 t know the rank N of the current architecture ? 5.Many numbers in paragraph 4 for Section 4.1 do not correspond to any number in any table . 6.Why isn \u2019 t the BANANAS performance bolded in Table 1 ? 7.Table 4 should include at least one weight-sharing method such as GDAS ( Dong & Yang , 2019 ) , which is much faster and performs reasonably well . # Post-response update Thank you to the authors for answering some of my questions and clarifying the search and evaluation of GA-NAS . I believe my original assessment that the contributed method was complex remains accurate ; while the authors note that other methods like ENAS also use an RNN controller , in my view those methods are also complex . This paper increases this complexity with a GNN and an adversarial training setup . Use of such additions require showing significant improvements over baselines like random search , which I do not believe is achieved . I thus stand by my initial rating .", "rating": "5: Marginally below acceptance threshold", "reply_text": "1.Regarding questions on complexity and computational cost , we would like to add that the search cost of GA-NAS is about 8 GPU hours on NAS-Bench-101 if we do not use weight-sharing , which is shorter compared to the time that would be spent on evaluations if architectures are not pre-labeled . With weight-sharing GA-NAS takes about 1 GPU day for search and 1 GPU day for supernet training on NAS-Bench-101 search space , making a total cost of 2 GPU days . We \u2019 ll add this information to the paper . In terms of network complexity , our GraphRNN generator has about the same complexity as some of the existing NAS works that use a popular RNN controller ( ENAS , NAO ) . The only additional component here is an additional GNN encoder , which does not increase the training or inference time significantly and is able to better capture the useful features of a candidate architecture . 2.Regarding your concerns on comparing to BANANAS on NAS-Bench-301 , when we conducted the NAS-Bench-301 experiment , we could only find a figure in the NAS-Bench-301 paper that reports the performance of BANANAS , since that figure has a y-axis with varying scales it is difficult to compare quantitatively . And we are not able to locate the official code that tests BANANAS on NAS-Bench-301 , so it is not included as a baseline . However , we are able to locate the official BANANAS code for NAS-Bench-101 and hence we reported its performance there and believed that it would be enough . 3.Regarding your concerns on adding more baselines for constrained search , we agree with your point and are working on adding a RS baseline . 4.Regarding the code publication , it needs to be approved internally and we are looking into that . The code will be made public after all the paperwork is done . 5.Regarding your comments on adding GDAS in Table 4 , we would like to clarify that in Table 4 , we only test and compare with results obtained using non-weight-sharing as reported in NAS-Bench-201 paper , i.e. , querying the benchmark for accuracy . We didn \u2019 t train a supernet for NAS-Bench-201 and didn \u2019 t include other methods reported in NB201 that relied on weight sharing . Since GDAS results in NAS-Bench-201 paper were obtained using weight-sharing methods , adding GDAS to Table 4 wo n't be a fair comparison to the listed methods . For weight sharing results , in addition to Table 3 on NAS-Bench101 and EfficientNet , we have an unconstrained-search experiment running GA-NAS on ProxylessNas search space with weight sharing , which achieves 75.52 % final evaluation accuracy on full ImageNet , outperforming the original ProxylessNAS ( 75.1 % ) . We did not add that result to the paper due to page limit , but we could add it to the manuscript , as more results on weight sharing . 6.Last but not least , we would like to thank you again for spotting the typos , formatting and style issues , these will be addressed ."}, "2": {"review_id": "-757TnNDwIn-2", "review_text": "Thanks for your informative response addressing my comments . After the revision , the description of the method is clearer ( Sec 3.2 ) , and the experimental results are clearer ( Sec 4 ) . I 'll stay with my original accept-score . Summary : The paper provides interesting results for neural architecture search . In particular , this paper proposes a search strategy for NAS problems , Generative Adversarial NAS ( GA-NAS ) , using importance sampling , which can be applied to micro/macro , constrained/unconstrained search problems . GA-NAS beats the state-of-the-art search algorithms proposed for NAS on public benchmarks , including NAS-Bench-101 , NAS-Bench-201 , and NAS-Bench-301 . Also , on the EfficientNet macro search space , GA-NAS finds a new architecture with higher ImageNet accuracy and a lower number of parameters than EfficientNet-B0 . Pros : 1.The proposed method achieves higher performance to compare to previous methods with better robustness , reproducibility , and efficiency . 2.The idea of NAS based on importance sampling for rare event simulation in the method seems interesting . The proposed method at the same time could be broadly applied to micro/macro , constrained/unconstrained search problems . 3.This paper provides comprehensive experiments , including various ablation studies , to show the effectiveness of the proposed framework . Cons : 1.I suggest the authors conduct further ablation studies to enhance the understanding of the approach and readability of the paper : ( 1 ) Comparison of computational resources ( e.g.wall clock inference time ) required for each query in Table 1 and Table 3 . To be a fair comparison , it would be better to compare [ number of queries * resource consumed per query ] . ( 2 ) For the update algorithm of the generator , the proposed method uses JS-divergence minimization referring to [ 29 ] . Section 3.1 mentions that JS-divergence is more robust than KL-divergence , but I think a further analysis could strengthen the point . ( 3 ) Adding FLOPs or inference speed to Table 5 and Table 6 would be helpful in explaining the performance of the new architecture found by GA-NAS . 2.Section 3 , 4 need to be polished for better readability . For example , an explanation about the method and the concept of evaluation metric `` rank '' should be improved for the readers . Some typos : ( 1 ) In the section 3.2 \u201c \\tau=\\ { C_0 , C_1 , \\ldotsC_ { N-1 } \\ } \u201d - > \u201c \\tau=\\ { C_0 , C_1 , \\ldots , C_ { N-1 } \\ } \u201d ( 2 ) In the equation ( 10 ) of appendix , \u201c - ( 1-\\rho ) + P ( X\\leq \\zeta^ * ) \\geq 0. \u201d - > \u201c - ( 1-\\rho ) + P ( X\\leq \\zeta^ * ) \\geq 0 , \u201d Some suggestions : `` \\cdots '' s are used in the expression such as `` X_1 , \\cdots , X_N '' and `` S ( X_1 ) , \\cdots , S ( X_N ) '' in the proof of the theorem 6.2 in the appendix . I think `` \\ldots '' is more syntactically typical here .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the constructive feedback on improving the writing of the paper . 1.Computational cost : Regarding your questions on computational cost , we would like to note that the time needed for querying an architecture on NAS-Bench-101 or 201 does not vary between different search algorithms and is cheap . If we directly query the benchmark for performance then the computational cost is just the search time needed by the algorithm . The total search time of GA-NAS is about 8 GPU hours on NAS-Bench-101 , including all the training of Discriminator and PPO , and querying the datasets . Other baselines do not report their wall-clock time on NAS-Bench-101 . So it is difficult to compare this in a table . We believe # of queries is a fair metric for the resource consumption because in a real-world setting , if we do not use a weight-sharing supernet , then full-train evaluation is dominating . The number of queries made to the benchmark can directly represent the number of evaluations needed , thus approximating the total cost . Even if weight-sharing is used , it will still take some major time to train the supernet and evaluate each architecture using weights inherited from the supernet . In both cases , the total cost is mainly bottlenecked by evaluation cost . In the case of weight-sharing-based search in Table 3 , GA-NAS takes about 1 GPU day for search and 1GPU day for supernet training on NAS-Bench-101 , making a total cost of 2 GPU days . We \u2019 ll be adding this information to the paper . 2.Regarding your suggestion of showing the FLOPS of the found architectures in Table 5,6 : We are looking into whether that is possible now . But we would like to clarify that for ResNet-constrained search , we used both the number of trainable weights and the training time as constraints , and for EfficientNet-constrained search we used the number of trainable weights as the constraint . Since our constrained search experiments did not incorporate FLOPS or inference latency as a constraint during search , it is unpredictable how they would compare to the original example model , i.e. , the constraints we used may not correlate positively with FLOPS . The idea of Table 5 , 6 is to show that GA-NAS is able to incorporate any ( hard ) constraints into search ( including FLOPS/latency although not demonstrated here ) , and beat the original model . So if one is interested in finding better architectures with lower FLOPS , he/she can easily include this as a constraint in the particular use of GA-NAS . 3.For JS Divergence vs KL Divergence : See the response to AnonReviewer3 titled \u201c Explaining advantage of the symmetric JS Divergence \u201d ."}, "3": {"review_id": "-757TnNDwIn-3", "review_text": "In this paper , the authors introduce a NAS technique with an adversarial component . The discriminator learns to tell the difference between a set of good networks and randomly generated ones . This is quite a nice idea . A few comments . I don \u2019 t think Section 3.1 adds anything and would be better off in an appendix . The connection between Algorithm 1 and Algorithms 2 seems fairly tenuous to me ( although I could be wrong ) . The bibliography is unacceptable . All papers with more than two authors are written as \u201c et al. \u201d , and there are glaring inconsistencies . arXiv is mentioned in multiple different ways and different fonts are used for different entries . On a formatting issue , the text in the paper doesn \u2019 t look right compared to other ICLR submissions ( it \u2019 s too pale ) . It would be worth looking in to this . The paper is otherwise fairly well written . Referring to EfficientNet as \u201c Google \u2019 s EfficientNet \u201d is quite odd . ResNet is not written as \u201c Microsoft \u2019 s ResNet \u201d . I would recommend crediting the authors and not the institution . Table 2 appears above Table 1 . The details regarding training and the generator architecture are relegated to the appendix . These are very elaborate , which makes it very difficult to tell what is exactly contributing to the algorithm working . Table 7 seems to indicate that the discriminator itself can be removed for quite a small change in mean accuracy ( 94.2ish to 94.1ish ) . Without the discriminator the algorithm appears to be REINFORCE but with a more complicated generator network . On a related note , the comparison to REINFORCE is missing in table 1 which makes me suspect that this algorithm is basically the same thing in outcome , if not in effect . Table 9 in the appendix ( minor note \u2014 it \u2019 s not really an appendix when it \u2019 s in a separate file ! ) seems to show that different means of varying the size of the pool of networks over training has very little effect . ( Having 94.227 in bold above 94.22 doesn \u2019 t change the fact that we are talking about 0.007 % ! ) The evaluation in Table 1 is very odd , as the authors are reporting the best acc instead of mean+-std as is common practice . This is unreliable , as when deploying these algorithms in the wild we are far more interested in how they do in expectation ( particularly if they fail completely some of the time ) . Although the idea of using an adversarial framework to tell apart architectures in a search space is nice , the implementation has many moving parts , and doesn \u2019 t appear noticeably different to the standard REINFORCE NAS approach ( which just has a generator as an RNN ) . The presence of a discriminator has a very minimal effect , which is a shame . Some evaluation choices are very questionable . I am inclined towards rejection .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Algorithm 1 in Section 3.1 presents the algorithmic steps of a general importance sampling framework for optimizing a black-box function , which is key to the proposed GA-NAS . In Section 3.2 , we instantiate Algorithm 1 for our specific NAS problem with GAN and RL components . Step 3 of Algorithm 1 minimizes the JS-divergence between the two distributions in each iteration . It is well known that GAN solves a minimax problem in an iterative fashion and is equivalent to minimizing the JS-divergence between true data and generated data distributions . A particular implementation of the JS-minimizer ( step 3 ) of Algorithm 1 is based on adversarial training and is described in Algorithm 2 , where a discriminator and an RL-based generator are trained alternately in each iteration . Furthermore , the relationship between the $ \\rho_t $ parameter in Algorithm 1 and the top-k parameter in Algorithm 2 is explained in Section 3.2 ."}}