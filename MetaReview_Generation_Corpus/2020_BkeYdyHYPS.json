{"year": "2020", "forum": "BkeYdyHYPS", "title": "Evo-NAS: Evolutionary-Neural Hybrid Agent for Architecture Search", "decision": "Reject", "meta_review": "Thanks to the authors for the revision and discussion. This paper provides a neural architecture search (NAS) method, called Evolutionary-Neural hybrid agents (Evo-NAS), which combines NN-based NAS and Aging EVO. While the authors' response addressed some of the reviewers' comments, during discussion period there is a new concern that the idea proposed here highly overlaps with the method of RENAS, which stands for Reinforced Evolutionary Neural Architecture Search. Reviewers acknowledge that this might discount the novelty of the paper. Overall, there is not sufficient support for acceptance.", "reviews": [{"review_id": "BkeYdyHYPS-0", "review_text": "In this paper, the authors proposed to combine both NN-based NAS and Aging EVO to get the benefit of both world: good global and local sample efficiency. The main idea is to use Aging EVO algorithm to guide the overall search process, and a NN predicting the final performance is used to guide the mutation process. The combined EVO-NAS has showed consistent good performance over a range of tasks. Overall, while the novelty of the paper is not exceptional, since it is a rather straightforward combination of two existing approaches, the end-results is promising. I would like to see more in-depth analysis on the combined algorithm to validate authors' hypothesis on why EVO-NAS works better. More detailed comments can be found below. 1. The experiment on the synthetic task is not very helpful, since the domain can be far apart from the real NAS applications. One evidence is that the NN significantly outperform EVO in this task but not in the other tasks. 2. In difficult tasks, the proposed EVO-NAS and the original Aging EVO are very close in the first few hundreds of trials, however, later the gap remains the same. It would be interesting to see if we can eliminate the gap by adding the NN component in the middle of the Aging EVO experiment (e.g., at the point of 2000 trials). 3. I am also curious about the difference between the NN learned from Neural agent vs. those learned from EVO-NAS agent. Moreover, do we need a NN for the EVO-NAS? Or something simpler would be sufficient to guide the search.", "rating": "3: Weak Reject", "reply_text": "Thank you for your comments and suggestions ! Please let us know if you have any more feedback about our work , since we would be happy to use it to make the paper better . Below we specifically address each of the points that you made . 1.We agree that the synthetic task is far from real NAS applications . In our paper we do focus on NAS , and the other lines of experiments ( NAS-Bench , NLP datasets , ImageNet ) are all real-life architecture search scenarios . However , as we hint in the \u201c Conclusion \u201d section , it is possible to apply Evo-NAS to tasks outside of architecture search . Discrete optimization problems such as our toy task could be one example , and we see that Evo-NAS also performs very well in this domain . We think that makes the approach even more promising in terms of inspiring future work . 2.As we understand the proposal , you would consider running the NN learning component not from the beginning of the experiment , but start learning later on . This would save some computational cost ; are we missing any other advantages of this approach ? Our initial intuition is that the savings might be limited due to low overhead of running the NN compared to the training of candidate architectures . However , it would be interesting to experiment further in this direction in the future . 3.You made a good point that we could use something simpler than a Neural Network for the learning part of Evo-NAS . Since we wanted our approach to be comparable to existing methods , we used a NN , since it \u2019 s somewhat standard in the NAS community . Also , from a theoretical standpoint , a NN has a useful property of being a universal approximator . On the other hand , it does seem likely that guiding the evolutionary search ( i.e.producing a good prior over mutations ) might be an easier problem than learning the structure of all solutions ( i.e.producing a good prior over the entire search space ) . Therefore , it \u2019 s certainly possible that using something simpler as the learning part of Evo-NAS would be sufficient to get good results . However , again note that in terms of computational cost the NN part is negligible - so any gains would not be in terms of compute , but perhaps in enhanced explainability of the learning component ."}, {"review_id": "BkeYdyHYPS-1", "review_text": "It is a nice paper that combines the deep reinforcement learning and evolutionary learning techniques to neural architecture search problem. Experimental results are promising. However, I still have some concerns on the current submission. 1.In Fig 1,2 &3, it seems that the performances of Neural (PQT) keeps increasing. For better compassion, we recommend the authors reports the performances of compared algorithms until they are convergent. 2.The different training algorithms (Reinforce and PQT) have difference performances whether because different training algorithms converge to difference local minima or stationary points. ", "rating": "6: Weak Accept", "reply_text": "Thank you for you comments ! Below we specifically address both of the points that you raised . Let us know if you have any more concerns - we are happy to make improvements to the paper . 1.To make the assessment that the methods have converged , we looked at the best reward attained ( right plots ) , not reward running average ( left plots ) . By \u201c reward running average \u201d , we mean sliding a constant size window over the reward , and computing the average reward in each such window . This measure is not a good metric to compare the algorithms , and we use it just to show how much the reward varies from sample to sample for different methods . Note that learning-based methods ( like Neural PQT which you mentioned ) do see small improvements in running average reward even long after they stop exploring new parts of the search space - this is simply because they keep making their distributions over actions \u201c sharper \u201d . Therefore , it is more informative to look at best reward attained over time . With that in mind , we review below the experiments that you pointed to - that is , Figures 2 and 3 . You also mentioned Figure 1 , which does not correspond to any experiment , so we assumed this to be a typo - but let us know if you meant something else there , and we can discuss further . - For NAS-Bench ( Figure 3 ) , the methods see only marginal improvements at the end of the experiment . - For the synthetic task ( Figure 2 ) , all methods see small improvements of the reward if ran longer , but the overall pattern remains the same ( Evo-NAS ( PQT ) attaining the best results , matched by Neural ( PQT ) from around 4500 trials onward ) . 2.Our understanding is that PQT provides a stronger training signal than Reinforce . This comes at a cost , since Reinforce is known to be unbiased , and therefore it produces gradients that ( in expectation ) match the gradient of the expected reward . On the other hand , PQT is a heuristic , and it gives no theoretical guarantees . We wouldn \u2019 t say that PQT is always better than Reinforce - this is likely very domain dependent . Our focus is architecture search , where the number of trials is typically low given the complexity of the search spaces . In this domain , as seen in our experiments , trading off theoretical guarantees and lack of bias for a stronger and more greedy training signal leads to better results . Nevertheless , comparing PQT to classical RL algorithms ( like Reinforce , but also many others ) is a very interesting research direction . Since our main goal was hybridizing RL-based and Evolution-based approaches to architecture search , we did not explore this direction in our work ."}, {"review_id": "BkeYdyHYPS-2", "review_text": "This paper is well organized. The applied methods are introduced in detail. But it lacks some more detailed analysis. My concerns are as follows. 1. The performance differences between Evolutionary agent and EVO-NAS agent seem not significant. Please conduct additional statistical tests such as the Wilcoxon signed-rank test to verify the performance improvements are significant. 2. Many studies have been conducted to automatically adjust control parameters such as crossover and mutation probabilities in evolutionary algorithm literature. It would be better to compare one of these approaches in the experiments.", "rating": "3: Weak Reject", "reply_text": "Thank you for your review ! We address each of your concerns below . Please let us know if you have some further comments - we are happy to make changes to the paper to improve it . 1.While the gains are not enormous , we agree with Reviewer # 2 in saying that they are consistent across a range of different tasks : both in real-life AS scenarios ( NAS-Bench , NLP , ImageNet ) , and in the synthetic task . However , making sure that differences between results are statistically significant is certainly an important point . For example , note that for NLP ( Table 1 ) , we ran 10 runs for each ( approach , dataset ) pair . We claim that Evo-NAS matches the better of the baselines in all cases , and in 3 out of 7 it outperforms the best baseline with statistical significance . We made this claim for the three datasets where the difference exceeded twice the standard-error-of-the-mean . This can also be easily translated into p-values : for example , for the ConsumerComplaints dataset , the p-value for Evo-NAS \u2019 s gains over either baseline being statistically significant is < 0.02 ( so we reject the null hypothesis that there is no gain ) . We used simple statistical measures such as the ones described above to assess significance . However , we are open to performing additional nonparametric tests , such as the Wilcoxon Signed-Rank Test you proposed . Please let us know what would be the exact test that you had in mind ( i.e.what would be the null hypothesis ) . For example , we could consider pairs of ( average result of Evolution , average result of Evo-NAS ) over all benchmarks considered in the paper ( there are 10 in total since we should count each NLP dataset separately ) , and perform the test on these pairs . In that case , the Signed-Rank test does show significance , as in all cases the mean result of Evo-NAS is strictly better than the mean result of Evolution . 2.You are right in saying that there are many smart evolutionary algorithms out there . However , to our knowledge , the NAS domain is dominated by \u201c non-adaptive \u201d evolution ( i.e.with uniform prior over mutations ) , which also seems to perform very well across different AS domains . Since here we focus on architecture search , we compare our method to two strong , common and most recent baselines ( the combination of which gives Evo-NAS ) . Note that our hybrid may not necessarily be the best algorithm of this kind ; our intention was to show that combining these two worlds ( learning-based and evolution-based ) in a relatively simple way we can get consistent gains across multiple domains . One kind of follow-up works that we wish to inspire is exactly what you are proposing : bringing different \u201c smart \u201d genetic algorithms from the evolution community to the NAS community , and investigating their behavior on architecture search . It is surely an exciting direction , although one that is not the focus of our current work ."}], "0": {"review_id": "BkeYdyHYPS-0", "review_text": "In this paper, the authors proposed to combine both NN-based NAS and Aging EVO to get the benefit of both world: good global and local sample efficiency. The main idea is to use Aging EVO algorithm to guide the overall search process, and a NN predicting the final performance is used to guide the mutation process. The combined EVO-NAS has showed consistent good performance over a range of tasks. Overall, while the novelty of the paper is not exceptional, since it is a rather straightforward combination of two existing approaches, the end-results is promising. I would like to see more in-depth analysis on the combined algorithm to validate authors' hypothesis on why EVO-NAS works better. More detailed comments can be found below. 1. The experiment on the synthetic task is not very helpful, since the domain can be far apart from the real NAS applications. One evidence is that the NN significantly outperform EVO in this task but not in the other tasks. 2. In difficult tasks, the proposed EVO-NAS and the original Aging EVO are very close in the first few hundreds of trials, however, later the gap remains the same. It would be interesting to see if we can eliminate the gap by adding the NN component in the middle of the Aging EVO experiment (e.g., at the point of 2000 trials). 3. I am also curious about the difference between the NN learned from Neural agent vs. those learned from EVO-NAS agent. Moreover, do we need a NN for the EVO-NAS? Or something simpler would be sufficient to guide the search.", "rating": "3: Weak Reject", "reply_text": "Thank you for your comments and suggestions ! Please let us know if you have any more feedback about our work , since we would be happy to use it to make the paper better . Below we specifically address each of the points that you made . 1.We agree that the synthetic task is far from real NAS applications . In our paper we do focus on NAS , and the other lines of experiments ( NAS-Bench , NLP datasets , ImageNet ) are all real-life architecture search scenarios . However , as we hint in the \u201c Conclusion \u201d section , it is possible to apply Evo-NAS to tasks outside of architecture search . Discrete optimization problems such as our toy task could be one example , and we see that Evo-NAS also performs very well in this domain . We think that makes the approach even more promising in terms of inspiring future work . 2.As we understand the proposal , you would consider running the NN learning component not from the beginning of the experiment , but start learning later on . This would save some computational cost ; are we missing any other advantages of this approach ? Our initial intuition is that the savings might be limited due to low overhead of running the NN compared to the training of candidate architectures . However , it would be interesting to experiment further in this direction in the future . 3.You made a good point that we could use something simpler than a Neural Network for the learning part of Evo-NAS . Since we wanted our approach to be comparable to existing methods , we used a NN , since it \u2019 s somewhat standard in the NAS community . Also , from a theoretical standpoint , a NN has a useful property of being a universal approximator . On the other hand , it does seem likely that guiding the evolutionary search ( i.e.producing a good prior over mutations ) might be an easier problem than learning the structure of all solutions ( i.e.producing a good prior over the entire search space ) . Therefore , it \u2019 s certainly possible that using something simpler as the learning part of Evo-NAS would be sufficient to get good results . However , again note that in terms of computational cost the NN part is negligible - so any gains would not be in terms of compute , but perhaps in enhanced explainability of the learning component ."}, "1": {"review_id": "BkeYdyHYPS-1", "review_text": "It is a nice paper that combines the deep reinforcement learning and evolutionary learning techniques to neural architecture search problem. Experimental results are promising. However, I still have some concerns on the current submission. 1.In Fig 1,2 &3, it seems that the performances of Neural (PQT) keeps increasing. For better compassion, we recommend the authors reports the performances of compared algorithms until they are convergent. 2.The different training algorithms (Reinforce and PQT) have difference performances whether because different training algorithms converge to difference local minima or stationary points. ", "rating": "6: Weak Accept", "reply_text": "Thank you for you comments ! Below we specifically address both of the points that you raised . Let us know if you have any more concerns - we are happy to make improvements to the paper . 1.To make the assessment that the methods have converged , we looked at the best reward attained ( right plots ) , not reward running average ( left plots ) . By \u201c reward running average \u201d , we mean sliding a constant size window over the reward , and computing the average reward in each such window . This measure is not a good metric to compare the algorithms , and we use it just to show how much the reward varies from sample to sample for different methods . Note that learning-based methods ( like Neural PQT which you mentioned ) do see small improvements in running average reward even long after they stop exploring new parts of the search space - this is simply because they keep making their distributions over actions \u201c sharper \u201d . Therefore , it is more informative to look at best reward attained over time . With that in mind , we review below the experiments that you pointed to - that is , Figures 2 and 3 . You also mentioned Figure 1 , which does not correspond to any experiment , so we assumed this to be a typo - but let us know if you meant something else there , and we can discuss further . - For NAS-Bench ( Figure 3 ) , the methods see only marginal improvements at the end of the experiment . - For the synthetic task ( Figure 2 ) , all methods see small improvements of the reward if ran longer , but the overall pattern remains the same ( Evo-NAS ( PQT ) attaining the best results , matched by Neural ( PQT ) from around 4500 trials onward ) . 2.Our understanding is that PQT provides a stronger training signal than Reinforce . This comes at a cost , since Reinforce is known to be unbiased , and therefore it produces gradients that ( in expectation ) match the gradient of the expected reward . On the other hand , PQT is a heuristic , and it gives no theoretical guarantees . We wouldn \u2019 t say that PQT is always better than Reinforce - this is likely very domain dependent . Our focus is architecture search , where the number of trials is typically low given the complexity of the search spaces . In this domain , as seen in our experiments , trading off theoretical guarantees and lack of bias for a stronger and more greedy training signal leads to better results . Nevertheless , comparing PQT to classical RL algorithms ( like Reinforce , but also many others ) is a very interesting research direction . Since our main goal was hybridizing RL-based and Evolution-based approaches to architecture search , we did not explore this direction in our work ."}, "2": {"review_id": "BkeYdyHYPS-2", "review_text": "This paper is well organized. The applied methods are introduced in detail. But it lacks some more detailed analysis. My concerns are as follows. 1. The performance differences between Evolutionary agent and EVO-NAS agent seem not significant. Please conduct additional statistical tests such as the Wilcoxon signed-rank test to verify the performance improvements are significant. 2. Many studies have been conducted to automatically adjust control parameters such as crossover and mutation probabilities in evolutionary algorithm literature. It would be better to compare one of these approaches in the experiments.", "rating": "3: Weak Reject", "reply_text": "Thank you for your review ! We address each of your concerns below . Please let us know if you have some further comments - we are happy to make changes to the paper to improve it . 1.While the gains are not enormous , we agree with Reviewer # 2 in saying that they are consistent across a range of different tasks : both in real-life AS scenarios ( NAS-Bench , NLP , ImageNet ) , and in the synthetic task . However , making sure that differences between results are statistically significant is certainly an important point . For example , note that for NLP ( Table 1 ) , we ran 10 runs for each ( approach , dataset ) pair . We claim that Evo-NAS matches the better of the baselines in all cases , and in 3 out of 7 it outperforms the best baseline with statistical significance . We made this claim for the three datasets where the difference exceeded twice the standard-error-of-the-mean . This can also be easily translated into p-values : for example , for the ConsumerComplaints dataset , the p-value for Evo-NAS \u2019 s gains over either baseline being statistically significant is < 0.02 ( so we reject the null hypothesis that there is no gain ) . We used simple statistical measures such as the ones described above to assess significance . However , we are open to performing additional nonparametric tests , such as the Wilcoxon Signed-Rank Test you proposed . Please let us know what would be the exact test that you had in mind ( i.e.what would be the null hypothesis ) . For example , we could consider pairs of ( average result of Evolution , average result of Evo-NAS ) over all benchmarks considered in the paper ( there are 10 in total since we should count each NLP dataset separately ) , and perform the test on these pairs . In that case , the Signed-Rank test does show significance , as in all cases the mean result of Evo-NAS is strictly better than the mean result of Evolution . 2.You are right in saying that there are many smart evolutionary algorithms out there . However , to our knowledge , the NAS domain is dominated by \u201c non-adaptive \u201d evolution ( i.e.with uniform prior over mutations ) , which also seems to perform very well across different AS domains . Since here we focus on architecture search , we compare our method to two strong , common and most recent baselines ( the combination of which gives Evo-NAS ) . Note that our hybrid may not necessarily be the best algorithm of this kind ; our intention was to show that combining these two worlds ( learning-based and evolution-based ) in a relatively simple way we can get consistent gains across multiple domains . One kind of follow-up works that we wish to inspire is exactly what you are proposing : bringing different \u201c smart \u201d genetic algorithms from the evolution community to the NAS community , and investigating their behavior on architecture search . It is surely an exciting direction , although one that is not the focus of our current work ."}}