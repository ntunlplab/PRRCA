{"year": "2021", "forum": "PiKUvDj5jyN", "title": "Relational Learning with Variational Bayes", "decision": "Reject", "meta_review": "This paper presents a variational learning framework for inferring relations between data points. The authors further introduce novel regularizers to avoid unfavorable solutions to their relational learning problem. Qualitative results are provided on rotated versions of MNIST. Additional qualitative results on the Yale face dataset are provided in the appendix.\n\nThe reviewers agree that the idea overall is interesting, and the chosen experiments certainly provide some insight into the idea behind the method and demonstrate that the method is learning reasonable representations of relations between data points. I share the sentiment of the reviewers, however, that this paper is not yet ready for publication. The paper in its current form lacks clear positioning against related problems and related research in this community, and the experiments are all qualitative in nature without the attempt to rigorously compare the proposed method against established techniques. The argument brought forward by the authors that the proposed problem is completely novel and therefore no baselines exist is unconvincing as pointed out by the reviewers, as there is related research on e.g. spatial transformer networks [1], neural relational inference [2], and discovery of causal mechanisms [3], which similarly address the problem of discovering relations, interactions, or transformations. Even if these methods don't exactly fit the problem setup presented in this paper, an attempt should be made to design an evaluation that allows one to compare against some of these approaches, especially given that the paper claims to address the general problem of inferring relations between pairs of data points. Overall, I am confident that this would make the paper stronger and more relevant to this community.\n\n[1] Jaderberg et al., Spatial Transformer Networks (NeurIPS 2015)\n[2] Kipf et al., Neural Relational Inference for Interacting Systems (ICML 2018)\n[3] Parascandolo et al., Learning Independent Causal Mechanisms (ICML 2018)\n", "reviews": [{"review_id": "PiKUvDj5jyN-0", "review_text": "This paper proposes a model to infer the relationship between multiple instances in a dataset by inferring a latent variable . The authors accomplish this by defining an optimization problem that optimizes the ELBO of the proposed graphical model . The paper presents a nice solution to some of the identification issues that can arise when inferring the latent variable , in particular the so called \u201c information shortcut \u201d when the model overfits to only learning the \u201c absolute \u201d property of the dataset , rather than inferring the shared latent traits . I think that the formulation that the authors present and the proposed optimization model is quite interesting , however I have additional questions / concerns : It \u2019 s not clear exactly what the authors mean by a relationship . Why is there a latent variable causing b but not a ? Why are we assuming that a causes b ? It \u2019 s unclear to me why we refer to the relational property as \u2018 z \u2019 . According to figure 1 z has no effect on a , so it doesn \u2019 t seem like it would describe the relationship between a and b ? Wouldn \u2019 t theta be serving that purpose ? I \u2019 m also confused about the difference between what is proposed here and a latent factor model . It would appear that the authors are proposing a model to integrate a latent factor model into a generative net , which is interesting , but does not come through in the text as it currently reads . It would seem that in order to interpret z as a relational variable we should be able to extract some kind of meaning from it ? Also , if there are multiple relationships , say \u201c animal \u201d , \u201c rotation \u201d , and \u201c saturation \u201d , should we expect to be able to disentangle these concepts with the proposed model ? I found the experiments to be a bit underwhelming . It is unclear how the authors decided on the architecture , hyperparameters , and number of latent variables for the proposed model . It would seem that the model would be quite sensitive to these . In addition , it seems that some of these evaluations would benefit from comparison to more traditional methods . For example , the faces example appears in \u201c A Dependence Maximization View of Clustering \u201d , within a very similar context . Overall , I think this is an interesting idea , but I would like to see the paper a bit more refined before recommending acceptance .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer : We would first like to express our deep appreciation for your time and insightful comments . Please find our response to your concerns ( referring to as R1 ) in the order they are raised : 1 . We interpret the first questions/concerns as stemming from our overly succinct introduction and explanation of the proposed VRL-PGM and its connection to the original relational learning problem . Please allow us to clarify : first , we interpret the relationship ( relational property ) as any information governing the variation between the data sample ( a-b ) while absolute property as any information governing the features within the data sample ( a or b ) . The goal of relational learning is to discover a set of relational property that is independent ( decoupled ) from the absolute property . With this goal in mind , we proposed our VRL-PGM model for representing the abstract relational learning problem . The random variables in our VRL-PGM represent the three key information in the original problem : the observed random variables representing a , b and the latent variable z representing the unobserved relational property . We connected a , b , and z with a directed acyclic graph ( as shown in VRL-PGM ) that reflects our priority as well as compromises for adopting a rigorous PGM to the original abstract problem : our first and foremost priority is to learn a decoupled relational property and this is achieved by the inherit property of VRL-PGM where z is independent of a . In VRL-PGM , the relational property , as represented by latent variable z , can be interpreted as any additional information not found in a but can help to better predict b ; in other words , VRL-PGM interpret z ( relational property ) as an external factor ( not found in a ) that dictates the relationship ( represented by the conditional distribution p ( b|a , z ) ) between a and b . Regarding the specific question on `` Why is there a latent variable causing b but not a ? `` , the answer is that we are not interested in learning about a and the fact that b is caused by BOTH a and z can be interpreted as b derives its absolute property from a and relationship information from z ( the absolute property of a alone is not enough to characterize b , we need additional relationship information from z ) . Regarding the specific question on `` According to figure 1 z has no effect on a , so it doesn \u2019 t seem like it would describe the relationship between a and b ? `` , the key to answer this question is to recognize the fact that in the proposed VRL-PGM z and a are independent ( R1 correctly stated this in the first part of the question ) ; however , they are no longer independent when b is observed ( a well-known yet counterintuitive property , see ( Bishop , 2006 , Chap.8.2.1 ) ) ! This means that it is valid to inquire about the posterior p ( z|a , b ) and it is in this sense that z captures the relationship information between a and b ! . Next , we note that there are at least two compromises made by VRL-PGM : first , the original problem specifies that the relational property be decoupled from both a \u2019 s and b \u2019 s absolute properties ; however , the latent variable z that is used to represent relational property in VRL-PGM is only independent of a but not b . Second , we note that the original problem is inherently undirected with no cause-effect relationship between a and b , whereas VRL-PGM is based a directed acyclic graph that artificially introduces conditional dependency between a and b . However , we argue that the application of VRL does not require the true conditional dependency between ( a ; b ) be known in advance only that it is maintained consistently throughout learning and inference , i.e. , VRL can be applied in the same way to learn about the relational property between ( b ; a ) , where we swap a and b . In short , the artificial causal relationship introduced by VRL-PGM reflects our priority and compromise for representing the original abstract relational learning problem with a PGM : we sacrificed some identifiability of the original abstract problem but gained a rigorous and mathematical tractable statistical model ( reasoning and learning in a directed graph is much easier compared to undirected graph ) while achieving our primary objective of learning an independent ( decoupled ) relational property . We hope this discussion clarify R1 \u2019 s questions regarding the meaning of z as well as its relationship to a and b . We have added this important discussion in the appendix . 2.We think the proposed VRL-PGM is a form of latent ( factor ) variable model where we designate the latent variable z to represent the relational property and derive a variational inference algorithm that estimate z from a and b . We also estimate the likelihood function p ( b|z , a ) as a part of the VI learning process . One can certainly view p ( b|z , a ) as a data generating function for b given a and z , but we do not ( at least not the focus of this work ) learn p ( b|a ) or p ( b , a ) . ( To be continued in the appending post )"}, {"review_id": "PiKUvDj5jyN-1", "review_text": "Relational learning is an important capability exhibited by humans of learning relations between objects . This work considers a fairly general setup for relational learning and addresses learning of the relation through a method based on variational inference . I am not familiar with the VAE literature ; having said that , the method seems novel . The problem setup is so general that it is unlikely to be novel , but the main technical contribution of the paper appears to be the derivation of the objective ( 4 ) . Of course , given that the objective is a lower bound of the probability we want to optimize , and since no theory is provided on the quality of the approximation , it is important to have strong demonstrations of the method in which it is shown to have unique advantages compared to existing approaches . Unfortunately , the paper falls short in this regard , as I will go into more detail . The equation ( 4 ) does seem plausible and it is rather elegant . Note that ( 4 ) can also be seen as likelihood + regularization on z + entropy of z , hence it encourages latent variables z that are not too large according to the regularization , while the entropy term prevents z from collapsing to the mode . This makes sense , but at the same time it is difficult to see how independence between z and a would be enforced using this objective function . The other technical contribution is the identification of optimization issues via the `` information shortcut '' and `` deterministic mapping '' issues . This is very insightful . The idea of RPDA is interesting , and in many cases it could be applied at least via data-augmentation . The main demonstration is based on learning rotation angles in MNIST . The relationship of `` A is 30 degrees rotated compared to B '' does qualify as a relation , albeit one that seems fairly artificial . ( EDIT : since discussion , I retract the following in brackets ) . [ More worrisome is that the example used , if I understand correctly , does not follow the causal model adopted by the authors . The model assumes that a and z cause b . However , the example has b as the MNIST label . This is causally incorrect because for handwritten digits , the label is the cause of the image , not vice-versa . Presumably the causal assumptions should be correct for the model to work well ? It would be helpful if the authors could comment on this . ] The Yale face dataset in the supplement seems a better fit to the causal assumptions . However neither example is satisfactory in showing a unique benefit of this approach that can not be already obtained by domain-specific methods , such as spatial transformer networks in the case of learning image rotations/scaling ( https : //papers.nips.cc/paper/5854-spatial-transformer-networks.pdf ) and emotion learning ML methods for facial images ( https : //www.sciencedirect.com/science/article/pii/S1877050917327679 ) . Perhaps the authors could propose an application that would showcase the advantages of the approach , even if it is currently infeasible to obtain results ? If we contrast the relational learning approach to the existing body of work on learning disentangled representations , the main difference is that in existing representation learning work , one learns the latent variable _z_ from _a_ only . But here , _z_ can be learned from both _a_ and _b_ . This is obviously an important difference , but one could reduce the problem of learning _z_ from _a_ , _b_ to methods that learn _z1_ from _a_ and _z2_ from _b_ separately . For example , in learning the rotation between _a_ and _b_ ( both MNIST digits ) , one could reduce it to learning the orientation of _a_ , learning the orientation of _b_ separately , and then subtracting the orientations . Of course , this seems to be a property of the examples used rather than a general fact . If one considers more sophisticated relational learning problems such as properties and objects , then it may no longer be reducible . Yet this again indicates that the examples used in the paper fail to show the uniqueness of the proposed approach . # # # Pros : * interesting problem with applications to psychology * insightful analysis of the optimization involved and remedies for two pitfalls # # # Cons : * toy examples inadequate to show novelty and generalizability of the method * not obvious to me how the objective function encourages independence of _z_ and _a_ . * do we have any confidence that this method would work in a more complicated problem ? # # # Recommendation : The paper starts an interesting line of development but still seems to need more work in either the theory , the sophistication of demonstrations , or both , before one could make a convincing case that it adds something beyond similar approaches in the literature . Hence I recommend rejection of this paper . The authors are encouraged to look into adding more sophisticated and unique examples , and further analyzing their method . # # # Modifications since discussion * retracted objection about inappropriateness of MNIST example * raised score from 5 to 6", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer : We would first like to express our deep appreciation for your time and insightful comments . Please find our response to your concerns ( referring as R2 ) in the order they are raised : 1 . We viewed our main contribution as the proposal of a general and flexible variational learning framework for relational learning : including the proposal of VRL-PGM ; a learning objective ( variational lower bound ) derived from first principle ; and , a practical , end-to-end , completely unsupervised learning method . The proposed VRL-PGM that artificially introduces causal relationship by using a directed acyclic graph reflects our priority and compromise for representing the original abstract relational learning problem with a PGM : we sacrificed some identifiability of the original abstract problem but we gained a rigorous and mathematical tractable statistical model ( reasoning and learning in a directed graph is much easier compared to undirected graph ) while achieving our primary objective of learning an independent ( decoupled ) relational property . We have added a discussion of the proposed VRL-PGM and its connection to the original relational learning problem in the appendix . Please also see our response # 1 to R1 . 2.The independence between latent variable z and a is an inherent property of the proposed VRL-PGM and not something we need to enforce explicitly . Please allow us to give an intuitive explanation on this point in terms of decomposing the objective ( as noted by R2 ) into \u201c likelihood + regularization on z + entropy of z \u201d . We can focus the discussion on the likelihood term p ( b|z , a ) since this is the only term that is constrained by the data a and b ( other terms can be viewed as regularization on the unobserved z ) . In the maximization of the likehood p ( b|z , a ) , the objective is to learn a mapping to fit ONLY data b given a and z ; therefore , there is no incentive for z to learn redundant ( dependent ) information from a ; this effect is further \u201c enforced \u201d when the learning objective includes additional regularization term on z to learn a compact representation . In the information flow diagram shown in Fig.2 of our submission one may argue that since a also propagate information through the latent variable z it may introduce dependency between a and z ( in the posterior p ( z|a , b ) , z and a are no longer independent ! ) ; however , the information propagated from a through z is mainly used to maximize the likelihood function for predicting b , and since a already provide a direct propagation path to b in p ( b|a , z ) there is , again ( via regularization on z ) , no incentive for z to carry redundant absolute property information from a ( only need to carry the decoupled relationship information from a and b ) . In summary , the independence between z and a is an inherent property of VRL-PGM , the variational lower-bound ( learning objective ) we derived based on VRL-PGM from first principle naturally disincentives learning a z that carries redundant ( dependent ) information of a without the need to explicitly enforce it . However , we understand R1 \u2019 s concerns for taking a more conservative viewpoint and wanting to explicitly safeguard against learning dependency between z and a . To this end , we would like to present another advantage of the proposed VRL-PGM and the derived variational lower bound : we can add any non-positive function that measures the dependency between a and z with maximum attained when they are independent to Eq.4 without invalidating the lower bound . For example , we can safely add the mutual information between z and a , -I ( z , a ) , to Eq . ( 4 ) where since I ( z , a ) > =0 and I ( z , a ) =0 iff z and a are independent ( which they are in VRL-PGM ! ) . Please let us know if you would like us add this discussion to the submission . Lastly , we want to emphasize that the goal of relational learning is to learn a relational property ( represented by latent variable z ) that is independent from the absolute property of a. Relational property z will necessarily derive relationship information from both a and b but should not depend on the absolute property of a ( any information that describe a alone ) . ( To be continued in the appending post )"}, {"review_id": "PiKUvDj5jyN-2", "review_text": "The paper proposes variational relational learning by learning relations between two inputs via variational inference on a probabilistic graphical model ( PGM ) . The PGM that they use factors as p ( a ) p ( z ) p ( b|a , z ) where a , b are the two inputs and z is the supposed relationship between them . The example shown in the experiments is rotational mnist , where b is a rotated version of a , and z should encode the degree of rotation . The paper learns both the forward network and in inference network in a VAE-like approach ; the elbo derivations appear correct to me . In section 3.3 the paper describes two sources of problems with naively optimizing based on the above approach : 1 ) deterministic mapping where a completely determines b , and no learning of relation is necessary , and 2 ) information shortcut where z can completely encode b . To me 1 does not seem like a big drawback since it is more of a limitation with the underlying data . For 2 , the paper describes a data-augmentation technique they call RPDA , as well as using an informative prior on z . The experimental results seem reasonable , although having setting beyond mnist would have been nice . The two methods described to handle the information shortcut issue are not super satisfactory in my opinion . The data augmentation technique relies on designing augmentations g that preserve the relationship , so that ( a , b ) are related in the same way as ( g ( a ) , g ( b ) ) . But coming up with such augmentations seems to rely on us knowing properties of the relationship z , which is exactly what we are trying to learn in the first place . The alternative of using an informative prior is not super clear to me , since it seems that with a powerful enough model p ( b|a , z ) , scaling and shifting p ( z ) to fit a Gaussian will not prevent the model from completely encoding b as a function of z . In my opinion constraining the expressiveness of z seems like the right path , but instead of regularizing p ( z ) to be a Gaussian , I would consider using discrete z 's that explicitly limits the theoretical # of bits z can store , hence preventing it from encoding b. I think this also intuitively aligns with how many relations seem more discrete than continuous ( a is friend of b , a is adjective form of b , ... etc ) . I would be interested in seeing the experiments done using discrete z 's , which seems natural too given the 5 discrete rotations considered in the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer , We would first like to express our deep appreciation for your time and insightful comments . Please find our response to your concerns ( referring to as R3 ) in the order they are raised : 1 . We believe the provided MNIST relational learning examples represent novel and unique questions and clearly demonstrate the relational learning challenge that are not easily solvable with existing methods ( please see a detailed example and explanation in our response # 4 , # 5 to reviewer R2 ) . In addition to the MNIST example presented in the main text , we include three additional MNIST relational learning example in the appendix each with increasing complexity ( coupled relational property , multiple relational property , and continuous relational property ) . We demonstrate the robustness of the proposed method by successfully achieving satisfactory results on all four problems with the exact same model setup . To further demonstrate the generalization ability of the proposed method on a more complicated problem , we presented a face relational learning example and successfully solved it with a very similar setup as in our MNSIT example where we only made necessary adjustment to the model setup to accommodate the different types of data ( e.g. , larger image and continuous vs binary data ) . 2.We think R3 \u2019 s suggestion on \u201c constraining the expressiveness of z \u201d nicely sums up what we intended to say in our original submission . We think imposing an informative prior with restrictive constraint also falls under the strategy of \u201c constraining the expressiveness of z \u201d . We also agrees that a discrete prior is appropriate for learning a set of discrete relational property ( when they are known in advance and appropriate number of set is chosen ! ) . With R3 \u2019 s permission , we would like to go ahead with changing the wording in our submission as well as including the discussion of discrete prior . We hope to convince the readers that our main contribution is the proposal of a general and flexible variational learning framework for relational learning that can applied in conjunction with different prior based on the understanding/assumption of the problem . While we experiment with this idea , our original motivation for choosing a Gaussian ( continuous ) prior is to have a consistent model setup to test all four different relational learning scenarios : coupled relational property , decoupled relational property , multiple relational property , and continuous relational property . Our goal , beyond the fact that we want to strictly restrict our self to \u201c completely unsupervised \u201d setting where we don \u2019 t exploit the knowledge of knowing the under relational property to be discrete , is to demonstrate the robustness ( not sensitive to model setup ) and generalization of the proposed method . Lastly , we are grateful for your time and we hope we have adequately address your concerns . If you find the above response useful , we would like to seek your advice on which part you would like us to include in the submission . Please let us know if you have any questions . We look forward to additional discussion ."}, {"review_id": "PiKUvDj5jyN-3", "review_text": "In this paper , the author proposed a variational framework for relational learning that decouples relational property and absolute property among objects based on a pre-defined probabilistic graphical model ( PGM ) . The author also proposed the so called relation-preserving data augmentation ( RPDA ) strategy to address the challenges for the resulting optimization . Overall , the paper is well written and easy to follow . Below are some of my concerns . 1.It seems that RPDA is crucial for training . However , the relation preserving function D is often difficult to find for real data where the existing relation patterns are often unknown . 2.No ablation study is reported regarding the effect of RPDA . 3.Experiments seem to be too simple . It might be hard to find appropriate application cases though . 4.Can the author clarify the relation between the proposed PGM based relation learning framework and causal learning ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer : We would first like to express our deep appreciation for your time and insightful comments . Please find our response to your concerns ( referring to as R4 ) in the order they are raised : 1 . We acknowledge R4 \u2019 s comments that in practice relation preserving function D is not always possible to identify in the presence of unknown relationship a remark we have expressed in our original submission . However , our viewpoint is that in many practical problem settings , relation preserving function D can be designed without any knowledge of the underlying relational property . For example , in computer vision applications , if we want the learned model to be rotation invariant ( a common practice ) we can use image rotations as D. Another example may be : in many spectral imaging problem , the orientation of images is not preserved or not enforced ( only that they are consistent between the same paired images ) and in such problem settings it is safe to adopt image rotations as D. Yet , another example may be : for a discrete time-series data $ \\mathbf { a } [ t ] , \\mathbf { b } [ t ] $ that represent the input and output of a linear time-invariant ( LTI ) system ( commonly assumed in signal processing and control theory ) , and we want to learn a relational property that characterize the system 's impulse response . We have $ \\alpha \\mathbf { b } [ t-\\tau ] =\\alpha \\mathbf { a } [ t-\\tau ] , \\ ; \\forall \\alpha \\in \\mathbb { R } , \\tau \\in \\mathbb { Z } $ , and we can construct $ D $ with $ d ( \\mathbf { a } [ t ] , \\mathbf { b } [ t ] ; \\alpha , \\tau ) = ( \\alpha \\mathbf { a } [ t-\\tau ] , \\alpha \\mathbf { b } [ t-\\tau ] ) $ , $ \\alpha , \\tau \\in R=\\mathbb { R } \\times\\mathbb { Z } $ . In all of the above example , the relation preserving function D is selected not based on the underlying relational property of the data but the nature of the problem and constraint we impose on our model ; therefore , in many instances , RPDA can be designed without any knowledge of the underlying relational property . The bottom line is that RPDA is not central to the theory of the proposed method ( one can apply the proposed VRL method without RPDA ) but rather a practical data augmentation strategy for addressing the unique optimization challenge of VRL learning . In the experiments presented in our submission , we find that RPDA to be very effective in overcoming the information-shortcut problem but , like any data augmentations , this is problem dependent and we advocate to start without RPDA and only apply it when necessary ( when suspecting information-shortcut occurs ) . 2.We have conducted a detailed ablation study on RPDA in our original submission . Due to the page constraint , we include those results in the appendix . 3.We acknowledge that the presented experimented may seem contrived at first glance . However , we do believe that the provided MNIST relational learning examples represent novel and unique questions and clearly demonstrate the relational learning challenge that are not easily solvable with existing methods . In addition to the MNIST example presented in the main text , we include three additional MNIST relational learning example in the appendix each with increasing complexity ( coupled relational property , multiple relational property , and continuous relational property ) . We demonstrate the robustness of the proposed method by successfully achieving satisfactory results on all four problems with the exact same model setup . To further demonstrate the generalization ability of the proposed method on a more complicated problem , we presented a face relational learning example and successfully solved it with a very similar setup as in our MNSIT example where we only made necessary adjustment to the model setup to accommodate the different types of data ( e.g. , larger image and continuous vs binary data ) . Please see a detailed discussion in our response # 4 , # 5 to R2 . ( To be continued in the appending post )"}], "0": {"review_id": "PiKUvDj5jyN-0", "review_text": "This paper proposes a model to infer the relationship between multiple instances in a dataset by inferring a latent variable . The authors accomplish this by defining an optimization problem that optimizes the ELBO of the proposed graphical model . The paper presents a nice solution to some of the identification issues that can arise when inferring the latent variable , in particular the so called \u201c information shortcut \u201d when the model overfits to only learning the \u201c absolute \u201d property of the dataset , rather than inferring the shared latent traits . I think that the formulation that the authors present and the proposed optimization model is quite interesting , however I have additional questions / concerns : It \u2019 s not clear exactly what the authors mean by a relationship . Why is there a latent variable causing b but not a ? Why are we assuming that a causes b ? It \u2019 s unclear to me why we refer to the relational property as \u2018 z \u2019 . According to figure 1 z has no effect on a , so it doesn \u2019 t seem like it would describe the relationship between a and b ? Wouldn \u2019 t theta be serving that purpose ? I \u2019 m also confused about the difference between what is proposed here and a latent factor model . It would appear that the authors are proposing a model to integrate a latent factor model into a generative net , which is interesting , but does not come through in the text as it currently reads . It would seem that in order to interpret z as a relational variable we should be able to extract some kind of meaning from it ? Also , if there are multiple relationships , say \u201c animal \u201d , \u201c rotation \u201d , and \u201c saturation \u201d , should we expect to be able to disentangle these concepts with the proposed model ? I found the experiments to be a bit underwhelming . It is unclear how the authors decided on the architecture , hyperparameters , and number of latent variables for the proposed model . It would seem that the model would be quite sensitive to these . In addition , it seems that some of these evaluations would benefit from comparison to more traditional methods . For example , the faces example appears in \u201c A Dependence Maximization View of Clustering \u201d , within a very similar context . Overall , I think this is an interesting idea , but I would like to see the paper a bit more refined before recommending acceptance .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer : We would first like to express our deep appreciation for your time and insightful comments . Please find our response to your concerns ( referring to as R1 ) in the order they are raised : 1 . We interpret the first questions/concerns as stemming from our overly succinct introduction and explanation of the proposed VRL-PGM and its connection to the original relational learning problem . Please allow us to clarify : first , we interpret the relationship ( relational property ) as any information governing the variation between the data sample ( a-b ) while absolute property as any information governing the features within the data sample ( a or b ) . The goal of relational learning is to discover a set of relational property that is independent ( decoupled ) from the absolute property . With this goal in mind , we proposed our VRL-PGM model for representing the abstract relational learning problem . The random variables in our VRL-PGM represent the three key information in the original problem : the observed random variables representing a , b and the latent variable z representing the unobserved relational property . We connected a , b , and z with a directed acyclic graph ( as shown in VRL-PGM ) that reflects our priority as well as compromises for adopting a rigorous PGM to the original abstract problem : our first and foremost priority is to learn a decoupled relational property and this is achieved by the inherit property of VRL-PGM where z is independent of a . In VRL-PGM , the relational property , as represented by latent variable z , can be interpreted as any additional information not found in a but can help to better predict b ; in other words , VRL-PGM interpret z ( relational property ) as an external factor ( not found in a ) that dictates the relationship ( represented by the conditional distribution p ( b|a , z ) ) between a and b . Regarding the specific question on `` Why is there a latent variable causing b but not a ? `` , the answer is that we are not interested in learning about a and the fact that b is caused by BOTH a and z can be interpreted as b derives its absolute property from a and relationship information from z ( the absolute property of a alone is not enough to characterize b , we need additional relationship information from z ) . Regarding the specific question on `` According to figure 1 z has no effect on a , so it doesn \u2019 t seem like it would describe the relationship between a and b ? `` , the key to answer this question is to recognize the fact that in the proposed VRL-PGM z and a are independent ( R1 correctly stated this in the first part of the question ) ; however , they are no longer independent when b is observed ( a well-known yet counterintuitive property , see ( Bishop , 2006 , Chap.8.2.1 ) ) ! This means that it is valid to inquire about the posterior p ( z|a , b ) and it is in this sense that z captures the relationship information between a and b ! . Next , we note that there are at least two compromises made by VRL-PGM : first , the original problem specifies that the relational property be decoupled from both a \u2019 s and b \u2019 s absolute properties ; however , the latent variable z that is used to represent relational property in VRL-PGM is only independent of a but not b . Second , we note that the original problem is inherently undirected with no cause-effect relationship between a and b , whereas VRL-PGM is based a directed acyclic graph that artificially introduces conditional dependency between a and b . However , we argue that the application of VRL does not require the true conditional dependency between ( a ; b ) be known in advance only that it is maintained consistently throughout learning and inference , i.e. , VRL can be applied in the same way to learn about the relational property between ( b ; a ) , where we swap a and b . In short , the artificial causal relationship introduced by VRL-PGM reflects our priority and compromise for representing the original abstract relational learning problem with a PGM : we sacrificed some identifiability of the original abstract problem but gained a rigorous and mathematical tractable statistical model ( reasoning and learning in a directed graph is much easier compared to undirected graph ) while achieving our primary objective of learning an independent ( decoupled ) relational property . We hope this discussion clarify R1 \u2019 s questions regarding the meaning of z as well as its relationship to a and b . We have added this important discussion in the appendix . 2.We think the proposed VRL-PGM is a form of latent ( factor ) variable model where we designate the latent variable z to represent the relational property and derive a variational inference algorithm that estimate z from a and b . We also estimate the likelihood function p ( b|z , a ) as a part of the VI learning process . One can certainly view p ( b|z , a ) as a data generating function for b given a and z , but we do not ( at least not the focus of this work ) learn p ( b|a ) or p ( b , a ) . ( To be continued in the appending post )"}, "1": {"review_id": "PiKUvDj5jyN-1", "review_text": "Relational learning is an important capability exhibited by humans of learning relations between objects . This work considers a fairly general setup for relational learning and addresses learning of the relation through a method based on variational inference . I am not familiar with the VAE literature ; having said that , the method seems novel . The problem setup is so general that it is unlikely to be novel , but the main technical contribution of the paper appears to be the derivation of the objective ( 4 ) . Of course , given that the objective is a lower bound of the probability we want to optimize , and since no theory is provided on the quality of the approximation , it is important to have strong demonstrations of the method in which it is shown to have unique advantages compared to existing approaches . Unfortunately , the paper falls short in this regard , as I will go into more detail . The equation ( 4 ) does seem plausible and it is rather elegant . Note that ( 4 ) can also be seen as likelihood + regularization on z + entropy of z , hence it encourages latent variables z that are not too large according to the regularization , while the entropy term prevents z from collapsing to the mode . This makes sense , but at the same time it is difficult to see how independence between z and a would be enforced using this objective function . The other technical contribution is the identification of optimization issues via the `` information shortcut '' and `` deterministic mapping '' issues . This is very insightful . The idea of RPDA is interesting , and in many cases it could be applied at least via data-augmentation . The main demonstration is based on learning rotation angles in MNIST . The relationship of `` A is 30 degrees rotated compared to B '' does qualify as a relation , albeit one that seems fairly artificial . ( EDIT : since discussion , I retract the following in brackets ) . [ More worrisome is that the example used , if I understand correctly , does not follow the causal model adopted by the authors . The model assumes that a and z cause b . However , the example has b as the MNIST label . This is causally incorrect because for handwritten digits , the label is the cause of the image , not vice-versa . Presumably the causal assumptions should be correct for the model to work well ? It would be helpful if the authors could comment on this . ] The Yale face dataset in the supplement seems a better fit to the causal assumptions . However neither example is satisfactory in showing a unique benefit of this approach that can not be already obtained by domain-specific methods , such as spatial transformer networks in the case of learning image rotations/scaling ( https : //papers.nips.cc/paper/5854-spatial-transformer-networks.pdf ) and emotion learning ML methods for facial images ( https : //www.sciencedirect.com/science/article/pii/S1877050917327679 ) . Perhaps the authors could propose an application that would showcase the advantages of the approach , even if it is currently infeasible to obtain results ? If we contrast the relational learning approach to the existing body of work on learning disentangled representations , the main difference is that in existing representation learning work , one learns the latent variable _z_ from _a_ only . But here , _z_ can be learned from both _a_ and _b_ . This is obviously an important difference , but one could reduce the problem of learning _z_ from _a_ , _b_ to methods that learn _z1_ from _a_ and _z2_ from _b_ separately . For example , in learning the rotation between _a_ and _b_ ( both MNIST digits ) , one could reduce it to learning the orientation of _a_ , learning the orientation of _b_ separately , and then subtracting the orientations . Of course , this seems to be a property of the examples used rather than a general fact . If one considers more sophisticated relational learning problems such as properties and objects , then it may no longer be reducible . Yet this again indicates that the examples used in the paper fail to show the uniqueness of the proposed approach . # # # Pros : * interesting problem with applications to psychology * insightful analysis of the optimization involved and remedies for two pitfalls # # # Cons : * toy examples inadequate to show novelty and generalizability of the method * not obvious to me how the objective function encourages independence of _z_ and _a_ . * do we have any confidence that this method would work in a more complicated problem ? # # # Recommendation : The paper starts an interesting line of development but still seems to need more work in either the theory , the sophistication of demonstrations , or both , before one could make a convincing case that it adds something beyond similar approaches in the literature . Hence I recommend rejection of this paper . The authors are encouraged to look into adding more sophisticated and unique examples , and further analyzing their method . # # # Modifications since discussion * retracted objection about inappropriateness of MNIST example * raised score from 5 to 6", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer : We would first like to express our deep appreciation for your time and insightful comments . Please find our response to your concerns ( referring as R2 ) in the order they are raised : 1 . We viewed our main contribution as the proposal of a general and flexible variational learning framework for relational learning : including the proposal of VRL-PGM ; a learning objective ( variational lower bound ) derived from first principle ; and , a practical , end-to-end , completely unsupervised learning method . The proposed VRL-PGM that artificially introduces causal relationship by using a directed acyclic graph reflects our priority and compromise for representing the original abstract relational learning problem with a PGM : we sacrificed some identifiability of the original abstract problem but we gained a rigorous and mathematical tractable statistical model ( reasoning and learning in a directed graph is much easier compared to undirected graph ) while achieving our primary objective of learning an independent ( decoupled ) relational property . We have added a discussion of the proposed VRL-PGM and its connection to the original relational learning problem in the appendix . Please also see our response # 1 to R1 . 2.The independence between latent variable z and a is an inherent property of the proposed VRL-PGM and not something we need to enforce explicitly . Please allow us to give an intuitive explanation on this point in terms of decomposing the objective ( as noted by R2 ) into \u201c likelihood + regularization on z + entropy of z \u201d . We can focus the discussion on the likelihood term p ( b|z , a ) since this is the only term that is constrained by the data a and b ( other terms can be viewed as regularization on the unobserved z ) . In the maximization of the likehood p ( b|z , a ) , the objective is to learn a mapping to fit ONLY data b given a and z ; therefore , there is no incentive for z to learn redundant ( dependent ) information from a ; this effect is further \u201c enforced \u201d when the learning objective includes additional regularization term on z to learn a compact representation . In the information flow diagram shown in Fig.2 of our submission one may argue that since a also propagate information through the latent variable z it may introduce dependency between a and z ( in the posterior p ( z|a , b ) , z and a are no longer independent ! ) ; however , the information propagated from a through z is mainly used to maximize the likelihood function for predicting b , and since a already provide a direct propagation path to b in p ( b|a , z ) there is , again ( via regularization on z ) , no incentive for z to carry redundant absolute property information from a ( only need to carry the decoupled relationship information from a and b ) . In summary , the independence between z and a is an inherent property of VRL-PGM , the variational lower-bound ( learning objective ) we derived based on VRL-PGM from first principle naturally disincentives learning a z that carries redundant ( dependent ) information of a without the need to explicitly enforce it . However , we understand R1 \u2019 s concerns for taking a more conservative viewpoint and wanting to explicitly safeguard against learning dependency between z and a . To this end , we would like to present another advantage of the proposed VRL-PGM and the derived variational lower bound : we can add any non-positive function that measures the dependency between a and z with maximum attained when they are independent to Eq.4 without invalidating the lower bound . For example , we can safely add the mutual information between z and a , -I ( z , a ) , to Eq . ( 4 ) where since I ( z , a ) > =0 and I ( z , a ) =0 iff z and a are independent ( which they are in VRL-PGM ! ) . Please let us know if you would like us add this discussion to the submission . Lastly , we want to emphasize that the goal of relational learning is to learn a relational property ( represented by latent variable z ) that is independent from the absolute property of a. Relational property z will necessarily derive relationship information from both a and b but should not depend on the absolute property of a ( any information that describe a alone ) . ( To be continued in the appending post )"}, "2": {"review_id": "PiKUvDj5jyN-2", "review_text": "The paper proposes variational relational learning by learning relations between two inputs via variational inference on a probabilistic graphical model ( PGM ) . The PGM that they use factors as p ( a ) p ( z ) p ( b|a , z ) where a , b are the two inputs and z is the supposed relationship between them . The example shown in the experiments is rotational mnist , where b is a rotated version of a , and z should encode the degree of rotation . The paper learns both the forward network and in inference network in a VAE-like approach ; the elbo derivations appear correct to me . In section 3.3 the paper describes two sources of problems with naively optimizing based on the above approach : 1 ) deterministic mapping where a completely determines b , and no learning of relation is necessary , and 2 ) information shortcut where z can completely encode b . To me 1 does not seem like a big drawback since it is more of a limitation with the underlying data . For 2 , the paper describes a data-augmentation technique they call RPDA , as well as using an informative prior on z . The experimental results seem reasonable , although having setting beyond mnist would have been nice . The two methods described to handle the information shortcut issue are not super satisfactory in my opinion . The data augmentation technique relies on designing augmentations g that preserve the relationship , so that ( a , b ) are related in the same way as ( g ( a ) , g ( b ) ) . But coming up with such augmentations seems to rely on us knowing properties of the relationship z , which is exactly what we are trying to learn in the first place . The alternative of using an informative prior is not super clear to me , since it seems that with a powerful enough model p ( b|a , z ) , scaling and shifting p ( z ) to fit a Gaussian will not prevent the model from completely encoding b as a function of z . In my opinion constraining the expressiveness of z seems like the right path , but instead of regularizing p ( z ) to be a Gaussian , I would consider using discrete z 's that explicitly limits the theoretical # of bits z can store , hence preventing it from encoding b. I think this also intuitively aligns with how many relations seem more discrete than continuous ( a is friend of b , a is adjective form of b , ... etc ) . I would be interested in seeing the experiments done using discrete z 's , which seems natural too given the 5 discrete rotations considered in the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer , We would first like to express our deep appreciation for your time and insightful comments . Please find our response to your concerns ( referring to as R3 ) in the order they are raised : 1 . We believe the provided MNIST relational learning examples represent novel and unique questions and clearly demonstrate the relational learning challenge that are not easily solvable with existing methods ( please see a detailed example and explanation in our response # 4 , # 5 to reviewer R2 ) . In addition to the MNIST example presented in the main text , we include three additional MNIST relational learning example in the appendix each with increasing complexity ( coupled relational property , multiple relational property , and continuous relational property ) . We demonstrate the robustness of the proposed method by successfully achieving satisfactory results on all four problems with the exact same model setup . To further demonstrate the generalization ability of the proposed method on a more complicated problem , we presented a face relational learning example and successfully solved it with a very similar setup as in our MNSIT example where we only made necessary adjustment to the model setup to accommodate the different types of data ( e.g. , larger image and continuous vs binary data ) . 2.We think R3 \u2019 s suggestion on \u201c constraining the expressiveness of z \u201d nicely sums up what we intended to say in our original submission . We think imposing an informative prior with restrictive constraint also falls under the strategy of \u201c constraining the expressiveness of z \u201d . We also agrees that a discrete prior is appropriate for learning a set of discrete relational property ( when they are known in advance and appropriate number of set is chosen ! ) . With R3 \u2019 s permission , we would like to go ahead with changing the wording in our submission as well as including the discussion of discrete prior . We hope to convince the readers that our main contribution is the proposal of a general and flexible variational learning framework for relational learning that can applied in conjunction with different prior based on the understanding/assumption of the problem . While we experiment with this idea , our original motivation for choosing a Gaussian ( continuous ) prior is to have a consistent model setup to test all four different relational learning scenarios : coupled relational property , decoupled relational property , multiple relational property , and continuous relational property . Our goal , beyond the fact that we want to strictly restrict our self to \u201c completely unsupervised \u201d setting where we don \u2019 t exploit the knowledge of knowing the under relational property to be discrete , is to demonstrate the robustness ( not sensitive to model setup ) and generalization of the proposed method . Lastly , we are grateful for your time and we hope we have adequately address your concerns . If you find the above response useful , we would like to seek your advice on which part you would like us to include in the submission . Please let us know if you have any questions . We look forward to additional discussion ."}, "3": {"review_id": "PiKUvDj5jyN-3", "review_text": "In this paper , the author proposed a variational framework for relational learning that decouples relational property and absolute property among objects based on a pre-defined probabilistic graphical model ( PGM ) . The author also proposed the so called relation-preserving data augmentation ( RPDA ) strategy to address the challenges for the resulting optimization . Overall , the paper is well written and easy to follow . Below are some of my concerns . 1.It seems that RPDA is crucial for training . However , the relation preserving function D is often difficult to find for real data where the existing relation patterns are often unknown . 2.No ablation study is reported regarding the effect of RPDA . 3.Experiments seem to be too simple . It might be hard to find appropriate application cases though . 4.Can the author clarify the relation between the proposed PGM based relation learning framework and causal learning ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer : We would first like to express our deep appreciation for your time and insightful comments . Please find our response to your concerns ( referring to as R4 ) in the order they are raised : 1 . We acknowledge R4 \u2019 s comments that in practice relation preserving function D is not always possible to identify in the presence of unknown relationship a remark we have expressed in our original submission . However , our viewpoint is that in many practical problem settings , relation preserving function D can be designed without any knowledge of the underlying relational property . For example , in computer vision applications , if we want the learned model to be rotation invariant ( a common practice ) we can use image rotations as D. Another example may be : in many spectral imaging problem , the orientation of images is not preserved or not enforced ( only that they are consistent between the same paired images ) and in such problem settings it is safe to adopt image rotations as D. Yet , another example may be : for a discrete time-series data $ \\mathbf { a } [ t ] , \\mathbf { b } [ t ] $ that represent the input and output of a linear time-invariant ( LTI ) system ( commonly assumed in signal processing and control theory ) , and we want to learn a relational property that characterize the system 's impulse response . We have $ \\alpha \\mathbf { b } [ t-\\tau ] =\\alpha \\mathbf { a } [ t-\\tau ] , \\ ; \\forall \\alpha \\in \\mathbb { R } , \\tau \\in \\mathbb { Z } $ , and we can construct $ D $ with $ d ( \\mathbf { a } [ t ] , \\mathbf { b } [ t ] ; \\alpha , \\tau ) = ( \\alpha \\mathbf { a } [ t-\\tau ] , \\alpha \\mathbf { b } [ t-\\tau ] ) $ , $ \\alpha , \\tau \\in R=\\mathbb { R } \\times\\mathbb { Z } $ . In all of the above example , the relation preserving function D is selected not based on the underlying relational property of the data but the nature of the problem and constraint we impose on our model ; therefore , in many instances , RPDA can be designed without any knowledge of the underlying relational property . The bottom line is that RPDA is not central to the theory of the proposed method ( one can apply the proposed VRL method without RPDA ) but rather a practical data augmentation strategy for addressing the unique optimization challenge of VRL learning . In the experiments presented in our submission , we find that RPDA to be very effective in overcoming the information-shortcut problem but , like any data augmentations , this is problem dependent and we advocate to start without RPDA and only apply it when necessary ( when suspecting information-shortcut occurs ) . 2.We have conducted a detailed ablation study on RPDA in our original submission . Due to the page constraint , we include those results in the appendix . 3.We acknowledge that the presented experimented may seem contrived at first glance . However , we do believe that the provided MNIST relational learning examples represent novel and unique questions and clearly demonstrate the relational learning challenge that are not easily solvable with existing methods . In addition to the MNIST example presented in the main text , we include three additional MNIST relational learning example in the appendix each with increasing complexity ( coupled relational property , multiple relational property , and continuous relational property ) . We demonstrate the robustness of the proposed method by successfully achieving satisfactory results on all four problems with the exact same model setup . To further demonstrate the generalization ability of the proposed method on a more complicated problem , we presented a face relational learning example and successfully solved it with a very similar setup as in our MNSIT example where we only made necessary adjustment to the model setup to accommodate the different types of data ( e.g. , larger image and continuous vs binary data ) . Please see a detailed discussion in our response # 4 , # 5 to R2 . ( To be continued in the appending post )"}}