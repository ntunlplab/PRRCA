{"year": "2021", "forum": "3AOj0RCNC2", "title": "Gradient Projection Memory for Continual Learning", "decision": "Accept (Oral)", "meta_review": "The paper proposes a new approach to continual learning with known task boundaries that is scalable and highly performant, while preserving data privacy.  To mitigate forgetting the proposed approach restricts gradient updates to fall in the orthogonal direction to the gradient space that are important for the past tasks. The main novelty of the approach is to estimate these subspaces by analysing the activations for the inputs linked for each given task. \n\nAll reviewers give accepting scores. R2, R3 and R4 strongly recommend accepting the paper, while R1 considers it borderline.\n\nThe authors provided an extensive response carefully considering all reviewers' comments. New experiments were introduced (training time analysis and comparisons with expansion-based methods), and several clarifications were added.\n\nAll reviewers agree that the paper is well written and its literature review adequate.\n\nThe main concern of R1 was the similarities with OGD (Farajtabar et al. 2020). R1 considered the authors\u2019 response acceptable. R2, R3 and R4 consider the contribution well motivated and significant and highlight its simplicity. The AC agrees with this assessment.\n\nThe empirical evaluation covers most of the typical benchmarks in CL. Very strong results are reported on a variety of tasks both in terms of performance and memory efficiency, as agreed by R2, R3 and R4.\n\nOverall the paper makes a strong contribution to the field of CL.\n", "reviews": [{"review_id": "3AOj0RCNC2-0", "review_text": "This manuscript proposes a new approach for continual learning problems . The key idea is to maintain bases of subspaces using SVD in the Gradient Projection Memory ( GPM ) , in which the update direction is orthogonal to the gradient subspaces deemed important for the past tasks . Image classification experiment was conducted to justify its better empirical performance in practice . Overall , the paper is well-written . I have the following comments . 1.The contribution of this manuscript is kind of incremental compared with OGD ( Farajtabar et al.2020 ) .From my understanding , the main improvement is using SVD to store the bases , which basically trades computational efficiency for memory . In addition , it is claimed that OGD only works for small learning rate . Why using SVD helps using a larger learning rate ? It is not explained in the paper . 2.Is it possible to report running time of the proposed approach and compare with other approaches ? SVD is very expensive in the high-dimensional setting such as deep neural networks . It might be impractical due to running time concerns . 3.What is the value of $ k $ in the experiments ( in inequality ( 9 ) ) for different $ \\epsilon_ { th } ^l $ ? It is better to report these values . In addition , when $ k $ becomes large , the update rule ( 6 ) and ( 7 ) may become computationally expensive . 4.It is mentioned that \u201c our proposed approach can perform inference without the task identity of test samples \u201d , but with no explanations . It is desired to illustrate the meaning in the main paper . 5.The literature review is not sufficient . There are several recent papers which also change the update direction using gradient projection . It is desired to see the difference between the proposed method and other relevant approaches . For example , [ 1 ] . Yu et al.Gradient surgery for multi-task learning . NeurIPS 2020 . [ 2 ] .Guo et al.Improved schemes for episodic memory-based lifelong learning . NeurIPS 2020 . ==POST REBUTTAL I would like to thank the authors to answer my questions . It addressed most of my concerns . Hence I increase my score to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Reviewer \u2019 s Comment 3 : * * * '' What is the value of k in the experiments ( in inequality ( 9 ) ) for different \u03f5thl ? ... In addition , when k becomes large , the update rule ( 6 ) and ( 7 ) may become computationally expensive . `` . * \\ * * Response : * * Our reported GPM size ( in Figure.2 ) implicitly captures the cumulative value of $ k $ after all the tasks . Additionally , following reviewer \u2019 s suggestion , we have added the values of $ k $ for the PMNIST and 10-split CIFAR-100 experiments in Table 10 in the appendix of the revised manuscript . \\ In our implementation , we precomputed the projection matrices ( $ MM^T $ ) ( in equation ( 6,7 ) ) from GPM bases at the start of each task and reused the values during training . Moreover , these projection matrices are of fixed and reasonable dimensions ( see appendix section C.8 ) for efficient computation in GPUs . Therefore , we do not see significant time differences with increasing value of $ k $ during training with update rule ( 6,7 ) . * * Reviewer \u2019 s Comment 4 : * * * '' It is mentioned that \u201c our proposed approach can perform inference without the task identity of test samples \u201d , but with no explanations '' . * \\ * * Response : * * We thank the reviewer for pointing this out . In our PMNIST experiment , we perform task-hint free inference , whereas in all the other experimental settings ( following the setup in baselines ) we use task hint to select classifier head during inference . We made this statement to highlight the findings from the PMNIST experiment . However , we acknowledge that our statement gives a general feeling that in all the experiments we are using task-free inference , which is not the case . Hence , we remove these statements in our revised manuscript . We show in the PMNIST experiment section how our method performs task-hint free inference in that experimental/evaluation setup . * * Reviewer \u2019 s Comment 5 : * * * \u201c The literature review is not sufficient . Show the difference between the Proposed method and [ 1 ] , [ 2 ] . \u201d * \\ * * Response : * * Yu et al . ( 2020 ) [ 1 ] use gradient projection in \u2018 multitask learning \u2019 setting where data from all the tasks are available for necessary reference gradient generation during ( joint ) training . In contrast , we operate in \u2018 continual learning \u2019 setting where data from only one task is available at a time ( and GPM provides necessary direction for gradient projection ) . Guo et al . [ 2 ] proposes a unified view of episodic memory-based CL methods , including GEM and A-GEM and improves performance over these methods utilizing loss-balancing update rule . In contrast to this method , we do not store raw data in the memory thus provide data privacy . We have added this reference to the related works section in our revised manuscript . [ 1 ] Yu et al.Gradient surgery for multi-task learning . NeurIPS 2020.\\ [ 2 ] Guo et al.Improved schemes for episodic memory-based lifelong learning . NeurIPS 2020 ."}, {"review_id": "3AOj0RCNC2-1", "review_text": "Summary : This work targets learning multi-class classifiers in the continual learning setting . The key idea is to learn new tasks by taking gradient steps in directions orthogonal to the gradient subspaces marked as crucial for previous past tasks . The method employs SVD after learning each task to find the crucial subspaces ( which it calls as Core Gradient Subspaces ) and stores them in a memory . Reasonable quantitative and qualitative evaluation has been performed to compare the method against existing SOTA baselines . Review : 1 . The paper is overall clearly written and the method is adequately described . 2.The work is relevant to audience at ICLR and provides reasonable details for reproducibility . 3.Relationship to previous works has been explained and relevant literature has been cited appropriately . Strengths : 1 . GPM shows very little degradation in performance on past tasks and is very resilient to forgetting . 2.It provides explicit control over forgetting via the \\eps_ { th } parameter . Weaknesses : 1 . GPM performs fairly close to or marginally better than HAT and EWC on most datasets . 2.GPM trades-off between ACC and BWT metrics by strictly controlling forgetting . Hence , it may not be able to identify tasks with very similar structure ( or repeated tasks ) and may not re-use the Core Gradient Space from previous tasks to improve performance on them ( BWT ) . So , while it minimizes negative BWT , it also restricts positive BWT . Potential improvements and clarifications : 1 . The authors have repeatedly emphasized that their method performs task-free inference . However , does n't this just mean that the tasks used are such that the inputs encode the task identity and do not require a separate task identifier . Further , on page 6 , under Network Architectures , the authors state that apart from permuted MNIST tasks , they use the multi-head setting , i.e. , each task has a separate classifier . How does this allow task-identifier free inference ? 2.All models have been trained using plain SGD . Would it be possible to extend the method to other optimizers , e.g. , Adam ? 3.3 runs are too few to average over . These are supervised learning experiments , so it should definitely be possible to use more runs ( at least 5 , but preferably 10 ) . 4.At first glance , it seems that the method may have scalability issues since it relies on performing SVD . However figure 2 counter-intuitively shows GPM to be both fast and memory efficient . Is this because these graphs in figure 2 are per-epoch ? What about in between tasks when GPM needs to run SVD on all layers ? Some more explanation and results are required to better understand how GPM achieves computational efficiency despite relying heavily on SVD per layer after every task . 5.Lastly , no experiments with a single class per task have been performed . This setting is known to be quite challenging in general and induces significantly more catastrophic forgetting ( see Kamra et al , 2017 ) . In this setting it is also generally harder to just set batch-norm parameters using just the first task . [ Kamra et al , 2017 ] Deep Generative Dual Memory Network for Continual Learning . Nitin Kamra , Umang Gupta and Yan Liu . ArXiv 2017 . Please respond to the 5 potential improvements and clarifications mentioned above . I will be happy to raise the score further if the above concerns are addressed . -- Post-rebuttal edit - The authors have answered all my questions satisfactorily and provided additional experiments wherever it was required including settings where GPM may not outperform other baselines . I believe that the paper is strong and makes a significant technical contribution to the field of CL . Hence , I recommend acceptance and I am updating my score to reflect the same .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "* * Reviewer \u2019 s Comment 4 : * * * \u201c At first glance , it seems that the method may have scalability issues since it relies on performing SVD ... Some more explanation and results are required to better understand how GPM achieves computational efficiency despite relying heavily on SVD per layer after every task. \u201d * \\ * * Response : * * Time reported in Figure 2 ( b ) is per-epoch training time for all the algorithms . Figure 2 ( a , c , d , e ) shows relative memory usage in either raw data storage or gradient base storage for different memory-based methods after learning all the tasks . In the per-epoch time comparison , time spent in memory management/update for both our method and the baselines is not included . To address the reviewer \u2019 s concern regarding the scalability of our algorithm , we report the training time of our algorithm and compare it with the baselines in Table 2 of the revised manuscript . For PMNIST and CIFAR-100 our algorithm trains faster than all the other baselines while spending only 0.2 % , and 3 % of its total training time in GPM update ( using SVD ) respectively . Even in 5-dataset experiment , where ResNet18 is used , our algorithm runs faster than others using only 6 % of the training time in GPM update . \\ Although we perform SVD at each layer for GPM update , this is done only once per task . Our formulation uses GPM bases and projection matrices ( $ MM^T $ ) of reasonable dimensions ( see appendix section C.8 ) . Precomputation of these bases and the projection matrices at the start of each task leads to faster per-epoch training in GPU . This gain in time essentially compensates for the extra time required for GPM update through SVD . Therefore , our method despite relying on SVD computation is fast and scalable for deeper networks . We have added a summary of this discussion in the revised manuscript . * * Reviewer \u2019 s Comment 5 : * * * \u201c Lastly , no experiments with a single class per task have been performed\u2026 .. In this setting it is also generally harder to just set batch-norm parameters using just the first task. \u201d * \\ * * Response : * * The experiment suggested by the reviewer requires a different evaluation protocol than those used in our paper . This is an instance of class-incremental learning ( iCaRL , CVPR \u2019 17 ) where disjoint classes are learned one by one and classification is performed within all the learned classes without task-hint . This scenario is very challenging and often infeasible for the regularization ( HAT , EWC etc . ) and expansion-based ( DEN , APD etc . ) methods which do not use old data replay . Using the experimental setting from the paper [ 1 ] referred by the reviewer , we implemented the \u2018 Digit dataset \u2019 experiment where a single class of MNIST digit is learned per task . Results are the following : * * Result Format -- Method : ACC % ( BWT ) * * \\ EWC : 10 % ( -1.00 ) || DGR : 59.6 % ( -0.43 ) || DGDMN : 81.8 % ( -0.15 ) || GPM : 70.7 % ( -0.26 ) While EWC forgets catastrophically , we perform better than DGR [ 2 ] , which employs data replay through old data generation . DGDMN [ 1 ] , an improved data replay method , outperforms all . In this setup , we believe subset of old data replay from storage or via generation may be inevitable for attaining best performance with minimal forgetting ( iCaRL , [ 3 ] ) . In that quest , a hybrid approach such as combining GPM with small data replay would be an interesting direction for future exploration . [ 1 ] Deep Generative Dual Memory Network for Continual Learning . Nitin Kamra , Umang Gupta and Yan Liu . ArXiv 2017\\ [ 2 ] Shin , Hanul , Lee , Jung Kwon , Kim , Jaehong , and Kim , Jiwon . Continual learning with deep generative replay . In Advances in Neural Information Processing Systems , pp . 2994\u20133003 , 2017\\ [ 3 ] J. Rajasegaran , M. Hayat , S. H. Khan , F. S. Khan , and L. Shao . Random path selection for continual learning . In Advances in Neural Information Processing Systems , pages 12648\u201312658 , 2019"}, {"review_id": "3AOj0RCNC2-2", "review_text": "Summary : The paper proposes one of the most scalable approaches to sequential continual learning with known task boundaries and related tasks , while taking steps towards enforcing data privacy and removing some of the task label constraints . At all levels in expressive deep models , SVD is used on learned representations to identify important bases of task gradient spaces and a memory is populated with such directions . Learning progresses only in directions orthogonal to gradient memory . Several recent evaluation methodologies are used to empirically validate the approach with significant success . Strong points : - Principled and relatively simple approach , yet scalable to interesting deep models . - Manageable computational overhead . - Memory of gradients introduces a layer of data privacy , and advantage compared to many other memory-based approaches . - Strong empirical results , although not apples-to-apples in all cases , see the comment posted earlier . - Clearly written paper . - Analysis of scalability in terms of memory and computation is a welcome bonus . Weak points : - The sequential task learning setup is of limited practical use and not too realistic . - It \u2019 s hard to say how the algorithm would be used in practical continual learning settings , e.g.reinforcement learning of single tasks without forgetting , or where clear task boundaries do not exist . - Little effort is made to see the approach relevance for non-similar sequential task learning , where there could be interference between learned representations in terms of negative forward transfer . Recommendation and Rationale : I strongly recommend acceptance because the method is simple , practical and the paper is well written both in terms of clarity and also analysis . Further questions : - Do you see any positive forward transfer , e.g.later tasks are learned quicker due to previous tasks ? - What are the limitations and roadblocks to extending this method to sequences of hundreds of tasks which are not necessarily related ? Please discuss in the manuscript .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "# # # # # * * Clarification on multitask baseline * * * * Response : * * In Multitask baseline , all tasks are jointly learned ( in a multitask learning fashion ) using the entire dataset at once in a single network . HAT , A-GEM and ER-Res used this baseline to show the upper-bound on the average accuracy on all tasks . In APD , STL baseline is used to show such upper bound . STL is different from Multitask because in STL each task is independently learned in a separate network . Since , in our paper , for 10-split CIFAR-100 experiment we used a bigger architecture , AlexNet as opposed to APD \u2019 s LeNet-5 and trained with a different learning rate schedule , our average accuracy upper bound ( Multitask ) over 10-tasks is significantly higher than APD \u2019 s ( STL ) . We have clarified the definition of multitask baseline in the revised manuscript . # # # # # * * Discussion on Results * * * * Response : * * For training our method and the baselines we did not use any data augmentation . Thus , compared to APD \u2019 s 10-split CIFAR-100 experiments , the accuracy improvements in our CIFAR-100 experiments ( for both our methods and baselines ) come from training better network architecture ( AlexNet ) with higher learning rate schedules and optimum hyperparameters ( table 6 in GPM ) . Moreover , with direct comparison with APD ( in LeNet-5 ) we have shown that our algorithm induces more sharing and yields better performance than APD through efficient use of the GPM . [ 1 ] Scalable and Order-robust Continual Learning with Additive Parameter Decomposition . Jaehong Yoon , Saehoon Kim , Eunho Yang , Sung Ju Hwang.\\ [ 2 ] Continual Learning with Adaptive Weights ( CLAW ) . Tameem Adel , Han Zhao , Richard E. Turner.\\ [ 3 ] An Adaptive Random Path Selection Approach for Incremental Learning . Jathushan Rajasegaran , Munawar Hayat , Salman Khan , Fahad Shahbaz Khan , Ling Shao , Ming-Hsuan Yang ."}, {"review_id": "3AOj0RCNC2-3", "review_text": "I found the idea quite novel . Lately in continual learning the focus has been more on NAS type ideas and algorithms , but this work is a nice divergence from this direction . The idea of optimizing in a space orthogonal to the previous task is novel . The execution of the idea is nothing special since it 's using standard linear algebra , but I gave the authors full merit to the idea itself . My higher score is mostly due to the fact that the experiments are limited . The benchmark algorithms definitely miss some recent works from 2019 and 2020 . They should be included as otherwise the superior performance of the algorithms is questionable . See for example : https : //arxiv.org/abs/2006.04027 Ju Xu and Zhanxing Zhu . Reinforced continual learning . In Advances in Neural Information Processing Systems , pages 899\u2013908 , 2018", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the review and comments on our paper . We are happy to hear that he/she finds our idea interesting . We address the reviewer \u2019 s comment below : * * Reviewer \u2019 s Comment : * * * \u201c The benchmark algorithms definitely miss some recent works from 2019 and 2020 . They should be included as otherwise the superior performance of the algorithms is questionable. \u201d * \\ * * Response * * : In our paper , we have already compared our method with the state-of-the-art ( SOTA ) regularization and memory-based methods . As per reviewer \u2019 s suggestion , we have added a new experiment to compare with some of the representative works from network expansion-based continual learning methods which include Reinforced Continual Learning ( RCL ) [ 1 ] and recently proposed SOTA method \u2013 Adaptive Parameter Decomposition ( APD ) [ 2 ] . In this experiment , 20 sequential tasks are learned from CIFAR-100 Superclass dataset [ 2 ] , where each task contains 5 different but semantically related classes . Results are given in Table 3 of the revised manuscript with necessary explanations . Brief summary of the results is given below : * * Methods/Metric | ACC ( % ) | Network Capacity ( % ) * * \\ RCL | 51.99 % | 184 % \\ APD | 56.81 % | 130 % \\ GPM | 57.72 % | 100 % Our method ( GPM ) outperforms all the other methods while using significantly less network capacity . This implies GPM encourages higher sharing among tasks thus makes efficient usage of given network capacity . \\ Following suggestions from the reviewers , we have revised our manuscript , where changes are marked in blue . [ 1 ] Ju Xu and Zhanxing Zhu . Reinforced continual learning . In Advances in Neural Information Processing Systems , pages 899\u2013908 , 2018\\ [ 2 ] Jaehong Yoon , Saehoon Kim , Eunho Yang , Sung Ju Hwang . Scalable and Order-robust Continual Learning with Additive Parameter Decomposition . ICLR , 2020 ."}], "0": {"review_id": "3AOj0RCNC2-0", "review_text": "This manuscript proposes a new approach for continual learning problems . The key idea is to maintain bases of subspaces using SVD in the Gradient Projection Memory ( GPM ) , in which the update direction is orthogonal to the gradient subspaces deemed important for the past tasks . Image classification experiment was conducted to justify its better empirical performance in practice . Overall , the paper is well-written . I have the following comments . 1.The contribution of this manuscript is kind of incremental compared with OGD ( Farajtabar et al.2020 ) .From my understanding , the main improvement is using SVD to store the bases , which basically trades computational efficiency for memory . In addition , it is claimed that OGD only works for small learning rate . Why using SVD helps using a larger learning rate ? It is not explained in the paper . 2.Is it possible to report running time of the proposed approach and compare with other approaches ? SVD is very expensive in the high-dimensional setting such as deep neural networks . It might be impractical due to running time concerns . 3.What is the value of $ k $ in the experiments ( in inequality ( 9 ) ) for different $ \\epsilon_ { th } ^l $ ? It is better to report these values . In addition , when $ k $ becomes large , the update rule ( 6 ) and ( 7 ) may become computationally expensive . 4.It is mentioned that \u201c our proposed approach can perform inference without the task identity of test samples \u201d , but with no explanations . It is desired to illustrate the meaning in the main paper . 5.The literature review is not sufficient . There are several recent papers which also change the update direction using gradient projection . It is desired to see the difference between the proposed method and other relevant approaches . For example , [ 1 ] . Yu et al.Gradient surgery for multi-task learning . NeurIPS 2020 . [ 2 ] .Guo et al.Improved schemes for episodic memory-based lifelong learning . NeurIPS 2020 . ==POST REBUTTAL I would like to thank the authors to answer my questions . It addressed most of my concerns . Hence I increase my score to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Reviewer \u2019 s Comment 3 : * * * '' What is the value of k in the experiments ( in inequality ( 9 ) ) for different \u03f5thl ? ... In addition , when k becomes large , the update rule ( 6 ) and ( 7 ) may become computationally expensive . `` . * \\ * * Response : * * Our reported GPM size ( in Figure.2 ) implicitly captures the cumulative value of $ k $ after all the tasks . Additionally , following reviewer \u2019 s suggestion , we have added the values of $ k $ for the PMNIST and 10-split CIFAR-100 experiments in Table 10 in the appendix of the revised manuscript . \\ In our implementation , we precomputed the projection matrices ( $ MM^T $ ) ( in equation ( 6,7 ) ) from GPM bases at the start of each task and reused the values during training . Moreover , these projection matrices are of fixed and reasonable dimensions ( see appendix section C.8 ) for efficient computation in GPUs . Therefore , we do not see significant time differences with increasing value of $ k $ during training with update rule ( 6,7 ) . * * Reviewer \u2019 s Comment 4 : * * * '' It is mentioned that \u201c our proposed approach can perform inference without the task identity of test samples \u201d , but with no explanations '' . * \\ * * Response : * * We thank the reviewer for pointing this out . In our PMNIST experiment , we perform task-hint free inference , whereas in all the other experimental settings ( following the setup in baselines ) we use task hint to select classifier head during inference . We made this statement to highlight the findings from the PMNIST experiment . However , we acknowledge that our statement gives a general feeling that in all the experiments we are using task-free inference , which is not the case . Hence , we remove these statements in our revised manuscript . We show in the PMNIST experiment section how our method performs task-hint free inference in that experimental/evaluation setup . * * Reviewer \u2019 s Comment 5 : * * * \u201c The literature review is not sufficient . Show the difference between the Proposed method and [ 1 ] , [ 2 ] . \u201d * \\ * * Response : * * Yu et al . ( 2020 ) [ 1 ] use gradient projection in \u2018 multitask learning \u2019 setting where data from all the tasks are available for necessary reference gradient generation during ( joint ) training . In contrast , we operate in \u2018 continual learning \u2019 setting where data from only one task is available at a time ( and GPM provides necessary direction for gradient projection ) . Guo et al . [ 2 ] proposes a unified view of episodic memory-based CL methods , including GEM and A-GEM and improves performance over these methods utilizing loss-balancing update rule . In contrast to this method , we do not store raw data in the memory thus provide data privacy . We have added this reference to the related works section in our revised manuscript . [ 1 ] Yu et al.Gradient surgery for multi-task learning . NeurIPS 2020.\\ [ 2 ] Guo et al.Improved schemes for episodic memory-based lifelong learning . NeurIPS 2020 ."}, "1": {"review_id": "3AOj0RCNC2-1", "review_text": "Summary : This work targets learning multi-class classifiers in the continual learning setting . The key idea is to learn new tasks by taking gradient steps in directions orthogonal to the gradient subspaces marked as crucial for previous past tasks . The method employs SVD after learning each task to find the crucial subspaces ( which it calls as Core Gradient Subspaces ) and stores them in a memory . Reasonable quantitative and qualitative evaluation has been performed to compare the method against existing SOTA baselines . Review : 1 . The paper is overall clearly written and the method is adequately described . 2.The work is relevant to audience at ICLR and provides reasonable details for reproducibility . 3.Relationship to previous works has been explained and relevant literature has been cited appropriately . Strengths : 1 . GPM shows very little degradation in performance on past tasks and is very resilient to forgetting . 2.It provides explicit control over forgetting via the \\eps_ { th } parameter . Weaknesses : 1 . GPM performs fairly close to or marginally better than HAT and EWC on most datasets . 2.GPM trades-off between ACC and BWT metrics by strictly controlling forgetting . Hence , it may not be able to identify tasks with very similar structure ( or repeated tasks ) and may not re-use the Core Gradient Space from previous tasks to improve performance on them ( BWT ) . So , while it minimizes negative BWT , it also restricts positive BWT . Potential improvements and clarifications : 1 . The authors have repeatedly emphasized that their method performs task-free inference . However , does n't this just mean that the tasks used are such that the inputs encode the task identity and do not require a separate task identifier . Further , on page 6 , under Network Architectures , the authors state that apart from permuted MNIST tasks , they use the multi-head setting , i.e. , each task has a separate classifier . How does this allow task-identifier free inference ? 2.All models have been trained using plain SGD . Would it be possible to extend the method to other optimizers , e.g. , Adam ? 3.3 runs are too few to average over . These are supervised learning experiments , so it should definitely be possible to use more runs ( at least 5 , but preferably 10 ) . 4.At first glance , it seems that the method may have scalability issues since it relies on performing SVD . However figure 2 counter-intuitively shows GPM to be both fast and memory efficient . Is this because these graphs in figure 2 are per-epoch ? What about in between tasks when GPM needs to run SVD on all layers ? Some more explanation and results are required to better understand how GPM achieves computational efficiency despite relying heavily on SVD per layer after every task . 5.Lastly , no experiments with a single class per task have been performed . This setting is known to be quite challenging in general and induces significantly more catastrophic forgetting ( see Kamra et al , 2017 ) . In this setting it is also generally harder to just set batch-norm parameters using just the first task . [ Kamra et al , 2017 ] Deep Generative Dual Memory Network for Continual Learning . Nitin Kamra , Umang Gupta and Yan Liu . ArXiv 2017 . Please respond to the 5 potential improvements and clarifications mentioned above . I will be happy to raise the score further if the above concerns are addressed . -- Post-rebuttal edit - The authors have answered all my questions satisfactorily and provided additional experiments wherever it was required including settings where GPM may not outperform other baselines . I believe that the paper is strong and makes a significant technical contribution to the field of CL . Hence , I recommend acceptance and I am updating my score to reflect the same .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "* * Reviewer \u2019 s Comment 4 : * * * \u201c At first glance , it seems that the method may have scalability issues since it relies on performing SVD ... Some more explanation and results are required to better understand how GPM achieves computational efficiency despite relying heavily on SVD per layer after every task. \u201d * \\ * * Response : * * Time reported in Figure 2 ( b ) is per-epoch training time for all the algorithms . Figure 2 ( a , c , d , e ) shows relative memory usage in either raw data storage or gradient base storage for different memory-based methods after learning all the tasks . In the per-epoch time comparison , time spent in memory management/update for both our method and the baselines is not included . To address the reviewer \u2019 s concern regarding the scalability of our algorithm , we report the training time of our algorithm and compare it with the baselines in Table 2 of the revised manuscript . For PMNIST and CIFAR-100 our algorithm trains faster than all the other baselines while spending only 0.2 % , and 3 % of its total training time in GPM update ( using SVD ) respectively . Even in 5-dataset experiment , where ResNet18 is used , our algorithm runs faster than others using only 6 % of the training time in GPM update . \\ Although we perform SVD at each layer for GPM update , this is done only once per task . Our formulation uses GPM bases and projection matrices ( $ MM^T $ ) of reasonable dimensions ( see appendix section C.8 ) . Precomputation of these bases and the projection matrices at the start of each task leads to faster per-epoch training in GPU . This gain in time essentially compensates for the extra time required for GPM update through SVD . Therefore , our method despite relying on SVD computation is fast and scalable for deeper networks . We have added a summary of this discussion in the revised manuscript . * * Reviewer \u2019 s Comment 5 : * * * \u201c Lastly , no experiments with a single class per task have been performed\u2026 .. In this setting it is also generally harder to just set batch-norm parameters using just the first task. \u201d * \\ * * Response : * * The experiment suggested by the reviewer requires a different evaluation protocol than those used in our paper . This is an instance of class-incremental learning ( iCaRL , CVPR \u2019 17 ) where disjoint classes are learned one by one and classification is performed within all the learned classes without task-hint . This scenario is very challenging and often infeasible for the regularization ( HAT , EWC etc . ) and expansion-based ( DEN , APD etc . ) methods which do not use old data replay . Using the experimental setting from the paper [ 1 ] referred by the reviewer , we implemented the \u2018 Digit dataset \u2019 experiment where a single class of MNIST digit is learned per task . Results are the following : * * Result Format -- Method : ACC % ( BWT ) * * \\ EWC : 10 % ( -1.00 ) || DGR : 59.6 % ( -0.43 ) || DGDMN : 81.8 % ( -0.15 ) || GPM : 70.7 % ( -0.26 ) While EWC forgets catastrophically , we perform better than DGR [ 2 ] , which employs data replay through old data generation . DGDMN [ 1 ] , an improved data replay method , outperforms all . In this setup , we believe subset of old data replay from storage or via generation may be inevitable for attaining best performance with minimal forgetting ( iCaRL , [ 3 ] ) . In that quest , a hybrid approach such as combining GPM with small data replay would be an interesting direction for future exploration . [ 1 ] Deep Generative Dual Memory Network for Continual Learning . Nitin Kamra , Umang Gupta and Yan Liu . ArXiv 2017\\ [ 2 ] Shin , Hanul , Lee , Jung Kwon , Kim , Jaehong , and Kim , Jiwon . Continual learning with deep generative replay . In Advances in Neural Information Processing Systems , pp . 2994\u20133003 , 2017\\ [ 3 ] J. Rajasegaran , M. Hayat , S. H. Khan , F. S. Khan , and L. Shao . Random path selection for continual learning . In Advances in Neural Information Processing Systems , pages 12648\u201312658 , 2019"}, "2": {"review_id": "3AOj0RCNC2-2", "review_text": "Summary : The paper proposes one of the most scalable approaches to sequential continual learning with known task boundaries and related tasks , while taking steps towards enforcing data privacy and removing some of the task label constraints . At all levels in expressive deep models , SVD is used on learned representations to identify important bases of task gradient spaces and a memory is populated with such directions . Learning progresses only in directions orthogonal to gradient memory . Several recent evaluation methodologies are used to empirically validate the approach with significant success . Strong points : - Principled and relatively simple approach , yet scalable to interesting deep models . - Manageable computational overhead . - Memory of gradients introduces a layer of data privacy , and advantage compared to many other memory-based approaches . - Strong empirical results , although not apples-to-apples in all cases , see the comment posted earlier . - Clearly written paper . - Analysis of scalability in terms of memory and computation is a welcome bonus . Weak points : - The sequential task learning setup is of limited practical use and not too realistic . - It \u2019 s hard to say how the algorithm would be used in practical continual learning settings , e.g.reinforcement learning of single tasks without forgetting , or where clear task boundaries do not exist . - Little effort is made to see the approach relevance for non-similar sequential task learning , where there could be interference between learned representations in terms of negative forward transfer . Recommendation and Rationale : I strongly recommend acceptance because the method is simple , practical and the paper is well written both in terms of clarity and also analysis . Further questions : - Do you see any positive forward transfer , e.g.later tasks are learned quicker due to previous tasks ? - What are the limitations and roadblocks to extending this method to sequences of hundreds of tasks which are not necessarily related ? Please discuss in the manuscript .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "# # # # # * * Clarification on multitask baseline * * * * Response : * * In Multitask baseline , all tasks are jointly learned ( in a multitask learning fashion ) using the entire dataset at once in a single network . HAT , A-GEM and ER-Res used this baseline to show the upper-bound on the average accuracy on all tasks . In APD , STL baseline is used to show such upper bound . STL is different from Multitask because in STL each task is independently learned in a separate network . Since , in our paper , for 10-split CIFAR-100 experiment we used a bigger architecture , AlexNet as opposed to APD \u2019 s LeNet-5 and trained with a different learning rate schedule , our average accuracy upper bound ( Multitask ) over 10-tasks is significantly higher than APD \u2019 s ( STL ) . We have clarified the definition of multitask baseline in the revised manuscript . # # # # # * * Discussion on Results * * * * Response : * * For training our method and the baselines we did not use any data augmentation . Thus , compared to APD \u2019 s 10-split CIFAR-100 experiments , the accuracy improvements in our CIFAR-100 experiments ( for both our methods and baselines ) come from training better network architecture ( AlexNet ) with higher learning rate schedules and optimum hyperparameters ( table 6 in GPM ) . Moreover , with direct comparison with APD ( in LeNet-5 ) we have shown that our algorithm induces more sharing and yields better performance than APD through efficient use of the GPM . [ 1 ] Scalable and Order-robust Continual Learning with Additive Parameter Decomposition . Jaehong Yoon , Saehoon Kim , Eunho Yang , Sung Ju Hwang.\\ [ 2 ] Continual Learning with Adaptive Weights ( CLAW ) . Tameem Adel , Han Zhao , Richard E. Turner.\\ [ 3 ] An Adaptive Random Path Selection Approach for Incremental Learning . Jathushan Rajasegaran , Munawar Hayat , Salman Khan , Fahad Shahbaz Khan , Ling Shao , Ming-Hsuan Yang ."}, "3": {"review_id": "3AOj0RCNC2-3", "review_text": "I found the idea quite novel . Lately in continual learning the focus has been more on NAS type ideas and algorithms , but this work is a nice divergence from this direction . The idea of optimizing in a space orthogonal to the previous task is novel . The execution of the idea is nothing special since it 's using standard linear algebra , but I gave the authors full merit to the idea itself . My higher score is mostly due to the fact that the experiments are limited . The benchmark algorithms definitely miss some recent works from 2019 and 2020 . They should be included as otherwise the superior performance of the algorithms is questionable . See for example : https : //arxiv.org/abs/2006.04027 Ju Xu and Zhanxing Zhu . Reinforced continual learning . In Advances in Neural Information Processing Systems , pages 899\u2013908 , 2018", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the review and comments on our paper . We are happy to hear that he/she finds our idea interesting . We address the reviewer \u2019 s comment below : * * Reviewer \u2019 s Comment : * * * \u201c The benchmark algorithms definitely miss some recent works from 2019 and 2020 . They should be included as otherwise the superior performance of the algorithms is questionable. \u201d * \\ * * Response * * : In our paper , we have already compared our method with the state-of-the-art ( SOTA ) regularization and memory-based methods . As per reviewer \u2019 s suggestion , we have added a new experiment to compare with some of the representative works from network expansion-based continual learning methods which include Reinforced Continual Learning ( RCL ) [ 1 ] and recently proposed SOTA method \u2013 Adaptive Parameter Decomposition ( APD ) [ 2 ] . In this experiment , 20 sequential tasks are learned from CIFAR-100 Superclass dataset [ 2 ] , where each task contains 5 different but semantically related classes . Results are given in Table 3 of the revised manuscript with necessary explanations . Brief summary of the results is given below : * * Methods/Metric | ACC ( % ) | Network Capacity ( % ) * * \\ RCL | 51.99 % | 184 % \\ APD | 56.81 % | 130 % \\ GPM | 57.72 % | 100 % Our method ( GPM ) outperforms all the other methods while using significantly less network capacity . This implies GPM encourages higher sharing among tasks thus makes efficient usage of given network capacity . \\ Following suggestions from the reviewers , we have revised our manuscript , where changes are marked in blue . [ 1 ] Ju Xu and Zhanxing Zhu . Reinforced continual learning . In Advances in Neural Information Processing Systems , pages 899\u2013908 , 2018\\ [ 2 ] Jaehong Yoon , Saehoon Kim , Eunho Yang , Sung Ju Hwang . Scalable and Order-robust Continual Learning with Additive Parameter Decomposition . ICLR , 2020 ."}}