{"year": "2021", "forum": "j9Rv7qdXjd", "title": "Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels", "decision": "Accept (Poster)", "meta_review": "Most reviewers found the method proposed to be technically sound, well-motivated and particularly interesting due to the interpretability of its results. Indeed, the extraction of interpretable motifs from NAS is a valuable contribution. One of the reviewers was particularly concerned by the lack of guarantees of the proposed method and a perceived failure mode of averaged gradients. We thank both the reviewer and the authors for the detailed discussion on these points. Ultimately, the benefits of the method proposed and the magnitude of the contributions in the paper outweigh these concerns.", "reviews": [{"review_id": "j9Rv7qdXjd-0", "review_text": "- After feedback - First of all , I greatly appreciate the authors patient response to me during the feedback period . The discussion was really fruitful . Unfortunately , I still have a concern about interpretability of the proposed method , which is a central topic in the paper . > First , we find it a bit strange when the reviewer says \u201c it is difficult to find importance/meaning of comparing motifs is unclear \u201d , we clearly show our method does find importance in NAS-Bench-101 , all 3 tasks of NAS-Bench-201 and DARTS search space ( Fig 1 and 7 ) -- if we ca n't find importance/distinguish different motifs , none of the results we 've shown would 've been possible . Even when a method works empirically , if a rationale behind the procedure is not clarified , a paper would not be scientifically convincing . Thus , I still do not think my claim is strange . > Second , the example the reviewer gives is not a case when averaged gradient fails . On the contrary , it is exactly an example of when averaged gradient works . A motif with high and diverse local gradient magnitudes but average to near-0 is not important for the purpose of interpretability , as it doesn \u2019 t consistently explain the network performance by itself ( just based on such motifs , one can not conclusively deduce the impact on performance of an arbitrary , unseen architecture in general ) ... In the last response , the authors explained the interpretability issue through combination of motifs , but it did not resolve my concern . To simplify the discussion , consider a bit extreme case in which only one motif is employed in a network simultaneously , and assume WL parameter h = 0 . Let g ( c ) = d \\mu / d \\phi^j |_\\phi^j=c . Then , consider a hypothetical case as follows : motif a ) g ( 1 ) = 10 , g ( 2 ) = 10 ... g ( 10 ) = 10 , g ( 11 ) = -10 , .... g ( 20 ) = -10 : AG = 0 motif b ) g ( 1 ) = 1 , g ( 2 ) = 1 ... g ( 10 ) = 1 , g ( 11 ) = 1 , .... g ( 20 ) = 1 : AG = 20 In this example , b ) has a larger AG , but a ) can have larger importance in practice , and now , since only one kind of motif is employed simultaneously , the explanation of the authors can not be applied . For the exploration purpose , I do not find any rationale to consider that b ) is more important than a ) . I know that these are extreme examples and may depend on an application scenario , but my point is that these examples reveal difficulty of interpretation of AG . The authors introduce AG at Section3.2 as an importance measure without carefully discussing how it can be interpreted in the context of the WL based exploration ( just referring other papers without discussing details in a sense of the above averaging ) . The explanation through marginalization also does not get rid of this question . Since the interpretability is a main theme of the paper , providing a better interpretability of AG would be desired . - Before feedback - The paper proposes to use Weisfeiler-Lehman ( WL ) kernels for neural architecture search . WL kernel can incorporate the topological structure of the network , and the authors combines WL kernels with Bayesian optimization ( BO ) to optimize the validation performance of the network . Further , the authors also claim that WL kernels provide a useful interpretation about good / bad network structures by using the derivative of Gaussian process ( GP ) , which can also be used for 'pruning ' architectures . The performance is shown for several benchmark datasets . Overall , the idea would be reasonable , and the approach would be useful . However , I 'd have to say that the technical novelty and depth would be somewhat weak because the standard WL kernel is directly used without any significant modification , and a gradient-based importance evaluation is also a known technique ( and its interpretation in this context is a bit difficult ) . Further , in my understanding , the paper should have provided more general discussions , not only to show data-specific observations . Detailed comments are as follows . The proposal of the paper is not fully clear for me because the strategies are described for each one of datasets , separately . I could n't find general procedures for the architecture search , from the main text of the paper . In practice , of-course , tuning on each dataset would be required , but showing specific tunings for well-known benchmark datasets is not attractive . A strategy applicable to wide range of tasks would be required for a methodology paper . Interpretation of the gradient-based motif identification is difficult for me . Even when a motif has a large positive or negative gradient value , it only implies 'local ' importance around the given architecture . To derive general insight , more careful treatment would be required . The authors provide the motif discovery procedure in D.1 , but it should be shown in main text because interpretability is one of main theme of this paper . In Section D.1 , the authors described an approach taking average of all possible values , but the average is also difficult to interpret importance because it compresses the entire space , and as the author admitted , the computation would be often intractable in practical settings . Although the authors claim that good/bad motif identification is useful for 'pruning ' , no detailed general pruning procedure is shown in the main text . Providing a general algorithm would be required . What does 'prune ' mean in this context ? If a motif is regarded as 'bad ' once , it is discarded forever , or can revive somewhere ? As I mentioned above , gradient information is only local information . Even when a motif is 'bad ' , simply discarding it completely would be risky . Even when the 'average ' gradient is used , the problem would not be mitigated , because even if the average gradient suggests a motif is useless , it may help to improve accuracy locally . For me , the rationale behind the exploration strategy with the pruning in the paper is quite unclear . The authors claim that the identified motif is trasferable . However , evidence of this claim is not fully clear . It seems empirical suggestions only from a few ( similar ) datasets . When is transferring effective , how do you know it holds when , and can it harm in some case ? I think that a general discussion is missing in the paper . Another difficulty of the gradient-based importance evaluation is that the lack of uncertainty evaluation . The gradient ( 3.2 ) is the expected value of the predictive distribution of GP . Therefore , the variance is not considered . For example , if GP does not have any observations , the expected gradient would be 0 ( when prior is f ( x ) = 0 with the unit variance for any x , which is a standard setting ) , but variance of gradient would be large , meaning that a motif is still has a potential to become important . Again , discarding a motif by the expected gradient without considering uncertainty is seemingly risky , though the paper lacks this kind of discussion on uncertainty , though the uncertainty evaluation is a central issue on in the context of BO .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Many thanks for your constructive and insightful feedback ! We have incorporated many of your suggestions in the updated paper , and please see below for our detailed response and we hope that would resolve any concerns you had : 1 . * * the gradient of a motif only implies 'local ' importance around a given architecture ... average is also difficult to interpret because it compresses the entire space and the computation would be often intractable ... * * We agree that the procedure in App . D.1 is an important part of the paper , and have moved it to Sec.3.2 now . [ Local vs Averaged Gradients ] The local gradient describes the local effect of a motif and is specific to a particular architecture . By taking the average of local gradients of a motif , we marginalise over all architectures containing the motif and obtain a global measure of importance regarding the motif feature . In fact , the integration or averaging of some sort over local gradients , as we did , is also an approach taken in many principled gradient-based global attribution methods , such as Occlusion-1 [ 1 ] and Integrated Gradient [ 2 ] , to analyse the contribution of input features to the model prediction . To the best of our knowledge , we are the first in introducing this technique in combination with interpretable features to explain the NAS performance . [ Tractability of Averaging ] Computing motifs is actually tractable in a practical setting \u2013 as this only involves averaging over all training/queried data , and the number of queries tend to be small under the BO , and especially the NAS setting . If the reviewer is referring to our comment in App . D.1 on intractability of sampling in DARTS search space , we would like to clarify that this intractability is about * * assessing * * the usefulness of the motifs on a held-out validation set of 1,000 architectures sampled , which would require us to fully train and evaluate all these 1,000 architectures . NAS-Bench datasets already did this expensive evaluation for us so we can do such assessment instantly . Note the gradient computation and motifs identification only depend on the surrogate model and don \u2019 t * * need * * additional expensive and extensive architecture evaluations mentioned above . Furthermore , on an empirical note , we have shown quantitatively that we can obtain valuable information on the network performance by simply analysing the motifs contained without the need to actually evaluating the networks ( Fig 1 ) , and qualitatively our approach also sheds lights into the existence of sub-structures which correlate with good network performance and are implicitly picked up by various search strategies in previous works ( Fig.2 ) .We hope these empirical evidence also supports the effectiveness of our proposed motif assessment criterion . [ 1 ] Ancona , M. , Ceolini , E. , \u00d6ztireli , C. and Gross , M. , 2018 . Towards better understanding of gradient-based attribution methods for deep neural networks . In International Conference on Learning Representations . [ 2 ] Sundararajan , M. , Taly , A. and Yan , Q. , 2017 . Axiomatic attribution for deep networks . International Conference on Machine Learning , PMLR 70 . 2 . * * the general pruning procedure is missing in the main text . What does 'prune ' mean in this context ? If a motif is regarded as 'bad ' once , it is discarded forever , or can revive somewhere ? * * We have added the algorithm for pruning in Algorithm 1 . The number of possible architectures contained in the search space is often huge . Pruning here refers to defining and applying the constraints on such a large search space to obtain a much smaller , yet promising , pool of candidate architectures , thus speeding up the search . In our case , we transfer motifs learnt from a previous , related task as well as the new target task for this purpose : at each BO iteration , we generate a large set of candidate architectures , and discard those without at least one favorable motif . As such , the BO agent now only needs to select from a smaller subset of architectures deemed more promising , thereby \u201c warm starting \u201d the search on a new task . [ Discarding bad motifs is risky ] Ultimately , the trade-off here is whether the efficiency gained from constraining the search space justifies the risk of missing out better solutions in the discarded/unexplored region . We argue that it is the case in NAS where the search space is immense ( DARTS [ 3 ] search space with only 4 operation blocks features 4.4e12 combinations ) and each query is computationally expensive . Furthermore , experimentally , we show in Fig 1 and 7 that across various search spaces , at least in the experiments we conducted the \u201c good \u201d yet smaller subsets still contain the best candidates among those randomly sampled , and in Fig 4 , the final solutions with pruning are not worse than those without . This supports the informativeness of our learnt motifs and we hope this also alleviates the concern about the risk of pre-emptively discarding architectures based on motifs . ( Continued below )"}, {"review_id": "j9Rv7qdXjd-1", "review_text": "Summary == This work proposed a BO based NAS method using Weisfeiler-Lehman kernel . The idea is novel and natural considering the neural network architectures as acyclic directed graphs . I am a bit surprised to see no one tried it before in the NAS field and it is great to know that using WL kernel leads to competitive NAS performance comparing to other NAS methods and at the same time improves interpretability . Pros = * The proposed idea is novel and natural , given the graph natures of network architecture . * The notes on the interpretability is very interesting and differ the work from other methods . * Extensive empirical studies and ablation studies . * Extensive detail for reproducibility . * The paper is very well written . Minor comments I think this is a really nice work and I only have some minor comments : * There is another line of work using BO for NAS : Ru , Binxin , Pedro Esperanca , and Fabio Carlucci . `` Neural Architecture Generator Optimization . '' arXiv preprint arXiv:2004.01395 ( 2020 ) . Would be nice to know how does the proposed method compared to it . * The appendix C mentioned about using MKL to combine WL and MLP kernels . But in the end the author used 0.7 and 0.3 as the weights for them . I am wondering whether some simple MKL algorithm such as ALIGNF can improve the performance here . You can find more detail in this paper : Cortes C , Mohri M , Rostamizadeh A. Algorithms for learning kernels based on centered alignment [ J ] . The Journal of Machine Learning Research , 2012 , 13 ( 1 ) : 795-828 . Reason for score == I liked this work a lot , it bridged NAS and BO through the usage of graph kernels ( WL kernel ) . As a result , NAS becomes more sample efficient , which is empirically verified by extensive study in this work . The author did a very good job on the empirical evaluations , they are thorough , solid and contains many ablation studies to understand their methods . Post rebuttal comments = I thank the authors for their responses . I encourage the authors to continue the line of work on replacing the random sampling of NAGO . Given NAS-BOWL surrogate , one can do Thompson Sampling instead of random sampling . On the MKL side , the same weights for all the kernels might be the cause of worse performance . I would also encourage the authors to verify that . Nevertheless , those are minor comments and I still think this is an important work to bridge BO and NAS . I will keep my score .", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Many thanks for your positive and constructive review ! Please see below for our response to your comments : 1 . * * How does the proposed method compare to another BO-for-NAS work [ 1 ] ? * * We thank the reviewer for the suggestion . Neural Architecture Generator Optimization ( NAGO ) [ 1 ] is an interesting and relevant work but is orthogonal to our method . The key contribution of NAGO is a novel graph-generator-based * * search space * * which is motivated by recasting NAS as searching for an optimal architecture generator/distribution instead of an optimal architecture . On the other hand , our work proposes a new * * search strategy * * that can be applied to a wide variety of search spaces including that of NAGO . In fact , we \u2019 ve shown in the paper that our GPWL surrogate performs very well on the randomly wired networks [ 2 ] , which inspires NAGO and has very similar architectures as those in NAGO [ 2 ] . Note that we haven \u2019 t tested our method on NAGO search space because their code is not released yet . More interestingly , we believe our method can be applied to improve NAGO : NAGO returns an optimal architecture generator but resort to random sampling to obtain the exact architecture for final use . Our NAS-BOWL can be used in place of random sampling to extract the best architecture from the good generator ; One potential way is simply to use the good architecture generator learnt by NAGO to generate candidate architectures for optimising the acquisition functions in NAS-BOWL instead of using the mutation scheme . This would be an interesting future work . [ 1 ] Ru , B. , Esperanca , P. and Carlucci , F. , 2020 . Neural Architecture Generator Optimization.Advances in Neural Information Processing Systems , 33 . [ 2 ] Xie , S. , Kirillov , A. , Girshick , R. and He , K. , 2019 . Exploring randomly wired neural networks for image recognition . In Proceedings of the IEEE International Conference on Computer Vision ( pp.1284-1293 ) . 2 . * * Suggestion on MKL algorithm such as ALIGNF * * Many thanks for the suggestion . We have added your comment in the discussion of App . C , as a more concrete future direction . As we said , while it seems that in the present setup , using additive kernels does not improve the predictive performance by much ( and MLP is quite expensive to compute ) , we believe leveraging on MKL literature might deliver more significant performance boosts in larger , more complicated search spaces ."}, {"review_id": "j9Rv7qdXjd-2", "review_text": "# Summary of the paper The paper presents a new Bayesian optimization strategy based on Weisfeiler-Lehman Kernels for neural architecture search . The proposed method is more sample efficient than other state-of-the-art Bayesian optimization ( BO ) methods and allows to identify repeating motifs in well performing architectures . Overall , I really enjoyed reading the paper and I am somewhat surprised that nobody has tried this before . The usage of the Weisfeiler-Lehman kernel is well motivated and enables users to obtain a better intuition which parts of an architecture lead to a good performance . Also , the proposed BO method outperforms other state-of-the-art BO methods across a range of competitive benchmarks . However a few points need to be clarified , and it would be great if the authors could address them in the rebuttal . # Merits - The paper is clearly written and the approach is well motivated - The proposed method achieves strong results compared to other Bayesian optimization strategies based on Gaussian process surrogate models . Also , the paper presents a thorough empirical evaluation to other state-of-the-art BO method on well established benchmarks - As far as I know , this is the first Bayesian optimization for NAS that provides interpretable features to explain which motifs of neural networks architectures work well . # Concerns - Looking at Figure 12 in the appendix , it seems that the proposed method GPWL gets its main boost from the mutation strategy used to optimize the acquisition function . This makes me wonder whether the model is actually better than , for example BANANAS , or whether the gain in performance is mostly due to the mutation strategy ? How well would BANANAS works with this mutation strategy ? - Could you also add the plots in Figure 3 with the regret on the y-axis ( as in the original papers ) ? This would show how far away from the optimum an optimizer actually is . This is somewhat hidden with ranking plots , where an optimizer might have found an architecture with a negligible performance difference to the global optimum but has a lower rank . - Why does start GPWL at a higher rank than the other baselines in Figure 3 ? And why does it stop earlier ? Also the dashed red horizontal line is not explained in the caption . # Post Rebuttal : I thank the authors for taking the time to address my concerns . The paper is well written and the proposed approached is promising . I therefor recommend acceptance .", "rating": "7: Good paper, accept", "reply_text": "Many thanks for your positive and constructive review ! Please see below for our response to your comments , and we hope that resolves any concerns you had : 1 . * * Whether the model is better than BANANAS \u2019 s\u2026 performance gain is due to the mutation strategy ? Compare with BANANAS with mutation strategy ... * * We have indeed compared with BANANAS with mutation strategy ( dark blue triangle , BANANASm ) on NAS-Bench-101 in both Fig.4 and 10 and show that our NAS-BOWL with mutation ( NASBOWLm ) outperforms that ; similarly , NAS-BOWL with random strategy ( NASBOWLr ) also outperforms BANANAS with random ( BANANASr ) in the same Figs . Moreover , in our surrogate experiments in Fig , 3 , we also include a baseline which combines the path-encoding scheme in BANANAS , its key contribution , with GP and shows that our GPWL surrogate achieves better predictive performance with less than \u2153 of the training data and thus is more query efficient . 2 . * * GPWL starts at a higher rank than the other baselines in Fig.3 ? And why does it stop earlier ? Dashed red horizontal line is not explained in the caption * * . For all surrogate models , we start with 10 initial training data ( it might be a bit difficult to discern from the figure but the lines start at 10 not 0 ) .The high rank prediction achieved by GPWL at the start shows that GPWL can well predict the performance ranking of validation architectures after training on only 10 initial architecture data . This shows exactly the query efficiency and competitive predictive power of our GPWL which are highly valued for the real-world NAS applications . Following the practice in [ 1 ] for comparing query efficiency across different baselines , we run the other baselines for more training data than our GPWL in order to see whether they can catch up the GPWL \u2019 s performance with more training data . E.g.on NAS-Bench-201 ( CIFAR100 ) task , Path-encoding and GNN manage to achieve the similar performance ( GPWL at 100 training data ) with 300 training data . This again demonstrates the query efficiency of our surrogate model and it \u2019 s evident from Fig.3 that such gain in query efficiency is larger for larger search space ( NAS-Bench-101 and Flowers102 ) . The dashed red horizontal lines are there to help comparison between the rank correlation performance of the GPWL with other baselines . [ 1 ] Ru , B. , Cobb , A. , Blaas , A. and Gal , Y. , 2020 . Bayesopt adversarial attack . In International Conference on Learning Representations . 3 . * * Add the regret on the y-axis in Fig.3 ( rank correlation plots ) to show how far away from the optimum * * To clarify , Fig.3 shows the * * prediction/regression * * not * * optimisation * * performance of different surrogate models over increasing number of randomly sampled training points . We assess the predictive performance using Spearman \u2019 s rank correlation because relative performance ranking is important for comparing architectures in NAS . Fig.8 in the App . also shows that our GPWL surrogate can predict the exact validation error very well , especially in the low error regime in which we are more interested . To better assess the regret , we have added in a dashed line indicating the global optimum for different NAS-Bench datasets ( close-domain search spaces ) for Fig.4.If needed , we can change the y-axis from error to regret to better illustrate that ."}, {"review_id": "j9Rv7qdXjd-3", "review_text": "The authors propose a new neural architecture search algorithm combining Bayesian optimization with the expressive and popular Weisfeiler-Lehman ( WL ) Graph Kernel . One advantage of using WL is the interpretable results that stem from the nature of how the kernel is computed , namely a propagation scheme through the graph . Combined the derivative of Eq.3.2 , one can extract subgraphs that are directly responsible for increased performance . In a variety of experiments , the authors show not only increased performance of detected architectures but also find subgraphs that are found by other algorithms as well . Even though my expertise does not lie in the field of NAS , I find this work quite appealing . It is an innovative application for graph kernels , which suffer from scalability which in this setting is less of a problem . I find the aspects of novelty , interpretability , and quantitative results convincing enough to recommend acceptance . Furthermore , the work is largely well structured and written , and the figures are legible and relevant . W.r.t whether the comparison to other SOTA NAS algorithms is of good quality and fair , I think the input from reviewers with a NAS background would be valuable . Minor comments : \u2022 By itself a graph kernel is a similarity measure and does not perform any subgraph selection . It happens that due to the WL propagation scheme , the WL graph kernel consists of interpretable features while computing the similarity . I would clarify this a little more in 3.1 . Since this is , in my opinion , the most innovative part of the manuscript , I would even consider bringing Figure 5 from the appendix into the main paper ( maybe in a condensed form ) . To get some space for this you could shorten some descriptions of experiment parameters and comparison methods to the appendix . Also , I think Figure 5 is not 100 % complete . To make it even easier to parse , I would put boxes around the subgraphs in the $ h=1 features $ box and annotate them with their respective index . E.g.the upper graph with 4 outgoing edges should be annotated with 5 and so on . This also makes it a little clearer how WL leads to larger networks in each round of propagation . \u2022 I would be interested in some statistics about the chosen h parameter ( do you mostly find small subnetworks to lead to high performance ? ) and how it is being optimized ( due to its discrete nature ) . \u2022 Structure : I would move 3.2 into a subsection under the experiments . In general , the manuscript reads a little squished as you have a lot of references to the appendix . It is not easy to remedy this as you don \u2019 t want the work you did go unnoticed but maybe you can leave some references out and submit a longer version of this work to a journal where you don \u2019 t suffer the space constraints of a conference paper . \u2022 From Table 1 it seems like the Avg . error gains are not significant as they overlap ( in terms of standard deviations ) for example with the DARTS results which is not bad as you still save one day of training .", "rating": "7: Good paper, accept", "reply_text": "Many thanks for your constructive and positive feedback ! Please see below for our detailed response to your comments : 1 . * * Some statistics about the chosen hyperparameter H and how it is being optimized * * We optimise the hyperparameter H via Bayesian model selection ; given a uniform prior over different discrete values of H , we choose the H value that leads to the maximum marginal likelihood . This approach encodes a natural Occam \u2019 s razor factor and thus automatically balances data-fitting and model complexity . And given our GP surrogate , the marginal likelihood can be computed analytically . A detailed description of the H hyperparameter statistics is shown in Fig 9 in App . and discussed in App . E.3.2 . * * From Table 1 it seems the avg . error gains are not significant as they overlap with the DARTS results which is not bad as you still save one day of training * * The performance of different search strategies on DARTS search space on CIFAR-10 tend to be very close . As shown in [ 1 ] , the std of test error among 214 fully trained random architectures from DARTS search space is around 0.23 % . Thus , the 0.15 % drop in average test error obtained by our NAS-BOWL over DARTS is quite significant . Thanks for your appreciation on the cost-saving obtained by our method . [ 1 ] Yang , A. , Esperan\u00e7a , P.M. and Carlucci , F.M. , 2019 , September . NAS evaluation is frustratingly hard . In International Conference on Learning Representations . 3 . * * Advice for making our novel use of WL more explicit and modifying Fig.5 * * Thank you for the suggestions . We \u2019 ve added the point on our novel use of WL kernel in Sec.3.1 and also amended Fig.5.We \u2019 ll keep Fig.5 in the App . for now to avoid the confusion over figure numbers , but we will bring it to the main text after rebuttal . 4 . * * Structures of the paper . * * Many thanks for the suggestions and sympathizing with us on the space constraints . The position of Sec.3.2 is indeed a bit awkward as it has a mix of results and methods ( probably more evident in the revised paper now ) ; we decide to keep it under Sec 3 , as we feel interpretability is a key part of our methodology . We do share your concern and will further optimise the paper structure for a possible future journal submission ."}], "0": {"review_id": "j9Rv7qdXjd-0", "review_text": "- After feedback - First of all , I greatly appreciate the authors patient response to me during the feedback period . The discussion was really fruitful . Unfortunately , I still have a concern about interpretability of the proposed method , which is a central topic in the paper . > First , we find it a bit strange when the reviewer says \u201c it is difficult to find importance/meaning of comparing motifs is unclear \u201d , we clearly show our method does find importance in NAS-Bench-101 , all 3 tasks of NAS-Bench-201 and DARTS search space ( Fig 1 and 7 ) -- if we ca n't find importance/distinguish different motifs , none of the results we 've shown would 've been possible . Even when a method works empirically , if a rationale behind the procedure is not clarified , a paper would not be scientifically convincing . Thus , I still do not think my claim is strange . > Second , the example the reviewer gives is not a case when averaged gradient fails . On the contrary , it is exactly an example of when averaged gradient works . A motif with high and diverse local gradient magnitudes but average to near-0 is not important for the purpose of interpretability , as it doesn \u2019 t consistently explain the network performance by itself ( just based on such motifs , one can not conclusively deduce the impact on performance of an arbitrary , unseen architecture in general ) ... In the last response , the authors explained the interpretability issue through combination of motifs , but it did not resolve my concern . To simplify the discussion , consider a bit extreme case in which only one motif is employed in a network simultaneously , and assume WL parameter h = 0 . Let g ( c ) = d \\mu / d \\phi^j |_\\phi^j=c . Then , consider a hypothetical case as follows : motif a ) g ( 1 ) = 10 , g ( 2 ) = 10 ... g ( 10 ) = 10 , g ( 11 ) = -10 , .... g ( 20 ) = -10 : AG = 0 motif b ) g ( 1 ) = 1 , g ( 2 ) = 1 ... g ( 10 ) = 1 , g ( 11 ) = 1 , .... g ( 20 ) = 1 : AG = 20 In this example , b ) has a larger AG , but a ) can have larger importance in practice , and now , since only one kind of motif is employed simultaneously , the explanation of the authors can not be applied . For the exploration purpose , I do not find any rationale to consider that b ) is more important than a ) . I know that these are extreme examples and may depend on an application scenario , but my point is that these examples reveal difficulty of interpretation of AG . The authors introduce AG at Section3.2 as an importance measure without carefully discussing how it can be interpreted in the context of the WL based exploration ( just referring other papers without discussing details in a sense of the above averaging ) . The explanation through marginalization also does not get rid of this question . Since the interpretability is a main theme of the paper , providing a better interpretability of AG would be desired . - Before feedback - The paper proposes to use Weisfeiler-Lehman ( WL ) kernels for neural architecture search . WL kernel can incorporate the topological structure of the network , and the authors combines WL kernels with Bayesian optimization ( BO ) to optimize the validation performance of the network . Further , the authors also claim that WL kernels provide a useful interpretation about good / bad network structures by using the derivative of Gaussian process ( GP ) , which can also be used for 'pruning ' architectures . The performance is shown for several benchmark datasets . Overall , the idea would be reasonable , and the approach would be useful . However , I 'd have to say that the technical novelty and depth would be somewhat weak because the standard WL kernel is directly used without any significant modification , and a gradient-based importance evaluation is also a known technique ( and its interpretation in this context is a bit difficult ) . Further , in my understanding , the paper should have provided more general discussions , not only to show data-specific observations . Detailed comments are as follows . The proposal of the paper is not fully clear for me because the strategies are described for each one of datasets , separately . I could n't find general procedures for the architecture search , from the main text of the paper . In practice , of-course , tuning on each dataset would be required , but showing specific tunings for well-known benchmark datasets is not attractive . A strategy applicable to wide range of tasks would be required for a methodology paper . Interpretation of the gradient-based motif identification is difficult for me . Even when a motif has a large positive or negative gradient value , it only implies 'local ' importance around the given architecture . To derive general insight , more careful treatment would be required . The authors provide the motif discovery procedure in D.1 , but it should be shown in main text because interpretability is one of main theme of this paper . In Section D.1 , the authors described an approach taking average of all possible values , but the average is also difficult to interpret importance because it compresses the entire space , and as the author admitted , the computation would be often intractable in practical settings . Although the authors claim that good/bad motif identification is useful for 'pruning ' , no detailed general pruning procedure is shown in the main text . Providing a general algorithm would be required . What does 'prune ' mean in this context ? If a motif is regarded as 'bad ' once , it is discarded forever , or can revive somewhere ? As I mentioned above , gradient information is only local information . Even when a motif is 'bad ' , simply discarding it completely would be risky . Even when the 'average ' gradient is used , the problem would not be mitigated , because even if the average gradient suggests a motif is useless , it may help to improve accuracy locally . For me , the rationale behind the exploration strategy with the pruning in the paper is quite unclear . The authors claim that the identified motif is trasferable . However , evidence of this claim is not fully clear . It seems empirical suggestions only from a few ( similar ) datasets . When is transferring effective , how do you know it holds when , and can it harm in some case ? I think that a general discussion is missing in the paper . Another difficulty of the gradient-based importance evaluation is that the lack of uncertainty evaluation . The gradient ( 3.2 ) is the expected value of the predictive distribution of GP . Therefore , the variance is not considered . For example , if GP does not have any observations , the expected gradient would be 0 ( when prior is f ( x ) = 0 with the unit variance for any x , which is a standard setting ) , but variance of gradient would be large , meaning that a motif is still has a potential to become important . Again , discarding a motif by the expected gradient without considering uncertainty is seemingly risky , though the paper lacks this kind of discussion on uncertainty , though the uncertainty evaluation is a central issue on in the context of BO .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Many thanks for your constructive and insightful feedback ! We have incorporated many of your suggestions in the updated paper , and please see below for our detailed response and we hope that would resolve any concerns you had : 1 . * * the gradient of a motif only implies 'local ' importance around a given architecture ... average is also difficult to interpret because it compresses the entire space and the computation would be often intractable ... * * We agree that the procedure in App . D.1 is an important part of the paper , and have moved it to Sec.3.2 now . [ Local vs Averaged Gradients ] The local gradient describes the local effect of a motif and is specific to a particular architecture . By taking the average of local gradients of a motif , we marginalise over all architectures containing the motif and obtain a global measure of importance regarding the motif feature . In fact , the integration or averaging of some sort over local gradients , as we did , is also an approach taken in many principled gradient-based global attribution methods , such as Occlusion-1 [ 1 ] and Integrated Gradient [ 2 ] , to analyse the contribution of input features to the model prediction . To the best of our knowledge , we are the first in introducing this technique in combination with interpretable features to explain the NAS performance . [ Tractability of Averaging ] Computing motifs is actually tractable in a practical setting \u2013 as this only involves averaging over all training/queried data , and the number of queries tend to be small under the BO , and especially the NAS setting . If the reviewer is referring to our comment in App . D.1 on intractability of sampling in DARTS search space , we would like to clarify that this intractability is about * * assessing * * the usefulness of the motifs on a held-out validation set of 1,000 architectures sampled , which would require us to fully train and evaluate all these 1,000 architectures . NAS-Bench datasets already did this expensive evaluation for us so we can do such assessment instantly . Note the gradient computation and motifs identification only depend on the surrogate model and don \u2019 t * * need * * additional expensive and extensive architecture evaluations mentioned above . Furthermore , on an empirical note , we have shown quantitatively that we can obtain valuable information on the network performance by simply analysing the motifs contained without the need to actually evaluating the networks ( Fig 1 ) , and qualitatively our approach also sheds lights into the existence of sub-structures which correlate with good network performance and are implicitly picked up by various search strategies in previous works ( Fig.2 ) .We hope these empirical evidence also supports the effectiveness of our proposed motif assessment criterion . [ 1 ] Ancona , M. , Ceolini , E. , \u00d6ztireli , C. and Gross , M. , 2018 . Towards better understanding of gradient-based attribution methods for deep neural networks . In International Conference on Learning Representations . [ 2 ] Sundararajan , M. , Taly , A. and Yan , Q. , 2017 . Axiomatic attribution for deep networks . International Conference on Machine Learning , PMLR 70 . 2 . * * the general pruning procedure is missing in the main text . What does 'prune ' mean in this context ? If a motif is regarded as 'bad ' once , it is discarded forever , or can revive somewhere ? * * We have added the algorithm for pruning in Algorithm 1 . The number of possible architectures contained in the search space is often huge . Pruning here refers to defining and applying the constraints on such a large search space to obtain a much smaller , yet promising , pool of candidate architectures , thus speeding up the search . In our case , we transfer motifs learnt from a previous , related task as well as the new target task for this purpose : at each BO iteration , we generate a large set of candidate architectures , and discard those without at least one favorable motif . As such , the BO agent now only needs to select from a smaller subset of architectures deemed more promising , thereby \u201c warm starting \u201d the search on a new task . [ Discarding bad motifs is risky ] Ultimately , the trade-off here is whether the efficiency gained from constraining the search space justifies the risk of missing out better solutions in the discarded/unexplored region . We argue that it is the case in NAS where the search space is immense ( DARTS [ 3 ] search space with only 4 operation blocks features 4.4e12 combinations ) and each query is computationally expensive . Furthermore , experimentally , we show in Fig 1 and 7 that across various search spaces , at least in the experiments we conducted the \u201c good \u201d yet smaller subsets still contain the best candidates among those randomly sampled , and in Fig 4 , the final solutions with pruning are not worse than those without . This supports the informativeness of our learnt motifs and we hope this also alleviates the concern about the risk of pre-emptively discarding architectures based on motifs . ( Continued below )"}, "1": {"review_id": "j9Rv7qdXjd-1", "review_text": "Summary == This work proposed a BO based NAS method using Weisfeiler-Lehman kernel . The idea is novel and natural considering the neural network architectures as acyclic directed graphs . I am a bit surprised to see no one tried it before in the NAS field and it is great to know that using WL kernel leads to competitive NAS performance comparing to other NAS methods and at the same time improves interpretability . Pros = * The proposed idea is novel and natural , given the graph natures of network architecture . * The notes on the interpretability is very interesting and differ the work from other methods . * Extensive empirical studies and ablation studies . * Extensive detail for reproducibility . * The paper is very well written . Minor comments I think this is a really nice work and I only have some minor comments : * There is another line of work using BO for NAS : Ru , Binxin , Pedro Esperanca , and Fabio Carlucci . `` Neural Architecture Generator Optimization . '' arXiv preprint arXiv:2004.01395 ( 2020 ) . Would be nice to know how does the proposed method compared to it . * The appendix C mentioned about using MKL to combine WL and MLP kernels . But in the end the author used 0.7 and 0.3 as the weights for them . I am wondering whether some simple MKL algorithm such as ALIGNF can improve the performance here . You can find more detail in this paper : Cortes C , Mohri M , Rostamizadeh A. Algorithms for learning kernels based on centered alignment [ J ] . The Journal of Machine Learning Research , 2012 , 13 ( 1 ) : 795-828 . Reason for score == I liked this work a lot , it bridged NAS and BO through the usage of graph kernels ( WL kernel ) . As a result , NAS becomes more sample efficient , which is empirically verified by extensive study in this work . The author did a very good job on the empirical evaluations , they are thorough , solid and contains many ablation studies to understand their methods . Post rebuttal comments = I thank the authors for their responses . I encourage the authors to continue the line of work on replacing the random sampling of NAGO . Given NAS-BOWL surrogate , one can do Thompson Sampling instead of random sampling . On the MKL side , the same weights for all the kernels might be the cause of worse performance . I would also encourage the authors to verify that . Nevertheless , those are minor comments and I still think this is an important work to bridge BO and NAS . I will keep my score .", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Many thanks for your positive and constructive review ! Please see below for our response to your comments : 1 . * * How does the proposed method compare to another BO-for-NAS work [ 1 ] ? * * We thank the reviewer for the suggestion . Neural Architecture Generator Optimization ( NAGO ) [ 1 ] is an interesting and relevant work but is orthogonal to our method . The key contribution of NAGO is a novel graph-generator-based * * search space * * which is motivated by recasting NAS as searching for an optimal architecture generator/distribution instead of an optimal architecture . On the other hand , our work proposes a new * * search strategy * * that can be applied to a wide variety of search spaces including that of NAGO . In fact , we \u2019 ve shown in the paper that our GPWL surrogate performs very well on the randomly wired networks [ 2 ] , which inspires NAGO and has very similar architectures as those in NAGO [ 2 ] . Note that we haven \u2019 t tested our method on NAGO search space because their code is not released yet . More interestingly , we believe our method can be applied to improve NAGO : NAGO returns an optimal architecture generator but resort to random sampling to obtain the exact architecture for final use . Our NAS-BOWL can be used in place of random sampling to extract the best architecture from the good generator ; One potential way is simply to use the good architecture generator learnt by NAGO to generate candidate architectures for optimising the acquisition functions in NAS-BOWL instead of using the mutation scheme . This would be an interesting future work . [ 1 ] Ru , B. , Esperanca , P. and Carlucci , F. , 2020 . Neural Architecture Generator Optimization.Advances in Neural Information Processing Systems , 33 . [ 2 ] Xie , S. , Kirillov , A. , Girshick , R. and He , K. , 2019 . Exploring randomly wired neural networks for image recognition . In Proceedings of the IEEE International Conference on Computer Vision ( pp.1284-1293 ) . 2 . * * Suggestion on MKL algorithm such as ALIGNF * * Many thanks for the suggestion . We have added your comment in the discussion of App . C , as a more concrete future direction . As we said , while it seems that in the present setup , using additive kernels does not improve the predictive performance by much ( and MLP is quite expensive to compute ) , we believe leveraging on MKL literature might deliver more significant performance boosts in larger , more complicated search spaces ."}, "2": {"review_id": "j9Rv7qdXjd-2", "review_text": "# Summary of the paper The paper presents a new Bayesian optimization strategy based on Weisfeiler-Lehman Kernels for neural architecture search . The proposed method is more sample efficient than other state-of-the-art Bayesian optimization ( BO ) methods and allows to identify repeating motifs in well performing architectures . Overall , I really enjoyed reading the paper and I am somewhat surprised that nobody has tried this before . The usage of the Weisfeiler-Lehman kernel is well motivated and enables users to obtain a better intuition which parts of an architecture lead to a good performance . Also , the proposed BO method outperforms other state-of-the-art BO methods across a range of competitive benchmarks . However a few points need to be clarified , and it would be great if the authors could address them in the rebuttal . # Merits - The paper is clearly written and the approach is well motivated - The proposed method achieves strong results compared to other Bayesian optimization strategies based on Gaussian process surrogate models . Also , the paper presents a thorough empirical evaluation to other state-of-the-art BO method on well established benchmarks - As far as I know , this is the first Bayesian optimization for NAS that provides interpretable features to explain which motifs of neural networks architectures work well . # Concerns - Looking at Figure 12 in the appendix , it seems that the proposed method GPWL gets its main boost from the mutation strategy used to optimize the acquisition function . This makes me wonder whether the model is actually better than , for example BANANAS , or whether the gain in performance is mostly due to the mutation strategy ? How well would BANANAS works with this mutation strategy ? - Could you also add the plots in Figure 3 with the regret on the y-axis ( as in the original papers ) ? This would show how far away from the optimum an optimizer actually is . This is somewhat hidden with ranking plots , where an optimizer might have found an architecture with a negligible performance difference to the global optimum but has a lower rank . - Why does start GPWL at a higher rank than the other baselines in Figure 3 ? And why does it stop earlier ? Also the dashed red horizontal line is not explained in the caption . # Post Rebuttal : I thank the authors for taking the time to address my concerns . The paper is well written and the proposed approached is promising . I therefor recommend acceptance .", "rating": "7: Good paper, accept", "reply_text": "Many thanks for your positive and constructive review ! Please see below for our response to your comments , and we hope that resolves any concerns you had : 1 . * * Whether the model is better than BANANAS \u2019 s\u2026 performance gain is due to the mutation strategy ? Compare with BANANAS with mutation strategy ... * * We have indeed compared with BANANAS with mutation strategy ( dark blue triangle , BANANASm ) on NAS-Bench-101 in both Fig.4 and 10 and show that our NAS-BOWL with mutation ( NASBOWLm ) outperforms that ; similarly , NAS-BOWL with random strategy ( NASBOWLr ) also outperforms BANANAS with random ( BANANASr ) in the same Figs . Moreover , in our surrogate experiments in Fig , 3 , we also include a baseline which combines the path-encoding scheme in BANANAS , its key contribution , with GP and shows that our GPWL surrogate achieves better predictive performance with less than \u2153 of the training data and thus is more query efficient . 2 . * * GPWL starts at a higher rank than the other baselines in Fig.3 ? And why does it stop earlier ? Dashed red horizontal line is not explained in the caption * * . For all surrogate models , we start with 10 initial training data ( it might be a bit difficult to discern from the figure but the lines start at 10 not 0 ) .The high rank prediction achieved by GPWL at the start shows that GPWL can well predict the performance ranking of validation architectures after training on only 10 initial architecture data . This shows exactly the query efficiency and competitive predictive power of our GPWL which are highly valued for the real-world NAS applications . Following the practice in [ 1 ] for comparing query efficiency across different baselines , we run the other baselines for more training data than our GPWL in order to see whether they can catch up the GPWL \u2019 s performance with more training data . E.g.on NAS-Bench-201 ( CIFAR100 ) task , Path-encoding and GNN manage to achieve the similar performance ( GPWL at 100 training data ) with 300 training data . This again demonstrates the query efficiency of our surrogate model and it \u2019 s evident from Fig.3 that such gain in query efficiency is larger for larger search space ( NAS-Bench-101 and Flowers102 ) . The dashed red horizontal lines are there to help comparison between the rank correlation performance of the GPWL with other baselines . [ 1 ] Ru , B. , Cobb , A. , Blaas , A. and Gal , Y. , 2020 . Bayesopt adversarial attack . In International Conference on Learning Representations . 3 . * * Add the regret on the y-axis in Fig.3 ( rank correlation plots ) to show how far away from the optimum * * To clarify , Fig.3 shows the * * prediction/regression * * not * * optimisation * * performance of different surrogate models over increasing number of randomly sampled training points . We assess the predictive performance using Spearman \u2019 s rank correlation because relative performance ranking is important for comparing architectures in NAS . Fig.8 in the App . also shows that our GPWL surrogate can predict the exact validation error very well , especially in the low error regime in which we are more interested . To better assess the regret , we have added in a dashed line indicating the global optimum for different NAS-Bench datasets ( close-domain search spaces ) for Fig.4.If needed , we can change the y-axis from error to regret to better illustrate that ."}, "3": {"review_id": "j9Rv7qdXjd-3", "review_text": "The authors propose a new neural architecture search algorithm combining Bayesian optimization with the expressive and popular Weisfeiler-Lehman ( WL ) Graph Kernel . One advantage of using WL is the interpretable results that stem from the nature of how the kernel is computed , namely a propagation scheme through the graph . Combined the derivative of Eq.3.2 , one can extract subgraphs that are directly responsible for increased performance . In a variety of experiments , the authors show not only increased performance of detected architectures but also find subgraphs that are found by other algorithms as well . Even though my expertise does not lie in the field of NAS , I find this work quite appealing . It is an innovative application for graph kernels , which suffer from scalability which in this setting is less of a problem . I find the aspects of novelty , interpretability , and quantitative results convincing enough to recommend acceptance . Furthermore , the work is largely well structured and written , and the figures are legible and relevant . W.r.t whether the comparison to other SOTA NAS algorithms is of good quality and fair , I think the input from reviewers with a NAS background would be valuable . Minor comments : \u2022 By itself a graph kernel is a similarity measure and does not perform any subgraph selection . It happens that due to the WL propagation scheme , the WL graph kernel consists of interpretable features while computing the similarity . I would clarify this a little more in 3.1 . Since this is , in my opinion , the most innovative part of the manuscript , I would even consider bringing Figure 5 from the appendix into the main paper ( maybe in a condensed form ) . To get some space for this you could shorten some descriptions of experiment parameters and comparison methods to the appendix . Also , I think Figure 5 is not 100 % complete . To make it even easier to parse , I would put boxes around the subgraphs in the $ h=1 features $ box and annotate them with their respective index . E.g.the upper graph with 4 outgoing edges should be annotated with 5 and so on . This also makes it a little clearer how WL leads to larger networks in each round of propagation . \u2022 I would be interested in some statistics about the chosen h parameter ( do you mostly find small subnetworks to lead to high performance ? ) and how it is being optimized ( due to its discrete nature ) . \u2022 Structure : I would move 3.2 into a subsection under the experiments . In general , the manuscript reads a little squished as you have a lot of references to the appendix . It is not easy to remedy this as you don \u2019 t want the work you did go unnoticed but maybe you can leave some references out and submit a longer version of this work to a journal where you don \u2019 t suffer the space constraints of a conference paper . \u2022 From Table 1 it seems like the Avg . error gains are not significant as they overlap ( in terms of standard deviations ) for example with the DARTS results which is not bad as you still save one day of training .", "rating": "7: Good paper, accept", "reply_text": "Many thanks for your constructive and positive feedback ! Please see below for our detailed response to your comments : 1 . * * Some statistics about the chosen hyperparameter H and how it is being optimized * * We optimise the hyperparameter H via Bayesian model selection ; given a uniform prior over different discrete values of H , we choose the H value that leads to the maximum marginal likelihood . This approach encodes a natural Occam \u2019 s razor factor and thus automatically balances data-fitting and model complexity . And given our GP surrogate , the marginal likelihood can be computed analytically . A detailed description of the H hyperparameter statistics is shown in Fig 9 in App . and discussed in App . E.3.2 . * * From Table 1 it seems the avg . error gains are not significant as they overlap with the DARTS results which is not bad as you still save one day of training * * The performance of different search strategies on DARTS search space on CIFAR-10 tend to be very close . As shown in [ 1 ] , the std of test error among 214 fully trained random architectures from DARTS search space is around 0.23 % . Thus , the 0.15 % drop in average test error obtained by our NAS-BOWL over DARTS is quite significant . Thanks for your appreciation on the cost-saving obtained by our method . [ 1 ] Yang , A. , Esperan\u00e7a , P.M. and Carlucci , F.M. , 2019 , September . NAS evaluation is frustratingly hard . In International Conference on Learning Representations . 3 . * * Advice for making our novel use of WL more explicit and modifying Fig.5 * * Thank you for the suggestions . We \u2019 ve added the point on our novel use of WL kernel in Sec.3.1 and also amended Fig.5.We \u2019 ll keep Fig.5 in the App . for now to avoid the confusion over figure numbers , but we will bring it to the main text after rebuttal . 4 . * * Structures of the paper . * * Many thanks for the suggestions and sympathizing with us on the space constraints . The position of Sec.3.2 is indeed a bit awkward as it has a mix of results and methods ( probably more evident in the revised paper now ) ; we decide to keep it under Sec 3 , as we feel interpretability is a key part of our methodology . We do share your concern and will further optimise the paper structure for a possible future journal submission ."}}