{"year": "2020", "forum": "rkl3m1BFDB", "title": "Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "This was a contentious paper, with quite a large variance in the ratings, and ultimately a lack of consensus. After reading the paper myself, I found it to be a valuable synthesis of common usage of saliency maps and a critique of their improper interpretation. Further, the demonstration of more rigorous methods of evaluating agents based on salience maps using case studies is quite illustrative and compelling. I think we as a field can agree that we\u2019d like to gain better understanding our deep RL models. This is not possible if we don\u2019t have a good understanding of the analysis tools we\u2019re using.\n\nR2 rightly pointed out a need for quantitative justification for their results, in the form of statistical tests, which the authors were able to provide, leading the reviewer to revise their score to the highest value of 8. I thank them for instigating the discussion.\n\nR1 continues to feel that the lack of a methodological contribution (in the form of improving learning within an agent) is a weakness. However, I don\u2019t believe that all papers at deep learning conferences have to have the goal of empirically \u201clearning better\u201d on some benchmark task or dataset, and that there\u2019s room at ICLR for more analysis papers. Indeed, it\u2019d be nice to see more papers like this.\n \nFor this reason, I\u2019m inclined to recommend accept for this paper. However this paper does have weaknesses, in that the framework proposed could be made more rigorous and formal. Currently it seems rather adhoc and on a task-by-task basis (ie we need to have access to game states or define them ourselves for the task). It\u2019s also disappointing that it doesn\u2019t work for recurrent agents, which limits its applicability for analyzing current SOTA deep RL agents. I wonder if authors can comment on possible extensions that would allow for this.\n", "reviews": [{"review_id": "rkl3m1BFDB-0", "review_text": "[score raised from weak accept to accept due to rebuttal/improvements] Summary The paper investigates the practice of using pixel-level saliency maps in deep RL to \u201cexplain\u201d agent behavior in terms of semantics of the scene. The main problem, according to the paper, is that pixel-level saliency maps often correlate with semantic objects, however turning these correlations into explanations would require counterfactual analysis / interventions, which are almost never performed in practice. The paper highlights this issue with an extensive literature survey, and proposes a simple method to formulate \u201cexplanations\u201d found via saliency maps into falsifiable hypotheses. Three experiments show how to apply the methodology in practice - in all three cases pixel-level correlations cannot be easily mapped to semantic-level explanations that hold (counterfactual) validation. Contributions The paper nicely summarizes the main contributions, namely: (i) a literature survey on pixel-saliency methods in deep RL and their use to \u201cexplain\u201d agent behavior, (ii) a detailed description of the problem with the latter and a proposal to mitigate the main issues, and (iii) three experimental case-studies to illustrate the problem further and show how the proposed method can help. Quality, Clarity, Novelty, Impact The paper addresses a highly important issue in the field of interpretable deep learning. The main message is that a lack of scientific rigor, namely stating falsifiable hypotheses and validation of claimed hypotheses, can easily lead to misinterpretation of deep RL systems. This is a somewhat disenchanting message, but I personally think it is important to ensure that this message is heard in the field of interpretable ML in particular, and in the wider deep learning community in general. It is tempting to give simple answers to complex problems, and while I think saliency maps will play a large role in interpreting deep network decisions, I am also convinced that we need causal explanations, which salience maps (currently) cannot provide on a semantic level. The paper is well written and clear, the literature survey is quite extensive and valuable. The experimental results are nice, however they currently crucially lack quantitative statements that back up the qualitative results (see improvements below). While the latter must be included for publication, I am fairly confident that this can be rectified during the rebuttal phase and therefore (tentatively) vote for acceptance. Improvements a) Mandatory for publication! Back the results-plots up by numbers! In particular: visually estimating densities / correlations from scatter plots is often impossible and misleading - while the plots are nice to have, the claims regarding Figure 5, 8, 9 (b) and (c) must be backed up by reporting actual correlations / statistical tests. For instance, it is impossible to judge visually whether there\u2019s any trend in 5 (c). Please report correlations for 5, 8, 9 (b) and perform suitable statistical tests for measuring increase/decrease in correlation for 5, 8, 9 (c). Similarly, please report an appropriate metric to quantitatively judge the difference between the curves in 4, 6, 7 (c). It\u2019s fine to include tables reporting the quantitative results in the appendix, they don\u2019t necessarily have to be in the main paper. b) Experimental details. Please report the details required to reproduce the experiments. In particular, what was the precise architecture for A2C and the hyper-parameter settings (particularly since the reference that is cited is not a paper, but a GitHub repo). For the figures, please report how saliency was measured exactly (was there a bounding-box around saliency/enemies? What was its size? Were intensities somehow normalized, were distances to enemies measured between centers of bounding-boxes, \u2026?) c) (Optional). It would be nice to see an example where the method is used but the original hypothesis is not rejected (i.e. there\u2019s now stronger evidence for the original hypothesis due to the counterfactual analysis). I understand that this is beyond the scope of the rebuttal, and feel free to completely ignore this. Major Comments I) Please state whether the paper was written with feed-forward deep RL agents only in mind, or whether the paper is intended to also include recurrent deep RL agents (it would also be helpful to know whether the experiments used a feed-forward, or a recurrent version of A2C). While I think that many aspects carry over from feedforward architectures to recurrent ones, I personally think that some issues with counterfactual analysis could become more intricate with recurrent agents. For instance, on page 6, the described invariance in the first paragraph under \u2018Counterfactual Evaluation of Claims\u2019 is fine for feed-forward agents, but could be debatable with recurrent agents. If you agree, please make this distinction clear in the paper (where appropriate) or state that the paper only applies to feedforward agents. If you disagree please indicate this during the rebuttal discussion. II) Page 6, just above Sec. 5: \u201cSince the learned policies should be semantically invariant under manipulations of the RL environment...\u201d. I agree that they should ideally be invariant, for the semantic interventions to make sense, but please comment on whether this is a trivial assumption, how this assumption could (in principle) be verified and the potential consequences of this assumption being violated. I personally think that there\u2019s a fair chance that the semantic space carved up by the agent (that potentially overfits a task/family of tasks) might be quite different from the semantic space given by the latent factors of the environment. This mismatch and its potential interference with the method should be discussed as a current shortcoming. Minor Comments I) A potential subtlety (which I don\u2019t expect you to resolve/discuss in the paper) is that feed-forward agents in an MDP environment can behave like recurrent agents by offloading memory into the environment. E.g. a breakout agent could \u201cmemorize\u201d that it is in \u201ctunnel-digging mode\u201d by moving the paddle by a few pixels - this could then potentially shift it\u2019s saliency away from the actual tunnel to the corresponding pixels around the paddle. Such cases might be very hard to interpret via saliency maps or interventional analysis, but I acknowledge that this is perhaps a more exotic case, given the current state of interpretable deep RL. Just a thought for future work perhaps...", "rating": "8: Accept", "reply_text": "We greatly appreciate Reviewer 2 's extensive comments and suggested improvements . We have incorporated the suggested improvements to the best of our ability during the response period , and we summarize those revisions in a separate official comment . Specifically , we have provided statistics that quantify experimental effects , run and reported the results of hypothesis tests , and provided additional experimental details . Below are some additional notes on specific comments from the review : Application to recurrent deep RL agents \u2014 As Reviewer 2 notes , the paper was written with feed-forward deep RL agents in mind . That said , the proposed methodology is post-hoc ( i.e. , not model-dependent ) , so aspects of the approach will carry over to recurrent RL agents . Our proposed methodology would not work for repeated interventions on recurrent RL agents due to their capacity for memorization . We have noted this distinction in Section 6 . Semantic Invariance \u2014 We completely agree that the semantic space devised by the agent might be quite different from the semantic space given by the latent factors of the environment . It is crucial to note that this mismatch is one aspect of what plays out when researchers create hypotheses about agent behavior , and the methodology we provide in this work demonstrates how to evaluate hypotheses that reflect that mismatch . Positive Example \u2014 We agree it would have been ideal to provide an example where the original hypothesis was not rejected . Unfortunately , we exhausted the set of obvious hypotheses for the two games we considered , and all were rejected . We were surprised by these results , but they support the idea that saliency maps are easily misinterpreted . Memory in feed-forward agents \u2014 The subtlety that Reviewer 2 points about feed-forward agents behaving like recurrent agents by offloading memory into the environment is extremely interesting . Assessing whether memorization leads to different saliency behavior is a fascinating direction for future work ."}, {"review_id": "rkl3m1BFDB-1", "review_text": "Abstract: The author suggests that saliency maps should be viewed as exploratory tools rather than explanation. The explore this idea in the context of a game. Here is my main issue: Although I believe there is a value in studies like this. I am not sure ICLR is the right venue for it. The paper is well written but it reads like a long opinion/blog-post. There is no overarching theory or generalizable observation not to mention a solution. Yes, I agree that the method of interpreting the black box has a lot of issues and the counterfactual approach/causal approach is probably the right way to go but this is hardly news to the community. In short: what the generalizable contribution of the paper? I am open to change my mind if the discussion is convincing. ", "rating": "3: Weak Reject", "reply_text": "We appreciate Reviewer 1 \u2019 s comments , and address the main points of the review below : Generalizable contribution \u2014 The paper makes several contributions : ( 1 ) a survey of how saliency maps are currently used to explain the behavior of deep RL agents ; ( 2 ) a new method to empirically evaluate the inferences made from saliency maps ; and ( 3 ) an experimental evaluation that uses our proposed method to measure how well saliency maps correspond to the semantic-level inferences of humans . Each of these contributions applies to any use of common saliency-map methods to understand the behavior of deep RL agents learned using feed-forward architectures . Overarching theory \u2014 As we describe in section 2 , our proposed method is based on a formal theory of counterfactual intervention . Though the graphical model in Figure 2 represents an Atari game environment , researchers can reason about interventions in different vision-based RL domains by substituting different content for the state and pixels . We added a specific graphical model for Breakout in the Appendix ( Figure 6 ) to clarify how the generalized causal graphical model in Figure 2 can be specified to a given domain . Section 6 contains additional points on the generalization of the proposed methodology . ICLR as a venue for this paper \u2014 ICLR is a nearly ideal venue for this work . The paper that first introduced saliency maps was published in a 2014 ICLR workshop ( Simonyan et al.2014 ) .Subsequent ICLR papers have introduced new saliency map methods ( e.g. , Zintgraf et al.2017 ) and analyzed these methods ( e.g. , Ancona et al.2018 ) .Many studies have critiqued the use of saliency maps in computer vision ( Adebayo et al. , 2018 ; Samek et al. , 2018 ; Kindermans et al. , 2019 ) , but we are the first to analyze the utility of saliency maps for understanding the behavior of deep RL agents . Finally , while methodological papers are relatively uncommon in machine learning conferences ( including ICLR ) , effective evaluation of learned representations is vital to progress in the field . Saliency maps have become one of the primary methods to visualize the representations learned by deep neural networks , and better understanding the utility of saliency maps is central to understanding their proper role in research . Adebayo et al . `` Sanity checks for saliency maps . '' NeurIPS 2018 . Ancona et al. \u201c Towards better understanding of gradient-based attribution methods for deep neural networks. \u201d ICLR 2018 . Kindermans et al . `` The ( un ) reliability of saliency methods . '' Explainable AI : Interpreting , Explaining and Visualizing Deep Learning 2019 . Samek et al . `` Evaluating the visualization of what a deep neural network has learned . '' IEEE Transactions on Neural Networks and Learning Systems 2017 . Simonyan et al. \u201c Deep inside convolutional networks : visualising image classification models and saliency maps. \u201d ICLR Workshop 2014 . Zintgraf et al. \u201c Visualizing deep neural network decisions : prediction difference analysis. \u201d ICLR 2017 ."}, {"review_id": "rkl3m1BFDB-2", "review_text": "The paper has a double aim. First, it is a survey on saliency maps used in explaining deep reinforcement learning models. Second, it is a proposal of a method that should overcome limitations of the current approaches described in the survey. This double aim makes the paper hard to understand as the survey is not complete and the model is not well explained. The main limitations the novel model aims to solve seems to be the production of \"falsifiable\" hypothesis in the explaination with saliency maps. However, experiments are really hard to follow and it is not clear why this is the case.", "rating": "1: Reject", "reply_text": "We appreciate Reviewer 3 \u2019 s comments . The review has several main points that we address below : Double aim of the paper \u2014 Our principal contribution is a new method for empirical evaluation of explanations generated from saliency maps about the behavior of deep RL agents . We intentionally structured our paper to include both a survey of current practice and an application of our proposed approach . Both elements were intended to aid reader understanding . The survey describes the inferences that require evaluation , and the application demonstrates the surprising conclusions supported by the evaluation method . We have attempted to improve our description of this approach ( see \u201c Clarity \u201d below ) . Completeness of the survey \u2014 As we note in section 3 , we surveyed 90 papers , each of which cited one or more key papers that described one of four saliency map methods . We would be happy to include additional papers in our survey , expand our survey criteria , or consider additional updates to the survey , if Reviewer 3 could provide specific suggestions . Clarity \u2014 We have added additional details to Section 4 on how saliency is measured . We have also added more quantitative results from the experiments to Section 5 and Appendix E ( see Tables 3 , 6 , 7 and 8 ) , and more details on the model used for training in Section 5 and Appendix B . We hope these additions make the paper more clear , and we welcome additional recommendations ."}], "0": {"review_id": "rkl3m1BFDB-0", "review_text": "[score raised from weak accept to accept due to rebuttal/improvements] Summary The paper investigates the practice of using pixel-level saliency maps in deep RL to \u201cexplain\u201d agent behavior in terms of semantics of the scene. The main problem, according to the paper, is that pixel-level saliency maps often correlate with semantic objects, however turning these correlations into explanations would require counterfactual analysis / interventions, which are almost never performed in practice. The paper highlights this issue with an extensive literature survey, and proposes a simple method to formulate \u201cexplanations\u201d found via saliency maps into falsifiable hypotheses. Three experiments show how to apply the methodology in practice - in all three cases pixel-level correlations cannot be easily mapped to semantic-level explanations that hold (counterfactual) validation. Contributions The paper nicely summarizes the main contributions, namely: (i) a literature survey on pixel-saliency methods in deep RL and their use to \u201cexplain\u201d agent behavior, (ii) a detailed description of the problem with the latter and a proposal to mitigate the main issues, and (iii) three experimental case-studies to illustrate the problem further and show how the proposed method can help. Quality, Clarity, Novelty, Impact The paper addresses a highly important issue in the field of interpretable deep learning. The main message is that a lack of scientific rigor, namely stating falsifiable hypotheses and validation of claimed hypotheses, can easily lead to misinterpretation of deep RL systems. This is a somewhat disenchanting message, but I personally think it is important to ensure that this message is heard in the field of interpretable ML in particular, and in the wider deep learning community in general. It is tempting to give simple answers to complex problems, and while I think saliency maps will play a large role in interpreting deep network decisions, I am also convinced that we need causal explanations, which salience maps (currently) cannot provide on a semantic level. The paper is well written and clear, the literature survey is quite extensive and valuable. The experimental results are nice, however they currently crucially lack quantitative statements that back up the qualitative results (see improvements below). While the latter must be included for publication, I am fairly confident that this can be rectified during the rebuttal phase and therefore (tentatively) vote for acceptance. Improvements a) Mandatory for publication! Back the results-plots up by numbers! In particular: visually estimating densities / correlations from scatter plots is often impossible and misleading - while the plots are nice to have, the claims regarding Figure 5, 8, 9 (b) and (c) must be backed up by reporting actual correlations / statistical tests. For instance, it is impossible to judge visually whether there\u2019s any trend in 5 (c). Please report correlations for 5, 8, 9 (b) and perform suitable statistical tests for measuring increase/decrease in correlation for 5, 8, 9 (c). Similarly, please report an appropriate metric to quantitatively judge the difference between the curves in 4, 6, 7 (c). It\u2019s fine to include tables reporting the quantitative results in the appendix, they don\u2019t necessarily have to be in the main paper. b) Experimental details. Please report the details required to reproduce the experiments. In particular, what was the precise architecture for A2C and the hyper-parameter settings (particularly since the reference that is cited is not a paper, but a GitHub repo). For the figures, please report how saliency was measured exactly (was there a bounding-box around saliency/enemies? What was its size? Were intensities somehow normalized, were distances to enemies measured between centers of bounding-boxes, \u2026?) c) (Optional). It would be nice to see an example where the method is used but the original hypothesis is not rejected (i.e. there\u2019s now stronger evidence for the original hypothesis due to the counterfactual analysis). I understand that this is beyond the scope of the rebuttal, and feel free to completely ignore this. Major Comments I) Please state whether the paper was written with feed-forward deep RL agents only in mind, or whether the paper is intended to also include recurrent deep RL agents (it would also be helpful to know whether the experiments used a feed-forward, or a recurrent version of A2C). While I think that many aspects carry over from feedforward architectures to recurrent ones, I personally think that some issues with counterfactual analysis could become more intricate with recurrent agents. For instance, on page 6, the described invariance in the first paragraph under \u2018Counterfactual Evaluation of Claims\u2019 is fine for feed-forward agents, but could be debatable with recurrent agents. If you agree, please make this distinction clear in the paper (where appropriate) or state that the paper only applies to feedforward agents. If you disagree please indicate this during the rebuttal discussion. II) Page 6, just above Sec. 5: \u201cSince the learned policies should be semantically invariant under manipulations of the RL environment...\u201d. I agree that they should ideally be invariant, for the semantic interventions to make sense, but please comment on whether this is a trivial assumption, how this assumption could (in principle) be verified and the potential consequences of this assumption being violated. I personally think that there\u2019s a fair chance that the semantic space carved up by the agent (that potentially overfits a task/family of tasks) might be quite different from the semantic space given by the latent factors of the environment. This mismatch and its potential interference with the method should be discussed as a current shortcoming. Minor Comments I) A potential subtlety (which I don\u2019t expect you to resolve/discuss in the paper) is that feed-forward agents in an MDP environment can behave like recurrent agents by offloading memory into the environment. E.g. a breakout agent could \u201cmemorize\u201d that it is in \u201ctunnel-digging mode\u201d by moving the paddle by a few pixels - this could then potentially shift it\u2019s saliency away from the actual tunnel to the corresponding pixels around the paddle. Such cases might be very hard to interpret via saliency maps or interventional analysis, but I acknowledge that this is perhaps a more exotic case, given the current state of interpretable deep RL. Just a thought for future work perhaps...", "rating": "8: Accept", "reply_text": "We greatly appreciate Reviewer 2 's extensive comments and suggested improvements . We have incorporated the suggested improvements to the best of our ability during the response period , and we summarize those revisions in a separate official comment . Specifically , we have provided statistics that quantify experimental effects , run and reported the results of hypothesis tests , and provided additional experimental details . Below are some additional notes on specific comments from the review : Application to recurrent deep RL agents \u2014 As Reviewer 2 notes , the paper was written with feed-forward deep RL agents in mind . That said , the proposed methodology is post-hoc ( i.e. , not model-dependent ) , so aspects of the approach will carry over to recurrent RL agents . Our proposed methodology would not work for repeated interventions on recurrent RL agents due to their capacity for memorization . We have noted this distinction in Section 6 . Semantic Invariance \u2014 We completely agree that the semantic space devised by the agent might be quite different from the semantic space given by the latent factors of the environment . It is crucial to note that this mismatch is one aspect of what plays out when researchers create hypotheses about agent behavior , and the methodology we provide in this work demonstrates how to evaluate hypotheses that reflect that mismatch . Positive Example \u2014 We agree it would have been ideal to provide an example where the original hypothesis was not rejected . Unfortunately , we exhausted the set of obvious hypotheses for the two games we considered , and all were rejected . We were surprised by these results , but they support the idea that saliency maps are easily misinterpreted . Memory in feed-forward agents \u2014 The subtlety that Reviewer 2 points about feed-forward agents behaving like recurrent agents by offloading memory into the environment is extremely interesting . Assessing whether memorization leads to different saliency behavior is a fascinating direction for future work ."}, "1": {"review_id": "rkl3m1BFDB-1", "review_text": "Abstract: The author suggests that saliency maps should be viewed as exploratory tools rather than explanation. The explore this idea in the context of a game. Here is my main issue: Although I believe there is a value in studies like this. I am not sure ICLR is the right venue for it. The paper is well written but it reads like a long opinion/blog-post. There is no overarching theory or generalizable observation not to mention a solution. Yes, I agree that the method of interpreting the black box has a lot of issues and the counterfactual approach/causal approach is probably the right way to go but this is hardly news to the community. In short: what the generalizable contribution of the paper? I am open to change my mind if the discussion is convincing. ", "rating": "3: Weak Reject", "reply_text": "We appreciate Reviewer 1 \u2019 s comments , and address the main points of the review below : Generalizable contribution \u2014 The paper makes several contributions : ( 1 ) a survey of how saliency maps are currently used to explain the behavior of deep RL agents ; ( 2 ) a new method to empirically evaluate the inferences made from saliency maps ; and ( 3 ) an experimental evaluation that uses our proposed method to measure how well saliency maps correspond to the semantic-level inferences of humans . Each of these contributions applies to any use of common saliency-map methods to understand the behavior of deep RL agents learned using feed-forward architectures . Overarching theory \u2014 As we describe in section 2 , our proposed method is based on a formal theory of counterfactual intervention . Though the graphical model in Figure 2 represents an Atari game environment , researchers can reason about interventions in different vision-based RL domains by substituting different content for the state and pixels . We added a specific graphical model for Breakout in the Appendix ( Figure 6 ) to clarify how the generalized causal graphical model in Figure 2 can be specified to a given domain . Section 6 contains additional points on the generalization of the proposed methodology . ICLR as a venue for this paper \u2014 ICLR is a nearly ideal venue for this work . The paper that first introduced saliency maps was published in a 2014 ICLR workshop ( Simonyan et al.2014 ) .Subsequent ICLR papers have introduced new saliency map methods ( e.g. , Zintgraf et al.2017 ) and analyzed these methods ( e.g. , Ancona et al.2018 ) .Many studies have critiqued the use of saliency maps in computer vision ( Adebayo et al. , 2018 ; Samek et al. , 2018 ; Kindermans et al. , 2019 ) , but we are the first to analyze the utility of saliency maps for understanding the behavior of deep RL agents . Finally , while methodological papers are relatively uncommon in machine learning conferences ( including ICLR ) , effective evaluation of learned representations is vital to progress in the field . Saliency maps have become one of the primary methods to visualize the representations learned by deep neural networks , and better understanding the utility of saliency maps is central to understanding their proper role in research . Adebayo et al . `` Sanity checks for saliency maps . '' NeurIPS 2018 . Ancona et al. \u201c Towards better understanding of gradient-based attribution methods for deep neural networks. \u201d ICLR 2018 . Kindermans et al . `` The ( un ) reliability of saliency methods . '' Explainable AI : Interpreting , Explaining and Visualizing Deep Learning 2019 . Samek et al . `` Evaluating the visualization of what a deep neural network has learned . '' IEEE Transactions on Neural Networks and Learning Systems 2017 . Simonyan et al. \u201c Deep inside convolutional networks : visualising image classification models and saliency maps. \u201d ICLR Workshop 2014 . Zintgraf et al. \u201c Visualizing deep neural network decisions : prediction difference analysis. \u201d ICLR 2017 ."}, "2": {"review_id": "rkl3m1BFDB-2", "review_text": "The paper has a double aim. First, it is a survey on saliency maps used in explaining deep reinforcement learning models. Second, it is a proposal of a method that should overcome limitations of the current approaches described in the survey. This double aim makes the paper hard to understand as the survey is not complete and the model is not well explained. The main limitations the novel model aims to solve seems to be the production of \"falsifiable\" hypothesis in the explaination with saliency maps. However, experiments are really hard to follow and it is not clear why this is the case.", "rating": "1: Reject", "reply_text": "We appreciate Reviewer 3 \u2019 s comments . The review has several main points that we address below : Double aim of the paper \u2014 Our principal contribution is a new method for empirical evaluation of explanations generated from saliency maps about the behavior of deep RL agents . We intentionally structured our paper to include both a survey of current practice and an application of our proposed approach . Both elements were intended to aid reader understanding . The survey describes the inferences that require evaluation , and the application demonstrates the surprising conclusions supported by the evaluation method . We have attempted to improve our description of this approach ( see \u201c Clarity \u201d below ) . Completeness of the survey \u2014 As we note in section 3 , we surveyed 90 papers , each of which cited one or more key papers that described one of four saliency map methods . We would be happy to include additional papers in our survey , expand our survey criteria , or consider additional updates to the survey , if Reviewer 3 could provide specific suggestions . Clarity \u2014 We have added additional details to Section 4 on how saliency is measured . We have also added more quantitative results from the experiments to Section 5 and Appendix E ( see Tables 3 , 6 , 7 and 8 ) , and more details on the model used for training in Section 5 and Appendix B . We hope these additions make the paper more clear , and we welcome additional recommendations ."}}