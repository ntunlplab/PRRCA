{"year": "2020", "forum": "rygMWT4twS", "title": "Stochastic Gradient Descent with Biased but Consistent Gradient Estimators", "decision": "Reject", "meta_review": "This paper analyzes the convergence of SGD with a biased yet consistent gradient estimator. The main result is that this biased estimator results in the same convergence rate as does using unbiased ones. The main application is on learning representations on graphs (e.g., GCNs), and FastGCN is a closely related work. I agree that this paper has valuable contributions, but it can be further strengthened by considering the review comments, such as on the key assumptions.", "reviews": [{"review_id": "rygMWT4twS-0", "review_text": "The authors study SGD algorithms for problems where obtaining unbiased gradients is potentially computationally expensive. In such cases while obtaining, unbiased gradients is expensive, it might be possible to establish consistent estimators of the gradient. The authors then establish that SGD algorithm when run with consistent gradient estimators (but not necessarily unbiased) have similar convergence properties as SGD algorithms when run with unbiased gradient estimators. The example problem class considered is the problem of learning embeddings for graph problems, where the task is to get embeddings for nodes. Such embeddings can be used to do node classification or solve any other downstream task that involves the nodes of the graph. For such graph problems learning embeddings requires us to look at the neighbours of a node, neighbours-of-neighbours and so on, which means that in the worst case calculating gradient w.r.t. a single node can be of time complexity O(N). Consistent gradient estimators have been proposed for such graph problems in the past but this paper establishes theoretical properties of SGD with such estimators. The paper is well written and the results are convincing. I have a few questions/comments 1. In all the experimental results the loss curves are shown w.r.t. the number of epochs. It is clear that using unbiased SGD, unbiased ADAM is better of than using biased SGD. However, these plots do not tell the complete story as the key point behind using consistent SGD is not achieving lower loss, but actually faster computation. I would suggest that the authors show run-time plots that show how the run-time scales with epochs. 2. I appreciate the authors efforts in explaining their assumptions and how different assumptions kick in. 3. I wonder if a similar methodology can be applied even to the case of ranking problems (say rank net, see reference below). In ranknet training proceeds via choosing a pair-of-documents and performing gradient updates w.r.t. the pair. However, if one were to pick a single document, the gradient update w.r.t. that document (d1) should involve all other documents (d2) that are less relevant than d1. My question is does applying the consistent gradient methodology in this paper reveal a new algorithm for training ranknets? ", "rating": "6: Weak Accept", "reply_text": "Thank you very much for the suggestive comments . We appreciate them a lot . RE : Time versus epoch . Indeed , the advantage of consistent SGD lies in the smaller computational cost . We do , however , caution that ( 1 ) a lower loss does not necessarily mean a better classification accuracy , in light of the balance between optimization error , hypothesis class error , and sampling error ; and ( 2 ) in practice , early stopping is used when one observes that the validation error ( not training error ) does not improve . Hence , timing is a tricky game . An extensive timing comparison has been reported by FastGCN ( Chen et al.2018 ) .In this paper , for completeness we report the per epoch time to validate the computational advantage of sampling . This new information is in Appendix B.4 of the updated paper . RE : Application to ranking . Very interesting application and the answer is affirmative ! Consider eqn ( 8 ) of the paper https : //papers.nips.cc/paper/2971-learning-to-rank-with-nonsmooth-cost-functions.pdf . Here , s_i means the ranking function of document i ( and similarly for s_j ) and C_ { i , j } ^R is the ranking loss for a pair { i , j } . The overall RankNet loss is a double sum over i and j . The referenced paper proposes speeding up training by taking the sum over j first and then over i ( see eqn ( 7 ) ) . This process is still O ( n^2 ) but the authors argue that the n^2 calculation of C_ { i , j } ^R is far cheaper than the O ( n ) forward computation of s_i and O ( n ) backward propagation . By using our methodology , we propose a stochastic training whereby one samples i and j separately and backpropagates the mini-batch sample loss . Because of nonlinearity in ( 8 ) , the separate samplings of i and j do not result in an unbiased gradient ; rather , the gradient estimate can be consistent , following a similar argument to the proof of Theorem 1 in our paper . Then , our stochastic training theory applies . We have included such discussion in the conclusion section of the paper ."}, {"review_id": "rygMWT4twS-1", "review_text": "The paper studies stochastic optimization with consistent (may not be unbiased) estimators. This problem is well-motivated through the example of learning graph representations where consistent estimators are easier to obtain than unbiased one. Under the assumption that the estimate converges to the consistent gradient exponentially fast w.r.t. the sample size, the authors give convergence rates for convex, strongly-convex and non-convex optimization. The authors validate their theory through synthetic experiments. Overall, the paper is well-motivated and well-written however it lacks technically novelty. Under the assumption of exponentially fast convergence to small error, the setup is more like gradient descent (have access to approximate full gradient) than stochastic gradient descent as the paper supposes. The main convergence theorems seem to follow from standard techniques for inexact/noisy gradients. In [1], convergence rates for various first-order methods are proven under the assumption that the error is additive, that is, ||g - h|| <= \\delta. Since the authors implicitly convert the multiplicative error to an additive error in their analysis, their assumptions are comparable to [1]. Also, since the analysis is more like GD, in the strongly-convex setting one can actually get faster convergence rates (logarithmic) as long as \\delta is small (in comparison to the strong convexity parameter) unlike the O(1/T) ones mentioned in the paper. Additional comments: Assumption - There should be an additive error along with the multiplicative error as in the current setup. If ||h|| is very small then according to the assumption, the estimates of the gradient are very tight; this may not be true. Also, this assumption seems to only be needed for sample complexity purposes, making the tails weaker would only give a larger sample complexity without affecting the convergence rates. Would be good to separate these. Convergence Analysis - As mentioned above, since the setting is like GD with noisy gradients, a more careful analysis in the strongly convex setting can improve the convergence result. Refer [2] for the standard analysis without noise. Upper bound on l - In Thm 2, the authors assume l <= G/||w_1 \u2212 w^*||. Why is this needed? Increasing l should make the problem more convex and easier. [1] Devolder, Olivier, Fran\u00e7ois Glineur, and Yurii Nesterov. \"First-order methods of smooth convex optimization with inexact oracle.\" Mathematical Programming 146, no. 1-2 (2014): 37-75. [2] Robert M. Gower. Convergence Theorems for Gradient Descent. https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf", "rating": "1: Reject", "reply_text": "Thank you very much for the critical comments . We respectfully debate regarding the significance of the paper , as well as the connection to deterministic gradient descent and the gradient error assumption . We also answer other comments and update the paper with an additional result regarding linear convergence . We will be happy to discuss further to achieve a consensus understanding . Thank you . RE : Significance of the work . Technical matter aside , we consider that the primary contribution of the work is the introduction of a novel problem of practical significance in machine learning , together with a framework with which the problem may be approached . As outlined in Section 1.1 , unbiased gradients are not always a viable choice in training machine learning models . Consistent gradients are one rescue , whose effectiveness has been corroborated in the training of graph convolutional networks ( Chen et al. , 2018 ) . We formalize the problem in this paper and show that traditional analysis of stochastic gradients can be adapted when probability concentration is taken into account ( as is often the case by using Chernoff bounds or existence of moments ) . We do believe that the problem of consistent gradients is a healthy introduction to the field , as evident in part by the increasing independent citations of the work prior to being formally published . RE : More similar to gradient descent than stochastic gradient descent ? It is true that if the gradient error is sufficiently small , consistent gradient behaves like full gradient and hence the theory should be consistent with that of deterministic gradient descent asymptotically . However , such as a viewpoint does not devalue the theory of consistent gradient . Here is why . One may similarly argue that for unbiased gradient , if the variance is sufficiently small then it also behaves like full gradient . But this understanding should not undermine the significance of stochastic gradient descent theory developed over the past several decades . Drawing analogy , the minibatch size affects the variance of the unbiased gradient estimate ; and the sample size affects the failure probability for the use of consistent gradient . It is these interesting factors that make stochastic gradient descent and our work distinct from deterministic gradient descent . RE : Faster convergence rate . Indeed one may prove linear convergence for the strongly convex case , at the cost of a positive right-hand side term that prevents the error from going to zero . This situation is similar to standard SGD , evidenced by Theorem 3.2 of the paper [ 2 ] that you reference , as well as other papers such as Bottou et al . ( 2016 ) .Note that the value of such results is debatable . These results mean that in the limit , the difference between the iterate and the optimal solution is bounded by a nonnegligible positive term . However , in fact , both standard SGD analysis and our analysis with consistent gradients show that the difference can go to zero , albeit at a sublinear speed . In this regard , linear convergence bounds are much weaker than sublinear convergence bounds . On the other hand , one may argue from a machine learning perspective , that optimization error should be balanced with model class error and sampling error . In this perspective , having a nonzero optimization error is not that bad and the crucial point is that one may attain this error in a linear rate . We acknowledge both viewpoints . We have included the linear convergence result and discussions in the updated version of the paper . See Theorem 4 . RE : Assumption on multiplicative gradient error . We believe that this assumption functions as well as the use of the additive form . Changing from the multiplicative form in ( 5 ) to an additive form only means that the gradient error is scaled by ||h_k|| . Such a scaling can be absorbed in the right-hand side , leading to a different expression for the sample size N_k ( that is , the new N_k is related to the old N_k by some expression of ||h_k|| and \\delta ) . Such a change does not offer new insight but will make subsequent bounds look extremely complicated . Hence , using the additive form does not seem a good idea . You mention that \u201c If ||h|| is very small then according to the assumption , the estimates of the gradient are very tight ; this may not be true. \u201d We agree that when ||h|| is small , the probability may be small given the same \\delta , but this probability will be reflected in the sample size N_k , so * IT IS ALWAYS TRUE * . We also concur that some work uses the additive form , such as reference [ 1 ] you mentioned . However , specifically for that work , it analyzes a proximal-kind of algorithm rather than the standard ( stochastic ) gradient algorithm . Although both are first-order methods , the technique does not seem straightforwardly extensible from one to the other . Reference [ 1 ] is nevertheless valuable and we have cited it in the updated paper ."}, {"review_id": "rygMWT4twS-2", "review_text": "The analysis of the convergence of SGD with biases gradient estimates dates back to Robins&Monroe, but the authors of this paper focused on a recent original algorithm that shows that once can estimate the approximate gradient of a large GNN network, simply by sampling nodes randomly. When I first read of the paper, I was enthusiastic because I did not know the FastGCN approach presented at ICLR the previous year, which showed that the gradient of a GCN could be efficiently approximated by sampling a subset of the nodes. After reading FastGCN, I was less enthusiastic as most of the originality relied on the consistent estimate of the gradient, when t (number of sampled in the neighbours of the output nodes) increases. The main contribution of the paper is the proof that the algorithm converge, but there is no theoretical analysis of the key quantity \"t\", which is the number of sampled nodes in the neighbours of the output nodes. I would expect to see the number of sample grow as the algorithm converge to the optimal solution since the gradient needs less bias when the algorithm converges. However, the authors do not address this point. I did not into the details of the proofs, but it seems to me that they are quite loose and several details such as the functional spaces, and the boundedness assumptions, are not mentioned. Here are few examples: - In the first sentence: P(x, y) of data x and associated label y. The space of x, the space of y and the probability space are not defined. In fact, no set in which variables belong is defined in the paper. - The Theorem 1 is strange to me. I would assume that one needs some assumption of boundedness of Q and finite moments for P to avoid pathological examples where the integral (for the asymptotic expectation) is infinite, but the finite sum G_{st} is always finite, contradicting the limit in theorem 1. Overall, while proving that the FastGCN algorithm is consistent is important, it is hard to understand how useful the results are and how they can be useful in practice. For example, what can we interpret or what can we learn from the bounds given by Theorem 2? Finally, I might miss something, but the empirical results showed do not seem to show better gains than the Adam algorithm. The theory shows that the more bias we have, the less accurate we should be, why isn't it apparent in Table 1. Is there something such as the computational cost of Adam, that I'm missing, especially when looking at the graphs? I'm sorry if I did not get the main message of the experiments, but even after reading the paper 3 times, I did not understand what the authors wanted the reader to conclude with these experiments. ", "rating": "3: Weak Reject", "reply_text": "Thank you very much for the comments . In what follows , we address them point by point . We will be happy to clear your doubts should more questions arise . RE : Biased , unbiased , and consistent . At the beginning of your comments , you mentioned that analysis of SGD with biased gradient dates back to Robins & Monroe . It may be a typo , but in case confusion arises , we would like to stress that along the history , most analysis was for the * unbiased * case , rather than the biased case . The biased case was sporadically addressed while analysis of the consistent case was never seen before . The novelty and main contribution of this paper is the analysis of the consistent case . A minor note is that , whereas we prove that in FastGCN the gradient estimator is consistent , it is only one motivating application ; indeed , Reviewer # 1 even suggested another application , training RankNets , which we have added to the discussion in the paper . Our focus is to prove optimization convergence under consistent gradient estimators , regardless what application they are meant for . RE : Sample size \u201c t \u201d and convergence , part 1 . Sample size is indeed a key quantity . First , we note that the sample size does not need to increase as the algorithm approaches convergence . The gradient does not need to be less and less biased , either . The analogy to the standard unbiased SGD is that the gradient variance does not need to shrink on approaching convergence . For our case , the analysis is based on convergence under high probability , as elaborated below . RE : Sample size \u201c t \u201d and convergence , part 2 . The analysis strategy we employ is to show that with probability related to the sample size , convergence occurs at a certain rate . To conform to notational convention , in most of the places we use N to denote the sample size . In all theorems , the probability ( 1-\\epsilon ) under which convergence occurs is related to N in equation ( 6 ) . Just like most of the concentration analysis in the literature , the results read that with a sufficiently large sample size N , with probability at least 1 - epsilon ( N ) , a certain fact holds . Our results follow the same format . RE : Space of P ( x , y ) . The notion of joint probability distribution P ( x , y ) of data x and label y comes from standard statistical learning theory . One may assume that x belongs to a vector space ( e.g. , R^d ) and y is a real-valued scalar either bounded or unbounded , and either continuous or discrete . P is a probability distribution over the product space of x and y . In reality , P is unknown . RE : Assumptions of Theorem 1 . The most important assumption for Theorem 1 to hold is that q is continuous ( as stated ) , since the proof invokes the continuous mapping theorem . The proof also invokes the law of large numbers , which could either be the weak law or the strong law , since the plim result follows the weak law . Both laws require only that the expectation is finite ( eqn ( 4 ) ) . No higher order moments need be finite and no boundedness assumption need be made . We have updated the paper with the assumption that f is finite . RE : Interpretations of results ( e.g. , Theorem 2 ) . Overall , our convergence results provide a guarantee that running SGD with consistent gradient estimators will converge , just like the traditional case of unbiased gradient . But unlike the traditional case , convergence does not occur in the expectation sense ; rather , it occurs in a probabilistic sense . The more samples one uses to estimate the gradient , the higher the probability that convergence occurs . Meanwhile , the convergence rate is the same as for the case of unbiased gradients . For example , Theorem 2 states that the convergence rate is O ( 1/T ) and the probability is 1-\\epsilon , where \\epsilon is a decreasing function of the sample size N in equation ( 6 ) . RE : Empirical comparison with Adam and computational cost . The purpose of this paper is not to propose an algorithm faster than Adam , but rather to show that if one is unable to efficiently compute the unbiased gradient estimates that Adam needs , then it is still possible to use other gradient estimators such that SGD converges . In the FastGCN case , such a gradient estimate comes from sampling the neighbors and it is a consistent estimator . Indeed , we do not expect that SGD converges to a better place than does Adam . The point is that computing the consistent gradient estimate is more efficient than computing the unbiased estimate , as elaborated in Sections 1 and 2 . Further , please see timing results in Appendix B.4 that validate the computational advantage . RE : Table 1 . As such , using consistent gradient estimators will not result in more accurate parameters or classification accuracies . They are just faster to compute . The numbers in Table 1 show that while one uses less effort to perform the computation , the accuracies are not sacrificed ."}, {"review_id": "rygMWT4twS-3", "review_text": "This paper aims to solve the stochastic optimization problems in machine learning where the unbiased gradient estimator is expensive to compute, and instead use a consistent gradient estimator. The main contributions are the convergence analyses of the consistent gradient estimator for different objectives (i.e., convex, strongly convex, and non-convex). Overall, it is interesting and important, but I still have some concerns. Which objective function do the authors aim to minimize, the expected risk or empirical risk? I guess it's the empirical risk, right? If so, the sample size $N$ is constant. This may break the condition (sufficiently large sample sizes satisfying (6)) of theorems (i.e., Theorem 2, 3, 4, 5). Thus, it will narrow the application domains of the theorem. For the proofs of theorems, the main difference between SGD and this paper is the involvement of Lemma 9, which is one part of the assumption. Besides, this assumption involves the failure probability $\\epsilon$. The convergence theorem (e.g., Theorem 2) has a probability condition to hold the Eq.(7) (or (8)). Maybe some comments below the theorem can be discussed to decrease $\\epsilon$, although authors discuss it in experiments (\"This phenomenon qualitatively agrees with the theoretical results; namely, larger sample size improves the error bound.\"). For the experiments, the authors focus on the training of GCN model. I think it can be considered a doubly stochastic sampling process, which is one for the sample and the other for its neighbor. Is that right? Besides, for Figure 1, can the \"SGD unbiased\" be viewed as \"SGD consistent (sampl $n$)\"? If not, I think it's important to compare these two because this will clearly show the performance difference between the unbiased and consistent estimator. ", "rating": "3: Weak Reject", "reply_text": "Thank you very much for the questions . In what follows we answer them one by one and hope that the replies help reassess the work . RE : Expected risk or empirical risk ? Regarding the sample size , indeed the framework of consistent gradient is more amenable to the expected risk setting , because the sample size in empirical risk is often finite . However , this is not always the case . One may consider an online setting where samples are streamed in . For example , for dynamic graphs where the node set endlessly grows , one may face a large graph such that under a desired probability , the sufficiently large sample size is met . In practice , the required sample size is often overly conservatively estimated . As we have shown in the numerical examples , even a small sample size yields sufficiently good classification results . Furthermore , even when the sample size is finite , the bounds are still useful and can be used to say with what probability we expect convergence to hold . RE : Decreasing the probability \\epsilon . Indeed , like most work in other fields based on probability concentration ( e.g. , in the field of randomized linear algebra ) , the failure probability \\epsilon is a quantity controlled by the sample size . Such discussion also appears in Section 4.3 following the theorems . RE : Doubly stochastic process . In a sense , the training of the GCN model in the FastGCN way may be interpreted as a doubly stochastic process , bearing certain similarity with the Doubly Stochastic Gradients work by Dai et al in NIPS 2014 . In their work , one stochasticity occurs in the sample and the other occurs in the approximation of the kernel . A major distinction is that in their work , the double stochastic sampling remains unbiased because the functional gradient is linear , whereas in our case , because of the nonlinearity of the neural network , unbiasedness is lost . One obtains only consistency . In fact , the lack of unbiasedness is the major motivation for us to develop theory for consistent gradients . In the particular case of GCN , indeed \u201c SGD unbiased \u201d implies \u201c SGD consistent \u201d ( note , no sample size regarding neighbors is involved in the unbiased estimator ) . One may understand this fact by simply invoking the law of large numbers on the samples ."}], "0": {"review_id": "rygMWT4twS-0", "review_text": "The authors study SGD algorithms for problems where obtaining unbiased gradients is potentially computationally expensive. In such cases while obtaining, unbiased gradients is expensive, it might be possible to establish consistent estimators of the gradient. The authors then establish that SGD algorithm when run with consistent gradient estimators (but not necessarily unbiased) have similar convergence properties as SGD algorithms when run with unbiased gradient estimators. The example problem class considered is the problem of learning embeddings for graph problems, where the task is to get embeddings for nodes. Such embeddings can be used to do node classification or solve any other downstream task that involves the nodes of the graph. For such graph problems learning embeddings requires us to look at the neighbours of a node, neighbours-of-neighbours and so on, which means that in the worst case calculating gradient w.r.t. a single node can be of time complexity O(N). Consistent gradient estimators have been proposed for such graph problems in the past but this paper establishes theoretical properties of SGD with such estimators. The paper is well written and the results are convincing. I have a few questions/comments 1. In all the experimental results the loss curves are shown w.r.t. the number of epochs. It is clear that using unbiased SGD, unbiased ADAM is better of than using biased SGD. However, these plots do not tell the complete story as the key point behind using consistent SGD is not achieving lower loss, but actually faster computation. I would suggest that the authors show run-time plots that show how the run-time scales with epochs. 2. I appreciate the authors efforts in explaining their assumptions and how different assumptions kick in. 3. I wonder if a similar methodology can be applied even to the case of ranking problems (say rank net, see reference below). In ranknet training proceeds via choosing a pair-of-documents and performing gradient updates w.r.t. the pair. However, if one were to pick a single document, the gradient update w.r.t. that document (d1) should involve all other documents (d2) that are less relevant than d1. My question is does applying the consistent gradient methodology in this paper reveal a new algorithm for training ranknets? ", "rating": "6: Weak Accept", "reply_text": "Thank you very much for the suggestive comments . We appreciate them a lot . RE : Time versus epoch . Indeed , the advantage of consistent SGD lies in the smaller computational cost . We do , however , caution that ( 1 ) a lower loss does not necessarily mean a better classification accuracy , in light of the balance between optimization error , hypothesis class error , and sampling error ; and ( 2 ) in practice , early stopping is used when one observes that the validation error ( not training error ) does not improve . Hence , timing is a tricky game . An extensive timing comparison has been reported by FastGCN ( Chen et al.2018 ) .In this paper , for completeness we report the per epoch time to validate the computational advantage of sampling . This new information is in Appendix B.4 of the updated paper . RE : Application to ranking . Very interesting application and the answer is affirmative ! Consider eqn ( 8 ) of the paper https : //papers.nips.cc/paper/2971-learning-to-rank-with-nonsmooth-cost-functions.pdf . Here , s_i means the ranking function of document i ( and similarly for s_j ) and C_ { i , j } ^R is the ranking loss for a pair { i , j } . The overall RankNet loss is a double sum over i and j . The referenced paper proposes speeding up training by taking the sum over j first and then over i ( see eqn ( 7 ) ) . This process is still O ( n^2 ) but the authors argue that the n^2 calculation of C_ { i , j } ^R is far cheaper than the O ( n ) forward computation of s_i and O ( n ) backward propagation . By using our methodology , we propose a stochastic training whereby one samples i and j separately and backpropagates the mini-batch sample loss . Because of nonlinearity in ( 8 ) , the separate samplings of i and j do not result in an unbiased gradient ; rather , the gradient estimate can be consistent , following a similar argument to the proof of Theorem 1 in our paper . Then , our stochastic training theory applies . We have included such discussion in the conclusion section of the paper ."}, "1": {"review_id": "rygMWT4twS-1", "review_text": "The paper studies stochastic optimization with consistent (may not be unbiased) estimators. This problem is well-motivated through the example of learning graph representations where consistent estimators are easier to obtain than unbiased one. Under the assumption that the estimate converges to the consistent gradient exponentially fast w.r.t. the sample size, the authors give convergence rates for convex, strongly-convex and non-convex optimization. The authors validate their theory through synthetic experiments. Overall, the paper is well-motivated and well-written however it lacks technically novelty. Under the assumption of exponentially fast convergence to small error, the setup is more like gradient descent (have access to approximate full gradient) than stochastic gradient descent as the paper supposes. The main convergence theorems seem to follow from standard techniques for inexact/noisy gradients. In [1], convergence rates for various first-order methods are proven under the assumption that the error is additive, that is, ||g - h|| <= \\delta. Since the authors implicitly convert the multiplicative error to an additive error in their analysis, their assumptions are comparable to [1]. Also, since the analysis is more like GD, in the strongly-convex setting one can actually get faster convergence rates (logarithmic) as long as \\delta is small (in comparison to the strong convexity parameter) unlike the O(1/T) ones mentioned in the paper. Additional comments: Assumption - There should be an additive error along with the multiplicative error as in the current setup. If ||h|| is very small then according to the assumption, the estimates of the gradient are very tight; this may not be true. Also, this assumption seems to only be needed for sample complexity purposes, making the tails weaker would only give a larger sample complexity without affecting the convergence rates. Would be good to separate these. Convergence Analysis - As mentioned above, since the setting is like GD with noisy gradients, a more careful analysis in the strongly convex setting can improve the convergence result. Refer [2] for the standard analysis without noise. Upper bound on l - In Thm 2, the authors assume l <= G/||w_1 \u2212 w^*||. Why is this needed? Increasing l should make the problem more convex and easier. [1] Devolder, Olivier, Fran\u00e7ois Glineur, and Yurii Nesterov. \"First-order methods of smooth convex optimization with inexact oracle.\" Mathematical Programming 146, no. 1-2 (2014): 37-75. [2] Robert M. Gower. Convergence Theorems for Gradient Descent. https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf", "rating": "1: Reject", "reply_text": "Thank you very much for the critical comments . We respectfully debate regarding the significance of the paper , as well as the connection to deterministic gradient descent and the gradient error assumption . We also answer other comments and update the paper with an additional result regarding linear convergence . We will be happy to discuss further to achieve a consensus understanding . Thank you . RE : Significance of the work . Technical matter aside , we consider that the primary contribution of the work is the introduction of a novel problem of practical significance in machine learning , together with a framework with which the problem may be approached . As outlined in Section 1.1 , unbiased gradients are not always a viable choice in training machine learning models . Consistent gradients are one rescue , whose effectiveness has been corroborated in the training of graph convolutional networks ( Chen et al. , 2018 ) . We formalize the problem in this paper and show that traditional analysis of stochastic gradients can be adapted when probability concentration is taken into account ( as is often the case by using Chernoff bounds or existence of moments ) . We do believe that the problem of consistent gradients is a healthy introduction to the field , as evident in part by the increasing independent citations of the work prior to being formally published . RE : More similar to gradient descent than stochastic gradient descent ? It is true that if the gradient error is sufficiently small , consistent gradient behaves like full gradient and hence the theory should be consistent with that of deterministic gradient descent asymptotically . However , such as a viewpoint does not devalue the theory of consistent gradient . Here is why . One may similarly argue that for unbiased gradient , if the variance is sufficiently small then it also behaves like full gradient . But this understanding should not undermine the significance of stochastic gradient descent theory developed over the past several decades . Drawing analogy , the minibatch size affects the variance of the unbiased gradient estimate ; and the sample size affects the failure probability for the use of consistent gradient . It is these interesting factors that make stochastic gradient descent and our work distinct from deterministic gradient descent . RE : Faster convergence rate . Indeed one may prove linear convergence for the strongly convex case , at the cost of a positive right-hand side term that prevents the error from going to zero . This situation is similar to standard SGD , evidenced by Theorem 3.2 of the paper [ 2 ] that you reference , as well as other papers such as Bottou et al . ( 2016 ) .Note that the value of such results is debatable . These results mean that in the limit , the difference between the iterate and the optimal solution is bounded by a nonnegligible positive term . However , in fact , both standard SGD analysis and our analysis with consistent gradients show that the difference can go to zero , albeit at a sublinear speed . In this regard , linear convergence bounds are much weaker than sublinear convergence bounds . On the other hand , one may argue from a machine learning perspective , that optimization error should be balanced with model class error and sampling error . In this perspective , having a nonzero optimization error is not that bad and the crucial point is that one may attain this error in a linear rate . We acknowledge both viewpoints . We have included the linear convergence result and discussions in the updated version of the paper . See Theorem 4 . RE : Assumption on multiplicative gradient error . We believe that this assumption functions as well as the use of the additive form . Changing from the multiplicative form in ( 5 ) to an additive form only means that the gradient error is scaled by ||h_k|| . Such a scaling can be absorbed in the right-hand side , leading to a different expression for the sample size N_k ( that is , the new N_k is related to the old N_k by some expression of ||h_k|| and \\delta ) . Such a change does not offer new insight but will make subsequent bounds look extremely complicated . Hence , using the additive form does not seem a good idea . You mention that \u201c If ||h|| is very small then according to the assumption , the estimates of the gradient are very tight ; this may not be true. \u201d We agree that when ||h|| is small , the probability may be small given the same \\delta , but this probability will be reflected in the sample size N_k , so * IT IS ALWAYS TRUE * . We also concur that some work uses the additive form , such as reference [ 1 ] you mentioned . However , specifically for that work , it analyzes a proximal-kind of algorithm rather than the standard ( stochastic ) gradient algorithm . Although both are first-order methods , the technique does not seem straightforwardly extensible from one to the other . Reference [ 1 ] is nevertheless valuable and we have cited it in the updated paper ."}, "2": {"review_id": "rygMWT4twS-2", "review_text": "The analysis of the convergence of SGD with biases gradient estimates dates back to Robins&Monroe, but the authors of this paper focused on a recent original algorithm that shows that once can estimate the approximate gradient of a large GNN network, simply by sampling nodes randomly. When I first read of the paper, I was enthusiastic because I did not know the FastGCN approach presented at ICLR the previous year, which showed that the gradient of a GCN could be efficiently approximated by sampling a subset of the nodes. After reading FastGCN, I was less enthusiastic as most of the originality relied on the consistent estimate of the gradient, when t (number of sampled in the neighbours of the output nodes) increases. The main contribution of the paper is the proof that the algorithm converge, but there is no theoretical analysis of the key quantity \"t\", which is the number of sampled nodes in the neighbours of the output nodes. I would expect to see the number of sample grow as the algorithm converge to the optimal solution since the gradient needs less bias when the algorithm converges. However, the authors do not address this point. I did not into the details of the proofs, but it seems to me that they are quite loose and several details such as the functional spaces, and the boundedness assumptions, are not mentioned. Here are few examples: - In the first sentence: P(x, y) of data x and associated label y. The space of x, the space of y and the probability space are not defined. In fact, no set in which variables belong is defined in the paper. - The Theorem 1 is strange to me. I would assume that one needs some assumption of boundedness of Q and finite moments for P to avoid pathological examples where the integral (for the asymptotic expectation) is infinite, but the finite sum G_{st} is always finite, contradicting the limit in theorem 1. Overall, while proving that the FastGCN algorithm is consistent is important, it is hard to understand how useful the results are and how they can be useful in practice. For example, what can we interpret or what can we learn from the bounds given by Theorem 2? Finally, I might miss something, but the empirical results showed do not seem to show better gains than the Adam algorithm. The theory shows that the more bias we have, the less accurate we should be, why isn't it apparent in Table 1. Is there something such as the computational cost of Adam, that I'm missing, especially when looking at the graphs? I'm sorry if I did not get the main message of the experiments, but even after reading the paper 3 times, I did not understand what the authors wanted the reader to conclude with these experiments. ", "rating": "3: Weak Reject", "reply_text": "Thank you very much for the comments . In what follows , we address them point by point . We will be happy to clear your doubts should more questions arise . RE : Biased , unbiased , and consistent . At the beginning of your comments , you mentioned that analysis of SGD with biased gradient dates back to Robins & Monroe . It may be a typo , but in case confusion arises , we would like to stress that along the history , most analysis was for the * unbiased * case , rather than the biased case . The biased case was sporadically addressed while analysis of the consistent case was never seen before . The novelty and main contribution of this paper is the analysis of the consistent case . A minor note is that , whereas we prove that in FastGCN the gradient estimator is consistent , it is only one motivating application ; indeed , Reviewer # 1 even suggested another application , training RankNets , which we have added to the discussion in the paper . Our focus is to prove optimization convergence under consistent gradient estimators , regardless what application they are meant for . RE : Sample size \u201c t \u201d and convergence , part 1 . Sample size is indeed a key quantity . First , we note that the sample size does not need to increase as the algorithm approaches convergence . The gradient does not need to be less and less biased , either . The analogy to the standard unbiased SGD is that the gradient variance does not need to shrink on approaching convergence . For our case , the analysis is based on convergence under high probability , as elaborated below . RE : Sample size \u201c t \u201d and convergence , part 2 . The analysis strategy we employ is to show that with probability related to the sample size , convergence occurs at a certain rate . To conform to notational convention , in most of the places we use N to denote the sample size . In all theorems , the probability ( 1-\\epsilon ) under which convergence occurs is related to N in equation ( 6 ) . Just like most of the concentration analysis in the literature , the results read that with a sufficiently large sample size N , with probability at least 1 - epsilon ( N ) , a certain fact holds . Our results follow the same format . RE : Space of P ( x , y ) . The notion of joint probability distribution P ( x , y ) of data x and label y comes from standard statistical learning theory . One may assume that x belongs to a vector space ( e.g. , R^d ) and y is a real-valued scalar either bounded or unbounded , and either continuous or discrete . P is a probability distribution over the product space of x and y . In reality , P is unknown . RE : Assumptions of Theorem 1 . The most important assumption for Theorem 1 to hold is that q is continuous ( as stated ) , since the proof invokes the continuous mapping theorem . The proof also invokes the law of large numbers , which could either be the weak law or the strong law , since the plim result follows the weak law . Both laws require only that the expectation is finite ( eqn ( 4 ) ) . No higher order moments need be finite and no boundedness assumption need be made . We have updated the paper with the assumption that f is finite . RE : Interpretations of results ( e.g. , Theorem 2 ) . Overall , our convergence results provide a guarantee that running SGD with consistent gradient estimators will converge , just like the traditional case of unbiased gradient . But unlike the traditional case , convergence does not occur in the expectation sense ; rather , it occurs in a probabilistic sense . The more samples one uses to estimate the gradient , the higher the probability that convergence occurs . Meanwhile , the convergence rate is the same as for the case of unbiased gradients . For example , Theorem 2 states that the convergence rate is O ( 1/T ) and the probability is 1-\\epsilon , where \\epsilon is a decreasing function of the sample size N in equation ( 6 ) . RE : Empirical comparison with Adam and computational cost . The purpose of this paper is not to propose an algorithm faster than Adam , but rather to show that if one is unable to efficiently compute the unbiased gradient estimates that Adam needs , then it is still possible to use other gradient estimators such that SGD converges . In the FastGCN case , such a gradient estimate comes from sampling the neighbors and it is a consistent estimator . Indeed , we do not expect that SGD converges to a better place than does Adam . The point is that computing the consistent gradient estimate is more efficient than computing the unbiased estimate , as elaborated in Sections 1 and 2 . Further , please see timing results in Appendix B.4 that validate the computational advantage . RE : Table 1 . As such , using consistent gradient estimators will not result in more accurate parameters or classification accuracies . They are just faster to compute . The numbers in Table 1 show that while one uses less effort to perform the computation , the accuracies are not sacrificed ."}, "3": {"review_id": "rygMWT4twS-3", "review_text": "This paper aims to solve the stochastic optimization problems in machine learning where the unbiased gradient estimator is expensive to compute, and instead use a consistent gradient estimator. The main contributions are the convergence analyses of the consistent gradient estimator for different objectives (i.e., convex, strongly convex, and non-convex). Overall, it is interesting and important, but I still have some concerns. Which objective function do the authors aim to minimize, the expected risk or empirical risk? I guess it's the empirical risk, right? If so, the sample size $N$ is constant. This may break the condition (sufficiently large sample sizes satisfying (6)) of theorems (i.e., Theorem 2, 3, 4, 5). Thus, it will narrow the application domains of the theorem. For the proofs of theorems, the main difference between SGD and this paper is the involvement of Lemma 9, which is one part of the assumption. Besides, this assumption involves the failure probability $\\epsilon$. The convergence theorem (e.g., Theorem 2) has a probability condition to hold the Eq.(7) (or (8)). Maybe some comments below the theorem can be discussed to decrease $\\epsilon$, although authors discuss it in experiments (\"This phenomenon qualitatively agrees with the theoretical results; namely, larger sample size improves the error bound.\"). For the experiments, the authors focus on the training of GCN model. I think it can be considered a doubly stochastic sampling process, which is one for the sample and the other for its neighbor. Is that right? Besides, for Figure 1, can the \"SGD unbiased\" be viewed as \"SGD consistent (sampl $n$)\"? If not, I think it's important to compare these two because this will clearly show the performance difference between the unbiased and consistent estimator. ", "rating": "3: Weak Reject", "reply_text": "Thank you very much for the questions . In what follows we answer them one by one and hope that the replies help reassess the work . RE : Expected risk or empirical risk ? Regarding the sample size , indeed the framework of consistent gradient is more amenable to the expected risk setting , because the sample size in empirical risk is often finite . However , this is not always the case . One may consider an online setting where samples are streamed in . For example , for dynamic graphs where the node set endlessly grows , one may face a large graph such that under a desired probability , the sufficiently large sample size is met . In practice , the required sample size is often overly conservatively estimated . As we have shown in the numerical examples , even a small sample size yields sufficiently good classification results . Furthermore , even when the sample size is finite , the bounds are still useful and can be used to say with what probability we expect convergence to hold . RE : Decreasing the probability \\epsilon . Indeed , like most work in other fields based on probability concentration ( e.g. , in the field of randomized linear algebra ) , the failure probability \\epsilon is a quantity controlled by the sample size . Such discussion also appears in Section 4.3 following the theorems . RE : Doubly stochastic process . In a sense , the training of the GCN model in the FastGCN way may be interpreted as a doubly stochastic process , bearing certain similarity with the Doubly Stochastic Gradients work by Dai et al in NIPS 2014 . In their work , one stochasticity occurs in the sample and the other occurs in the approximation of the kernel . A major distinction is that in their work , the double stochastic sampling remains unbiased because the functional gradient is linear , whereas in our case , because of the nonlinearity of the neural network , unbiasedness is lost . One obtains only consistency . In fact , the lack of unbiasedness is the major motivation for us to develop theory for consistent gradients . In the particular case of GCN , indeed \u201c SGD unbiased \u201d implies \u201c SGD consistent \u201d ( note , no sample size regarding neighbors is involved in the unbiased estimator ) . One may understand this fact by simply invoking the law of large numbers on the samples ."}}