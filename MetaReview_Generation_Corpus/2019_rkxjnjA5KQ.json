{"year": "2019", "forum": "rkxjnjA5KQ", "title": "Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation", "decision": "Reject", "meta_review": "The paper proposes an transfer learning approach to reinforcement learning, where observations from a target domain are mapped to a source domain in which the algorithm was originally trained. Using unsupervised GAN models to learn this mapping from unaligned samples, the authors show that such a mapping allows the RL agent to successfully interact with the target domain without further training (apart from training the GAN models). The approach is empirically validated on modified versions of the Atari game breakout, as well as subsequent levels of Road Fighter, showing good performance on the transfer domain with a fraction of the samples that would be required for retraining the RL algorithm from scratch. \n\nThe reviewers and AC note the strong motivation for this work and emphasize that they find the idea interesting and novel. Reviewer 3 emphasizes the detailed analysis and results. Reviewer 2 notes the innovative idea to evaluate GANs in this application domain. Reviewer 1 identifies a key contribution in the thorough empirical analysis of the generalization issues that plaque current RL algorithms, as well as the comparison between different GAN models and finding their performance to be task-specific.\n\nThe reviewers and AC noted several potential weaknesses: The proposed training based on images collected by an untrained agent focus the data on experience that agents would see very early on in the game, and may lead to generalization issues in more advanced parts of the game. Indeed these generalization issues are one possible explanation for the discrepancies between qualitative and quantitative results noted by reviewer 1. While the quantitative results indicate good performance on the target task, the image to image translation makes substantial errors, e.g., hallucinating blocks in breakout and erasing cars in Road Fighter. To the AC, the current paper does not provide enough insight into why the translation approach works even in cases where key elements are added or removed from the scene. The paper would benefit from a revision that thoroughly analyses such cases as well as the reason why the trained RL policy is able to generalize to them.\n\nR1 further notes that the paper does not address the RL generalization issue, but rather presents an empirical study that shows that in specific cases it is easier to translate from a target to a source domain, than to learn a policy for the target domain. The AC shares this concern, especially given the limited error analysis and conceptual insights derived from the empirical study. There are further concerns about the experimental protocol and hyper-parameter selection on the target tasks. Finally reviewer 1 questions the claim of whether data efficiency matters more than training efficiency in the proposed setting.\n\nThere is disagreement about this paper. Reviewers 2 and 3 gave high scores and positive reviews, but did not provide sufficient feedback to the concerns raised by reviewer 1, who put forward significant concerns. \n\nThe AC is particularly concerned about the experimental protocol and hyper-parameter tuning directly on the test tasks. The authors counter this point by noting that \"We agree that selecting configurations based on the test set is far from ideal, but we also note that this is the de-facto standard in video game-playing RL works, so we do not believe our work is any worse than others in the literature in this regard.\" The AC worries about the lack of motivation to identify a strong empirical setup to arrive at the strongest possible contribution. A key concern here is that the results seem to vary substantially by task, GAN model used, etc. and substantial tuning on the target domain seems to be required. This makes it hard to draw any generalizable conclusions. This concern can be alleviated by including additional analysis, e.g., error analysis of where a proposed approach fails, or additional experiments designed to isolate the factors that contribute to a particular performance level. However, the current paper does not go to this detail of empirical exploration. Given these concerns, I recommend not accepting the paper at the current stage.", "reviews": [{"review_id": "rkxjnjA5KQ-0", "review_text": "# Summary This paper proposes to improve the sample efficiency of transfer learning for Deep RL by mapping a new visual domain (target) onto the training one (source) using GANs. First, a deep RL policy is trained on a source domain (e.g., level 1 of the Atari Road Fighter game). Second, a GAN (e.g. UNIT or CycleGAN) is trained for unsupervised domain adaptation from target images (e.g., level 2 of Road Fighter) to source ones. Third, the policy learned in the source domain is applied directly on the GAN-translated target domain. The experimental evaluation uses two Atari games: i) transfer from Breakout to Breakout with static visual distractors inpainted on the screen, ii) from one Road Fighter level to others. Results suggest that this transfer learning approach requires less images than retraining from scratch in the new domain, including when fine-tuning does not work. # Strengths Controlled toy experiments of Deep RL generalization issues: The experiments on Breakout quantify how badly A3C overfits in this case, as it shows catastrophic performance degradation even with trivial static visual input perturbations (which are not even adversarial attacks). The fine-tuning experiments also quantify well how brittle the initial policy is, motivating further the importance of the problem studied by the paper. Investigating the impact of different GANs on the end task: The experiments evaluate two different image translation algorithms: one based on UNIT, the other based on CycleGAN. The results suggest that this choice is key and depends on the target domain. This suggests that the adaptation is in fact task dependent, confirming the direction pursued by others in task-specific unsupervised domain adaptation (cf. below). # Weaknesses Discrepancy between quantitative and qualitative results: The good quantitative results (accumulated rewards) reported in the experiments are not reflected in the qualitative results. As can be seen from the videos, these results seem more to be representative of a bias in the data. For instance, in the Road Fighter videos, one can clearly see that the geometry of the road (width, curves) and dynamic obstacles are almost completely erased in the image translation process. The main reasons the quantitative results are good seem to be i) in the non-translated case the agent crashes immediately, ii) the \"translated\" image is a wide straight road identical to level 1 where the policy just keeps the car in the middle (thus crashing as soon as there is a turn or a collision with an obstacle). Even in the breakout case, there are catastrophic translation failures for some of the studied variations although the domain gap is static and small. The image translation results look underwhelming compared to state of the art GANs used for much more complex tasks and environments (e.g., the original CycleGAN paper and follow-up works, or the ICLR'18 progressive growing of GANs paper). This might be due to a hyper-parameter tuning issue, but it is unclear why the adaptation results seem not on par with previous results although the paper is in a visually simpler domain (Atari games). Does not address the RL generalization issues: Although it is the main goal of the paper, the method is fundamentally side-stepping the problem as it does not improve in any way the policy or the Deep RL algorithm (they are left untouched). It is mapping the target environment to the source one, without consideration for the end task besides tuning GAN hyper-parameters. If the initial policy is very brittle (as convincingly shown in section 2), then just mapping to the source domain does not improve the generalization capabilities of the Deep RL algorithm, or even improves transfer learning: it just enables the policy to be used in other contexts that can be reduced to the training one (which is independent of the learning algorithm, RL or otherwise). So it is unclear whether the main contribution is the one claimed. The contribution seems instead an experimental observation that it might be easier to reduce related domains to the training one instead of retraining a new (specialised and brittle) policy. Existing works have actually gone further, learning jointly the image translation and task network, including for very challenging problems, e.g. in unsupervised sim-to-real visual domain adaptation (e.g., Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks from Bousmalis et al at CVPR'17, which is not cited here). Experimental protocol: The experimental conclusions are not clear and lack generality, because the optimal methods (e.g., choice of GAN, number of iterations) vary significantly depending on the task (cf. Table 3 for instance). Furthermore, the best configurations seem selected on the test set for every experiment. Data efficiency vs actual training efficiency: The main claim is that it is better to do image translation instead of fine-tuning or full re-training. The basis of that argument is the experimentally observed need for less frames to do the image translation (Table 2). However, it is not clear that training GANs for unsupervised image translation is actually any easier / faster. What about training instability, mode collapse, hyper-parameter tuning, and actual training time comparisons on the same hardware? # First Recommendation Using image translation via GANs for unsupervised domain adaptation is a popular idea, used in the context of RL for Atari games here. Although the experiments show that mapping a target visual domain to a source one can enable reusing a deep RL policy as is, the qualitative results suggest this is in fact due to a bias in the data used here and the experimental protocol does not yield general insights. Furthermore, this approach is not specific to RL and its observed generalization issues. It does not improve the learning of the policy or improve its transferability, thus having only limited new insights compared to existing approaches that jointly learn image translation and target task-specific networks in much more challenging conditions. I believe this submission is at the start of an interesting direction, and requires further work on more challenging tasks, bigger domain gaps, and towards more joint training or actual policy transfer to go beyond this first set of encouraging but preliminary results. # Post-rebuttal Recommendation Thanks to the authors for their detailed reply. The clarifications around overfitting, UNIT-GAN in Section 4, and the paper claims are helpful. I also agree that the quantitative experiments are serious. I have bumped my score by +1 as a result. Nonetheless, the results still seem preliminary and limited in scope for the aforementioned reasons. The discussion in the comments about the learned policies and transfer are ad-hoc. A lot of the shortcomings mentioned in the review are outright dismissed (e.g., \"de facto standard in RL\"), downplayed (esp. generalization, which is puzzling for a transfer learning paper), or left for future work. As there is no strong technical contribution beyond the experimental observations in the current submission, I suggest the authors try to address the GAN shortcomings both mentioned in reviews and their reply, instead of just observing / reporting them. As this paper's main focus is to use image translation in the proposed RL setting (with standard GAN and RL methods), I do not think it is just someone else's problem to improve the image translation part. Proposing a technical contribution there would make the paper much stronger and appealing to a broader ICLR audience. This might also require adding a third game to ensure more generalizable experimental insights.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Experimental protocol : We agree that selecting configurations based on the test set is far from ideal , but we also note that this is the de-facto standard in video game-playing RL works , so we do not believe our work is any worse than others in the literature in this regard . Regarding \u201c optimal methods ( e.g. , choice of GAN , number of iterations ) vary significantly depending on the task \u201d - indeed , in current GANs works , configurations ( and even pre-processing in many cases ) change between tasks . Given the amount of interest in GAN research , we expect this aspect to improve over time . However , we stress that in the Experiments section ( Section 4 ) we test our approach using UNIT-GAN * only * . We test different GANs only in Section 5 as we evaluate them by comparing their results on our tasks . Data efficiency vs actual training efficiency : The training process of the actor-critic algorithms mainly depends on CPU where the GAN is trained using GPU therefore , you can not compare the time on the same hardware . We agree that GANs today still suffers from many issues and are not stable enough , we also mention some of their limitation in Section 4 . As noted above , we expect these aspects of GANs to improve . Despite the limitations , this method still manages to succeed in most tasks and clearly shows how transfer can be achieved by a visual mapping . More generally , we presented a novel transfer approach for model-free RL that decouples the visual transfer from the policy transfer , by using unaligned GANs . While the approach is not perfect ( and we explicitly discuss many of the points raised by rev1 in the paper ) , the method is effective , and , to our knowledge , has not been proposed or demonstrated to work before in the context of RL . Rev1 seems to dislike the fact that we did not address the \u201c core \u201d problems of model-free RL directly , but rather proposed a \u201c workaround \u201d in the form of GAN-based mapping which is external to the RL process . In contrast , we see precisely this separation as the main idea and strength of our proposal : we let the agent re-use its learned policy by helping it map the new environment to its \u201c previous experiences \u201d . Furthermore , rev1 dismisses our reported quantitative results because , in their opinion , they are not reflected in the videos . However , we argue that both videos reflect the success of our approach -- the Breakout video clearly shows how the agent follows the learned policy perfectly and the Road Fighter video demonstrate how the agent applies the learned techniques from level 1 in each of the successive levels . It is possible that future iterations of the idea will improve on our method with more complex machinery , and we look forward to seeing others expand on our research ."}, {"review_id": "rkxjnjA5KQ-1", "review_text": "This paper propose an intermediate stage before transfer learning on playing new games that is with slight visual change. The intermediate stage is basically a mapping function to translate the images in the new game to old game with certain correspondence. The paper claims that the adding of intermediate stage can be much more efficient than re-train the model instead. Then the paper compares different baselines without the mapping model. The baselines are either re-trained from scratch or (partially) initialized with trained model. The learning curves show that fine-tuning fails to transfer knowledge from the source domain to target domain. The mapping model is constructed based on unaligned GAN. And the experiments are setup and results are shown. Pros: + The paper makes a very good start from analogizing human being adjusting himself between similar tasks. + The paper demonstrates strong motivation on improving the existing transfer learnings that are either fail or take too much time to train from scratch. + The paper clearly illustrate the learning curve of multiple approaches for transferring knowledge across tasks. + The paper proves detailed analysis why using unaligned GAN to learn the mapping model, and gives + I also like the experiment section. It is well written, especially the discussions section answer all my questions. Questions: 1. Why fine-tuning from a model that is trained from related task does not help, even decelerate the learning process? Could you explain it more? 2. Could you please also include in figure 2 the proposed transfer learning curve with the mapping model G? I\u2019m curious how much faster it will converge than the Full-FT. And I suppose the retrain from scratch can be extremely slow and will exceed the training epoch scope in the figure. 3. In dataset collection, you use untrained agent to collect source domain image. Will it improve the results if you use well trained agent, or even human agent, instead? 4. I hope, if possible, you can share the source code in the near future. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your time in reviewing our paper . 1.Our paper discusses tasks in which the inputs of the model in each step are raw pixels . The A3C model learned the detail and noise in the training images to the extent that small and insignificant modifications create data that is unrecognizable to the model , which prevent the model from following the policy it learned and making optimal decisions . 2.Figure 2 presents the results of the RL agents during training . In that matter , our approach is a zero-shot transfer approach in which there is no need for any additional RL training . The number of GAN iterations needed for the maximum scores and # of iterations for each task is presented in Table 1 , 2 . 3.Yes , if the agent is allowed to collect images from later stages of the game it needs to transfer to , results improve a little . However , we did not consider this a realistic scenario , because the untrained agent can not reach this stages . The idea of human demonstration is an interesting one , and we expect it could work well . 4.We will share source code upon publication ."}, {"review_id": "rkxjnjA5KQ-2", "review_text": "The paper seeks to generalize the reinforcement learning agents to related tasks. The authors first show the failure of conventional transfer learning techniques, then use GANs to translate the images in the target task to those in the source task. It is an interesting attempt to use the style-transferred images for generalization of RL agents. The paper is well written and easy to follow. Pros: 1. It is a novel attempt to use GANs to generate pictures that help RL agents transfer the policies to other related environments. 2. It is an interesting viewpoint to use the performance of RL agent to evaluate the quality of images generated by GANS. Cons: 1. The pictures generated by GANs can be hardly controlled, and extra noise or unseen objects might be generated, and may fool the RL agent during training. Other feedback: In Figure 2, it seems the fine-tuning methods also achieve comparable results (Full-FT and Partial-FT), such as Figure 2(b) and Figure 2(c). Besides, the plot is only averaged over 3 runs, whereas the areas of standard deviation still overlap with each other. It may not be convincing enough to claim the failure of fine-tuning methods. Minor typos: 1. In 2.1, second paragraph: 80x80 -> $80 \\times 80$ 2. In 2.1, second paragraph: chose -> choose ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your time in reviewing our paper . * Although the results between the runs are different , in all cases the fine-tuning results don \u2019 t outperform training from scratch . We would expect a generalized model to adjust quickly to small modifications in the images since most pixels are the same as in the original games and the dynamics didn \u2019 t change . Instead , the model behaves as if the modified games are completely new tasks . * Thank you for the corrections , will change the paragraph accordingly ."}], "0": {"review_id": "rkxjnjA5KQ-0", "review_text": "# Summary This paper proposes to improve the sample efficiency of transfer learning for Deep RL by mapping a new visual domain (target) onto the training one (source) using GANs. First, a deep RL policy is trained on a source domain (e.g., level 1 of the Atari Road Fighter game). Second, a GAN (e.g. UNIT or CycleGAN) is trained for unsupervised domain adaptation from target images (e.g., level 2 of Road Fighter) to source ones. Third, the policy learned in the source domain is applied directly on the GAN-translated target domain. The experimental evaluation uses two Atari games: i) transfer from Breakout to Breakout with static visual distractors inpainted on the screen, ii) from one Road Fighter level to others. Results suggest that this transfer learning approach requires less images than retraining from scratch in the new domain, including when fine-tuning does not work. # Strengths Controlled toy experiments of Deep RL generalization issues: The experiments on Breakout quantify how badly A3C overfits in this case, as it shows catastrophic performance degradation even with trivial static visual input perturbations (which are not even adversarial attacks). The fine-tuning experiments also quantify well how brittle the initial policy is, motivating further the importance of the problem studied by the paper. Investigating the impact of different GANs on the end task: The experiments evaluate two different image translation algorithms: one based on UNIT, the other based on CycleGAN. The results suggest that this choice is key and depends on the target domain. This suggests that the adaptation is in fact task dependent, confirming the direction pursued by others in task-specific unsupervised domain adaptation (cf. below). # Weaknesses Discrepancy between quantitative and qualitative results: The good quantitative results (accumulated rewards) reported in the experiments are not reflected in the qualitative results. As can be seen from the videos, these results seem more to be representative of a bias in the data. For instance, in the Road Fighter videos, one can clearly see that the geometry of the road (width, curves) and dynamic obstacles are almost completely erased in the image translation process. The main reasons the quantitative results are good seem to be i) in the non-translated case the agent crashes immediately, ii) the \"translated\" image is a wide straight road identical to level 1 where the policy just keeps the car in the middle (thus crashing as soon as there is a turn or a collision with an obstacle). Even in the breakout case, there are catastrophic translation failures for some of the studied variations although the domain gap is static and small. The image translation results look underwhelming compared to state of the art GANs used for much more complex tasks and environments (e.g., the original CycleGAN paper and follow-up works, or the ICLR'18 progressive growing of GANs paper). This might be due to a hyper-parameter tuning issue, but it is unclear why the adaptation results seem not on par with previous results although the paper is in a visually simpler domain (Atari games). Does not address the RL generalization issues: Although it is the main goal of the paper, the method is fundamentally side-stepping the problem as it does not improve in any way the policy or the Deep RL algorithm (they are left untouched). It is mapping the target environment to the source one, without consideration for the end task besides tuning GAN hyper-parameters. If the initial policy is very brittle (as convincingly shown in section 2), then just mapping to the source domain does not improve the generalization capabilities of the Deep RL algorithm, or even improves transfer learning: it just enables the policy to be used in other contexts that can be reduced to the training one (which is independent of the learning algorithm, RL or otherwise). So it is unclear whether the main contribution is the one claimed. The contribution seems instead an experimental observation that it might be easier to reduce related domains to the training one instead of retraining a new (specialised and brittle) policy. Existing works have actually gone further, learning jointly the image translation and task network, including for very challenging problems, e.g. in unsupervised sim-to-real visual domain adaptation (e.g., Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks from Bousmalis et al at CVPR'17, which is not cited here). Experimental protocol: The experimental conclusions are not clear and lack generality, because the optimal methods (e.g., choice of GAN, number of iterations) vary significantly depending on the task (cf. Table 3 for instance). Furthermore, the best configurations seem selected on the test set for every experiment. Data efficiency vs actual training efficiency: The main claim is that it is better to do image translation instead of fine-tuning or full re-training. The basis of that argument is the experimentally observed need for less frames to do the image translation (Table 2). However, it is not clear that training GANs for unsupervised image translation is actually any easier / faster. What about training instability, mode collapse, hyper-parameter tuning, and actual training time comparisons on the same hardware? # First Recommendation Using image translation via GANs for unsupervised domain adaptation is a popular idea, used in the context of RL for Atari games here. Although the experiments show that mapping a target visual domain to a source one can enable reusing a deep RL policy as is, the qualitative results suggest this is in fact due to a bias in the data used here and the experimental protocol does not yield general insights. Furthermore, this approach is not specific to RL and its observed generalization issues. It does not improve the learning of the policy or improve its transferability, thus having only limited new insights compared to existing approaches that jointly learn image translation and target task-specific networks in much more challenging conditions. I believe this submission is at the start of an interesting direction, and requires further work on more challenging tasks, bigger domain gaps, and towards more joint training or actual policy transfer to go beyond this first set of encouraging but preliminary results. # Post-rebuttal Recommendation Thanks to the authors for their detailed reply. The clarifications around overfitting, UNIT-GAN in Section 4, and the paper claims are helpful. I also agree that the quantitative experiments are serious. I have bumped my score by +1 as a result. Nonetheless, the results still seem preliminary and limited in scope for the aforementioned reasons. The discussion in the comments about the learned policies and transfer are ad-hoc. A lot of the shortcomings mentioned in the review are outright dismissed (e.g., \"de facto standard in RL\"), downplayed (esp. generalization, which is puzzling for a transfer learning paper), or left for future work. As there is no strong technical contribution beyond the experimental observations in the current submission, I suggest the authors try to address the GAN shortcomings both mentioned in reviews and their reply, instead of just observing / reporting them. As this paper's main focus is to use image translation in the proposed RL setting (with standard GAN and RL methods), I do not think it is just someone else's problem to improve the image translation part. Proposing a technical contribution there would make the paper much stronger and appealing to a broader ICLR audience. This might also require adding a third game to ensure more generalizable experimental insights.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Experimental protocol : We agree that selecting configurations based on the test set is far from ideal , but we also note that this is the de-facto standard in video game-playing RL works , so we do not believe our work is any worse than others in the literature in this regard . Regarding \u201c optimal methods ( e.g. , choice of GAN , number of iterations ) vary significantly depending on the task \u201d - indeed , in current GANs works , configurations ( and even pre-processing in many cases ) change between tasks . Given the amount of interest in GAN research , we expect this aspect to improve over time . However , we stress that in the Experiments section ( Section 4 ) we test our approach using UNIT-GAN * only * . We test different GANs only in Section 5 as we evaluate them by comparing their results on our tasks . Data efficiency vs actual training efficiency : The training process of the actor-critic algorithms mainly depends on CPU where the GAN is trained using GPU therefore , you can not compare the time on the same hardware . We agree that GANs today still suffers from many issues and are not stable enough , we also mention some of their limitation in Section 4 . As noted above , we expect these aspects of GANs to improve . Despite the limitations , this method still manages to succeed in most tasks and clearly shows how transfer can be achieved by a visual mapping . More generally , we presented a novel transfer approach for model-free RL that decouples the visual transfer from the policy transfer , by using unaligned GANs . While the approach is not perfect ( and we explicitly discuss many of the points raised by rev1 in the paper ) , the method is effective , and , to our knowledge , has not been proposed or demonstrated to work before in the context of RL . Rev1 seems to dislike the fact that we did not address the \u201c core \u201d problems of model-free RL directly , but rather proposed a \u201c workaround \u201d in the form of GAN-based mapping which is external to the RL process . In contrast , we see precisely this separation as the main idea and strength of our proposal : we let the agent re-use its learned policy by helping it map the new environment to its \u201c previous experiences \u201d . Furthermore , rev1 dismisses our reported quantitative results because , in their opinion , they are not reflected in the videos . However , we argue that both videos reflect the success of our approach -- the Breakout video clearly shows how the agent follows the learned policy perfectly and the Road Fighter video demonstrate how the agent applies the learned techniques from level 1 in each of the successive levels . It is possible that future iterations of the idea will improve on our method with more complex machinery , and we look forward to seeing others expand on our research ."}, "1": {"review_id": "rkxjnjA5KQ-1", "review_text": "This paper propose an intermediate stage before transfer learning on playing new games that is with slight visual change. The intermediate stage is basically a mapping function to translate the images in the new game to old game with certain correspondence. The paper claims that the adding of intermediate stage can be much more efficient than re-train the model instead. Then the paper compares different baselines without the mapping model. The baselines are either re-trained from scratch or (partially) initialized with trained model. The learning curves show that fine-tuning fails to transfer knowledge from the source domain to target domain. The mapping model is constructed based on unaligned GAN. And the experiments are setup and results are shown. Pros: + The paper makes a very good start from analogizing human being adjusting himself between similar tasks. + The paper demonstrates strong motivation on improving the existing transfer learnings that are either fail or take too much time to train from scratch. + The paper clearly illustrate the learning curve of multiple approaches for transferring knowledge across tasks. + The paper proves detailed analysis why using unaligned GAN to learn the mapping model, and gives + I also like the experiment section. It is well written, especially the discussions section answer all my questions. Questions: 1. Why fine-tuning from a model that is trained from related task does not help, even decelerate the learning process? Could you explain it more? 2. Could you please also include in figure 2 the proposed transfer learning curve with the mapping model G? I\u2019m curious how much faster it will converge than the Full-FT. And I suppose the retrain from scratch can be extremely slow and will exceed the training epoch scope in the figure. 3. In dataset collection, you use untrained agent to collect source domain image. Will it improve the results if you use well trained agent, or even human agent, instead? 4. I hope, if possible, you can share the source code in the near future. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your time in reviewing our paper . 1.Our paper discusses tasks in which the inputs of the model in each step are raw pixels . The A3C model learned the detail and noise in the training images to the extent that small and insignificant modifications create data that is unrecognizable to the model , which prevent the model from following the policy it learned and making optimal decisions . 2.Figure 2 presents the results of the RL agents during training . In that matter , our approach is a zero-shot transfer approach in which there is no need for any additional RL training . The number of GAN iterations needed for the maximum scores and # of iterations for each task is presented in Table 1 , 2 . 3.Yes , if the agent is allowed to collect images from later stages of the game it needs to transfer to , results improve a little . However , we did not consider this a realistic scenario , because the untrained agent can not reach this stages . The idea of human demonstration is an interesting one , and we expect it could work well . 4.We will share source code upon publication ."}, "2": {"review_id": "rkxjnjA5KQ-2", "review_text": "The paper seeks to generalize the reinforcement learning agents to related tasks. The authors first show the failure of conventional transfer learning techniques, then use GANs to translate the images in the target task to those in the source task. It is an interesting attempt to use the style-transferred images for generalization of RL agents. The paper is well written and easy to follow. Pros: 1. It is a novel attempt to use GANs to generate pictures that help RL agents transfer the policies to other related environments. 2. It is an interesting viewpoint to use the performance of RL agent to evaluate the quality of images generated by GANS. Cons: 1. The pictures generated by GANs can be hardly controlled, and extra noise or unseen objects might be generated, and may fool the RL agent during training. Other feedback: In Figure 2, it seems the fine-tuning methods also achieve comparable results (Full-FT and Partial-FT), such as Figure 2(b) and Figure 2(c). Besides, the plot is only averaged over 3 runs, whereas the areas of standard deviation still overlap with each other. It may not be convincing enough to claim the failure of fine-tuning methods. Minor typos: 1. In 2.1, second paragraph: 80x80 -> $80 \\times 80$ 2. In 2.1, second paragraph: chose -> choose ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your time in reviewing our paper . * Although the results between the runs are different , in all cases the fine-tuning results don \u2019 t outperform training from scratch . We would expect a generalized model to adjust quickly to small modifications in the images since most pixels are the same as in the original games and the dynamics didn \u2019 t change . Instead , the model behaves as if the modified games are completely new tasks . * Thank you for the corrections , will change the paragraph accordingly ."}}