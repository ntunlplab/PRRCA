{"year": "2020", "forum": "BJx040EFvH", "title": "Fast is better than free: Revisiting adversarial training", "decision": "Accept (Poster)", "meta_review": "This paper provides a surprising result: that randomization and FGSM can produce robust models faster than previous methods given the right mix of cyclic learning rate, mixed precision, etc. This paper produced a fair bit of controversy among both the community and the reviewers to the point where there were suggestions of bugs, evaluation problems, and other issues leading to the results. In the end, the authors released the code (and made significant updates to the paper based on all the feedback). Multiple reviewers checked the code and were happy. There was an extensive author response, and all the reviewers indicated that their primary concerns were address, save concerns about the sensitivity of step-size and the impact of early stopping.\n\nOverall, the paper is well written and clear. The proposed approach is simple and well explained. The result is certainly interesting, and this paper will continue to generate fruitful debate. There are still things to address to improve the paper, listed above. I strongly encourage the authors to continue to improve the work and make a more concerted effort to carefully discuss the impacts of early stopping.\n", "reviews": [{"review_id": "BJx040EFvH-0", "review_text": "The main claim of this paper is that a simple strategy of randomization plus fast gradient sign method (FGSM) adversarial training yields robust neural networks. This is somewhat surprising because previous works indicate that FGSM is not a powerful attack compared to iterative versions of it like projected gradient descent (PGD), and it has not been shown before that models trained on FGSM can defend against PGD attacks. Judging from the results in the paper alone, there are some issues with the experiment results that could be due to bugs or other unexplained experiment settings. The most alarming part of the results is the catastrophic failure with larger step sizes 16/255 for CIFAR10 in Table 1. This is very strange because the method works well when using epsilon=10/255 to defend against an adversary with epsilon=8/255. The authors explain this with overfitting, but this is not satisfactory. Suppose I want to use the method to defend against an adversary with power epsilon=14/255, then it is conceivable that I would use a slightly larger step size, say 16/255, as suggested by the authors. The results in the table tells me that this method will fail completely, because it cannot defend against epsilon=8/255, let alone the target perturbation 14/255. The method is probably not failing completely, because it does have good accuracy on clean data and does learn something. So it cannot be due to the model not having enough capacity to learn against an epsilon=16/255 adversary. The authors should check some potential issues with the experiments: 1. Is there any label leakage in the FGSM training? 2. The pseudo-code does not contain any projection onto the feasible set; the authors should check it. Since the claim of this paper is somewhat unexpected given previous works on defending against adversaries, the experiment results have to be very solid. With these issues with the experiments I don't believe the current paper is ready for publication yet. After rebuttal: The authors' new experiments and response answers most of my concerns. ", "rating": "6: Weak Accept", "reply_text": "Thanks for your feedback . With regards to the catastrophic overfitting observed at larger step sizes , we first clarify a misunderstanding here : defending against an adversary with radius epsilon means that we project the perturbation onto the ball with radius epsilon . With regards to your example , indeed , a step size of 16/255 , * when projected onto a ball of radius 8/255 * , results in overfitting , as large step size forces the generated adversarial examples to be clustered at the boundary . However , if , as you describe , we instead want to defend against an adversary with radius 14/255 using a step size of 16/255 , then note that we project the FGSM step on the ball of radius 14/255 . This is a fundamentally different scenario from the results in the table , which project onto a ball of radius 8/255 , and so the table does not imply that the method is guaranteed to fail . Indeed , since the projected radius is larger , the adversarial examples are not clustered at the boundary , and so there is no overfitting . In short : a large step size of 16/255 fails when projected onto a radius of 8/255 , but works perfectly well when projected onto a similarly large radius ( e.g.14/255 ) .This is why we wrote alpha=1.25 * epsilon . As for the potential issues in the experiments , we note below that they are not at all issues , and hope that the reviewer can reconsider : Label leaking : We do not observe label leaking ( as defined in `` Adversarial Machine Learning at Scale '' by Kurakin et al.2017 ) .You can see this in all of our experimental results , e.g.in Table 1 , Table 2 , Table 4 , and Figure 2 , where the standard accuracy always is above the adversarial accuracy , and this behavior can be verified in the models that we have released . Projection in pseudocode : The projection is in fact present in our pseudocode . It is the line that says \\delta = max ( min ( \\delta , \\epsilon ) , -\\epsilon ) . It is also in our submitted code . If you are referring to clipping at the [ 0,255 ] bounds from the image , this is also done in our code ( as described in the public discussion ) ."}, {"review_id": "BJx040EFvH-1", "review_text": "This paper revisits Random+FGSM method to train robust models against strong PGD evasion attacks. Coupled together with tricks for accelerating natural training, such as cyclic learning rate, mixed precision, the robust models can be trained faster than previous methods. +The experimental results are impressive. The trained model is robust (at Madry\u2019s PGD level), and the total training procedure is fast (6 min for CIFAR-10 and 12 hr for ImageNet). + The method is simple, and I guess reproducible. +The paper shows surprising facts of a well-known method. +The paper is generally well-written and easy to follow. I do have some concerns of the work - The paper is empirical and the techniques are combinations of previous methods. Even for the surprising fact that Random+FGSM, it has been discussed in several previous papers, for example, ICLR 2019 Defensive Quantization: When Efficiency Meets Robustness https://openreview.net/forum?id=ryetZ20ctX. So the main contribution of the paper is limited to show RFGSM works well when combined with optimization tricks like cyclic learning rate. -In previous methods claiming random+FGSM can train robust model, their method seems to be slightly different from Alg 3 in page 4 of this paper. The alg in this paper seems to be identical to Madry\u2019s implementation of R-FGSM, which is shown not robust to PGD attacks. See discussions in https://openreview.net/forum?id=rJzIBfZAb and https://openreview.net/forum?id=ryetZ20ctX. I would like the authors to clarify their method to resolve such conflicts and make it clear how R-FGSM can be as robust as PGD as in table 1. -The first two paragraphs of section 4.1 seem to be inaccurate. One important trick in the \u201cadversarial training for free\u201d paper is to replay each minibatch m times. It is hard to say how much nonzero initialization helps. According to \u201cuniversal adversarial training\u201d (https://arxiv.org/pdf/1811.11304.pdf). It may help, but cannot compete with Madry\u2019s PGD training when defending against PGD attacks. -I am not sure if using a larger norm 1.25 * \\epsilon is a fair comparison. A baseline of PGD training bounded by 1.25 * \\epsilon would help. -Could the authors combine table 4 and 5 for easy comparison of robust accuracy and training time? Did the authors try the optimization tricks on ImageNet for the baseline free adversarial training method? ================== after rebuttal ================= I change my rating to weak accept. I tend to accept for the following reason (1) there seems to be no obvious flaw in the authors implementation. I quickly skimmed their code, and looks like a few researchers have tried their code and responded in public discussion. The surprising robustness of RFGSM, though the originality is questionable and the technical difference comparing to previous methods are subtle, seems to hold true. (2) The authors work hard to address the comments. I still have some concerns, mainly regarding the fairness of experimental comparison. (1) As pointed out in public discussion, the success of the proposed RFGSM relies on early stopping. I am wondering if early stopping also helps other methods since it turns out to be some sort of selection procedure. (2) The authors did not update time in table 1 for CIFAR-10 results, which I consider almost no extra efforts. I am wondering how much more time each method needs from 45% in figure 2 to higher robust accuracy in table 1. (3) I cannot understand why the proposed method is a particular good fit with cyclic LR and low precision tricks comparing to other methods. ", "rating": "6: Weak Accept", "reply_text": "Thanks for your feedback regarding the connections to other randomized FGSM methods . This topic has already occurred in the public discussion , and so our response will largely reflect that . We will first discuss the main differentiating factors between our approach and the previous R+FGSM approach by Tramer et al. , and follow up by answering the remaining comments . R+FGSM : Indeed , there has been previous work on using randomization with FGSM ( e.g.R+FGSM as done by Tramer et al. , which is the one used in `` Defensive Quantization : When Efficiency Meets Robustness '' ) . We note , however , that the R+FGSM approach from Tramer et al.is also the same randomized FGSM approach considered by Madry et al. , which considers both the vanilla , non-randomized attack ( which they conclude is not robust ) as well as the R+FGSM attack from Tramer et al.as mentioned in their paper in Table 5 on page 17 of the Appendix . Our approach at using randomization with FGSM is very similar to Tramer et al.but differs in two aspects which are quite critical to the consistency and effectiveness of the defense . In fact , a lot of this discussion has already occurred in the public comments below ( e.g.see our discussion with Florian here , where we explicitly compare our approach with R+FGSM : https : //openreview.net/forum ? id=BJx040EFvH & noteId=HJe2trIsDS ) , but we can summarize the main points for your convenience : namely , by using 1 ) a different random initialization and 2 ) a larger step size , our version of randomized FGSM adversarial training converges to better defended models with much higher consistency . By rerunning the approaches multiple times with different random seeds , one gets a fuller picture : R+FGSM as done by Tramer et al.has high variance and worse performance with respect to random seeds , whereas our approach consistent achieves results comparable to PGD adversarial training regardless of random seed ( see the table at https : //github.com/anonymous-sushi-armadillo/openreview/blob/master/README.md which we generated for the referenced public discussion ) . So we believe that the contribution of this paper extends beyond just incorporating DAWNBench speedups . On non-zero initialization and the connection to Free Adversarial Training : The usage of minibatch-replay in free adversarial training is indeed the second key difference between free and FGSM adversarial training . While we do n't mention this at the start of section 4.1 , we do discuss this difference later in the last paragraph of the same section . However , we focused primarily on the initialization , because in our experiment in Table 1 , we show the effect of using various initializations for FGSM adversarial training without changing any other parameters : simply going from zero to non-zero results in large gains in robustness . Note that the Universal Adversarial Training paper also uses R+FGSM as done by Tramer et al. , and so it suffers from the same drawbacks as described above . On the `` larger norm '' and fair comparison : We believe there may be a misunderstanding here : the model is trained against an adversary bounded by epsilon , but the alpha=1.25 * epsilon is merely the step size for the FGSM attack . Indeed , regardless of the step size , the FGSM attack is still projected back to the epsilon boundary . As a PGD adversary is allowed to tune the step size and the number of steps it takes to find an adversarial example , it should also be fair for the FGSM adversary to also tune its singular step size , as both methods project onto the same radius ball . Other comments : Thank you for your suggestion , yes we can certainly add the training times to Table 4 to make it easier to connect the two . We primarily focused on optimizing the DAWNBench improvements with Free adversarial training in the CIFAR10 setting , since the problem setting allows for extensive tuning of all parameters ( which is not as feasible in the ImageNet setting ) ."}, {"review_id": "BJx040EFvH-2", "review_text": "The authors claimed a classic adversarial training method, FGSM with random start, can indeed train a model that is robust to strong PGD attacks. Moreover, when it is combined with some fast training methods, such as cyclic learning rate scheduling and mixed precision, the adversarial training time can be significantly decreased. The experiment verifies the authors' claim convincingly. Overall, the paper provides a novel finding that could significantly change the adversarial training strategy. The paper is clearly written and easy to follow. I recommend the acceptance. ", "rating": "8: Accept", "reply_text": "Thanks for your review . Indeed , we hope this this work inspires new analysis which can perhaps quantify the degree to which the inner maximization must be solved in order to perform robust optimization ."}], "0": {"review_id": "BJx040EFvH-0", "review_text": "The main claim of this paper is that a simple strategy of randomization plus fast gradient sign method (FGSM) adversarial training yields robust neural networks. This is somewhat surprising because previous works indicate that FGSM is not a powerful attack compared to iterative versions of it like projected gradient descent (PGD), and it has not been shown before that models trained on FGSM can defend against PGD attacks. Judging from the results in the paper alone, there are some issues with the experiment results that could be due to bugs or other unexplained experiment settings. The most alarming part of the results is the catastrophic failure with larger step sizes 16/255 for CIFAR10 in Table 1. This is very strange because the method works well when using epsilon=10/255 to defend against an adversary with epsilon=8/255. The authors explain this with overfitting, but this is not satisfactory. Suppose I want to use the method to defend against an adversary with power epsilon=14/255, then it is conceivable that I would use a slightly larger step size, say 16/255, as suggested by the authors. The results in the table tells me that this method will fail completely, because it cannot defend against epsilon=8/255, let alone the target perturbation 14/255. The method is probably not failing completely, because it does have good accuracy on clean data and does learn something. So it cannot be due to the model not having enough capacity to learn against an epsilon=16/255 adversary. The authors should check some potential issues with the experiments: 1. Is there any label leakage in the FGSM training? 2. The pseudo-code does not contain any projection onto the feasible set; the authors should check it. Since the claim of this paper is somewhat unexpected given previous works on defending against adversaries, the experiment results have to be very solid. With these issues with the experiments I don't believe the current paper is ready for publication yet. After rebuttal: The authors' new experiments and response answers most of my concerns. ", "rating": "6: Weak Accept", "reply_text": "Thanks for your feedback . With regards to the catastrophic overfitting observed at larger step sizes , we first clarify a misunderstanding here : defending against an adversary with radius epsilon means that we project the perturbation onto the ball with radius epsilon . With regards to your example , indeed , a step size of 16/255 , * when projected onto a ball of radius 8/255 * , results in overfitting , as large step size forces the generated adversarial examples to be clustered at the boundary . However , if , as you describe , we instead want to defend against an adversary with radius 14/255 using a step size of 16/255 , then note that we project the FGSM step on the ball of radius 14/255 . This is a fundamentally different scenario from the results in the table , which project onto a ball of radius 8/255 , and so the table does not imply that the method is guaranteed to fail . Indeed , since the projected radius is larger , the adversarial examples are not clustered at the boundary , and so there is no overfitting . In short : a large step size of 16/255 fails when projected onto a radius of 8/255 , but works perfectly well when projected onto a similarly large radius ( e.g.14/255 ) .This is why we wrote alpha=1.25 * epsilon . As for the potential issues in the experiments , we note below that they are not at all issues , and hope that the reviewer can reconsider : Label leaking : We do not observe label leaking ( as defined in `` Adversarial Machine Learning at Scale '' by Kurakin et al.2017 ) .You can see this in all of our experimental results , e.g.in Table 1 , Table 2 , Table 4 , and Figure 2 , where the standard accuracy always is above the adversarial accuracy , and this behavior can be verified in the models that we have released . Projection in pseudocode : The projection is in fact present in our pseudocode . It is the line that says \\delta = max ( min ( \\delta , \\epsilon ) , -\\epsilon ) . It is also in our submitted code . If you are referring to clipping at the [ 0,255 ] bounds from the image , this is also done in our code ( as described in the public discussion ) ."}, "1": {"review_id": "BJx040EFvH-1", "review_text": "This paper revisits Random+FGSM method to train robust models against strong PGD evasion attacks. Coupled together with tricks for accelerating natural training, such as cyclic learning rate, mixed precision, the robust models can be trained faster than previous methods. +The experimental results are impressive. The trained model is robust (at Madry\u2019s PGD level), and the total training procedure is fast (6 min for CIFAR-10 and 12 hr for ImageNet). + The method is simple, and I guess reproducible. +The paper shows surprising facts of a well-known method. +The paper is generally well-written and easy to follow. I do have some concerns of the work - The paper is empirical and the techniques are combinations of previous methods. Even for the surprising fact that Random+FGSM, it has been discussed in several previous papers, for example, ICLR 2019 Defensive Quantization: When Efficiency Meets Robustness https://openreview.net/forum?id=ryetZ20ctX. So the main contribution of the paper is limited to show RFGSM works well when combined with optimization tricks like cyclic learning rate. -In previous methods claiming random+FGSM can train robust model, their method seems to be slightly different from Alg 3 in page 4 of this paper. The alg in this paper seems to be identical to Madry\u2019s implementation of R-FGSM, which is shown not robust to PGD attacks. See discussions in https://openreview.net/forum?id=rJzIBfZAb and https://openreview.net/forum?id=ryetZ20ctX. I would like the authors to clarify their method to resolve such conflicts and make it clear how R-FGSM can be as robust as PGD as in table 1. -The first two paragraphs of section 4.1 seem to be inaccurate. One important trick in the \u201cadversarial training for free\u201d paper is to replay each minibatch m times. It is hard to say how much nonzero initialization helps. According to \u201cuniversal adversarial training\u201d (https://arxiv.org/pdf/1811.11304.pdf). It may help, but cannot compete with Madry\u2019s PGD training when defending against PGD attacks. -I am not sure if using a larger norm 1.25 * \\epsilon is a fair comparison. A baseline of PGD training bounded by 1.25 * \\epsilon would help. -Could the authors combine table 4 and 5 for easy comparison of robust accuracy and training time? Did the authors try the optimization tricks on ImageNet for the baseline free adversarial training method? ================== after rebuttal ================= I change my rating to weak accept. I tend to accept for the following reason (1) there seems to be no obvious flaw in the authors implementation. I quickly skimmed their code, and looks like a few researchers have tried their code and responded in public discussion. The surprising robustness of RFGSM, though the originality is questionable and the technical difference comparing to previous methods are subtle, seems to hold true. (2) The authors work hard to address the comments. I still have some concerns, mainly regarding the fairness of experimental comparison. (1) As pointed out in public discussion, the success of the proposed RFGSM relies on early stopping. I am wondering if early stopping also helps other methods since it turns out to be some sort of selection procedure. (2) The authors did not update time in table 1 for CIFAR-10 results, which I consider almost no extra efforts. I am wondering how much more time each method needs from 45% in figure 2 to higher robust accuracy in table 1. (3) I cannot understand why the proposed method is a particular good fit with cyclic LR and low precision tricks comparing to other methods. ", "rating": "6: Weak Accept", "reply_text": "Thanks for your feedback regarding the connections to other randomized FGSM methods . This topic has already occurred in the public discussion , and so our response will largely reflect that . We will first discuss the main differentiating factors between our approach and the previous R+FGSM approach by Tramer et al. , and follow up by answering the remaining comments . R+FGSM : Indeed , there has been previous work on using randomization with FGSM ( e.g.R+FGSM as done by Tramer et al. , which is the one used in `` Defensive Quantization : When Efficiency Meets Robustness '' ) . We note , however , that the R+FGSM approach from Tramer et al.is also the same randomized FGSM approach considered by Madry et al. , which considers both the vanilla , non-randomized attack ( which they conclude is not robust ) as well as the R+FGSM attack from Tramer et al.as mentioned in their paper in Table 5 on page 17 of the Appendix . Our approach at using randomization with FGSM is very similar to Tramer et al.but differs in two aspects which are quite critical to the consistency and effectiveness of the defense . In fact , a lot of this discussion has already occurred in the public comments below ( e.g.see our discussion with Florian here , where we explicitly compare our approach with R+FGSM : https : //openreview.net/forum ? id=BJx040EFvH & noteId=HJe2trIsDS ) , but we can summarize the main points for your convenience : namely , by using 1 ) a different random initialization and 2 ) a larger step size , our version of randomized FGSM adversarial training converges to better defended models with much higher consistency . By rerunning the approaches multiple times with different random seeds , one gets a fuller picture : R+FGSM as done by Tramer et al.has high variance and worse performance with respect to random seeds , whereas our approach consistent achieves results comparable to PGD adversarial training regardless of random seed ( see the table at https : //github.com/anonymous-sushi-armadillo/openreview/blob/master/README.md which we generated for the referenced public discussion ) . So we believe that the contribution of this paper extends beyond just incorporating DAWNBench speedups . On non-zero initialization and the connection to Free Adversarial Training : The usage of minibatch-replay in free adversarial training is indeed the second key difference between free and FGSM adversarial training . While we do n't mention this at the start of section 4.1 , we do discuss this difference later in the last paragraph of the same section . However , we focused primarily on the initialization , because in our experiment in Table 1 , we show the effect of using various initializations for FGSM adversarial training without changing any other parameters : simply going from zero to non-zero results in large gains in robustness . Note that the Universal Adversarial Training paper also uses R+FGSM as done by Tramer et al. , and so it suffers from the same drawbacks as described above . On the `` larger norm '' and fair comparison : We believe there may be a misunderstanding here : the model is trained against an adversary bounded by epsilon , but the alpha=1.25 * epsilon is merely the step size for the FGSM attack . Indeed , regardless of the step size , the FGSM attack is still projected back to the epsilon boundary . As a PGD adversary is allowed to tune the step size and the number of steps it takes to find an adversarial example , it should also be fair for the FGSM adversary to also tune its singular step size , as both methods project onto the same radius ball . Other comments : Thank you for your suggestion , yes we can certainly add the training times to Table 4 to make it easier to connect the two . We primarily focused on optimizing the DAWNBench improvements with Free adversarial training in the CIFAR10 setting , since the problem setting allows for extensive tuning of all parameters ( which is not as feasible in the ImageNet setting ) ."}, "2": {"review_id": "BJx040EFvH-2", "review_text": "The authors claimed a classic adversarial training method, FGSM with random start, can indeed train a model that is robust to strong PGD attacks. Moreover, when it is combined with some fast training methods, such as cyclic learning rate scheduling and mixed precision, the adversarial training time can be significantly decreased. The experiment verifies the authors' claim convincingly. Overall, the paper provides a novel finding that could significantly change the adversarial training strategy. The paper is clearly written and easy to follow. I recommend the acceptance. ", "rating": "8: Accept", "reply_text": "Thanks for your review . Indeed , we hope this this work inspires new analysis which can perhaps quantify the degree to which the inner maximization must be solved in order to perform robust optimization ."}}