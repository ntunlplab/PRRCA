{"year": "2021", "forum": "H6ZWlQrPGS2", "title": "Fast Binarized Neural Network Training with Partial Pre-training", "decision": "Reject", "meta_review": "## Description \n\nThe paper asks the question whether it is possible to accelerate training a binarized neural network from scratch to a given target accuracy [by starting with training a full-precision network]. The main claimed contributions are: the idea to use *partially* pretrained networks, experimental evidence regarding the split of the training budget and measuring the speed-up.\n\n## Review Process and Decision\n\nAll four reviewers agreed in the low rating of the paper and in the opinion that the paper is not a significant contribution. The area chair supports rejection.\n\n## Details\n\nIt has been already observed that pre-training  in some form is needed for achieving the best accuracy: \nRastegari (2016) XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\nBulat (2019), \"Improved training of binary networks for human pose estimation and image recognition\"\nMartinez (2020): \"Training Binary Neural Networks with Real-to-Binary Convolutions\"\n\nAlizadeh, (2019 fig. 4) notice that pre-training can be viewed as a speed-up, but in their setup find that training from scratch gives a better accuracy. Bulat (2019) and Martinez (2020), on the contrary do use pre-training to achieve the best accuracy.  It is questionable whether the pre-training in these works is partial or not. I believe the result largely depends on the pre-training method used, which is not discussed in depth in the submission. More generally, some graduated optimization methods such as graduated smoothing or graduated non-convexity are known to help in finding better solutions / lead to faster optimization and in fact Bulat (2019) use pre-training with gradual transition from smooth activation to the sign function.  Relative to these points the technical contribution (one paragraph in the paper) is not significant. The empirical part of the contribution shows some effect, but does not indicate a breakthrough on its own. An investigation / design of pre-training schemes could make it more substantial.\n\nThe empirical analysis proposed does not rule out, and in fact supports, the methodology that for the best final accuracy, the full rather than partial pre-training is useful.\n\nThe gain of speed-up by a factor 1.3 (diminishing to close to 1 if we are interested in the best accuracy), is of little practical interest. In particular, a slight code optimization can give a similar speed-up without the complexity and hyperparameters involved in pre-training. The authors write  \"we are not aware of any effort to exploit binarization during the training phase\"\nThere are available public implementations that can optimize the forward pass of binary networks, in particular on GPU, while backward pass can stay in full precision. It could give a similar speed-up. In particular Courbariaux (2016) provides a GPU kernel and proposes a variant of BatchNorm with bit shifts rather than multiplications, specifically used at training time.\nMaking the emphasis on a relatively small speed-up that can be obtained to train sub-optimal models, in my view is not a good strategy to present this work. Rather the phenomenon that (partial) pre-training helps with the goal to improve the training methods more substantially I find of higher interest.\n\nFinally, I agree with the reviewers that the lottery ticket hypothesis (Frankle, 2020) work speaks of the speed-up only hypothetically and its main (and fairly in-depth) contribution is in demonstrating and investigating an interesting phenomenon about training and initialization, which I do not see relevant to this submission.\n", "reviews": [{"review_id": "H6ZWlQrPGS2-0", "review_text": "The paper suggests a method for training binary neural networks . The proposed method is to partially train with full precision and then continue with binarized training using the straight-through estimator . The method is very simple and there is very limited technical contribution , so in order to be worthy of publication it needs to be supported with compelling experimental results . Unfortunately this is not the case . The main claim is speeding up training by a factor of 1.2/1.6 . While this can help the importance of speeding up training ( unless by a much larger factor ) is quiet limited . The useful speedup BNN present is at inference time , and the time to train a network is of much less importance ( at least for this kind of speedup ) . From Fig.1 I am not convinced that the speedup claims hold . You can see the test accuracy for binary training on 3 of the experiments reaches the Partial Pre-training level but it does n't look like it completely flattened out yet . It looks like if you want take the best model on test , then you do n't get any speedup Results on cifar-10 seem quiet poor ( both full precision and 1-bit ) . For example `` Learning discrete weights using the local reparametrization trick '' gets 93.2 1-bit acc on cifar-10 with VGG .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for feedback and commentary ! We have provided a general response to all reviewers above ( [ https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ] ( https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ) ) . Here we will provide response to your individual points . > The main claim is speeding up training by a factor of 1.2/1.6 . While this can help the importance of speeding up training ( unless by a much larger factor ) is quiet limited . The useful speedup BNN present is at inference time , and the time to train a network is of much less importance ( at least for this kind of speedup ) . Please see the discussion of significance of training speedup in the general response above . > It looks like if you want take the best model on test , then you do n't get any speedup ... Please see the discussion of final accuracy in the general response above . > Results on cifar-10 seem quiet poor ( both full precision and 1-bit ) ... The accuracies reported for CIFAR-10 are standard for the network and hyperparameters used in the evaluation [ 7 ; Tables 3,4 ] . Partial pre-training can be applied with an arbitrary low-precision training setup ( including other quantization approaches or other training schemes like that of [ 8 ] ) in the second phase , though validation of this for other quantization approaches is left to future work . Please also see the discussion of hyperparameter choices and sweeps in the general response above . # # # Citations [ 7 ] `` Binary Neural Networks : A Survey , '' Haotong Qin , Ruihao Gong , Xianglong Liu , Xiao Bai , Jingkuan Song , Nicu Sebe ."}, {"review_id": "H6ZWlQrPGS2-1", "review_text": "This paper proposed one simple method called partial pre-training to speed up the training of binary neural networks ( BNN ) . The pros and cons are as follows : Pros : 1 . The partial pre-training method is simple and easy to implement ; 2 . For standard binary optimizer like the straight-through-estimator ( STE ) , the method improves the training speed to some extent ; Cons : 1 . The main concern of the proposed method is kind of heuristic and there is a lack of theoretical explanation , whether rigorous or not , why this method works . 2.As described in Section 7 by the author themselves , there are several apparent limitations of current evaluation , e.g. , several dimensions of the hyper-parameters are not explored , other Binary optimizers are not considered , different learning rate schedules , etc . As a result , it is unconvinced that the partial pre-training could universally improve the speed as a general method . In addition , the improvement of speed-up are not very apparent especially for ResNet-20 and ResNet-34 as shown in Fig.1 and Fig.2 , i.e. , it took approximately the same time to reach the final precision even though the proposed method achieves higher accuracy before saturation . Given the inadequate evaluations and lack of theoretical explanations , this might be due to unfair comparison . 3.There is a lack of explanation of the contradiction result with previous result proposed in Alizadeh et al ( 2019 ) , which dismissed the approach of pre-training . Is there good explanation of such an opposite result ? It would be better to show results of different split between full-precision training and low-precision training . 4.Regarding pre-training for BNN , there are some related works from the Bayesian perspective . In Shayer et al . ( 2018 ) , they used the result of full-precision training as the prior for the binary training , which improves the final result , as opposed to Alizadeh et al ( 2019 ) . The Bayesian perspective provides an explanation of the effectiveness of a good prior . In Meng et al . ( 2020 ) , they showed that STE could be viewed as Bayesian and obtained good result even with a uniform prior . Also , the posterior obtained after full-training ( binary ) could be used as prior to enable continual learning , which shows effectiveness of the prior . Given the above results , since the authors demonstrate that partial pre-training can increase the speed for STE , does this imply that partial pre-training provides a better prior than full pre-training ? If so , why ? Shayer , O. , Levi , D. , and Fetaya , E. Learning discrete weights using the local reparameterization trick . ICLR , 2018 . Meng , X , Bachmann . R. , Khan.E . Training Binary Neural Networks using the Bayesian Learning Rule . ICML , 2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for feedback and commentary ! We have provided a general response to all reviewers above ( [ https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ] ( https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ) ) . Here we will provide response to your individual points . > The main concern of the proposed method is kind of heuristic and there is a lack of theoretical explanation , whether rigorous or not , why this method works . Please see the discussion of theoretical analysis in the general response above . > There is a lack of explanation of the contradiction result with previous result proposed in Alizadeh et al ( 2019 ) , which dismissed the approach of pre-training . Is there good explanation of such an opposite result ? The results compare the accuracies of each technique when training the network from random initialization with a fixed training budget . Compared to standard low-precision training , partial pre-training is therefore allotted half as much time in each phase , meaning the methods are compared at equal training budgets . We will clarify this in the final version of the paper . > It would be better to show results of different split between full-precision training and low-precision training . Results for different splits between full-precision training and low-precision training are presented and discussed in Section 6 . > Regarding pre-training for BNN , there are some related works from the Bayesian perspective ... [ D ] oes this imply that partial pre-training provides a better prior than full pre-training ? The connection to the Bayesian perspective of [ 8,9 ] is an interesting connection to explore in future work . To clarify the exact relationship between our claims and the analysis of [ 9 ] , our results do not imply that partial pre-training provides a better prior than full pre-training . Figure 3a shows that for a fixed amount of low-precision training ( i.e. , elements on any given row ) a larger amount of full-precision training ( i.e. , farther to the right ) results in higher accuracy . # # # Citations [ 8 ] `` Learning Discrete Weights Using the Local Reparameterization Trick , '' Oran Shayer , Dan Levi , Ethan Fetaya . [ 9 ] `` Training Binary Neural Networks using the Bayesian Learning Rule , '' Xiangming Meng , Roman Bachmann , Mohammad Emtiyaz Khan ."}, {"review_id": "H6ZWlQrPGS2-2", "review_text": "This paper addresses the problem of slower training speed with low-precision training of neural nets . It presents a simple solution : first train with full precision on half of the budgeted trainining time , then train with low pecision in the remaining time . This achieves 1.2x - 1.6x speedup compared to low-precision training . Pro : - The proposed idea is simple and it is nice to see that it works Cons : - I feel there is not enough content ( in terms of ideas and experiments ) to warrant a full paper . Compared to other ICLR papers , the contributions seem on the low side . See suggestions below . - The paper mainly compares the proposed method with low-precision training , but the results would be stronger if also compared with full-precision training followed by quantization . This is especially because all the low-precision accuracies trail behind the full precision ones in the results . Suggestions : - Experiment with more quantization methods . Currently we do not know whether the proposed method is uniquely suited to the PACT method used , or is a general technique . - A 1.2x-1.6x speedup on a training process that takes 600 seconds ( e.g.CIFG-10 result in Fig 2 ) seems not so impactful in the grander scheme of things . Even the 12500 second training time is just < 4 hours . I understand the results should transfer , but the results would be more impressive if done on larger training runs . Minor questions/comments : - Please comment on the terminology . Is what you call low precision training similar to the mixed-precision training now implemented in TensorCore NVIDIA GPUs ? - Table 2 : what is it/s . Is it iterations per second ? - Table 1 shows learning rate schedule for t up to 60k in CIFAR or 450k in ImageNet , but Figure 1 stops way before those points in the x-axis . This was somewhat confusing . - Another clarification point about Fig 1 and 2 : for the proposed method , is the full precision training part of the time included in the calculation ? I believe so but just want to double-check .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for feedback and commentary ! We have provided a general response to all reviewers above ( [ https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ] ( https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ) ) . Here we will provide response to your individual points . > The paper mainly compares the proposed method with low-precision training , but the results would be stronger if also compared with full-precision training followed by quantization ... Based on our understanding of this question ( quantizing a pre-trained network then fine-tuning it for a relatively short duration ) , we do compare against this approach in Section 6 , Figure 3 . Please see the discussion of other splits between full-precision training and low-precision training in the general response . > Experiment with more quantization methods ... Please see the discussion of hyperparameter choices and sweeps in the general response above . > A 1.2x-1.6x speedup on a training process ... seems not so impactful in the grander scheme of things Please see the discussion of significance of training speedup in the general response above . > Is what you call low precision training similar to the mixed-precision training now implemented in TensorCore NVIDIA GPUs ? What we define as low-precision training ( training with weights constrained to lie in a small fixed set ) is possible to accelerate in some cases in NVIDIA Tensor Cores , though not always . Specifically , according to [ https : //developer.nvidia.com/blog/tensor-cores-mixed-precision-scientific-computing/ ] ( https : //developer.nvidia.com/blog/tensor-cores-mixed-precision-scientific-computing/ ) NVIDIA Tensor Cores can accelerate low-precision training when the weights are represented by as few as 8 bits . In this paper , we consider low-precision training with 1 bit , which NVIDIA Tensor Cores do not have native support for accelerating . As noted in Section 2 , we are not aware of any existing results that show faster wall-clock speeds for training binarized neural networks . > Table 2 : what is it/s . Is it iterations per second ? Correct , this is iterations per second . We will clarify this in the final version of the paper . > Table 1 shows learning rate schedule for t up to 60k in CIFAR or 450k in ImageNet , but Figure 1 stops way before those points in the x-axis . This was somewhat confusing . Each point on the plot shows a network trained for the specified amount of time using a learning rate schedule which is compressed down to that amount of time , as specified in Algorithm 1 . We will clarify this in the final version of the paper . > ... is the full precision training part of the time included in the calculation ? Correct , full precision training time is included in the calculation . Each point on the plot shows the accuracy when training from random initialization within the given budget ."}, {"review_id": "H6ZWlQrPGS2-3", "review_text": "# # # # Comments Summary : The authors propose a fast binarized neural network training algorithm that splits the whole training process into the full precision training stage and the binary training stage . The experimental results show some improvement about training speed in terms of iterations and wall clock time . Generally , the paper is well written . Strength : -- The idea is reasonable and the method is presented clearly . -- The experimental results indicate that the proposed method can accelerate the convergence of the binary networks on image classification and collaborative filtering . Weakness : -- The comparison between the proposed method and quantization-finetuning method is lacked , which seems like a closely related work . -- The analysis of the proposed method should be also enhanced . The reason why partial pretraining can improve the training of binary neural networks should be investigated more deeply . Comments : ( 1 ) The authors claim that the proposed method allows for faster from-scratch training of binarized neural . This seems contradictory to the partial pretraining . The authors may provide more discussion and clarification . ( 2 ) The improvement of the proposed method is not significant . The proposed algorithm speeds up the training marginally and can not improve the final test accuracy . This limits the contribution . Overall , the reviewer doesn \u2019 t recommend accepting this manuscript at its form . The author may demonstrate more differences between the proposed method and the standard quantization-finetuning method .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for feedback and commentary ! We have provided a general response to all reviewers above ( [ https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ] ( https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ) ) . Here we will provide response to your individual points . > The comparison between the proposed method and quantization-finetuning method is lacked Based on our understanding of this question ( quantizing a pre-trained network then fine-tuning it for a relatively short duration ) , we do compare against this approach in Section 6 , Figure 3 . Please see the discussion of other splits between full-precision training and low-precision training in the general response . > The analysis of the proposed method should be also enhanced . Please see the discussion of theoretical and causal results in the general response above . > The authors claim that the proposed method allows for faster from-scratch training of binarized neural . This seems contradictory to the partial pretraining . The authors may provide more discussion and clarification . The results compare the accuracies of each technique when training the network from random initialization with a fixed training budget . Compared to standard low-precision training , partial pre-training is therefore allotted half as much time in each phase , meaning the methods are compared at equal training budgets . We will clarify this in the final version of the paper . > The improvement of the proposed method is not significant . The proposed algorithm speeds up the training marginally and can not improve the final test accuracy . This limits the contribution . Please see both the discussions of significance of training speedup and also of final accuracy in the general response above ."}], "0": {"review_id": "H6ZWlQrPGS2-0", "review_text": "The paper suggests a method for training binary neural networks . The proposed method is to partially train with full precision and then continue with binarized training using the straight-through estimator . The method is very simple and there is very limited technical contribution , so in order to be worthy of publication it needs to be supported with compelling experimental results . Unfortunately this is not the case . The main claim is speeding up training by a factor of 1.2/1.6 . While this can help the importance of speeding up training ( unless by a much larger factor ) is quiet limited . The useful speedup BNN present is at inference time , and the time to train a network is of much less importance ( at least for this kind of speedup ) . From Fig.1 I am not convinced that the speedup claims hold . You can see the test accuracy for binary training on 3 of the experiments reaches the Partial Pre-training level but it does n't look like it completely flattened out yet . It looks like if you want take the best model on test , then you do n't get any speedup Results on cifar-10 seem quiet poor ( both full precision and 1-bit ) . For example `` Learning discrete weights using the local reparametrization trick '' gets 93.2 1-bit acc on cifar-10 with VGG .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for feedback and commentary ! We have provided a general response to all reviewers above ( [ https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ] ( https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ) ) . Here we will provide response to your individual points . > The main claim is speeding up training by a factor of 1.2/1.6 . While this can help the importance of speeding up training ( unless by a much larger factor ) is quiet limited . The useful speedup BNN present is at inference time , and the time to train a network is of much less importance ( at least for this kind of speedup ) . Please see the discussion of significance of training speedup in the general response above . > It looks like if you want take the best model on test , then you do n't get any speedup ... Please see the discussion of final accuracy in the general response above . > Results on cifar-10 seem quiet poor ( both full precision and 1-bit ) ... The accuracies reported for CIFAR-10 are standard for the network and hyperparameters used in the evaluation [ 7 ; Tables 3,4 ] . Partial pre-training can be applied with an arbitrary low-precision training setup ( including other quantization approaches or other training schemes like that of [ 8 ] ) in the second phase , though validation of this for other quantization approaches is left to future work . Please also see the discussion of hyperparameter choices and sweeps in the general response above . # # # Citations [ 7 ] `` Binary Neural Networks : A Survey , '' Haotong Qin , Ruihao Gong , Xianglong Liu , Xiao Bai , Jingkuan Song , Nicu Sebe ."}, "1": {"review_id": "H6ZWlQrPGS2-1", "review_text": "This paper proposed one simple method called partial pre-training to speed up the training of binary neural networks ( BNN ) . The pros and cons are as follows : Pros : 1 . The partial pre-training method is simple and easy to implement ; 2 . For standard binary optimizer like the straight-through-estimator ( STE ) , the method improves the training speed to some extent ; Cons : 1 . The main concern of the proposed method is kind of heuristic and there is a lack of theoretical explanation , whether rigorous or not , why this method works . 2.As described in Section 7 by the author themselves , there are several apparent limitations of current evaluation , e.g. , several dimensions of the hyper-parameters are not explored , other Binary optimizers are not considered , different learning rate schedules , etc . As a result , it is unconvinced that the partial pre-training could universally improve the speed as a general method . In addition , the improvement of speed-up are not very apparent especially for ResNet-20 and ResNet-34 as shown in Fig.1 and Fig.2 , i.e. , it took approximately the same time to reach the final precision even though the proposed method achieves higher accuracy before saturation . Given the inadequate evaluations and lack of theoretical explanations , this might be due to unfair comparison . 3.There is a lack of explanation of the contradiction result with previous result proposed in Alizadeh et al ( 2019 ) , which dismissed the approach of pre-training . Is there good explanation of such an opposite result ? It would be better to show results of different split between full-precision training and low-precision training . 4.Regarding pre-training for BNN , there are some related works from the Bayesian perspective . In Shayer et al . ( 2018 ) , they used the result of full-precision training as the prior for the binary training , which improves the final result , as opposed to Alizadeh et al ( 2019 ) . The Bayesian perspective provides an explanation of the effectiveness of a good prior . In Meng et al . ( 2020 ) , they showed that STE could be viewed as Bayesian and obtained good result even with a uniform prior . Also , the posterior obtained after full-training ( binary ) could be used as prior to enable continual learning , which shows effectiveness of the prior . Given the above results , since the authors demonstrate that partial pre-training can increase the speed for STE , does this imply that partial pre-training provides a better prior than full pre-training ? If so , why ? Shayer , O. , Levi , D. , and Fetaya , E. Learning discrete weights using the local reparameterization trick . ICLR , 2018 . Meng , X , Bachmann . R. , Khan.E . Training Binary Neural Networks using the Bayesian Learning Rule . ICML , 2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for feedback and commentary ! We have provided a general response to all reviewers above ( [ https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ] ( https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ) ) . Here we will provide response to your individual points . > The main concern of the proposed method is kind of heuristic and there is a lack of theoretical explanation , whether rigorous or not , why this method works . Please see the discussion of theoretical analysis in the general response above . > There is a lack of explanation of the contradiction result with previous result proposed in Alizadeh et al ( 2019 ) , which dismissed the approach of pre-training . Is there good explanation of such an opposite result ? The results compare the accuracies of each technique when training the network from random initialization with a fixed training budget . Compared to standard low-precision training , partial pre-training is therefore allotted half as much time in each phase , meaning the methods are compared at equal training budgets . We will clarify this in the final version of the paper . > It would be better to show results of different split between full-precision training and low-precision training . Results for different splits between full-precision training and low-precision training are presented and discussed in Section 6 . > Regarding pre-training for BNN , there are some related works from the Bayesian perspective ... [ D ] oes this imply that partial pre-training provides a better prior than full pre-training ? The connection to the Bayesian perspective of [ 8,9 ] is an interesting connection to explore in future work . To clarify the exact relationship between our claims and the analysis of [ 9 ] , our results do not imply that partial pre-training provides a better prior than full pre-training . Figure 3a shows that for a fixed amount of low-precision training ( i.e. , elements on any given row ) a larger amount of full-precision training ( i.e. , farther to the right ) results in higher accuracy . # # # Citations [ 8 ] `` Learning Discrete Weights Using the Local Reparameterization Trick , '' Oran Shayer , Dan Levi , Ethan Fetaya . [ 9 ] `` Training Binary Neural Networks using the Bayesian Learning Rule , '' Xiangming Meng , Roman Bachmann , Mohammad Emtiyaz Khan ."}, "2": {"review_id": "H6ZWlQrPGS2-2", "review_text": "This paper addresses the problem of slower training speed with low-precision training of neural nets . It presents a simple solution : first train with full precision on half of the budgeted trainining time , then train with low pecision in the remaining time . This achieves 1.2x - 1.6x speedup compared to low-precision training . Pro : - The proposed idea is simple and it is nice to see that it works Cons : - I feel there is not enough content ( in terms of ideas and experiments ) to warrant a full paper . Compared to other ICLR papers , the contributions seem on the low side . See suggestions below . - The paper mainly compares the proposed method with low-precision training , but the results would be stronger if also compared with full-precision training followed by quantization . This is especially because all the low-precision accuracies trail behind the full precision ones in the results . Suggestions : - Experiment with more quantization methods . Currently we do not know whether the proposed method is uniquely suited to the PACT method used , or is a general technique . - A 1.2x-1.6x speedup on a training process that takes 600 seconds ( e.g.CIFG-10 result in Fig 2 ) seems not so impactful in the grander scheme of things . Even the 12500 second training time is just < 4 hours . I understand the results should transfer , but the results would be more impressive if done on larger training runs . Minor questions/comments : - Please comment on the terminology . Is what you call low precision training similar to the mixed-precision training now implemented in TensorCore NVIDIA GPUs ? - Table 2 : what is it/s . Is it iterations per second ? - Table 1 shows learning rate schedule for t up to 60k in CIFAR or 450k in ImageNet , but Figure 1 stops way before those points in the x-axis . This was somewhat confusing . - Another clarification point about Fig 1 and 2 : for the proposed method , is the full precision training part of the time included in the calculation ? I believe so but just want to double-check .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for feedback and commentary ! We have provided a general response to all reviewers above ( [ https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ] ( https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ) ) . Here we will provide response to your individual points . > The paper mainly compares the proposed method with low-precision training , but the results would be stronger if also compared with full-precision training followed by quantization ... Based on our understanding of this question ( quantizing a pre-trained network then fine-tuning it for a relatively short duration ) , we do compare against this approach in Section 6 , Figure 3 . Please see the discussion of other splits between full-precision training and low-precision training in the general response . > Experiment with more quantization methods ... Please see the discussion of hyperparameter choices and sweeps in the general response above . > A 1.2x-1.6x speedup on a training process ... seems not so impactful in the grander scheme of things Please see the discussion of significance of training speedup in the general response above . > Is what you call low precision training similar to the mixed-precision training now implemented in TensorCore NVIDIA GPUs ? What we define as low-precision training ( training with weights constrained to lie in a small fixed set ) is possible to accelerate in some cases in NVIDIA Tensor Cores , though not always . Specifically , according to [ https : //developer.nvidia.com/blog/tensor-cores-mixed-precision-scientific-computing/ ] ( https : //developer.nvidia.com/blog/tensor-cores-mixed-precision-scientific-computing/ ) NVIDIA Tensor Cores can accelerate low-precision training when the weights are represented by as few as 8 bits . In this paper , we consider low-precision training with 1 bit , which NVIDIA Tensor Cores do not have native support for accelerating . As noted in Section 2 , we are not aware of any existing results that show faster wall-clock speeds for training binarized neural networks . > Table 2 : what is it/s . Is it iterations per second ? Correct , this is iterations per second . We will clarify this in the final version of the paper . > Table 1 shows learning rate schedule for t up to 60k in CIFAR or 450k in ImageNet , but Figure 1 stops way before those points in the x-axis . This was somewhat confusing . Each point on the plot shows a network trained for the specified amount of time using a learning rate schedule which is compressed down to that amount of time , as specified in Algorithm 1 . We will clarify this in the final version of the paper . > ... is the full precision training part of the time included in the calculation ? Correct , full precision training time is included in the calculation . Each point on the plot shows the accuracy when training from random initialization within the given budget ."}, "3": {"review_id": "H6ZWlQrPGS2-3", "review_text": "# # # # Comments Summary : The authors propose a fast binarized neural network training algorithm that splits the whole training process into the full precision training stage and the binary training stage . The experimental results show some improvement about training speed in terms of iterations and wall clock time . Generally , the paper is well written . Strength : -- The idea is reasonable and the method is presented clearly . -- The experimental results indicate that the proposed method can accelerate the convergence of the binary networks on image classification and collaborative filtering . Weakness : -- The comparison between the proposed method and quantization-finetuning method is lacked , which seems like a closely related work . -- The analysis of the proposed method should be also enhanced . The reason why partial pretraining can improve the training of binary neural networks should be investigated more deeply . Comments : ( 1 ) The authors claim that the proposed method allows for faster from-scratch training of binarized neural . This seems contradictory to the partial pretraining . The authors may provide more discussion and clarification . ( 2 ) The improvement of the proposed method is not significant . The proposed algorithm speeds up the training marginally and can not improve the final test accuracy . This limits the contribution . Overall , the reviewer doesn \u2019 t recommend accepting this manuscript at its form . The author may demonstrate more differences between the proposed method and the standard quantization-finetuning method .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for feedback and commentary ! We have provided a general response to all reviewers above ( [ https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ] ( https : //openreview.net/forum ? id=H6ZWlQrPGS2 & noteId=h4-wXzhNMG ) ) . Here we will provide response to your individual points . > The comparison between the proposed method and quantization-finetuning method is lacked Based on our understanding of this question ( quantizing a pre-trained network then fine-tuning it for a relatively short duration ) , we do compare against this approach in Section 6 , Figure 3 . Please see the discussion of other splits between full-precision training and low-precision training in the general response . > The analysis of the proposed method should be also enhanced . Please see the discussion of theoretical and causal results in the general response above . > The authors claim that the proposed method allows for faster from-scratch training of binarized neural . This seems contradictory to the partial pretraining . The authors may provide more discussion and clarification . The results compare the accuracies of each technique when training the network from random initialization with a fixed training budget . Compared to standard low-precision training , partial pre-training is therefore allotted half as much time in each phase , meaning the methods are compared at equal training budgets . We will clarify this in the final version of the paper . > The improvement of the proposed method is not significant . The proposed algorithm speeds up the training marginally and can not improve the final test accuracy . This limits the contribution . Please see both the discussions of significance of training speedup and also of final accuracy in the general response above ."}}