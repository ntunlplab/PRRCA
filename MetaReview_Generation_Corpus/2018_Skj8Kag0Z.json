{"year": "2018", "forum": "Skj8Kag0Z", "title": "Stabilizing Adversarial Nets with Prediction Methods", "decision": "Accept (Poster)", "meta_review": "This paper provides a simple technique for stabilizing GAN training, and works over a variety of GAN models.\n\nOne of the reviewers expressed concerns with the value of the theory. I think that it would be worth emphasizing that similar arguments could be made for alternating gradient descent, and simultaneous gradient descent. In this case, if possible, it would be good to highlight how the convergence of the prediction method approach differs from the alternating descent approach. Otherwise, highlight that this theory simply shows that the prediction method is not a completely crazy idea (in that it doesn't break existing theory).\n\nPractically, I think the experiments are sufficiently interesting to show that this approach has promise. I don't see the updated results for Stacked GAN for a fixed set of epochs (20 and 40 at different learning rates). Perhaps put this below Table 1.", "reviews": [{"review_id": "Skj8Kag0Z-0", "review_text": "NOTE: I'm very willing to change my recommendation if I turn out to be wrong about the issues I'm addressing and if certain parts of the experiments are fixed. Having said that, I do (think I) have some serious issues: both with the experimental evaluation and with the theoretical results. I'm pretty sure about the experimental evaluation and less sure about the theoretical results. THEORETICAL CLAIMS: These are the complaints I'm not as sure about: Theorem 1 assumes that L is convex/concave. This is not generally true for GANs. That's fine and it doesn't necessarily make the statement useless, but: If we are willing to assume that L is convex/concave, then there already exist other algorithms that will provably converge to a saddle point (I think). [1] contains an explanation of this. Given that there are other algorithms with the same theoretical guarantees, and that those algorithms don't magically make GANs work better, I am much less convinced about the value of your theorem. In [0] they show that GANs trained with simultaneous gradient descent are locally asymptotically stable, even when L is not convex/concave. This seems like it makes your result a lot less interesting, though perhaps I'm wrong to think this? Finally, I'm not totally sure you can show that simultaneous gradient descent won't converge as well under the assumptions you made. If you actually can't show that, then the therom *is* useless, but it's also the thing I've said that I'm the least sure about. EXPERIMENTAL EVALUATION: Regarding the claims of being able to train with a higher learning rate: I would consider this a useful contribution if it were shown that (by some measure of GAN 'goodness') a high goodness was achieved faster because a higher learning rate was used. Your experiments don't support this claim presently, because you evaluate all the models at the same step. In fact, it seems like both evaluated Stacked GAN models get worse performance with the higher learning rate. This calls into question the usefulness of training with a higher learning rate. The performance is not a huge amount worse though (based on my understanding of Inception Scores), so if it turns out that you could get that performance in 1/10th the time then that wouldn't be so bad. Regarding the experiment with Stacked GANs, the scores you report are lower than what they report [2]. Their reported mean score for joint training is 8.59. Are the baseline scores you report from an independent reproduction? Also, the model they have trained uses label information. Does your model use label information? Given that your reported improvements are small, it would be nice to know what the proposed mechanism is by which the score is improved. With a score of 7.9 and a standard deviation of 0.08, presumably none of the baseline model runs had 'stability issues', so it doesn't seem like 'more stable training' can be the answer. Finally, papers making claims about fixing GAN stability should support those claims by solving problems with GANs that people previously had a hard time solving (due to instability). I don't believe this is true of CIFAR10 (especially if you're using the class information). See [3] for an example of a paper that does this by generating 128x128 Imagenet samples with a single generator. I didn't pay as much attention to the non-GAN experiments because a) I don't have as much context for evaluating them, because they are a bit non-standard. b) I had a lot of issues with the GAN experiments already and I don't think the paper should be accepted unless those are addressed. [0] https://arxiv.org/abs/1706.04156 (Gradient Descent GAN Optimization is Locally Stable) [1] https://arxiv.org/pdf/1705.07215.pdf (On Convergence and Stability of GANs) [2] https://arxiv.org/abs/1612.04357 (Stacked GAN) [3] https://openreview.net/forum?id=B1QRgziT (Spectral Regularization for GANs) EDIT: As discussed below, I have slightly raised my score. I would raise it more if more of my suggestions were implemented (although I'm aware that the authors don't have much (any?) time for this - and that I am partially to blame for that, since I didn't respond that quickly). I have also slightly raised my confidence. This is because now I've had more time to think about the paper, and because the authors didn't really address a lot of my criticisms (which to me seems like evidence that some of my criticisms were correct).", "rating": "4: Ok but not good enough - rejection", "reply_text": "We agree with the reviewer that theory in this area ( and in deep learning in general ) often requires assumptions that don \u2019 t hold for neural networks . Nonetheless , we think it is worth taking time to explore conditions under which algorithms are guaranteed to work , because this provides a theoretical proof-of-concept , and thinking through theoretical properties of a new algorithm makes it more than just another hack . The purpose of our result is to do just that for our proposed algorithm . We don \u2019 t disagree that analysis exists for other algorithms , but we don \u2019 t think the existence of other algorithms gets us \u201c off the hook \u201d from thinking about the theoretical implications of our approach . That being said , we think the reviewer is overestimating the state of the art in theory for GANs . There is currently no theoretical result that does not make strong assumptions , and many results ( including those referenced by the reviewer ) are quite different from ( and in many ways weaker than ) our own . The result in [ 1 ] shares certain assumptions with our own ( convex-concave assumptions , bounded problem domain , and an ergodic measure of convergence ) . However , the result in [ 1 ] does not prove convergence in the usual sense , but rather that the error will decay to within an o ( 1 ) constant . In contrast , our result shows that the error decays to zero . The result in [ 1 ] also requires simultaneous gradient descent , which is not commonly used in practice ( because it requires more RAM to store [ extremely large ] iterates and it uses a stale iterate when updating the generator and discriminator one-at-a-time ) . In contrast , our result concerns the commonly used alternating direction approach . The result in [ 0 ] shows stability using a range of assumptions that are different from ( but not necessarily stronger or weaker than ) our own . They require the discriminator to be a linear classifier , and make a strict concavity assumption on the loss function . They also require an assumption ( called Property I ) that is analogous to the \u201c strict saddle \u201d assumption in the saddle-point literature ( see , e.g.Lee 2016 , \u201c Gradient Descent Converges to Minimizers \u201d ) , which is known not to hold for general neural nets . Also , note that the result in [ 0 ] is only a local stability result ( it only holds once the iterates get close to a saddle satisfying the assumptions ) , whereas our result is a global convergence result that holds for any initialization . Finally , we emphasize that both [ 0 ] and [ 1 ] are great works that make numerous important contributions to this field and address a host of issues beyond just convergence proofs . Our purpose here is not to make any claims that our result is \u201c better \u201d than theirs , but rather to state what differentiates our result from the literature , and why we felt it was worth putting it in the paper ."}, {"review_id": "Skj8Kag0Z-1", "review_text": "This paper proposes a simple modification to the standard alternating stochastic gradient method for GAN training, which stabilizes training, by adding a prediction step. This is a clever and useful idea, and the paper is very well written. The proposed method is very clearly motivated, both intuitively and mathematically, and the authors also provide theoretical guarantees on its convergence behavior. I particularly liked the analogy with the damped harmonic oscillator. The experiments are well designed and provide clear evidence in favor of the usefulness of the proposed technique. I believe that the method proposed in this paper will have a significant impact in the area of GAN training. I have only one minor question: in the prediction step, why not use a step size, say $\\bar{u}_k+1 = u_{k+1} + \\gamma_k (u_{k+1} \u2212 u_k)$, such that the \"amount of predition\" may be adjusted? ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thanks for the thoughtful comments ! To answer your question : it is indeed possible to generalize this method by adding an extra stepsize parameter for the prediction step , and this is something that we have experimented with extensively . It can be shown that your proposed \u201c gamma \u201d parameter method is stable ( under convexity assumptions ) whenever gamma is between 0 and 2 . However , we have not been able to find any worthwhile advantages to choosing any gamma different from 1 . Choosing a smaller gamma weakens the stability benefits of prediction , and choosing a larger gamma seems to slow down convergence a bit . The latter effect can be compensated for by choosing a larger learning rate , but even in this case the method doesn \u2019 t run noticeably faster than with gamma=1 . For this reason , including this \u201c gamma \u201d seemed like unnecessarily added complexity , so we removed it and went with a cleaner presentation ."}, {"review_id": "Skj8Kag0Z-2", "review_text": "This work proposes a framework for stabilizing adversarial nets using a prediction step. The prediction step is motivated by primal-dual algorithms in convex optimization where the term having both variables is bi-linear. The authors prove a convergence result when the function is convex in one variable and concave in the other. This problem is more general than the previous one in convex optimization. Then this prediction step is applied in many recent applications in training adversarial nets and compared with state-of-the-art solvers. The better performance of this simple step is shown in most of the numerical experiments. Though this work applies one step from the convex optimization to solve a more complicated problem and obtain improved performance, there is more work to be done. Whether there is a better generalization of this prediction step? There are also other variants of primal-dual algorithms in convex optimization; can other modification including the accelerated variants be applied?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the thoughtful comments and suggestions for future work . We think the idea of pursuing accelerated methods is particularly interesting . We have actually already done some experiments with Nesterov-type acceleration ( as described for saddle-point problems by Chambolle and Pock ) , however it seems that the benefits of acceleration vanish when we move from deterministic to stochastic updates . We \u2019 ve made similar observations for standard convex ( non-saddle ) problems . That being said , we \u2019 re still interested in this direction , and are keeping our eyes peeled for possible ways forward ."}], "0": {"review_id": "Skj8Kag0Z-0", "review_text": "NOTE: I'm very willing to change my recommendation if I turn out to be wrong about the issues I'm addressing and if certain parts of the experiments are fixed. Having said that, I do (think I) have some serious issues: both with the experimental evaluation and with the theoretical results. I'm pretty sure about the experimental evaluation and less sure about the theoretical results. THEORETICAL CLAIMS: These are the complaints I'm not as sure about: Theorem 1 assumes that L is convex/concave. This is not generally true for GANs. That's fine and it doesn't necessarily make the statement useless, but: If we are willing to assume that L is convex/concave, then there already exist other algorithms that will provably converge to a saddle point (I think). [1] contains an explanation of this. Given that there are other algorithms with the same theoretical guarantees, and that those algorithms don't magically make GANs work better, I am much less convinced about the value of your theorem. In [0] they show that GANs trained with simultaneous gradient descent are locally asymptotically stable, even when L is not convex/concave. This seems like it makes your result a lot less interesting, though perhaps I'm wrong to think this? Finally, I'm not totally sure you can show that simultaneous gradient descent won't converge as well under the assumptions you made. If you actually can't show that, then the therom *is* useless, but it's also the thing I've said that I'm the least sure about. EXPERIMENTAL EVALUATION: Regarding the claims of being able to train with a higher learning rate: I would consider this a useful contribution if it were shown that (by some measure of GAN 'goodness') a high goodness was achieved faster because a higher learning rate was used. Your experiments don't support this claim presently, because you evaluate all the models at the same step. In fact, it seems like both evaluated Stacked GAN models get worse performance with the higher learning rate. This calls into question the usefulness of training with a higher learning rate. The performance is not a huge amount worse though (based on my understanding of Inception Scores), so if it turns out that you could get that performance in 1/10th the time then that wouldn't be so bad. Regarding the experiment with Stacked GANs, the scores you report are lower than what they report [2]. Their reported mean score for joint training is 8.59. Are the baseline scores you report from an independent reproduction? Also, the model they have trained uses label information. Does your model use label information? Given that your reported improvements are small, it would be nice to know what the proposed mechanism is by which the score is improved. With a score of 7.9 and a standard deviation of 0.08, presumably none of the baseline model runs had 'stability issues', so it doesn't seem like 'more stable training' can be the answer. Finally, papers making claims about fixing GAN stability should support those claims by solving problems with GANs that people previously had a hard time solving (due to instability). I don't believe this is true of CIFAR10 (especially if you're using the class information). See [3] for an example of a paper that does this by generating 128x128 Imagenet samples with a single generator. I didn't pay as much attention to the non-GAN experiments because a) I don't have as much context for evaluating them, because they are a bit non-standard. b) I had a lot of issues with the GAN experiments already and I don't think the paper should be accepted unless those are addressed. [0] https://arxiv.org/abs/1706.04156 (Gradient Descent GAN Optimization is Locally Stable) [1] https://arxiv.org/pdf/1705.07215.pdf (On Convergence and Stability of GANs) [2] https://arxiv.org/abs/1612.04357 (Stacked GAN) [3] https://openreview.net/forum?id=B1QRgziT (Spectral Regularization for GANs) EDIT: As discussed below, I have slightly raised my score. I would raise it more if more of my suggestions were implemented (although I'm aware that the authors don't have much (any?) time for this - and that I am partially to blame for that, since I didn't respond that quickly). I have also slightly raised my confidence. This is because now I've had more time to think about the paper, and because the authors didn't really address a lot of my criticisms (which to me seems like evidence that some of my criticisms were correct).", "rating": "4: Ok but not good enough - rejection", "reply_text": "We agree with the reviewer that theory in this area ( and in deep learning in general ) often requires assumptions that don \u2019 t hold for neural networks . Nonetheless , we think it is worth taking time to explore conditions under which algorithms are guaranteed to work , because this provides a theoretical proof-of-concept , and thinking through theoretical properties of a new algorithm makes it more than just another hack . The purpose of our result is to do just that for our proposed algorithm . We don \u2019 t disagree that analysis exists for other algorithms , but we don \u2019 t think the existence of other algorithms gets us \u201c off the hook \u201d from thinking about the theoretical implications of our approach . That being said , we think the reviewer is overestimating the state of the art in theory for GANs . There is currently no theoretical result that does not make strong assumptions , and many results ( including those referenced by the reviewer ) are quite different from ( and in many ways weaker than ) our own . The result in [ 1 ] shares certain assumptions with our own ( convex-concave assumptions , bounded problem domain , and an ergodic measure of convergence ) . However , the result in [ 1 ] does not prove convergence in the usual sense , but rather that the error will decay to within an o ( 1 ) constant . In contrast , our result shows that the error decays to zero . The result in [ 1 ] also requires simultaneous gradient descent , which is not commonly used in practice ( because it requires more RAM to store [ extremely large ] iterates and it uses a stale iterate when updating the generator and discriminator one-at-a-time ) . In contrast , our result concerns the commonly used alternating direction approach . The result in [ 0 ] shows stability using a range of assumptions that are different from ( but not necessarily stronger or weaker than ) our own . They require the discriminator to be a linear classifier , and make a strict concavity assumption on the loss function . They also require an assumption ( called Property I ) that is analogous to the \u201c strict saddle \u201d assumption in the saddle-point literature ( see , e.g.Lee 2016 , \u201c Gradient Descent Converges to Minimizers \u201d ) , which is known not to hold for general neural nets . Also , note that the result in [ 0 ] is only a local stability result ( it only holds once the iterates get close to a saddle satisfying the assumptions ) , whereas our result is a global convergence result that holds for any initialization . Finally , we emphasize that both [ 0 ] and [ 1 ] are great works that make numerous important contributions to this field and address a host of issues beyond just convergence proofs . Our purpose here is not to make any claims that our result is \u201c better \u201d than theirs , but rather to state what differentiates our result from the literature , and why we felt it was worth putting it in the paper ."}, "1": {"review_id": "Skj8Kag0Z-1", "review_text": "This paper proposes a simple modification to the standard alternating stochastic gradient method for GAN training, which stabilizes training, by adding a prediction step. This is a clever and useful idea, and the paper is very well written. The proposed method is very clearly motivated, both intuitively and mathematically, and the authors also provide theoretical guarantees on its convergence behavior. I particularly liked the analogy with the damped harmonic oscillator. The experiments are well designed and provide clear evidence in favor of the usefulness of the proposed technique. I believe that the method proposed in this paper will have a significant impact in the area of GAN training. I have only one minor question: in the prediction step, why not use a step size, say $\\bar{u}_k+1 = u_{k+1} + \\gamma_k (u_{k+1} \u2212 u_k)$, such that the \"amount of predition\" may be adjusted? ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thanks for the thoughtful comments ! To answer your question : it is indeed possible to generalize this method by adding an extra stepsize parameter for the prediction step , and this is something that we have experimented with extensively . It can be shown that your proposed \u201c gamma \u201d parameter method is stable ( under convexity assumptions ) whenever gamma is between 0 and 2 . However , we have not been able to find any worthwhile advantages to choosing any gamma different from 1 . Choosing a smaller gamma weakens the stability benefits of prediction , and choosing a larger gamma seems to slow down convergence a bit . The latter effect can be compensated for by choosing a larger learning rate , but even in this case the method doesn \u2019 t run noticeably faster than with gamma=1 . For this reason , including this \u201c gamma \u201d seemed like unnecessarily added complexity , so we removed it and went with a cleaner presentation ."}, "2": {"review_id": "Skj8Kag0Z-2", "review_text": "This work proposes a framework for stabilizing adversarial nets using a prediction step. The prediction step is motivated by primal-dual algorithms in convex optimization where the term having both variables is bi-linear. The authors prove a convergence result when the function is convex in one variable and concave in the other. This problem is more general than the previous one in convex optimization. Then this prediction step is applied in many recent applications in training adversarial nets and compared with state-of-the-art solvers. The better performance of this simple step is shown in most of the numerical experiments. Though this work applies one step from the convex optimization to solve a more complicated problem and obtain improved performance, there is more work to be done. Whether there is a better generalization of this prediction step? There are also other variants of primal-dual algorithms in convex optimization; can other modification including the accelerated variants be applied?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the thoughtful comments and suggestions for future work . We think the idea of pursuing accelerated methods is particularly interesting . We have actually already done some experiments with Nesterov-type acceleration ( as described for saddle-point problems by Chambolle and Pock ) , however it seems that the benefits of acceleration vanish when we move from deterministic to stochastic updates . We \u2019 ve made similar observations for standard convex ( non-saddle ) problems . That being said , we \u2019 re still interested in this direction , and are keeping our eyes peeled for possible ways forward ."}}