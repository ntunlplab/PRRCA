{"year": "2021", "forum": "fhcMwjavKEZ", "title": "A Simple and General Graph Neural Network with Stochastic Message Passing", "decision": "Reject", "meta_review": "This work proposes a modification of a GNN architecture by feeding random node features to bootstrap the message propagation. This enables the discriminability of automorphic node pairs with a lightweight, simple change. Experiments are reported showing improvements over baselines. \nReviewers had mixed impressions of this work. On one hand, they found the proposed model principled and with strong empirical performance. On the other hand, they perceived a general lack of novelty and a somewhat misleading theoretical analysis. After careful review, the AC ultimately believes that this work does require an extra iteration that further solidifies the contributions and aligns the theoretical analysis with the empirical performance. In particular, the use of random initialization is folklore in the GNN literature, especially with regards to spectral methods (e.g. power iterations are typically initialized using a random vector, and these constitute the simplest forms of linear GNNs). The authors are encouraged to address these comparisons with further detail, as well as the excellent feedback given by the reviewers. ", "reviews": [{"review_id": "fhcMwjavKEZ-0", "review_text": "While the paper is easy to follow , I found all the results in this paper trivial and already-known . Pros : 1.The paper is well-written and easy to follow . Cons : 1.The entire Section 3 is not novel . It is a mixture of the preliminaries of GNNs and the trivial results of Corollary 1 and Theorem 1 . Moreover , Theorem 1 is not rigorous . What if walk-based proximity is just a constant function ? What if a given graph does not contain any automorphism ? 2. \u201c Preserve walk-based similarity \u201d is not rigorously defined . It seems that it just means all nodes have different embeddings . 3.The proposed model is not permutation-equivariant after adding Gaussian noise . It is trivial that the model becomes permutation-equivariant when the Gaussian noise is ignored ( because the model just reduces to an ordinary GNN ) . 4.The claim that SMP preserves walk-based proximity is trivial and just relies on the fact that randomly-sampled vectors from a Gaussian distribution are different from each other . 5.The idea of using node identifiers ( essentially equivalent to the Gaussian noise ) to make GNNs position-aware is not new . In fact , this idea is clearly mentioned in the P-GNN paper already ( Section 6.2 of [ 1 ] \u201c for inductive tasks , augmenting node attributes with one-hot identifiers restricts a model \u2019 s generalization ability \u201d ) . 6.In Section 5.3 , it is unclear why the OGB node classification datasets are not used . [ 1 ] https : //arxiv.org/abs/1906.04817", "rating": "3: Clear rejection", "reply_text": "Thank you for your detailed comments . Here are our responses to your questions . Q1-1.The entire Section 3 is not novel . It is a mixture of the preliminaries of GNNs and the trivial results of Corollary 1 and Theorem 1 . A1-1.Section 3 provides the necessary preliminaries of GNNs and several key definitions , which is important to keep our paper self-contained . Besides , we give novel proof that the existing permutation-equivariant GNNs can not preserve node proximities , which lays the foundations for further developing our proposed method . Q1-2.Moreover , Theorem 1 is not rigorous . What if walk-based proximity is just a constant function ? A1-2.Thanks for the suggestion . We have clarified that we only consider non-trivial walk-based proximity in the revised version . Please refer to Theorem 1 of the revised manuscript for details . Q1-3.What if a given graph does not contain any automorphism ? A1-3 : As we show in Theorem 1 , ( non-trivial ) automorphism presents an important limitation to the existing GNNs , i.e. , preventing GNNs from being proximity-aware . Considering the importance of this limitation , we think it is critical to study this problem even not all graphs contain automorphism . Q2. \u201c Preserve walk-based similarity \u201d is not rigorously defined . It seems that it just means all nodes have different embeddings . A2.We kindly do not agree that preserving walk-based proximity is equivalent to having different node embeddings . The definition was formally provided in Definition 4 in Appendix A.1 due to the page limit and we have moved it into the main paper ( Section 3 ) in the revised version . In a nutshell , a GNN is said to be able to preserve walk-based similarities if we can recover the similarity between two nodes from the corresponding two node embeddings . Thus , the embedding vectors need to encode sufficient information of the walk-based proximity rather than simply being unique node identifiers . Please refer to the revised manuscript for details . Q3.The proposed model is not permutation-equivariant after adding Gaussian noise . It is trivial that the model becomes permutation-equivariant when the Gaussian noise is ignored ( because the model just reduces to an ordinary GNN ) . A3.We kindly do not agree that these theorems should be considered trivial . Our Remark 1 and Corollary 2 indeed show that we can easily recover a permutation-equivariant GNN by ignoring the stochastics representations . Though not using sophisticated proof strategies , these theorems are indispensable to show that our proposed method is able to handle both permutation-equivariant and proximity-aware tasks . On the contrary , though P-GNN can preserve node proximities to a certain extent , it can not reduce to a permutation-equivariant GNN and thus fail to handle tasks where permutation-equivariance is helpful . We also empirically validate the importance of this result in experiments , i.e. , in the node classification task in Section 5.3 , our proposed SMP achieves comparable results as GCNs but P-GNN performs poorly . As a result , we believe these theorems are important for our proposed method . Q4.The claim that SMP preserves walk-based proximity is trivial and just relies on the fact that randomly-sampled vectors from a Gaussian distribution are different from each other . A4.We kindly do not agree that the theorem is trivial . If only i.i.d.Gaussian random vectors are adopted as the reviewer suggests , it is impossible to preserve walk-based proximities since these random vectors are independent of the proximity . In fact , we need to properly perform message-passing on those random vectors and carefully decode the node representations to preserve the proximity . We have given and proved the exact procedure for both the linear case ( see Theorem 2 ) and the non-linear case ( see Theorem 3 ) . Q5.The idea of using node identifiers ( essentially equivalent to the Gaussian noise ) to make GNNs position-aware is not new . In fact , this idea is clearly mentioned in the P-GNN paper already ( Section 6.2 of [ 1 ] \u201c for inductive tasks , augmenting node attributes with one-hot identifiers restricts a model \u2019 s generalization ability \u201d ) . [ 1 ] https : //arxiv.org/abs/1906.04817 A5 . We humbly do not agree that using Gaussian random vectors is equivalent to using one-hot identifiers . We have compared our proposed method using random vectors with using one-hot IDs as node identifiers in Table 8 in the appendix . The results show that our proposed method generally achieves better results . Besides , using one-hot IDs will drastically increase the number of parameters since the input features of GNNs have a dimensionality of O ( n ) . Besides , we \u2019 d like to point out that we have compared with P-GNN as a baseline in all the experiments ."}, {"review_id": "fhcMwjavKEZ-1", "review_text": "This paper proposed a proximity-aware graph neural network while maintaining the permutation equivariance property . The proposed model , dubbed as stochastic message passing ( SMP ) , arguments the existing GNNs with stochastic node representations . The author proved the proposed method can model proximity-aware representations based on random projection theory . The experimental results show that the SMP can be used for multiple graphs and tasks . Although the proposed technique is relatively simple , the theorem shows that why such stochastic representations are beneficial for proximity-aware tasks . The experimental results also suggest that this simple technique is quite effective in many tasks . There are a few things that I 'd like to comment on or ask listed below : - Multiple tasks have been conducted to show the performance of the proposed model , but it is unclear which dataset or which task requires to be proximity-aware . It would be great if there 's some quantitative metric that shows the importance of proximity in a given task . For example , the number of automorphic node pairs ( within k-hop ) and the ratio of label ( dis ) agreement would be a possible metric in node classification tasks . This will characterize the differences between datasets and highlight the contribution of the proposed method . Additionally , showing some examples of automorphic node pairs and the performance on these nodes could demonstrate the difference between models . - Resampling random matrix at each epoch is emphasized multiple times in the manuscript , but without any empirical experiments . Would it be beneficial to resample this random matrix at every epoch ? Although in theory , it would be possible to learn GNN that preserves node proximity , if a given task does n't need to model proximity-aware representations , random resampling may hinder the convergence of the proposed method . - There was a bug in the official release of the P-GNN paper , which has been recently fixed ( please check the GitHub pull request history at https : //github.com/JiaxuanYou/P-GNN/pull/12 ) . I wonder which codebase the authors used for the experiments . / It would be also good if there 's some comment on what makes P-GNN memory hunger . Is it because of improper implementation or because of some inherent limitations ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your recognition and comments . Here are our responses to your questions . Q1 : Multiple tasks have been conducted to show the performance of the proposed model , but it is unclear which dataset or which task requires to be proximity-aware . It would be great if there 's some quantitative metric that shows the importance of proximity in a given task . For example , the number of automorphic node pairs ( within k-hop ) and the ratio of label ( dis ) agreement would be a possible metric in node classification tasks . This will characterize the differences between datasets and highlight the contribution of the proposed method . Additionally , showing some examples of automorphic node pairs and the performance on these nodes could demonstrate the difference between models . A1 : Thank you for this insightful suggestion . We agree that a metric to quantify to what degree a task is proximity-aware or permutation-equivariant will be helpful for further studying GNNs . However , we find that such a metric is difficult to design due to two reasons . Firstly , only comparing whether two nodes are strictly automorphic , as you have suggested , is important but may not be sufficient since if the local structures of two nodes are very similar ( but not strictly automorphic ) , the results of GNNs will also be affected . Thus , we need to quantify the similarities between local structures of different nodes ( rather than only comparing whether they are exactly automorphic ) . However , this is an interesting research question itself , e.g. , commonly studied in graph kernels . Secondly , besides this metric related to permutation-equivariance , we need another metric to quantify whether the task can benefit from proximity-awareness , which is also non-trivial to develop . All things considered , we leave studying this interesting topic as important future works . Q2 : Resampling random matrix at each epoch is emphasized multiple times in the manuscript , but without any empirical experiments . Would it be beneficial to resample this random matrix at every epoch ? Although in theory , it would be possible to learn GNN that preserves node proximity , if a given task does n't need to model proximity-aware representations , random resampling may hinder the convergence of the proposed method . A2 : Thank you for this comment . As we explain in Section 4.1 , we find that fixing the stochastic matrix can help the model to memorize the stochastic representation and distinguish different nodes , while resampling the matrix in each epoch will make our method more capable of handling nodes not seen during training . As a result , we fix the stochastic matrix on transductive datasets and resample the stochastic matrix on inductive datasets . Following your comments , we have added an ablation study in Table 12 in Appendix B.5 in the revised manuscript . The results verify that our design leads to better results in most cases , i.e. , fixing the stochastic matrix on transductive datasets and resampling the stochastic matrix on inductive datasets . Please refer to the revised manuscript for details . Q3 : There was a bug in the official release of the P-GNN paper , which has been recently fixed ( please check the GitHub pull request history at https : //github.com/JiaxuanYou/P-GNN/pull/12 ) . I wonder which codebase the authors used for the experiments . / It would be also good if there 's some comment on what makes P-GNN memory hunger . Is it because of improper implementation or because of some inherent limitations ? A3 : Thank you for this notice . We indeed adopt the implementation by the authors . Since this bug is fixed after our submission , the results in the paper are based on the implementation before the fix . We have tried the fixed codes and find the results comparable or even slightly worse than the results reported in the paper . As for the memory issue , we think it is caused by the mechanism of P-GNN , i.e. , explicit construction of the anchor nodes and performing message-passing on them ."}, {"review_id": "fhcMwjavKEZ-2", "review_text": "The authors propose to add random node features to the input of message passing graph neural networks for them to become proximity-aware . The paper provides exhaustive theoretical and empirical analysis of this idea , highlighting the advantageous properties . Strengths : - The paper is well written . - The authors substantiate their claims with proofs and experiments . - The method and proofs seem to be mathematically sound . - Including the experiments in the appendix , the empirical analysis is very exhaustive and seems to be reproducible . - The method is simple but effective , which is good . It also does not lead to a significant computational overhead but fits nicely into the existing message passing framework with linear time complexity ( in number of edges ) . - The paper provides a formal framework and analysis for a trick that has already been used successfully in practice . Comments and Questions : - The work of Sato et al . [ 1 ] seems to be closer to this work than the authors let know . I would welcome a more in-depth discussion than given in related work , even if the paper is only on arxiv . - Definition 4 should go to the main text , as it is crucial for understanding . - The proof of Theorem 1 shows the result only for graphs with 2 connected components . I think the theorem also holds for connected graphs ( with the right automorphisms ) , which is important . If it would n't , the result would not always be relevant for practice . I think the current way of proving the theorem lacks that insight and discussion . Is n't it possible to prove the theorem for all graphs with a certain automorphism , regardless of number of connected components ? - Can the authors verify the suspicion that the non-linear variants overfit ( line 270 ) by presenting the results on training data ? - It might be of interest to the authors that the idea was already used in a practical graph matching method by [ 2 ] ( page 5 , last paragraph ) , although without any theoretical analysis . It is nice to have a formal framework that justifies the application of this trick . - In general , GNNs to solve matching tasks are a very fitting application for the proposed method , which the authors do not consider . They often rely on comparing distance measures in both domains , thus need proximity awareness . Typos : - Line 87 : `` computationally expansive '' - > expensive - Line 607 : `` computationally expansive '' - > expensive All in all , there is not much to complain about . The paper achieves what it sets out to do in providing an exhaustive theoretical and empirical analysis of a simple but effective idea . The method itself is straight-forward and also not entirely novel . However , the formal framework and analysis is a contribution that is of interest to the community . Due to the shown properties and the high efficiency of the approach , the paper can have significant impact on future GNN architectures in practice . I therefore recommend to accept the paper . [ 1 ] Sato et al. , Random features strengthen graph neural networks . arXiv:2002.03155 [ 2 ] Fey et al. , Deep Graph Matching Consensus , ICLR 2020", "rating": "7: Good paper, accept", "reply_text": "Thanks for your kind words and detailed comments . Here are our responses to your comments . Q1.The work of Sato et al . [ 1 ] seems to be closer to this work than the authors let know . I would welcome a more in-depth discussion than given in related work , even if the paper is only on arxiv . [ 1 ] Sato et al. , Random features strengthen graph neural networks . arXiv:2002.03155 A1 . Thanks for your suggestion . We have revised the related works as follows : \u201d For example , Sato et al . ( Sato et al. , 2020 ) novelly show that random numbers can enhance GNNs in tackling two important graph-based NP problems with a theoretical guarantee , namely the minimum dominating set and the maximum matching problem\u2026Our work differs in that we systematically study how to preserve permutation-equivariance and proximity-awareness simultaneously in a simple yet effective framework , which is a new topic different from these existing works . Besides , we theoretically prove that our proposed method can preserve walk-based proximities by using the random projection literature . We also demonstrate the effectiveness of our method on various large-scale benchmarks for both node- and edge-level tasks , while no similar results are reported in the literature. \u201d Q2 . Definition 4 should go to the main text , as it is crucial for understanding . A2 : Thanks for the suggestion . We have moved the definition to the main paper in the revised version . Q3.The proof of Theorem 1 shows the result only for graphs with 2 connected components . I think the theorem also holds for connected graphs ( with the right automorphisms ) , which is important . If it would n't , the result would not always be relevant for practice . I think the current way of proving the theorem lacks that insight and discussion . Is n't it possible to prove the theorem for all graphs with a certain automorphism , regardless of number of connected components ? A3.Thanks for this insightful and constructive comment . We indeed find that we can prove the theorem using one connected component under a mild assumption ( i.e. , the walk-based proximity is of finite length ) . We have provided this additional proof in Appendix A.1 in the revised version . The essential idea is to construct automorphism using three copies of the graph and three bridges to connect these copies . Please refer to the revised manuscript for details . Q4.Can the authors verify the suspicion that the non-linear variants overfit ( line 270 ) by presenting the results on training data ? A4 : Thank you for this insightful comment . We have provided your suggested experiments in Appendix B.6 in the revised version . The results show that non-linear variants of GNNs such as GAT indeed exhibit more serious overfitting , i.e. , the margins between the training accuracies and the testing accuracies are usually larger than the linear variant SGC . Besides , non-linear variants are also difficult to train , i.e. , the training accuracies are sometimes also worse than the linear variant SGC . We have clarified our expressions as follows : \u201c Some plausible reasons include that the additional model complexity brought by non-linear operators makes the models tend to overfit and also difficult to train. \u201d Q5 . It might be of interest to the authors that the idea was already used in a practical graph matching method by [ 2 ] ( page 5 , last paragraph ) , although without any theoretical analysis . It is nice to have a formal framework that justifies the application of this trick . [ 2 ] Fey et al. , Deep Graph Matching Consensus , ICLR 2020 A5 . Thanks for your notice on this interesting relevant literature . We have added it to the related works in the revised version . Q6.In general , GNNs to solve matching tasks are a very fitting application for the proposed method , which the authors do not consider . They often rely on comparing distance measures in both domains , thus need proximity awareness . A6.Thanks for your constructive suggestion . We will consider trying our proposed method in graph matching tasks . Q7 : Typos : Line 87 : `` computationally expansive '' - > expensive Line 607 : `` computationally expansive '' - > expensive A7 : Thanks for pointing them out . We have fixed them in the revised version . Q8.All in all , there is not much to complain about . The paper achieves what it sets out to do in providing an exhaustive theoretical and empirical analysis of a simple but effective idea . The method itself is straight-forward and also not entirely novel . However , the formal framework and analysis is a contribution that is of interest to the community . Due to the shown properties and the high efficiency of the approach , the paper can have significant impact on future GNN architectures in practice . I therefore recommend to accept the paper . A8 : Thank you for your recognition that our paper can significantly impact future GNNs ! We intend to further explore this direction in the near future ."}, {"review_id": "fhcMwjavKEZ-3", "review_text": "Since the publication of P-GNN ( You et al. , 2019 ) , it has become clear to the graph ML community that node positional information can be effectively leveraged for link prediction and pairwise node classification tasks . This paper introduces SMP , a novel stochastic message passing approach that preserves both permutation-equivariance ( common to GNN models ) and node proximities . Extensive experimental results show that SMP not only achieves competitive performance on many common graph ML datasets , but also succeeds to combine together the expressiveness of a standard GNN with P-GNN ( without incurring the scalability problem of P-GNN ) . I thoroughly enjoyed reading this paper , both for the insights and the technical soundness . I have very few remarks about the paper , as I believe that i ) SMP is an ingenious idea , ii ) the experimental setting is adequate , iii ) the quality of the writeup is high , iv ) and the results appear to be reproducible . If I had to nitpick , I 'd say that part of Table 7 ( currently in the Appendix ) belongs to the main paper , as I was convinced about the superior runtime performance of SMP only after reading those numbers . If the avg running time for an SMP epoch was significantly larger than the one for GAT ( for instance ) , I would have considered SMP yet another specialized model . Instead , given that the GPU consumption ( both in terms of computation and memory ) is similar to any other GNN model , I believe SMP could be adopted as a more flexible graph ML method ( which would avoid having to choose a method given the target task , e.g. , node classification vs. link prediction ) . One remark about L278 : SMP can not be considered anymore SotA for ogbl-ppa . The current SotA is more than 10 points above the performance achieved by SMP .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your recognition and kind words ! Here are our responses to your comments . Q : If I had to nitpick , I 'd say that part of Table 7 ( currently in the Appendix ) belongs to the main paper , as I was convinced about the superior runtime performance of SMP only after reading those numbers . If the avg running time for an SMP epoch was significantly larger than the one for GAT ( for instance ) , I would have considered SMP yet another specialized model . Instead , given that the GPU consumption ( both in terms of computation and memory ) is similar to any other GNN model , I believe SMP could be adopted as a more flexible graph ML method ( which would avoid having to choose a method given the target task , e.g. , node classification vs. link prediction ) . A : Thanks for the suggestion . We have moved the table to the main paper in the revised version . We also appreciate the reviewer \u2019 s comments that the high-efficiency makes our proposed method a flexible approach to handle graph-based machine learning tasks . Q : One remark about L278 : SMP can not be considered anymore SotA for ogbl-ppa . The current SotA is more than 10 points above the performance achieved by SMP . A : Thanks for the notice that SEAL recently achieves a new SOTA on PPA ( which happens after our submission ) . We have clarified our expression in the revised version . Besides , since SEAL is a GNN variant specifically designed for link prediction , we are interested to see whether our proposed method can further improve SEAL ."}], "0": {"review_id": "fhcMwjavKEZ-0", "review_text": "While the paper is easy to follow , I found all the results in this paper trivial and already-known . Pros : 1.The paper is well-written and easy to follow . Cons : 1.The entire Section 3 is not novel . It is a mixture of the preliminaries of GNNs and the trivial results of Corollary 1 and Theorem 1 . Moreover , Theorem 1 is not rigorous . What if walk-based proximity is just a constant function ? What if a given graph does not contain any automorphism ? 2. \u201c Preserve walk-based similarity \u201d is not rigorously defined . It seems that it just means all nodes have different embeddings . 3.The proposed model is not permutation-equivariant after adding Gaussian noise . It is trivial that the model becomes permutation-equivariant when the Gaussian noise is ignored ( because the model just reduces to an ordinary GNN ) . 4.The claim that SMP preserves walk-based proximity is trivial and just relies on the fact that randomly-sampled vectors from a Gaussian distribution are different from each other . 5.The idea of using node identifiers ( essentially equivalent to the Gaussian noise ) to make GNNs position-aware is not new . In fact , this idea is clearly mentioned in the P-GNN paper already ( Section 6.2 of [ 1 ] \u201c for inductive tasks , augmenting node attributes with one-hot identifiers restricts a model \u2019 s generalization ability \u201d ) . 6.In Section 5.3 , it is unclear why the OGB node classification datasets are not used . [ 1 ] https : //arxiv.org/abs/1906.04817", "rating": "3: Clear rejection", "reply_text": "Thank you for your detailed comments . Here are our responses to your questions . Q1-1.The entire Section 3 is not novel . It is a mixture of the preliminaries of GNNs and the trivial results of Corollary 1 and Theorem 1 . A1-1.Section 3 provides the necessary preliminaries of GNNs and several key definitions , which is important to keep our paper self-contained . Besides , we give novel proof that the existing permutation-equivariant GNNs can not preserve node proximities , which lays the foundations for further developing our proposed method . Q1-2.Moreover , Theorem 1 is not rigorous . What if walk-based proximity is just a constant function ? A1-2.Thanks for the suggestion . We have clarified that we only consider non-trivial walk-based proximity in the revised version . Please refer to Theorem 1 of the revised manuscript for details . Q1-3.What if a given graph does not contain any automorphism ? A1-3 : As we show in Theorem 1 , ( non-trivial ) automorphism presents an important limitation to the existing GNNs , i.e. , preventing GNNs from being proximity-aware . Considering the importance of this limitation , we think it is critical to study this problem even not all graphs contain automorphism . Q2. \u201c Preserve walk-based similarity \u201d is not rigorously defined . It seems that it just means all nodes have different embeddings . A2.We kindly do not agree that preserving walk-based proximity is equivalent to having different node embeddings . The definition was formally provided in Definition 4 in Appendix A.1 due to the page limit and we have moved it into the main paper ( Section 3 ) in the revised version . In a nutshell , a GNN is said to be able to preserve walk-based similarities if we can recover the similarity between two nodes from the corresponding two node embeddings . Thus , the embedding vectors need to encode sufficient information of the walk-based proximity rather than simply being unique node identifiers . Please refer to the revised manuscript for details . Q3.The proposed model is not permutation-equivariant after adding Gaussian noise . It is trivial that the model becomes permutation-equivariant when the Gaussian noise is ignored ( because the model just reduces to an ordinary GNN ) . A3.We kindly do not agree that these theorems should be considered trivial . Our Remark 1 and Corollary 2 indeed show that we can easily recover a permutation-equivariant GNN by ignoring the stochastics representations . Though not using sophisticated proof strategies , these theorems are indispensable to show that our proposed method is able to handle both permutation-equivariant and proximity-aware tasks . On the contrary , though P-GNN can preserve node proximities to a certain extent , it can not reduce to a permutation-equivariant GNN and thus fail to handle tasks where permutation-equivariance is helpful . We also empirically validate the importance of this result in experiments , i.e. , in the node classification task in Section 5.3 , our proposed SMP achieves comparable results as GCNs but P-GNN performs poorly . As a result , we believe these theorems are important for our proposed method . Q4.The claim that SMP preserves walk-based proximity is trivial and just relies on the fact that randomly-sampled vectors from a Gaussian distribution are different from each other . A4.We kindly do not agree that the theorem is trivial . If only i.i.d.Gaussian random vectors are adopted as the reviewer suggests , it is impossible to preserve walk-based proximities since these random vectors are independent of the proximity . In fact , we need to properly perform message-passing on those random vectors and carefully decode the node representations to preserve the proximity . We have given and proved the exact procedure for both the linear case ( see Theorem 2 ) and the non-linear case ( see Theorem 3 ) . Q5.The idea of using node identifiers ( essentially equivalent to the Gaussian noise ) to make GNNs position-aware is not new . In fact , this idea is clearly mentioned in the P-GNN paper already ( Section 6.2 of [ 1 ] \u201c for inductive tasks , augmenting node attributes with one-hot identifiers restricts a model \u2019 s generalization ability \u201d ) . [ 1 ] https : //arxiv.org/abs/1906.04817 A5 . We humbly do not agree that using Gaussian random vectors is equivalent to using one-hot identifiers . We have compared our proposed method using random vectors with using one-hot IDs as node identifiers in Table 8 in the appendix . The results show that our proposed method generally achieves better results . Besides , using one-hot IDs will drastically increase the number of parameters since the input features of GNNs have a dimensionality of O ( n ) . Besides , we \u2019 d like to point out that we have compared with P-GNN as a baseline in all the experiments ."}, "1": {"review_id": "fhcMwjavKEZ-1", "review_text": "This paper proposed a proximity-aware graph neural network while maintaining the permutation equivariance property . The proposed model , dubbed as stochastic message passing ( SMP ) , arguments the existing GNNs with stochastic node representations . The author proved the proposed method can model proximity-aware representations based on random projection theory . The experimental results show that the SMP can be used for multiple graphs and tasks . Although the proposed technique is relatively simple , the theorem shows that why such stochastic representations are beneficial for proximity-aware tasks . The experimental results also suggest that this simple technique is quite effective in many tasks . There are a few things that I 'd like to comment on or ask listed below : - Multiple tasks have been conducted to show the performance of the proposed model , but it is unclear which dataset or which task requires to be proximity-aware . It would be great if there 's some quantitative metric that shows the importance of proximity in a given task . For example , the number of automorphic node pairs ( within k-hop ) and the ratio of label ( dis ) agreement would be a possible metric in node classification tasks . This will characterize the differences between datasets and highlight the contribution of the proposed method . Additionally , showing some examples of automorphic node pairs and the performance on these nodes could demonstrate the difference between models . - Resampling random matrix at each epoch is emphasized multiple times in the manuscript , but without any empirical experiments . Would it be beneficial to resample this random matrix at every epoch ? Although in theory , it would be possible to learn GNN that preserves node proximity , if a given task does n't need to model proximity-aware representations , random resampling may hinder the convergence of the proposed method . - There was a bug in the official release of the P-GNN paper , which has been recently fixed ( please check the GitHub pull request history at https : //github.com/JiaxuanYou/P-GNN/pull/12 ) . I wonder which codebase the authors used for the experiments . / It would be also good if there 's some comment on what makes P-GNN memory hunger . Is it because of improper implementation or because of some inherent limitations ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your recognition and comments . Here are our responses to your questions . Q1 : Multiple tasks have been conducted to show the performance of the proposed model , but it is unclear which dataset or which task requires to be proximity-aware . It would be great if there 's some quantitative metric that shows the importance of proximity in a given task . For example , the number of automorphic node pairs ( within k-hop ) and the ratio of label ( dis ) agreement would be a possible metric in node classification tasks . This will characterize the differences between datasets and highlight the contribution of the proposed method . Additionally , showing some examples of automorphic node pairs and the performance on these nodes could demonstrate the difference between models . A1 : Thank you for this insightful suggestion . We agree that a metric to quantify to what degree a task is proximity-aware or permutation-equivariant will be helpful for further studying GNNs . However , we find that such a metric is difficult to design due to two reasons . Firstly , only comparing whether two nodes are strictly automorphic , as you have suggested , is important but may not be sufficient since if the local structures of two nodes are very similar ( but not strictly automorphic ) , the results of GNNs will also be affected . Thus , we need to quantify the similarities between local structures of different nodes ( rather than only comparing whether they are exactly automorphic ) . However , this is an interesting research question itself , e.g. , commonly studied in graph kernels . Secondly , besides this metric related to permutation-equivariance , we need another metric to quantify whether the task can benefit from proximity-awareness , which is also non-trivial to develop . All things considered , we leave studying this interesting topic as important future works . Q2 : Resampling random matrix at each epoch is emphasized multiple times in the manuscript , but without any empirical experiments . Would it be beneficial to resample this random matrix at every epoch ? Although in theory , it would be possible to learn GNN that preserves node proximity , if a given task does n't need to model proximity-aware representations , random resampling may hinder the convergence of the proposed method . A2 : Thank you for this comment . As we explain in Section 4.1 , we find that fixing the stochastic matrix can help the model to memorize the stochastic representation and distinguish different nodes , while resampling the matrix in each epoch will make our method more capable of handling nodes not seen during training . As a result , we fix the stochastic matrix on transductive datasets and resample the stochastic matrix on inductive datasets . Following your comments , we have added an ablation study in Table 12 in Appendix B.5 in the revised manuscript . The results verify that our design leads to better results in most cases , i.e. , fixing the stochastic matrix on transductive datasets and resampling the stochastic matrix on inductive datasets . Please refer to the revised manuscript for details . Q3 : There was a bug in the official release of the P-GNN paper , which has been recently fixed ( please check the GitHub pull request history at https : //github.com/JiaxuanYou/P-GNN/pull/12 ) . I wonder which codebase the authors used for the experiments . / It would be also good if there 's some comment on what makes P-GNN memory hunger . Is it because of improper implementation or because of some inherent limitations ? A3 : Thank you for this notice . We indeed adopt the implementation by the authors . Since this bug is fixed after our submission , the results in the paper are based on the implementation before the fix . We have tried the fixed codes and find the results comparable or even slightly worse than the results reported in the paper . As for the memory issue , we think it is caused by the mechanism of P-GNN , i.e. , explicit construction of the anchor nodes and performing message-passing on them ."}, "2": {"review_id": "fhcMwjavKEZ-2", "review_text": "The authors propose to add random node features to the input of message passing graph neural networks for them to become proximity-aware . The paper provides exhaustive theoretical and empirical analysis of this idea , highlighting the advantageous properties . Strengths : - The paper is well written . - The authors substantiate their claims with proofs and experiments . - The method and proofs seem to be mathematically sound . - Including the experiments in the appendix , the empirical analysis is very exhaustive and seems to be reproducible . - The method is simple but effective , which is good . It also does not lead to a significant computational overhead but fits nicely into the existing message passing framework with linear time complexity ( in number of edges ) . - The paper provides a formal framework and analysis for a trick that has already been used successfully in practice . Comments and Questions : - The work of Sato et al . [ 1 ] seems to be closer to this work than the authors let know . I would welcome a more in-depth discussion than given in related work , even if the paper is only on arxiv . - Definition 4 should go to the main text , as it is crucial for understanding . - The proof of Theorem 1 shows the result only for graphs with 2 connected components . I think the theorem also holds for connected graphs ( with the right automorphisms ) , which is important . If it would n't , the result would not always be relevant for practice . I think the current way of proving the theorem lacks that insight and discussion . Is n't it possible to prove the theorem for all graphs with a certain automorphism , regardless of number of connected components ? - Can the authors verify the suspicion that the non-linear variants overfit ( line 270 ) by presenting the results on training data ? - It might be of interest to the authors that the idea was already used in a practical graph matching method by [ 2 ] ( page 5 , last paragraph ) , although without any theoretical analysis . It is nice to have a formal framework that justifies the application of this trick . - In general , GNNs to solve matching tasks are a very fitting application for the proposed method , which the authors do not consider . They often rely on comparing distance measures in both domains , thus need proximity awareness . Typos : - Line 87 : `` computationally expansive '' - > expensive - Line 607 : `` computationally expansive '' - > expensive All in all , there is not much to complain about . The paper achieves what it sets out to do in providing an exhaustive theoretical and empirical analysis of a simple but effective idea . The method itself is straight-forward and also not entirely novel . However , the formal framework and analysis is a contribution that is of interest to the community . Due to the shown properties and the high efficiency of the approach , the paper can have significant impact on future GNN architectures in practice . I therefore recommend to accept the paper . [ 1 ] Sato et al. , Random features strengthen graph neural networks . arXiv:2002.03155 [ 2 ] Fey et al. , Deep Graph Matching Consensus , ICLR 2020", "rating": "7: Good paper, accept", "reply_text": "Thanks for your kind words and detailed comments . Here are our responses to your comments . Q1.The work of Sato et al . [ 1 ] seems to be closer to this work than the authors let know . I would welcome a more in-depth discussion than given in related work , even if the paper is only on arxiv . [ 1 ] Sato et al. , Random features strengthen graph neural networks . arXiv:2002.03155 A1 . Thanks for your suggestion . We have revised the related works as follows : \u201d For example , Sato et al . ( Sato et al. , 2020 ) novelly show that random numbers can enhance GNNs in tackling two important graph-based NP problems with a theoretical guarantee , namely the minimum dominating set and the maximum matching problem\u2026Our work differs in that we systematically study how to preserve permutation-equivariance and proximity-awareness simultaneously in a simple yet effective framework , which is a new topic different from these existing works . Besides , we theoretically prove that our proposed method can preserve walk-based proximities by using the random projection literature . We also demonstrate the effectiveness of our method on various large-scale benchmarks for both node- and edge-level tasks , while no similar results are reported in the literature. \u201d Q2 . Definition 4 should go to the main text , as it is crucial for understanding . A2 : Thanks for the suggestion . We have moved the definition to the main paper in the revised version . Q3.The proof of Theorem 1 shows the result only for graphs with 2 connected components . I think the theorem also holds for connected graphs ( with the right automorphisms ) , which is important . If it would n't , the result would not always be relevant for practice . I think the current way of proving the theorem lacks that insight and discussion . Is n't it possible to prove the theorem for all graphs with a certain automorphism , regardless of number of connected components ? A3.Thanks for this insightful and constructive comment . We indeed find that we can prove the theorem using one connected component under a mild assumption ( i.e. , the walk-based proximity is of finite length ) . We have provided this additional proof in Appendix A.1 in the revised version . The essential idea is to construct automorphism using three copies of the graph and three bridges to connect these copies . Please refer to the revised manuscript for details . Q4.Can the authors verify the suspicion that the non-linear variants overfit ( line 270 ) by presenting the results on training data ? A4 : Thank you for this insightful comment . We have provided your suggested experiments in Appendix B.6 in the revised version . The results show that non-linear variants of GNNs such as GAT indeed exhibit more serious overfitting , i.e. , the margins between the training accuracies and the testing accuracies are usually larger than the linear variant SGC . Besides , non-linear variants are also difficult to train , i.e. , the training accuracies are sometimes also worse than the linear variant SGC . We have clarified our expressions as follows : \u201c Some plausible reasons include that the additional model complexity brought by non-linear operators makes the models tend to overfit and also difficult to train. \u201d Q5 . It might be of interest to the authors that the idea was already used in a practical graph matching method by [ 2 ] ( page 5 , last paragraph ) , although without any theoretical analysis . It is nice to have a formal framework that justifies the application of this trick . [ 2 ] Fey et al. , Deep Graph Matching Consensus , ICLR 2020 A5 . Thanks for your notice on this interesting relevant literature . We have added it to the related works in the revised version . Q6.In general , GNNs to solve matching tasks are a very fitting application for the proposed method , which the authors do not consider . They often rely on comparing distance measures in both domains , thus need proximity awareness . A6.Thanks for your constructive suggestion . We will consider trying our proposed method in graph matching tasks . Q7 : Typos : Line 87 : `` computationally expansive '' - > expensive Line 607 : `` computationally expansive '' - > expensive A7 : Thanks for pointing them out . We have fixed them in the revised version . Q8.All in all , there is not much to complain about . The paper achieves what it sets out to do in providing an exhaustive theoretical and empirical analysis of a simple but effective idea . The method itself is straight-forward and also not entirely novel . However , the formal framework and analysis is a contribution that is of interest to the community . Due to the shown properties and the high efficiency of the approach , the paper can have significant impact on future GNN architectures in practice . I therefore recommend to accept the paper . A8 : Thank you for your recognition that our paper can significantly impact future GNNs ! We intend to further explore this direction in the near future ."}, "3": {"review_id": "fhcMwjavKEZ-3", "review_text": "Since the publication of P-GNN ( You et al. , 2019 ) , it has become clear to the graph ML community that node positional information can be effectively leveraged for link prediction and pairwise node classification tasks . This paper introduces SMP , a novel stochastic message passing approach that preserves both permutation-equivariance ( common to GNN models ) and node proximities . Extensive experimental results show that SMP not only achieves competitive performance on many common graph ML datasets , but also succeeds to combine together the expressiveness of a standard GNN with P-GNN ( without incurring the scalability problem of P-GNN ) . I thoroughly enjoyed reading this paper , both for the insights and the technical soundness . I have very few remarks about the paper , as I believe that i ) SMP is an ingenious idea , ii ) the experimental setting is adequate , iii ) the quality of the writeup is high , iv ) and the results appear to be reproducible . If I had to nitpick , I 'd say that part of Table 7 ( currently in the Appendix ) belongs to the main paper , as I was convinced about the superior runtime performance of SMP only after reading those numbers . If the avg running time for an SMP epoch was significantly larger than the one for GAT ( for instance ) , I would have considered SMP yet another specialized model . Instead , given that the GPU consumption ( both in terms of computation and memory ) is similar to any other GNN model , I believe SMP could be adopted as a more flexible graph ML method ( which would avoid having to choose a method given the target task , e.g. , node classification vs. link prediction ) . One remark about L278 : SMP can not be considered anymore SotA for ogbl-ppa . The current SotA is more than 10 points above the performance achieved by SMP .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your recognition and kind words ! Here are our responses to your comments . Q : If I had to nitpick , I 'd say that part of Table 7 ( currently in the Appendix ) belongs to the main paper , as I was convinced about the superior runtime performance of SMP only after reading those numbers . If the avg running time for an SMP epoch was significantly larger than the one for GAT ( for instance ) , I would have considered SMP yet another specialized model . Instead , given that the GPU consumption ( both in terms of computation and memory ) is similar to any other GNN model , I believe SMP could be adopted as a more flexible graph ML method ( which would avoid having to choose a method given the target task , e.g. , node classification vs. link prediction ) . A : Thanks for the suggestion . We have moved the table to the main paper in the revised version . We also appreciate the reviewer \u2019 s comments that the high-efficiency makes our proposed method a flexible approach to handle graph-based machine learning tasks . Q : One remark about L278 : SMP can not be considered anymore SotA for ogbl-ppa . The current SotA is more than 10 points above the performance achieved by SMP . A : Thanks for the notice that SEAL recently achieves a new SOTA on PPA ( which happens after our submission ) . We have clarified our expression in the revised version . Besides , since SEAL is a GNN variant specifically designed for link prediction , we are interested to see whether our proposed method can further improve SEAL ."}}