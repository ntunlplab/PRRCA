{"year": "2017", "forum": "B1vRTeqxg", "title": "Learning Continuous Semantic Representations of Symbolic Expressions", "decision": "Invite to Workshop Track", "meta_review": "As part of this meta-review, I read the paper and found some surprising claims, such as the somewhat poorly motivated claim that coercing the output of a sub-network be a unit vector my dividing it by its L2 norm is close to layer normalisation which is mathematically almost true, if the mean of of the activations is 0, and we accept a fixed offset in the calculation of the stddev, but conceptually a different form of normalisation. It is also curious that other methods of obtaining stable training in recursive networks, such as TreeLSTM (Zhu et al. 2015, Tai et al. 2015), were not compared to. None of these problems is particularly damning but it is slightly disappointing not to see these issues discussed in the review process.\n\nOverall, the reviews, which I found superficial in comparison to the other papers I am chairing, found the method proposed here sound, although some details lacked explanation. The consensus was that the general problem being addressed is interesting and timely, given the attention the topics of program induction and interpretation have been receiving in the community recently. There was also consensus that the setting the model was evaluated on was far too simple and unnatural, and that there is need for a more complex, task involving symbolic interpretation to validate the model. It is hard to tell, given all the design decisions made (l2-normalisation vs layer norm, softmax not working), whether the end product is tailored to the task at hand, and whether it will tell us something useful about how this approach generalises. I am inclined, on the basis of the reviewer's opinions of the setting and my own concerns outlined above, to recommend redirection to the workshop track.", "reviews": [{"review_id": "B1vRTeqxg-0", "review_text": "The authors propose a new model to learn symbolic expression representations. They do a reasonably extensive evaluation with similar approaches and motivate their approach well. As expressed in the preliminary questions, I think the authors could improve the motivation for their subexpforce loss. At the top of page 6 the authors mention that they compare to two-layer MLP w/o residual connections. I think having a direct comparison between a model with and w/o the subexpforce loss would be helpful too and should be included (i.e. keep the residual connections and normalization). My main concern is the evaluation \"score\". It appears to be precision on a per query basis. I believe a more standard metric, precision-recall or roc would be more informative. In particular the chosen metric is expected to perform better when the equivalence classes are larger, since this isn't taken into account in the denominator, but the likelihood of a random expression matching the query increases.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . We will improve the motivation of subexpforce . We do compare between the model with and without the subexpforce loss in the text ( page 7 ) : `` When training the network with and without subexpression forcing , on average , the area under the curve ( AUC ) of the score k decreases by 16.8 % on the SeenEqClass and 19.7 % on the UnseenEqClass . This difference is smaller in the transfer setting of Figure 2b-i and Figure 2b-ii , where AUC decreases by 8.8 % on average . '' Let us know if you 'd be interested in seeing a lengthier comparison . Regarding our metric , `` score_k '' can be interpreted as recall at rank k. Since at test-time our problem has more of an unsupervised flavor , because unseen equivalence classes will be observed , it is hard to compute a ROC or a precision/recall metric . Can you clarify how would you suggest computing the precision/recall-ROC in this setting ? We understand your concern about imbalance of the equivalence classes . If we were to report both the macro-averaged score_k ( first average per equivalence class and then average all equivalence classes ) and the micro-averaged score_k ( average the score_k of all points assigning equal weights to each expression ) , would that be sufficient to resolve your concern ?"}, {"review_id": "B1vRTeqxg-1", "review_text": "The goal of this paper is to learn vector representation of boolean and polynomial expressions, such that equivalent expressions have similar representations. The model proposed in the paper is based on recursive neural network, as introduced by Socher et al. (2012). Given the syntactic parse tree of a formula (either boolean or polynomial), the representation for a node is obtained by applying a MLP on the representation of the children. This process is applied recursively to obtain the representation of the full expression. Contrary to Socher et al. (2012), this paper proposes to use more than one layer (this is especially important to capture XOR operation, which is not surprising at all). The paper also introduces a reconstruction error (called SubexpForce), so that the expression of children can be recovered from the expression of the parent (if I understood correctly). The model is trained using a classification loss, where the label of a given expression corresponds to its equivalence class. The method is then evaluated on randomly generated data, and compared to baselines such as tf-idf, GRU RNN or standard recursive neural network. While I do agree with the authors that learning good representation for symbolic expressions (and to capture compositionality) is an important task, I am not entirely convinced by the experimental setting proposed in this paper. Indeed, as stated in the paper, the task of deciding if two boolean expressions are equivalent is NP-hard, and I do not understand if the model can do better than implicitly computing the truth table of expressions. While sometimes a bit hard to follow, the paper is technically sound. In particular, the proposed model is well adapted to the problem and outperforms the baselines. pros: - the model is relatively simple and sound. - using a classification loss over equivalence classes (should be compared with using similarity). cons: - not convinced by the setting: I do not believe that you can really do better than the truth table for boolean expr (or computing the value of the polynomial expression for randomly chosen points in [0, 1]^n). - some part of the paper are a bit hard to follow (e.g. justification of the SubexpForce, discussion of why softmax does not work, etc...). - comparison between classification loss and similarity loss is missing.", "rating": "5: Marginally below acceptance threshold", "reply_text": "> > `` I do not believe that you can really do better than the truth table for boolean expr '' Humans deciding on the equivalence of two expressions do not always need to build a truth table . Some times it is very easy to apply a number of standard transformations ( for instance , De Morgan 's laws in the case of boolean expressions ) to convert one expressions to another . For instance , two very long expressions might become equivalent by just one application of a rule - in this case no truth table computation would be required . The neural networks might be learning to compute such transformations implicitly . We agree that this is a simple setting . We chose it because we felt that any representation learning method for that attempts to approximately learn expression semantics should be able to handle it . As the reviewer notes , the method could learn to compute the truth table of symbolic expressions . Therefore we found it very striking that no standard architecture could learn to do this ! The idea behind this paper is that finding a representation learning method that * can * handle this simple setting is a necessary first step to a continuous representation learning method for program semantics ."}, {"review_id": "B1vRTeqxg-2", "review_text": " This work proposes to compute embeddings of symbolic expressions (e.g., boolean expressions, or polynomials) such that semantically equivalent expressions are near each other in the embedded space. The proposed model uses recursive neural networks where the architecture is made to match that of the parse tree of a given symbolic expression. To train the model parameters, the authors create a dataset of expressions where semantic equivalence relationships are known and minimize a loss function so that equivalent expressions are closer to each other than non-equivalent expressions via a max-margin loss function. The authors also use a \u201csubexpression forcing\u201d mechanism which, if I understand it correctly, encourages the embeddings to respect some kind of compositionality. Results are shown on a few symbolic expression datasets created by the authors and the proposed method is demonstrated to outperform baselines pretty convincingly. I especially like the PCA visualization where the action of negating an expression is shown to correspond roughly to negating the embedding in its vector space \u2014 it is a lot like the man - woman + queen = king type embeddings that we see in the word2vec and glove style papers. The weakest part of the paper is probably that the setting seems somewhat contrived \u2014 I can\u2019t really think of a real setting where it is easy to have a training set of known semantic equivalences, but still more worth it to use a neural network to do predictions. The authors have also punted on dealing with variable names, assuming that distinct variables refer to different entities in the domain. This is understandable, as variable names add a whole new layer of complexity on an already difficult problem, but also seems high limiting. For example, the proposed methods would not be useable in an \u201cequation search engine\u201d unless we were able to accurately canonicalize variable names in some way. Other miscellaneous points: * Regarding problem hardness, I believe that the problem of determining if two expressions are equivalent is actually undecidable \u2014 see the \u201cword problem for Thue systems\u201d. Related to this, I was not able to figure out how the authors determine ground truth equivalence in their training sets. They say that expressions are simplified into a canonical form and grouped, but this seems to not be possible in general, so one question is \u2014 is it possible that equivalent expressions in the training data would have been mapped to different canonical forms? Would it have been easier/possible to construct and compare truth tables? * The \u201cCOMBINE\u201d operation uses what the authors describe as a residual-like connection. Looking at the equations, the reason why this is not actually a residual connection is because of the weight matrix that is multiplied by the lower level l_0 features. A true residual connection would have passed the features through unchanged (identity connection) and would have also been better at fighting gradient explosion\u2026. so is there a reason why this was used rather than an identity connection? * In table 3, the first tf-idf entry: a + (c+a) * c seems equivalent to a + (c * (a+c)) * Vertical spacing between Figure 4 caption and body of text is very small and looks like the caption continues into the body of the text. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for pointing the typo in Table 3 and the issue with Fig4 . We 'll address them . We understand that the setting where the equivalence classes of two expressions are already known is limited . The reason that we focused on such a setting is that we found that it was already quite hard , and seemed to be a necessary first step to more realistic settings such as expression similarity in real compute programs . Although in this work we are learning semantic vector representations using equivalence , there are multiple ways of retrieving ( training ) expressions that are approximately equivalent ( e.g.identical outputs on a set of identical inputs ) . However , even in those cases , current neural network architectures would still not be able to capture semantics . Hence , in this work we make the first step and search for architectures that capture semantics in a setting where equivalence is known . And although we show that EqNets perform the best , it suggests that we need future research to first solve this problem , before moving to noisy data . For arbitrary expressions ( e.g.code ) the problem of determining equivalence is indeed undecidable , but for boolean and polynomial expressions it is not . To detect equivalence of boolean expressions we convert them to the canonical conjunctive normal form which is unique . We get the same behavior for polynomials where we can symbolically expand them and simplify them into the unique form v_0 * a + v_1 * b + v_2 * c + v_3 * a^2 + ... Our network is only `` residual-like '' ( and does _not_ use the standard residual architecture ) because the size of the input layer is different from the size of the output layer ( Fig.1b ) .For example , for a node with two children , the input would have size 2 * D , while the output is of size D. Therefore , using the identity is not exactly possible . There are many possible design choices to solve this , so our choice is just one sensible option . Finally , although the multiplication with W_ { o0 , \\tau_n } contributes to diminishing gradients , we retain the important element of residual networks by _not_ using a non-linearity ."}], "0": {"review_id": "B1vRTeqxg-0", "review_text": "The authors propose a new model to learn symbolic expression representations. They do a reasonably extensive evaluation with similar approaches and motivate their approach well. As expressed in the preliminary questions, I think the authors could improve the motivation for their subexpforce loss. At the top of page 6 the authors mention that they compare to two-layer MLP w/o residual connections. I think having a direct comparison between a model with and w/o the subexpforce loss would be helpful too and should be included (i.e. keep the residual connections and normalization). My main concern is the evaluation \"score\". It appears to be precision on a per query basis. I believe a more standard metric, precision-recall or roc would be more informative. In particular the chosen metric is expected to perform better when the equivalence classes are larger, since this isn't taken into account in the denominator, but the likelihood of a random expression matching the query increases.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . We will improve the motivation of subexpforce . We do compare between the model with and without the subexpforce loss in the text ( page 7 ) : `` When training the network with and without subexpression forcing , on average , the area under the curve ( AUC ) of the score k decreases by 16.8 % on the SeenEqClass and 19.7 % on the UnseenEqClass . This difference is smaller in the transfer setting of Figure 2b-i and Figure 2b-ii , where AUC decreases by 8.8 % on average . '' Let us know if you 'd be interested in seeing a lengthier comparison . Regarding our metric , `` score_k '' can be interpreted as recall at rank k. Since at test-time our problem has more of an unsupervised flavor , because unseen equivalence classes will be observed , it is hard to compute a ROC or a precision/recall metric . Can you clarify how would you suggest computing the precision/recall-ROC in this setting ? We understand your concern about imbalance of the equivalence classes . If we were to report both the macro-averaged score_k ( first average per equivalence class and then average all equivalence classes ) and the micro-averaged score_k ( average the score_k of all points assigning equal weights to each expression ) , would that be sufficient to resolve your concern ?"}, "1": {"review_id": "B1vRTeqxg-1", "review_text": "The goal of this paper is to learn vector representation of boolean and polynomial expressions, such that equivalent expressions have similar representations. The model proposed in the paper is based on recursive neural network, as introduced by Socher et al. (2012). Given the syntactic parse tree of a formula (either boolean or polynomial), the representation for a node is obtained by applying a MLP on the representation of the children. This process is applied recursively to obtain the representation of the full expression. Contrary to Socher et al. (2012), this paper proposes to use more than one layer (this is especially important to capture XOR operation, which is not surprising at all). The paper also introduces a reconstruction error (called SubexpForce), so that the expression of children can be recovered from the expression of the parent (if I understood correctly). The model is trained using a classification loss, where the label of a given expression corresponds to its equivalence class. The method is then evaluated on randomly generated data, and compared to baselines such as tf-idf, GRU RNN or standard recursive neural network. While I do agree with the authors that learning good representation for symbolic expressions (and to capture compositionality) is an important task, I am not entirely convinced by the experimental setting proposed in this paper. Indeed, as stated in the paper, the task of deciding if two boolean expressions are equivalent is NP-hard, and I do not understand if the model can do better than implicitly computing the truth table of expressions. While sometimes a bit hard to follow, the paper is technically sound. In particular, the proposed model is well adapted to the problem and outperforms the baselines. pros: - the model is relatively simple and sound. - using a classification loss over equivalence classes (should be compared with using similarity). cons: - not convinced by the setting: I do not believe that you can really do better than the truth table for boolean expr (or computing the value of the polynomial expression for randomly chosen points in [0, 1]^n). - some part of the paper are a bit hard to follow (e.g. justification of the SubexpForce, discussion of why softmax does not work, etc...). - comparison between classification loss and similarity loss is missing.", "rating": "5: Marginally below acceptance threshold", "reply_text": "> > `` I do not believe that you can really do better than the truth table for boolean expr '' Humans deciding on the equivalence of two expressions do not always need to build a truth table . Some times it is very easy to apply a number of standard transformations ( for instance , De Morgan 's laws in the case of boolean expressions ) to convert one expressions to another . For instance , two very long expressions might become equivalent by just one application of a rule - in this case no truth table computation would be required . The neural networks might be learning to compute such transformations implicitly . We agree that this is a simple setting . We chose it because we felt that any representation learning method for that attempts to approximately learn expression semantics should be able to handle it . As the reviewer notes , the method could learn to compute the truth table of symbolic expressions . Therefore we found it very striking that no standard architecture could learn to do this ! The idea behind this paper is that finding a representation learning method that * can * handle this simple setting is a necessary first step to a continuous representation learning method for program semantics ."}, "2": {"review_id": "B1vRTeqxg-2", "review_text": " This work proposes to compute embeddings of symbolic expressions (e.g., boolean expressions, or polynomials) such that semantically equivalent expressions are near each other in the embedded space. The proposed model uses recursive neural networks where the architecture is made to match that of the parse tree of a given symbolic expression. To train the model parameters, the authors create a dataset of expressions where semantic equivalence relationships are known and minimize a loss function so that equivalent expressions are closer to each other than non-equivalent expressions via a max-margin loss function. The authors also use a \u201csubexpression forcing\u201d mechanism which, if I understand it correctly, encourages the embeddings to respect some kind of compositionality. Results are shown on a few symbolic expression datasets created by the authors and the proposed method is demonstrated to outperform baselines pretty convincingly. I especially like the PCA visualization where the action of negating an expression is shown to correspond roughly to negating the embedding in its vector space \u2014 it is a lot like the man - woman + queen = king type embeddings that we see in the word2vec and glove style papers. The weakest part of the paper is probably that the setting seems somewhat contrived \u2014 I can\u2019t really think of a real setting where it is easy to have a training set of known semantic equivalences, but still more worth it to use a neural network to do predictions. The authors have also punted on dealing with variable names, assuming that distinct variables refer to different entities in the domain. This is understandable, as variable names add a whole new layer of complexity on an already difficult problem, but also seems high limiting. For example, the proposed methods would not be useable in an \u201cequation search engine\u201d unless we were able to accurately canonicalize variable names in some way. Other miscellaneous points: * Regarding problem hardness, I believe that the problem of determining if two expressions are equivalent is actually undecidable \u2014 see the \u201cword problem for Thue systems\u201d. Related to this, I was not able to figure out how the authors determine ground truth equivalence in their training sets. They say that expressions are simplified into a canonical form and grouped, but this seems to not be possible in general, so one question is \u2014 is it possible that equivalent expressions in the training data would have been mapped to different canonical forms? Would it have been easier/possible to construct and compare truth tables? * The \u201cCOMBINE\u201d operation uses what the authors describe as a residual-like connection. Looking at the equations, the reason why this is not actually a residual connection is because of the weight matrix that is multiplied by the lower level l_0 features. A true residual connection would have passed the features through unchanged (identity connection) and would have also been better at fighting gradient explosion\u2026. so is there a reason why this was used rather than an identity connection? * In table 3, the first tf-idf entry: a + (c+a) * c seems equivalent to a + (c * (a+c)) * Vertical spacing between Figure 4 caption and body of text is very small and looks like the caption continues into the body of the text. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for pointing the typo in Table 3 and the issue with Fig4 . We 'll address them . We understand that the setting where the equivalence classes of two expressions are already known is limited . The reason that we focused on such a setting is that we found that it was already quite hard , and seemed to be a necessary first step to more realistic settings such as expression similarity in real compute programs . Although in this work we are learning semantic vector representations using equivalence , there are multiple ways of retrieving ( training ) expressions that are approximately equivalent ( e.g.identical outputs on a set of identical inputs ) . However , even in those cases , current neural network architectures would still not be able to capture semantics . Hence , in this work we make the first step and search for architectures that capture semantics in a setting where equivalence is known . And although we show that EqNets perform the best , it suggests that we need future research to first solve this problem , before moving to noisy data . For arbitrary expressions ( e.g.code ) the problem of determining equivalence is indeed undecidable , but for boolean and polynomial expressions it is not . To detect equivalence of boolean expressions we convert them to the canonical conjunctive normal form which is unique . We get the same behavior for polynomials where we can symbolically expand them and simplify them into the unique form v_0 * a + v_1 * b + v_2 * c + v_3 * a^2 + ... Our network is only `` residual-like '' ( and does _not_ use the standard residual architecture ) because the size of the input layer is different from the size of the output layer ( Fig.1b ) .For example , for a node with two children , the input would have size 2 * D , while the output is of size D. Therefore , using the identity is not exactly possible . There are many possible design choices to solve this , so our choice is just one sensible option . Finally , although the multiplication with W_ { o0 , \\tau_n } contributes to diminishing gradients , we retain the important element of residual networks by _not_ using a non-linearity ."}}