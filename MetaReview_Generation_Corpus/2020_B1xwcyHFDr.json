{"year": "2020", "forum": "B1xwcyHFDr", "title": "Learning Robust Representations via Multi-View Information Bottleneck", "decision": "Accept (Poster)", "meta_review": "This paper extends the information bottleneck method to the unsupervised representation learning under the multi-view assumption. The work couples the multi-view InfoMax principle with the information bottleneck principle to derive an objective which encourages the representations to contain only the information shared by both views and thus eliminate the effect of independent factors of variations. Recent advances in estimating lower-bounds on mutual information are applied to perform approximate optimisation in practice. The authors empirically validate the proposed approach in two standard multi-view settings.\nOverall, the reviewers found the presentation clear, and the paper well written and well motivated. The issues raised by the reviewers were addressed in the rebuttal and we feel that the work is well suited for ICLR. We ask the authors to carefully integrate the detailed comments from the reviewers into the manuscript. Finally, the work should investigate and briefly establish a connection to [1].\n\n[1] Wang et al. \"Deep Multi-view Information Bottleneck\". International Conference on Data Mining 2019 (https://epubs.siam.org/doi/pdf/10.1137/1.9781611975673.5)", "reviews": [{"review_id": "B1xwcyHFDr-0", "review_text": "In this paper, the authors extend the Information Bottleneck method (to build robust representations by removing information unrelated to the target labels) to the unsupervised setting. Since label information is not available in this setting, the authors leverage multi-view information (e.g., using two images of the same object) , which requires assuming that both views contain all necessary information for the subsequent label prediction task. The representation should then focus on capturing the information shared by both views and discarding the rest. A loss function for learning such representations is proposed. The effectiveness of the proposed technique is confirmed on two datasets. It is also shown to work when doing data augmentation with a single view. Overall the paper is well motivated, well placed in the literature and well written. Mathematical derivations are provided. Experimental methodology follows the existing literature, seem reasonable and results are convincing. I do not have major negative comments for the authors. This is however not my research area and have only a limited knowledge of the existing body of work. Comments/Questions: - How limiting is the multi-view assumption? Are there well-known cases where it doesn't hold? I feel it would be hard to use, say, with text. Has this been discussed in the literature? Some pointers or discussion would be interesting. - Sketchy dataset: Could the DSH algorithm (one of the best prior results) be penalized by not using the same feature extractor you used? - Sketchy dataset: Can a reference for the {Siamese,Triplet}-AlexNet results be provided? - Sketchy dataset: for reproducibility, what is the selected \\beta? - I find it very hard to believe that the accuracy stays constant no matter the number of examples per label used. How can an encoder be trained on 10 images? Did I misunderstand the meaning of this number? Can this be clarified? - Again for reproducibility, listing the raw numbers for the MNIST experiments would be nice. - If I understood the experiments correctly, \"scarce label regime\" is used for both the MIR-Flickr and MNIST datasets, meaning two different things (number of labels per example vs number of examples per label), which is slightly confusing. Typos: Page 1: it's -> its Page 6: the the -> the Page 7: classifer -> classifier Page 8: independently -> independent ", "rating": "8: Accept", "reply_text": "1 ) How limiting is the multi-view assumption ? Are there well-known cases where it does n't hold ? I feel it would be hard to use , say , with text . Has this been discussed in the literature ? Some pointers or discussion would be interesting . We answered this above as part of shared question ( 1 ) 2 ) Sketchy dataset : Could the DSH algorithm ( one of the best prior results ) be penalized by not using the same feature extractor you used ? The DSH algorithm may be limited by their use of an AlexNet instead of a VGG network , but they also fine-tune the AlexNet as part of their model , while our model directly uses the features unmodified . So it 's unclear how these two changes affect the performance on the balance . We were unable to produce an updated version of their results with a VGG network because they do not provide code or hyper-parameter settings for training their models , or details on how they did the pretraining . In order to facilitate future comparison , we will release the preprocessed dataset with the 5 splits used for our experiments upon paper acceptance . 3 ) Sketchy dataset : Can a reference for the { Siamese , Triplet } -AlexNet results be provided ? For reproducibility , what is the selected \\beta ? A reference for { Siamese , Triplet } -AlexNet results has been added to Table 1 together with the selected value of beta for MIB . 4 ) I find it very hard to believe that the accuracy stays constant no matter the number of examples per label used . How can an encoder be trained on 10 images ? Did I misunderstand the meaning of this number ? Can this be clarified ? The MNIST experiments are performed with the following procedure : i ) Using the whole training set without any labels ( i.e.unsupervised ) we train the encoder network which generates a representation for each input picture . We then freeze the weight of the encoder network . ii ) Then the randomly chosen set of training labels are used to train a linear classifier to map from encoded samples to the labels . Note here that in our experiments , the chosen amount of labels does indeed range from one example per label , up to and including all labels . iii ) Each classifier is evaluated on a disjoint test set . This is done by first encoding the unseen test set using the encoder trained in i ) and plugging the representations into the linear classifier trained in ii ) In this setting , the encoder has access to the whole ( unlabeled ) training set , but the classifier is trained using different amounts of examples . The MIB model produces a representation that contains approximately 2.3 nats ( Figure 4 ) which corresponds roughly to the amount of information associated with a categorical distribution on 10 classes with uniform probability ( ln10 ~2.3 nats ) . Further investigating this interesting property , we visualized the representation produced by our model by projecting the representation of the test set on the 2 principal components ( Figure 7 added in Appendix G.4.1 ) . The representation for the test digits roughly consists of 10 linearly separable clusters , which explains why 10 examples are sufficient to align cluster centroids and labels . 5 ) Again for reproducibility , listing the raw numbers for the MNIST experiments would be nice . We added appendix G.4.1 in which we report accuracy for different numbers of examples and mutual information estimation corresponding to the comparison reported in Figure 4 . 6 ) If I understood the experiments correctly , `` scarce label regime '' is used for both the MIR-Flickr and MNIST datasets , meaning two different things ( number of labels per example vs number of examples per label ) , which is slightly confusing . By \u201c scarce label regime \u201d , we mean a reduced number of labeled examples . This translates into slightly different settings for single-label ( MNIST ) and multi-label ( Flickr ) classification problems . In both cases , we simulate the lack of labels by picking a subset of labeled examples with the same distribution p ( x , y ) as the original training set . Since the label distribution on MNIST is uniform , we generate training subsets by picking the same number of examples for each class . The same procedure is not possible for the Flickr experiments as the label distribution is uneven and each example has multiple labels . For this reason , we uniformly subsample the original training set without replacement . The x-axis in Figure 3 and 4 have been updated to consistently report the number of labeled examples used for training a classifier on top of the specified representation . 7 ) Typos We thank the reviewer for identifying the mistakes , which have been fixed in the current version of the paper ."}, {"review_id": "B1xwcyHFDr-1", "review_text": "This paper extends the information bottleneck method of Tishby et al. (2000) to the unsupervised setting. By taking advantage of multi-view data, they provide two views of the same underlying entity. Experimetal results on two standard multi-view datasets validate the efficacy of the proposed method. I have three questions about this work. 1. The proposed method only provides two views of the same underlying entity, what about 3 or more views? 2. Can this method be used for multi-modality case? 3. What about the time efficiency of the proposed method?", "rating": "6: Weak Accept", "reply_text": "1 ) The proposed method only provides two views of the same underlying entity , what about 3 or more views ? We addressed this above as part of shared question ( 2 ) 2 ) Can this method be used for multi-modality case ? We addressed this above as part of shared question ( 1 ) 3 ) What about the time efficiency of the proposed method ? The proposed MIB architecture involves training 2 encoders and one auxiliary architecture as in [ Tian et al . ( 2019 ) , Bachman et al . ( 2019 ) ] .Such models typically train faster than the ones that involve the use of decoders ( e.g.VAE , MVAE , VCCA ) , or involve more complicated architectures consisting of multiple modules ( e.g.GDH , DSH ) . On the other hand , since the beta hyper-parameter has to be slowly increased during training to ensure stability , the total number of training steps required for convergence is slightly bigger than InfoMax and MV-InfoMax . Both training time per epoch and the total number of training steps for convergence mostly depend on the specific choice of mutual information estimation and corresponding auxiliary neural network architecture and is a current subject of exploration in recent work [ Poole et al . ( 2019 ) , Belghazi et al . ( 2018 ) ] ."}, {"review_id": "B1xwcyHFDr-2", "review_text": "This is a good multiview representation learning paper with new insights. The authors propose to learn variables z_1 and z_2, which are consistent, contain view-invariant information but discard as much view-specific information as possible. The paper relies on mutual information estimation and is reconstruction-free. It is mentioned in some previous works (e.g. Aaron van den Oord et al. 2018), that reconstruction loss can introduce bias that has a negative effect on the learned representation. Comparing to existing multiview representation learning approaches that try to maximize the mutual information between learned representation and the view(s), this paper clearly defines superfluous information that we should try to throw away and figure out how to obtain sufficiency learned representation for output. The authors also draw clear connections between a few existing (multiview) representation learning methods to their proposed approaches. The experimental results on the right side of Figure 3, deliver a very interesting conclusion. In low-resource case, robust feature (obtained by using the larger beta, discarding more superfluous information) is crucial for achieving good performance. While when the amount of labeled data samples is enough, vice-versa. Here are my major concerns: 1. In the paper, the authors said the original formulation of IB is only applicable to supervised learning. That is true, but the variational information bottleneck paper [Alexander A. Alem et al. 2017] already showed the connection of unsupervised VIB to VAE in the appendix. 2. I would not consider the data augmentation used to extend single-view data to \u201cpseudo-multiview\u201d as a contribution. This has been done before (e.g. in the multiview MNIST experiment part of the paper \"On Deep Multi-View Representation Learning\"). 3. Which MV-InfoMax do you really compare to? You listed a few of them: (Ji et al., 2019; Henaff et al., \u00b4 2019; Tian et al., 2019; Bachman et al., 2019) in the related work section. 4. I think the authors should also make a more careful claim on their results in MIR-Flickr. I\u2019d rather not saying MIB generally outperforms MV-InfoMax on MIR-Flickr, as MIB does not (clearly) outperform MV-InfoMax when enough labeled data is available for training downstream recognizers. But MIB does clearly outperform MV-InfoMax when scaling down the percentage of labeled samples used. 5. Regarding baselines/experiments a. In Figure 4, it seems that VAE (with beta=4) outperforms MV-InfoMax. Why the \"\"pseudo-second view\" does not help Mv-Infomax in this scenario? Why VAE is clearly better than Infomax? b. In Figure 3, you might also tune beta for VCCA and its variants, like what you did for VAE/VIB in a single view. 6. Do you think your approach can be extended to more than two views easily? For me, it seems the extension is not trivial, as it requires o(n^2) terms in your loss for n views. But this is minor. ", "rating": "8: Accept", "reply_text": "1 ) In the paper , the authors said the original formulation of IB is only applicable to supervised learning . That is true , but the variational information bottleneck paper [ Alexander A. Alem et al.2017 ] already showed the connection of unsupervised VIB to VAE in the appendix . In supervised settings , VIB allows one to create a representation that discards irrelevant input information . The unsupervised extension of VIB mentioned in [ Alemi et al . ( 2017 ) ] is equivalent to the beta-VAE model , in which the beta hyper-parameter regulates the trade-off between distortion and rate [ Alemi et al . ( 2018 ) ] .In this setting , however , we have no guarantees that the information discarded by the model is irrelevant for the task . Our model , on the other hand , makes use of a source of redundant information ( unsupervised multi-view setting ) to create a representation that discards only irrelevant information . We updated our claims in the introduction to clarify the distinction between the three models . 2 ) I would not consider the data augmentation used to extend single-view data to \u201c pseudo-multiview \u201d as a contribution . This has been done before ( e.g.in the multiview MNIST experiment part of the paper `` On Deep Multi-View Representation Learning '' ) . We updated claim ( 3 ) in the introduction to clarify that our contribution does not consist in the definition of \u201c pseudo-multiview \u201d but rather in connecting the well-known data augmentation procedure to the mutual redundancy condition introduced in this work . We believe that this connection could be useful as it defines some constraints on the function class used for augmentation , which can be expressed and described using information-theoretic quantities . 3 ) Which MV-InfoMax do you really compare to ? You listed a few of them : ( Ji et al. , 2019 ; Henaff et al. , \u00b4 2019 ; Tian et al. , 2019 ; Bachman et al. , 2019 ) in the related work section . The version of MV-InfoMax reported in the experiments is based on Tian et al ( 2019 ) , with the only difference that we used an alternative mutual information estimator . The InfoNCE estimator used in [ Tian et al ( 2019 ) ] allows for faster computation but usually results in slightly worse estimations than the Jensen-Shannon estimator [ Poole 2018 ] . For this reason , we decided to consistently use the Jensen-Shannon estimator for the rest of the experiments as it results in better performance for all three models . We added appendix G.4.1 to report the performance and mutual information estimation obtained by using both InfoNCE and Jensen-Shannon estimators for InfoMax , MV-InfoMax , and MIB . We also added a few sentences in the experimental section to clarify our mutual information estimation choice . 4 ) I think the authors should also make a more careful claim on their results in MIR-Flickr . I \u2019 d rather not saying MIB generally outperforms MV-InfoMax on MIR-Flickr , as MIB does not ( clearly ) outperform MV-InfoMax when enough labeled data is available for training downstream recognizers . But MIB does clearly outperform MV-InfoMax when scaling down the percentage of labeled samples used . We updated our claim in the abstract of the paper to clarify this point by specifically noting that we provide state-of-the-art results only in the label-limited regime ."}], "0": {"review_id": "B1xwcyHFDr-0", "review_text": "In this paper, the authors extend the Information Bottleneck method (to build robust representations by removing information unrelated to the target labels) to the unsupervised setting. Since label information is not available in this setting, the authors leverage multi-view information (e.g., using two images of the same object) , which requires assuming that both views contain all necessary information for the subsequent label prediction task. The representation should then focus on capturing the information shared by both views and discarding the rest. A loss function for learning such representations is proposed. The effectiveness of the proposed technique is confirmed on two datasets. It is also shown to work when doing data augmentation with a single view. Overall the paper is well motivated, well placed in the literature and well written. Mathematical derivations are provided. Experimental methodology follows the existing literature, seem reasonable and results are convincing. I do not have major negative comments for the authors. This is however not my research area and have only a limited knowledge of the existing body of work. Comments/Questions: - How limiting is the multi-view assumption? Are there well-known cases where it doesn't hold? I feel it would be hard to use, say, with text. Has this been discussed in the literature? Some pointers or discussion would be interesting. - Sketchy dataset: Could the DSH algorithm (one of the best prior results) be penalized by not using the same feature extractor you used? - Sketchy dataset: Can a reference for the {Siamese,Triplet}-AlexNet results be provided? - Sketchy dataset: for reproducibility, what is the selected \\beta? - I find it very hard to believe that the accuracy stays constant no matter the number of examples per label used. How can an encoder be trained on 10 images? Did I misunderstand the meaning of this number? Can this be clarified? - Again for reproducibility, listing the raw numbers for the MNIST experiments would be nice. - If I understood the experiments correctly, \"scarce label regime\" is used for both the MIR-Flickr and MNIST datasets, meaning two different things (number of labels per example vs number of examples per label), which is slightly confusing. Typos: Page 1: it's -> its Page 6: the the -> the Page 7: classifer -> classifier Page 8: independently -> independent ", "rating": "8: Accept", "reply_text": "1 ) How limiting is the multi-view assumption ? Are there well-known cases where it does n't hold ? I feel it would be hard to use , say , with text . Has this been discussed in the literature ? Some pointers or discussion would be interesting . We answered this above as part of shared question ( 1 ) 2 ) Sketchy dataset : Could the DSH algorithm ( one of the best prior results ) be penalized by not using the same feature extractor you used ? The DSH algorithm may be limited by their use of an AlexNet instead of a VGG network , but they also fine-tune the AlexNet as part of their model , while our model directly uses the features unmodified . So it 's unclear how these two changes affect the performance on the balance . We were unable to produce an updated version of their results with a VGG network because they do not provide code or hyper-parameter settings for training their models , or details on how they did the pretraining . In order to facilitate future comparison , we will release the preprocessed dataset with the 5 splits used for our experiments upon paper acceptance . 3 ) Sketchy dataset : Can a reference for the { Siamese , Triplet } -AlexNet results be provided ? For reproducibility , what is the selected \\beta ? A reference for { Siamese , Triplet } -AlexNet results has been added to Table 1 together with the selected value of beta for MIB . 4 ) I find it very hard to believe that the accuracy stays constant no matter the number of examples per label used . How can an encoder be trained on 10 images ? Did I misunderstand the meaning of this number ? Can this be clarified ? The MNIST experiments are performed with the following procedure : i ) Using the whole training set without any labels ( i.e.unsupervised ) we train the encoder network which generates a representation for each input picture . We then freeze the weight of the encoder network . ii ) Then the randomly chosen set of training labels are used to train a linear classifier to map from encoded samples to the labels . Note here that in our experiments , the chosen amount of labels does indeed range from one example per label , up to and including all labels . iii ) Each classifier is evaluated on a disjoint test set . This is done by first encoding the unseen test set using the encoder trained in i ) and plugging the representations into the linear classifier trained in ii ) In this setting , the encoder has access to the whole ( unlabeled ) training set , but the classifier is trained using different amounts of examples . The MIB model produces a representation that contains approximately 2.3 nats ( Figure 4 ) which corresponds roughly to the amount of information associated with a categorical distribution on 10 classes with uniform probability ( ln10 ~2.3 nats ) . Further investigating this interesting property , we visualized the representation produced by our model by projecting the representation of the test set on the 2 principal components ( Figure 7 added in Appendix G.4.1 ) . The representation for the test digits roughly consists of 10 linearly separable clusters , which explains why 10 examples are sufficient to align cluster centroids and labels . 5 ) Again for reproducibility , listing the raw numbers for the MNIST experiments would be nice . We added appendix G.4.1 in which we report accuracy for different numbers of examples and mutual information estimation corresponding to the comparison reported in Figure 4 . 6 ) If I understood the experiments correctly , `` scarce label regime '' is used for both the MIR-Flickr and MNIST datasets , meaning two different things ( number of labels per example vs number of examples per label ) , which is slightly confusing . By \u201c scarce label regime \u201d , we mean a reduced number of labeled examples . This translates into slightly different settings for single-label ( MNIST ) and multi-label ( Flickr ) classification problems . In both cases , we simulate the lack of labels by picking a subset of labeled examples with the same distribution p ( x , y ) as the original training set . Since the label distribution on MNIST is uniform , we generate training subsets by picking the same number of examples for each class . The same procedure is not possible for the Flickr experiments as the label distribution is uneven and each example has multiple labels . For this reason , we uniformly subsample the original training set without replacement . The x-axis in Figure 3 and 4 have been updated to consistently report the number of labeled examples used for training a classifier on top of the specified representation . 7 ) Typos We thank the reviewer for identifying the mistakes , which have been fixed in the current version of the paper ."}, "1": {"review_id": "B1xwcyHFDr-1", "review_text": "This paper extends the information bottleneck method of Tishby et al. (2000) to the unsupervised setting. By taking advantage of multi-view data, they provide two views of the same underlying entity. Experimetal results on two standard multi-view datasets validate the efficacy of the proposed method. I have three questions about this work. 1. The proposed method only provides two views of the same underlying entity, what about 3 or more views? 2. Can this method be used for multi-modality case? 3. What about the time efficiency of the proposed method?", "rating": "6: Weak Accept", "reply_text": "1 ) The proposed method only provides two views of the same underlying entity , what about 3 or more views ? We addressed this above as part of shared question ( 2 ) 2 ) Can this method be used for multi-modality case ? We addressed this above as part of shared question ( 1 ) 3 ) What about the time efficiency of the proposed method ? The proposed MIB architecture involves training 2 encoders and one auxiliary architecture as in [ Tian et al . ( 2019 ) , Bachman et al . ( 2019 ) ] .Such models typically train faster than the ones that involve the use of decoders ( e.g.VAE , MVAE , VCCA ) , or involve more complicated architectures consisting of multiple modules ( e.g.GDH , DSH ) . On the other hand , since the beta hyper-parameter has to be slowly increased during training to ensure stability , the total number of training steps required for convergence is slightly bigger than InfoMax and MV-InfoMax . Both training time per epoch and the total number of training steps for convergence mostly depend on the specific choice of mutual information estimation and corresponding auxiliary neural network architecture and is a current subject of exploration in recent work [ Poole et al . ( 2019 ) , Belghazi et al . ( 2018 ) ] ."}, "2": {"review_id": "B1xwcyHFDr-2", "review_text": "This is a good multiview representation learning paper with new insights. The authors propose to learn variables z_1 and z_2, which are consistent, contain view-invariant information but discard as much view-specific information as possible. The paper relies on mutual information estimation and is reconstruction-free. It is mentioned in some previous works (e.g. Aaron van den Oord et al. 2018), that reconstruction loss can introduce bias that has a negative effect on the learned representation. Comparing to existing multiview representation learning approaches that try to maximize the mutual information between learned representation and the view(s), this paper clearly defines superfluous information that we should try to throw away and figure out how to obtain sufficiency learned representation for output. The authors also draw clear connections between a few existing (multiview) representation learning methods to their proposed approaches. The experimental results on the right side of Figure 3, deliver a very interesting conclusion. In low-resource case, robust feature (obtained by using the larger beta, discarding more superfluous information) is crucial for achieving good performance. While when the amount of labeled data samples is enough, vice-versa. Here are my major concerns: 1. In the paper, the authors said the original formulation of IB is only applicable to supervised learning. That is true, but the variational information bottleneck paper [Alexander A. Alem et al. 2017] already showed the connection of unsupervised VIB to VAE in the appendix. 2. I would not consider the data augmentation used to extend single-view data to \u201cpseudo-multiview\u201d as a contribution. This has been done before (e.g. in the multiview MNIST experiment part of the paper \"On Deep Multi-View Representation Learning\"). 3. Which MV-InfoMax do you really compare to? You listed a few of them: (Ji et al., 2019; Henaff et al., \u00b4 2019; Tian et al., 2019; Bachman et al., 2019) in the related work section. 4. I think the authors should also make a more careful claim on their results in MIR-Flickr. I\u2019d rather not saying MIB generally outperforms MV-InfoMax on MIR-Flickr, as MIB does not (clearly) outperform MV-InfoMax when enough labeled data is available for training downstream recognizers. But MIB does clearly outperform MV-InfoMax when scaling down the percentage of labeled samples used. 5. Regarding baselines/experiments a. In Figure 4, it seems that VAE (with beta=4) outperforms MV-InfoMax. Why the \"\"pseudo-second view\" does not help Mv-Infomax in this scenario? Why VAE is clearly better than Infomax? b. In Figure 3, you might also tune beta for VCCA and its variants, like what you did for VAE/VIB in a single view. 6. Do you think your approach can be extended to more than two views easily? For me, it seems the extension is not trivial, as it requires o(n^2) terms in your loss for n views. But this is minor. ", "rating": "8: Accept", "reply_text": "1 ) In the paper , the authors said the original formulation of IB is only applicable to supervised learning . That is true , but the variational information bottleneck paper [ Alexander A. Alem et al.2017 ] already showed the connection of unsupervised VIB to VAE in the appendix . In supervised settings , VIB allows one to create a representation that discards irrelevant input information . The unsupervised extension of VIB mentioned in [ Alemi et al . ( 2017 ) ] is equivalent to the beta-VAE model , in which the beta hyper-parameter regulates the trade-off between distortion and rate [ Alemi et al . ( 2018 ) ] .In this setting , however , we have no guarantees that the information discarded by the model is irrelevant for the task . Our model , on the other hand , makes use of a source of redundant information ( unsupervised multi-view setting ) to create a representation that discards only irrelevant information . We updated our claims in the introduction to clarify the distinction between the three models . 2 ) I would not consider the data augmentation used to extend single-view data to \u201c pseudo-multiview \u201d as a contribution . This has been done before ( e.g.in the multiview MNIST experiment part of the paper `` On Deep Multi-View Representation Learning '' ) . We updated claim ( 3 ) in the introduction to clarify that our contribution does not consist in the definition of \u201c pseudo-multiview \u201d but rather in connecting the well-known data augmentation procedure to the mutual redundancy condition introduced in this work . We believe that this connection could be useful as it defines some constraints on the function class used for augmentation , which can be expressed and described using information-theoretic quantities . 3 ) Which MV-InfoMax do you really compare to ? You listed a few of them : ( Ji et al. , 2019 ; Henaff et al. , \u00b4 2019 ; Tian et al. , 2019 ; Bachman et al. , 2019 ) in the related work section . The version of MV-InfoMax reported in the experiments is based on Tian et al ( 2019 ) , with the only difference that we used an alternative mutual information estimator . The InfoNCE estimator used in [ Tian et al ( 2019 ) ] allows for faster computation but usually results in slightly worse estimations than the Jensen-Shannon estimator [ Poole 2018 ] . For this reason , we decided to consistently use the Jensen-Shannon estimator for the rest of the experiments as it results in better performance for all three models . We added appendix G.4.1 to report the performance and mutual information estimation obtained by using both InfoNCE and Jensen-Shannon estimators for InfoMax , MV-InfoMax , and MIB . We also added a few sentences in the experimental section to clarify our mutual information estimation choice . 4 ) I think the authors should also make a more careful claim on their results in MIR-Flickr . I \u2019 d rather not saying MIB generally outperforms MV-InfoMax on MIR-Flickr , as MIB does not ( clearly ) outperform MV-InfoMax when enough labeled data is available for training downstream recognizers . But MIB does clearly outperform MV-InfoMax when scaling down the percentage of labeled samples used . We updated our claim in the abstract of the paper to clarify this point by specifically noting that we provide state-of-the-art results only in the label-limited regime ."}}