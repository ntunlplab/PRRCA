{"year": "2017", "forum": "SJZAb5cel", "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "decision": "Reject", "meta_review": "There is a bit of spread in the reviewer scores, but ultimately the paper does not meet the high bar for acceptance to ICLR. The lack of author responses to the reviews does not help either.", "reviews": [{"review_id": "SJZAb5cel-0", "review_text": "The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks). Novelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP \"complexity\") of those tasks. In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one. There are some existing literature in the area (first two Google results found: https://arxiv.org/pdf/1512.04412v1.pdf, (computer vision) and https://www.aclweb.org/anthology/P/P16/P16-1147.pdf (NLP)). In addition to the architecture, the authors propose a regularization technique they call \"successive regularization\". Experiments: - The authors performed a number of experimental analysis to clarify what parts of their architecture are important, which is very valuable; - The information \"transferred\" from one task to the next one is represented both using a smooth label embedding and the hidden representation of the previous task. At this point there is no analysis of which one is actually important, or if they are redundant (update: the authors mentioned they would add something there). Also, it is likely one would have tried first to feed label scores from one task to the next one, instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing. - The successive regularization does not appear to be important in Table 8; a variance analysis would help to conclude.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your comments . According to your comments , we will improve our paper as much as possible . > > At this point there is no analysis of which one is actually important , or if they are redundant ( update : the authors mentioned they would add something there ) . Thank you for pointing out this key observation . Indeed , we have discussed the results shown in Table 13 corresponding to the paragraph `` Vertical connections '' in Section 6.3 , and we will explain the results more clearly . > > Also , it is likely one would have tried first to feed label scores from one task to the next one , instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing . Thank you for the suggestion . We try to compare the results . The use of the label embeddings is similar to using the word embeddings ; for example , the label embeddings capture similarities between labels as shown in [ 1 ] . Thus , the POS and chunking label embeddings could be effective in using the output information in different computation units ( e.g. , the four different matrix multiplications in Equation ( 1 ) ) . [ 1 ] Danqi Chen and Christopher D. Manning . 2014.A fast and accurate dependency parser using neural networks . In EMNLP ."}, {"review_id": "SJZAb5cel-1", "review_text": "this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. the end-to-end model improves over models trained solely on target tasks. although the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness: first, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity. second, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. third, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. minor issue: - chunking is not a word-level task although the annotation is word-level. chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2]. [1] http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf [2] http://www.cs.cmu.edu/~nasmith/LSP/", "rating": "3: Clear rejection", "reply_text": "Thank you for the comments . > > first , a very simple multi-task learning baseline [ 1 ] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity . That is right . To show the importance of the linguistic hierarchy , we provided the results with and without using the hierarchy of the multi-layer RNNs in Table 12 with the paragraph `` Different layers for different tasks '' . Although the model architecture is different from the suggested baseline [ 1 ] , the results in Table 12 show that the use of the hierarchy is more important than the number of the model parameters . > > second , since the test set of chunking is included in training data of dependency parsing , the results related to chunking with JMT_all are not informative . That is right . We put more weight on higher-level tasks ( dependency parsing , relatedness , entailment ) in the JMT_all setting , while the chunking results with JMT_AB on the test set can be comparable with other published results . > > third , since the model does not guarantee well-formed dependency trees , thus , results in table 4 are not fair . This aspect should be further discussed in our paper . As discussed in Appendix B , 22 parsing results are not well-formed trees in terms of ROOT nodes on the development data including 1,700 sentences . In addition , we have found that 61 parsing results have cycles . In total , more than 95 % of the greedy parsing results are well-formed trees . I applied 1st order Eisner 's algorithm to the 83 parsing results which are not well-formed trees , and as a result , the overall accuracy does not significantly change . For example , in the case of JMT_ABC shown in Table 12 , the UAS is now 94.53 % ( previously , 94.52 % ) , and the LAS is 92.62 % ( previously , 92.61 % ) . Now , all parsing results can be well-formed trees while keeping the accuracy ."}, {"review_id": "SJZAb5cel-2", "review_text": "The paper introduce a way to train joint models for many NLP tasks. Traditionally, we treat these tasks as \u201cpipeline\u201d \u2014 the later tasks will depending on the output of the previous tasks. Here, the authors propose a neural approach which includes all the tasks in one single model. The higher level tasks takes (1) the predictions from the lower level tasks and (2) the hidden representations of the lower level tasks. Also proposed in this paper, is the successive regularization. Intuitively, this means that, when training the high level tasks, we don\u2019t want to change the model in the lower levels by too much so that the lower level tasks can keep a reasonable accuracy of prediction. On the modeling side, I think the proposed model is very similar comparing to (Zhang and Weiss, ACL 2016) and SPINN (Bowman et al, 2016) in a even simpler way. The number of the experiments are good. But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there \u2014 sometimes, the performance of the higher level tasks even goes down when training with more tasks (sometimes it does go up, but also not very significant and stable). The dependency scores, although I don\u2019t think this is a serious problem, comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn\u2019t strictly speaking fair. I admit that the successive regularization make sense intuitively and is a very interesting direction to try. However, without a careful study of the training schema of such model, the current results on successive regularization do not convince me that it should be the right thing to do in such models (the current results are not strong enough to show that). The training methods need to be explored here including things as iteratively train on different tasks, and the relationship between the number of training iterations of a task and it\u2019s training set size (and loss on this task etc).", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your comments . > > On the modeling side , I think the proposed model is very similar comparing to ( Zhang and Weiss , ACL 2016 ) and SPINN ( Bowman et al , 2016 ) in a even simpler way . Thank you for the pointer . I will consider mentioning the relationship or difference in our paper . > > But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there As mentioned in the preliminary response , in our model , five different types of tasks are handled in the single model , and it is not obvious when to stop the training while trying to maximize the scores of all the tasks . As the first step , we focused on maximizing the accuracy of dependency parsing on the development data . However , the sizes of the training data are different across the different tasks ; for example , the semantic relatedness and entailment tasks include only 4,500 sentence pairs for training , and the dependency parsing dataset includes 39,832 sentences with word-level annotations . Thus , in general , dependency parsing requires more training epochs than the semantic tasks , but currently , our model trains all of the tasks for the same training epochs . We observed that better scores on the development sets of the semantic tasks can be achieved before the accuracy of dependency parsing reaches the best score . Therefore , it should be an important research direction to investigate a method for achieving the best scores for all of the tasks at the same time . > > The dependency scores , although I don \u2019 t think this is a serious problem , comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn \u2019 t strictly speaking fair . This aspect should be further discussed in our paper . As discussed in Appendix B , 22 parsing results are not well-formed trees in terms of ROOT nodes on the development data including 1,700 sentences . In addition , we have found that 61 parsing results have cycles . In total , more than 95 % of the greedy parsing results are well-formed trees . I applied 1st order Eisner 's algorithm to the 83 parsing results which are not well-formed trees , and as a result , the overall accuracy does not significantly change . For example , in the case of JMT_ABC shown in Table 12 , the UAS is now 94.53 % ( previously , 94.52 % ) , and the LAS is 92.62 % ( previously , 92.61 % ) . Now , all parsing results can be well-formed trees while keeping the accuracy ."}], "0": {"review_id": "SJZAb5cel-0", "review_text": "The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks). Novelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP \"complexity\") of those tasks. In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one. There are some existing literature in the area (first two Google results found: https://arxiv.org/pdf/1512.04412v1.pdf, (computer vision) and https://www.aclweb.org/anthology/P/P16/P16-1147.pdf (NLP)). In addition to the architecture, the authors propose a regularization technique they call \"successive regularization\". Experiments: - The authors performed a number of experimental analysis to clarify what parts of their architecture are important, which is very valuable; - The information \"transferred\" from one task to the next one is represented both using a smooth label embedding and the hidden representation of the previous task. At this point there is no analysis of which one is actually important, or if they are redundant (update: the authors mentioned they would add something there). Also, it is likely one would have tried first to feed label scores from one task to the next one, instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing. - The successive regularization does not appear to be important in Table 8; a variance analysis would help to conclude.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your comments . According to your comments , we will improve our paper as much as possible . > > At this point there is no analysis of which one is actually important , or if they are redundant ( update : the authors mentioned they would add something there ) . Thank you for pointing out this key observation . Indeed , we have discussed the results shown in Table 13 corresponding to the paragraph `` Vertical connections '' in Section 6.3 , and we will explain the results more clearly . > > Also , it is likely one would have tried first to feed label scores from one task to the next one , instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing . Thank you for the suggestion . We try to compare the results . The use of the label embeddings is similar to using the word embeddings ; for example , the label embeddings capture similarities between labels as shown in [ 1 ] . Thus , the POS and chunking label embeddings could be effective in using the output information in different computation units ( e.g. , the four different matrix multiplications in Equation ( 1 ) ) . [ 1 ] Danqi Chen and Christopher D. Manning . 2014.A fast and accurate dependency parser using neural networks . In EMNLP ."}, "1": {"review_id": "SJZAb5cel-1", "review_text": "this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. the end-to-end model improves over models trained solely on target tasks. although the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness: first, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity. second, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. third, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. minor issue: - chunking is not a word-level task although the annotation is word-level. chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2]. [1] http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf [2] http://www.cs.cmu.edu/~nasmith/LSP/", "rating": "3: Clear rejection", "reply_text": "Thank you for the comments . > > first , a very simple multi-task learning baseline [ 1 ] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity . That is right . To show the importance of the linguistic hierarchy , we provided the results with and without using the hierarchy of the multi-layer RNNs in Table 12 with the paragraph `` Different layers for different tasks '' . Although the model architecture is different from the suggested baseline [ 1 ] , the results in Table 12 show that the use of the hierarchy is more important than the number of the model parameters . > > second , since the test set of chunking is included in training data of dependency parsing , the results related to chunking with JMT_all are not informative . That is right . We put more weight on higher-level tasks ( dependency parsing , relatedness , entailment ) in the JMT_all setting , while the chunking results with JMT_AB on the test set can be comparable with other published results . > > third , since the model does not guarantee well-formed dependency trees , thus , results in table 4 are not fair . This aspect should be further discussed in our paper . As discussed in Appendix B , 22 parsing results are not well-formed trees in terms of ROOT nodes on the development data including 1,700 sentences . In addition , we have found that 61 parsing results have cycles . In total , more than 95 % of the greedy parsing results are well-formed trees . I applied 1st order Eisner 's algorithm to the 83 parsing results which are not well-formed trees , and as a result , the overall accuracy does not significantly change . For example , in the case of JMT_ABC shown in Table 12 , the UAS is now 94.53 % ( previously , 94.52 % ) , and the LAS is 92.62 % ( previously , 92.61 % ) . Now , all parsing results can be well-formed trees while keeping the accuracy ."}, "2": {"review_id": "SJZAb5cel-2", "review_text": "The paper introduce a way to train joint models for many NLP tasks. Traditionally, we treat these tasks as \u201cpipeline\u201d \u2014 the later tasks will depending on the output of the previous tasks. Here, the authors propose a neural approach which includes all the tasks in one single model. The higher level tasks takes (1) the predictions from the lower level tasks and (2) the hidden representations of the lower level tasks. Also proposed in this paper, is the successive regularization. Intuitively, this means that, when training the high level tasks, we don\u2019t want to change the model in the lower levels by too much so that the lower level tasks can keep a reasonable accuracy of prediction. On the modeling side, I think the proposed model is very similar comparing to (Zhang and Weiss, ACL 2016) and SPINN (Bowman et al, 2016) in a even simpler way. The number of the experiments are good. But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there \u2014 sometimes, the performance of the higher level tasks even goes down when training with more tasks (sometimes it does go up, but also not very significant and stable). The dependency scores, although I don\u2019t think this is a serious problem, comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn\u2019t strictly speaking fair. I admit that the successive regularization make sense intuitively and is a very interesting direction to try. However, without a careful study of the training schema of such model, the current results on successive regularization do not convince me that it should be the right thing to do in such models (the current results are not strong enough to show that). The training methods need to be explored here including things as iteratively train on different tasks, and the relationship between the number of training iterations of a task and it\u2019s training set size (and loss on this task etc).", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your comments . > > On the modeling side , I think the proposed model is very similar comparing to ( Zhang and Weiss , ACL 2016 ) and SPINN ( Bowman et al , 2016 ) in a even simpler way . Thank you for the pointer . I will consider mentioning the relationship or difference in our paper . > > But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there As mentioned in the preliminary response , in our model , five different types of tasks are handled in the single model , and it is not obvious when to stop the training while trying to maximize the scores of all the tasks . As the first step , we focused on maximizing the accuracy of dependency parsing on the development data . However , the sizes of the training data are different across the different tasks ; for example , the semantic relatedness and entailment tasks include only 4,500 sentence pairs for training , and the dependency parsing dataset includes 39,832 sentences with word-level annotations . Thus , in general , dependency parsing requires more training epochs than the semantic tasks , but currently , our model trains all of the tasks for the same training epochs . We observed that better scores on the development sets of the semantic tasks can be achieved before the accuracy of dependency parsing reaches the best score . Therefore , it should be an important research direction to investigate a method for achieving the best scores for all of the tasks at the same time . > > The dependency scores , although I don \u2019 t think this is a serious problem , comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn \u2019 t strictly speaking fair . This aspect should be further discussed in our paper . As discussed in Appendix B , 22 parsing results are not well-formed trees in terms of ROOT nodes on the development data including 1,700 sentences . In addition , we have found that 61 parsing results have cycles . In total , more than 95 % of the greedy parsing results are well-formed trees . I applied 1st order Eisner 's algorithm to the 83 parsing results which are not well-formed trees , and as a result , the overall accuracy does not significantly change . For example , in the case of JMT_ABC shown in Table 12 , the UAS is now 94.53 % ( previously , 94.52 % ) , and the LAS is 92.62 % ( previously , 92.61 % ) . Now , all parsing results can be well-formed trees while keeping the accuracy ."}}