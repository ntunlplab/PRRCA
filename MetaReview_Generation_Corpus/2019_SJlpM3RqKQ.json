{"year": "2019", "forum": "SJlpM3RqKQ", "title": "Expanding the Reach of Federated Learning by Reducing Client Resource Requirements", "decision": "Reject", "meta_review": "This paper focuses on  communication efficient Federated Learning (FL) and proposes an approach for  training  large models on heterogeneous edge devices.   The paper is well-written and the approach is promising, but all reviewers pointed out that both novelty of the approach and empirical evaluation, including comparison with state-of-art, are somewhat limited. We hope that suggestions provided by the reviewers will be helpful for extending and improving this work.", "reviews": [{"review_id": "SJlpM3RqKQ-0", "review_text": "The paper presents some new approaches for communication efficient Federated Learning (FL) that allows for training of large models on heterogeneous edge devices. In FL, heterogeneous edge devices have access to potentially non-iid samples of data points and try to jointly learn a model by averaging their local models at a parameter server (the cloud). As the bandwidth of the up/downlink-link may be limited communication overheads may become the bottleneck during FL. Moreover, due to the heterogeneity of the hardware, large models may be hard to train on small devices. Due to that, there are several recent approaches that aim to minimize communication via methods of quantization, which also aim to allow for smaller models via methods of compression and model quantization. In this paper, the authors suggest a combination of two methods to reduce communication and allow for large model training by 1) using a lossy compressed model when that is communicated from the cloud to the edge devices, and 2) subsampling the gradients, a form of dropout, at the edge device side that allows for an overall smaller model update. The novelty of either of those techniques is quite limited as individually they have been suggested before, but the combination of both of them is interesting. The paper is overall well written, however there are two aspects that make the contribution lacking in novelty. First of all, the presented methods are a combination of existing techniques, that although interesting to combine together, are neither theoretically analyzed nor extensively tested. The model/update quantization technique has been used in the past extensively [eg 1-3]. Then, the \u201cfederated dropout\u201d can be seen as a \u201ccoordinate descent\u201d type of a technique, i.e., randomly zeroing out gradient elements per iteration. Since this is a more experimental paper, the setup tested is quite limited in its comparisons. For example, one would expect to see extensive comparisons with methods for quantizing gradients, eg QSGD, or Terngrad, and combinations of that with DeepCompression. Although the authors do make an effort to experiment with a different set of hyperparameters (dropout probability, quantization levels, etc), a comparison with state of the art methods is lacking. Overall, although the combination of the presented ideas has some merit, the lack of extensive experiments that would compare it with the state of the art is not convincing, and the overall effectiveness of this method is unclear at this point. [1] https://arxiv.org/pdf/1510.00149.pdf [2] https://arxiv.org/pdf/1803.03383.pdf [4] https://arxiv.org/pdf/1610.05492.pdf", "rating": "4: Ok but not good enough - rejection", "reply_text": "The second point we want to address is our lack of comparisons against previous existing work : 1 ) Comparison with QSGD or Terngrad : We did not compare with these for two reasons . a ) These methods were proposed for compression of gradient updates . In particular , the Terngrad paper argues for using the empirical distributions of the coefficients of such gradients . Even though those arguments would not directly apply to our setting , we could probably still use it for the Client-to-Server compression . However , we do not see a good reason why the proposal would be useful for compressing the state of the model being trained ( i.e.Server-to-Client ) , which is the central concern of our paper . b ) We performed a series of preliminary experiments where we compressed a variety of random vectors using QSGD and other techniques . The results of these small experiments suggested that in the tradeoff between accuracy and representation size , ( I ) uniform quantization was dominated by QSGD , and ( II ) QSGD was in turn dominated by the combination of Kashin \u2019 s representation and uniform quantization . We are happy to improve our Related Work section but , unfortunately , the rebuttal period will not be enough to fully recreate experiments using QSGD and Terngrad . What we could do in the time given is add the results of the simple experiments we mention above . We thus ask the reviewer , in light of our previous reasoning and the findings of our preliminary results , whether they consider the full comparison necessary , or whether adding the simpler evaluation would be sufficient . 2 ) Comparison with HALP : As far as we can see , the ideas introduced in that paper are largely compatible with our proposed methods ( particularly regarding how we compute gradients locally ) but would not replace them . We were previously unaware of this paper though , and we will add an appropriate reference to it . 3 ) Comparison with https : //arxiv.org/abs/1610.05492 : We clearly call out that we build on that work , and extend in two significant aspects . First , we introduce the use of Kashin \u2019 s representation ( novel in ML in general ) to further improve efficiency of uniform quantization . Second , we show how we can use the techniques in reducing Server-to-Client communication as well ."}, {"review_id": "SJlpM3RqKQ-1", "review_text": "The paper tackles a major issue in distributed learning in general (and not only the federated scheme), which is communication bottleneck. I am not fully qualified to judge and would rather listen to the opinion of more qualified reviewers, I was annoyed by some aspects of the paper: 1) many claims required formal support (proofs), as an example: \"more aggressive dropout rates ted to slow down the convergence rate of the model, even if they sometimes result in a higher accuracy\" is a statement that would benefit from analyzing the dropout out effect on convergence, something that wouldn't be hard to do given the extensive theoretical toolbox on distributed optimization. 2) no comparison with other compression schemes (see e.g. Alistarh et al.'s ZipML (NIPS or ICML 2017) and followups) 3) proving an unbiased-ness guarantee out of the Probabilistic quantization (section 3.1) would have been a minimal requirement in my opinion. I encourage the authors to further expand those points, but would happily lighten-up my skepticism if more qualified reviewers say that we do not need such guarantees as the one in point 1 and 3. (the few compression papers I know provide that)", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their comments and for highlighting the relevance of our work for the broader distributed learning community . We proceed to address the three points you raised : 1 ) The particular observation you mention is in line with previous empirical observations of the effect of ( standard ) dropout . We don \u2019 t analyse this effect , however , as we are not aware of any rigorous argument of why standard dropout works in the first place . We understand dropout as a heuristic that has proven to be incredibly useful and is backed by some interesting intuitions , but not as a principled approach for which we can prove convergence . 2 ) The ZipML framework proposes using lower precision at various parts of the training pipeline . Many of these ideas are orthogonal , yet compatible with what we propose . The parts that can be seen as alternatives to our methods ( i.e.compressing gradients ) are best summarized in algorithms such as QSGD or Terngrad ( also called out by another reviewer ) . We copy our response here : We did not compare with these for two reasons . a ) These methods were proposed for compression of gradient updates . In particular , the Terngrad paper argues for using the empirical distributions of the coefficients of such gradients . Even though those arguments would not directly apply to our setting , we could probably still use it for the Client-to-Server compression . However , we do not see a good reason why the proposal would be useful for compressing the state of the model being trained ( i.e.Server-to-Client ) , which is the central concern of our paper . b ) We performed a series of preliminary experiments where we compressed a variety of random vectors using QSGD and other techniques . The results of these small experiments suggested that in the tradeoff between accuracy and representation size , ( I ) uniform quantization was dominated by QSGD , and ( II ) QSGD was in turn dominated by the combination of Kashin \u2019 s representation and uniform quantization . 3 ) The proof of this is elementary , and we do not want to appear to claim it is a novel insight . We are happy to provide explicit reference to an existing , more general argument , e.g. , one in Suresh et al.or in Konecny and Richtarik , both of which we cite . If you have other concrete comments on what would strengthen the paper , we will be more than happy to incorporate them ."}, {"review_id": "SJlpM3RqKQ-2", "review_text": "The paper is well written and addresses an interesting problem. Overall, I do find the federated dropout idea quite interesting. As for the lossy compression part, I am a bit skeptical on its application for this problem. In general, I believe that the manuscript could greatly benefit from answering the questions that I am raising below. It would certainly help me better appreciate the contributions of this work. The lossy aspect of the compression inevitably introduces performance downgrades. However, compression/communication systems are designed to make sure that the information dropped is not important for the task at hand (e.g., high frequencies that are not perceived by our eyes in the spatial domain are typically dropped when compressing images through zig zag scanning after transformation). Randomly dropping coefficients as suggested in this paper seems odd to me (the subsampling technique that is used). Can you justify this approach? The manuscript does hint that this approach provides lukewarm results. Could there be a better approach that focuses on parts of the model that deemed \u201cless\u201d important if a notion of coefficient importance can be derived? Can you emphasize more the benefits of compression and federated drop out, versus training a low capacity model with less parameters? The introduction refers to the low capacity approach as a naive model. Could this be compared experimentally? This would help better appreciate the benefits of the federated dropout strategies that are proposed here. In the experiments, could you explain why increases in q (quantization steps) seems to lead to limited or marginal accuracy improvements? For the results shown in Figure 4, did you also use any form of subsampling and quantization? Also, do you have a justification for why with some amounts of dropout, the accuracy may improve but at a slower pace (pretty much the punch line of these experiments)? It is an interesting finding but it is counter intuitive and requires explanations in my view. On the communication cost experiments, can you explain precisely how did you compute these reduction factors? Did you tolerate some form of accuracy degradation? Also, did you consider the fact that more \"rounds\" are needed to get to a target accuracy level? Is there a cost associated with these additional rounds and was that cost taken into consideration? Adding clarity on this would certainly help. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "- \u201c ... why with some amounts of dropout , the accuracy may improve but at a slower pace ? \u201d Answer : This is in line with the empirical observations of ( standard ) dropout . We only have weak suggestions for why this might be the case , which will require more work to support : The effect we see might be because the approach effectively creates a random ensemble of models within the single global model . Moreover , it might be possible to get a speed up stemming from the following observation : Since we generate an update for only a subset of the model parameters , we might be able to utilize a smarter server averaging scheme - instead of simply averaging the updates as done currently . Investigating this might be an interesting follow-up work . - \u201c On the communication cost experiments , can you explain precisely how did you compute these reduction factors ? \u201d Answer : The reduction factors do not tolerate any form of accuracy degradation , and are calculated from the client \u2019 s perspective . In particular , the presented reduction factors are computed from the \u201c Moderate \u201d compression scheme presented in Table 2 : the 9.6x reduction in server-to-client communication is the compounding of an 6.4x reduction due to quantization ( to 5 bits ) and a 1.5x reduction due to federated dropout ( rate of 0.8 corresponding to ~0.8 * 0.8 factor of saving ) ; the 1.5x reduction in local computation is due to federated dropout ( rate of 0.8 ) . The 24x reduction in upload communication is the compounding of a 16x reduction due to quantization ( 4 bits ) and subsampling ( s = 0.5 ) , and a 1.5x reduction due to federated dropout ( rate of 0.8 ) . However , notice that , with the addition of dropout for convolutional layers , these reductions changed ( improved ) slightly ( see note to all reviewers ) . We have updated the numbers in our submission . - \u201c ... did you consider the fact that more `` rounds '' are needed to get to a target accuracy level ? \u201d Answer : In practice , using compression and Federated Dropout will make the rounds complete faster . Thus , without access to an actual production deployment , it is generally impossible to say what will best in terms of runtime . Therefore , we think the number of rounds is the best \u201c fair \u201d comparison . At the same time , note that slightly longer runtime would be a welcome price to pay for higher final accuracy . We see this point is not clear in the paper and we will add a remark on this ."}], "0": {"review_id": "SJlpM3RqKQ-0", "review_text": "The paper presents some new approaches for communication efficient Federated Learning (FL) that allows for training of large models on heterogeneous edge devices. In FL, heterogeneous edge devices have access to potentially non-iid samples of data points and try to jointly learn a model by averaging their local models at a parameter server (the cloud). As the bandwidth of the up/downlink-link may be limited communication overheads may become the bottleneck during FL. Moreover, due to the heterogeneity of the hardware, large models may be hard to train on small devices. Due to that, there are several recent approaches that aim to minimize communication via methods of quantization, which also aim to allow for smaller models via methods of compression and model quantization. In this paper, the authors suggest a combination of two methods to reduce communication and allow for large model training by 1) using a lossy compressed model when that is communicated from the cloud to the edge devices, and 2) subsampling the gradients, a form of dropout, at the edge device side that allows for an overall smaller model update. The novelty of either of those techniques is quite limited as individually they have been suggested before, but the combination of both of them is interesting. The paper is overall well written, however there are two aspects that make the contribution lacking in novelty. First of all, the presented methods are a combination of existing techniques, that although interesting to combine together, are neither theoretically analyzed nor extensively tested. The model/update quantization technique has been used in the past extensively [eg 1-3]. Then, the \u201cfederated dropout\u201d can be seen as a \u201ccoordinate descent\u201d type of a technique, i.e., randomly zeroing out gradient elements per iteration. Since this is a more experimental paper, the setup tested is quite limited in its comparisons. For example, one would expect to see extensive comparisons with methods for quantizing gradients, eg QSGD, or Terngrad, and combinations of that with DeepCompression. Although the authors do make an effort to experiment with a different set of hyperparameters (dropout probability, quantization levels, etc), a comparison with state of the art methods is lacking. Overall, although the combination of the presented ideas has some merit, the lack of extensive experiments that would compare it with the state of the art is not convincing, and the overall effectiveness of this method is unclear at this point. [1] https://arxiv.org/pdf/1510.00149.pdf [2] https://arxiv.org/pdf/1803.03383.pdf [4] https://arxiv.org/pdf/1610.05492.pdf", "rating": "4: Ok but not good enough - rejection", "reply_text": "The second point we want to address is our lack of comparisons against previous existing work : 1 ) Comparison with QSGD or Terngrad : We did not compare with these for two reasons . a ) These methods were proposed for compression of gradient updates . In particular , the Terngrad paper argues for using the empirical distributions of the coefficients of such gradients . Even though those arguments would not directly apply to our setting , we could probably still use it for the Client-to-Server compression . However , we do not see a good reason why the proposal would be useful for compressing the state of the model being trained ( i.e.Server-to-Client ) , which is the central concern of our paper . b ) We performed a series of preliminary experiments where we compressed a variety of random vectors using QSGD and other techniques . The results of these small experiments suggested that in the tradeoff between accuracy and representation size , ( I ) uniform quantization was dominated by QSGD , and ( II ) QSGD was in turn dominated by the combination of Kashin \u2019 s representation and uniform quantization . We are happy to improve our Related Work section but , unfortunately , the rebuttal period will not be enough to fully recreate experiments using QSGD and Terngrad . What we could do in the time given is add the results of the simple experiments we mention above . We thus ask the reviewer , in light of our previous reasoning and the findings of our preliminary results , whether they consider the full comparison necessary , or whether adding the simpler evaluation would be sufficient . 2 ) Comparison with HALP : As far as we can see , the ideas introduced in that paper are largely compatible with our proposed methods ( particularly regarding how we compute gradients locally ) but would not replace them . We were previously unaware of this paper though , and we will add an appropriate reference to it . 3 ) Comparison with https : //arxiv.org/abs/1610.05492 : We clearly call out that we build on that work , and extend in two significant aspects . First , we introduce the use of Kashin \u2019 s representation ( novel in ML in general ) to further improve efficiency of uniform quantization . Second , we show how we can use the techniques in reducing Server-to-Client communication as well ."}, "1": {"review_id": "SJlpM3RqKQ-1", "review_text": "The paper tackles a major issue in distributed learning in general (and not only the federated scheme), which is communication bottleneck. I am not fully qualified to judge and would rather listen to the opinion of more qualified reviewers, I was annoyed by some aspects of the paper: 1) many claims required formal support (proofs), as an example: \"more aggressive dropout rates ted to slow down the convergence rate of the model, even if they sometimes result in a higher accuracy\" is a statement that would benefit from analyzing the dropout out effect on convergence, something that wouldn't be hard to do given the extensive theoretical toolbox on distributed optimization. 2) no comparison with other compression schemes (see e.g. Alistarh et al.'s ZipML (NIPS or ICML 2017) and followups) 3) proving an unbiased-ness guarantee out of the Probabilistic quantization (section 3.1) would have been a minimal requirement in my opinion. I encourage the authors to further expand those points, but would happily lighten-up my skepticism if more qualified reviewers say that we do not need such guarantees as the one in point 1 and 3. (the few compression papers I know provide that)", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their comments and for highlighting the relevance of our work for the broader distributed learning community . We proceed to address the three points you raised : 1 ) The particular observation you mention is in line with previous empirical observations of the effect of ( standard ) dropout . We don \u2019 t analyse this effect , however , as we are not aware of any rigorous argument of why standard dropout works in the first place . We understand dropout as a heuristic that has proven to be incredibly useful and is backed by some interesting intuitions , but not as a principled approach for which we can prove convergence . 2 ) The ZipML framework proposes using lower precision at various parts of the training pipeline . Many of these ideas are orthogonal , yet compatible with what we propose . The parts that can be seen as alternatives to our methods ( i.e.compressing gradients ) are best summarized in algorithms such as QSGD or Terngrad ( also called out by another reviewer ) . We copy our response here : We did not compare with these for two reasons . a ) These methods were proposed for compression of gradient updates . In particular , the Terngrad paper argues for using the empirical distributions of the coefficients of such gradients . Even though those arguments would not directly apply to our setting , we could probably still use it for the Client-to-Server compression . However , we do not see a good reason why the proposal would be useful for compressing the state of the model being trained ( i.e.Server-to-Client ) , which is the central concern of our paper . b ) We performed a series of preliminary experiments where we compressed a variety of random vectors using QSGD and other techniques . The results of these small experiments suggested that in the tradeoff between accuracy and representation size , ( I ) uniform quantization was dominated by QSGD , and ( II ) QSGD was in turn dominated by the combination of Kashin \u2019 s representation and uniform quantization . 3 ) The proof of this is elementary , and we do not want to appear to claim it is a novel insight . We are happy to provide explicit reference to an existing , more general argument , e.g. , one in Suresh et al.or in Konecny and Richtarik , both of which we cite . If you have other concrete comments on what would strengthen the paper , we will be more than happy to incorporate them ."}, "2": {"review_id": "SJlpM3RqKQ-2", "review_text": "The paper is well written and addresses an interesting problem. Overall, I do find the federated dropout idea quite interesting. As for the lossy compression part, I am a bit skeptical on its application for this problem. In general, I believe that the manuscript could greatly benefit from answering the questions that I am raising below. It would certainly help me better appreciate the contributions of this work. The lossy aspect of the compression inevitably introduces performance downgrades. However, compression/communication systems are designed to make sure that the information dropped is not important for the task at hand (e.g., high frequencies that are not perceived by our eyes in the spatial domain are typically dropped when compressing images through zig zag scanning after transformation). Randomly dropping coefficients as suggested in this paper seems odd to me (the subsampling technique that is used). Can you justify this approach? The manuscript does hint that this approach provides lukewarm results. Could there be a better approach that focuses on parts of the model that deemed \u201cless\u201d important if a notion of coefficient importance can be derived? Can you emphasize more the benefits of compression and federated drop out, versus training a low capacity model with less parameters? The introduction refers to the low capacity approach as a naive model. Could this be compared experimentally? This would help better appreciate the benefits of the federated dropout strategies that are proposed here. In the experiments, could you explain why increases in q (quantization steps) seems to lead to limited or marginal accuracy improvements? For the results shown in Figure 4, did you also use any form of subsampling and quantization? Also, do you have a justification for why with some amounts of dropout, the accuracy may improve but at a slower pace (pretty much the punch line of these experiments)? It is an interesting finding but it is counter intuitive and requires explanations in my view. On the communication cost experiments, can you explain precisely how did you compute these reduction factors? Did you tolerate some form of accuracy degradation? Also, did you consider the fact that more \"rounds\" are needed to get to a target accuracy level? Is there a cost associated with these additional rounds and was that cost taken into consideration? Adding clarity on this would certainly help. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "- \u201c ... why with some amounts of dropout , the accuracy may improve but at a slower pace ? \u201d Answer : This is in line with the empirical observations of ( standard ) dropout . We only have weak suggestions for why this might be the case , which will require more work to support : The effect we see might be because the approach effectively creates a random ensemble of models within the single global model . Moreover , it might be possible to get a speed up stemming from the following observation : Since we generate an update for only a subset of the model parameters , we might be able to utilize a smarter server averaging scheme - instead of simply averaging the updates as done currently . Investigating this might be an interesting follow-up work . - \u201c On the communication cost experiments , can you explain precisely how did you compute these reduction factors ? \u201d Answer : The reduction factors do not tolerate any form of accuracy degradation , and are calculated from the client \u2019 s perspective . In particular , the presented reduction factors are computed from the \u201c Moderate \u201d compression scheme presented in Table 2 : the 9.6x reduction in server-to-client communication is the compounding of an 6.4x reduction due to quantization ( to 5 bits ) and a 1.5x reduction due to federated dropout ( rate of 0.8 corresponding to ~0.8 * 0.8 factor of saving ) ; the 1.5x reduction in local computation is due to federated dropout ( rate of 0.8 ) . The 24x reduction in upload communication is the compounding of a 16x reduction due to quantization ( 4 bits ) and subsampling ( s = 0.5 ) , and a 1.5x reduction due to federated dropout ( rate of 0.8 ) . However , notice that , with the addition of dropout for convolutional layers , these reductions changed ( improved ) slightly ( see note to all reviewers ) . We have updated the numbers in our submission . - \u201c ... did you consider the fact that more `` rounds '' are needed to get to a target accuracy level ? \u201d Answer : In practice , using compression and Federated Dropout will make the rounds complete faster . Thus , without access to an actual production deployment , it is generally impossible to say what will best in terms of runtime . Therefore , we think the number of rounds is the best \u201c fair \u201d comparison . At the same time , note that slightly longer runtime would be a welcome price to pay for higher final accuracy . We see this point is not clear in the paper and we will add a remark on this ."}}