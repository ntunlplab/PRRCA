{"year": "2018", "forum": "Sk9yuql0Z", "title": "Mitigating Adversarial Effects Through Randomization", "decision": "Accept (Poster)", "meta_review": "Paper proposes adding randomization steps during inference time to CNNs in order to defend against adversarial attacks.\n\nPros:\n\n- Results demonstrate good performance, and the team achieve a high rank (2nd place) on a public benchmark.\n- The benefit of the proposed approach is that it does not require any additional training or retraining.\n\nCons:\n\n- The approach is very simple, common sense would tend to suggest that adding noise to images would make adversarial attempts more difficult. Though perhaps simplicity is a good thing.\n- Update: Paper does not cite related and relevant work, which takes a similar approach of requiring no retraining, but rather changing the inference stage: https://arxiv.org/pdf/1709.05583.pdf\n\u2028\u2028\nGrammatical Suggestions:\n\nThis paper would benefit from polishing. For example:\n\n- Abstract: sentence 1: replace \u201ctheir powerful ability\u201d to \u201chigh accuracy\u201d\n- Abstract: sentence 3: replace \u201cI.e., clean images\u2026\u201d with \u201cFor example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail\u201d\n- Abstract: sentence 4: replace \u201cutilize randomization\u201d to \u201cimplement randomization at inference time\u201d or something similar to make more clear that this procedure is not done during training.\n- Abstract: sentence 7: replace \u201calso enjoys\u201d with \u201cprovides\u201d\n\nMain Text: Capitalize references to figures (i.e. \u201cfigure 1\u201d to \u201cFigure 1\u201d).\n\nIntroduction: Paragraph 4: Again, please replace \u201crandomization\u201d with \u201crandomization at inference time\u201d or something similar to better address reviewer concerns.\n", "reviews": [{"review_id": "Sk9yuql0Z-0", "review_text": "The authors propose a simple defense against adversarial attacks, which is to add randomization in the input of the CNNs. They experiment with different CNNs and published adversarial training techniques and show that randomized inputs mitigate adversarial attacks. Pros: (+) The idea introduced is simple and flexible to be used for any CNN architecture (+) Experiments on ImageNet1k prove demonstrate its effectiveness Cons: (-) Experiments are not thorougly explained (-) Novelty is extremely limited (-) Some baselines missing The experimental section of the paper was rather confusing. The authors should explain the experiments and the settings in the table, as those are not very clear. In particular, it was not clear whether the defense model was trained with the input randomization layers? Also, in Tables 1-6, how was the target model trained? How do the training procedures of target vs. defense model differ? In those tables, what is the testing procedure for the target model and how does it compare to the defense model? The gap between the target and defense model in Table 4 (ensemble pattern attack scenario) shrinks for single step attack methods. This means that when the attacker is aware of the randomization parameters, the effect of randomization might diminish. A baseline that reports the performance when the attacker is fully aware of the randomization of the defender (parameters, patterns etc.) is missing but is very useful. While the experiments show that the randomization layers mitigate the effect of randomization attacks, it's not clear whether the effectiveness of this very simple approach is heavily biased towards the published ways of generating adversarial attacks and the particular problem (i.e. classification). The form of attacks studied in the paper is that of additive noise. But there is many types of attacks that could be closely related to the randomization procedure of the input and that could lead to very different results.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for the comments . We have updated our paper , especially the experiment section . Below are the detailed answers to your concerns . \u201c experiment confusing \u201d : Sorry for the confusion , and we have made this clearer in the updated paper . The defense model is simply adding two randomization layers to the beginning of the original classification networks . There is no re-training and fine-tuning needed . This is an advantage of our method . We choose Inception-v3 , ResNet-v2 , Inception-ResNet-v2 and ens-adv-Inception-ResNet-v2 as the original CNN models , and these models are public available under Tensorflow github repo . The target models are the models used by attackers to generate adversarial examples . The target models differ under different attack scenarios : ( 1 ) vanilla attack : the target model is the original CNN model , e.g.Inception-v3 ; ( 2 ) single-pattern attack : target model is the original CNN model + randomization layers with only one predefined pattern ; ( 3 ) ensemble-pattern attack : the target model is the original CNN model + randomization layers with an ensemble of predefined patterns . Note that the structure and weights of the classification network in target model and defense model are exactly the same . In tables 2-6 , the attackers first use target model to generate adversarial examples , and then tests the top-1 accuracy on target model and defense model . Specifically , ( 1 ) for target model , a lower accuracy indicates a more successful attack ; ( 2 ) for defense model , a higher accuracy indicates a more successful defense . \u201c stronger baseline when the attacker is fully aware the patterns \u201d : We agree that the performance gap between the target and defense model will shrink as more randomization patterns are considered in the attack process . This is expected . Here we want to emphasize that during defense , the padding and resizing are done randomly , so there is no way for both the attacker and the defender to know the exact instantiated patterns . The strongest possible attack would be that the attackers consider ALL possible patterns when generating the adversarial examples . However , this is not possible . Failing all patterns takes extremely long time , and may not even converge . For example , under our randomization setting , the total number of patterns ( resizing + padding ) is 12528 . Thus , instead of choosing such a large number , we choose 21 representative patterns in our ensemble attack scenario , which becomes computationally manageable . Increasing the number of ensembled patterns means : ( 1 ) more computation time ( take C & W for example , it takes around 0.56 min to generate an adversarial example under vanilla attack , but takes around 8 min to generate an adversarial example under ensemble attack ) ; ( 2 ) more memory consumption ( at most an ensemble of 30 different patterns can be utilized as one batch to generated adversarial examples for one 12GB GPU , more patterns indicates more GPUs or the GPU with larger memory ) ; ( 3 ) larger magnitude of adversarial perturbation . \u201c biased towards the published adversarial attacks \u201d : Our defense method is not trained using any adversarial examples , so we don \u2019 t think it is biased towards any attacks . We extensively test our method on the most popular attacks ( one single-step attack FGSM , and two representative iterative attacks DeepFool and C & W ) , with various network structures , and using large-scale ImageNet datasets . Moreover , we submit this method to a public adversarial defense challenge . Our method is evaluated against 156 different attacks and we are ranked Top 2 , which indicates the effectiveness of our method . \u201c particular problem ( e.g.classification ) and additive noise \u201d : Currently most works on this topic focus on classification problem and assume additive noise as adversarial perturbation . We follow this setting in this paper . We have two future directions to explore : 1 ) apply randomization to other vision tasks , 2 ) apply randomization to other types of attack instead of additive noise . Thanks for the comments ."}, {"review_id": "Sk9yuql0Z-1", "review_text": "This paper proposes an extremely simple methodology to improve the network's performance by adding extra random perturbations (resizing/padding) at evaluation time. Although the paper is very basic, it creates a good baseline for defending about various types of attacks and got good results in kaggle competition. The main merit of the paper is to study this simple but efficient baseline method extensively and shows how adversarial attacks can be mitigated by some extent. Cons of the paper: there is not much novel insight or really exciting new ideas presented. Pros: It gives a convincing very simple baseline and the evaluation of all subsequent results on defending against adversaries will need to incorporate this simple defense method in addition to any future proposed defenses, since it is very easy to implement and evaluate and seems to improve the defense capabilities of the network to a significant degree. So I assume that this paper will be influential in the future just by the virtue of its easy applicability and effectiveness. ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for the appreciation of our work . The method is indeed simple and effective . Although the randomization idea is not new , we in this paper apply it to mitigate adversarial effects at test time systematically . And we demonstrate the effectiveness on large-scale ImageNet dataset , which is very challenging . Very few defense papers worked on ImageNet before . We hope our method could be served as a simple new baseline for adversarial example defense in the future works ."}, {"review_id": "Sk9yuql0Z-2", "review_text": "The paper basically propose keep using the typical data-augmentation transformations done during training also in evaluation time, to prevent adversarial attacks. In the paper they analyze only 2 random resizing and random padding, but I suppose others like random contrast, random relighting, random colorization, ... could be applicable. Some of the pros of the proposed tricks is that it doesn't require re-training existing models, although as the authors pointed out re-training for adversarial images is necessary to obtain good results. Typically images have different sizes, however in the Dataset are described as having 299x299x3 size, are all the test images resized before hand? How would this method work with variable size images? The proposed defense requires increasing the size of the input images, have you analyzed the impact in performance? Also it would be good to know how robust is the method for smaller sizes. Section 4.6.2 seems to indicate that 1 pixel padding or just resizing 1 pixel is enough to get most of the benefit, please provide an analysis of how results improve as the padding or size increase. In section 5 for the challenge authors used a lot more evaluations per image, could you provide how much extra computation is needed for that model? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for the comments , which significantly improve the quality of our paper . We have conducted additional experiments to answer the concerns . These experiments results are included as appendix in the updated paper . \u201c Other operations \u201d : Yes , other random operations also apply . We tried four operations separately : random brightness , random contrast , random saturation , and random hue . For each individual operation , we add it to the beginning of the original classification network . We found that these operations nearly have no hurts on the performance of clean images ( shown in table 7 ) , but they are not as effective as the proposed randomization layers on defending adversarial examples ( shown in table 8-11 ) . By combining these random operations with the proposed randomization layers , the performance on defending adversarial examples can be slightly improved . We have updated these new results in the Appendix A . \u201c resized beforehand \u201d : Yes , the test images are resized beforehand . There are two reasons : ( 1 ) easy to form a batch ( e.g. , one batch contains 100 images ) for classification ; ( 2 ) stay aligned with the format of the public competition , where the test dataset are all of the size 299x299x3 . For the images with variable sizes , we can first resize them to 299x299x3 , and then applied the proposed method to defend adversarial examples . \u201c impact of size in performance \u201d : Adding two randomization layers ( increasing size from 299 to 331 ) slightly downgrades the performance on clean images , as shown in Table 1 . This decrease becomes negligible for stronger models . In addition , we also tried applying randomization to smaller-sized images . Specifically , we first resize the images to a size randomly sampled from the range [ 267 , 299 ) , and then randomly pad it to 299x299x3 . We evaluate the performance on both the 5000 clean images and the adversarial examples generated under the vanilla attack scenario ( shown in table 12 ) . We see that the randomization method works well with smaller sizes , but using larger sizes produces slightly better results . We hypothesize that this is because resizing an image to smaller sizes may lose some information . We have updated the new results in the Appendix B . \u201c padding or resizing increase \u201d : As the padding size or resizing size increase , there will be a lot more random patterns . So it becomes much harder for the attackers to generate the adversarial example that can fail all the patterns at the same time . Thus , larger size and more paddings will significantly increase the robustness . Notice that the motivation for the experiments in Sec 4.6 is to decouple the effect of padding and resizing . We want to show that ( 1 ) adversarial example generated on one padding pattern is hard to transfer to another padding pattern ; ( 2 ) adversarial example generated on one size is hard to transfer to another size . Using 1-pixel padding and resizing provide a controllable way to verify these two points . \u201c multiple iterations per image \u201d : The computation time increases linearly with number of iteration per image ( e.g. , 30x time in our challenge submission ) . We argue that one iteration is enough to get the most benefits , and additional evaluations only provide marginal gain ( as shown in figures 3-5 ) , which is good for the challenge . The experiments that show the relationship between the classification performance and iteration number is included in Appendix C ."}], "0": {"review_id": "Sk9yuql0Z-0", "review_text": "The authors propose a simple defense against adversarial attacks, which is to add randomization in the input of the CNNs. They experiment with different CNNs and published adversarial training techniques and show that randomized inputs mitigate adversarial attacks. Pros: (+) The idea introduced is simple and flexible to be used for any CNN architecture (+) Experiments on ImageNet1k prove demonstrate its effectiveness Cons: (-) Experiments are not thorougly explained (-) Novelty is extremely limited (-) Some baselines missing The experimental section of the paper was rather confusing. The authors should explain the experiments and the settings in the table, as those are not very clear. In particular, it was not clear whether the defense model was trained with the input randomization layers? Also, in Tables 1-6, how was the target model trained? How do the training procedures of target vs. defense model differ? In those tables, what is the testing procedure for the target model and how does it compare to the defense model? The gap between the target and defense model in Table 4 (ensemble pattern attack scenario) shrinks for single step attack methods. This means that when the attacker is aware of the randomization parameters, the effect of randomization might diminish. A baseline that reports the performance when the attacker is fully aware of the randomization of the defender (parameters, patterns etc.) is missing but is very useful. While the experiments show that the randomization layers mitigate the effect of randomization attacks, it's not clear whether the effectiveness of this very simple approach is heavily biased towards the published ways of generating adversarial attacks and the particular problem (i.e. classification). The form of attacks studied in the paper is that of additive noise. But there is many types of attacks that could be closely related to the randomization procedure of the input and that could lead to very different results.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for the comments . We have updated our paper , especially the experiment section . Below are the detailed answers to your concerns . \u201c experiment confusing \u201d : Sorry for the confusion , and we have made this clearer in the updated paper . The defense model is simply adding two randomization layers to the beginning of the original classification networks . There is no re-training and fine-tuning needed . This is an advantage of our method . We choose Inception-v3 , ResNet-v2 , Inception-ResNet-v2 and ens-adv-Inception-ResNet-v2 as the original CNN models , and these models are public available under Tensorflow github repo . The target models are the models used by attackers to generate adversarial examples . The target models differ under different attack scenarios : ( 1 ) vanilla attack : the target model is the original CNN model , e.g.Inception-v3 ; ( 2 ) single-pattern attack : target model is the original CNN model + randomization layers with only one predefined pattern ; ( 3 ) ensemble-pattern attack : the target model is the original CNN model + randomization layers with an ensemble of predefined patterns . Note that the structure and weights of the classification network in target model and defense model are exactly the same . In tables 2-6 , the attackers first use target model to generate adversarial examples , and then tests the top-1 accuracy on target model and defense model . Specifically , ( 1 ) for target model , a lower accuracy indicates a more successful attack ; ( 2 ) for defense model , a higher accuracy indicates a more successful defense . \u201c stronger baseline when the attacker is fully aware the patterns \u201d : We agree that the performance gap between the target and defense model will shrink as more randomization patterns are considered in the attack process . This is expected . Here we want to emphasize that during defense , the padding and resizing are done randomly , so there is no way for both the attacker and the defender to know the exact instantiated patterns . The strongest possible attack would be that the attackers consider ALL possible patterns when generating the adversarial examples . However , this is not possible . Failing all patterns takes extremely long time , and may not even converge . For example , under our randomization setting , the total number of patterns ( resizing + padding ) is 12528 . Thus , instead of choosing such a large number , we choose 21 representative patterns in our ensemble attack scenario , which becomes computationally manageable . Increasing the number of ensembled patterns means : ( 1 ) more computation time ( take C & W for example , it takes around 0.56 min to generate an adversarial example under vanilla attack , but takes around 8 min to generate an adversarial example under ensemble attack ) ; ( 2 ) more memory consumption ( at most an ensemble of 30 different patterns can be utilized as one batch to generated adversarial examples for one 12GB GPU , more patterns indicates more GPUs or the GPU with larger memory ) ; ( 3 ) larger magnitude of adversarial perturbation . \u201c biased towards the published adversarial attacks \u201d : Our defense method is not trained using any adversarial examples , so we don \u2019 t think it is biased towards any attacks . We extensively test our method on the most popular attacks ( one single-step attack FGSM , and two representative iterative attacks DeepFool and C & W ) , with various network structures , and using large-scale ImageNet datasets . Moreover , we submit this method to a public adversarial defense challenge . Our method is evaluated against 156 different attacks and we are ranked Top 2 , which indicates the effectiveness of our method . \u201c particular problem ( e.g.classification ) and additive noise \u201d : Currently most works on this topic focus on classification problem and assume additive noise as adversarial perturbation . We follow this setting in this paper . We have two future directions to explore : 1 ) apply randomization to other vision tasks , 2 ) apply randomization to other types of attack instead of additive noise . Thanks for the comments ."}, "1": {"review_id": "Sk9yuql0Z-1", "review_text": "This paper proposes an extremely simple methodology to improve the network's performance by adding extra random perturbations (resizing/padding) at evaluation time. Although the paper is very basic, it creates a good baseline for defending about various types of attacks and got good results in kaggle competition. The main merit of the paper is to study this simple but efficient baseline method extensively and shows how adversarial attacks can be mitigated by some extent. Cons of the paper: there is not much novel insight or really exciting new ideas presented. Pros: It gives a convincing very simple baseline and the evaluation of all subsequent results on defending against adversaries will need to incorporate this simple defense method in addition to any future proposed defenses, since it is very easy to implement and evaluate and seems to improve the defense capabilities of the network to a significant degree. So I assume that this paper will be influential in the future just by the virtue of its easy applicability and effectiveness. ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for the appreciation of our work . The method is indeed simple and effective . Although the randomization idea is not new , we in this paper apply it to mitigate adversarial effects at test time systematically . And we demonstrate the effectiveness on large-scale ImageNet dataset , which is very challenging . Very few defense papers worked on ImageNet before . We hope our method could be served as a simple new baseline for adversarial example defense in the future works ."}, "2": {"review_id": "Sk9yuql0Z-2", "review_text": "The paper basically propose keep using the typical data-augmentation transformations done during training also in evaluation time, to prevent adversarial attacks. In the paper they analyze only 2 random resizing and random padding, but I suppose others like random contrast, random relighting, random colorization, ... could be applicable. Some of the pros of the proposed tricks is that it doesn't require re-training existing models, although as the authors pointed out re-training for adversarial images is necessary to obtain good results. Typically images have different sizes, however in the Dataset are described as having 299x299x3 size, are all the test images resized before hand? How would this method work with variable size images? The proposed defense requires increasing the size of the input images, have you analyzed the impact in performance? Also it would be good to know how robust is the method for smaller sizes. Section 4.6.2 seems to indicate that 1 pixel padding or just resizing 1 pixel is enough to get most of the benefit, please provide an analysis of how results improve as the padding or size increase. In section 5 for the challenge authors used a lot more evaluations per image, could you provide how much extra computation is needed for that model? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for the comments , which significantly improve the quality of our paper . We have conducted additional experiments to answer the concerns . These experiments results are included as appendix in the updated paper . \u201c Other operations \u201d : Yes , other random operations also apply . We tried four operations separately : random brightness , random contrast , random saturation , and random hue . For each individual operation , we add it to the beginning of the original classification network . We found that these operations nearly have no hurts on the performance of clean images ( shown in table 7 ) , but they are not as effective as the proposed randomization layers on defending adversarial examples ( shown in table 8-11 ) . By combining these random operations with the proposed randomization layers , the performance on defending adversarial examples can be slightly improved . We have updated these new results in the Appendix A . \u201c resized beforehand \u201d : Yes , the test images are resized beforehand . There are two reasons : ( 1 ) easy to form a batch ( e.g. , one batch contains 100 images ) for classification ; ( 2 ) stay aligned with the format of the public competition , where the test dataset are all of the size 299x299x3 . For the images with variable sizes , we can first resize them to 299x299x3 , and then applied the proposed method to defend adversarial examples . \u201c impact of size in performance \u201d : Adding two randomization layers ( increasing size from 299 to 331 ) slightly downgrades the performance on clean images , as shown in Table 1 . This decrease becomes negligible for stronger models . In addition , we also tried applying randomization to smaller-sized images . Specifically , we first resize the images to a size randomly sampled from the range [ 267 , 299 ) , and then randomly pad it to 299x299x3 . We evaluate the performance on both the 5000 clean images and the adversarial examples generated under the vanilla attack scenario ( shown in table 12 ) . We see that the randomization method works well with smaller sizes , but using larger sizes produces slightly better results . We hypothesize that this is because resizing an image to smaller sizes may lose some information . We have updated the new results in the Appendix B . \u201c padding or resizing increase \u201d : As the padding size or resizing size increase , there will be a lot more random patterns . So it becomes much harder for the attackers to generate the adversarial example that can fail all the patterns at the same time . Thus , larger size and more paddings will significantly increase the robustness . Notice that the motivation for the experiments in Sec 4.6 is to decouple the effect of padding and resizing . We want to show that ( 1 ) adversarial example generated on one padding pattern is hard to transfer to another padding pattern ; ( 2 ) adversarial example generated on one size is hard to transfer to another size . Using 1-pixel padding and resizing provide a controllable way to verify these two points . \u201c multiple iterations per image \u201d : The computation time increases linearly with number of iteration per image ( e.g. , 30x time in our challenge submission ) . We argue that one iteration is enough to get the most benefits , and additional evaluations only provide marginal gain ( as shown in figures 3-5 ) , which is good for the challenge . The experiments that show the relationship between the classification performance and iteration number is included in Appendix C ."}}