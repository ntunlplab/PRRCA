{"year": "2019", "forum": "r1NJqsRctX", "title": "Auxiliary Variational MCMC", "decision": "Accept (Poster)", "meta_review": "The reviewers all argued for acceptance citing the novelty and potential of the work as strengths.  They all found the experiments a little underwhelming and asked for more exciting empirical evaluation.  The authors have addressed this somewhat by including multi-modal experiments in the discussion period.  The paper would be more impactful if the authors could demonstrate significant improvements on really challenging problems where MCMC is currently prohibitively expensive, such as improving over HMC for highly parameterized deep neural networks.  Overall, however, this is a very nice paper and warrants acceptance to the conference.", "reviews": [{"review_id": "r1NJqsRctX-0", "review_text": "This paper proposes a clever and sensible approach to using the structure learned by the auxiliary variational method to accelerate random-walk MCMC. The idea is to learn a low-dimensional latent space that explains much of the variation in the original parameter space, then do random-walk sampling in that space (while also updating a state variable in the original state, which is necessary to ensure correctness). I like this idea and think the paper merits acceptance, although there are some important unanswered questions. For example: - How does the method work on higher-dimensional target distributions? I would think it would be hard for a low-dimensional auxiliary space to have high mutual information with a much higher-dimensional space. In principle neural networks can do all sorts of crazy things, but phenomena like VAEs with low-dimensional latent spaces generating blurry samples make me suspect that auxiliary dimension should be important. - How does the method work with hierarchical models, heavy-tailed models, etc.? Rings, MoGs, and flat logistic regressions are already pretty easy targets. - Is it really so valuable to not need gradients? High-quality automatic differentiation systems are widely available, and variational inference on discrete parameters with neural nets remains a pretty hard problem in general. Some other comments: * It\u2019s probably worth citing Ranganath et al. (2015; \u201cHierarchical Variational Models\u201d), who combine the auxiliary variational method with modern stochastic VI. Also, I wonder if there are connections to approximate Bayesian computation (ABC). * I think you could prove the validity of the procedure in section 2.1 more succinctly by interpreting it as alternating a Gibbs sampling update for \u201ca\u201d with a Metropolis-Hastings update for \u201cx\u201d. If we treat \u201ca\u201d as an auxiliary variable such that p(a | x) = \\tilde q(a | x) p(x | a) \\propto p(x) \\tilde q(a | x) then the equation (2) is the correct M-H acceptance probability for the proposal \\tilde q(a\u2019, x\u2019) = \u03b4(a\u2019-a) \\tilde q(x\u2019 | a). Alternating between this proposal and a Gibbs update for \u201ca\u201d yields the mixture proposal in section 2.1. * It\u2019s also possibly worth noting that this procedure will have a strictly lower acceptance rate than the ideal procedure of using the marginal \\tilde q(x\u2019|x) as a M-H proposal directly. Unfortunately that marginal density usually can\u2019t be computed, which makes this ideal procedure impractical. It might be interesting to try to say something about how large this gap is for the proposed method. * \"We choose not to investigate burn-in since AVS is initialized by the variational distribution and therefore has negligible if any burn-in time.\u201d This claim seems unjustified to me. It\u2019s only true insofar as the variational distribution is an excellent approximation to the posterior (in which case why use MCMC at all?). It\u2019s easy to find examples where an MCMC chain initialized with a sample from a variational distribution takes quite a while to burn in.", "rating": "7: Good paper, accept", "reply_text": "> > * I think you could prove the validity of the procedure in section 2.1 more succinctly by interpreting it as alternating a Gibbs sampling update for \u201c a \u201d with a Metropolis-Hastings update for \u201c x \u201d . If we treat \u201c a \u201d as an auxiliary variable such that > > p ( a | x ) = \\tilde q ( a | x ) > > p ( x | a ) \\propto p ( x ) \\tilde q ( a | x ) > > then the equation ( 2 ) is the correct M-H acceptance probability for the proposal > > \\tilde q ( a \u2019 , x \u2019 ) = \u03b4 ( a \u2019 -a ) \\tilde q ( x \u2019 | a ) . > > Alternating between this proposal and a Gibbs update for \u201c a \u201d yields the mixture proposal in section 2.1 . We agree that there are perhaps more direct proofs of our method , as has been suggested by another reviewer as well . However , rigorously handling deterministic proposals in Metropolis Hastings is quite an advanced topic and we feel that our proof whilst algebraically more involved is conceptually simpler . We also feel that the extension of the proof as given to multiple auxiliary variables is more straight forward . Unless the reviewers feel very strongly on this point , we would prefer to maintain the proof as is . > > It \u2019 s also possibly worth noting that this procedure will have a strictly lower acceptance rate than the ideal procedure of using the marginal > > \\tilde q ( x \u2019 |x ) > > as a M-H proposal directly . Unfortunately that marginal density usually can \u2019 t be computed , which makes this ideal procedure impractical . It might be interesting to try to say something about how large this gap is for the proposed method . This is an interesting question and not one we had investigated in detail . We shall think carefully about this and if we have any insights prior to the final deadline shall update the paper . > > `` We choose not to investigate burn-in since AVS is initialized by the variational distribution and therefore has negligible if any burn-in time. \u201d This claim seems unjustified to me . It \u2019 s only true insofar as the variational distribution is an excellent approximation to the posterior ( in which case why use MCMC at all ? ) . It \u2019 s easy to find examples where an MCMC chain initialized with a sample from a variational distribution takes quite a while to burn in . Perhaps the claim as stated is slightly too strong but it seems plausible to the authors that initialising with a variational distribution will likely start the chain in a region of high target density even if the variational approximation is poor . This in turn will reduce burn-in time relative to a method that is not initialised in a region of high density . Indeed , this is the reason for the common practice of initialising MCMC algorithms by first running an optimisation procedure to find the mode . In any case , all the authors were hoping to convey was that burn-in is relatively fast for AVS and thus not the most interesting point of comparison relative to other methods . We will amend the text to read : `` We choose not to focus our investigation on burn-in time since AVS , being initialized by the variational distribution , often has short burn-in relative to competitive methods . \u201d"}, {"review_id": "r1NJqsRctX-1", "review_text": "In my opinion, the paper contains very interesting novel ideas. However, some parts needs a future clarification and the state-of-the-art must be improved. - First of all, Sections 2.3.1 or 2.3.2 can be improved and clarified. For instance, I believe you can create a unique section with title \" Choice of Proposal density \" and then schematically describe each proposal from the simplest to the more sophisticated one. - At the beginning of Section 2, please devote more sentence to explain why extending the space and apply the variational inference is good for finding a suitable good proposal density. - Related to Section 2 ( theMixture Proposal MCMC contribution), the authors should discuss (in the introduction and also in the related works section) the Multiple Try Metropolis schemes with correlated candidates where, for instance, a path of candidates is generated and one of them is selected and tested with MH-type acceptance probability, in a proper way. This is more general that your scheme but very related. Please see Qin, Z.S., Liu, J.S., 2001. Multi-point Metropolis method with application to hybrid Monte Carlo. Journal of Computational Physics 172, 827\u2013840. L. Martino, V. P. Del Olmo, J. Read, \"A multi-point Metropolis scheme with generic weight functions\", Statistics and Probability Letters, Volume 82, Issue 7, Pages: 1445-1453, 2012. L. Martino, \"A Review of Multiple Try MCMC algorithms for Signal Processing\", Digital Signal Processing, Volume 75, Pages: 134-152, 2018. - Related again with the state-of-the-art description, the references regarding Adaptive Mixture Metropolis methods are completely missed. If I have properly understood, you also adapt a mixture via variational inference. Please, in Section 4, consider the different works that considers an adapting mixture proposal for a Metropolis-type algorithm, P. Giordani and R. Kohn, \u201cAdaptive independent Metropolis-Hastings by fast estimation of mixtures of normals,\u201d Journal of Computational and Graphical Statistics, vol. 19, no. 2, pp. 243\u2013259, September 2010. Tran, M.-N., M. K. Pitt, and R. Kohn. Adaptive Metropolis\u2013Hastings sampling using reversible dependent mixture proposals. Statistics and Computing, 26, 1\u201321, 2014. D. Luengo, L. Martino, \"Fully Adaptive Gaussian Mixture Metropolis-Hastings Algorithm\", IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Vancouver (Canada), 2013. Roberts, G. O. and J. S. Rosenthal (2009). Examples of adaptive MCMC. Journal of Computational and Graphical Statistics 18, 349\u2013367. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and well considered comments . > > Review : In my opinion , the paper contains very interesting novel ideas . > > However , some parts needs a future clarification and the state-of-the-art must be improved . > > First of all , Sections 2.3.1 or 2.3.2 can be improved and clarified . For instance , I believe you can create a unique section with title `` Choice of Proposal density `` and then schematically describe each proposal from the simplest to the more sophisticated one . Thanks for the feedback . As we edit the paper to include other changes we 'll bear this in mind . We 'd hoped this was what we had done already but will try to make it clearer . > > At the beginning of Section 2 , please devote more sentence to explain why extending the space and apply the variational inference is good for finding a suitable good proposal density . We believe that this was made clear in sections 2.3.1 and 2.3.2 . In particular the discussion immediately following equation ( 9 ) tries to make this point . However , we can add a further sentence emphasising this at the start of section 2 as well . > > Related to Section 2 ( theMixture Proposal MCMC contribution ) , the authors should discuss ( in the introduction and also in the related works section ) the Multiple Try Metropolis schemes with correlated candidates were , for instance , a path of candidates is generated and one of them is selected and tested with MH-type acceptance probability , in a proper way . This is more general that your scheme but very related . Please see > > Qin , Z.S. , Liu , J.S. , 2001 . Multi-point Metropolis method with application to hybrid Monte Carlo . Journal of Computational Physics 172 , 827\u2013840 . > > L. Martino , V. P. Del Olmo , J . Read , `` A multi-point Metropolis scheme with generic weight functions '' , Statistics and Probability Letters , Volume 82 , Issue 7 , Pages : 1445-1453 , 2012 . > > L.Martino , `` A Review of Multiple Try MCMC algorithms for Signal Processing '' , Digital Signal Processing , Volume 75 , Pages : 134-152 , 2018 . Thanks for the pointers to these papers . We are aware of Multiple Try Metropolis ( MTM ) but many of the references you provided below were new to us . Whilst we acknowledge that MTM is a powerful tool in the MCMC arsenal , we felt that it was quite different to our method and really offers an orthogonal direction for improvement . We do n't attempt a thorough review of the state-of-the-art in MCMC , which we feel is beyond the scope here , but instead try to focus our discussion on other neural adaptive samplers such as L2HMC and A-NICE-MCMC . > > related again with the state-of-the-art description , the references regarding Adaptive Mixture Metropolis methods are completely missed . If I have properly understood , you also adapt a mixture via variational inference . Please , in Section 4 , consider the different works that considers an adapting mixture proposal for a Metropolis-type algorithm , Thanks for the pointers to these papers . These were mostly new to us and do seem very related . After reading the papers more closely we will try to include them in our references . > > P.Giordani and R. Kohn , \u201c Adaptive independent Metropolis-Hastings by fast estimation of mixtures of normals , \u201d Journal of Computational and Graphical Statistics , vol . 19 , no.2 , pp.243\u2013259 , September 2010 . > > Tran , M.-N. , M. K. Pitt , and R. Kohn . Adaptive Metropolis\u2013Hastings sampling using reversible dependent mixture proposals . Statistics and Computing , 26 , 1\u201321 , 2014 . > > D.Luengo , L. Martino , `` Fully Adaptive Gaussian Mixture Metropolis-Hastings Algorithm '' , IEEE International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ) , Vancouver ( Canada ) , 2013 . > > Roberts , G. O. and J. S. Rosenthal ( 2009 ) . Examples of adaptive MCMC . Journal of Computational and Graphical Statistics 18 , 349\u2013367"}, {"review_id": "r1NJqsRctX-2", "review_text": "This paper proposes an auxiliary variable MCMC scheme involving variational inference for efficient MCMC. Given a target distribution p(x), the authors introduce an auxiliary variable a, and learn conditional distributions p(a|x) and q(a|x) by minimizing the KL divergence between p(x)p(a|x) and q(a)q(x|a), with q(a) something simple (the authors use Gaussian). A MH proposal step involves simulating x givea the current MCMC sample x (from p(a|x), taking a step in A-space, and then returning back to the X space (using q(x|a)). The authors show how to calculate the acceptance probability. I think the idea is nice and useful (I'm surprised people haven't thought of this before), though I think the paper presents this in a less clear way (as an extension of ideas from Agakov and Barber's \"Auxiliary variational method\"). While this is correct and perhaps more general, in my mind it slightly obscures the main idea, as well as the strong ties with variational autoencoders: express a complex distribution as a (learnt) transformation of a simple distribution (this is the actual approach taken in the experiments). The motivation of the approach is that the nonlinear encoding network can transform the complex p(x) into a simpler q(a). For this reason, I think an important baseline is the independent MH sampler from equation 8 (I think this essentially uses a trained VAE generative model as a proposal distribution). The authors talk about how producing independent proposals can be sub-optimal, yet it seems to me that if the encoder and decoder neural networks are powerful enough, this should do a good job. I think excluding this baseline hurts the paper a bit. The proof of correctness while correct is a bit unclear, can perhaps be simplified if you view the MCMC algorithm as operating on an augmented space (x,a,x') with stationary distribution p(x)q(a|x)q(x'|a) (writing writing q for \\tilde(q)). This clearly has the right distribution over x. Each MCMC iteration starts with x and proceeds as follow: 1) Given x, sample a and x' from q(a|x) and q(x'|a) 2) Make a deterministic proposal on the augmented space to swap (x,x'). The acceptance probability is now equation 2. 3) Discard a,x'. In figure 4, the authors use HMC as an \"improved MCMC algorithm\", yet this is not an algorithm that deals with multimodality well. More useful would be to include some tempering algorithm like serial or parallel tempering. While I like the idea, I unfortunately don't think the experiments are very convincing (and the authors barely discuss their results). Other than mixture of Gaussians, HMC (which involves no training) appears to be superior. With some tempering, I expect it to outperform the proposed method for the MoG case Table 2 left: since HMC involves no training, does this mean that, taking training time into account, HMC is 5-6 orders of magnitude more efficient. L?ke I mentioned earlier, these results need more discussion. It would also help to provide absolute training and run times, so the reader can better understand whether the proposed method of ANICE is better. Figure 3: why don't the authors also plot the histogram of values in the auxiliary space, p(a). It would be interesting to see how Gaussian this is (this is what variational inference is trying to achieve). Also, does Figure 3(a) mean that conditioned on x, p(a|x) is basically a delta function? This would suggest that the encoder is basically learning a deterministic transformation to a simpler low-dimensional space? There is some work in this direction in the statistics literature, e.g. \"Variable transformation to obtain geometric ergodicity in the random-walk Metropolis algorithm\" The authors some refers to the distribution of a|x as q(a|x) sometimes (in section 2.1) and sometimes as p(a|x) which is a bit confusing. Figure 2: the labels are wrong.", "rating": "7: Good paper, accept", "reply_text": "> > While I like the idea , I unfortunately do n't think the experiments are very convincing ( and the authors barely discuss their results ) . Other than mixture of Gaussians , HMC ( which involves no training ) appears to be superior . With some tempering , I expect it to outperform the proposed method for the MoG case > > Table 2 left : since HMC involves no training , does this mean that , taking training time into account , HMC is 5-6 orders of magnitude more efficient . Like I mentioned earlier , these results need more discussion . > > It would also help to provide absolute training and run times , so the reader can better understand whether the proposed method of ANICE is better . The reviewer is correct that , presently , if training time is included HMC significantly outperforms not just the method proposed in this paper but all of the learned samplers explored thus far . The size of the difference is dramatic here because the data-sets used are small but as the size of the data-set grows we expect the computational cost of HMC to grow much faster than the learned samplers . This is because HMC will require many gradients of the log-density per iteration and the cost of each of these scales linearly in the size of the data . Whilst calculating acceptance ratios also scales linearly in data-size , it only needs to be performed once per iteration . HMC , on the other hand , will often require 10s or 100s of gradients per iteration . Another point that we 'd emphasise is that although there is an increased computational time for these methods , because they are black-box they potentially save on much more vital human time , HMC being quite tricky to tune . We will update the paper to include absolute training times . To give the reviewer an indication here of the difference in absolute training times ( in seconds ) we provide preliminary figures taken from the last batch of experiments we ran : Ring/ Mog/ Logistic Regression/ ANICE : 120/ 1290 / 3310 AVS : 70 / 2 / 2 L2HMC : 3310/ 2620 / 4590 We will try to expand the discussion of the results , their brevity was primarily a result of space restrictions . > > Figure 3 : why do n't the authors also plot the histogram of values in the auxiliary space , p ( a ) . It would be interesting to see how Gaussian this is ( this is what variational inference is trying to achieve ) . We agree that this might be interesting but felt that in the limited space given it was n't central to our argument . > > Also , does Figure 3 ( a ) mean that conditioned on x , p ( a|x ) is basically a delta function ? This would suggest that the encoder is basically learning a deterministic transformation to a simpler low-dimensional space ? There is some work in this direction in the statistics literature , e.g.In this case , yes the conditional became close to a delta-function though its not clear how often this will be the case . > > '' Variable transformation to obtain geometric ergodicity in the random-walk Metropolis algorithm '' Thanks also for the pointer to this paper . We were not aware of it but it seems highly relevant . We 'll try to add it to our discussion after we study it in more detail . > > The authors some refers to the distribution of a|x as q ( a|x ) sometimes ( in section 2.1 ) and sometimes as p ( a|x ) which is a bit confusing . Thanks this was a mistake and will be corrected . > > Figure 2 : the labels are wrong . Thanks again , we will correct this ."}], "0": {"review_id": "r1NJqsRctX-0", "review_text": "This paper proposes a clever and sensible approach to using the structure learned by the auxiliary variational method to accelerate random-walk MCMC. The idea is to learn a low-dimensional latent space that explains much of the variation in the original parameter space, then do random-walk sampling in that space (while also updating a state variable in the original state, which is necessary to ensure correctness). I like this idea and think the paper merits acceptance, although there are some important unanswered questions. For example: - How does the method work on higher-dimensional target distributions? I would think it would be hard for a low-dimensional auxiliary space to have high mutual information with a much higher-dimensional space. In principle neural networks can do all sorts of crazy things, but phenomena like VAEs with low-dimensional latent spaces generating blurry samples make me suspect that auxiliary dimension should be important. - How does the method work with hierarchical models, heavy-tailed models, etc.? Rings, MoGs, and flat logistic regressions are already pretty easy targets. - Is it really so valuable to not need gradients? High-quality automatic differentiation systems are widely available, and variational inference on discrete parameters with neural nets remains a pretty hard problem in general. Some other comments: * It\u2019s probably worth citing Ranganath et al. (2015; \u201cHierarchical Variational Models\u201d), who combine the auxiliary variational method with modern stochastic VI. Also, I wonder if there are connections to approximate Bayesian computation (ABC). * I think you could prove the validity of the procedure in section 2.1 more succinctly by interpreting it as alternating a Gibbs sampling update for \u201ca\u201d with a Metropolis-Hastings update for \u201cx\u201d. If we treat \u201ca\u201d as an auxiliary variable such that p(a | x) = \\tilde q(a | x) p(x | a) \\propto p(x) \\tilde q(a | x) then the equation (2) is the correct M-H acceptance probability for the proposal \\tilde q(a\u2019, x\u2019) = \u03b4(a\u2019-a) \\tilde q(x\u2019 | a). Alternating between this proposal and a Gibbs update for \u201ca\u201d yields the mixture proposal in section 2.1. * It\u2019s also possibly worth noting that this procedure will have a strictly lower acceptance rate than the ideal procedure of using the marginal \\tilde q(x\u2019|x) as a M-H proposal directly. Unfortunately that marginal density usually can\u2019t be computed, which makes this ideal procedure impractical. It might be interesting to try to say something about how large this gap is for the proposed method. * \"We choose not to investigate burn-in since AVS is initialized by the variational distribution and therefore has negligible if any burn-in time.\u201d This claim seems unjustified to me. It\u2019s only true insofar as the variational distribution is an excellent approximation to the posterior (in which case why use MCMC at all?). It\u2019s easy to find examples where an MCMC chain initialized with a sample from a variational distribution takes quite a while to burn in.", "rating": "7: Good paper, accept", "reply_text": "> > * I think you could prove the validity of the procedure in section 2.1 more succinctly by interpreting it as alternating a Gibbs sampling update for \u201c a \u201d with a Metropolis-Hastings update for \u201c x \u201d . If we treat \u201c a \u201d as an auxiliary variable such that > > p ( a | x ) = \\tilde q ( a | x ) > > p ( x | a ) \\propto p ( x ) \\tilde q ( a | x ) > > then the equation ( 2 ) is the correct M-H acceptance probability for the proposal > > \\tilde q ( a \u2019 , x \u2019 ) = \u03b4 ( a \u2019 -a ) \\tilde q ( x \u2019 | a ) . > > Alternating between this proposal and a Gibbs update for \u201c a \u201d yields the mixture proposal in section 2.1 . We agree that there are perhaps more direct proofs of our method , as has been suggested by another reviewer as well . However , rigorously handling deterministic proposals in Metropolis Hastings is quite an advanced topic and we feel that our proof whilst algebraically more involved is conceptually simpler . We also feel that the extension of the proof as given to multiple auxiliary variables is more straight forward . Unless the reviewers feel very strongly on this point , we would prefer to maintain the proof as is . > > It \u2019 s also possibly worth noting that this procedure will have a strictly lower acceptance rate than the ideal procedure of using the marginal > > \\tilde q ( x \u2019 |x ) > > as a M-H proposal directly . Unfortunately that marginal density usually can \u2019 t be computed , which makes this ideal procedure impractical . It might be interesting to try to say something about how large this gap is for the proposed method . This is an interesting question and not one we had investigated in detail . We shall think carefully about this and if we have any insights prior to the final deadline shall update the paper . > > `` We choose not to investigate burn-in since AVS is initialized by the variational distribution and therefore has negligible if any burn-in time. \u201d This claim seems unjustified to me . It \u2019 s only true insofar as the variational distribution is an excellent approximation to the posterior ( in which case why use MCMC at all ? ) . It \u2019 s easy to find examples where an MCMC chain initialized with a sample from a variational distribution takes quite a while to burn in . Perhaps the claim as stated is slightly too strong but it seems plausible to the authors that initialising with a variational distribution will likely start the chain in a region of high target density even if the variational approximation is poor . This in turn will reduce burn-in time relative to a method that is not initialised in a region of high density . Indeed , this is the reason for the common practice of initialising MCMC algorithms by first running an optimisation procedure to find the mode . In any case , all the authors were hoping to convey was that burn-in is relatively fast for AVS and thus not the most interesting point of comparison relative to other methods . We will amend the text to read : `` We choose not to focus our investigation on burn-in time since AVS , being initialized by the variational distribution , often has short burn-in relative to competitive methods . \u201d"}, "1": {"review_id": "r1NJqsRctX-1", "review_text": "In my opinion, the paper contains very interesting novel ideas. However, some parts needs a future clarification and the state-of-the-art must be improved. - First of all, Sections 2.3.1 or 2.3.2 can be improved and clarified. For instance, I believe you can create a unique section with title \" Choice of Proposal density \" and then schematically describe each proposal from the simplest to the more sophisticated one. - At the beginning of Section 2, please devote more sentence to explain why extending the space and apply the variational inference is good for finding a suitable good proposal density. - Related to Section 2 ( theMixture Proposal MCMC contribution), the authors should discuss (in the introduction and also in the related works section) the Multiple Try Metropolis schemes with correlated candidates where, for instance, a path of candidates is generated and one of them is selected and tested with MH-type acceptance probability, in a proper way. This is more general that your scheme but very related. Please see Qin, Z.S., Liu, J.S., 2001. Multi-point Metropolis method with application to hybrid Monte Carlo. Journal of Computational Physics 172, 827\u2013840. L. Martino, V. P. Del Olmo, J. Read, \"A multi-point Metropolis scheme with generic weight functions\", Statistics and Probability Letters, Volume 82, Issue 7, Pages: 1445-1453, 2012. L. Martino, \"A Review of Multiple Try MCMC algorithms for Signal Processing\", Digital Signal Processing, Volume 75, Pages: 134-152, 2018. - Related again with the state-of-the-art description, the references regarding Adaptive Mixture Metropolis methods are completely missed. If I have properly understood, you also adapt a mixture via variational inference. Please, in Section 4, consider the different works that considers an adapting mixture proposal for a Metropolis-type algorithm, P. Giordani and R. Kohn, \u201cAdaptive independent Metropolis-Hastings by fast estimation of mixtures of normals,\u201d Journal of Computational and Graphical Statistics, vol. 19, no. 2, pp. 243\u2013259, September 2010. Tran, M.-N., M. K. Pitt, and R. Kohn. Adaptive Metropolis\u2013Hastings sampling using reversible dependent mixture proposals. Statistics and Computing, 26, 1\u201321, 2014. D. Luengo, L. Martino, \"Fully Adaptive Gaussian Mixture Metropolis-Hastings Algorithm\", IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Vancouver (Canada), 2013. Roberts, G. O. and J. S. Rosenthal (2009). Examples of adaptive MCMC. Journal of Computational and Graphical Statistics 18, 349\u2013367. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and well considered comments . > > Review : In my opinion , the paper contains very interesting novel ideas . > > However , some parts needs a future clarification and the state-of-the-art must be improved . > > First of all , Sections 2.3.1 or 2.3.2 can be improved and clarified . For instance , I believe you can create a unique section with title `` Choice of Proposal density `` and then schematically describe each proposal from the simplest to the more sophisticated one . Thanks for the feedback . As we edit the paper to include other changes we 'll bear this in mind . We 'd hoped this was what we had done already but will try to make it clearer . > > At the beginning of Section 2 , please devote more sentence to explain why extending the space and apply the variational inference is good for finding a suitable good proposal density . We believe that this was made clear in sections 2.3.1 and 2.3.2 . In particular the discussion immediately following equation ( 9 ) tries to make this point . However , we can add a further sentence emphasising this at the start of section 2 as well . > > Related to Section 2 ( theMixture Proposal MCMC contribution ) , the authors should discuss ( in the introduction and also in the related works section ) the Multiple Try Metropolis schemes with correlated candidates were , for instance , a path of candidates is generated and one of them is selected and tested with MH-type acceptance probability , in a proper way . This is more general that your scheme but very related . Please see > > Qin , Z.S. , Liu , J.S. , 2001 . Multi-point Metropolis method with application to hybrid Monte Carlo . Journal of Computational Physics 172 , 827\u2013840 . > > L. Martino , V. P. Del Olmo , J . Read , `` A multi-point Metropolis scheme with generic weight functions '' , Statistics and Probability Letters , Volume 82 , Issue 7 , Pages : 1445-1453 , 2012 . > > L.Martino , `` A Review of Multiple Try MCMC algorithms for Signal Processing '' , Digital Signal Processing , Volume 75 , Pages : 134-152 , 2018 . Thanks for the pointers to these papers . We are aware of Multiple Try Metropolis ( MTM ) but many of the references you provided below were new to us . Whilst we acknowledge that MTM is a powerful tool in the MCMC arsenal , we felt that it was quite different to our method and really offers an orthogonal direction for improvement . We do n't attempt a thorough review of the state-of-the-art in MCMC , which we feel is beyond the scope here , but instead try to focus our discussion on other neural adaptive samplers such as L2HMC and A-NICE-MCMC . > > related again with the state-of-the-art description , the references regarding Adaptive Mixture Metropolis methods are completely missed . If I have properly understood , you also adapt a mixture via variational inference . Please , in Section 4 , consider the different works that considers an adapting mixture proposal for a Metropolis-type algorithm , Thanks for the pointers to these papers . These were mostly new to us and do seem very related . After reading the papers more closely we will try to include them in our references . > > P.Giordani and R. Kohn , \u201c Adaptive independent Metropolis-Hastings by fast estimation of mixtures of normals , \u201d Journal of Computational and Graphical Statistics , vol . 19 , no.2 , pp.243\u2013259 , September 2010 . > > Tran , M.-N. , M. K. Pitt , and R. Kohn . Adaptive Metropolis\u2013Hastings sampling using reversible dependent mixture proposals . Statistics and Computing , 26 , 1\u201321 , 2014 . > > D.Luengo , L. Martino , `` Fully Adaptive Gaussian Mixture Metropolis-Hastings Algorithm '' , IEEE International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ) , Vancouver ( Canada ) , 2013 . > > Roberts , G. O. and J. S. Rosenthal ( 2009 ) . Examples of adaptive MCMC . Journal of Computational and Graphical Statistics 18 , 349\u2013367"}, "2": {"review_id": "r1NJqsRctX-2", "review_text": "This paper proposes an auxiliary variable MCMC scheme involving variational inference for efficient MCMC. Given a target distribution p(x), the authors introduce an auxiliary variable a, and learn conditional distributions p(a|x) and q(a|x) by minimizing the KL divergence between p(x)p(a|x) and q(a)q(x|a), with q(a) something simple (the authors use Gaussian). A MH proposal step involves simulating x givea the current MCMC sample x (from p(a|x), taking a step in A-space, and then returning back to the X space (using q(x|a)). The authors show how to calculate the acceptance probability. I think the idea is nice and useful (I'm surprised people haven't thought of this before), though I think the paper presents this in a less clear way (as an extension of ideas from Agakov and Barber's \"Auxiliary variational method\"). While this is correct and perhaps more general, in my mind it slightly obscures the main idea, as well as the strong ties with variational autoencoders: express a complex distribution as a (learnt) transformation of a simple distribution (this is the actual approach taken in the experiments). The motivation of the approach is that the nonlinear encoding network can transform the complex p(x) into a simpler q(a). For this reason, I think an important baseline is the independent MH sampler from equation 8 (I think this essentially uses a trained VAE generative model as a proposal distribution). The authors talk about how producing independent proposals can be sub-optimal, yet it seems to me that if the encoder and decoder neural networks are powerful enough, this should do a good job. I think excluding this baseline hurts the paper a bit. The proof of correctness while correct is a bit unclear, can perhaps be simplified if you view the MCMC algorithm as operating on an augmented space (x,a,x') with stationary distribution p(x)q(a|x)q(x'|a) (writing writing q for \\tilde(q)). This clearly has the right distribution over x. Each MCMC iteration starts with x and proceeds as follow: 1) Given x, sample a and x' from q(a|x) and q(x'|a) 2) Make a deterministic proposal on the augmented space to swap (x,x'). The acceptance probability is now equation 2. 3) Discard a,x'. In figure 4, the authors use HMC as an \"improved MCMC algorithm\", yet this is not an algorithm that deals with multimodality well. More useful would be to include some tempering algorithm like serial or parallel tempering. While I like the idea, I unfortunately don't think the experiments are very convincing (and the authors barely discuss their results). Other than mixture of Gaussians, HMC (which involves no training) appears to be superior. With some tempering, I expect it to outperform the proposed method for the MoG case Table 2 left: since HMC involves no training, does this mean that, taking training time into account, HMC is 5-6 orders of magnitude more efficient. L?ke I mentioned earlier, these results need more discussion. It would also help to provide absolute training and run times, so the reader can better understand whether the proposed method of ANICE is better. Figure 3: why don't the authors also plot the histogram of values in the auxiliary space, p(a). It would be interesting to see how Gaussian this is (this is what variational inference is trying to achieve). Also, does Figure 3(a) mean that conditioned on x, p(a|x) is basically a delta function? This would suggest that the encoder is basically learning a deterministic transformation to a simpler low-dimensional space? There is some work in this direction in the statistics literature, e.g. \"Variable transformation to obtain geometric ergodicity in the random-walk Metropolis algorithm\" The authors some refers to the distribution of a|x as q(a|x) sometimes (in section 2.1) and sometimes as p(a|x) which is a bit confusing. Figure 2: the labels are wrong.", "rating": "7: Good paper, accept", "reply_text": "> > While I like the idea , I unfortunately do n't think the experiments are very convincing ( and the authors barely discuss their results ) . Other than mixture of Gaussians , HMC ( which involves no training ) appears to be superior . With some tempering , I expect it to outperform the proposed method for the MoG case > > Table 2 left : since HMC involves no training , does this mean that , taking training time into account , HMC is 5-6 orders of magnitude more efficient . Like I mentioned earlier , these results need more discussion . > > It would also help to provide absolute training and run times , so the reader can better understand whether the proposed method of ANICE is better . The reviewer is correct that , presently , if training time is included HMC significantly outperforms not just the method proposed in this paper but all of the learned samplers explored thus far . The size of the difference is dramatic here because the data-sets used are small but as the size of the data-set grows we expect the computational cost of HMC to grow much faster than the learned samplers . This is because HMC will require many gradients of the log-density per iteration and the cost of each of these scales linearly in the size of the data . Whilst calculating acceptance ratios also scales linearly in data-size , it only needs to be performed once per iteration . HMC , on the other hand , will often require 10s or 100s of gradients per iteration . Another point that we 'd emphasise is that although there is an increased computational time for these methods , because they are black-box they potentially save on much more vital human time , HMC being quite tricky to tune . We will update the paper to include absolute training times . To give the reviewer an indication here of the difference in absolute training times ( in seconds ) we provide preliminary figures taken from the last batch of experiments we ran : Ring/ Mog/ Logistic Regression/ ANICE : 120/ 1290 / 3310 AVS : 70 / 2 / 2 L2HMC : 3310/ 2620 / 4590 We will try to expand the discussion of the results , their brevity was primarily a result of space restrictions . > > Figure 3 : why do n't the authors also plot the histogram of values in the auxiliary space , p ( a ) . It would be interesting to see how Gaussian this is ( this is what variational inference is trying to achieve ) . We agree that this might be interesting but felt that in the limited space given it was n't central to our argument . > > Also , does Figure 3 ( a ) mean that conditioned on x , p ( a|x ) is basically a delta function ? This would suggest that the encoder is basically learning a deterministic transformation to a simpler low-dimensional space ? There is some work in this direction in the statistics literature , e.g.In this case , yes the conditional became close to a delta-function though its not clear how often this will be the case . > > '' Variable transformation to obtain geometric ergodicity in the random-walk Metropolis algorithm '' Thanks also for the pointer to this paper . We were not aware of it but it seems highly relevant . We 'll try to add it to our discussion after we study it in more detail . > > The authors some refers to the distribution of a|x as q ( a|x ) sometimes ( in section 2.1 ) and sometimes as p ( a|x ) which is a bit confusing . Thanks this was a mistake and will be corrected . > > Figure 2 : the labels are wrong . Thanks again , we will correct this ."}}