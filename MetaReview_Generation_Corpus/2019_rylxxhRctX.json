{"year": "2019", "forum": "rylxxhRctX", "title": "Coverage and Quality Driven Training of Generative Image Models", "decision": "Reject", "meta_review": "The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciate the contributions so taking the comments into account and resubmit elsewhere is encouraged. ", "reviews": [{"review_id": "rylxxhRctX-0", "review_text": "The paper presents the use of an invertible transformtion layer in addition to the conventional variational autoencoder to map samples from decoder to image space, and shows it improves over both synthesis quality and diversity. The paper is well motivated, and the main motivation is nicely presented in Fig.1, and the main idea clearly shown in Fig.2 in an easy-to-understand manner. Existing works are properly discussed in the context before and after the main method. Convincing results are presented in the experimental section, with ablation tests in Tables 1-3, quantitative comparison in Table 4, and qualitative visual images in Figs.4-5. I incline to my current score after reading the response and other reviews.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and appreciation of our work . We are happy to discuss any questions that you may have during the discussion period ."}, {"review_id": "rylxxhRctX-1", "review_text": " The paper proposes to alleviate two issues with VAEs and GANs Mode covering behaviour of the MLE loss used in VAEs causing blurry image samples despite high likelihood scores (Coverage driven training in the paper) Poor likelihood scores in GAN based models despite great looking amples (Quality driven training in the paper) Both of these issues are well known and have been previously described in the literature (e.g. MLE models: bishop 2006, GANs: arjovsky et. al. 2017 or sonderby et. al. 2017) The main contribution of the paper are described in the Eq (8) augmenting the VAE ELBO (L_C term) with a GAN-discriminator based loss (L_Q term). Combining the fact that the L_Q loss (essentially MLE) minimises KL[data|model] and L_Q discriminator loss used minimises KL[model|data] the authors show that this in theory minimizes a lower bound on the forward and reverse KL divergence i.e. somewhere between mode-covering and mode-seeking behavior As a secondary contribution the authors show that adding a flow based module to the generative model p(x|z) increases both the likelihood and the fidelity of the samples. The experimental section seems sound with extensive results on CIFAR10 and some additional results on STL, LSUN, CelebA and ImageNet. Comments Overall i find the paper well written and easy to understand and the experiments seems sound. My main criticism of the paper is the novelty of the proposed model. Adding a VAE and a GAN has been done before (e.g. Larsen 2016 as the authors cite as well). Outside of generative models the combination of MLE and GAN based losses have been studied in e.g. Super-Resolution (Shi et. al. 2016). In both papers of these papers the GAN based losses are added for exactly the same reasons as provided in this work i.e. to increase the sample fidelity. I\u2019m unsure if adding a flow to the output of the VAE generative model have been done before, however in spirit the approach is quite similar in the ideas in \u201cVariational Lossy Autoencoder\u201d (Chen 2016) or PixelVAEs (Gulrajani 2016) where local covariance is added through conditional factorization. Questions Q1) One criticism of the results is that the authors claim that the model achieves likelihood values competitive with state-of-the-art where the results seems to suggest that a more sound conclusion is that the the likelihoods are better than GANs but worse than MLE models such as PixelCNN++. Similarly for the Inception/FID scores where the model is better than VAEs but most of the time slightly worse than pure GAN models. ? Q2) I find the claim \u201cwe propose a unified training approach that leverages coverage and quality based criteria\u201d a bit overreaching. The loss in Eq(8) is simply the sum of a VAE and a GAN based loss and as such does not provide any unification of the two ? Q3) Related to the previous question The KL-divergence interpretation of the GAN based loss assumes a bayes optimal discriminator. In practice this is usually never achieved why it is not really known what divergence GANs minimize if any (see e.g Fedus et. al. 2017 for a non-divergence minimization view). If the KL-divergenve minimization interpretation does not hold the proposed model is essentially a VAE with an auxiliary GAN loss biasing the model towards mode-seeking training? Q4) There haven\u2019t been many successful GAN usages outside of of the CNN-realm which suggests that a successful GAN is tightly connected to the inductive bias provided by CNNs for images. Have the authors tried there model on something else than images? (I suspect that the GAN based L_Q loss will be hard to apply outside of the realm of images) Overall i find the paper well written and easy to understand and the experiments seems sound. However I think that closely related variants of the main contributions in the paper have been tried elsewhere which somewhat reduces the novelty of the proposed model. Given my comments above I think the paper is marginally below acceptance however I could be be convinced otherwise by e.g. a solid use-case for models like this? Lastly, I'm sorry for the late review (I was called upon late to review the paper) - I hope that you will find the time for a proper rebuttal to my questions. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "[ Q2 ) I find the claim \u201c we propose a unified training approach that leverages coverage and quality based criteria \u201d a bit overreaching . The loss in Eq ( 8 ) is simply the sum of a VAE and a GAN based loss and as such does not provide any unification of the two ? ] Regarding the loss in Eq ( 8 ) : it is indeed a sum , and perhaps not as 'unifying ' as expected . We will tone down the wording in this respect ( using for example \u201c combining \u201d ? ) , to avoid any deception here . To effectively benefit from this joint objective function , and obtain both good samples and likelihood scores , we additionally propose an adapted architecture leveraging inverse-autoregressive flow and top-down sampling ( Kingma , NIPS \u2019 16 ) and invertible NVP ( Dinh , ICLR \u2019 17 ) layers to obtain a non-factorial non-Gaussian decoder . Without this , optimizing Eq ( 8 ) does not lead to models that have both compelling samples and good likelihood and in this sense , our proposed approach unifies the two . [ Q3 ) Related to the previous question The KL-divergence interpretation of the GAN based loss assumes a bayes optimal discriminator . In practice this is usually never achieved why it is not really known what divergence GANs minimize if any ( see e.g Fedus et.al.2017 for a non-divergence minimization view ) . If the KL-divergence minimization interpretation does not hold the proposed model is essentially a VAE with an auxiliary GAN loss biasing the model towards mode-seeking training ? ] Agreed : in practice optimality is never achieved , and worse : there is no way of knowing to what extent . The KL view is an 'idealized ' view of the loss . In practice though , what we have is indeed a mode covering mechanism combined with a mode-seeking training mechanism . Although the quality of the reverse KL approximation is unknown , the GAN literature has shown beyond any doubt that it is a powerful technique to achieve mode-seeking training . [ Q4 ) There haven \u2019 t been many successful GAN usages outside of of the CNN-realm which suggests that a successful GAN is tightly connected to the inductive bias provided by CNNs for images . Have the authors tried there model on something else than images ? ( I suspect that the GAN based L_Q loss will be hard to apply outside of the realm of images ) ] Thanks for your comment . We have not tried it outside of image domains . The inductive bias of convolutional layers indeed seems connected to GAN success , and so it is not clear that L_Q could be easily applied the way we do it in other domains . We will study this further in the context of NLP in the future . For example , ( Wu et al , \u201c Adversarial Neural Machine Translation \u201d ) have explored adversarial training for machine translation . [ Overall i find the paper well written and easy to understand and the experiments seems sound . However I think that closely related variants of the main contributions in the paper have been tried elsewhere which somewhat reduces the novelty of the proposed model . Given my comments above I think the paper is marginally below acceptance however I could be be convinced otherwise by e.g.a solid use-case for models like this ? ] In applications where GANs are typically used ( for instance popular \u2018 photoshop++ \u2019 applications that automatically make people look older , or bald ) our approach could be used to objectively select the model that best covers the full dataset , with little cost in terms of quality . This would improve performance in otherwise mode-dropped regions of the support , and fixes the lack of an objective , reliable measure to guide developpement choices . In applications where MLE models are used because the compression metric is needed , our model would provide samples of strikingly greater quality , though for now at a cost in terms of BPD . We insist also that we are the first to report both BPD and IS+FID measurements on a collection of seven common datasets . We sincerely hope that this provides the basis for more complete evaluation of generative image models in the future ."}, {"review_id": "rylxxhRctX-2", "review_text": "It is well known that optimizing divergences of different directions leads to different learning behaviors - the mode covering behavior of maximum likelihood training and the mode missing behavior of GAN training. This paper makes a good presentation in explaining coverage-driven training and quality-driven training. Techinically, this paper make two contributions. First, extend VAEs by using deterministic invertible transformation layers to map samples from the decoder to the image space. Second, use the loss Eq. (8) to train the generator. However, there are some unclear issues. First, the differences between losses may not fully explain the behavior of GANs [I. J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv:1701.00160, 2017], as also seen from some recent studies. For example, using the sum of two divergences [Chen et al., 2018] does not make a significant improvement. Also shown in Table 1, CQ performs better than VAE, but is inferior to GAN. Using the two-term loss Eq. (8) may not be the key for improvement. Only after adding the additional flow-based layers, CQF outperforms GAN. Therefore, in Table 1, it would be better to include the result of VAE using the additional flow-based layes for ablation study. Second, it is also not clear from the paper that such VAE using the additional flow-based layes is new or not. Third, the results are not as good as the state-of-the-art. In Table 4, SN-GANs perform the best in three out of four cases. Fourth, the model consists of three networks - encoder, decoder and discriminator. Evaluation about the model's inference capability is necessary in addition to showing its generation capability, since it is equipped with a decoder. Some typos: P5: L_Q(p*)+L_Q(p*) ? Table 4: QSF ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "[ Third , the results are not as good as the state-of-the-art . In Table 4 , SN-GANs perform the best in three out of four cases . ] We would like to point out that the fourth measure , BPD , is simply unavailable for GAN methods , since they have a degenerate low-dimensional support in the image space , and lack a mechanism to evaluate the density on the support . In particular held-out train data is likely to be off the low dimensional support , and have an infinite negative log-likelihood . This drawback is fixed in likelihood based models ( including ours ) that have a full support over the image space , and offer exact or approximate likelihood evaluation . Although you are right that our approach does not beat the state of the art IS and FID numbers on the CIFAR-10 and STL datasets , it does give results that are fairly close ( and better in one case ) . This is the contribution of our paper : showing that we can have ( near ) state of the art sample quality and a full support and likelihood evaluation on held-out train data . For our model CQF [ +Residual , +flow , +large D ] ( Ours ) the comparison is as follows : IS CIFAR-10 : ours 8.1 , state-of-the-art 8.2 ( higher is better ) IS STL : ours 8.6 , state-of-the-art 9.1 ( higher is better ) FID CIFAR-10 : ours 18.6 , state-of-the-art 21.7 ( lower is better ) FID STL : ours 52.7 , state-of-the-art 40.1 ( lower is better ) [ Fourth , the model consists of three networks - encoder , decoder and discriminator . Evaluation about the model 's inference capability is necessary in addition to showing its generation capability , since it is equipped with a decoder . ] To evaluate our model \u2019 s inference capability , we will include reconstructions of encoded images in the revision of our paper . As is typical for flexible VAEs ( IAF for instance ) they are indistinguishable from the corresponding ground truth images to the naked eye , and testify outstanding inference performance ."}], "0": {"review_id": "rylxxhRctX-0", "review_text": "The paper presents the use of an invertible transformtion layer in addition to the conventional variational autoencoder to map samples from decoder to image space, and shows it improves over both synthesis quality and diversity. The paper is well motivated, and the main motivation is nicely presented in Fig.1, and the main idea clearly shown in Fig.2 in an easy-to-understand manner. Existing works are properly discussed in the context before and after the main method. Convincing results are presented in the experimental section, with ablation tests in Tables 1-3, quantitative comparison in Table 4, and qualitative visual images in Figs.4-5. I incline to my current score after reading the response and other reviews.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and appreciation of our work . We are happy to discuss any questions that you may have during the discussion period ."}, "1": {"review_id": "rylxxhRctX-1", "review_text": " The paper proposes to alleviate two issues with VAEs and GANs Mode covering behaviour of the MLE loss used in VAEs causing blurry image samples despite high likelihood scores (Coverage driven training in the paper) Poor likelihood scores in GAN based models despite great looking amples (Quality driven training in the paper) Both of these issues are well known and have been previously described in the literature (e.g. MLE models: bishop 2006, GANs: arjovsky et. al. 2017 or sonderby et. al. 2017) The main contribution of the paper are described in the Eq (8) augmenting the VAE ELBO (L_C term) with a GAN-discriminator based loss (L_Q term). Combining the fact that the L_Q loss (essentially MLE) minimises KL[data|model] and L_Q discriminator loss used minimises KL[model|data] the authors show that this in theory minimizes a lower bound on the forward and reverse KL divergence i.e. somewhere between mode-covering and mode-seeking behavior As a secondary contribution the authors show that adding a flow based module to the generative model p(x|z) increases both the likelihood and the fidelity of the samples. The experimental section seems sound with extensive results on CIFAR10 and some additional results on STL, LSUN, CelebA and ImageNet. Comments Overall i find the paper well written and easy to understand and the experiments seems sound. My main criticism of the paper is the novelty of the proposed model. Adding a VAE and a GAN has been done before (e.g. Larsen 2016 as the authors cite as well). Outside of generative models the combination of MLE and GAN based losses have been studied in e.g. Super-Resolution (Shi et. al. 2016). In both papers of these papers the GAN based losses are added for exactly the same reasons as provided in this work i.e. to increase the sample fidelity. I\u2019m unsure if adding a flow to the output of the VAE generative model have been done before, however in spirit the approach is quite similar in the ideas in \u201cVariational Lossy Autoencoder\u201d (Chen 2016) or PixelVAEs (Gulrajani 2016) where local covariance is added through conditional factorization. Questions Q1) One criticism of the results is that the authors claim that the model achieves likelihood values competitive with state-of-the-art where the results seems to suggest that a more sound conclusion is that the the likelihoods are better than GANs but worse than MLE models such as PixelCNN++. Similarly for the Inception/FID scores where the model is better than VAEs but most of the time slightly worse than pure GAN models. ? Q2) I find the claim \u201cwe propose a unified training approach that leverages coverage and quality based criteria\u201d a bit overreaching. The loss in Eq(8) is simply the sum of a VAE and a GAN based loss and as such does not provide any unification of the two ? Q3) Related to the previous question The KL-divergence interpretation of the GAN based loss assumes a bayes optimal discriminator. In practice this is usually never achieved why it is not really known what divergence GANs minimize if any (see e.g Fedus et. al. 2017 for a non-divergence minimization view). If the KL-divergenve minimization interpretation does not hold the proposed model is essentially a VAE with an auxiliary GAN loss biasing the model towards mode-seeking training? Q4) There haven\u2019t been many successful GAN usages outside of of the CNN-realm which suggests that a successful GAN is tightly connected to the inductive bias provided by CNNs for images. Have the authors tried there model on something else than images? (I suspect that the GAN based L_Q loss will be hard to apply outside of the realm of images) Overall i find the paper well written and easy to understand and the experiments seems sound. However I think that closely related variants of the main contributions in the paper have been tried elsewhere which somewhat reduces the novelty of the proposed model. Given my comments above I think the paper is marginally below acceptance however I could be be convinced otherwise by e.g. a solid use-case for models like this? Lastly, I'm sorry for the late review (I was called upon late to review the paper) - I hope that you will find the time for a proper rebuttal to my questions. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "[ Q2 ) I find the claim \u201c we propose a unified training approach that leverages coverage and quality based criteria \u201d a bit overreaching . The loss in Eq ( 8 ) is simply the sum of a VAE and a GAN based loss and as such does not provide any unification of the two ? ] Regarding the loss in Eq ( 8 ) : it is indeed a sum , and perhaps not as 'unifying ' as expected . We will tone down the wording in this respect ( using for example \u201c combining \u201d ? ) , to avoid any deception here . To effectively benefit from this joint objective function , and obtain both good samples and likelihood scores , we additionally propose an adapted architecture leveraging inverse-autoregressive flow and top-down sampling ( Kingma , NIPS \u2019 16 ) and invertible NVP ( Dinh , ICLR \u2019 17 ) layers to obtain a non-factorial non-Gaussian decoder . Without this , optimizing Eq ( 8 ) does not lead to models that have both compelling samples and good likelihood and in this sense , our proposed approach unifies the two . [ Q3 ) Related to the previous question The KL-divergence interpretation of the GAN based loss assumes a bayes optimal discriminator . In practice this is usually never achieved why it is not really known what divergence GANs minimize if any ( see e.g Fedus et.al.2017 for a non-divergence minimization view ) . If the KL-divergence minimization interpretation does not hold the proposed model is essentially a VAE with an auxiliary GAN loss biasing the model towards mode-seeking training ? ] Agreed : in practice optimality is never achieved , and worse : there is no way of knowing to what extent . The KL view is an 'idealized ' view of the loss . In practice though , what we have is indeed a mode covering mechanism combined with a mode-seeking training mechanism . Although the quality of the reverse KL approximation is unknown , the GAN literature has shown beyond any doubt that it is a powerful technique to achieve mode-seeking training . [ Q4 ) There haven \u2019 t been many successful GAN usages outside of of the CNN-realm which suggests that a successful GAN is tightly connected to the inductive bias provided by CNNs for images . Have the authors tried there model on something else than images ? ( I suspect that the GAN based L_Q loss will be hard to apply outside of the realm of images ) ] Thanks for your comment . We have not tried it outside of image domains . The inductive bias of convolutional layers indeed seems connected to GAN success , and so it is not clear that L_Q could be easily applied the way we do it in other domains . We will study this further in the context of NLP in the future . For example , ( Wu et al , \u201c Adversarial Neural Machine Translation \u201d ) have explored adversarial training for machine translation . [ Overall i find the paper well written and easy to understand and the experiments seems sound . However I think that closely related variants of the main contributions in the paper have been tried elsewhere which somewhat reduces the novelty of the proposed model . Given my comments above I think the paper is marginally below acceptance however I could be be convinced otherwise by e.g.a solid use-case for models like this ? ] In applications where GANs are typically used ( for instance popular \u2018 photoshop++ \u2019 applications that automatically make people look older , or bald ) our approach could be used to objectively select the model that best covers the full dataset , with little cost in terms of quality . This would improve performance in otherwise mode-dropped regions of the support , and fixes the lack of an objective , reliable measure to guide developpement choices . In applications where MLE models are used because the compression metric is needed , our model would provide samples of strikingly greater quality , though for now at a cost in terms of BPD . We insist also that we are the first to report both BPD and IS+FID measurements on a collection of seven common datasets . We sincerely hope that this provides the basis for more complete evaluation of generative image models in the future ."}, "2": {"review_id": "rylxxhRctX-2", "review_text": "It is well known that optimizing divergences of different directions leads to different learning behaviors - the mode covering behavior of maximum likelihood training and the mode missing behavior of GAN training. This paper makes a good presentation in explaining coverage-driven training and quality-driven training. Techinically, this paper make two contributions. First, extend VAEs by using deterministic invertible transformation layers to map samples from the decoder to the image space. Second, use the loss Eq. (8) to train the generator. However, there are some unclear issues. First, the differences between losses may not fully explain the behavior of GANs [I. J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv:1701.00160, 2017], as also seen from some recent studies. For example, using the sum of two divergences [Chen et al., 2018] does not make a significant improvement. Also shown in Table 1, CQ performs better than VAE, but is inferior to GAN. Using the two-term loss Eq. (8) may not be the key for improvement. Only after adding the additional flow-based layers, CQF outperforms GAN. Therefore, in Table 1, it would be better to include the result of VAE using the additional flow-based layes for ablation study. Second, it is also not clear from the paper that such VAE using the additional flow-based layes is new or not. Third, the results are not as good as the state-of-the-art. In Table 4, SN-GANs perform the best in three out of four cases. Fourth, the model consists of three networks - encoder, decoder and discriminator. Evaluation about the model's inference capability is necessary in addition to showing its generation capability, since it is equipped with a decoder. Some typos: P5: L_Q(p*)+L_Q(p*) ? Table 4: QSF ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "[ Third , the results are not as good as the state-of-the-art . In Table 4 , SN-GANs perform the best in three out of four cases . ] We would like to point out that the fourth measure , BPD , is simply unavailable for GAN methods , since they have a degenerate low-dimensional support in the image space , and lack a mechanism to evaluate the density on the support . In particular held-out train data is likely to be off the low dimensional support , and have an infinite negative log-likelihood . This drawback is fixed in likelihood based models ( including ours ) that have a full support over the image space , and offer exact or approximate likelihood evaluation . Although you are right that our approach does not beat the state of the art IS and FID numbers on the CIFAR-10 and STL datasets , it does give results that are fairly close ( and better in one case ) . This is the contribution of our paper : showing that we can have ( near ) state of the art sample quality and a full support and likelihood evaluation on held-out train data . For our model CQF [ +Residual , +flow , +large D ] ( Ours ) the comparison is as follows : IS CIFAR-10 : ours 8.1 , state-of-the-art 8.2 ( higher is better ) IS STL : ours 8.6 , state-of-the-art 9.1 ( higher is better ) FID CIFAR-10 : ours 18.6 , state-of-the-art 21.7 ( lower is better ) FID STL : ours 52.7 , state-of-the-art 40.1 ( lower is better ) [ Fourth , the model consists of three networks - encoder , decoder and discriminator . Evaluation about the model 's inference capability is necessary in addition to showing its generation capability , since it is equipped with a decoder . ] To evaluate our model \u2019 s inference capability , we will include reconstructions of encoded images in the revision of our paper . As is typical for flexible VAEs ( IAF for instance ) they are indistinguishable from the corresponding ground truth images to the naked eye , and testify outstanding inference performance ."}}