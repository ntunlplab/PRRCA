{"year": "2019", "forum": "rklz9iAcKQ", "title": "Deep Graph Infomax", "decision": "Accept (Poster)", "meta_review": "Because of strong support from two of the reviewers I am recommending accepting this paper. However, I believe reviewer 1's concerns should be taken seriously. Although I disagree with the reviewer that a general \"framework\" method is a bad thing, I agree with them that additional experiments would be valuable.", "reviews": [{"review_id": "rklz9iAcKQ-0", "review_text": "This paper adapts the Deep Informax (DIM; Hjelm et al. 2018) method, which was used on image data, into the graph domain. The architecture of the neural network and the learning cost function are given by figure 1 and eq.(1), respectively. The idea is to maximize the mutual information between a local representation (of a \"patch\" defined by graph adjacency) and a global representation (of the entire graph), so those different local patches are encouraged to carry some shared global information. This is in contrast to most unsupervised graph encoders, where the objective is to fit the random walk similarities (node adjacency on the graph). In an unsupervised learning scenario, where the graph structure and node features are given, the authors achieved state-of-the-art performance on transductive and inductive node classification tasks, in some cases even better than supervised baselines. The paper is well written. I recommend acceptance and have the following concerns. Main comment 1 The title suggests that there are some information theory contents. However, section 3 does not include much information theory. Rather, the author(s) directly give eq.(1) with pointers to references and informal discussions. This is not so helpful. It is not straightforward for the reader to relate eq.(1) with the definition of mutual information. Ideally, before eq.(1) there should be one or two equations (with text) to introduce the Jesen-Shannon MI estimation and information theoretic bounds etc. Overall, due to this, the contribution is mainly on adapting the DIM method info the graph domain. Although the experimental results are good, there is not much theoretical insight or \"recreative\" introduction of the DIM method from the authors' perspectives. This is the main reason for that it is not a strong accept. Main comment 2 A motivation of the proposition is to \"not rely on random walks\", or graph node adjacency. Notice that random walks can be intuitively regarded as higher order node adjacency. However, the encoder, which is based on GCN, does rely on the adjacency matrix, as the convolution is done in local neighborhoods (that can also be defined based on random-walk similarities). The authors are therefore suggested to make it clear in related places that, it is the cost function which is not based on node adjacency, although the neural network structure does rely on it. As a related question, in the inductive experiments, in the mini-batch of 256 nodes randomly selected, or selected by a local patch of the graph which is connected or nearby? If it is the latter case, the cost function does rely on random-walk similarities, as the summary vector will be a local patch average. Questions: -The summary vector is the average of all node features. On large graphs, the average may carry less information as compared to small graphs. It can be observed that on Pubmed and Reddit, the performance improvement is not as high as the other small graphs. Could you comment on this? -In the baseline \"DeepWalk+features\", are the two different types of features directly concatenated? -Is it straightforward to apply DGI to link prediction tasks? -It that a concern that the random corruption function will cause a high variance of the gradient? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the very careful review and kind words about our contributions . Regarding your comment about our information-theoretic contributions , please see our global comment on mutual information and the JSD . We reorganised Sections 3.2 / 3.3 to include more concrete theoretical motivation from mutual information maximisation , and separate this from the specifics that are important for implementation . You make an exceptionally good point regarding our method \u2019 s reliance on random walks - thanks ! It is , in fact , our main claim that combining random-walk * objectives * with GCN-like encoders is potentially unsuitable ( given that the GCN already encodes the \u201c random-walk \u201d information within its structural inductive biases ) . We have appropriately modified our abstract to reflect this intention . To answer your remaining questions : - The minibatch of 256 nodes for Reddit is randomly selected , and therefore the cost function does not rely on random-walk similarities in this case . - Regarding the averaging readout : this is a great point , and we expect that the performance on larger graphs will decrease somewhat , especially when using averaging as a readout function , since it is known that the quality of graph-level embeddings degrades as the number of nodes increases when using simple averaging . That said , we expect this problem could be alleviated by applying more sophisticated set2vec and/or pooling approaches for the readout function , and we mention this point in the revised paper -- -immediately after the averaging is introduced in Section 4.2 . Moreover , in this particular case , the smaller performance improvement on the Reddit data is also simply due to the fact that most GCN approaches are already in the 90+ % F1 range , and therefore there is limited room for improvement . - The \u201c DeepWalk+features \u201d baseline directly concatenates the two kinds of features , as was done in all prior work utilising this baseline ( e.g.Hamilton et al. , NIPS 2017 ) . - We note that we specifically designed the model with node classification tasks in mind . However , in principle , the generated embeddings could be used for link prediction , as with node2vec embeddings , etc . That said , we expect that strong performance on link prediction could require minor modifications ( e.g. , tweaking the negative sampling function ) and we plan to investigate this in future work . - Regarding your concern about high variance of the gradient , we haven \u2019 t found any issues regarding learning stability -- -as long as an appropriate choice of learning rate is made . We thank you once again for your review , which has definitely helped make our paper \u2019 s contributions stronger !"}, {"review_id": "rklz9iAcKQ-1", "review_text": "This paper describes an approach for unsupervised learning of node features on a graph (with known structure), so that learned local representations represent community information that has high mutual info with a graph-level summary. The general idea is they apply InfoMax to graphs via graph convolutional networks (GCN), and report impressive results, including rivaling supervised learning methods for node classification. The 3 experiments are on paper topic classification, social network modeling, and protein classification. The idea of using InfoMax with GCNs for unsupervised node learning is clever and timely, the technical contribution is solid, the experiments are executed well, and the paper is clear and easy to read.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you very much for the extremely kind review , and we are very glad you enjoyed the paper ! We have made further updates to the paper -- -some details of which are outlined in our global comment above ."}, {"review_id": "rklz9iAcKQ-2", "review_text": "This paper proposes an unsupervised approach to learning node representations. The basic steps are: (1) use an encoder E to learn node vectors, (2) use a readout function R to summarize node vectors into the graph vector, (3) use a scoring function D to score how much the node vectors are aligned with the graph vector, and (4) maximize the scores for the given graph meanwhile minimize the those from the negative distribution. I feel that the idea is interesting; however, the paper is less well written and the realization of the idea has drawbacks as well. 1. Presentation of Section 3.2 can be improved. The proposed approach becomes clear only toward the end. 2. Naming and wording is misleading. The title and the whole paper use the wording \"mutual information\", whereas in reality, the loss function is a cross entropy. 3. In equation (1), it is unclear why the authors take expectation with respect to the distribution of graphs before summing the scores for one particular graph. Should the order of the expectation and summation be swapped? 4. The proposal is more like a framework than a specific method. The encoder and the negative distribution need to be separately designed for different graphs. Good things about the proposal: 5. The downstream classification results are quite comparable to those of supervised methods (except for the PPI data). 6. The learned node representations possess a clear clustering structure (Figure 3). Minor comments: 7. In the third paragraph of section 4.3, the authors state that \"... for the GCN model in the fully supervised setting\". GCN should be a semi-supervised method rather than a fully-supervised one. 8. In the last paragraph of section 4.3, what is a \"randomly initialized graph convolutional network\" and how is it different from the proposal? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Firstly , thanks so much for your thorough review ! Towards your comment about Section 3.2 and Equation 1 , please see our global comment on mutual information and the JSD . We reorganised Sections 3.2 / 3.3 to include more concrete theoretical motivation from mutual information maximisation , and separate this from the specifics that are important for implementation . We agree with your comment that the GCN is in fact used in a semi-supervised setting ( as not all nodes are labelled ) . What we referred to is that the learning objective is fully supervised ( solely cross-entropy on the training nodes \u2019 labels ) , and have revised the paper accordingly . We acknowledge your comment about our method having the traits of a framework , but claim that such features are a consequence of the current state of the art in graph neural networks , rather than any limitations of our methodology . Namely : - Similar to other recently proposed GCN methods , such as the DiffPool algorithm ( Ying et al. , NIPS 2018 ) , we indeed are agnostic to the choice of the GCN layer . This is because graph convolutional networks are a very active area of research and we do n't currently have a \u201c catch-all \u201d layer for all possible scenarios ( e.g.transductive vs. inductive ) . - The fact that different high-level architectures are used is normal , and constitutes hyperparameter optimisation and/or relating the work to previous successful architectures . - Our negative distribution choice is , in fact , mostly uniform . We \u2019 d like to use different input graphs as negative examples ( as DIM does ) , but this is only possible ( with a limited pool of examples ) for PPI . In all other case we use node-wise shuffling , which was demonstrably robust -- -and we also motivate this robustness with further studies in Appendix C. A randomly initialised graph convolutional network is basically the setting in which we set the number of training epochs to zero -- -i.e.we start with weights initialised according to Xavier initialisation , and then immediately proceed to use this as our encoder rather than performing any unsupervised training of the encoder . We thank you once again for your review , which has definitely helped make our paper \u2019 s contributions stronger !"}], "0": {"review_id": "rklz9iAcKQ-0", "review_text": "This paper adapts the Deep Informax (DIM; Hjelm et al. 2018) method, which was used on image data, into the graph domain. The architecture of the neural network and the learning cost function are given by figure 1 and eq.(1), respectively. The idea is to maximize the mutual information between a local representation (of a \"patch\" defined by graph adjacency) and a global representation (of the entire graph), so those different local patches are encouraged to carry some shared global information. This is in contrast to most unsupervised graph encoders, where the objective is to fit the random walk similarities (node adjacency on the graph). In an unsupervised learning scenario, where the graph structure and node features are given, the authors achieved state-of-the-art performance on transductive and inductive node classification tasks, in some cases even better than supervised baselines. The paper is well written. I recommend acceptance and have the following concerns. Main comment 1 The title suggests that there are some information theory contents. However, section 3 does not include much information theory. Rather, the author(s) directly give eq.(1) with pointers to references and informal discussions. This is not so helpful. It is not straightforward for the reader to relate eq.(1) with the definition of mutual information. Ideally, before eq.(1) there should be one or two equations (with text) to introduce the Jesen-Shannon MI estimation and information theoretic bounds etc. Overall, due to this, the contribution is mainly on adapting the DIM method info the graph domain. Although the experimental results are good, there is not much theoretical insight or \"recreative\" introduction of the DIM method from the authors' perspectives. This is the main reason for that it is not a strong accept. Main comment 2 A motivation of the proposition is to \"not rely on random walks\", or graph node adjacency. Notice that random walks can be intuitively regarded as higher order node adjacency. However, the encoder, which is based on GCN, does rely on the adjacency matrix, as the convolution is done in local neighborhoods (that can also be defined based on random-walk similarities). The authors are therefore suggested to make it clear in related places that, it is the cost function which is not based on node adjacency, although the neural network structure does rely on it. As a related question, in the inductive experiments, in the mini-batch of 256 nodes randomly selected, or selected by a local patch of the graph which is connected or nearby? If it is the latter case, the cost function does rely on random-walk similarities, as the summary vector will be a local patch average. Questions: -The summary vector is the average of all node features. On large graphs, the average may carry less information as compared to small graphs. It can be observed that on Pubmed and Reddit, the performance improvement is not as high as the other small graphs. Could you comment on this? -In the baseline \"DeepWalk+features\", are the two different types of features directly concatenated? -Is it straightforward to apply DGI to link prediction tasks? -It that a concern that the random corruption function will cause a high variance of the gradient? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the very careful review and kind words about our contributions . Regarding your comment about our information-theoretic contributions , please see our global comment on mutual information and the JSD . We reorganised Sections 3.2 / 3.3 to include more concrete theoretical motivation from mutual information maximisation , and separate this from the specifics that are important for implementation . You make an exceptionally good point regarding our method \u2019 s reliance on random walks - thanks ! It is , in fact , our main claim that combining random-walk * objectives * with GCN-like encoders is potentially unsuitable ( given that the GCN already encodes the \u201c random-walk \u201d information within its structural inductive biases ) . We have appropriately modified our abstract to reflect this intention . To answer your remaining questions : - The minibatch of 256 nodes for Reddit is randomly selected , and therefore the cost function does not rely on random-walk similarities in this case . - Regarding the averaging readout : this is a great point , and we expect that the performance on larger graphs will decrease somewhat , especially when using averaging as a readout function , since it is known that the quality of graph-level embeddings degrades as the number of nodes increases when using simple averaging . That said , we expect this problem could be alleviated by applying more sophisticated set2vec and/or pooling approaches for the readout function , and we mention this point in the revised paper -- -immediately after the averaging is introduced in Section 4.2 . Moreover , in this particular case , the smaller performance improvement on the Reddit data is also simply due to the fact that most GCN approaches are already in the 90+ % F1 range , and therefore there is limited room for improvement . - The \u201c DeepWalk+features \u201d baseline directly concatenates the two kinds of features , as was done in all prior work utilising this baseline ( e.g.Hamilton et al. , NIPS 2017 ) . - We note that we specifically designed the model with node classification tasks in mind . However , in principle , the generated embeddings could be used for link prediction , as with node2vec embeddings , etc . That said , we expect that strong performance on link prediction could require minor modifications ( e.g. , tweaking the negative sampling function ) and we plan to investigate this in future work . - Regarding your concern about high variance of the gradient , we haven \u2019 t found any issues regarding learning stability -- -as long as an appropriate choice of learning rate is made . We thank you once again for your review , which has definitely helped make our paper \u2019 s contributions stronger !"}, "1": {"review_id": "rklz9iAcKQ-1", "review_text": "This paper describes an approach for unsupervised learning of node features on a graph (with known structure), so that learned local representations represent community information that has high mutual info with a graph-level summary. The general idea is they apply InfoMax to graphs via graph convolutional networks (GCN), and report impressive results, including rivaling supervised learning methods for node classification. The 3 experiments are on paper topic classification, social network modeling, and protein classification. The idea of using InfoMax with GCNs for unsupervised node learning is clever and timely, the technical contribution is solid, the experiments are executed well, and the paper is clear and easy to read.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you very much for the extremely kind review , and we are very glad you enjoyed the paper ! We have made further updates to the paper -- -some details of which are outlined in our global comment above ."}, "2": {"review_id": "rklz9iAcKQ-2", "review_text": "This paper proposes an unsupervised approach to learning node representations. The basic steps are: (1) use an encoder E to learn node vectors, (2) use a readout function R to summarize node vectors into the graph vector, (3) use a scoring function D to score how much the node vectors are aligned with the graph vector, and (4) maximize the scores for the given graph meanwhile minimize the those from the negative distribution. I feel that the idea is interesting; however, the paper is less well written and the realization of the idea has drawbacks as well. 1. Presentation of Section 3.2 can be improved. The proposed approach becomes clear only toward the end. 2. Naming and wording is misleading. The title and the whole paper use the wording \"mutual information\", whereas in reality, the loss function is a cross entropy. 3. In equation (1), it is unclear why the authors take expectation with respect to the distribution of graphs before summing the scores for one particular graph. Should the order of the expectation and summation be swapped? 4. The proposal is more like a framework than a specific method. The encoder and the negative distribution need to be separately designed for different graphs. Good things about the proposal: 5. The downstream classification results are quite comparable to those of supervised methods (except for the PPI data). 6. The learned node representations possess a clear clustering structure (Figure 3). Minor comments: 7. In the third paragraph of section 4.3, the authors state that \"... for the GCN model in the fully supervised setting\". GCN should be a semi-supervised method rather than a fully-supervised one. 8. In the last paragraph of section 4.3, what is a \"randomly initialized graph convolutional network\" and how is it different from the proposal? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Firstly , thanks so much for your thorough review ! Towards your comment about Section 3.2 and Equation 1 , please see our global comment on mutual information and the JSD . We reorganised Sections 3.2 / 3.3 to include more concrete theoretical motivation from mutual information maximisation , and separate this from the specifics that are important for implementation . We agree with your comment that the GCN is in fact used in a semi-supervised setting ( as not all nodes are labelled ) . What we referred to is that the learning objective is fully supervised ( solely cross-entropy on the training nodes \u2019 labels ) , and have revised the paper accordingly . We acknowledge your comment about our method having the traits of a framework , but claim that such features are a consequence of the current state of the art in graph neural networks , rather than any limitations of our methodology . Namely : - Similar to other recently proposed GCN methods , such as the DiffPool algorithm ( Ying et al. , NIPS 2018 ) , we indeed are agnostic to the choice of the GCN layer . This is because graph convolutional networks are a very active area of research and we do n't currently have a \u201c catch-all \u201d layer for all possible scenarios ( e.g.transductive vs. inductive ) . - The fact that different high-level architectures are used is normal , and constitutes hyperparameter optimisation and/or relating the work to previous successful architectures . - Our negative distribution choice is , in fact , mostly uniform . We \u2019 d like to use different input graphs as negative examples ( as DIM does ) , but this is only possible ( with a limited pool of examples ) for PPI . In all other case we use node-wise shuffling , which was demonstrably robust -- -and we also motivate this robustness with further studies in Appendix C. A randomly initialised graph convolutional network is basically the setting in which we set the number of training epochs to zero -- -i.e.we start with weights initialised according to Xavier initialisation , and then immediately proceed to use this as our encoder rather than performing any unsupervised training of the encoder . We thank you once again for your review , which has definitely helped make our paper \u2019 s contributions stronger !"}}