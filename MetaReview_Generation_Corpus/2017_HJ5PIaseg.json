{"year": "2017", "forum": "HJ5PIaseg", "title": "Towards an automatic Turing test: Learning to evaluate dialogue responses", "decision": "Invite to Workshop Track", "meta_review": "Noting the authors' concern about one of the reviewers, I read the paper myself and offer my own brief review.\n \n Evaluation is an extremely important question that does not get enough attention in the machine learning community, so the authors' effort is welcomed. The task that the authors are trying to evaluate is especially hard; in fact, it is not even clear to me how humans make these judgments. The low kappa scores on some of the non-\"overall\" dimensions, and only moderate agreement on \"overall,\" are quite worrisome. What makes a good \"chat\" dialogue? The authors seem not to have qualitatively grappled with this key question, rather defining it empirically as \"whatever our human judges think it is.\" This is, I think the deepest flaw of the work; the authors are rushing to automate evaluation without taking the time to ponder what good performance actually is. \n \n That aside, I think the idea of automatic evaluation as a modeling problem is worth studying. The authors note that this has been done for other problems, such as machine translation. They give only a cursory discussion of this prior work, however, and miss the seminal reference, \"Regression for sentence-level MT evaluation with pseudo references,\" Albrecht, Joshua, and Rebecca Hwa, ACL 2007.\n \n The paper would be much stronger with some robustness analysis; does the quality of the evaluation hold up if the design decisions are made differently, if less data are used to estimate the evaluation model, etc.? How does it hold up across datasets, and across different types of dialogue systems? As a methodological note, there are a lot of significance tests here and no mention of any attempt to correct for this (e.g., Bonferroni, FDR, etc.).\n \n As interesting as the ideas here are, I can't see the dialogue community rushing to adopt this approach to evaluation based on the findings in this paper. I do think that the ideas it presents should be hashed out in a public forum sooner rather than later, and therefore recommend it as one of a few papers to be presented at the workshop.", "reviews": [{"review_id": "HJ5PIaseg-0", "review_text": "This paper addresses the issue of how to evaluate automatic dialogue responses. This is an important issue because current practice to automatically evaluate (e.g. BLEU, based on N-gram overlap, etc.) is NOT correlated well with the desired quality (i.e. human annotation). The proposed approach is based on the use of an LSTM encoding of dialogue context, reference response and model response with appropriate scoring, with the essence of training one dialogue model to evaluate another model. However, the proposed solution depends on a reasonably good dialogue model to begin with, which is not guaranteed, rendering the new metric possibly meaningless. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their comments . We are glad the reviewer agrees that we are addressing an important problem . However , we strongly oppose the only point of criticism offered by the reviewer . The reviewer states : \u201c However , the proposed solution depends on a reasonably good dialogue model to begin with , which is not guaranteed , rendering the new metric possibly meaningless. \u201d First of all , the statement that our solution \u201c depends on a good dialogue model \u201d is false -- as we show in the experiments , ADEM works almost equally well using tweet2vec , which is a freely available sentence embedding method . Second , while our evaluation procedure does require a method of producing embeddings for utterances ( of which a dialogue model is only one example ) , we strongly maintain that this does not \u201c render the metric possibly meaningless \u201d . In fact , we maintain it is not even a weakness of the paper , for the following reasons : 1 ) We conduct our evaluations on a dataset ( the Twitter Corpus ) that has seen a huge amount of use in the literature ( e.g.Ritter et al . ( 2011 ) , Sordoni et al . ( 2015 ) , 2 papers by Serban et al . ( 2016 ) , Li et al . ( 2015 ) , Li et al . ( 2016 ) , etc .. ) . It is obvious that , in this setting , there are an abundance of dialogue models ( and embedding methods ) , and we simply use one such published model ( VHRED ) for our procedure . Since we show strong correlation with human judgement for Twitter , our model is clearly useful for evaluating these models ( and future models that will be built on this dataset ) . Thus , the statement that our metric is \u201c meaningless \u201d is unfounded . 2 ) More generally , the entire point of our evaluation procedure is to evaluate dialogue models that are * data-driven * . If there is not enough data in some domain to train a dialogue model ( which is what the reviewer is implying when they say a dialogue model may not be available ) , then there is no point to have an evaluation procedure for this domain ! The only exception to this is if someone wants to craft a * rule-based * system in a domain that both ( a ) has very little data , and ( b ) is in a highly specialized area that is different from existing datasets , such that no transfer learning can be used . The fact that our evaluation procedure would not work in this setting is hardly a limitation ( we do not claim our evaluation model would work in this setting , but we will clarify this in the paper ) . Given that we examine our method using a range of experiments in a very popular domain , showing that our method is a strong first step towards addressing an important open problem ( as admitted by the reviewer ) , we find the claim that our metric is \u201c possibly meaningless \u201d to be without basis . If the reviewer has specific concerns about the evaluation procedure in the paper we would be happy to address them ."}, {"review_id": "HJ5PIaseg-1", "review_text": "This paper propose a new evaluation metric for dialogue systems, and show it has a higher correlation with human annotation. I agree the MT based metrics like BLEU are too simple to capture enough semantic information, but the metric proposed in this paper seems to be too compliciated to explain. On the other hand, we could also use equation 1 as a retrieval based dialogue system. So what is suggested in this paper is basically to train one dialogue model to evaluate another model. Then, the high-level question is why we should trust this model? This question is also relevant to the last item of my detail comments. Detail comments: - How to justify what is captured/evaluated by this metric? In terms of BLEU, we know it actually capture n-gram overlap. But for this model, I guess it is hard to say what is captured. If this is true, then it is also difficult to answer the question like: will the data dependence be a problem? - why not build model incrementally? As shown in equation (1), this metric uses both context and reference to compute a score. Is it possible to show the score function using only reference? It will guarantee this metric use the same information source as BLEU or ROUGE. - Another question about equation (1), is it possible to design the metric to be a nonlinear function. Since from what I can tell, the comparison between BLEU (or ROUGE) and the new metric in Figure 3 is much like a comparison between the exponential scale and the linear scale. - I found the two reasons in section 5.2 are not convincing if we put them together. Based on these two reasons, I would like to see the correlation with average score. A more reasonable way is to show the results both with and without averaging. - In table 6, it looks like the metric favors the short responses. If that is true, this metric basically does the opposite of BLEU, since BLEU will panelize short sentences. On the other hand, human annotators also tends to give short respones high scores, since long sentences will have a higher chance to contain some irrelevant words. Can we eliminate the length factor during the annotation? Otherwise, it is not surprise that the correlation.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their comments . We respond to the specific questions and criticisms raised by the reviewer below . Q : the metric proposed in this paper seems to be too compliciated to explain . A : By \u201c too complicated to explain \u201d , we assume the reviewer means the method is not as interpretable as scores such as BLEU ( we maintain that the method itself is not hard to explain : it is simply a linear combination of utterance embeddings , as detailed in Section 4 ) . It is certainly true that a learned metric with neural networks is less interpretable than one that is composed of simple statistics ( such as BLEU ) . But , conversely , such a learned metric is far more powerful than metrics such as BLEU . In fact , interpretability of an evaluation metric is only useful so long as the metric * actually works * ; yet it has been demonstrated in Liu et al . ( 2016 ) ( and further demonstrated in our paper ) that BLEU and other word-overlap variants * do not work * for evaluating the quality of dialogue responses . Thus , in this paper , we make a first step towards a metric obtaining correlation with human judgements by leveraging neural networks . We do not claim it should be the only metric used to evaluate dialogue responses , but that it should be used in addition to existing ( more interpretable ) metrics . We will add the above argument to the paper . The idea of using learned objective functions has proved enormously successful in the literature -- generative adversarial networks ( GANs ) are in fact generative models in which the generator is trained with a learned objective function . Although the learned objective ( discriminator ) is not interpretable , GANs are among the state-of-the-art generative models . Since these approaches have been successful in a variety of domains , it is natural to explore similar approaches for learning dialogue evaluation metrics , which is an open problem . Q : On the other hand , we could also use equation 1 as a retrieval based dialogue system . So what is suggested in this paper is basically to train one dialogue model to evaluate another model . Then , the high-level question is why we should trust this model ? A : To be clear , the evaluation model is * * not * * a dialogue retrieval model . The fundamental difference between a retrieval model and our proposed evaluation model , is that the evaluation model has access to the * reference response * . In other words , the evaluation model is conditioned on one reference response generated by humans . As such , the evaluation model has the ability to compare a model \u2019 s response to a response that is known to be good , making its job significantly easier compared to a retrieval model . One could also easily extend our framework to incorporate the use of multiple reference responses if they are available , which would be considered \u2018 cheating \u2019 for retrieval models . We will clarify this in the paper . Why should we trust this model ? Unlike existing automatic metrics for dialogue response evaluation , it correlates strongly with human judgements . We believe this is sufficient to add the ADEM score to the scores that are normally reported for dialogue evaluation . Q : How to justify what is captured/evaluated by this metric ? will the data dependence be a problem ? A : We respond to the first question above ( re : interpretability ) . To determine whether data dependence is a problem , this must be done empirically . We believe our generalization results in Table 5 suggest that data dependence is unlikely in the Twitter domain ( which is the domain that the majority of conversational models in the literature have been trained on -- e.g.Ritter et al . ( 2011 ) , Sordoni et al . ( 2015 ) , Serban et al . ( 2016 ) , etc . ) . We leave the investigation of other domains to future work . Q : why not build model incrementally ? A : This is a good idea , and we thank the reviewer for their suggestion -- we will report the results for each component of the model in an updated version of the paper . Q : Another question about equation ( 1 ) , is it possible to design the metric to be a nonlinear function . Since from what I can tell , the comparison between BLEU ( or ROUGE ) and the new metric in Figure 3 is much like a comparison between the exponential scale and the linear scale . A : One could indeed use a nonlinear function in equation ( 1 ) , but this has nothing to do with Figure 3 . Figure 3 simply shows an ( automatically scaled ) scatterplot of metric scores against human scores , along with a line of best fit . Note that the correlation scores reported in the paper ( Spearman , Pearson ) are independent of the scaling of the metrics -- thus , our better performance is not a matter of scaling , but is because ADEM is better at predicting human response scores than word-overlap metrics . Q : I found the two reasons in section 5.2 are not convincing if we put them together . Based on these two reasons , I would like to see the correlation with average score . A : Unfortunately , we can not compute the correlation with the average score for our dataset , because the vast majority of questions were only evaluated by one human annotator . We did this because we had a fixed financial budget , and wanted to collect the maximum number of examples under this budget , since a human score on a new response is more informative than on a response that has already been evaluated . Since there is only one score per response , this will result in higher variance , and lower correlations . Q : In table 6 , it looks like the metric favors the short responses . human annotators also tends to give short responses high scores , since long sentences will have a higher chance to contain some irrelevant words . Can we eliminate the length factor during the annotation ? A : Yes , humans appear to favour shorter responses , which is actually not desirable for a learned evaluation model . We do not currently analyze this in the paper , making the implicit assumption that the humans scoring the responses are always correct . But we will work to quantify the role of the length factor in the human score predictions , and will add this to an updated version of the paper ( and derive methods to correct it if necessary ) ."}, {"review_id": "HJ5PIaseg-2", "review_text": " Overall the paper address an important problem: how to evaluate more appropriately automatic dialogue responses given the fact that current practice to automatically evaluate (BLEU, METEOR, ...) is often insufficient and sometimes misleading. The proposed approach using an LSTM-based encoding of dialogue context, reference response and model response(s) that are then scored in a linearly transformed space. While the overall approach is simple it is also quite intuitiv and allows end-to-end training. As the authors rightly argue simplicity is a feature both for interpretation as well as for speed. The experimental section reports on quite a range of experiments that seem fine to me and aim to convince the reader about the applicability of the approach. As mentioned also by others more insights from the experiments would have been great. I mentioned an in-depth failure case analysis and I would also suggest to go beyond the current dataset to really show generalizability of the proposed approach. In my opinion the paper is somewhat weaker on that front that it should have been. Overall I like the ideas put forward and the approach seems sensible though and the paper can thus be accepted. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their comments . The recommendations made by the reviewer are good ones -- we will certainly update the paper with a more in-depth failure analysis , as indeed this will show where the model performs poorly . Testing the model in other domains will require additional annotated data of response quality for each domain , which is both expensive and time consuming -- thus we can not guarantee that this will be completed by the deadline ."}], "0": {"review_id": "HJ5PIaseg-0", "review_text": "This paper addresses the issue of how to evaluate automatic dialogue responses. This is an important issue because current practice to automatically evaluate (e.g. BLEU, based on N-gram overlap, etc.) is NOT correlated well with the desired quality (i.e. human annotation). The proposed approach is based on the use of an LSTM encoding of dialogue context, reference response and model response with appropriate scoring, with the essence of training one dialogue model to evaluate another model. However, the proposed solution depends on a reasonably good dialogue model to begin with, which is not guaranteed, rendering the new metric possibly meaningless. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their comments . We are glad the reviewer agrees that we are addressing an important problem . However , we strongly oppose the only point of criticism offered by the reviewer . The reviewer states : \u201c However , the proposed solution depends on a reasonably good dialogue model to begin with , which is not guaranteed , rendering the new metric possibly meaningless. \u201d First of all , the statement that our solution \u201c depends on a good dialogue model \u201d is false -- as we show in the experiments , ADEM works almost equally well using tweet2vec , which is a freely available sentence embedding method . Second , while our evaluation procedure does require a method of producing embeddings for utterances ( of which a dialogue model is only one example ) , we strongly maintain that this does not \u201c render the metric possibly meaningless \u201d . In fact , we maintain it is not even a weakness of the paper , for the following reasons : 1 ) We conduct our evaluations on a dataset ( the Twitter Corpus ) that has seen a huge amount of use in the literature ( e.g.Ritter et al . ( 2011 ) , Sordoni et al . ( 2015 ) , 2 papers by Serban et al . ( 2016 ) , Li et al . ( 2015 ) , Li et al . ( 2016 ) , etc .. ) . It is obvious that , in this setting , there are an abundance of dialogue models ( and embedding methods ) , and we simply use one such published model ( VHRED ) for our procedure . Since we show strong correlation with human judgement for Twitter , our model is clearly useful for evaluating these models ( and future models that will be built on this dataset ) . Thus , the statement that our metric is \u201c meaningless \u201d is unfounded . 2 ) More generally , the entire point of our evaluation procedure is to evaluate dialogue models that are * data-driven * . If there is not enough data in some domain to train a dialogue model ( which is what the reviewer is implying when they say a dialogue model may not be available ) , then there is no point to have an evaluation procedure for this domain ! The only exception to this is if someone wants to craft a * rule-based * system in a domain that both ( a ) has very little data , and ( b ) is in a highly specialized area that is different from existing datasets , such that no transfer learning can be used . The fact that our evaluation procedure would not work in this setting is hardly a limitation ( we do not claim our evaluation model would work in this setting , but we will clarify this in the paper ) . Given that we examine our method using a range of experiments in a very popular domain , showing that our method is a strong first step towards addressing an important open problem ( as admitted by the reviewer ) , we find the claim that our metric is \u201c possibly meaningless \u201d to be without basis . If the reviewer has specific concerns about the evaluation procedure in the paper we would be happy to address them ."}, "1": {"review_id": "HJ5PIaseg-1", "review_text": "This paper propose a new evaluation metric for dialogue systems, and show it has a higher correlation with human annotation. I agree the MT based metrics like BLEU are too simple to capture enough semantic information, but the metric proposed in this paper seems to be too compliciated to explain. On the other hand, we could also use equation 1 as a retrieval based dialogue system. So what is suggested in this paper is basically to train one dialogue model to evaluate another model. Then, the high-level question is why we should trust this model? This question is also relevant to the last item of my detail comments. Detail comments: - How to justify what is captured/evaluated by this metric? In terms of BLEU, we know it actually capture n-gram overlap. But for this model, I guess it is hard to say what is captured. If this is true, then it is also difficult to answer the question like: will the data dependence be a problem? - why not build model incrementally? As shown in equation (1), this metric uses both context and reference to compute a score. Is it possible to show the score function using only reference? It will guarantee this metric use the same information source as BLEU or ROUGE. - Another question about equation (1), is it possible to design the metric to be a nonlinear function. Since from what I can tell, the comparison between BLEU (or ROUGE) and the new metric in Figure 3 is much like a comparison between the exponential scale and the linear scale. - I found the two reasons in section 5.2 are not convincing if we put them together. Based on these two reasons, I would like to see the correlation with average score. A more reasonable way is to show the results both with and without averaging. - In table 6, it looks like the metric favors the short responses. If that is true, this metric basically does the opposite of BLEU, since BLEU will panelize short sentences. On the other hand, human annotators also tends to give short respones high scores, since long sentences will have a higher chance to contain some irrelevant words. Can we eliminate the length factor during the annotation? Otherwise, it is not surprise that the correlation.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their comments . We respond to the specific questions and criticisms raised by the reviewer below . Q : the metric proposed in this paper seems to be too compliciated to explain . A : By \u201c too complicated to explain \u201d , we assume the reviewer means the method is not as interpretable as scores such as BLEU ( we maintain that the method itself is not hard to explain : it is simply a linear combination of utterance embeddings , as detailed in Section 4 ) . It is certainly true that a learned metric with neural networks is less interpretable than one that is composed of simple statistics ( such as BLEU ) . But , conversely , such a learned metric is far more powerful than metrics such as BLEU . In fact , interpretability of an evaluation metric is only useful so long as the metric * actually works * ; yet it has been demonstrated in Liu et al . ( 2016 ) ( and further demonstrated in our paper ) that BLEU and other word-overlap variants * do not work * for evaluating the quality of dialogue responses . Thus , in this paper , we make a first step towards a metric obtaining correlation with human judgements by leveraging neural networks . We do not claim it should be the only metric used to evaluate dialogue responses , but that it should be used in addition to existing ( more interpretable ) metrics . We will add the above argument to the paper . The idea of using learned objective functions has proved enormously successful in the literature -- generative adversarial networks ( GANs ) are in fact generative models in which the generator is trained with a learned objective function . Although the learned objective ( discriminator ) is not interpretable , GANs are among the state-of-the-art generative models . Since these approaches have been successful in a variety of domains , it is natural to explore similar approaches for learning dialogue evaluation metrics , which is an open problem . Q : On the other hand , we could also use equation 1 as a retrieval based dialogue system . So what is suggested in this paper is basically to train one dialogue model to evaluate another model . Then , the high-level question is why we should trust this model ? A : To be clear , the evaluation model is * * not * * a dialogue retrieval model . The fundamental difference between a retrieval model and our proposed evaluation model , is that the evaluation model has access to the * reference response * . In other words , the evaluation model is conditioned on one reference response generated by humans . As such , the evaluation model has the ability to compare a model \u2019 s response to a response that is known to be good , making its job significantly easier compared to a retrieval model . One could also easily extend our framework to incorporate the use of multiple reference responses if they are available , which would be considered \u2018 cheating \u2019 for retrieval models . We will clarify this in the paper . Why should we trust this model ? Unlike existing automatic metrics for dialogue response evaluation , it correlates strongly with human judgements . We believe this is sufficient to add the ADEM score to the scores that are normally reported for dialogue evaluation . Q : How to justify what is captured/evaluated by this metric ? will the data dependence be a problem ? A : We respond to the first question above ( re : interpretability ) . To determine whether data dependence is a problem , this must be done empirically . We believe our generalization results in Table 5 suggest that data dependence is unlikely in the Twitter domain ( which is the domain that the majority of conversational models in the literature have been trained on -- e.g.Ritter et al . ( 2011 ) , Sordoni et al . ( 2015 ) , Serban et al . ( 2016 ) , etc . ) . We leave the investigation of other domains to future work . Q : why not build model incrementally ? A : This is a good idea , and we thank the reviewer for their suggestion -- we will report the results for each component of the model in an updated version of the paper . Q : Another question about equation ( 1 ) , is it possible to design the metric to be a nonlinear function . Since from what I can tell , the comparison between BLEU ( or ROUGE ) and the new metric in Figure 3 is much like a comparison between the exponential scale and the linear scale . A : One could indeed use a nonlinear function in equation ( 1 ) , but this has nothing to do with Figure 3 . Figure 3 simply shows an ( automatically scaled ) scatterplot of metric scores against human scores , along with a line of best fit . Note that the correlation scores reported in the paper ( Spearman , Pearson ) are independent of the scaling of the metrics -- thus , our better performance is not a matter of scaling , but is because ADEM is better at predicting human response scores than word-overlap metrics . Q : I found the two reasons in section 5.2 are not convincing if we put them together . Based on these two reasons , I would like to see the correlation with average score . A : Unfortunately , we can not compute the correlation with the average score for our dataset , because the vast majority of questions were only evaluated by one human annotator . We did this because we had a fixed financial budget , and wanted to collect the maximum number of examples under this budget , since a human score on a new response is more informative than on a response that has already been evaluated . Since there is only one score per response , this will result in higher variance , and lower correlations . Q : In table 6 , it looks like the metric favors the short responses . human annotators also tends to give short responses high scores , since long sentences will have a higher chance to contain some irrelevant words . Can we eliminate the length factor during the annotation ? A : Yes , humans appear to favour shorter responses , which is actually not desirable for a learned evaluation model . We do not currently analyze this in the paper , making the implicit assumption that the humans scoring the responses are always correct . But we will work to quantify the role of the length factor in the human score predictions , and will add this to an updated version of the paper ( and derive methods to correct it if necessary ) ."}, "2": {"review_id": "HJ5PIaseg-2", "review_text": " Overall the paper address an important problem: how to evaluate more appropriately automatic dialogue responses given the fact that current practice to automatically evaluate (BLEU, METEOR, ...) is often insufficient and sometimes misleading. The proposed approach using an LSTM-based encoding of dialogue context, reference response and model response(s) that are then scored in a linearly transformed space. While the overall approach is simple it is also quite intuitiv and allows end-to-end training. As the authors rightly argue simplicity is a feature both for interpretation as well as for speed. The experimental section reports on quite a range of experiments that seem fine to me and aim to convince the reader about the applicability of the approach. As mentioned also by others more insights from the experiments would have been great. I mentioned an in-depth failure case analysis and I would also suggest to go beyond the current dataset to really show generalizability of the proposed approach. In my opinion the paper is somewhat weaker on that front that it should have been. Overall I like the ideas put forward and the approach seems sensible though and the paper can thus be accepted. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their comments . The recommendations made by the reviewer are good ones -- we will certainly update the paper with a more in-depth failure analysis , as indeed this will show where the model performs poorly . Testing the model in other domains will require additional annotated data of response quality for each domain , which is both expensive and time consuming -- thus we can not guarantee that this will be completed by the deadline ."}}