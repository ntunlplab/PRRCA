{"year": "2018", "forum": "SyMvJrdaW", "title": "Decoupling the Layers in Residual Networks", "decision": "Accept (Poster)", "meta_review": "This paper proposes a \u201cwarp operator\u201d based on Taylor expansion that can replace a block of layers in a residual network, allowing for parallelization. Taking advantage of multi-GPU parallelization the paper shows increased speedup with similar performance on CIFAR-10 and CIFAR-100. R1 asked for clarification on rotational symmetry. The authors instead removed the discussion that was causing confusion (replacing with additional experimental results that had been requested). R2 had the most detailed review and thought that the idea and analysis were interesting. They also had difficulty following the discussion of symmetry (noted above). They also pointed out several other issues around clarity and had several suggestions for improving the experiments which seem to have been taken to heart by the authors, who detailed their changes in response to this review. There was also an anonymous public comment that pointed out a \u201cfatal mathematical flaw and weak experiments\u201d. There was a lengthy exchange between this reviewer and the authors, and the paper was actually corrected and clarified in the process. This anonymous poster was rather demanding of the authors, asking for latex-formatted equations, pseudo-code, and giving direction on how to respond to his/her rebuttal. I don't agree with the point that the paper is flawed by \"only\" presenting a speed-up over ResNet, and furthermore the comment of \"not everyone has access to parallelization\" isn\u2019t a fair criticism of the paper.", "reviews": [{"review_id": "SyMvJrdaW-0", "review_text": "Motivated via Talor approximation of the Residual network on a local minima, this paper proposed a warp operator that can replace a block of a consecutive number of residual layers. While having the same number of parameters as the original residual network, the new operator has the property that the computation can be parallelized. As demonstrated in the paper, this improves the training time with multi-GPU parallelization, while maintaining similar performance on CIFAR-10 and CIFAR-100. One thing that is currently not very clear to me is about the rotational symmetry. The paper mentioned rotated filters, but continue to talk about the rotation in the sense of an orthogonal matrix applying to the weight matrix of a convolution layer. The rotation of the filters (as 2D images or images with depth) seem to be quite different from \"rotating\" a general N-dim vectors in an abstract Euclidean space. It would be helpful to make the description here more explicit and clear.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer1 , Thank you for your comments . We are sorry about the confusion in our discussion . We decided to remove the discussion on symmetry breaking and use the space to present a much more extensive experimental study of WarpNet on CIFAR-10 and CIFAR-100 , an analysis on ImageNet , a scale-up of the warp factor from K=2 to K=3 , and a comparison to data parallelism . In addition , we clarified the notations in derivatives and added a theorem and its proof ( in Appendix ) . We have posted the new version of the paper . We do feel that the paper is much stronger than before , thanks to the reviewers \u2019 suggestions . ''"}, {"review_id": "SyMvJrdaW-1", "review_text": "Paper proposes a shallow model for approximating stacks of Resnet layers, based on mathematical approximations to the Resnet equations and experimental insights, and uses this technique to train Resnet-like models in half the time on CIFAR-10 and CIFAR-100. While the experiments are not particularly impressive, I liked the originality of this paper. ", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer3 , Thank you for your support ! In this revision we have included extensive experimental results on CIFAR-10 , CIFAR-100 , and ImageNet data sets using various settings for WarpNet , including increasing the warp factor from K=2 to K=3 . We also compared our parallelization with data parallelization using mini-batches for ResNets . Please see the new experimental results in Section 4 ."}, {"review_id": "SyMvJrdaW-2", "review_text": "The main contribution of this paper is a particular Taylor expansion of the outputs of a ResNet which is shown to be exact at almost all points in the input space. This expression is used to develop a new layer called a \u201cwarp layer\u201d which essentially tries to compute several layers of the residual network using the Taylor expansion expression \u2014 however in this expression, things can be done in parallel, and interestingly, the authors show that the gradients also decouple when the (ResNet) model is close to a local minimum in a certain sense, which may motivate the decoupling of layers to begin with. Finally the authors stack these warp layers to create a \u201cwarped resnet\u201d which they show does about as well as an ordinary ResNet but has better parallelization properties. To me the analytical parts of the paper are the most interesting, particularly in showing how the gradients approximately decouple. However there are several weaknesses to the paper (or maybe just things I didn\u2019t understand). First, a major part of the paper tries to make the case that there is a symmetry breaking property of the proposed model, which I am afraid I simply was not able to follow. Some of the notation is confusing here \u2014 for example, presumably the rotations refer to image level rotations rather than literally multiplying the inputs by an orthogonal matrix, which the notation suggests to be the case. It is also never precisely spelled out what the final theoretical guarantee is (preferably the authors would do this in the form of a proposition or theorem). Throughout, the authors write out equations as if the weights in all layers are equal, but this is confusing even if the authors say that this is what they are doing, since their explanation is not very clear. The confusion is particularly acute in places where derivatives are taken, because the derivatives continue to be taken as if the weights were untied, but then written as if they happened to be the same. Finally the experimental results are okay but perhaps a bit preliminary. I have a few recommendations here: * It would be stronger to evaluate results on a larger dataset like ILSVRC. * The relative speed-up of WarpNet compared to ResNet needs to be better explained \u2014 the authors break the computation of the WarpNet onto two GPUs, but it\u2019s not clear if they do this for the (vanilla) ResNet as well. In batch mode, the easiest way to parallelize is to have each GPU evaluate half the batch. Even in a streaming mode where images need to be evaluated one by one, there are ways to pipeline execution of the residual blocks, and I do not see any discussion of these alternatives in the paper. * In the experimental results, K is set to be 2, and the authors only mention in passing that they have tried larger K in the conclusion. It would be good to have a more thorough experimental evaluation of the trade-offs of setting K to be higher values. A few remaining questions for the authors: * There is a parallel submission (presumably by different authors called \u201cResidual Connections Encourage Iterative Inference\u201d) which contains some related insights. I wonder what are the differences between the two Taylor expansions, and whether the insights of this paper could be used to help the other paper and vice versa? * On implementation - the authors mention using Tensorflow\u2019s auto-differentiation. My question here is \u2014 are gradients being re-used intelligently as suggested in Section 3.1? * I notice that the analysis about the vanishing Hessian could be applied to most of the popular neural network architectures available now. How much of the ideas offered in this paper would then generalize to non-resnet settings? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer2 , Thank you for your constructive comments . We have revised the paper accordingly , considering all your suggestions . In particular , we conducted a much more extensive experimental study of WarpNet , clarified the notations in derivatives and added a theorem and its proof . Below we describe how we addressed each of your points : * Throughout , the authors write out equations as if the weights in all layers are equal , but this is confusing even if the authors say that this is ( not ) what they are doing , since their explanation is not very clear . The confusion is particularly acute in places where derivatives are taken , because the derivatives continue to be taken as if the weights were untied , but then written as if they happened to be the same . We have added a general formula for the first order Taylor series for all K in Equation ( 3 ) to clarify how the equation should be read . The exponential gradient scaling result can then be derived by differentiating this expression with respect to x . The only non vanishing term in the differentiation comes from the right-most factor , F , since all F '' = 0 almost exactly when ReLU is used . Then setting all weights to be equal results in the binomial coefficient and ( F ' ) to the power of k. We have also clarified the derivation of the formula in the gradient decoupling paragraph . In addition , we have provided a proof of binomial path lengths in the appendix and we hope that this will clarify the presentation of this paper . * It would be stronger to evaluate results on a larger dataset like ILSVRC . We have added a subsection ( 4.2 ) that compares WarpNet with ResNet on ImageNet on a few different settings . Using the ImageNet data set , we also illustrates the `` almost-exactness '' of the first order Taylor series in Figure 2 , where the validation curves of the WRN-73-2 and it 's approximation WarpNet-73-2 ( K=2 ) approximation lie almost on top of each other during training . * The relative speed-up of WarpNet compared to ResNet needs to be better explained \u2014 the authors break the computation of the WarpNet onto two GPUs , but it \u2019 s not clear if they do this for the ( vanilla ) ResNet as well . In batch mode , the easiest way to parallelize is to have each GPU evaluate half the batch . Even in a streaming mode where images need to be evaluated one by one , there are ways to pipeline execution of the residual blocks , and I do not see any discussion of these alternatives in the paper . We have performed experiments on ResNet with data parallelization using mini-batches . The results and discussion that compare WarpNet to ResNet with data parallelism are shown in Section 4.3 . * In the experimental results , K is set to be 2 , and the authors only mention in passing that they have tried larger K in the conclusion . It would be good to have a more thorough experimental evaluation of the trade-offs of setting K to be higher values . We have performed experiments with K=3 . It is a much more interesting case than K=2 , as there are 8 terms in the K=3 case and we only have 4 GPUs available . We made further approximations to the Taylor series by simply omitting terms in the Warp operator . Although in this case , WarpNet is not an exact first-order approximation of the ResNet . Results for K=3 are added in Section 4.1 ( Table 3 ) . * A major part of the paper tries to make the case that there is a symmetry breaking property of the proposed model , which I am afraid I simply was not able to follow . Some of the notation is confusing here \u2014 for example , presumably the rotations refer to image level rotations rather than literally multiplying the inputs by an orthogonal matrix , which the notation suggests to be the case . Sorry about the confusion . We decided to remove the discussion on symmetry breaking and use the space to show more extensive experimental results to demonstrate the effectiveness of WarpNet ."}], "0": {"review_id": "SyMvJrdaW-0", "review_text": "Motivated via Talor approximation of the Residual network on a local minima, this paper proposed a warp operator that can replace a block of a consecutive number of residual layers. While having the same number of parameters as the original residual network, the new operator has the property that the computation can be parallelized. As demonstrated in the paper, this improves the training time with multi-GPU parallelization, while maintaining similar performance on CIFAR-10 and CIFAR-100. One thing that is currently not very clear to me is about the rotational symmetry. The paper mentioned rotated filters, but continue to talk about the rotation in the sense of an orthogonal matrix applying to the weight matrix of a convolution layer. The rotation of the filters (as 2D images or images with depth) seem to be quite different from \"rotating\" a general N-dim vectors in an abstract Euclidean space. It would be helpful to make the description here more explicit and clear.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer1 , Thank you for your comments . We are sorry about the confusion in our discussion . We decided to remove the discussion on symmetry breaking and use the space to present a much more extensive experimental study of WarpNet on CIFAR-10 and CIFAR-100 , an analysis on ImageNet , a scale-up of the warp factor from K=2 to K=3 , and a comparison to data parallelism . In addition , we clarified the notations in derivatives and added a theorem and its proof ( in Appendix ) . We have posted the new version of the paper . We do feel that the paper is much stronger than before , thanks to the reviewers \u2019 suggestions . ''"}, "1": {"review_id": "SyMvJrdaW-1", "review_text": "Paper proposes a shallow model for approximating stacks of Resnet layers, based on mathematical approximations to the Resnet equations and experimental insights, and uses this technique to train Resnet-like models in half the time on CIFAR-10 and CIFAR-100. While the experiments are not particularly impressive, I liked the originality of this paper. ", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer3 , Thank you for your support ! In this revision we have included extensive experimental results on CIFAR-10 , CIFAR-100 , and ImageNet data sets using various settings for WarpNet , including increasing the warp factor from K=2 to K=3 . We also compared our parallelization with data parallelization using mini-batches for ResNets . Please see the new experimental results in Section 4 ."}, "2": {"review_id": "SyMvJrdaW-2", "review_text": "The main contribution of this paper is a particular Taylor expansion of the outputs of a ResNet which is shown to be exact at almost all points in the input space. This expression is used to develop a new layer called a \u201cwarp layer\u201d which essentially tries to compute several layers of the residual network using the Taylor expansion expression \u2014 however in this expression, things can be done in parallel, and interestingly, the authors show that the gradients also decouple when the (ResNet) model is close to a local minimum in a certain sense, which may motivate the decoupling of layers to begin with. Finally the authors stack these warp layers to create a \u201cwarped resnet\u201d which they show does about as well as an ordinary ResNet but has better parallelization properties. To me the analytical parts of the paper are the most interesting, particularly in showing how the gradients approximately decouple. However there are several weaknesses to the paper (or maybe just things I didn\u2019t understand). First, a major part of the paper tries to make the case that there is a symmetry breaking property of the proposed model, which I am afraid I simply was not able to follow. Some of the notation is confusing here \u2014 for example, presumably the rotations refer to image level rotations rather than literally multiplying the inputs by an orthogonal matrix, which the notation suggests to be the case. It is also never precisely spelled out what the final theoretical guarantee is (preferably the authors would do this in the form of a proposition or theorem). Throughout, the authors write out equations as if the weights in all layers are equal, but this is confusing even if the authors say that this is what they are doing, since their explanation is not very clear. The confusion is particularly acute in places where derivatives are taken, because the derivatives continue to be taken as if the weights were untied, but then written as if they happened to be the same. Finally the experimental results are okay but perhaps a bit preliminary. I have a few recommendations here: * It would be stronger to evaluate results on a larger dataset like ILSVRC. * The relative speed-up of WarpNet compared to ResNet needs to be better explained \u2014 the authors break the computation of the WarpNet onto two GPUs, but it\u2019s not clear if they do this for the (vanilla) ResNet as well. In batch mode, the easiest way to parallelize is to have each GPU evaluate half the batch. Even in a streaming mode where images need to be evaluated one by one, there are ways to pipeline execution of the residual blocks, and I do not see any discussion of these alternatives in the paper. * In the experimental results, K is set to be 2, and the authors only mention in passing that they have tried larger K in the conclusion. It would be good to have a more thorough experimental evaluation of the trade-offs of setting K to be higher values. A few remaining questions for the authors: * There is a parallel submission (presumably by different authors called \u201cResidual Connections Encourage Iterative Inference\u201d) which contains some related insights. I wonder what are the differences between the two Taylor expansions, and whether the insights of this paper could be used to help the other paper and vice versa? * On implementation - the authors mention using Tensorflow\u2019s auto-differentiation. My question here is \u2014 are gradients being re-used intelligently as suggested in Section 3.1? * I notice that the analysis about the vanishing Hessian could be applied to most of the popular neural network architectures available now. How much of the ideas offered in this paper would then generalize to non-resnet settings? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer2 , Thank you for your constructive comments . We have revised the paper accordingly , considering all your suggestions . In particular , we conducted a much more extensive experimental study of WarpNet , clarified the notations in derivatives and added a theorem and its proof . Below we describe how we addressed each of your points : * Throughout , the authors write out equations as if the weights in all layers are equal , but this is confusing even if the authors say that this is ( not ) what they are doing , since their explanation is not very clear . The confusion is particularly acute in places where derivatives are taken , because the derivatives continue to be taken as if the weights were untied , but then written as if they happened to be the same . We have added a general formula for the first order Taylor series for all K in Equation ( 3 ) to clarify how the equation should be read . The exponential gradient scaling result can then be derived by differentiating this expression with respect to x . The only non vanishing term in the differentiation comes from the right-most factor , F , since all F '' = 0 almost exactly when ReLU is used . Then setting all weights to be equal results in the binomial coefficient and ( F ' ) to the power of k. We have also clarified the derivation of the formula in the gradient decoupling paragraph . In addition , we have provided a proof of binomial path lengths in the appendix and we hope that this will clarify the presentation of this paper . * It would be stronger to evaluate results on a larger dataset like ILSVRC . We have added a subsection ( 4.2 ) that compares WarpNet with ResNet on ImageNet on a few different settings . Using the ImageNet data set , we also illustrates the `` almost-exactness '' of the first order Taylor series in Figure 2 , where the validation curves of the WRN-73-2 and it 's approximation WarpNet-73-2 ( K=2 ) approximation lie almost on top of each other during training . * The relative speed-up of WarpNet compared to ResNet needs to be better explained \u2014 the authors break the computation of the WarpNet onto two GPUs , but it \u2019 s not clear if they do this for the ( vanilla ) ResNet as well . In batch mode , the easiest way to parallelize is to have each GPU evaluate half the batch . Even in a streaming mode where images need to be evaluated one by one , there are ways to pipeline execution of the residual blocks , and I do not see any discussion of these alternatives in the paper . We have performed experiments on ResNet with data parallelization using mini-batches . The results and discussion that compare WarpNet to ResNet with data parallelism are shown in Section 4.3 . * In the experimental results , K is set to be 2 , and the authors only mention in passing that they have tried larger K in the conclusion . It would be good to have a more thorough experimental evaluation of the trade-offs of setting K to be higher values . We have performed experiments with K=3 . It is a much more interesting case than K=2 , as there are 8 terms in the K=3 case and we only have 4 GPUs available . We made further approximations to the Taylor series by simply omitting terms in the Warp operator . Although in this case , WarpNet is not an exact first-order approximation of the ResNet . Results for K=3 are added in Section 4.1 ( Table 3 ) . * A major part of the paper tries to make the case that there is a symmetry breaking property of the proposed model , which I am afraid I simply was not able to follow . Some of the notation is confusing here \u2014 for example , presumably the rotations refer to image level rotations rather than literally multiplying the inputs by an orthogonal matrix , which the notation suggests to be the case . Sorry about the confusion . We decided to remove the discussion on symmetry breaking and use the space to show more extensive experimental results to demonstrate the effectiveness of WarpNet ."}}