{"year": "2019", "forum": "BJgvg30ctX", "title": "Information Regularized Neural Networks", "decision": "Reject", "meta_review": "This paper proposes an approach to regularizing classifiers based on invertible networks using concepts from the information bottleneck theory. Because mutual information is invariant under invertible maps, the regularizer only considers the latent representation produced by the last hidden layer in the network and the network parameters that transform that representation into a classification decision. This leads to a combined \u21131 regularization on the final weights, W, and \u21132 regularization on W^{T} F(x), where F(x) is the latent representation produced by the last hidden layer. Experiments on CIFAR-100 image classification show that the proposed regularization can improve test performance. The reviewers liked the theoretical analysis, especially proposition 2.1 and its proof, but even after discussion and revision wanted a more careful empirical comparison to established forms of regularization to establish that the proposed approach has practical merit. The authors are encouraged to continue this line of research, building on the fruitful discussions they had with the reviewers.", "reviews": [{"review_id": "BJgvg30ctX-0", "review_text": " The authors propose a regularizer placed on the final linear layer of invertible networks that penalizes confident predictions, leading to better generalization. The algorithm is theoretically grounded and even though SOTA networks do not meet some theoretical requirements in practice, it seems to be effective. The ideas presented are interesting, but the paper is confusing at times and some motivations seem hand-wavy (see below). Even though penalizing overly confident predictions is an important topic, it has been attacked by various approaches in the past. It is not clear how the proposed method empirically compares to other approaches from the literature. On the theoretical side, proposition 2.1 and its proof are the main contribution. This very interesting observation could potentially be very useful in many tasks and shows once again why invertible neural networks are an important class of deep networks. Main concerns: The authors do not compare their method to other approaches from the literature with similar goals, such as [1]. Therefore, it is hard to judge the performance of the proposed regularizer. The authors claim that their InvNet is approximately invertible but there is no guarantee for this, making empirical conclusions unclear. The experiments would be more conclusive if a network that is fully invertible by construction is used. Such networks exist and perform on par with ResNets [2], so there is no reason not to use them. This would remove the need for analysis or discussion of this matter, as this issue clutters the main contribution and makes the claims rather fuzzy right now. Minor - Why are citations displayed in blue? This does not seem to be ICLR formatting standard. [1] Pereyra et al., \"Regularizing neural networks by penalizing confident output distributions.\" [2] Jacobsen et al., \"i-RevNet: Deep Invertible Networks\"", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the insightful suggestions ! 1.We have implemented i-RevNet ( Jacobsen et al. , 2018 ) with our regularization , CP - confidence penalizing with entropy ( Pereyra et al. , 2017 ) and LS - label smoothing ( Szegedy et al. , 2015 ) over some choices of hyperparameters . The performance results on CIFAR100 over 5 trails are provided below . We also reproduce other experiments on i-RevNet in Appendix G of the revised version . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- | baseline | Our Regularization | CP | LS | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- | | alpha1=0 , alpha2=1e-3 | alpha1=1e-6 , alpha2=1e-3 | beta=0.1 | beta=0.01 | eps=0.01 | eps=0.1 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- We do n't claim that our regularizer is the state of art that uniformly outperforms all other existing regularizers . Our theoretical result gives justification to any regularizers that effectively control the norm of w^TF ( X ) , which includes our regularizer , weight normalization ( Salimans et al. , 2016 ) , batch normalization ( Ioffe et al. , 2015 ) , etc .. The optimal set of hyperparameters depends on the architecture of the model , but for a reasonable choice of hyperparameters ( the additional loss introduced by regularizer is comparable with the classification loss ) we find our regularizer is always effective on large scale models that tend to overfit . As mentioned in ( Neyshabur et al. , 2015 ) , neural networks are equivalent up to some scaling factors passed among layers . As a consequence , there exist unbalanced neural networks with large l2 weights but are equivalent to those with small l2 weights . So the common belief that l2 regularization can `` simplify '' a model does not necessarily make sense for deep model . One of our main contributions is to interpret the use of l1 & l2 regularization in the deep learning setting . We reduce the deep structure into a linear one with MI objective and invertibility of ResNet , and formally justify the use of l2 regularization on both features F ( X ) and parameters w in the last layer with an interpretation of compressing irrelevant information explained in our proposed information optimization problem . We derive a theoretically gounded regularizer from our proposed information optimization problem . Our regularizer may seem unusual as it involves feature F ( X ) . We experimentally verify that regularization on F ( X ) is necessary for deep learning . In addition we observe that naive regularization on w results in a blow up on the norm of F ( X ) , meaning the neural network can absorb the regularization effect by upscaling the feature . We show in Figure 4 that our regularization can hold the magnitude of F ( X ) while compressing w. We believe we provide a new perspective to understand regularizations for deep models . 2.We have fixed the citation format in the revised version ."}, {"review_id": "BJgvg30ctX-1", "review_text": "In this paper, the authors propose to train a model from the point of view of maximizing mutual information between the predictions and the true outputs, with a regularization term that minimizes irrelevant information while learning. They show that the objective can be minimized by looking to make the final layer vectors be as uncorrelated as possible to the final layer representations, and simplify the same by applying Holder\u2019s inequality to make the optimization tractable. They also apply an L1 penalty on the final layer. Experiments on CIFAR and MNIST show that using their regularizer to train DNN models yield gains in performance. The presence of the L1 penalty also makes the results more interpretable (to the extend possible by looking at a subset of features in the last layer of a DNN). COMMENTS: - Meta Point: To really see that the regularization framework you\u2019re proposing is good, why not just pick a simple, feedforward model or convNet, see the performance and then compare it with the regularizer you\u2019re proposing? That will help hit the point home. - Page 1: before jumping to equations (1) and (2), please formally define Mutual Information. The actual definition is much later in the text, but it\u2019s better to define first. - Beyond referring the user to section 3 on Page 1, please also mention a couple of key references in the appropriate locations. - Page 3 paragraph 2: \u201cMutual information is bounded \u2026 correct them\u201d : Can you provide some formulas for this and make this concrete? Or perhaps provide some references? This line is vague. - prop 2.1 and 2.2: can you define what you mean by \u201cempirical version\u201d? Again, it\u2019s probably good to have these terms crisply defined before using them. - eqn (6) is interesting. Holder\u2019s inequality gives you the product terms. Then you can also apply the AM-GM inequality, and get a sum. So then at the end of it all, you\u2019re left with the standard elastic net penalty and not the product form. In that case, aren\u2019t we back to just the usual regularization strategy? And in which case, should I interpret the results you have in sec 4 as \u201cusing L1 penalties with L2 is good\u201d ? - To the point above, I guess one difference after the AM-GM step is that you will not have a squared L2 norm, but just L2. This is reminiscent of linear models where they use L2 loss instead of squared L2 loss. But on the penalty, squaring just adds smoothness. Can you comment on this? - sec 4.2.1: I don\u2019t see how fig (2) (L) is \u201croughly Gaussian\u201d. Can you explain? Maybe plot the histogram? Also for fig (2, R): the coefficients are approximately sparse. It\u2019s not sparse as you claim since there are almost no zeros in the coefficients. - I don\u2019t get the point of sec 4.3: How does this claim not apply to all deep learning models, regardless of the penalizations you propose? edit: I have read the responses and the other reviews. The authors have addressed the few major points I had. I still think there are a few gaps that need to be addressed (as pointed by the other reviewers) ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank for the reviewer for the thoughts and comments ! We provide detailed response to each comment as below . 1 . `` comment on AM-GM inequality for L2 loss '' The naive l2 regularization only involves regularizing the parameters w. Our derivation on the mutual information objective shows that feature regularization on F ( X ) is also necessary for good generalization of deep neural network . We observe that naive regularization on w results in a blow up on the norm of F ( X ) , meaning the neural network can absorb the regularization effect by upscaling the feature . We show in Figure 4 that our regularization can hold the magnitude of F ( X ) while compressing w. As mentioned in ( Neyshabur et al. , 2015 ) , neural networks are equivalent up to some scaling factors passed among layers . As a consequence , there exist unbalanced neural networks with large l2 weights but are equivalent to those with small l2 weights . So the common belief that l2 regularization can `` simplify '' a model does not necessarily make sense for deep model . 2 . `` pick a simple , feedforward model or convNet , see the performance and then compare it with the regularizer '' We perform our regularization on a simple structure named `` 34-layer plain '' in ( He et al. , 2015 ) and the result on CIFAR10 is shown in Table 2 . There are some marginal improvement on this network but it 's less robust against the choice of our hyperparameters . Our theoretical framework is built upon the assumption that the models have decent invertibility property . We emphasize that invertible neural networks are an important class of deep networks by citing the related work in Section 3 and proving in Appendix C that lower bound for the classification error is itself lower bounded by a constant , which is attained if the network is invertible . We perform our experiments on ResNet because we observe it 's composed of blocks in a intrinsic invertible functional form I+L so we expect it has a good invertibility property . We adopt the suggestion from Reviewer 1 that we should also perform all our experimental results on more theoretically grounded invertible neural networks such as i-RevNet . 3 . `` sec 4.3 : How does this claim not apply to all deep learning models , regardless of the penalizations you propose '' In the literature of machine learning l2 regularization only makes sense for shallow structure like logistic regression and SVM . For logistic regression , we can interpret it with heuristics from Occam 's razor or , from a probablitic point of view , a Gaussian prior . For SVM it means a larger margin for the support features . But these interpretation becomes less intuitive for deep learning as the interactions between parameters become extremely complicated . In our work we reduce the deep structure into a linear one with MI objective and invertibility of ResNet , and formally justify the use of l2 regularization on both features F ( X ) and parameters w in the last layer ; under this setting we interpret the meaning of l1/l2 regularization for deep models from classical perspectives in Section 4.3 . We fix our description in the revised version . 4 . `` I don \u2019 t see how fig ( 2 ) ( L ) is \u201c roughly Gaussian \u201d '' `` Also for fig ( 2 , R ) : the coefficients are not sparse as you claim '' We agree that the use of the term `` roughly Gaussian '' is imprecise . We have plotted the histogram of the values of feature entries of digit 9 in Appendix H of the revised version . Consider an perturbed image by some Gaussian noise , if the noise were to make an impact to the output of the model , it must modify the values of feature entries where the model assigns the corresponding weights with high values . But for our regularized model , the number of weights with high values is smaller compared to that of normal model so it 's harder for a random noise to make huge impact to the output . We fix our use of terms in the revised version . 5 . `` \u201c Mutual information is bounded \u2026 correct them \u201d : Can you provide some formulas for this and make this concrete '' We explain in detail in Appendix I of the revised version . Neural networks in known to be occasionally over-confident in its prediction which is in fact wrong . In particular we want to punish some logits with large absolute value |w^F ( X ) | but have the wrong `` sign '' ( for binary case ) . We show that information objective does not provide good gradient to fix this problem . 6 . `` prop 2.1 and 2.2 : can you define what you mean by \u201c empirical version \u201d '' We have added the necessary definition in our proposition stated in the main text of the revised version . In general by empirical version we mean a Monte Carlo approximation of the population quantities . We show in our proof that if the sample size N is large enough , our Monte Carlo approximation is accurate with high probability . We have fixed the citation and definition of MI as recommended in the revised version ."}, {"review_id": "BJgvg30ctX-2", "review_text": "This paper proposed to decompose the parameters into an invertible feature map F and a linear transformation w in the last layer. They aim to maximize mutual information I(Y, \\hat{Y}) while constraining irrelevant information, which further transfers to regularization on F and w. The authors also spend pages explaining how the hyper-parameters can be chosen. Comments: 1. The experimental results showed a noticeable improvement on CIFAR-100 and is fairly robust to alpha_2. 2. The formulation seems plausible. 3. For Figure 2 and discussion in Section 4.2.1, I'm less convinced that the entries with high feature mean is 'relevant' and the others are not by looking at just digit 9 samples. For example, an entry with small feature mean should still be given high w_10 value if for all other 9 digits the same entry has even smaller feature mean. --------UPDATE AFTER READING THE AUTHORS' COMMENTS----------- 1. Appendix F lacks explanation. So I'm going to say what I meant in details. In order to achieve high accuracy the model must assign high values on some entries of weights to separate the different classes. w_10 is a linear separator, not necessarily entry-wise (unless the features are independent). I would take 1k features of each class and compute their principal components. Check if these components are different from class to class and plot the dot product of components and weights. If the following happens I would be more convinced: 1) principal components of digit 0 and digit 9 differs a lot AND 2) w_0 weights components of digit 0 higher but weights those of digit 9 lower 2. \"But for our regularized model, the number of weights with high values is smaller compared to that of normal model ...\" I'm not convinced. When perturbed by Gaussian noise, the variance on output does not necessarily depends on sparsity. In fact, it depends on the norm of the weights. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the comments ! We provide the detailed response to questions as below . 1 . `` an entry with small feature mean should still be given high w_10 value if for all other 9 digits the same entry has even smaller feature mean '' Neural networks should assign high w_10 values to feature entries of 9 with high values , so their product can contribute to the logits significantly . We agree that neural networks may also assign high w_10 values to feature entries of 9 with small absolute value but relatively large compared with the same feature entries of other digits ; in this case we expect w_10 should hold even higher value so the product w^TF ( X ) contributes significantly to the logits . Our derivation in ( 6 ) shows that we should regularize the inner product ( w^TF ( X ) ) _i for class i , which is the sum of product from each entry . It is possible that for one entry we have small feature value but large classifier value , but if the product of them is relatively small compared to other entry product then we will not consider it as an important feature entry to classification . We have reproduced the feature statistics plot of all digits for InvNet on MNIST in Appendix F of the revised version . We observe that each digits have their specific entries with high value assigned to both weights and feature means . 2 . `` how the proposed model tends to overlook irrelevant information '' Consider an image perturbed by some Gaussian noise , if the noise were to make an impact to the output of the model , it must significantly modify the values of feature entries where the model assigns the corresponding weights with high values . But for our regularized model , the number of weights with high values is smaller compared to that of normal model so it 's harder for a random noise to make huge impact to the output ( unless this noise is maliciously designed ) . Our belief is under our regularization , the weight w is shaped into a `` sparse '' form adapted to the particular input data distribution , so it is hard for any irrelevant information induced on the data to make an impact to the model 's output ."}], "0": {"review_id": "BJgvg30ctX-0", "review_text": " The authors propose a regularizer placed on the final linear layer of invertible networks that penalizes confident predictions, leading to better generalization. The algorithm is theoretically grounded and even though SOTA networks do not meet some theoretical requirements in practice, it seems to be effective. The ideas presented are interesting, but the paper is confusing at times and some motivations seem hand-wavy (see below). Even though penalizing overly confident predictions is an important topic, it has been attacked by various approaches in the past. It is not clear how the proposed method empirically compares to other approaches from the literature. On the theoretical side, proposition 2.1 and its proof are the main contribution. This very interesting observation could potentially be very useful in many tasks and shows once again why invertible neural networks are an important class of deep networks. Main concerns: The authors do not compare their method to other approaches from the literature with similar goals, such as [1]. Therefore, it is hard to judge the performance of the proposed regularizer. The authors claim that their InvNet is approximately invertible but there is no guarantee for this, making empirical conclusions unclear. The experiments would be more conclusive if a network that is fully invertible by construction is used. Such networks exist and perform on par with ResNets [2], so there is no reason not to use them. This would remove the need for analysis or discussion of this matter, as this issue clutters the main contribution and makes the claims rather fuzzy right now. Minor - Why are citations displayed in blue? This does not seem to be ICLR formatting standard. [1] Pereyra et al., \"Regularizing neural networks by penalizing confident output distributions.\" [2] Jacobsen et al., \"i-RevNet: Deep Invertible Networks\"", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the insightful suggestions ! 1.We have implemented i-RevNet ( Jacobsen et al. , 2018 ) with our regularization , CP - confidence penalizing with entropy ( Pereyra et al. , 2017 ) and LS - label smoothing ( Szegedy et al. , 2015 ) over some choices of hyperparameters . The performance results on CIFAR100 over 5 trails are provided below . We also reproduce other experiments on i-RevNet in Appendix G of the revised version . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- | baseline | Our Regularization | CP | LS | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- | | alpha1=0 , alpha2=1e-3 | alpha1=1e-6 , alpha2=1e-3 | beta=0.1 | beta=0.01 | eps=0.01 | eps=0.1 | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- We do n't claim that our regularizer is the state of art that uniformly outperforms all other existing regularizers . Our theoretical result gives justification to any regularizers that effectively control the norm of w^TF ( X ) , which includes our regularizer , weight normalization ( Salimans et al. , 2016 ) , batch normalization ( Ioffe et al. , 2015 ) , etc .. The optimal set of hyperparameters depends on the architecture of the model , but for a reasonable choice of hyperparameters ( the additional loss introduced by regularizer is comparable with the classification loss ) we find our regularizer is always effective on large scale models that tend to overfit . As mentioned in ( Neyshabur et al. , 2015 ) , neural networks are equivalent up to some scaling factors passed among layers . As a consequence , there exist unbalanced neural networks with large l2 weights but are equivalent to those with small l2 weights . So the common belief that l2 regularization can `` simplify '' a model does not necessarily make sense for deep model . One of our main contributions is to interpret the use of l1 & l2 regularization in the deep learning setting . We reduce the deep structure into a linear one with MI objective and invertibility of ResNet , and formally justify the use of l2 regularization on both features F ( X ) and parameters w in the last layer with an interpretation of compressing irrelevant information explained in our proposed information optimization problem . We derive a theoretically gounded regularizer from our proposed information optimization problem . Our regularizer may seem unusual as it involves feature F ( X ) . We experimentally verify that regularization on F ( X ) is necessary for deep learning . In addition we observe that naive regularization on w results in a blow up on the norm of F ( X ) , meaning the neural network can absorb the regularization effect by upscaling the feature . We show in Figure 4 that our regularization can hold the magnitude of F ( X ) while compressing w. We believe we provide a new perspective to understand regularizations for deep models . 2.We have fixed the citation format in the revised version ."}, "1": {"review_id": "BJgvg30ctX-1", "review_text": "In this paper, the authors propose to train a model from the point of view of maximizing mutual information between the predictions and the true outputs, with a regularization term that minimizes irrelevant information while learning. They show that the objective can be minimized by looking to make the final layer vectors be as uncorrelated as possible to the final layer representations, and simplify the same by applying Holder\u2019s inequality to make the optimization tractable. They also apply an L1 penalty on the final layer. Experiments on CIFAR and MNIST show that using their regularizer to train DNN models yield gains in performance. The presence of the L1 penalty also makes the results more interpretable (to the extend possible by looking at a subset of features in the last layer of a DNN). COMMENTS: - Meta Point: To really see that the regularization framework you\u2019re proposing is good, why not just pick a simple, feedforward model or convNet, see the performance and then compare it with the regularizer you\u2019re proposing? That will help hit the point home. - Page 1: before jumping to equations (1) and (2), please formally define Mutual Information. The actual definition is much later in the text, but it\u2019s better to define first. - Beyond referring the user to section 3 on Page 1, please also mention a couple of key references in the appropriate locations. - Page 3 paragraph 2: \u201cMutual information is bounded \u2026 correct them\u201d : Can you provide some formulas for this and make this concrete? Or perhaps provide some references? This line is vague. - prop 2.1 and 2.2: can you define what you mean by \u201cempirical version\u201d? Again, it\u2019s probably good to have these terms crisply defined before using them. - eqn (6) is interesting. Holder\u2019s inequality gives you the product terms. Then you can also apply the AM-GM inequality, and get a sum. So then at the end of it all, you\u2019re left with the standard elastic net penalty and not the product form. In that case, aren\u2019t we back to just the usual regularization strategy? And in which case, should I interpret the results you have in sec 4 as \u201cusing L1 penalties with L2 is good\u201d ? - To the point above, I guess one difference after the AM-GM step is that you will not have a squared L2 norm, but just L2. This is reminiscent of linear models where they use L2 loss instead of squared L2 loss. But on the penalty, squaring just adds smoothness. Can you comment on this? - sec 4.2.1: I don\u2019t see how fig (2) (L) is \u201croughly Gaussian\u201d. Can you explain? Maybe plot the histogram? Also for fig (2, R): the coefficients are approximately sparse. It\u2019s not sparse as you claim since there are almost no zeros in the coefficients. - I don\u2019t get the point of sec 4.3: How does this claim not apply to all deep learning models, regardless of the penalizations you propose? edit: I have read the responses and the other reviews. The authors have addressed the few major points I had. I still think there are a few gaps that need to be addressed (as pointed by the other reviewers) ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank for the reviewer for the thoughts and comments ! We provide detailed response to each comment as below . 1 . `` comment on AM-GM inequality for L2 loss '' The naive l2 regularization only involves regularizing the parameters w. Our derivation on the mutual information objective shows that feature regularization on F ( X ) is also necessary for good generalization of deep neural network . We observe that naive regularization on w results in a blow up on the norm of F ( X ) , meaning the neural network can absorb the regularization effect by upscaling the feature . We show in Figure 4 that our regularization can hold the magnitude of F ( X ) while compressing w. As mentioned in ( Neyshabur et al. , 2015 ) , neural networks are equivalent up to some scaling factors passed among layers . As a consequence , there exist unbalanced neural networks with large l2 weights but are equivalent to those with small l2 weights . So the common belief that l2 regularization can `` simplify '' a model does not necessarily make sense for deep model . 2 . `` pick a simple , feedforward model or convNet , see the performance and then compare it with the regularizer '' We perform our regularization on a simple structure named `` 34-layer plain '' in ( He et al. , 2015 ) and the result on CIFAR10 is shown in Table 2 . There are some marginal improvement on this network but it 's less robust against the choice of our hyperparameters . Our theoretical framework is built upon the assumption that the models have decent invertibility property . We emphasize that invertible neural networks are an important class of deep networks by citing the related work in Section 3 and proving in Appendix C that lower bound for the classification error is itself lower bounded by a constant , which is attained if the network is invertible . We perform our experiments on ResNet because we observe it 's composed of blocks in a intrinsic invertible functional form I+L so we expect it has a good invertibility property . We adopt the suggestion from Reviewer 1 that we should also perform all our experimental results on more theoretically grounded invertible neural networks such as i-RevNet . 3 . `` sec 4.3 : How does this claim not apply to all deep learning models , regardless of the penalizations you propose '' In the literature of machine learning l2 regularization only makes sense for shallow structure like logistic regression and SVM . For logistic regression , we can interpret it with heuristics from Occam 's razor or , from a probablitic point of view , a Gaussian prior . For SVM it means a larger margin for the support features . But these interpretation becomes less intuitive for deep learning as the interactions between parameters become extremely complicated . In our work we reduce the deep structure into a linear one with MI objective and invertibility of ResNet , and formally justify the use of l2 regularization on both features F ( X ) and parameters w in the last layer ; under this setting we interpret the meaning of l1/l2 regularization for deep models from classical perspectives in Section 4.3 . We fix our description in the revised version . 4 . `` I don \u2019 t see how fig ( 2 ) ( L ) is \u201c roughly Gaussian \u201d '' `` Also for fig ( 2 , R ) : the coefficients are not sparse as you claim '' We agree that the use of the term `` roughly Gaussian '' is imprecise . We have plotted the histogram of the values of feature entries of digit 9 in Appendix H of the revised version . Consider an perturbed image by some Gaussian noise , if the noise were to make an impact to the output of the model , it must modify the values of feature entries where the model assigns the corresponding weights with high values . But for our regularized model , the number of weights with high values is smaller compared to that of normal model so it 's harder for a random noise to make huge impact to the output . We fix our use of terms in the revised version . 5 . `` \u201c Mutual information is bounded \u2026 correct them \u201d : Can you provide some formulas for this and make this concrete '' We explain in detail in Appendix I of the revised version . Neural networks in known to be occasionally over-confident in its prediction which is in fact wrong . In particular we want to punish some logits with large absolute value |w^F ( X ) | but have the wrong `` sign '' ( for binary case ) . We show that information objective does not provide good gradient to fix this problem . 6 . `` prop 2.1 and 2.2 : can you define what you mean by \u201c empirical version \u201d '' We have added the necessary definition in our proposition stated in the main text of the revised version . In general by empirical version we mean a Monte Carlo approximation of the population quantities . We show in our proof that if the sample size N is large enough , our Monte Carlo approximation is accurate with high probability . We have fixed the citation and definition of MI as recommended in the revised version ."}, "2": {"review_id": "BJgvg30ctX-2", "review_text": "This paper proposed to decompose the parameters into an invertible feature map F and a linear transformation w in the last layer. They aim to maximize mutual information I(Y, \\hat{Y}) while constraining irrelevant information, which further transfers to regularization on F and w. The authors also spend pages explaining how the hyper-parameters can be chosen. Comments: 1. The experimental results showed a noticeable improvement on CIFAR-100 and is fairly robust to alpha_2. 2. The formulation seems plausible. 3. For Figure 2 and discussion in Section 4.2.1, I'm less convinced that the entries with high feature mean is 'relevant' and the others are not by looking at just digit 9 samples. For example, an entry with small feature mean should still be given high w_10 value if for all other 9 digits the same entry has even smaller feature mean. --------UPDATE AFTER READING THE AUTHORS' COMMENTS----------- 1. Appendix F lacks explanation. So I'm going to say what I meant in details. In order to achieve high accuracy the model must assign high values on some entries of weights to separate the different classes. w_10 is a linear separator, not necessarily entry-wise (unless the features are independent). I would take 1k features of each class and compute their principal components. Check if these components are different from class to class and plot the dot product of components and weights. If the following happens I would be more convinced: 1) principal components of digit 0 and digit 9 differs a lot AND 2) w_0 weights components of digit 0 higher but weights those of digit 9 lower 2. \"But for our regularized model, the number of weights with high values is smaller compared to that of normal model ...\" I'm not convinced. When perturbed by Gaussian noise, the variance on output does not necessarily depends on sparsity. In fact, it depends on the norm of the weights. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the comments ! We provide the detailed response to questions as below . 1 . `` an entry with small feature mean should still be given high w_10 value if for all other 9 digits the same entry has even smaller feature mean '' Neural networks should assign high w_10 values to feature entries of 9 with high values , so their product can contribute to the logits significantly . We agree that neural networks may also assign high w_10 values to feature entries of 9 with small absolute value but relatively large compared with the same feature entries of other digits ; in this case we expect w_10 should hold even higher value so the product w^TF ( X ) contributes significantly to the logits . Our derivation in ( 6 ) shows that we should regularize the inner product ( w^TF ( X ) ) _i for class i , which is the sum of product from each entry . It is possible that for one entry we have small feature value but large classifier value , but if the product of them is relatively small compared to other entry product then we will not consider it as an important feature entry to classification . We have reproduced the feature statistics plot of all digits for InvNet on MNIST in Appendix F of the revised version . We observe that each digits have their specific entries with high value assigned to both weights and feature means . 2 . `` how the proposed model tends to overlook irrelevant information '' Consider an image perturbed by some Gaussian noise , if the noise were to make an impact to the output of the model , it must significantly modify the values of feature entries where the model assigns the corresponding weights with high values . But for our regularized model , the number of weights with high values is smaller compared to that of normal model so it 's harder for a random noise to make huge impact to the output ( unless this noise is maliciously designed ) . Our belief is under our regularization , the weight w is shaped into a `` sparse '' form adapted to the particular input data distribution , so it is hard for any irrelevant information induced on the data to make an impact to the model 's output ."}}