{"year": "2021", "forum": "uUlGTEbBRL", "title": "Rethinking Compressed Convolution Neural Network from a Statistical Perspective", "decision": "Reject", "meta_review": "This paper presents a theoretical analysis of CNN compression using tensor methods. None of the three reviewers have strong opinion; there scores are 5, 6, and 5.\n\nThe attempt to understand the mechanism of how tensor decomposition compresses CNNs is meaningful and interesting. However, the main contribution of this work is not sufficiently distinct compared to the existing approaches and the analysis and proof is conduected only for simplified models as mentioned by reviewers. The practical benefit of this paper is not clear and the experimental validation is weak because only a small number of model architectures were tested on a few small datasets.\n\nThis is a borderline paper. However, this paper needs to extend its contribution by performing more comprehensive analysis for general CNNs. ", "reviews": [{"review_id": "uUlGTEbBRL-0", "review_text": "Summary : This paper formulated higher-order CNNs into a Tucker form and provides sample complexity analysis to higher-order CNNs and compressed designs of CNNs via tensor analysis . It uses then theoretically analyzes the efficiency of four block designs from ResNet , MobileNetV1 , and MobileNetV2 . The paper also conducts numerical experiments to verify its theoretical results and provide some empirical studies to show that increasing the expansive ratio of a bottleneck Pros : 1.This work provides a theoretical analysis for higher-order CNNs via analyzing its Tucker formulation using tensor methods . 2.The proposed theoretical analysis can be applied to compressed designs of CNNs . 3.This work also provides numerical experiments to verify its theoretical claims . Cons : 1.This work lacks comparisons with many important and relevant works ( e.g . [ 1-5 ] ) , which also formulate CNNs or higher-order CNNs using various tensor decomposition forms . Many of the works also provide theoretical analysis ( e.g.generalization bound in [ 5 ] ) for the proposed formulations . It would be nice for the authors to show how their formulation is different from the existing ones and what is novel about the proposed formulation . Since [ 3 ] and [ 4 ] both have formulations of higher-order CNNs/CNNs using Tucker decomposition , it seems to me that the current formulation proposed in this work lacks novelty . 2.The current presentation of this work could be much more improved via a ) providing more intuitions for its theoretical analysis , b ) putting more connections between the theoretical analysis and empirical experiments , and c ) adopting better notations ( e.g.definitions of tensor operations rather than using elementwise notations ) to make the theoretical analysis cleaner and clearer . 3.The finding discovered in this work via its theoretical analysis lacks sufficient experimental supports : the finding is only shown using one particular architecture design and the room for potential improvements is very limited . For example , in Table 2 , the test accuracy of even the smallest model is already very close to the state-of-the-art results on these datasets , which leaves very limited room for potential improvements of test accuracies by simply increasing the expansion ratio . 4.As mentioned above , because a ) the proposed formulation of higher-order CNNs lack proper comparisons with existing works and has limited novelty and b ) the finding from theoretical analysis lack sufficient experimental supports , the contribution of this paper is limited and it would be nice for the authors to provide more justifications for its novelty and better designs of experiments to convey the message . [ 1 ] Kossaifi , Jean , et al . `` Tensor contraction layers for parsimonious deep nets . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops . 2017 . [ 2 ] Kossaifi , Jean , et al . `` T-net : Parametrizing fully convolutional nets with a single high-order tensor . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2019 . [ 3 ] Su , Jiahao , et al . `` Tensorial neural networks : Generalization of neural networks and application to model compression . '' arXiv preprint arXiv:1805.10352 ( 2018 ) . [ 4 ] Kossaifi , Jean , et al . `` Tensor regression networks . '' Journal of Machine Learning Research 21.123 ( 2020 ) : 1-21 . [ 5 ] Li , Jingling , et al . `` Understanding Generalization in Deep Learning via Tensor Methods . '' arXiv preprint arXiv:2001.05070 ( 2020 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your detailed comments and suggestions ! __Updates based on your comments__ Based on your suggestions , we have reorganized the whole Section 1 `` Introduction '' by including more discussions about ( i ) relevant works in tensor methods for CNN compression and , ( ii ) how the literature on generalization error differs from our sample complexity analysis in both goal and methodology . We also added extra discussions in Section 5 `` Conclusion and Discussion '' . We plan to include more experiment studies on the $ K/R $ ratio in more recent state-of-the-art networks in Section 4 `` Experiments '' . __Con 1__ > This work lacks comparisons with many important and relevant works ( e.g . [ 1-5 ] ) , which also formulate CNNs or higher-order CNNs using various tensor decomposition forms . Many of the works also provide theoretical analysis ( e.g.generalization bound in [ 5 ] ) for the proposed formulations . It would be nice for the authors to show how their formulation is different from the existing ones and what is novel about the proposed formulation . Since [ 3 ] and [ 4 ] both have formulations of higher-order CNNs/CNNs using Tucker decomposition , it seems to me that the current formulation proposed in this work lacks novelty . Briefly speaking , our formulation summarizes all weights of a high-order CNN into a single tensor , akin to [ 2 ] , but our summarized tensor has an explicit `` nested doll '' structure to account for the interactions between weights across layers ; see for instance , equations ( 4 ) & ( 7 ) . While the literature on generalization error aims to understand why deep neural networks ( including CNNs ) can generalize well via various regularization methods , we aim to theoretically explain how much compressibility is achieved in a compressed CNN architecture and whether there remains model redundancy to be reduced . For this purpose , we formulate the networks exactly and conduct statistical sample complexity analysis . __Relation to [ 1 ] & [ 4 ] __ : It is common in literature to apply tensor decomposition to CNNs layer-by-layer . On the one hand , when tensor decomposition is applied to the convolution layers , it corresponds to various block designs , which we focus on in our theoretical study in Theorem 2 . On the other hand , when tensor decomposition is applied to the fully-connected layer instead , it corresponds to the tensor contraction in [ 1 ] or tensor regression layer in [ 4 ] . In fact , our CNN formulation allows us to easily extend the theoretical analysis to the fully-connected weights $ \\mathcal { B } $ with low rank structure . And we can hence establish the sample complexity analysis for the CNNs with tensor regression layer , which we leave as future work ; see Section 5 . __Relation to [ 2 ] __ : Our formulation is similar to [ 2 ] in that we also parametrize the network weights into a single tensor . While [ 2 ] assumes the tensor to have a heuristic Tucker form , we replicate the layer-by-layer operations of CNNs and show that the summarized tensor has a `` nested doll '' structure . In other words , the low-rank structure of the previous layer is nested within that of the current layer . __Relation to [ 3 ] __ : Both our paper and [ 3 ] target towards tensor structured inputs . However , each layer of Tensorial Neural Network in [ 3 ] still conducts a 2D convolution along two selected dimensions of the input ( see Fig 4 ( d ) in [ 3 ] ) . Meanwhile , we allow for a multidimensional ( high-order ) convolution along all input dimensions , which helps to explore the data structure more efficiently . __Relation to [ 5 ] __ : We include a subsection 1.1 to make the comparison between our theoretical approach and the generalization bound . In short , most studies on generalization bound , including [ 5 ] , aim to understand the generalization ability of DNNs . Hence , they do not require an explicit network architecture but requires some form of regularization . Li [ 5 ] proposed to use tensor decomposition as their regularization technique . But their study is essentially still model-agnostic , since the ranks of the CP layers depend solely on trained weights . In conclusion , their approach is not suitable for explaining fixed network architectures with pre-designed low-rank layers ( regardless of training ) . An additional comment is that the parameter efficiency in [ 1 ] - [ 4 ] was heuristically justified by methods , such as FLOPs counting , naive parameter counting and/or empirical running time . However , there is still lack of a theoretical study to understand the mechanism of how tensor decomposition can compress CNNs . Our paper attempts to fill this gap from statistical perspectives ."}, {"review_id": "uUlGTEbBRL-1", "review_text": "This paper formulates CNNs with high-order inputs into an explicit Tucker model , and provides sample complexity analysis to CNNs as well as compressed CNNs via tensor decomposition . Experiments support their theoretical results . Sample complexity analysis of CNNs and compressed CNNs is an interesting topic . This paper is well written and is easy to follow . Cons : 1.Technical novelty is limited . It has been well understood that CNNs can be formulated as a tensor model , see Lebedev et al . ( 2015 ) ; Hayashi et al . ( 2019 ) ; Kossaifi et al . ( 2019 ) .By assuming a realization model , the sample complexity analysis of CNNs and compressed CNNs was transferred to the sample complexity analysis of tensor regression model and low-rank tensor regression model . The latter analysis is not new given a rich literature on this topic , see e.g. , Bahadori et al . ( 2014 ) ; Yu and Liu ( 2016 ) ; Kossaifi et al . ( 2020 ) .Lebedev , V. , et al . `` Speeding-up convolutional neural networks using fine-tuned CP-decomposition . '' 3rd International Conference on Learning Representations , ICLR 2015-Conference Track Proceedings . 2015.Hayashi , Kohei , et al . `` Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks . '' Advances in Neural Information Processing Systems . 2019.Kossaifi , Jean , et al . `` T-net : Parametrizing fully convolutional nets with a single high-order tensor . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2019.Bahadori , Mohammad Taha , Qi Rose Yu , and Yan Liu . `` Fast multivariate spatio-temporal analysis via low rank tensor learning . '' Advances in neural information processing systems . 2014.Yu , Rose , and Yan Liu . `` Learning from multiway data : Simple and efficient tensor regression . '' International Conference on Machine Learning . 2016.Kossaifi , Jean , et al . `` Tensor regression networks . '' Journal of Machine Learning Research 21.123 ( 2020 ) : 1-21 . 2.The sample complexity analysis is for the global minimizer from a non-convex optimization , see ( 5 ) . It would be more interesting to study the sample complexity analysis for the estimator from some polynomial algorithm .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comment ! Following the suggestions from you and another reviewer , we have reorganized the whole Section 1 `` Introduction '' to better motivate our work . Specifically , a ) we have made more comparisons between our work and other tensor methods for CNN compression ; b ) we have better motivated the method for statistical sample complexity , which is model-based , rather than algorithm-based . __Con 1__ > 1 . Technical novelty is limited . It has been well understood that CNNs can be formulated as a tensor model , see Lebedev et al . ( 2015 ) ; Hayashi et al . ( 2019 ) ; Kossaifi et al . ( 2019 ) .By assuming a realization model , the sample complexity analysis of CNNs and compressed CNNs was transferred to the sample complexity analysis of tensor regression model and low-rank tensor regression model . The latter analysis is not new given a rich literature on this topic , see e.g. , Bahadori et al . ( 2014 ) ; Yu and Liu ( 2016 ) ; Kossaifi et al . ( 2020 ) .Tensor methods have been widely adopted to compress or decompose CNN layers [ 1 ] - [ 8 ] . However , all of these work simply summarize the weights into tensors , and then apply CP , Tucker or Tensor-Train decomposition to the summarized weight tensors . They do not explicitly account for the interaction between weights and inputs and , to our best knowledge , the whole CNN model has never been depicted in a tensor regression form before . In fact , such a formulation is only meaningful when considering a high-order convolution ( in our case , a general $ N $ dimensional convolution ) , while most of the existing work ( [ 1 ] - [ 7 ] ) focuses on 2D convolution . Kossaifi et al . [ 8 ] considered using CP to factorize high-order convolution kernels . However , they focused on empirical studies on the factorized layers , instead of formulating the CNN as a whole . So , establishing the tensor regression form for a high-order CNN can be considered as one of our contributions . By building this connection , more tensor regression based methodology can hopefully be introduced into the theoretical investigation of CNNs . Moreover , as discussed at the end of Section 2.3 ( Page 5 ) , though we assume a simple regression model at equation ( 5 ) , our sample complexity analysis can be extended to classification problems as well . We have developed one whole new section * '' A.4 Classification problems '' * to the Appendix on pages 24-31 . In the section , we have provided some theorem and corollaries for both binary and multiclass classification problems , and provide a complete proof of the new theorem . Now , we discuss in details how this paper relates and differs from the references you provided . __Relation to [ 1 ] , [ 2 ] \\ & [ 4 ] __ It is common in literature to apply tensor decomposition to CNNs layer-by-layer . On the one hand , when tensor decomposition is applied to the convolution layers , it corresponds to various block designs in [ 1 ] \\ & [ 2 ] , which we focus on in our theoretical study ( Theorem 2 ) . On the other hand , when tensor decomposition is applied to the fully-connected layer instead , it corresponds to the tensor regression layer in [ 4 ] . Parameter efficiency in [ 1 ] , [ 2 ] \\ & [ 4 ] was heuristically justified by methods , such as FLOPs counting , naive parameter counting , the amount of space savings and/or empirical running time . However , there is still lack of a theoretical study to understand the mechanism of how tensor decomposition can compress CNNs . This paper attempts to fill this gap from statistical perspectives . In our paper , we focus on studying the low-rank structure on convolution kernel $ \\mathcal { A } $ ( corresponding to various block structures ) . In fact , our CNN formulation allows us to easily extend the theoretical analysis to the fully-connected weights $ \\mathcal { B } $ with low rank structure . And we can hence establish the sample complexity analysis for the CNNs with tensor regression layer [ 4 ] , which we leave as future work ; see Section 5 . __Relation to [ 3 ] __ Our formulation is similar to [ 3 ] in that we also parametrize the network weights into a single tensor . While [ 3 ] assumes the tensor to have a heuristic Tucker form , we replicate the layer-by-layer operations of CNNs and show that the summarized tensor has a `` nested doll '' structure . In other words , the low-rank structure of the previous layer is nested within that of the current layer . [ 3 ] also uses the heuristic naive parameter counting ( compression ratio ) method to measure the efficiency ."}, {"review_id": "uUlGTEbBRL-2", "review_text": "This paper provides theoretical analysis of the estimating power of CNN ( 3 and 5 layers ) . By formulating the problem using tensors , the authors showed that the estimating error of the learned CNN weights with respect to the true weights is of the order $ \\sqrt { d/n } $ where $ d $ measures model complexity and $ n $ is the training sample size . In addition , the authors considered low rank approximation to the convolution tensor through CP and Tucker decompositions , and they derived convergence result for the CNN weights in this case . The authors then applied these results to analyze different block designs through numerical experiments and ablation studies . The writing is generally clear . The use of tensors give a very compact representation of an CNN , however tensor notations and indices can quickly become complicated in higher dimensions . The convergence results are a nice contribution to the growing literature on neural networks ' theoretical analysis . In particular , the analysis of various tensor decompositions help guide the design of blocks in CNNs . Although the results are interesting , I feel that important aspects of CNNs were not analyzed nor discussed in this paper . Here are some of my comments and questions . One of the main uses of CNNs is to perform classification where there is a softmax layer after the fully connected layer . Hence model ( 2 ) does not reflect what is done in practice and $ \\boldsymbol { y } $ should be a vector of probabilities . In fact I think the authors did classification on CIFAR-10 and SVHN in the numerical experiments ( although it is not mentioned ) using residual networks and not the model in ( 2 ) . Also , why do you add sub-Gaussian errors to CNN outputs as in ( 2 ) ? In the experiments , the authors used ReLU instead of linear activation , and there is batch normalization between convolution layers . In addition , there are residual connections in the ResNet ( block ) architecture . All these factors that affect convergence rates were not considered in the theoretical analysis . Although I understand that these issues were omitted to simplify the proofs , I feel that there should be consistency between the theory and implementation parts . Hence Table 2 and Figure 3 do not necessarily provide empirical evidence to the theoretical results derived , as these networks have different architectures than the ones considered in Section 2 . Consequently , the results hold true for a special type of CNN and it is not clear to me whether these results will still hold for CNNs used in practice ( with non-linear activation , batch norm , max pooling etc . ) , or when the true weight tensor does not have the same tensor product structure as the learned weight tensor . Equation ( 5 ) seems to assume that you can optimize the weights for the full connected layer and the convolution kernel perfectly . However in the numerical experiments and in practice , stochastic gradient descent with momentum is used for training and this will contribute an error term to the right hand side of Theorem 1 . In Theorem 1 , can you give more explanation as to the meaning of model complexity $ d_ { \\mathcal { M } } $ ? Is this the effective number of parameters ? Also what are $ P $ and $ L $ here ? In addition , is $ d_ { \\mathcal { M } } $ always larger than $ \\delta $ ? As there is no restriction on $ \\delta $ , it is possible that $ \\delta $ is much larger than $ d_ { \\mathcal { M } } $ . The same comment also applies to Theorem 2 . Some other comments : 1 . In ( 2 ) , state that $ 1\\leq i\\leq n $ 2 . In the paragraph after Assumption 1 , what do you mean by $ \\lambda_ { \\mathrm { min } } ( f_X ( \\theta ) ) $ as $ f_X ( \\theta ) $ is not a matrix . 3.On Page 5 the 2nd line after the first display , mode- $ 3 $ multiplication between two tensors does not seem to be introduced in the notations", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your constructive comments and suggestions ! Firstly , we would like to bring to your attention that we have updated our draft . Based on your comment , we have added one whole new section * '' A.4 Classification problems '' * to the Appendix on pages 24-31 . In the section , we provide some theorem and corollaries for both binary and multiclass classification problems . Since our theoretical analysis is model-based , and changes in model assumption will require different proof techniques , we have also provided a complete proof for the new theorem . Also , the numbering of some equations has changed , and in our response below , we will use the numbering of the newest version of the draft . For instance , the equation ( 2 ) in the old version is the equation ( 3 ) in the new version . __Comment 1__ > One of the main uses of CNNs is to perform classification where there is a softmax layer after the fully connected layer . Hence model ( 2 ) does not reflect what is done in practice and y should be a vector of probabilities . In fact I think the authors did classification on CIFAR-10 and SVHN in the numerical experiments ( although it is not mentioned ) using residual networks and not the model in ( 2 ) . Also , why do you add sub-Gaussian errors to CNN outputs as in ( 2 ) ? We consider a simple regression problem for our theoretical analysis , and hence our formulation at ( 3 ) [ equation ( 2 ) , originally ] . We assume our CNN Model at ( 3 ) has an additive error $ \\xi $ , which follows a sub-Gaussian distribution , akin to [ 1 ] . For a regression problem , the additive error assumption is very natural and corresponds to many frequently used loss functions , including the mean square loss considered in our paper . The sub-Gaussian distribution include normal distribution and many other normal-like distributions . This setting is considered as the most fundamental case , but we can indeed extend our theoretical framework to classification problems . Due to the page limit , we organized the classification problem as a whole new section into our Appendix , since the regression setting ( which allows for both discrete and continuous output ) is more general than classification setting ( which allows for only discrete label output ) . Specifically , consider the settings in our Appendix A.4 , we present the following theoretical results , together with some proofs . - For binary classification problems , __Theorem 3 ( Classification : CNN ) __ > Under some technical assumptions , suppose that $ n\\gtrsim d_\\mathcal { M } $ , where $ d_\\mathcal { M } = K ( P+L+1 ) $ . Then , for some $ \\delta > 0 $ , $ $ \\|\\mathcal { \\widehat { W } } - \\mathcal { W } ^ * \\|_\\mathrm { F } \\leq \\frac { 2\\sqrt { \\kappa_U } } { \\kappa_1 } \\left ( \\sqrt { \\frac { d_\\mathcal { M } } { n } } +\\sqrt { \\frac { \\delta } { n } } \\right ) , $ $ with probability $ 1-\\exp ( -0.25cn + 9d_\\mathcal { M } ) -2\\exp ( -c_\\gamma d_\\mathcal { M } -c\\delta ) $ , where $ \\delta = O_p ( 1 ) $ , $ c $ and $ c_\\gamma $ are some positive constants . Denote $ d_\\mathcal { M } ^\\mathrm { TU } = \\prod_ { j=1 } ^ { N+1 } R_j+\\sum_ { i=1 } ^ { N } l_iR_i+R_ { N+1 } P $ and $ d_\\mathcal { M } ^\\mathrm { CP } =R^ { N+1 } +R ( \\sum_ { i=1 } ^ { N } l_i+P ) $ . __Corollary 2 ( Classification : Compressed CNN ) __ > Let $ ( \\mathcal { \\widehat { W } } , d_ { \\mathcal { M } } ) $ be $ ( \\mathcal { \\widehat { W } } _ { \\mathrm { TU } } , d_\\mathcal { M } ^\\mathrm { TU } ) $ for Tucker decomposition , or $ ( \\mathcal { \\widehat { W } } _\\mathrm { CP } , d_\\mathcal { M } ^\\mathrm { CP } ) $ for CP decomposition . Under some technical assumptions , suppose that $ n\\gtrsim c_Nd_\\mathcal { M } $ . Then , for some $ \\delta > 0 $ , $ $ \\|\\mathcal { \\widehat { W } } - \\mathcal { W } ^ * \\|_\\mathrm { F } \\leq \\frac { 2\\sqrt { \\kappa_U } } { \\kappa_1 } \\left ( \\sqrt { \\frac { 3c_Nd_\\mathcal { M } } { n } } +\\sqrt { \\frac { \\delta } { n } } \\right ) , $ $ with probability $ 1-4\\exp ( -0.25cn + 3c_Nd_\\mathcal { M } ) -2\\exp ( -c_\\gamma d_\\mathcal { M } -c\\delta ) $ , where $ \\delta = O_p ( 1 ) $ , $ c $ and $ c_\\gamma $ are some positive constants , and $ c_N $ is defined as in Theorem 2 ."}], "0": {"review_id": "uUlGTEbBRL-0", "review_text": "Summary : This paper formulated higher-order CNNs into a Tucker form and provides sample complexity analysis to higher-order CNNs and compressed designs of CNNs via tensor analysis . It uses then theoretically analyzes the efficiency of four block designs from ResNet , MobileNetV1 , and MobileNetV2 . The paper also conducts numerical experiments to verify its theoretical results and provide some empirical studies to show that increasing the expansive ratio of a bottleneck Pros : 1.This work provides a theoretical analysis for higher-order CNNs via analyzing its Tucker formulation using tensor methods . 2.The proposed theoretical analysis can be applied to compressed designs of CNNs . 3.This work also provides numerical experiments to verify its theoretical claims . Cons : 1.This work lacks comparisons with many important and relevant works ( e.g . [ 1-5 ] ) , which also formulate CNNs or higher-order CNNs using various tensor decomposition forms . Many of the works also provide theoretical analysis ( e.g.generalization bound in [ 5 ] ) for the proposed formulations . It would be nice for the authors to show how their formulation is different from the existing ones and what is novel about the proposed formulation . Since [ 3 ] and [ 4 ] both have formulations of higher-order CNNs/CNNs using Tucker decomposition , it seems to me that the current formulation proposed in this work lacks novelty . 2.The current presentation of this work could be much more improved via a ) providing more intuitions for its theoretical analysis , b ) putting more connections between the theoretical analysis and empirical experiments , and c ) adopting better notations ( e.g.definitions of tensor operations rather than using elementwise notations ) to make the theoretical analysis cleaner and clearer . 3.The finding discovered in this work via its theoretical analysis lacks sufficient experimental supports : the finding is only shown using one particular architecture design and the room for potential improvements is very limited . For example , in Table 2 , the test accuracy of even the smallest model is already very close to the state-of-the-art results on these datasets , which leaves very limited room for potential improvements of test accuracies by simply increasing the expansion ratio . 4.As mentioned above , because a ) the proposed formulation of higher-order CNNs lack proper comparisons with existing works and has limited novelty and b ) the finding from theoretical analysis lack sufficient experimental supports , the contribution of this paper is limited and it would be nice for the authors to provide more justifications for its novelty and better designs of experiments to convey the message . [ 1 ] Kossaifi , Jean , et al . `` Tensor contraction layers for parsimonious deep nets . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops . 2017 . [ 2 ] Kossaifi , Jean , et al . `` T-net : Parametrizing fully convolutional nets with a single high-order tensor . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2019 . [ 3 ] Su , Jiahao , et al . `` Tensorial neural networks : Generalization of neural networks and application to model compression . '' arXiv preprint arXiv:1805.10352 ( 2018 ) . [ 4 ] Kossaifi , Jean , et al . `` Tensor regression networks . '' Journal of Machine Learning Research 21.123 ( 2020 ) : 1-21 . [ 5 ] Li , Jingling , et al . `` Understanding Generalization in Deep Learning via Tensor Methods . '' arXiv preprint arXiv:2001.05070 ( 2020 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your detailed comments and suggestions ! __Updates based on your comments__ Based on your suggestions , we have reorganized the whole Section 1 `` Introduction '' by including more discussions about ( i ) relevant works in tensor methods for CNN compression and , ( ii ) how the literature on generalization error differs from our sample complexity analysis in both goal and methodology . We also added extra discussions in Section 5 `` Conclusion and Discussion '' . We plan to include more experiment studies on the $ K/R $ ratio in more recent state-of-the-art networks in Section 4 `` Experiments '' . __Con 1__ > This work lacks comparisons with many important and relevant works ( e.g . [ 1-5 ] ) , which also formulate CNNs or higher-order CNNs using various tensor decomposition forms . Many of the works also provide theoretical analysis ( e.g.generalization bound in [ 5 ] ) for the proposed formulations . It would be nice for the authors to show how their formulation is different from the existing ones and what is novel about the proposed formulation . Since [ 3 ] and [ 4 ] both have formulations of higher-order CNNs/CNNs using Tucker decomposition , it seems to me that the current formulation proposed in this work lacks novelty . Briefly speaking , our formulation summarizes all weights of a high-order CNN into a single tensor , akin to [ 2 ] , but our summarized tensor has an explicit `` nested doll '' structure to account for the interactions between weights across layers ; see for instance , equations ( 4 ) & ( 7 ) . While the literature on generalization error aims to understand why deep neural networks ( including CNNs ) can generalize well via various regularization methods , we aim to theoretically explain how much compressibility is achieved in a compressed CNN architecture and whether there remains model redundancy to be reduced . For this purpose , we formulate the networks exactly and conduct statistical sample complexity analysis . __Relation to [ 1 ] & [ 4 ] __ : It is common in literature to apply tensor decomposition to CNNs layer-by-layer . On the one hand , when tensor decomposition is applied to the convolution layers , it corresponds to various block designs , which we focus on in our theoretical study in Theorem 2 . On the other hand , when tensor decomposition is applied to the fully-connected layer instead , it corresponds to the tensor contraction in [ 1 ] or tensor regression layer in [ 4 ] . In fact , our CNN formulation allows us to easily extend the theoretical analysis to the fully-connected weights $ \\mathcal { B } $ with low rank structure . And we can hence establish the sample complexity analysis for the CNNs with tensor regression layer , which we leave as future work ; see Section 5 . __Relation to [ 2 ] __ : Our formulation is similar to [ 2 ] in that we also parametrize the network weights into a single tensor . While [ 2 ] assumes the tensor to have a heuristic Tucker form , we replicate the layer-by-layer operations of CNNs and show that the summarized tensor has a `` nested doll '' structure . In other words , the low-rank structure of the previous layer is nested within that of the current layer . __Relation to [ 3 ] __ : Both our paper and [ 3 ] target towards tensor structured inputs . However , each layer of Tensorial Neural Network in [ 3 ] still conducts a 2D convolution along two selected dimensions of the input ( see Fig 4 ( d ) in [ 3 ] ) . Meanwhile , we allow for a multidimensional ( high-order ) convolution along all input dimensions , which helps to explore the data structure more efficiently . __Relation to [ 5 ] __ : We include a subsection 1.1 to make the comparison between our theoretical approach and the generalization bound . In short , most studies on generalization bound , including [ 5 ] , aim to understand the generalization ability of DNNs . Hence , they do not require an explicit network architecture but requires some form of regularization . Li [ 5 ] proposed to use tensor decomposition as their regularization technique . But their study is essentially still model-agnostic , since the ranks of the CP layers depend solely on trained weights . In conclusion , their approach is not suitable for explaining fixed network architectures with pre-designed low-rank layers ( regardless of training ) . An additional comment is that the parameter efficiency in [ 1 ] - [ 4 ] was heuristically justified by methods , such as FLOPs counting , naive parameter counting and/or empirical running time . However , there is still lack of a theoretical study to understand the mechanism of how tensor decomposition can compress CNNs . Our paper attempts to fill this gap from statistical perspectives ."}, "1": {"review_id": "uUlGTEbBRL-1", "review_text": "This paper formulates CNNs with high-order inputs into an explicit Tucker model , and provides sample complexity analysis to CNNs as well as compressed CNNs via tensor decomposition . Experiments support their theoretical results . Sample complexity analysis of CNNs and compressed CNNs is an interesting topic . This paper is well written and is easy to follow . Cons : 1.Technical novelty is limited . It has been well understood that CNNs can be formulated as a tensor model , see Lebedev et al . ( 2015 ) ; Hayashi et al . ( 2019 ) ; Kossaifi et al . ( 2019 ) .By assuming a realization model , the sample complexity analysis of CNNs and compressed CNNs was transferred to the sample complexity analysis of tensor regression model and low-rank tensor regression model . The latter analysis is not new given a rich literature on this topic , see e.g. , Bahadori et al . ( 2014 ) ; Yu and Liu ( 2016 ) ; Kossaifi et al . ( 2020 ) .Lebedev , V. , et al . `` Speeding-up convolutional neural networks using fine-tuned CP-decomposition . '' 3rd International Conference on Learning Representations , ICLR 2015-Conference Track Proceedings . 2015.Hayashi , Kohei , et al . `` Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks . '' Advances in Neural Information Processing Systems . 2019.Kossaifi , Jean , et al . `` T-net : Parametrizing fully convolutional nets with a single high-order tensor . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2019.Bahadori , Mohammad Taha , Qi Rose Yu , and Yan Liu . `` Fast multivariate spatio-temporal analysis via low rank tensor learning . '' Advances in neural information processing systems . 2014.Yu , Rose , and Yan Liu . `` Learning from multiway data : Simple and efficient tensor regression . '' International Conference on Machine Learning . 2016.Kossaifi , Jean , et al . `` Tensor regression networks . '' Journal of Machine Learning Research 21.123 ( 2020 ) : 1-21 . 2.The sample complexity analysis is for the global minimizer from a non-convex optimization , see ( 5 ) . It would be more interesting to study the sample complexity analysis for the estimator from some polynomial algorithm .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comment ! Following the suggestions from you and another reviewer , we have reorganized the whole Section 1 `` Introduction '' to better motivate our work . Specifically , a ) we have made more comparisons between our work and other tensor methods for CNN compression ; b ) we have better motivated the method for statistical sample complexity , which is model-based , rather than algorithm-based . __Con 1__ > 1 . Technical novelty is limited . It has been well understood that CNNs can be formulated as a tensor model , see Lebedev et al . ( 2015 ) ; Hayashi et al . ( 2019 ) ; Kossaifi et al . ( 2019 ) .By assuming a realization model , the sample complexity analysis of CNNs and compressed CNNs was transferred to the sample complexity analysis of tensor regression model and low-rank tensor regression model . The latter analysis is not new given a rich literature on this topic , see e.g. , Bahadori et al . ( 2014 ) ; Yu and Liu ( 2016 ) ; Kossaifi et al . ( 2020 ) .Tensor methods have been widely adopted to compress or decompose CNN layers [ 1 ] - [ 8 ] . However , all of these work simply summarize the weights into tensors , and then apply CP , Tucker or Tensor-Train decomposition to the summarized weight tensors . They do not explicitly account for the interaction between weights and inputs and , to our best knowledge , the whole CNN model has never been depicted in a tensor regression form before . In fact , such a formulation is only meaningful when considering a high-order convolution ( in our case , a general $ N $ dimensional convolution ) , while most of the existing work ( [ 1 ] - [ 7 ] ) focuses on 2D convolution . Kossaifi et al . [ 8 ] considered using CP to factorize high-order convolution kernels . However , they focused on empirical studies on the factorized layers , instead of formulating the CNN as a whole . So , establishing the tensor regression form for a high-order CNN can be considered as one of our contributions . By building this connection , more tensor regression based methodology can hopefully be introduced into the theoretical investigation of CNNs . Moreover , as discussed at the end of Section 2.3 ( Page 5 ) , though we assume a simple regression model at equation ( 5 ) , our sample complexity analysis can be extended to classification problems as well . We have developed one whole new section * '' A.4 Classification problems '' * to the Appendix on pages 24-31 . In the section , we have provided some theorem and corollaries for both binary and multiclass classification problems , and provide a complete proof of the new theorem . Now , we discuss in details how this paper relates and differs from the references you provided . __Relation to [ 1 ] , [ 2 ] \\ & [ 4 ] __ It is common in literature to apply tensor decomposition to CNNs layer-by-layer . On the one hand , when tensor decomposition is applied to the convolution layers , it corresponds to various block designs in [ 1 ] \\ & [ 2 ] , which we focus on in our theoretical study ( Theorem 2 ) . On the other hand , when tensor decomposition is applied to the fully-connected layer instead , it corresponds to the tensor regression layer in [ 4 ] . Parameter efficiency in [ 1 ] , [ 2 ] \\ & [ 4 ] was heuristically justified by methods , such as FLOPs counting , naive parameter counting , the amount of space savings and/or empirical running time . However , there is still lack of a theoretical study to understand the mechanism of how tensor decomposition can compress CNNs . This paper attempts to fill this gap from statistical perspectives . In our paper , we focus on studying the low-rank structure on convolution kernel $ \\mathcal { A } $ ( corresponding to various block structures ) . In fact , our CNN formulation allows us to easily extend the theoretical analysis to the fully-connected weights $ \\mathcal { B } $ with low rank structure . And we can hence establish the sample complexity analysis for the CNNs with tensor regression layer [ 4 ] , which we leave as future work ; see Section 5 . __Relation to [ 3 ] __ Our formulation is similar to [ 3 ] in that we also parametrize the network weights into a single tensor . While [ 3 ] assumes the tensor to have a heuristic Tucker form , we replicate the layer-by-layer operations of CNNs and show that the summarized tensor has a `` nested doll '' structure . In other words , the low-rank structure of the previous layer is nested within that of the current layer . [ 3 ] also uses the heuristic naive parameter counting ( compression ratio ) method to measure the efficiency ."}, "2": {"review_id": "uUlGTEbBRL-2", "review_text": "This paper provides theoretical analysis of the estimating power of CNN ( 3 and 5 layers ) . By formulating the problem using tensors , the authors showed that the estimating error of the learned CNN weights with respect to the true weights is of the order $ \\sqrt { d/n } $ where $ d $ measures model complexity and $ n $ is the training sample size . In addition , the authors considered low rank approximation to the convolution tensor through CP and Tucker decompositions , and they derived convergence result for the CNN weights in this case . The authors then applied these results to analyze different block designs through numerical experiments and ablation studies . The writing is generally clear . The use of tensors give a very compact representation of an CNN , however tensor notations and indices can quickly become complicated in higher dimensions . The convergence results are a nice contribution to the growing literature on neural networks ' theoretical analysis . In particular , the analysis of various tensor decompositions help guide the design of blocks in CNNs . Although the results are interesting , I feel that important aspects of CNNs were not analyzed nor discussed in this paper . Here are some of my comments and questions . One of the main uses of CNNs is to perform classification where there is a softmax layer after the fully connected layer . Hence model ( 2 ) does not reflect what is done in practice and $ \\boldsymbol { y } $ should be a vector of probabilities . In fact I think the authors did classification on CIFAR-10 and SVHN in the numerical experiments ( although it is not mentioned ) using residual networks and not the model in ( 2 ) . Also , why do you add sub-Gaussian errors to CNN outputs as in ( 2 ) ? In the experiments , the authors used ReLU instead of linear activation , and there is batch normalization between convolution layers . In addition , there are residual connections in the ResNet ( block ) architecture . All these factors that affect convergence rates were not considered in the theoretical analysis . Although I understand that these issues were omitted to simplify the proofs , I feel that there should be consistency between the theory and implementation parts . Hence Table 2 and Figure 3 do not necessarily provide empirical evidence to the theoretical results derived , as these networks have different architectures than the ones considered in Section 2 . Consequently , the results hold true for a special type of CNN and it is not clear to me whether these results will still hold for CNNs used in practice ( with non-linear activation , batch norm , max pooling etc . ) , or when the true weight tensor does not have the same tensor product structure as the learned weight tensor . Equation ( 5 ) seems to assume that you can optimize the weights for the full connected layer and the convolution kernel perfectly . However in the numerical experiments and in practice , stochastic gradient descent with momentum is used for training and this will contribute an error term to the right hand side of Theorem 1 . In Theorem 1 , can you give more explanation as to the meaning of model complexity $ d_ { \\mathcal { M } } $ ? Is this the effective number of parameters ? Also what are $ P $ and $ L $ here ? In addition , is $ d_ { \\mathcal { M } } $ always larger than $ \\delta $ ? As there is no restriction on $ \\delta $ , it is possible that $ \\delta $ is much larger than $ d_ { \\mathcal { M } } $ . The same comment also applies to Theorem 2 . Some other comments : 1 . In ( 2 ) , state that $ 1\\leq i\\leq n $ 2 . In the paragraph after Assumption 1 , what do you mean by $ \\lambda_ { \\mathrm { min } } ( f_X ( \\theta ) ) $ as $ f_X ( \\theta ) $ is not a matrix . 3.On Page 5 the 2nd line after the first display , mode- $ 3 $ multiplication between two tensors does not seem to be introduced in the notations", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your constructive comments and suggestions ! Firstly , we would like to bring to your attention that we have updated our draft . Based on your comment , we have added one whole new section * '' A.4 Classification problems '' * to the Appendix on pages 24-31 . In the section , we provide some theorem and corollaries for both binary and multiclass classification problems . Since our theoretical analysis is model-based , and changes in model assumption will require different proof techniques , we have also provided a complete proof for the new theorem . Also , the numbering of some equations has changed , and in our response below , we will use the numbering of the newest version of the draft . For instance , the equation ( 2 ) in the old version is the equation ( 3 ) in the new version . __Comment 1__ > One of the main uses of CNNs is to perform classification where there is a softmax layer after the fully connected layer . Hence model ( 2 ) does not reflect what is done in practice and y should be a vector of probabilities . In fact I think the authors did classification on CIFAR-10 and SVHN in the numerical experiments ( although it is not mentioned ) using residual networks and not the model in ( 2 ) . Also , why do you add sub-Gaussian errors to CNN outputs as in ( 2 ) ? We consider a simple regression problem for our theoretical analysis , and hence our formulation at ( 3 ) [ equation ( 2 ) , originally ] . We assume our CNN Model at ( 3 ) has an additive error $ \\xi $ , which follows a sub-Gaussian distribution , akin to [ 1 ] . For a regression problem , the additive error assumption is very natural and corresponds to many frequently used loss functions , including the mean square loss considered in our paper . The sub-Gaussian distribution include normal distribution and many other normal-like distributions . This setting is considered as the most fundamental case , but we can indeed extend our theoretical framework to classification problems . Due to the page limit , we organized the classification problem as a whole new section into our Appendix , since the regression setting ( which allows for both discrete and continuous output ) is more general than classification setting ( which allows for only discrete label output ) . Specifically , consider the settings in our Appendix A.4 , we present the following theoretical results , together with some proofs . - For binary classification problems , __Theorem 3 ( Classification : CNN ) __ > Under some technical assumptions , suppose that $ n\\gtrsim d_\\mathcal { M } $ , where $ d_\\mathcal { M } = K ( P+L+1 ) $ . Then , for some $ \\delta > 0 $ , $ $ \\|\\mathcal { \\widehat { W } } - \\mathcal { W } ^ * \\|_\\mathrm { F } \\leq \\frac { 2\\sqrt { \\kappa_U } } { \\kappa_1 } \\left ( \\sqrt { \\frac { d_\\mathcal { M } } { n } } +\\sqrt { \\frac { \\delta } { n } } \\right ) , $ $ with probability $ 1-\\exp ( -0.25cn + 9d_\\mathcal { M } ) -2\\exp ( -c_\\gamma d_\\mathcal { M } -c\\delta ) $ , where $ \\delta = O_p ( 1 ) $ , $ c $ and $ c_\\gamma $ are some positive constants . Denote $ d_\\mathcal { M } ^\\mathrm { TU } = \\prod_ { j=1 } ^ { N+1 } R_j+\\sum_ { i=1 } ^ { N } l_iR_i+R_ { N+1 } P $ and $ d_\\mathcal { M } ^\\mathrm { CP } =R^ { N+1 } +R ( \\sum_ { i=1 } ^ { N } l_i+P ) $ . __Corollary 2 ( Classification : Compressed CNN ) __ > Let $ ( \\mathcal { \\widehat { W } } , d_ { \\mathcal { M } } ) $ be $ ( \\mathcal { \\widehat { W } } _ { \\mathrm { TU } } , d_\\mathcal { M } ^\\mathrm { TU } ) $ for Tucker decomposition , or $ ( \\mathcal { \\widehat { W } } _\\mathrm { CP } , d_\\mathcal { M } ^\\mathrm { CP } ) $ for CP decomposition . Under some technical assumptions , suppose that $ n\\gtrsim c_Nd_\\mathcal { M } $ . Then , for some $ \\delta > 0 $ , $ $ \\|\\mathcal { \\widehat { W } } - \\mathcal { W } ^ * \\|_\\mathrm { F } \\leq \\frac { 2\\sqrt { \\kappa_U } } { \\kappa_1 } \\left ( \\sqrt { \\frac { 3c_Nd_\\mathcal { M } } { n } } +\\sqrt { \\frac { \\delta } { n } } \\right ) , $ $ with probability $ 1-4\\exp ( -0.25cn + 3c_Nd_\\mathcal { M } ) -2\\exp ( -c_\\gamma d_\\mathcal { M } -c\\delta ) $ , where $ \\delta = O_p ( 1 ) $ , $ c $ and $ c_\\gamma $ are some positive constants , and $ c_N $ is defined as in Theorem 2 ."}}