{"year": "2017", "forum": "BJuysoFeg", "title": "Revisiting Batch Normalization For Practical Domain Adaptation", "decision": "Reject", "meta_review": "The paper performs domain adaptation using a very simple trick inspired by BatchNorm. The paper received below margin scores. The reviewers both, liked the simplicity of the approach, and at the same time felt that the contribution was too thin. Given the high bar of ICLR, this paper falls short.", "reviews": [{"review_id": "BJuysoFeg-0", "review_text": "Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating. This paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset. Overall, I think the paper is more suitable for a workshop track rather than for the main conference track. My main concerns are the following: 1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so). 2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks). ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your insightful comments and suggestions of our work . 1.About writing . We have updated the writing of abstract to make our idea clearer . 2.About experiments . ( 1 ) We agree with the reviewer that same base network should be used for a fair comparison . Thus , we tried our best to transfer other methods to the Inception-BN model . As shown in our answers to the pre-review questions , we have implemented SA , GFK , LSSA and CORAL to reproduce results based on Inception-BN . For other methods , we had not reproduced promising results . This may be due to some missing implementation details ( some methods do not have public implementations ) or the uniqueness of AlexNet ( AlexNet has intrinsic different structure with Inception ) . Thus , we do not report these controversial results . ( 2 ) We respectfully disagree with the reviewer \u2019 s suggestion of training a model from scratch . Currently , most computer vision tasks ( including classification , detection and segmentation ) use an pre-trained model , and it has significant impact on the performance . For our domain adaptation task ( especially for Office dataset ) , the number of images is too small ( e.g.500 images for DSLR domain ) compared to that of ImageNet dataset ( 1M images ) . Training a DNN like AlexNet or Inception-BN with such amount images would suffer from severe over-fitting and significantly drop the performance . Thus , we think this comparison with a model trained from scratch is meaningless . It can not manifest the difference of different methods ."}, {"review_id": "BJuysoFeg-1", "review_text": "This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain. Pros: The method is very simple and easy to understand and apply. The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks. The analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network. Cons: There is little novelty -- the method is arguably too simple to be called a \u201cmethod.\u201d Rather, it\u2019s the most straightforward/intuitive approach when using a network with batch normalization for domain adaptation. The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what\u2019s done in the Inception BN results in Table 1-2?) The analysis in section 4.3.1 is superfluous except as a sanity check -- KL divergence between the distributions should be 0 when each distribution is shifted/scaled to N(0,1) by BN. Section 3.3: it\u2019s not clear to me what point is being made here. Overall, there\u2019s not much novelty here, but it\u2019s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with \u201cFrustratingly Easy Domain Adaptation\u201d). If accepted, Sections 4.3.1 and 3.3 should be removed or rewritten for clarity for a final version.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your insightful comments and suggestions of our work . 1.As mentioned in the original BN paper : \u201c the means and variances are fixed during inference. \u201d ( Ioffe & Szegedy , 2015 ) . In practice , many methods \u201c freeze the batch normalization parameters to those estimated during ImageNet pre-training \u201d ( \u201c Speed/accuracy trade-offs for modern convolutional object detectors \u201d ) for detection and segmentation tasks . Thus , we think modulating BN statistics in the inference is far from been explored , especially for the domain adaptation task . 2.About section 3.3 and section 4.3.1 Thanks for your suggestions . We have updated our paper . ( 1 ) We have revised section 3.3 to make it clearer and merge it to the section 3.2 . ( 2 ) We have removed the original section 4.3.1 . At the same time , we also added a new analysis section \u201c Adaptation Effect for Different BN Layers \u201d ."}, {"review_id": "BJuysoFeg-2", "review_text": "Overall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper. Detailed comments: Section 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-SNE embedding. This is not the particular feature of batch normalization: visualizing a set of intermediate features with AlexNet and one gets the same results. So the premise in section 3.1 is not accurate. Section 3.3: I have the same concern as the other reviewer. It seems to be quite detatched from the general idea of AdaBN. Equation 2 presents an obvious argument that the combined BN-fully_connected layer forms a linear transform, which is true in the original BN case and in this case as well. I do not think it adds much theoretical depth to the paper. (In general the novelty of this paper seems low) Experiments: - section 4.3.1 is not an accurate measure of the \"effectiveness\" of the proposed method, but a verification of a simple fact: previously, we normalize the source domain features into a Gaussian distribution. the proposed method is explicitly normalizing the target domain features into the same Gaussian distribution as well. So, it is obvious that the KL divergence between these two distributions are closer - in fact, one is *explicitly* making them close. However, this does not directly correlate to the effectiveness of the final classification performance. - section 4.3.2: the sensitivity analysis is a very interesting read, as it suggests that only a very few number of images are needed to account for the domain shift in the AdaBN parameter estimation. This seems to suggest that a single \"whitening\" operation is already good enough to offset the domain bias (in both cases shown, a single batch is sufficient to recover about 80% of the performance gain, although I cannot get data for even smaller number of examples from the figure). It would thus be useful to have a comparison between these approaches, and also a detailed analysis of the effect from each layer of the model - the current analysis seems a bit thin.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your insightful comments and suggestions of our work . 1.About section 3.1 Thanks for your suggestion and we have updated our writing . However , we think aligning the distribution of training data is not just a side effect . It is the key way to achieve the purpose of BN which is to avoid the problem of vanishing gradients and help optimization . In original BN paper , the authors \u2019 motivation is to address the problem of \u201c internal covariate shift \u201d , which means \u201c the change in the distributions of layers \u2019 inputs \u201d . Thus , BN is proposed to \u201c reduce internal covariate shift \u201d and make \u201c the distribution of nonlinearity inputs more stable \u201d . ( 2 ) We also directly visualize the intermediate features with Inception-BN network instead of our BN features in Sec.3.1.The figure can be seen at this link ( https : //s30.postimg.org/fdamc2l1t/a2d_feature_tsne.png ) . Red circles are features of samples from training domain ( Amazon ) while blue ones are testing features ( DSLR ) . It blends much more than that in Figure 2 . This demonstrates the statistics of BN layer indeed contain the traits of the data domain . The features of individual image can not be separated directly in terms of different domains . 2.About section 3.3 and section 4.3.1 Thanks for your suggestion . We have revised section 3.3 to make it clearer and merged it to the section 3.2 . According to your and Reviewer2 \u2019 s suggestion , we remove the previous section 4.3.1 . 3.About section 4.3.2 Thanks for your suggestion . We have updated additional experimental results in the revised paper . ( 1 ) We have experiments with smaller number of samples and found that the performance will drop more ( e.g.0.652 with 16 samples , 0.661 with 32 samples . ) We have updated the results in the section \u201c Sensitivity to target domain size \u201d ( section 4.3.1 now ) . ( 2 ) In the section \u201c Adaptation Effect for Different BN Layers \u201d ( section 4.3.2 now ) , we add the detailed analysis of adaptation effect for different BN layers of our AdaBN method ."}], "0": {"review_id": "BJuysoFeg-0", "review_text": "Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating. This paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset. Overall, I think the paper is more suitable for a workshop track rather than for the main conference track. My main concerns are the following: 1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so). 2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks). ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your insightful comments and suggestions of our work . 1.About writing . We have updated the writing of abstract to make our idea clearer . 2.About experiments . ( 1 ) We agree with the reviewer that same base network should be used for a fair comparison . Thus , we tried our best to transfer other methods to the Inception-BN model . As shown in our answers to the pre-review questions , we have implemented SA , GFK , LSSA and CORAL to reproduce results based on Inception-BN . For other methods , we had not reproduced promising results . This may be due to some missing implementation details ( some methods do not have public implementations ) or the uniqueness of AlexNet ( AlexNet has intrinsic different structure with Inception ) . Thus , we do not report these controversial results . ( 2 ) We respectfully disagree with the reviewer \u2019 s suggestion of training a model from scratch . Currently , most computer vision tasks ( including classification , detection and segmentation ) use an pre-trained model , and it has significant impact on the performance . For our domain adaptation task ( especially for Office dataset ) , the number of images is too small ( e.g.500 images for DSLR domain ) compared to that of ImageNet dataset ( 1M images ) . Training a DNN like AlexNet or Inception-BN with such amount images would suffer from severe over-fitting and significantly drop the performance . Thus , we think this comparison with a model trained from scratch is meaningless . It can not manifest the difference of different methods ."}, "1": {"review_id": "BJuysoFeg-1", "review_text": "This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain. Pros: The method is very simple and easy to understand and apply. The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks. The analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network. Cons: There is little novelty -- the method is arguably too simple to be called a \u201cmethod.\u201d Rather, it\u2019s the most straightforward/intuitive approach when using a network with batch normalization for domain adaptation. The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what\u2019s done in the Inception BN results in Table 1-2?) The analysis in section 4.3.1 is superfluous except as a sanity check -- KL divergence between the distributions should be 0 when each distribution is shifted/scaled to N(0,1) by BN. Section 3.3: it\u2019s not clear to me what point is being made here. Overall, there\u2019s not much novelty here, but it\u2019s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with \u201cFrustratingly Easy Domain Adaptation\u201d). If accepted, Sections 4.3.1 and 3.3 should be removed or rewritten for clarity for a final version.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your insightful comments and suggestions of our work . 1.As mentioned in the original BN paper : \u201c the means and variances are fixed during inference. \u201d ( Ioffe & Szegedy , 2015 ) . In practice , many methods \u201c freeze the batch normalization parameters to those estimated during ImageNet pre-training \u201d ( \u201c Speed/accuracy trade-offs for modern convolutional object detectors \u201d ) for detection and segmentation tasks . Thus , we think modulating BN statistics in the inference is far from been explored , especially for the domain adaptation task . 2.About section 3.3 and section 4.3.1 Thanks for your suggestions . We have updated our paper . ( 1 ) We have revised section 3.3 to make it clearer and merge it to the section 3.2 . ( 2 ) We have removed the original section 4.3.1 . At the same time , we also added a new analysis section \u201c Adaptation Effect for Different BN Layers \u201d ."}, "2": {"review_id": "BJuysoFeg-2", "review_text": "Overall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper. Detailed comments: Section 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-SNE embedding. This is not the particular feature of batch normalization: visualizing a set of intermediate features with AlexNet and one gets the same results. So the premise in section 3.1 is not accurate. Section 3.3: I have the same concern as the other reviewer. It seems to be quite detatched from the general idea of AdaBN. Equation 2 presents an obvious argument that the combined BN-fully_connected layer forms a linear transform, which is true in the original BN case and in this case as well. I do not think it adds much theoretical depth to the paper. (In general the novelty of this paper seems low) Experiments: - section 4.3.1 is not an accurate measure of the \"effectiveness\" of the proposed method, but a verification of a simple fact: previously, we normalize the source domain features into a Gaussian distribution. the proposed method is explicitly normalizing the target domain features into the same Gaussian distribution as well. So, it is obvious that the KL divergence between these two distributions are closer - in fact, one is *explicitly* making them close. However, this does not directly correlate to the effectiveness of the final classification performance. - section 4.3.2: the sensitivity analysis is a very interesting read, as it suggests that only a very few number of images are needed to account for the domain shift in the AdaBN parameter estimation. This seems to suggest that a single \"whitening\" operation is already good enough to offset the domain bias (in both cases shown, a single batch is sufficient to recover about 80% of the performance gain, although I cannot get data for even smaller number of examples from the figure). It would thus be useful to have a comparison between these approaches, and also a detailed analysis of the effect from each layer of the model - the current analysis seems a bit thin.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your insightful comments and suggestions of our work . 1.About section 3.1 Thanks for your suggestion and we have updated our writing . However , we think aligning the distribution of training data is not just a side effect . It is the key way to achieve the purpose of BN which is to avoid the problem of vanishing gradients and help optimization . In original BN paper , the authors \u2019 motivation is to address the problem of \u201c internal covariate shift \u201d , which means \u201c the change in the distributions of layers \u2019 inputs \u201d . Thus , BN is proposed to \u201c reduce internal covariate shift \u201d and make \u201c the distribution of nonlinearity inputs more stable \u201d . ( 2 ) We also directly visualize the intermediate features with Inception-BN network instead of our BN features in Sec.3.1.The figure can be seen at this link ( https : //s30.postimg.org/fdamc2l1t/a2d_feature_tsne.png ) . Red circles are features of samples from training domain ( Amazon ) while blue ones are testing features ( DSLR ) . It blends much more than that in Figure 2 . This demonstrates the statistics of BN layer indeed contain the traits of the data domain . The features of individual image can not be separated directly in terms of different domains . 2.About section 3.3 and section 4.3.1 Thanks for your suggestion . We have revised section 3.3 to make it clearer and merged it to the section 3.2 . According to your and Reviewer2 \u2019 s suggestion , we remove the previous section 4.3.1 . 3.About section 4.3.2 Thanks for your suggestion . We have updated additional experimental results in the revised paper . ( 1 ) We have experiments with smaller number of samples and found that the performance will drop more ( e.g.0.652 with 16 samples , 0.661 with 32 samples . ) We have updated the results in the section \u201c Sensitivity to target domain size \u201d ( section 4.3.1 now ) . ( 2 ) In the section \u201c Adaptation Effect for Different BN Layers \u201d ( section 4.3.2 now ) , we add the detailed analysis of adaptation effect for different BN layers of our AdaBN method ."}}