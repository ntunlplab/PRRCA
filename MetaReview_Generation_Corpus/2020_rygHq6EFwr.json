{"year": "2020", "forum": "rygHq6EFwr", "title": "GResNet: Graph Residual Network for Reviving Deep GNNs from Suspended Animation", "decision": "Reject", "meta_review": "This paper studies the \u201csuspended animation limit\u201d of various graph neural networks (GNNs) and provides some theoretical analysis to explain its cause. To overcome the limitation, the authors propose Graph Residual Network (GRESNET) framework to involve nodes\u2019 raw features or intermediate representations throughout the graph for all the model layers. The main concern of the reviewers is: the assumption made for theoretical analysis that the fully connected layer is identical mapping is too stringent. The paper does not gather sufficient support from the reviewers to merit acceptance, even after author response and reviewer discussion.  I thus recommend reject.", "reviews": [{"review_id": "rygHq6EFwr-0", "review_text": "The paper studies the causes of the empirically poor performance in deep structures that plagues existing GNNs, and identify the suspended animation problem as the main issue. In analogy to the Residual CNN network, a residual graph network is proposed to address such issue. Moreover, the underlying Markov chain property such as stationary distribution is theoretically analyzed, the so-called suspended animation limit is defined and its upper and lower bounds are established. Empirical experiments are relatived short and less sufficient, with comparisons on there datasets: Cora, Citeseer, and Pubmed. It would be more convincing to present its performance on a more diverse range of datasets. Note the results on Citeseer is inferior to existing method. It is helpful to clearly explain why this could be the case. Post rebuttal edition: After reading the reviews and the authors' reply, several questions such as the major concerns over this oversimplified linear assumptions surface out, as discussed in length by other reviewers. Meanwhile, I still believe there are useful merits of this paper. Thus I adjust my current rating to weak accept. ", "rating": "6: Weak Accept", "reply_text": "First of all , we would like to appreciate the reviewer for the support and the constructive suggestions . * * * * * * * * * * * * * * * * * * Question 1 : Empirical experiments on more diverse datasets other than Cora , Citeseer and Pubmed . We clarify that Cora , Citeseer and Pubmed are the top three commonly used benchmark datasets in graph neural network studies , e.g. , [ 1 ] [ 2 ] [ 3 ] [ 4 ] [ 5 ] [ 6 ] [ 7 ] [ 8 ] etc . For comparison fairness with the existing works , we only show the experimental results on these three datasets in this paper . We do have also witnessed several other datasets used in the recent graph neural network papers , but majority of them are still the variants of Cora , Citeseer and Pubmed ( by graph sampling or including more labeled data instances ) . The remaining ones are either private or rarely used by the existing papers . The reviewers can also refer to the webpage for more information about the major datasets studied by the community on graph neural network studies . https : //paperswithcode.com/task/node-classification During the rebuttal period , we tried to get the experimental results of GResNet on the PPI and Reddit datasets as well , which are two other public benchmark dataset used in some graph neural network works [ 2 ] [ 8 ] . The experimental results are reported in the updated version of the submitted PDF paper . The reviewer can refer to Table 4 in the appendix ( on page 15 ) of the updated PDF submitted above for more information about the experimental results . We need to mention that GAT used in GResNet is very slow on large datasets ( it is not our problem but the problem with the GAT base model ) , we can not get the results of GResNet ( GAT ) out on Reddit during the rebuttal period . Also since PPI and Reddit are not common used in other graph neural network papers , many of the entries in Table 4 are not provided . We will add these results in the camera-ready version of this paper later . We will seek for more public graph benchmark datasets as suggested by the reviewer . It may take longer time than the rebuttal time allows , so we plan to add the comparison experiments in the camera-ready version of this paper as well . * * * * * * * * * * * * * * * * * * Question 2 : Inferior experimental results on the Citeseer compared with the existing works . We clarify that the GResNet model proposed in this paper differs a lot from the latest graph neural network models , i.e. , APPNP , GOCN as reported in Table 3 . Instead of introducing new and complex model architectures ( e.g. , APPNP ) or complicated optimization approaches ( e.g. , GOCN ) merely for learning performance improvement , we aim to introduce a general framework , which can work for any base models to revive them from the suspended animation problem instead . So , the objective differences may lead to slightly different experimental performance of the models . As pointed out by the reviewer , the learning performance of our model GResNet ( GCN ) and GResNet ( LoopyNet ) is slightly inferior to APPNP and GOCN . Besides the main objective differences mentioned above for these different works , it can also be caused by : ( 1 ) weak base models GCN and LoopyNet used in these two methods , since we observe that GResNet ( GAT ) can achieve the best performance among all the comparison methods ( also better than APPNP and GOCN ) ; ( 2 ) slightly different learning settings , APPNP allows the model to involve more labeled training data for model learning , which may lead to slightly higher scores . However , we strictly follow the conventional learning setting ( labeled data ratio ) for our GResNet methods . The experimental results provided in Table 3 is just to provide the latest research results obtained by the current papers just for the reviewers ' and readers ' information . So , the slightly inferior performance of GResNet than APPNP does n't necessarily indicate that GResNet is not good . The main objective of this paper is still focused on studying the suspended animation problem with deep graph neural network models , not to compare the learning scores . * * * * * * * * * * * * * * * * * * Hope our response has resolved your concerns . If there is any proposed question about this paper not resolved in our response , welcome to let us know and we are happy to discuss more with you . References used in the above response : [ 1 ] GCN : Semi-Supervised Classification with Graph Convolutional Networks [ 2 ] GAT : Graph Attention Networks [ 3 ] DGI : Deep Graph Infomax [ 4 ] GraphStar : Graph Star Net for Generalized Multi-Task Learning [ 5 ] APPNP : Predict then Propagate : Graph Neural Networks meet Personalized PageRank [ 6 ] GOCN : Graph Optimized Convolutional Networks [ 7 ] GraphNAS : GraphNAS : Graph Neural Architecture Search with Reinforcement Learning [ 8 ] FastGCN : Fast Learning with Graph Convolutional Networks via Importance Sampling"}, {"review_id": "rygHq6EFwr-1", "review_text": "In this paper, the authors study the problem of adding residual connection to GNN for node classification. The authors first identify the problem referred to as \u201cSuspended Animation\u201d in GNN when the depth increases. Then the authors provide both empirical and theoretical characterization for the behavior. To handle this issue, the authors study several different ways to add residual connections in GNN including the na\u00efve method proposed in Kipf et al. The authors carry out extensive experiments on three datasets on different residual connections for the node classification task. Strength 1. The authors identify and study an interesting and important issue in GNN as the \u201cSuspended Animation\u201d issue. 2. The authors provide both empirical and theoretical analysis for the \u201cSuspended Animation\u201d behavior. Moreover, the authors provide theoretical justification for the added residual connection in GNN as gradient norm bound. 3. The authors carry out extensive experiments on different variants of residual links. Morover, the authors provide code online for reproducibility. Weakness: 1. In the theoretical analysis, the assumption that the FC layer is identical mapping is too simplistic. The analysis differs from the actual model especially when the residual links are considered in equation (8), where we have a sum of FC layer output and residual connection. Actually, the empirical results show that na\u00efve residual links work pretty good on several datasets. It goes against the analysis in Corollary 1. 2. Though the authors provide bound on the norm of gradient under residual links, it would be better if authors could justify the adding of residual link from the perspective of Theorem 1. 3. The authors study the depth of GNN up to 7 layers at most. It would be interesting to see how the model performs under really deep networks. 4. The authors mention several factors to affect GNN in section 4.2. It would be interesting to see how these factors like training data set and feature coding interacts with different ways of adding residual connections. 5. It is very informative that the authors compare their methods on the widely used three datasets. It would be better if the authors could carry out experiment on larger graphs to verify the empirical observations. For example, do we need to have deeper networks for larger graphs? ", "rating": "3: Weak Reject", "reply_text": "* * * * * * * * * * * * * * * * * * Question 6 : Experiments on larger graph datasets . Response : We clarify that Cora , Citeseer and Pubmed are the top three commonly used benchmark datasets in graph neural network studies , e.g. , [ 1 ] [ 2 ] [ 3 ] [ 4 ] [ 5 ] [ 6 ] [ 7 ] [ 8 ] etc . For comparison fairness with the existing works , we only show the experimental results on these three datasets in this paper . We do have also witnessed several other datasets used in the recent graph neural network papers , but majority of them are still the variants of Cora , Citeseer and Pubmed ( by graph sampling or including more labeled data instances ) . The remaining ones are either private or rarely used by the existing papers . The reviewers can also refer to the webpage for more information about the major datasets studied by the community on graph neural network studies . https : //paperswithcode.com/task/node-classification During the rebuttal period , as suggested by the reviewer , we also tried to get the experimental results of GResNet on the PPI and Reddit datasets as well , which are other public benchmark dataset used in graph neural network works [ 2 ] [ 8 ] . The experimental results are reported in the updated version of the submitted PDF paper . The reviewer can refer to updated Table 4 in the appendix of the paper ( on page 15 ) more information about the experimental results . We need to mention that GAT used in GResNet is very slow on large datasets ( it is not our problem but the problem with the GAT base model ) , we can not get the results of GResNet ( GAT ) out on Reddit during the rebuttal period . Since PPI and Reddit are not common used in other graph neural network papers , so many of the entries in Table 4 are not provided . We will add these results in the camera-ready version of this paper later . We will seek for more public graph benchmark datasets as suggested by the reviewer . It may take longer time than the rebuttal time allows , so we plan to add the comparison experiments in the camera-ready version of this paper instead . * * * * * * * * * * * * * * * * * * Hope our response has resolved your concerns . If there is any proposed question about this paper not resolved in our response , welcome to let us know and we are happy to discuss more with you . References used in the above response : [ 1 ] GCN : Semi-Supervised Classification with Graph Convolutional Networks [ 2 ] GAT : Graph Attention Networks [ 3 ] DGI : Deep Graph Infomax [ 4 ] GraphStar : Graph Star Net for Generalized Multi-Task Learning [ 5 ] APPNP : Predict then Propagate : Graph Neural Networks meet Personalized PageRank [ 6 ] GOCN : Graph Optimized Convolutional Networks [ 7 ] GraphNAS : GraphNAS : Graph Neural Architecture Search with Reinforcement Learning [ 8 ] FastGCN : Fast Learning with Graph Convolutional Networks via Importance Sampling [ 9 ] Improving Random Walk Estimation Accuracy with Uniform Restart ."}, {"review_id": "rygHq6EFwr-2", "review_text": "Summary: This paper studies the \u201csuspended animation limit\u201d of various GNNs \u2013 an important one for how to train a good Graph network. The authors provide sufficient analysis by simplify GNNs as a series of 1-step Markov chains (which is my concern as stated in the section on main issues), while pointing out the limitations quantitatively as in the Theorem 2. Under the assumption, the authors propose several new forms of ResNets for GCNs, which can successfully overcome the limitation. Overall, the motivation of this work is clear and meaninfgful. The proposed residual architecture is effective, and the presentation is clear and easy to understand. However, my main concerns are on the initial assumptions for analyzing the suspension of GNNs. See the following comments. This paper is generally well written and easy to understand. The organization of each part is well-balanced. Originality: To the best of my knowledge, numerous methods (i.e., targeting on applications) address this problem by augmenting A [1] or X [2] with similarity of feature representation learned from other sources. However, this paper specifically analyzes the problem in a principle way. I consider this work is generally novel. [1] X. Wang and A. Gupta. Videos as Space-Time Region Graphs. ECCV 2018. [2] N. Wang, Y. Zhang, Z. Li , Y. Fu, W, Liu, Y. Jiang. Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. ECCV 2018. Significance: The significance lies mostly in motivation and the proposed GResNet. Motivation: This paper studies the phenomenal that GNNs tends to not respond to the input data when certain depth of a network is reached, which the authors called as suspended animation limit. Studying problem is fundamental and important, and also unique since different with CNNs where data are independent, the data instance within GNNs are highly correlated. GResNet: Given the differences, and within the Theorem 2 where the residual formulation of CNNs does not apply to GNNs, the paper also proposes several new formulations, i.e., in figure 2, where the suspension is avoidable and the performance under the same experimental settings is obviously boosted. Main issues: My major concern to this work lies in the assumption used throughout section 3 and 4. At the beginning of Section 3.2, the authors assume that W is identity. Since X is assumed to be column-wise normalized, the nonlinearity is removable. However, this is not true in real cases: W is actually learnable and not bounded. When W is learned to be negative, Relu layer is not removable, and the behavior of the network will be completely different with what the paper depicted. Indeed, GNNs contain stacked linear+nonlinear functions, which cannot be simplified as a linear Markov chain. It is analogy to CNNs, which is not possibly be simplified as a group of average poolings. Minor issues: 1. I agree that under the assumption, eq. (11) shows that the differences between the learned representations are not discriminative, however the claim \u201cmajority of the nodes are of very small degrees\u201d is not justified and only apply to the internet topology in Faloutsos et al. (1999). 2. I feel the \u201cRaw Feature Coding\u201d and the \u201cNetwork Degree Distribution\u201d are sort of repetitive, and the eq. (11) is eq. (12) at the stationary point. ", "rating": "3: Weak Reject", "reply_text": "* * * * * * * * * * * * * * * * * * Question 3 : \u201c majority of the nodes are of very small degrees \u201d is not justified and only apply to the internet topology in Faloutsos et al . ( 1999 ) Response : We clarify that node degree \u201c power-law distribution \u201d is well-known concept in graph studies . It depicts the observation that \u201c majority of the nodes in a graph will have a small degree , and a small amount of the nodes can have a large degree \u201d . If the reviewer has the Faloutsos et al . ( 1999 ) paper available , it is suggested to refer to the Figure 5 and Figure 6 in the paper . These two plots are the representative plots on node degree power-law distribution . It is a log-log plot on node degree ( the x axis ) and the node number ( the y axis ) . From the plot we can observe that majority of the nodes have a degree less than 10 actually . Similar observation has been reported on the bibliographic network data as well in [ 3 ] ( Cora , Citeseer and Pubmed datasets used in this paper are all bibliographic networks actually ) . The reviewer can refer to Figure 6 in [ 3 ] for more information on the related bibliographic networks . We have also cited this related paper in our updated paper submitted above , and also added necessary words to make this concept clearer in Sec 4.2 . [ 3 ] M. E. J. Newman . The structure and function of complex networks * * * * * * * * * * * * * * * * * * Question 4 : I feel the \u201c Raw Feature Coding \u201d and the \u201c Network Degree Distribution \u201d are sort of repetitive , and the eq . ( 11 ) is eq . ( 12 ) at the stationary point . Response : We clarify that \u201c Raw Feature Coding \u201d and \u201c Network Degree Distribution \u201d are not repetitive and they are totally different factors . Their analyses are also very different actually . We want to clarify that Equ ( 11 ) used for the \u201c Network Degree Distribution \u201d is at the stationary point . However , Equ ( 12 ) for \u201c Raw Feature Coding \u201d is based on the raw representation in Equ ( 3 ) and Equ ( 4 ) , which doesn \u2019 t require the representations to be at the stationary point . As suggested by the reviewer , we have also revised and updated the presentations for these two factors in the updated version of this paper just to avoid unnecessary confusions for the readers . The changes are added just after Equ ( 12 ) in the updated PDF of this paper . * * * * * * * * * * * * * * * * * * Hope our response has resolved your concerns . If there is any proposed question about this paper not resolved in our response , welcome to let us know and we are happy to discuss more with you ."}], "0": {"review_id": "rygHq6EFwr-0", "review_text": "The paper studies the causes of the empirically poor performance in deep structures that plagues existing GNNs, and identify the suspended animation problem as the main issue. In analogy to the Residual CNN network, a residual graph network is proposed to address such issue. Moreover, the underlying Markov chain property such as stationary distribution is theoretically analyzed, the so-called suspended animation limit is defined and its upper and lower bounds are established. Empirical experiments are relatived short and less sufficient, with comparisons on there datasets: Cora, Citeseer, and Pubmed. It would be more convincing to present its performance on a more diverse range of datasets. Note the results on Citeseer is inferior to existing method. It is helpful to clearly explain why this could be the case. Post rebuttal edition: After reading the reviews and the authors' reply, several questions such as the major concerns over this oversimplified linear assumptions surface out, as discussed in length by other reviewers. Meanwhile, I still believe there are useful merits of this paper. Thus I adjust my current rating to weak accept. ", "rating": "6: Weak Accept", "reply_text": "First of all , we would like to appreciate the reviewer for the support and the constructive suggestions . * * * * * * * * * * * * * * * * * * Question 1 : Empirical experiments on more diverse datasets other than Cora , Citeseer and Pubmed . We clarify that Cora , Citeseer and Pubmed are the top three commonly used benchmark datasets in graph neural network studies , e.g. , [ 1 ] [ 2 ] [ 3 ] [ 4 ] [ 5 ] [ 6 ] [ 7 ] [ 8 ] etc . For comparison fairness with the existing works , we only show the experimental results on these three datasets in this paper . We do have also witnessed several other datasets used in the recent graph neural network papers , but majority of them are still the variants of Cora , Citeseer and Pubmed ( by graph sampling or including more labeled data instances ) . The remaining ones are either private or rarely used by the existing papers . The reviewers can also refer to the webpage for more information about the major datasets studied by the community on graph neural network studies . https : //paperswithcode.com/task/node-classification During the rebuttal period , we tried to get the experimental results of GResNet on the PPI and Reddit datasets as well , which are two other public benchmark dataset used in some graph neural network works [ 2 ] [ 8 ] . The experimental results are reported in the updated version of the submitted PDF paper . The reviewer can refer to Table 4 in the appendix ( on page 15 ) of the updated PDF submitted above for more information about the experimental results . We need to mention that GAT used in GResNet is very slow on large datasets ( it is not our problem but the problem with the GAT base model ) , we can not get the results of GResNet ( GAT ) out on Reddit during the rebuttal period . Also since PPI and Reddit are not common used in other graph neural network papers , many of the entries in Table 4 are not provided . We will add these results in the camera-ready version of this paper later . We will seek for more public graph benchmark datasets as suggested by the reviewer . It may take longer time than the rebuttal time allows , so we plan to add the comparison experiments in the camera-ready version of this paper as well . * * * * * * * * * * * * * * * * * * Question 2 : Inferior experimental results on the Citeseer compared with the existing works . We clarify that the GResNet model proposed in this paper differs a lot from the latest graph neural network models , i.e. , APPNP , GOCN as reported in Table 3 . Instead of introducing new and complex model architectures ( e.g. , APPNP ) or complicated optimization approaches ( e.g. , GOCN ) merely for learning performance improvement , we aim to introduce a general framework , which can work for any base models to revive them from the suspended animation problem instead . So , the objective differences may lead to slightly different experimental performance of the models . As pointed out by the reviewer , the learning performance of our model GResNet ( GCN ) and GResNet ( LoopyNet ) is slightly inferior to APPNP and GOCN . Besides the main objective differences mentioned above for these different works , it can also be caused by : ( 1 ) weak base models GCN and LoopyNet used in these two methods , since we observe that GResNet ( GAT ) can achieve the best performance among all the comparison methods ( also better than APPNP and GOCN ) ; ( 2 ) slightly different learning settings , APPNP allows the model to involve more labeled training data for model learning , which may lead to slightly higher scores . However , we strictly follow the conventional learning setting ( labeled data ratio ) for our GResNet methods . The experimental results provided in Table 3 is just to provide the latest research results obtained by the current papers just for the reviewers ' and readers ' information . So , the slightly inferior performance of GResNet than APPNP does n't necessarily indicate that GResNet is not good . The main objective of this paper is still focused on studying the suspended animation problem with deep graph neural network models , not to compare the learning scores . * * * * * * * * * * * * * * * * * * Hope our response has resolved your concerns . If there is any proposed question about this paper not resolved in our response , welcome to let us know and we are happy to discuss more with you . References used in the above response : [ 1 ] GCN : Semi-Supervised Classification with Graph Convolutional Networks [ 2 ] GAT : Graph Attention Networks [ 3 ] DGI : Deep Graph Infomax [ 4 ] GraphStar : Graph Star Net for Generalized Multi-Task Learning [ 5 ] APPNP : Predict then Propagate : Graph Neural Networks meet Personalized PageRank [ 6 ] GOCN : Graph Optimized Convolutional Networks [ 7 ] GraphNAS : GraphNAS : Graph Neural Architecture Search with Reinforcement Learning [ 8 ] FastGCN : Fast Learning with Graph Convolutional Networks via Importance Sampling"}, "1": {"review_id": "rygHq6EFwr-1", "review_text": "In this paper, the authors study the problem of adding residual connection to GNN for node classification. The authors first identify the problem referred to as \u201cSuspended Animation\u201d in GNN when the depth increases. Then the authors provide both empirical and theoretical characterization for the behavior. To handle this issue, the authors study several different ways to add residual connections in GNN including the na\u00efve method proposed in Kipf et al. The authors carry out extensive experiments on three datasets on different residual connections for the node classification task. Strength 1. The authors identify and study an interesting and important issue in GNN as the \u201cSuspended Animation\u201d issue. 2. The authors provide both empirical and theoretical analysis for the \u201cSuspended Animation\u201d behavior. Moreover, the authors provide theoretical justification for the added residual connection in GNN as gradient norm bound. 3. The authors carry out extensive experiments on different variants of residual links. Morover, the authors provide code online for reproducibility. Weakness: 1. In the theoretical analysis, the assumption that the FC layer is identical mapping is too simplistic. The analysis differs from the actual model especially when the residual links are considered in equation (8), where we have a sum of FC layer output and residual connection. Actually, the empirical results show that na\u00efve residual links work pretty good on several datasets. It goes against the analysis in Corollary 1. 2. Though the authors provide bound on the norm of gradient under residual links, it would be better if authors could justify the adding of residual link from the perspective of Theorem 1. 3. The authors study the depth of GNN up to 7 layers at most. It would be interesting to see how the model performs under really deep networks. 4. The authors mention several factors to affect GNN in section 4.2. It would be interesting to see how these factors like training data set and feature coding interacts with different ways of adding residual connections. 5. It is very informative that the authors compare their methods on the widely used three datasets. It would be better if the authors could carry out experiment on larger graphs to verify the empirical observations. For example, do we need to have deeper networks for larger graphs? ", "rating": "3: Weak Reject", "reply_text": "* * * * * * * * * * * * * * * * * * Question 6 : Experiments on larger graph datasets . Response : We clarify that Cora , Citeseer and Pubmed are the top three commonly used benchmark datasets in graph neural network studies , e.g. , [ 1 ] [ 2 ] [ 3 ] [ 4 ] [ 5 ] [ 6 ] [ 7 ] [ 8 ] etc . For comparison fairness with the existing works , we only show the experimental results on these three datasets in this paper . We do have also witnessed several other datasets used in the recent graph neural network papers , but majority of them are still the variants of Cora , Citeseer and Pubmed ( by graph sampling or including more labeled data instances ) . The remaining ones are either private or rarely used by the existing papers . The reviewers can also refer to the webpage for more information about the major datasets studied by the community on graph neural network studies . https : //paperswithcode.com/task/node-classification During the rebuttal period , as suggested by the reviewer , we also tried to get the experimental results of GResNet on the PPI and Reddit datasets as well , which are other public benchmark dataset used in graph neural network works [ 2 ] [ 8 ] . The experimental results are reported in the updated version of the submitted PDF paper . The reviewer can refer to updated Table 4 in the appendix of the paper ( on page 15 ) more information about the experimental results . We need to mention that GAT used in GResNet is very slow on large datasets ( it is not our problem but the problem with the GAT base model ) , we can not get the results of GResNet ( GAT ) out on Reddit during the rebuttal period . Since PPI and Reddit are not common used in other graph neural network papers , so many of the entries in Table 4 are not provided . We will add these results in the camera-ready version of this paper later . We will seek for more public graph benchmark datasets as suggested by the reviewer . It may take longer time than the rebuttal time allows , so we plan to add the comparison experiments in the camera-ready version of this paper instead . * * * * * * * * * * * * * * * * * * Hope our response has resolved your concerns . If there is any proposed question about this paper not resolved in our response , welcome to let us know and we are happy to discuss more with you . References used in the above response : [ 1 ] GCN : Semi-Supervised Classification with Graph Convolutional Networks [ 2 ] GAT : Graph Attention Networks [ 3 ] DGI : Deep Graph Infomax [ 4 ] GraphStar : Graph Star Net for Generalized Multi-Task Learning [ 5 ] APPNP : Predict then Propagate : Graph Neural Networks meet Personalized PageRank [ 6 ] GOCN : Graph Optimized Convolutional Networks [ 7 ] GraphNAS : GraphNAS : Graph Neural Architecture Search with Reinforcement Learning [ 8 ] FastGCN : Fast Learning with Graph Convolutional Networks via Importance Sampling [ 9 ] Improving Random Walk Estimation Accuracy with Uniform Restart ."}, "2": {"review_id": "rygHq6EFwr-2", "review_text": "Summary: This paper studies the \u201csuspended animation limit\u201d of various GNNs \u2013 an important one for how to train a good Graph network. The authors provide sufficient analysis by simplify GNNs as a series of 1-step Markov chains (which is my concern as stated in the section on main issues), while pointing out the limitations quantitatively as in the Theorem 2. Under the assumption, the authors propose several new forms of ResNets for GCNs, which can successfully overcome the limitation. Overall, the motivation of this work is clear and meaninfgful. The proposed residual architecture is effective, and the presentation is clear and easy to understand. However, my main concerns are on the initial assumptions for analyzing the suspension of GNNs. See the following comments. This paper is generally well written and easy to understand. The organization of each part is well-balanced. Originality: To the best of my knowledge, numerous methods (i.e., targeting on applications) address this problem by augmenting A [1] or X [2] with similarity of feature representation learned from other sources. However, this paper specifically analyzes the problem in a principle way. I consider this work is generally novel. [1] X. Wang and A. Gupta. Videos as Space-Time Region Graphs. ECCV 2018. [2] N. Wang, Y. Zhang, Z. Li , Y. Fu, W, Liu, Y. Jiang. Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. ECCV 2018. Significance: The significance lies mostly in motivation and the proposed GResNet. Motivation: This paper studies the phenomenal that GNNs tends to not respond to the input data when certain depth of a network is reached, which the authors called as suspended animation limit. Studying problem is fundamental and important, and also unique since different with CNNs where data are independent, the data instance within GNNs are highly correlated. GResNet: Given the differences, and within the Theorem 2 where the residual formulation of CNNs does not apply to GNNs, the paper also proposes several new formulations, i.e., in figure 2, where the suspension is avoidable and the performance under the same experimental settings is obviously boosted. Main issues: My major concern to this work lies in the assumption used throughout section 3 and 4. At the beginning of Section 3.2, the authors assume that W is identity. Since X is assumed to be column-wise normalized, the nonlinearity is removable. However, this is not true in real cases: W is actually learnable and not bounded. When W is learned to be negative, Relu layer is not removable, and the behavior of the network will be completely different with what the paper depicted. Indeed, GNNs contain stacked linear+nonlinear functions, which cannot be simplified as a linear Markov chain. It is analogy to CNNs, which is not possibly be simplified as a group of average poolings. Minor issues: 1. I agree that under the assumption, eq. (11) shows that the differences between the learned representations are not discriminative, however the claim \u201cmajority of the nodes are of very small degrees\u201d is not justified and only apply to the internet topology in Faloutsos et al. (1999). 2. I feel the \u201cRaw Feature Coding\u201d and the \u201cNetwork Degree Distribution\u201d are sort of repetitive, and the eq. (11) is eq. (12) at the stationary point. ", "rating": "3: Weak Reject", "reply_text": "* * * * * * * * * * * * * * * * * * Question 3 : \u201c majority of the nodes are of very small degrees \u201d is not justified and only apply to the internet topology in Faloutsos et al . ( 1999 ) Response : We clarify that node degree \u201c power-law distribution \u201d is well-known concept in graph studies . It depicts the observation that \u201c majority of the nodes in a graph will have a small degree , and a small amount of the nodes can have a large degree \u201d . If the reviewer has the Faloutsos et al . ( 1999 ) paper available , it is suggested to refer to the Figure 5 and Figure 6 in the paper . These two plots are the representative plots on node degree power-law distribution . It is a log-log plot on node degree ( the x axis ) and the node number ( the y axis ) . From the plot we can observe that majority of the nodes have a degree less than 10 actually . Similar observation has been reported on the bibliographic network data as well in [ 3 ] ( Cora , Citeseer and Pubmed datasets used in this paper are all bibliographic networks actually ) . The reviewer can refer to Figure 6 in [ 3 ] for more information on the related bibliographic networks . We have also cited this related paper in our updated paper submitted above , and also added necessary words to make this concept clearer in Sec 4.2 . [ 3 ] M. E. J. Newman . The structure and function of complex networks * * * * * * * * * * * * * * * * * * Question 4 : I feel the \u201c Raw Feature Coding \u201d and the \u201c Network Degree Distribution \u201d are sort of repetitive , and the eq . ( 11 ) is eq . ( 12 ) at the stationary point . Response : We clarify that \u201c Raw Feature Coding \u201d and \u201c Network Degree Distribution \u201d are not repetitive and they are totally different factors . Their analyses are also very different actually . We want to clarify that Equ ( 11 ) used for the \u201c Network Degree Distribution \u201d is at the stationary point . However , Equ ( 12 ) for \u201c Raw Feature Coding \u201d is based on the raw representation in Equ ( 3 ) and Equ ( 4 ) , which doesn \u2019 t require the representations to be at the stationary point . As suggested by the reviewer , we have also revised and updated the presentations for these two factors in the updated version of this paper just to avoid unnecessary confusions for the readers . The changes are added just after Equ ( 12 ) in the updated PDF of this paper . * * * * * * * * * * * * * * * * * * Hope our response has resolved your concerns . If there is any proposed question about this paper not resolved in our response , welcome to let us know and we are happy to discuss more with you ."}}