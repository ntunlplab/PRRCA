{"year": "2019", "forum": "BJlgNh0qKQ", "title": "Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a Structured Variational Autoencoder", "decision": "Accept (Poster)", "meta_review": "This paper proposes a method for unsupervised learning that uses a latent variable generative model for semi-supervised dependency parsing. The key learning method consists of making perturbations to the logits going into a parsing algorithm, to make it possible to sample within the variational auto-encoder framework. Significant gains are found through semi-supervised learning.\n\nThe largest reviewer concern was that the baselines were potentially not strong enough, as significantly better numbers have been reported in previous work, which may have a result of over-stating the perceived utility.\n\nOverall though it seems that the reviewers appreciated the novel solution to an important problem, and in general would like to see the paper accepted.", "reviews": [{"review_id": "BJlgNh0qKQ-0", "review_text": "The paper describes a VAE-based approach to semi-supervised learning of dependency parsing. The encoder in the VAE is a neural edge-factored parser allowing inference using Eisner's dynamic programming algorithms. The decoder generates sentences left-to-right, at each point conditioning on head-modifier dependencies specified by the tree. A key technical step is to develop a method for \"differentiable\" sampling/parsing, using a modification of the dynamic program, and the Gumbel-max trick. I thought this was an excellent paper - very clear, an important problem, a very useful set of techniques and results. I would strongly recommend acceptance. Some comments: * I do wonder how well this approach would work with orders of magnitude more unlabeled data. The amount of unlabeled data used is quite small. * Similarly, I wonder how well the approach works as the amount of unlabeled data is decreased (or increased, for that matter). It should be possible to provide graphs showing this. * Are there natural generalizations to multi-lingual data, for example settings where supervised data is only available for languages other than the language of interest? * It would be interesting to see an analysis of accuracy improvements on different dependency labels. The \"root\" case is in some sense just one of the labels (nsubj, dobj, prep, etc.) that could be analyzed. * I wonder also if this method would be particularly helpful in domain transfer, for example from Wall Street Journal text to Wikipedia or Web data in general. The improvements could be more dramatic in this case - that kind of effect has been seen with ELMO for example.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Many thanks for the positive feedback and suggestions . > Varying amounts of unlabeled data We will do our best to include these results in a subsequent revision . Using more unlabeled data is harder for Swedish and French , as we would need to re-tokenize in the form consistent with our labeled data . > Are there natural generalizations to multi-lingual data for example settings where supervised data is only available for languages other than the language of interest ? This is a very interesting direction . We hope that using \u2018 unlabeled \u2019 and \u2018 labeled \u2019 terms in the objective would make the multilingual model capture correspondences between surface regularities and the underlying syntax , for a given language . This should be especially helpful in the suggested one-shot learning scenario , where only unlabeled term will present for the target language . We suspect that part-of-speech tags ( not currently used in our model ) would be needed to facilitate learning the cross-lingual correspondences . > I wonder also if this method would be particularly helpful in domain transfer Yes , we would like to look into this in the future work . > It would be interesting to see an analysis of accuracy improvementson different dependency labels . We performed analysis on English , there are some interesting cases : 1 . Multi-word expressions : the recall / precision scores of the semi-supervised model are 90.70 / 84.78 while the one of the supervised model are 75.58 / 81.25 . We suspect that the reason is that MWEs are relatively infrequent . 2.Adverbial modifiers : we observe an increase in precision without compromising on recall : 87.32 / 87.51 versus 87.27 / 85.95 . 3.Appositional modifiers : we also observe a significant increase for the recall in this category : 81.39 / 81.03 versus 77.49 / 80.27 We included the results in the new version of the paper ."}, {"review_id": "BJlgNh0qKQ-1", "review_text": "[Summary] This paper proposes to do semi-supervised learning , via a generative model, of an arc-factored dependency parser by using amortized variational inference. The parse tree is the latent variable, the parser is the encoder that maps a sentence to a distribution over parse-trees, and the decoder is a generative model that maps a parse tree to a distribution over sentences. [Pros] Semi-supervised learning for dependency parsing is both important and difficult and this paper presents a novel approach using variational auto-encoders. And the semi-supervised learning method in this paper gives a small but non-zero improvement over a reasonably strong baseline. [Cons] 1. My main concern with this paper currently are the \"explanations\" provided in the paper which are quite hand-wavy. E.g. the authors state that using a KL term in semi-supervised learning is exactly opposite to the \"low density separation assumption\". And therefore they set the KL term to be zero. One has to wonder that why is the \"low density separation assumption\" so critical for dependency parsing only? VAEs have been used with a prior for semi-supervised learning before, why didn't this assumption affect those models ? A better explanation will have been that since the authors first trained the parser in a supervised fashion, therefore their inference network already represents a \"good\" distribution over parses, even though this distribution is specified only upto sampling but not in a mathematically closed form. Finally, setting the KL divergence between the posterior of the inference network and the prior to be zero is the same as dynamically specifying the prior to be the same as the inference network's distribution. 2. A number of important details are missing in the submitted version of the paper which the authors addressed in their reply to my public comment. 3. The current paper does not contain any comparison to self-training which is a natural baseline for this work. The authors replied to my comment saying that self-training requires a number of heuristics but it's not clear to me how much more difficult can these heuristics be than the tuning required for training their VAE.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your suggestions and the positive feedback . > hand-wavy explanations We toned down our speculation , and incorporated your suggestions . Please let us know if you think , we could improve this further . > A number of important details are missing in the submitted version of the paper which the authors addressed in their reply to my public comment . The submission has now been updated , reflecting what we described in our public comment ."}, {"review_id": "BJlgNh0qKQ-2", "review_text": "This paper proposed a variational autoencoder-based method for semi-supervised dependency parsing. Given an input sentence s, an LSTM-based encoder generates a sentence embedding z, and a NN of Kiperwasser & Goldberg (2016) generates a dependency structure T. Gradients over the tree encoder are approximated by (1) adding a perturbation matrix over the weight matrix and (2) relax dynamic programming-based parsing algorithm to a differentiable format. The decoder combines standard LSTM and Graph Convolutional Network to generate the input sentence from z and T. The authors evaluated the proposed method on three languages, using 10% of the original training data as labeled and the rest as unlabeled data. Pros 1. I like the idea of this sentence->tree->sentence autoencoder for semi-supervised parsing. The authors proposed a novel and nice way to tackle key challenges in gradient computation. VAE involves marginalization over all possible dependency trees, which is computationally infeasible, and the proposed method used a Gumbel-Max trick to approximate it. The tree inference procedure involves non-differentiable structured prediction, and the authors used a peaked-softmax method to address the issue. The whole model is fully differentiable and can be thus trained end to end. 2. The direction of semi-supervised parsing is useful and promising, not only for resource-poor languages, but also for popular languages like English. A successful research on this direction could be potentially helpful for lots of future work. Cons, and suggestions on experiments My main concerns are around experiments. Overall I think they are not strong enough to demonstrate that this paper has sufficient contribution to semi-supervised parsing. Below are details. 1. The current version only used 10% of original training data as labeled and the rest as unlabeled data. This makes the reported numbers way below existing state-of-the-art performance. For example, the SOTA UAS on English PTB has been >95%. Ideally, the authors should be able to train a competitive supervised parser on full training data (English or other languages), and get huge amount of unlabeled data from other sources (e.g. News) to further push up the performance. The current setting makes it hard to justify how useful the proposed method could be in practice. 2. The best numbers from the proposed model is lower than baseline (Kipperwasser & Goldberg) on English, and only marginally better on Swedish. This probably means the supervised baseline is weak, and it's hard to tell if the gains from VAE will retain if applied to a stronger supervised. 3. A performance curve with different amount of labeled and unlabeled data would be useful to better understand the impact of semi-supervised learning. 4. What's the impact of perturbation? One could simply use T=Eisner(W) as approximation. Did you observe any significant benefits from sampling? Other questions 1. What's the impact of keeping the tree constraint on dependencies during backpropagation? Have you tried removing the tree constraint like previous work? 2. Are sentence embedding and trees generated from two separate LSTM encoders? Are there any parameter sharing between the two? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comments and for finding the method novel and interesting . We would like first to clarify that we are not making claiming that our method is appropriate in the high resource scenario ( i.e.full in-domain English PTB parsing ) . However , large datasets are available only for a few languages , so the lower resource setting we study here is important and common . We use a sufficiently strong baseline ( e.g. , already using external word embeddings ) and obtain improvements across all 3 languages . Interestingly , we observe that there are certain phenomena which our semi-supervised parser captures considerably more accurately than the baseline model ( e.g. , long distance dependencies and multi-word expression , see reply to R1 ) . Very few studies have been done for semi-supervised structured prediction with neural generative models , especially for the more challenging parsing task , so we think these results are interesting . We also think that our differentiable perturb-and-parse operator is interesting on its own , and has other potential applications . For example , it could be used in the context of latent structure induction , where there is no supervision ( i.e.no treebank ) . Our sampling technique has properties which are different from those of previously proposed latent induction methods : - unlike structured attention [ 4 ] , we sample global structures rather than compute marginals ( e.g. , we preserve higher-order statistics ) - unlike SPIGOT [ 2 ] , we can impose tree constraints directly rather than compute an approximation - unlike us , [ 3 ] relies on sparse distributions so that marginalization is feasible . While sparse distributions have many interesting properties , they yield flat areas in the optimization landscape that can be difficult to escape from . - unlike sampling with shift-reduce parsing models , we do not seem to have issues with bias which was argued to negatively affect its results [ 1 ] . > A performance curve with different amount of labeled and unlabeled data We will do our best to include these results in a subsequent revision . Using more unlabeled data is harder for Swedish and French , as we would need to re-tokenize in the form consistent with our labeled data . > What 's the impact of perturbation ? In our experiments , using sampling is beneficial so that improvements are consistent across languages . For example , UAS results in French for the model that does not us sentence embeddings are as follows : - supervised : 84.09 - semi-supervised without sampling : 84.27 - semi-supervised with sampling : 84.69 > What 's the impact of keeping the tree constraint on dependencies during backpropagation ? We thought that the main motivation for dropping the constraint in previous work ( e.g. , SPIGOT ) was efficiency . Since it does not seriously affect computation cost in our approach , we have not experimented with dropping it . > Are sentence embedding and trees generated from two separate LSTM encoders ? Yes.There are no shared parameters in our model : the LSTM of the parser , the LSTM generating the sentence embeddings and the decoder are all separate . Introducing parameter sharing would likely be beneficial . However , our set-up is more controlled , as we can make sure that the improvements are due to modeling latent syntactic structure rather than getting better word representations ( i.e.from using the multi-task learning objective ) . [ 1 ] Andrew Drozdov and Samuel Bowman , The Coadaptation Problem when Learning How and What to Compose ( 2nd Workshop on Representation Learning for NLP , 2017 ) [ 2 ] Hao Peng , Sam Thomson and Noah Smith , Backpropagating through Structured Argmax using a SPIGOT ( ACL 2018 ) [ 3 ] Vlad Niculae , Andr\u00e9 Martins and Claire Cardie , Towards Dynamic Computation Graphs via Sparse Latent Structure ( EMNLP 2018 ) [ 5 ] Yoon Kim , Carl Denton , Luong Hoang and Alexander Rush , Structured Attention Networks ( ICLR 2017 )"}], "0": {"review_id": "BJlgNh0qKQ-0", "review_text": "The paper describes a VAE-based approach to semi-supervised learning of dependency parsing. The encoder in the VAE is a neural edge-factored parser allowing inference using Eisner's dynamic programming algorithms. The decoder generates sentences left-to-right, at each point conditioning on head-modifier dependencies specified by the tree. A key technical step is to develop a method for \"differentiable\" sampling/parsing, using a modification of the dynamic program, and the Gumbel-max trick. I thought this was an excellent paper - very clear, an important problem, a very useful set of techniques and results. I would strongly recommend acceptance. Some comments: * I do wonder how well this approach would work with orders of magnitude more unlabeled data. The amount of unlabeled data used is quite small. * Similarly, I wonder how well the approach works as the amount of unlabeled data is decreased (or increased, for that matter). It should be possible to provide graphs showing this. * Are there natural generalizations to multi-lingual data, for example settings where supervised data is only available for languages other than the language of interest? * It would be interesting to see an analysis of accuracy improvements on different dependency labels. The \"root\" case is in some sense just one of the labels (nsubj, dobj, prep, etc.) that could be analyzed. * I wonder also if this method would be particularly helpful in domain transfer, for example from Wall Street Journal text to Wikipedia or Web data in general. The improvements could be more dramatic in this case - that kind of effect has been seen with ELMO for example.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Many thanks for the positive feedback and suggestions . > Varying amounts of unlabeled data We will do our best to include these results in a subsequent revision . Using more unlabeled data is harder for Swedish and French , as we would need to re-tokenize in the form consistent with our labeled data . > Are there natural generalizations to multi-lingual data for example settings where supervised data is only available for languages other than the language of interest ? This is a very interesting direction . We hope that using \u2018 unlabeled \u2019 and \u2018 labeled \u2019 terms in the objective would make the multilingual model capture correspondences between surface regularities and the underlying syntax , for a given language . This should be especially helpful in the suggested one-shot learning scenario , where only unlabeled term will present for the target language . We suspect that part-of-speech tags ( not currently used in our model ) would be needed to facilitate learning the cross-lingual correspondences . > I wonder also if this method would be particularly helpful in domain transfer Yes , we would like to look into this in the future work . > It would be interesting to see an analysis of accuracy improvementson different dependency labels . We performed analysis on English , there are some interesting cases : 1 . Multi-word expressions : the recall / precision scores of the semi-supervised model are 90.70 / 84.78 while the one of the supervised model are 75.58 / 81.25 . We suspect that the reason is that MWEs are relatively infrequent . 2.Adverbial modifiers : we observe an increase in precision without compromising on recall : 87.32 / 87.51 versus 87.27 / 85.95 . 3.Appositional modifiers : we also observe a significant increase for the recall in this category : 81.39 / 81.03 versus 77.49 / 80.27 We included the results in the new version of the paper ."}, "1": {"review_id": "BJlgNh0qKQ-1", "review_text": "[Summary] This paper proposes to do semi-supervised learning , via a generative model, of an arc-factored dependency parser by using amortized variational inference. The parse tree is the latent variable, the parser is the encoder that maps a sentence to a distribution over parse-trees, and the decoder is a generative model that maps a parse tree to a distribution over sentences. [Pros] Semi-supervised learning for dependency parsing is both important and difficult and this paper presents a novel approach using variational auto-encoders. And the semi-supervised learning method in this paper gives a small but non-zero improvement over a reasonably strong baseline. [Cons] 1. My main concern with this paper currently are the \"explanations\" provided in the paper which are quite hand-wavy. E.g. the authors state that using a KL term in semi-supervised learning is exactly opposite to the \"low density separation assumption\". And therefore they set the KL term to be zero. One has to wonder that why is the \"low density separation assumption\" so critical for dependency parsing only? VAEs have been used with a prior for semi-supervised learning before, why didn't this assumption affect those models ? A better explanation will have been that since the authors first trained the parser in a supervised fashion, therefore their inference network already represents a \"good\" distribution over parses, even though this distribution is specified only upto sampling but not in a mathematically closed form. Finally, setting the KL divergence between the posterior of the inference network and the prior to be zero is the same as dynamically specifying the prior to be the same as the inference network's distribution. 2. A number of important details are missing in the submitted version of the paper which the authors addressed in their reply to my public comment. 3. The current paper does not contain any comparison to self-training which is a natural baseline for this work. The authors replied to my comment saying that self-training requires a number of heuristics but it's not clear to me how much more difficult can these heuristics be than the tuning required for training their VAE.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your suggestions and the positive feedback . > hand-wavy explanations We toned down our speculation , and incorporated your suggestions . Please let us know if you think , we could improve this further . > A number of important details are missing in the submitted version of the paper which the authors addressed in their reply to my public comment . The submission has now been updated , reflecting what we described in our public comment ."}, "2": {"review_id": "BJlgNh0qKQ-2", "review_text": "This paper proposed a variational autoencoder-based method for semi-supervised dependency parsing. Given an input sentence s, an LSTM-based encoder generates a sentence embedding z, and a NN of Kiperwasser & Goldberg (2016) generates a dependency structure T. Gradients over the tree encoder are approximated by (1) adding a perturbation matrix over the weight matrix and (2) relax dynamic programming-based parsing algorithm to a differentiable format. The decoder combines standard LSTM and Graph Convolutional Network to generate the input sentence from z and T. The authors evaluated the proposed method on three languages, using 10% of the original training data as labeled and the rest as unlabeled data. Pros 1. I like the idea of this sentence->tree->sentence autoencoder for semi-supervised parsing. The authors proposed a novel and nice way to tackle key challenges in gradient computation. VAE involves marginalization over all possible dependency trees, which is computationally infeasible, and the proposed method used a Gumbel-Max trick to approximate it. The tree inference procedure involves non-differentiable structured prediction, and the authors used a peaked-softmax method to address the issue. The whole model is fully differentiable and can be thus trained end to end. 2. The direction of semi-supervised parsing is useful and promising, not only for resource-poor languages, but also for popular languages like English. A successful research on this direction could be potentially helpful for lots of future work. Cons, and suggestions on experiments My main concerns are around experiments. Overall I think they are not strong enough to demonstrate that this paper has sufficient contribution to semi-supervised parsing. Below are details. 1. The current version only used 10% of original training data as labeled and the rest as unlabeled data. This makes the reported numbers way below existing state-of-the-art performance. For example, the SOTA UAS on English PTB has been >95%. Ideally, the authors should be able to train a competitive supervised parser on full training data (English or other languages), and get huge amount of unlabeled data from other sources (e.g. News) to further push up the performance. The current setting makes it hard to justify how useful the proposed method could be in practice. 2. The best numbers from the proposed model is lower than baseline (Kipperwasser & Goldberg) on English, and only marginally better on Swedish. This probably means the supervised baseline is weak, and it's hard to tell if the gains from VAE will retain if applied to a stronger supervised. 3. A performance curve with different amount of labeled and unlabeled data would be useful to better understand the impact of semi-supervised learning. 4. What's the impact of perturbation? One could simply use T=Eisner(W) as approximation. Did you observe any significant benefits from sampling? Other questions 1. What's the impact of keeping the tree constraint on dependencies during backpropagation? Have you tried removing the tree constraint like previous work? 2. Are sentence embedding and trees generated from two separate LSTM encoders? Are there any parameter sharing between the two? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comments and for finding the method novel and interesting . We would like first to clarify that we are not making claiming that our method is appropriate in the high resource scenario ( i.e.full in-domain English PTB parsing ) . However , large datasets are available only for a few languages , so the lower resource setting we study here is important and common . We use a sufficiently strong baseline ( e.g. , already using external word embeddings ) and obtain improvements across all 3 languages . Interestingly , we observe that there are certain phenomena which our semi-supervised parser captures considerably more accurately than the baseline model ( e.g. , long distance dependencies and multi-word expression , see reply to R1 ) . Very few studies have been done for semi-supervised structured prediction with neural generative models , especially for the more challenging parsing task , so we think these results are interesting . We also think that our differentiable perturb-and-parse operator is interesting on its own , and has other potential applications . For example , it could be used in the context of latent structure induction , where there is no supervision ( i.e.no treebank ) . Our sampling technique has properties which are different from those of previously proposed latent induction methods : - unlike structured attention [ 4 ] , we sample global structures rather than compute marginals ( e.g. , we preserve higher-order statistics ) - unlike SPIGOT [ 2 ] , we can impose tree constraints directly rather than compute an approximation - unlike us , [ 3 ] relies on sparse distributions so that marginalization is feasible . While sparse distributions have many interesting properties , they yield flat areas in the optimization landscape that can be difficult to escape from . - unlike sampling with shift-reduce parsing models , we do not seem to have issues with bias which was argued to negatively affect its results [ 1 ] . > A performance curve with different amount of labeled and unlabeled data We will do our best to include these results in a subsequent revision . Using more unlabeled data is harder for Swedish and French , as we would need to re-tokenize in the form consistent with our labeled data . > What 's the impact of perturbation ? In our experiments , using sampling is beneficial so that improvements are consistent across languages . For example , UAS results in French for the model that does not us sentence embeddings are as follows : - supervised : 84.09 - semi-supervised without sampling : 84.27 - semi-supervised with sampling : 84.69 > What 's the impact of keeping the tree constraint on dependencies during backpropagation ? We thought that the main motivation for dropping the constraint in previous work ( e.g. , SPIGOT ) was efficiency . Since it does not seriously affect computation cost in our approach , we have not experimented with dropping it . > Are sentence embedding and trees generated from two separate LSTM encoders ? Yes.There are no shared parameters in our model : the LSTM of the parser , the LSTM generating the sentence embeddings and the decoder are all separate . Introducing parameter sharing would likely be beneficial . However , our set-up is more controlled , as we can make sure that the improvements are due to modeling latent syntactic structure rather than getting better word representations ( i.e.from using the multi-task learning objective ) . [ 1 ] Andrew Drozdov and Samuel Bowman , The Coadaptation Problem when Learning How and What to Compose ( 2nd Workshop on Representation Learning for NLP , 2017 ) [ 2 ] Hao Peng , Sam Thomson and Noah Smith , Backpropagating through Structured Argmax using a SPIGOT ( ACL 2018 ) [ 3 ] Vlad Niculae , Andr\u00e9 Martins and Claire Cardie , Towards Dynamic Computation Graphs via Sparse Latent Structure ( EMNLP 2018 ) [ 5 ] Yoon Kim , Carl Denton , Luong Hoang and Alexander Rush , Structured Attention Networks ( ICLR 2017 )"}}