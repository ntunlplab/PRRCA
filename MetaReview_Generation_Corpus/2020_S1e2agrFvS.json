{"year": "2020", "forum": "S1e2agrFvS", "title": "Geom-GCN: Geometric Graph Convolutional Networks", "decision": "Accept (Spotlight)", "meta_review": "This paper is consistently supported by all three reviewers and thus an accept is recommended.", "reviews": [{"review_id": "S1e2agrFvS-0", "review_text": " This work proposes geometric aggregation scheme for GCNs, which aims to overcome the limitations in traditional GCNs; those are lacking long distance dependencies and structure information in nodes. In particular, each node is transformed into a latent space. To overcome the first limitation, some nodes that are not directly connected but fall in a near range are also used in aggregation. A relational operator is used to provide position information for each pair of nodes. In this way, the structure information in graph can be used. The method proposed in this work is novel and interesting. However, I am confused how this method can overcome the two limitations faced by previous GCNs. To my understanding, GEOM-GCN maps all node in to a 2D latent space. This can be treated as a lower-dimension representation for each node. Based on this, some similar nodes are clustered together. The relational operator is a kind of ranking operator that can rank two nodes based on latent space representations. If my understanding is wrong, please correct me. Based on this understanding, I didn't find this method can solve the two limitations. 1. To overcome the long-term dependency limitation, GEOM-GCN selects some nodes that are close but not directly connected for aggregation. However, the selected nodes in this way may not connect to the center node. This is a issue that if two nodes that are not connected should be aggregated. The authors should clarify this. 2. The relational operator is used to provide a ranking between two nodes. However, how such kind of operators can be used to aggregate the structure information as described in GIN. For example, how to distinguish those example graphs using this work. I think it would be a plus if authors can make this clear in the paper. 3. The experimental studies are quite weak. Some ablation studies should be done to evaluate the contribution of each proposed methods. For example, how N_{s}(v) contributes to the performance. This is very important for fully evaluating your methods. 4. More tasks and datasets can be added such as graph classification and social networks. 5. Some notations are quite confusing. Like in eq.(1), why m is bold but W is not.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the thoughtful comments . We have revised and updated the paper according to your suggestions and would like to answer questions as follows : Q1 . To overcome the long-term dependency limitation , GEOM-GCN selects some nodes that are close but not directly connected for aggregation . However , the selected nodes in this way may not connect to the center node . This is an issue that if two nodes that are not connected should be aggregated . The authors should clarify this . Response : Long-term dependency is common in real-world graphs . For instance , in transcription networks , the sign-sensitive accelerators that speed up the response time of the target gene expression , are far apart from each other [ Mangan 2003 ] . In the C. elegans frontal neuronal network , three-ring motor neurons that serve as the source of information , are also not directly connected [ Kaiser 2006 ] . However , the existing message passing neural networks ( MPNNs ) , such as GCN , can not handle such graphs because in those models relevant information from distant nodes is mixed with a large number of 1 ) proximal but irrelevant and 2 ) other distant ( and truly irrelevant ) nodes . This limitation has been analyzed deeply in JK-Net [ Xu 2018 ] . The issue of long-term dependency ( from distant but relevant ) can be addressed if the relevant information from distant nodes can be effectively filtered and aggregated in someway . For instance , TO-GCN could automatically connect two disconnected nodes in one class by topology optimization [ Yang 2019 ] . Thus , as the literature above and our own observations indicate , we believe long-term dependency is a true issue -- and to filter/aggregate relevant information from disconnected nodes is a solution we advocate , which this paper attempts to realize by mapping nodes to an embedded space where distant but relevant nodes become close . The key to this solution is how to identify those disconnected nodes with relevant information . In Geom-GCN we expect that such nodes can be mapped into the neighborhood of a center node in an embedded latent space . Then , the relevant information can be extracted via the aggregation in the latent space . We acknowledge that the efficacy of Geom-GCN would depend on the selected embedded space . From experiments , we indeed observe that relevant information is aggregated from disconnected nodes in disassortative graphs ( see Table 4 in the revision ) . As future work , we will explore techniques for automatically choosing a right embedding method\u2013- depending not only on input graphs but also on target applications . [ Mangan 2003 ] Mangan , S. , & Alon , U . Structure and function of the feed-forward loop network motif . Proceedings of the National Academy of Sciences ( PNAS ) , 2003 , 100 ( 21 ) , 11980-11985 . [ Kaiser 2006 ] Kaiser , M. , & Hilgetag , C. C. Nonoptimal component placement , but short processing paths , due to long-distance projections in neural systems . PLoS computational biology , 2006 , 2 ( 7 ) , e95 . [ Xu 2018 ] Xu , K. , Li , C. , Tian , Y. , Sonobe , T. , Kawarabayashi , K. I. , & Jegelka , S. Representation learning on graphs with jumping knowledge networks . International Conference on Machine Learning ( ICML ) . 2018 , 5449\u20135458 . [ Yang 2019 ] Yang , L. , Kang , Z. , Cao , X. , Jin , D. , Yang , B. , & Guo , Y. Topology optimization based graph convolutional network . In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence , IJCAI . 2019 , 4054-4061 . Q2.The relational operator is used to provide a ranking between two nodes . However , how such kind of operators can be used to aggregate the structure information as described in GIN . For example , how to distinguish those example graphs using this work . I think it would be a plus if authors can make this clear in the paper . Response : Indeed , the non-isomorphic example graphs in GIN can be distinguished by applying simple aggregator , mean , or maximum , in a structural neighborhood . We provide and describe a detailed solution in the revision ( see Section 2.1.1 ) ."}, {"review_id": "S1e2agrFvS-1", "review_text": "The work is based on the premise that existing MPNNs have two main weaknesses: (i) the loss function doesn't properly capture the spatial information during graph convolution, and (ii) the difficulty to manage the information encoded in the long range connections. The main contribution of this work is a novel method called geometric aggregation. The proposed method is based on two elements: (i) a latent space mapping to capture spatial information using a new bi-level operator, (ii) an integration of geometric aggregation inside GCN, namely geom-GCN. More in detail the idea reported in this work is to map the input graph into an embedding where the geometric relations between nodes are preserved. The graph is embedded using a usual embedding function that guarantees to preserve some graph property of interest like the hierarchy of nodes. After the node embedding, the authors propose to create a structural neighbourhood both (i) in the latent space, taking the nodes within an arbitrary radius, and (ii) in the original space, by taking the adjacent nodes. The expectation here is that with the proper embedding the latent space can catch connections, which are long range in the original space. The message passing is actuated exploiting first the structural neighbourhood to do low-level aggregation, which aggregates nodes that have the same geometric relationship using permutation invariant operators, and then the result of these aggregations, which are virtual nodes, are aggregated again through high-level aggregation making use of operators like concatenation. The goal of this work is clearly formulated by posing the proper research questions. The topic is relevant and it is part of the research agenda of ICLR. A key point of the proposed method is the ortogonalithy of geometric aggregation with respect to other aggregators like GAT. The design of the structural neighboorhood allows the network to choose which neighbors are the most important for the learning task. Some minor comments. The strong dependency from the embedding fuction does not guarantee the discovery of long range connections. It may happen that the proposed embedding does not catch the relevant information for the task; in these cases the a-priori knowledge on the task becomes crucial. This potential issue is partially supported by the results presented in the mauscript, where there is a gain only when the correct embedding is chosen. A further critical point is the choice of the radius. Such a choice can be operated only with an empirical assessment. It is not clear whether it migth be meaningful to choose a radius thatwould encode the same neighbourhood as in the original sapce of data. The authors claim that even if there are more hops between two nodes the relevant information would arrive from the far node to the target node. Nevertheless we may conceive a situation where the relevant information is washed out during the hops. It may happen when the information of the far node is relevant for the target node, but it is not relevant for the target neighbour nodes. The use of concatenation as high level operator is critically dependent from the radius and from the number of edges in the graph. In cases of large values for radius or very dense graphs, the concatenation may increase the spatial complexity of the networks. Concerning the Section on empirical analysis, it might be of interest to investigate whether with a proper number of layers a GCN would emulate a geom-GCN.", "rating": "8: Accept", "reply_text": "We would like to thank the reviewer for their thoughtful comments and appreciation . We would like to answer the reviewer \u2019 s questions as follows : Q1 . The strong dependency from the embedding function does not guarantee the discovery of long range connections . It may happen that the proposed embedding does not catch the relevant information for the task ; in these cases the a priori knowledge on the task becomes crucial . This potential issue is partially supported by the results presented in the manuscript , where there is a gain only when the correct embedding is chosen . Response : We acknowledge that the efficacy of Geom-GCN would depend on the selected embedded space . To further evaluate the influence on performance from the embedded space choice , we add two new sections in the revision ( Section 4.3.1 and 4.3.2 ) . The two sections analyze this issue from two different perspectives . Both of them indicate that some embedded spaces have a larger contribution/influence than the others . Thus , we believe it 's significant future work to design an end-to-end framework that can automatically determine the right embedded space for Geom-GCN . Q2.A further critical point is the choice of the radius . Such a choice can be operated only with an empirical assessment . It is not clear whether it might be meaningful to choose a radius that would encode the same neighborhood as in the original space of data . Response : Thanks for your suggestion . Radius indeed is a very important hyper-parameters for Geom-GCN . When the radius is too small , relevant information can not be aggregated comprehensively because the neighborhood in latent space becomes too small . And when the radius is too large , relevant information may be \u201c washed out \u201d by too much irrelevant information from the neighborhood in latent space . We will conduct sensibility tests for the radius on each dataset to determine a good radius in the future . Q3.The use of concatenation as high level operator is critically dependent from the radius and from the number of edges in the graph . In cases of large values for radius or very dense graphs , the concatenation may increase the spatial complexity of the networks . Response : Thanks for your suggestion . We agree that both the radius and the number of edges can increase the spatial complexity of Geom-GCN . However , what they affect is the low-level aggregation rather than the high-level aggregation . The reason is that the input of low-level aggregation is the representations of nodes in neighborhoods , and the input of high-level aggregation is the representations of virtual nodes , where the number of virtual nodes is fixed , i.e. , 2|R| , |R| is the number of geometric relationships . Thus , we can employ concatenation for high-level aggregation when |R| is not too large . Q4.The authors claim that even if there are more hops between two nodes the relevant information would arrive from the far node to the target node . Nevertheless , we may conceive a situation where the relevant information is washed out during the hops . It may happen when the information of the far node is relevant for the target node , but it is not relevant for the target neighbor nodes . Response : A fundamental weakness of existing message passing neural networks ( MPNNs ) is lacking the ability to capture long-range dependencies . The reason is exactly what you mentioned , the relevant information is washed out during the many hops . This weakness can be addressed if the relevant information from distant nodes can be effectively filtered and aggregated in someway . For instance , TO-GCN could automatically connect two disconnected nodes in one class by topology optimization [ Yang 2019 ] . In this paper , we attempt to map the distant nodes with relevant information into a small area in an embedded space . Then the relevant information can be aggregated effectively in the neighborhood defined in the embedded space . We acknowledge that the efficacy of such aggregation would depend on the selected embedded space . From experiments , we indeed observe that relevant information is aggregated from disconnected nodes in disassortative graphs ( see Table 4 in the revision ) . As future work , we will explore techniques for automatically choosing a right embedding method\u2013- depending not only on input graphs but also on target applications . [ Yang 2019 ] Yang , L. , Kang , Z. , Cao , X. , Jin , D. , Yang , B. , & Guo , Y. Topology optimization based graph convolutional network . In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence , IJCAI . 2019 , 4054-4061 ."}, {"review_id": "S1e2agrFvS-2", "review_text": "GEOM-GCN: GEOMETRIC GRAPH CONVOLUTIONAL NETWORKS The paper introduces a novel GCN framework, whose purpose is to overcome weaknesses of existing GCN approaches, namely loss of structural neighbor information and failure to capture important dependencies between distant nodes. The paper uses a mapping from nodes to an embedded space and introduces a second type of a neighborhood: a proximity in the embedded space. In the embedded space, a set of relations of nodes is defined. For each node v, the paper uses a 2-stage convolution scheme: 1) for each neighborhood type, the nodes in the same relation with v are combined; 2) the resulting nodes are again combined into a new feature vector. This approach allows one to overcome the issues described above. The experiments show that in most cases the approach outperforms the existing GCN solutions, sometimes with a large gap. I have the following concerns about the paper: -- My main concern is the learning time, which is an issue for a straightforward GCN implementation. There were multiple attempts to decrease it (GraphSAGE, FastGCN, etc.). Therefore, I would like to see running times on the presented graphs as well as on relatively large graphs (see e.g. https://arxiv.org/pdf/1902.07153.pdf for candidates). If some techniques were used to make the implementation faster, I would be good to include them in the paper (or, if they are standard, they should be referenced). At the very least, I believe it should be prioritized as a future direction. -- It\u2019s unclear why we should use the same latent space and the same \u03c4 for both N_g and N_s. I would expect that mapping into different spaces could provide better results: the two neighborhood types seem very different, and I don\u2019t see why the neighbors should be aggregated in the same way. If using different spaces doesn\u2019t provide an improvement, an explanation for this would be very useful. -- \u03b1 and \u03b2 are defined and shown in Table 2, but they are never used (as it stands now, \u03b1 and \u03b2 can simply be removed). If the results in Table 3 correlate with them, then this dependence should be highlighted. In such case, it would also be better to move \u03b1 and \u03b2 to Table 3. -- The paper uses 3 different node embedding strategies. These strategies can be combined in q with different weights (which can be learned as hyperparameters). Will it produce the best of 3 (or better) result? -- \u201cWe use an embedding space of dimension 2 for ease of explanation\u201d But what \u03c4 is used in the real implementation? -- There are various GCN implementations; however, the comparison is performed with only 2 of them. I would like to see either comparison with more implementations, or the explanation why the comparison with the given two suffices. -- Is it possible to make the implementation available? While there are a lot of possible improvements, I believe that some of them can be addressed in a future research, and the paper\u2019s novel approach is noteworthy in itself. My current verdict is 5/10, and I\u2019ll be happy to improve it if the above issues are fixed. Presentation issues: -- The notation used in definition of m_v^l is unclear. -- Why \u03c4 is a part of each node\u2019s structural neighborhood? It\u2019s a global function, isn\u2019t it? -- Introduction: I believe that the exact problems which GCNs solve (e.g. node classification) should be mentioned. -- The flow in Section 2.1 is a bit weird. Namely, it says \u201cTo overcome the first weakness\u201d, but the first wickness wasn\u2019t stated in the previous paragraph (of course, one can deduce it, and it also was defined long ago, but it\u2019s disturbing for a reader). -- Figure 1B is confusing: it looks like the nodes from N_g(v) lie in a small region around v. -- I think that splitting Figure 1C into 2 figures would make it clearer. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the helpful comments . We have revised the paper according to the suggestions and would like to answer the reviewer \u2019 s questions as follows : Q1 . My main concern is the learning time , which is an issue for a straightforward GCN implementation . There were multiple attempts to decrease it ( GraphSAGE , FastGCN , etc . ) . Therefore , I would like to see running times on the presented graphs as well as on relatively large graphs ( see e.g.https : //arxiv.org/pdf/1902.07153.pdf for candidates ) . If some techniques were used to make the implementation faster , I would be good to include them in the paper ( or , if they are standard , they should be referenced ) . At the very least , I believe it should be prioritized as a future direction . Response : It 's a great suggestion . According to it , we add a new section ( Section 4.3.3 ) in the revision to systematically analyze the running time of the proposed Geom-GCN . In this section , we firstly present the theoretical time complexity of Geom-GCN and then compare the real running time of GCN , GAT , and Geom-GCN . To decrease the running time , we think it 's promising future work to apply the accelerating technologies for GCN ( e.g. , FastGCN and SCG ) to Geom-GCN . Q2.It \u2019 s unclear why we should use the same latent space and the same $ \\tau $ for both N_g and N_s . I would expect that mapping into different spaces could provide better results : the two neighborhood types seem very different , and I don \u2019 t see why the neighbors should be aggregated in the same way . If using different spaces doesn \u2019 t provide an improvement , an explanation for this would be very useful . Response : Thanks for the suggestion . We add a new section ( Section 4.3.2 ) in the revision to study how the embedded latent spaces influence the structural neighborhood . To this end , we construct several new Geom-GCN variants , which use a combination of neighborhoods defined by different embedded spaces . From Table 5 , we can observe that several variants achieve better performance than the original Geom-GCN with neighborhoods defined by only one embedded space . On the other hand , there are also many variants that have bad performances . That is , the efficacy of Geom-GCN would depend on the selected embedded spaces . Thus , we think it 's important future work to design an end-to-end framework that is able to automatically determine the right embedded spaces for Geom-GCN . We have finished the analysis on six small datasets in the current revision , and comprehensive results will be added in the future revision . Q3. $ \\alpha $ and $ \\beta $ are defined and shown in Table 2 , but they are never used ( as it stands now , $ \\alpha $ and $ beta $ can simply be removed ) . If the results in Table 3 correlate with them , then this dependence should be highlighted . In such a case , it would also be better to move $ \\alpha $ and $ \\beta $ to Table 3 . Response : Thanks for the suggestion . In the revision , we analysis the correlate between $ \\beta $ and the results in Table 4 . We find that the latent space neighborhoods have larger contributions in disassortative graphs ( with a small $ \\beta $ ) than assortative ones , which implies relevant information from disconnected nodes is aggregated effectively in the neighborhood of the latent space . Please see Section 4.3.1 in revision for details . And we removed $ \\alpha $ in the revision . Q4.The paper uses 3 different node embedding strategies . These strategies can be combined in q with different weights ( which can be learned as hyperparameters ) . Will it produce the best of 3 ( or better ) result ? Response : It 's a very interesting idea . To implement the idea , we construct a new Geom-GCN variant that contains six neighborhoods defined by all the three embedded spaces . Then we evaluate the variant on the graph datasets . However , its performance is not good . We think it is because of two reasons , i ) the variant is very hard to train since it has too many parameters ; ii ) there is too much irrelevant information from the six neighborhoods , which implies that the relevant information may be \u201c washed out \u201d by too much irrelevant information . We also release the code of this experiment in GitHub anonymously , please find the link in the following . Q5. \u201c We use an embedding space of dimension 2 for ease of explanation \u201d But what $ \\tau $ is used in the real implementation ? Response : In the real implementation , we use the $ \\tau $ that we defined in Table 1 ( section 3 ) . In the revision , we exactly mention which $ \\tau $ is used in the experiment section ."}], "0": {"review_id": "S1e2agrFvS-0", "review_text": " This work proposes geometric aggregation scheme for GCNs, which aims to overcome the limitations in traditional GCNs; those are lacking long distance dependencies and structure information in nodes. In particular, each node is transformed into a latent space. To overcome the first limitation, some nodes that are not directly connected but fall in a near range are also used in aggregation. A relational operator is used to provide position information for each pair of nodes. In this way, the structure information in graph can be used. The method proposed in this work is novel and interesting. However, I am confused how this method can overcome the two limitations faced by previous GCNs. To my understanding, GEOM-GCN maps all node in to a 2D latent space. This can be treated as a lower-dimension representation for each node. Based on this, some similar nodes are clustered together. The relational operator is a kind of ranking operator that can rank two nodes based on latent space representations. If my understanding is wrong, please correct me. Based on this understanding, I didn't find this method can solve the two limitations. 1. To overcome the long-term dependency limitation, GEOM-GCN selects some nodes that are close but not directly connected for aggregation. However, the selected nodes in this way may not connect to the center node. This is a issue that if two nodes that are not connected should be aggregated. The authors should clarify this. 2. The relational operator is used to provide a ranking between two nodes. However, how such kind of operators can be used to aggregate the structure information as described in GIN. For example, how to distinguish those example graphs using this work. I think it would be a plus if authors can make this clear in the paper. 3. The experimental studies are quite weak. Some ablation studies should be done to evaluate the contribution of each proposed methods. For example, how N_{s}(v) contributes to the performance. This is very important for fully evaluating your methods. 4. More tasks and datasets can be added such as graph classification and social networks. 5. Some notations are quite confusing. Like in eq.(1), why m is bold but W is not.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the thoughtful comments . We have revised and updated the paper according to your suggestions and would like to answer questions as follows : Q1 . To overcome the long-term dependency limitation , GEOM-GCN selects some nodes that are close but not directly connected for aggregation . However , the selected nodes in this way may not connect to the center node . This is an issue that if two nodes that are not connected should be aggregated . The authors should clarify this . Response : Long-term dependency is common in real-world graphs . For instance , in transcription networks , the sign-sensitive accelerators that speed up the response time of the target gene expression , are far apart from each other [ Mangan 2003 ] . In the C. elegans frontal neuronal network , three-ring motor neurons that serve as the source of information , are also not directly connected [ Kaiser 2006 ] . However , the existing message passing neural networks ( MPNNs ) , such as GCN , can not handle such graphs because in those models relevant information from distant nodes is mixed with a large number of 1 ) proximal but irrelevant and 2 ) other distant ( and truly irrelevant ) nodes . This limitation has been analyzed deeply in JK-Net [ Xu 2018 ] . The issue of long-term dependency ( from distant but relevant ) can be addressed if the relevant information from distant nodes can be effectively filtered and aggregated in someway . For instance , TO-GCN could automatically connect two disconnected nodes in one class by topology optimization [ Yang 2019 ] . Thus , as the literature above and our own observations indicate , we believe long-term dependency is a true issue -- and to filter/aggregate relevant information from disconnected nodes is a solution we advocate , which this paper attempts to realize by mapping nodes to an embedded space where distant but relevant nodes become close . The key to this solution is how to identify those disconnected nodes with relevant information . In Geom-GCN we expect that such nodes can be mapped into the neighborhood of a center node in an embedded latent space . Then , the relevant information can be extracted via the aggregation in the latent space . We acknowledge that the efficacy of Geom-GCN would depend on the selected embedded space . From experiments , we indeed observe that relevant information is aggregated from disconnected nodes in disassortative graphs ( see Table 4 in the revision ) . As future work , we will explore techniques for automatically choosing a right embedding method\u2013- depending not only on input graphs but also on target applications . [ Mangan 2003 ] Mangan , S. , & Alon , U . Structure and function of the feed-forward loop network motif . Proceedings of the National Academy of Sciences ( PNAS ) , 2003 , 100 ( 21 ) , 11980-11985 . [ Kaiser 2006 ] Kaiser , M. , & Hilgetag , C. C. Nonoptimal component placement , but short processing paths , due to long-distance projections in neural systems . PLoS computational biology , 2006 , 2 ( 7 ) , e95 . [ Xu 2018 ] Xu , K. , Li , C. , Tian , Y. , Sonobe , T. , Kawarabayashi , K. I. , & Jegelka , S. Representation learning on graphs with jumping knowledge networks . International Conference on Machine Learning ( ICML ) . 2018 , 5449\u20135458 . [ Yang 2019 ] Yang , L. , Kang , Z. , Cao , X. , Jin , D. , Yang , B. , & Guo , Y. Topology optimization based graph convolutional network . In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence , IJCAI . 2019 , 4054-4061 . Q2.The relational operator is used to provide a ranking between two nodes . However , how such kind of operators can be used to aggregate the structure information as described in GIN . For example , how to distinguish those example graphs using this work . I think it would be a plus if authors can make this clear in the paper . Response : Indeed , the non-isomorphic example graphs in GIN can be distinguished by applying simple aggregator , mean , or maximum , in a structural neighborhood . We provide and describe a detailed solution in the revision ( see Section 2.1.1 ) ."}, "1": {"review_id": "S1e2agrFvS-1", "review_text": "The work is based on the premise that existing MPNNs have two main weaknesses: (i) the loss function doesn't properly capture the spatial information during graph convolution, and (ii) the difficulty to manage the information encoded in the long range connections. The main contribution of this work is a novel method called geometric aggregation. The proposed method is based on two elements: (i) a latent space mapping to capture spatial information using a new bi-level operator, (ii) an integration of geometric aggregation inside GCN, namely geom-GCN. More in detail the idea reported in this work is to map the input graph into an embedding where the geometric relations between nodes are preserved. The graph is embedded using a usual embedding function that guarantees to preserve some graph property of interest like the hierarchy of nodes. After the node embedding, the authors propose to create a structural neighbourhood both (i) in the latent space, taking the nodes within an arbitrary radius, and (ii) in the original space, by taking the adjacent nodes. The expectation here is that with the proper embedding the latent space can catch connections, which are long range in the original space. The message passing is actuated exploiting first the structural neighbourhood to do low-level aggregation, which aggregates nodes that have the same geometric relationship using permutation invariant operators, and then the result of these aggregations, which are virtual nodes, are aggregated again through high-level aggregation making use of operators like concatenation. The goal of this work is clearly formulated by posing the proper research questions. The topic is relevant and it is part of the research agenda of ICLR. A key point of the proposed method is the ortogonalithy of geometric aggregation with respect to other aggregators like GAT. The design of the structural neighboorhood allows the network to choose which neighbors are the most important for the learning task. Some minor comments. The strong dependency from the embedding fuction does not guarantee the discovery of long range connections. It may happen that the proposed embedding does not catch the relevant information for the task; in these cases the a-priori knowledge on the task becomes crucial. This potential issue is partially supported by the results presented in the mauscript, where there is a gain only when the correct embedding is chosen. A further critical point is the choice of the radius. Such a choice can be operated only with an empirical assessment. It is not clear whether it migth be meaningful to choose a radius thatwould encode the same neighbourhood as in the original sapce of data. The authors claim that even if there are more hops between two nodes the relevant information would arrive from the far node to the target node. Nevertheless we may conceive a situation where the relevant information is washed out during the hops. It may happen when the information of the far node is relevant for the target node, but it is not relevant for the target neighbour nodes. The use of concatenation as high level operator is critically dependent from the radius and from the number of edges in the graph. In cases of large values for radius or very dense graphs, the concatenation may increase the spatial complexity of the networks. Concerning the Section on empirical analysis, it might be of interest to investigate whether with a proper number of layers a GCN would emulate a geom-GCN.", "rating": "8: Accept", "reply_text": "We would like to thank the reviewer for their thoughtful comments and appreciation . We would like to answer the reviewer \u2019 s questions as follows : Q1 . The strong dependency from the embedding function does not guarantee the discovery of long range connections . It may happen that the proposed embedding does not catch the relevant information for the task ; in these cases the a priori knowledge on the task becomes crucial . This potential issue is partially supported by the results presented in the manuscript , where there is a gain only when the correct embedding is chosen . Response : We acknowledge that the efficacy of Geom-GCN would depend on the selected embedded space . To further evaluate the influence on performance from the embedded space choice , we add two new sections in the revision ( Section 4.3.1 and 4.3.2 ) . The two sections analyze this issue from two different perspectives . Both of them indicate that some embedded spaces have a larger contribution/influence than the others . Thus , we believe it 's significant future work to design an end-to-end framework that can automatically determine the right embedded space for Geom-GCN . Q2.A further critical point is the choice of the radius . Such a choice can be operated only with an empirical assessment . It is not clear whether it might be meaningful to choose a radius that would encode the same neighborhood as in the original space of data . Response : Thanks for your suggestion . Radius indeed is a very important hyper-parameters for Geom-GCN . When the radius is too small , relevant information can not be aggregated comprehensively because the neighborhood in latent space becomes too small . And when the radius is too large , relevant information may be \u201c washed out \u201d by too much irrelevant information from the neighborhood in latent space . We will conduct sensibility tests for the radius on each dataset to determine a good radius in the future . Q3.The use of concatenation as high level operator is critically dependent from the radius and from the number of edges in the graph . In cases of large values for radius or very dense graphs , the concatenation may increase the spatial complexity of the networks . Response : Thanks for your suggestion . We agree that both the radius and the number of edges can increase the spatial complexity of Geom-GCN . However , what they affect is the low-level aggregation rather than the high-level aggregation . The reason is that the input of low-level aggregation is the representations of nodes in neighborhoods , and the input of high-level aggregation is the representations of virtual nodes , where the number of virtual nodes is fixed , i.e. , 2|R| , |R| is the number of geometric relationships . Thus , we can employ concatenation for high-level aggregation when |R| is not too large . Q4.The authors claim that even if there are more hops between two nodes the relevant information would arrive from the far node to the target node . Nevertheless , we may conceive a situation where the relevant information is washed out during the hops . It may happen when the information of the far node is relevant for the target node , but it is not relevant for the target neighbor nodes . Response : A fundamental weakness of existing message passing neural networks ( MPNNs ) is lacking the ability to capture long-range dependencies . The reason is exactly what you mentioned , the relevant information is washed out during the many hops . This weakness can be addressed if the relevant information from distant nodes can be effectively filtered and aggregated in someway . For instance , TO-GCN could automatically connect two disconnected nodes in one class by topology optimization [ Yang 2019 ] . In this paper , we attempt to map the distant nodes with relevant information into a small area in an embedded space . Then the relevant information can be aggregated effectively in the neighborhood defined in the embedded space . We acknowledge that the efficacy of such aggregation would depend on the selected embedded space . From experiments , we indeed observe that relevant information is aggregated from disconnected nodes in disassortative graphs ( see Table 4 in the revision ) . As future work , we will explore techniques for automatically choosing a right embedding method\u2013- depending not only on input graphs but also on target applications . [ Yang 2019 ] Yang , L. , Kang , Z. , Cao , X. , Jin , D. , Yang , B. , & Guo , Y. Topology optimization based graph convolutional network . In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence , IJCAI . 2019 , 4054-4061 ."}, "2": {"review_id": "S1e2agrFvS-2", "review_text": "GEOM-GCN: GEOMETRIC GRAPH CONVOLUTIONAL NETWORKS The paper introduces a novel GCN framework, whose purpose is to overcome weaknesses of existing GCN approaches, namely loss of structural neighbor information and failure to capture important dependencies between distant nodes. The paper uses a mapping from nodes to an embedded space and introduces a second type of a neighborhood: a proximity in the embedded space. In the embedded space, a set of relations of nodes is defined. For each node v, the paper uses a 2-stage convolution scheme: 1) for each neighborhood type, the nodes in the same relation with v are combined; 2) the resulting nodes are again combined into a new feature vector. This approach allows one to overcome the issues described above. The experiments show that in most cases the approach outperforms the existing GCN solutions, sometimes with a large gap. I have the following concerns about the paper: -- My main concern is the learning time, which is an issue for a straightforward GCN implementation. There were multiple attempts to decrease it (GraphSAGE, FastGCN, etc.). Therefore, I would like to see running times on the presented graphs as well as on relatively large graphs (see e.g. https://arxiv.org/pdf/1902.07153.pdf for candidates). If some techniques were used to make the implementation faster, I would be good to include them in the paper (or, if they are standard, they should be referenced). At the very least, I believe it should be prioritized as a future direction. -- It\u2019s unclear why we should use the same latent space and the same \u03c4 for both N_g and N_s. I would expect that mapping into different spaces could provide better results: the two neighborhood types seem very different, and I don\u2019t see why the neighbors should be aggregated in the same way. If using different spaces doesn\u2019t provide an improvement, an explanation for this would be very useful. -- \u03b1 and \u03b2 are defined and shown in Table 2, but they are never used (as it stands now, \u03b1 and \u03b2 can simply be removed). If the results in Table 3 correlate with them, then this dependence should be highlighted. In such case, it would also be better to move \u03b1 and \u03b2 to Table 3. -- The paper uses 3 different node embedding strategies. These strategies can be combined in q with different weights (which can be learned as hyperparameters). Will it produce the best of 3 (or better) result? -- \u201cWe use an embedding space of dimension 2 for ease of explanation\u201d But what \u03c4 is used in the real implementation? -- There are various GCN implementations; however, the comparison is performed with only 2 of them. I would like to see either comparison with more implementations, or the explanation why the comparison with the given two suffices. -- Is it possible to make the implementation available? While there are a lot of possible improvements, I believe that some of them can be addressed in a future research, and the paper\u2019s novel approach is noteworthy in itself. My current verdict is 5/10, and I\u2019ll be happy to improve it if the above issues are fixed. Presentation issues: -- The notation used in definition of m_v^l is unclear. -- Why \u03c4 is a part of each node\u2019s structural neighborhood? It\u2019s a global function, isn\u2019t it? -- Introduction: I believe that the exact problems which GCNs solve (e.g. node classification) should be mentioned. -- The flow in Section 2.1 is a bit weird. Namely, it says \u201cTo overcome the first weakness\u201d, but the first wickness wasn\u2019t stated in the previous paragraph (of course, one can deduce it, and it also was defined long ago, but it\u2019s disturbing for a reader). -- Figure 1B is confusing: it looks like the nodes from N_g(v) lie in a small region around v. -- I think that splitting Figure 1C into 2 figures would make it clearer. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the helpful comments . We have revised the paper according to the suggestions and would like to answer the reviewer \u2019 s questions as follows : Q1 . My main concern is the learning time , which is an issue for a straightforward GCN implementation . There were multiple attempts to decrease it ( GraphSAGE , FastGCN , etc . ) . Therefore , I would like to see running times on the presented graphs as well as on relatively large graphs ( see e.g.https : //arxiv.org/pdf/1902.07153.pdf for candidates ) . If some techniques were used to make the implementation faster , I would be good to include them in the paper ( or , if they are standard , they should be referenced ) . At the very least , I believe it should be prioritized as a future direction . Response : It 's a great suggestion . According to it , we add a new section ( Section 4.3.3 ) in the revision to systematically analyze the running time of the proposed Geom-GCN . In this section , we firstly present the theoretical time complexity of Geom-GCN and then compare the real running time of GCN , GAT , and Geom-GCN . To decrease the running time , we think it 's promising future work to apply the accelerating technologies for GCN ( e.g. , FastGCN and SCG ) to Geom-GCN . Q2.It \u2019 s unclear why we should use the same latent space and the same $ \\tau $ for both N_g and N_s . I would expect that mapping into different spaces could provide better results : the two neighborhood types seem very different , and I don \u2019 t see why the neighbors should be aggregated in the same way . If using different spaces doesn \u2019 t provide an improvement , an explanation for this would be very useful . Response : Thanks for the suggestion . We add a new section ( Section 4.3.2 ) in the revision to study how the embedded latent spaces influence the structural neighborhood . To this end , we construct several new Geom-GCN variants , which use a combination of neighborhoods defined by different embedded spaces . From Table 5 , we can observe that several variants achieve better performance than the original Geom-GCN with neighborhoods defined by only one embedded space . On the other hand , there are also many variants that have bad performances . That is , the efficacy of Geom-GCN would depend on the selected embedded spaces . Thus , we think it 's important future work to design an end-to-end framework that is able to automatically determine the right embedded spaces for Geom-GCN . We have finished the analysis on six small datasets in the current revision , and comprehensive results will be added in the future revision . Q3. $ \\alpha $ and $ \\beta $ are defined and shown in Table 2 , but they are never used ( as it stands now , $ \\alpha $ and $ beta $ can simply be removed ) . If the results in Table 3 correlate with them , then this dependence should be highlighted . In such a case , it would also be better to move $ \\alpha $ and $ \\beta $ to Table 3 . Response : Thanks for the suggestion . In the revision , we analysis the correlate between $ \\beta $ and the results in Table 4 . We find that the latent space neighborhoods have larger contributions in disassortative graphs ( with a small $ \\beta $ ) than assortative ones , which implies relevant information from disconnected nodes is aggregated effectively in the neighborhood of the latent space . Please see Section 4.3.1 in revision for details . And we removed $ \\alpha $ in the revision . Q4.The paper uses 3 different node embedding strategies . These strategies can be combined in q with different weights ( which can be learned as hyperparameters ) . Will it produce the best of 3 ( or better ) result ? Response : It 's a very interesting idea . To implement the idea , we construct a new Geom-GCN variant that contains six neighborhoods defined by all the three embedded spaces . Then we evaluate the variant on the graph datasets . However , its performance is not good . We think it is because of two reasons , i ) the variant is very hard to train since it has too many parameters ; ii ) there is too much irrelevant information from the six neighborhoods , which implies that the relevant information may be \u201c washed out \u201d by too much irrelevant information . We also release the code of this experiment in GitHub anonymously , please find the link in the following . Q5. \u201c We use an embedding space of dimension 2 for ease of explanation \u201d But what $ \\tau $ is used in the real implementation ? Response : In the real implementation , we use the $ \\tau $ that we defined in Table 1 ( section 3 ) . In the revision , we exactly mention which $ \\tau $ is used in the experiment section ."}}