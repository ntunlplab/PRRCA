{"year": "2017", "forum": "Hku9NK5lx", "title": "Training Compressed Fully-Connected Networks with a Density-Diversity Penalty", "decision": "Accept (Poster)", "meta_review": "The reviewers unanimously recommended accepting the paper.", "reviews": [{"review_id": "Hku9NK5lx-0", "review_text": "The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights. This density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer. Regularly, the most frequent value in the weight matrix is set to zero to encourage sparsity. As weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient. The training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights. The experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance. The paper is presented very clearly, presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data. The result tables are a bit confusing unfortunately. minor issues: p1 english mistake: \u201cwhile networks *that* consist of convolutional layers\u201d. p6-p7 Table 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse: In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better. I assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thanks for pointing out that our paper is strong . Regarding the confusion about the results tables , 1-sparsity ( or density , as we will now call it based on your comments ) is presented instead of sparsity . Sparsity represents the fraction of weights that are zero , while we report percentage of non-zero entries . Thus , our methods achieve very sparse weight matrices . We choose to report density instead of sparsity because density ( sometimes known as the percentage of non-zero entries ) is used in related work ( e.g. , the `` deep compression '' paper , Han et al. , 2015a ) ; also , density is often a small number and , hence , is easier to read in plots ; finally , and perhaps most importantly , for both density and diversity , lower is better meaning they are more mutually consistent ."}, {"review_id": "Hku9NK5lx-1", "review_text": "This work introduces a number of techniques to compress fully-connected neural networks while maintaining similar performance, including a density-diversity penalty and associated training algorithm. The core technique of this paper is to explicitly penalize both the overall magnitude of the weights as well as diversity between weights. This approach results in sparse weight matrices comprised of relatively few unique values. Despite introducing a more efficient means of computing the gradient with respect to the diversity penalty, the authors still find it necessary to apply the penalty with some low probability (1-5%) per mini-batch. The approach achieves impressive compression of fully connected layers with relatively little loss of accuracy. I wonder if the cost of having to sort weights (even for only 1 or 2 out of 100 mini-batches) might make this method intractable for larger networks. Perhaps the sparsity could help remove some of this cost? I think the biggest fault this paper has is the number of different things going on in the approach that are not well explored independently. Sparse initialization, weight tying, probabilistic application of density-diversity penalty and setting the mode to 0, and alternating schedule between weight tied standard training and diversity penalty training. The authors don't provide enough discussion of the relative importance of these parts. Furthermore, the only quantitative metric shown is the compression rate which is a function of both sparsity and diversity such that they cannot be compared on their own. I would really like to see how each component of the algorithm affects diversity, sparsity, and overall compression. A quick verification: Section 3.1 claims the density-diversity penalty is applied with a fixed probability per batch while 3.4 implies structured phases alternating between application of density-diversity and weight tied standard cross entropy. Is this scheme in 3.4 only applying the density-diversity penalty probabilistically when it is in the density-diversity phase? Preliminary rating: I think this is an interesting paper but lacks sufficient empirical evaluation of its many components. As a result, the algorithm has the appearance of a collection of tricks that in the end result in good performance without fully explaining why it is effective. Minor notes: Please resize equation 4 to fit within the margins (\\resizebox{\\columnwidth}{!}{ blah } works well in latex for this)", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your questions about applying the components of the proposed method independently . Applying regularization alone gives diversity similar to what we reported in the paper but at a cost of much higher sparsity . Also , even worse , performance degrades as well in this case . In fact , we began our investigation without using sparse initialization and weight tying , and we found that performance is far worse in this case . Applying sparse initialization or weight tying independently would have no effect on the compression task . Overall , we achieve good compression results only when sparse initialization , density-diversity penalty , and weight tying are all applied simultaneously . A good compression requires low diversity ( which is achieved by applying the density-diversity penalty ) , high sparsity ( which is achieved by applying both the density-diversity penalty and the sparse initialization ) and no loss of performance ( which is achieved by training with weight tying ) . The other components of the algorithm proposed in the paper ( e.g.applying the density-diversity penalty with a certain small probability ) , are included to make the method more computationally efficient . We have added a paragraph to the end of section 3.4 in the latest version of the paper , and we will be happy to add more discussions into the final version of the paper , should we be given the opportunity . We are also optimistic about the applicability of our method to large networks . First , the experiments on TIMIT uses a network with matrices of size 2048 * 2048 , i.e. , reasonably representative of modern big networks . Moreover , sorting can be greatly accelerated by : 1 ) sorting in parallel ; 2 ) using the sparse structure of the weight matrices , as also pointed out by the other reviewers ; and 3 ) using the sorting results of previous batches . Regarding the `` quick verification '' , the penalty is not applied at all during the weight tying phase , and the weight tying phase would not change the diversity or sparsity of the trained weight matrices . The weight tying phase is solely for the purpose of getting the compressed network performance similar to the original , uncompressed performance ."}, {"review_id": "Hku9NK5lx-2", "review_text": "The paper shows promising results but it is difficult to read and follow. It presents different things closely related and it is difficult to asses the performance of each one. Diversity, sparsity, regularization term, tying weights. Anyway results are good.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for pointing out our strong empirical results . We are working on making the paper clearer . The diversity , sparsity , and tying weights are the three key components of the proposed method . Applying any of the component individually does not yield nearly as good results . Please see the discussion with AnonReviewer3 for more details on this point ."}], "0": {"review_id": "Hku9NK5lx-0", "review_text": "The method proposes to compress the weight matrices of deep networks using a new density-diversity penalty together with a computing trick (sorting weights) to make computation affordable and a strategy of tying weights. This density-diversity penalty consists of an added cost corresponding to the l2-norm of the weights (density) and the l1-norm of all the pairwise differences in a layer. Regularly, the most frequent value in the weight matrix is set to zero to encourage sparsity. As weights collapse to the same values with the diversity penalty, they are tied together and then updated using the averaged gradient. The training process then alternates between training with 1. the density-diversity penalty and untied weights, and 2. training without this penalty but with tied weights. The experiments on two datasets (MNIST for vision and TIMIT for speech) shows that the method achieves very good compression rates without loss of performance. The paper is presented very clearly, presents very interesting ideas and seems to be state of the art for compression. The approach opens many new avenues of research and the strategy of weight-tying may be of great interest outside of the compression domain to learn regularities in data. The result tables are a bit confusing unfortunately. minor issues: p1 english mistake: \u201cwhile networks *that* consist of convolutional layers\u201d. p6-p7 Table 1,2,3 are confusing. Compared to the baseline (DC), your method (DP) seems to perform worse: In Table 1 overall, Table 2 overall FC, Table 3 overall, DP is less sparse and more diverse than the DC baseline. This would suggest a worse compression rate for DP and is inconsistent with the text which says they should be similar or better. I assume the sparsity value is inverted and that you in fact report the number of non-modal values as a fraction of the total.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thanks for pointing out that our paper is strong . Regarding the confusion about the results tables , 1-sparsity ( or density , as we will now call it based on your comments ) is presented instead of sparsity . Sparsity represents the fraction of weights that are zero , while we report percentage of non-zero entries . Thus , our methods achieve very sparse weight matrices . We choose to report density instead of sparsity because density ( sometimes known as the percentage of non-zero entries ) is used in related work ( e.g. , the `` deep compression '' paper , Han et al. , 2015a ) ; also , density is often a small number and , hence , is easier to read in plots ; finally , and perhaps most importantly , for both density and diversity , lower is better meaning they are more mutually consistent ."}, "1": {"review_id": "Hku9NK5lx-1", "review_text": "This work introduces a number of techniques to compress fully-connected neural networks while maintaining similar performance, including a density-diversity penalty and associated training algorithm. The core technique of this paper is to explicitly penalize both the overall magnitude of the weights as well as diversity between weights. This approach results in sparse weight matrices comprised of relatively few unique values. Despite introducing a more efficient means of computing the gradient with respect to the diversity penalty, the authors still find it necessary to apply the penalty with some low probability (1-5%) per mini-batch. The approach achieves impressive compression of fully connected layers with relatively little loss of accuracy. I wonder if the cost of having to sort weights (even for only 1 or 2 out of 100 mini-batches) might make this method intractable for larger networks. Perhaps the sparsity could help remove some of this cost? I think the biggest fault this paper has is the number of different things going on in the approach that are not well explored independently. Sparse initialization, weight tying, probabilistic application of density-diversity penalty and setting the mode to 0, and alternating schedule between weight tied standard training and diversity penalty training. The authors don't provide enough discussion of the relative importance of these parts. Furthermore, the only quantitative metric shown is the compression rate which is a function of both sparsity and diversity such that they cannot be compared on their own. I would really like to see how each component of the algorithm affects diversity, sparsity, and overall compression. A quick verification: Section 3.1 claims the density-diversity penalty is applied with a fixed probability per batch while 3.4 implies structured phases alternating between application of density-diversity and weight tied standard cross entropy. Is this scheme in 3.4 only applying the density-diversity penalty probabilistically when it is in the density-diversity phase? Preliminary rating: I think this is an interesting paper but lacks sufficient empirical evaluation of its many components. As a result, the algorithm has the appearance of a collection of tricks that in the end result in good performance without fully explaining why it is effective. Minor notes: Please resize equation 4 to fit within the margins (\\resizebox{\\columnwidth}{!}{ blah } works well in latex for this)", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your questions about applying the components of the proposed method independently . Applying regularization alone gives diversity similar to what we reported in the paper but at a cost of much higher sparsity . Also , even worse , performance degrades as well in this case . In fact , we began our investigation without using sparse initialization and weight tying , and we found that performance is far worse in this case . Applying sparse initialization or weight tying independently would have no effect on the compression task . Overall , we achieve good compression results only when sparse initialization , density-diversity penalty , and weight tying are all applied simultaneously . A good compression requires low diversity ( which is achieved by applying the density-diversity penalty ) , high sparsity ( which is achieved by applying both the density-diversity penalty and the sparse initialization ) and no loss of performance ( which is achieved by training with weight tying ) . The other components of the algorithm proposed in the paper ( e.g.applying the density-diversity penalty with a certain small probability ) , are included to make the method more computationally efficient . We have added a paragraph to the end of section 3.4 in the latest version of the paper , and we will be happy to add more discussions into the final version of the paper , should we be given the opportunity . We are also optimistic about the applicability of our method to large networks . First , the experiments on TIMIT uses a network with matrices of size 2048 * 2048 , i.e. , reasonably representative of modern big networks . Moreover , sorting can be greatly accelerated by : 1 ) sorting in parallel ; 2 ) using the sparse structure of the weight matrices , as also pointed out by the other reviewers ; and 3 ) using the sorting results of previous batches . Regarding the `` quick verification '' , the penalty is not applied at all during the weight tying phase , and the weight tying phase would not change the diversity or sparsity of the trained weight matrices . The weight tying phase is solely for the purpose of getting the compressed network performance similar to the original , uncompressed performance ."}, "2": {"review_id": "Hku9NK5lx-2", "review_text": "The paper shows promising results but it is difficult to read and follow. It presents different things closely related and it is difficult to asses the performance of each one. Diversity, sparsity, regularization term, tying weights. Anyway results are good.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for pointing out our strong empirical results . We are working on making the paper clearer . The diversity , sparsity , and tying weights are the three key components of the proposed method . Applying any of the component individually does not yield nearly as good results . Please see the discussion with AnonReviewer3 for more details on this point ."}}