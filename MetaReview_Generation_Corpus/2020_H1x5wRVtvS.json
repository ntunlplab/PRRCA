{"year": "2020", "forum": "H1x5wRVtvS", "title": "Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling", "decision": "Accept (Poster)", "meta_review": "This paper proposes a bidirectional joint image-text model using a variational hetero-encoder (VHE) randomized generative adversarial network (GAN). The proposed VHE-GAN model encodes an image to decode its associated text. Three reviewers have split reviews. Reviewer #3 is overall positive about this work. Reviewer #1 rated weak acceptance, while request more comparison with latest works. Reviewer  #2 rated weak reject raised concerns on the motivation of the approach, the lack of ablation and lack of comparison with the latest work. During the rebuttal, the authors provide additional comparison and ablation, which seem to address the major concerns. Given the overall positive feedback and the quality of rebuttal, the AC recommends acceptance.", "reviews": [{"review_id": "H1x5wRVtvS-0", "review_text": "Summary: The authors design a new model for bidirectional joint image-text modeling using a variational hetero-encoder (VHE) randomized generative adversarial network (GAN) that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into an end-to-end multimodal model. Their proposed VHE-GAN model encodes an image to decode its associated text and feeds the variational posterior as the source of randomness into the GAN image generator. The authors also incorporate a deep topic model, a ladder-structured image encoder, and StackGAN++ into their framework for improved photo-realistic images. Strengths: - The authors have proposed a nice multimodal model that allows inference of latent variables given only text or image, and also allows realistic synthesis of images from images, text, or noise. - The paper is quite dense but generally well written. Weaknesses: - The experimental comparison only included old baselines and the authors should compare to some more recent work such as TA-GAN (NIPS18), and Object-GAN (CVPR19). - It would help if the paper contained more ablation studies across different modules that the framework uses. ### Post rebuttal ### Thank you for your detailed answers to my questions. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments and suggestions . Following your suggestions , we have performed comparisons against both TA-GAN and Obj-GAN , and included two additional variations of VHE-raster-scan-GAN for ablation studies . The newly added discussions and results have been highlighted in blue in the revised paper . Please see our `` Response to All '' for more details ."}, {"review_id": "H1x5wRVtvS-1", "review_text": "This paper proposed VHE-GAN for the text-to-image generation task. The proposed method utilizes the off-the-shell modules and feeds the VHE variational posterior into the generator. The experiments are conducted on three datasets. The motivation for the paper is not clear. Most of the components used, such as text-encoder, image-encoder, generator-discriminator follow previous works. Therefore, the authors should claim how the proposed VHE variational posterior can help the task. However, I did not see the clear motivation for this part. Besides the basic version VHE-StackGAN++, it proposed another version VHE-raster-scan-GAN. However, the paper also fails to tell the intuition of the deep topic model and PGBN text decoder. The experimental results are not solid. The comparison only included old baselines. However, several recent state-of-the-art approaches are missing: a. attnGAN (CVPR18), b. TA-GAN (NIPS18), c. Object-GAN (CVPR19). Without these comparisons, it is difficult to evaluate how the method works. In addition, the paper does not provide an ablation study to analyze the effect of each component proposed (e.g., Poisson gamma belief network, a deep topic mode). ", "rating": "3: Weak Reject", "reply_text": "We thank AnonReviewer4 for the comments . We 'd like to strongly argue against these comments that were used to support the low rating . Please see our point-by-point response below . First , we emphasize that the proposed VHE-GANs are deep generative models that can not only perform text-to-image generation tasks , but also many other tasks , such as text based zero shot learning ( image-to-text ) and noise to image-text pairs generation . Please see our `` Response to All '' on the versatility of the proposed models . Second , a key motivation of the paper is to build VHE-raster-scan-GAN . While it does include a variety existing modeling components , both the VHE-GAN framework , which integrates various components for bidirectional image-text modeling , and the raster-scan structure , which leads to state-of-the-art results in a variety of tasks , are first proposed in this paper . Please see our `` Response to All '' on additional ablation studies that have been added to better demonstrate the importance of having both the stacking structure and raster-scan structure . Third , we did explain the intuition of why using the the PGBN deep topic model ( together with the raster-scan structure ) in multiple places , such as in the last paragraph of Section 2.2 , first paragraph of Section 2.3 , and last paragraph of Section 3.1 . More specifically , a key intuition behind VHE-raster-scan-GAN is to use the PGBN deep topic model to help capture hierarchical semantic structures , which are related to coarse-to-fine visual concepts with the help of the raster-scan structure , as visually demonstrated in Figs . 4 , 5 , 21-23 and a variety of comparisons between VHE-StackGAN++ and VHE-raster-scan-GAN . Higher layer topics are mainly related to the general shapes and colors of objects , or backgrounds , while the lower layer ones are focused on finer details . VHE-raster-scan-GAN exploits the hierarchical semantic structure , which matches coarse-to-fine visual concepts , to gradually refine its generation under the proposed VHE-GAN framework . Fourth , we note in the original submission , we did include AttnGAN for comparison in Table 1 , and an ablation study in Table 4 to examine the effect of varying the depth of PGBN used in VHE-raster-scan-GAN , as discussed in the last paragraph of Section 3.2 . In Fig.2 of our original manuscript , we did not show the results of AttnGAN as it provided no results on Flower . To address your concerns , we have now added AttnGAN into Fig.2 and more results of AttnGAN in Appendices C.2 and C.3 in our revised paper , included two additional variations of VHE-raster-scan-GAN for ablation studies , and added both TA-GAN and Obj-GAN into comparisons . The newly added discussions and results have been highlighted in blue in the revised paper . Please see our `` Response to All '' for more details ."}, {"review_id": "H1x5wRVtvS-2", "review_text": "This paper proposes a combined architecture for image-text modeling. Though the proposed architecture is extremely detailed, the authors explain clearly the overarching concepts and methods used, within limited space. The experimental results are extremely strong, especially on sub-domains where conditional generative models have historically struggled such as images with angular, global features - often mechanical or human constructed objects. \"Computers\" and \"cars\" images in Figure 2 show this quite clearly. The model also functions for tagging and annotating images - performing well compared to models designed *only* for this task. The authors have done a commendable job adding detail, further analysis, and experiments in the appendix of the paper. Combined with the included code release, this paper should be of interest to many. My chief criticisms come for the density of the paper - while it is difficult to dilute such a complex model to 8 pages, and the included appendix clarifies many questions in the text body, it would be worth further passes through the main paper with a specific focus on clarity and brevity, to aid in the accessibility of this work. As usual, more experiments are always welcome, and given the strengths of GAN based generators for faces a text based facial image generator could have been a great addition. The existing experiments are more than sufficient for proof-of-concept though. Finally, though this version of the paper includes code directly in a google drive link it would be ideal for the final version to reference a github code link - again to aid access to interested individuals. Being able to read the code online, without downloading and opening locally can be nice, along with other benefits from open source release. However the authors should release the code however they see fit, this is more of a personal preference on the part of this reviewer. To improve my score, the primary changes would be more editing and re-writing, focused on clarity and brevity of the text in the core paper.", "rating": "8: Accept", "reply_text": "Thank you for your comments and suggestions . We have revised our paper to add additional comparisons to recent work , more ablation studies , and more experimental results , with the major additions highlighted in blue . Please see our `` Response to All '' for details . Below please find our additional response . To enhance clarity and brevity , we have modified our ablation studies to better focus on presenting the proposed VHE-raster-scan-GAN . We have added the task of text-to-face-image generation . More specifically , we have trained our models on the CelebA dataset , where each facial image is described by 40 textual attributes . We have added preliminary results of text-attributes-to-face-image generation to Appendix B of the revised paper . Note limited by the rebuttal time and our computational resource , these examples facial images at 128 * 128 resolution were generated from a relatively small network trained with only 20 epochs . We are working on adding more training epochs and increasing the network size and image resolution to further improve these preliminary results , which will be included in our next revision . We are also seeking facial image datasets with textual descriptions beyond only attributes ( it seems that we might need to build them by our own ) , which will be our future work . We released the code on Google drive to help better preserve anonymity . After the acceptance of this paper , we will release it in GitHub for better access ."}], "0": {"review_id": "H1x5wRVtvS-0", "review_text": "Summary: The authors design a new model for bidirectional joint image-text modeling using a variational hetero-encoder (VHE) randomized generative adversarial network (GAN) that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into an end-to-end multimodal model. Their proposed VHE-GAN model encodes an image to decode its associated text and feeds the variational posterior as the source of randomness into the GAN image generator. The authors also incorporate a deep topic model, a ladder-structured image encoder, and StackGAN++ into their framework for improved photo-realistic images. Strengths: - The authors have proposed a nice multimodal model that allows inference of latent variables given only text or image, and also allows realistic synthesis of images from images, text, or noise. - The paper is quite dense but generally well written. Weaknesses: - The experimental comparison only included old baselines and the authors should compare to some more recent work such as TA-GAN (NIPS18), and Object-GAN (CVPR19). - It would help if the paper contained more ablation studies across different modules that the framework uses. ### Post rebuttal ### Thank you for your detailed answers to my questions. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments and suggestions . Following your suggestions , we have performed comparisons against both TA-GAN and Obj-GAN , and included two additional variations of VHE-raster-scan-GAN for ablation studies . The newly added discussions and results have been highlighted in blue in the revised paper . Please see our `` Response to All '' for more details ."}, "1": {"review_id": "H1x5wRVtvS-1", "review_text": "This paper proposed VHE-GAN for the text-to-image generation task. The proposed method utilizes the off-the-shell modules and feeds the VHE variational posterior into the generator. The experiments are conducted on three datasets. The motivation for the paper is not clear. Most of the components used, such as text-encoder, image-encoder, generator-discriminator follow previous works. Therefore, the authors should claim how the proposed VHE variational posterior can help the task. However, I did not see the clear motivation for this part. Besides the basic version VHE-StackGAN++, it proposed another version VHE-raster-scan-GAN. However, the paper also fails to tell the intuition of the deep topic model and PGBN text decoder. The experimental results are not solid. The comparison only included old baselines. However, several recent state-of-the-art approaches are missing: a. attnGAN (CVPR18), b. TA-GAN (NIPS18), c. Object-GAN (CVPR19). Without these comparisons, it is difficult to evaluate how the method works. In addition, the paper does not provide an ablation study to analyze the effect of each component proposed (e.g., Poisson gamma belief network, a deep topic mode). ", "rating": "3: Weak Reject", "reply_text": "We thank AnonReviewer4 for the comments . We 'd like to strongly argue against these comments that were used to support the low rating . Please see our point-by-point response below . First , we emphasize that the proposed VHE-GANs are deep generative models that can not only perform text-to-image generation tasks , but also many other tasks , such as text based zero shot learning ( image-to-text ) and noise to image-text pairs generation . Please see our `` Response to All '' on the versatility of the proposed models . Second , a key motivation of the paper is to build VHE-raster-scan-GAN . While it does include a variety existing modeling components , both the VHE-GAN framework , which integrates various components for bidirectional image-text modeling , and the raster-scan structure , which leads to state-of-the-art results in a variety of tasks , are first proposed in this paper . Please see our `` Response to All '' on additional ablation studies that have been added to better demonstrate the importance of having both the stacking structure and raster-scan structure . Third , we did explain the intuition of why using the the PGBN deep topic model ( together with the raster-scan structure ) in multiple places , such as in the last paragraph of Section 2.2 , first paragraph of Section 2.3 , and last paragraph of Section 3.1 . More specifically , a key intuition behind VHE-raster-scan-GAN is to use the PGBN deep topic model to help capture hierarchical semantic structures , which are related to coarse-to-fine visual concepts with the help of the raster-scan structure , as visually demonstrated in Figs . 4 , 5 , 21-23 and a variety of comparisons between VHE-StackGAN++ and VHE-raster-scan-GAN . Higher layer topics are mainly related to the general shapes and colors of objects , or backgrounds , while the lower layer ones are focused on finer details . VHE-raster-scan-GAN exploits the hierarchical semantic structure , which matches coarse-to-fine visual concepts , to gradually refine its generation under the proposed VHE-GAN framework . Fourth , we note in the original submission , we did include AttnGAN for comparison in Table 1 , and an ablation study in Table 4 to examine the effect of varying the depth of PGBN used in VHE-raster-scan-GAN , as discussed in the last paragraph of Section 3.2 . In Fig.2 of our original manuscript , we did not show the results of AttnGAN as it provided no results on Flower . To address your concerns , we have now added AttnGAN into Fig.2 and more results of AttnGAN in Appendices C.2 and C.3 in our revised paper , included two additional variations of VHE-raster-scan-GAN for ablation studies , and added both TA-GAN and Obj-GAN into comparisons . The newly added discussions and results have been highlighted in blue in the revised paper . Please see our `` Response to All '' for more details ."}, "2": {"review_id": "H1x5wRVtvS-2", "review_text": "This paper proposes a combined architecture for image-text modeling. Though the proposed architecture is extremely detailed, the authors explain clearly the overarching concepts and methods used, within limited space. The experimental results are extremely strong, especially on sub-domains where conditional generative models have historically struggled such as images with angular, global features - often mechanical or human constructed objects. \"Computers\" and \"cars\" images in Figure 2 show this quite clearly. The model also functions for tagging and annotating images - performing well compared to models designed *only* for this task. The authors have done a commendable job adding detail, further analysis, and experiments in the appendix of the paper. Combined with the included code release, this paper should be of interest to many. My chief criticisms come for the density of the paper - while it is difficult to dilute such a complex model to 8 pages, and the included appendix clarifies many questions in the text body, it would be worth further passes through the main paper with a specific focus on clarity and brevity, to aid in the accessibility of this work. As usual, more experiments are always welcome, and given the strengths of GAN based generators for faces a text based facial image generator could have been a great addition. The existing experiments are more than sufficient for proof-of-concept though. Finally, though this version of the paper includes code directly in a google drive link it would be ideal for the final version to reference a github code link - again to aid access to interested individuals. Being able to read the code online, without downloading and opening locally can be nice, along with other benefits from open source release. However the authors should release the code however they see fit, this is more of a personal preference on the part of this reviewer. To improve my score, the primary changes would be more editing and re-writing, focused on clarity and brevity of the text in the core paper.", "rating": "8: Accept", "reply_text": "Thank you for your comments and suggestions . We have revised our paper to add additional comparisons to recent work , more ablation studies , and more experimental results , with the major additions highlighted in blue . Please see our `` Response to All '' for details . Below please find our additional response . To enhance clarity and brevity , we have modified our ablation studies to better focus on presenting the proposed VHE-raster-scan-GAN . We have added the task of text-to-face-image generation . More specifically , we have trained our models on the CelebA dataset , where each facial image is described by 40 textual attributes . We have added preliminary results of text-attributes-to-face-image generation to Appendix B of the revised paper . Note limited by the rebuttal time and our computational resource , these examples facial images at 128 * 128 resolution were generated from a relatively small network trained with only 20 epochs . We are working on adding more training epochs and increasing the network size and image resolution to further improve these preliminary results , which will be included in our next revision . We are also seeking facial image datasets with textual descriptions beyond only attributes ( it seems that we might need to build them by our own ) , which will be our future work . We released the code on Google drive to help better preserve anonymity . After the acceptance of this paper , we will release it in GitHub for better access ."}}