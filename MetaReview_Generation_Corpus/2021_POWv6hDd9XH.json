{"year": "2021", "forum": "POWv6hDd9XH", "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction", "decision": "Accept (Poster)", "meta_review": "This paper proposes a new method for post-training quantization, achieving very good results. After the author's response, all the reviewers were positive. There were some issues regarding clarity, and about explaining why the methods work better than just optimizing the loss, but I think the reviewers were eventually satisfied.  Following some info after the author's response phase, I'll just ask the authors to verify their published code works with publicly available PyTorch packages, so their method could be easily used.", "reviews": [{"review_id": "POWv6hDd9XH-0", "review_text": "This paper proposes BRECQ which is a new Post Training Quantization ( PTQ ) method . The goal of the paper is to push the limit of PTQ to low bit precision ( INT2 ) . They try to address this by considering both inter and intra-layer sensitivity to find the best update to the model parameters so that the output from a block is minimally changed/perturbed . Furthermore , the authors also consider mixed precision quantization setting . Empirical results are shown for multiple NNs for image classification and object detection . While the paper is trying to address an interesting problem and there are a lot of empirical results , but I had a very hard time to follow the paper 's main idea and the `` final '' proposed algorithm . It would be great if the authors could answer the questions below and I will reconsider my score accordingly . - What is the final algorithm ? The data provided in Algorithm 1 is very vague and there are no equations . It is not clear if you are actually using any second-order information since based on Eq 11 it seems that the proposed method is only using the gradient , and not the second order information . If so are the discussion from page 2-4 necessary ? - It is not clear if the equality given in Eq 11 is correct . With the softmax layer the Hessian diagonal would not be the same as gradient^2 . - Why not use the change in loss in Eq 4 ( left ) instead of using a second-order Taylor series ? Second-order Taylor series is an approximation and does not hold for large perturbations . Also when we have access to evaluating the change in loss , why do we even need to focus on measuring the second order term ? This does not seem to be correct/necessary at least for uniform quantization . - An ablation study is actually needed for the above . - What is the experimental setting for the ablation study in Table 1 ? while not mentioned , from the accuracy it seems the results are on ImageNet but this is not mentioned . Did the authors consider other bit precision settings ? - In page 3 , it is stated that using the Hessian directly is not possible due to memory and computational overhead . However , you can use matrix free methods which do not require forming the Hessian explicitly .", "rating": "6: Marginally above acceptance threshold", "reply_text": "+ Q : `` Why not use the change in loss in Eq 4 ( left ) instead of using a second-order Taylor series ? Second-order Taylor series is an approximation and does not hold for large perturbations . Also when we have access to evaluating the change in loss , why do we even need to focus on measuring the second order term ? This does not seem to be correct/necessary at least for uniform quantization . An ablation study is actually needed for the above . '' A : This is an interesting point in PTQ . The reviewer notes it correctly that there exists an alternative training method : QAT with limited images ( 1024 ) . In fact , we verified this method in our early exploration . The network loss optimization does not perform well , we think the reasons are twofold : ( 1 ) There are only limited images in the calibration set , ( 2 ) we fold the BN layers before reconstruction to provide higher speed-up in deployment . Without BN to update the activation statistics , the whole network optimization is hard to converge to global minimum . In fact , the network reconstruction defined in our paper is very similar to this method . Both of these two methods optimize all network parameters according to the loss computed by the network output . Likewise , it performs worse than block-wise reconstruction . The reason why block reconstruction achieves good results might be : ( 1 ) most cross-layer dependencies are centered inside a block and thus we can divide the whole network optimization into the block-level , ( 2 ) A block with several layers is easy to learn on such a small calibration dataset . Here we show the results of the ablation study . ( all hyper-parameters are aligned and we update this code in the Google Drive link ) . * * Model | Precision | Block Reconstruction | Net Reconstruction | Net Loss Optimization * * Res18 | W4A8 | 70.70 | 68.73 | 56.20 Res18 | W2A8 | 66.30 | 54.15 | 4.33 MobV2 | W4A8 | 71.66 | 66.68 | 51.27 MobV2 | W2A8 | 59.67 | 40.76 | 2.57 + Q : `` What is the experimental setting for the ablation study in Table 1 ? while not mentioned , from the accuracy it seems the results are on ImageNet but this is not mentioned . Did the authors consider other bit precision settings ? '' A : All classification experiments are conducted on ImageNet , sorry that we did not make this clear before . In other bit precision settings , we also observe the same trend in these four kinds of reconstruction granularity . But in general , higher bit-width produces less distinct results because higher bit precision incurs small perturbation and is easy to optimize . Besides , we also provided the implementation for the reviewers to verify the settings that they are interested in . + Q : `` In page 3 , it is stated that using the Hessian directly is not possible due to memory and computational overhead . However , you can use matrix free methods which do not require forming the Hessian explicitly '' A : By mentioning matrix-free optimization , we assume the reviewer is talking about the conjugate gradients method , which indeed avoids the explicit computation of the Hessian . However , the conjugate method only applies to quadratic optimization when weights are not constrained . In this work , weights are quantized and restricted to integers and therefore the conjugate gradient method is not directly applicable . But it would be interesting to see further works about conjugate gradient in quantized neural network optimization . If you have suggestions on other matrix-free methods , we are happy to discuss them . [ 1 ] Markus Nagel , Rana Ali Amjad , Mart van Baalen , Christos Louizos , and Tijmen Blankevoort . Up or down ? adaptive rounding for post-training quantization . arXiv preprint arXiv:2004.10568 , 2020 . [ 2 ] Diederik P Kingma and Jimmy Ba . Adam : A method for stochastic optimization . arXiv preprint arXiv:1412.6980 , 2014 [ 3 ] Hyeyoung Park , Shun-ichi Amari , and Kenji Fukumizu . Adaptive natural gradient learning algorithms for various stochastic models . Neural Networks , 13 ( 7 ) :755\u2013764 , 2000 ."}, {"review_id": "POWv6hDd9XH-1", "review_text": "This paper explores the post-training inference quantization . Based on second-order quantization error analysis , it proposes to reconstruct quantized model in a block level to achieve SOTA accuracy for INT2 weight quantization , which distinguish this paper from previous reported layer-wise reconstruction approach . The proposed approach is intuitive and supported by extensive experiments across a wide range of image classification and object detection tasks . Overall , this paper is well written . The idea is straightforward but comes from a detailed theoretical analysis and supported by strong experimental results . The authors also proposed solutions to approximate pre-activation Hessian by using Fisher Information Matrix which sounds reasonable and is easy to implement . The authors did comprehensive comparison with SOTA approaches , not just PTQ , but also quantization aware training and mix-precision frameworks . I have some questions wish to be clarified . 1 ) .It is not clear to me , in the reconstruction process , how does the short-cut ( e.g.in ResNet ) are handled . 2 ) .In the error analysis , batch normalization layers are not considered . Batch norm will have direct impact on the output activation , hence the quantization bias term . Plus , batch norm statistics will be changed during the reconstruction phase . Could the authors comment on the impact of batch norm layers ? 3 ) .The authors use 1024 training samples to do the reconstruction . What happens when more samples are used ? What determines the number of samples needed ? 4 ) .The data presented seems to focus mainly on weight quantization , while leaving activation in relative higher precision . What happens when quantize activations in 2 bits together with the weights in this approach ? What are the challenges ? 5 ) .It would have been nice to verify this approach on tasks other than vision , such as speech and NLP tasks . 6 ) .Seems a format error on one of the reference : Zichao Guo , Xiangyu Zhang , Haoyuan Mu , Wen Heng , Zechun Liu , Yichen Wei , and Jian Sun . { SINGLE } { path } { one } - { shot } { neural } { architecture } { search } { with } { uniform } { sampling } , 2020 . URL https : //openreview.net/forum ? id=r1gPoCEKvH .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your positive feedback on this work , we wish we can address your concerns in the response below . - Q : `` It is not clear to me , in the reconstruction process , how does the short-cut ( e.g.in ResNet ) are handled . '' A : We are sorry we did not make this point clear . During block reconstruction , the block output is the output after the elemental-wise addition of two paths . Therefore , we consider the effect of shortcuts in block reconstruction . In fact , we think the shortcut component in the modern CNN block increases the intra-block dependency , therefore it is more demanding to conduct block reconstruction . - Q : `` In the error analysis , batch normalization layers are not considered . Batch norm will have a direct impact on the output activation , hence the quantization bias term . Plus , batch norm statistics will be changed during the reconstruction phase . Could the authors comment on the impact of batch norm layers ? '' A : For all of our experiments , we fold BN layers into convolutional layers as did in [ 1 ] , since BN layers require floating-point operations and will slow down the inference . As a result , there is no batch norm layer in the network and no statistics update in the reconstruction phase . - Q : `` The authors use 1024 training samples to do the reconstruction . What happens when more samples are used ? What determines the number of samples needed ? '' A : In general , more samples bring higher final performance . Please see our experimental results in Appendix.B.2 . In 4-bit quantization , the effect of numbers of samples is trivial . But in 2-bit quantization , more samples can increase 5 % accuracy . As for * What determines the number of samples needed ? * We think that it depends on the practical situation , that how much data we can get to do PTQ . In this work , we choose 1024 and align experiments setting for all baselines . - Q : '' The data presented seems to focus mainly on weight quantization , while leaving activation in relative higher precision . What happens when quantize activations in 2 bits together with the weights in this approach ? What are the challenges ? '' A : In PTQ , 2-bit activation quantization will lead to catastrophic results because activations are quantized at run-time and vary greatly . So we can not learn a per-element quantization mechanism for activations since they vary with different input images . All we can do in post-training activation quantization is to find a good clipping range . The reason why activation quantization can be okay in QAT is that the BN statistics can be updated to remember the activation distribution and adapt to a better mean/variance . In PTQ , it is almost impossible to calibrate accurate BN statistics with limited data , so low-bit activation quantization is harder . - Q : `` It would have been nice to verify this approach on tasks other than vision , such as speech and NLP tasks . '' A : This is a good suggestion for this paper and we would be glad to test BRECQ on other tasks such as NLP . But we did not conduct much research in this area and it takes some time for us to prepare the code and dataset . Considering the limited time for rebuttal , we will try to implement BRECQ on these tasks but we could not promise to complete the experiments . - Q : `` Seems a format error on one of the reference '' A : Thanks for the advice , we have revised it . [ 1 ] Raghuraman Krishnamoorthi . Quantizing deep convolutional networks for ef\ufb01cient inference : A whitepaper . arXiv preprint arXiv:1806.08342 , 2018 ."}, {"review_id": "POWv6hDd9XH-2", "review_text": "Post-training quantization is an important problem , especially for industry . This paper leverages the basic building blocks and conducts a block-wise quantization . Nice results are obtained with the proposed method . The proposed method is cheap to implement and pushes the post-training quantization to 2-bit . The measurement problem of mixed precision literature raised in this paper is of insights . This problem may inspire the community to find a better measurement in future work . Extensive experiments on various methods ( handcrafted and designed by NAS ) , various tasks ( classification , detection ) , various configurations ( different bits , latency , model size ) are impressive . Various baselines are also included to make the results stronger . Questions : The block-diagonal scheme is selected according to experimental results . Is it possible to visualize the real Hessian of stage-wise settings ? ( it may be impossible to the full Hessian matrix for the whole network ) . If we see a few non-zero elements at the off-diagonal for the block-wise setting , this choice can be better motivated .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your insights and feedback on this paper . Your suggestion about `` Is it possible to visualize the real Hessian of stage-wise settings ? ( it may be impossible to the full Hessian matrix for the whole network ) '' is of great value . We visualize a Hessian matrix on a tiny ResNet . The code can be found in this Google Colab Link : https : //colab.research.google.com/drive/1hLe-Avgtdy0l5gKibR_zr2SjqApang9D ? usp=sharing . The tiny ResNet contains 3 stages with channels of { 4 , 8 , 16 } ( so that we can calculate the Hessian easily . ) The ResNet has 99.37\\ % accuracy on the MNIST dataset . We compute the Hessian of the first stage . The first stage has 2 blocks , where each block has 2 layers . Each layer in the stage has 144 number of elements . We can find that while most of the values in the Hessian matrix are close to 0 , the upper left Hessian ( which corresponds to the first block ) indeed has slightly higher absolute values , which confirms the observation in our paper . Whereas most of the inter-block Hessian , which is the upper right and lower left part of the Hessian , is close to 0 ."}, {"review_id": "POWv6hDd9XH-3", "review_text": "I could n't follow the method described in the paper . The authors are basically trying to address post-training quantization by perturbing the the weights of a trained DNN . The goal is to perturb the weights so that the quantized DNN will behave similar to the original full-precision DNN . The authors draw a link between this optimization problem and optimizing for the `` reconstruction '' of the output activations of a block ( see Equation 7 ) . The technique BRECQ , shown in Algorithm 1 , is basically to optimize the perturbation of the weights for the right hand side of Equation 7 for each block of a DNN . The method only seems to work on ReLU networks , so it 's restricted to CNNs , but this is still extremely useful . The experimental results are very strong . BRECQ is the first to achieve 4-bit integer post-training weight quantization that nearly matches the fp32 baseline on ImageNet . Even at 2-bit , BRECQ can often get within 5 % of the baseline while other methods leave the model with inoperable loss of accuracy . I could n't pinpoint any major flaws in the paper , and the results are extremely impressive .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your reviews and feedback on this paper . Here is the detailed response . + Q : `` I could n't follow the method described in the paper . The authors are basically trying to address post-training quantization by perturbing the the weights of a trained DNN . The goal is to perturb the weights so that the quantized DNN will behave similar to the original full-precision DNN . The authors draw a link between this optimization problem and optimizing for the `` reconstruction '' of the output activations of a block ( see Equation 7 ) . The technique BRECQ , shown in Algorithm 1 , is basically to optimize the perturbation of the weights for the right hand side of Equation 7 for each block of a DNN . '' A : Your summary of this paper is right , and we obtain the final results through some theoretical analysis . Here is the core analysis process of this paper : 1 . Based on Eq.7 , we show that the second-order error can be transformed into the output ( pre-activations ) . Reconstructing the network output will consider all the cross-layer dependency . 2.Based on Eq.9 , we show that using more layers to conduct reconstruction would lead to higher approximation error for quantization bias . 3.To achieve the best trade-off between the cross-layer dependency and first-order approximation error , we choose an intermediate level : BLOCK , to perform reconstruction . And we find this rule applies to many architectures and tasks . + Q : `` The method only seems to work on ReLU networks , so it 's restricted to CNNs , but this is still extremely useful . '' A : Theorem 1 actually can be applied to any activation function that has first-order gradients , since the approximation is made by $ \\Delta z\\approx J\\Delta w $ . But Theorem 2 is indeed built upon ReLU network , so there are indeed some restrictions . But as you say , ReLU networks are popular nowadays for many tasks besides CV , such as the NLP tasks . For example , the FFN in Transfomer is formed by FC and ReLU . Thanks for pointing this out , and we will try to analyze and evaluate the effect of BRECQ on other types of models in the future ."}], "0": {"review_id": "POWv6hDd9XH-0", "review_text": "This paper proposes BRECQ which is a new Post Training Quantization ( PTQ ) method . The goal of the paper is to push the limit of PTQ to low bit precision ( INT2 ) . They try to address this by considering both inter and intra-layer sensitivity to find the best update to the model parameters so that the output from a block is minimally changed/perturbed . Furthermore , the authors also consider mixed precision quantization setting . Empirical results are shown for multiple NNs for image classification and object detection . While the paper is trying to address an interesting problem and there are a lot of empirical results , but I had a very hard time to follow the paper 's main idea and the `` final '' proposed algorithm . It would be great if the authors could answer the questions below and I will reconsider my score accordingly . - What is the final algorithm ? The data provided in Algorithm 1 is very vague and there are no equations . It is not clear if you are actually using any second-order information since based on Eq 11 it seems that the proposed method is only using the gradient , and not the second order information . If so are the discussion from page 2-4 necessary ? - It is not clear if the equality given in Eq 11 is correct . With the softmax layer the Hessian diagonal would not be the same as gradient^2 . - Why not use the change in loss in Eq 4 ( left ) instead of using a second-order Taylor series ? Second-order Taylor series is an approximation and does not hold for large perturbations . Also when we have access to evaluating the change in loss , why do we even need to focus on measuring the second order term ? This does not seem to be correct/necessary at least for uniform quantization . - An ablation study is actually needed for the above . - What is the experimental setting for the ablation study in Table 1 ? while not mentioned , from the accuracy it seems the results are on ImageNet but this is not mentioned . Did the authors consider other bit precision settings ? - In page 3 , it is stated that using the Hessian directly is not possible due to memory and computational overhead . However , you can use matrix free methods which do not require forming the Hessian explicitly .", "rating": "6: Marginally above acceptance threshold", "reply_text": "+ Q : `` Why not use the change in loss in Eq 4 ( left ) instead of using a second-order Taylor series ? Second-order Taylor series is an approximation and does not hold for large perturbations . Also when we have access to evaluating the change in loss , why do we even need to focus on measuring the second order term ? This does not seem to be correct/necessary at least for uniform quantization . An ablation study is actually needed for the above . '' A : This is an interesting point in PTQ . The reviewer notes it correctly that there exists an alternative training method : QAT with limited images ( 1024 ) . In fact , we verified this method in our early exploration . The network loss optimization does not perform well , we think the reasons are twofold : ( 1 ) There are only limited images in the calibration set , ( 2 ) we fold the BN layers before reconstruction to provide higher speed-up in deployment . Without BN to update the activation statistics , the whole network optimization is hard to converge to global minimum . In fact , the network reconstruction defined in our paper is very similar to this method . Both of these two methods optimize all network parameters according to the loss computed by the network output . Likewise , it performs worse than block-wise reconstruction . The reason why block reconstruction achieves good results might be : ( 1 ) most cross-layer dependencies are centered inside a block and thus we can divide the whole network optimization into the block-level , ( 2 ) A block with several layers is easy to learn on such a small calibration dataset . Here we show the results of the ablation study . ( all hyper-parameters are aligned and we update this code in the Google Drive link ) . * * Model | Precision | Block Reconstruction | Net Reconstruction | Net Loss Optimization * * Res18 | W4A8 | 70.70 | 68.73 | 56.20 Res18 | W2A8 | 66.30 | 54.15 | 4.33 MobV2 | W4A8 | 71.66 | 66.68 | 51.27 MobV2 | W2A8 | 59.67 | 40.76 | 2.57 + Q : `` What is the experimental setting for the ablation study in Table 1 ? while not mentioned , from the accuracy it seems the results are on ImageNet but this is not mentioned . Did the authors consider other bit precision settings ? '' A : All classification experiments are conducted on ImageNet , sorry that we did not make this clear before . In other bit precision settings , we also observe the same trend in these four kinds of reconstruction granularity . But in general , higher bit-width produces less distinct results because higher bit precision incurs small perturbation and is easy to optimize . Besides , we also provided the implementation for the reviewers to verify the settings that they are interested in . + Q : `` In page 3 , it is stated that using the Hessian directly is not possible due to memory and computational overhead . However , you can use matrix free methods which do not require forming the Hessian explicitly '' A : By mentioning matrix-free optimization , we assume the reviewer is talking about the conjugate gradients method , which indeed avoids the explicit computation of the Hessian . However , the conjugate method only applies to quadratic optimization when weights are not constrained . In this work , weights are quantized and restricted to integers and therefore the conjugate gradient method is not directly applicable . But it would be interesting to see further works about conjugate gradient in quantized neural network optimization . If you have suggestions on other matrix-free methods , we are happy to discuss them . [ 1 ] Markus Nagel , Rana Ali Amjad , Mart van Baalen , Christos Louizos , and Tijmen Blankevoort . Up or down ? adaptive rounding for post-training quantization . arXiv preprint arXiv:2004.10568 , 2020 . [ 2 ] Diederik P Kingma and Jimmy Ba . Adam : A method for stochastic optimization . arXiv preprint arXiv:1412.6980 , 2014 [ 3 ] Hyeyoung Park , Shun-ichi Amari , and Kenji Fukumizu . Adaptive natural gradient learning algorithms for various stochastic models . Neural Networks , 13 ( 7 ) :755\u2013764 , 2000 ."}, "1": {"review_id": "POWv6hDd9XH-1", "review_text": "This paper explores the post-training inference quantization . Based on second-order quantization error analysis , it proposes to reconstruct quantized model in a block level to achieve SOTA accuracy for INT2 weight quantization , which distinguish this paper from previous reported layer-wise reconstruction approach . The proposed approach is intuitive and supported by extensive experiments across a wide range of image classification and object detection tasks . Overall , this paper is well written . The idea is straightforward but comes from a detailed theoretical analysis and supported by strong experimental results . The authors also proposed solutions to approximate pre-activation Hessian by using Fisher Information Matrix which sounds reasonable and is easy to implement . The authors did comprehensive comparison with SOTA approaches , not just PTQ , but also quantization aware training and mix-precision frameworks . I have some questions wish to be clarified . 1 ) .It is not clear to me , in the reconstruction process , how does the short-cut ( e.g.in ResNet ) are handled . 2 ) .In the error analysis , batch normalization layers are not considered . Batch norm will have direct impact on the output activation , hence the quantization bias term . Plus , batch norm statistics will be changed during the reconstruction phase . Could the authors comment on the impact of batch norm layers ? 3 ) .The authors use 1024 training samples to do the reconstruction . What happens when more samples are used ? What determines the number of samples needed ? 4 ) .The data presented seems to focus mainly on weight quantization , while leaving activation in relative higher precision . What happens when quantize activations in 2 bits together with the weights in this approach ? What are the challenges ? 5 ) .It would have been nice to verify this approach on tasks other than vision , such as speech and NLP tasks . 6 ) .Seems a format error on one of the reference : Zichao Guo , Xiangyu Zhang , Haoyuan Mu , Wen Heng , Zechun Liu , Yichen Wei , and Jian Sun . { SINGLE } { path } { one } - { shot } { neural } { architecture } { search } { with } { uniform } { sampling } , 2020 . URL https : //openreview.net/forum ? id=r1gPoCEKvH .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your positive feedback on this work , we wish we can address your concerns in the response below . - Q : `` It is not clear to me , in the reconstruction process , how does the short-cut ( e.g.in ResNet ) are handled . '' A : We are sorry we did not make this point clear . During block reconstruction , the block output is the output after the elemental-wise addition of two paths . Therefore , we consider the effect of shortcuts in block reconstruction . In fact , we think the shortcut component in the modern CNN block increases the intra-block dependency , therefore it is more demanding to conduct block reconstruction . - Q : `` In the error analysis , batch normalization layers are not considered . Batch norm will have a direct impact on the output activation , hence the quantization bias term . Plus , batch norm statistics will be changed during the reconstruction phase . Could the authors comment on the impact of batch norm layers ? '' A : For all of our experiments , we fold BN layers into convolutional layers as did in [ 1 ] , since BN layers require floating-point operations and will slow down the inference . As a result , there is no batch norm layer in the network and no statistics update in the reconstruction phase . - Q : `` The authors use 1024 training samples to do the reconstruction . What happens when more samples are used ? What determines the number of samples needed ? '' A : In general , more samples bring higher final performance . Please see our experimental results in Appendix.B.2 . In 4-bit quantization , the effect of numbers of samples is trivial . But in 2-bit quantization , more samples can increase 5 % accuracy . As for * What determines the number of samples needed ? * We think that it depends on the practical situation , that how much data we can get to do PTQ . In this work , we choose 1024 and align experiments setting for all baselines . - Q : '' The data presented seems to focus mainly on weight quantization , while leaving activation in relative higher precision . What happens when quantize activations in 2 bits together with the weights in this approach ? What are the challenges ? '' A : In PTQ , 2-bit activation quantization will lead to catastrophic results because activations are quantized at run-time and vary greatly . So we can not learn a per-element quantization mechanism for activations since they vary with different input images . All we can do in post-training activation quantization is to find a good clipping range . The reason why activation quantization can be okay in QAT is that the BN statistics can be updated to remember the activation distribution and adapt to a better mean/variance . In PTQ , it is almost impossible to calibrate accurate BN statistics with limited data , so low-bit activation quantization is harder . - Q : `` It would have been nice to verify this approach on tasks other than vision , such as speech and NLP tasks . '' A : This is a good suggestion for this paper and we would be glad to test BRECQ on other tasks such as NLP . But we did not conduct much research in this area and it takes some time for us to prepare the code and dataset . Considering the limited time for rebuttal , we will try to implement BRECQ on these tasks but we could not promise to complete the experiments . - Q : `` Seems a format error on one of the reference '' A : Thanks for the advice , we have revised it . [ 1 ] Raghuraman Krishnamoorthi . Quantizing deep convolutional networks for ef\ufb01cient inference : A whitepaper . arXiv preprint arXiv:1806.08342 , 2018 ."}, "2": {"review_id": "POWv6hDd9XH-2", "review_text": "Post-training quantization is an important problem , especially for industry . This paper leverages the basic building blocks and conducts a block-wise quantization . Nice results are obtained with the proposed method . The proposed method is cheap to implement and pushes the post-training quantization to 2-bit . The measurement problem of mixed precision literature raised in this paper is of insights . This problem may inspire the community to find a better measurement in future work . Extensive experiments on various methods ( handcrafted and designed by NAS ) , various tasks ( classification , detection ) , various configurations ( different bits , latency , model size ) are impressive . Various baselines are also included to make the results stronger . Questions : The block-diagonal scheme is selected according to experimental results . Is it possible to visualize the real Hessian of stage-wise settings ? ( it may be impossible to the full Hessian matrix for the whole network ) . If we see a few non-zero elements at the off-diagonal for the block-wise setting , this choice can be better motivated .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your insights and feedback on this paper . Your suggestion about `` Is it possible to visualize the real Hessian of stage-wise settings ? ( it may be impossible to the full Hessian matrix for the whole network ) '' is of great value . We visualize a Hessian matrix on a tiny ResNet . The code can be found in this Google Colab Link : https : //colab.research.google.com/drive/1hLe-Avgtdy0l5gKibR_zr2SjqApang9D ? usp=sharing . The tiny ResNet contains 3 stages with channels of { 4 , 8 , 16 } ( so that we can calculate the Hessian easily . ) The ResNet has 99.37\\ % accuracy on the MNIST dataset . We compute the Hessian of the first stage . The first stage has 2 blocks , where each block has 2 layers . Each layer in the stage has 144 number of elements . We can find that while most of the values in the Hessian matrix are close to 0 , the upper left Hessian ( which corresponds to the first block ) indeed has slightly higher absolute values , which confirms the observation in our paper . Whereas most of the inter-block Hessian , which is the upper right and lower left part of the Hessian , is close to 0 ."}, "3": {"review_id": "POWv6hDd9XH-3", "review_text": "I could n't follow the method described in the paper . The authors are basically trying to address post-training quantization by perturbing the the weights of a trained DNN . The goal is to perturb the weights so that the quantized DNN will behave similar to the original full-precision DNN . The authors draw a link between this optimization problem and optimizing for the `` reconstruction '' of the output activations of a block ( see Equation 7 ) . The technique BRECQ , shown in Algorithm 1 , is basically to optimize the perturbation of the weights for the right hand side of Equation 7 for each block of a DNN . The method only seems to work on ReLU networks , so it 's restricted to CNNs , but this is still extremely useful . The experimental results are very strong . BRECQ is the first to achieve 4-bit integer post-training weight quantization that nearly matches the fp32 baseline on ImageNet . Even at 2-bit , BRECQ can often get within 5 % of the baseline while other methods leave the model with inoperable loss of accuracy . I could n't pinpoint any major flaws in the paper , and the results are extremely impressive .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your reviews and feedback on this paper . Here is the detailed response . + Q : `` I could n't follow the method described in the paper . The authors are basically trying to address post-training quantization by perturbing the the weights of a trained DNN . The goal is to perturb the weights so that the quantized DNN will behave similar to the original full-precision DNN . The authors draw a link between this optimization problem and optimizing for the `` reconstruction '' of the output activations of a block ( see Equation 7 ) . The technique BRECQ , shown in Algorithm 1 , is basically to optimize the perturbation of the weights for the right hand side of Equation 7 for each block of a DNN . '' A : Your summary of this paper is right , and we obtain the final results through some theoretical analysis . Here is the core analysis process of this paper : 1 . Based on Eq.7 , we show that the second-order error can be transformed into the output ( pre-activations ) . Reconstructing the network output will consider all the cross-layer dependency . 2.Based on Eq.9 , we show that using more layers to conduct reconstruction would lead to higher approximation error for quantization bias . 3.To achieve the best trade-off between the cross-layer dependency and first-order approximation error , we choose an intermediate level : BLOCK , to perform reconstruction . And we find this rule applies to many architectures and tasks . + Q : `` The method only seems to work on ReLU networks , so it 's restricted to CNNs , but this is still extremely useful . '' A : Theorem 1 actually can be applied to any activation function that has first-order gradients , since the approximation is made by $ \\Delta z\\approx J\\Delta w $ . But Theorem 2 is indeed built upon ReLU network , so there are indeed some restrictions . But as you say , ReLU networks are popular nowadays for many tasks besides CV , such as the NLP tasks . For example , the FFN in Transfomer is formed by FC and ReLU . Thanks for pointing this out , and we will try to analyze and evaluate the effect of BRECQ on other types of models in the future ."}}