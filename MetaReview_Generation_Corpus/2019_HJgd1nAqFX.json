{"year": "2019", "forum": "HJgd1nAqFX", "title": "DOM-Q-NET:  Grounded RL on Structured Language", "decision": "Accept (Poster)", "meta_review": "This paper considers the task of web navigation, i.e. given a goal expressed in natural language, the task is to navigate webs by filling up fields and clicking links. The proposed model uses reinforcement learning, introducing a novel extension where the graph embedding of the pages is incorporated into the Q-function. The results are sound, and the paper is overall well-written.\n\nThe reviewers and AC note the following potential weaknesses. The primary concern that was raised was the novelty. Since the task could potentially be framed as semantic parsing, reviewer 4 mentioned there may be readily available approaches for baselines that the authors did not consider. The comparison to semantic parsing required a more detailed discussion, pointing not only the differences but also the similarities, that would encourage the two communities to explore novel approaches to their tasks. Further, reviewer 2 was concerned about the limited novelty, given the extensive work that combines GNN and RL, such as NerveNet.\n\nThe authors provided comments and a revision to address these issues. They described why it is not trivial to formulate their setup as a semantic parsing problem, partly due to the fact that the environment is partially observable.\nSimilarly, the authors described the differences between the proposed approach and methods like NerveNet, such as the use of a dynamic graph and off-policy RL, making the latter not a viable baseline for the task. These changes addressed most of the concerns raised by the reviewers.\n\nThe reviewers agreed that this paper should be accepted.", "reviews": [{"review_id": "HJgd1nAqFX-0", "review_text": "Caveat: I am an emergency reviewer filling in for someone that fell through on their commitment to review for ICLR. The framing of this paper is quite outside my typical area, so I am not super familiar with the related work here, nor do I have time to get familiar with it for this last-minute review. This paper presents a new model for deep reinforcement learning on web pages, where the system is given a goal (stated in text) and is supposed to interact with the web page (through clicking and entering text) in order to achieve that goal. The supervision is a positive reward when the sequence of actions taken matches the goal. The novel model presented in this paper is a modular Q function that incorporates graph embeddings of the web page's DOM, as well as similarity scores between elements in the DOM with words in the goal. Just judging the presentation of the paper, it looks sound. The methods seem reasonable (very similar to methods that are known to work well on related problems; more on that below), and the experiments look to be well done. The paper is reasonably well written. I don't know the RL community well enough to know how impactful this particular piece of work would be there - it's a new model architecture, basically, that gives improved performance. I'd probably give a similar paper in my area a 3.5-4 out of 5 for an ACL conference. The one major drawback I see in this paper is that it is _so_ similar to work on semantic parsing, but doesn't realize it. I am not a \"reinforcement learning\" researcher, though I am a \"semantic parsing\" researcher. The problem statement in this paper reads to me exactly like a semantic parsing problem: map a piece of text to a statement in some formal language. In this case, the \"statement\" is a sequence of actions on the DOM of a web page. The web page is possibly unseen at test time (the particulars of the data setup weren't totally clear to me), so the model has to be able to handle linking words in the sentence to pieces of the DOM in a way that doesn't rely on having seen those DOM elements during training. This setup seems almost identical to the WikiTableQuestions dataset (Pasupat and Liang 2015), which has seen several RL-inspired works recently (e.g., https://arxiv.org/abs/1807.02322). The way that the authors propose to use attention scores in the \"global module\" is _very_ similar to the linking mechanism proposed by Krishnamurthy, Dasigi and Gardner (EMNLP 2017) for WikiTableQuestions, and the way that the \"word-token selection\" only allows words in the goal sentence is very reminiscent of Chen Liang's language for parsing questions in WikiTableQuestions, which has similar restrictions for similar reasons. I think the main difference between what we call \"weakly-supervised semantic parsing\" and what you call \"deep reinforcement learning\" is that semantic parsing leverages the fact that we know the language we're parsing into, so we don't need to use model-free RL methods like Q-learning. We know the model, so we can be much smarter about learning. Again, I'm not super familiar with the tasks you're looking at here, but I'm pretty sure there are much better _supervised_ learning techniques that you could apply to these problems. All of this is to say that the methods proposed here look _very_ similar to methods that have been studied for quite a while in the semantic parsing literature (I gave only recent references above, but the basic problems go back decades; e.g., http://aclweb.org/anthology/P09-1010, or http://www.cs.utexas.edu/~ml/papers/senior-aaai-2008.pdf). Yet this paper only cites recent deep RL papers. I think the authors would benefit greatly from familiarizing themselves with this literature. I think the semantic parsing community would also benefit from this, as there are surely ideas in the deep RL community that we could benefit from, too. But the two communities don't really talk to each other much, it seems, even though in some cases we are working on _very_ similar problems. So, to summarize: the paper seems reasonable enough. I'm guessing that the RL community would find it at least moderately interesting, and it appears well written and well executed. My one concern is that it's totally oblivious to the fact that it's sitting right next to a well-established literature that could probably teach it a thing or two about mapping language to actions. -------------- After seeing the authors engage at least a little with the related semantic parsing literature, I've increased my score to a 7.", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer , We thank the reviewer for the valuable comments in pointing out the connection between our work and semantic parsing . We have updated the background section to discuss the semantic parsing methods to solve the problem like Question Answering from manipulating the data on HTML tables . We would like to emphasize that the main focus of this work is to train an end-to-end RL agent to directly interact with any standard web browser through mouse clicking and typing . Our direct approach avoids the challenge of designing the base-level primitive logical forms in semantic parsing for web navigation . The problem setup of mapping language to click/type action was introduced in the MiniWoB [ 7 ] , which is a set of standard benchmark environments for web agents . The authors of [ 7 ] , in fact , found that reinforcement learning approaches often significantly outperform supervised learning on these benchmark tasks . We have added a new illustration in Figure 2 to further clarify our problem setup . [ Concern ] \u201c This setup seems almost identical to the WikiTableQuestions dataset ( Pasupat and Liang 2015 ) , which has seen several RL-inspired works recently \u201d [ Reply ] We appreciate pointing out the relevant works , and we explained the problem setup of this dataset [ 5 ] as well as the difference between this task and web navigation in the background sec2.5 and the appendix 6.7 for further details . In short , WikiTableQuestions [ 5 ] provides structured HTML tables that only contain the text attributes from the original HTML page . To execute the parsed logical form , an executor is provided . However , MiniWoB [ 7 ] environment has a set of more diverse web pages beyond tables . The agent needs to understand raw HTMLs that contain text fields , buttons and checkboxes . Each MiniWoB task is given by natural language goal instructions on different web pages . This RL environment only accepts basic actions like \u201c click DOM indexed i \u201d , \u201c Type a string on DOM indexed i \u201d . So we took a direct approach that allows our agent to control clicking and typing . which avoids the challenge of designing rich primitives and formal language for web navigation . [ Concern ] \u201c The problem statement in this paper reads to me exactly like a semantic parsing problem \u201d [ Reply ] Our main goal is to train an agent end-to-end to directly click the DOMs and type strings on the standard browser . We are converting natural language to a sequence of clicking and typing actions that a browser can execute . \u201c Map a piece of text to a statement in some formal language \u201d - This is what we hope to avoid because the standard browser only accepts \u201c which DOM to type/click \u201d , \u201c what to type \u201d as valid actions , so we can not have a complex logical form as an output of the model . For web navigation , it is not trivial to design formal language and the primitives of the formal language . However , we noted in the section 2.6 that Liu et al [ 6 ] defined their minimalistic formal language to constrain the exploration , but they still use RL to perform the same set of actions as ours to a standard browser . The focus of our work is to learn an end-to-end RL agent that can act directly in a web browser . Empirically , we found our end-to-end agent matches and outperforms ( for some tasks ) the models augmented with formal language studied in Liu et al [ 6 ] . [ 1 ] Jayant Krishnamurthy , Pradeep Dasigi , and Matt Gardner . Neural semantic parsing with type constraints for semi-structured tables . In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pp . 1516\u20131526 . Association for Computational Linguistics , 2017. doi : 10.18653/v1/D17-1160 . URL http : //aclweb.org/anthology/D17-1160 . [ 2 ] Satchuthananthavale RK Branavan , Harr Chen , Luke S Zettlemoyer , and Regina Barzilay . Reinforcement learning for mapping instructions to actions . In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP : Volume 1-Volume 1 , pp . 82\u201390.Association for Computational Linguistics , 2009 . [ 3 ] Chen Liang , Mohammad Norouzi , Jonathan Berant , Quoc Le , and Ni Lao . Memory augmented policy optimization for program synthesis with generalization . arXiv preprint arXiv:1807.02322 , 2018 [ 4 ] Raymond J Mooney . Learning to connect language and perception . In AAAI , pp . 1598\u20131601 , 2008 . [ 5 ] Panupong Pasupat and Percy Liang . Compositional semantic parsing on semi-structured tables . arXiv preprint arXiv:1508.00305 , 2015 . [ 6 ] Evan Zheran Liu , Kelvin Guu , Panupong Pasupat , Tianlin Shi , and Percy Liang . Reinforcement learning on web interfaces using workflow-guided exploration . In ICLR , 2018 . [ 7 ] Tianlin Shi , Andrej Karpathy , Linxi Fan , Jonathan Hernandez , and Percy Liang . World of bits : An open-domain platform for web-based agents . In ICML , 2017 ."}, {"review_id": "HJgd1nAqFX-1", "review_text": "DOM-Q-NET: GROUNDED RL ON STRUCTURED LANGUAGE This paper presents a somewhat novel graph-based Q-learning method for web navigation benchmark tasks. The authors show that multi-task learning helps in this case and their method is able to learn without BC as previous works have needed. While this work is interesting and to my knowledge somewhat novel. I concerns with one aspect of the evaluation. In some part it was stated that they show the highest success rate for testing on 100 episodes, if this is indeed the maximum success rate, it is unclear if these results are misleading or not. It is possible that there was a lucky seed in those 100 episodes leading to a higher max that is not representative of the algorithm performance. Also, please have the submission proof-read for English style and grammar issues. There are many minor mistakes, some of which are pointed out below. I am rating marginally below due mainly to the potentially misleading results from the comment on using the highest success rate to report results and to a minor extent due to the novelty aspect (though this is an interesting application). Comments: - \u201cEvaluation metric: we plot the moving average of the reward for last 100 episodes, and report the highest success rate for testing on 100 episodes.\u201d \u2014> This is unclear, do you mean you only displayed the maximum success rate out of all 100 episodes? So if the success rates are [0, 100, 0, 0, 0], Figure 2 shows 100% success? If so, this is somewhat misleading and a better metric may have been the average success rate with confidence intervals. Otherwise you may have just gotten a lucky random seed potentially. - I would\u2019ve liked to see if this is the only method which benefits from multitask learning or do DOMNETs also benefit. This however, is just a nice to have. - I appreciate the inclusion of hyper parameters and commitment to releasing the code in an effort to promote reproducibility! Great job there. - I really like the idea of using graph networks with RL, though I\u2019m not sure if it\u2019s novel to this work. Interesting line of work! - While this is an interesting application, I\u2019m not sure about the novelty. I suggest spending a bit more time discussing how this work contrasts with methods like Wang et al., or others cited here. Typos: \u201cMiniWoB(Shi et al., 2017) benchmark tasks. \u201c \u2014> missing space between citation \u201cQ network architecture with graph neural network\u201d \u2014> with a graph neural network \"MiniWoB(Shi et al., 2017)\u201d \u2014> MiniWoB (Shi et al., 2017) (missing space) \u201cachieved the state\u201d \u2014> achieved state of the art \u201c2016; Wang et al., 2018)as main\u201d \u2014> missing space \u201cseries of attentions between DOM elements and goal\u201d \u2014> series of attention (modules?) between the DOM elements and the goal (?) \u201cconstrained action set\u201d \u2014> constrained action sets \u201cIn appendix, we define our criteria for difficulties of different tasks.\u201d \u2014> In the appendix", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear reviewer , Thank you for taking the time to review our paper . We appreciate the valuable comments that improve the readability and the clarity of the paper and we will incorporate all the changes and fix the typos in our latest revision . [ Concern1 ] \u201c Unclear evaluation metric \u201d [ Reply ] Our experimental protocol follows the previous works on the same environment [ 1 , 4 ] . We report the success rate of the 100 test episodes at the end of the training once the agent has converged to its highest performance on the training episodes . The final success rates reported in Figure 2 of the original submission were averaged across 4 different random seeds/runs . We apologize for the poor wording that has been corrected in our latest revision . In detail , what we did was to evaluate the RL agent after training for a fixed number of frames depending on the difficulty of the task . In the original paper , we mentioned in the appendix that we used three different number of frames { 5000 , 50000 , 200000 } for training based on the difficulty { easy , medium , hard } of the 23 tasks in MiniWOB . In the initial experiments , we observed that some tasks were solved with far less number of frames than others due to varying difficulties , so we categorized 23 tasks in three difficulty groups to shorten the experiment time for simpler tasks . This alleviated unnecessary computational cost for a large number of experiments . The results and the plots we presented in the paper , are based on the following number of experiments . Number of experiments = ( 23 ( number of tasks ) + 9 ( number of tasks concurrently running in multitask ) ) * 4 ( types of goal encoding ) * 4 ( minimum num of runs for average ) + 2 ( tasks for ablation study ) * 3 ( discounted model ) * 4 ( minimum num of runs for average ) = 536 experiments for one set of hyperparameters . For further details on our experiment protocols , please check the updated `` evaluation metric '' in Sec4.1 and Appendix6.5 So our success rate reported in Figure 2 ( original paper ) is based on the average success rate of 4 runs . [ Concern2 ] \u201c Novelty issue \u201d , `` Lack of comparisons with previous works on GNN+RL '' [ Reply ] To our knowledge , this is the first work that applies graph neural networks ( GNNs ) to represent the HTML structure in standard web pages . This leads to our novel deep Q-network architecture that incorporates both the goal attention mechanism and the GNN representation to learn the state-action value function for Q learning . We appreciate the reviewer to point out the similarity and lack of comparisons with other GNN+RL models , e.g.Nervenet [ 2 ] . Unfortunately , previous works on GNN+RL are not directly applicable to our web navigation problem . Please see the following paragraph that has been added to the latest revision for a detailed explanation . - Our main contribution is to propose a new architecture for parameterizing factorized Q functions using goal attention , local word embeddings and graph neural network ( GNN ) . We also contributed to the formulation of web navigation with this architecture . GNN is one of the components , and we investigated in the ablation study that some tasks need GNN for neural message passing [ 3 ] and some tasks do not necessarily need it though the sample efficiency is better with GNN . We also showed how proposed goal attention can be used with GNN for even better sample efficiency when multitasking . Computing the output model of GNN with goal attention is unique in our goal-oriented RL setting with graph state/action representations . Previously proposed Graph Attention Networks [ 5 ] uses attention in neural-message passing phase , and is experimented in non-RL settings . In general , GNN is not actively used in RL settings as seen in this comprehensive survey paper of GNN [ 6 ] , and the previous papers [ 2 , 7 ] use GNN for mimicking the physical bodies of different robots . We would like to mention some differences when using GNN for representing web pages ."}, {"review_id": "HJgd1nAqFX-2", "review_text": "The authors propose a novel architecture for RL-based web navigation to address both of these problems, DOM-Q-NET, which utilizes a graph neural network to represent tree-structured HTML along with a shared state space across multiple tasks. It is believed more flexible to be probed on WorldOfBits environments. Significant improvements are shown by experiment.", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer , Thank you for the positive feedback and taking the time to review our paper . In order to pursue further reproducibility , we clarified our experiment protocols in sec4.1 and the appendix 6.5 for further details . We have also added background sec2.5 and the appendix 6.7 for further details on a task solved by semantic parsing and how it is different from MiniWoB . In addition , a demo of a successful trajectory , figure2 in the revision , is added to further demonstrate our problem setup and the instances for the tuple of actions ."}], "0": {"review_id": "HJgd1nAqFX-0", "review_text": "Caveat: I am an emergency reviewer filling in for someone that fell through on their commitment to review for ICLR. The framing of this paper is quite outside my typical area, so I am not super familiar with the related work here, nor do I have time to get familiar with it for this last-minute review. This paper presents a new model for deep reinforcement learning on web pages, where the system is given a goal (stated in text) and is supposed to interact with the web page (through clicking and entering text) in order to achieve that goal. The supervision is a positive reward when the sequence of actions taken matches the goal. The novel model presented in this paper is a modular Q function that incorporates graph embeddings of the web page's DOM, as well as similarity scores between elements in the DOM with words in the goal. Just judging the presentation of the paper, it looks sound. The methods seem reasonable (very similar to methods that are known to work well on related problems; more on that below), and the experiments look to be well done. The paper is reasonably well written. I don't know the RL community well enough to know how impactful this particular piece of work would be there - it's a new model architecture, basically, that gives improved performance. I'd probably give a similar paper in my area a 3.5-4 out of 5 for an ACL conference. The one major drawback I see in this paper is that it is _so_ similar to work on semantic parsing, but doesn't realize it. I am not a \"reinforcement learning\" researcher, though I am a \"semantic parsing\" researcher. The problem statement in this paper reads to me exactly like a semantic parsing problem: map a piece of text to a statement in some formal language. In this case, the \"statement\" is a sequence of actions on the DOM of a web page. The web page is possibly unseen at test time (the particulars of the data setup weren't totally clear to me), so the model has to be able to handle linking words in the sentence to pieces of the DOM in a way that doesn't rely on having seen those DOM elements during training. This setup seems almost identical to the WikiTableQuestions dataset (Pasupat and Liang 2015), which has seen several RL-inspired works recently (e.g., https://arxiv.org/abs/1807.02322). The way that the authors propose to use attention scores in the \"global module\" is _very_ similar to the linking mechanism proposed by Krishnamurthy, Dasigi and Gardner (EMNLP 2017) for WikiTableQuestions, and the way that the \"word-token selection\" only allows words in the goal sentence is very reminiscent of Chen Liang's language for parsing questions in WikiTableQuestions, which has similar restrictions for similar reasons. I think the main difference between what we call \"weakly-supervised semantic parsing\" and what you call \"deep reinforcement learning\" is that semantic parsing leverages the fact that we know the language we're parsing into, so we don't need to use model-free RL methods like Q-learning. We know the model, so we can be much smarter about learning. Again, I'm not super familiar with the tasks you're looking at here, but I'm pretty sure there are much better _supervised_ learning techniques that you could apply to these problems. All of this is to say that the methods proposed here look _very_ similar to methods that have been studied for quite a while in the semantic parsing literature (I gave only recent references above, but the basic problems go back decades; e.g., http://aclweb.org/anthology/P09-1010, or http://www.cs.utexas.edu/~ml/papers/senior-aaai-2008.pdf). Yet this paper only cites recent deep RL papers. I think the authors would benefit greatly from familiarizing themselves with this literature. I think the semantic parsing community would also benefit from this, as there are surely ideas in the deep RL community that we could benefit from, too. But the two communities don't really talk to each other much, it seems, even though in some cases we are working on _very_ similar problems. So, to summarize: the paper seems reasonable enough. I'm guessing that the RL community would find it at least moderately interesting, and it appears well written and well executed. My one concern is that it's totally oblivious to the fact that it's sitting right next to a well-established literature that could probably teach it a thing or two about mapping language to actions. -------------- After seeing the authors engage at least a little with the related semantic parsing literature, I've increased my score to a 7.", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer , We thank the reviewer for the valuable comments in pointing out the connection between our work and semantic parsing . We have updated the background section to discuss the semantic parsing methods to solve the problem like Question Answering from manipulating the data on HTML tables . We would like to emphasize that the main focus of this work is to train an end-to-end RL agent to directly interact with any standard web browser through mouse clicking and typing . Our direct approach avoids the challenge of designing the base-level primitive logical forms in semantic parsing for web navigation . The problem setup of mapping language to click/type action was introduced in the MiniWoB [ 7 ] , which is a set of standard benchmark environments for web agents . The authors of [ 7 ] , in fact , found that reinforcement learning approaches often significantly outperform supervised learning on these benchmark tasks . We have added a new illustration in Figure 2 to further clarify our problem setup . [ Concern ] \u201c This setup seems almost identical to the WikiTableQuestions dataset ( Pasupat and Liang 2015 ) , which has seen several RL-inspired works recently \u201d [ Reply ] We appreciate pointing out the relevant works , and we explained the problem setup of this dataset [ 5 ] as well as the difference between this task and web navigation in the background sec2.5 and the appendix 6.7 for further details . In short , WikiTableQuestions [ 5 ] provides structured HTML tables that only contain the text attributes from the original HTML page . To execute the parsed logical form , an executor is provided . However , MiniWoB [ 7 ] environment has a set of more diverse web pages beyond tables . The agent needs to understand raw HTMLs that contain text fields , buttons and checkboxes . Each MiniWoB task is given by natural language goal instructions on different web pages . This RL environment only accepts basic actions like \u201c click DOM indexed i \u201d , \u201c Type a string on DOM indexed i \u201d . So we took a direct approach that allows our agent to control clicking and typing . which avoids the challenge of designing rich primitives and formal language for web navigation . [ Concern ] \u201c The problem statement in this paper reads to me exactly like a semantic parsing problem \u201d [ Reply ] Our main goal is to train an agent end-to-end to directly click the DOMs and type strings on the standard browser . We are converting natural language to a sequence of clicking and typing actions that a browser can execute . \u201c Map a piece of text to a statement in some formal language \u201d - This is what we hope to avoid because the standard browser only accepts \u201c which DOM to type/click \u201d , \u201c what to type \u201d as valid actions , so we can not have a complex logical form as an output of the model . For web navigation , it is not trivial to design formal language and the primitives of the formal language . However , we noted in the section 2.6 that Liu et al [ 6 ] defined their minimalistic formal language to constrain the exploration , but they still use RL to perform the same set of actions as ours to a standard browser . The focus of our work is to learn an end-to-end RL agent that can act directly in a web browser . Empirically , we found our end-to-end agent matches and outperforms ( for some tasks ) the models augmented with formal language studied in Liu et al [ 6 ] . [ 1 ] Jayant Krishnamurthy , Pradeep Dasigi , and Matt Gardner . Neural semantic parsing with type constraints for semi-structured tables . In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pp . 1516\u20131526 . Association for Computational Linguistics , 2017. doi : 10.18653/v1/D17-1160 . URL http : //aclweb.org/anthology/D17-1160 . [ 2 ] Satchuthananthavale RK Branavan , Harr Chen , Luke S Zettlemoyer , and Regina Barzilay . Reinforcement learning for mapping instructions to actions . In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP : Volume 1-Volume 1 , pp . 82\u201390.Association for Computational Linguistics , 2009 . [ 3 ] Chen Liang , Mohammad Norouzi , Jonathan Berant , Quoc Le , and Ni Lao . Memory augmented policy optimization for program synthesis with generalization . arXiv preprint arXiv:1807.02322 , 2018 [ 4 ] Raymond J Mooney . Learning to connect language and perception . In AAAI , pp . 1598\u20131601 , 2008 . [ 5 ] Panupong Pasupat and Percy Liang . Compositional semantic parsing on semi-structured tables . arXiv preprint arXiv:1508.00305 , 2015 . [ 6 ] Evan Zheran Liu , Kelvin Guu , Panupong Pasupat , Tianlin Shi , and Percy Liang . Reinforcement learning on web interfaces using workflow-guided exploration . In ICLR , 2018 . [ 7 ] Tianlin Shi , Andrej Karpathy , Linxi Fan , Jonathan Hernandez , and Percy Liang . World of bits : An open-domain platform for web-based agents . In ICML , 2017 ."}, "1": {"review_id": "HJgd1nAqFX-1", "review_text": "DOM-Q-NET: GROUNDED RL ON STRUCTURED LANGUAGE This paper presents a somewhat novel graph-based Q-learning method for web navigation benchmark tasks. The authors show that multi-task learning helps in this case and their method is able to learn without BC as previous works have needed. While this work is interesting and to my knowledge somewhat novel. I concerns with one aspect of the evaluation. In some part it was stated that they show the highest success rate for testing on 100 episodes, if this is indeed the maximum success rate, it is unclear if these results are misleading or not. It is possible that there was a lucky seed in those 100 episodes leading to a higher max that is not representative of the algorithm performance. Also, please have the submission proof-read for English style and grammar issues. There are many minor mistakes, some of which are pointed out below. I am rating marginally below due mainly to the potentially misleading results from the comment on using the highest success rate to report results and to a minor extent due to the novelty aspect (though this is an interesting application). Comments: - \u201cEvaluation metric: we plot the moving average of the reward for last 100 episodes, and report the highest success rate for testing on 100 episodes.\u201d \u2014> This is unclear, do you mean you only displayed the maximum success rate out of all 100 episodes? So if the success rates are [0, 100, 0, 0, 0], Figure 2 shows 100% success? If so, this is somewhat misleading and a better metric may have been the average success rate with confidence intervals. Otherwise you may have just gotten a lucky random seed potentially. - I would\u2019ve liked to see if this is the only method which benefits from multitask learning or do DOMNETs also benefit. This however, is just a nice to have. - I appreciate the inclusion of hyper parameters and commitment to releasing the code in an effort to promote reproducibility! Great job there. - I really like the idea of using graph networks with RL, though I\u2019m not sure if it\u2019s novel to this work. Interesting line of work! - While this is an interesting application, I\u2019m not sure about the novelty. I suggest spending a bit more time discussing how this work contrasts with methods like Wang et al., or others cited here. Typos: \u201cMiniWoB(Shi et al., 2017) benchmark tasks. \u201c \u2014> missing space between citation \u201cQ network architecture with graph neural network\u201d \u2014> with a graph neural network \"MiniWoB(Shi et al., 2017)\u201d \u2014> MiniWoB (Shi et al., 2017) (missing space) \u201cachieved the state\u201d \u2014> achieved state of the art \u201c2016; Wang et al., 2018)as main\u201d \u2014> missing space \u201cseries of attentions between DOM elements and goal\u201d \u2014> series of attention (modules?) between the DOM elements and the goal (?) \u201cconstrained action set\u201d \u2014> constrained action sets \u201cIn appendix, we define our criteria for difficulties of different tasks.\u201d \u2014> In the appendix", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear reviewer , Thank you for taking the time to review our paper . We appreciate the valuable comments that improve the readability and the clarity of the paper and we will incorporate all the changes and fix the typos in our latest revision . [ Concern1 ] \u201c Unclear evaluation metric \u201d [ Reply ] Our experimental protocol follows the previous works on the same environment [ 1 , 4 ] . We report the success rate of the 100 test episodes at the end of the training once the agent has converged to its highest performance on the training episodes . The final success rates reported in Figure 2 of the original submission were averaged across 4 different random seeds/runs . We apologize for the poor wording that has been corrected in our latest revision . In detail , what we did was to evaluate the RL agent after training for a fixed number of frames depending on the difficulty of the task . In the original paper , we mentioned in the appendix that we used three different number of frames { 5000 , 50000 , 200000 } for training based on the difficulty { easy , medium , hard } of the 23 tasks in MiniWOB . In the initial experiments , we observed that some tasks were solved with far less number of frames than others due to varying difficulties , so we categorized 23 tasks in three difficulty groups to shorten the experiment time for simpler tasks . This alleviated unnecessary computational cost for a large number of experiments . The results and the plots we presented in the paper , are based on the following number of experiments . Number of experiments = ( 23 ( number of tasks ) + 9 ( number of tasks concurrently running in multitask ) ) * 4 ( types of goal encoding ) * 4 ( minimum num of runs for average ) + 2 ( tasks for ablation study ) * 3 ( discounted model ) * 4 ( minimum num of runs for average ) = 536 experiments for one set of hyperparameters . For further details on our experiment protocols , please check the updated `` evaluation metric '' in Sec4.1 and Appendix6.5 So our success rate reported in Figure 2 ( original paper ) is based on the average success rate of 4 runs . [ Concern2 ] \u201c Novelty issue \u201d , `` Lack of comparisons with previous works on GNN+RL '' [ Reply ] To our knowledge , this is the first work that applies graph neural networks ( GNNs ) to represent the HTML structure in standard web pages . This leads to our novel deep Q-network architecture that incorporates both the goal attention mechanism and the GNN representation to learn the state-action value function for Q learning . We appreciate the reviewer to point out the similarity and lack of comparisons with other GNN+RL models , e.g.Nervenet [ 2 ] . Unfortunately , previous works on GNN+RL are not directly applicable to our web navigation problem . Please see the following paragraph that has been added to the latest revision for a detailed explanation . - Our main contribution is to propose a new architecture for parameterizing factorized Q functions using goal attention , local word embeddings and graph neural network ( GNN ) . We also contributed to the formulation of web navigation with this architecture . GNN is one of the components , and we investigated in the ablation study that some tasks need GNN for neural message passing [ 3 ] and some tasks do not necessarily need it though the sample efficiency is better with GNN . We also showed how proposed goal attention can be used with GNN for even better sample efficiency when multitasking . Computing the output model of GNN with goal attention is unique in our goal-oriented RL setting with graph state/action representations . Previously proposed Graph Attention Networks [ 5 ] uses attention in neural-message passing phase , and is experimented in non-RL settings . In general , GNN is not actively used in RL settings as seen in this comprehensive survey paper of GNN [ 6 ] , and the previous papers [ 2 , 7 ] use GNN for mimicking the physical bodies of different robots . We would like to mention some differences when using GNN for representing web pages ."}, "2": {"review_id": "HJgd1nAqFX-2", "review_text": "The authors propose a novel architecture for RL-based web navigation to address both of these problems, DOM-Q-NET, which utilizes a graph neural network to represent tree-structured HTML along with a shared state space across multiple tasks. It is believed more flexible to be probed on WorldOfBits environments. Significant improvements are shown by experiment.", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer , Thank you for the positive feedback and taking the time to review our paper . In order to pursue further reproducibility , we clarified our experiment protocols in sec4.1 and the appendix 6.5 for further details . We have also added background sec2.5 and the appendix 6.7 for further details on a task solved by semantic parsing and how it is different from MiniWoB . In addition , a demo of a successful trajectory , figure2 in the revision , is added to further demonstrate our problem setup and the instances for the tuple of actions ."}}