{"year": "2019", "forum": "HkxStoC5F7", "title": "Meta-Learning Probabilistic Inference for Prediction", "decision": "Accept (Poster)", "meta_review": "The paper proposes a decision-theoretic framework for meta-learning. The ideas and analysis are interesting and well-motivated, and the experiments are thorough. The primary concerns of the reviewers have been addressed in new revisions of the paper. The reviewers all agree that the paper should be accepted. Hence, I recommend acceptance.", "reviews": [{"review_id": "HkxStoC5F7-0", "review_text": "This paper proposes both a general meta-learning framework with approximate probabilistic inference, and implements an instance of it for few-shot learning. First, they propose Meta-Learning Probabilistic inference for Prediction (ML-PIP) which trains the meta-learner to minimize the KL-divergence between the approximate predictive distribution generated from it and predictive distribution for each class. Then, they use this framework to implement Versatile Amortized Inference, which they call VERSA. VERSA replaces the optimization for test time with efficient posterior inference, by generating distribution over task-specific parameters in a single forward pass. The authors validate VERSA against amortized and non-amortized variational inference which it outperforms. VERSA is also highly versatile as it can be trained with varying number of classes and shots. Pros - The proposed general meta-learning framework that aims to learn the meta-learner that approximates the predictive distribution over multiple tasks is quite novel and makes sense. - VERSA obtains impressive performance on both benchmark datasets for few-shot learning and is versatile in terms of number of classes and shots. - The appendix section has in-depth analysis and additional experimental results which are quite helpful in understanding the paper. Cons - The main paper feels quite empty, especially the experimental validation parts with limited number of baselines. It would have been good if some of the experiments could be moved into the main paper. Some experimental results such as Figure 4 on versatility does not add much insight to the main story and could be moved to appendix. - It would have been good if there was some validation of the time-performance of the model as one motivation of meta-learning is rapid adaptation to a test-time task. In sum, since the proposed meta-learning probabilistic inference framework is novel and effective I vote for accepting the paper. However the structure and organization of the paper could be improved by moving some of the methodological details and experimental results in the appendix to the main paper. ", "rating": "7: Good paper, accept", "reply_text": "\u201c It would have been good if some of the experiments could be moved into the main paper . \u2026 the structure and organization of the paper could be improved by moving some of the methodological details and experimental results in the appendix to the main paper. \u201d We agree that a significant portion of interesting content has been relegated to the appendix in our submission . Much of this , of course , has to do with space constraints . However , we have addressed this in the revised version in line with your suggestions by ( i ) moving the appendix containing the toy-data experimentation to the main body of the paper ( see Section 5.1 ) , and ( ii ) moving some methodological details from the appendix in to the experiments section ( see Section 5 ) . \u201c It would have been good if there was some validation of the time-performance of the model as one motivation of meta-learning is rapid adaptation to a test-time task . \u201c We strongly agree that the issue of performance timing is of great interest , and it is useful and important to validate this experimentally . We were originally hesitant to add any timing results as code released with research papers is often optimized for correctness as opposed to speed . That said , we measured the test time performance of both MAML ( as implemented in the authors ' publicly available repository at https : //github.com/cbfinn/maml ) and Versa in 5-shot 5-way experiments on mini-ImageNet , using the same architectures for both . We found Versa to achieve 5x speed up compared to MAML , while achieving significantly better accuracy ( see Table 3 ) . We have amended the paper to include this experimental data ( see Section 5.2 for details ) . We believe this data demonstrates the performance gains achieved by relieving the need for test time optimization procedures ."}, {"review_id": "HkxStoC5F7-1", "review_text": "This paper presents two different sections: 1. A generalized framework to describe a range of meta-learning algorithms. 2. A meta-learning algorithm that allows few shot inference over new tasks without the need for retraining. The important aspect of the algorithm is the context independence assumption between posteriors of different classes for learning weights. This reduces the number of parameters to amortize during meta-training. More importantly, it makes it independent of the number of classes in a task, and effectively doing meta-training across class inference instead of each task. The idea sounds great, but I am skeptical of the justification behind the independence assumption which, as per its justifications sounds contrived and only empirical. Overall, I feel the paper makes some progress in important aspects of meta-learning.", "rating": "6: Marginally above acceptance threshold", "reply_text": "\u201c The important aspect of the algorithm is the context independence assumption between posteriors of different classes for learning weights . \u2026 The idea sounds great , but I am skeptical of the justification behind the independence assumption which , as per its justifications sounds contrived and only empirical. \u201d We thank the reviewer for imploring us to think more carefully about this point . We share the concern that providing only an empirical justification for the context independent assumption is slightly troubling . We have therefore considered this more carefully , and have found that there is a principled justification of this design choice , which is best understood through the lens of density ratio estimation [ i , ii ] . Results from Density Ratio Estimation [ i , ii ] show that an optimal softmax classifier learns the ratio of the densities Softmax ( y=k | x ) = p ( x | y=k ) / Sum_j p ( x | y=j ) assuming equal a priori probability for each class . Our system follows his optimal form by setting : log p ( \\tilde { x } | y=c ) proportional h_theta ( \\tilde { x } ) ^T w_c where w_c ~ q_phi ( w | { x_n ; y_n=c } ) for each class in a given task . Here { ( x_n , y_n ) } are the few-shot training examples , and $ \\tilde { x } $ is the test example . This argument states that under ideal conditions ( i.e. , we can perfectly estimate p ( y=c | x ) ) , the context-independent assumption is correct , and further motivates our design . We have amended the paper to include this argument ( see Appendix B ) . We thank the reviewer for pointing to this important issue , and we hope that this alleviates some of their concerns . [ i ] - S. Mohamed . The Density Ratio Trick . The Spectator ( Blog ) . 2018 [ ii ] - M. Sugiyama , T. Suzuki , and T. Kanamori . Density ratio estimation in machine learning . 2012"}, {"review_id": "HkxStoC5F7-2", "review_text": "Summary: This work tackles few-shot (or meta) learning from a probabilistic inference viewpoint. Compared to previous work, it uses a simpler setup, performing task-specific inference only for single-layer head models, and employs an objective based on predictive distributions on train/test splits for each task (rather than an approximation to log marginal likelihood). Inference is done amortized by a network, whose input is the task training split. The same network is used for parameters of each class (only feeding training points of that class), which allows an arbitrary number of classes per task. At test time, inference just requires forward passes through this network, attractive compared to non-amortized approaches which need optimization or gradients here. It provides a clean, decision-theoretic derivation, and clarifies relationships to previous work. The experimental results are encouraging: the method achieves a new best on 5-way, 5-shot miniImageNet, despite the simple setup. In general, explanations in the main text could be more complete (see questions). I'd recommend shortening Section 4, which is pretty obvious. - Quality: Several interesting differences to prior work. Well-done experiments - Clarity: Clean derivation, easy to understand. Some details could be spelled out better - Originality: Several important novelties (predictive criterion, simple model setup, amortized inference network). Closely related to \"neural processes\" work, but this happened roughly at the same time - Significance: The few-shot learning results are competitive, in particular given they use a simpler model setup than most previous work. I am not an expert on these kind of experiments, but I found the comparisons fair and rather extensive Interesting about this work: - Clean Bayesian decision-theoretic viewpoint. Key question is of course whether an inference network of this simple structure (no correlations, sum combination of datapoints, same network for each class) can deliver a good approximation to the true posterior. - Different to previous work, task-specific inference is done only on the weights of single-layer head models (logistic regression models, with shared features). Highly encouraging that this is sufficient for state-of-the-art few-shot classification performance. The authors could be more clear about this point. - Simple and efficient amortized inference model, which along with the neural network features, is learned on all data jointly - Optimization criterion is based on predictive distributions on train/test splits, not on the log marginal likelihood. Has some odd consequences (question below), but clearly works better for few-shot classification Experiments: - 5.1: Convincing results, in particular given the simplicity of the model setup and the inference network. But some important points are not explained: - Which of the competitors (if any) use the same restricted model setup (inference only on the top-layer weights)? Clearly, MAML does not, right? Please state this explicitly. - For Versa, you use k_c training and 15 test points per task update during training. Do competitors without train/test split also get k_c + 15 points, or only k_c points? The former would be fair, the latter not so much. - 5.2: This seems a challenging problem, and both your numbers and reconstructions look better than the competitor. I cannot say more, based on the very brief explanations provided here. The main paper does not really state what the model or the likelihood is. From F.4 in the Appendix, this model does not have the form of your classification models, but psi is input at the bottom of the network. Also, the final layer has sigmoid activation. What likelihood do you use? One observation: If you used the same \"inference on final layer weights\" setup here, and Gaussian likelihood, you could compute the posterior over psi in closed form, no amortization needed. Would this setup apply to your problem? Further questions: - Confused about the input to the inference network. Real Bayesian inference would just see features h_theta(x) as inputs, not the x's. Why not simply feed features in then? Please do improve the description of the inference network, this is a major novelty of this paper, and even the appendix is only understandable by reading other work as well. Be clear how it depends on theta (I think nothing is lost by feeding in the h_theta(x)). - The learning criterion based on predictive distributions on train/test splits seem to work better than ELBO-like criteria, for few-shot classification. But there are some worrying aspects. The marginal likelihood has an Occam's razor argument to prevent overfitting. Why would your criterion prevent overfitting? And it is quite worrying that the prior p(psi | theta) drops out of the method entirely. Can you comment more on that? Small: - p(psi_t | tilde{x}_t, D_t, theta) should be p(psi_t | D_t, theta). Please avoid a more general notation early on, if you do not do it later on. This is confusing ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "\u201c Which of the competitors ( if any ) use the same restricted model setup ( inference only on the top-layer weights ) ? \u201d To the best of our knowledge , almost all the competing methods adapt the entire network for new tasks . We have amended the paper to clarify this point ( see Section 5.2 ) . \u201c Do competitors without train/test split also get k_c + 15 points , or only k_c points ? \u201d To the best of our knowledge , all methods we compare to use train/test splits , with the exception of the VI methods referenced in Table 1 . The VI methods used the same number of observations at train time ( i.e. , the data available to all methods was identical ) . \u201c The main paper does not really state what the model or the likelihood is [ in the ShapeNet experiments ] . From F.4 in the Appendix , this model does not have the form of your classification models , but psi is input at the bottom of the network . Also , the final layer has sigmoid activation . What likelihood do you use ? \u201d The terseness of the ShapeNet model details was a result of space constraints . We have amended the paper to include additional explanatory details ( see Section 3 ) . You are correct in observing that psi plays a different role from the classification case , namely as an input to the image-generator . The likelihood we used is Gaussian , the sigmoid activation ensures that the mean is between 0 and 1 , reflecting the constraints on pixel-intensities . Your observation that using top-layer weights would allow us to perform exact inference is very insightful . We decided to use an architecture that passed the latent parameters underlying each shape instance through multiple non-linearities , but it would be very interesting to compare to the simpler baseline that you suggest . As this is a significant undertaking , we will leave it to future work , \u201c Real Bayesian inference would just see features h_theta ( x ) as inputs , not the x 's . Why not simply feed features in then ? \u2026 Be clear how it depends on theta ( I think nothing is lost by feeding in the h_theta ( x ) ) . \u201d Thank you for suggesting this cleaner way of presenting our work . We agree with your observations on the input to the inference network . We have amended Fig.2 accordingly , and have improved the descriptions in Section 3 . \u201c The marginal likelihood has an Occam 's razor argument to prevent overfitting . Why would your criterion prevent overfitting ? \u201d The mechanism preventing overfitting in our criterion is the meta train / test splits , which explicitly encourages the model to generalize from the training observations to the test data . Methods based on held-out sets , like cross validation , are known to favor models which are more complex than those favoured by Bayesian model comparison [ i , ii ] . However , as is empirically demonstrated in the experimental section , our proposed criterion consistently outperformed variational objectives . \u201c It is quite worrying that the prior p ( psi | theta ) drops out of the method entirely . Can you comment more on that ? \u201d This is a subtle point that we view as both a feature and a bug . It is a feature in the sense that a prior is learned implicitly through the sampling procedure ( as is shown for example in the simple Gaussian experiment -- see Section 5.1 ) . This can be compared to VI , for example , where the prior enters through a KL regularization term which often favours underfitting . It is a bug if , for example , the user has a priori knowledge about the parameters that they would like to leverage . In this case , it could be possible to use synthetic training data to incorporate such knowledge into the scheme . However , for the predictive purposes explored in this work , we did not find that the lack of prior posed an issue . [ i ] - C. E. Rasumessen and Z. Ghahramani . Occam \u2019 s razor . 2001 . [ ii ] - I. Murray and Z. Ghahramani . A note on the evidence and Bayesian Occam \u2019 s razor . 2005 ."}], "0": {"review_id": "HkxStoC5F7-0", "review_text": "This paper proposes both a general meta-learning framework with approximate probabilistic inference, and implements an instance of it for few-shot learning. First, they propose Meta-Learning Probabilistic inference for Prediction (ML-PIP) which trains the meta-learner to minimize the KL-divergence between the approximate predictive distribution generated from it and predictive distribution for each class. Then, they use this framework to implement Versatile Amortized Inference, which they call VERSA. VERSA replaces the optimization for test time with efficient posterior inference, by generating distribution over task-specific parameters in a single forward pass. The authors validate VERSA against amortized and non-amortized variational inference which it outperforms. VERSA is also highly versatile as it can be trained with varying number of classes and shots. Pros - The proposed general meta-learning framework that aims to learn the meta-learner that approximates the predictive distribution over multiple tasks is quite novel and makes sense. - VERSA obtains impressive performance on both benchmark datasets for few-shot learning and is versatile in terms of number of classes and shots. - The appendix section has in-depth analysis and additional experimental results which are quite helpful in understanding the paper. Cons - The main paper feels quite empty, especially the experimental validation parts with limited number of baselines. It would have been good if some of the experiments could be moved into the main paper. Some experimental results such as Figure 4 on versatility does not add much insight to the main story and could be moved to appendix. - It would have been good if there was some validation of the time-performance of the model as one motivation of meta-learning is rapid adaptation to a test-time task. In sum, since the proposed meta-learning probabilistic inference framework is novel and effective I vote for accepting the paper. However the structure and organization of the paper could be improved by moving some of the methodological details and experimental results in the appendix to the main paper. ", "rating": "7: Good paper, accept", "reply_text": "\u201c It would have been good if some of the experiments could be moved into the main paper . \u2026 the structure and organization of the paper could be improved by moving some of the methodological details and experimental results in the appendix to the main paper. \u201d We agree that a significant portion of interesting content has been relegated to the appendix in our submission . Much of this , of course , has to do with space constraints . However , we have addressed this in the revised version in line with your suggestions by ( i ) moving the appendix containing the toy-data experimentation to the main body of the paper ( see Section 5.1 ) , and ( ii ) moving some methodological details from the appendix in to the experiments section ( see Section 5 ) . \u201c It would have been good if there was some validation of the time-performance of the model as one motivation of meta-learning is rapid adaptation to a test-time task . \u201c We strongly agree that the issue of performance timing is of great interest , and it is useful and important to validate this experimentally . We were originally hesitant to add any timing results as code released with research papers is often optimized for correctness as opposed to speed . That said , we measured the test time performance of both MAML ( as implemented in the authors ' publicly available repository at https : //github.com/cbfinn/maml ) and Versa in 5-shot 5-way experiments on mini-ImageNet , using the same architectures for both . We found Versa to achieve 5x speed up compared to MAML , while achieving significantly better accuracy ( see Table 3 ) . We have amended the paper to include this experimental data ( see Section 5.2 for details ) . We believe this data demonstrates the performance gains achieved by relieving the need for test time optimization procedures ."}, "1": {"review_id": "HkxStoC5F7-1", "review_text": "This paper presents two different sections: 1. A generalized framework to describe a range of meta-learning algorithms. 2. A meta-learning algorithm that allows few shot inference over new tasks without the need for retraining. The important aspect of the algorithm is the context independence assumption between posteriors of different classes for learning weights. This reduces the number of parameters to amortize during meta-training. More importantly, it makes it independent of the number of classes in a task, and effectively doing meta-training across class inference instead of each task. The idea sounds great, but I am skeptical of the justification behind the independence assumption which, as per its justifications sounds contrived and only empirical. Overall, I feel the paper makes some progress in important aspects of meta-learning.", "rating": "6: Marginally above acceptance threshold", "reply_text": "\u201c The important aspect of the algorithm is the context independence assumption between posteriors of different classes for learning weights . \u2026 The idea sounds great , but I am skeptical of the justification behind the independence assumption which , as per its justifications sounds contrived and only empirical. \u201d We thank the reviewer for imploring us to think more carefully about this point . We share the concern that providing only an empirical justification for the context independent assumption is slightly troubling . We have therefore considered this more carefully , and have found that there is a principled justification of this design choice , which is best understood through the lens of density ratio estimation [ i , ii ] . Results from Density Ratio Estimation [ i , ii ] show that an optimal softmax classifier learns the ratio of the densities Softmax ( y=k | x ) = p ( x | y=k ) / Sum_j p ( x | y=j ) assuming equal a priori probability for each class . Our system follows his optimal form by setting : log p ( \\tilde { x } | y=c ) proportional h_theta ( \\tilde { x } ) ^T w_c where w_c ~ q_phi ( w | { x_n ; y_n=c } ) for each class in a given task . Here { ( x_n , y_n ) } are the few-shot training examples , and $ \\tilde { x } $ is the test example . This argument states that under ideal conditions ( i.e. , we can perfectly estimate p ( y=c | x ) ) , the context-independent assumption is correct , and further motivates our design . We have amended the paper to include this argument ( see Appendix B ) . We thank the reviewer for pointing to this important issue , and we hope that this alleviates some of their concerns . [ i ] - S. Mohamed . The Density Ratio Trick . The Spectator ( Blog ) . 2018 [ ii ] - M. Sugiyama , T. Suzuki , and T. Kanamori . Density ratio estimation in machine learning . 2012"}, "2": {"review_id": "HkxStoC5F7-2", "review_text": "Summary: This work tackles few-shot (or meta) learning from a probabilistic inference viewpoint. Compared to previous work, it uses a simpler setup, performing task-specific inference only for single-layer head models, and employs an objective based on predictive distributions on train/test splits for each task (rather than an approximation to log marginal likelihood). Inference is done amortized by a network, whose input is the task training split. The same network is used for parameters of each class (only feeding training points of that class), which allows an arbitrary number of classes per task. At test time, inference just requires forward passes through this network, attractive compared to non-amortized approaches which need optimization or gradients here. It provides a clean, decision-theoretic derivation, and clarifies relationships to previous work. The experimental results are encouraging: the method achieves a new best on 5-way, 5-shot miniImageNet, despite the simple setup. In general, explanations in the main text could be more complete (see questions). I'd recommend shortening Section 4, which is pretty obvious. - Quality: Several interesting differences to prior work. Well-done experiments - Clarity: Clean derivation, easy to understand. Some details could be spelled out better - Originality: Several important novelties (predictive criterion, simple model setup, amortized inference network). Closely related to \"neural processes\" work, but this happened roughly at the same time - Significance: The few-shot learning results are competitive, in particular given they use a simpler model setup than most previous work. I am not an expert on these kind of experiments, but I found the comparisons fair and rather extensive Interesting about this work: - Clean Bayesian decision-theoretic viewpoint. Key question is of course whether an inference network of this simple structure (no correlations, sum combination of datapoints, same network for each class) can deliver a good approximation to the true posterior. - Different to previous work, task-specific inference is done only on the weights of single-layer head models (logistic regression models, with shared features). Highly encouraging that this is sufficient for state-of-the-art few-shot classification performance. The authors could be more clear about this point. - Simple and efficient amortized inference model, which along with the neural network features, is learned on all data jointly - Optimization criterion is based on predictive distributions on train/test splits, not on the log marginal likelihood. Has some odd consequences (question below), but clearly works better for few-shot classification Experiments: - 5.1: Convincing results, in particular given the simplicity of the model setup and the inference network. But some important points are not explained: - Which of the competitors (if any) use the same restricted model setup (inference only on the top-layer weights)? Clearly, MAML does not, right? Please state this explicitly. - For Versa, you use k_c training and 15 test points per task update during training. Do competitors without train/test split also get k_c + 15 points, or only k_c points? The former would be fair, the latter not so much. - 5.2: This seems a challenging problem, and both your numbers and reconstructions look better than the competitor. I cannot say more, based on the very brief explanations provided here. The main paper does not really state what the model or the likelihood is. From F.4 in the Appendix, this model does not have the form of your classification models, but psi is input at the bottom of the network. Also, the final layer has sigmoid activation. What likelihood do you use? One observation: If you used the same \"inference on final layer weights\" setup here, and Gaussian likelihood, you could compute the posterior over psi in closed form, no amortization needed. Would this setup apply to your problem? Further questions: - Confused about the input to the inference network. Real Bayesian inference would just see features h_theta(x) as inputs, not the x's. Why not simply feed features in then? Please do improve the description of the inference network, this is a major novelty of this paper, and even the appendix is only understandable by reading other work as well. Be clear how it depends on theta (I think nothing is lost by feeding in the h_theta(x)). - The learning criterion based on predictive distributions on train/test splits seem to work better than ELBO-like criteria, for few-shot classification. But there are some worrying aspects. The marginal likelihood has an Occam's razor argument to prevent overfitting. Why would your criterion prevent overfitting? And it is quite worrying that the prior p(psi | theta) drops out of the method entirely. Can you comment more on that? Small: - p(psi_t | tilde{x}_t, D_t, theta) should be p(psi_t | D_t, theta). Please avoid a more general notation early on, if you do not do it later on. This is confusing ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "\u201c Which of the competitors ( if any ) use the same restricted model setup ( inference only on the top-layer weights ) ? \u201d To the best of our knowledge , almost all the competing methods adapt the entire network for new tasks . We have amended the paper to clarify this point ( see Section 5.2 ) . \u201c Do competitors without train/test split also get k_c + 15 points , or only k_c points ? \u201d To the best of our knowledge , all methods we compare to use train/test splits , with the exception of the VI methods referenced in Table 1 . The VI methods used the same number of observations at train time ( i.e. , the data available to all methods was identical ) . \u201c The main paper does not really state what the model or the likelihood is [ in the ShapeNet experiments ] . From F.4 in the Appendix , this model does not have the form of your classification models , but psi is input at the bottom of the network . Also , the final layer has sigmoid activation . What likelihood do you use ? \u201d The terseness of the ShapeNet model details was a result of space constraints . We have amended the paper to include additional explanatory details ( see Section 3 ) . You are correct in observing that psi plays a different role from the classification case , namely as an input to the image-generator . The likelihood we used is Gaussian , the sigmoid activation ensures that the mean is between 0 and 1 , reflecting the constraints on pixel-intensities . Your observation that using top-layer weights would allow us to perform exact inference is very insightful . We decided to use an architecture that passed the latent parameters underlying each shape instance through multiple non-linearities , but it would be very interesting to compare to the simpler baseline that you suggest . As this is a significant undertaking , we will leave it to future work , \u201c Real Bayesian inference would just see features h_theta ( x ) as inputs , not the x 's . Why not simply feed features in then ? \u2026 Be clear how it depends on theta ( I think nothing is lost by feeding in the h_theta ( x ) ) . \u201d Thank you for suggesting this cleaner way of presenting our work . We agree with your observations on the input to the inference network . We have amended Fig.2 accordingly , and have improved the descriptions in Section 3 . \u201c The marginal likelihood has an Occam 's razor argument to prevent overfitting . Why would your criterion prevent overfitting ? \u201d The mechanism preventing overfitting in our criterion is the meta train / test splits , which explicitly encourages the model to generalize from the training observations to the test data . Methods based on held-out sets , like cross validation , are known to favor models which are more complex than those favoured by Bayesian model comparison [ i , ii ] . However , as is empirically demonstrated in the experimental section , our proposed criterion consistently outperformed variational objectives . \u201c It is quite worrying that the prior p ( psi | theta ) drops out of the method entirely . Can you comment more on that ? \u201d This is a subtle point that we view as both a feature and a bug . It is a feature in the sense that a prior is learned implicitly through the sampling procedure ( as is shown for example in the simple Gaussian experiment -- see Section 5.1 ) . This can be compared to VI , for example , where the prior enters through a KL regularization term which often favours underfitting . It is a bug if , for example , the user has a priori knowledge about the parameters that they would like to leverage . In this case , it could be possible to use synthetic training data to incorporate such knowledge into the scheme . However , for the predictive purposes explored in this work , we did not find that the lack of prior posed an issue . [ i ] - C. E. Rasumessen and Z. Ghahramani . Occam \u2019 s razor . 2001 . [ ii ] - I. Murray and Z. Ghahramani . A note on the evidence and Bayesian Occam \u2019 s razor . 2005 ."}}