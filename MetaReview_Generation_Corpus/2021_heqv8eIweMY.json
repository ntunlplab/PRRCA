{"year": "2021", "forum": "heqv8eIweMY", "title": "Non-Local Graph Neural Networks", "decision": "Reject", "meta_review": "This paper is right at the borderline: the reviewers agree it is well written, proposing a simple but interesting idea. However, there was a feeling among the reviewers (especially reviewer 1) that the paper could be strengthened considerably with a better discussion/some theory on the sufficiency of the calibration vectors, as well as experiments on larger datasets. Doing one of these would have substantially strengthened the paper. Due to the remaining shortcomings, the recommendation is not to accept the paper in its present state.", "reviews": [{"review_id": "heqv8eIweMY-0", "review_text": "This paper targets on addressing the node embedding problem in disassortative graphs . A non-local aggregation framework is proposed , since local aggregation may be harmful for some disassortative graphs . To address the high computational cost in the recent Geom-GCN model that has an attention-like step to compute the Euclidean distance between every pair of nodes , an idea of attention-guided sorting is introduced . It learns an ordering of nodes , such that distant but informative nodes are put near each other . The sorting order depends on the attention scores computed with the local embedding vector of a node . Then Covn ( . ) function is applied on the sorted sequence of local node embeddings to obtain the non-local embedding . The final node embedding is then the concatenation of the local and non-local embedding , which is used for node classification . The presented simple approach is an interesting idea to \u201c push \u201d the distant but informative nodes together . However , it is unclear how the \u201c attention-guided sorting \u201d is aware of the \u201c distant \u201d nodes . The local node embedding vectors z can be obtained either by the node content , or by GNN . If z is from the node content only , the attention score a is calculated without consideration how nodes are close or distant on the graph . The whole approach works purely for node content classification . If z is from GNN , nodes close on the graph have similar z embedding vectors and thus will be sorted next to each other . Then , the sorting doesn \u2019 t take \u201c distant \u201d nodes close . Although the experimental results show the proposed approach performs better than several baselines , more and stronger GNN models are expected to be compared with , e.g. , GINs . Especially on Chameleon and Squirrel datasets , theses two \u201c disassortative \u201d graphs can be handled by GNN kinds of models . The node classification in other four \u201c disassortative \u201d graphs in fact can be treated as a standard class classification task by ignoring the graph structures , as MLP on node features is already good . Thanks for the clarifications from the authors . The discussion was very helpful .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the helpful comments . We address the concerns as follows . Q1 : The presented simple approach is an interesting idea to \u201c push \u201d the distant but informative nodes together . However , it is unclear how the \u201c attention-guided sorting \u201d is aware of the \u201c distant \u201d nodes . The local node embedding vectors z can be obtained either by the node content , or by GNN . If z is from the node content only , the attention score a is calculated without consideration how nodes are close or distant on the graph . The whole approach works purely for node content classification . If z is from GNN , nodes close on the graph have similar z embedding vectors and thus will be sorted next to each other . Then , the sorting doesn \u2019 t take \u201c distant \u201d nodes close . A1 : Intuitively , if $ z $ is from the node content only , nodes with similar embedding $ z $ will obtain similar attention scores . Then , these nodes can be further aggregated by our subsequent 1-D convolution . This aggregation makes the feature vectors of nodes in the same class to be similar with each other , thus easing the following classification . That \u2019 s why our NLMLP can outperform MLP , as show in Table 3 in our paper . If $ z $ is derived by local GNN , nodes close on graph would have similar $ z $ embedding . However , informative distant nodes might also have similar $ z $ embeddings since this is a disassortative graph . Thus , such long-range dependencies can be captured by the subsequent attention-guided sorting and 1-D convolution . This has been verified by the improvement shown in Table 4 . Empirically , the visualization of homophily in figure 1 demonstrates that our non-local GNN indeed perform desired non-local aggregation on disassortative graphs . Q2 : Although the experimental results show the proposed approach performs better than several baselines , more and stronger GNN models are expected to be compared with , e.g. , GINs . Especially on Chameleon and Squirrel datasets , theses two \u201c disassortative \u201d graphs can be handled by GNN kinds of models . The node classification in other four \u201c disassortative \u201d graphs in fact can be treated as a standard class classification task by ignoring the graph structures , as MLP on node features is already good . A2 : ( 1 ) We did n't include the GIN as baselines for the following reason . GIN is proved to be powerful for distinguishing different graph structures , and it shines on graph-level tasks . However , as shown empirically in previous works , such as [ 1 ] , GIN usually performs not as good as GCN on node classification tasks . To make it more convincing , we add the result of GIN on Chameleon and Squirrel for reference , as shown below . | |Chameleon| Squirrel | |GIN | 66.6+/-3.0| 50.9+/-3.4| |NLGCN| 70.1+/-2.9| 59.0+/-1.2| ( 2 ) As shown in our paper , MLP actually performs much better than local GNNs on several disassortative graphs . This shows local aggregation which adopted by most GNNs hurts the performance , and the community should explore more effective methods for disassortative graphs . Our non-local GNN is an early attempt for such goal and has been shown to be effective to some degree . Most of the existing graph learning methods are based on the assumption of high homophily . We argue that the information contained in the disassortative graphs is also valuable and deserves more investigation and attention from the community . Thank you again for your insightful and constructive review . We hope that we addressed you concerns . [ 1 ] Wang et el .. Demystifying graph neural network via graph filter assessment . 2019 ."}, {"review_id": "heqv8eIweMY-1", "review_text": "This paper proposes a way of speeding up non-local aggregation on graph convolutional neural networks based on sorting the nodes into an ordering , and performing a 1-D convolution on this resulting ordering . This algorithm has the advantage of being asymptotically faster than other non-local aggregation schemes , and the paper demonstrates that empirically it can do at least as well as some of the other methods . Strengths : + the proposed approach is simple , quite general , and rather different from other tools for graph neural nets that I 'm aware of . + the experimental evaluation methodology is sound , and comparisons with several previous works are made Weaknesses : - the approach is difficult to interpret : it 's difficult to convince someone working on GCNs why it would work . - on some of the data sets , the gains observed as inconclusive . The experiments also focused on small data sets : it 's unclear how such gains extend to more general settings . I work mostly on graph algorithms , and only know a little about neural networks . So I 'm evaluating this paper mostly as a practical graph algorithms . The effectiveness of such global sorting schemes based on a single score is very surprising , almost too surprising . On the other hand , my general impression is that graph algorithms is full of such surprises : many by now classical algorithms are arrived at by analyzing strange phenomenon that happen to work well . So I 'm quite willing to suspend disbelief about why something like this would work , as that 's a much more detailed process . From the discussions , it seems that there are quite a bit of concerns raised about the experimentation process . On the other hand , the responses , and presentations in the paper , are also quite convincing to me . So I believe this result is ready to appear in the conference , if anything for the further discussion/interest it will generate , and would still like to recommend acceptance of this paper .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your positive comments ."}, {"review_id": "heqv8eIweMY-2", "review_text": "Summary : The goal of the paper is to perform node classification for graphs . The authors propose a strategy to augment message passing graph neural networks with information from non-local nodes in the graph - with a focus on dis-assortative graphs . Dis-assortative graphs are graph datasets - where nodes with identical node labels are distant from each other in terms of edge connectivity . With node representation learnt from standard graph neural networks , etc. , the authors propose to use an attention guided sorting mechanism , to create a proxy graph , where nodes which may have identical node labels be connected to each other ( analogous to creating a k-nearest neighbor graph ) . Message passing is then employed on the proxy graph to learn final representations for the nodes . Since the authors employ a single vector , namely ' c ' ( which they call calibration vector ) , to capture the 'importance of information ' shared across different nodes - there is a speedup in comparison to strategies which employ a pairwise comparison between all nodes in the graph . Pros : 1.The idea to create a proxy graph to capture non local information is interesting 2 . The proposed technique can be augmented with almost any existing GNN My Concerns : 1 . ( Dis-assortative or i.i.d . ) : - The authors in Figure 1 - show that homophily of the created proxy graph is a value larger than that of the original graph . However , from table A.2 in the appendix - it is clear to see that MLP 's outperform GNN 's with or without the attention sorting in the dis-assortative graphs - and the performance of the MLP 's and proposed augmented NLMLP are well within one standard deviation from each other . This questions the need to employ a proxy graph construction on top of MLP 's for these datasets as it appears like the data can be treated as i.i.d ( and not relational ) . Moreover , these datasets ( used from Pei et al.2020 ) are extremely small to draw any significant conclusion . Also almost no gains are seen on the assortative datasets Citeseer , Cora , Pubmed ( Please add datasets from OGB ) - and their running times ( when augmented with a proxy graph - are the gains worthy of increased run times ? ) . 2 . ( Baselines ) : Since the authors propose a strategy to construct a proxy graph ( and the number of neighbors of each node in the proxy graph is the same ? ? ) - baselines such as creating graphs where nodes with identical labels are connected are also connected to each other / GNN on simple k-Nearest neighbors created using initial features ( While a simple k-NN might appear more expensive - but the computation here is a single time effort ) appears crucial . Also add a baseline , where adjacency structure of the graphs are iteratively updated during training such as - Learning discrete structures for graph neural networks ( Franceschi , et al.ICML 2019 ) 3 . ( Sufficiency , lack of details ) : The use of a single calibration vector may not be sufficient to sort the nodes - there are no guarantees in the paper to say when a single calibration vector would suffice . Also , the number of classes of nodes in each of the datasets used here are very small - and also does not trivially extend to multi-label classification of nodes . Also how do you also determine the number of neighbors in a proxy graph and do all nodes need to have the same number of neighbors in the proxy graph ? ? ? There are missing equations about how the calibration vector ' c ' is learnt ( what are the objective , etc ) and the effect of running time when there are a large number of neighbors considered in the proxy graph - without any equations its hard to argue against the case that the number of gradients to be computed would explode , when the number of neighbors are increased in the proxy graph ( especially when jointly learning the GNN and the proposed augmentation ) . Other minor concerns : If possible , please include the difference between assortative and non-assortative graphs in the introduction - it makes it easier for a reader . If details are added and the concerns are addressed , I will be happy to improve my score .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q3 : ( Sufficiency , lack of details ) : The use of a single calibration vector may not be sufficient to sort the nodes - there are no guarantees in the paper to say when a single calibration vector would suffice . Also , the number of classes of nodes in each of the datasets used here are very small - and also does not trivially extend to multi-label classification of nodes . Also how do you determine the number of neighbors in a proxy graph and do all nodes need to have the same number of neighbors in the proxy graph ? ? ? There are missing equations about how the calibration vector ' c ' is learnt ( what are the objective , etc ) and the effect of running time when there are a large number of neighbors considered in the proxy graph - without any equations its hard to argue against the case that the number of gradients to be computed would explode , when the number of neighbors are increased in the proxy graph ( especially when jointly learning the GNN and the proposed augmentation ) . A3 : ( 1 ) Great point ! We use a single calibration vector in our model just because one single calibration vector performs good and increasing the number of calibration vectors does not bring obvious gains on these available datasets . Yes , our attention-guided sorting step can also be augmented with multi-head versions , like the general attention mechanism . Hence , for the mentioned multi-label classification problem ( although this is out of the scope of our paper ) , we might use multi-head classifier with multi-head attention-guided sorting to make multi-label predictions . ( 2 ) the number of neighbors in a \u201c proxy graph \u201d is like a normal hyperparameter . We tune it based on the validation set , as other hyperparameters . Particularly , the hyperparameter is set to $ 3 $ or $ 5 $ . If we want to consider a large neighborhood in a \u201c proxy graph \u201d , we can stack more 1-D convolutional layers . This is identical to the general deep convolutional network , which is widely used and computationally affordable . Hence , we don \u2019 t need to worry about the inexistent exploding computation issue . ( 3 ) The calibration vector ' c ' is learnt by receiving the gradient from Equation $ 6 $ , as described in the paragraph under that equation . Q4 : Other minor concerns : If possible , please include the difference between assortative and non-assortative graphs in the introduction - it makes it easier for a reader . A4 : Thank you for this helpful suggestion . We will add this clarification in the introducing in our revised version . Thank you again for these insightful and thorough reviews . Hope we addressed your concerns ."}, {"review_id": "heqv8eIweMY-3", "review_text": "This paper points out an interesting and important issue of GNNs , i.e. , local aggregation is harmful for some disassortative graphs . It further proposes non-local GNNs by first sorting the nodes followed by aggregation . The paper is well written and easy to follow . + Positives 1 . The paper studies an important problem . The proposed Non-local GNNs by first sorting the nodes followed by aggregation is interesting and makes sense . 2.The paper is well written and easy to follow . 3.Experiments well support the claim of the paper . The results demonstrated the effectiveness of the proposed method for disassortative graphs for node classification . In addition , the authors show the running time to demonstrate its efficiency and analyze the sorted nodes to demonstrate that the proposed method can learn non-local graphs . -Negative 1 . It seems that for some disassortative graphs such as Actor , Cornell , Texas and Wisconsin , using the node attributes to build the non-local graph is much effective than using the attributed graph . The authors may also need to compare a baseline that simply use MLP to learn node embedding , then construct the graph by calculating pairwise node similarity , followed by GNN for node classification . This can be treated as a variants of the proposed NLMLP to show that sorting the nodes is more efficient and more effective .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the insightful review and positive words . We address the concerns as follows . Q1 : It seems that for some disassortative graphs such as Actor , Cornell , Texas and Wisconsin , using the node attributes to build the non-local graph is much effective than using the attributed graph . The authors may also need to compare a baseline that simply use MLP to learn node embedding , then construct the graph by calculating pairwise node similarity , followed by GNN for node classification . This can be treated as a variant of the proposed NLMLP to show that sorting the nodes is more efficient and more effective . A1 : Thank you for providing this great suggestion . We conduct a comparison between our NLMLP with MLP+KNN+GCN , as described by the reviewer . The results are shown in the following table . |Method |Actor|Cornell|Texas|Wisconsin| |MLP |35.1 |81.6 |81.3 |84.9 | |MLP+KNN+GCN|36.8 |81.2 |81.0 |86.0 | |NLMLP |37.9 |84.9 |85.4 |87.3 | Overall , MLP+KNN+GCN performs better ( sometimes not obvious ) than na\u00efve MLP . Our NLMLP outperforms baselines consistently . Also , MLP+KNN+GCN is much more computationally expensive when the number of nodes is large since the time complexity of computing pairwise similarity is $ O ( n^2 ) $ . For example , on the Actor dataset ( 7600 nodes ) , the training times per epoch ( averaged over $ 500 $ epochs ) of MLP+KNN+GCN and NLMLP are $ 464 $ ms and $ 8 $ ms , respectively . Thank you again for your constructive comments ."}], "0": {"review_id": "heqv8eIweMY-0", "review_text": "This paper targets on addressing the node embedding problem in disassortative graphs . A non-local aggregation framework is proposed , since local aggregation may be harmful for some disassortative graphs . To address the high computational cost in the recent Geom-GCN model that has an attention-like step to compute the Euclidean distance between every pair of nodes , an idea of attention-guided sorting is introduced . It learns an ordering of nodes , such that distant but informative nodes are put near each other . The sorting order depends on the attention scores computed with the local embedding vector of a node . Then Covn ( . ) function is applied on the sorted sequence of local node embeddings to obtain the non-local embedding . The final node embedding is then the concatenation of the local and non-local embedding , which is used for node classification . The presented simple approach is an interesting idea to \u201c push \u201d the distant but informative nodes together . However , it is unclear how the \u201c attention-guided sorting \u201d is aware of the \u201c distant \u201d nodes . The local node embedding vectors z can be obtained either by the node content , or by GNN . If z is from the node content only , the attention score a is calculated without consideration how nodes are close or distant on the graph . The whole approach works purely for node content classification . If z is from GNN , nodes close on the graph have similar z embedding vectors and thus will be sorted next to each other . Then , the sorting doesn \u2019 t take \u201c distant \u201d nodes close . Although the experimental results show the proposed approach performs better than several baselines , more and stronger GNN models are expected to be compared with , e.g. , GINs . Especially on Chameleon and Squirrel datasets , theses two \u201c disassortative \u201d graphs can be handled by GNN kinds of models . The node classification in other four \u201c disassortative \u201d graphs in fact can be treated as a standard class classification task by ignoring the graph structures , as MLP on node features is already good . Thanks for the clarifications from the authors . The discussion was very helpful .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the helpful comments . We address the concerns as follows . Q1 : The presented simple approach is an interesting idea to \u201c push \u201d the distant but informative nodes together . However , it is unclear how the \u201c attention-guided sorting \u201d is aware of the \u201c distant \u201d nodes . The local node embedding vectors z can be obtained either by the node content , or by GNN . If z is from the node content only , the attention score a is calculated without consideration how nodes are close or distant on the graph . The whole approach works purely for node content classification . If z is from GNN , nodes close on the graph have similar z embedding vectors and thus will be sorted next to each other . Then , the sorting doesn \u2019 t take \u201c distant \u201d nodes close . A1 : Intuitively , if $ z $ is from the node content only , nodes with similar embedding $ z $ will obtain similar attention scores . Then , these nodes can be further aggregated by our subsequent 1-D convolution . This aggregation makes the feature vectors of nodes in the same class to be similar with each other , thus easing the following classification . That \u2019 s why our NLMLP can outperform MLP , as show in Table 3 in our paper . If $ z $ is derived by local GNN , nodes close on graph would have similar $ z $ embedding . However , informative distant nodes might also have similar $ z $ embeddings since this is a disassortative graph . Thus , such long-range dependencies can be captured by the subsequent attention-guided sorting and 1-D convolution . This has been verified by the improvement shown in Table 4 . Empirically , the visualization of homophily in figure 1 demonstrates that our non-local GNN indeed perform desired non-local aggregation on disassortative graphs . Q2 : Although the experimental results show the proposed approach performs better than several baselines , more and stronger GNN models are expected to be compared with , e.g. , GINs . Especially on Chameleon and Squirrel datasets , theses two \u201c disassortative \u201d graphs can be handled by GNN kinds of models . The node classification in other four \u201c disassortative \u201d graphs in fact can be treated as a standard class classification task by ignoring the graph structures , as MLP on node features is already good . A2 : ( 1 ) We did n't include the GIN as baselines for the following reason . GIN is proved to be powerful for distinguishing different graph structures , and it shines on graph-level tasks . However , as shown empirically in previous works , such as [ 1 ] , GIN usually performs not as good as GCN on node classification tasks . To make it more convincing , we add the result of GIN on Chameleon and Squirrel for reference , as shown below . | |Chameleon| Squirrel | |GIN | 66.6+/-3.0| 50.9+/-3.4| |NLGCN| 70.1+/-2.9| 59.0+/-1.2| ( 2 ) As shown in our paper , MLP actually performs much better than local GNNs on several disassortative graphs . This shows local aggregation which adopted by most GNNs hurts the performance , and the community should explore more effective methods for disassortative graphs . Our non-local GNN is an early attempt for such goal and has been shown to be effective to some degree . Most of the existing graph learning methods are based on the assumption of high homophily . We argue that the information contained in the disassortative graphs is also valuable and deserves more investigation and attention from the community . Thank you again for your insightful and constructive review . We hope that we addressed you concerns . [ 1 ] Wang et el .. Demystifying graph neural network via graph filter assessment . 2019 ."}, "1": {"review_id": "heqv8eIweMY-1", "review_text": "This paper proposes a way of speeding up non-local aggregation on graph convolutional neural networks based on sorting the nodes into an ordering , and performing a 1-D convolution on this resulting ordering . This algorithm has the advantage of being asymptotically faster than other non-local aggregation schemes , and the paper demonstrates that empirically it can do at least as well as some of the other methods . Strengths : + the proposed approach is simple , quite general , and rather different from other tools for graph neural nets that I 'm aware of . + the experimental evaluation methodology is sound , and comparisons with several previous works are made Weaknesses : - the approach is difficult to interpret : it 's difficult to convince someone working on GCNs why it would work . - on some of the data sets , the gains observed as inconclusive . The experiments also focused on small data sets : it 's unclear how such gains extend to more general settings . I work mostly on graph algorithms , and only know a little about neural networks . So I 'm evaluating this paper mostly as a practical graph algorithms . The effectiveness of such global sorting schemes based on a single score is very surprising , almost too surprising . On the other hand , my general impression is that graph algorithms is full of such surprises : many by now classical algorithms are arrived at by analyzing strange phenomenon that happen to work well . So I 'm quite willing to suspend disbelief about why something like this would work , as that 's a much more detailed process . From the discussions , it seems that there are quite a bit of concerns raised about the experimentation process . On the other hand , the responses , and presentations in the paper , are also quite convincing to me . So I believe this result is ready to appear in the conference , if anything for the further discussion/interest it will generate , and would still like to recommend acceptance of this paper .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your positive comments ."}, "2": {"review_id": "heqv8eIweMY-2", "review_text": "Summary : The goal of the paper is to perform node classification for graphs . The authors propose a strategy to augment message passing graph neural networks with information from non-local nodes in the graph - with a focus on dis-assortative graphs . Dis-assortative graphs are graph datasets - where nodes with identical node labels are distant from each other in terms of edge connectivity . With node representation learnt from standard graph neural networks , etc. , the authors propose to use an attention guided sorting mechanism , to create a proxy graph , where nodes which may have identical node labels be connected to each other ( analogous to creating a k-nearest neighbor graph ) . Message passing is then employed on the proxy graph to learn final representations for the nodes . Since the authors employ a single vector , namely ' c ' ( which they call calibration vector ) , to capture the 'importance of information ' shared across different nodes - there is a speedup in comparison to strategies which employ a pairwise comparison between all nodes in the graph . Pros : 1.The idea to create a proxy graph to capture non local information is interesting 2 . The proposed technique can be augmented with almost any existing GNN My Concerns : 1 . ( Dis-assortative or i.i.d . ) : - The authors in Figure 1 - show that homophily of the created proxy graph is a value larger than that of the original graph . However , from table A.2 in the appendix - it is clear to see that MLP 's outperform GNN 's with or without the attention sorting in the dis-assortative graphs - and the performance of the MLP 's and proposed augmented NLMLP are well within one standard deviation from each other . This questions the need to employ a proxy graph construction on top of MLP 's for these datasets as it appears like the data can be treated as i.i.d ( and not relational ) . Moreover , these datasets ( used from Pei et al.2020 ) are extremely small to draw any significant conclusion . Also almost no gains are seen on the assortative datasets Citeseer , Cora , Pubmed ( Please add datasets from OGB ) - and their running times ( when augmented with a proxy graph - are the gains worthy of increased run times ? ) . 2 . ( Baselines ) : Since the authors propose a strategy to construct a proxy graph ( and the number of neighbors of each node in the proxy graph is the same ? ? ) - baselines such as creating graphs where nodes with identical labels are connected are also connected to each other / GNN on simple k-Nearest neighbors created using initial features ( While a simple k-NN might appear more expensive - but the computation here is a single time effort ) appears crucial . Also add a baseline , where adjacency structure of the graphs are iteratively updated during training such as - Learning discrete structures for graph neural networks ( Franceschi , et al.ICML 2019 ) 3 . ( Sufficiency , lack of details ) : The use of a single calibration vector may not be sufficient to sort the nodes - there are no guarantees in the paper to say when a single calibration vector would suffice . Also , the number of classes of nodes in each of the datasets used here are very small - and also does not trivially extend to multi-label classification of nodes . Also how do you also determine the number of neighbors in a proxy graph and do all nodes need to have the same number of neighbors in the proxy graph ? ? ? There are missing equations about how the calibration vector ' c ' is learnt ( what are the objective , etc ) and the effect of running time when there are a large number of neighbors considered in the proxy graph - without any equations its hard to argue against the case that the number of gradients to be computed would explode , when the number of neighbors are increased in the proxy graph ( especially when jointly learning the GNN and the proposed augmentation ) . Other minor concerns : If possible , please include the difference between assortative and non-assortative graphs in the introduction - it makes it easier for a reader . If details are added and the concerns are addressed , I will be happy to improve my score .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q3 : ( Sufficiency , lack of details ) : The use of a single calibration vector may not be sufficient to sort the nodes - there are no guarantees in the paper to say when a single calibration vector would suffice . Also , the number of classes of nodes in each of the datasets used here are very small - and also does not trivially extend to multi-label classification of nodes . Also how do you determine the number of neighbors in a proxy graph and do all nodes need to have the same number of neighbors in the proxy graph ? ? ? There are missing equations about how the calibration vector ' c ' is learnt ( what are the objective , etc ) and the effect of running time when there are a large number of neighbors considered in the proxy graph - without any equations its hard to argue against the case that the number of gradients to be computed would explode , when the number of neighbors are increased in the proxy graph ( especially when jointly learning the GNN and the proposed augmentation ) . A3 : ( 1 ) Great point ! We use a single calibration vector in our model just because one single calibration vector performs good and increasing the number of calibration vectors does not bring obvious gains on these available datasets . Yes , our attention-guided sorting step can also be augmented with multi-head versions , like the general attention mechanism . Hence , for the mentioned multi-label classification problem ( although this is out of the scope of our paper ) , we might use multi-head classifier with multi-head attention-guided sorting to make multi-label predictions . ( 2 ) the number of neighbors in a \u201c proxy graph \u201d is like a normal hyperparameter . We tune it based on the validation set , as other hyperparameters . Particularly , the hyperparameter is set to $ 3 $ or $ 5 $ . If we want to consider a large neighborhood in a \u201c proxy graph \u201d , we can stack more 1-D convolutional layers . This is identical to the general deep convolutional network , which is widely used and computationally affordable . Hence , we don \u2019 t need to worry about the inexistent exploding computation issue . ( 3 ) The calibration vector ' c ' is learnt by receiving the gradient from Equation $ 6 $ , as described in the paragraph under that equation . Q4 : Other minor concerns : If possible , please include the difference between assortative and non-assortative graphs in the introduction - it makes it easier for a reader . A4 : Thank you for this helpful suggestion . We will add this clarification in the introducing in our revised version . Thank you again for these insightful and thorough reviews . Hope we addressed your concerns ."}, "3": {"review_id": "heqv8eIweMY-3", "review_text": "This paper points out an interesting and important issue of GNNs , i.e. , local aggregation is harmful for some disassortative graphs . It further proposes non-local GNNs by first sorting the nodes followed by aggregation . The paper is well written and easy to follow . + Positives 1 . The paper studies an important problem . The proposed Non-local GNNs by first sorting the nodes followed by aggregation is interesting and makes sense . 2.The paper is well written and easy to follow . 3.Experiments well support the claim of the paper . The results demonstrated the effectiveness of the proposed method for disassortative graphs for node classification . In addition , the authors show the running time to demonstrate its efficiency and analyze the sorted nodes to demonstrate that the proposed method can learn non-local graphs . -Negative 1 . It seems that for some disassortative graphs such as Actor , Cornell , Texas and Wisconsin , using the node attributes to build the non-local graph is much effective than using the attributed graph . The authors may also need to compare a baseline that simply use MLP to learn node embedding , then construct the graph by calculating pairwise node similarity , followed by GNN for node classification . This can be treated as a variants of the proposed NLMLP to show that sorting the nodes is more efficient and more effective .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the insightful review and positive words . We address the concerns as follows . Q1 : It seems that for some disassortative graphs such as Actor , Cornell , Texas and Wisconsin , using the node attributes to build the non-local graph is much effective than using the attributed graph . The authors may also need to compare a baseline that simply use MLP to learn node embedding , then construct the graph by calculating pairwise node similarity , followed by GNN for node classification . This can be treated as a variant of the proposed NLMLP to show that sorting the nodes is more efficient and more effective . A1 : Thank you for providing this great suggestion . We conduct a comparison between our NLMLP with MLP+KNN+GCN , as described by the reviewer . The results are shown in the following table . |Method |Actor|Cornell|Texas|Wisconsin| |MLP |35.1 |81.6 |81.3 |84.9 | |MLP+KNN+GCN|36.8 |81.2 |81.0 |86.0 | |NLMLP |37.9 |84.9 |85.4 |87.3 | Overall , MLP+KNN+GCN performs better ( sometimes not obvious ) than na\u00efve MLP . Our NLMLP outperforms baselines consistently . Also , MLP+KNN+GCN is much more computationally expensive when the number of nodes is large since the time complexity of computing pairwise similarity is $ O ( n^2 ) $ . For example , on the Actor dataset ( 7600 nodes ) , the training times per epoch ( averaged over $ 500 $ epochs ) of MLP+KNN+GCN and NLMLP are $ 464 $ ms and $ 8 $ ms , respectively . Thank you again for your constructive comments ."}}