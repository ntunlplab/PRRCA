{"year": "2019", "forum": "SyVuRiC5K7", "title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "decision": "Accept (Poster)", "meta_review": "As far as I know, this is the first paper to combine transductive learning with few-shot classification. The proposed algorithm, TPN, combines label propagation with episodic training, as well as learning an adaptive kernel bandwidth in order to determine the label propagation graph. The reviewers liked the idea, however there were concerns of novelty and clarity. I think the contributions of the paper and the strong empirical results are sufficient to merit acceptance, however the paper has not undergone a revision since September. It is therefore recommended that the authors improve the clarity based on the reviewer feedback. In particular, clarifying the details around learning \\sigma_i and graph construction. It would also be useful to include the discussion of timing complexity in the final draft.", "reviews": [{"review_id": "SyVuRiC5K7-0", "review_text": "This paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner. Semi-supervised few-shot learning is important considering the limitation of the very few labeled instances. This is an interesting work. The merits of this paper lie in the following aspects: (1) It is the first to learn label propagation for transductive few-shot learning. (2) The proposed approach produced effective empirical results. The drawbacks of the work include the following: (1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective. (2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation. Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature. For example, on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work. This is a major concern. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers . Here we respond to your specific comments . `` ( 1 ) There is not much technical contribution . It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning . Considering the optimization problem involved in the learning process , it is hard to judge whether the effect of such a procedure from the optimization perspective . '' > > > As mentioned in the main response , the proposed TPN is not a mere combination of CNN representation learning and label propagation . The original label propagation constructs a fixed graph ( Eq ( 1 ) ) to explore the correlation between examples . While in our work , we adaptively construct the graph structure for each episode ( training task ) with a learnable graph construction module ( Figure 4 , Appendix A ) . This leads to better generalization ability for test tasks . In Table 1 and Table 2 , the proposed TPN achieved much higher accuracy than the mere combination model ( referred to as `` Label Propagation '' ) . `` ( 2 ) Empirically , it seems TPN achieved very small improvements over the very baseline label propagation . Moreover , the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature . For example , on miniImageNet , TADAM ( Oreshkin et al , 2018 ) reported 58.5 ( 1-shot ) and 76.7 ( 5-shot ) , which are way better than the results reported in this work . This is a major concern . '' > > > At first , we want to clarify the few-shot network architecture setting . Currently , there are two common network architectures : 4-layer ConvNets ( e.g. , [ 1 ] [ 2 ] [ 3 ] ) and 12-layer ResNet ( e.g. , [ 4 ] [ 5 ] [ 6 ] [ 7 ] ) . Our method belongs to the first one , which contains much fewer layers than the ResNet setting . Thus , it is more reasonable to compare TADAM with ResNet version of our method . To better relieve the reviewer 's concern , we implemented our algorithm with ResNet architecture on miniImagenet dataset and show the results as follow : Method 1-shot 5-shot SNAIL [ 4 ] 55.71 68.88 adaResNet [ 5 ] 56.88 71.94 Discriminative k-shot [ 6 ] 56.30 73.90 TADAM [ 7 ] 58.50 76.70 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ours 59.46 75.65 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- It can be seen that we beat TADAM for 1-shot setting . For 5-shot , we outperform all other recent high-performance methods except for TADAM . > > > We want to clarify that `` Label Propagation '' in Table 1 and Table 2 is a strong baseline . It combines label propagation method [ 8 ] with episodic meta-learning . The usage of transductive inference makes this baseline outperform most published state-of-the-art methods . Moreover , the performance of TPN over label propagation is not very small . For example , in miniImagenet , TPN outperforms label propagation with 1.44 % and 1.25 % for 1-shot and 5-shot respectively , but this advantage grows to 3.20 % and 1.68 % with `` Higher Shot '' training . The improvements are even larger for tieredImagenet with 4.68 % and 2.87 % . We believe in few-shot learning , this is a large improvement . [ 1 ] Finn , Chelsea , Pieter Abbeel , and Sergey Levine . `` Model-agnostic meta-learning for fast adaptation of deep networks . '' ICML.2017 . [ 2 ] Snell , Jake , Kevin Swersky , and Richard Zemel . `` Prototypical networks for few-shot learning . '' NIPS.2017 . [ 3 ] Yang , Flood Sung Yongxin et al . `` Learning to compare : Relation network for few-shot learning . '' CVPR.2018 . [ 4 ] Mishra , Nikhil et al . `` A simple neural attentive meta-learner . '' ICLR.2018 . [ 5 ] Munkhdalai , Tsendsuren et al . `` Rapid adaptation with conditionally shifted neurons . '' ICML.2018 . [ 6 ] Bauer , Matthias et al . `` Discriminative k-shot learning using probabilistic models . '' arXiv.2017 . [ 7 ] Oreshkin , B.N. , Lacoste , A. and Rodriguez , P. , 2018 . `` TADAM : Task dependent adaptive metric for improved few-shot learning . '' NIPS.2018 . [ 8 ] Zhou , Denny , et al . `` Learning with local and global consistency . '' NIPS.2004 ."}, {"review_id": "SyVuRiC5K7-1", "review_text": "Summary This paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner. The proposed approaches are evaluated on two few-shot datasets and achieves the state-of-the-art results. Pros. -This paper is well-motivated. Studying label propagation in the meta-learning setting is interesting and novel. Intuitively, transductive label propagation should improve supervised learning when the number of labeled instances is low. -The empirical results show improvement over the baselines, which are expected. Cons. -Some technical details are missing. In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing. Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations? -Does episode training help label propagation? How about the results of label propagation without the episode training? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers . Here we respond to your specific comments . `` Some technical details are missing . In Section 3.2.2 , the authors only explain how they learn example-based \\sigma , but details on how to make graph construction end-to-end trainable are missing . Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation . Can you give more explanations ? '' > > > Thanks for pointing out the details . We want to clarify the few-shot setting . We follow the widely-used episodic paradigm proposed by Matching Networks [ 1 ] . In each episode ( training batch ) , our algorithm solves a small classification problem which contains N classes each having K support and Q query examples ( e.g. , N=5 , K=1 , Q=15 , totally 80 examples ) . The weight matrix is constructed on the support and query examples in each episode rather than the whole dataset . This is very fast and efficient . In deep neural networks , there is a common trick in computing the gradient of operations non-differentiable at some points , but differentiable elsewhere , such as Max-Pooling ( top-1 ) and top-k . In forward computation pass , the index position of the max ( or top-k ) values are stored . While in the back propagation pass , the gradient is computed only with respect to these saved positions . This trick is implemented in modern deep learning frameworks such as tensorflow and pytorch . In our paper , we use the tensorflow function tf.nn.top_k ( ) to compute k-nearest neighbor operation . `` Does episode training help label propagation ? How about the results of label propagation without the episode training ? `` > > > In our paper , the length scale parameter \\sigma is trained in an example-wise and episodic-wise way , as described in section 3.2.2 and Figure 4 of Appendix A . In order to investigate the benefit of episodic training , we combine the heuristic-based label propagation methods [ 2 ] with meta-learning to serve as a transductive baseline . Please refer to Table 1 and Table 2 line `` Label Propagation '' . It can be seen that TPN outperforms naive label propagation with a large margin , thus verifying the effectiveness of episode training . [ 1 ] Vinyals , Oriol et al . `` Matching networks for one shot learning . '' NIPS.2016 . [ 2 ] Zhou , Denny et al . `` Learning with local and global consistency . '' NIPS.2004 ."}, {"review_id": "SyVuRiC5K7-2", "review_text": "The paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples. There is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly. However, the paper does cover a setup that I am not aware that was studied before. The paper is written clearly, and the experiments seem solid. Comments: -- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. -- It is not clear how the per-example scalar sigma-i is learned. (for Eq 2) -- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization ", "rating": "7: Good paper, accept", "reply_text": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers . Here we respond to your specific comments . `` What can be said about how computationally demanding the procedure is ? running label propagation within meta learning might be too costly. `` > > > In few-shot learning , episodic paradigm proposed by Matching Networks [ 1 ] is widely adopted by current researchers ( we follow the same setting to make a fair comparison ) . In each episode , a small subset of N-way K-shot Q-query examples is sampled from the training set . Typically , for 1-shot experiments , N=5 , K=1 , Q=15 and for 5-shot experiments , N=5 , K=5 , Q=15 . Thus , the number of training examples are Nx ( K+Q ) ( 80 for 1-shot and 100 for 5-shot ) . Constructing label propagation matrix W involves both support and query examples ( 80 or 100 ) . So the dimension of W is either 80x80 or 100x100 . Running label propagation on such small matrix is quite efficient . `` It is not clear how the per-example scalar sigma-i is learned . ( for Eq 2 ) '' > > > In Figure 4 of appendix A , we describe the detailed structure of the graph construction module . After we get the per-example feature representation f_ { \\varphi } ( x_i ) for x_i , we feed it into the graph construction module g_ { \\phi } . The output of this module is a one-dimensional scalar . f and g are learned in an end-to-end way in our approach . `` solving Eq 3 by matrix inversion does not scale . Would be best to also show results using iterative optimization `` > > > We want to answer this question from two aspects . On one hand , few-shot learning assumes that training examples in each class are quite small ( only 1 or 5 ) . In this situation , Eq ( 3 ) and the closed-form version can be efficiently solved , since the dimension of S is only 80x80 or 100x100 . On the other hand , there is plenty of prior work on the scalability and efficiency of label propagation , such as [ 2 ] , [ 3 ] , [ 4 ] , which can extend our work to large-scale data . On miniImagenet , we performed iterative optimization and got 53.05/68.75 for 1-shot/5-shot experiments with only 10 steps . This is slightly worse than closed-form version ( 53.75/69.43 ) , because of the inaccurate computation and unstable gradients caused by multiple step iterations . [ 1 ] Vinyals , Oriol , et al . `` Matching networks for one shot learning . '' NIPS.2016 . [ 2 ] Liang , De-Ming , and Yu-Feng Li . `` Lightweight Label Propagation for Large-Scale Network Data . '' IJCAI.2018 . [ 3 ] Fujiwara , Yasuhiro , and Go Irie . `` Efficient label propagation . '' ICML.2014 . [ 4 ] Weston , Jason . `` Large-Scale Semi-Supervised Learning . ''"}], "0": {"review_id": "SyVuRiC5K7-0", "review_text": "This paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner. Semi-supervised few-shot learning is important considering the limitation of the very few labeled instances. This is an interesting work. The merits of this paper lie in the following aspects: (1) It is the first to learn label propagation for transductive few-shot learning. (2) The proposed approach produced effective empirical results. The drawbacks of the work include the following: (1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective. (2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation. Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature. For example, on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work. This is a major concern. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers . Here we respond to your specific comments . `` ( 1 ) There is not much technical contribution . It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning . Considering the optimization problem involved in the learning process , it is hard to judge whether the effect of such a procedure from the optimization perspective . '' > > > As mentioned in the main response , the proposed TPN is not a mere combination of CNN representation learning and label propagation . The original label propagation constructs a fixed graph ( Eq ( 1 ) ) to explore the correlation between examples . While in our work , we adaptively construct the graph structure for each episode ( training task ) with a learnable graph construction module ( Figure 4 , Appendix A ) . This leads to better generalization ability for test tasks . In Table 1 and Table 2 , the proposed TPN achieved much higher accuracy than the mere combination model ( referred to as `` Label Propagation '' ) . `` ( 2 ) Empirically , it seems TPN achieved very small improvements over the very baseline label propagation . Moreover , the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature . For example , on miniImageNet , TADAM ( Oreshkin et al , 2018 ) reported 58.5 ( 1-shot ) and 76.7 ( 5-shot ) , which are way better than the results reported in this work . This is a major concern . '' > > > At first , we want to clarify the few-shot network architecture setting . Currently , there are two common network architectures : 4-layer ConvNets ( e.g. , [ 1 ] [ 2 ] [ 3 ] ) and 12-layer ResNet ( e.g. , [ 4 ] [ 5 ] [ 6 ] [ 7 ] ) . Our method belongs to the first one , which contains much fewer layers than the ResNet setting . Thus , it is more reasonable to compare TADAM with ResNet version of our method . To better relieve the reviewer 's concern , we implemented our algorithm with ResNet architecture on miniImagenet dataset and show the results as follow : Method 1-shot 5-shot SNAIL [ 4 ] 55.71 68.88 adaResNet [ 5 ] 56.88 71.94 Discriminative k-shot [ 6 ] 56.30 73.90 TADAM [ 7 ] 58.50 76.70 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Ours 59.46 75.65 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- It can be seen that we beat TADAM for 1-shot setting . For 5-shot , we outperform all other recent high-performance methods except for TADAM . > > > We want to clarify that `` Label Propagation '' in Table 1 and Table 2 is a strong baseline . It combines label propagation method [ 8 ] with episodic meta-learning . The usage of transductive inference makes this baseline outperform most published state-of-the-art methods . Moreover , the performance of TPN over label propagation is not very small . For example , in miniImagenet , TPN outperforms label propagation with 1.44 % and 1.25 % for 1-shot and 5-shot respectively , but this advantage grows to 3.20 % and 1.68 % with `` Higher Shot '' training . The improvements are even larger for tieredImagenet with 4.68 % and 2.87 % . We believe in few-shot learning , this is a large improvement . [ 1 ] Finn , Chelsea , Pieter Abbeel , and Sergey Levine . `` Model-agnostic meta-learning for fast adaptation of deep networks . '' ICML.2017 . [ 2 ] Snell , Jake , Kevin Swersky , and Richard Zemel . `` Prototypical networks for few-shot learning . '' NIPS.2017 . [ 3 ] Yang , Flood Sung Yongxin et al . `` Learning to compare : Relation network for few-shot learning . '' CVPR.2018 . [ 4 ] Mishra , Nikhil et al . `` A simple neural attentive meta-learner . '' ICLR.2018 . [ 5 ] Munkhdalai , Tsendsuren et al . `` Rapid adaptation with conditionally shifted neurons . '' ICML.2018 . [ 6 ] Bauer , Matthias et al . `` Discriminative k-shot learning using probabilistic models . '' arXiv.2017 . [ 7 ] Oreshkin , B.N. , Lacoste , A. and Rodriguez , P. , 2018 . `` TADAM : Task dependent adaptive metric for improved few-shot learning . '' NIPS.2018 . [ 8 ] Zhou , Denny , et al . `` Learning with local and global consistency . '' NIPS.2004 ."}, "1": {"review_id": "SyVuRiC5K7-1", "review_text": "Summary This paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner. The proposed approaches are evaluated on two few-shot datasets and achieves the state-of-the-art results. Pros. -This paper is well-motivated. Studying label propagation in the meta-learning setting is interesting and novel. Intuitively, transductive label propagation should improve supervised learning when the number of labeled instances is low. -The empirical results show improvement over the baselines, which are expected. Cons. -Some technical details are missing. In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing. Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations? -Does episode training help label propagation? How about the results of label propagation without the episode training? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers . Here we respond to your specific comments . `` Some technical details are missing . In Section 3.2.2 , the authors only explain how they learn example-based \\sigma , but details on how to make graph construction end-to-end trainable are missing . Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation . Can you give more explanations ? '' > > > Thanks for pointing out the details . We want to clarify the few-shot setting . We follow the widely-used episodic paradigm proposed by Matching Networks [ 1 ] . In each episode ( training batch ) , our algorithm solves a small classification problem which contains N classes each having K support and Q query examples ( e.g. , N=5 , K=1 , Q=15 , totally 80 examples ) . The weight matrix is constructed on the support and query examples in each episode rather than the whole dataset . This is very fast and efficient . In deep neural networks , there is a common trick in computing the gradient of operations non-differentiable at some points , but differentiable elsewhere , such as Max-Pooling ( top-1 ) and top-k . In forward computation pass , the index position of the max ( or top-k ) values are stored . While in the back propagation pass , the gradient is computed only with respect to these saved positions . This trick is implemented in modern deep learning frameworks such as tensorflow and pytorch . In our paper , we use the tensorflow function tf.nn.top_k ( ) to compute k-nearest neighbor operation . `` Does episode training help label propagation ? How about the results of label propagation without the episode training ? `` > > > In our paper , the length scale parameter \\sigma is trained in an example-wise and episodic-wise way , as described in section 3.2.2 and Figure 4 of Appendix A . In order to investigate the benefit of episodic training , we combine the heuristic-based label propagation methods [ 2 ] with meta-learning to serve as a transductive baseline . Please refer to Table 1 and Table 2 line `` Label Propagation '' . It can be seen that TPN outperforms naive label propagation with a large margin , thus verifying the effectiveness of episode training . [ 1 ] Vinyals , Oriol et al . `` Matching networks for one shot learning . '' NIPS.2016 . [ 2 ] Zhou , Denny et al . `` Learning with local and global consistency . '' NIPS.2004 ."}, "2": {"review_id": "SyVuRiC5K7-2", "review_text": "The paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples. There is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly. However, the paper does cover a setup that I am not aware that was studied before. The paper is written clearly, and the experiments seem solid. Comments: -- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. -- It is not clear how the per-example scalar sigma-i is learned. (for Eq 2) -- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization ", "rating": "7: Good paper, accept", "reply_text": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers . Here we respond to your specific comments . `` What can be said about how computationally demanding the procedure is ? running label propagation within meta learning might be too costly. `` > > > In few-shot learning , episodic paradigm proposed by Matching Networks [ 1 ] is widely adopted by current researchers ( we follow the same setting to make a fair comparison ) . In each episode , a small subset of N-way K-shot Q-query examples is sampled from the training set . Typically , for 1-shot experiments , N=5 , K=1 , Q=15 and for 5-shot experiments , N=5 , K=5 , Q=15 . Thus , the number of training examples are Nx ( K+Q ) ( 80 for 1-shot and 100 for 5-shot ) . Constructing label propagation matrix W involves both support and query examples ( 80 or 100 ) . So the dimension of W is either 80x80 or 100x100 . Running label propagation on such small matrix is quite efficient . `` It is not clear how the per-example scalar sigma-i is learned . ( for Eq 2 ) '' > > > In Figure 4 of appendix A , we describe the detailed structure of the graph construction module . After we get the per-example feature representation f_ { \\varphi } ( x_i ) for x_i , we feed it into the graph construction module g_ { \\phi } . The output of this module is a one-dimensional scalar . f and g are learned in an end-to-end way in our approach . `` solving Eq 3 by matrix inversion does not scale . Would be best to also show results using iterative optimization `` > > > We want to answer this question from two aspects . On one hand , few-shot learning assumes that training examples in each class are quite small ( only 1 or 5 ) . In this situation , Eq ( 3 ) and the closed-form version can be efficiently solved , since the dimension of S is only 80x80 or 100x100 . On the other hand , there is plenty of prior work on the scalability and efficiency of label propagation , such as [ 2 ] , [ 3 ] , [ 4 ] , which can extend our work to large-scale data . On miniImagenet , we performed iterative optimization and got 53.05/68.75 for 1-shot/5-shot experiments with only 10 steps . This is slightly worse than closed-form version ( 53.75/69.43 ) , because of the inaccurate computation and unstable gradients caused by multiple step iterations . [ 1 ] Vinyals , Oriol , et al . `` Matching networks for one shot learning . '' NIPS.2016 . [ 2 ] Liang , De-Ming , and Yu-Feng Li . `` Lightweight Label Propagation for Large-Scale Network Data . '' IJCAI.2018 . [ 3 ] Fujiwara , Yasuhiro , and Go Irie . `` Efficient label propagation . '' ICML.2014 . [ 4 ] Weston , Jason . `` Large-Scale Semi-Supervised Learning . ''"}}