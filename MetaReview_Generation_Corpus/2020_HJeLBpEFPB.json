{"year": "2020", "forum": "HJeLBpEFPB", "title": "Unsupervised Universal Self-Attention Network for Graph Classification", "decision": "Reject", "meta_review": "All three reviewers are consistently negative on this paper. Thus a reject is recommended.", "reviews": [{"review_id": "HJeLBpEFPB-0", "review_text": "In this paper, the authors developed a graph embedding method called U2GAN based on self-attention mechanism. Similar to many existing graph neural network, U2GAN samples and aggregate neighboring features for each node in a graph. The aggregation function is similar to GAT, i.e., a query based attention layer. The difference is an incorporation of a transition function followed the attention layer. By minimizing Eq. 7, node embeddings can be inferred, which are summed up to obtain a graph embedding, for the downstream graph classification task. There are several points that are unclear in the paper. 1. The major argument of the advantage of using self-attention for neighborhood aggregation is to facilitate memorizing the dependencies between nodes and explore the graph structure similarities locally and globally. This argument, however, was not clearly discussed in the paper. First, it is not clear on why existing GNN methods, such as GCN, GraphSage, and GAT, cannot do so. Second, it is not clear on how does the proposed U2GAN achieve it. The current paper only provides some high-level descriptions. A more specific or theoretical discussion is desired. 2. Since the attention based aggregation is similar to GAT, a discussion on the difference is important. 3. Several model designs are not well justified. In Eq. 2, 3, the reason to employ Layernorm is missing. In Eq. 7, the intuition on how does the loss function help learn effective embeddings remains to be clarified. Also, it may be better to evaluate different pooling method to obtain graph embedding to justify the choice of sum in Eq. 1. 4. Since the proposed method aims to learn node embeddings in an unsupervised manner, it is better to see some descriptions on why graph classification was selected as the task in evaluation, instead of node classification. 5. In the experiments, some methods such as deepwalk, node2vec, graphsage and GAT are missing in comparison. In particular, due to the similarity between the proposed method GAT, it is interesting to evaluate GAT by replacing its supervised loss by Eq. 7 as a compared method. Moreover, in fig. 4, other visualizations of other methods can be compared to demonstrate the difference between the proposed method and others. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your comments . For some clarifications : 1 . We find that the the propagating phase in the GNN approaches ( e.g.GraphSAGE , GCN , GIN ) is not advanced enough to be able to determine latent potential relationships among nodes . You can see the the propagating phase of GCN , GraphSAGE and GAT in Appendix A . 2.GAT borrows the standard attention from [ 1 ] in using a single-layer feedforward neural network parametrized by a weight vector and then applying the non-linear function followed by the softmax function to compute importance weights of neighbors for a given node . Therefore , GAT is much different with the self-attention mechanism . You can see more details about GAT in Appendix A of our revision . [ 1 ] Neural machine translation by jointly learning to align and translate . ICLR 2015 . 3.Xu et.al . [ 2 ] showed that the sum pooling performs better than the mean and max poolings . Finally , our current results with the sum pooling are very promising . [ 2 ] How Powerful are Graph Neural Networks ? Xu et.al. , ICLR 2019 . 4.To the best of our knowledge , our work is the first to show that a unsupervised model can noticeably outperform most of up-to-date supervised approaches by a large margin . This is important in both industry and academic applications in reality where expanding unsupervised GNN models is more suitable due to the limited availability of class labels . We plan to investigate the effectiveness of our model on other important tasks such as node classification and link prediction in the future work . 5.As mentioned in our revision , our `` supervised '' model produces new state-of-the-art accuracies on DD , IMDBBINARY , IMDBMULTI , PROTEINS and PTC ; and obtains competitive accuracies on remaining datasets . 6.We included the results of GraphSAGE in our revision . Existing works do not include DeepWalk or Node2Vec as baselines for the downstream task of graph classification . We do not have experimental results for GAT . But Shchur et . al . [ 3 ] showed that GCN outperforms GAT in different split settings . [ 3 ] Pitfalls of Graph Neural Network Evaluation . Shchur et . al.2018.We did not include the embedding visualization of other baselines because those methods are very different from our unsupervised model . We are looking forward to your new updated comments on our revised submission . Thanks ."}, {"review_id": "HJeLBpEFPB-1", "review_text": " The paper presents an new unsupervised model for graph classification. It borrows the idea from universal self-attention network and applies it to graph learning. It achieves surprisingly good results on benchmark datasets. Despite the good results, I do not think the technical quality is good enough to make it accepted. My concerns contain the following aspects: 1. If we compare the proposed model with the graph attention networks (GAT), it just adds the recurrent transition and the layer normalizer, which are also from the universal self-attention. This makes the paper not novel enough. Furthermore, adding these components are not so related to unsupervised learning, it does not add any value to the unsupervised learning strategy. 2. The description of the unsupervised learning objective is not clear. From Algorithm 1, it seems $o_v$ is equal to $h_v^T$, I cannot understand the meaning of Eq. (7) at all. 3. The results are too good to be true. Although we cannot judge it based on this belief, the authors have to convince the readers and explain how the huge performance gain is obtained (on some datasets U2GAN is even 27% higher than all of other methods). I understand the experimental setting is transductive, but even that cannot explain everything. To justify the experiments, the authors need to do a lot of ablation study, such as comparing with supervised learning version of this model, while in the paper there is no ablation model to explain it. ", "rating": "1: Reject", "reply_text": "Thank you for your comments . For some clarifications : 1 . GAT borrows the standard attention from [ 1 ] in using a single-layer feedforward neural network parametrized by a weight vector and then applying the non-linear function followed by the softmax function to compute importance weights of neighbors for a given node . Therefore , GAT is much different with the self-attention mechanism . You can see more details about GAT in Appendix A of our revision . [ 1 ] Neural machine translation by jointly learning to align and translate . ICLR 2015 . 2.Regarding the unsupervised fashion , we aim to maximize the similarity between h^T_v and the node embedding o_v of a given node v , and also to minimize the similarity between h^T_v and the embeddings of `` negative nodes '' . In addition , h^T_v may depend on sampling the neighbors of node v. Hence , after training , we choose to use o_v as the final representation of node v. Regarding the supervised fashion where we do not need to learn the node embeddings `` separately '' , we can use h^T_v as the final representation of node v ( i.e. , o_v = h^T_v ) in order to produce the vector representation o_G . 3.We shared our code and running command scripts to make sure : ( i ) you can verify that our implementation is correct , and ( ii ) you can reproduce our experimental results . As mentioned in our revision , our `` supervised '' model produces new state-of-the-art accuracies on DD , IMDBBINARY , IMDBMULTI , PROTEINS and PTC ; and obtains competitive accuracies on remaining datasets . 4.To the best of our knowledge , our work is the first to show that a unsupervised model can noticeably outperform most of up-to-date supervised approaches by a large margin . This is important in both industry and academic applications in reality where expanding unsupervised GNN models is more suitable due to the limited availability of class labels . We are looking forward to your new updated comments on our revised submission . Thanks ."}, {"review_id": "HJeLBpEFPB-2", "review_text": "The submission proposes a graph neural network based on propagation with the attention mechanism. Then the output function uses the summation of node vectors to read out information about the graph. While the design is good, all components are all known techniques: the sampling procedure is like GraphSAGE; the propagation rule is similar to GAT, and the output function is wide uses in graph neural networks. Critics: The writing is not clear. At the top of page 4: quote: \"... and produce an output sequence {h_vi^t}i=1^N+1\". Do you keep only the vector h_v1 and throw away other vectors? Because you will also put v_2 at the center and compute its vector in a different self-attention computation. If this is the case, why not just say the output is h_v1? If this is not the case, then each node will have multiple vectors: one is computed when the node is at the center, and others are computed when the node is sampled as a neighbor. Below Boris Knyazev has several comments, which are not well addressed by authors. There is a discussion about transductive learning and inductive learning. However, it seems the authors still don't know how to run inductive learning on the graph classification task (quote \"... still do not have a standard inductive setting for the graph classification task where we only use a part of each graph during training.\"). Boris does not suggest to use part of each graph; instead, he suggests not using test graphs. I believe this is the standard practice in inductive learning (e.g. kernel methods). Another comment from Boris about the case when T=1, and the response is \"T=1 does not correspond to a single layer network\". I don't understand the response either. When T=1, a node only gets information from its neighbors. It is similar to a one-layer GCN or GAT, in which a node also only gets information from its immediate neighbors. I also don't understand why the author insists that the proposed model has a layer-based architecture. In my view, it is a graph neural network by the standard of propagation rule and output function. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your comments . For some clarifications : 1 . As described after Equation 6 in the manuscript , at the final step ( the T-th step ) , each node v can have multiple vectors as you mentioned ; and we consider to only use the vector ( at the center ) at the final step to infer the embedding of the node v. 2 . We \u2019 ve discussed with Borsi that the graph classification task and \u201c Not using test graphs \u201d do not means we are testing inductive setting like in the node classification task . We suggested Boris to point a paper which describes an inductive setting for the graph classification task as reference , but got no response for this . 3.To have better comprehensive experiments , we have implemented our model in the supervised fashion , and posted the results on October 28 . In the revised manuscript , we show that our supervised model produces new state-of-the-art accuracies on DD , IMDBBINARY , IMDBMULTI , PROTEINS and PTC ; and obtains competitive accuracies on remaining datasets . We have also shared our code to make sure you can verify and reproduce our results . 4.In the layer-based GNN architecture like GCN/GAT , we have to construct ( k+1 ) layers ( i.e. , multiple layers stacked on top of each other ) to reach to k-hops neighbors of a given node . In our proposed model , T is not equal to the number of GCN/GAT layers . Not only T=1 , but also for any value of T , we update the vector representation of each node v by recursively propagating the representations of its neighbors . This is reason why we do not need to implement a layer-based GNN architecture like GCN/GAT . We are looking forward to your new updated comments on our revised submission . Thanks ."}], "0": {"review_id": "HJeLBpEFPB-0", "review_text": "In this paper, the authors developed a graph embedding method called U2GAN based on self-attention mechanism. Similar to many existing graph neural network, U2GAN samples and aggregate neighboring features for each node in a graph. The aggregation function is similar to GAT, i.e., a query based attention layer. The difference is an incorporation of a transition function followed the attention layer. By minimizing Eq. 7, node embeddings can be inferred, which are summed up to obtain a graph embedding, for the downstream graph classification task. There are several points that are unclear in the paper. 1. The major argument of the advantage of using self-attention for neighborhood aggregation is to facilitate memorizing the dependencies between nodes and explore the graph structure similarities locally and globally. This argument, however, was not clearly discussed in the paper. First, it is not clear on why existing GNN methods, such as GCN, GraphSage, and GAT, cannot do so. Second, it is not clear on how does the proposed U2GAN achieve it. The current paper only provides some high-level descriptions. A more specific or theoretical discussion is desired. 2. Since the attention based aggregation is similar to GAT, a discussion on the difference is important. 3. Several model designs are not well justified. In Eq. 2, 3, the reason to employ Layernorm is missing. In Eq. 7, the intuition on how does the loss function help learn effective embeddings remains to be clarified. Also, it may be better to evaluate different pooling method to obtain graph embedding to justify the choice of sum in Eq. 1. 4. Since the proposed method aims to learn node embeddings in an unsupervised manner, it is better to see some descriptions on why graph classification was selected as the task in evaluation, instead of node classification. 5. In the experiments, some methods such as deepwalk, node2vec, graphsage and GAT are missing in comparison. In particular, due to the similarity between the proposed method GAT, it is interesting to evaluate GAT by replacing its supervised loss by Eq. 7 as a compared method. Moreover, in fig. 4, other visualizations of other methods can be compared to demonstrate the difference between the proposed method and others. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your comments . For some clarifications : 1 . We find that the the propagating phase in the GNN approaches ( e.g.GraphSAGE , GCN , GIN ) is not advanced enough to be able to determine latent potential relationships among nodes . You can see the the propagating phase of GCN , GraphSAGE and GAT in Appendix A . 2.GAT borrows the standard attention from [ 1 ] in using a single-layer feedforward neural network parametrized by a weight vector and then applying the non-linear function followed by the softmax function to compute importance weights of neighbors for a given node . Therefore , GAT is much different with the self-attention mechanism . You can see more details about GAT in Appendix A of our revision . [ 1 ] Neural machine translation by jointly learning to align and translate . ICLR 2015 . 3.Xu et.al . [ 2 ] showed that the sum pooling performs better than the mean and max poolings . Finally , our current results with the sum pooling are very promising . [ 2 ] How Powerful are Graph Neural Networks ? Xu et.al. , ICLR 2019 . 4.To the best of our knowledge , our work is the first to show that a unsupervised model can noticeably outperform most of up-to-date supervised approaches by a large margin . This is important in both industry and academic applications in reality where expanding unsupervised GNN models is more suitable due to the limited availability of class labels . We plan to investigate the effectiveness of our model on other important tasks such as node classification and link prediction in the future work . 5.As mentioned in our revision , our `` supervised '' model produces new state-of-the-art accuracies on DD , IMDBBINARY , IMDBMULTI , PROTEINS and PTC ; and obtains competitive accuracies on remaining datasets . 6.We included the results of GraphSAGE in our revision . Existing works do not include DeepWalk or Node2Vec as baselines for the downstream task of graph classification . We do not have experimental results for GAT . But Shchur et . al . [ 3 ] showed that GCN outperforms GAT in different split settings . [ 3 ] Pitfalls of Graph Neural Network Evaluation . Shchur et . al.2018.We did not include the embedding visualization of other baselines because those methods are very different from our unsupervised model . We are looking forward to your new updated comments on our revised submission . Thanks ."}, "1": {"review_id": "HJeLBpEFPB-1", "review_text": " The paper presents an new unsupervised model for graph classification. It borrows the idea from universal self-attention network and applies it to graph learning. It achieves surprisingly good results on benchmark datasets. Despite the good results, I do not think the technical quality is good enough to make it accepted. My concerns contain the following aspects: 1. If we compare the proposed model with the graph attention networks (GAT), it just adds the recurrent transition and the layer normalizer, which are also from the universal self-attention. This makes the paper not novel enough. Furthermore, adding these components are not so related to unsupervised learning, it does not add any value to the unsupervised learning strategy. 2. The description of the unsupervised learning objective is not clear. From Algorithm 1, it seems $o_v$ is equal to $h_v^T$, I cannot understand the meaning of Eq. (7) at all. 3. The results are too good to be true. Although we cannot judge it based on this belief, the authors have to convince the readers and explain how the huge performance gain is obtained (on some datasets U2GAN is even 27% higher than all of other methods). I understand the experimental setting is transductive, but even that cannot explain everything. To justify the experiments, the authors need to do a lot of ablation study, such as comparing with supervised learning version of this model, while in the paper there is no ablation model to explain it. ", "rating": "1: Reject", "reply_text": "Thank you for your comments . For some clarifications : 1 . GAT borrows the standard attention from [ 1 ] in using a single-layer feedforward neural network parametrized by a weight vector and then applying the non-linear function followed by the softmax function to compute importance weights of neighbors for a given node . Therefore , GAT is much different with the self-attention mechanism . You can see more details about GAT in Appendix A of our revision . [ 1 ] Neural machine translation by jointly learning to align and translate . ICLR 2015 . 2.Regarding the unsupervised fashion , we aim to maximize the similarity between h^T_v and the node embedding o_v of a given node v , and also to minimize the similarity between h^T_v and the embeddings of `` negative nodes '' . In addition , h^T_v may depend on sampling the neighbors of node v. Hence , after training , we choose to use o_v as the final representation of node v. Regarding the supervised fashion where we do not need to learn the node embeddings `` separately '' , we can use h^T_v as the final representation of node v ( i.e. , o_v = h^T_v ) in order to produce the vector representation o_G . 3.We shared our code and running command scripts to make sure : ( i ) you can verify that our implementation is correct , and ( ii ) you can reproduce our experimental results . As mentioned in our revision , our `` supervised '' model produces new state-of-the-art accuracies on DD , IMDBBINARY , IMDBMULTI , PROTEINS and PTC ; and obtains competitive accuracies on remaining datasets . 4.To the best of our knowledge , our work is the first to show that a unsupervised model can noticeably outperform most of up-to-date supervised approaches by a large margin . This is important in both industry and academic applications in reality where expanding unsupervised GNN models is more suitable due to the limited availability of class labels . We are looking forward to your new updated comments on our revised submission . Thanks ."}, "2": {"review_id": "HJeLBpEFPB-2", "review_text": "The submission proposes a graph neural network based on propagation with the attention mechanism. Then the output function uses the summation of node vectors to read out information about the graph. While the design is good, all components are all known techniques: the sampling procedure is like GraphSAGE; the propagation rule is similar to GAT, and the output function is wide uses in graph neural networks. Critics: The writing is not clear. At the top of page 4: quote: \"... and produce an output sequence {h_vi^t}i=1^N+1\". Do you keep only the vector h_v1 and throw away other vectors? Because you will also put v_2 at the center and compute its vector in a different self-attention computation. If this is the case, why not just say the output is h_v1? If this is not the case, then each node will have multiple vectors: one is computed when the node is at the center, and others are computed when the node is sampled as a neighbor. Below Boris Knyazev has several comments, which are not well addressed by authors. There is a discussion about transductive learning and inductive learning. However, it seems the authors still don't know how to run inductive learning on the graph classification task (quote \"... still do not have a standard inductive setting for the graph classification task where we only use a part of each graph during training.\"). Boris does not suggest to use part of each graph; instead, he suggests not using test graphs. I believe this is the standard practice in inductive learning (e.g. kernel methods). Another comment from Boris about the case when T=1, and the response is \"T=1 does not correspond to a single layer network\". I don't understand the response either. When T=1, a node only gets information from its neighbors. It is similar to a one-layer GCN or GAT, in which a node also only gets information from its immediate neighbors. I also don't understand why the author insists that the proposed model has a layer-based architecture. In my view, it is a graph neural network by the standard of propagation rule and output function. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your comments . For some clarifications : 1 . As described after Equation 6 in the manuscript , at the final step ( the T-th step ) , each node v can have multiple vectors as you mentioned ; and we consider to only use the vector ( at the center ) at the final step to infer the embedding of the node v. 2 . We \u2019 ve discussed with Borsi that the graph classification task and \u201c Not using test graphs \u201d do not means we are testing inductive setting like in the node classification task . We suggested Boris to point a paper which describes an inductive setting for the graph classification task as reference , but got no response for this . 3.To have better comprehensive experiments , we have implemented our model in the supervised fashion , and posted the results on October 28 . In the revised manuscript , we show that our supervised model produces new state-of-the-art accuracies on DD , IMDBBINARY , IMDBMULTI , PROTEINS and PTC ; and obtains competitive accuracies on remaining datasets . We have also shared our code to make sure you can verify and reproduce our results . 4.In the layer-based GNN architecture like GCN/GAT , we have to construct ( k+1 ) layers ( i.e. , multiple layers stacked on top of each other ) to reach to k-hops neighbors of a given node . In our proposed model , T is not equal to the number of GCN/GAT layers . Not only T=1 , but also for any value of T , we update the vector representation of each node v by recursively propagating the representations of its neighbors . This is reason why we do not need to implement a layer-based GNN architecture like GCN/GAT . We are looking forward to your new updated comments on our revised submission . Thanks ."}}