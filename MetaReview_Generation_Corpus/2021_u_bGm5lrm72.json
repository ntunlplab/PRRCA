{"year": "2021", "forum": "u_bGm5lrm72", "title": "DIET-SNN: A Low-Latency Spiking Neural Network with Direct Input Encoding & Leakage and Threshold Optimization", "decision": "Reject", "meta_review": "The manuscript presents a training method for Spiking Neural Networks (SNN). The method jointly optimizes input spike encoding parameters, spiking neuron parameters (membrane leak and voltage threshold), and weights in an end-to-end fashion using gradient descent. SNNs are very interesting for energy-efficient implementations of neural networks. Their energy efficiency strongly depends on inference latency (SNNs compute in time, unlike feed-forward ANNs) and activation sparsity. \n\nAll reviewers acknowledged that the approach directly improves inference latency and activation sparsity on large convolutional models at very good performance levels.\n\nThe main concern of all reviewers was the limited conceptual novelty. The paper combines some known techniques (hybrid SNN training, direct input encoding, training of neuron parameters like leak time constant and threshold) and scale the setup up to large networks and datasets (e.g. ImageNet).\n\nIn summary, the paper presents impressive results, but the conceptual innovation is missing. \n\n\n", "reviews": [{"review_id": "u_bGm5lrm72-0", "review_text": "This paper proposes a Spiking Neural Network ( SNN ) training method that jointly optimizes input spike encoding parameters , spiking neuron parameters ( membrane leak and voltage threshold ) , and weights in an end-to-end fashion using gradient descent . Compared to SNNs with only weight optimization , the SNN trained by the proposed method significantly decreases the inference latency and results in high activation sparsity with minimal accuracy decrease . Highlights : 1 . Energy-efficiency is currently the main advantage of SNN . Two primary factors that determine the energy-efficiency of an SNN are inference latency and activation sparsity . The method proposed in the paper directly targets both factors , and the intuitions are very clear . 2.The results show the joint training approach directly contributes to the improvement of SNN 's inference latency and activation sparsity ( as shown in Fig.2 and Fig.3 in the paper ) . It 's also interesting to see the method can learn low membrane leak and high voltage threshold at the same time for later layers of VGG16 , which significantly improve activation sparsity . 3.The paper proposes a new spike generation function for LIF neurons in equation ( 2 ) of section 3 . The function places the voltage threshold in the computational graph of backpropagation , making training voltage threshold possible in an elegant way . Concerns : 1 . The reviewer \u2019 s main concern with the paper is its limited novelty . The joint end-to-end training of neuron parameters has already been proposed in a recent paper ( [ Bojian Yin et al.2020 ] ) .Moreover , in addition to gradient-based training , there are also local learning methods like intrinsic plasticity that tune the neuron parameters based on the activity of the SNN ( for example , [ Wenrui Zhang et al.2019 ] , [ Anguo Zhang et al.2019 ] ) .The paper needs to better position the proposed method with respect to these existing related works . 2.Table 3 shows the computation overheads as a result of the direct encoding layer is around 1 % . However , the large number of encoded spiking activities can slow down the inference speed when the SNN is deployed on currently available neuromorphic chips ( for example , Intel 's Loihi or IBM 's TrueNorth ) since these chips lack an efficient solution for injecting external spikes . It will be great if the paper can show how the DIET-SNN performs with different numbers of convolution channels in the encoding layer and examine if using fewer input neurons for encoding will hurt the accuracy . 3.The proposed method uses backpropagation through time ( BPTT ) to train the SNN . Many recent papers that train SNN using spike-train level learning methods also achieve low latency and sparse synaptic activities ( for example , [ Jibin Wu et al. , 2019 ] , [ Yingyezhe Jin et al. , 2018 ] ) . The paper lacks experiments for comparing the performance , inference latency , and activation sparsity with these existing methods . 4.The ablation study in section 6 uses IF neurons for the first 3 experiments and LIF neurons for the last experiment which seems unfair . The ablation study should show that DIET-SNN can do better than an SNN using LIF neurons with fixed ( preset ) membrane leak and voltage threshold . The current ablation study can not rule out the possibility that an SNN with fixed ( preset ) low membrane leak and high voltage threshold performs as well as or even better than DIET-SNN in terms of accuracy and activation sparsity . 5.The ablation study in section 6 shows the performance of only one inference timesteps value for each SNN . To show a complete picture of how the performance changes with the latency , the paper needs to train each SNN with the same group of latency values ( for example , all 4 SNN trained with 150 , 35 , 25 , and 20 timesteps ) . 6.In section 6 , the paper claims that optimizing the threshold has similar consequences as optimizing the weights . The co-optimization of them can lead to faster convergence and lower latency . However , there are no experiments to support this argument . The reviewer suggests conducting additional experiments for this . I want to thank the author for addressing my concerns . Many of my concerns are resolved . I have updated my rating after the discussion . I agree with the authors that the novelty of the submission lies in scaling up the existing methods . However , I 'm not sure if this is enough for the paper 's scientific significance required by this conference . Bojian Yin et al.2020 , Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks Wenrui Zhang et al.2019 , Information-theoretic intrinsic plasticity for online unsupervised learning in spiking neural networks Auguo Zhang et al.2019 , Fast and robust learning in Spiking Feed-forward Neural Networks based on Intrinsic Plasticity mechanism Jibin Wu et al. , 2019 , A Tandem Learning Rule for Effective Training and Rapid Inference of Deep Spiking Neural Networks Yingyezhe Jin et al. , 2018 , Hybrid macro/micro level backpropagation for training deep spiking neural networks", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Reviewer 's comment : * * The reviewer \u2019 s main concern with the paper is its limited novelty . The joint end-to-end training of neuron parameters has already been proposed in a recent paper ( [ Bojian Yin et al.2020 ] ) .Moreover , in addition to gradient-based training , there are also local learning methods like intrinsic plasticity that tune the neuron parameters based on the activity of the SNN ( for example , [ Wenrui Zhang et al.2019 ] , [ Anguo Zhang et al.2019 ] ) .The paper needs to better position the proposed method with respect to these existing related works . * * Author 's response : * * Thank you for your comments . We wrote a common reply on the top regarding the novelty of the work . Bojian Yin et al.proposed a spiking recurrent neural network ( SRNN ) with an adaptive threshold ( threshold changes with time ) and trained with end-to-end backpropagation-through-time ( BPTT ) . Though they optimize neuron parameters with gradient-descent , our work significantly differs in the following ways : 1 . The LIF equations used in [ 1 ] are more complex and have more parameters compared to our work . This increases the memory requirement and the associated energy in fetching and storing the parameters . 2.We employ feedforward deep ( > 16 layers ) spiking convolutional network compared to shallow ( < 3 layers ) spiking recurrent networks used in [ 1 ] . 3.For static image datasets , the authors in [ 1 ] process the image pixels as sequence and therefore the number of timesteps is proportional to the image size . In our work , we process the images with convolutional layers that are more suited for image datasets . The authors in [ 1 ] report results for only MNIST images , whereas in this work we show the scalability of this approach on larger and challenging datasets like CIFAR100 and ImageNet . The authors in [ 2,3 ] proposed an intrinsic plasticity ( IP ) rule to update the leaky resistance and the time constant of membrane potential in LIF neurons . The authors in [ 2 ] do not specifically talk about any method to update the weights , but in the experiments , they used a previously proposed spike-based learning rule . They show the efficacy of their approach by training a liquid state machine ( LSM ) where the proposed IP method is applied only on the reservoir neurons . Therefore , it is not clear how this method will perform in a network with multiple layers . They report their results only for the TI46 speech corpus and CitySpace dataset and therefore we can not compare the results directly with our work . The authors in [ 3 ] perform ANN-SNN conversion and then employ the IP method to update neuron parameters . The ANN is trained with end-to-end backpropagation and the local IP rule is only applied to update the neuron parameters . Here again , the experiments were limited to the MNIST dataset with fully-connected networks . The intrinsic plasticity rules and other local learning rules proposed for SNNs perform well on simple tasks ( MNIST ) but do not scale to challenging datasets ( CIFAR100 , ImageNet ) . Though local learning rules are desirable for online learning in neuromorphic hardware , they still do not perform as well as the backpropagation based methods . In our work , the goal is to achieve state-of-the-art accuracy on difficult tasks with minimum inference latency and energy . Therefore , we adopt the end-to-end backpropagation method and report better performance on challenging datasets with low inference latency . [ 1 ] Bojian Yin et al.2020 , Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks [ 2 ] Wenrui Zhang et al.2019 , Information-theoretic intrinsic plasticity for online unsupervised learning in spiking neural networks [ 3 ] Auguo Zhang et al.2019 , Fast and robust learning in Spiking Feed-forward Neural Networks based on Intrinsic Plasticity mechanism [ 4 ] Jibin Wu et al. , 2019 , A Tandem Learning Rule for Effective Training and Rapid Inference of Deep Spiking Neural Networks [ 5 ] Yingyezhe Jin et al. , 2018 , Hybrid macro/micro level backpropagation for training deep spiking neural networks"}, {"review_id": "u_bGm5lrm72-1", "review_text": "Tile : training SNNs for visual categorization with few timesteps PROS * high accuracy with only 20-25 timesteps CONS * nothing really new The authors train convolutional SNNs for image classification , using surrogate gradient learning ( SGL ) . They combine 4 mechanisms : 1 ) Hybrid SNN Training . First train an ANN with the same architecture , and use the resulting weights as initial values for SGL , to accelerate convergence . This has already been proposed in ( Rathi et al. , 2020 ) . 2 ) Direct Input Encoding . Instead of converting the image RGB values into spike trains using Poisson rate coding , the authors feed the analog RGB values in the first convolutional layer , which treats them as input current , and emits spikes using the LIF neuron model . This allows using fewer time steps , since Poisson rate coding imposes long time windows to estimate rates and average out noise . But again , this is not new . It 's been done for example in ( Rueckauer et al. , 2017 ; Lu & Sengupta , 2020 ) , which they cite . 3 ) Leak timescale training . The time scale of the leak is an important parameter in SNNs . It can be trained by SGL , just like the weights . Again this is not new . See for example ( which should be cited ) : Fang W ( 2020 ) Leaky Integrate-and-Fire Spiking Neuron with Learnable Membrane Time Parameter . Yin B , Corradi F , Boht\u00e9 SM ( 2020 ) Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks . arXiv.Zimmer R , Pellegrini T , Singh Fateh S , Masquelier T ( 2019 ) Technical report : supervised training of convolutional spiking neural networks with PyTorch . arXiv.4 ) Threshold training . Again this is not new . For example Zimmer et al 2019 ( ref above ) did it already . Also , training both the weights and the thresholds seems redundant , since only the ratio between weights and threshold matters . About that the author say : `` Intuitively , the effect of optimizing either weights or thresholds in SNNs with IF neurons should have similar consequences . But the threshold affects the activity of each neuron , whereas the weights are shared among multiple neurons . '' I do n't understand this sentence . Weights are shared only in the convolutional layers . Plus earlier the authors said that the threshold is the same for all neurons of a given layer . So in short , the authors successfully combine known approaches , but do not propose anything new at a conceptual or theoretical level . In my opinion this paper is mostly an engineering effort . That being said , it seems that no one can claim a better accuracy on CIFAR and ImageNet using this nb of timesteps ( or fewer ) . MINOR POINTS : * How much longer is convergence if the authors skip the ANN training , and start the SGL training from random weights ? * p2 : `` The length of timesteps '' - > `` The number of timesteps '' * `` The neurons in the convolutional and linear layers are defined by the LIF model '' I guess the authors meant `` dense '' or `` fully connected '' instead of `` linear '' , since the LIF is not linear * I liked a lot the `` iso-accuracy '' analysis of section 6 . But I suggest the authors do the same for their energy analysis of section 5 . The SNNs consume less than the ANNs ( Table 3 ) but are also less accurate ( Table 1 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Reviewer 's comment : * * Leak timescale training . The time scale of the leak is an important parameter in SNNs . It can be trained by SGL , just like the weights . Again this is not new . See for example ( which should be cited ) : Fang W ( 2020 ) Leaky Integrate-and-Fire Spiking Neuron with Learnable Membrane Time Parameter . Yin B , Corradi F , Boht\u00e9 SM ( 2020 ) Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks . arXiv.Zimmer R , Pellegrini T , Singh Fateh S , Masquelier T ( 2019 ) Technical report : supervised training of convolutional spiking neural networks with PyTorch . ArXiv . * * Author 's response : * * Thank you for pointing this out . We have cited the mentioned works in the updated manuscript . Although the idea of training leak has been proposed elsewhere , its application on deeper networks and challenging datasets have not been shown before . Though Fang W et al.achieve competitive accuracy on the CIFAR10 dataset , they fail to mention the number of timesteps used in their experiments . Yin B et al.and Zimmer R et al.do not show results for any of the three datasets ( CIFAR10 , CIFAR100 , ImageNet ) considered in our work . * * Reviewer 's comment : * * Threshold training . Again this is not new . For example Zimmer et al 2019 ( ref above ) did it already . Also , training both the weights and the thresholds seems redundant , since only the ratio between weights and threshold matters . About that the author say : `` Intuitively , the effect of optimizing either weights or thresholds in SNNs with IF neurons should have similar consequences . But the threshold affects the activity of each neuron , whereas the weights are shared among multiple neurons . '' I do n't understand this sentence . Weights are shared only in the convolutional layers . Plus earlier the authors said that the threshold is the same for all neurons of a given layer . * * Author 's response : * * Yes , we were referring to convolutional layers ( updated the writing in manuscript ) . The activity of a neuron can be modulated by changing its threshold . To achieve the same effect by modulating weights , multiple weights need to be changed both for fully-connected or convolutional layers . Additionally in convolutional layers , as the weights are shared , it may affect the activity of other neurons as well . You are right , this argument may not hold if we have the same threshold for all neurons in a layer . But we did notice a lower number of timesteps in the network where both the weights and threshold were trained jointly ( network ( b ) and ( c ) in Section 6 ) . The reason may be that the optimizer is able to find a better setting when both the parameters are tunable . We did try having an individual threshold for each neuron but that did not lead to better performance . We continue to investigate this with different optimizers . * * Reviewer 's comment : * * So in short , the authors successfully combine known approaches , but do not propose anything new at a conceptual or theoretical level . In my opinion this paper is mostly an engineering effort . That being said , it seems that no one can claim a better accuracy on CIFAR and ImageNet using this nb of timesteps ( or fewer ) . * * Author 's response : * * We agree some of the concepts are proposed in other works as well , but as you mentioned , in our work we implement the methods on large-scale datasets and thereby addressing the engineering challenges which were not covered in previous works . This work achieves the best performance on CIFAR and ImageNet datasets with lower number of timesteps . * * MINOR POINTS : * * * How much longer is convergence if the authors skip the ANN training , and start the SGL training from random weights ? We did train a network with 4 convolutional layers and 2 fully-connected layers from scratch and compared its performance with hybrid learning . The network from scratch required more than 300 epochs of training whereas the hybrid method needed only 100 epochs of training with spike-based backpropagation . * p2 : `` The length of timesteps '' - > `` The number of timesteps '' Thank you for pointing this out , we have updated it in the manuscript . * `` The neurons in the convolutional and linear layers are defined by the LIF model '' I guess the authors meant `` dense '' or `` fully connected '' instead of `` linear '' , since the LIF is not linear Yes , you are correct . We have updated the manuscript and refer it as fully-connected layers . * I liked a lot the `` iso-accuracy '' analysis of section 6 . But I suggest the authors do the same for their energy analysis of section 5 . The SNNs consume less than the ANNs ( Table 3 ) but are also less accurate ( Table 1 ) . The ANN/SNN energy ratio for the networks ( a ) - ( d ) in Section 6 are 0.2 , 2.6 , 3.4 , and 12.4 , respectively . We have updated Section 6 in the manuscript with the energy numbers . Yes , we agree that ANNs generally achieve higher accuracy . We have updated the results in Table-1 where the gap between ANN and SNN accuracies is reduced ."}, {"review_id": "u_bGm5lrm72-2", "review_text": "Strength : I appreciate the experimental results demonstrated on challenging datasets like CIFAR100 and ImageNet . Weakness ( 1 ) There are two existing papers emphasizing direct training SNN with extremely low latency [ 1 ] [ 2 ] . Therefore , the authors should comment more on how the proposed method is different or is better compared to these two papers ( they use a smaller number of time steps . ) . In addition , I hope the authors to show the performance comparison with these two references . In the experimental results , the paper compares performance with [ 1 ] . However , the network size is significantly different . I think a comparison of the same network size can help to demonstrate the effectiveness of the proposed method . ( 2 ) To my understanding , the method proposed in this paper has nothing different compared to the existing BPTT method with the surrogate gradient . I hope the authors can claim clearly the novelty of this paper . For example , how is the proposed method different or better from [ 3 ] and [ 4 ] . As far as I know , the only difference is that the threshold and leaky parameters are also trained in this paper . However , this can also be easily done in other existing methods like [ 2 ] [ 3 ] [ 4 ] . There 's no doubt their performance can also be improved since it introduces more tunable parameters . ( 3 ) Tuning threshold and leaky parameters may be unbiologically plausible . ( 4 ) What is the weight optimization method ? As shown in Table 1 , it seems the weight optimization contributes more to the performance than the proposed method . [ 1 ] Wu , Y. , Deng , L. , Li , G. , Zhu , J. , Xie , Y. , & Shi , L. ( 2019 , July ) . Direct training for spiking neural networks : Faster , larger , better . In Proceedings of the AAAI Conference on Artificial Intelligence ( Vol.33 , pp.1311-1318 ) . [ 2 ] Zhang , W. , & Li , P. ( 2020 ) . Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks . arXiv preprint arXiv:2002.10085 . [ 3 ] Wu , Y. , Deng , L. , Li , G. , Zhu , J. , & Shi , L. ( 2018 ) . Spatio-temporal backpropagation for training high-performance spiking neural networks . Frontiers in neuroscience , 12 , 331 . [ 4 ] Shrestha , S. B. , & Orchard , G. ( 2018 ) . Slayer : Spike layer error reassignment in time . In Advances in Neural Information Processing Systems ( pp.1412-1421 ) .", "rating": "3: Clear rejection", "reply_text": "* * Reviewer 's comment : * * Weakness ( 1 ) There are two existing papers emphasizing direct training SNN with extremely low latency [ 1 ] [ 2 ] . Therefore , the authors should comment more on how the proposed method is different or is better compared to these two papers ( they use a smaller number of time steps . ) . In addition , I hope the authors to show the performance comparison with these two references . In the experimental results , the paper compares performance with [ 1 ] . However , the network size is significantly different . I think a comparison of the same network size can help to demonstrate the effectiveness of the proposed method . * * Author 's response : * * The authors in [ 1 ] proposed a normalization method ( NeuNorm ) that computes a weighted summation of spike count and uses that quantity as the input to the convolutional layer instead of the raw spike signals . Therefore , the convolution requires the multiply-and-accumulate ( MAC ) operation as both the input and the weight are real-valued quantities . In SNN , the major advantage is that the expensive MAC operation ( needed in ANN ) is reduced to simple additions due to binary inputs . Although the authors achieved competitive accuracy ( 90.53 % ) in 12 time-steps for CIFAR10 , the proposed normalization method loses the energy benefits of SNNs and is similar to ANN in terms of the type of computation . The authors in [ 2 ] propose a learning rule that requires computing loss at every time-step and the goal of the training is to teach output neurons to produce a desired firing sequence . Therefore , for classification tasks the output neuron of the correct class is trained to spike at every time-step ; this will increase the overall spiking activity of the network ( 4 % of neurons spike at every time-step ) and diminish the energy-efficiency . We compare the results for the same architecture in the Table below and our approach performs better than both the networks for the same or less number of time-steps . Additionally , we show that our method is scalable to networks with residual connections and for larger datasets like CIFAR100 , and ImageNet | Model | Method | Dataset | Architecture | Accuracy | Timesteps | | -- |- |- | | -- | | | [ 1 ] | Surrogate gradient with normalization ( requires MAC ) | CIFAR10 | CIFARNet | 90.53 % | 12 | | [ 2 ] | Inter-neuron and intra-neuron optimization | CIFAR10 | CIFARNet | 91.41 % | 5 | | Our work | DIET-SNN | CIFAR10 | CIFARNet | 91.59 % | 5 | CIFARNet : 128C3 ( encoding ) -256C3-AP2-512C3-AP2-1024C3-512C3-1024FC-512FC-10 [ 1 ] Wu , Y. , Deng , L. , Li , G. , Zhu , J. , Xie , Y. , & Shi , L. ( 2019 , July ) . Direct training for spiking neural networks : Faster , larger , better . In Proceedings of the AAAI Conference on Artificial Intelligence ( Vol.33 , pp.1311-1318 ) . [ 2 ] Zhang , W. , & Li , P. ( 2020 ) . Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks . arXiv preprint arXiv:2002.10085 ."}], "0": {"review_id": "u_bGm5lrm72-0", "review_text": "This paper proposes a Spiking Neural Network ( SNN ) training method that jointly optimizes input spike encoding parameters , spiking neuron parameters ( membrane leak and voltage threshold ) , and weights in an end-to-end fashion using gradient descent . Compared to SNNs with only weight optimization , the SNN trained by the proposed method significantly decreases the inference latency and results in high activation sparsity with minimal accuracy decrease . Highlights : 1 . Energy-efficiency is currently the main advantage of SNN . Two primary factors that determine the energy-efficiency of an SNN are inference latency and activation sparsity . The method proposed in the paper directly targets both factors , and the intuitions are very clear . 2.The results show the joint training approach directly contributes to the improvement of SNN 's inference latency and activation sparsity ( as shown in Fig.2 and Fig.3 in the paper ) . It 's also interesting to see the method can learn low membrane leak and high voltage threshold at the same time for later layers of VGG16 , which significantly improve activation sparsity . 3.The paper proposes a new spike generation function for LIF neurons in equation ( 2 ) of section 3 . The function places the voltage threshold in the computational graph of backpropagation , making training voltage threshold possible in an elegant way . Concerns : 1 . The reviewer \u2019 s main concern with the paper is its limited novelty . The joint end-to-end training of neuron parameters has already been proposed in a recent paper ( [ Bojian Yin et al.2020 ] ) .Moreover , in addition to gradient-based training , there are also local learning methods like intrinsic plasticity that tune the neuron parameters based on the activity of the SNN ( for example , [ Wenrui Zhang et al.2019 ] , [ Anguo Zhang et al.2019 ] ) .The paper needs to better position the proposed method with respect to these existing related works . 2.Table 3 shows the computation overheads as a result of the direct encoding layer is around 1 % . However , the large number of encoded spiking activities can slow down the inference speed when the SNN is deployed on currently available neuromorphic chips ( for example , Intel 's Loihi or IBM 's TrueNorth ) since these chips lack an efficient solution for injecting external spikes . It will be great if the paper can show how the DIET-SNN performs with different numbers of convolution channels in the encoding layer and examine if using fewer input neurons for encoding will hurt the accuracy . 3.The proposed method uses backpropagation through time ( BPTT ) to train the SNN . Many recent papers that train SNN using spike-train level learning methods also achieve low latency and sparse synaptic activities ( for example , [ Jibin Wu et al. , 2019 ] , [ Yingyezhe Jin et al. , 2018 ] ) . The paper lacks experiments for comparing the performance , inference latency , and activation sparsity with these existing methods . 4.The ablation study in section 6 uses IF neurons for the first 3 experiments and LIF neurons for the last experiment which seems unfair . The ablation study should show that DIET-SNN can do better than an SNN using LIF neurons with fixed ( preset ) membrane leak and voltage threshold . The current ablation study can not rule out the possibility that an SNN with fixed ( preset ) low membrane leak and high voltage threshold performs as well as or even better than DIET-SNN in terms of accuracy and activation sparsity . 5.The ablation study in section 6 shows the performance of only one inference timesteps value for each SNN . To show a complete picture of how the performance changes with the latency , the paper needs to train each SNN with the same group of latency values ( for example , all 4 SNN trained with 150 , 35 , 25 , and 20 timesteps ) . 6.In section 6 , the paper claims that optimizing the threshold has similar consequences as optimizing the weights . The co-optimization of them can lead to faster convergence and lower latency . However , there are no experiments to support this argument . The reviewer suggests conducting additional experiments for this . I want to thank the author for addressing my concerns . Many of my concerns are resolved . I have updated my rating after the discussion . I agree with the authors that the novelty of the submission lies in scaling up the existing methods . However , I 'm not sure if this is enough for the paper 's scientific significance required by this conference . Bojian Yin et al.2020 , Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks Wenrui Zhang et al.2019 , Information-theoretic intrinsic plasticity for online unsupervised learning in spiking neural networks Auguo Zhang et al.2019 , Fast and robust learning in Spiking Feed-forward Neural Networks based on Intrinsic Plasticity mechanism Jibin Wu et al. , 2019 , A Tandem Learning Rule for Effective Training and Rapid Inference of Deep Spiking Neural Networks Yingyezhe Jin et al. , 2018 , Hybrid macro/micro level backpropagation for training deep spiking neural networks", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Reviewer 's comment : * * The reviewer \u2019 s main concern with the paper is its limited novelty . The joint end-to-end training of neuron parameters has already been proposed in a recent paper ( [ Bojian Yin et al.2020 ] ) .Moreover , in addition to gradient-based training , there are also local learning methods like intrinsic plasticity that tune the neuron parameters based on the activity of the SNN ( for example , [ Wenrui Zhang et al.2019 ] , [ Anguo Zhang et al.2019 ] ) .The paper needs to better position the proposed method with respect to these existing related works . * * Author 's response : * * Thank you for your comments . We wrote a common reply on the top regarding the novelty of the work . Bojian Yin et al.proposed a spiking recurrent neural network ( SRNN ) with an adaptive threshold ( threshold changes with time ) and trained with end-to-end backpropagation-through-time ( BPTT ) . Though they optimize neuron parameters with gradient-descent , our work significantly differs in the following ways : 1 . The LIF equations used in [ 1 ] are more complex and have more parameters compared to our work . This increases the memory requirement and the associated energy in fetching and storing the parameters . 2.We employ feedforward deep ( > 16 layers ) spiking convolutional network compared to shallow ( < 3 layers ) spiking recurrent networks used in [ 1 ] . 3.For static image datasets , the authors in [ 1 ] process the image pixels as sequence and therefore the number of timesteps is proportional to the image size . In our work , we process the images with convolutional layers that are more suited for image datasets . The authors in [ 1 ] report results for only MNIST images , whereas in this work we show the scalability of this approach on larger and challenging datasets like CIFAR100 and ImageNet . The authors in [ 2,3 ] proposed an intrinsic plasticity ( IP ) rule to update the leaky resistance and the time constant of membrane potential in LIF neurons . The authors in [ 2 ] do not specifically talk about any method to update the weights , but in the experiments , they used a previously proposed spike-based learning rule . They show the efficacy of their approach by training a liquid state machine ( LSM ) where the proposed IP method is applied only on the reservoir neurons . Therefore , it is not clear how this method will perform in a network with multiple layers . They report their results only for the TI46 speech corpus and CitySpace dataset and therefore we can not compare the results directly with our work . The authors in [ 3 ] perform ANN-SNN conversion and then employ the IP method to update neuron parameters . The ANN is trained with end-to-end backpropagation and the local IP rule is only applied to update the neuron parameters . Here again , the experiments were limited to the MNIST dataset with fully-connected networks . The intrinsic plasticity rules and other local learning rules proposed for SNNs perform well on simple tasks ( MNIST ) but do not scale to challenging datasets ( CIFAR100 , ImageNet ) . Though local learning rules are desirable for online learning in neuromorphic hardware , they still do not perform as well as the backpropagation based methods . In our work , the goal is to achieve state-of-the-art accuracy on difficult tasks with minimum inference latency and energy . Therefore , we adopt the end-to-end backpropagation method and report better performance on challenging datasets with low inference latency . [ 1 ] Bojian Yin et al.2020 , Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks [ 2 ] Wenrui Zhang et al.2019 , Information-theoretic intrinsic plasticity for online unsupervised learning in spiking neural networks [ 3 ] Auguo Zhang et al.2019 , Fast and robust learning in Spiking Feed-forward Neural Networks based on Intrinsic Plasticity mechanism [ 4 ] Jibin Wu et al. , 2019 , A Tandem Learning Rule for Effective Training and Rapid Inference of Deep Spiking Neural Networks [ 5 ] Yingyezhe Jin et al. , 2018 , Hybrid macro/micro level backpropagation for training deep spiking neural networks"}, "1": {"review_id": "u_bGm5lrm72-1", "review_text": "Tile : training SNNs for visual categorization with few timesteps PROS * high accuracy with only 20-25 timesteps CONS * nothing really new The authors train convolutional SNNs for image classification , using surrogate gradient learning ( SGL ) . They combine 4 mechanisms : 1 ) Hybrid SNN Training . First train an ANN with the same architecture , and use the resulting weights as initial values for SGL , to accelerate convergence . This has already been proposed in ( Rathi et al. , 2020 ) . 2 ) Direct Input Encoding . Instead of converting the image RGB values into spike trains using Poisson rate coding , the authors feed the analog RGB values in the first convolutional layer , which treats them as input current , and emits spikes using the LIF neuron model . This allows using fewer time steps , since Poisson rate coding imposes long time windows to estimate rates and average out noise . But again , this is not new . It 's been done for example in ( Rueckauer et al. , 2017 ; Lu & Sengupta , 2020 ) , which they cite . 3 ) Leak timescale training . The time scale of the leak is an important parameter in SNNs . It can be trained by SGL , just like the weights . Again this is not new . See for example ( which should be cited ) : Fang W ( 2020 ) Leaky Integrate-and-Fire Spiking Neuron with Learnable Membrane Time Parameter . Yin B , Corradi F , Boht\u00e9 SM ( 2020 ) Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks . arXiv.Zimmer R , Pellegrini T , Singh Fateh S , Masquelier T ( 2019 ) Technical report : supervised training of convolutional spiking neural networks with PyTorch . arXiv.4 ) Threshold training . Again this is not new . For example Zimmer et al 2019 ( ref above ) did it already . Also , training both the weights and the thresholds seems redundant , since only the ratio between weights and threshold matters . About that the author say : `` Intuitively , the effect of optimizing either weights or thresholds in SNNs with IF neurons should have similar consequences . But the threshold affects the activity of each neuron , whereas the weights are shared among multiple neurons . '' I do n't understand this sentence . Weights are shared only in the convolutional layers . Plus earlier the authors said that the threshold is the same for all neurons of a given layer . So in short , the authors successfully combine known approaches , but do not propose anything new at a conceptual or theoretical level . In my opinion this paper is mostly an engineering effort . That being said , it seems that no one can claim a better accuracy on CIFAR and ImageNet using this nb of timesteps ( or fewer ) . MINOR POINTS : * How much longer is convergence if the authors skip the ANN training , and start the SGL training from random weights ? * p2 : `` The length of timesteps '' - > `` The number of timesteps '' * `` The neurons in the convolutional and linear layers are defined by the LIF model '' I guess the authors meant `` dense '' or `` fully connected '' instead of `` linear '' , since the LIF is not linear * I liked a lot the `` iso-accuracy '' analysis of section 6 . But I suggest the authors do the same for their energy analysis of section 5 . The SNNs consume less than the ANNs ( Table 3 ) but are also less accurate ( Table 1 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Reviewer 's comment : * * Leak timescale training . The time scale of the leak is an important parameter in SNNs . It can be trained by SGL , just like the weights . Again this is not new . See for example ( which should be cited ) : Fang W ( 2020 ) Leaky Integrate-and-Fire Spiking Neuron with Learnable Membrane Time Parameter . Yin B , Corradi F , Boht\u00e9 SM ( 2020 ) Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks . arXiv.Zimmer R , Pellegrini T , Singh Fateh S , Masquelier T ( 2019 ) Technical report : supervised training of convolutional spiking neural networks with PyTorch . ArXiv . * * Author 's response : * * Thank you for pointing this out . We have cited the mentioned works in the updated manuscript . Although the idea of training leak has been proposed elsewhere , its application on deeper networks and challenging datasets have not been shown before . Though Fang W et al.achieve competitive accuracy on the CIFAR10 dataset , they fail to mention the number of timesteps used in their experiments . Yin B et al.and Zimmer R et al.do not show results for any of the three datasets ( CIFAR10 , CIFAR100 , ImageNet ) considered in our work . * * Reviewer 's comment : * * Threshold training . Again this is not new . For example Zimmer et al 2019 ( ref above ) did it already . Also , training both the weights and the thresholds seems redundant , since only the ratio between weights and threshold matters . About that the author say : `` Intuitively , the effect of optimizing either weights or thresholds in SNNs with IF neurons should have similar consequences . But the threshold affects the activity of each neuron , whereas the weights are shared among multiple neurons . '' I do n't understand this sentence . Weights are shared only in the convolutional layers . Plus earlier the authors said that the threshold is the same for all neurons of a given layer . * * Author 's response : * * Yes , we were referring to convolutional layers ( updated the writing in manuscript ) . The activity of a neuron can be modulated by changing its threshold . To achieve the same effect by modulating weights , multiple weights need to be changed both for fully-connected or convolutional layers . Additionally in convolutional layers , as the weights are shared , it may affect the activity of other neurons as well . You are right , this argument may not hold if we have the same threshold for all neurons in a layer . But we did notice a lower number of timesteps in the network where both the weights and threshold were trained jointly ( network ( b ) and ( c ) in Section 6 ) . The reason may be that the optimizer is able to find a better setting when both the parameters are tunable . We did try having an individual threshold for each neuron but that did not lead to better performance . We continue to investigate this with different optimizers . * * Reviewer 's comment : * * So in short , the authors successfully combine known approaches , but do not propose anything new at a conceptual or theoretical level . In my opinion this paper is mostly an engineering effort . That being said , it seems that no one can claim a better accuracy on CIFAR and ImageNet using this nb of timesteps ( or fewer ) . * * Author 's response : * * We agree some of the concepts are proposed in other works as well , but as you mentioned , in our work we implement the methods on large-scale datasets and thereby addressing the engineering challenges which were not covered in previous works . This work achieves the best performance on CIFAR and ImageNet datasets with lower number of timesteps . * * MINOR POINTS : * * * How much longer is convergence if the authors skip the ANN training , and start the SGL training from random weights ? We did train a network with 4 convolutional layers and 2 fully-connected layers from scratch and compared its performance with hybrid learning . The network from scratch required more than 300 epochs of training whereas the hybrid method needed only 100 epochs of training with spike-based backpropagation . * p2 : `` The length of timesteps '' - > `` The number of timesteps '' Thank you for pointing this out , we have updated it in the manuscript . * `` The neurons in the convolutional and linear layers are defined by the LIF model '' I guess the authors meant `` dense '' or `` fully connected '' instead of `` linear '' , since the LIF is not linear Yes , you are correct . We have updated the manuscript and refer it as fully-connected layers . * I liked a lot the `` iso-accuracy '' analysis of section 6 . But I suggest the authors do the same for their energy analysis of section 5 . The SNNs consume less than the ANNs ( Table 3 ) but are also less accurate ( Table 1 ) . The ANN/SNN energy ratio for the networks ( a ) - ( d ) in Section 6 are 0.2 , 2.6 , 3.4 , and 12.4 , respectively . We have updated Section 6 in the manuscript with the energy numbers . Yes , we agree that ANNs generally achieve higher accuracy . We have updated the results in Table-1 where the gap between ANN and SNN accuracies is reduced ."}, "2": {"review_id": "u_bGm5lrm72-2", "review_text": "Strength : I appreciate the experimental results demonstrated on challenging datasets like CIFAR100 and ImageNet . Weakness ( 1 ) There are two existing papers emphasizing direct training SNN with extremely low latency [ 1 ] [ 2 ] . Therefore , the authors should comment more on how the proposed method is different or is better compared to these two papers ( they use a smaller number of time steps . ) . In addition , I hope the authors to show the performance comparison with these two references . In the experimental results , the paper compares performance with [ 1 ] . However , the network size is significantly different . I think a comparison of the same network size can help to demonstrate the effectiveness of the proposed method . ( 2 ) To my understanding , the method proposed in this paper has nothing different compared to the existing BPTT method with the surrogate gradient . I hope the authors can claim clearly the novelty of this paper . For example , how is the proposed method different or better from [ 3 ] and [ 4 ] . As far as I know , the only difference is that the threshold and leaky parameters are also trained in this paper . However , this can also be easily done in other existing methods like [ 2 ] [ 3 ] [ 4 ] . There 's no doubt their performance can also be improved since it introduces more tunable parameters . ( 3 ) Tuning threshold and leaky parameters may be unbiologically plausible . ( 4 ) What is the weight optimization method ? As shown in Table 1 , it seems the weight optimization contributes more to the performance than the proposed method . [ 1 ] Wu , Y. , Deng , L. , Li , G. , Zhu , J. , Xie , Y. , & Shi , L. ( 2019 , July ) . Direct training for spiking neural networks : Faster , larger , better . In Proceedings of the AAAI Conference on Artificial Intelligence ( Vol.33 , pp.1311-1318 ) . [ 2 ] Zhang , W. , & Li , P. ( 2020 ) . Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks . arXiv preprint arXiv:2002.10085 . [ 3 ] Wu , Y. , Deng , L. , Li , G. , Zhu , J. , & Shi , L. ( 2018 ) . Spatio-temporal backpropagation for training high-performance spiking neural networks . Frontiers in neuroscience , 12 , 331 . [ 4 ] Shrestha , S. B. , & Orchard , G. ( 2018 ) . Slayer : Spike layer error reassignment in time . In Advances in Neural Information Processing Systems ( pp.1412-1421 ) .", "rating": "3: Clear rejection", "reply_text": "* * Reviewer 's comment : * * Weakness ( 1 ) There are two existing papers emphasizing direct training SNN with extremely low latency [ 1 ] [ 2 ] . Therefore , the authors should comment more on how the proposed method is different or is better compared to these two papers ( they use a smaller number of time steps . ) . In addition , I hope the authors to show the performance comparison with these two references . In the experimental results , the paper compares performance with [ 1 ] . However , the network size is significantly different . I think a comparison of the same network size can help to demonstrate the effectiveness of the proposed method . * * Author 's response : * * The authors in [ 1 ] proposed a normalization method ( NeuNorm ) that computes a weighted summation of spike count and uses that quantity as the input to the convolutional layer instead of the raw spike signals . Therefore , the convolution requires the multiply-and-accumulate ( MAC ) operation as both the input and the weight are real-valued quantities . In SNN , the major advantage is that the expensive MAC operation ( needed in ANN ) is reduced to simple additions due to binary inputs . Although the authors achieved competitive accuracy ( 90.53 % ) in 12 time-steps for CIFAR10 , the proposed normalization method loses the energy benefits of SNNs and is similar to ANN in terms of the type of computation . The authors in [ 2 ] propose a learning rule that requires computing loss at every time-step and the goal of the training is to teach output neurons to produce a desired firing sequence . Therefore , for classification tasks the output neuron of the correct class is trained to spike at every time-step ; this will increase the overall spiking activity of the network ( 4 % of neurons spike at every time-step ) and diminish the energy-efficiency . We compare the results for the same architecture in the Table below and our approach performs better than both the networks for the same or less number of time-steps . Additionally , we show that our method is scalable to networks with residual connections and for larger datasets like CIFAR100 , and ImageNet | Model | Method | Dataset | Architecture | Accuracy | Timesteps | | -- |- |- | | -- | | | [ 1 ] | Surrogate gradient with normalization ( requires MAC ) | CIFAR10 | CIFARNet | 90.53 % | 12 | | [ 2 ] | Inter-neuron and intra-neuron optimization | CIFAR10 | CIFARNet | 91.41 % | 5 | | Our work | DIET-SNN | CIFAR10 | CIFARNet | 91.59 % | 5 | CIFARNet : 128C3 ( encoding ) -256C3-AP2-512C3-AP2-1024C3-512C3-1024FC-512FC-10 [ 1 ] Wu , Y. , Deng , L. , Li , G. , Zhu , J. , Xie , Y. , & Shi , L. ( 2019 , July ) . Direct training for spiking neural networks : Faster , larger , better . In Proceedings of the AAAI Conference on Artificial Intelligence ( Vol.33 , pp.1311-1318 ) . [ 2 ] Zhang , W. , & Li , P. ( 2020 ) . Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks . arXiv preprint arXiv:2002.10085 ."}}