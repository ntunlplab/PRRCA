{"year": "2021", "forum": "wG5XIGi6nrt", "title": "Learning Private Representations with Focal Entropy", "decision": "Reject", "meta_review": "This paper focuses on a notion of privacy in learning representations. \n\nOne of the primary concerns of the reviewers was clarity of the writing and results. Numerous concerns are mentioned in the reviews, and also more engagement with the fairness literature was desired. One reviewer felt that some of the claims in the paper were unsubstantiated, for example: understanding the sanitization process in a human-understandable visual way\", \"integration of a notion of interpretability\". It was felt that the changes required were more than could be expected for a camera ready version. The authors are recommended to revise the paper with a particular eye for clarity to a new reader.\n\nThe notion and measurement of privacy was also considered to be somewhat shaky. It is understood that the nature of privacy considered in this paper is different from differential privacy. That said, the latter is a rigorous definition, and the one in this paper seems to be rather empirical in nature. There are no formal guarantees in terms of privacy preservation, and it is not clear whether the representations could leak information when evaluated with a different network. As privacy is a mission-critical property, some justification of why the heuristic measurement of privacy is acceptable.\n\nAs a side note, the authors should consider using the \\citep command for parenthetical citations in the text.", "reviews": [{"review_id": "wG5XIGi6nrt-0", "review_text": "# # # Summary This paper presented a method for learning private representations . This method is based on adversarial representation learning and the main technique contribution comes from focal entropy . The experimental results show that focal entropy can improve the accuracy of the target predictor without increasing adversarial accuracy . # # # Pros 1 . Focal entropy is effective in reducing the information leakage of the learned representation while improves the target accuracy over the state-of-the-art . 2.This paper covers a wide range of experiments . # # # Cons 1 . Some technique details are not presented clearly . I have listed some of them here : a . How to divide the representation $ z $ encoded by the encoder into two sub-representations , namely , target and residual representations . b.There is a high-level description of focal entropy instead of equations . 2.Some notations are very confusing . For example , in the optimization goal ( Equation ( 2 ) ) , the adversarial loss $ \\phi_ { tilde { S } } $ is related to the parameter $ \\theta_ { res } $ . However , in the following description , this loss is related to $ \\theta_ { tar } $ ( Equation ( 6 ) ) # # # Comments 1 . The optimization objective involves many individual terms . The experiments should further provide results to show how the trade-off parameters $ \\beta $ controls privacy leakage and target accuracy . 2.This work is done in an adversarial training manner , can leakage reduction be achieved in a differentially private training manner , i.e. , training the encoder using dp-sgd ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for constructive feedback . * * 1 ) \u201c How to divide the representation $ z $ encoded by the encoder into two sub-representations , namely , target and residual representations. \u201d * * The representation is divided equally , with dimensionality was chosen to be comparable to other approaches . Dividing the representation imposes no hard restriction -- as long as the latent dimensionality provides ample capacity for the downstream tasks . Furthermore , to ensure sufficient capacity and avoid degenerate solutions , we also evaluated the classification accuracy not only for the target variable on $ z_ { tar } $ but also of the sensitive variable on $ z_ { res } $ . * * 2 ) \u201c There is a high-level description of focal entropy instead of equations. \u201d * * To make the concept of focal entropy more clear , we added equations ( 9 ) and ( 10 ) to the manuscript , which details the recalibration of probabilities to achieve the off-centered maximum of the entropy . * * 3 ) `` Some notations are very confusing . For example , in the optimization goal ( Equation ( 1 ) ) , the adversarial loss on target is related to the parameter $ \\theta_ { res } $ . However , in the following description , this loss is related to $ \\theta_ { tar } $ ( Equation ( 4 ) ) '' * * We thank the reviewer for the hint . To make the manuscript clearer and avoid potential misunderstanding , we adapted the notation , added color coding , and the corresponding graphical model ( see Fig.1 ) . * * 4 ) \u201c This work is done in an adversarial training manner , can leakage reduction be achieved in a differentially private training manner , i.e. , training the encoder using dp-sgd ? \u201d * * Our method is fundamentally different from DP-SGD , as we aim to learn a private * representation * instead of preserving privacy at the * parameter * level . While we do not consider a DP framework here , our method could employ differential privacy during the post-classifier training . We emphasize that the methods dealing with DP are rather orthogonal to the direction of the proposed work . So focal-entropy can be integrated seamlessly into DP frameworks , making representation learning more robust ."}, {"review_id": "wG5XIGi6nrt-1", "review_text": "Summary : This paper gives a method in the class of learning representations which have some information censored . In particular , the authors propose a setup where there are many \u201c private \u201d classes , and some classes are more similar than others \u2013 this maps ( I think ) onto the privacy setting , where each class is like one individual . They give a modification of an entropy loss , focal entropy , which is conducive to this type of learning , and show in experiments that this method can be successful . I recommend reject due to a lack of clarity in the formulation , motivation , and writing of this paper . I think the idea has promise , the experiments are fine , and if correctly communicated can be a successful paper , but as it stands I found the paper pretty confusing and I \u2019 m not really sure what the method is for . Strong points : - The problem as I understand it is interesting and the work is well-situated in the privacy literature . - Focal entropy seems like a good idea and as far as I know is novel as a loss . - Experiments on CIFAR and CelebA demonstrate some good behaviours from this method , mostly beating baselines - Experiments are mostly good , decently thorough Weak points and Clarifications : - The main weak point of this paper is the exposition and clarity \u2013 I find that the problem setup is not explained particularly well . Especially as someone who is more familiar with the fair representation learning literature , I get confused when the authors refer to a sensitive attribute , or private part of the data , in this setting \u2013 it seems to be a different notion than I am used to and it is never clearly defined . If I squint I can see how it maps onto privacy nicely but I would prefer if the authors make that clear . - The introduction could use a rewrite \u2013 it doesn \u2019 t set up the main points of the paper particularly clearly and leaves the reader a little confused . Consider stating more clearly off the top what the problem statement is , and move much of the content of p1 to related work - The motivation of \u201c highly overlapping information \u201d is pretty imprecise . Having read the paper I can kind of see what you \u2019 re getting at but it \u2019 s not clear \u2013 I think you mean instead information hierarchy ? Sub and super classes - As far as I can tell the exact equation for focal entropy is never actually given ? Let me know if I \u2019 m wrong - Figure 3b \u2013 you mentioned the better tradeoff in the high accuracy domain , but not what appears to be the worse tradeoff in the low accuracy domain . Is there anything I \u2019 m missing about understanding this figure ? Other feedback : - P2 \u201c the solutions mentioned earlier can only meet its practical promises \u2026 \u201d \u2013 need a citation for this or explanation - P2 : explain these suboptimalities more , I \u2019 m not sure exactly what you \u2019 re referring to - P2 : you bring up a \u201c vast number of dissimilar classes \u201d without explaining why that \u2019 s relevant \u2013 in much work we deal with binary attributes/classes - Eq 4 : looks a lot like standard fair representation learning , so need to explain more clearly in motivation why it \u2019 s different - \u201c Although maximization of entropy is sufficient for nonprivate attributes to minimize information leakage across representation partitions , we postulate that proper sanitization must be conducted w.r.t to focus classes in a similarity-aware fashion \u201d \u2013 this sentence is very important to the paper , and I don \u2019 t quite understand any of it . If you work hard on this sentence ( why do you postulate that ? What is a focus class ? Why would we ever consider similarity-awareness and what is that ? ) you \u2019 ll go a long way to clarifying your intro - Top of 3.2 \u2013 need to clarify what s is - Eq 7 \u2013 this makes it look like Y_similar is a set of tuples but I think you mean set of elements of Y ? - \u201c would like the entropy peak shifted such that uniformity wrt similar classes gets dominant \u201d \u2013 this is another idea which is not being clearly communicated right now and seems central to this work - The \u201c reweighting vector \u201d is not introduced \u2013 not clear what we will do with it . You never actually show how this is included in the entropy calculation - Would like to see some sort of statistic on hubs , along with these pictures", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * \u201c Problem setup is not explained well \u201d `` Difference to the fair RL * * The proposed approach does not specifically consider the notion of fairness . However , as the concepts are related our method theoretically could also be extended in this regard , leading to exciting future research . A key difference to fairness approaches is that our approach deals with attributes exhibiting * different levels * of privateness . Specifically , the model does not have any prior info about each attribute 's \u201c privateness \u201d level . It should learn a representation , which contains information about non-sensitive attributes while refraining from encoding sensitive attributes . As far as we know , no previous work in fairness literature looked at this . In contrast to that , the notion of sensitive and non-sensitive attributes is typically given a-priori in the fairness literature . To shed more light on the correlation between privacy and individual attributes , we added the \u201c Attribute-level Privacy Analysis \u201d results to the appendix . It manifests that the more related an attribute is to the identity , i.e. , the higher the privateness , the lower the achievable accuracy ( e.g. , male and mustache ) . * * \u201c Motivation of \u201c highly overlapping information \u201d * * With overlap in information , we refer to the correlation between sensitive and non-sensitive , i.e. , $ S \\not\\perp T $ . This is crucial , as this determines the degree of possible disentanglement in the adversarial setup . Whereas CelebA does not permit such clear segregation between public and private due to the inherent correlation , CIFAR-100 does permit such a separation due to the \u2018 artificial \u2019 hierarchy between the sub and superclass . * * \u201c Fig 3b - trade-off curve \u201d * * Please , see answer * Results * to * AnonReviewer1 * . * * \u201c Exact equation for focal entropy \u201d \u201c reweighting vector is not introduced \u201d * * To make the concept of focal entropy and its optimization clearer , we added Equation ( 9 ) and ( 10 ) in the revised manuscript in Section 3 . It explicitly presents how to obtain off-centered entropy . * * Clarifying : \u201c proper sanitization must be conducted w.r.t to focus classes \u201d and \u201c entropy peak shifted such that uniformity wrt similar classes gets dominant \u201d * * The proposed approach builds upon the idea of calibrating the distribution matching for the sensitive attribute , tailoring it to a privacy setup . This is achieved by a simple yet effective modification of Shannon entropy . Shannon-entropy has its maximum in a class-agnostic fashion at uniform distribution . Disregarding the inherent structure makes optimization susceptible to shallow sanitization ( shortcuts ) . In contrast , focal entropy enforces group-wise equiprobability by dividing the probability mass . Hence , maximization with focal-entropy is analogous to an asymmetric entropy . Asymmetry leads to inverse skew-sensitivity due to matching an imbalanced distribution . The high-level notion of focal entropy is further illustrated in Fig.2.Please also see the discussion about * equilibrium * with * AnonReviewer1 * for more details . We added equations ( 9 ) and ( 10 ) in Section 3 to make the focal entropy concept clearer . * * Clarifying : \u201c Explain the suboptimality \u201d and \u201c the solutions mentioned earlier can only meet its practical promises when the private attr . do not strongly correlate with the target attr. \u201d * * Standard ARL entails maximization of two likelihoods . First , an adversarial network that seeks to classify and extract sensitive information from a given representation . Second , an encoder network seeks to infer a compact data representation while preventing sensitive information leakage . This results in a zero-sum minimax game formulation , which is practically sub-optimal from the perspective of preventing information leakage . This is due to the requirement of two conditions to be fulfilled to reach optimality : First , the existence of an equilibrium of minimax game players , and second , the practical optimization ability procedures to converge to such an equilibrium . Inability to reach equilibrium in terms of likelihoods consequently entails leaking the most amount of information . In contrast , optimizing for the uniformity in distribution over the sensitive labels ( maximization of entropies ) provides no information to the adversary and hence is less likely to leak sensitive information . However , the optimality condition of uniform log-likelihood of the predictor is predicated on the existence of an optimal discriminator ( oracle classifier ) and equilibrium between the players , which in practice is unattainable . Please see * ( Roy & Bodeti , 2019 ) * for a comprehensive discussion . * * \u201c Would like to see some sort of statistic on hubs , along with these pictures \u201d * * In the revised manuscript , we added the average node degree for each graph . Starting with conventional entropy and then continuing to focal entropy - the average node degree decreases monotonically with the growing size of k-NN . See the very end of section 4.2 ."}, {"review_id": "wG5XIGi6nrt-2", "review_text": "# # Summary - The paper tackles the problem of adversarial representation learning i.e. , to learn a low-dimensional representation of the image that encodes target-relevant non-sensitive information , but not ( often correlated ) sensitive information . - The approach follows a VAE-based adversarial framework , with the core novelty being the use of maximizing focal entropy ( i.e. , entropy among a set of similar attributes ) over sensitive attributes in the representation . - Evaluation performed on CIFAR100 and CelebA indicates that the approach outperforms similar baselines under certain conditions . # # Strengths * * 1 . Focal entropy * * - I appreciate the insight exploited by the authors -- to encourage representations with high entropy over a small set of correlated sensitive attributes * * 2 . Evaluation * * - I found the evaluation quite thorough . The paper compares with many recent baselines , demonstrates performances curves and additional analysis to explain the model 's behaviour . # # Concerns # # # Major Concerns * * 1 . Objective * * - There are a few things unclear to me in the objective ( Eq.1-6 ) and would appreciate if the authors clarified them . - ( a ) I do n't get the motivation for the two pairs of the classifiers $ ( T , S ) $ and adversarial $ ( \\tilde { T } , \\tilde { S } ) $ that tries to simultaneously minimize/maximize both the target $ p ( a|z ) $ and sensitive attributes $ p ( y|z ) $ . - ( b ) Especially , I do n't understand the reasoning behind having an adversarial loss on the target attribute -- should n't this be maximized in all cases ? - ( c ) I am also confused with the sanitization term ( Eq.6 ) .In particular of why the sensitive attribute classifier $ \\tilde { S } $ is a function of the target attribute classifier $ \\tilde { \\theta } _ { tar } $ ? - ( d ) The term $ \\phi_ { \\tilde { T } } $ appears to be undefined to complement Eq.6.- ( e ) It would also be nice if the authors extended Eq.6 ( which maximizes standard entropy ) with the final objective to maximize focal entropy . - ( f ) Is the VAE term ( Eq.5 ) necessary ( esp.reconstruction ) ? After all , this appears as a secondary objective opposed to the primary objective of learning a representation $ z $ which minimizes information leakage . * * 2.Writing - Sec.3 * * - My issues in understanding can be partly attributed to the writing in Sec.3 , which I found difficult to follow for a few reasons . - ( a ) Some terms seem to be overloaded e.g. , two notations for encoder $ q ( x ; \\theta_E ) $ and $ E ( x ; \\theta_E ) $ . I was also thrown off by many ways the components of the model/objective are conveyed : Losses $ \\phi_T $ , parameters $ \\theta_T $ , player $ T $ , adv . player $ \\hat { T } $ , etc . I recommend finetuning the second paragraph on page 4 . - ( b ) While the architecture in Fig.1 is designed to accompany the text , I find it somewhat incomplete and unclear . For one , there are six players/blocks in the proposed architecture . However , only the encoder and decoder are shown in the figure . Furthermore , while there are five loss terms , only four of them are shown in the figure . I am also not sure if the blue/red backgrounds in the residual and target streams code something in particular . Perhaps one solution is to split the figure into two : one for the architecture and another for information flow ? * * 3.Results * * - While the focal entropy makes sense to me ( i.e. , by increasing entropy over a small set of similar classes ) , I wonder if the results confidently back up that is it indeed better than related baselines . - ( a ) The improvements seem somewhat marginal e.g. , an improvement of 2 % accuracy ( Table 1 ) over prior work on both CIFAR100 and CelebA . - ( b ) But what I find more revealing of the performance is the trade-off curve in Fig.3b ( thanks for the presenting this ! ) . It appears that the proposed focal entropy approach ( blue curve ) outperforms Kernel-SARL ( red curve ) only for in a small operating range of high target accuracy . Overall it appears that MaxEnt-ARL and Kernel-SRL offers better trade-offs . # # # Minor Concerns * * 4 . Decoder/Reconstruction * * - Since the task is to partly perform reconstruction as well , I find missing evaluation on how good the reconstructed samples are . * * 5.Same equilibrium issues as before ? * * - The introduction ( second paragraph , p2 ) remarks that the adversarial min-max formulation of the problem has an issue that there is significant leakage if the optimization does not reach equilibrium . - Would n't this be an issue in the proposed work as well ? # # # Nitpicks * * 6 . Some nitpicks * * - Please label the axes in Fig.2.- Not sure what this sentence means `` increases the uncertainty in a more organic fashion '' - please rephrase - Typo in citation `` Radovanovi263 et al . ( 2010 ) ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * \u201c ( a ) Motivation for two pairs of the classifiers \u201d \u201c ( b ) Reasoning behind adv . loss on target \u201d * * The non-adversarial predictors enforce the utility and the presence of the associated information in the representation ( target information in the $ z_ { tar } $ and sensitive in $ z_ { res } $ , the adversaries inhibit undesirable information leakage across the representation partitions.Thus are responsible for disentanglement and sanitization . Moreover , as the adversaries are learned during training , they act as surrogates for oracle post-classifiers . * * ( c ) \u201c Why is $ \\tilde { S } $ a function of $ \\tilde { \\theta } _ { tar } $ ? \u201d ( d ) \u201c Adv . predictor is not defined \u201d ( e ) `` Extend Eq.5 \u201d * * We significantly modified the method section and incorporated the suggested hints . Specifically , we adapted the notation towards simpler terms and fixed typos that confused . In terms of the sanitization terms , we added Equation ( 9 ) and ( 10 ) to make the focal entropy concept and its optimization clearer . * * ( f ) VAE * * Integration of VAE into the proposed approach was done to * i ) * improve representation learning accompanied by disentanglement , and * ii ) * provide interpretability . * i ) * It has been shown that integration VAEs into representation learning in combination with supervision boosts the performance of downstream tasks ( Le et al . ( 2018 ) ; Gyawali et al . ( 2019 ) ) .We similarly observed higher predictor acc . Additionally , VAEs are known to promote disentanglement , which can be attributed to the encoder KL-term via independent priors ( Burgess et al. , ( 2018 ) ) . In our scenario , this serves the objective of separating sensitive and non-sensitive attributes into separate subspaces . * ii ) * Interpretability is crucial yet often overlooked in privacy literature . Integrating the VAE into the minimax game facilitates understanding the sanitization in a more human interpretable fashion . * * '' increases the uncertainty in a more organic fashion '' * * This refers to the removal of sensitive information in a semantic-aware and systematic fashion , leveraging the inherent class structure . Given the example of CIFAR , we want a 'whale ' to be more confused with another type of fish , rather than , e.g. , with the class \u2018 vehicles \u2019 . Doing so helps eliminate \u2018 fine-grained \u2019 features , giving rise to the guided sanitization . * * Results * * Given a correlation between the target and sensitive attributes , the minimax nature of the ARL entails a compromise between privacy and utility . The proposed approach accommodates this trade-off in a principled fashion , as changing the * '' focus '' * and the size of it ( i.e. , nearest neighbors ) directly affects the shape of the trade-off response curve ( see Figure 2 ) . In this respect , * MaxEnt-ARL * ( Roy & Bodeti , 2019 ) } which imposes uniformity in a class agnostic fashion is a special case of our proposed focal entropy . In the trade-off curve , focal-entropy performs significantly better than * MaxEnt-ARL * in the relevant operating range ( i.e. , high target acc . ) . We also outperform * Kernel-SARL * sharply in the \u2018 linear \u2019 case . In the \u2018 kernelized \u2019 case , we gained higher performance in the targeted utility domain as well ( achieving 2 % accuracy improvement at an identical privacy rate is extremely challenging ) . The \u2018 kernelized \u2019 case , however , can not be directly optimized in an end-to-end fashion , so it is not directly applicable to DNN-based solutions ( as admitted by the authors ) . Furthermore , the performance reported by both methods is obtained as non-dominated solutions and may not be directly comparable . * * \u201c leakage of information when not achieving equilibrium \u201d * * Employing * ML-ARL * is subject to information leakage due to optimization of log-likelihood for sanitization . In contrast , the theoretical optimality of MaxEnt-ARL for sanitization is predicated on the existence of an optimal discriminator ( oracle classifier ) and minimax equilibrium . Despite the theoretical optimality , the objective of the latter is unattainable in practice . Our proposed Focal-entropy method goes towards maximization of entropy by integrating MaxEnt-ARL as a special case . This is achieved trading in optimality with quasi-optimality by means of \u2018 approximating \u2019 the optimization objective . Such approximation is simply achieved by enforcing two separate uniform distributions for the \u2018 similar \u2019 and \u2018 dissimilar \u2019 classes ( see revised manuscript for a discussion ) . * * Evaluation on reconstructed samples * * The purpose of reconstruction is to improve representation learning and interpretability . As the reconstruction quality was a minor point for us , we just added some example reconstructions to the supplementary material . However , we consider reporting the Inception Score of the reconstruction of our method ( with the final version of this manuscript ) for the sake of future comparison . * * \u201c Some terms seem to be overloaded \u201d * * We adapted and , in particular , streamlined the notation to avoid misunderstanding and clutter ."}, {"review_id": "wG5XIGi6nrt-3", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The paper studies how to learn private representations that only captures the non-sensitive attributes of the dataset . They propose an adversarial representation learning method that employs VAEs . Specifically , the architecture in the VAE contains 6 players with 2 as adversarial classifiers . They introduce focal entropy as the objective function instead of entropy for adversarial classifiers to achieve deep sanitization . They empirically evaluate the method by reporting the target task accuracy and attribute inference accuracy on two datasets . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : I like the paper largely . It provides a nice practical method for sanitization , although it is hard to give theoretical privacy guarantees of VAEs . The method has shown to be a good defense for individual attribute inference attacks . My main concern is the experimental results are only on two datasets with one task/sensitive attribute setting . As an empirical/methodology paper , I would expect more empirical results . Also , I have a question regarding the sanitization section . It 's not clear if focal entropy should be used in both $ \\tilde { T } $ and $ \\tilde { S } $ . The paper mentioned `` training of $ \\tilde { T } $ leverages a modification of entropy ... '' However , I think it should be $ \\tilde { S } $ or both . It seems that Equation ( 6 ) should be negative KL divergence since we want to force the distribution to be close to the uniform distribution . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Minor Comments : 1 . In Equation ( 6 ) , the bracket should be after $ \\theta $ . 2.I do n't think $ \\pi $ is clearly mentioned before Equation ( 9 ) . The notation should be more precise . 3.I 'm guessing it can also protect the dataset-level ( proprietary ) attribute inference . I 'm interested in seeing the results , but it is not required for this submission .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the valuable and constructive feedback . We incorporated the feedback of the review in the revised manuscript . * * 1 ) \u201c My main concern is the experimental results are only on two datasets [ ... ] '' * * We perform an extensive set of experiments on two standard benchmarks . Please also see supplementary material . As the first testbed , we adopt the \u201c simulated \u201d privacy problem proposed by Roy & Boddeti ( 2019 ) designed on the CIFAR-100 dataset . We picked this benchmark as it captures a \u201c perfect \u201d separation between target and sensitive information , i.e. , $ S \\perp T $ . We compare with state of the art adversarial and entropy-based representation learning methods for privacy on this benchmark . Second , the CelebA dataset where no clear segregation between target and residual spaces is possible , i.e. , $ S \\not\\perp T $ . The \u201c in-the-wild \u201d nature of face images offers a richer testbed for our method as both identities , and contingent factors are significant sources of variation . We compare with many recent baselines on this challenging benchmark , demonstrate performance curves , and additional analysis to explain the model 's behavior ( as pointed out by other reviewers too ) . We agree that having more \u201c large-scale \u201d datasets for the evaluation would be better to substantiate the claim of the empirical effectiveness of the proposed method . However , such benchmarks are still pretty limited , which can largely be attributed to the nature of the topic -- privacy . We expect more of such datasets to emerge as the topic lately has enjoyed increased attention in the community . Nevertheless , we are convinced that the results with the proposed approach would show the same pattern on other datasets . To the best of our knowledge , our paper is the first work that proposes to taking the class similarity into account for the entropy of an adversary . * * 2 ) \u201c I 'm guessing it can also protect the dataset-level ( proprietary ) attribute inference . I 'm interested in seeing the results [ ... ] \u201d * * We thank the reviewer for the suggestion and agree on this point . In this regard , we added an analysis of the correlation between privacy and individual attributes on CelebA . In line with the reviewer 's expectation , our attribute-level analysis shows that our method achieves relatively lower target accuracy for dataset-level proprietary attributes compared to the more generic attributes . See the \u201c Attribute-level Privacy Analysis \u201d section in the supplementary material . * * 3 ) \u201c It 's not clear if focal entropy should be used in both $ \\tilde { T } $ and $ \\tilde { S } $ . The paper mentioned `` training of $ \\tilde { T } $ leverages a modification of entropy \u201d * * Whereas we want the target representation to be sanitized w.r.t.sensitive attributes , we \u201c only \u201d enforce disentanglement w.r.t.target attributes on the residual representation . As the target attributes on the residual representation are of no privacy relevance , we leverage only conventional entropy Equation ( 5 ) to enforce disentanglement . Whereas for sanitization of the target representation , we leverage focal-entropy Equation ( 9 ) , ( 10 ) . * * 4 ) \u201c I do n't think \u03c0 is clearly mentioned before Equation ( 9 ) . The notation should be more precise. \u201d * * To make the focal entropy concept and its optimization clearer , we added Equations ( 9 ) and ( 10 ) in the revised manuscript . As requested , we further elaborate on the transformation of probabilities and the final objective to maximize focal entropy ."}], "0": {"review_id": "wG5XIGi6nrt-0", "review_text": "# # # Summary This paper presented a method for learning private representations . This method is based on adversarial representation learning and the main technique contribution comes from focal entropy . The experimental results show that focal entropy can improve the accuracy of the target predictor without increasing adversarial accuracy . # # # Pros 1 . Focal entropy is effective in reducing the information leakage of the learned representation while improves the target accuracy over the state-of-the-art . 2.This paper covers a wide range of experiments . # # # Cons 1 . Some technique details are not presented clearly . I have listed some of them here : a . How to divide the representation $ z $ encoded by the encoder into two sub-representations , namely , target and residual representations . b.There is a high-level description of focal entropy instead of equations . 2.Some notations are very confusing . For example , in the optimization goal ( Equation ( 2 ) ) , the adversarial loss $ \\phi_ { tilde { S } } $ is related to the parameter $ \\theta_ { res } $ . However , in the following description , this loss is related to $ \\theta_ { tar } $ ( Equation ( 6 ) ) # # # Comments 1 . The optimization objective involves many individual terms . The experiments should further provide results to show how the trade-off parameters $ \\beta $ controls privacy leakage and target accuracy . 2.This work is done in an adversarial training manner , can leakage reduction be achieved in a differentially private training manner , i.e. , training the encoder using dp-sgd ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for constructive feedback . * * 1 ) \u201c How to divide the representation $ z $ encoded by the encoder into two sub-representations , namely , target and residual representations. \u201d * * The representation is divided equally , with dimensionality was chosen to be comparable to other approaches . Dividing the representation imposes no hard restriction -- as long as the latent dimensionality provides ample capacity for the downstream tasks . Furthermore , to ensure sufficient capacity and avoid degenerate solutions , we also evaluated the classification accuracy not only for the target variable on $ z_ { tar } $ but also of the sensitive variable on $ z_ { res } $ . * * 2 ) \u201c There is a high-level description of focal entropy instead of equations. \u201d * * To make the concept of focal entropy more clear , we added equations ( 9 ) and ( 10 ) to the manuscript , which details the recalibration of probabilities to achieve the off-centered maximum of the entropy . * * 3 ) `` Some notations are very confusing . For example , in the optimization goal ( Equation ( 1 ) ) , the adversarial loss on target is related to the parameter $ \\theta_ { res } $ . However , in the following description , this loss is related to $ \\theta_ { tar } $ ( Equation ( 4 ) ) '' * * We thank the reviewer for the hint . To make the manuscript clearer and avoid potential misunderstanding , we adapted the notation , added color coding , and the corresponding graphical model ( see Fig.1 ) . * * 4 ) \u201c This work is done in an adversarial training manner , can leakage reduction be achieved in a differentially private training manner , i.e. , training the encoder using dp-sgd ? \u201d * * Our method is fundamentally different from DP-SGD , as we aim to learn a private * representation * instead of preserving privacy at the * parameter * level . While we do not consider a DP framework here , our method could employ differential privacy during the post-classifier training . We emphasize that the methods dealing with DP are rather orthogonal to the direction of the proposed work . So focal-entropy can be integrated seamlessly into DP frameworks , making representation learning more robust ."}, "1": {"review_id": "wG5XIGi6nrt-1", "review_text": "Summary : This paper gives a method in the class of learning representations which have some information censored . In particular , the authors propose a setup where there are many \u201c private \u201d classes , and some classes are more similar than others \u2013 this maps ( I think ) onto the privacy setting , where each class is like one individual . They give a modification of an entropy loss , focal entropy , which is conducive to this type of learning , and show in experiments that this method can be successful . I recommend reject due to a lack of clarity in the formulation , motivation , and writing of this paper . I think the idea has promise , the experiments are fine , and if correctly communicated can be a successful paper , but as it stands I found the paper pretty confusing and I \u2019 m not really sure what the method is for . Strong points : - The problem as I understand it is interesting and the work is well-situated in the privacy literature . - Focal entropy seems like a good idea and as far as I know is novel as a loss . - Experiments on CIFAR and CelebA demonstrate some good behaviours from this method , mostly beating baselines - Experiments are mostly good , decently thorough Weak points and Clarifications : - The main weak point of this paper is the exposition and clarity \u2013 I find that the problem setup is not explained particularly well . Especially as someone who is more familiar with the fair representation learning literature , I get confused when the authors refer to a sensitive attribute , or private part of the data , in this setting \u2013 it seems to be a different notion than I am used to and it is never clearly defined . If I squint I can see how it maps onto privacy nicely but I would prefer if the authors make that clear . - The introduction could use a rewrite \u2013 it doesn \u2019 t set up the main points of the paper particularly clearly and leaves the reader a little confused . Consider stating more clearly off the top what the problem statement is , and move much of the content of p1 to related work - The motivation of \u201c highly overlapping information \u201d is pretty imprecise . Having read the paper I can kind of see what you \u2019 re getting at but it \u2019 s not clear \u2013 I think you mean instead information hierarchy ? Sub and super classes - As far as I can tell the exact equation for focal entropy is never actually given ? Let me know if I \u2019 m wrong - Figure 3b \u2013 you mentioned the better tradeoff in the high accuracy domain , but not what appears to be the worse tradeoff in the low accuracy domain . Is there anything I \u2019 m missing about understanding this figure ? Other feedback : - P2 \u201c the solutions mentioned earlier can only meet its practical promises \u2026 \u201d \u2013 need a citation for this or explanation - P2 : explain these suboptimalities more , I \u2019 m not sure exactly what you \u2019 re referring to - P2 : you bring up a \u201c vast number of dissimilar classes \u201d without explaining why that \u2019 s relevant \u2013 in much work we deal with binary attributes/classes - Eq 4 : looks a lot like standard fair representation learning , so need to explain more clearly in motivation why it \u2019 s different - \u201c Although maximization of entropy is sufficient for nonprivate attributes to minimize information leakage across representation partitions , we postulate that proper sanitization must be conducted w.r.t to focus classes in a similarity-aware fashion \u201d \u2013 this sentence is very important to the paper , and I don \u2019 t quite understand any of it . If you work hard on this sentence ( why do you postulate that ? What is a focus class ? Why would we ever consider similarity-awareness and what is that ? ) you \u2019 ll go a long way to clarifying your intro - Top of 3.2 \u2013 need to clarify what s is - Eq 7 \u2013 this makes it look like Y_similar is a set of tuples but I think you mean set of elements of Y ? - \u201c would like the entropy peak shifted such that uniformity wrt similar classes gets dominant \u201d \u2013 this is another idea which is not being clearly communicated right now and seems central to this work - The \u201c reweighting vector \u201d is not introduced \u2013 not clear what we will do with it . You never actually show how this is included in the entropy calculation - Would like to see some sort of statistic on hubs , along with these pictures", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * \u201c Problem setup is not explained well \u201d `` Difference to the fair RL * * The proposed approach does not specifically consider the notion of fairness . However , as the concepts are related our method theoretically could also be extended in this regard , leading to exciting future research . A key difference to fairness approaches is that our approach deals with attributes exhibiting * different levels * of privateness . Specifically , the model does not have any prior info about each attribute 's \u201c privateness \u201d level . It should learn a representation , which contains information about non-sensitive attributes while refraining from encoding sensitive attributes . As far as we know , no previous work in fairness literature looked at this . In contrast to that , the notion of sensitive and non-sensitive attributes is typically given a-priori in the fairness literature . To shed more light on the correlation between privacy and individual attributes , we added the \u201c Attribute-level Privacy Analysis \u201d results to the appendix . It manifests that the more related an attribute is to the identity , i.e. , the higher the privateness , the lower the achievable accuracy ( e.g. , male and mustache ) . * * \u201c Motivation of \u201c highly overlapping information \u201d * * With overlap in information , we refer to the correlation between sensitive and non-sensitive , i.e. , $ S \\not\\perp T $ . This is crucial , as this determines the degree of possible disentanglement in the adversarial setup . Whereas CelebA does not permit such clear segregation between public and private due to the inherent correlation , CIFAR-100 does permit such a separation due to the \u2018 artificial \u2019 hierarchy between the sub and superclass . * * \u201c Fig 3b - trade-off curve \u201d * * Please , see answer * Results * to * AnonReviewer1 * . * * \u201c Exact equation for focal entropy \u201d \u201c reweighting vector is not introduced \u201d * * To make the concept of focal entropy and its optimization clearer , we added Equation ( 9 ) and ( 10 ) in the revised manuscript in Section 3 . It explicitly presents how to obtain off-centered entropy . * * Clarifying : \u201c proper sanitization must be conducted w.r.t to focus classes \u201d and \u201c entropy peak shifted such that uniformity wrt similar classes gets dominant \u201d * * The proposed approach builds upon the idea of calibrating the distribution matching for the sensitive attribute , tailoring it to a privacy setup . This is achieved by a simple yet effective modification of Shannon entropy . Shannon-entropy has its maximum in a class-agnostic fashion at uniform distribution . Disregarding the inherent structure makes optimization susceptible to shallow sanitization ( shortcuts ) . In contrast , focal entropy enforces group-wise equiprobability by dividing the probability mass . Hence , maximization with focal-entropy is analogous to an asymmetric entropy . Asymmetry leads to inverse skew-sensitivity due to matching an imbalanced distribution . The high-level notion of focal entropy is further illustrated in Fig.2.Please also see the discussion about * equilibrium * with * AnonReviewer1 * for more details . We added equations ( 9 ) and ( 10 ) in Section 3 to make the focal entropy concept clearer . * * Clarifying : \u201c Explain the suboptimality \u201d and \u201c the solutions mentioned earlier can only meet its practical promises when the private attr . do not strongly correlate with the target attr. \u201d * * Standard ARL entails maximization of two likelihoods . First , an adversarial network that seeks to classify and extract sensitive information from a given representation . Second , an encoder network seeks to infer a compact data representation while preventing sensitive information leakage . This results in a zero-sum minimax game formulation , which is practically sub-optimal from the perspective of preventing information leakage . This is due to the requirement of two conditions to be fulfilled to reach optimality : First , the existence of an equilibrium of minimax game players , and second , the practical optimization ability procedures to converge to such an equilibrium . Inability to reach equilibrium in terms of likelihoods consequently entails leaking the most amount of information . In contrast , optimizing for the uniformity in distribution over the sensitive labels ( maximization of entropies ) provides no information to the adversary and hence is less likely to leak sensitive information . However , the optimality condition of uniform log-likelihood of the predictor is predicated on the existence of an optimal discriminator ( oracle classifier ) and equilibrium between the players , which in practice is unattainable . Please see * ( Roy & Bodeti , 2019 ) * for a comprehensive discussion . * * \u201c Would like to see some sort of statistic on hubs , along with these pictures \u201d * * In the revised manuscript , we added the average node degree for each graph . Starting with conventional entropy and then continuing to focal entropy - the average node degree decreases monotonically with the growing size of k-NN . See the very end of section 4.2 ."}, "2": {"review_id": "wG5XIGi6nrt-2", "review_text": "# # Summary - The paper tackles the problem of adversarial representation learning i.e. , to learn a low-dimensional representation of the image that encodes target-relevant non-sensitive information , but not ( often correlated ) sensitive information . - The approach follows a VAE-based adversarial framework , with the core novelty being the use of maximizing focal entropy ( i.e. , entropy among a set of similar attributes ) over sensitive attributes in the representation . - Evaluation performed on CIFAR100 and CelebA indicates that the approach outperforms similar baselines under certain conditions . # # Strengths * * 1 . Focal entropy * * - I appreciate the insight exploited by the authors -- to encourage representations with high entropy over a small set of correlated sensitive attributes * * 2 . Evaluation * * - I found the evaluation quite thorough . The paper compares with many recent baselines , demonstrates performances curves and additional analysis to explain the model 's behaviour . # # Concerns # # # Major Concerns * * 1 . Objective * * - There are a few things unclear to me in the objective ( Eq.1-6 ) and would appreciate if the authors clarified them . - ( a ) I do n't get the motivation for the two pairs of the classifiers $ ( T , S ) $ and adversarial $ ( \\tilde { T } , \\tilde { S } ) $ that tries to simultaneously minimize/maximize both the target $ p ( a|z ) $ and sensitive attributes $ p ( y|z ) $ . - ( b ) Especially , I do n't understand the reasoning behind having an adversarial loss on the target attribute -- should n't this be maximized in all cases ? - ( c ) I am also confused with the sanitization term ( Eq.6 ) .In particular of why the sensitive attribute classifier $ \\tilde { S } $ is a function of the target attribute classifier $ \\tilde { \\theta } _ { tar } $ ? - ( d ) The term $ \\phi_ { \\tilde { T } } $ appears to be undefined to complement Eq.6.- ( e ) It would also be nice if the authors extended Eq.6 ( which maximizes standard entropy ) with the final objective to maximize focal entropy . - ( f ) Is the VAE term ( Eq.5 ) necessary ( esp.reconstruction ) ? After all , this appears as a secondary objective opposed to the primary objective of learning a representation $ z $ which minimizes information leakage . * * 2.Writing - Sec.3 * * - My issues in understanding can be partly attributed to the writing in Sec.3 , which I found difficult to follow for a few reasons . - ( a ) Some terms seem to be overloaded e.g. , two notations for encoder $ q ( x ; \\theta_E ) $ and $ E ( x ; \\theta_E ) $ . I was also thrown off by many ways the components of the model/objective are conveyed : Losses $ \\phi_T $ , parameters $ \\theta_T $ , player $ T $ , adv . player $ \\hat { T } $ , etc . I recommend finetuning the second paragraph on page 4 . - ( b ) While the architecture in Fig.1 is designed to accompany the text , I find it somewhat incomplete and unclear . For one , there are six players/blocks in the proposed architecture . However , only the encoder and decoder are shown in the figure . Furthermore , while there are five loss terms , only four of them are shown in the figure . I am also not sure if the blue/red backgrounds in the residual and target streams code something in particular . Perhaps one solution is to split the figure into two : one for the architecture and another for information flow ? * * 3.Results * * - While the focal entropy makes sense to me ( i.e. , by increasing entropy over a small set of similar classes ) , I wonder if the results confidently back up that is it indeed better than related baselines . - ( a ) The improvements seem somewhat marginal e.g. , an improvement of 2 % accuracy ( Table 1 ) over prior work on both CIFAR100 and CelebA . - ( b ) But what I find more revealing of the performance is the trade-off curve in Fig.3b ( thanks for the presenting this ! ) . It appears that the proposed focal entropy approach ( blue curve ) outperforms Kernel-SARL ( red curve ) only for in a small operating range of high target accuracy . Overall it appears that MaxEnt-ARL and Kernel-SRL offers better trade-offs . # # # Minor Concerns * * 4 . Decoder/Reconstruction * * - Since the task is to partly perform reconstruction as well , I find missing evaluation on how good the reconstructed samples are . * * 5.Same equilibrium issues as before ? * * - The introduction ( second paragraph , p2 ) remarks that the adversarial min-max formulation of the problem has an issue that there is significant leakage if the optimization does not reach equilibrium . - Would n't this be an issue in the proposed work as well ? # # # Nitpicks * * 6 . Some nitpicks * * - Please label the axes in Fig.2.- Not sure what this sentence means `` increases the uncertainty in a more organic fashion '' - please rephrase - Typo in citation `` Radovanovi263 et al . ( 2010 ) ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * \u201c ( a ) Motivation for two pairs of the classifiers \u201d \u201c ( b ) Reasoning behind adv . loss on target \u201d * * The non-adversarial predictors enforce the utility and the presence of the associated information in the representation ( target information in the $ z_ { tar } $ and sensitive in $ z_ { res } $ , the adversaries inhibit undesirable information leakage across the representation partitions.Thus are responsible for disentanglement and sanitization . Moreover , as the adversaries are learned during training , they act as surrogates for oracle post-classifiers . * * ( c ) \u201c Why is $ \\tilde { S } $ a function of $ \\tilde { \\theta } _ { tar } $ ? \u201d ( d ) \u201c Adv . predictor is not defined \u201d ( e ) `` Extend Eq.5 \u201d * * We significantly modified the method section and incorporated the suggested hints . Specifically , we adapted the notation towards simpler terms and fixed typos that confused . In terms of the sanitization terms , we added Equation ( 9 ) and ( 10 ) to make the focal entropy concept and its optimization clearer . * * ( f ) VAE * * Integration of VAE into the proposed approach was done to * i ) * improve representation learning accompanied by disentanglement , and * ii ) * provide interpretability . * i ) * It has been shown that integration VAEs into representation learning in combination with supervision boosts the performance of downstream tasks ( Le et al . ( 2018 ) ; Gyawali et al . ( 2019 ) ) .We similarly observed higher predictor acc . Additionally , VAEs are known to promote disentanglement , which can be attributed to the encoder KL-term via independent priors ( Burgess et al. , ( 2018 ) ) . In our scenario , this serves the objective of separating sensitive and non-sensitive attributes into separate subspaces . * ii ) * Interpretability is crucial yet often overlooked in privacy literature . Integrating the VAE into the minimax game facilitates understanding the sanitization in a more human interpretable fashion . * * '' increases the uncertainty in a more organic fashion '' * * This refers to the removal of sensitive information in a semantic-aware and systematic fashion , leveraging the inherent class structure . Given the example of CIFAR , we want a 'whale ' to be more confused with another type of fish , rather than , e.g. , with the class \u2018 vehicles \u2019 . Doing so helps eliminate \u2018 fine-grained \u2019 features , giving rise to the guided sanitization . * * Results * * Given a correlation between the target and sensitive attributes , the minimax nature of the ARL entails a compromise between privacy and utility . The proposed approach accommodates this trade-off in a principled fashion , as changing the * '' focus '' * and the size of it ( i.e. , nearest neighbors ) directly affects the shape of the trade-off response curve ( see Figure 2 ) . In this respect , * MaxEnt-ARL * ( Roy & Bodeti , 2019 ) } which imposes uniformity in a class agnostic fashion is a special case of our proposed focal entropy . In the trade-off curve , focal-entropy performs significantly better than * MaxEnt-ARL * in the relevant operating range ( i.e. , high target acc . ) . We also outperform * Kernel-SARL * sharply in the \u2018 linear \u2019 case . In the \u2018 kernelized \u2019 case , we gained higher performance in the targeted utility domain as well ( achieving 2 % accuracy improvement at an identical privacy rate is extremely challenging ) . The \u2018 kernelized \u2019 case , however , can not be directly optimized in an end-to-end fashion , so it is not directly applicable to DNN-based solutions ( as admitted by the authors ) . Furthermore , the performance reported by both methods is obtained as non-dominated solutions and may not be directly comparable . * * \u201c leakage of information when not achieving equilibrium \u201d * * Employing * ML-ARL * is subject to information leakage due to optimization of log-likelihood for sanitization . In contrast , the theoretical optimality of MaxEnt-ARL for sanitization is predicated on the existence of an optimal discriminator ( oracle classifier ) and minimax equilibrium . Despite the theoretical optimality , the objective of the latter is unattainable in practice . Our proposed Focal-entropy method goes towards maximization of entropy by integrating MaxEnt-ARL as a special case . This is achieved trading in optimality with quasi-optimality by means of \u2018 approximating \u2019 the optimization objective . Such approximation is simply achieved by enforcing two separate uniform distributions for the \u2018 similar \u2019 and \u2018 dissimilar \u2019 classes ( see revised manuscript for a discussion ) . * * Evaluation on reconstructed samples * * The purpose of reconstruction is to improve representation learning and interpretability . As the reconstruction quality was a minor point for us , we just added some example reconstructions to the supplementary material . However , we consider reporting the Inception Score of the reconstruction of our method ( with the final version of this manuscript ) for the sake of future comparison . * * \u201c Some terms seem to be overloaded \u201d * * We adapted and , in particular , streamlined the notation to avoid misunderstanding and clutter ."}, "3": {"review_id": "wG5XIGi6nrt-3", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The paper studies how to learn private representations that only captures the non-sensitive attributes of the dataset . They propose an adversarial representation learning method that employs VAEs . Specifically , the architecture in the VAE contains 6 players with 2 as adversarial classifiers . They introduce focal entropy as the objective function instead of entropy for adversarial classifiers to achieve deep sanitization . They empirically evaluate the method by reporting the target task accuracy and attribute inference accuracy on two datasets . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : I like the paper largely . It provides a nice practical method for sanitization , although it is hard to give theoretical privacy guarantees of VAEs . The method has shown to be a good defense for individual attribute inference attacks . My main concern is the experimental results are only on two datasets with one task/sensitive attribute setting . As an empirical/methodology paper , I would expect more empirical results . Also , I have a question regarding the sanitization section . It 's not clear if focal entropy should be used in both $ \\tilde { T } $ and $ \\tilde { S } $ . The paper mentioned `` training of $ \\tilde { T } $ leverages a modification of entropy ... '' However , I think it should be $ \\tilde { S } $ or both . It seems that Equation ( 6 ) should be negative KL divergence since we want to force the distribution to be close to the uniform distribution . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Minor Comments : 1 . In Equation ( 6 ) , the bracket should be after $ \\theta $ . 2.I do n't think $ \\pi $ is clearly mentioned before Equation ( 9 ) . The notation should be more precise . 3.I 'm guessing it can also protect the dataset-level ( proprietary ) attribute inference . I 'm interested in seeing the results , but it is not required for this submission .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the valuable and constructive feedback . We incorporated the feedback of the review in the revised manuscript . * * 1 ) \u201c My main concern is the experimental results are only on two datasets [ ... ] '' * * We perform an extensive set of experiments on two standard benchmarks . Please also see supplementary material . As the first testbed , we adopt the \u201c simulated \u201d privacy problem proposed by Roy & Boddeti ( 2019 ) designed on the CIFAR-100 dataset . We picked this benchmark as it captures a \u201c perfect \u201d separation between target and sensitive information , i.e. , $ S \\perp T $ . We compare with state of the art adversarial and entropy-based representation learning methods for privacy on this benchmark . Second , the CelebA dataset where no clear segregation between target and residual spaces is possible , i.e. , $ S \\not\\perp T $ . The \u201c in-the-wild \u201d nature of face images offers a richer testbed for our method as both identities , and contingent factors are significant sources of variation . We compare with many recent baselines on this challenging benchmark , demonstrate performance curves , and additional analysis to explain the model 's behavior ( as pointed out by other reviewers too ) . We agree that having more \u201c large-scale \u201d datasets for the evaluation would be better to substantiate the claim of the empirical effectiveness of the proposed method . However , such benchmarks are still pretty limited , which can largely be attributed to the nature of the topic -- privacy . We expect more of such datasets to emerge as the topic lately has enjoyed increased attention in the community . Nevertheless , we are convinced that the results with the proposed approach would show the same pattern on other datasets . To the best of our knowledge , our paper is the first work that proposes to taking the class similarity into account for the entropy of an adversary . * * 2 ) \u201c I 'm guessing it can also protect the dataset-level ( proprietary ) attribute inference . I 'm interested in seeing the results [ ... ] \u201d * * We thank the reviewer for the suggestion and agree on this point . In this regard , we added an analysis of the correlation between privacy and individual attributes on CelebA . In line with the reviewer 's expectation , our attribute-level analysis shows that our method achieves relatively lower target accuracy for dataset-level proprietary attributes compared to the more generic attributes . See the \u201c Attribute-level Privacy Analysis \u201d section in the supplementary material . * * 3 ) \u201c It 's not clear if focal entropy should be used in both $ \\tilde { T } $ and $ \\tilde { S } $ . The paper mentioned `` training of $ \\tilde { T } $ leverages a modification of entropy \u201d * * Whereas we want the target representation to be sanitized w.r.t.sensitive attributes , we \u201c only \u201d enforce disentanglement w.r.t.target attributes on the residual representation . As the target attributes on the residual representation are of no privacy relevance , we leverage only conventional entropy Equation ( 5 ) to enforce disentanglement . Whereas for sanitization of the target representation , we leverage focal-entropy Equation ( 9 ) , ( 10 ) . * * 4 ) \u201c I do n't think \u03c0 is clearly mentioned before Equation ( 9 ) . The notation should be more precise. \u201d * * To make the focal entropy concept and its optimization clearer , we added Equations ( 9 ) and ( 10 ) in the revised manuscript . As requested , we further elaborate on the transformation of probabilities and the final objective to maximize focal entropy ."}}