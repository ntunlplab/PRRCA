{"year": "2019", "forum": "HkekMnR5Ym", "title": "Meta-Learning Neural Bloom Filters", "decision": "Reject", "meta_review": "This work proposes and interesting approach to learn approximate set membership. While the proposed architecture is rather closely related to existing work, it is still interesting, as recognized by reviewers. Authors's substantial rewrites has also helped make the paper clearer. However, the empirical merits of the approach are still a bit limited; when combined with the narrow novelty compared to existing work, this makes the overall contribution a bit too thin for ICLR. Authors are encouraged to strengthen their work by showing more convincing practical benefit of their approach.", "reviews": [{"review_id": "HkekMnR5Ym-0", "review_text": "The paper proposes a learnable bloom filter architecture. While the details of the architecture seemed a bit too complicated for me to grasp (see more on this later), via experiments the authors show that the learned bloom filters are more compact that regular bloom filters and can outperform other neural architectures when it comes to retrieving seen items. A bloom filter is fairly simple, K hash functions hash seen items into K bit vectors. During retrieval, if all of the bits hashed to are 1 then we say we've seen the query. I think there's simpler ways to derive a continuous, differentiable version of this which begs the question why the authors chose a relatively more elaborate architecture involving ZCA transform and first/second moments. Perhaps the authors need to motivate their architecture a bit better. In their experiments, a simple LSTM seems to perform remarkably well (it is close to the best in 2 (a), (b); and crashes in (c) but the proposed technique is also outperformed by vanilla bloom filters in (c)). This is not surprising to me since LSTMs are remarkably good at remembering patterns. Perhaps the authors would like to comment on why they did not develop the LSTM further to remedy it of its shortcomings. Some of the positive results attained using neural bloom filters is a bit tempered by the fact that the experiments were using a back up bloom filter. Also, the neural bloom filters do well only when there is some sort of querying pattern. All of these details would seem to reduce the applicability of the proposed approach. The authors have addressed most (if not all) of my comments in their revised version. I applaud the authors for being particularly responsive. Their explanations and additional experiments go a long way towards lending the insights that were missing from the original draft of the paper. I have upped my rating to a 7.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review , and for your keen eye to detail . Re. \u201c why not develop further an LSTM \u201d . We are principally interested in whether it is possible to learn a compressive set membership data-structure in one-shot . Because many applications of Bloom Filters are in highly dynamic settings ( e.g.databases ) , the requirement that a network may be able to beat a Bloom Filter with only a single computational pass over the data is quite important . It wasn \u2019 t clear from the beginning of this research project ( to us and our peers ) whether it would be possible , and if so - in what setting . Thus we feel that it would be a worthwhile scientific contribution to show this is the case with any model -- - even an LSTM . Firstly , it is worth noting the LSTM is non-trivially less efficient for the database task even when the sequence length is quite short . But the real issue with an LSTM and other RNNs ( partially covered in the Reviewer 3 response ) is that it they are difficult to scale to larger set sizes , because one has to BPTT over the entire input sequence linearly ( elements of our storage size S ) during training . Say S contains 5,000 elements\u2026 One would have to train with sequences of length 5,000 , insert all elements sequentially and BPTT over the 5,000 long sequence . Training is way too slow , and the optimization problem becomes intractable ( network fails to learn ) . Furthermore the LSTM has quadratic computation cost with respect to the hidden state size . Since set membership is order-invariant , it seemed preferable to try out a memory architecture which does not rely on sequential computation and BPTT ( like a memory network ) that is still very compressive ( unlike a memory network ) . Our mistake in the original exposition , which you rightly point out , is that we have presented our solution ( the architecture ) without much of the motivation that lead to its incarnation . We have re-written the model section to remedy this . But we will also briefly state the model motivation here : - We want a simple write scheme and no BPTT - > additive write . ( it \u2019 s order-invariant , and alike to the Bloom Filter \u2019 s logical-or write ) . - We want the network to choose where to write , as well as what to write - > address is a softmax over memory based on content . ( alike to the Kanerva Machine Wu et al . ( 2018 ) ) - We want the network \u2019 s network trainable parameters to be small and independent of memory size - > make the addressing matrix A non-trainable . - We want the addressing to be efficient - > make it sparse ( alike to Rae et al . ( 2016 ) ) .- We found a sparse address led to the network fixating on a subset of memory - > whiten the query vector . Whitening ( or sphering ) may appear complex but was only necessary if one adopts the sparse attention for efficiency . We implemented it in four lines of TensorFlow code , so at least it is not too complex from an engineering standpoint . Whitening has been used within deep learning literature before , e.g. \u201c natural neural networks \u201d [ 1 ] . An alternative to whitening would be to use a \u201c flow \u201d such as real NVP [ 2 ] which actually transforms the query to something which appears to be truly gaussian . Crucially , this was a trick to get sparse attention working , if one wishes to avoid sparse attention and just use the full softmax over memory then this side-detail of whitening can be ignored . -- Re. \u201c Also , the neural bloom filters do well only when there is some sort of querying pattern . All of these details would seem to reduce the applicability of the proposed approach. \u201d Fortunately the proposed approach does well if there is structure to the query pattern * or * storage set . In the case of the database task , our queries are picked uniformly from the universe -- - there is not much structure . However there is structure to the storage sets ( which represent row keys in an disk file within a database ) and this is why our approach outperforms the classical data-structures so significantly . More generally we think the research area of using neural networks to replace data-structures , in this case a bloom filter , is so exciting because ( we would argue ) they are very rarely applied to data that contains no structure . Using a neural network to exploit redundancy and save space feels like a very impactful thing to do , and thought leaders within Computer Science ( e.g.Jeff Dean , a co-author of the kraska et al.2018 paper ) appear to believe so . There are patterns to the rowkey schema that is used within our databases , there are patterns to blacklist URLs and IPs within our firewalls , there are patterns to our search queries . We have re-written the model and experiment section to address your concerns ! [ 1 ] https : //deepmind.com/research/publications/natural-neural-networks/ [ 2 ] https : //arxiv.org/abs/1605.08803"}, {"review_id": "HkekMnR5Ym-1", "review_text": "SUMMARY The paper proposes a neural network based architecture to solve the approximate set membership problem, in the distributional setting where the in-set and out-of-set elements come from two unknown and possibly different distributions. COMMENTARY The topic of the paper is interesting, and falls into the popular trend of enhancing classical data structures with learning algorithms. For the approximate set membership problem, this approach was already suggested by (Kraska et al. 2018) and studied further in (Mitzenmacher 2018a,b). The difference in the current paper is that the proposed approach relies on \"meta-learning\", apparently to facilitate online training and/or learning across multiple sets arising from the same distribution; this is what I gather from the introduction, even though as I write below, I feel this point is not properly explained. My main issue with the paper is that its conceptual contribution seems limited and unclear. It suggests a specific architecture whose details seem mostly arbitrary, or at least this is the impression the reader is left with, as the paper does rather little in terms of discussing and motivating them or putting them in context. Moreover, since the solution ultimately relies on a backup Bloom Filter as in (Kraska et al. 2018), it is hard to not view it as just an instantiation of the model in (Kraska et al. 2018, Mitzenmacher 2018a) with a different plugging of learning component. It would help to flesh out and highlight what the authors claim are the main insights of the paper. Another issue I suggest revising pertains to the writing. The problem setting is only loosely sketched but not properly defined. How exactly do different subsets coming into play? Specifically, the term \"meta-learning\" appears in the title and throughout the paper, but is never defined or explained. The authors should write out what exactly they mean by this notion and what role it plays in the paper. This is important since to my understanding, this is the main point of departure from the aforementioned recent works on learning-enhanced Bloom Filters. The experiments do not seem to make a strong case for the empirical advantage of the Neural Bloom Filter. They show little to no improvement on the MNIST tasks, and some improvement on a non-standard database related task. One interesting thing to look at would be the workload partition between the learning component and the backup filter, meaning what is the rate of false negatives emitted by the former and caught by the latter, and how the space usage breaks down between them (vis-a-vis the formula in Appendix B). For example, it seems plausible that on the class familiarity task, the learning component simply learns to be a binary classifier for the chosen two MNIST classes and mostly ignores the backup filter, whereas in the uniform distribution setting, the learning component only memorizes a small number of true and false positives and defers almost the entire task to the backup filter. I am not sure what to expect on the intermediate exponential distribution task. Other comments/questions: 1. For the classical Bloom Filter, do the results reported in the experimental plots reflect the empirical false-positive rate measured in the experiment, or just the analytic bound? 2. On that note, it is worth noting that the false positive rate of the classical Bloom Filter is different than the one you report for the neural-net based architectures. The Bloom Filter FP probability is over its internal randomness (i.e. its hash functions) and is independent of the distribution of queries, which need not be randomized at all. For the neural-net based architectures, the measured FP rate is w.r.t. a specific distribution of queries. See the discussion in (Mitzenmacher 2018a), sections B-C. 3. The works (Mitzenmacher 2018a,b) should probably at least be referenced in the related work section. CONCLUSION While I like the overall topic of the paper, I currently find the conceptual contribution to be too thin, raising doubts on novelty and significance. In addition, the presentation is somewhat lacking in clarity, and the practical merit is not well established. Notwithstanding the public nature of ICLR submissions, I would suggest more work on the paper prior to publication. REFERENCES M. Mitzenmacher, A Model for Learned Bloom Filters and Related Structures, 2018, see https://arxiv.org/pdf/1802.00884.pdf. M. Mitzenmacher, Optimizing Learned Bloom Filters by Sandwiching, 2018, see https://arxiv.org/pdf/1803.01474.pdf. (Update: score revised, see below.)", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for this thoughtful and comprehensive review . -- We agree the recent Mitzenmacher arxiv posts should have been included in the related works , and they have now been added . -- Re. \u2018 strong empirical case for NBF\u2026 \u2019 The fact that an LSTM does well on the MNIST class-based familiarity task is a useful data-point . However we do see a substantial gain for the database task . However the main problem with RNNs such as the LSTM ( and DNC ) is that they are not scalable . need to be trained to store N items by ingesting the N elements sequentially , and then backpropagating over the entire sequence . For large N this does not end up being scalable ; e.g.for the large database task ( Table 1 ) where N = 5,000 . Thus we develop a memory model that does not rely on BPTT ( alike to memory networks ) but is compressive ( unlike memory networks ) . The crucial design-point of the model is that it uses a commutative write operation ( addition ) which is much simpler than the DNC & LSTM write ( e.g.no gating , no squashing of the state ) and is like a continuous relaxation of the Bloom Filter \u2019 s write ( logical or ) . A simple additive write scheme also means the model will produce the same external memory M regardless of the ordering of the inputs ( because addition is commutative ) which makes sense given that familiarity does not depend upon input ordering , thus we also do not get strange effects where older inputs have much worse performance than newer inputs ( which will occur with an RNN ) . We discuss the model \u2019 s motivation more explicitly in the revised text . -- Re. \u201c One interesting thing to look at would be the workload partition between the learning component and the backup filter \u201d . This is a very interesting question you ask here . Your intuition is absolutely pretty well for class-based familiarity , the backup filter is used where the encoder essentially miss-classifies a character ( so it is very lightly used ) . For uniform sampling , the model essentially captures a small random subset of inputs but mostly relies on the backup bloom filter . For the imbalanced data the model appears to store and characterise well the \u2018 heavy hitter \u2019 i.e.frequent elements in the state memory and uses the backup bloom filter for infrequent elements . -- Re. \u2018 problem setting is loosely sketched\u2026 \u2019 - the reviewer is correct , we originally wrote the paper for readers familiar with the recent one-shot memory-augmented meta-learning literature ( e.g.matching networks [ vinyals et al.2016 ] , MANN [ santoro et al.2016 ] ) but unfamiliar with Bloom Filters . This was an unfortunate choice , we have thus expanded on what we mean by meta-learning and described how the training regime works . It is the exact same training regime as that in vinyals et al.2016 and many follow-on works , only the classification problem is set membership , versus image classification . We have added a subsection with further explanation and an algorithm box with a succinct summary of the meta-learning training setup . We will just briefly summarize the training setup here . We have a collection of sets { S_1 , S_2 , , \u2026 S_m } reserved for training ( each set contains n points to insert ) ; and a collection of queries Q = { q1 , q2 , \u2026 , qL } and targets yi = 1 if qi in S and 0 otherwise . In the example of a database we can think of a given set Si = { k1 , \u2026 , kN } as a set of rowkeys for a given file on disk ( e.g.SSTable ) .We have many sets because we have many files ; for training we have reserved some for an offline training routine . During training we calculate M = f_write ( S ) , and then we calculate oi = f_read ( S , qi ) , our query responses having observed the set S only once . We calculate the cross-entropy loss L = \\sumi yi log ( oi ) + ( 1 - yi ) log ( 1-oi ) and backprogate through the network ( through the parameters controlling both the read , write , and encoder networks ) . One can consider the creation of M = f_write ( S ) as a fast one-shot learning procedure ; the network learns a state which can help it solve the classification problem , \u201c is q in S ? \u201d in one-shot . The slow-moving \u2018 meta-learning \u2019 process is in the network parameters , which are slowly being optimized over several set membership tasks , i.e.several different sets S_1 : m , to be effective at one-shot classification . At test time , when we observe a new subset ( or stream of elements ) we can insert them with f_write in one-shot and the resulting data-structure is the external memory , M. -- Re . Bloom Filter space usage , we indeed used the analytical bound . We have clarified this in the text . We feel this is fair as it makes the task of beating a Bloom Filter \u2019 s space performance slightly more difficult ( as the analytic bound is slightly more compressive than in-practice ) , and it absolves any dispute over the choice of Bloom Filter library / choice of hash function etc . -- We have clarified the false positive rate is with respect to the distribution of queries in the text ."}, {"review_id": "HkekMnR5Ym-2", "review_text": "The paper proposes a method whereby a neural network is trained and used as a data structure to assess approximate set membership. Unlike the Bloom filter, which uses hand-constructed hash functions to store data and a pre-specified method for answering queries, the Neural Bloom Filter learns both the Write function and the Read function (both are \"soft\" values rather than the hard binary values used in the Bloom filter). Experiments show that, when there is structure in the data set, the Neural Bloom Filter can achieve the same false positive rate with less space. I had a hard time understanding how the model is trained. There is an encoding function, a write function, and a query function. The paper talks about one-shot meta-learning over a stream of data, but doesn't make it clear how those functions are learned. A lot of details are relegated to the Appendix. For instance B.2 talks about the encoder architecture for one of the experiments. But even that does not contain much detail, and it's not obvious how this is related to one-shot learning. Overall, the paper is written from the perspective of someone fully immersed in the details of the area, but who is unable to pop out of the details to explain to people who are not already familiar with the approach how it works. I would suggest rewriting to give an end-to-end picture of how it works, including details, without appendices. The approach sounds promising, but the exposition is not clear at all.", "rating": "3: Clear rejection", "reply_text": "Thank you for reading the paper , and we apologize for its opacity upon first pass . We completely agree the paper has mis-judged its audience and was not easy to read straight-through , this feedback is very useful in correcting this . We wrote the paper for someone highly familiar with meta-learning memory-augmented neural networks but not familiar with bloom filters ; this left out an important audience . -- - Re. \u201c I had a hard time understanding how the model is trained ... \u201d The model learns in one-shot because it observes a set S = ( k1 , k2 , \u2026 kn ) and writes it to a memory ( or state ) M with only one observation of this dataset . It then answers queries \u201c is my query x in S \u201d using the read operation , conditioning on the memory , M. It is the same one-shot classification approach as `` Matching Networks '' Vinyals et al.2016 however we focus on classifying familiarity versus image or text class . We have added several paragraphs and an algorithm box with further explanation of the meta-learning training setup . We will just briefly summarize it here . We have a collection of sets Strain1 , Strain2 , , \u2026 Strainm reserved for training ; and a collection of queries Q = { q1 , q2 , \u2026 , qL } and targets yi = 1 if qi in S and 0 otherwise . In the example of a database we can think of a given set Si = { k1 , \u2026 , kN } as a set of rowkeys for a given file on disk ( e.g.SSTable ) .We have many sets because we have many files ; for training we have reserved some for an offline training routine . During training we calculate M = fwrite ( S ) , and then we calculate oi = fread ( S , qi ) . We calculate the cross-entropy loss L = \\sumi yi log ( oi ) + ( 1 - yi ) log ( 1-oi ) and backprogate through the network ( through the parameters controlling both the read , write , and encoder networks ) . One can consider the creation of M = fwrite ( S ) as a fast one-shot learning procedure ; the network learns a state which can help it solve the classification problem , \u201c is q in S ? \u201d . The slow-moving \u2018 meta-learning \u2019 process is in the network parameters , which are slowly being optimized over several set membership tasks , i.e.several different sets S1 : m , to be effective at one-shot classification . At test time , when we observe a new subset ( or stream of elements ) we can insert them with fwrite in one-shot and the resulting data-structure is the external memory , M. -- Re . \u201c A lot of details are relegated to the Appendix . For instance B.2 talks about the encoder architecture for one of the experiments. \u201d This is a good point . We have removed B . 2 from the appendix and promoted the details to the model section . Furthermore we have given an example instantiation of the full architecture in the model section , so one does not need to consult the appendix . We have not completely removed the appendix as some details are tangential discussion points ( e.g.how to implement the model in sub-linear time ) but other details , such as space comparison , are now described in more detail in the experiments section . We have significantly re-written the paper \u2019 s model and experiments section to remedy this -- - please take a look and let us know if this addresses concerns ."}], "0": {"review_id": "HkekMnR5Ym-0", "review_text": "The paper proposes a learnable bloom filter architecture. While the details of the architecture seemed a bit too complicated for me to grasp (see more on this later), via experiments the authors show that the learned bloom filters are more compact that regular bloom filters and can outperform other neural architectures when it comes to retrieving seen items. A bloom filter is fairly simple, K hash functions hash seen items into K bit vectors. During retrieval, if all of the bits hashed to are 1 then we say we've seen the query. I think there's simpler ways to derive a continuous, differentiable version of this which begs the question why the authors chose a relatively more elaborate architecture involving ZCA transform and first/second moments. Perhaps the authors need to motivate their architecture a bit better. In their experiments, a simple LSTM seems to perform remarkably well (it is close to the best in 2 (a), (b); and crashes in (c) but the proposed technique is also outperformed by vanilla bloom filters in (c)). This is not surprising to me since LSTMs are remarkably good at remembering patterns. Perhaps the authors would like to comment on why they did not develop the LSTM further to remedy it of its shortcomings. Some of the positive results attained using neural bloom filters is a bit tempered by the fact that the experiments were using a back up bloom filter. Also, the neural bloom filters do well only when there is some sort of querying pattern. All of these details would seem to reduce the applicability of the proposed approach. The authors have addressed most (if not all) of my comments in their revised version. I applaud the authors for being particularly responsive. Their explanations and additional experiments go a long way towards lending the insights that were missing from the original draft of the paper. I have upped my rating to a 7.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review , and for your keen eye to detail . Re. \u201c why not develop further an LSTM \u201d . We are principally interested in whether it is possible to learn a compressive set membership data-structure in one-shot . Because many applications of Bloom Filters are in highly dynamic settings ( e.g.databases ) , the requirement that a network may be able to beat a Bloom Filter with only a single computational pass over the data is quite important . It wasn \u2019 t clear from the beginning of this research project ( to us and our peers ) whether it would be possible , and if so - in what setting . Thus we feel that it would be a worthwhile scientific contribution to show this is the case with any model -- - even an LSTM . Firstly , it is worth noting the LSTM is non-trivially less efficient for the database task even when the sequence length is quite short . But the real issue with an LSTM and other RNNs ( partially covered in the Reviewer 3 response ) is that it they are difficult to scale to larger set sizes , because one has to BPTT over the entire input sequence linearly ( elements of our storage size S ) during training . Say S contains 5,000 elements\u2026 One would have to train with sequences of length 5,000 , insert all elements sequentially and BPTT over the 5,000 long sequence . Training is way too slow , and the optimization problem becomes intractable ( network fails to learn ) . Furthermore the LSTM has quadratic computation cost with respect to the hidden state size . Since set membership is order-invariant , it seemed preferable to try out a memory architecture which does not rely on sequential computation and BPTT ( like a memory network ) that is still very compressive ( unlike a memory network ) . Our mistake in the original exposition , which you rightly point out , is that we have presented our solution ( the architecture ) without much of the motivation that lead to its incarnation . We have re-written the model section to remedy this . But we will also briefly state the model motivation here : - We want a simple write scheme and no BPTT - > additive write . ( it \u2019 s order-invariant , and alike to the Bloom Filter \u2019 s logical-or write ) . - We want the network to choose where to write , as well as what to write - > address is a softmax over memory based on content . ( alike to the Kanerva Machine Wu et al . ( 2018 ) ) - We want the network \u2019 s network trainable parameters to be small and independent of memory size - > make the addressing matrix A non-trainable . - We want the addressing to be efficient - > make it sparse ( alike to Rae et al . ( 2016 ) ) .- We found a sparse address led to the network fixating on a subset of memory - > whiten the query vector . Whitening ( or sphering ) may appear complex but was only necessary if one adopts the sparse attention for efficiency . We implemented it in four lines of TensorFlow code , so at least it is not too complex from an engineering standpoint . Whitening has been used within deep learning literature before , e.g. \u201c natural neural networks \u201d [ 1 ] . An alternative to whitening would be to use a \u201c flow \u201d such as real NVP [ 2 ] which actually transforms the query to something which appears to be truly gaussian . Crucially , this was a trick to get sparse attention working , if one wishes to avoid sparse attention and just use the full softmax over memory then this side-detail of whitening can be ignored . -- Re. \u201c Also , the neural bloom filters do well only when there is some sort of querying pattern . All of these details would seem to reduce the applicability of the proposed approach. \u201d Fortunately the proposed approach does well if there is structure to the query pattern * or * storage set . In the case of the database task , our queries are picked uniformly from the universe -- - there is not much structure . However there is structure to the storage sets ( which represent row keys in an disk file within a database ) and this is why our approach outperforms the classical data-structures so significantly . More generally we think the research area of using neural networks to replace data-structures , in this case a bloom filter , is so exciting because ( we would argue ) they are very rarely applied to data that contains no structure . Using a neural network to exploit redundancy and save space feels like a very impactful thing to do , and thought leaders within Computer Science ( e.g.Jeff Dean , a co-author of the kraska et al.2018 paper ) appear to believe so . There are patterns to the rowkey schema that is used within our databases , there are patterns to blacklist URLs and IPs within our firewalls , there are patterns to our search queries . We have re-written the model and experiment section to address your concerns ! [ 1 ] https : //deepmind.com/research/publications/natural-neural-networks/ [ 2 ] https : //arxiv.org/abs/1605.08803"}, "1": {"review_id": "HkekMnR5Ym-1", "review_text": "SUMMARY The paper proposes a neural network based architecture to solve the approximate set membership problem, in the distributional setting where the in-set and out-of-set elements come from two unknown and possibly different distributions. COMMENTARY The topic of the paper is interesting, and falls into the popular trend of enhancing classical data structures with learning algorithms. For the approximate set membership problem, this approach was already suggested by (Kraska et al. 2018) and studied further in (Mitzenmacher 2018a,b). The difference in the current paper is that the proposed approach relies on \"meta-learning\", apparently to facilitate online training and/or learning across multiple sets arising from the same distribution; this is what I gather from the introduction, even though as I write below, I feel this point is not properly explained. My main issue with the paper is that its conceptual contribution seems limited and unclear. It suggests a specific architecture whose details seem mostly arbitrary, or at least this is the impression the reader is left with, as the paper does rather little in terms of discussing and motivating them or putting them in context. Moreover, since the solution ultimately relies on a backup Bloom Filter as in (Kraska et al. 2018), it is hard to not view it as just an instantiation of the model in (Kraska et al. 2018, Mitzenmacher 2018a) with a different plugging of learning component. It would help to flesh out and highlight what the authors claim are the main insights of the paper. Another issue I suggest revising pertains to the writing. The problem setting is only loosely sketched but not properly defined. How exactly do different subsets coming into play? Specifically, the term \"meta-learning\" appears in the title and throughout the paper, but is never defined or explained. The authors should write out what exactly they mean by this notion and what role it plays in the paper. This is important since to my understanding, this is the main point of departure from the aforementioned recent works on learning-enhanced Bloom Filters. The experiments do not seem to make a strong case for the empirical advantage of the Neural Bloom Filter. They show little to no improvement on the MNIST tasks, and some improvement on a non-standard database related task. One interesting thing to look at would be the workload partition between the learning component and the backup filter, meaning what is the rate of false negatives emitted by the former and caught by the latter, and how the space usage breaks down between them (vis-a-vis the formula in Appendix B). For example, it seems plausible that on the class familiarity task, the learning component simply learns to be a binary classifier for the chosen two MNIST classes and mostly ignores the backup filter, whereas in the uniform distribution setting, the learning component only memorizes a small number of true and false positives and defers almost the entire task to the backup filter. I am not sure what to expect on the intermediate exponential distribution task. Other comments/questions: 1. For the classical Bloom Filter, do the results reported in the experimental plots reflect the empirical false-positive rate measured in the experiment, or just the analytic bound? 2. On that note, it is worth noting that the false positive rate of the classical Bloom Filter is different than the one you report for the neural-net based architectures. The Bloom Filter FP probability is over its internal randomness (i.e. its hash functions) and is independent of the distribution of queries, which need not be randomized at all. For the neural-net based architectures, the measured FP rate is w.r.t. a specific distribution of queries. See the discussion in (Mitzenmacher 2018a), sections B-C. 3. The works (Mitzenmacher 2018a,b) should probably at least be referenced in the related work section. CONCLUSION While I like the overall topic of the paper, I currently find the conceptual contribution to be too thin, raising doubts on novelty and significance. In addition, the presentation is somewhat lacking in clarity, and the practical merit is not well established. Notwithstanding the public nature of ICLR submissions, I would suggest more work on the paper prior to publication. REFERENCES M. Mitzenmacher, A Model for Learned Bloom Filters and Related Structures, 2018, see https://arxiv.org/pdf/1802.00884.pdf. M. Mitzenmacher, Optimizing Learned Bloom Filters by Sandwiching, 2018, see https://arxiv.org/pdf/1803.01474.pdf. (Update: score revised, see below.)", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for this thoughtful and comprehensive review . -- We agree the recent Mitzenmacher arxiv posts should have been included in the related works , and they have now been added . -- Re. \u2018 strong empirical case for NBF\u2026 \u2019 The fact that an LSTM does well on the MNIST class-based familiarity task is a useful data-point . However we do see a substantial gain for the database task . However the main problem with RNNs such as the LSTM ( and DNC ) is that they are not scalable . need to be trained to store N items by ingesting the N elements sequentially , and then backpropagating over the entire sequence . For large N this does not end up being scalable ; e.g.for the large database task ( Table 1 ) where N = 5,000 . Thus we develop a memory model that does not rely on BPTT ( alike to memory networks ) but is compressive ( unlike memory networks ) . The crucial design-point of the model is that it uses a commutative write operation ( addition ) which is much simpler than the DNC & LSTM write ( e.g.no gating , no squashing of the state ) and is like a continuous relaxation of the Bloom Filter \u2019 s write ( logical or ) . A simple additive write scheme also means the model will produce the same external memory M regardless of the ordering of the inputs ( because addition is commutative ) which makes sense given that familiarity does not depend upon input ordering , thus we also do not get strange effects where older inputs have much worse performance than newer inputs ( which will occur with an RNN ) . We discuss the model \u2019 s motivation more explicitly in the revised text . -- Re. \u201c One interesting thing to look at would be the workload partition between the learning component and the backup filter \u201d . This is a very interesting question you ask here . Your intuition is absolutely pretty well for class-based familiarity , the backup filter is used where the encoder essentially miss-classifies a character ( so it is very lightly used ) . For uniform sampling , the model essentially captures a small random subset of inputs but mostly relies on the backup bloom filter . For the imbalanced data the model appears to store and characterise well the \u2018 heavy hitter \u2019 i.e.frequent elements in the state memory and uses the backup bloom filter for infrequent elements . -- Re. \u2018 problem setting is loosely sketched\u2026 \u2019 - the reviewer is correct , we originally wrote the paper for readers familiar with the recent one-shot memory-augmented meta-learning literature ( e.g.matching networks [ vinyals et al.2016 ] , MANN [ santoro et al.2016 ] ) but unfamiliar with Bloom Filters . This was an unfortunate choice , we have thus expanded on what we mean by meta-learning and described how the training regime works . It is the exact same training regime as that in vinyals et al.2016 and many follow-on works , only the classification problem is set membership , versus image classification . We have added a subsection with further explanation and an algorithm box with a succinct summary of the meta-learning training setup . We will just briefly summarize the training setup here . We have a collection of sets { S_1 , S_2 , , \u2026 S_m } reserved for training ( each set contains n points to insert ) ; and a collection of queries Q = { q1 , q2 , \u2026 , qL } and targets yi = 1 if qi in S and 0 otherwise . In the example of a database we can think of a given set Si = { k1 , \u2026 , kN } as a set of rowkeys for a given file on disk ( e.g.SSTable ) .We have many sets because we have many files ; for training we have reserved some for an offline training routine . During training we calculate M = f_write ( S ) , and then we calculate oi = f_read ( S , qi ) , our query responses having observed the set S only once . We calculate the cross-entropy loss L = \\sumi yi log ( oi ) + ( 1 - yi ) log ( 1-oi ) and backprogate through the network ( through the parameters controlling both the read , write , and encoder networks ) . One can consider the creation of M = f_write ( S ) as a fast one-shot learning procedure ; the network learns a state which can help it solve the classification problem , \u201c is q in S ? \u201d in one-shot . The slow-moving \u2018 meta-learning \u2019 process is in the network parameters , which are slowly being optimized over several set membership tasks , i.e.several different sets S_1 : m , to be effective at one-shot classification . At test time , when we observe a new subset ( or stream of elements ) we can insert them with f_write in one-shot and the resulting data-structure is the external memory , M. -- Re . Bloom Filter space usage , we indeed used the analytical bound . We have clarified this in the text . We feel this is fair as it makes the task of beating a Bloom Filter \u2019 s space performance slightly more difficult ( as the analytic bound is slightly more compressive than in-practice ) , and it absolves any dispute over the choice of Bloom Filter library / choice of hash function etc . -- We have clarified the false positive rate is with respect to the distribution of queries in the text ."}, "2": {"review_id": "HkekMnR5Ym-2", "review_text": "The paper proposes a method whereby a neural network is trained and used as a data structure to assess approximate set membership. Unlike the Bloom filter, which uses hand-constructed hash functions to store data and a pre-specified method for answering queries, the Neural Bloom Filter learns both the Write function and the Read function (both are \"soft\" values rather than the hard binary values used in the Bloom filter). Experiments show that, when there is structure in the data set, the Neural Bloom Filter can achieve the same false positive rate with less space. I had a hard time understanding how the model is trained. There is an encoding function, a write function, and a query function. The paper talks about one-shot meta-learning over a stream of data, but doesn't make it clear how those functions are learned. A lot of details are relegated to the Appendix. For instance B.2 talks about the encoder architecture for one of the experiments. But even that does not contain much detail, and it's not obvious how this is related to one-shot learning. Overall, the paper is written from the perspective of someone fully immersed in the details of the area, but who is unable to pop out of the details to explain to people who are not already familiar with the approach how it works. I would suggest rewriting to give an end-to-end picture of how it works, including details, without appendices. The approach sounds promising, but the exposition is not clear at all.", "rating": "3: Clear rejection", "reply_text": "Thank you for reading the paper , and we apologize for its opacity upon first pass . We completely agree the paper has mis-judged its audience and was not easy to read straight-through , this feedback is very useful in correcting this . We wrote the paper for someone highly familiar with meta-learning memory-augmented neural networks but not familiar with bloom filters ; this left out an important audience . -- - Re. \u201c I had a hard time understanding how the model is trained ... \u201d The model learns in one-shot because it observes a set S = ( k1 , k2 , \u2026 kn ) and writes it to a memory ( or state ) M with only one observation of this dataset . It then answers queries \u201c is my query x in S \u201d using the read operation , conditioning on the memory , M. It is the same one-shot classification approach as `` Matching Networks '' Vinyals et al.2016 however we focus on classifying familiarity versus image or text class . We have added several paragraphs and an algorithm box with further explanation of the meta-learning training setup . We will just briefly summarize it here . We have a collection of sets Strain1 , Strain2 , , \u2026 Strainm reserved for training ; and a collection of queries Q = { q1 , q2 , \u2026 , qL } and targets yi = 1 if qi in S and 0 otherwise . In the example of a database we can think of a given set Si = { k1 , \u2026 , kN } as a set of rowkeys for a given file on disk ( e.g.SSTable ) .We have many sets because we have many files ; for training we have reserved some for an offline training routine . During training we calculate M = fwrite ( S ) , and then we calculate oi = fread ( S , qi ) . We calculate the cross-entropy loss L = \\sumi yi log ( oi ) + ( 1 - yi ) log ( 1-oi ) and backprogate through the network ( through the parameters controlling both the read , write , and encoder networks ) . One can consider the creation of M = fwrite ( S ) as a fast one-shot learning procedure ; the network learns a state which can help it solve the classification problem , \u201c is q in S ? \u201d . The slow-moving \u2018 meta-learning \u2019 process is in the network parameters , which are slowly being optimized over several set membership tasks , i.e.several different sets S1 : m , to be effective at one-shot classification . At test time , when we observe a new subset ( or stream of elements ) we can insert them with fwrite in one-shot and the resulting data-structure is the external memory , M. -- Re . \u201c A lot of details are relegated to the Appendix . For instance B.2 talks about the encoder architecture for one of the experiments. \u201d This is a good point . We have removed B . 2 from the appendix and promoted the details to the model section . Furthermore we have given an example instantiation of the full architecture in the model section , so one does not need to consult the appendix . We have not completely removed the appendix as some details are tangential discussion points ( e.g.how to implement the model in sub-linear time ) but other details , such as space comparison , are now described in more detail in the experiments section . We have significantly re-written the paper \u2019 s model and experiments section to remedy this -- - please take a look and let us know if this addresses concerns ."}}