{"year": "2021", "forum": "PQ2Cel-1rJh", "title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation", "decision": "Reject", "meta_review": "This paper got mixed reviews. One for acceptance, one for reject and two borderline. After the rebuttal, AR2 raises the review to borderline.  AR1 gives the highest recommendation but did not provide detailed supporting evidence. Other reviews provide comment on the strength and also share the concerns. Most of the concerns concentrate on the motivation (whether the proposed method is violating the objective of knowledge distillation) and the brought additional computation overhead. Also the scope of this paper was defined wider than the actual one. The authors only did experiments for BERT but did not consider and compare with existing KD method. Overall, AC read the paper and also has the similar concerns, the novelty is limited. the brought increase in inference time is violating the KD objective and the scope of this paper was not defined clearly. The authors should improve the submission in these aspects. At its current status, AC does not recommend acceptance. ", "reviews": [{"review_id": "PQ2Cel-1rJh-0", "review_text": "This paper proposes a distillation method for BERT . The work is based on two-fold main ideas . First , as the student model is usually smaller in the number of parameters , the model capacity is limited . The authors propose to stack the layers that share parameters to counter this limitation . Second , the authors argue that the initialization of the student model is crucial , so they propose an pre-training strategy for boosting the student 's performance . Pros : The idea of learning good initializations for the student model is interesting . Cons : This paper has several writing problems which need be carefully addressed before its publication . - As all the experiments are conducted on GLUE and only BERT models are considered in the paper , the proposed method seems to be tailored for BERT . The authors should stress this point clearly and early in the paper ( in the title , abstract or introduction ) . Otherwise , the authors should provide some experimental results on other models or tasks , e.g. , the typical image recognition task in computer vision . - In the section of PROPOSED METHODs ( section 3.1 ) , the motivation and the main idea of the paper are introduced again . As no any new information is provided here compared to the abstraction and the introduction sections , it seems very redundant . - The introduction of SPS in Section 3.2 is quite confusing . What do Key and Query parameters stand for ? I have no idea what the author is talking about here . Maybe it is because I have little background in NLP and BERT . However , even so the authors are still responsible for making the paper easy to follow for readers like me . As for the motivation and the proposed method , my comments are as follows . - The motivation and the proposed method are somewhat problematic . Firstly , the authors argue that small student with few parameters are limited in capacity , so they propose to stack repeating layers to enlarge the model capacity . However , stacking repeating layer will introduce much more computation cost , which violates the goal of distillation . - Secondly , the authors propose to pre-train the student model with the teacher predictions to initialize the student . However , it is odd to view this step as pre-training as it actually adopts the teacher predictions to train the student . It is actually doing distillation ! The improvement in performance may simply come from the more training epochs . = post-rebuttal : I have read all the comments from other reviewers and replies from the authors . The revised version partially addresses my concerns so I raise the score from 4 to 5 . However , my concerns about the motivation of the work still exist , so I am still slightly leaning to reject this paper .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank Reviewer 2 for the insightful comments . Each comment and how we have addressed them in our paper is summarized below . [ Comment 1 ] \u201c As all the experiments are conducted on GLUE and only BERT models are considered in the paper , the proposed method seems to be tailored for BERT . The authors should stress this point clearly and early in the paper ( in the title , abstract or introduction ) . Otherwise , the authors should provide some experimental results on other models or tasks , e.g. , the typical image recognition task in computer vision. \u201d = > Based on your comment , we have modified our title to explicitly mention BERT . The new title is \u201c Pea-KD : Parameter-efficient and accurate Knowledge Distillation on BERT. \u201d We also clarified in our paper that these experiments were only conducted on BERT . [ Comment 2 ] \u201c In the section of PROPOSED METHODs ( section 3.1 ) , the motivation and the main idea of the paper are introduced again . As no any new information is provided here compared to the abstraction and the introduction sections , it seems very redundant. \u201d = > We apologize if our paper structure was not concise . Since we are introducing a multi-step procedure , we thought it would help the reader follow the paper better if we provided a quick recap of our main ideas mentioned in the introduction once again in Section 3.1 . Since the overview and the introduction are both intended to provide a general summary of the proposed method , we hope you will allow for some overlap . To differentiate the introduction from Section 3.1 , we have added more information based on multiple Reviewers \u2019 feedback , including discussions around the motivation and intuition behind how we came up with the process , why we intuitively thought this method will work , and the contributing factors to the increased model representation . We have also added more information about SPS and PTP in Section 3.1 based on other reviewers \u2019 feedback . [ Comment 3 ] \u201c The introduction of SPS in Section 3.2 is quite confusing . What do Key and Query parameters stand for ? I have no idea what the author is talking about here . Maybe it is because I have little background in NLP and BERT . However , even so the authors are still responsible for making the paper easy to follow for readers like me. \u201d = > We apologize for not providing enough background on the jargons used . We updated the first paragraph in Section 2 to include explanations of the transformer layers . [ Comment 4 ] \u201c The motivation and the proposed method are somewhat problematic . Firstly , the authors argue that small student with few parameters are limited in capacity , so they propose to stack repeating layers to enlarge the model capacity . However , stacking repeating layer will introduce much more computation cost , which violates the goal of distillation. \u201d = > Our primary focus was on improving performance without needing additional memory resources . Of the many important factors that we consider in model compression , such as memory storage , performance , and inference time , some factors are prioritized over others depending on the circumstances . In our paper , we prioritized maximum performance improvement while keeping memory constant , over speed . As you have pointed out , the SPS method has a drawback of additional inference time . Despite the longer inference time , our model does perform significantly better . We believe that the additional time could be acceptable in certain cases where the performance is much more important than inference time , with limited memory storage . We do acknowledge that the increased inference time is an important limitation of our SPS method and will certainly aim to reduce this in our future work ."}, {"review_id": "PQ2Cel-1rJh-1", "review_text": "The paper claims to present an improved form of knowledge distillation which tackles the following perceived weaknesses of existing kd systems : - student 's expressive power is less than original teacher model , and - unclear how to initialize the student model weights in a principled way The paper proposes two methods to improve kd : - using stacked layers , with weight sharing between the weights , and keys and queries swapped in the upper layers - this is proposed because it means the effective student geometry matches that of the teacher , or at least is significantly larger than without the shared layers , whilst the number of unique parameters remains small - getting the student to predict the confidence of the teacher on each example , rather than just the softmax output of the teacher - the confidence is classified as either 'strong ' or 'weak ' - the student must also predict whether the teacher gets the example correct or not ( which we know , since this is for training set examples , for which we have ground truth ) Strong points of the paper : - SPS sounds novel , but I felt it was only very weakly motivated - in addition it sounds to me like the inference time will be similar to the original model ? I feel that reducing the number of parameters in the student is not the only goal : the student model should be fast to execute . - the paper does admittedly not claim to provide fast inference times as a goal ; nevertheless I have a hard time imagining a BERT model running on an edge device with a limited amount of resources in a reasonable time , so I think that inference time is a critical metric which I feel this paper does not consider , or measure - PTP sounds to me only weakly novel , since typically the student will be trained to predict the full softmax output of the teacher , whereas in PTP , the student must predict whether the highest value of the softmax is high or low . I feel that little insight or motivation is given as to why this auxiliary task was used , or is better than predicting the full softmax distribution . Nor was a rigorous comparison carried out to compare using just this pre-training approach with just the softmax-matching approach , where both using appropriately tuned hyper-parameters , I feel . There was an experiment in 4.4 where the PTP task is dropped , but I got the impression that this was after tuning the hyper-parameters to rely on PTP task ? The hyper-parameters would I feel need to be retuned in the absence of the PTP task I think ? I mostly like the paper . I found it interesting . However , I have two main concerns : - the approaches used in the paper feel to me only weakly motivated . Little insight is given into why the approaches were chosen , and why they should work . For example , flipping the keys and queries was not well justified I felt . Nor was the PTP task relative to traditional softmax matching . - the paper avoids discussing inference times , and it 's unclear to me whether a distillation approach that results in few parameters but long inference times would be useful in practice ? - of course , the underlying techniques can still be interesting from a theoretical point of view Whilst I was reading through the paper , I wrote down some more detailed reactions : Introduction : `` stacking the layers that share parameters '' = > `` stacking layers that share parameters '' ( the former form of language is using 'that share parameters ' as a filter to choose from existing layers ; the latter form implies that we create new layers , which we then stack , and which we configure to have shared parameters ) 3.1 Overview So , it looks like SPS means the number of unique parameters is kept low whilst the effective model complexity is unchanged compared to original BERT . How does this affect inference speed ? It sounds to me like the inference speed under SPS will be unchanged from a BERT model of equivalent model complexity ? Often , a use-case of distillation is to reduce inference time , and requirements in terms of power , eg for mobile devices , or production inference . Assuming that the inference time is unchanged from the original model of equal complexity , what is the target use-case of SPS/Pea-KD ? 3.2 SPS No insight or intuition is given as to why swapping query and key is likely to be a useful thing to do . I think it would be good to provide such insight and intuition . Like , you say that it increases the expressive power , but you do n't justify why you feel this is true . For the six-layer student , I couldnt quite undersatnd why it looks like the 6-layer student has 9 layers ? Please consider clarifying this point : - why do you call it ' 6 layer ' if it has 9 effective layers ? Perhaps consider renaming it to 9-layer student ? - why do you make the 6 layer model become 9 layers , and not 12 layers ? ie , following the initial recipe detailed in section 3.2 , of doubling all the layers . 3.3 PTP I feel that the column for 'PTP label ' in table 1 should be on the right hand side : - the inputs should be I feel on the left , ie 'teachers prediction correct ' , and 'confidence > t ' , - and then we can read off the 'output ' of the table as the right-hand column - ( currently I find the table hard to read ) The pretraining task itself seems interesting to me . How do you get the student to predict this result ? Do you put a linear layer on the output from the student ? Why do you use a classification task ( 4 classes ) instead of using a regression task to predict the confidence ? did you try a regression task to predict the confidence ? Why use a classification into 4 classes , rather than two classifiers into two classes each ? 4.Experiments : For table 2 , I was n't really sure how these numbers compare to two key baselines : - a simple bilstm with attention , without any pretraining , and - a BERT-base model So , I dug these out , and here are the peabert numbers from table 2 , put into the context of these two baselines ( which I feel are lower and upperbound really for what we 'd expect to see from PeaBERT ) : BiLSTM+Attn ( single-task training ) BERT-base PeaBERT1 PeaBERT2 PeaBERT3 RTE : 51.9 66.4 53.0 64.1 64.5 MRPC 68.5 88.9 81.0 82.7 85.0 SST2 85.9 93.5 86.9 88.2 90.4 QNLI 77.2 90.5 78.8 86.0 87.0 I got the BiLSTM+Attn numbers from GLUE paper , and BERT numbers from https : //arxiv.org/pdf/1910.03176.pdf I kind of feel that I might not be the only person who might want to see the PeaBERT numbers in the context of such lower and upperbound baselines ? Maybe consider adding these baselinse into table 2 ? Basically , my take away from this is that PeaBERT1 is barely better than the simple BiLSTM+Attn baseline , but PeaBERT3 approaches the score of a full BERT-base model ? I kind of think the sentence `` For example , DistilBERT took approximately 90 hours with eight 16GB V100 GPUs while PeaBERT took a minimum of one minute ( PeaBERT1 with RTE ) to a maximum of one hour ( PeaBERT3 with QNLI ) using just two NVIDIA T4 GPUs '' should be highlighted in a table somewhere somehow perhaps , rather than buried in text ? Not sure if that 's a good idea , just occurred to me though . Question : why only show results on a subset of the GLUE tasks ? eg patient-kd paper shows results on additionally : - QQP - MNLI-m - MNLI-mm ( whilst also showing results for : SST-2 , MRPC , QNLI and RTE , as here ) It is unclear from this whether you ran against all , and only showed the four tasks that show a benefit , or whether you simply did n't have time to run on all 7 tasks . Preference to show results for all 7 tasks that Patient-KD paper reports results for . For ablation studies , I feel it would be interesting to see an ablation study that removes each of various losses in equation 3 in turn . Importantly , none of the experiments mention inference time , which I feel is a key metric to report for distillation ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "[ Comment 1 ] `` the approaches used in the paper feel to me only weakly motivated . Little insight is given into why the approaches were chosen , and why they should work . For example , flipping the keys and queries was not well justified I felt . Nor was the PTP task relative to traditional softmax matching . '' = > SPS Step 2 ( Shuffling ) We believe the main factor contributing to the power of SPS step 2 is as follows : Maximizing features learned by parameters by taking on different roles within the model . Taking the BERT $ _ { 3 } $ case as an example , let us look at the difference between SPS-1 and SPS-2 and how that contributes to the learning process of the parameters . As a recap , while SPS-1 applies only step1 to BERT , SPS-2 applies both step 1 and step 2 , shuffling the parameters around . Consider the first layer 's Query parameter . Under SPS-1 , this parameter is used as a Query parameter in both the first layer and in the shared fourth layer . Since this parameter is used only as Query , it will learn only the information relevant to Query . Under SPS-2 , however , the first layer \u2019 s Query parameter \u2019 s function changes due to shuffling . The first layer \u2019 s Query parameter is used as a Key parameter in the shared fourth layer . This one parameter has had the opportunity to learn the important features of both the Query and the Key functions , gaining a more diverse and wider breadth of knowledge . Based on the average accuracy increase of 1.9 percent driven by shuffling ( Table 4 of the paper ) , we believe that the shuffled parameters were able to learn a more diverse set of information , and this improvement in parameter efficiency contributed to enriching model capacity . We also see this in a similar but slightly different point of view . Since the parameters get to learn diverse features and get to function in diverse positions we believe this could act as an additional regularization . Therefore it helps the model prevent overfitting and leads to improvement in performance . = > PTP The core idea is to make PTP labels by explicitly expressing important information from the teachers \u2019 softmax outputs , such as whether the teacher model predicted correctly or how confident the teacher model is . Pretraining using these labels would help the student acquire the teacher \u2019 s generalized knowledge latent in the teacher \u2019 s softmax output more easily . This pretraining makes the student model better prepared for the actual KD process . For example , if a teacher makes an incorrect prediction for a data instance x , then we also know that x is generally a difficult one to predict . Since this knowledge is obtainable only by directly comparing the true label with the teachers output , it would be difficult for the student to acquire this in the conventional KD process . Representing this type of information through PTP labels and training the student to predict them could help the student acquire deeply latent knowledge included in the teacher \u2019 s output much easily . Intuitively , we expect that a student model that has undergone such a pretraining session is better prepared for the actual KD process and will likely achieve better results . [ Comment 2 & 3 ] `` the paper avoids discussing inference times , and it 's unclear to me whether a distillation approach that results in few parameters but long inference times would be useful in practice ? '' \u2022 of course , the underlying techniques can still be interesting from a theoretical point of view & So , it looks like SPS means the number of unique parameters is kept low whilst the effective model complexity is unchanged compared to original BERT . How does this affect inference speed ? It sounds to me like the inference speed under SPS will be unchanged from a BERT model of equivalent model complexity ? Often , a use-case of distillation is to reduce inference time , and requirements in terms of power , eg for mobile devices , or production inference . Assuming that the inference time is unchanged from the original model of equal complexity , what is the target use-case of SPS/Pea-KD ? = > Our primary focus is on improving performance without needing additional memory resources . Of the many important factors that we consider in model compression , such as memory storage , performance , and inference time , some factors are prioritized over others depending on the circumstances . In our paper , we prioritized maximum performance improvement while keeping memory constant , over speed . As you have pointed out , the SPS method has a drawback of additional inference time . However , despite the longer inference time , our model does perform significantly better . We believe that the additional time could be acceptable in certain cases where the performance is much more important than inference time , with limited memory storage . We do acknowledge that the increased inference time is an important limitation of our SPS method and will certainly aim to reduce this in our future work . ( Continued )"}, {"review_id": "PQ2Cel-1rJh-2", "review_text": "This paper proposes a parameter-efficient KD which consists of two main parts : Shuffled Parameter Sharing ( SPS ) and Pretraining with Teacher \u2019 s Predictions ( PTP ) . This work explores some new framework like parameters-sharing and shuffling in KD and obtain promising results . Strengths : 1 . The paper is well-written and easy to follow . 2.The experiment part explores the overall performance , the effects of SPS , and PTP separately , which is clear and makes readers easy to understand the effects of each part ( SPS : step1+step2 , PTP ) in Pea-KD . Weaknesses : 1 . When applying SPS , the number of layers in the student model is double than the normal case , even the parameters are the same as the counterpart , I guess the FLOPs will increase or be doubled than the original one . I think for a small student model , the number of parameters should not be the only metric but also consider the FLOPs . Why this paper doesn \u2019 t discuss this ? This is my main concern . 2.From Table4 , we can find that the shuffling in SPS has great effects . step1+step2 would have much greater improvement than only applying step1 , how about only conduct shuffling without parameters sharing ( only step2 by shuffling the original parameters without step1 ) ? On the other hand , what is the potential reason why shuffling can enrich the model capacity ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank Reviewer 3 for the insightful and detailed feedback . The order of responses to your questions is intentionally switched for easier explanation . 1.Potential reasons for why shuffling can enrich model capacity We believe the main factor contributing to the power of SPS step 2 is as follows : Increased model capacity by learning diverse set of information with the same parameters . Taking the BERT $ _ { 3 } $ case as an example , let us look at the difference between SPS-1 and SPS-2 and how that contributes to the learning process of the parameters . As a recap , while SPS-1 applies only step1 to BERT , SPS-2 applies both step 1 and step 2 , shuffling the parameters around . Consider the first layer 's Query parameter . Under SPS-1 , this parameter is used as a Query parameter in both the first layer and in the shared fourth layer . Since this parameter is only used as Query , it will only learn the information relevant to Query . Under SPS-2 , however , the first layer \u2019 s Query parameter \u2019 s function changes due to shuffling . The first layer \u2019 s Query parameter is used as a Key parameter in the shared fourth layer . This one parameter has had the opportunity to learn the important features of both the Query and the Key functions , gaining a more diverse and wider breadth of knowledge . Based on the average accuracy increase of 1.9 percent driven by shuffling ( Table 4 of the paper ) , we believe that the shuffled parameters were able to learn a more diverse set of information , and this improvement in parameter efficiency contributed to enriching model capacity . We also see this in a similar but slightly different point of view . Since the parameters get to learn diverse features and get to function in diverse positions we believe this could act as an additional regularization . Therefore it helps the model prevent overfitting and leads to improvement in performance . 2.Regarding only applying the SPS step 2 to the student model . We initially did not try applying SPS step 2 on a standalone basis , because we created step 2 with the intention of applying it to the shuffled parameters derived from step 1 . We believe step 2 would be meaningful only when preceded by step 1 . Since Query and Key are identical ( 768 , 768 ) linear layers with different names , the performance of BERT and BERT with only step 2 applied would yield the same results . Without step 1 , step 2 would not make an impact . However , thanks to reviewer 3 , we realized that , since we are using the pre-trained BERT supplied by Huggingface , BERT and ` BERT + only step 2 ' could potentially yield different results in this case . Given the pre-trained Query and Key parameters by Huggingface , we switched the order of them and performed finetuning . This experiment of using 'BERT + only step 2 ' showed that applying step 2 alone actually showed decreased average accuracy of 0.6\\ % . As we expected , step1 needs to be preceded for step2 to have desirable effects . |models|MRPC|RTE|SST-2|QNLI|AVG| | : -| : -| : | : -| : -- | : -- | |BERT $ _ { 3 } $ | 84 . 7| 62.0|88.3|85.1|80.0| |BERT $ _ { 3 } $ +step2| 84.8 | 61.9 | 87.4 | 83.5| 79.4| 3 . Regarding the increased inference time . Compared to the original BERT model , our SPS model uses the same amount of memory storage to load and run but achieves much higher accuracy , an average improvement of 4.4\\ % . However , we are aware and fully acknowledge that our approach has a drawback of additional inference time . This is mostly because we use additional shared layers as the reviewer supposed , which takes about 50\\ % to 100\\ % additional time to run for PeaBERT ( 50\\ % for PeaBERT6 and 100\\ % for PeaBERT3 ) . Despite the longer inference time , our model does perform significantly better . We believe that the additional time could be acceptable in certain cases where the performance is much more important than inference time , with limited memory storage . We do acknowledge that the increased inference time is an important limitation of our SPS method and will definitely aim to reduce this in our future work ."}, {"review_id": "PQ2Cel-1rJh-3", "review_text": "This paper proposed a framework for knowledge distillation with smaller number of parameters.The authors proposed a new parameter sharing method that allows a greater model complexity for the student model . Another contribution is that a KD-specialized initialization method named Pretraining with Teacher \u2019 s Predictions can improve the student 's performance . The author combined these two methods to improve the performance of the student model on existing tasks , which has surpassed the existing knowledge distillation baseline . The SPS method is a very natural idea of extending the student model . It expands the model complexity of the student model and improves the representation ability of the model without increasing the number of parameters . The idea of PTP is an excellent initializing method . At the same time , the teacher model 's generalization of knowledge is given to students through initialization , and then fine-tuned . The experimental part of this article is also quite sufficient . The ablation experiment illustrates the effectiveness of the two steps for the optimization of results .", "rating": "7: Good paper, accept", "reply_text": "We would like to thank Reviewer 1 for the detailed and thoughtful review . We were delighted to hear that you found our core ideas interesting . Based on reviewers ' feedback , we have included more ablation studies to prove the effectiveness of our method ( Section 4.4 ) and additional explanation behind how we came up with our approach and why we intuitively thought this will improve performance ( Sections 3.2 and 3.3 ) . We hope these revisions made our paper more robust . We would truly appreciate it if you could review our updated paper once more . Thank you ."}], "0": {"review_id": "PQ2Cel-1rJh-0", "review_text": "This paper proposes a distillation method for BERT . The work is based on two-fold main ideas . First , as the student model is usually smaller in the number of parameters , the model capacity is limited . The authors propose to stack the layers that share parameters to counter this limitation . Second , the authors argue that the initialization of the student model is crucial , so they propose an pre-training strategy for boosting the student 's performance . Pros : The idea of learning good initializations for the student model is interesting . Cons : This paper has several writing problems which need be carefully addressed before its publication . - As all the experiments are conducted on GLUE and only BERT models are considered in the paper , the proposed method seems to be tailored for BERT . The authors should stress this point clearly and early in the paper ( in the title , abstract or introduction ) . Otherwise , the authors should provide some experimental results on other models or tasks , e.g. , the typical image recognition task in computer vision . - In the section of PROPOSED METHODs ( section 3.1 ) , the motivation and the main idea of the paper are introduced again . As no any new information is provided here compared to the abstraction and the introduction sections , it seems very redundant . - The introduction of SPS in Section 3.2 is quite confusing . What do Key and Query parameters stand for ? I have no idea what the author is talking about here . Maybe it is because I have little background in NLP and BERT . However , even so the authors are still responsible for making the paper easy to follow for readers like me . As for the motivation and the proposed method , my comments are as follows . - The motivation and the proposed method are somewhat problematic . Firstly , the authors argue that small student with few parameters are limited in capacity , so they propose to stack repeating layers to enlarge the model capacity . However , stacking repeating layer will introduce much more computation cost , which violates the goal of distillation . - Secondly , the authors propose to pre-train the student model with the teacher predictions to initialize the student . However , it is odd to view this step as pre-training as it actually adopts the teacher predictions to train the student . It is actually doing distillation ! The improvement in performance may simply come from the more training epochs . = post-rebuttal : I have read all the comments from other reviewers and replies from the authors . The revised version partially addresses my concerns so I raise the score from 4 to 5 . However , my concerns about the motivation of the work still exist , so I am still slightly leaning to reject this paper .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank Reviewer 2 for the insightful comments . Each comment and how we have addressed them in our paper is summarized below . [ Comment 1 ] \u201c As all the experiments are conducted on GLUE and only BERT models are considered in the paper , the proposed method seems to be tailored for BERT . The authors should stress this point clearly and early in the paper ( in the title , abstract or introduction ) . Otherwise , the authors should provide some experimental results on other models or tasks , e.g. , the typical image recognition task in computer vision. \u201d = > Based on your comment , we have modified our title to explicitly mention BERT . The new title is \u201c Pea-KD : Parameter-efficient and accurate Knowledge Distillation on BERT. \u201d We also clarified in our paper that these experiments were only conducted on BERT . [ Comment 2 ] \u201c In the section of PROPOSED METHODs ( section 3.1 ) , the motivation and the main idea of the paper are introduced again . As no any new information is provided here compared to the abstraction and the introduction sections , it seems very redundant. \u201d = > We apologize if our paper structure was not concise . Since we are introducing a multi-step procedure , we thought it would help the reader follow the paper better if we provided a quick recap of our main ideas mentioned in the introduction once again in Section 3.1 . Since the overview and the introduction are both intended to provide a general summary of the proposed method , we hope you will allow for some overlap . To differentiate the introduction from Section 3.1 , we have added more information based on multiple Reviewers \u2019 feedback , including discussions around the motivation and intuition behind how we came up with the process , why we intuitively thought this method will work , and the contributing factors to the increased model representation . We have also added more information about SPS and PTP in Section 3.1 based on other reviewers \u2019 feedback . [ Comment 3 ] \u201c The introduction of SPS in Section 3.2 is quite confusing . What do Key and Query parameters stand for ? I have no idea what the author is talking about here . Maybe it is because I have little background in NLP and BERT . However , even so the authors are still responsible for making the paper easy to follow for readers like me. \u201d = > We apologize for not providing enough background on the jargons used . We updated the first paragraph in Section 2 to include explanations of the transformer layers . [ Comment 4 ] \u201c The motivation and the proposed method are somewhat problematic . Firstly , the authors argue that small student with few parameters are limited in capacity , so they propose to stack repeating layers to enlarge the model capacity . However , stacking repeating layer will introduce much more computation cost , which violates the goal of distillation. \u201d = > Our primary focus was on improving performance without needing additional memory resources . Of the many important factors that we consider in model compression , such as memory storage , performance , and inference time , some factors are prioritized over others depending on the circumstances . In our paper , we prioritized maximum performance improvement while keeping memory constant , over speed . As you have pointed out , the SPS method has a drawback of additional inference time . Despite the longer inference time , our model does perform significantly better . We believe that the additional time could be acceptable in certain cases where the performance is much more important than inference time , with limited memory storage . We do acknowledge that the increased inference time is an important limitation of our SPS method and will certainly aim to reduce this in our future work ."}, "1": {"review_id": "PQ2Cel-1rJh-1", "review_text": "The paper claims to present an improved form of knowledge distillation which tackles the following perceived weaknesses of existing kd systems : - student 's expressive power is less than original teacher model , and - unclear how to initialize the student model weights in a principled way The paper proposes two methods to improve kd : - using stacked layers , with weight sharing between the weights , and keys and queries swapped in the upper layers - this is proposed because it means the effective student geometry matches that of the teacher , or at least is significantly larger than without the shared layers , whilst the number of unique parameters remains small - getting the student to predict the confidence of the teacher on each example , rather than just the softmax output of the teacher - the confidence is classified as either 'strong ' or 'weak ' - the student must also predict whether the teacher gets the example correct or not ( which we know , since this is for training set examples , for which we have ground truth ) Strong points of the paper : - SPS sounds novel , but I felt it was only very weakly motivated - in addition it sounds to me like the inference time will be similar to the original model ? I feel that reducing the number of parameters in the student is not the only goal : the student model should be fast to execute . - the paper does admittedly not claim to provide fast inference times as a goal ; nevertheless I have a hard time imagining a BERT model running on an edge device with a limited amount of resources in a reasonable time , so I think that inference time is a critical metric which I feel this paper does not consider , or measure - PTP sounds to me only weakly novel , since typically the student will be trained to predict the full softmax output of the teacher , whereas in PTP , the student must predict whether the highest value of the softmax is high or low . I feel that little insight or motivation is given as to why this auxiliary task was used , or is better than predicting the full softmax distribution . Nor was a rigorous comparison carried out to compare using just this pre-training approach with just the softmax-matching approach , where both using appropriately tuned hyper-parameters , I feel . There was an experiment in 4.4 where the PTP task is dropped , but I got the impression that this was after tuning the hyper-parameters to rely on PTP task ? The hyper-parameters would I feel need to be retuned in the absence of the PTP task I think ? I mostly like the paper . I found it interesting . However , I have two main concerns : - the approaches used in the paper feel to me only weakly motivated . Little insight is given into why the approaches were chosen , and why they should work . For example , flipping the keys and queries was not well justified I felt . Nor was the PTP task relative to traditional softmax matching . - the paper avoids discussing inference times , and it 's unclear to me whether a distillation approach that results in few parameters but long inference times would be useful in practice ? - of course , the underlying techniques can still be interesting from a theoretical point of view Whilst I was reading through the paper , I wrote down some more detailed reactions : Introduction : `` stacking the layers that share parameters '' = > `` stacking layers that share parameters '' ( the former form of language is using 'that share parameters ' as a filter to choose from existing layers ; the latter form implies that we create new layers , which we then stack , and which we configure to have shared parameters ) 3.1 Overview So , it looks like SPS means the number of unique parameters is kept low whilst the effective model complexity is unchanged compared to original BERT . How does this affect inference speed ? It sounds to me like the inference speed under SPS will be unchanged from a BERT model of equivalent model complexity ? Often , a use-case of distillation is to reduce inference time , and requirements in terms of power , eg for mobile devices , or production inference . Assuming that the inference time is unchanged from the original model of equal complexity , what is the target use-case of SPS/Pea-KD ? 3.2 SPS No insight or intuition is given as to why swapping query and key is likely to be a useful thing to do . I think it would be good to provide such insight and intuition . Like , you say that it increases the expressive power , but you do n't justify why you feel this is true . For the six-layer student , I couldnt quite undersatnd why it looks like the 6-layer student has 9 layers ? Please consider clarifying this point : - why do you call it ' 6 layer ' if it has 9 effective layers ? Perhaps consider renaming it to 9-layer student ? - why do you make the 6 layer model become 9 layers , and not 12 layers ? ie , following the initial recipe detailed in section 3.2 , of doubling all the layers . 3.3 PTP I feel that the column for 'PTP label ' in table 1 should be on the right hand side : - the inputs should be I feel on the left , ie 'teachers prediction correct ' , and 'confidence > t ' , - and then we can read off the 'output ' of the table as the right-hand column - ( currently I find the table hard to read ) The pretraining task itself seems interesting to me . How do you get the student to predict this result ? Do you put a linear layer on the output from the student ? Why do you use a classification task ( 4 classes ) instead of using a regression task to predict the confidence ? did you try a regression task to predict the confidence ? Why use a classification into 4 classes , rather than two classifiers into two classes each ? 4.Experiments : For table 2 , I was n't really sure how these numbers compare to two key baselines : - a simple bilstm with attention , without any pretraining , and - a BERT-base model So , I dug these out , and here are the peabert numbers from table 2 , put into the context of these two baselines ( which I feel are lower and upperbound really for what we 'd expect to see from PeaBERT ) : BiLSTM+Attn ( single-task training ) BERT-base PeaBERT1 PeaBERT2 PeaBERT3 RTE : 51.9 66.4 53.0 64.1 64.5 MRPC 68.5 88.9 81.0 82.7 85.0 SST2 85.9 93.5 86.9 88.2 90.4 QNLI 77.2 90.5 78.8 86.0 87.0 I got the BiLSTM+Attn numbers from GLUE paper , and BERT numbers from https : //arxiv.org/pdf/1910.03176.pdf I kind of feel that I might not be the only person who might want to see the PeaBERT numbers in the context of such lower and upperbound baselines ? Maybe consider adding these baselinse into table 2 ? Basically , my take away from this is that PeaBERT1 is barely better than the simple BiLSTM+Attn baseline , but PeaBERT3 approaches the score of a full BERT-base model ? I kind of think the sentence `` For example , DistilBERT took approximately 90 hours with eight 16GB V100 GPUs while PeaBERT took a minimum of one minute ( PeaBERT1 with RTE ) to a maximum of one hour ( PeaBERT3 with QNLI ) using just two NVIDIA T4 GPUs '' should be highlighted in a table somewhere somehow perhaps , rather than buried in text ? Not sure if that 's a good idea , just occurred to me though . Question : why only show results on a subset of the GLUE tasks ? eg patient-kd paper shows results on additionally : - QQP - MNLI-m - MNLI-mm ( whilst also showing results for : SST-2 , MRPC , QNLI and RTE , as here ) It is unclear from this whether you ran against all , and only showed the four tasks that show a benefit , or whether you simply did n't have time to run on all 7 tasks . Preference to show results for all 7 tasks that Patient-KD paper reports results for . For ablation studies , I feel it would be interesting to see an ablation study that removes each of various losses in equation 3 in turn . Importantly , none of the experiments mention inference time , which I feel is a key metric to report for distillation ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "[ Comment 1 ] `` the approaches used in the paper feel to me only weakly motivated . Little insight is given into why the approaches were chosen , and why they should work . For example , flipping the keys and queries was not well justified I felt . Nor was the PTP task relative to traditional softmax matching . '' = > SPS Step 2 ( Shuffling ) We believe the main factor contributing to the power of SPS step 2 is as follows : Maximizing features learned by parameters by taking on different roles within the model . Taking the BERT $ _ { 3 } $ case as an example , let us look at the difference between SPS-1 and SPS-2 and how that contributes to the learning process of the parameters . As a recap , while SPS-1 applies only step1 to BERT , SPS-2 applies both step 1 and step 2 , shuffling the parameters around . Consider the first layer 's Query parameter . Under SPS-1 , this parameter is used as a Query parameter in both the first layer and in the shared fourth layer . Since this parameter is used only as Query , it will learn only the information relevant to Query . Under SPS-2 , however , the first layer \u2019 s Query parameter \u2019 s function changes due to shuffling . The first layer \u2019 s Query parameter is used as a Key parameter in the shared fourth layer . This one parameter has had the opportunity to learn the important features of both the Query and the Key functions , gaining a more diverse and wider breadth of knowledge . Based on the average accuracy increase of 1.9 percent driven by shuffling ( Table 4 of the paper ) , we believe that the shuffled parameters were able to learn a more diverse set of information , and this improvement in parameter efficiency contributed to enriching model capacity . We also see this in a similar but slightly different point of view . Since the parameters get to learn diverse features and get to function in diverse positions we believe this could act as an additional regularization . Therefore it helps the model prevent overfitting and leads to improvement in performance . = > PTP The core idea is to make PTP labels by explicitly expressing important information from the teachers \u2019 softmax outputs , such as whether the teacher model predicted correctly or how confident the teacher model is . Pretraining using these labels would help the student acquire the teacher \u2019 s generalized knowledge latent in the teacher \u2019 s softmax output more easily . This pretraining makes the student model better prepared for the actual KD process . For example , if a teacher makes an incorrect prediction for a data instance x , then we also know that x is generally a difficult one to predict . Since this knowledge is obtainable only by directly comparing the true label with the teachers output , it would be difficult for the student to acquire this in the conventional KD process . Representing this type of information through PTP labels and training the student to predict them could help the student acquire deeply latent knowledge included in the teacher \u2019 s output much easily . Intuitively , we expect that a student model that has undergone such a pretraining session is better prepared for the actual KD process and will likely achieve better results . [ Comment 2 & 3 ] `` the paper avoids discussing inference times , and it 's unclear to me whether a distillation approach that results in few parameters but long inference times would be useful in practice ? '' \u2022 of course , the underlying techniques can still be interesting from a theoretical point of view & So , it looks like SPS means the number of unique parameters is kept low whilst the effective model complexity is unchanged compared to original BERT . How does this affect inference speed ? It sounds to me like the inference speed under SPS will be unchanged from a BERT model of equivalent model complexity ? Often , a use-case of distillation is to reduce inference time , and requirements in terms of power , eg for mobile devices , or production inference . Assuming that the inference time is unchanged from the original model of equal complexity , what is the target use-case of SPS/Pea-KD ? = > Our primary focus is on improving performance without needing additional memory resources . Of the many important factors that we consider in model compression , such as memory storage , performance , and inference time , some factors are prioritized over others depending on the circumstances . In our paper , we prioritized maximum performance improvement while keeping memory constant , over speed . As you have pointed out , the SPS method has a drawback of additional inference time . However , despite the longer inference time , our model does perform significantly better . We believe that the additional time could be acceptable in certain cases where the performance is much more important than inference time , with limited memory storage . We do acknowledge that the increased inference time is an important limitation of our SPS method and will certainly aim to reduce this in our future work . ( Continued )"}, "2": {"review_id": "PQ2Cel-1rJh-2", "review_text": "This paper proposes a parameter-efficient KD which consists of two main parts : Shuffled Parameter Sharing ( SPS ) and Pretraining with Teacher \u2019 s Predictions ( PTP ) . This work explores some new framework like parameters-sharing and shuffling in KD and obtain promising results . Strengths : 1 . The paper is well-written and easy to follow . 2.The experiment part explores the overall performance , the effects of SPS , and PTP separately , which is clear and makes readers easy to understand the effects of each part ( SPS : step1+step2 , PTP ) in Pea-KD . Weaknesses : 1 . When applying SPS , the number of layers in the student model is double than the normal case , even the parameters are the same as the counterpart , I guess the FLOPs will increase or be doubled than the original one . I think for a small student model , the number of parameters should not be the only metric but also consider the FLOPs . Why this paper doesn \u2019 t discuss this ? This is my main concern . 2.From Table4 , we can find that the shuffling in SPS has great effects . step1+step2 would have much greater improvement than only applying step1 , how about only conduct shuffling without parameters sharing ( only step2 by shuffling the original parameters without step1 ) ? On the other hand , what is the potential reason why shuffling can enrich the model capacity ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank Reviewer 3 for the insightful and detailed feedback . The order of responses to your questions is intentionally switched for easier explanation . 1.Potential reasons for why shuffling can enrich model capacity We believe the main factor contributing to the power of SPS step 2 is as follows : Increased model capacity by learning diverse set of information with the same parameters . Taking the BERT $ _ { 3 } $ case as an example , let us look at the difference between SPS-1 and SPS-2 and how that contributes to the learning process of the parameters . As a recap , while SPS-1 applies only step1 to BERT , SPS-2 applies both step 1 and step 2 , shuffling the parameters around . Consider the first layer 's Query parameter . Under SPS-1 , this parameter is used as a Query parameter in both the first layer and in the shared fourth layer . Since this parameter is only used as Query , it will only learn the information relevant to Query . Under SPS-2 , however , the first layer \u2019 s Query parameter \u2019 s function changes due to shuffling . The first layer \u2019 s Query parameter is used as a Key parameter in the shared fourth layer . This one parameter has had the opportunity to learn the important features of both the Query and the Key functions , gaining a more diverse and wider breadth of knowledge . Based on the average accuracy increase of 1.9 percent driven by shuffling ( Table 4 of the paper ) , we believe that the shuffled parameters were able to learn a more diverse set of information , and this improvement in parameter efficiency contributed to enriching model capacity . We also see this in a similar but slightly different point of view . Since the parameters get to learn diverse features and get to function in diverse positions we believe this could act as an additional regularization . Therefore it helps the model prevent overfitting and leads to improvement in performance . 2.Regarding only applying the SPS step 2 to the student model . We initially did not try applying SPS step 2 on a standalone basis , because we created step 2 with the intention of applying it to the shuffled parameters derived from step 1 . We believe step 2 would be meaningful only when preceded by step 1 . Since Query and Key are identical ( 768 , 768 ) linear layers with different names , the performance of BERT and BERT with only step 2 applied would yield the same results . Without step 1 , step 2 would not make an impact . However , thanks to reviewer 3 , we realized that , since we are using the pre-trained BERT supplied by Huggingface , BERT and ` BERT + only step 2 ' could potentially yield different results in this case . Given the pre-trained Query and Key parameters by Huggingface , we switched the order of them and performed finetuning . This experiment of using 'BERT + only step 2 ' showed that applying step 2 alone actually showed decreased average accuracy of 0.6\\ % . As we expected , step1 needs to be preceded for step2 to have desirable effects . |models|MRPC|RTE|SST-2|QNLI|AVG| | : -| : -| : | : -| : -- | : -- | |BERT $ _ { 3 } $ | 84 . 7| 62.0|88.3|85.1|80.0| |BERT $ _ { 3 } $ +step2| 84.8 | 61.9 | 87.4 | 83.5| 79.4| 3 . Regarding the increased inference time . Compared to the original BERT model , our SPS model uses the same amount of memory storage to load and run but achieves much higher accuracy , an average improvement of 4.4\\ % . However , we are aware and fully acknowledge that our approach has a drawback of additional inference time . This is mostly because we use additional shared layers as the reviewer supposed , which takes about 50\\ % to 100\\ % additional time to run for PeaBERT ( 50\\ % for PeaBERT6 and 100\\ % for PeaBERT3 ) . Despite the longer inference time , our model does perform significantly better . We believe that the additional time could be acceptable in certain cases where the performance is much more important than inference time , with limited memory storage . We do acknowledge that the increased inference time is an important limitation of our SPS method and will definitely aim to reduce this in our future work ."}, "3": {"review_id": "PQ2Cel-1rJh-3", "review_text": "This paper proposed a framework for knowledge distillation with smaller number of parameters.The authors proposed a new parameter sharing method that allows a greater model complexity for the student model . Another contribution is that a KD-specialized initialization method named Pretraining with Teacher \u2019 s Predictions can improve the student 's performance . The author combined these two methods to improve the performance of the student model on existing tasks , which has surpassed the existing knowledge distillation baseline . The SPS method is a very natural idea of extending the student model . It expands the model complexity of the student model and improves the representation ability of the model without increasing the number of parameters . The idea of PTP is an excellent initializing method . At the same time , the teacher model 's generalization of knowledge is given to students through initialization , and then fine-tuned . The experimental part of this article is also quite sufficient . The ablation experiment illustrates the effectiveness of the two steps for the optimization of results .", "rating": "7: Good paper, accept", "reply_text": "We would like to thank Reviewer 1 for the detailed and thoughtful review . We were delighted to hear that you found our core ideas interesting . Based on reviewers ' feedback , we have included more ablation studies to prove the effectiveness of our method ( Section 4.4 ) and additional explanation behind how we came up with our approach and why we intuitively thought this will improve performance ( Sections 3.2 and 3.3 ) . We hope these revisions made our paper more robust . We would truly appreciate it if you could review our updated paper once more . Thank you ."}}