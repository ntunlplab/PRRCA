{"year": "2017", "forum": "S1X7nhsxl", "title": "Improving Generative Adversarial Networks with Denoising Feature Matching", "decision": "Accept (Poster)", "meta_review": "The idea of using a denoising autoencoder on features of the discriminator is sensible, and explored and described well here. The qualitative results are pretty good, but it would be nice to try some of the more recent likelihood-based methods for quantitative evaluation, as the inception score is not very satisfying. Also it would be interesting to see if this additional term helps in scaling up to larger images.", "reviews": [{"review_id": "S1X7nhsxl-0", "review_text": "This paper is well written, and well presented. This method is using denoise autoencoder to learn an implicit probability distribution helps reduce training difficulty, which is neat. In my view, joint training with an auto-encoder is providing extra auxiliary gradient information to improve generator. Providing auxiliary information may be a methodology to improve GAN. Extra comment: Please add more discussion with EBGAN in next version. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . A revision to be posted shortly will incorporate more discussion of the relationship as described in my question reply ."}, {"review_id": "S1X7nhsxl-1", "review_text": "This paper is about using denoising autoencoders to improve performance in GANs. In particular, the features as determined by the discriminator, of images generated by the generator, are fed into a denoising AE and we try to have these be reconstructed well. I think it's an interesting idea to use this \"extra information\" -- namely the feature representations learned by the discriminator. It seems very much in the spirit of ICLR! My main concern, though, is that I'm not wholly convinced on the nature of the improvement. This method achieves higher inception scores than other methods in some cases, but I have a hard time interpreting these scores and thus a hard time getting excited by the results. In particular, the authors have not convinced me that the benefits outweigh the required additional sophistication both conceptually and implementation-wise (speaking of which, will code be released?). One thing I'd be curious to know is, how hard is it to get this thing to actually work? Also, I view GANs as a means to an end -- while I'm not particularly excited about generating realistic images (especially in 32x32), I'm very excited about the future potential of GAN-based systems. So it would have been nice to see these improvements in inception score translate into improvements in a more useful task. But this criticism could probably apply to many GAN papers and so perhaps isn't fair here. I do think the idea of exploiting \"extra information\" (like discriminator features) is interesting both inside and outside the context of this paper. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and happy new year . I can sympathize with the suspicion that a method adds complexity in a way that may be quite brittle to tuning . I will , however , say that in our case , the method is quite robust ; faster convergence towards reasonable-looking samples as well as increased objectness are visible even with small denoisers , although adding depth improved things . We did not extensively tune the architectural hyperparameters of the denoiser , simply using a hidden layer dimension equal to the input dimension in most cases , and using very simple fully connected networks , trained with the same learning rate as everything else ( 1e-4 , approximately the same as Radford et al ) . This robustness will be further emphasized in a revision to be posted shortly . I hope to release the code in the next week or so after it is cleaned up and pulled together from a few disparate codebases , I apologize for not releasing it sooner as I have been nursing an injury . I too see GANs largely as a means to an end ; the unconditional generation setting allows us to study some known shortcomings in isolation . It is likely that this method can be extended to the conditional case where most applications lie but there are non-obvious choices to be made about incorporating conditioning information into the denoiser , We demonstrate here that the idea appears to bear fruit in the simple case , making it worth investigating such extensions . The Inception score is certainly far from a perfect measure but provides at least some quantitative backing to our more qualitative claims ."}, {"review_id": "S1X7nhsxl-2", "review_text": "The authors present a way to complement the Gerative Adversarial Network traning procedure with an additional term based on denoising autoencoders. The use of denoising autoencoders is motivated by the observation that they implicitly capture the distribution of the data they were trained on. While sampling methods based denoising autoencoders alone don't amount to very interesting generative models (at least no-one could demonstrate otherwise), this paper shows that it can be combined successfully with generative adversarial networks. My overall assessment of this paper is that it is well written, well reasoned, and presents a good idea motivated from first principles. I feel that the idea presented here is not revolutionary or a very radical departure from what has been done before, I would have liked a slightly more structured experiments section which focusses on and provides insights into the relative merits of different choices one could make (see pre-review questions for details), rather than focussing just on demonstrating that a chosen variant works. In addition to this general review, I have already posted specific questions and criticism in the pre-review questions - thanks for the authors' responses. Based on those responses the area I am most uncomfortable about is whether the (Alain & Bengio, 2014) intuition about the denoising autoencoders is valid if it all happens in a nonlinear featurespace. If the denoiser function's behaviour ends up depending on the Jacobian of the nonlinear transformation Phi, another question is whether this dependence is exploitable by the optimization scheme.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review , and happy new year , apologies for the late response , as I said above I have been nursing an injury that makes typing rather unpleasant . The score of the feature distribution ( in the case of an invertible feature extractor ) is related to the score of the original distribution via a locally affine function involving the Jacobian of phi , namely a multiplication by the inverse Jacobian of phi plus an additive term that is essentially the gradient of the log determinant of the same with respect to the point x ( again multiplied by the Jacobian inverse ) . This latter term is rather difficult to interpret . The derivation will be included in a revision to be posted shortly . We would stand by the statement that the procedure remains , as presented , a useful heuristic , making more efficient use of computation already being done in the discriminator forward pass . In addition , I am running two additional experiments ( later than I 'd hoped due to the aforementioned circumstances ) , using a pretrained classifier of identical architecture ( save for the output layer ) as the feature space , and incorporating gradients from the denoising loss into the updates for discriminator parameters . These will be incorporated as soon as possible ."}], "0": {"review_id": "S1X7nhsxl-0", "review_text": "This paper is well written, and well presented. This method is using denoise autoencoder to learn an implicit probability distribution helps reduce training difficulty, which is neat. In my view, joint training with an auto-encoder is providing extra auxiliary gradient information to improve generator. Providing auxiliary information may be a methodology to improve GAN. Extra comment: Please add more discussion with EBGAN in next version. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments . A revision to be posted shortly will incorporate more discussion of the relationship as described in my question reply ."}, "1": {"review_id": "S1X7nhsxl-1", "review_text": "This paper is about using denoising autoencoders to improve performance in GANs. In particular, the features as determined by the discriminator, of images generated by the generator, are fed into a denoising AE and we try to have these be reconstructed well. I think it's an interesting idea to use this \"extra information\" -- namely the feature representations learned by the discriminator. It seems very much in the spirit of ICLR! My main concern, though, is that I'm not wholly convinced on the nature of the improvement. This method achieves higher inception scores than other methods in some cases, but I have a hard time interpreting these scores and thus a hard time getting excited by the results. In particular, the authors have not convinced me that the benefits outweigh the required additional sophistication both conceptually and implementation-wise (speaking of which, will code be released?). One thing I'd be curious to know is, how hard is it to get this thing to actually work? Also, I view GANs as a means to an end -- while I'm not particularly excited about generating realistic images (especially in 32x32), I'm very excited about the future potential of GAN-based systems. So it would have been nice to see these improvements in inception score translate into improvements in a more useful task. But this criticism could probably apply to many GAN papers and so perhaps isn't fair here. I do think the idea of exploiting \"extra information\" (like discriminator features) is interesting both inside and outside the context of this paper. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and happy new year . I can sympathize with the suspicion that a method adds complexity in a way that may be quite brittle to tuning . I will , however , say that in our case , the method is quite robust ; faster convergence towards reasonable-looking samples as well as increased objectness are visible even with small denoisers , although adding depth improved things . We did not extensively tune the architectural hyperparameters of the denoiser , simply using a hidden layer dimension equal to the input dimension in most cases , and using very simple fully connected networks , trained with the same learning rate as everything else ( 1e-4 , approximately the same as Radford et al ) . This robustness will be further emphasized in a revision to be posted shortly . I hope to release the code in the next week or so after it is cleaned up and pulled together from a few disparate codebases , I apologize for not releasing it sooner as I have been nursing an injury . I too see GANs largely as a means to an end ; the unconditional generation setting allows us to study some known shortcomings in isolation . It is likely that this method can be extended to the conditional case where most applications lie but there are non-obvious choices to be made about incorporating conditioning information into the denoiser , We demonstrate here that the idea appears to bear fruit in the simple case , making it worth investigating such extensions . The Inception score is certainly far from a perfect measure but provides at least some quantitative backing to our more qualitative claims ."}, "2": {"review_id": "S1X7nhsxl-2", "review_text": "The authors present a way to complement the Gerative Adversarial Network traning procedure with an additional term based on denoising autoencoders. The use of denoising autoencoders is motivated by the observation that they implicitly capture the distribution of the data they were trained on. While sampling methods based denoising autoencoders alone don't amount to very interesting generative models (at least no-one could demonstrate otherwise), this paper shows that it can be combined successfully with generative adversarial networks. My overall assessment of this paper is that it is well written, well reasoned, and presents a good idea motivated from first principles. I feel that the idea presented here is not revolutionary or a very radical departure from what has been done before, I would have liked a slightly more structured experiments section which focusses on and provides insights into the relative merits of different choices one could make (see pre-review questions for details), rather than focussing just on demonstrating that a chosen variant works. In addition to this general review, I have already posted specific questions and criticism in the pre-review questions - thanks for the authors' responses. Based on those responses the area I am most uncomfortable about is whether the (Alain & Bengio, 2014) intuition about the denoising autoencoders is valid if it all happens in a nonlinear featurespace. If the denoiser function's behaviour ends up depending on the Jacobian of the nonlinear transformation Phi, another question is whether this dependence is exploitable by the optimization scheme.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review , and happy new year , apologies for the late response , as I said above I have been nursing an injury that makes typing rather unpleasant . The score of the feature distribution ( in the case of an invertible feature extractor ) is related to the score of the original distribution via a locally affine function involving the Jacobian of phi , namely a multiplication by the inverse Jacobian of phi plus an additive term that is essentially the gradient of the log determinant of the same with respect to the point x ( again multiplied by the Jacobian inverse ) . This latter term is rather difficult to interpret . The derivation will be included in a revision to be posted shortly . We would stand by the statement that the procedure remains , as presented , a useful heuristic , making more efficient use of computation already being done in the discriminator forward pass . In addition , I am running two additional experiments ( later than I 'd hoped due to the aforementioned circumstances ) , using a pretrained classifier of identical architecture ( save for the output layer ) as the feature space , and incorporating gradients from the denoising loss into the updates for discriminator parameters . These will be incorporated as soon as possible ."}}