{"year": "2019", "forum": "SygInj05Fm", "title": "Physiological Signal Embeddings (PHASE) via Interpretable Stacked Models", "decision": "Reject", "meta_review": "Authors present a technique to learn embeddings over physiological signals independently using univariate LSTMs tasked to predict future values. Supervised methods are them employed over these embeddings. Univariate approach is taken to improve transferability across institutions, and Shapley values are used to provide interpretable insight. The work is interesting, and authors have made a good attempt at answering reviewers' concerns, but more work remains to be done.\n\nPros:\n- R1 & R3: Well written.\n- R3: Transferrable embeddings are useful in this domain, and not often researched.\n\nCons: \n- R3: Method builds embeddings that assume that future task will be relevant to drops in signals. Authors confirm.\n- R3: Performance improvement is marginal versus baselines. Authors essentially confirm that the small improvement is the accurate number.\n- R2 & R3: Interpretability evaluation is not sufficient. Medical expert should rate interpretability of results. Authors did not include or revise according to suggestion.\n\n", "reviews": [{"review_id": "SygInj05Fm-0", "review_text": "The authors present a new method for learning unsupervised embeddings of physiological signals (e.g. time series data) in a healthcare setting. The primary motivation of their paper is transfer learning - the embeddings created by their approach are able to generalize to other hospitals and healthcare settings. Overall I did like this paper. I found it to be easy to read, well motivated, and addressing an important problem in the healthcare domain. As a researcher in this area, it is very true that we are all using our own \"siloed\" data and do not generally have access to large pre-trained models. I hope that others will produce these kinds of models for the community to use. The authors do not explicitly state that they plan to release their code and pre-trained models, but I sincerely hope that is there intent. If they do not plan to do this, then the impact of this work is dramatically reduced. However, I do have a few concerns about the paper, listed below: - It might not be fair to truly call this an unsupervised model. The labels used for evaluation are thresholds on the signals themselves (e.g. SaO2 < 92%) , so the \"unsupervised\" model actually receives some form of supervision, at least using the current evaluation method. Using a truly different prediction task not directly based on the physiological signals (e.g. mortality, complication during surgery, etc) would provide a cleaner example of unsupervised embeddings that are useful for transfer learning. - Differences between PHASE and EMA are statistically significant but unlikely to be clinically meaningful - the largest absolute difference in AP is 0.04, and most are much smaller than this. It's unclear if the performance gains enjoyed by PHASE would meaningfully change clinical decision making in any significant way. - I appreciate the use of XGBoost due to its impressive Kaggle performance, but it strikes me as odd that the authors did not try to fine tune their base model, as that is standard practice for transfer learning. The successes they point to in CV and NLP all use a fine tuning approach, so the evaluation seems incomplete without a performance assessment of fine tuning the base model. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewers for their careful consideration of this manuscript and many suggestions for improvement . In response to the reviewers \u2019 comments we have made changes that we feel substantially improve the manuscript and address the reviewers \u2019 concerns , which we have responded to point-by-point . * '' The authors do not explicitly state \u2026 reduced . '' We thank the reviewer for the excellent point . We intend to release code pertinent to training the LSTM models , obtaining embeddings , predicting with XGB models , and model stacking feature attributions - submitted as a pull request to the SHAP github ( https : //github.com/slundberg/shap ) . We have indicated our intent to do so in the conclusion ( Section 5 Paragraph 3 ) . Additionally , we intend to release our embedding models , which we recommend for use in forecasting `` hypo '' predictions . * '' However , I do have a few concerns about the paper , listed below : - It might not be fair to truly call this an unsupervised model \u2026 useful for transfer learning . '' We thank the reviewer for the great point . When we considered prediction problems for our paper , we focused on largely two aspects : ( i ) clinical importance , and ( ii ) real-time prediction problems , which are an appropriate evaluation setting for time-series embedding methods . Although predicting mortality makes PHASE a purely unsupervised method , mortality is neither a real-time outcome nor is it reliably measured in our data set . The outcomes we considered - hypoxemia , hypotension , and hypocapnia - are representative adverse real-time events caused by surgery complications and are a significant cause of anesthesia-related complications ( Barak et al.Sci.World.Journal , 2015 ; Curley et al.Crit.Care.Med. , 2010 ) . Predicting these events in advance has been considered a promising approach to enable proactive intervention of these events ( Lundberg et al.Nature BME 2018 ) . In order to address the reviewer \u2019 s great point , we create a simulated \u201c unsupervised \u201d setting - when predicting each event , we excluded the corresponding physiological signal from our features . For example , we assumed that SaO2 is not recorded when predicting hypoxemia . Under this setting , we must rely on the remaining signals to predict hypoxemia . This setting is a more unsupervised evaluation in the sense that our outcome is not derived from a signal we create an embedding for . As our results show ( Section 6.3 ; Figure 9 ) , PHASE \u2019 s outperformance is consistent in this setting for hypocapnia and hypotension . For hypoxemia , all representations perform poorly because predicting hypoxemia heavily relies on SaO2 , leaving little signal for the remaining features . Finally , to further address the reviewer \u2019 s comments , we have mitigated claims of PHASE being unsupervised and instead called our LSTM models \u201c partially supervised \u201d throughout the entirety of the manuscript . We denote \u201c partially supervised \u201d to mean LSTMs trained with prediction tasks related to the final downstream prediction . Furthermore , we have refined the discussion in Sections 4.2.1 and 4.2.2 to emphasize that completely unsupervised LSTMs ( e.g. , autoencoders ) are insufficient for downstream \u201c hypo \u201d predictions , which are clinically important perioperative outcomes . In fact , on our datasets , we found that closeness in the LSTM prediction tasks to the ultimate downstream prediction tasks is beneficial to performance as well as transference . In order to change the message of our paper , we have added this to our conclusion as well ( Section 5 Paragraph 2 ) ."}, {"review_id": "SygInj05Fm-1", "review_text": "Summary of the paper: This paper proposes PHASE, a framework to learn the embeddings for physiological signals from medical records, which can be used in downstream prediction tasks, possibly across domains (i.e. different patient distribution). The authors employ separate LSTMs for each signal channel that are trained to predicts the minimum value of the signal in the fixed future time window (5 minutes in this paper). After training the LSTMs, the learned signal embeddings are fed to gradient boosted trees for a specific prediction task (e.g. predicting whether hypoxemia will occur in 5 minutes). Once the LSTMs are trained, they can be re-used for another dataset; the LSMTs are fixed, and generate embeddings that are fed to a new trainable gradient boosted trees for performing a similar task. The authors also combine existing attribution methods (DeepSHAP and Independent TreeSHAP) to provide some explanation of PHASE. The authors use three different datasets to test PHASE's prediction performance, transferability of the embeddings, and interpretation. Pros: - The paper is well-motivated, well-organized and clearly written. The reading experience was smooth. - Given the importance of physiological signals in ICU settings, transferable embeddings can be an important technique in practice - As the authors claim, I am not aware of any notable prior work on transferable physiological signal embeddings. The authors tackle a relatively unexplored territory. Issues: - The authors claim PHASE learns signal embeddings that are transferable. However, the authors train the embeddings to predict the minimum value within the next five time steps, because the downstream tasks are all predicting whether a certain signal goes below some threshold (\"hypo\"xenia, \"hypo\"capnia, \"hypo\"tension). This means the authors designed the embedding learning process with a priori knowledge of the downstream tasks, which significantly weakens theirs claim that PHASE learns transferable embeddings. Word embeddings trained on Wikipedia, or ConvNets trained on ImageNet are not designed to be used in a specific type of downstream tasks. What PHASE demonstrates is basically that \"hypo\"xxxx predictions can be accurately made with pre-training the embeddings to predict a very relevant task. - The authors claim that transferred PHASE embeddings significantly outperform EMA or Raw. But I wouldn't call 0.005-0.02 AP improvement \"significant\". Model 12 in Figure 3 shows better performance than model 2 and 4, but the gap is not that large. - More importantly, the fact that model 10 and model 12 show similar performance is not very surprising. The two hospitals are in the same city, only miles away. Naturally the distribution of the patients would not be too different. Given this, claiming that PHASE embeddings are transferable does not have a strong ground. - The claim for transferable embedding is further weakened by Figure 4. Model 1^p in Figure 4 clearly performs worse than Raw, which means embeddings learned from significantly different setting (hospital P) is actually making it harder for XGB than simply looking at raw signals. If PHASE was learning a robust embeddings, then the learned embeddings should at least not hurt the performance of XGB. - Evaluating the interpretation of the model is weak. All the authors did was pick four examples and provide qualitative explanation. And they do not even describe whether this interpretation is from model 9 or 10. It would have been much better if at least one medical expert took a look at more than a few examples. In the current form, we cannot be sure if the model is using the SaO2 signal in a medically meaningful way. Also, if this is the interpretation of model 10 or 12, then we should look at the attributions for other signals as well. - Lack of description on experiment setup. The authors do not describe how they pre-trained the LSTMs to obtain Min^h, Auto^h and Hypox^h, which significantly hurts reproducibility. Also I couldn't find any description regarding train/test splits or cross validations, or size of the LSTM cells. - More description is necessary as to how Raw was used to train XGB. Was the entire sequence of 15 signals fed to XGB? - Y-axis of Figure 5 is not on the same scale. This makes it hard to intuitively understand the change of SaO2.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank the reviewers for their careful consideration of this manuscript and many suggestions for improvement . In response to the reviewers \u2019 comments we have made changes that we feel substantially improve the manuscript and address the reviewers \u2019 concerns , which we have responded to point-by-point . * \u201d - The authors claim PHASE learns signal embeddings that are transferable . \u2026 to predict a very relevant task. \u201d We thank the reviewer for bringing up an excellent point . When we considered prediction problems for our paper , we focused on largely two aspects : ( i ) clinical importance , and ( ii ) real-time prediction problems , which are an appropriate evaluation setting for time-series embedding methods . The outcomes we considered - hypoxemia , hypotension , and hypocapnia - are representative adverse real-time events caused by surgery complications and are a significant cause of anesthesia-related complications ( Lundberg et al.Nature BME 2018 , Barak et al.Sci.World.Journal , 2015 ; Curley et al.Crit.Care.Med. , 2010 ) . As further justification , perioperative adverse outcomes are often due to signals that are too low in terms of magnitude ( Exclamado et.al.The Laryngoscope 1989 ) . Therefore training models on the lower boundaries of signals ( \u201c hypo \u201d ) would , in all likelihood , cover a non-trivial group of important adverse outcomes . Future work training \u201c hyper \u201d models as well as working with physicians to identify other such groupings of physiological prediction tasks would certainly be meaningful as well . Finally , to further address the reviewer \u2019 s comments , we have mitigated claims of PHASE being unsupervised and instead called our LSTM models \u201c partially supervised \u201d throughout the entirety of the manuscript . We denote \u201c partially supervised \u201d to mean LSTMs trained with prediction tasks related to the final downstream prediction . Furthermore , we have refined the discussion in Sections 4.2.1 Paragraph 2 and 4.2.2 Paragraph 3 to emphasize that completely unsupervised LSTMs ( e.g. , autoencoders ) are insufficient for downstream \u201c hypo \u201d predictions , which are clinically important perioperative outcomes . In fact , on our datasets , we found that closeness in the LSTM prediction tasks to the ultimate downstream prediction tasks is beneficial to performance as well as transference . In order to change the message of our paper , we have added this to our conclusion as well ( Section 5 Paragraph 2 ) . As a last note , we recommend our models for use in forecasting `` hypo '' predictions , a statement we have added to the conclusion ( Section 5 end of Paragraph 3 ) ."}, {"review_id": "SygInj05Fm-2", "review_text": "The authors claim contributions in three areas: 1) Learning representations on physiological signals. The proposed approach uses LSTMS with a loss function that aims at predicting the next five minutes of the physiological signals. Based on their experiments, using this criteria outperforms LSTM autoencoder approaches that are tuned to reconstruct the original signals. The description of this work needs more details. It would be good to have clarity on these loss functions and also on the architecture of the LSTM autoencoder that is claimed here. Is it a standard seq2seq model? Is it something else? 2) They use the hidden state of the LSTMs as a representation of the inputs signals. From this representation, they have setup a set of supervised/predictive tasks to measure the efficacy of the representation. For this, they used gradient boosting machines. 3) They propose a way to estimate interpretability by tracking the impact of the input data on the predictions using an model agnostic approach using Shapley values. I have found this part of the paper particularly obscure. I recommend shedding some light on the structure of this model that generates these Shapley values. The experimental result section also needs work in my opinion. First of all, the authors may want to better describe the data used. How many patients are in this set? How was the data partitioned for training, testing, validation? Any hyper-paremeter tuning? I have found the \u201ctransference\u201d arguments a bit weak. First of all, the physical distance between hospital should not be mentioned as a way to compare \u201chospitals\u201d. How did the authors select these features shown on Figure 2? MIMIC has more features than this. Why were these additional features discarded? Is the data coming from the same type of operating rooms in the case of hospital 0 and 1? I am somehow skeptical on the transfer of embeddings learned in an ICU setting to an OR setting. It would be great to provide details on the type of patients that are being monitored. It is quite hard to argue from what\u2019s presented in 4.3.3 that the proposed approach is interpretable. Can the authors explain how a visual inspection of Figure 5 \u201cmakes sense\u201d as stated in the paper? What is the point that\u2019s being made here? Any reason why more conventional attention mechanisms have not been looked at for interpretability? Overall, I have found the problem addressed here interesting. However, I think that the paper needs work, both on the presentation of the methodology and also on the presentation of more convincing experimental arguments. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank the reviewers for their careful consideration of this manuscript and many suggestions for improvement . In response to the reviewers \u2019 comments we have made changes that we feel substantially improve the manuscript and address the reviewers \u2019 concerns , which we have responded to point-by-point . * \u201c The description of this work needs more details\u2026 Is it something else ? \u201c We thank the reviewer for the beneficial feedback . In terms of the loss functions . we utilize MSE loss for regression objectives ( Min and Auto ) , whereas we use binary cross entropy for classification objects ( Hypox ) . The LSTM autoencoder is a seq2seq model with two layers with 200 LSTM cells each . To better clarify our experimental setup , hyperparameter and model architecture details on the architectures of our LSTM and XGB models have been included in the Appendix : Section 6.2 . * \u201d 3 ) They propose a way to estimate interpretability \u2026 I recommend shedding some light on the structure of this model that generates these Shapley values. \u201d We thank the reviewer for highlighting this point of confusion . Our objective is to validate our model stacked feature attributions on a straightforward univariate model ( corresponding to model 9 in Figure 2a ) , which has been clarified in Section 4.2.3 along with a new quantitative evaluation of interpretability . Appendix : Section 6.5 Figure 11 illustrates the model setup for the interpretability analysis . * \u201d The experimental result section also needs work in my opinion \u2026 hyper-paremeter tuning ? \u201d We thank the reviewer for raising this important point . Although the number of patients is reported in Table 1 , the number of samples in each training , validation , and test set varies depending on the label being evaluated ( which we have included in the Appendix : Section 6.1 Tables 3 , 4 , and 6 ) . Additionally we have added a paragraph describing the labelling methodology in more detail in Section 6.1 as well . There was minimal hyperparameter tuning primarily due to the amount of models trained ( 90+ LSTMs and 108+ XGB models ) . Instead , we have now included the final hyper parameter settings utilized in Appendix : Section 6.2 . * \u201c I have found the \u201c transference \u201d arguments a bit weak . First of all , the physical distance between hospital should not be mentioned as a way to compare \u201c hospitals \u201d . \u201c We thank the reviewer for bringing up a great point . Our aim was simply to suggest the distance between the hospitals might imply a domain shift without revealing the location of hospitals 0 and 1 . We have removed this point . Instead , we describe in detail the distributions of statistics in each hospital - one being operating room data from a level 1 trauma center , one being operating room data from a university medical center , and the third one being waveform data from an ICU ( in Appendix : Section 6.1 , Figures 6 and 7 ) . Additionally , we report the top ten diagnoses from each hospitals in Appendix : Section 6.1 . We find no overlap apart from \u201c CALCULUS OF KIDNEY \u201d between hospitals 0 and 1 . * \u201c How did the authors select these features shown on Figure 2 ? MIMIC has more features than this . Why were these additional features discarded ? \u201c We thank the reviewer for the question . To clarify Figure 5 ( previously Figure 2 ) , the features are from hospitals 0/1 , not hospital P ( MIMIC ) . The hospital P dataset has a set of features that cover 7 ( out of 15 ) features collected in hospitals 0/1 . Therefore , incorporating MIMIC data to our experiments enables us to test PHASE \u2019 s ability to transfer relevant information from physiological signals in a challenging situation where hospitals have different features . Here , we chose to simplify our analysis and focus on hypoxemia , because ( i ) most of the signal for forecasting hypoxemia comes from SaO2 ( which is also a feature that is consistently measured in hospital P ) , and ( ii ) we can test PHASE in a very challenging setting . Additionally , this experimental setting has the added benefit of investigating whether the hospital P embeddings have interaction effects with hospital 0/1 embeddings ."}], "0": {"review_id": "SygInj05Fm-0", "review_text": "The authors present a new method for learning unsupervised embeddings of physiological signals (e.g. time series data) in a healthcare setting. The primary motivation of their paper is transfer learning - the embeddings created by their approach are able to generalize to other hospitals and healthcare settings. Overall I did like this paper. I found it to be easy to read, well motivated, and addressing an important problem in the healthcare domain. As a researcher in this area, it is very true that we are all using our own \"siloed\" data and do not generally have access to large pre-trained models. I hope that others will produce these kinds of models for the community to use. The authors do not explicitly state that they plan to release their code and pre-trained models, but I sincerely hope that is there intent. If they do not plan to do this, then the impact of this work is dramatically reduced. However, I do have a few concerns about the paper, listed below: - It might not be fair to truly call this an unsupervised model. The labels used for evaluation are thresholds on the signals themselves (e.g. SaO2 < 92%) , so the \"unsupervised\" model actually receives some form of supervision, at least using the current evaluation method. Using a truly different prediction task not directly based on the physiological signals (e.g. mortality, complication during surgery, etc) would provide a cleaner example of unsupervised embeddings that are useful for transfer learning. - Differences between PHASE and EMA are statistically significant but unlikely to be clinically meaningful - the largest absolute difference in AP is 0.04, and most are much smaller than this. It's unclear if the performance gains enjoyed by PHASE would meaningfully change clinical decision making in any significant way. - I appreciate the use of XGBoost due to its impressive Kaggle performance, but it strikes me as odd that the authors did not try to fine tune their base model, as that is standard practice for transfer learning. The successes they point to in CV and NLP all use a fine tuning approach, so the evaluation seems incomplete without a performance assessment of fine tuning the base model. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewers for their careful consideration of this manuscript and many suggestions for improvement . In response to the reviewers \u2019 comments we have made changes that we feel substantially improve the manuscript and address the reviewers \u2019 concerns , which we have responded to point-by-point . * '' The authors do not explicitly state \u2026 reduced . '' We thank the reviewer for the excellent point . We intend to release code pertinent to training the LSTM models , obtaining embeddings , predicting with XGB models , and model stacking feature attributions - submitted as a pull request to the SHAP github ( https : //github.com/slundberg/shap ) . We have indicated our intent to do so in the conclusion ( Section 5 Paragraph 3 ) . Additionally , we intend to release our embedding models , which we recommend for use in forecasting `` hypo '' predictions . * '' However , I do have a few concerns about the paper , listed below : - It might not be fair to truly call this an unsupervised model \u2026 useful for transfer learning . '' We thank the reviewer for the great point . When we considered prediction problems for our paper , we focused on largely two aspects : ( i ) clinical importance , and ( ii ) real-time prediction problems , which are an appropriate evaluation setting for time-series embedding methods . Although predicting mortality makes PHASE a purely unsupervised method , mortality is neither a real-time outcome nor is it reliably measured in our data set . The outcomes we considered - hypoxemia , hypotension , and hypocapnia - are representative adverse real-time events caused by surgery complications and are a significant cause of anesthesia-related complications ( Barak et al.Sci.World.Journal , 2015 ; Curley et al.Crit.Care.Med. , 2010 ) . Predicting these events in advance has been considered a promising approach to enable proactive intervention of these events ( Lundberg et al.Nature BME 2018 ) . In order to address the reviewer \u2019 s great point , we create a simulated \u201c unsupervised \u201d setting - when predicting each event , we excluded the corresponding physiological signal from our features . For example , we assumed that SaO2 is not recorded when predicting hypoxemia . Under this setting , we must rely on the remaining signals to predict hypoxemia . This setting is a more unsupervised evaluation in the sense that our outcome is not derived from a signal we create an embedding for . As our results show ( Section 6.3 ; Figure 9 ) , PHASE \u2019 s outperformance is consistent in this setting for hypocapnia and hypotension . For hypoxemia , all representations perform poorly because predicting hypoxemia heavily relies on SaO2 , leaving little signal for the remaining features . Finally , to further address the reviewer \u2019 s comments , we have mitigated claims of PHASE being unsupervised and instead called our LSTM models \u201c partially supervised \u201d throughout the entirety of the manuscript . We denote \u201c partially supervised \u201d to mean LSTMs trained with prediction tasks related to the final downstream prediction . Furthermore , we have refined the discussion in Sections 4.2.1 and 4.2.2 to emphasize that completely unsupervised LSTMs ( e.g. , autoencoders ) are insufficient for downstream \u201c hypo \u201d predictions , which are clinically important perioperative outcomes . In fact , on our datasets , we found that closeness in the LSTM prediction tasks to the ultimate downstream prediction tasks is beneficial to performance as well as transference . In order to change the message of our paper , we have added this to our conclusion as well ( Section 5 Paragraph 2 ) ."}, "1": {"review_id": "SygInj05Fm-1", "review_text": "Summary of the paper: This paper proposes PHASE, a framework to learn the embeddings for physiological signals from medical records, which can be used in downstream prediction tasks, possibly across domains (i.e. different patient distribution). The authors employ separate LSTMs for each signal channel that are trained to predicts the minimum value of the signal in the fixed future time window (5 minutes in this paper). After training the LSTMs, the learned signal embeddings are fed to gradient boosted trees for a specific prediction task (e.g. predicting whether hypoxemia will occur in 5 minutes). Once the LSTMs are trained, they can be re-used for another dataset; the LSMTs are fixed, and generate embeddings that are fed to a new trainable gradient boosted trees for performing a similar task. The authors also combine existing attribution methods (DeepSHAP and Independent TreeSHAP) to provide some explanation of PHASE. The authors use three different datasets to test PHASE's prediction performance, transferability of the embeddings, and interpretation. Pros: - The paper is well-motivated, well-organized and clearly written. The reading experience was smooth. - Given the importance of physiological signals in ICU settings, transferable embeddings can be an important technique in practice - As the authors claim, I am not aware of any notable prior work on transferable physiological signal embeddings. The authors tackle a relatively unexplored territory. Issues: - The authors claim PHASE learns signal embeddings that are transferable. However, the authors train the embeddings to predict the minimum value within the next five time steps, because the downstream tasks are all predicting whether a certain signal goes below some threshold (\"hypo\"xenia, \"hypo\"capnia, \"hypo\"tension). This means the authors designed the embedding learning process with a priori knowledge of the downstream tasks, which significantly weakens theirs claim that PHASE learns transferable embeddings. Word embeddings trained on Wikipedia, or ConvNets trained on ImageNet are not designed to be used in a specific type of downstream tasks. What PHASE demonstrates is basically that \"hypo\"xxxx predictions can be accurately made with pre-training the embeddings to predict a very relevant task. - The authors claim that transferred PHASE embeddings significantly outperform EMA or Raw. But I wouldn't call 0.005-0.02 AP improvement \"significant\". Model 12 in Figure 3 shows better performance than model 2 and 4, but the gap is not that large. - More importantly, the fact that model 10 and model 12 show similar performance is not very surprising. The two hospitals are in the same city, only miles away. Naturally the distribution of the patients would not be too different. Given this, claiming that PHASE embeddings are transferable does not have a strong ground. - The claim for transferable embedding is further weakened by Figure 4. Model 1^p in Figure 4 clearly performs worse than Raw, which means embeddings learned from significantly different setting (hospital P) is actually making it harder for XGB than simply looking at raw signals. If PHASE was learning a robust embeddings, then the learned embeddings should at least not hurt the performance of XGB. - Evaluating the interpretation of the model is weak. All the authors did was pick four examples and provide qualitative explanation. And they do not even describe whether this interpretation is from model 9 or 10. It would have been much better if at least one medical expert took a look at more than a few examples. In the current form, we cannot be sure if the model is using the SaO2 signal in a medically meaningful way. Also, if this is the interpretation of model 10 or 12, then we should look at the attributions for other signals as well. - Lack of description on experiment setup. The authors do not describe how they pre-trained the LSTMs to obtain Min^h, Auto^h and Hypox^h, which significantly hurts reproducibility. Also I couldn't find any description regarding train/test splits or cross validations, or size of the LSTM cells. - More description is necessary as to how Raw was used to train XGB. Was the entire sequence of 15 signals fed to XGB? - Y-axis of Figure 5 is not on the same scale. This makes it hard to intuitively understand the change of SaO2.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We would like to thank the reviewers for their careful consideration of this manuscript and many suggestions for improvement . In response to the reviewers \u2019 comments we have made changes that we feel substantially improve the manuscript and address the reviewers \u2019 concerns , which we have responded to point-by-point . * \u201d - The authors claim PHASE learns signal embeddings that are transferable . \u2026 to predict a very relevant task. \u201d We thank the reviewer for bringing up an excellent point . When we considered prediction problems for our paper , we focused on largely two aspects : ( i ) clinical importance , and ( ii ) real-time prediction problems , which are an appropriate evaluation setting for time-series embedding methods . The outcomes we considered - hypoxemia , hypotension , and hypocapnia - are representative adverse real-time events caused by surgery complications and are a significant cause of anesthesia-related complications ( Lundberg et al.Nature BME 2018 , Barak et al.Sci.World.Journal , 2015 ; Curley et al.Crit.Care.Med. , 2010 ) . As further justification , perioperative adverse outcomes are often due to signals that are too low in terms of magnitude ( Exclamado et.al.The Laryngoscope 1989 ) . Therefore training models on the lower boundaries of signals ( \u201c hypo \u201d ) would , in all likelihood , cover a non-trivial group of important adverse outcomes . Future work training \u201c hyper \u201d models as well as working with physicians to identify other such groupings of physiological prediction tasks would certainly be meaningful as well . Finally , to further address the reviewer \u2019 s comments , we have mitigated claims of PHASE being unsupervised and instead called our LSTM models \u201c partially supervised \u201d throughout the entirety of the manuscript . We denote \u201c partially supervised \u201d to mean LSTMs trained with prediction tasks related to the final downstream prediction . Furthermore , we have refined the discussion in Sections 4.2.1 Paragraph 2 and 4.2.2 Paragraph 3 to emphasize that completely unsupervised LSTMs ( e.g. , autoencoders ) are insufficient for downstream \u201c hypo \u201d predictions , which are clinically important perioperative outcomes . In fact , on our datasets , we found that closeness in the LSTM prediction tasks to the ultimate downstream prediction tasks is beneficial to performance as well as transference . In order to change the message of our paper , we have added this to our conclusion as well ( Section 5 Paragraph 2 ) . As a last note , we recommend our models for use in forecasting `` hypo '' predictions , a statement we have added to the conclusion ( Section 5 end of Paragraph 3 ) ."}, "2": {"review_id": "SygInj05Fm-2", "review_text": "The authors claim contributions in three areas: 1) Learning representations on physiological signals. The proposed approach uses LSTMS with a loss function that aims at predicting the next five minutes of the physiological signals. Based on their experiments, using this criteria outperforms LSTM autoencoder approaches that are tuned to reconstruct the original signals. The description of this work needs more details. It would be good to have clarity on these loss functions and also on the architecture of the LSTM autoencoder that is claimed here. Is it a standard seq2seq model? Is it something else? 2) They use the hidden state of the LSTMs as a representation of the inputs signals. From this representation, they have setup a set of supervised/predictive tasks to measure the efficacy of the representation. For this, they used gradient boosting machines. 3) They propose a way to estimate interpretability by tracking the impact of the input data on the predictions using an model agnostic approach using Shapley values. I have found this part of the paper particularly obscure. I recommend shedding some light on the structure of this model that generates these Shapley values. The experimental result section also needs work in my opinion. First of all, the authors may want to better describe the data used. How many patients are in this set? How was the data partitioned for training, testing, validation? Any hyper-paremeter tuning? I have found the \u201ctransference\u201d arguments a bit weak. First of all, the physical distance between hospital should not be mentioned as a way to compare \u201chospitals\u201d. How did the authors select these features shown on Figure 2? MIMIC has more features than this. Why were these additional features discarded? Is the data coming from the same type of operating rooms in the case of hospital 0 and 1? I am somehow skeptical on the transfer of embeddings learned in an ICU setting to an OR setting. It would be great to provide details on the type of patients that are being monitored. It is quite hard to argue from what\u2019s presented in 4.3.3 that the proposed approach is interpretable. Can the authors explain how a visual inspection of Figure 5 \u201cmakes sense\u201d as stated in the paper? What is the point that\u2019s being made here? Any reason why more conventional attention mechanisms have not been looked at for interpretability? Overall, I have found the problem addressed here interesting. However, I think that the paper needs work, both on the presentation of the methodology and also on the presentation of more convincing experimental arguments. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank the reviewers for their careful consideration of this manuscript and many suggestions for improvement . In response to the reviewers \u2019 comments we have made changes that we feel substantially improve the manuscript and address the reviewers \u2019 concerns , which we have responded to point-by-point . * \u201c The description of this work needs more details\u2026 Is it something else ? \u201c We thank the reviewer for the beneficial feedback . In terms of the loss functions . we utilize MSE loss for regression objectives ( Min and Auto ) , whereas we use binary cross entropy for classification objects ( Hypox ) . The LSTM autoencoder is a seq2seq model with two layers with 200 LSTM cells each . To better clarify our experimental setup , hyperparameter and model architecture details on the architectures of our LSTM and XGB models have been included in the Appendix : Section 6.2 . * \u201d 3 ) They propose a way to estimate interpretability \u2026 I recommend shedding some light on the structure of this model that generates these Shapley values. \u201d We thank the reviewer for highlighting this point of confusion . Our objective is to validate our model stacked feature attributions on a straightforward univariate model ( corresponding to model 9 in Figure 2a ) , which has been clarified in Section 4.2.3 along with a new quantitative evaluation of interpretability . Appendix : Section 6.5 Figure 11 illustrates the model setup for the interpretability analysis . * \u201d The experimental result section also needs work in my opinion \u2026 hyper-paremeter tuning ? \u201d We thank the reviewer for raising this important point . Although the number of patients is reported in Table 1 , the number of samples in each training , validation , and test set varies depending on the label being evaluated ( which we have included in the Appendix : Section 6.1 Tables 3 , 4 , and 6 ) . Additionally we have added a paragraph describing the labelling methodology in more detail in Section 6.1 as well . There was minimal hyperparameter tuning primarily due to the amount of models trained ( 90+ LSTMs and 108+ XGB models ) . Instead , we have now included the final hyper parameter settings utilized in Appendix : Section 6.2 . * \u201c I have found the \u201c transference \u201d arguments a bit weak . First of all , the physical distance between hospital should not be mentioned as a way to compare \u201c hospitals \u201d . \u201c We thank the reviewer for bringing up a great point . Our aim was simply to suggest the distance between the hospitals might imply a domain shift without revealing the location of hospitals 0 and 1 . We have removed this point . Instead , we describe in detail the distributions of statistics in each hospital - one being operating room data from a level 1 trauma center , one being operating room data from a university medical center , and the third one being waveform data from an ICU ( in Appendix : Section 6.1 , Figures 6 and 7 ) . Additionally , we report the top ten diagnoses from each hospitals in Appendix : Section 6.1 . We find no overlap apart from \u201c CALCULUS OF KIDNEY \u201d between hospitals 0 and 1 . * \u201c How did the authors select these features shown on Figure 2 ? MIMIC has more features than this . Why were these additional features discarded ? \u201c We thank the reviewer for the question . To clarify Figure 5 ( previously Figure 2 ) , the features are from hospitals 0/1 , not hospital P ( MIMIC ) . The hospital P dataset has a set of features that cover 7 ( out of 15 ) features collected in hospitals 0/1 . Therefore , incorporating MIMIC data to our experiments enables us to test PHASE \u2019 s ability to transfer relevant information from physiological signals in a challenging situation where hospitals have different features . Here , we chose to simplify our analysis and focus on hypoxemia , because ( i ) most of the signal for forecasting hypoxemia comes from SaO2 ( which is also a feature that is consistently measured in hospital P ) , and ( ii ) we can test PHASE in a very challenging setting . Additionally , this experimental setting has the added benefit of investigating whether the hospital P embeddings have interaction effects with hospital 0/1 embeddings ."}}