{"year": "2020", "forum": "rJehllrtDS", "title": "Rethinking deep active learning: Using unlabeled data at model training", "decision": "Reject", "meta_review": "This paper argues that incorporating unsupervised/semi-supervised learning into the training process can dramatically increase the performance of models. In particular, its incorporation can result in performance gains that dwarf the gains obtained by collecting data actively alone. The experiments effectively demonstrate this phenomenon. \n\nThe paper is written with a tone that implicitly assumes that \"active learning for deep learning is effective\" and therefore it is a surprise and a challenge to the status quo that using unlabelled data in intelligent ways alone gets such a boost. On the contrary, reviewers found that active learning not working very well for deep learning is a well-known state of affairs. This is not surprising because the most effective theoretically justifiable active learning algorithms rely on finite capacity assumptions about the model class, which deep learning disobeys. \n\nThus, the reviewers found the conclusions to lack novelty as the power of semi-supervised and unsupervised learning is well known. Reject. ", "reviews": [{"review_id": "rJehllrtDS-0", "review_text": "This paper argues that active learning (AL) methods shold combine unsupervised and semi-supervised learning during the iterative training process. Combining these complementary is indeed sensible, and this work is therefore a welcome effort. However, the results are quite mixed, and in fact seem to suggest that AL is rather ineffective. Therefore, what one might take from these results is that unsupervised and semi-supervised learning methods can boost predictive performance; but I think this is widely appreciated already. Perhaps a better framing for this work is: AL using standard metrics seems to be comparatively ineffective, especially when one uses pre-training/semi-supervised learning. Some specific comments and questions: - The authors have decided to frame this paper in terms of improving AL using un/semi-supervised learning. But given that, by the authors' own admission, the \"random baseline may actually outperform all other acquisition strategies by a large margin\", what is the motivation for adopting \"AL\" at all? I mean, if we are performing random (iid) sampling, this just reduces to vanilla learning with pre-training and semi-supervision; the 'active' component becomes irrelevant. - I think the characterization of AL is not quite right on page 2. The authors write that AL is focuses on the \"least certain\" instances. This is often true -- namely under the popular uncertainty sampling regime -- but not all acquisition strategies use this heuristic. Indeed, even the geometry method the authors use explicitly ignores classifier confidence. - The use of sampling in the SSL component is interesting, although an ablation here investigating this specific choice (as opposed to, say, naive sampling with uniform probability over unlabeled instances). - I would not characterize the gains brought by unlabeled data here as \"spectacular\". - As is often the case in work on AL, there is no real notion of a 'test set' here; instead the authors repeat experiments using different seed label sets. It is not entirely clear how much hyperparameter/architecture fine tuning was performed informally, but there is a lot going on here, so I would assume at least some. Therefore there is a risk that all results reported are in some sense optimistic, potentially being \"overfit\" to these datasets. It would be best to provide additional comparisons of approaches on completely unseen datasets. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your review ! Please find the response below . 1 . `` one might take from these results is that unsupervised and semi-supervised learning methods can boost predictive performance ; but I think this is widely appreciated already . '' This is not exactly our main message . Of course unsupervised and semi-supervised learning methods are effective by themselves , and there is a lot of recent progress as discussed in our related work . The main point is the effect of unsupervised and semi-supervised learning methods when used in AL , relative to the effect of acquisition strategies ( which are the core of AL ) and the differences thereof . This is not appreciated as much . On the contrary , we clearly discuss cases like [ Wang et al. , 2017 ; Ducoffe & Precioso , 2018 ; Gal et al. , 2017 ] where the combination of semi-supervised and AL is found not so effective or even harmful . 2 . `` Perhaps a better framing for this work is : AL using standard metrics seems to be comparatively ineffective , especially when one uses pre-training/semi-supervised learning . '' Exactly.This is our main message . In the abstract for instance , we say 'we find that the use of unlabeled data during model training brings a spectacular accuracy improvement in image classification , compared to the differences between acquisition strategies . ' ( See below about the criticism on `` spectacular '' . ) We are not the first to observe the similar performance of different acquisition strategies in the context of deep learning . See for instance [ Gissin & Shalev-Shwartz , 2018 ; Chitta et al. , 2019 ; Beluch et al.,2018 ] , discussed extensively in our paper . We confirm their findings and in addition , as a main contribution , we systematically evaluate a number of strategies in the presence or not of unsupervised and semi-supervised learning , showing the relative effectiveness of all ideas in the same experimental setup . 3 . `` given that , by the authors ' own admission , the `` random baseline may actually outperform all other acquisition strategies by a large margin '' , what is the motivation for adopting `` AL '' at all ? I mean , if we are performing random ( iid ) sampling , this just reduces to vanilla learning with pre-training and semi-supervision ; the 'active ' component becomes irrelevant . '' Exactly . 'Random baseline ' means , as always , 'no AL ' . So , overall , we evaluate the learning process with/without all three components : unsupervised , semi-supervised , active . In addition , we consider several acquisition strategies in the case of active . Now , on the actual result : indeed , when the label budget is small , random may be better than any other strategy . For instance , Fig.1 ( a ) , SVHN ( b=100 ) . This we attribute to the weak representation obtained from few labels . The effect is amplified in the +PRE+SEMI version , Fig.4 ( a ) .In the case of CIFAR-10 ( b=100 ) , random and other strategies are all similar in Fig.1 ( b ) , but in the +PRE+SEMI version , Fig.4 ( b ) , this effect happens again . Now , by comparing Fig 4 ( b ) with 4 ( c ) , one realizes that random and the other strategies actually cross at some point around 1000 labels , after which AL becomes effective . This is a non-trivial result that can be very helpful in improving AL strategies . Therefore , our message is not necessarily negative : our recommendation is that unsupervised and semi-supervised should be used in the evaluation of AL methods and this may give rise to ideas for improved strategies , especially in the few label regime . We believe it is beyond the scope of this work to investigate such ideas ."}, {"review_id": "rJehllrtDS-1", "review_text": "This paper explores the setting where unsupervised/semi-supervised learning is combined with active learning. The results are that active learning doesn't really help. This paper is interesting in that it provides additional experiments for the intersection of active learning and unsupervised/semi-supervised learning. However, I don't really see the point of this paper. Active learning and unsupervised/semi-supervised learning have been combined before and there are other papers submitted to ICLR this year that combine these. The paper does not claim to provide anything new algorithmically (other than jLP which appears to work no better than random and isn't really advertised as the point of this paper). The only conclusion that I can draw is that sometimes unsupervised/semi-supervised learning works better than active learning, but no understanding of when and why this is the case (from other papers, it is not always the case). Comments: - Although the paper claims to yield a general framework, it only does so partially. For instance, the framework in this work is restricted to semi-supervised methods that use pseudo-labels. - It may be the case that active learning doesn't help or even hurts because the batch size is too large and/or the initial seed set size is too small. Although this paper varies the acquisition strategies, these other hyper-parameters are equally, if not more, important.", "rating": "3: Weak Reject", "reply_text": "Thank you for your review ! Please find the response below . 1 . `` The only conclusion that I can draw is that sometimes unsupervised/semi-supervised learning works better than active learning , but no understanding of when and why this is the case ( from other papers , it is not always the case ) . '' Unsupervised/semi-supervised learning helps in all cases and the gain is significantly greater than the differences between AL acquisition strategies . Moreover , in certain cases of few labels ( small budget b ) , all acquisition strategies are outperformed by Random , an effect that is amplified by the presence of unsupervised/semi-supervised learning . We can not see where the conclusion above is drawn from . Could the reviewer please elaborate ? What other papers are meant ? 2 . `` the framework in this work is restricted to semi-supervised methods that use pseudo-labels . '' In each cycle , a new set of labels becomes available . The set of labeled example grows and there is also a set of unlabeled examples . At this point , a model is learned . Standard AL uses only the labeled examples to learn the model . Semi-supervised methods use the unlabeled examples too . There is no constraint as to what semi-supervised method one can choose . We chose a method that uses pseudo-labels . Any other method could be used . 3 . `` It may be the case that active learning does n't help or even hurts because the batch size is too large and/or the initial seed set size is too small . '' We investigated different label budget scenarios including choices commonly used in AL papers ( budget of 1k ) as well as providing extreme cases ( budget of 10 ) . In general , we actually use equal or smaller budgets than prior work , because few labels is the most interesting case . The initial seed set is the same as the label budget per cycle according to the standard protocol . According to [ Gissin & Shalev-Shwartz ; 2018 ] , differences between acquisition functions are even smaller when using a larger budget ( 5k ) . 4 . `` Active learning and unsupervised/semi-supervised learning have been combined before ... '' Please see R1 point 6 . 5. '' ... there are other papers submitted to ICLR this year that combine these . '' Maybe the reviewer wants to reconsider this comment ."}, {"review_id": "rJehllrtDS-2", "review_text": "The authors study the problem of incorporating unsupervised (representation pre-training) learning and semi-supervised learning into active learning for image classification; specifically, performing pre-training before active learning starts [Caron, et al., 2018] and then applying inductive label propagation [Issen, et al., 2019] (slightly modification in the cost function to look more like importance sampling) before active learning querying occurs for each round (Algorithm 1). The most novel technical innovation of this submission is the joint label propagation (jLP) querying function (which is a method of \u2018spanning\u2019 the learned manifold space). Experiments are conducted on four (multi-class) image classification datasets (MNIST, SVHN, CIFAR-10, CIFAR-100), showing that unsupervised learning and semi-supervised learning can improve active learning on these datasets \u2014 although random selection often works better (as best as I can tell) implying that negative results are also a contribution of this paper. Finally, some active learning experiments are conducted using a per-round label budget of one example per class \u2014 also demonstrating mixed results with random sampling performing better in general. In my mind, this paper has two primary components: (1) taking the position that semi-supervised and unsupervised learning can improve overall performance and, in principle, help with active learning and (2) propose jLP, which is a learning algorithm agnostic approach to spanning the manifold space. However, jLP doesn\u2019t really seem to work in general. Thus, the main result is the first point \u2014 updating previous (pre-deep learning) results on SS/US AL to deep learning. Honestly, I think the primary conclusion is that semi-supervised and unsupervised learning has improved over the past decade (especially semi-supervised learning for image classification). The second result is that active learning in deep learning (at least for this application) hasn\u2019t kept up. Wrt to (1), as the authors have pointed out, many others have applied semi-supervised learning to AL (including more that the authors didn\u2019t include). Additionally, many have used unsupervised learning for AL (which the authors seem less aware of) from pre-clustering (e.g., [Nguyen & Smeulders, Active Learning using Pre-clustering; ICML04]) to one/few-shot learning (e.g., [Woodward & Finn, Active One-Shot Learning; NeurIPS16 workshop]) to using pre-trained embeddings for many \u2018real-world tasks\u2019 (e.g., NER [Shen, et al., Deep Active Learning for Named Entity Recognition; ICLR18] using word2vec). Thus, the interesting question would be to compare multiple pre-training techniques and ideally the relative effect on the active learning component (assuming this is the focus of the paper). With respect to semi-supervised learning, they have validated that inductive label propagation [Issen, et al., 2019] works for this task, but haven\u2019t shown that this helps with active learning. Since this is a negative results without a theoretical contribution, I would again expect trying several semi-supervised algorithm and evaluating their relative performance in general and wrt the active learning querying strategy. Accordingly, I don\u2019t think the contribution of this work in its current state is sufficiently well-developed \u2014 and would lean toward rejecting in its current form. Below are some additional detailed comments (some also covered above): \u2014 Given that this points toward a negative result, a more convincing direction to take would be to consider more combinations of unsupervised and semi-supervised approaches \u2014 specifically emphasizing how they affect the active learning component. This might point to more general findings and maybe toward a theory (maybe even consider a second application). \u2014 The empirical emphasis is more around overall performance rather than the interaction between unsupervised representation learning and active learning, which is more toward the stated goal of the paper. \u2014 Wouldn\u2019t the right way to do (deep) representation learning in multiple rounds be to fine-tune at least some fraction of the time? If the only claim is pre-training or pre-clustering, people certainly do this \u2014 just often not as a point of emphasis. \u2014 The \u2018first semi-supervised\u2019 claim really only holds in the context of deep learning; however, scope is really more like semi-supervised applied to image classification, which would be a pretty narrow contribution in scope. \u2014 Overall, there is a general overstatement of contributions and results: this is certainly not the first SSAL or USAL and the statement relative to deep learning is subtle; some of the empirical results are interesting, but I am not sure about \u2018spectacular gains\u2019 (and these gains aren\u2019t seemingly due to the contribution of the paper). \u2014 I don\u2019t understand the ensemble model analogy in the abstract; is it because it is a \u2018meta-algorithm\u2019? Some more positive notes: + It is interesting that there is some contradictory evidence relative to [Wang, et al., 2017; Gal, et al., 2017]; this is probably worth digging into a bit deeper. + The experimental details well-described given space constraints. In summary, there are some interesting observations that are probably worth pursuing. However, the current contribution is basically that: (1) active learning doesn\u2019t seem to really help, (2) semi-supervised learning and unsupervised learning improve performance for this task. Since (1) was really the point of the paper (as stated) in the title, I don\u2019t think there is enough here to accept in its current form.", "rating": "3: Weak Reject", "reply_text": "Thank you for your review . Please find our response below . 1. '' ... the interesting question would be to compare multiple pre-training techniques and ideally the relative effect on the active learning component ( assuming this is the focus of the paper ) . ... Since this is a negative results without a theoretical contribution , I would again expect trying several semi-supervised algorithm and evaluating their relative performance in general and wrt the active learning querying strategy . Accordingly , I don \u2019 t think the contribution of this work in its current state is sufficiently well-developed - and would lean toward rejecting in its current form . '' The focus is indeed AL , this is why we consider several options for acquisition function . The focus is not unsupervised or semi-supervised learning , this is why we make a single choice for each . As we explicitly discuss in section 7 , 'Our pipeline is as simple as possible , facilitating comparisons with more effective choices , which can only strengthen our results ' . Stated otherwise , there is at least one choice of unsupervised and semi-supervised learning that yields significantly greater gain than any AL acquisition function over Random , or even makes all AL acquisition functions significantly inferior to Random in certain cases in the few label regime . This raises the question of rethinking at least how we should evaluate deep AL methods , as implied by the title . Any stronger unsupervised pre-training or semi-supervised method could only increase the gain , thus strengthening our conclusions . Considering that we already experiment on several acquisition functions and cycles , several datasets and label budgets , with/without PRE , with/without SEMI , adding any more options would also make our experiments cluttered ; the plots of Fig.4 are already hardly readable . We can not follow the argument that a negative empirical result needs to be compensated by exhaustive sets of experiments . If a future work needs to validate that a new acquisition function outperforms others even in the presence of unsupervised/semi-supervised learning , would that validation need to include several options too ? 2 . `` However , the current contribution is basically that : ( 1 ) active learning doesn \u2019 t seem to really help , ( 2 ) semi-supervised learning and unsupervised learning improve performance for this task . Since ( 1 ) was really the point of the paper ( as stated ) in the title , I don \u2019 t think there is enough here to accept in its current form . '' Does that mean that negative results are not welcome ? 3 . `` With respect to semi-supervised learning , they have validated that inductive label propagation [ Issen , et al. , 2019 ] works for this task , but haven \u2019 t shown that this helps with active learning . ... The empirical emphasis is more around overall performance rather than the interaction between unsupervised representation learning and active learning , which is more toward the stated goal of the paper . '' It is known that unsupervised pre-training and semi-supervised learning help . The same is known for AL . What is not known is what is the relative gain of each of the three components on the same experimental setup . Algorithm 1 is exactly a combination of the three components and different combinations are systematically evaluated across all datasets , label budgets , and acquisition strategies . Our work is exactly on the interaction of the different components : - The finding that AL strategies are all outperformed by Random in certain cases of limited labels , indicates the effect of the quality of the representation on AL . - The a new acquisition function ( jLP ) , more than a technical innovation , is exactly investigating whether manifold similarity ( an idea coming from label propagation ) helps in acquisition ( which it does n't , in line with our claim that unlabeled data should contribute to parameter updates ) . - The study of Appendix B investigates the effect of selected examples on label propagation , partially explaining why different acquisition strategies perform similarly , at least in the presence of label propagation ."}], "0": {"review_id": "rJehllrtDS-0", "review_text": "This paper argues that active learning (AL) methods shold combine unsupervised and semi-supervised learning during the iterative training process. Combining these complementary is indeed sensible, and this work is therefore a welcome effort. However, the results are quite mixed, and in fact seem to suggest that AL is rather ineffective. Therefore, what one might take from these results is that unsupervised and semi-supervised learning methods can boost predictive performance; but I think this is widely appreciated already. Perhaps a better framing for this work is: AL using standard metrics seems to be comparatively ineffective, especially when one uses pre-training/semi-supervised learning. Some specific comments and questions: - The authors have decided to frame this paper in terms of improving AL using un/semi-supervised learning. But given that, by the authors' own admission, the \"random baseline may actually outperform all other acquisition strategies by a large margin\", what is the motivation for adopting \"AL\" at all? I mean, if we are performing random (iid) sampling, this just reduces to vanilla learning with pre-training and semi-supervision; the 'active' component becomes irrelevant. - I think the characterization of AL is not quite right on page 2. The authors write that AL is focuses on the \"least certain\" instances. This is often true -- namely under the popular uncertainty sampling regime -- but not all acquisition strategies use this heuristic. Indeed, even the geometry method the authors use explicitly ignores classifier confidence. - The use of sampling in the SSL component is interesting, although an ablation here investigating this specific choice (as opposed to, say, naive sampling with uniform probability over unlabeled instances). - I would not characterize the gains brought by unlabeled data here as \"spectacular\". - As is often the case in work on AL, there is no real notion of a 'test set' here; instead the authors repeat experiments using different seed label sets. It is not entirely clear how much hyperparameter/architecture fine tuning was performed informally, but there is a lot going on here, so I would assume at least some. Therefore there is a risk that all results reported are in some sense optimistic, potentially being \"overfit\" to these datasets. It would be best to provide additional comparisons of approaches on completely unseen datasets. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your review ! Please find the response below . 1 . `` one might take from these results is that unsupervised and semi-supervised learning methods can boost predictive performance ; but I think this is widely appreciated already . '' This is not exactly our main message . Of course unsupervised and semi-supervised learning methods are effective by themselves , and there is a lot of recent progress as discussed in our related work . The main point is the effect of unsupervised and semi-supervised learning methods when used in AL , relative to the effect of acquisition strategies ( which are the core of AL ) and the differences thereof . This is not appreciated as much . On the contrary , we clearly discuss cases like [ Wang et al. , 2017 ; Ducoffe & Precioso , 2018 ; Gal et al. , 2017 ] where the combination of semi-supervised and AL is found not so effective or even harmful . 2 . `` Perhaps a better framing for this work is : AL using standard metrics seems to be comparatively ineffective , especially when one uses pre-training/semi-supervised learning . '' Exactly.This is our main message . In the abstract for instance , we say 'we find that the use of unlabeled data during model training brings a spectacular accuracy improvement in image classification , compared to the differences between acquisition strategies . ' ( See below about the criticism on `` spectacular '' . ) We are not the first to observe the similar performance of different acquisition strategies in the context of deep learning . See for instance [ Gissin & Shalev-Shwartz , 2018 ; Chitta et al. , 2019 ; Beluch et al.,2018 ] , discussed extensively in our paper . We confirm their findings and in addition , as a main contribution , we systematically evaluate a number of strategies in the presence or not of unsupervised and semi-supervised learning , showing the relative effectiveness of all ideas in the same experimental setup . 3 . `` given that , by the authors ' own admission , the `` random baseline may actually outperform all other acquisition strategies by a large margin '' , what is the motivation for adopting `` AL '' at all ? I mean , if we are performing random ( iid ) sampling , this just reduces to vanilla learning with pre-training and semi-supervision ; the 'active ' component becomes irrelevant . '' Exactly . 'Random baseline ' means , as always , 'no AL ' . So , overall , we evaluate the learning process with/without all three components : unsupervised , semi-supervised , active . In addition , we consider several acquisition strategies in the case of active . Now , on the actual result : indeed , when the label budget is small , random may be better than any other strategy . For instance , Fig.1 ( a ) , SVHN ( b=100 ) . This we attribute to the weak representation obtained from few labels . The effect is amplified in the +PRE+SEMI version , Fig.4 ( a ) .In the case of CIFAR-10 ( b=100 ) , random and other strategies are all similar in Fig.1 ( b ) , but in the +PRE+SEMI version , Fig.4 ( b ) , this effect happens again . Now , by comparing Fig 4 ( b ) with 4 ( c ) , one realizes that random and the other strategies actually cross at some point around 1000 labels , after which AL becomes effective . This is a non-trivial result that can be very helpful in improving AL strategies . Therefore , our message is not necessarily negative : our recommendation is that unsupervised and semi-supervised should be used in the evaluation of AL methods and this may give rise to ideas for improved strategies , especially in the few label regime . We believe it is beyond the scope of this work to investigate such ideas ."}, "1": {"review_id": "rJehllrtDS-1", "review_text": "This paper explores the setting where unsupervised/semi-supervised learning is combined with active learning. The results are that active learning doesn't really help. This paper is interesting in that it provides additional experiments for the intersection of active learning and unsupervised/semi-supervised learning. However, I don't really see the point of this paper. Active learning and unsupervised/semi-supervised learning have been combined before and there are other papers submitted to ICLR this year that combine these. The paper does not claim to provide anything new algorithmically (other than jLP which appears to work no better than random and isn't really advertised as the point of this paper). The only conclusion that I can draw is that sometimes unsupervised/semi-supervised learning works better than active learning, but no understanding of when and why this is the case (from other papers, it is not always the case). Comments: - Although the paper claims to yield a general framework, it only does so partially. For instance, the framework in this work is restricted to semi-supervised methods that use pseudo-labels. - It may be the case that active learning doesn't help or even hurts because the batch size is too large and/or the initial seed set size is too small. Although this paper varies the acquisition strategies, these other hyper-parameters are equally, if not more, important.", "rating": "3: Weak Reject", "reply_text": "Thank you for your review ! Please find the response below . 1 . `` The only conclusion that I can draw is that sometimes unsupervised/semi-supervised learning works better than active learning , but no understanding of when and why this is the case ( from other papers , it is not always the case ) . '' Unsupervised/semi-supervised learning helps in all cases and the gain is significantly greater than the differences between AL acquisition strategies . Moreover , in certain cases of few labels ( small budget b ) , all acquisition strategies are outperformed by Random , an effect that is amplified by the presence of unsupervised/semi-supervised learning . We can not see where the conclusion above is drawn from . Could the reviewer please elaborate ? What other papers are meant ? 2 . `` the framework in this work is restricted to semi-supervised methods that use pseudo-labels . '' In each cycle , a new set of labels becomes available . The set of labeled example grows and there is also a set of unlabeled examples . At this point , a model is learned . Standard AL uses only the labeled examples to learn the model . Semi-supervised methods use the unlabeled examples too . There is no constraint as to what semi-supervised method one can choose . We chose a method that uses pseudo-labels . Any other method could be used . 3 . `` It may be the case that active learning does n't help or even hurts because the batch size is too large and/or the initial seed set size is too small . '' We investigated different label budget scenarios including choices commonly used in AL papers ( budget of 1k ) as well as providing extreme cases ( budget of 10 ) . In general , we actually use equal or smaller budgets than prior work , because few labels is the most interesting case . The initial seed set is the same as the label budget per cycle according to the standard protocol . According to [ Gissin & Shalev-Shwartz ; 2018 ] , differences between acquisition functions are even smaller when using a larger budget ( 5k ) . 4 . `` Active learning and unsupervised/semi-supervised learning have been combined before ... '' Please see R1 point 6 . 5. '' ... there are other papers submitted to ICLR this year that combine these . '' Maybe the reviewer wants to reconsider this comment ."}, "2": {"review_id": "rJehllrtDS-2", "review_text": "The authors study the problem of incorporating unsupervised (representation pre-training) learning and semi-supervised learning into active learning for image classification; specifically, performing pre-training before active learning starts [Caron, et al., 2018] and then applying inductive label propagation [Issen, et al., 2019] (slightly modification in the cost function to look more like importance sampling) before active learning querying occurs for each round (Algorithm 1). The most novel technical innovation of this submission is the joint label propagation (jLP) querying function (which is a method of \u2018spanning\u2019 the learned manifold space). Experiments are conducted on four (multi-class) image classification datasets (MNIST, SVHN, CIFAR-10, CIFAR-100), showing that unsupervised learning and semi-supervised learning can improve active learning on these datasets \u2014 although random selection often works better (as best as I can tell) implying that negative results are also a contribution of this paper. Finally, some active learning experiments are conducted using a per-round label budget of one example per class \u2014 also demonstrating mixed results with random sampling performing better in general. In my mind, this paper has two primary components: (1) taking the position that semi-supervised and unsupervised learning can improve overall performance and, in principle, help with active learning and (2) propose jLP, which is a learning algorithm agnostic approach to spanning the manifold space. However, jLP doesn\u2019t really seem to work in general. Thus, the main result is the first point \u2014 updating previous (pre-deep learning) results on SS/US AL to deep learning. Honestly, I think the primary conclusion is that semi-supervised and unsupervised learning has improved over the past decade (especially semi-supervised learning for image classification). The second result is that active learning in deep learning (at least for this application) hasn\u2019t kept up. Wrt to (1), as the authors have pointed out, many others have applied semi-supervised learning to AL (including more that the authors didn\u2019t include). Additionally, many have used unsupervised learning for AL (which the authors seem less aware of) from pre-clustering (e.g., [Nguyen & Smeulders, Active Learning using Pre-clustering; ICML04]) to one/few-shot learning (e.g., [Woodward & Finn, Active One-Shot Learning; NeurIPS16 workshop]) to using pre-trained embeddings for many \u2018real-world tasks\u2019 (e.g., NER [Shen, et al., Deep Active Learning for Named Entity Recognition; ICLR18] using word2vec). Thus, the interesting question would be to compare multiple pre-training techniques and ideally the relative effect on the active learning component (assuming this is the focus of the paper). With respect to semi-supervised learning, they have validated that inductive label propagation [Issen, et al., 2019] works for this task, but haven\u2019t shown that this helps with active learning. Since this is a negative results without a theoretical contribution, I would again expect trying several semi-supervised algorithm and evaluating their relative performance in general and wrt the active learning querying strategy. Accordingly, I don\u2019t think the contribution of this work in its current state is sufficiently well-developed \u2014 and would lean toward rejecting in its current form. Below are some additional detailed comments (some also covered above): \u2014 Given that this points toward a negative result, a more convincing direction to take would be to consider more combinations of unsupervised and semi-supervised approaches \u2014 specifically emphasizing how they affect the active learning component. This might point to more general findings and maybe toward a theory (maybe even consider a second application). \u2014 The empirical emphasis is more around overall performance rather than the interaction between unsupervised representation learning and active learning, which is more toward the stated goal of the paper. \u2014 Wouldn\u2019t the right way to do (deep) representation learning in multiple rounds be to fine-tune at least some fraction of the time? If the only claim is pre-training or pre-clustering, people certainly do this \u2014 just often not as a point of emphasis. \u2014 The \u2018first semi-supervised\u2019 claim really only holds in the context of deep learning; however, scope is really more like semi-supervised applied to image classification, which would be a pretty narrow contribution in scope. \u2014 Overall, there is a general overstatement of contributions and results: this is certainly not the first SSAL or USAL and the statement relative to deep learning is subtle; some of the empirical results are interesting, but I am not sure about \u2018spectacular gains\u2019 (and these gains aren\u2019t seemingly due to the contribution of the paper). \u2014 I don\u2019t understand the ensemble model analogy in the abstract; is it because it is a \u2018meta-algorithm\u2019? Some more positive notes: + It is interesting that there is some contradictory evidence relative to [Wang, et al., 2017; Gal, et al., 2017]; this is probably worth digging into a bit deeper. + The experimental details well-described given space constraints. In summary, there are some interesting observations that are probably worth pursuing. However, the current contribution is basically that: (1) active learning doesn\u2019t seem to really help, (2) semi-supervised learning and unsupervised learning improve performance for this task. Since (1) was really the point of the paper (as stated) in the title, I don\u2019t think there is enough here to accept in its current form.", "rating": "3: Weak Reject", "reply_text": "Thank you for your review . Please find our response below . 1. '' ... the interesting question would be to compare multiple pre-training techniques and ideally the relative effect on the active learning component ( assuming this is the focus of the paper ) . ... Since this is a negative results without a theoretical contribution , I would again expect trying several semi-supervised algorithm and evaluating their relative performance in general and wrt the active learning querying strategy . Accordingly , I don \u2019 t think the contribution of this work in its current state is sufficiently well-developed - and would lean toward rejecting in its current form . '' The focus is indeed AL , this is why we consider several options for acquisition function . The focus is not unsupervised or semi-supervised learning , this is why we make a single choice for each . As we explicitly discuss in section 7 , 'Our pipeline is as simple as possible , facilitating comparisons with more effective choices , which can only strengthen our results ' . Stated otherwise , there is at least one choice of unsupervised and semi-supervised learning that yields significantly greater gain than any AL acquisition function over Random , or even makes all AL acquisition functions significantly inferior to Random in certain cases in the few label regime . This raises the question of rethinking at least how we should evaluate deep AL methods , as implied by the title . Any stronger unsupervised pre-training or semi-supervised method could only increase the gain , thus strengthening our conclusions . Considering that we already experiment on several acquisition functions and cycles , several datasets and label budgets , with/without PRE , with/without SEMI , adding any more options would also make our experiments cluttered ; the plots of Fig.4 are already hardly readable . We can not follow the argument that a negative empirical result needs to be compensated by exhaustive sets of experiments . If a future work needs to validate that a new acquisition function outperforms others even in the presence of unsupervised/semi-supervised learning , would that validation need to include several options too ? 2 . `` However , the current contribution is basically that : ( 1 ) active learning doesn \u2019 t seem to really help , ( 2 ) semi-supervised learning and unsupervised learning improve performance for this task . Since ( 1 ) was really the point of the paper ( as stated ) in the title , I don \u2019 t think there is enough here to accept in its current form . '' Does that mean that negative results are not welcome ? 3 . `` With respect to semi-supervised learning , they have validated that inductive label propagation [ Issen , et al. , 2019 ] works for this task , but haven \u2019 t shown that this helps with active learning . ... The empirical emphasis is more around overall performance rather than the interaction between unsupervised representation learning and active learning , which is more toward the stated goal of the paper . '' It is known that unsupervised pre-training and semi-supervised learning help . The same is known for AL . What is not known is what is the relative gain of each of the three components on the same experimental setup . Algorithm 1 is exactly a combination of the three components and different combinations are systematically evaluated across all datasets , label budgets , and acquisition strategies . Our work is exactly on the interaction of the different components : - The finding that AL strategies are all outperformed by Random in certain cases of limited labels , indicates the effect of the quality of the representation on AL . - The a new acquisition function ( jLP ) , more than a technical innovation , is exactly investigating whether manifold similarity ( an idea coming from label propagation ) helps in acquisition ( which it does n't , in line with our claim that unlabeled data should contribute to parameter updates ) . - The study of Appendix B investigates the effect of selected examples on label propagation , partially explaining why different acquisition strategies perform similarly , at least in the presence of label propagation ."}}