{"year": "2019", "forum": "BJl_VnR9Km", "title": "A  Model Cortical Network for Spatiotemporal Sequence Learning and Prediction", "decision": "Reject", "meta_review": "There was major disagreement between reviewers on this paper. Two reviewers recommend acceptance, and one firm rejection. The initial version of the manuscript was of poor quality in terms of exposition, as noted by all reviewers. However, the authors responded carefully and thoroughly to reviewer comments, and major clarity and technical issues were resolved by all authors. \n\nI ask PCs to note that the paper, as originally submitted, was not fit for acceptance, and reviewers noted major changes during the review process. I do believe this behavior should be discouraged, since it effectively requires reviewers to examine the paper twice. Regardless, the final overall score of the paper does not meet the bar for acceptance into ICLR.", "reviews": [{"review_id": "BJl_VnR9Km-0", "review_text": "Summary: The paper presents a novel architecture for video prediction consisting of a feed-forward path with sparse convolutions and an LSTM generating predictions of chunks of video based on the sequence of input chunks. A feedback path links the LSTMs of the different sparse prediction modules. Experiments in video prediction are performed on moving-MNIST and the KTH action recognition dataset and the model achieves state-of-the-art performance on both. Interestingly, the model is exhibits prediction suppressions effects as have been observed during neurophysiological experiments in the inferotemporal cortex of macaque monkeys. The proposed method exhibits prediction suppression effects also in the lower layers, motivating a neurophysiological experiment in the earlier V1/V2 regions, which yielded an observation similar to the model\u2019s prediction. Strengths: The performance improvements over competing methods on Moving-MNIST and KTH presented in the experimental section are significant. The analysis seems fairly thorough. Weaknesses and requests for clarification: - The description of the sparse predictive module is difficult to follow, and I am not sure I understood it completely. I find it a bit unintuitive to start the description with the errors, instead of explaining what is computed from beginning to end. The section reads more like a loose description of isolated parts instead of an integrated whole. Maybe walking the reader step-by-step through one complete iteration of the computation helps to clarify this. Also, not every character in equations 1-5 and the algorithm has been defined. For example, what is L? - The text makes it sound like the idea of using 3d convolutions in a convLSTM is novel. 3D convLSTMs have been previously used in 3d vision, see Choy, C. B., Xu, D., Gwak, J., Chen, K., & Savarese, S. (2016, October). 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In European conference on computer vision (pp. 628-644). Springer, Cham. The application of 3d convLSTMs to video might be new, but the mentioned paper by Choy et al. (2016) should be cited. - You mention that padding is used for rows and columns. Are you using padding on the temporal axis as well? - The paper seems to be written in a rush, as it contains way too many typos and grammar mistakes, e.g. \u201ca hierarchical of\u201d (should be \u201ca hierarchy of\u201c or just \u201chierarchical\u201d), \u201cfeedforwad\u201d, \u201cExpriment\u201d (section 4 heading), \u201cachievedbetter\u201d, \u201ctrained monkeys to image pairs\u201d, \u201cpervious\u201d, \u201cperserves\u201d, \u201cprocessure\u201d, \u201csequnence\u201d \u201cviusal\u201d. Many typos could have been caught by a spellcheck! This would improve readability a lot! - The citations are not properly formatted: (1) If the author names are used as part of the sentence, use e.g. Lotter et al. (2016), else (2) If the author names are not part of the sentence, use (Lotter et al., 2016). These two styles are mixed randomly in the current draft. This makes the manuscript, which already contains a lot of language mistakes, difficult to read. - Abbreviations that are used but not introduced: CNN, IT, PSTH, DCNN, LSTM. - The related work section could benefit from referring to some of the related work in neuroscience. - Adding a sentence explaining the intuition behind using SatLU in equation (1) might be helpful To summarize my feedback: I think experimental results and analysis are strong, but the presentation is strongly lacking! The description of the approach definitely needs to be improved to make replication of the results easier. It might help to have someone who doesn\u2019t know the model already read the description and explain it back to you while revising the draft. I hope I could provide some helpful suggestions. I would recommend the manuscript for acceptance, if the presentation is significantly improved!", "rating": "7: Good paper, accept", "reply_text": "Thank you for the valuable feedback and comments . Below we address your comments point by point . 1.The description of the sparse predictive module is difficult to follow , and I am not sure I understood it completely . I find it a bit unintuitive to start the description with the errors , instead of explaining what is computed from beginning to end . The section reads more like a loose description of isolated parts instead of an integrated whole . Maybe walking the reader step-by-step through one complete iteration of the computation helps to clarify this . Also , not every character in equations 1-5 and the algorithm has been defined . For example , what is L ? Re : Now we call each layer a \u2018 Cortical Module ( CM ) \u2019 and provide a more concise and precise description of the model and algorithm in section 3 . We also provide a step-by-step description of the flow of the algorithm per your advice . For clarity , we decompose the description of the feedforward path into Figure 1 ( a ) and Figure 1 ( b ) into a normal DCNN path and a sparsified DCNN part to make it more understandable . The feedforward path is just a normal convolutional neural network but it is trainable by self-supervised learning because its feedforward input does project to the LSTM in each layer . The sparse convolution scheme ( Figure 1b ) only serves to make it more efficient ( see Figure 4d ) , and is not really a critical part of the model . 2.The text makes it sound like the idea of using 3d convolutions in a convLSTM is novel . 3D convLSTMs have been previously used in 3d vision , see Choy , C. B. , Xu , D. , Gwak , J. , Chen , K. , & Savarese , S. ( 2016 , October ) . 3d-r2n2 : A unified approach for single and multi-view 3d object reconstruction . In European conference on computer vision ( pp.628-644 ) .Springer , Cham . The application of 3d convLSTMs to video might be new , but the mentioned paper by Choy et al . ( 2016 ) should be cited . Re : Thank you for pointing that out . We added the citation of Choy et al.We believe , as you pointed out , that using 3D convLSTM in video , especially for video prediction , might be new though it seems to be an obvious thing to do . But we don \u2019 t think this is the main contribution of the paper . 3.You mention that padding is used for rows and columns . Are you using padding on the temporal axis as well ? Re : We used padding on both spatial and temporal domains . 4.The paper seems to be written in a rush , as it contains way too many typos and grammar mistakes , e.g. \u201c a hierarchical of \u201d ( should be \u201c a hierarchy of \u201c or just \u201c hierarchical \u201d ) , \u201c feedforwad \u201d , \u201c Experiment \u201d ( section 4 heading ) , \u201c achieved better \u201d , \u201c trained monkeys to image pairs \u201d , \u201c pervious \u201d , \u201c perserves \u201d , \u201c processure \u201d , \u201c sequnence \u201d \u201c viusal \u201d . Many typos could have been caught by a spellcheck ! This would improve readability a lot ! Re : Yes , absolutely . We are embarrassed and are terribly sorry . Indeed , the paper was written in a rush . We submitted the paper literally in the last minute , pushing the submit button 1 minute before the deadline . Well , that is obviously not a good excuse , and we are very grateful indeed that the reviewers are still willing to spend the time to read the paper despite its obvious shortcomings ! We hope we have redeemed ourselves by putting in an enormous amount of effort into this revision . 5.The citations are not properly formatted : ( 1 ) If the author names are used as part of the sentence , use e.g.Lotter et al . ( 2016 ) , else ( 2 ) If the author names are not part of the sentence , use ( Lotter et al. , 2016 ) . These two styles are mixed randomly in the current draft . This makes the manuscript , which already contains a lot of language mistakes , difficult to read . Re : Yes.We agreed and corrected them accordingly . 6.Abbreviations that are used but not introduced : CNN , IT , PSTH , DCNN , LSTM. # Re : Our bad . Now , we added the full names of each abbreviated term before using them and tried to minimizes the use of special terminologies by calling IT inferotemporal cortex and PSTH temporal responses of the neurons . 7.The related work section could benefit from referring to some of the related work in neuroscience . Re : We have added more background from theoretical neuroscience -- Mumford \u2019 s ideas on analysis by synthesis and Ullman \u2019 s counter-stream model , which is the inspiration of the development of our model . We also provided some recent neurophysiological studies on prediction errors in the inferotemporal cortex ( Meyer and Olson 2012 ) , as well as prediction related memory recall phenomena in the primary visual cortex ( V1 ) of mice ( Han et al.2008 , Xu et al.2012 ) .Our study on V1 and V2 neurons \u2019 sensitivity to memory of familiar complex video episodes is novel . We moved our simulation results of Meyer and Olson ( 2012 ) to the Appendix to yield room for some additional clarifying discussion on this experiment ."}, {"review_id": "BJl_VnR9Km-1", "review_text": "The authors propose a biologically inspired ANN to predict a video sequence, that performs better than previous biologically inspired video sequence predictors (>PredNet and >PredRNN+). Their model also accounts for familiarity effects (i.e. decrease in neural activations when repeatedly presenting the same visual sequence) found in primate early visual system V1/V2 (data recorded for this article) and late visual system IT. This work is interesting because it proposes a sequence prediction technique that accounts well for familiarity effects found in different regions of the visual system. However one of the claims does not seem supported by data: 1. The authors claim repeatedly that using the prediction error framework is computationally more efficient than alternatives but they do not show this. Furthermore, the article would benefit from the following clarifications: 2. It is unclear how their network performance compares to state-of-the-art NON neurally plausible models of sequence prediction. 3. It is unclear from the introduction how they modified the network proposed by (Pan et al) to obtain their network. 4. \"The SSIM index over time shows that the C-C method is more effective than C-F method, for C-F method performs better than C-C method in the short term perdiction when ground truth images are provided, but setting sliding window is too time-consuming, much more than the performance increase\" Please clarify this statement. 5. Macaque experiments: Some experiments on macaques were performed for this article, but there is no mention of ethical guidelines and whether they were respected. 6. Many typos are present in the text! I believe this work at the intersection of deep learning and neuroscience is an interesting contribution for both fields. However, the paper would benefit from these clarifications and a thorough proof-reading for the many typos present in the text. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the valuable feedback and comments . Below we address your comments point by point . 1.The authors claim repeatedly that using the prediction error framework is computationally more efficient than alternatives but they do not show this . Re : In Pan et al. \u2019 s CVPR 2018 paper , Recurrent Residual Module for Fast Inference in Videos , they have shown the efficiency of using residual error sparse convolution over normal convolution neural network for video processing . We also found that PredNet runs much faster than our F-F model because it builds a hierarchy of prediction errors , i.e.the errors of errors , which should be sparse and hence it also trains much faster than PredRNN++ , which presumably learn some hierarchy of spatiotemporal memories . Our model HPN took more time even than PredRNN++ ( by 10 % ) because it processed video in the unit of spatiotemporal blocks with spatiotemporal convolution rather than frame by frame as in the other two baseline models . We now added a quantitative comparison of training time of the different models in Figure 4 ( d ) . Using Pan et al \u2019 s sparse convolution scheme in our feedforward path saved our training time by about 15 % . 2.It is unclear how their network performance compares to state-of-the-art NON neurally plausible models of sequence prediction . Re : PredNet a neurally inspired model . PredRNN++ is the state-of-the-art NON neurally plausible model for video prediction . In PredRNN++ , which is a paper published in ICLM 2018 , the authors documented its performance against other computer vision models for video sequence prediction of the same kind , and showed that it is the state-of-the-art performing model for this task . Thus , we thought it sufficient to compare our model against PredRNN++ . 3.It is unclear from the introduction how they modified the network proposed by ( Pan et al ) to obtain their network . Re : Pan et al.network learns a dense convolutional kernel to process the first frame , but learns a sparse convolution kernel to process the subsequent frames . We just learn one sparse kernel for all the frames , including the first to reduce the parameters by half . We feel this parsimonious approach , even though it is slightly less accurate in the beginning prediction , is more reasonable and neurally plausible . We discuss this in the paper . Overall , the adoption of Pan \u2019 s idea reduced our training time by about 13-15 % and is not a critical part of the current model , although sparse convolution might be an important design principle in future refinements of the networks . We now separate the main idea having a DCNN feedforward path from this minor refinement into Figure 1 ( a ) and Figure 1 ( b ) to facilitate conceptual understanding . 4 . `` The SSIM index over time shows that the C-C method is more effective than C-F method , for C-F method performs better than C-C method in the short term prediction when ground truth images are provided , but setting sliding window is too time-consuming , much more than the performance increase '' Please clarify this statement . Re : We apologized for our lack of clarity in this explanation . Now we changed Chunk to Block for a more accurate and consistent exposition . We have rewritten those explanations and hope they are clear now . Essentially , the B-F ( used to be called C-F ) method takes in a spatiotemporal block as input to predict an individual frame . In this method , we have to move essentially frame by frame to predict one frame at a time , using a block of frames as input . The B-B ( used to be called C-C ) method can take a temporal stride as large as 5 frames at a time ( if the spatiotemporal block contains 5 frames ) . B-F can be considered as B-B with sliding window of 1 frame . Obviously , the B-F method produces a more accurate near-term prediction than B-B , or F-F , but it is time consuming and underperforms overall . B-B however is faster , when it takes a stride of 5 frames , and actually produces more accurate results for long range predictions . 5.Macaque experiments : Some experiments on macaques were performed for this article , but there is no mention of ethical guidelines and whether they were respected . Re : Thank you for reminding us . We have now added a footnote in the description of the experiment stating that \u201c All experimental procedures were approved by the XX University Institutional Animal Care and Use Committee and were in compliance with the guidelines set forth in the United States Public Health Service Guide for the Care and Use of Laboratory Animals. \u201d 6 . Many typos are present in the text ! Re : Yes , our apologies . We have revised our paper very carefully and extensively ."}, {"review_id": "BJl_VnR9Km-2", "review_text": "This paper proposes a network architecture inspired by the primate visual cortex. The architecture includes feedforward, feedback, and local recurrent connections, which together implement a predictive coding scheme. Some versions of the network are shown to outperform the similar PredNet and PredRNN architectures on two video prediction tasks: moving MNIST and KTH human actions. Finally, the authors provide neural data from monkeys and argue that their network shows similarities to the biological data. The paper contains intriguing ideas about the benefits of sparse and predictive coding, and the direct comparison to biological data potentially broadens the impact of the work. However, major claims are unsubstantiated, and accuracy and clarity need to be improved to make the manuscript acceptable. Major concerns: 1. The authors claim that their architecture is more efficient because it uses sparse coding of residuals. Implementation details and some quantitative arguments, ideally benchmarks, need to be provided to show that their architecture is actually more efficient than PredRNN++ and PredNet. 2. It is unclear whether the PredRNN++ should be compared to the C-C or C-F version of the network. Does the PredRNN++ have access to as many current and future frames as the C-C net? Is this a fair comparison? Please provide a clearer description of the different versions of your network and how they relate to the baseline models. That section in particular has many confusing typos (frame-by-chunk, chunk-by-frame abbreviations mixed up). 3. In Figure 6, the authors claim that more layers lead to \u201cbetter\u201d representations. What does \u201cbetter\u201d mean? It is implied that the networks with more layers actually make the different motions more discriminable. Please quantify this. For example, a linear classifier could be trained on the neural activations. Also, how is this related to the rest of the paper? Do the authors claim that this result is unique to the proposed architecture? In that case, please provide a quantitative comparison to the PredNet or PredRNN++. 4. In Figure 9, the presentation is highly confusing. Plots (c) to (h) are clearly made to look like the monkey data in (b) (nonlinear x-axes?), but show totally different timescales (training epochs vs. milliseconds). Please explain why it makes sense to compare these timescales. Also, what does it mean for a training epoch to have a negative value? Minor comments: 1. I don\u2019t understand the \u201ctension\u201d between hierarchical feature representations and residual representations brought up in Section 2. Do the PredNet and PredRNN++ not contain a hierarchy of representations? 2. Figure 1 is not fully annotated and could be clearer. What does the asterisk mean? Why are there multiple arrows between the P\u2019s? What do the small arrows next to the big arrows mean? Please expand the legend. Consider using colors to differentiate between components. 3. I don\u2019t understand Figure 4c. According to the text, this plot shows \u201ceffectiveness as a function of time\u201d, but the x-axis is labeled \u201cLayer Number\u201d. What does \u201ceffectiveness over time\u201d mean? What does the y-label mean (SSIM per day?)? What is \u201ctrunk prediction\u201d (not mentioned anywhere in the text)? 4. For Figure 9, it is pointed out that activity is expected to be lower for E neurons, but is also lower for R and P. This is interesting and also applies to Figure 8, so it would be good to see Figure 8 split up by E/R/P, too. 5. The word \u201cFigure\u201d is missing before figure references. 6. Please proof-read for typography, punctuation and grammar.", "rating": "3: Clear rejection", "reply_text": "Thank you for the valuable feedback and comments . Below we address your comments point by point . The paper contains intriguing ideas about the benefits of sparse and predictive coding , and the direct comparison to biological data potentially broadens the impact of the work . However , major claims are unsubstantiated , and accuracy and clarity need to be improved to make the manuscript acceptable . Major concerns : 1 . The authors claim that their architecture is more efficiency because it uses sparse coding of residuals . Implementation details and some quantitative arguments , ideally benchmarks , need to be provided to show that their architecture is actually more efficient than PredRNN++ and PredNet . Re : We might have given the wrong impression in our statements that our model was more efficient or faster than PredRNN++ and PredNet . In fact , our training time is actually longer . We have now provided a Figure 4 ( d ) showing the training time for different versions of our model and PredRNN++ and PredNet . PredNet is fastest to train , because it is learning representations of sparse prediction residuals . Our B-B version of the network , which performed better than PredRNN++ , took 10 % longer to train than PredRNN++ . This is not surprising because our model uses more loops than PredRNN++ and we used spatiotemporal blocks as data units and 3D convolutional LSTM for prediction , so naturally our model is more complex than PredRNN++ and will take longer time to train . Adopting Pan \u2019 s sparsificaiton scheme decreased B-B network 's training time by 13 % ( comparing B-B ( sparse ) versus B-B ( non-sparse ) in Figure 4 ( d ) . Thus , the statement that sparse convolution improves efficiency is true . We have now added a benchmark comparison in Figure 4 ( d ) showing the training time and clarifying the limited contribution of sparse convolution to our model . Thank you for pointing out this potential confusion . 2.It is unclear whether the PredRNN++ should be compared to the C-C or C-F version of the network . Does the PredRNN++ have access to as many current and future frames as the C-C net ? Is this a fair comparison ? Please provide a clearer description of the different versions of your network and how they relate to the baseline models . That section in particular has many confusing typos ( frame-by-block , block-by-frame abbreviations mixed up ) . Re : Note , we have changed C ( chunk ) to B ( block ) in order to have more consistent notations and terminology . Is it a fair comparison with the baseline model PredRNN++ ? During testing , all the five networks ( B-B , B-F , F-F , PredNet , PredRNN++ ) had access to the same number of frames ( the first 20 frames ) and have to predict the future 20 frames of the 40-frame test sets . During training , they were all trained on 40 frames movies of the training sets drawn from the same database . The comparison is fair in the sense that they have equal access to the same amount of information and they have to solve the same problem . Both PredNet and PredRNN++ took in one frame at a time to predict one frame at a time , while our B-B took in a block of frames to predict a block of frames . PredRNN++ used a stack of LSTM to remember sequences and learn the feature transformation in the fashion of an autoencoder , while HPNet used the idea of a spatiotemporal block as well as a hierarchy of LSTM to do the same . Absolute fair comparison is difficult but we are fair at least in the amount of information available to each model , as reviewer asked ."}], "0": {"review_id": "BJl_VnR9Km-0", "review_text": "Summary: The paper presents a novel architecture for video prediction consisting of a feed-forward path with sparse convolutions and an LSTM generating predictions of chunks of video based on the sequence of input chunks. A feedback path links the LSTMs of the different sparse prediction modules. Experiments in video prediction are performed on moving-MNIST and the KTH action recognition dataset and the model achieves state-of-the-art performance on both. Interestingly, the model is exhibits prediction suppressions effects as have been observed during neurophysiological experiments in the inferotemporal cortex of macaque monkeys. The proposed method exhibits prediction suppression effects also in the lower layers, motivating a neurophysiological experiment in the earlier V1/V2 regions, which yielded an observation similar to the model\u2019s prediction. Strengths: The performance improvements over competing methods on Moving-MNIST and KTH presented in the experimental section are significant. The analysis seems fairly thorough. Weaknesses and requests for clarification: - The description of the sparse predictive module is difficult to follow, and I am not sure I understood it completely. I find it a bit unintuitive to start the description with the errors, instead of explaining what is computed from beginning to end. The section reads more like a loose description of isolated parts instead of an integrated whole. Maybe walking the reader step-by-step through one complete iteration of the computation helps to clarify this. Also, not every character in equations 1-5 and the algorithm has been defined. For example, what is L? - The text makes it sound like the idea of using 3d convolutions in a convLSTM is novel. 3D convLSTMs have been previously used in 3d vision, see Choy, C. B., Xu, D., Gwak, J., Chen, K., & Savarese, S. (2016, October). 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In European conference on computer vision (pp. 628-644). Springer, Cham. The application of 3d convLSTMs to video might be new, but the mentioned paper by Choy et al. (2016) should be cited. - You mention that padding is used for rows and columns. Are you using padding on the temporal axis as well? - The paper seems to be written in a rush, as it contains way too many typos and grammar mistakes, e.g. \u201ca hierarchical of\u201d (should be \u201ca hierarchy of\u201c or just \u201chierarchical\u201d), \u201cfeedforwad\u201d, \u201cExpriment\u201d (section 4 heading), \u201cachievedbetter\u201d, \u201ctrained monkeys to image pairs\u201d, \u201cpervious\u201d, \u201cperserves\u201d, \u201cprocessure\u201d, \u201csequnence\u201d \u201cviusal\u201d. Many typos could have been caught by a spellcheck! This would improve readability a lot! - The citations are not properly formatted: (1) If the author names are used as part of the sentence, use e.g. Lotter et al. (2016), else (2) If the author names are not part of the sentence, use (Lotter et al., 2016). These two styles are mixed randomly in the current draft. This makes the manuscript, which already contains a lot of language mistakes, difficult to read. - Abbreviations that are used but not introduced: CNN, IT, PSTH, DCNN, LSTM. - The related work section could benefit from referring to some of the related work in neuroscience. - Adding a sentence explaining the intuition behind using SatLU in equation (1) might be helpful To summarize my feedback: I think experimental results and analysis are strong, but the presentation is strongly lacking! The description of the approach definitely needs to be improved to make replication of the results easier. It might help to have someone who doesn\u2019t know the model already read the description and explain it back to you while revising the draft. I hope I could provide some helpful suggestions. I would recommend the manuscript for acceptance, if the presentation is significantly improved!", "rating": "7: Good paper, accept", "reply_text": "Thank you for the valuable feedback and comments . Below we address your comments point by point . 1.The description of the sparse predictive module is difficult to follow , and I am not sure I understood it completely . I find it a bit unintuitive to start the description with the errors , instead of explaining what is computed from beginning to end . The section reads more like a loose description of isolated parts instead of an integrated whole . Maybe walking the reader step-by-step through one complete iteration of the computation helps to clarify this . Also , not every character in equations 1-5 and the algorithm has been defined . For example , what is L ? Re : Now we call each layer a \u2018 Cortical Module ( CM ) \u2019 and provide a more concise and precise description of the model and algorithm in section 3 . We also provide a step-by-step description of the flow of the algorithm per your advice . For clarity , we decompose the description of the feedforward path into Figure 1 ( a ) and Figure 1 ( b ) into a normal DCNN path and a sparsified DCNN part to make it more understandable . The feedforward path is just a normal convolutional neural network but it is trainable by self-supervised learning because its feedforward input does project to the LSTM in each layer . The sparse convolution scheme ( Figure 1b ) only serves to make it more efficient ( see Figure 4d ) , and is not really a critical part of the model . 2.The text makes it sound like the idea of using 3d convolutions in a convLSTM is novel . 3D convLSTMs have been previously used in 3d vision , see Choy , C. B. , Xu , D. , Gwak , J. , Chen , K. , & Savarese , S. ( 2016 , October ) . 3d-r2n2 : A unified approach for single and multi-view 3d object reconstruction . In European conference on computer vision ( pp.628-644 ) .Springer , Cham . The application of 3d convLSTMs to video might be new , but the mentioned paper by Choy et al . ( 2016 ) should be cited . Re : Thank you for pointing that out . We added the citation of Choy et al.We believe , as you pointed out , that using 3D convLSTM in video , especially for video prediction , might be new though it seems to be an obvious thing to do . But we don \u2019 t think this is the main contribution of the paper . 3.You mention that padding is used for rows and columns . Are you using padding on the temporal axis as well ? Re : We used padding on both spatial and temporal domains . 4.The paper seems to be written in a rush , as it contains way too many typos and grammar mistakes , e.g. \u201c a hierarchical of \u201d ( should be \u201c a hierarchy of \u201c or just \u201c hierarchical \u201d ) , \u201c feedforwad \u201d , \u201c Experiment \u201d ( section 4 heading ) , \u201c achieved better \u201d , \u201c trained monkeys to image pairs \u201d , \u201c pervious \u201d , \u201c perserves \u201d , \u201c processure \u201d , \u201c sequnence \u201d \u201c viusal \u201d . Many typos could have been caught by a spellcheck ! This would improve readability a lot ! Re : Yes , absolutely . We are embarrassed and are terribly sorry . Indeed , the paper was written in a rush . We submitted the paper literally in the last minute , pushing the submit button 1 minute before the deadline . Well , that is obviously not a good excuse , and we are very grateful indeed that the reviewers are still willing to spend the time to read the paper despite its obvious shortcomings ! We hope we have redeemed ourselves by putting in an enormous amount of effort into this revision . 5.The citations are not properly formatted : ( 1 ) If the author names are used as part of the sentence , use e.g.Lotter et al . ( 2016 ) , else ( 2 ) If the author names are not part of the sentence , use ( Lotter et al. , 2016 ) . These two styles are mixed randomly in the current draft . This makes the manuscript , which already contains a lot of language mistakes , difficult to read . Re : Yes.We agreed and corrected them accordingly . 6.Abbreviations that are used but not introduced : CNN , IT , PSTH , DCNN , LSTM. # Re : Our bad . Now , we added the full names of each abbreviated term before using them and tried to minimizes the use of special terminologies by calling IT inferotemporal cortex and PSTH temporal responses of the neurons . 7.The related work section could benefit from referring to some of the related work in neuroscience . Re : We have added more background from theoretical neuroscience -- Mumford \u2019 s ideas on analysis by synthesis and Ullman \u2019 s counter-stream model , which is the inspiration of the development of our model . We also provided some recent neurophysiological studies on prediction errors in the inferotemporal cortex ( Meyer and Olson 2012 ) , as well as prediction related memory recall phenomena in the primary visual cortex ( V1 ) of mice ( Han et al.2008 , Xu et al.2012 ) .Our study on V1 and V2 neurons \u2019 sensitivity to memory of familiar complex video episodes is novel . We moved our simulation results of Meyer and Olson ( 2012 ) to the Appendix to yield room for some additional clarifying discussion on this experiment ."}, "1": {"review_id": "BJl_VnR9Km-1", "review_text": "The authors propose a biologically inspired ANN to predict a video sequence, that performs better than previous biologically inspired video sequence predictors (>PredNet and >PredRNN+). Their model also accounts for familiarity effects (i.e. decrease in neural activations when repeatedly presenting the same visual sequence) found in primate early visual system V1/V2 (data recorded for this article) and late visual system IT. This work is interesting because it proposes a sequence prediction technique that accounts well for familiarity effects found in different regions of the visual system. However one of the claims does not seem supported by data: 1. The authors claim repeatedly that using the prediction error framework is computationally more efficient than alternatives but they do not show this. Furthermore, the article would benefit from the following clarifications: 2. It is unclear how their network performance compares to state-of-the-art NON neurally plausible models of sequence prediction. 3. It is unclear from the introduction how they modified the network proposed by (Pan et al) to obtain their network. 4. \"The SSIM index over time shows that the C-C method is more effective than C-F method, for C-F method performs better than C-C method in the short term perdiction when ground truth images are provided, but setting sliding window is too time-consuming, much more than the performance increase\" Please clarify this statement. 5. Macaque experiments: Some experiments on macaques were performed for this article, but there is no mention of ethical guidelines and whether they were respected. 6. Many typos are present in the text! I believe this work at the intersection of deep learning and neuroscience is an interesting contribution for both fields. However, the paper would benefit from these clarifications and a thorough proof-reading for the many typos present in the text. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the valuable feedback and comments . Below we address your comments point by point . 1.The authors claim repeatedly that using the prediction error framework is computationally more efficient than alternatives but they do not show this . Re : In Pan et al. \u2019 s CVPR 2018 paper , Recurrent Residual Module for Fast Inference in Videos , they have shown the efficiency of using residual error sparse convolution over normal convolution neural network for video processing . We also found that PredNet runs much faster than our F-F model because it builds a hierarchy of prediction errors , i.e.the errors of errors , which should be sparse and hence it also trains much faster than PredRNN++ , which presumably learn some hierarchy of spatiotemporal memories . Our model HPN took more time even than PredRNN++ ( by 10 % ) because it processed video in the unit of spatiotemporal blocks with spatiotemporal convolution rather than frame by frame as in the other two baseline models . We now added a quantitative comparison of training time of the different models in Figure 4 ( d ) . Using Pan et al \u2019 s sparse convolution scheme in our feedforward path saved our training time by about 15 % . 2.It is unclear how their network performance compares to state-of-the-art NON neurally plausible models of sequence prediction . Re : PredNet a neurally inspired model . PredRNN++ is the state-of-the-art NON neurally plausible model for video prediction . In PredRNN++ , which is a paper published in ICLM 2018 , the authors documented its performance against other computer vision models for video sequence prediction of the same kind , and showed that it is the state-of-the-art performing model for this task . Thus , we thought it sufficient to compare our model against PredRNN++ . 3.It is unclear from the introduction how they modified the network proposed by ( Pan et al ) to obtain their network . Re : Pan et al.network learns a dense convolutional kernel to process the first frame , but learns a sparse convolution kernel to process the subsequent frames . We just learn one sparse kernel for all the frames , including the first to reduce the parameters by half . We feel this parsimonious approach , even though it is slightly less accurate in the beginning prediction , is more reasonable and neurally plausible . We discuss this in the paper . Overall , the adoption of Pan \u2019 s idea reduced our training time by about 13-15 % and is not a critical part of the current model , although sparse convolution might be an important design principle in future refinements of the networks . We now separate the main idea having a DCNN feedforward path from this minor refinement into Figure 1 ( a ) and Figure 1 ( b ) to facilitate conceptual understanding . 4 . `` The SSIM index over time shows that the C-C method is more effective than C-F method , for C-F method performs better than C-C method in the short term prediction when ground truth images are provided , but setting sliding window is too time-consuming , much more than the performance increase '' Please clarify this statement . Re : We apologized for our lack of clarity in this explanation . Now we changed Chunk to Block for a more accurate and consistent exposition . We have rewritten those explanations and hope they are clear now . Essentially , the B-F ( used to be called C-F ) method takes in a spatiotemporal block as input to predict an individual frame . In this method , we have to move essentially frame by frame to predict one frame at a time , using a block of frames as input . The B-B ( used to be called C-C ) method can take a temporal stride as large as 5 frames at a time ( if the spatiotemporal block contains 5 frames ) . B-F can be considered as B-B with sliding window of 1 frame . Obviously , the B-F method produces a more accurate near-term prediction than B-B , or F-F , but it is time consuming and underperforms overall . B-B however is faster , when it takes a stride of 5 frames , and actually produces more accurate results for long range predictions . 5.Macaque experiments : Some experiments on macaques were performed for this article , but there is no mention of ethical guidelines and whether they were respected . Re : Thank you for reminding us . We have now added a footnote in the description of the experiment stating that \u201c All experimental procedures were approved by the XX University Institutional Animal Care and Use Committee and were in compliance with the guidelines set forth in the United States Public Health Service Guide for the Care and Use of Laboratory Animals. \u201d 6 . Many typos are present in the text ! Re : Yes , our apologies . We have revised our paper very carefully and extensively ."}, "2": {"review_id": "BJl_VnR9Km-2", "review_text": "This paper proposes a network architecture inspired by the primate visual cortex. The architecture includes feedforward, feedback, and local recurrent connections, which together implement a predictive coding scheme. Some versions of the network are shown to outperform the similar PredNet and PredRNN architectures on two video prediction tasks: moving MNIST and KTH human actions. Finally, the authors provide neural data from monkeys and argue that their network shows similarities to the biological data. The paper contains intriguing ideas about the benefits of sparse and predictive coding, and the direct comparison to biological data potentially broadens the impact of the work. However, major claims are unsubstantiated, and accuracy and clarity need to be improved to make the manuscript acceptable. Major concerns: 1. The authors claim that their architecture is more efficient because it uses sparse coding of residuals. Implementation details and some quantitative arguments, ideally benchmarks, need to be provided to show that their architecture is actually more efficient than PredRNN++ and PredNet. 2. It is unclear whether the PredRNN++ should be compared to the C-C or C-F version of the network. Does the PredRNN++ have access to as many current and future frames as the C-C net? Is this a fair comparison? Please provide a clearer description of the different versions of your network and how they relate to the baseline models. That section in particular has many confusing typos (frame-by-chunk, chunk-by-frame abbreviations mixed up). 3. In Figure 6, the authors claim that more layers lead to \u201cbetter\u201d representations. What does \u201cbetter\u201d mean? It is implied that the networks with more layers actually make the different motions more discriminable. Please quantify this. For example, a linear classifier could be trained on the neural activations. Also, how is this related to the rest of the paper? Do the authors claim that this result is unique to the proposed architecture? In that case, please provide a quantitative comparison to the PredNet or PredRNN++. 4. In Figure 9, the presentation is highly confusing. Plots (c) to (h) are clearly made to look like the monkey data in (b) (nonlinear x-axes?), but show totally different timescales (training epochs vs. milliseconds). Please explain why it makes sense to compare these timescales. Also, what does it mean for a training epoch to have a negative value? Minor comments: 1. I don\u2019t understand the \u201ctension\u201d between hierarchical feature representations and residual representations brought up in Section 2. Do the PredNet and PredRNN++ not contain a hierarchy of representations? 2. Figure 1 is not fully annotated and could be clearer. What does the asterisk mean? Why are there multiple arrows between the P\u2019s? What do the small arrows next to the big arrows mean? Please expand the legend. Consider using colors to differentiate between components. 3. I don\u2019t understand Figure 4c. According to the text, this plot shows \u201ceffectiveness as a function of time\u201d, but the x-axis is labeled \u201cLayer Number\u201d. What does \u201ceffectiveness over time\u201d mean? What does the y-label mean (SSIM per day?)? What is \u201ctrunk prediction\u201d (not mentioned anywhere in the text)? 4. For Figure 9, it is pointed out that activity is expected to be lower for E neurons, but is also lower for R and P. This is interesting and also applies to Figure 8, so it would be good to see Figure 8 split up by E/R/P, too. 5. The word \u201cFigure\u201d is missing before figure references. 6. Please proof-read for typography, punctuation and grammar.", "rating": "3: Clear rejection", "reply_text": "Thank you for the valuable feedback and comments . Below we address your comments point by point . The paper contains intriguing ideas about the benefits of sparse and predictive coding , and the direct comparison to biological data potentially broadens the impact of the work . However , major claims are unsubstantiated , and accuracy and clarity need to be improved to make the manuscript acceptable . Major concerns : 1 . The authors claim that their architecture is more efficiency because it uses sparse coding of residuals . Implementation details and some quantitative arguments , ideally benchmarks , need to be provided to show that their architecture is actually more efficient than PredRNN++ and PredNet . Re : We might have given the wrong impression in our statements that our model was more efficient or faster than PredRNN++ and PredNet . In fact , our training time is actually longer . We have now provided a Figure 4 ( d ) showing the training time for different versions of our model and PredRNN++ and PredNet . PredNet is fastest to train , because it is learning representations of sparse prediction residuals . Our B-B version of the network , which performed better than PredRNN++ , took 10 % longer to train than PredRNN++ . This is not surprising because our model uses more loops than PredRNN++ and we used spatiotemporal blocks as data units and 3D convolutional LSTM for prediction , so naturally our model is more complex than PredRNN++ and will take longer time to train . Adopting Pan \u2019 s sparsificaiton scheme decreased B-B network 's training time by 13 % ( comparing B-B ( sparse ) versus B-B ( non-sparse ) in Figure 4 ( d ) . Thus , the statement that sparse convolution improves efficiency is true . We have now added a benchmark comparison in Figure 4 ( d ) showing the training time and clarifying the limited contribution of sparse convolution to our model . Thank you for pointing out this potential confusion . 2.It is unclear whether the PredRNN++ should be compared to the C-C or C-F version of the network . Does the PredRNN++ have access to as many current and future frames as the C-C net ? Is this a fair comparison ? Please provide a clearer description of the different versions of your network and how they relate to the baseline models . That section in particular has many confusing typos ( frame-by-block , block-by-frame abbreviations mixed up ) . Re : Note , we have changed C ( chunk ) to B ( block ) in order to have more consistent notations and terminology . Is it a fair comparison with the baseline model PredRNN++ ? During testing , all the five networks ( B-B , B-F , F-F , PredNet , PredRNN++ ) had access to the same number of frames ( the first 20 frames ) and have to predict the future 20 frames of the 40-frame test sets . During training , they were all trained on 40 frames movies of the training sets drawn from the same database . The comparison is fair in the sense that they have equal access to the same amount of information and they have to solve the same problem . Both PredNet and PredRNN++ took in one frame at a time to predict one frame at a time , while our B-B took in a block of frames to predict a block of frames . PredRNN++ used a stack of LSTM to remember sequences and learn the feature transformation in the fashion of an autoencoder , while HPNet used the idea of a spatiotemporal block as well as a hierarchy of LSTM to do the same . Absolute fair comparison is difficult but we are fair at least in the amount of information available to each model , as reviewer asked ."}}