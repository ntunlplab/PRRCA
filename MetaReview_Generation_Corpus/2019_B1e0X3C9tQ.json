{"year": "2019", "forum": "B1e0X3C9tQ", "title": "Diagnosing and Enhancing VAE Models", "decision": "Accept (Poster)", "meta_review": "The reviewers acknowledge the value of the careful analysis of Gaussian encoder/decoder VAE presented in the paper. The proposed algorithm shows impressive FID scores that are comparable to those obtained by state of the art GANs. The paper will be a valuable addition to the ICLR program. \n\n", "reviews": [{"review_id": "B1e0X3C9tQ-0", "review_text": "This paper proposed a two-stage VAE method to generate high-quality samples and avoid blurriness. It is accomplished by utilizing a VAE structure on the observation and latent variable separately. The paper exploited a collection of interesting properties of VAE and point out the problem existed in the generative process of VAE. I have several concerns about the paper: 1. It is necessary to explain why the second-stage VAE can have its latent variable more closely resemble N(u|0,I). Even if the latent variable closely resemble N(u|0,I), How does it make sure the generated images are realistic? I admit that the VAE model can reconstruct realistic data based on its inferred latent variable, however, when given a random sample from N(u|0,I), the generated images are not good, which is true when the dimension of the latent space is high. I still can\u2019t understand why a second-stage VAE can relief this problem. 2. The adversarial auto-encoder is also proposed to solve the latent space problem, by comparison, what is the advantage of this paper? 3. Why do you set the model as two separate stages? Will it enhance the performance if we train theses two-stages all together? 4. The proofs for the theory 2 and 3 are under the assumption that the manifold dimension of the observation is r, while in reality it is difficult to obtain this r, do these theories applicable if we choose a value for the dimension of the latent space that is smaller than the real manifold dimension of the observation? How will it affect the performance of the proposed method? 5. The value of r and k in each experiment should be specified. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "2.Reviewer Comment : The adversarial autoencoder is also proposed to solve the latent space problem , by comparison , what is the advantage of this paper ? Our Response : The adversarial autoencoder ( Makhzani et al. , 2016 ) requires adversarial training , meaning that like all GAN-related models , a complex min-max problem must be optimized in search of a saddle point . A well-recognized advantage of VAEs is that the training involves pure minimization of a fixed variational energy function , which is generally more stable and resistant to mode collapse . We should also point out that unlike VAEs , the adversarial autoencoder has no mechanism for pruning superfluous dimensions in the latent space . Regardless of these key differences , we are aware of no published work where the adversarial autoencoder has been shown to produce competitive results generating novel samples like other GAN-related models ( rather it has been tested on auxiliary tasks like semi-supervised learning , which is not in our scope ) . Indeed the exhaustive recent testing from ( Lucic et al. , 2018 ) upon which we based our experiments , does not even include the adversarial autoencoder as a benchmark . 3.Reviewer Comment : Why train the model as two separate stages ? Will it enhance the performance if we train these two stages together ? Our Response : We have addressed this question on the bottom of page 7 , which states the following : `` It should also be emphasized that concatenating the two stages and jointly training does not improve the performance . If trained jointly the few extra second-stage parameters are simply hijacked by the dominant objective from the first stage and forced to work on an incrementally better fit of the manifold . As expected then , on empirical tests ( not shown ) we have found that this does not improve upon standard VAE baselines . '' Our theoretical results and algorithm development from Sections 2-4 also directly support this conclusion . Regardless , we are happy to clarify further if needed . 4.Reviewer Comment : Do the technical proofs require knowledge of the ground-truth manifold dimensions r ? And how is the proposed algorithm affected when r is unknown ? Our Response : None of our proofs require that the ground-truth r is known explicitly in advance . All that is required is that we set kappa > = r ( please see proof statements for Theorems 1-3 ) . In other words , we only need to set the latent dimension kappa to be bigger than the ground-truth manifold dimension r. The VAE then has a natural mechanism in place for discarding superfluous dimensions . Of course obviously in practice if we set kappa to be far too large , then the training could potentially become a bit more difficult , since in addition to learning the correct ground-truth manifold , we are also burdening the model to detect a much larger number of unnecessary dimensions . But the VAE is arguably more robust to kappa than most methods , and the basic point still holds : we need not set kappa = r , we just need to choose kappa to be a reasonable value that is at least as big as r. In contrast , if we set kappa < r , then the theory starts to break down and practical performance will begin to degrade as expected . 5.Reviewer Comment : The value of r and kappa in each experiment should be specified . Our Response : The true latent manifold dimension r is unknown in all of our experiments since we are using real-world data . However , for the dimension of the VAE latent code , we chose kappa = 64 for all experiments , except for the 2-Stage VAE * model results , where we used 32 for MNIST and Fashion-MNIST , 192 for CIFAR-10 , and 256 for CelebA . Note that these values were not carefully tuned and need not be exact per the arguments responding to reviewer comment 4 above . We just tried a single smaller value for the simpler data ( MNIST and FashionMNIST ) , and a couple larger values for the more complex ones ( CIFAR-10 and CelebA ) ."}, {"review_id": "B1e0X3C9tQ-1", "review_text": "Overview: I thank the authors for their interesting and detailed work in this paper. I believe it has the potential to provide strong value to the community interested in using VAEs with an explicit and simple parameterization of the approximate posterior and likelihood as Gaussian. Gaussianity can be appropriate in many cases where no sequential or discrete structure needs to be induced in the model. I find the mathematical arguments interesting and enlightening. However, the authors somewhat mischaracterize the scope of applicability of VAE models in contemporary machine learning, and don't show familiarity with the broad literature around VAEs outside of this case (that is, where a Gaussian model of the output would be manifestly inappropriate). Since the core of the paper is valuable and salvageable from a clarity standpoint, my comments below are geared towards what changes the authors may make to move this paper into the \"pass\" category. Pros: - Mathematical insights are well reasoned and interesting. Based on the insight from the analysis in the supplementary materials, the authors propose a two-stage VAE which separate learning the a parsimonious representation of the low-dimensional (lower than the ambient dimension of the input space), and the training a second VAE to learn the unknown approximate posterior. The two-stage training procedure is both theoretically motivated and appears to enhance the output quality of VAEs w.r.t. FID score, making them rival GAN architectures on this metric. Cons: - The title and general tone of the paper is too broad: it is only VAE models with Gaussian approximate posteriors and likelihoods. This is hardly the norm for most applications, contrary to the claims of the authors. VAEs are commonly used for discrete random variables, for example. Many cases where VAEs are applied cannot use a Gaussian assumption for the likelihood, which is the key requirement for the proofs in the supplement to be valid (then, the true posterior is also Gaussian, and the KL divergence between that and the approximate posterior can be driven to zero during optimization--clearly a Gaussian approximate posterior will never have zero KL divergence with a non-Gaussian true posterior). - None of the proofs consider the approximation error garnered by only having access to empirical samples through a sample of the ground truth population. (The ground-truth distribution must be defined with respect to the population rather just the dataset in hand, otherwise we lose all generalizability from a model.) Moreover, the proofs hold asymptotically. Generalization bounds and error from finite time approximations are very pertinent issues and these are ignored by the presented analyses. Such concerns have motivated many of the recent developments in approximate posterior distributions. Overall, the paper contains little evidence of familiarity with the recent advances in approximate Bayesian inference that have occurred over the past two years. - A central claim of the paper is that the two-stage VAE obviates the need for highly adaptive approximate posteriors. However, no comparison against those models is done in the paper. How does a two-stage VAE compare against one with, e.g., a normalizing flow approximate posterior? I acknowledge that the purpose of the paper was to argue for the Gaussianity assumption as less stringent than previously believed, but all of the mathematical arguments take place in an imagined world with infinite time and unbounded access to the population distribution. This is not really the domain of interest in modern computational statistics / machine learning, where issues of generalization and computational efficiency are paramount. - While the mathematical insights are well developed, the specifics of the algorithm used to implement the two-stage VAE are a little opaque. Ancestral sampling now takes place using latent samples from a second VAE. An algorithm box is badly needed for reproducibility. Recommendations / Typos I noted a few typos and omissions that need correction. - Generally, the mathematical proofs in section 7 of the supplement are clear. At the top of page 11, though, the paragraph correctly begins by stating that the composition of invertible functions is invertible, but fails to establish that G is also invertible. Clearly it is so by construction, but the explicit reasons should be stated (as a prior sentence promises), and so I assume this is an accidental omission. - The title of Section 8.1 has a typo: clearly is it is the negative log of p_{theta_t} (x) which approaches its infimum rather than p_{theta_t} (x) approaching negative infinity. - Equation (4): the true posterior has an x as its argument instead of the latent z. - Missing parenthesis under Case 2 and wrong indentation. This analysis also seems to be cut off. Is the case r > d relevant here? * EDIT: I have read the authors' detailed response. It has clarified a few key issues, and convinced me of the value to the community for publication in its present (slightly edited according to the reviwers' feedback) form. I would like to see this published and discussed at ICLR and have revised my score accordingly. *", "rating": "7: Good paper, accept", "reply_text": "- Reviewer Comment : No comparisons against VAE models with more flexible approximate posteriors such as those produced via normalizing flows Our Response : We agree that more flexible , explicitly non-Gaussian approximate posteriors have recently been proposed , such as the many flavors that utilize normalizing flows . But such models have not as of yet been objectively shown to improve sampling quality ( see comments above ) despite the tremendous community-wide incentive to publish such a demonstration . Moreover , the added flexibility often comes with a significant cost ( e.g. , increased training difficulty , more expensive inference ) . Furthermore , if we consider broader VAE modifications beyond just the encoder , then even within this wider domain , the only VAE-related enhancement we are aware of that objectively/quantitatively produces improved samples is the WAE model from ICLR this year ( Tolstikhin et al. , 2018 ) , which is already explicitly addressed in Section 5 of our submission . Consequently , unless there is some very recent reference we may have missed , our experiments represent the state-of-the-art for non-adversarial VAE/autoencoer-based structures in term of the objective evaluation of generated samples , and the first to close the gap with GANs ( this is also consistent with the comments from AnonReviewer2 ) . - Reviewer Comment : Some details about the proposed 2-stage process are unclear Although there was unfortunately no space for a separate algorithm box in our submission , the three bullet points on page 6 describe the specific process we used . Note that the ancestral sampling required is very straightforward as described in bullet point 3 on page 6 . This is exactly what we followed for generating new samples via our method , but we are happy to provide further clarification if the reviewer has a specific suggestion . - Reviewer Comment : Recommendations/Typos Our Response : We sincerely appreciate the effort in finding typos and checking the proofs . We have corrected each of the cases the reviewer uncovered . This will certainly be of benefit to future readers . Additionally , r can never be greater than d , because r is the manifold dimension within the ambient space of dimension d ."}, {"review_id": "B1e0X3C9tQ-2", "review_text": "The paper provides a number of novel interesting theoretical results on \"vanilla\" Gaussian Variational Auto-Encoders (VAEs) (sections 1, 2, and 3), which are then used to build a new algorithm called \"2 stage VAEs\" (Section 4). The resulting algorithm is as stable as VAEs to train (it is free of any sort of adversarial training, it comes with a little overhead in terms of extra parameters), while achieving a quality of samples which is *very impressive* for an Auto-Encoder (AE) based generative modeling techniques (Section 5). In particular, the method achieves FID score 24 on the CelebA dataset which is on par with the best GAN-based models as reported in [1], thus sufficiently reducing the gap between the generative quality of the GAN-based and AE-based models reported in the literature. Main theoretical contributions: 1. In some cases the variational bound of Gaussian VAEs can get tight (Theorem 1). In the context of vanilla Gaussian VAEs (Gaussian prior, encoders, and decoders) the authors show that if (a) the intrinsic data dimensionality r is equal to the data space dimensionality d and (b) the latent space dimensionality k is not smaller than r then there is a sequence of encoder-decoder pairs achieving the global minimum of the VAE objective and simultaneously (a) zeroing the variational gap and (b) precisely matching the true data distribution. In other words, in this setting the variational bound and the Gaussian model does not prevent the true data distribution from being recovered. 2. In other cases Gaussian VAEs may not recover the actual distribution, but they will recover the real manifold (Theorems 2, 3, 4 and discussions on page 5). In case when r < d, that is when the data distribution is supported on a low dimensional smooth manifold in the input space, things are quite different. The authors show that there are still sequences of encoder-decoder pairs which achieves the global minimum of the VAE objective. However, this time only *some* of these sequences converge to the model which is in a way indistinguishable from the true data distribution (and thus again Gaussian VAEs do not fundamentally prevent the true distribution from being recovered). Nevertheless, all sequences mentioned above recover the true data manifold in that (a) the optimal encoder learns to use r dimensional linear subspace in the latent space to encode the inputs in a lossless and noise-free way, while filling the remaining k - r dimensions with a white Gaussian noise and (b) the decoder learns to ignore the k - r noisy dimensions and use the r \"informative\" dimensions to produce the outputs perfectly landing on the true data manifold. Main algorithmic contributions: (0) A simple 2 stage algorithm, where first a vanilla Gaussian VAE is trained on the input dataset and second a separate vanilla Gaussian VAE is trained to match the aggregate posterior obtained after the first stage. The authors support this algorithm with a reasonable theoretical argument based on theoretical insights listed above (see end of page 6 - beginning of page 7). The algorithm achieves state-of-art FID scores across several data sets among AE based models existing in the literature. Review summary: I would like to say that this paper was a breath of fresh air to me. I really liked how the authors make a strong point that *it is not the Gaussian assumptions that harm the performance of VAEs* in contrast to what is usually believed in the field nowadays. Also, I think *the reported FID scores alone may be considered as a significant enough contribution*, because to my knowledge this is the first paper significantly closing the gap between generative quality of GAN-based models and non-adversarial AE-based methods. *************** *** Couple of comments and typos: *************** (0) Is the code / checkpoints going to be available anytime soon? (1) I would mention [2] which in a way used a very similar approach, where the aggregate posterior of the implicit generative model was modeled with a separate implicit generative model. Of course, two approaches are very different ([2] used an adversarial training to match the aggregate posterior), however I believe the paper is worth mentioning. (2) In light of the discussion on page 6 as well as some of the conclusions regarding commonly reported blurriness of the VAE models, results of Section 4.1 of [3] look quite relevant. (3) It would be nice to specify the dimensionality of the Sz matrix in definition 1. (4) Line ater Eq. 3: I think it should be $\\int p_gt(x) \\log p_\\theta(x) dx$ ? (5) Eq 4: p_\\theta(x|x) (6) Page 4: \"... mass to most all measurable...\". (7) Eq 34. Is it sqrt(\\gamma_t) or just \\gamma_t? (8) Line after Eq 40. Why exactly D(u^*) is finite? I only checked proofs of Theorems 1 and 2 in details and those looked correct. [1] Lucic et al., 2018. [2] Zhao et al., Adversarially regularized autoencoders, 2017, http://proceedings.mlr.press/v80/zhao18b.html [3] Bousquet et al., From optimal transport to generative modeling: the VEGAN cookbook. 2017, https://arxiv.org/abs/1705.07642", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We appreciate the detailed and positive comments , which truly reflect many of the essential contributions of our work . Likewise to the best of our knowledge , the FID scores we report are indeed the first to close the gap between GANs and non-adversarial AE-based methods as the reviewer points out . Regarding the small comments concluding the review , we answer as follows : - Reviewer Comment : Is the code / checkpoints going to be available anytime soon ? Our Response : It was our original intention to simply post the code on Github after decisions were issued and papers were de-anonymized . However , if there is a need to make the code available earlier while preserving anonymity , we could presumably pursue that as well ( but not sure if this is considered acceptable under ICLR guidelines ) . - Reviewer Comment : Reference to an alternative method for estimating the aggregate posterior , and another paper addressing causes of blurry VAE representations . Our Response : Thanks for the nice references . These papers actually look very interesting ; we can cite them and provide context in the revision . - Reviewer Comment : Line after Eq.3 : I think it should be \\int p_gt ( x ) \\log p_\\theta ( x ) dx ? Our Response : It is true that L ( \\theta , \\phi ) > = - \\int p_gt ( x ) \\log p_\\theta ( x ) dx . However , we further have that - \\int p_gt ( x ) \\log p_\\theta ( x ) dx > = -\\int p_gt ( x ) \\log p_gt ( x ) dx , which is the expression we include below Eq . ( 3 ) in the paper . The equality holds iff KL [ q_\\phi ( z|x ) || p_\\theta ( z|x ) ] = 0 and p_\\theta ( x ) = p_gt ( x ) almost everywhere . - Reviewer Comment : Line after Eq 40 . Why exactly is D ( u^ * ) finite ? Our Response : Because \\varphi ( u ) is a diffeomorphism , is has a differentiable inverse and \\Lambda ( u ) = ( d\\varphi ( u ) ^-1/du ) ^\\top ( d\\varphi ( u ) ^-1/du ) is always finite . Furthermore , D ( u^ * ) is the maximum of \\Lambda ( u ) in a closed set centered at u^ * , so it is finite . We will update the proof to include these extra details . - Reviewer Comment : Minor typos/corrections Our Response : Thanks for catching each of these and also checking the proofs carefully . We have fixed each typo/suggestion in a revised version ."}], "0": {"review_id": "B1e0X3C9tQ-0", "review_text": "This paper proposed a two-stage VAE method to generate high-quality samples and avoid blurriness. It is accomplished by utilizing a VAE structure on the observation and latent variable separately. The paper exploited a collection of interesting properties of VAE and point out the problem existed in the generative process of VAE. I have several concerns about the paper: 1. It is necessary to explain why the second-stage VAE can have its latent variable more closely resemble N(u|0,I). Even if the latent variable closely resemble N(u|0,I), How does it make sure the generated images are realistic? I admit that the VAE model can reconstruct realistic data based on its inferred latent variable, however, when given a random sample from N(u|0,I), the generated images are not good, which is true when the dimension of the latent space is high. I still can\u2019t understand why a second-stage VAE can relief this problem. 2. The adversarial auto-encoder is also proposed to solve the latent space problem, by comparison, what is the advantage of this paper? 3. Why do you set the model as two separate stages? Will it enhance the performance if we train theses two-stages all together? 4. The proofs for the theory 2 and 3 are under the assumption that the manifold dimension of the observation is r, while in reality it is difficult to obtain this r, do these theories applicable if we choose a value for the dimension of the latent space that is smaller than the real manifold dimension of the observation? How will it affect the performance of the proposed method? 5. The value of r and k in each experiment should be specified. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "2.Reviewer Comment : The adversarial autoencoder is also proposed to solve the latent space problem , by comparison , what is the advantage of this paper ? Our Response : The adversarial autoencoder ( Makhzani et al. , 2016 ) requires adversarial training , meaning that like all GAN-related models , a complex min-max problem must be optimized in search of a saddle point . A well-recognized advantage of VAEs is that the training involves pure minimization of a fixed variational energy function , which is generally more stable and resistant to mode collapse . We should also point out that unlike VAEs , the adversarial autoencoder has no mechanism for pruning superfluous dimensions in the latent space . Regardless of these key differences , we are aware of no published work where the adversarial autoencoder has been shown to produce competitive results generating novel samples like other GAN-related models ( rather it has been tested on auxiliary tasks like semi-supervised learning , which is not in our scope ) . Indeed the exhaustive recent testing from ( Lucic et al. , 2018 ) upon which we based our experiments , does not even include the adversarial autoencoder as a benchmark . 3.Reviewer Comment : Why train the model as two separate stages ? Will it enhance the performance if we train these two stages together ? Our Response : We have addressed this question on the bottom of page 7 , which states the following : `` It should also be emphasized that concatenating the two stages and jointly training does not improve the performance . If trained jointly the few extra second-stage parameters are simply hijacked by the dominant objective from the first stage and forced to work on an incrementally better fit of the manifold . As expected then , on empirical tests ( not shown ) we have found that this does not improve upon standard VAE baselines . '' Our theoretical results and algorithm development from Sections 2-4 also directly support this conclusion . Regardless , we are happy to clarify further if needed . 4.Reviewer Comment : Do the technical proofs require knowledge of the ground-truth manifold dimensions r ? And how is the proposed algorithm affected when r is unknown ? Our Response : None of our proofs require that the ground-truth r is known explicitly in advance . All that is required is that we set kappa > = r ( please see proof statements for Theorems 1-3 ) . In other words , we only need to set the latent dimension kappa to be bigger than the ground-truth manifold dimension r. The VAE then has a natural mechanism in place for discarding superfluous dimensions . Of course obviously in practice if we set kappa to be far too large , then the training could potentially become a bit more difficult , since in addition to learning the correct ground-truth manifold , we are also burdening the model to detect a much larger number of unnecessary dimensions . But the VAE is arguably more robust to kappa than most methods , and the basic point still holds : we need not set kappa = r , we just need to choose kappa to be a reasonable value that is at least as big as r. In contrast , if we set kappa < r , then the theory starts to break down and practical performance will begin to degrade as expected . 5.Reviewer Comment : The value of r and kappa in each experiment should be specified . Our Response : The true latent manifold dimension r is unknown in all of our experiments since we are using real-world data . However , for the dimension of the VAE latent code , we chose kappa = 64 for all experiments , except for the 2-Stage VAE * model results , where we used 32 for MNIST and Fashion-MNIST , 192 for CIFAR-10 , and 256 for CelebA . Note that these values were not carefully tuned and need not be exact per the arguments responding to reviewer comment 4 above . We just tried a single smaller value for the simpler data ( MNIST and FashionMNIST ) , and a couple larger values for the more complex ones ( CIFAR-10 and CelebA ) ."}, "1": {"review_id": "B1e0X3C9tQ-1", "review_text": "Overview: I thank the authors for their interesting and detailed work in this paper. I believe it has the potential to provide strong value to the community interested in using VAEs with an explicit and simple parameterization of the approximate posterior and likelihood as Gaussian. Gaussianity can be appropriate in many cases where no sequential or discrete structure needs to be induced in the model. I find the mathematical arguments interesting and enlightening. However, the authors somewhat mischaracterize the scope of applicability of VAE models in contemporary machine learning, and don't show familiarity with the broad literature around VAEs outside of this case (that is, where a Gaussian model of the output would be manifestly inappropriate). Since the core of the paper is valuable and salvageable from a clarity standpoint, my comments below are geared towards what changes the authors may make to move this paper into the \"pass\" category. Pros: - Mathematical insights are well reasoned and interesting. Based on the insight from the analysis in the supplementary materials, the authors propose a two-stage VAE which separate learning the a parsimonious representation of the low-dimensional (lower than the ambient dimension of the input space), and the training a second VAE to learn the unknown approximate posterior. The two-stage training procedure is both theoretically motivated and appears to enhance the output quality of VAEs w.r.t. FID score, making them rival GAN architectures on this metric. Cons: - The title and general tone of the paper is too broad: it is only VAE models with Gaussian approximate posteriors and likelihoods. This is hardly the norm for most applications, contrary to the claims of the authors. VAEs are commonly used for discrete random variables, for example. Many cases where VAEs are applied cannot use a Gaussian assumption for the likelihood, which is the key requirement for the proofs in the supplement to be valid (then, the true posterior is also Gaussian, and the KL divergence between that and the approximate posterior can be driven to zero during optimization--clearly a Gaussian approximate posterior will never have zero KL divergence with a non-Gaussian true posterior). - None of the proofs consider the approximation error garnered by only having access to empirical samples through a sample of the ground truth population. (The ground-truth distribution must be defined with respect to the population rather just the dataset in hand, otherwise we lose all generalizability from a model.) Moreover, the proofs hold asymptotically. Generalization bounds and error from finite time approximations are very pertinent issues and these are ignored by the presented analyses. Such concerns have motivated many of the recent developments in approximate posterior distributions. Overall, the paper contains little evidence of familiarity with the recent advances in approximate Bayesian inference that have occurred over the past two years. - A central claim of the paper is that the two-stage VAE obviates the need for highly adaptive approximate posteriors. However, no comparison against those models is done in the paper. How does a two-stage VAE compare against one with, e.g., a normalizing flow approximate posterior? I acknowledge that the purpose of the paper was to argue for the Gaussianity assumption as less stringent than previously believed, but all of the mathematical arguments take place in an imagined world with infinite time and unbounded access to the population distribution. This is not really the domain of interest in modern computational statistics / machine learning, where issues of generalization and computational efficiency are paramount. - While the mathematical insights are well developed, the specifics of the algorithm used to implement the two-stage VAE are a little opaque. Ancestral sampling now takes place using latent samples from a second VAE. An algorithm box is badly needed for reproducibility. Recommendations / Typos I noted a few typos and omissions that need correction. - Generally, the mathematical proofs in section 7 of the supplement are clear. At the top of page 11, though, the paragraph correctly begins by stating that the composition of invertible functions is invertible, but fails to establish that G is also invertible. Clearly it is so by construction, but the explicit reasons should be stated (as a prior sentence promises), and so I assume this is an accidental omission. - The title of Section 8.1 has a typo: clearly is it is the negative log of p_{theta_t} (x) which approaches its infimum rather than p_{theta_t} (x) approaching negative infinity. - Equation (4): the true posterior has an x as its argument instead of the latent z. - Missing parenthesis under Case 2 and wrong indentation. This analysis also seems to be cut off. Is the case r > d relevant here? * EDIT: I have read the authors' detailed response. It has clarified a few key issues, and convinced me of the value to the community for publication in its present (slightly edited according to the reviwers' feedback) form. I would like to see this published and discussed at ICLR and have revised my score accordingly. *", "rating": "7: Good paper, accept", "reply_text": "- Reviewer Comment : No comparisons against VAE models with more flexible approximate posteriors such as those produced via normalizing flows Our Response : We agree that more flexible , explicitly non-Gaussian approximate posteriors have recently been proposed , such as the many flavors that utilize normalizing flows . But such models have not as of yet been objectively shown to improve sampling quality ( see comments above ) despite the tremendous community-wide incentive to publish such a demonstration . Moreover , the added flexibility often comes with a significant cost ( e.g. , increased training difficulty , more expensive inference ) . Furthermore , if we consider broader VAE modifications beyond just the encoder , then even within this wider domain , the only VAE-related enhancement we are aware of that objectively/quantitatively produces improved samples is the WAE model from ICLR this year ( Tolstikhin et al. , 2018 ) , which is already explicitly addressed in Section 5 of our submission . Consequently , unless there is some very recent reference we may have missed , our experiments represent the state-of-the-art for non-adversarial VAE/autoencoer-based structures in term of the objective evaluation of generated samples , and the first to close the gap with GANs ( this is also consistent with the comments from AnonReviewer2 ) . - Reviewer Comment : Some details about the proposed 2-stage process are unclear Although there was unfortunately no space for a separate algorithm box in our submission , the three bullet points on page 6 describe the specific process we used . Note that the ancestral sampling required is very straightforward as described in bullet point 3 on page 6 . This is exactly what we followed for generating new samples via our method , but we are happy to provide further clarification if the reviewer has a specific suggestion . - Reviewer Comment : Recommendations/Typos Our Response : We sincerely appreciate the effort in finding typos and checking the proofs . We have corrected each of the cases the reviewer uncovered . This will certainly be of benefit to future readers . Additionally , r can never be greater than d , because r is the manifold dimension within the ambient space of dimension d ."}, "2": {"review_id": "B1e0X3C9tQ-2", "review_text": "The paper provides a number of novel interesting theoretical results on \"vanilla\" Gaussian Variational Auto-Encoders (VAEs) (sections 1, 2, and 3), which are then used to build a new algorithm called \"2 stage VAEs\" (Section 4). The resulting algorithm is as stable as VAEs to train (it is free of any sort of adversarial training, it comes with a little overhead in terms of extra parameters), while achieving a quality of samples which is *very impressive* for an Auto-Encoder (AE) based generative modeling techniques (Section 5). In particular, the method achieves FID score 24 on the CelebA dataset which is on par with the best GAN-based models as reported in [1], thus sufficiently reducing the gap between the generative quality of the GAN-based and AE-based models reported in the literature. Main theoretical contributions: 1. In some cases the variational bound of Gaussian VAEs can get tight (Theorem 1). In the context of vanilla Gaussian VAEs (Gaussian prior, encoders, and decoders) the authors show that if (a) the intrinsic data dimensionality r is equal to the data space dimensionality d and (b) the latent space dimensionality k is not smaller than r then there is a sequence of encoder-decoder pairs achieving the global minimum of the VAE objective and simultaneously (a) zeroing the variational gap and (b) precisely matching the true data distribution. In other words, in this setting the variational bound and the Gaussian model does not prevent the true data distribution from being recovered. 2. In other cases Gaussian VAEs may not recover the actual distribution, but they will recover the real manifold (Theorems 2, 3, 4 and discussions on page 5). In case when r < d, that is when the data distribution is supported on a low dimensional smooth manifold in the input space, things are quite different. The authors show that there are still sequences of encoder-decoder pairs which achieves the global minimum of the VAE objective. However, this time only *some* of these sequences converge to the model which is in a way indistinguishable from the true data distribution (and thus again Gaussian VAEs do not fundamentally prevent the true distribution from being recovered). Nevertheless, all sequences mentioned above recover the true data manifold in that (a) the optimal encoder learns to use r dimensional linear subspace in the latent space to encode the inputs in a lossless and noise-free way, while filling the remaining k - r dimensions with a white Gaussian noise and (b) the decoder learns to ignore the k - r noisy dimensions and use the r \"informative\" dimensions to produce the outputs perfectly landing on the true data manifold. Main algorithmic contributions: (0) A simple 2 stage algorithm, where first a vanilla Gaussian VAE is trained on the input dataset and second a separate vanilla Gaussian VAE is trained to match the aggregate posterior obtained after the first stage. The authors support this algorithm with a reasonable theoretical argument based on theoretical insights listed above (see end of page 6 - beginning of page 7). The algorithm achieves state-of-art FID scores across several data sets among AE based models existing in the literature. Review summary: I would like to say that this paper was a breath of fresh air to me. I really liked how the authors make a strong point that *it is not the Gaussian assumptions that harm the performance of VAEs* in contrast to what is usually believed in the field nowadays. Also, I think *the reported FID scores alone may be considered as a significant enough contribution*, because to my knowledge this is the first paper significantly closing the gap between generative quality of GAN-based models and non-adversarial AE-based methods. *************** *** Couple of comments and typos: *************** (0) Is the code / checkpoints going to be available anytime soon? (1) I would mention [2] which in a way used a very similar approach, where the aggregate posterior of the implicit generative model was modeled with a separate implicit generative model. Of course, two approaches are very different ([2] used an adversarial training to match the aggregate posterior), however I believe the paper is worth mentioning. (2) In light of the discussion on page 6 as well as some of the conclusions regarding commonly reported blurriness of the VAE models, results of Section 4.1 of [3] look quite relevant. (3) It would be nice to specify the dimensionality of the Sz matrix in definition 1. (4) Line ater Eq. 3: I think it should be $\\int p_gt(x) \\log p_\\theta(x) dx$ ? (5) Eq 4: p_\\theta(x|x) (6) Page 4: \"... mass to most all measurable...\". (7) Eq 34. Is it sqrt(\\gamma_t) or just \\gamma_t? (8) Line after Eq 40. Why exactly D(u^*) is finite? I only checked proofs of Theorems 1 and 2 in details and those looked correct. [1] Lucic et al., 2018. [2] Zhao et al., Adversarially regularized autoencoders, 2017, http://proceedings.mlr.press/v80/zhao18b.html [3] Bousquet et al., From optimal transport to generative modeling: the VEGAN cookbook. 2017, https://arxiv.org/abs/1705.07642", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We appreciate the detailed and positive comments , which truly reflect many of the essential contributions of our work . Likewise to the best of our knowledge , the FID scores we report are indeed the first to close the gap between GANs and non-adversarial AE-based methods as the reviewer points out . Regarding the small comments concluding the review , we answer as follows : - Reviewer Comment : Is the code / checkpoints going to be available anytime soon ? Our Response : It was our original intention to simply post the code on Github after decisions were issued and papers were de-anonymized . However , if there is a need to make the code available earlier while preserving anonymity , we could presumably pursue that as well ( but not sure if this is considered acceptable under ICLR guidelines ) . - Reviewer Comment : Reference to an alternative method for estimating the aggregate posterior , and another paper addressing causes of blurry VAE representations . Our Response : Thanks for the nice references . These papers actually look very interesting ; we can cite them and provide context in the revision . - Reviewer Comment : Line after Eq.3 : I think it should be \\int p_gt ( x ) \\log p_\\theta ( x ) dx ? Our Response : It is true that L ( \\theta , \\phi ) > = - \\int p_gt ( x ) \\log p_\\theta ( x ) dx . However , we further have that - \\int p_gt ( x ) \\log p_\\theta ( x ) dx > = -\\int p_gt ( x ) \\log p_gt ( x ) dx , which is the expression we include below Eq . ( 3 ) in the paper . The equality holds iff KL [ q_\\phi ( z|x ) || p_\\theta ( z|x ) ] = 0 and p_\\theta ( x ) = p_gt ( x ) almost everywhere . - Reviewer Comment : Line after Eq 40 . Why exactly is D ( u^ * ) finite ? Our Response : Because \\varphi ( u ) is a diffeomorphism , is has a differentiable inverse and \\Lambda ( u ) = ( d\\varphi ( u ) ^-1/du ) ^\\top ( d\\varphi ( u ) ^-1/du ) is always finite . Furthermore , D ( u^ * ) is the maximum of \\Lambda ( u ) in a closed set centered at u^ * , so it is finite . We will update the proof to include these extra details . - Reviewer Comment : Minor typos/corrections Our Response : Thanks for catching each of these and also checking the proofs carefully . We have fixed each typo/suggestion in a revised version ."}}