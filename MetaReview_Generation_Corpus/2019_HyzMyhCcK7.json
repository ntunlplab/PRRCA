{"year": "2019", "forum": "HyzMyhCcK7", "title": "ProxQuant: Quantized Neural Networks via Proximal Operators", "decision": "Accept (Poster)", "meta_review": "A novel  approach for quantized deep neural nets is proposed,  which is more principled than commonly used  straight-through gradient method. A theoretical analysis of the algorithm's converegence  is presented, and empirical results show advantages of the proposed approach. ", "reviews": [{"review_id": "HyzMyhCcK7-0", "review_text": "This paper proposed ProxQuant method to train neural networks with quantized weights. ProxQuant relax the quantization constraint to a continuous regularizer and then solve the optimization problem with proximal gradient method. The authors argues that previous solvers straight through estimator (STE) in BinaryConnect (Courbariaux et al. 2015) may not converge, and the proposed ProxQuant is better. I have concerns about both theoretical and experimental contributions 1. The proposed regularizer for relaxing quantized constraint looks similar to BinaryRelax (Yin et al. 2018 BinaryRelax: A Relaxation Approach For Training Deep Neural Networks With Quantized Weights.), which is not cited. I hope the authors can discuss this work and clarify the novelty of the proposed method. One difference I noticed is that BinaryRelax use lazy prox-graident, while the proposed ProxQuant use non-lazy update. It is unclear which one is better. 2. On page 5, the authors claim \u2018\u2019Our proposed method can be viewed as \u2026 generalization ...\" in page 5. It seems inaccurate because unlike proposed method, BinaryConnect use lazy prox-gradient. 3. What\u2019s the purpose of equation (4)? I am confused and did not find it explained in the content. 4. The proposed method introduced more hyper-parameters, like the regularizer parameter \\lambda, and the epoch to perform hard quantization. In section 4.2, it is indicated that parameter \\lambda is tuned on validation set. I have doubts about the fairness comparing with baseline BinaryConnect. Though BC does not have this parameter, we can still tune learning rate. 5. ProxQuant is fine-tuned based on the pre-trained real-value weights. Is BinaryConnect also fine-tuned? For a CIFAR-10 experiments, 600 epochs are a lot for fine-tuning. As a comparison, training real-value weights usually use less than 300 epochs. BinaryConnect can be trained from scratch using same number of epochs. What does it mean to hard-quantize BinaryConnect? The weights are already quantized after projection step in BinaryConnect. 6. The authors claim there are no reported results with ResNets on CIFAR-10 for BinaryConnect, which is not true. (Li et al. 2017 Training Quantized Nets: A Deeper Understanding) report results on ResNet-56, which I encourage authors to compare with. 7. What is the benefit of ProxQuant? Is it faster than BinaryConnect? If yes, please show convergence curves. Does it generate better results? Table 1 and 2 does not look convincing, especially considering the fairness of comparison. 8. How to interpret Theorem 5.1? For example, Li et al. 2017 show the real-value weights in BinaryConnect can converge for quadratic function, does it contradict with Theorem 5.1? 9. I would suggest authors to rephrase the last two paragraphs of section 5.2. It first states \u2018\u2019one needs to travel further to find a better net\", and then state ProxQuant find good result nearby, which is confusing. 10. The theoretical benefit of ProxQuant is only intuitively explained, it looks to me there lacks a rigorous proof to show ProxQuant will converge to a solution of the original quantization constrained problem. 11. The draft is about 9 pages, which is longer than expected. Though the paper is well written and I generally enjoyed reading, I would appreciate it if the authors could shorten the content. My main concerns are novelty of the proposed method, and fairness of experiments. ======================= after rebuttal ======================= I appreciate the authors' efforts and am generally satisfied with the revision. I raised my score. The authors show advantage of the proposed ProxQuant over previous BinaryConnect and BinaryRelax in both theory and practice. The analysis bring insights into training quantized neural networks and should be welcomed by the community. However, I still have concerns about novelty and experiments. - The proposed ProxQuant is similar to BinaryRelax except for non-lazy vs. lazy updates. I personally like the theoretical analysis showing ProxQuant is better, although it is based on smooth assumptions. However, I am quite surprised BinaryRelax is so much worse than ProxQuant and BinaryConnect in practice (table 1). I would encourage the authors to give more unintuitive explanation. - The training time is still long, and the experimental setting seems uncommon. I appreciate the authors' efforts on shortening the finetuning time, and provide more parameter tuning. However, 200 epochs training full precision network and 300 epochs for finetuning is still a long time, consider previous works like BinaryConnect can train from scratch without a full precision warm start. In this long-training setting, the empirical advantage of ProxQuant over baselines is not much (less than 0.3% for cifar-10 in table 1, and comparable with Xu 2018 in table 2). ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the very concrete and thoughtful feedback ! We have found the comments very useful and constructive for revising the paper . We have made some initial revisions to address the comments -- please find our changes as well as our response to the comments below . Novelty and Fairness of Experiments Point 1 -- As you have pointed out , the main algorithmic difference between ours and Yin et al . ( 2018 ) is that we use a non-lazy , standard prox-gradient method whereas their BinaryRelax is a lazy prox-gradient . The further novelty of our paper lies in the new observation that BinaryConnect suffers from more optimization instability , which are both theoretically and empirically justified in our Section 5 . We have addressed Yin et al . ( 2018 ) as well as a few other related literature in the Prior Work subsection ( within the \u201c Principled Methods \u201d paragraph ) , comparing them with our work and highlighting our novelty . Point 5 -- Both BinaryConnect and our ProxQuant are initialized at pre-trained full-precision nets , which are trained with 200 epochs over CIFAR-10 . For quantization , our schedule is essentially 400 epochs training , and the additional 200 epochs after hard quantization is mostly for fine-tuning the BatchNorm layers . Such fine-tuning was found very useful for * both ProxQuant and BinaryConnect * . Indeed , for BinaryConnect , the signed net keeps changing ( in a tiny proportion ) even at epoch 400 , and the BatchNorm layer hesitates around without being optimized towards any fixed binary net . Hard quantizing forces BinaryConnect to stay at a specific binary net , after which the BatchNorm layer can approach this optimal and boosts performance . We have modified Section 4.1 to clarify this . Theoretical Results Point 8 -- Li et al. \u2019 s convergence bound involves an additive error O ( \\Delta ) that does not vanish over iterations , where \\Delta is the grid size for quantization . Hence , their result is only useful when \\Delta is small . In contrast , we consider the original BinaryConnect with \\Delta = 1 , in which case the error makes Li et al. \u2019 s bound vacuous . We have added a remark after Theorem 5.1 to clarify that . Point 9 -- We have rephrased the last two paragraphs in Section 5.2 a bit , to first state our finding and then analyze why it shows the power of ProxQuant over BinaryConnect . Point 10 -- We have added a convergence guarantee for ProxQuant in Appendix D , showing that ProxQuant converges to a stationary point of the regularized loss . Presentation Point 2 -- We have added that we are also using the non-lazy prox to highlight our difference from BinaryConnect . Point 3 -- The Eq ( 4 ) was just an expanded formula for the prox-gradient method . As it did not really mean to say anything and the prox operator has been already defined , we have removed it for clarity . Point 11 -- We would indeed like to shorten the paper . We will do that once we have a better idea of the potential additional materials that we would present . Please stay tuned . Additional Experiments Point 4 , 6 , 7 -- We will work on some additional experiments to address these points . Please stay tuned and we will let you know once it \u2019 s done . For Point 6 -- The baseline classification error of Adam + BinaryConnect on ResNet56 in Li et . al is 8.10 % , whereas we already achieve a better error 7.79 % on ResNet44 . We suspect this is due to the difference in the initializing FP net ."}, {"review_id": "HyzMyhCcK7-1", "review_text": "After the rebuttal: 1. Still, the novelty is limited. The authors want to tell a more motivated storyline from Nestrove-dual-average, but that does not contribute to the novelty of this paper. The real difference to the existing works is \"using soft instead of hard constraint\" for BNN. 2. The convergence is a decoration. It is easy to be obtained from existing convergence proof of proximal gradient algorithms, e.g. [accelerated proximal gradient methods for nonconvex programming. NIPS. 2015]. --------------------------- This paper proposes solving binary nets and it variants using proximal gradient descent. To motivate their method, authors connect lazy projected SGD with straight-through estimator. The connection looks interesting and the paper is well presented. However, the novelty of the submission is limited. 1. My main concern is on the novelty of this paper. While authors find a good story for their method, for example, - A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training - Training Ternary Neural Networks with Exact Proximal Operator - Loss-aware Binarization of Deep Networks All above papers are not mentioned in the submission. Thus, from my perspective, the real novelty of this paper is to replace the hard constraint with a soft (penalized) one (section 3.2). 2. Could authors perform experiments with ImageNet? 3. Could authors show the impact of lambda_t on the final performance? e.g., lambda_t = sqrt(t) lambda, lambda_t = sqrt(t^2 lambda", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable feedback ! We have made a revision to the paper to address all the comments . We will respond to the specific questions in the following . Novelty -- - We agree that there has been a large literature on replacing the straight-through estimator with prox-type algorithms . Our novelty comes in two aspects : ( 1 ) The proposal of combining non-lazy proximal gradient method with a finite ( soft ) regularization , as well as principled methods for quantizing to binary , ternary , and multi-bit . ( 2 ) A new challenge to the straight-through gradient estimate in its optimization instability through systematic theoretical and empirical investigations . In particular , we show that the convergence criterion of BinaryConnect is very stringent ( Theorem 5.1 ) , while our proposed ProxQuant is guaranteed to converge on smooth problems Theorem D.1 ) . Our sign change experiment in Section 5.2 further shows that BinaryConnect is indeed highly unstable in its optimization , as well as giving a lower-performance solution , compared with ProxQuant . We have updated the related work section ( in particular the \u201c Principled methods \u201d part ) to include these citations . ImageNet experiments -- - Due to time constraints , we didn \u2019 t have time to perform ImageNet experiments for this submission . We have experimental results on LSTMs ( Section 4.2 ) to be complementary with the CIFAR-10 results . Performing ImageNet experiments will be of our interest as a future direction . Experiments with \\lambda_t -- - We have thought about that , but we chose to use the linear scheme \\lambda_t = \\lambda * t for simplicity and to demonstrate that a simple choice would work well . We suspect that changing the schemes would not boost the performance by a great deal -- but we would like to test it experimentally . Please stay tuned and we would potentially add that in our next revision ."}, {"review_id": "HyzMyhCcK7-2", "review_text": "This paper proposes a new approach to learning quantized deep neural networks, which overcome some of the drawbacks of previous methods, namely the lack of understanding of why straight-through gradient works and its optimization instability. The core of the proposal is the use of quantization-encouraging regularization, and the derivation of the corresponding proximity operators. Building on that core, the rest of the approach is reasonably standard, based on stochastic proximal gradient descent, with a homotopy scheme. The experiments on benchmark datasets provide clear evidence that the proposed method doesn't suffer from the drawbacks of straight-through gradient, does contributing to the state-of-the-art of this class of methods. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for the valuable feedback !"}], "0": {"review_id": "HyzMyhCcK7-0", "review_text": "This paper proposed ProxQuant method to train neural networks with quantized weights. ProxQuant relax the quantization constraint to a continuous regularizer and then solve the optimization problem with proximal gradient method. The authors argues that previous solvers straight through estimator (STE) in BinaryConnect (Courbariaux et al. 2015) may not converge, and the proposed ProxQuant is better. I have concerns about both theoretical and experimental contributions 1. The proposed regularizer for relaxing quantized constraint looks similar to BinaryRelax (Yin et al. 2018 BinaryRelax: A Relaxation Approach For Training Deep Neural Networks With Quantized Weights.), which is not cited. I hope the authors can discuss this work and clarify the novelty of the proposed method. One difference I noticed is that BinaryRelax use lazy prox-graident, while the proposed ProxQuant use non-lazy update. It is unclear which one is better. 2. On page 5, the authors claim \u2018\u2019Our proposed method can be viewed as \u2026 generalization ...\" in page 5. It seems inaccurate because unlike proposed method, BinaryConnect use lazy prox-gradient. 3. What\u2019s the purpose of equation (4)? I am confused and did not find it explained in the content. 4. The proposed method introduced more hyper-parameters, like the regularizer parameter \\lambda, and the epoch to perform hard quantization. In section 4.2, it is indicated that parameter \\lambda is tuned on validation set. I have doubts about the fairness comparing with baseline BinaryConnect. Though BC does not have this parameter, we can still tune learning rate. 5. ProxQuant is fine-tuned based on the pre-trained real-value weights. Is BinaryConnect also fine-tuned? For a CIFAR-10 experiments, 600 epochs are a lot for fine-tuning. As a comparison, training real-value weights usually use less than 300 epochs. BinaryConnect can be trained from scratch using same number of epochs. What does it mean to hard-quantize BinaryConnect? The weights are already quantized after projection step in BinaryConnect. 6. The authors claim there are no reported results with ResNets on CIFAR-10 for BinaryConnect, which is not true. (Li et al. 2017 Training Quantized Nets: A Deeper Understanding) report results on ResNet-56, which I encourage authors to compare with. 7. What is the benefit of ProxQuant? Is it faster than BinaryConnect? If yes, please show convergence curves. Does it generate better results? Table 1 and 2 does not look convincing, especially considering the fairness of comparison. 8. How to interpret Theorem 5.1? For example, Li et al. 2017 show the real-value weights in BinaryConnect can converge for quadratic function, does it contradict with Theorem 5.1? 9. I would suggest authors to rephrase the last two paragraphs of section 5.2. It first states \u2018\u2019one needs to travel further to find a better net\", and then state ProxQuant find good result nearby, which is confusing. 10. The theoretical benefit of ProxQuant is only intuitively explained, it looks to me there lacks a rigorous proof to show ProxQuant will converge to a solution of the original quantization constrained problem. 11. The draft is about 9 pages, which is longer than expected. Though the paper is well written and I generally enjoyed reading, I would appreciate it if the authors could shorten the content. My main concerns are novelty of the proposed method, and fairness of experiments. ======================= after rebuttal ======================= I appreciate the authors' efforts and am generally satisfied with the revision. I raised my score. The authors show advantage of the proposed ProxQuant over previous BinaryConnect and BinaryRelax in both theory and practice. The analysis bring insights into training quantized neural networks and should be welcomed by the community. However, I still have concerns about novelty and experiments. - The proposed ProxQuant is similar to BinaryRelax except for non-lazy vs. lazy updates. I personally like the theoretical analysis showing ProxQuant is better, although it is based on smooth assumptions. However, I am quite surprised BinaryRelax is so much worse than ProxQuant and BinaryConnect in practice (table 1). I would encourage the authors to give more unintuitive explanation. - The training time is still long, and the experimental setting seems uncommon. I appreciate the authors' efforts on shortening the finetuning time, and provide more parameter tuning. However, 200 epochs training full precision network and 300 epochs for finetuning is still a long time, consider previous works like BinaryConnect can train from scratch without a full precision warm start. In this long-training setting, the empirical advantage of ProxQuant over baselines is not much (less than 0.3% for cifar-10 in table 1, and comparable with Xu 2018 in table 2). ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the very concrete and thoughtful feedback ! We have found the comments very useful and constructive for revising the paper . We have made some initial revisions to address the comments -- please find our changes as well as our response to the comments below . Novelty and Fairness of Experiments Point 1 -- As you have pointed out , the main algorithmic difference between ours and Yin et al . ( 2018 ) is that we use a non-lazy , standard prox-gradient method whereas their BinaryRelax is a lazy prox-gradient . The further novelty of our paper lies in the new observation that BinaryConnect suffers from more optimization instability , which are both theoretically and empirically justified in our Section 5 . We have addressed Yin et al . ( 2018 ) as well as a few other related literature in the Prior Work subsection ( within the \u201c Principled Methods \u201d paragraph ) , comparing them with our work and highlighting our novelty . Point 5 -- Both BinaryConnect and our ProxQuant are initialized at pre-trained full-precision nets , which are trained with 200 epochs over CIFAR-10 . For quantization , our schedule is essentially 400 epochs training , and the additional 200 epochs after hard quantization is mostly for fine-tuning the BatchNorm layers . Such fine-tuning was found very useful for * both ProxQuant and BinaryConnect * . Indeed , for BinaryConnect , the signed net keeps changing ( in a tiny proportion ) even at epoch 400 , and the BatchNorm layer hesitates around without being optimized towards any fixed binary net . Hard quantizing forces BinaryConnect to stay at a specific binary net , after which the BatchNorm layer can approach this optimal and boosts performance . We have modified Section 4.1 to clarify this . Theoretical Results Point 8 -- Li et al. \u2019 s convergence bound involves an additive error O ( \\Delta ) that does not vanish over iterations , where \\Delta is the grid size for quantization . Hence , their result is only useful when \\Delta is small . In contrast , we consider the original BinaryConnect with \\Delta = 1 , in which case the error makes Li et al. \u2019 s bound vacuous . We have added a remark after Theorem 5.1 to clarify that . Point 9 -- We have rephrased the last two paragraphs in Section 5.2 a bit , to first state our finding and then analyze why it shows the power of ProxQuant over BinaryConnect . Point 10 -- We have added a convergence guarantee for ProxQuant in Appendix D , showing that ProxQuant converges to a stationary point of the regularized loss . Presentation Point 2 -- We have added that we are also using the non-lazy prox to highlight our difference from BinaryConnect . Point 3 -- The Eq ( 4 ) was just an expanded formula for the prox-gradient method . As it did not really mean to say anything and the prox operator has been already defined , we have removed it for clarity . Point 11 -- We would indeed like to shorten the paper . We will do that once we have a better idea of the potential additional materials that we would present . Please stay tuned . Additional Experiments Point 4 , 6 , 7 -- We will work on some additional experiments to address these points . Please stay tuned and we will let you know once it \u2019 s done . For Point 6 -- The baseline classification error of Adam + BinaryConnect on ResNet56 in Li et . al is 8.10 % , whereas we already achieve a better error 7.79 % on ResNet44 . We suspect this is due to the difference in the initializing FP net ."}, "1": {"review_id": "HyzMyhCcK7-1", "review_text": "After the rebuttal: 1. Still, the novelty is limited. The authors want to tell a more motivated storyline from Nestrove-dual-average, but that does not contribute to the novelty of this paper. The real difference to the existing works is \"using soft instead of hard constraint\" for BNN. 2. The convergence is a decoration. It is easy to be obtained from existing convergence proof of proximal gradient algorithms, e.g. [accelerated proximal gradient methods for nonconvex programming. NIPS. 2015]. --------------------------- This paper proposes solving binary nets and it variants using proximal gradient descent. To motivate their method, authors connect lazy projected SGD with straight-through estimator. The connection looks interesting and the paper is well presented. However, the novelty of the submission is limited. 1. My main concern is on the novelty of this paper. While authors find a good story for their method, for example, - A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training - Training Ternary Neural Networks with Exact Proximal Operator - Loss-aware Binarization of Deep Networks All above papers are not mentioned in the submission. Thus, from my perspective, the real novelty of this paper is to replace the hard constraint with a soft (penalized) one (section 3.2). 2. Could authors perform experiments with ImageNet? 3. Could authors show the impact of lambda_t on the final performance? e.g., lambda_t = sqrt(t) lambda, lambda_t = sqrt(t^2 lambda", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable feedback ! We have made a revision to the paper to address all the comments . We will respond to the specific questions in the following . Novelty -- - We agree that there has been a large literature on replacing the straight-through estimator with prox-type algorithms . Our novelty comes in two aspects : ( 1 ) The proposal of combining non-lazy proximal gradient method with a finite ( soft ) regularization , as well as principled methods for quantizing to binary , ternary , and multi-bit . ( 2 ) A new challenge to the straight-through gradient estimate in its optimization instability through systematic theoretical and empirical investigations . In particular , we show that the convergence criterion of BinaryConnect is very stringent ( Theorem 5.1 ) , while our proposed ProxQuant is guaranteed to converge on smooth problems Theorem D.1 ) . Our sign change experiment in Section 5.2 further shows that BinaryConnect is indeed highly unstable in its optimization , as well as giving a lower-performance solution , compared with ProxQuant . We have updated the related work section ( in particular the \u201c Principled methods \u201d part ) to include these citations . ImageNet experiments -- - Due to time constraints , we didn \u2019 t have time to perform ImageNet experiments for this submission . We have experimental results on LSTMs ( Section 4.2 ) to be complementary with the CIFAR-10 results . Performing ImageNet experiments will be of our interest as a future direction . Experiments with \\lambda_t -- - We have thought about that , but we chose to use the linear scheme \\lambda_t = \\lambda * t for simplicity and to demonstrate that a simple choice would work well . We suspect that changing the schemes would not boost the performance by a great deal -- but we would like to test it experimentally . Please stay tuned and we would potentially add that in our next revision ."}, "2": {"review_id": "HyzMyhCcK7-2", "review_text": "This paper proposes a new approach to learning quantized deep neural networks, which overcome some of the drawbacks of previous methods, namely the lack of understanding of why straight-through gradient works and its optimization instability. The core of the proposal is the use of quantization-encouraging regularization, and the derivation of the corresponding proximity operators. Building on that core, the rest of the approach is reasonably standard, based on stochastic proximal gradient descent, with a homotopy scheme. The experiments on benchmark datasets provide clear evidence that the proposed method doesn't suffer from the drawbacks of straight-through gradient, does contributing to the state-of-the-art of this class of methods. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for the valuable feedback !"}}