{"year": "2017", "forum": "BJ3filKll", "title": "Efficient Representation of Low-Dimensional Manifolds using Deep Networks", "decision": "Accept (Poster)", "meta_review": "There is consensus among the reviewers that the paper presents an interesting and novel direction of study. Having said that, there also appears to be a sense that the proposed construction can be studied in more detail: in particular, (1) an average-case analysis is essential as the worst-case bounds appear extremely loose and (2) the learning problem needs to be addressed in more detail. Nevertheless, this paper deserves to appear at the conference.", "reviews": [{"review_id": "BJ3filKll-0", "review_text": "The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call \"monotonic chains of linear segments\", which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction. While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very \"compatible\". In particular, I have three main concerns with respect to the results presented in this paper: (1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex? (2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses. (3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the \"bound for this case is very loose\". The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets. I would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning. Minor comments: - In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio's group) and parametric t-SNE (van der Maaten, 2009). - What loss do the authors use in their experiments? Using \"the difference between the ground truth distance ... and the distance computed by the network\" seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for these constructive comments . ( 1 ) 'work on learning data representations from sets of local tangent planes ' Thank you for the references , which we have included in a revision of the paper . In particular , Zhang and Zha 's analysis indicates that in the limit when the input points are sampled densely and lie near a piecewise linear manifold their embedding algorithm projects points that are off the manifold orthogonally onto the manifold . Our analysis , in contrast , indicates that a neural network with an efficient architecture will generally not project these points orthogonally . We note however that less efficient networks with significantly more units can also learn to project such points orthogonally onto the manifold . Our main objective in this paper is to show that networks can be significantly more efficient , although this comes at the price of increased error when the data is not fit exactly by the piecewise linear approximation . 'how these old techniques compare to the deep network trained to produce the embedding of Figure 6 ' Unfortunately , it is not viable to experimentally compare the results of traditional manifold learning approaches and our constructions . In our experiments , which serve to illustrate the types of constructions that can be learned by a deep network , we use supervision so that the network learns to map input points to a ground truth embedding . Traditional manifold learning is unsupervised , and so must solve a more difficult problem . In response to the reviewers comments we have applied LLE to the data used to produce Figure 6 . However , while a neural network can learn a good embedding with supervision , learning a construction similar to the one we derive theoretically , LLE is not able to produce a sufficiently good embedding using the somewhat limited data in our experiment , as it does not make use of supervision . It learns an embedding that maps input faces to a straight line based on their azimuth , ignoring elevation . However , we are grateful for the reviewer \u2019 s suggestion , and feel that it will be interesting in future work to more comprehensively explore how the manifold embedding with deep networks is related to these now classical approaches . ( 2 ) 'data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space ' Similar to existing manifold learning techniques , our approach also assumes there are sufficiently many data points to allow identification of the manifold structure . We note that when the data is sparsely spread in a high dimensional space manifold embedding is probably not a preferred strategy . 'network architectures that are not pure ReLU networks ' Common activation functions like softplus , sigmoid and tanh can be approximated accurately with piecewise linear functions that include just a few pieces . With such approximations our analysis can be readily shown to apply also to these activation functions . 'most modern networks use a variant of batch normalization ' Our analysis would not change significantly with batch normalization . Ioffe and Szegedy ( 2015 ) show that a network with batch normalization can represent any function that could be represented by a network without batch normalization , since added parameters allow a linear transformation to be learned that recovers the original network . ( 3 ) 'The error bound presented in Section 4 appears vacuous for any practical setting ' Our error analysis represents a worst case analysis , illustrating what the error would be when the projection directions for every segment chosen in training align with the direction of the largest singular vector of the projection operator . Our experiments indicate that the average case is significantly more favorable . We hope to further analyze this case in future work . 'only refer to fully supervised siamese network approaches ' We thank the reviewer for these references and have included them in our revision . We note that our construction can be used both in supervised and unsupervised settings , but for our experiments we only show results in supervised ones . 'What loss do the authors use in their experiments ? ' In our experiments we use the square loss || y - \\hat y ||^2 where for an input point x ( in ambient space ) y denotes the embedding coordinates produced by the network and \\hat y denotes the ground truth embedding coordinates . We also tested our method in a siamese setting where for two inputs xi xj we used the square loss ( ||yi - yj|| - ||\\hat yi - \\hat yj|| ) ^2 . These experiments yielded similar results . We also somewhat disagree with the reviewer that our results are not surprising . It may seem natural that a neural network can embed a manifold , but we find it very surprising that this can be done so efficiently . In contrast to these results , as we discuss in our paper , Shaham et al . ( 2015 ) have recently published a related construction that is much less efficient ."}, {"review_id": "BJ3filKll-1", "review_text": "Summary: In this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space. They define a class of manifolds, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a construction to embed such a chain with a neural network with one hidden layer. They also give a bound on the number of parameters required to do so, and examine what happens when the manifold is noisy. Experiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss. This experiment supports the theoretical bound on number of parameters needed to embed the monotonic chain. Another experiment varies the elevation and azimuth of of faces, which are known to lie on a monotonic chain, on a regression loss. Comments: The direction of investigation in the paper (looking at what happens to manifolds in a neural network), is very compelling, and I strongly encourage the authors to continue exploring this direction. However, the current version of the paper could use some more work: The experiments are all with a regression loss and a shallow network, and as part of the reason for interest in this question is the very large, high dimensional datasets we use now, which require a deeper network, it seems important to address this case. It also seems important to confirm that embedding works well when *classification* loss is used, instead of regression The theory sections could do with being more clearly written -- I\u2019m not as familiar with the literature in this area, and while the proof method used is relatively elementary, it was difficult to understand what exactly was being proved -- e.g. formally stating what could be expected of an embedding that \u201caccurately and efficiently\u201d preserves a monotonic chain, etc. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for these constructive comments . 'The experiments are all with a regression loss and a shallow network ' Indeed monotonic chains can efficiently be handled with a shallow network . Our construction , however , can be incorporated in deep networks in various ways , both in order to represent complex manifolds by using higher layers to combine several monotonic chains ( as we have demonstrated for the swiss roll ) or in producing hierarchical representations of the data ( such as when an m-dimensional manifold lies in an l-dimensional linear subspace of the ambient space ) . We refer the reviewer to Appendix D in the revision , which now includes further discussion of such deep architectures . 'It also seems important to confirm that embedding works well when * classification * loss is used , instead of regression ' Classification loss is generally more permissive than a regression loss . Consequently , if we apply a classification loss to an architecture that can perform embedding in the lower levels and classification in the higher levels we will generally obtain a distorted embedding , in which each linear segment can deform ( e.g.in the directions of class separation ) -- such embeddings are often sufficient to achieve accurate classification . We now demonstrate this and provide further discussion in Appendix D of our revised manuscript . 'The theory sections could do with being more clearly written ' We appreciate the reviewers suggestions for improving the presentation of the theory , and will incorporate these in a revised version of the paper ."}, {"review_id": "BJ3filKll-2", "review_text": "SUMMARY This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. PROS Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. CONS The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). COMMENTS It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. MINOR COMMENTS - Figure 1 could be referenced first in the text. - \"Color coded\" where the color codes what? - Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. - On page 5, mention how the orthogonal projection on S_k is realized in the network. - On page 6 \"divided into segments\" here `segments' is maybe not the best word. - On page 6 \"The mean relative error is 0.98\" what is the baseline here, or what does this number mean? ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for these additional comments . 'It would be interesting to study the ramifications of the presented observations for the case of deep ( er ) networks ' We agree . Our construction can be incorporated in deep networks in various ways , both in order to represent complex manifolds by using higher layers to combine several monotonic chains ( as we have demonstrated for the swiss roll ) or in producing hierarchical representations of the data ( such as when an m-dimensional manifold lies in an l-dimensional linear subspace of the ambient space ) . We refer the reviewer to Appendix D in the revised version of our paper , which now includes further discussion of such deep architectures . 'to what extent the proposed picture describes the totality of functions that are representable by the networks ' Our approach suggests an efficient way to represent low-dimensional manifolds with neural nets . This can serve as part of representations of functions defined on manifolds . Our work does not suggest specific methods to represent such functions , although , as noted , Shaham et al . ( 2015 ) suggest one way to approach this problem . We appreciate the minor comments of the reviewer and have incorporated them into our revision . Regarding the minor comment 'On page 5 , mention how the orthogonal projection on S_k is realized in the network ' please note that the orthogonal projection is not realized in the network . It is used at this point merely to derive a bound on the error.\u200b"}], "0": {"review_id": "BJ3filKll-0", "review_text": "The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call \"monotonic chains of linear segments\", which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction. While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very \"compatible\". In particular, I have three main concerns with respect to the results presented in this paper: (1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex? (2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses. (3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the \"bound for this case is very loose\". The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets. I would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning. Minor comments: - In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio's group) and parametric t-SNE (van der Maaten, 2009). - What loss do the authors use in their experiments? Using \"the difference between the ground truth distance ... and the distance computed by the network\" seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for these constructive comments . ( 1 ) 'work on learning data representations from sets of local tangent planes ' Thank you for the references , which we have included in a revision of the paper . In particular , Zhang and Zha 's analysis indicates that in the limit when the input points are sampled densely and lie near a piecewise linear manifold their embedding algorithm projects points that are off the manifold orthogonally onto the manifold . Our analysis , in contrast , indicates that a neural network with an efficient architecture will generally not project these points orthogonally . We note however that less efficient networks with significantly more units can also learn to project such points orthogonally onto the manifold . Our main objective in this paper is to show that networks can be significantly more efficient , although this comes at the price of increased error when the data is not fit exactly by the piecewise linear approximation . 'how these old techniques compare to the deep network trained to produce the embedding of Figure 6 ' Unfortunately , it is not viable to experimentally compare the results of traditional manifold learning approaches and our constructions . In our experiments , which serve to illustrate the types of constructions that can be learned by a deep network , we use supervision so that the network learns to map input points to a ground truth embedding . Traditional manifold learning is unsupervised , and so must solve a more difficult problem . In response to the reviewers comments we have applied LLE to the data used to produce Figure 6 . However , while a neural network can learn a good embedding with supervision , learning a construction similar to the one we derive theoretically , LLE is not able to produce a sufficiently good embedding using the somewhat limited data in our experiment , as it does not make use of supervision . It learns an embedding that maps input faces to a straight line based on their azimuth , ignoring elevation . However , we are grateful for the reviewer \u2019 s suggestion , and feel that it will be interesting in future work to more comprehensively explore how the manifold embedding with deep networks is related to these now classical approaches . ( 2 ) 'data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space ' Similar to existing manifold learning techniques , our approach also assumes there are sufficiently many data points to allow identification of the manifold structure . We note that when the data is sparsely spread in a high dimensional space manifold embedding is probably not a preferred strategy . 'network architectures that are not pure ReLU networks ' Common activation functions like softplus , sigmoid and tanh can be approximated accurately with piecewise linear functions that include just a few pieces . With such approximations our analysis can be readily shown to apply also to these activation functions . 'most modern networks use a variant of batch normalization ' Our analysis would not change significantly with batch normalization . Ioffe and Szegedy ( 2015 ) show that a network with batch normalization can represent any function that could be represented by a network without batch normalization , since added parameters allow a linear transformation to be learned that recovers the original network . ( 3 ) 'The error bound presented in Section 4 appears vacuous for any practical setting ' Our error analysis represents a worst case analysis , illustrating what the error would be when the projection directions for every segment chosen in training align with the direction of the largest singular vector of the projection operator . Our experiments indicate that the average case is significantly more favorable . We hope to further analyze this case in future work . 'only refer to fully supervised siamese network approaches ' We thank the reviewer for these references and have included them in our revision . We note that our construction can be used both in supervised and unsupervised settings , but for our experiments we only show results in supervised ones . 'What loss do the authors use in their experiments ? ' In our experiments we use the square loss || y - \\hat y ||^2 where for an input point x ( in ambient space ) y denotes the embedding coordinates produced by the network and \\hat y denotes the ground truth embedding coordinates . We also tested our method in a siamese setting where for two inputs xi xj we used the square loss ( ||yi - yj|| - ||\\hat yi - \\hat yj|| ) ^2 . These experiments yielded similar results . We also somewhat disagree with the reviewer that our results are not surprising . It may seem natural that a neural network can embed a manifold , but we find it very surprising that this can be done so efficiently . In contrast to these results , as we discuss in our paper , Shaham et al . ( 2015 ) have recently published a related construction that is much less efficient ."}, "1": {"review_id": "BJ3filKll-1", "review_text": "Summary: In this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space. They define a class of manifolds, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a construction to embed such a chain with a neural network with one hidden layer. They also give a bound on the number of parameters required to do so, and examine what happens when the manifold is noisy. Experiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss. This experiment supports the theoretical bound on number of parameters needed to embed the monotonic chain. Another experiment varies the elevation and azimuth of of faces, which are known to lie on a monotonic chain, on a regression loss. Comments: The direction of investigation in the paper (looking at what happens to manifolds in a neural network), is very compelling, and I strongly encourage the authors to continue exploring this direction. However, the current version of the paper could use some more work: The experiments are all with a regression loss and a shallow network, and as part of the reason for interest in this question is the very large, high dimensional datasets we use now, which require a deeper network, it seems important to address this case. It also seems important to confirm that embedding works well when *classification* loss is used, instead of regression The theory sections could do with being more clearly written -- I\u2019m not as familiar with the literature in this area, and while the proof method used is relatively elementary, it was difficult to understand what exactly was being proved -- e.g. formally stating what could be expected of an embedding that \u201caccurately and efficiently\u201d preserves a monotonic chain, etc. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for these constructive comments . 'The experiments are all with a regression loss and a shallow network ' Indeed monotonic chains can efficiently be handled with a shallow network . Our construction , however , can be incorporated in deep networks in various ways , both in order to represent complex manifolds by using higher layers to combine several monotonic chains ( as we have demonstrated for the swiss roll ) or in producing hierarchical representations of the data ( such as when an m-dimensional manifold lies in an l-dimensional linear subspace of the ambient space ) . We refer the reviewer to Appendix D in the revision , which now includes further discussion of such deep architectures . 'It also seems important to confirm that embedding works well when * classification * loss is used , instead of regression ' Classification loss is generally more permissive than a regression loss . Consequently , if we apply a classification loss to an architecture that can perform embedding in the lower levels and classification in the higher levels we will generally obtain a distorted embedding , in which each linear segment can deform ( e.g.in the directions of class separation ) -- such embeddings are often sufficient to achieve accurate classification . We now demonstrate this and provide further discussion in Appendix D of our revised manuscript . 'The theory sections could do with being more clearly written ' We appreciate the reviewers suggestions for improving the presentation of the theory , and will incorporate these in a revised version of the paper ."}, "2": {"review_id": "BJ3filKll-2", "review_text": "SUMMARY This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. PROS Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. CONS The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). COMMENTS It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. MINOR COMMENTS - Figure 1 could be referenced first in the text. - \"Color coded\" where the color codes what? - Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. - On page 5, mention how the orthogonal projection on S_k is realized in the network. - On page 6 \"divided into segments\" here `segments' is maybe not the best word. - On page 6 \"The mean relative error is 0.98\" what is the baseline here, or what does this number mean? ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for these additional comments . 'It would be interesting to study the ramifications of the presented observations for the case of deep ( er ) networks ' We agree . Our construction can be incorporated in deep networks in various ways , both in order to represent complex manifolds by using higher layers to combine several monotonic chains ( as we have demonstrated for the swiss roll ) or in producing hierarchical representations of the data ( such as when an m-dimensional manifold lies in an l-dimensional linear subspace of the ambient space ) . We refer the reviewer to Appendix D in the revised version of our paper , which now includes further discussion of such deep architectures . 'to what extent the proposed picture describes the totality of functions that are representable by the networks ' Our approach suggests an efficient way to represent low-dimensional manifolds with neural nets . This can serve as part of representations of functions defined on manifolds . Our work does not suggest specific methods to represent such functions , although , as noted , Shaham et al . ( 2015 ) suggest one way to approach this problem . We appreciate the minor comments of the reviewer and have incorporated them into our revision . Regarding the minor comment 'On page 5 , mention how the orthogonal projection on S_k is realized in the network ' please note that the orthogonal projection is not realized in the network . It is used at this point merely to derive a bound on the error.\u200b"}}