{"year": "2021", "forum": "SRzz6RtOdKR", "title": "Batch Inverse-Variance Weighting: Deep Heteroscedastic Regression", "decision": "Reject", "meta_review": "The manuscript presents a deep network approach for heteroscedastic regression problem. It assumes the variance of heteroscedastic noise is known as privileged information and suggests to reweight the samples by their noise variance in the loss.\n\nThree reviewers agreed that the manuscript is not ready for publication. The major issue is the lack of novelty. Heteroscedastic regression is a classic problem in statistics. And reweighting using the inverse variance is a textbook method. \n\nR2 and R4 confirmed that they have read author response. The rebuttals are useful to clarify some points, especially related to experimental settings and results. However, they are not convinced by the authors' argument on novelty and whether the assumption is realistic. \n", "reviews": [{"review_id": "SRzz6RtOdKR-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Paper Summary : This work targets regression tasks with noisy labels , and proposes to incorporate knowledge about the variance of the gaussian noise corrupting the observed labels to weight the loss function , at training time . The proposed method is evaluated in a series of experiments involving deep networks trained according to the weighted loss function , and compared to a baseline method that omits training samples that have a label noise variance larger than a threshold . Results indicate the proposed method is more robust to noisy labels when compared to alternatives that do not exploit the information on the noise affecting labels . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : On the one hand , the paper is extremely well written and somehow pedagogic , in that it provides compelling motivations for considering heteroscedasticity , and its possible sources , and general , intuitive descriptions of the proposed method before specialising them to the instance they evaluate . On the other hand , I think the prose lacks sufficient technical depth , on the model they use , its relation to \u201c text book \u201d material on heteroscedasticity , e.g.for Maximum Likelihood Estimation ( MLE ) , and on the properties of the proposed method . The experimental evaluation , while representing a reasonable starting point , is not sufficient to fully understand the behaviour and the properties of the proposed method . For these reasons , I think this work can not be accepted as is . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Positive points : 1 ) The editorial quality of this paper is high , and the overall motivations given to support the problem statement are compelling and well discussed . This also relates to the fundamental assumption underlying this work : access to privileged information , taking the form of knowledge of the stochastic noise variance affecting observed labels , at training time . 2 ) The proposed method appears to be well positioned w.r.t.the recent literature on statistical modelling with noisy labels , especially concerning neural network based methods . It is unfortunate though that the literature scan doesn \u2019 t cover well-known approaches to tackle heteroscedastic noise in simple linear models , or in general MLE frameworks , which may be considered text-book material . 3 ) The experimental evaluation considers two regression tasks on their respective UCI/Bike sharing , and UTK Face datasets , considering several variants of noise generation processes , affecting labels in different and sufficiently realistic manner . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Negative points : 1 ) My main concern is the \u201c thin \u201d contribution of this paper . The technical details of the proposed method are not sufficiently developed . Drawing inspiration from Fisher information calls for an appropriate discussion on the likelihood model , its noise model , to begin with . Then , I think the relation of the proposed idea to simple linear models , for which heteroscedastic regression has been studied in great detail ( e.g . [ 1 ] , for a general reference ) , and for MLE ( e.g. , [ 2 ] ) , would become more clear and would give the opportunity for the authors to develop what are the merits of their proposed method . For example , the weighted least square method is very similar to what is proposed in this paper . [ 1 ] Econometric analysis , Greene , William H , 2003 , Pearson Education India [ 2 ] Maximum Likelihood Estimators with Heteroscedastic Errors , G. R. Fisher , Review of the International Statistical Institute Vol . 25 , 1957 See also \u201c Pattern Recognition and Machine Learning \u201d , Bishop , 2006 , ( chapter 5 and 6 ) 2 ) The experimental evaluation is not sufficient to appreciate the virtues of the proposed method . For several noise distributions ( including the additional ones considered in the supplement ) , the proposed method BIV does not seem to behave much better than the proposed baseline , that uses a simple threshold . Additionally , the figures are cropped and do not allow to get a sense of what happens for all competing methods ( Fig1 and Fig2 ) . In Fig3 , the figures report test loss and as there seems to be overfitting kicking in . Also , it is mentioned the baseline method requires to set a cutoff parameter , but the proposed method also depends on a hyper parameter to optimise ( done in the appendix ) . As a consequence , it is difficult to appreciate the main advantage of BIV w.r.t.the baseline . Finally , in Fig.3 , there are clear signs of overfitting : why did the authors suggest ( end of Sec.3 ) that their work does not require regularisation ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Main criticism : I think , overall , the main criticism I have for this work is that the contribution is not sufficient . The main idea proposed in the paper fits sec 4.2 , and it is based on well known results from text books . In eq . ( 5 ) , the summation term is MLE with heteroscedasticity . The loss is scaled by a coefficient that collects statistics on the sample batch : if this batch is very small , or its elements not sufficiently diverse , I am afraid it could have a negative impact on the optimisation process ( this is why , in the experiments , the authors chose a batch size of 256 ) . One possible way to overcome this criticism is to clarify the likelihood model , and compare the proposed method to existing approaches to address heteroscedastic Gaussian noise . A possible advice would be to reduce ( or move to the appendix ) the discursive parts on heteroscedasticity , and the general formulations ( e.g. , sec.2.2 , sec.4.1 ) , and gain more space to explain how BIV is different from what is known . Additional comments : A note on experiments using the UTKFace dataset . In this case , the MLP used with 4 layers may be a bit \u201c too simple \u201d , in light of the high test loss on GT labels . Did you try with convolutional architectures , even simple ones ? Just to clarify , the test loss reported in the figures , as a function of training steps : what is the * test * batch size ? Same as training batch size ? The test loss is computer according to \\sum_ { k \\in \\text { test batch } } \\mathcal { L } ( f ( \\mathbf { x } _k , \\theta ) , \\tilde { y } _k ) right ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their in-depth analysis of our work and their constructive remarks . As some elements came several times in the review , we took the liberty to summarize them in the two first paragraphs . * * There is a need for clarification and discussion on the noise likelihood model . * * We thank the reviewer for this remark . We indeed assume that the noise likelihood is Gaussian , and this asks for justification . We added some explanations in this respect at the beginning of section 2.1. , before equation ( 1 ) : Noise on measured or estimated values is often represented by Gaussian distribution , based on the central limit theorem as most noisy processes are the sum of several independent variables . Gaussian distributions are also practical , although it has some drawbacks as it can only represent uni-modal and symmetric noise models ( Thrun et al. , 2006 ) . * * There is a need for a thorough comparison to existing approaches to address heteroscedastic Gaussian noise ( for simple linear models , weighted least squares with inverse variance weighting is the MLE solution ) * * Indeed , re-weighting using the inverse variance is a common strategy in regression , as it is the solution to MLE for Gaussian heteroscedastic noise . We hope to have answered this concern in the response to all reviewers posted above . We thank the reviewer for the references they provided , and have added some in the article . * * For several noise distributions ( including the additional ones considered in the supplement ) , the proposed method BIV does not seem to behave much better than the proposed baseline , that uses a simple threshold . * * The performance of BIV with respect to the baseline ( \u201c cutoff \u201d ) is strongly dependent on the variance distribution $ P ( \\sigma^2 ) $ . When the \u201c cutoff \u201d can get a lot of low-noise data , such as the gamma distribution with alpha = .25 ( fig 8 and 9 ) , then the performances are very similar , and a lot better than L2 . This is to be expected , as it resembles the binary uniform situation leading to Fig 2 . When on the other side there is not a lot of support for low variances , BIV is doing similar to L2 ( fig 10 with $ \\alpha > 1 $ ) and so would \u201c cutoff \u201d ( or worse , as its dataset would be very few data ) . However , in other cases when the distribution is uniform ( fig.7 ) or somehow balanced , as for gamma with $ \\alpha = 1 $ ( fig 3 ) , BIV is clearly better than all cutoff and L2 . What is important here is the consistency with which BIV is always at least better than the best of L2 and cutoffs , regardless of $ P ( \\sigma^2 ) $ . We have underlined this in section 6.3 . * * Also , it is mentioned the baseline method requires to set a cutoff parameter , but the proposed method also depends on a hyper parameter to optimise ( done in the appendix ) . As a consequence , it is difficult to appreciate the main advantage of BIV w.r.t.the baseline . * * The optimal value for $ \\epsilon $ depends on the distribution of labels in the dataset . If the latter is normalized ( unit variance ) , then the optimal epsilon is between 0.01 and 0.1 , regardless of $ P ( \\sigma^2 ) $ . As $ \\epsilon $ is a \u201c minimal variance \u201d , it is straightforward to scale to any other label distribution . This is very different for the cutoff parameter , which would need to be optimized for every dataset as it is dependent on $ P ( \\sigma^2 ) $ , as shown in our experiments . We agree that this advantage was not well explained , and we have clarified it in section 6.3 . * * Additionally , the figures are cropped and do not allow to get a sense of what happens for all competing methods ( Fig1 and Fig2 ) . * * The competing methods that are cropped in Fig 1 and Fig 2 are the ones using the L2 loss , and they are actually overfitting because of the noise . We have taken advantage of having a bit more space to crop these figures a bit less , so that the viewer can have a better sense of what is happening ."}, {"review_id": "SRzz6RtOdKR-1", "review_text": "In this paper , a reweighting technique is proposed to suppress the impact of heteroscedastic label noise in regression model training . The objective function of the regression model training process is composed of a weighted combination of instance-wise training loss . The instance-wise weight is determined by the estimated noise variance based on prior information of the label generation process . The weighting formulation is inspired by the best possible estimator of noisy measurements reaching the Cramer-Rao bound . The paper is clearly written . It explains well the problem definition and the methodological formulation . However , we think the innovation in this work is limited . The downside of this paper is as follows : 1 . It is a strong and usually impractical assumption to know a priori knowledge of label noise in the regression model training process . Especially , in the proposed method , the estimate of the noise variance needs to be accurate enough , so as to help suppress the noise impact accordingly . It is better to incorporate jointly the learning of noise distribution and the regression/classification model , so as to optimize the tolerance against the data-dependent noise . 2.It only considers the noisy learning process for regression . However , in classification scenarios , noise ( with respect to labels ) is usually presented in the form of label flipping . The proposed reweighing technique is not directly applicable in that case . Please refer to the following work for further reading : Learning with Noisy Labels , Nagarajan Natarajan , Inderjit S. Dhillon , Pradeep Ravikumar , and Ambuj Tewari , NIPS 2013 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their constructive review . * * It is a strong and usually impractical assumption to know a priori knowledge of label noise in the regression model training process . * * We hope to have answered this concern in the response to all reviewers posted above . * * Especially , in the proposed method , the estimate of the noise variance needs to be accurate enough , so as to help suppress the noise impact accordingly . * * Indeed , further studies on the robustness of BIV to noise in the noise variance estimation should be made to ensure that this method is applicable in real world cases . We are planning to add new experimental results in this respect in the following days . We thank the reviewer for this suggestion ! * * It is better to incorporate jointly the learning of noise distribution and the regression/classification model , so as to optimize the tolerance against the data-dependent noise . * * This strategy is indeed used in many frameworks , such as Kendall et al. , 2017 . The main problem , as correctly underlined by the reviewer , is that it assumes that the noise is data-dependent , which disregards the noise due to the label generator . A mixture of both methods could be interesting however when both kinds of noise are present . We thank the reviewer for their comment , as it will fuel our future reflection on how to improve this work . * * Please refer to the following work for further reading : Learning with Noisy Labels , Nagarajan Natarajan , Inderjit S. Dhillon , Pradeep Ravikumar , and Ambuj Tewari , NIPS 2013 . * * We have added the reference , thank you very much ! It is actually very interesting , as this work assumes the knowledge of the probability \\rho of mislabeling , which is very close to our assumption of knowing the variance . As it is applied in binary classification with the same \\rho for each category , the authors are able to optimize these two hyperparameters using cross-validation , which is of course not possible in our case where each sample has its own noise distribution . * * It only considers the noisy learning process for regression . However , in classification scenarios , noise ( with respect to labels ) is usually presented in the form of label flipping . The proposed reweighing technique is not directly applicable in that case . * * There is a lot of work in the case of classification , including the reference the reviewer provided , which takes into account the probability of label flipping , and which are not applicable to regression . In our case , we try to solve the problem in regression , where a lot less work has been made in the field of learning with noisy labels ."}, {"review_id": "SRzz6RtOdKR-2", "review_text": "The paper propose to address the heteroscedastic regression problem using deep neural networks . It assumes the variance of heteroscedastic noise is known as privileged information and suggests to reweight the samples by their noise variance in the loss . The major issue to me is the lack of novelty . Heteroscedastic regression is a classic problem in statistics . And reweighting using the inverse variance is a textbook method . See Chapter 10 of http : //www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf This paper failed to cite any relevant reference and clarify the novelty . To apply the method to a deep learning setting , some interesting problem can be how to estimate the variance with deep network in a reliable way ( this was done previously using classic models ) . However , this paper did not tackle this harder ( and more interesting ) problem . Instead , it assume the variance is simply given during training . This is not very realistic in real world setting . The experiments are all synthetic and are not particularly convincing . Finally , the paper claims a lot of connection with privileged information ( LUPI ) . But I found it hard to consider this variance a similar concept as privileged information , which is realistic and interesting .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for their constructive review . * * Heteroscedastic regression is a classic problem in statistics . And reweighting using the inverse variance is a textbook method . See Chapter 10 of http : //www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf * * We hope to have answered this concern in the response to all reviewers posted above . We thank the reviewer for the reference , which we have added in the article . * * Instead , it assumes the variance is simply given during training . This is not very realistic in real world setting . * * We also hope to have answered this concern in the response to all reviewers posted above . * * The experiments are all synthetic and are not particularly convincing . * * Given the fact that no regression dataset currently makes the uncertainty available , we were not able to make non-synthetic experiments . However , we do think that our results show convincingly that the method does perform significantly better than L2 and consistently better than a baseline . We are conscious that more detailed experiments , such as on the robustness to errors in the variance estimation , can be made , and we plan to add them to the paper in the following days . * * Finally , the paper claims a lot of connection with privileged information ( LUPI ) . But I found it hard to consider this variance a similar concept as privileged information , which is realistic and interesting . * * On further review based on this comment , we agree that the concepts of LUPI and variance heteroscedastic regressions are different , although interesting parallels can be made at the problem definition and objective levels . We have removed the direct references to LUPI both in the title and in several places in the text , and confined the comparison to section 2.2 . We thank the reviewer for underlining that the connection claim was too strong . * * To apply the method to a deep learning setting , some interesting problem can be how to estimate the variance with deep network in a reliable way ( this was done previously using classic models ) . However , this paper did not tackle this harder ( and more interesting ) problem . * * Indeed , the problem of reliably estimating the variance with deep networks is extremely interesting and has seen a lot of recent attention ( Kendall & Gal , 2017 ; Gal & Ghahramani , 2016 ; Peretroukhin , 2019 ) . The method we propose has a completely different objective however , which is to improve learning when confronted with noisy labels . We believe that the combination of both problems will be beneficial in complex neural architectures where the output of neural networks is a noisy label used for training another network ."}], "0": {"review_id": "SRzz6RtOdKR-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Paper Summary : This work targets regression tasks with noisy labels , and proposes to incorporate knowledge about the variance of the gaussian noise corrupting the observed labels to weight the loss function , at training time . The proposed method is evaluated in a series of experiments involving deep networks trained according to the weighted loss function , and compared to a baseline method that omits training samples that have a label noise variance larger than a threshold . Results indicate the proposed method is more robust to noisy labels when compared to alternatives that do not exploit the information on the noise affecting labels . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : On the one hand , the paper is extremely well written and somehow pedagogic , in that it provides compelling motivations for considering heteroscedasticity , and its possible sources , and general , intuitive descriptions of the proposed method before specialising them to the instance they evaluate . On the other hand , I think the prose lacks sufficient technical depth , on the model they use , its relation to \u201c text book \u201d material on heteroscedasticity , e.g.for Maximum Likelihood Estimation ( MLE ) , and on the properties of the proposed method . The experimental evaluation , while representing a reasonable starting point , is not sufficient to fully understand the behaviour and the properties of the proposed method . For these reasons , I think this work can not be accepted as is . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Positive points : 1 ) The editorial quality of this paper is high , and the overall motivations given to support the problem statement are compelling and well discussed . This also relates to the fundamental assumption underlying this work : access to privileged information , taking the form of knowledge of the stochastic noise variance affecting observed labels , at training time . 2 ) The proposed method appears to be well positioned w.r.t.the recent literature on statistical modelling with noisy labels , especially concerning neural network based methods . It is unfortunate though that the literature scan doesn \u2019 t cover well-known approaches to tackle heteroscedastic noise in simple linear models , or in general MLE frameworks , which may be considered text-book material . 3 ) The experimental evaluation considers two regression tasks on their respective UCI/Bike sharing , and UTK Face datasets , considering several variants of noise generation processes , affecting labels in different and sufficiently realistic manner . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Negative points : 1 ) My main concern is the \u201c thin \u201d contribution of this paper . The technical details of the proposed method are not sufficiently developed . Drawing inspiration from Fisher information calls for an appropriate discussion on the likelihood model , its noise model , to begin with . Then , I think the relation of the proposed idea to simple linear models , for which heteroscedastic regression has been studied in great detail ( e.g . [ 1 ] , for a general reference ) , and for MLE ( e.g. , [ 2 ] ) , would become more clear and would give the opportunity for the authors to develop what are the merits of their proposed method . For example , the weighted least square method is very similar to what is proposed in this paper . [ 1 ] Econometric analysis , Greene , William H , 2003 , Pearson Education India [ 2 ] Maximum Likelihood Estimators with Heteroscedastic Errors , G. R. Fisher , Review of the International Statistical Institute Vol . 25 , 1957 See also \u201c Pattern Recognition and Machine Learning \u201d , Bishop , 2006 , ( chapter 5 and 6 ) 2 ) The experimental evaluation is not sufficient to appreciate the virtues of the proposed method . For several noise distributions ( including the additional ones considered in the supplement ) , the proposed method BIV does not seem to behave much better than the proposed baseline , that uses a simple threshold . Additionally , the figures are cropped and do not allow to get a sense of what happens for all competing methods ( Fig1 and Fig2 ) . In Fig3 , the figures report test loss and as there seems to be overfitting kicking in . Also , it is mentioned the baseline method requires to set a cutoff parameter , but the proposed method also depends on a hyper parameter to optimise ( done in the appendix ) . As a consequence , it is difficult to appreciate the main advantage of BIV w.r.t.the baseline . Finally , in Fig.3 , there are clear signs of overfitting : why did the authors suggest ( end of Sec.3 ) that their work does not require regularisation ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Main criticism : I think , overall , the main criticism I have for this work is that the contribution is not sufficient . The main idea proposed in the paper fits sec 4.2 , and it is based on well known results from text books . In eq . ( 5 ) , the summation term is MLE with heteroscedasticity . The loss is scaled by a coefficient that collects statistics on the sample batch : if this batch is very small , or its elements not sufficiently diverse , I am afraid it could have a negative impact on the optimisation process ( this is why , in the experiments , the authors chose a batch size of 256 ) . One possible way to overcome this criticism is to clarify the likelihood model , and compare the proposed method to existing approaches to address heteroscedastic Gaussian noise . A possible advice would be to reduce ( or move to the appendix ) the discursive parts on heteroscedasticity , and the general formulations ( e.g. , sec.2.2 , sec.4.1 ) , and gain more space to explain how BIV is different from what is known . Additional comments : A note on experiments using the UTKFace dataset . In this case , the MLP used with 4 layers may be a bit \u201c too simple \u201d , in light of the high test loss on GT labels . Did you try with convolutional architectures , even simple ones ? Just to clarify , the test loss reported in the figures , as a function of training steps : what is the * test * batch size ? Same as training batch size ? The test loss is computer according to \\sum_ { k \\in \\text { test batch } } \\mathcal { L } ( f ( \\mathbf { x } _k , \\theta ) , \\tilde { y } _k ) right ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their in-depth analysis of our work and their constructive remarks . As some elements came several times in the review , we took the liberty to summarize them in the two first paragraphs . * * There is a need for clarification and discussion on the noise likelihood model . * * We thank the reviewer for this remark . We indeed assume that the noise likelihood is Gaussian , and this asks for justification . We added some explanations in this respect at the beginning of section 2.1. , before equation ( 1 ) : Noise on measured or estimated values is often represented by Gaussian distribution , based on the central limit theorem as most noisy processes are the sum of several independent variables . Gaussian distributions are also practical , although it has some drawbacks as it can only represent uni-modal and symmetric noise models ( Thrun et al. , 2006 ) . * * There is a need for a thorough comparison to existing approaches to address heteroscedastic Gaussian noise ( for simple linear models , weighted least squares with inverse variance weighting is the MLE solution ) * * Indeed , re-weighting using the inverse variance is a common strategy in regression , as it is the solution to MLE for Gaussian heteroscedastic noise . We hope to have answered this concern in the response to all reviewers posted above . We thank the reviewer for the references they provided , and have added some in the article . * * For several noise distributions ( including the additional ones considered in the supplement ) , the proposed method BIV does not seem to behave much better than the proposed baseline , that uses a simple threshold . * * The performance of BIV with respect to the baseline ( \u201c cutoff \u201d ) is strongly dependent on the variance distribution $ P ( \\sigma^2 ) $ . When the \u201c cutoff \u201d can get a lot of low-noise data , such as the gamma distribution with alpha = .25 ( fig 8 and 9 ) , then the performances are very similar , and a lot better than L2 . This is to be expected , as it resembles the binary uniform situation leading to Fig 2 . When on the other side there is not a lot of support for low variances , BIV is doing similar to L2 ( fig 10 with $ \\alpha > 1 $ ) and so would \u201c cutoff \u201d ( or worse , as its dataset would be very few data ) . However , in other cases when the distribution is uniform ( fig.7 ) or somehow balanced , as for gamma with $ \\alpha = 1 $ ( fig 3 ) , BIV is clearly better than all cutoff and L2 . What is important here is the consistency with which BIV is always at least better than the best of L2 and cutoffs , regardless of $ P ( \\sigma^2 ) $ . We have underlined this in section 6.3 . * * Also , it is mentioned the baseline method requires to set a cutoff parameter , but the proposed method also depends on a hyper parameter to optimise ( done in the appendix ) . As a consequence , it is difficult to appreciate the main advantage of BIV w.r.t.the baseline . * * The optimal value for $ \\epsilon $ depends on the distribution of labels in the dataset . If the latter is normalized ( unit variance ) , then the optimal epsilon is between 0.01 and 0.1 , regardless of $ P ( \\sigma^2 ) $ . As $ \\epsilon $ is a \u201c minimal variance \u201d , it is straightforward to scale to any other label distribution . This is very different for the cutoff parameter , which would need to be optimized for every dataset as it is dependent on $ P ( \\sigma^2 ) $ , as shown in our experiments . We agree that this advantage was not well explained , and we have clarified it in section 6.3 . * * Additionally , the figures are cropped and do not allow to get a sense of what happens for all competing methods ( Fig1 and Fig2 ) . * * The competing methods that are cropped in Fig 1 and Fig 2 are the ones using the L2 loss , and they are actually overfitting because of the noise . We have taken advantage of having a bit more space to crop these figures a bit less , so that the viewer can have a better sense of what is happening ."}, "1": {"review_id": "SRzz6RtOdKR-1", "review_text": "In this paper , a reweighting technique is proposed to suppress the impact of heteroscedastic label noise in regression model training . The objective function of the regression model training process is composed of a weighted combination of instance-wise training loss . The instance-wise weight is determined by the estimated noise variance based on prior information of the label generation process . The weighting formulation is inspired by the best possible estimator of noisy measurements reaching the Cramer-Rao bound . The paper is clearly written . It explains well the problem definition and the methodological formulation . However , we think the innovation in this work is limited . The downside of this paper is as follows : 1 . It is a strong and usually impractical assumption to know a priori knowledge of label noise in the regression model training process . Especially , in the proposed method , the estimate of the noise variance needs to be accurate enough , so as to help suppress the noise impact accordingly . It is better to incorporate jointly the learning of noise distribution and the regression/classification model , so as to optimize the tolerance against the data-dependent noise . 2.It only considers the noisy learning process for regression . However , in classification scenarios , noise ( with respect to labels ) is usually presented in the form of label flipping . The proposed reweighing technique is not directly applicable in that case . Please refer to the following work for further reading : Learning with Noisy Labels , Nagarajan Natarajan , Inderjit S. Dhillon , Pradeep Ravikumar , and Ambuj Tewari , NIPS 2013 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their constructive review . * * It is a strong and usually impractical assumption to know a priori knowledge of label noise in the regression model training process . * * We hope to have answered this concern in the response to all reviewers posted above . * * Especially , in the proposed method , the estimate of the noise variance needs to be accurate enough , so as to help suppress the noise impact accordingly . * * Indeed , further studies on the robustness of BIV to noise in the noise variance estimation should be made to ensure that this method is applicable in real world cases . We are planning to add new experimental results in this respect in the following days . We thank the reviewer for this suggestion ! * * It is better to incorporate jointly the learning of noise distribution and the regression/classification model , so as to optimize the tolerance against the data-dependent noise . * * This strategy is indeed used in many frameworks , such as Kendall et al. , 2017 . The main problem , as correctly underlined by the reviewer , is that it assumes that the noise is data-dependent , which disregards the noise due to the label generator . A mixture of both methods could be interesting however when both kinds of noise are present . We thank the reviewer for their comment , as it will fuel our future reflection on how to improve this work . * * Please refer to the following work for further reading : Learning with Noisy Labels , Nagarajan Natarajan , Inderjit S. Dhillon , Pradeep Ravikumar , and Ambuj Tewari , NIPS 2013 . * * We have added the reference , thank you very much ! It is actually very interesting , as this work assumes the knowledge of the probability \\rho of mislabeling , which is very close to our assumption of knowing the variance . As it is applied in binary classification with the same \\rho for each category , the authors are able to optimize these two hyperparameters using cross-validation , which is of course not possible in our case where each sample has its own noise distribution . * * It only considers the noisy learning process for regression . However , in classification scenarios , noise ( with respect to labels ) is usually presented in the form of label flipping . The proposed reweighing technique is not directly applicable in that case . * * There is a lot of work in the case of classification , including the reference the reviewer provided , which takes into account the probability of label flipping , and which are not applicable to regression . In our case , we try to solve the problem in regression , where a lot less work has been made in the field of learning with noisy labels ."}, "2": {"review_id": "SRzz6RtOdKR-2", "review_text": "The paper propose to address the heteroscedastic regression problem using deep neural networks . It assumes the variance of heteroscedastic noise is known as privileged information and suggests to reweight the samples by their noise variance in the loss . The major issue to me is the lack of novelty . Heteroscedastic regression is a classic problem in statistics . And reweighting using the inverse variance is a textbook method . See Chapter 10 of http : //www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf This paper failed to cite any relevant reference and clarify the novelty . To apply the method to a deep learning setting , some interesting problem can be how to estimate the variance with deep network in a reliable way ( this was done previously using classic models ) . However , this paper did not tackle this harder ( and more interesting ) problem . Instead , it assume the variance is simply given during training . This is not very realistic in real world setting . The experiments are all synthetic and are not particularly convincing . Finally , the paper claims a lot of connection with privileged information ( LUPI ) . But I found it hard to consider this variance a similar concept as privileged information , which is realistic and interesting .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for their constructive review . * * Heteroscedastic regression is a classic problem in statistics . And reweighting using the inverse variance is a textbook method . See Chapter 10 of http : //www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf * * We hope to have answered this concern in the response to all reviewers posted above . We thank the reviewer for the reference , which we have added in the article . * * Instead , it assumes the variance is simply given during training . This is not very realistic in real world setting . * * We also hope to have answered this concern in the response to all reviewers posted above . * * The experiments are all synthetic and are not particularly convincing . * * Given the fact that no regression dataset currently makes the uncertainty available , we were not able to make non-synthetic experiments . However , we do think that our results show convincingly that the method does perform significantly better than L2 and consistently better than a baseline . We are conscious that more detailed experiments , such as on the robustness to errors in the variance estimation , can be made , and we plan to add them to the paper in the following days . * * Finally , the paper claims a lot of connection with privileged information ( LUPI ) . But I found it hard to consider this variance a similar concept as privileged information , which is realistic and interesting . * * On further review based on this comment , we agree that the concepts of LUPI and variance heteroscedastic regressions are different , although interesting parallels can be made at the problem definition and objective levels . We have removed the direct references to LUPI both in the title and in several places in the text , and confined the comparison to section 2.2 . We thank the reviewer for underlining that the connection claim was too strong . * * To apply the method to a deep learning setting , some interesting problem can be how to estimate the variance with deep network in a reliable way ( this was done previously using classic models ) . However , this paper did not tackle this harder ( and more interesting ) problem . * * Indeed , the problem of reliably estimating the variance with deep networks is extremely interesting and has seen a lot of recent attention ( Kendall & Gal , 2017 ; Gal & Ghahramani , 2016 ; Peretroukhin , 2019 ) . The method we propose has a completely different objective however , which is to improve learning when confronted with noisy labels . We believe that the combination of both problems will be beneficial in complex neural architectures where the output of neural networks is a noisy label used for training another network ."}}