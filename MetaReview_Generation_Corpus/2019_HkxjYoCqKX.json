{"year": "2019", "forum": "HkxjYoCqKX", "title": "Relaxed Quantization for Discretized Neural Networks", "decision": "Accept (Poster)", "meta_review": "This paper proposes an effective method to train neural networks with quantized reduced precision. It's fairly straight-forward idea and achieved good results and solid empirical work. reviewers have a consensus on acceptance. ", "reviews": [{"review_id": "HkxjYoCqKX-0", "review_text": "The authors proposes a unified and general way of training neural network with reduced precision quantized synaptic weights and activations. The use case where such a quantization can be of use is the deployment of neural network models on resource constrained devices, such as mobile phones and embedded devices. The paper is very well organized and systematically illustrates and motivates the ingredients that allows the authors to achieve their goal: a quantization grid with learnable position and range, stochastic quantization due to noise, and relaxing the hard categorical quantization assignment to a concrete distribution. The authors then validate their method on several architectures (LeNet-5, VGG7, Resnet and mobilnet) on several datasets (MNIST, CIFAR10 and ImageNet) demonstrating competitive results both in terms of precision reduction and accuracy. Minor comments: - It would be interesting to know whether training with the proposed relaxed quantization method is slower than with full-precision activations and weights. It would have been informative to show learning curves comparing learning speed in the two cases. - It seems that this work could be generalized in a relatively straight-forward way to a case in which the quantization grid is not uniform, but instead all quantization interval are being optimized independently. It would have been interesting if the authors discussed this scenario, or at least motivated why they only considered quantization on a regular grid. ", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 3 , Thank you for your review and comments for approval . Addressing the first point of training speed : training a neural network with the proposed method indeed imposes an additional burden in computing and sampling the categorical probabilities over the local grid for every weight and activation in the network . As such , this method introduces an overhead which is not present in methods that rely on deterministic rounding and the straight-through estimator for gradients . As for convergence speed , we will include an exemplary learning curve for the 32/32 , 8/8 and 2/2 bit VGG in the appendix . Addressing your second point about non-uniform grids : as you have stated , this method can be easily extended to non-uniform grids . Doing so would only require evaluating the CDF of the continuous signal at different points on the real line . We have mentioned this possibility of non-uniform grids in the conclusion to our work . The reason for why we consider uniform grids only lies in that non-uniform grids , although more powerful , generally do not allow for a straightforward implementation in today \u2019 s low-bit hardware . We mention that we explicitly focus on uniform grids for this specific reason of hardware suitability ."}, {"review_id": "HkxjYoCqKX-1", "review_text": "Quality: The work is well done. Experiments cover a range of problems and a range of quantization resolutions. Related work section in, particular, I thought was very nicely done. Empirical results are strong. In section 2.2, it bothers me that the amount of bias introduced by using the local grid approximation is never really assessed. How much probability mass is left out by truncating the Gumbel-softmax, in practice? Clarity: Well presented. I believe I'd be able to implement this, as a practitioner. Originality: Nice to see the concrete approximation having an impact in the quantization space. Significance: Quantization has obvious practical interest. The regularization aspect is striking (quantization yielded slightly improved test error on CIFAR-10; is that w/in the error bars?). A recent work [https://arxiv.org/abs/1804.05862] links model compressibility to generalization; while this work is more focused on activations, there is no reason that it couldn't be used for weights as well. Nits: top of pg 6 'reduced execution speeds' -> times, or increased exec speeds 'sparcity' misspelled", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 1 , Thank you for your review and comments for approval . Regarding the bias of the local grid approximation ; we mentioned in the main text that the local grid is constructed such that points that are within \\delta standard deviations from the mean are always part of it . For all of our experiments , we set \\delta = 3 , which means that , roughly , only 2 % of the probability mass of the logistic distribution is truncated . Unfortunately , due to lack of space we moved these experimental details about hyperparameters in the appendix . Regarding the regularization aspect ; indeed we observed that for VGG , quantizing to 8/8 bits resulted in consistent improved test errors . We are definitely aware of [ https : //arxiv.org/abs/1804.05862 ] and believe that further research in this direction is a fruitful direction ."}, {"review_id": "HkxjYoCqKX-2", "review_text": "Summary ======= This paper introduces a method for learning neural networks with quantized weights and activations. The main idea is to stochastically \u2013 rather than deterministically \u2013 quantize values, and to replace the resulting categorical distribution over quantized values with a continuous relaxation (the \"concrete distribution\" or \"Gumbel-Softax distribution\"; Maddison et al., 2016; Jang et al., 2016). Good empirical performance is demonstrated for LeNet-5 applied to MNIST, VGG applied to CIFAR-10, and MobileNet and ResNet-18 applied to ImageNet. Review ====== Relevance: Training non-differentiable neural networks is a challenging and important problem for several applications and a frequent topic at ICLR. Novelty: Conceptually, the proposed approach seems like a straight-forward application/extension of existing methods, but I'm unaware of any paper which uses the concrete distribution for the express purpose of improved efficiency as in this paper. There is a thorough discussion of related work, although I was missing Williams (1992), who used stochastic rounding before Gupta et al. (2015), and Soudry et al. (2014), who introduced a Bayesian approach to deal with discrete weights and activations. Results: The empirical work is thorough, achieving state-of-the-art results in several classification benchmarks. It would be interesting to see how well these methods perform in other tasks (e.g., compression or even regression), even though the literature on quantization seems to focus on classification. Clarity: The paper is well written and clear.", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 2 , Thank you for your review and comments for approval . We will make sure to update the related work section with the work of Soudry et al . ( 2014 ) .As for Williams ( 1992 ) ; to our understanding the focus of that paper was to introduce the unbiased score function estimator REINFORCE for the gradient of an expectation of a non-differentiable function . In this sense , Williams ( 1992 ) is more of a related work to the concrete / Gumbel-softmax approaches , rather than the stochastic rounding of Gupta et al . ( 2015 ) .We will update the submission to include a brief discussion between the REINFORCE and concrete / Gumbel-softmax as choices for the fourth element of RQ . Regarding experiments on different tasks ; we agree that it would be interesting to check performance on tasks that require more \u201c precision \u201d , such as regression . We chose classification for this submission , as this provides a large amount of literature to compare against , and leave the exploration of different tasks for future work ."}], "0": {"review_id": "HkxjYoCqKX-0", "review_text": "The authors proposes a unified and general way of training neural network with reduced precision quantized synaptic weights and activations. The use case where such a quantization can be of use is the deployment of neural network models on resource constrained devices, such as mobile phones and embedded devices. The paper is very well organized and systematically illustrates and motivates the ingredients that allows the authors to achieve their goal: a quantization grid with learnable position and range, stochastic quantization due to noise, and relaxing the hard categorical quantization assignment to a concrete distribution. The authors then validate their method on several architectures (LeNet-5, VGG7, Resnet and mobilnet) on several datasets (MNIST, CIFAR10 and ImageNet) demonstrating competitive results both in terms of precision reduction and accuracy. Minor comments: - It would be interesting to know whether training with the proposed relaxed quantization method is slower than with full-precision activations and weights. It would have been informative to show learning curves comparing learning speed in the two cases. - It seems that this work could be generalized in a relatively straight-forward way to a case in which the quantization grid is not uniform, but instead all quantization interval are being optimized independently. It would have been interesting if the authors discussed this scenario, or at least motivated why they only considered quantization on a regular grid. ", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 3 , Thank you for your review and comments for approval . Addressing the first point of training speed : training a neural network with the proposed method indeed imposes an additional burden in computing and sampling the categorical probabilities over the local grid for every weight and activation in the network . As such , this method introduces an overhead which is not present in methods that rely on deterministic rounding and the straight-through estimator for gradients . As for convergence speed , we will include an exemplary learning curve for the 32/32 , 8/8 and 2/2 bit VGG in the appendix . Addressing your second point about non-uniform grids : as you have stated , this method can be easily extended to non-uniform grids . Doing so would only require evaluating the CDF of the continuous signal at different points on the real line . We have mentioned this possibility of non-uniform grids in the conclusion to our work . The reason for why we consider uniform grids only lies in that non-uniform grids , although more powerful , generally do not allow for a straightforward implementation in today \u2019 s low-bit hardware . We mention that we explicitly focus on uniform grids for this specific reason of hardware suitability ."}, "1": {"review_id": "HkxjYoCqKX-1", "review_text": "Quality: The work is well done. Experiments cover a range of problems and a range of quantization resolutions. Related work section in, particular, I thought was very nicely done. Empirical results are strong. In section 2.2, it bothers me that the amount of bias introduced by using the local grid approximation is never really assessed. How much probability mass is left out by truncating the Gumbel-softmax, in practice? Clarity: Well presented. I believe I'd be able to implement this, as a practitioner. Originality: Nice to see the concrete approximation having an impact in the quantization space. Significance: Quantization has obvious practical interest. The regularization aspect is striking (quantization yielded slightly improved test error on CIFAR-10; is that w/in the error bars?). A recent work [https://arxiv.org/abs/1804.05862] links model compressibility to generalization; while this work is more focused on activations, there is no reason that it couldn't be used for weights as well. Nits: top of pg 6 'reduced execution speeds' -> times, or increased exec speeds 'sparcity' misspelled", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 1 , Thank you for your review and comments for approval . Regarding the bias of the local grid approximation ; we mentioned in the main text that the local grid is constructed such that points that are within \\delta standard deviations from the mean are always part of it . For all of our experiments , we set \\delta = 3 , which means that , roughly , only 2 % of the probability mass of the logistic distribution is truncated . Unfortunately , due to lack of space we moved these experimental details about hyperparameters in the appendix . Regarding the regularization aspect ; indeed we observed that for VGG , quantizing to 8/8 bits resulted in consistent improved test errors . We are definitely aware of [ https : //arxiv.org/abs/1804.05862 ] and believe that further research in this direction is a fruitful direction ."}, "2": {"review_id": "HkxjYoCqKX-2", "review_text": "Summary ======= This paper introduces a method for learning neural networks with quantized weights and activations. The main idea is to stochastically \u2013 rather than deterministically \u2013 quantize values, and to replace the resulting categorical distribution over quantized values with a continuous relaxation (the \"concrete distribution\" or \"Gumbel-Softax distribution\"; Maddison et al., 2016; Jang et al., 2016). Good empirical performance is demonstrated for LeNet-5 applied to MNIST, VGG applied to CIFAR-10, and MobileNet and ResNet-18 applied to ImageNet. Review ====== Relevance: Training non-differentiable neural networks is a challenging and important problem for several applications and a frequent topic at ICLR. Novelty: Conceptually, the proposed approach seems like a straight-forward application/extension of existing methods, but I'm unaware of any paper which uses the concrete distribution for the express purpose of improved efficiency as in this paper. There is a thorough discussion of related work, although I was missing Williams (1992), who used stochastic rounding before Gupta et al. (2015), and Soudry et al. (2014), who introduced a Bayesian approach to deal with discrete weights and activations. Results: The empirical work is thorough, achieving state-of-the-art results in several classification benchmarks. It would be interesting to see how well these methods perform in other tasks (e.g., compression or even regression), even though the literature on quantization seems to focus on classification. Clarity: The paper is well written and clear.", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 2 , Thank you for your review and comments for approval . We will make sure to update the related work section with the work of Soudry et al . ( 2014 ) .As for Williams ( 1992 ) ; to our understanding the focus of that paper was to introduce the unbiased score function estimator REINFORCE for the gradient of an expectation of a non-differentiable function . In this sense , Williams ( 1992 ) is more of a related work to the concrete / Gumbel-softmax approaches , rather than the stochastic rounding of Gupta et al . ( 2015 ) .We will update the submission to include a brief discussion between the REINFORCE and concrete / Gumbel-softmax as choices for the fourth element of RQ . Regarding experiments on different tasks ; we agree that it would be interesting to check performance on tasks that require more \u201c precision \u201d , such as regression . We chose classification for this submission , as this provides a large amount of literature to compare against , and leave the exploration of different tasks for future work ."}}