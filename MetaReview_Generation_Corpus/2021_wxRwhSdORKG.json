{"year": "2021", "forum": "wxRwhSdORKG", "title": "Learning Subgoal Representations with Slow Dynamics", "decision": "Accept (Poster)", "meta_review": "This paper introduces an HRL method that uses slow features to define subgoals (or abstract states), which can then be used by goal-conditioned policies. It is said that such an approach allows for efficient exploration. Most reviewers are recommending the acceptance of this paper, they found the method interesting and they think it introduces interesting ideas that are not that common to the HRL literature. Thus, I\u2019m recommending the acceptance of this paper.\n\nI\u2019d still encourage the authors to take the reviewers comments into consideration when preparing the final version of the paper. Specifically, it would be useful to explicitly discuss the \u201cchicken and egg problem\u201d and the fact that the agent has access to a function defining the distance to the goal before the goal was observed for the first time. Some baselines have the same assumption, but it is somewhat weird to discuss exploration in this setting without further clarifications.\n\n", "reviews": [{"review_id": "wxRwhSdORKG-0", "review_text": "* * Summary : * * This paper proposes a new method for learning subgoal representations in HRL . The method learns a representation that emphasises features that change slowly , through a \u201c slowness objective \u201d . The slowness objective minimises changes in the subgoal representation between low level time steps , while maximising feature changes between the high-level temporal intervals . This objective allows for efficient exploration , which the paper justifies theoretically , and supports with some empirical experiments on challenging control domains . * * Strengths : * * The issue of subgoal selection is a critical issue for HRL , and constructing or learning a good subgoal representation on which to create subgoals is important and interrelated . Thus , any significant progress in this area is useful . The paper presents an apparently novel method that would be of interest to HRL researchers and deep RL practitioners more generally . The paper is clearly written and the main ideas are generally very well explained ( except for maybe the use of the term \u201c state \u201d ; see weaknesses ) . The paper provides some theoretical justification for the slowness objective , which is useful to support the intuition behind it . The empirical results are conducted on challenging domains and they appear strong . However , including more independent runs in each domain would strengthen the conclusions significantly ( see weaknesses ) . * * Weaknesses : * * In sections 3.1 and 4.1 the paper uses the term `` state '' when describing the algorithm and other definitions . It is not clear if it is actually referring to a proper state in the Markov sense , or some approximation of a state , or even just an observation that might not be Markov at all . This is confusing , and could be clarified . How does the slowness objective interact with approximation when you do not have access to the true state ( which would be most of the time ) ? Does the theoretical result still hold ? I think 10 runs for the NChain environment , and 5 run for Mujoco are too few . More runs would give a much better sense of the variability of the runs , greatly strengthen the conclusions about the relative performance between the algorithms . 20 runs for the NChain environment , and 10 runs for Mujoco would be a significant improvement , but even more is better . Also , in the low run regime ( ~10 runs or less ) sometimes it can be more informative just to plot each of the learning curves for all the runs on the same plot , along with the mean . This gives a really good sense of the variability between runs . Presenting the results this way assumes less about the distribution that the performance samples are coming from . * * Recommendation : * * Overall , I recommend to accept this paper . A clarification about the use of the term \u201c state \u201d and adding more runs to the experiments would increase my score . * * Questions : * * Clarify the use of the term `` state '' ( see weaknesses ) . * * After Author Response and Discussion : * * Thanks to the authors for their responses . After reading the other reviews and the author responses , I am raising my score to 7 ( accept ) .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful comments . We appreciate it if you have any further questions or comments . - Q1.The use of the term \u201c state \u201d . - In the method section ( Section 3 ) , the term \u201c state \u201d can be either be a proper state in the Markov sense or an observation . Our method can generally work in both cases . In our experiments , we evaluated out method with inputs of vector states and image observations , respectively , as shown in Figure 3 . Our method significantly outperforms baselines in both settings . - In the theoretical analysis ( Section 4 ) , the term \u201c state \u201d refers to a proper state in the Markov sense . We have added a clarification in the paper . Extending the theoretical results to a more general setting will be an interesting future direction . - Q2.More runs for each task . We have increased to 20 runs for the NChain environment and 10 runs for the Mujoco tasks . The relative performance between algorithms almost remains the same . Please see the revised version of our paper , and thanks for your suggestion . We have also added the oracle learning curves in Figure 3 , as suggested by Reviewer 4 ."}, {"review_id": "wxRwhSdORKG-1", "review_text": "The main idea of this paper is very nice : that we want the features in the representation space for subgoals in HRL to be `` slow '' in the sense that they do n't change much over primitive steps of the policy , but do change significantly over `` macro '' steps . The margin-based criterion in the loss ( that says we want them to change by at least m at the high level ) seems well justified . It seems like , additionally , one might want to incentivize this representation to cover the state space s well ( that is , not map big chunks of s onto the same latent representation phi ) and I did n't exactly see how this would be encouragedI guess the drive for significant change at the high level accomplishes this implicitly . Using this loss function leads to a sensible algorithm , although I was curious about why the representation learning happens at the same rate as the low-level policy learning ; I could imagine that you might want to do that at a slower time scale ( more like the time scale for the high-level policy learning ) . The experimental results seem to make a compelling case for the effectiveness of this algorithm . The results for transfer learning are particularly strongin fact transfer seems to be one of the best motivations for learning hierarchical representations , which might be harder to get to pay off in the case of learning to control a single problem instance . Some of the explanation in this section did n't completely make sense to me : - Why do you attribute the success of your method to improved exploration ? It seems like another very plausible explanation is that you are providing a good inductive bias for learning a policy that generalizes well ( though 2 million is a lot of steps ! ) - ( Related ) What is the purpose of the experiments on state coverage ? I do n't completely see why that should be , in itself , a goal . In fact , one attribute of good RL methods that generalize well is that they do early exploration but then focus on parts of the state space that are profitable given the particular high-level reward function they are optimizing . All this being good , the theoretical part of the paper was quite weak . The definitions and theorem were not rigorous and possibly incorrect . First , from the expositional perspective , it would have helped to begin with crisp definitions and then the assumptions . Here are many points that came up for me as I read through the text : - What is the role of the target distribution ? Where would you get it ? - In the application of KL divergence It seems more like you 'd want the p to be the target distribution . - What is a `` fact '' ? Break it down into definitions and then a crisp formal assertion with a proof ( if there 's something in there beyond definitions ) . - In Fact 1 the term `` exploration process '' is not defined . - Importantly , I do n't think this is well-formed unless we assume that there is no learning happening at the low level . Are we assuming that ? ( If not , then the process is non-stationary ) . - The high-level policy is assumed to select subgoals `` randomly '' : Does this mean uniformly at random from S ? Is S bounded ? - What does it mean for the agent to be able to `` move independently and identically in the state space '' ? - What the X^c_t are like depends completely on the low-level policy ! It could always move left , in which case these variables would not be anything like IID . Is this analysis supposed to be assuming that the low level is perfectly able to achieve the targets ? Or at least always successfully move c steps in the target `` direction '' ( not sure what that means , exactly , though ) . - What , precisely , do you mean by `` the features are all independent '' ? - Brushing assumption ( a ) off as a general technique to simplify analysis is not so good . It 's okay to say you 're making it for now , but there 's no reason to think results obtained with this assumption will generalize ! - Assumption d would require changes to the algorithm , no ? - In Theorem 1 : `` r is large enough '' for what ? - I do n't really understand what it means to `` assume q is an isotropic Gaussian ... '' ; that is , what exactly is q intended to be . I see that later you say that if r is big enough it approximates a uniform , and so maybe what you 're trying to show is that our exploration is uniform . But a uniform needs to be over a bounded space . And even with large r , q is only sort of uniform in a region bounded around the mean . - What makes `` optimal hierarchical exploration '' ? I guess maybe this means : with a fixed set of features and low-level control policy we 're deciding which k features to use for subgoals , and showing that among those choices we get the best exploration ( in the sense of matching a Gaussian around the origin ) if we use the k slowest features . ( This is just a related thought , not really a review of this paper.One other principle for selecting features for a subgoal representation is that they should be , in some sense , locally achievable . Hierarchy works well when the subproblems are , in a sense , `` serializable '' : that we can achieve subgoal the first any way we want to , without thinking about ( and/or making it harder to achieve ) the next subgoal we 're going to be asked to do . One way to do this might be to encourage some `` disentanglement '' in the latent features , so that the policy for changing one dimension of the latent space tends not to change the other dimensions . ) In the end , I 'm positively inclined toward this paper because it made me think about the concepts of what we really want in a hierarchical representation , but the mathematical exposition really needs substantial cleaning up . But I 'd rather have an interesting paper than a perfectly-executed boring one .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the inspiring comments . We appreciate it if you have any further questions or comments . - Q1.Why not learn the representation function at a slower time scale ? Our method updates the representation $ \\phi ( s ) $ per timestep with only one minibatch ( 100 samples ) , which is quite slow and steady . We compared this updating method with learning the representation at a slower time scale , e.g. , updating $ \\phi ( s ) $ every $ c $ timesteps with one minibatch data and updating $ \\phi ( s ) $ every $ c $ timesteps with $ c $ minibatches . Those methods made no significant difference . The experimental results are shown in Appendix E. - Q2 . Why attribute the success of the method to improved exploration ? - The exploration strengths of our method are two folds . First , during the representation learning process , the generalizable representation can help the exploration of the hierarchical policy , and the samples collected by the hierarchical policy facilitate the representation learning , as shown in the newly added section ( Section 6.4 ) . - Second , when the learning of the low-level policy and the representation converges , our method can achieve a larger coverage area , since the high-level policy explores in a slow subgoal space , as analyzed in Section 4 . And the didactic example shown in Section 6.1 also supports this theoretical result . The larger state coverage can avoid the local sub-optimum caused by deceptive rewards . - Q3.The purpose of the experiments on state coverage . The state coverage is important for exploration , especially at the early learning stage . A better state coverage can help an agent avoid stuck in a local optimum , especially when the reward function can be sparse or deceptive . Empirical results in Figure 2 show that , compared to other method , our method can quickly achieve a better state coverage at the early exploration , which allows it focus on the promising parts of the state space . - Q4.The theoretical part of the paper . Thank you for the detailed comments . We have refined the theoretical part of our paper to make it more rigorous and clearer in our revision . As some of the reviewer \u2019 s questions are related to each other , we group these questions and provide corresponding clarification concisely . 1.Target distribution : The target distribution is a prior distribution that we want to approximate . We assume that the target distribution is an isotropic Gaussian distribution , and as we stated in Section 4.2 , when $ r $ is large enough , the target distribution approximates a uniform distribution . We make this assumption since there is no prior knowledge . Besides , we have exchanged the notations of the target distribution and the steady distribution , and used forward KL divergence as the measure for exploration in the revised version . 2.Random walk : The exploration process is defined as a process when the low-level policy is optimal , and the high-level policy chooses subgoals randomly since there is no extrinsic reward . That is why we consider this process as a random walk . We have revised Fact 1 to Definition 2 to make it clearer . The subgoal selection is bounded in the neighborhood of the current state . We have added a new experiment to demonstrate that our algorithm can approximately satisfy this assumption in Appendix C. 3 . Feature independence and disentanglement : We assume that the features are all independent so that the change in one feature dimension will not influence the others . Selecting slow features as the subgoal space can be seen as a disentanglement , in some sense . We encourage the agent to achieve subgoals with slow dynamics and ignore the fast-changing ones . It is an interesting future direction to relax this assumption . Thanks for your inspiration . 4.Question about Assumption ( a ) : We can not release Assumption ( a ) in our derivation currently , but we can think deeper in future work , and we really appreciate your suggestion ."}, {"review_id": "wxRwhSdORKG-2", "review_text": "This work presents a method for learning a slow-changing ( `` low-frequency '' ) embedding function , which can be used as a state-abstraction function in the context of Hierarchical RL . A high-level policy , trained to solve the environments task , acts by selecting abstracted states as targets for the low-level policy which is trained as in a goal-conditioned fashion ( acting in the environment itself attempting to get to the set goal ) . The paper provides theoretical motivation for the focus on `` slow '' features in a simplified synthetic example . The method itself is evaluated on high-dim control tasks ( As well as on an illustrative toy-example ) . There is a lot to like about this paper . The method is elegant , building on and extending previous approaches to HRL and particularly learning subgoal representations . The use of a low-frequency ( slowness ) criterion for this purpose is , to the best of my knowledge , a novel contribution . The work presents strong empirical evidence for the usefulness of the approach . The paper itself is well-structured and overall well-written , easy to follow , and with good references . I do have a few questions for which I would appreciate the authors comments : * In principle , the loss for the embedding function ( highly ) depends on the behavioral policy . This is because the embedding is trained to represent `` far away '' states as different from each other . And , generally , the distribution of $ s_t $ and $ s_ { t+c } $ can be rather different for , say , a random policy compared to a more purposeful/trained one . Since everything is trained jointly here ( embedding is used to train the policies which in turn can change the embedding and so on ) , this could in principle leads to instability of the training . I think this point deserve at least some discussion . * I find section 4 a bit confusing . `` Fact 1 '' seems more like a definition for me , because the properties of the `` random walk '' itself will be * determined * by $ p $ ( as $ p $ obviously depend on the behavioral policy.In fact from `` Fact 1 '' we can simply say that $ p $ is the steady-state/limiting/stationary distribution of the markov chain induced by the policy and the MDP ) . In general the relation between $ p $ and the policy is not entirely clear . Note that in general , the change metric $ \\Delta s^i_t $ is also policy dependent . I also do n't see the direct relation between $ p $ in this discussion , which ultimately is an expression of the ( low-level ) policy , to the embedding or representation principle used throughout the paper . * It seems the underlying assumption here is something like : the agent can independently control/change the state-features in an unconstrained way ( i.e an `` action '' is an offset vector s.t the next state is $ s_ { t+1 } =s_t+a_t $ , but in such a way that the environment somehow 'rescales ' $ a_t $ so that earlier entries are smaller ) . If this is indeed the case , I find this section not too motivating . If there is completely no structure in the environment ( state/action spaces ) and everything reduced down to a uniform random-walk , we can not learn much about even simple `` realistic '' RL settings . * Having said that I think the paper presents other good reasons and motivations for the focus on slow-features . In particular I think that the example discussed in section 6.1 is important , as well as the more qualitative explanations and the relation to the unsupervised-learning literature around similar ideas .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful and inspiring comments . We appreciate it if you have any further questions or comments . - Q1.The instability of the training . We have added a new section ( Section 6.4 ) to investigate the parallel learning of the representation function and the hierarchical policy . Figure 5 demonstrates that the learning of the representation function and the policy can promote each other . As the subgoal representation learning is conducted slowly and smoothly , our method is stable in most cases . - Q2.Question about Fact 1 . Thanks for your suggestion , and $ p ( x ) $ is indeed the steady distribution of the Markov chain , induced by a random high-level policy and an optimal low-level policy . ( By the way , we have exchanged the notations of the target distribution and the steady distribution in the revised version , and now $ q ( x ) $ stands for the steady distribution . ) - Q3.Is an \u201c action \u201d an offset vector ? No.We have made no assumption about the low-level action space , and we only consider every $ c $ -step change in the state space . Assuming each dimension of the change vector is bounded , we have taken the low-level action constraint into account but at a lower temporal resolution ( every $ c $ steps ) ."}, {"review_id": "wxRwhSdORKG-3", "review_text": "# # # Summary This paper proposes to use slow features as the subgoal representation space in the HRL setting . The proposed approach adopts a slowness objective to learn a representation ( high-level action space ) and use it to train goal-conditioned policies ( low-level ) . # # # Main contributions - Slowness is an interesting property for representation learning . This work 's approach to slowness is simple and easy to integrate ( in practice ) to the challenging setting of HRL . - This work also makes a theoretical effort ( section 4 ) to motivate the slowness property for subgoal space . - The proposed method is evaluated on continuous control tasks against other references approaches to HRL . # # # Main comments/concerns - In the definition ( def.1 ) of the measure of exploration , what justifies the choice of the reverse KL ? Is it still a good choice if q is multimodal ? Does this definition really align with your claim `` a larger coverage area leads to better exploration '' ? - The algorithm implementation is not clear : - How is the goal sampled ? Randomly in the large domain as mentioned in the Appendix or in the neighbourhood of s as mentioned in sections 3 and 4 ? - What is the extrinsic reward used to train the high level policy exactly ? - There is a chicken-and-egg problem regarding the parallel learning of the representation and its usage to train the low policies . The representation should first be trained on some area before trusting the induced training of the low policy , which requires exploring that area beforehand . It is not clear how the algorithm deals with this challenge . Moreover , the evaluation on Mujoco tasks could be misleading regarding this point ( see the following comment ) . - The method is evaluated on Mujoco environments that provide the xy-coordinates . The representation can learn to extract the coordinates by only training on a limited area ( e.g.neighbourhood of initial state ) -- being able to early stop the representation training seems to support this hypothesis . Was this phenomenon studied is your experiments ? How is it limiting the proposed method beyond this type of environments ? How does the proposed method work when this inductive bias ca n't be leveraged , and when exploring an area of the state space is required to build a useful representation of it ? - Figure 4 shows the learned representation . Was it learned from the states or the images setting ? If learned from states , how does it look like when learned from images ? And , related to the previous comment , how does it look like along the training ? - How was the oracle trained ? It would be interesting to see the oracle training curves along with the compared methods on Figure 3 . - How reliable/relevant is the slowness evaluation ( section 6.3 ) in comparing the different methods ? It seems that the slowness objective can learn arbitrarily slow features ( according to your measure ) by weighting the attraction term more than the repulsive one ( tuning m and c can also influence the slowness measure ) . - `` As the state space of the Point robot is simple , the dynamics of the randomly selected features are slow as well '' : What do you mean by simple state space ? - `` the high-level policy guides the agent to jump out of the local optimum of moving towards the goal '' : local optimum w.r.t which reward function ? - `` The transfer effect [ ... ] is more significant [ ... ] since the target task , Ant FourRooms , is of a larger size and more difficult '' : Is the transfer effect confirmed when transferring from AntPush and AntFall to the larger Ant FourRooms ? # # # Minor comments - The AntPush performance curves are limited to 2.5 and 2 million steps . How was this horizon decided for this environment ? How do these curves look like for a longer training ( 4 million steps ) ? - Equation 7 seems confusing : p ( x ) is the asymptotic distribution of Y_n , meaning that it should not depend on n as shown in equation 7 . - Why does Figure 5 ( a ) has a 4 millions step while 5 ( b ) has 8 million steps ? Clarity : The paper reads well , but the experiments and algorithm presentation is sometimes unclear if not confusing .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the detailed comments . We have refined our paper to make it more rigorous and clearer . We have also added some new experimental results to clarify the reviewer \u2019 s questions . We appreciate it if you have any further questions or comments . - Q1.The measure of exploration in def . 1.Thank you for the suggestion . Our theoretical claim also holds with the forward KL definition , as discussed in the revision . We used reverse KL to measure coverage , as we assume the target state distribution is Gaussian in our analysis , which is unimodal . We agree with the reviewer that , for a multimodal target state distribution , using forward KL to measure exploration may be more proper . We have added discussions in our revision . - Q2.Is the goal sampled in a large domain or in the neighborhood of s ? We have tested these two methods for sampling subgoals . They demonstrated similar performance , as shown in Appendix C. This is because distant subgoals are hard to reach for the low-level policy , the high-level agent will learn to selects subgoals nearby the current latent state during the learning process . For empirical results in our paper , we used the way described in the Appendix , and have corrected the description in sections 3 and 4 in the revision . We have clarified this point in Appendix B.3 . - Q3.What is the extrinsic reward ? We used a common reward setting ( e.g . [ 1,2,3 ] ) , where the extrinsic reward is given as the negative L2 distance to the environment goal . This extrinsic reward is deceptive , which can lead the learning to a local sub-optimum . The hierarchical structure can enable better exploration with larger state coverage and avoid such local optima . - Q4.The chicken-and-egg problem regarding the parallel learning of the representation and low-level policies . Empirical results show that the iterative learning of the subgoal representation and the low-level policies promotes each other . This is because a low-level policy , even not very good , will collect useful data for learning a good subgoal representation , which is updated in a slow and stable manner and will also generalize to other parts of the state space . This gradually improved representation provides dense rewards to the low level , and the trajectories collected by the learned policy are utilized to train the representation . This co-adaptation process is demonstrated in Figure 5 ( a ) ~ ( c ) in the revision . - Q5.How does the proposed method perform beyond environments providing the xy coordinates ? - Our approach also works with visual observations * * without * * coordinate information , since the proposed inductive bias ( slow dynamics ) has a good generalization ability . - The training process of the representation learned from images * * without * * coordinates are shown in Section 6.4 in the revision . When the hierarchical agent has learned how to reach the easy goal near the initial states , the trajectory embeddings in this area ( red to yellow part in Figure 5 ( a ) $ \\sim $ ( c ) ) align with the coordinates . And the learned representation can be generalized to further areas , although a little chaotic , shown by the yellow to blue trajectory part in Figure 5 ( d ) $ \\sim $ ( f ) . This generalizable subgoal representation can facilitate the hierarchical policy exploration for the distant hard goal . By the way , early stopping the representation training is to make the learning faster in terms of wall time . We have added an ablation study of early stopping in Appendix D. - Q6 . What was the representation learned from in Figure 4 ? It was learned from the states . We have added the representation learned from images along with the training in Section 6.4 . As we stated in Q4 , the representation learned from images can facilitate policy learning , and vice versa . [ 1 ] Nachum et al. \u201c Near-Optimal Representation Learning for Hierarchical Reinforcement Learning. \u201d In ICLR , 2019 . [ 2 ] Nachum et al. \u201c Data-Efficient Hierarchical Reinforcement Learning. \u201d In NeurIPS 2018 . [ 3 ] Jinnai et al. \u201c Exploration in Reinforcement Learning with Deep Covering Options. \u201d In ICLR , 2020 ."}], "0": {"review_id": "wxRwhSdORKG-0", "review_text": "* * Summary : * * This paper proposes a new method for learning subgoal representations in HRL . The method learns a representation that emphasises features that change slowly , through a \u201c slowness objective \u201d . The slowness objective minimises changes in the subgoal representation between low level time steps , while maximising feature changes between the high-level temporal intervals . This objective allows for efficient exploration , which the paper justifies theoretically , and supports with some empirical experiments on challenging control domains . * * Strengths : * * The issue of subgoal selection is a critical issue for HRL , and constructing or learning a good subgoal representation on which to create subgoals is important and interrelated . Thus , any significant progress in this area is useful . The paper presents an apparently novel method that would be of interest to HRL researchers and deep RL practitioners more generally . The paper is clearly written and the main ideas are generally very well explained ( except for maybe the use of the term \u201c state \u201d ; see weaknesses ) . The paper provides some theoretical justification for the slowness objective , which is useful to support the intuition behind it . The empirical results are conducted on challenging domains and they appear strong . However , including more independent runs in each domain would strengthen the conclusions significantly ( see weaknesses ) . * * Weaknesses : * * In sections 3.1 and 4.1 the paper uses the term `` state '' when describing the algorithm and other definitions . It is not clear if it is actually referring to a proper state in the Markov sense , or some approximation of a state , or even just an observation that might not be Markov at all . This is confusing , and could be clarified . How does the slowness objective interact with approximation when you do not have access to the true state ( which would be most of the time ) ? Does the theoretical result still hold ? I think 10 runs for the NChain environment , and 5 run for Mujoco are too few . More runs would give a much better sense of the variability of the runs , greatly strengthen the conclusions about the relative performance between the algorithms . 20 runs for the NChain environment , and 10 runs for Mujoco would be a significant improvement , but even more is better . Also , in the low run regime ( ~10 runs or less ) sometimes it can be more informative just to plot each of the learning curves for all the runs on the same plot , along with the mean . This gives a really good sense of the variability between runs . Presenting the results this way assumes less about the distribution that the performance samples are coming from . * * Recommendation : * * Overall , I recommend to accept this paper . A clarification about the use of the term \u201c state \u201d and adding more runs to the experiments would increase my score . * * Questions : * * Clarify the use of the term `` state '' ( see weaknesses ) . * * After Author Response and Discussion : * * Thanks to the authors for their responses . After reading the other reviews and the author responses , I am raising my score to 7 ( accept ) .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful comments . We appreciate it if you have any further questions or comments . - Q1.The use of the term \u201c state \u201d . - In the method section ( Section 3 ) , the term \u201c state \u201d can be either be a proper state in the Markov sense or an observation . Our method can generally work in both cases . In our experiments , we evaluated out method with inputs of vector states and image observations , respectively , as shown in Figure 3 . Our method significantly outperforms baselines in both settings . - In the theoretical analysis ( Section 4 ) , the term \u201c state \u201d refers to a proper state in the Markov sense . We have added a clarification in the paper . Extending the theoretical results to a more general setting will be an interesting future direction . - Q2.More runs for each task . We have increased to 20 runs for the NChain environment and 10 runs for the Mujoco tasks . The relative performance between algorithms almost remains the same . Please see the revised version of our paper , and thanks for your suggestion . We have also added the oracle learning curves in Figure 3 , as suggested by Reviewer 4 ."}, "1": {"review_id": "wxRwhSdORKG-1", "review_text": "The main idea of this paper is very nice : that we want the features in the representation space for subgoals in HRL to be `` slow '' in the sense that they do n't change much over primitive steps of the policy , but do change significantly over `` macro '' steps . The margin-based criterion in the loss ( that says we want them to change by at least m at the high level ) seems well justified . It seems like , additionally , one might want to incentivize this representation to cover the state space s well ( that is , not map big chunks of s onto the same latent representation phi ) and I did n't exactly see how this would be encouragedI guess the drive for significant change at the high level accomplishes this implicitly . Using this loss function leads to a sensible algorithm , although I was curious about why the representation learning happens at the same rate as the low-level policy learning ; I could imagine that you might want to do that at a slower time scale ( more like the time scale for the high-level policy learning ) . The experimental results seem to make a compelling case for the effectiveness of this algorithm . The results for transfer learning are particularly strongin fact transfer seems to be one of the best motivations for learning hierarchical representations , which might be harder to get to pay off in the case of learning to control a single problem instance . Some of the explanation in this section did n't completely make sense to me : - Why do you attribute the success of your method to improved exploration ? It seems like another very plausible explanation is that you are providing a good inductive bias for learning a policy that generalizes well ( though 2 million is a lot of steps ! ) - ( Related ) What is the purpose of the experiments on state coverage ? I do n't completely see why that should be , in itself , a goal . In fact , one attribute of good RL methods that generalize well is that they do early exploration but then focus on parts of the state space that are profitable given the particular high-level reward function they are optimizing . All this being good , the theoretical part of the paper was quite weak . The definitions and theorem were not rigorous and possibly incorrect . First , from the expositional perspective , it would have helped to begin with crisp definitions and then the assumptions . Here are many points that came up for me as I read through the text : - What is the role of the target distribution ? Where would you get it ? - In the application of KL divergence It seems more like you 'd want the p to be the target distribution . - What is a `` fact '' ? Break it down into definitions and then a crisp formal assertion with a proof ( if there 's something in there beyond definitions ) . - In Fact 1 the term `` exploration process '' is not defined . - Importantly , I do n't think this is well-formed unless we assume that there is no learning happening at the low level . Are we assuming that ? ( If not , then the process is non-stationary ) . - The high-level policy is assumed to select subgoals `` randomly '' : Does this mean uniformly at random from S ? Is S bounded ? - What does it mean for the agent to be able to `` move independently and identically in the state space '' ? - What the X^c_t are like depends completely on the low-level policy ! It could always move left , in which case these variables would not be anything like IID . Is this analysis supposed to be assuming that the low level is perfectly able to achieve the targets ? Or at least always successfully move c steps in the target `` direction '' ( not sure what that means , exactly , though ) . - What , precisely , do you mean by `` the features are all independent '' ? - Brushing assumption ( a ) off as a general technique to simplify analysis is not so good . It 's okay to say you 're making it for now , but there 's no reason to think results obtained with this assumption will generalize ! - Assumption d would require changes to the algorithm , no ? - In Theorem 1 : `` r is large enough '' for what ? - I do n't really understand what it means to `` assume q is an isotropic Gaussian ... '' ; that is , what exactly is q intended to be . I see that later you say that if r is big enough it approximates a uniform , and so maybe what you 're trying to show is that our exploration is uniform . But a uniform needs to be over a bounded space . And even with large r , q is only sort of uniform in a region bounded around the mean . - What makes `` optimal hierarchical exploration '' ? I guess maybe this means : with a fixed set of features and low-level control policy we 're deciding which k features to use for subgoals , and showing that among those choices we get the best exploration ( in the sense of matching a Gaussian around the origin ) if we use the k slowest features . ( This is just a related thought , not really a review of this paper.One other principle for selecting features for a subgoal representation is that they should be , in some sense , locally achievable . Hierarchy works well when the subproblems are , in a sense , `` serializable '' : that we can achieve subgoal the first any way we want to , without thinking about ( and/or making it harder to achieve ) the next subgoal we 're going to be asked to do . One way to do this might be to encourage some `` disentanglement '' in the latent features , so that the policy for changing one dimension of the latent space tends not to change the other dimensions . ) In the end , I 'm positively inclined toward this paper because it made me think about the concepts of what we really want in a hierarchical representation , but the mathematical exposition really needs substantial cleaning up . But I 'd rather have an interesting paper than a perfectly-executed boring one .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the inspiring comments . We appreciate it if you have any further questions or comments . - Q1.Why not learn the representation function at a slower time scale ? Our method updates the representation $ \\phi ( s ) $ per timestep with only one minibatch ( 100 samples ) , which is quite slow and steady . We compared this updating method with learning the representation at a slower time scale , e.g. , updating $ \\phi ( s ) $ every $ c $ timesteps with one minibatch data and updating $ \\phi ( s ) $ every $ c $ timesteps with $ c $ minibatches . Those methods made no significant difference . The experimental results are shown in Appendix E. - Q2 . Why attribute the success of the method to improved exploration ? - The exploration strengths of our method are two folds . First , during the representation learning process , the generalizable representation can help the exploration of the hierarchical policy , and the samples collected by the hierarchical policy facilitate the representation learning , as shown in the newly added section ( Section 6.4 ) . - Second , when the learning of the low-level policy and the representation converges , our method can achieve a larger coverage area , since the high-level policy explores in a slow subgoal space , as analyzed in Section 4 . And the didactic example shown in Section 6.1 also supports this theoretical result . The larger state coverage can avoid the local sub-optimum caused by deceptive rewards . - Q3.The purpose of the experiments on state coverage . The state coverage is important for exploration , especially at the early learning stage . A better state coverage can help an agent avoid stuck in a local optimum , especially when the reward function can be sparse or deceptive . Empirical results in Figure 2 show that , compared to other method , our method can quickly achieve a better state coverage at the early exploration , which allows it focus on the promising parts of the state space . - Q4.The theoretical part of the paper . Thank you for the detailed comments . We have refined the theoretical part of our paper to make it more rigorous and clearer in our revision . As some of the reviewer \u2019 s questions are related to each other , we group these questions and provide corresponding clarification concisely . 1.Target distribution : The target distribution is a prior distribution that we want to approximate . We assume that the target distribution is an isotropic Gaussian distribution , and as we stated in Section 4.2 , when $ r $ is large enough , the target distribution approximates a uniform distribution . We make this assumption since there is no prior knowledge . Besides , we have exchanged the notations of the target distribution and the steady distribution , and used forward KL divergence as the measure for exploration in the revised version . 2.Random walk : The exploration process is defined as a process when the low-level policy is optimal , and the high-level policy chooses subgoals randomly since there is no extrinsic reward . That is why we consider this process as a random walk . We have revised Fact 1 to Definition 2 to make it clearer . The subgoal selection is bounded in the neighborhood of the current state . We have added a new experiment to demonstrate that our algorithm can approximately satisfy this assumption in Appendix C. 3 . Feature independence and disentanglement : We assume that the features are all independent so that the change in one feature dimension will not influence the others . Selecting slow features as the subgoal space can be seen as a disentanglement , in some sense . We encourage the agent to achieve subgoals with slow dynamics and ignore the fast-changing ones . It is an interesting future direction to relax this assumption . Thanks for your inspiration . 4.Question about Assumption ( a ) : We can not release Assumption ( a ) in our derivation currently , but we can think deeper in future work , and we really appreciate your suggestion ."}, "2": {"review_id": "wxRwhSdORKG-2", "review_text": "This work presents a method for learning a slow-changing ( `` low-frequency '' ) embedding function , which can be used as a state-abstraction function in the context of Hierarchical RL . A high-level policy , trained to solve the environments task , acts by selecting abstracted states as targets for the low-level policy which is trained as in a goal-conditioned fashion ( acting in the environment itself attempting to get to the set goal ) . The paper provides theoretical motivation for the focus on `` slow '' features in a simplified synthetic example . The method itself is evaluated on high-dim control tasks ( As well as on an illustrative toy-example ) . There is a lot to like about this paper . The method is elegant , building on and extending previous approaches to HRL and particularly learning subgoal representations . The use of a low-frequency ( slowness ) criterion for this purpose is , to the best of my knowledge , a novel contribution . The work presents strong empirical evidence for the usefulness of the approach . The paper itself is well-structured and overall well-written , easy to follow , and with good references . I do have a few questions for which I would appreciate the authors comments : * In principle , the loss for the embedding function ( highly ) depends on the behavioral policy . This is because the embedding is trained to represent `` far away '' states as different from each other . And , generally , the distribution of $ s_t $ and $ s_ { t+c } $ can be rather different for , say , a random policy compared to a more purposeful/trained one . Since everything is trained jointly here ( embedding is used to train the policies which in turn can change the embedding and so on ) , this could in principle leads to instability of the training . I think this point deserve at least some discussion . * I find section 4 a bit confusing . `` Fact 1 '' seems more like a definition for me , because the properties of the `` random walk '' itself will be * determined * by $ p $ ( as $ p $ obviously depend on the behavioral policy.In fact from `` Fact 1 '' we can simply say that $ p $ is the steady-state/limiting/stationary distribution of the markov chain induced by the policy and the MDP ) . In general the relation between $ p $ and the policy is not entirely clear . Note that in general , the change metric $ \\Delta s^i_t $ is also policy dependent . I also do n't see the direct relation between $ p $ in this discussion , which ultimately is an expression of the ( low-level ) policy , to the embedding or representation principle used throughout the paper . * It seems the underlying assumption here is something like : the agent can independently control/change the state-features in an unconstrained way ( i.e an `` action '' is an offset vector s.t the next state is $ s_ { t+1 } =s_t+a_t $ , but in such a way that the environment somehow 'rescales ' $ a_t $ so that earlier entries are smaller ) . If this is indeed the case , I find this section not too motivating . If there is completely no structure in the environment ( state/action spaces ) and everything reduced down to a uniform random-walk , we can not learn much about even simple `` realistic '' RL settings . * Having said that I think the paper presents other good reasons and motivations for the focus on slow-features . In particular I think that the example discussed in section 6.1 is important , as well as the more qualitative explanations and the relation to the unsupervised-learning literature around similar ideas .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful and inspiring comments . We appreciate it if you have any further questions or comments . - Q1.The instability of the training . We have added a new section ( Section 6.4 ) to investigate the parallel learning of the representation function and the hierarchical policy . Figure 5 demonstrates that the learning of the representation function and the policy can promote each other . As the subgoal representation learning is conducted slowly and smoothly , our method is stable in most cases . - Q2.Question about Fact 1 . Thanks for your suggestion , and $ p ( x ) $ is indeed the steady distribution of the Markov chain , induced by a random high-level policy and an optimal low-level policy . ( By the way , we have exchanged the notations of the target distribution and the steady distribution in the revised version , and now $ q ( x ) $ stands for the steady distribution . ) - Q3.Is an \u201c action \u201d an offset vector ? No.We have made no assumption about the low-level action space , and we only consider every $ c $ -step change in the state space . Assuming each dimension of the change vector is bounded , we have taken the low-level action constraint into account but at a lower temporal resolution ( every $ c $ steps ) ."}, "3": {"review_id": "wxRwhSdORKG-3", "review_text": "# # # Summary This paper proposes to use slow features as the subgoal representation space in the HRL setting . The proposed approach adopts a slowness objective to learn a representation ( high-level action space ) and use it to train goal-conditioned policies ( low-level ) . # # # Main contributions - Slowness is an interesting property for representation learning . This work 's approach to slowness is simple and easy to integrate ( in practice ) to the challenging setting of HRL . - This work also makes a theoretical effort ( section 4 ) to motivate the slowness property for subgoal space . - The proposed method is evaluated on continuous control tasks against other references approaches to HRL . # # # Main comments/concerns - In the definition ( def.1 ) of the measure of exploration , what justifies the choice of the reverse KL ? Is it still a good choice if q is multimodal ? Does this definition really align with your claim `` a larger coverage area leads to better exploration '' ? - The algorithm implementation is not clear : - How is the goal sampled ? Randomly in the large domain as mentioned in the Appendix or in the neighbourhood of s as mentioned in sections 3 and 4 ? - What is the extrinsic reward used to train the high level policy exactly ? - There is a chicken-and-egg problem regarding the parallel learning of the representation and its usage to train the low policies . The representation should first be trained on some area before trusting the induced training of the low policy , which requires exploring that area beforehand . It is not clear how the algorithm deals with this challenge . Moreover , the evaluation on Mujoco tasks could be misleading regarding this point ( see the following comment ) . - The method is evaluated on Mujoco environments that provide the xy-coordinates . The representation can learn to extract the coordinates by only training on a limited area ( e.g.neighbourhood of initial state ) -- being able to early stop the representation training seems to support this hypothesis . Was this phenomenon studied is your experiments ? How is it limiting the proposed method beyond this type of environments ? How does the proposed method work when this inductive bias ca n't be leveraged , and when exploring an area of the state space is required to build a useful representation of it ? - Figure 4 shows the learned representation . Was it learned from the states or the images setting ? If learned from states , how does it look like when learned from images ? And , related to the previous comment , how does it look like along the training ? - How was the oracle trained ? It would be interesting to see the oracle training curves along with the compared methods on Figure 3 . - How reliable/relevant is the slowness evaluation ( section 6.3 ) in comparing the different methods ? It seems that the slowness objective can learn arbitrarily slow features ( according to your measure ) by weighting the attraction term more than the repulsive one ( tuning m and c can also influence the slowness measure ) . - `` As the state space of the Point robot is simple , the dynamics of the randomly selected features are slow as well '' : What do you mean by simple state space ? - `` the high-level policy guides the agent to jump out of the local optimum of moving towards the goal '' : local optimum w.r.t which reward function ? - `` The transfer effect [ ... ] is more significant [ ... ] since the target task , Ant FourRooms , is of a larger size and more difficult '' : Is the transfer effect confirmed when transferring from AntPush and AntFall to the larger Ant FourRooms ? # # # Minor comments - The AntPush performance curves are limited to 2.5 and 2 million steps . How was this horizon decided for this environment ? How do these curves look like for a longer training ( 4 million steps ) ? - Equation 7 seems confusing : p ( x ) is the asymptotic distribution of Y_n , meaning that it should not depend on n as shown in equation 7 . - Why does Figure 5 ( a ) has a 4 millions step while 5 ( b ) has 8 million steps ? Clarity : The paper reads well , but the experiments and algorithm presentation is sometimes unclear if not confusing .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the detailed comments . We have refined our paper to make it more rigorous and clearer . We have also added some new experimental results to clarify the reviewer \u2019 s questions . We appreciate it if you have any further questions or comments . - Q1.The measure of exploration in def . 1.Thank you for the suggestion . Our theoretical claim also holds with the forward KL definition , as discussed in the revision . We used reverse KL to measure coverage , as we assume the target state distribution is Gaussian in our analysis , which is unimodal . We agree with the reviewer that , for a multimodal target state distribution , using forward KL to measure exploration may be more proper . We have added discussions in our revision . - Q2.Is the goal sampled in a large domain or in the neighborhood of s ? We have tested these two methods for sampling subgoals . They demonstrated similar performance , as shown in Appendix C. This is because distant subgoals are hard to reach for the low-level policy , the high-level agent will learn to selects subgoals nearby the current latent state during the learning process . For empirical results in our paper , we used the way described in the Appendix , and have corrected the description in sections 3 and 4 in the revision . We have clarified this point in Appendix B.3 . - Q3.What is the extrinsic reward ? We used a common reward setting ( e.g . [ 1,2,3 ] ) , where the extrinsic reward is given as the negative L2 distance to the environment goal . This extrinsic reward is deceptive , which can lead the learning to a local sub-optimum . The hierarchical structure can enable better exploration with larger state coverage and avoid such local optima . - Q4.The chicken-and-egg problem regarding the parallel learning of the representation and low-level policies . Empirical results show that the iterative learning of the subgoal representation and the low-level policies promotes each other . This is because a low-level policy , even not very good , will collect useful data for learning a good subgoal representation , which is updated in a slow and stable manner and will also generalize to other parts of the state space . This gradually improved representation provides dense rewards to the low level , and the trajectories collected by the learned policy are utilized to train the representation . This co-adaptation process is demonstrated in Figure 5 ( a ) ~ ( c ) in the revision . - Q5.How does the proposed method perform beyond environments providing the xy coordinates ? - Our approach also works with visual observations * * without * * coordinate information , since the proposed inductive bias ( slow dynamics ) has a good generalization ability . - The training process of the representation learned from images * * without * * coordinates are shown in Section 6.4 in the revision . When the hierarchical agent has learned how to reach the easy goal near the initial states , the trajectory embeddings in this area ( red to yellow part in Figure 5 ( a ) $ \\sim $ ( c ) ) align with the coordinates . And the learned representation can be generalized to further areas , although a little chaotic , shown by the yellow to blue trajectory part in Figure 5 ( d ) $ \\sim $ ( f ) . This generalizable subgoal representation can facilitate the hierarchical policy exploration for the distant hard goal . By the way , early stopping the representation training is to make the learning faster in terms of wall time . We have added an ablation study of early stopping in Appendix D. - Q6 . What was the representation learned from in Figure 4 ? It was learned from the states . We have added the representation learned from images along with the training in Section 6.4 . As we stated in Q4 , the representation learned from images can facilitate policy learning , and vice versa . [ 1 ] Nachum et al. \u201c Near-Optimal Representation Learning for Hierarchical Reinforcement Learning. \u201d In ICLR , 2019 . [ 2 ] Nachum et al. \u201c Data-Efficient Hierarchical Reinforcement Learning. \u201d In NeurIPS 2018 . [ 3 ] Jinnai et al. \u201c Exploration in Reinforcement Learning with Deep Covering Options. \u201d In ICLR , 2020 ."}}