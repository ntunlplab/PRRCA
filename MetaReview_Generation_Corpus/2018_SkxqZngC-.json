{"year": "2018", "forum": "SkxqZngC-", "title": "A Bayesian Nonparametric Topic Model with Variational Auto-Encoders", "decision": "Reject", "meta_review": "The paper proposes a BNP topic model that uses a stick-breaking prior over document topics and performs VAE-style inference over them. Unfortunately, the novelty of this work is limited, as VAE-like inference for LDA-like models, inference with stick-breaking priors for VAEs, and placing a prior on the concentration parameter in a non-parametric topic model have all been done before (see e.g. Srivastava & Sutton (2017), Nalisnick & Smyth (2017), and Teh, Kurihara & Welling (2007) respectively). There are also concerns about the correctness of treating topics as parameters (as opposed to random variables) in the proposed model. The authors' clarification regarding this point was helpful but not sufficient to show the validity of the approach.", "reviews": [{"review_id": "SkxqZngC--0", "review_text": "\"topic modeling of text documents one of most important tasks\" Does this claim have any backing? \"inference of HDP is more complicated and not easy to be applied to new models\" Really an artifact of the misguided nature of earlier work. The posterior for the $\\vec\\pi$ of a elements of DP or HDP can be made a Dirichlet, made finite by keeping a \"remainder\" term and appropriate augmentation. Hughes, Kim and Sudderth (2015) have avoided stick-breaking and CRPs altogether, as have others in earlier work. Extensive models building on simple HDP doing all sorts of things have been developed. Variational stick-breaking methods never seemed to have worked well. I suspect you could achieve better results by replacing them as well, but you would have to replace the tree of betas and extend your Kumaraswamy distribution, so it may not work. Anyway, perhaps an avenue for future work. \"infinite topic models\" I've always taken the view that the use of the word \"infinite\" in machine learning is a kind of NIPSian machismo. In HDP-LDA at least, the major benefit in model performance comes from fitting what you call $\\vec\\pi$, which is uniform in vanilla LDA, and note that the number of topics \"found\" by a HDP-LDA sampler can be made to vary quite widely by varying what you call $\\alpha$, so any statement about the \"right\" number of topics is questionable. So the claim in 3rd paragraph of Section 2, \"superior\" and \"self-determined topic number\" I'd say are misguided. Plenty of experimental work to support this. In Related Work, you seem to only mention HDP for non-parametric topic models. More work exists, for instance using Pitman-Yor distributions for modelling words and using Gibbs samplers that are efficient and don't rely on the memory hungry HCRP. Good to see a prior is placed on the concentration parameter. Very important and not well done in the community, usually. ADDED: Originally done by Teh et al for HDP-LDA, and subsequently done by several, including Kim et al 2016. Others stress the importance of this. You need to cite at least Teh et al. in 5.4 to show this isn't new and the importance is well known. The Prod version is a very nice idea. Great results. This looks original, but I'm not expert enough in the huge masses of new deep neural network research popping up. You've upped the standard a bit by doing good experimental work. Oftentimes this is done poorly and one is left wondering. A lot of effort went into this. ADDED: usually like to see more data sets experimented with What code is used for HDP-LDA? Teh's original Matlab HCRP sampler does pretty well because at least he samples hyperparameters and can scale to 100k documents (yes, I tried). The comparison with LDA makes me suspicious. For instance, on 20News, a good non-parametric LDA will find well over 400 topics and roundly beat LDA on just 50 or 200. If reporting LDA, or HDP-LDA, it should be standard to do hyperparameter fitting and you need to mention what you did as this makes a big difference. ADDED: 20News results still poor for HPD, but its probably the implementation used ... their online variational algorithm only has advantages for large data sets Pros: * interesting new prod model with good results * alternative \"deep\" approach to a HDL-LDA model * good(-ish) experimental work Cons: * could do with a competitive non-parametric LDA implementation ADDED: good review responses generally ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the insightful and valuable comments , and we will consider seriously the avenue of future work you point out to us . We address the concerns of the reviewer as follows . Q1. \u201c Does this claim have any backing ? \u2019 Inference of HDP is more complicated and not easy to be applied to new models \u2018 \u2026 \u201d Thanks very much for this comment . We agree with you that the posterior of DP or HDP can be made to a Dirichlet by keeping a remainder term and appropriate augmentation , which makes the model easier to be optimized . By saying \u201c Inference of HDP is more complicated and not easy to be applied to new models even with small changes in the generative process \u201d , we mean that , unlike the black-box inference based models , people might need to redesign the inference methods when there are some changes in the generative process of HDP , which is not quite flexible . We have made this point clearer in the revision . Q2. \u201c So the claim in 3rd paragraph of Section 2 , `` superior '' and `` self-determined topic number '' I 'd say are misguided . \u201d Thanks for the suggestion that makes our paper more rigorous . We have modified the sentence to \u201c The Bayesian nonparametric topic models , such as HDP , potentially have infinite topic capacity and are able to adapt the topic number to data. \u201d Q3 . \u201c In Related Work , you seem to only mention HDP for non-parametric topic models . More work exists , for instance using Pitman-Yor distributions for modelling words and using Gibbs samplers that are efficient and do n't rely on the memory hungry HCRP. \u201d Thanks for the suggestion ! We have added more references for nonparametric topic model in the related work of the revision , such as Kim & Sudderth ( 2011 ) ; Archambeau et al . ( 2015 ) and Lim et al . ( 2016 ) .Q4. \u201c What code is used for HDP-LDA ? \u201c Thanks for this comment . We take the results of HDP and LDA from Miao et al . ( 2017 ) , since we use the exactly same datasets as Miao et al . ( 2017 ) ( We are appreciated that Miao provides the exactly same datasets to us privately , hence we can take the results directly ) . According to Miao et al . ( 2017 ) , the HDP is based on Wang et al . ( 2011 ) , which is an online variational inference algorithm , and the LDA is based on the online variational inference model of Hoffman et al. , ( 2010 ) . The results of LDA in Figure 1 are also based on Hoffman et al. , ( 2010 ) , where we use the implementation from http : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html . We have made it clearer in the revision ."}, {"review_id": "SkxqZngC--1", "review_text": "The paper constructs infinite Topic Model with Variational Auto-Encoders (iTM-VAE) by combining stick-breaking variational auto-encoder (SB-VAE) of Nalisnick & Smyth (2017) with latent Dirichlet allocation (LDA) and several inference techniques used in Miao et al. (2016 & 2017). A main difference from Autoencoded Variational Inference For Topic Model (AVITM) of Srivastava & Sutton (2017), which had already applied VAE to LDA, is that the Dirichlet-distributed topic distribution vector for each document is now imposed with a stick-breaking prior. To address the challenge of reparameterizing the beta distributions used in stick-breaking, the paper follows SB-VAE to use the Kumaraswamy distributions to approximate the beta distributions. The novelty of the paper does not appear to be significant, considering that most of the key techniques used in the paper had already appeared in several related papers, such as Nalisnick & Smyth (2017), Srivastava & Sutton (2017), and Miao et al. (2016 & 2017). While experiments show the proposed models outperform the others quantitatively (perplexity and coherence), the paper does not provide sufficient justifications on why ITM-VAE is better. In particular, it provides little information about how the two baselines, LDA and HDP, are implemented (e.g., via mean-field variational inference, VAE, or MCMC?) and how their perplexities and topic coherences are computed. In addition, achieving the best performance with about 20 topics seem quite surprising for 20news and RCV-v2. It is hard to imagine 20 news, which consists of articles in 20 different newsgroups, can be well characterized by about 20 different topics. Is there a tuning parameter that significantly impacts the number of topics inferred by iTM-VAE? Another clear problem of the paper is that the \u201cBayesian nonparametric\u201d generative procedure specified in Section 4.1 is not correct in theory. More specifically, independently drawing the document-specific pi vectors from the stick-breaking processes will lead to zero sharing between the atoms of different stick-breaking process draws. To make the paper theoretically sound as a Bayesian nonparametric topic model that uses the stick-breaking construction, please refer to Teh et al. (2006, 2008) and Wang et al. (2011) for the correct construction that ties the document-specific pi vectors with a globally shared stick-breaking process. ", "rating": "3: Clear rejection", "reply_text": "Q4. \u201c In particular , it provides little information about how the two baselines , LDA and HDP , are implemented ( e.g. , via mean-field variational inference , VAE , or MCMC ? ) and how their perplexities and topic coherences are computed. \u201d Thanks very much for this comment . We take the results of LDA and HDP from Miao et al . ( 2017 ) , since we use the exactly same datasets as them ( Yes , we are appreciated that Miao kindly provides the exactly same datasets to us privately . ) . According to Miao et al . ( 2017 ) , they use Hoffman et al. , ( 2010 ) for LDA and Wang et al . ( 2011 ) for HDP , which are both based on variational inference . The LDA results in Figure 1 are also based on Hoffman et al. , ( 2010 ) ( http : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html ) . We have listed the equation for perplexity in Section 5.1 , and we use the code provided by Lau et al ( 2014 ) ( the code is at https : //github.com/jhlau/topic_interpretability/ , which is also used by Srivastava & Sutton ( 2017 ) ) to compute topic coherence . We make this point clearer in the revision . Q5. \u201c It is hard to imagine 20 news , which consists of articles in 20 different newsgroups , can be well characterized by about 20 different topics. \u201d Yes , it seems amazing that about 20 different topics can explain 20News dataset quite well . However , according to the description of 20News ( http : //qwone.com/~jason/20Newsgroups/ ) , some of the newsgroups are very closely related to each other . Since 20News is a quite small dataset and many groups are very closely related ( e.g.comp.graphics , comp.sys.mac.hardware , comp.windows.x ) , it is reasonable to explain the data with a small number of topics . Table 5 in the revision shows that among 50 topics learned by AVITM , there are many redundant topics . Moreover , the curves in Figure 1- ( a ) also confirms that about 20 topics is a good choice for the other topic models . Q6. \u201c Is there a tuning parameter that significantly impacts the number of topics inferred by iTM-VAE ? \u201d The concentration parameter $ \\alpha $ is the one that significantly impacts the number of topics inferred by iTM-VAE . Since it is a very important parameter , we place a prior on $ \\alpha $ which helps the model to adjust $ \\alpha $ to data automatically . Section 5.4 demonstrates the effectiveness of the prior . If we do not place the prior , iTM-VAE-Prod with strong decoder can not adapt well to different sub-sampled subsets of 20News dataset . Common training techniques , e.g.KL annealing , decoder regularization , do not alleviate this problem significantly . While iTM-VAE-G has better adaptive ability w.r.t dataset size . This is also one of the key contributions of the paper . We hope that this rebuttal can address the concerns and some misunderstandings of the reviewer . Please let us know whether the reviewer has other comments . We are looking forward to hearing from you ."}, {"review_id": "SkxqZngC--2", "review_text": "The paper proposes a VAE inference network for a non-parametric topic model. The model on page 4 is confusing to me since this is a topic model, so document-specific topic distributions are required, but what is shown is only stick-breaking for a mixture model. From what I can tell, the model itself is not new, only the fact that a VAE is used to approximate the posterior. In this case, if the model is nonparametric, then comparing with Wang, et al (2011) seems the most relevant non-deep approach. Given the factorization used in that paper, the q distributions are provably optimal by the standard method. Therefore, something must be gained by the VAE due to a non-factorized q. This would be best shown by comparing with the corresponding non-deep version of the model rather than LDA and other deep models.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the valuable comments . The main concerns raised by the reviewer are : 1 ) About the model : There are no document-specific topic distributions , but only a stick-breaking process for a mixture model ; 2 ) About the novelty : The difference with Wang , et al ( 2011 ) is that VAE is used to approximate the posterior . Hence the model should compare with Wang , et al ( 2011 ) since it is the most relevant non-deep approach . We address all these concerns raised the reviewer as follows : Q1 : \u201c The model on page 4 is confusing to me since this is a topic model , so document-specific topic distributions are required , but what is shown is only stick-breaking for a mixture model. \u201d There might be some misunderstandings . We do have the document-specific topic distributions on page 4 , which is $ \\pi $ in the generative process . The difference with LDA is that our model samples the document-specific topic distributions from a GEM distribution , while LDA samples them from a Dirichlet distribution . Actually , the generative procedure of iTM-VAE of Section 4.1 is similar to LDA , i.e.1 ) sample a document-specific topic distributions $ pi $ ; Then , for each word $ w_i $ in the document : 2 ) draw a topic $ \\hat { \\theta } _i $ ; 3 ) draw $ w_i $ from $ Cat ( \\hat { \\theta } _i ) $ . Moreover , according to the section 3.2 of ( Blei2003 , JMLR , http : //www.jmlr.org/papers/volume3/blei03a/blei03a.pdf ) , LDA itself is also a mixture model . Q2. \u201c In this case , if the model is nonparametric , then comparing with Wang , et al ( 2011 ) seems the most relevant non-deep approach. \u201d , \u201c This would be best shown by comparing with the corresponding non-deep version of the model rather than LDA and other deep models. \u201d We agree with the reviewer that it would be best to compare with Wang , et al ( 2011 ) to see the gain from VAE . Actually , the HDP in Table 1 is taken from Miao , et al ( 2017 ) , which is actually based on Wang , et al ( 2011 ) . We have clarified this point in the revision . Thanks very much for the suggestion ! Q3.About the novelty of the paper . We would like to clarify the novelty of the paper : 1 ) As is pointed out by the reviewer , we introduce a global inference net to model the variational posterior of BNP topic models , and carry out optimization under the VAE framework . To our best knowledge , this is the first time that BNP topic models are combined with AEVB . This technique brings 2 benefits for BNP topic models . ( 1 ) No further variational updates are needed on the test data but only a feed-forward pass on inference net . Hence , the model is very efficient . ( 2 ) The optimization of VAE framework is very general . The generative model can be adjusted and enhanced without additional mathematic derivation . Hence , it is very flexible . 2 ) We propose iTM-VAE-G , in which a prior is added on the concentration parameter . This technique helps the model to adjust the concentration parameter to data automatically , and we have shown the effect of the prior in Section 5.4 . To further demonstrate the advantage of iTM-VAE-G , we compare it with iTM-VAE-Prod which does not have the prior over the concentration parameter . This newly added experiments are shown in Table 2 in the revision . We can see that when the decoder is strong , the number of effective topics learned by iTM-VAE-Prod ( without the prior ) can not adapt well to different sub-sampled subsets of 20News dataset . Common training techniques , e.g.KL annealing , decoder regularization , do not help much . iTM-VAE-G can increase the adaptive power of the model in an elegant way . After the prior is added , the restriction on the latent representation is relaxed , the model will learn an appropriate and highly-confident corpus-level posterior for the concentration parameter , and can adapt its power according to the dataset size , even if the decoder is strong . As commented by AR3 , this technique is \u201c very important and not well done in the community , usually \u201d . 3 ) The experimental results confirm the advantage of the model . Please let us know whether the rebuttal solves the concerns of the reviewer . We are looking forward to hearing from you . Thanks very much !"}], "0": {"review_id": "SkxqZngC--0", "review_text": "\"topic modeling of text documents one of most important tasks\" Does this claim have any backing? \"inference of HDP is more complicated and not easy to be applied to new models\" Really an artifact of the misguided nature of earlier work. The posterior for the $\\vec\\pi$ of a elements of DP or HDP can be made a Dirichlet, made finite by keeping a \"remainder\" term and appropriate augmentation. Hughes, Kim and Sudderth (2015) have avoided stick-breaking and CRPs altogether, as have others in earlier work. Extensive models building on simple HDP doing all sorts of things have been developed. Variational stick-breaking methods never seemed to have worked well. I suspect you could achieve better results by replacing them as well, but you would have to replace the tree of betas and extend your Kumaraswamy distribution, so it may not work. Anyway, perhaps an avenue for future work. \"infinite topic models\" I've always taken the view that the use of the word \"infinite\" in machine learning is a kind of NIPSian machismo. In HDP-LDA at least, the major benefit in model performance comes from fitting what you call $\\vec\\pi$, which is uniform in vanilla LDA, and note that the number of topics \"found\" by a HDP-LDA sampler can be made to vary quite widely by varying what you call $\\alpha$, so any statement about the \"right\" number of topics is questionable. So the claim in 3rd paragraph of Section 2, \"superior\" and \"self-determined topic number\" I'd say are misguided. Plenty of experimental work to support this. In Related Work, you seem to only mention HDP for non-parametric topic models. More work exists, for instance using Pitman-Yor distributions for modelling words and using Gibbs samplers that are efficient and don't rely on the memory hungry HCRP. Good to see a prior is placed on the concentration parameter. Very important and not well done in the community, usually. ADDED: Originally done by Teh et al for HDP-LDA, and subsequently done by several, including Kim et al 2016. Others stress the importance of this. You need to cite at least Teh et al. in 5.4 to show this isn't new and the importance is well known. The Prod version is a very nice idea. Great results. This looks original, but I'm not expert enough in the huge masses of new deep neural network research popping up. You've upped the standard a bit by doing good experimental work. Oftentimes this is done poorly and one is left wondering. A lot of effort went into this. ADDED: usually like to see more data sets experimented with What code is used for HDP-LDA? Teh's original Matlab HCRP sampler does pretty well because at least he samples hyperparameters and can scale to 100k documents (yes, I tried). The comparison with LDA makes me suspicious. For instance, on 20News, a good non-parametric LDA will find well over 400 topics and roundly beat LDA on just 50 or 200. If reporting LDA, or HDP-LDA, it should be standard to do hyperparameter fitting and you need to mention what you did as this makes a big difference. ADDED: 20News results still poor for HPD, but its probably the implementation used ... their online variational algorithm only has advantages for large data sets Pros: * interesting new prod model with good results * alternative \"deep\" approach to a HDL-LDA model * good(-ish) experimental work Cons: * could do with a competitive non-parametric LDA implementation ADDED: good review responses generally ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the insightful and valuable comments , and we will consider seriously the avenue of future work you point out to us . We address the concerns of the reviewer as follows . Q1. \u201c Does this claim have any backing ? \u2019 Inference of HDP is more complicated and not easy to be applied to new models \u2018 \u2026 \u201d Thanks very much for this comment . We agree with you that the posterior of DP or HDP can be made to a Dirichlet by keeping a remainder term and appropriate augmentation , which makes the model easier to be optimized . By saying \u201c Inference of HDP is more complicated and not easy to be applied to new models even with small changes in the generative process \u201d , we mean that , unlike the black-box inference based models , people might need to redesign the inference methods when there are some changes in the generative process of HDP , which is not quite flexible . We have made this point clearer in the revision . Q2. \u201c So the claim in 3rd paragraph of Section 2 , `` superior '' and `` self-determined topic number '' I 'd say are misguided . \u201d Thanks for the suggestion that makes our paper more rigorous . We have modified the sentence to \u201c The Bayesian nonparametric topic models , such as HDP , potentially have infinite topic capacity and are able to adapt the topic number to data. \u201d Q3 . \u201c In Related Work , you seem to only mention HDP for non-parametric topic models . More work exists , for instance using Pitman-Yor distributions for modelling words and using Gibbs samplers that are efficient and do n't rely on the memory hungry HCRP. \u201d Thanks for the suggestion ! We have added more references for nonparametric topic model in the related work of the revision , such as Kim & Sudderth ( 2011 ) ; Archambeau et al . ( 2015 ) and Lim et al . ( 2016 ) .Q4. \u201c What code is used for HDP-LDA ? \u201c Thanks for this comment . We take the results of HDP and LDA from Miao et al . ( 2017 ) , since we use the exactly same datasets as Miao et al . ( 2017 ) ( We are appreciated that Miao provides the exactly same datasets to us privately , hence we can take the results directly ) . According to Miao et al . ( 2017 ) , the HDP is based on Wang et al . ( 2011 ) , which is an online variational inference algorithm , and the LDA is based on the online variational inference model of Hoffman et al. , ( 2010 ) . The results of LDA in Figure 1 are also based on Hoffman et al. , ( 2010 ) , where we use the implementation from http : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html . We have made it clearer in the revision ."}, "1": {"review_id": "SkxqZngC--1", "review_text": "The paper constructs infinite Topic Model with Variational Auto-Encoders (iTM-VAE) by combining stick-breaking variational auto-encoder (SB-VAE) of Nalisnick & Smyth (2017) with latent Dirichlet allocation (LDA) and several inference techniques used in Miao et al. (2016 & 2017). A main difference from Autoencoded Variational Inference For Topic Model (AVITM) of Srivastava & Sutton (2017), which had already applied VAE to LDA, is that the Dirichlet-distributed topic distribution vector for each document is now imposed with a stick-breaking prior. To address the challenge of reparameterizing the beta distributions used in stick-breaking, the paper follows SB-VAE to use the Kumaraswamy distributions to approximate the beta distributions. The novelty of the paper does not appear to be significant, considering that most of the key techniques used in the paper had already appeared in several related papers, such as Nalisnick & Smyth (2017), Srivastava & Sutton (2017), and Miao et al. (2016 & 2017). While experiments show the proposed models outperform the others quantitatively (perplexity and coherence), the paper does not provide sufficient justifications on why ITM-VAE is better. In particular, it provides little information about how the two baselines, LDA and HDP, are implemented (e.g., via mean-field variational inference, VAE, or MCMC?) and how their perplexities and topic coherences are computed. In addition, achieving the best performance with about 20 topics seem quite surprising for 20news and RCV-v2. It is hard to imagine 20 news, which consists of articles in 20 different newsgroups, can be well characterized by about 20 different topics. Is there a tuning parameter that significantly impacts the number of topics inferred by iTM-VAE? Another clear problem of the paper is that the \u201cBayesian nonparametric\u201d generative procedure specified in Section 4.1 is not correct in theory. More specifically, independently drawing the document-specific pi vectors from the stick-breaking processes will lead to zero sharing between the atoms of different stick-breaking process draws. To make the paper theoretically sound as a Bayesian nonparametric topic model that uses the stick-breaking construction, please refer to Teh et al. (2006, 2008) and Wang et al. (2011) for the correct construction that ties the document-specific pi vectors with a globally shared stick-breaking process. ", "rating": "3: Clear rejection", "reply_text": "Q4. \u201c In particular , it provides little information about how the two baselines , LDA and HDP , are implemented ( e.g. , via mean-field variational inference , VAE , or MCMC ? ) and how their perplexities and topic coherences are computed. \u201d Thanks very much for this comment . We take the results of LDA and HDP from Miao et al . ( 2017 ) , since we use the exactly same datasets as them ( Yes , we are appreciated that Miao kindly provides the exactly same datasets to us privately . ) . According to Miao et al . ( 2017 ) , they use Hoffman et al. , ( 2010 ) for LDA and Wang et al . ( 2011 ) for HDP , which are both based on variational inference . The LDA results in Figure 1 are also based on Hoffman et al. , ( 2010 ) ( http : //scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html ) . We have listed the equation for perplexity in Section 5.1 , and we use the code provided by Lau et al ( 2014 ) ( the code is at https : //github.com/jhlau/topic_interpretability/ , which is also used by Srivastava & Sutton ( 2017 ) ) to compute topic coherence . We make this point clearer in the revision . Q5. \u201c It is hard to imagine 20 news , which consists of articles in 20 different newsgroups , can be well characterized by about 20 different topics. \u201d Yes , it seems amazing that about 20 different topics can explain 20News dataset quite well . However , according to the description of 20News ( http : //qwone.com/~jason/20Newsgroups/ ) , some of the newsgroups are very closely related to each other . Since 20News is a quite small dataset and many groups are very closely related ( e.g.comp.graphics , comp.sys.mac.hardware , comp.windows.x ) , it is reasonable to explain the data with a small number of topics . Table 5 in the revision shows that among 50 topics learned by AVITM , there are many redundant topics . Moreover , the curves in Figure 1- ( a ) also confirms that about 20 topics is a good choice for the other topic models . Q6. \u201c Is there a tuning parameter that significantly impacts the number of topics inferred by iTM-VAE ? \u201d The concentration parameter $ \\alpha $ is the one that significantly impacts the number of topics inferred by iTM-VAE . Since it is a very important parameter , we place a prior on $ \\alpha $ which helps the model to adjust $ \\alpha $ to data automatically . Section 5.4 demonstrates the effectiveness of the prior . If we do not place the prior , iTM-VAE-Prod with strong decoder can not adapt well to different sub-sampled subsets of 20News dataset . Common training techniques , e.g.KL annealing , decoder regularization , do not alleviate this problem significantly . While iTM-VAE-G has better adaptive ability w.r.t dataset size . This is also one of the key contributions of the paper . We hope that this rebuttal can address the concerns and some misunderstandings of the reviewer . Please let us know whether the reviewer has other comments . We are looking forward to hearing from you ."}, "2": {"review_id": "SkxqZngC--2", "review_text": "The paper proposes a VAE inference network for a non-parametric topic model. The model on page 4 is confusing to me since this is a topic model, so document-specific topic distributions are required, but what is shown is only stick-breaking for a mixture model. From what I can tell, the model itself is not new, only the fact that a VAE is used to approximate the posterior. In this case, if the model is nonparametric, then comparing with Wang, et al (2011) seems the most relevant non-deep approach. Given the factorization used in that paper, the q distributions are provably optimal by the standard method. Therefore, something must be gained by the VAE due to a non-factorized q. This would be best shown by comparing with the corresponding non-deep version of the model rather than LDA and other deep models.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the valuable comments . The main concerns raised by the reviewer are : 1 ) About the model : There are no document-specific topic distributions , but only a stick-breaking process for a mixture model ; 2 ) About the novelty : The difference with Wang , et al ( 2011 ) is that VAE is used to approximate the posterior . Hence the model should compare with Wang , et al ( 2011 ) since it is the most relevant non-deep approach . We address all these concerns raised the reviewer as follows : Q1 : \u201c The model on page 4 is confusing to me since this is a topic model , so document-specific topic distributions are required , but what is shown is only stick-breaking for a mixture model. \u201d There might be some misunderstandings . We do have the document-specific topic distributions on page 4 , which is $ \\pi $ in the generative process . The difference with LDA is that our model samples the document-specific topic distributions from a GEM distribution , while LDA samples them from a Dirichlet distribution . Actually , the generative procedure of iTM-VAE of Section 4.1 is similar to LDA , i.e.1 ) sample a document-specific topic distributions $ pi $ ; Then , for each word $ w_i $ in the document : 2 ) draw a topic $ \\hat { \\theta } _i $ ; 3 ) draw $ w_i $ from $ Cat ( \\hat { \\theta } _i ) $ . Moreover , according to the section 3.2 of ( Blei2003 , JMLR , http : //www.jmlr.org/papers/volume3/blei03a/blei03a.pdf ) , LDA itself is also a mixture model . Q2. \u201c In this case , if the model is nonparametric , then comparing with Wang , et al ( 2011 ) seems the most relevant non-deep approach. \u201d , \u201c This would be best shown by comparing with the corresponding non-deep version of the model rather than LDA and other deep models. \u201d We agree with the reviewer that it would be best to compare with Wang , et al ( 2011 ) to see the gain from VAE . Actually , the HDP in Table 1 is taken from Miao , et al ( 2017 ) , which is actually based on Wang , et al ( 2011 ) . We have clarified this point in the revision . Thanks very much for the suggestion ! Q3.About the novelty of the paper . We would like to clarify the novelty of the paper : 1 ) As is pointed out by the reviewer , we introduce a global inference net to model the variational posterior of BNP topic models , and carry out optimization under the VAE framework . To our best knowledge , this is the first time that BNP topic models are combined with AEVB . This technique brings 2 benefits for BNP topic models . ( 1 ) No further variational updates are needed on the test data but only a feed-forward pass on inference net . Hence , the model is very efficient . ( 2 ) The optimization of VAE framework is very general . The generative model can be adjusted and enhanced without additional mathematic derivation . Hence , it is very flexible . 2 ) We propose iTM-VAE-G , in which a prior is added on the concentration parameter . This technique helps the model to adjust the concentration parameter to data automatically , and we have shown the effect of the prior in Section 5.4 . To further demonstrate the advantage of iTM-VAE-G , we compare it with iTM-VAE-Prod which does not have the prior over the concentration parameter . This newly added experiments are shown in Table 2 in the revision . We can see that when the decoder is strong , the number of effective topics learned by iTM-VAE-Prod ( without the prior ) can not adapt well to different sub-sampled subsets of 20News dataset . Common training techniques , e.g.KL annealing , decoder regularization , do not help much . iTM-VAE-G can increase the adaptive power of the model in an elegant way . After the prior is added , the restriction on the latent representation is relaxed , the model will learn an appropriate and highly-confident corpus-level posterior for the concentration parameter , and can adapt its power according to the dataset size , even if the decoder is strong . As commented by AR3 , this technique is \u201c very important and not well done in the community , usually \u201d . 3 ) The experimental results confirm the advantage of the model . Please let us know whether the rebuttal solves the concerns of the reviewer . We are looking forward to hearing from you . Thanks very much !"}}