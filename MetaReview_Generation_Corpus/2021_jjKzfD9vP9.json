{"year": "2021", "forum": "jjKzfD9vP9", "title": "Saliency Grafting: Innocuous Attribution-Guided Mixup with Calibrated Label Mixing", "decision": "Reject", "meta_review": "Overall, this paper has been on the very borderline. All reviewers agree that the motivation and the idea of the paper are reasonable (although somewhat incremental) and make an interesting extension of mixup-type data augmentation. However, one expert reviewer raised some concerns which are unfortunately not fully resolved despite the intensive interaction. \n\nThe first one is the issue of diversity claimed in the paper. The authors explanation is mostly qualitative, and I, as an AC, also felt a logical jumping here, although I do understand that the proposed method somehow results in better generalization ability w.r.t. the number of generated samples as shown in Fig.3 in the appendix. This point is important because if the better generalization is coming from the label modification rather than the diversity in image space, then the novelty of the paper is limited.\nAnother concern is that there are some inconsistencies in the scores of previous methods implemented by the authors and in the original papers. We understand that the exact score is not always easy to reproduce, but at least it is desirable to follow the original setting (such as the number of epochs) for each method as far as possible, because the accuracy should be the most important criterion as the goal of data augmentation is better generalization. Then, authors may separately discuss computational efficiency. \n\nBased on the discussion with reviewers as above, I conclude that the paper should be further polished and completed before publication. Thus I recommend rejection for this time.", "reviews": [{"review_id": "jjKzfD9vP9-0", "review_text": "This paper advances the line of research in data augmentation following Mixup paper . Cutmix is a image-specific variant of mixup that pastes a rectangular region from a donor image to a target image ; however , it does this in a completely random fashion not paying attention to whether the discriminative parts of either image are retained . Recent methods alleviate this problem by using saliency guided region selection either preserving the most discriminative parts of the donor image ( Attentive Cutmix ) or both images ( PuzzleMix ) . However , in doing so they replace the random nature of Cutmix with a deterministic approach potentially reducing sample diversity . The first contribution of the paper is a saliency guide stochastic augmentation method which combines the best of both worlds . The second contribution is in the generation of more meaningful labels for the newly created image by taking into account the saliency of the regions used from both images rather than blindly using the mixing ratio . The contributions of the paper are somewhat incremental but are reasonable . The validity of using saliency in determining the label of the new sample is clear . The experimental results are mixed . Most experiments demonstrate minor improvements over previous methods . On the other hand the top-5 performance of CutMix is better than the proposed method in Table 2 . With such small differences and no standard deviations on accuracy reported it is not clear whether the improvements are really significant . In the data scarcity experiments , I find the statement \u201c Note that the performance of CutMix deteriorates as the number of data per class decreases due to their randomness occurring label mismatching. \u201d misleading . This is true of every method in that table not just CutMix , and the detoriation amounts are very similar . Clarity : Overall I found the paper well written and easy to read . The authors have provided standard errors which improves the confidence that the improvements in the experiments are significant .", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # [ Q2 : Most experiments demonstrate minor improvements over previous methods . With such small differences and no standard deviations on accuracy reported it is not clear whether the improvements are really significant . ] Thank you for your feedback . We report the standard errors of CIFAR-100 for PyramidNet-200 ( Table 2 in the main paper ) in Table A ( below ) . Also , in addition to the experiments in the main paper , we conducted 2 additional experiments to benchmark Saliency Grafting : TinyImageNet for ResNet-18 ( Table B ) and CIFAR100 for ResNet-18 ( Table C ) , and also report standard errors for both of them . In these experiments , Saliency Grafting * consistently * outperforms other methods . This performance gap is a major gain for a data augmentation method , as the gain is achieved not by changing architectural designs , which are the main powerhouse of performance , but by tweaking data processing schemes , which are small but effective finishing touches . We also observe that for all of these experiments , the standard errors are far from overlapping , conveying statistical significance . In the main paper , we omitted standard errors to align our experiments with previous papers , as they have not reported any standard errors . However , we agree with R3 that standard errors are necessary . Since the results reported in previous papers do not contain standard errors , we reproduced them using author codes and guidelines for repetition . However , for some methods , we were unable to reproduce the performance stated in their respective papers , albeit using the author \u2019 s code/hyperparameters and following their guidelines . Table A. Top-1/Top-5 errors on CIFAR-100 for PyramidNet-200 in comparison to state-of-the-art data augmentation methods . | Method | Top-1 Err . | Top-5 Err . | |-| : - : | : - : | | Saliency Grafting | * * 13.59 ( \u00b10.062 ) * * | * * 3.01 ( \u00b10.021 ) * * | | PuzzleMix | 16.52 ( \u00b10.066 ) | 3.70 ( \u00b10.065 ) | | CutMix | 14.51 ( \u00b10.149 ) | 3.08 ( \u00b10.024 ) | | Attentive CutMix | 15.24 ( \u00b10.085 ) | 3.46 ( \u00b10.061 ) | Table B. Top-1/Top-5 errors on TinyImageNet for ResNet-18 in comparison to state-of-the-art data augmentation methods . | Method | Top-1 Err . | Top-5 Err . | |-| : - : | : - : | | Saliency Grafting | * * 34.80 ( \u00b10.095 ) * * | * * 16.13 ( \u00b10.077 ) * * | | PuzzleMix | 36.67 ( \u00b10.081 ) | 16.92 ( \u00b10.074 ) | | CutMix | 36.13 ( \u00b10.064 ) | 16.20 ( \u00b10.042 ) | | Mixup | 39.01 ( \u00b10.153 ) | 19.56 ( \u00b10.052 ) | Table C. Top-1/Top-5 errors on CIFAR-100 for ResNet-18 in comparison to state-of-the-art data augmentation methods . | Method | Top-1 Err . | Top-5 Err . | |-| : - : | : - : | | Saliency Grafting | * * 20.39 ( \u00b10.074 ) * * | * * 5.80 ( \u00b10.052 ) * * | | PuzzleMix | 22.51 ( \u00b10.092 ) | 6.39 ( \u00b10.101 ) | | CutMix | 20.97 ( \u00b10.060 ) | 5.89 ( \u00b10.026 ) | | Mixup | 22.04 ( \u00b10.124 ) | 6.96 ( \u00b10.101 ) | # # # [ Q3 : Not only CutMix but all methods struggle against data scarcity . ] The reviewer misunderstood our intentions . Indeed , the performances of all methods degrade as the number of data per class decreases . However , the meaning we tried to convey was that CutMix suffers the most from data scarcity . In Table 4 of the main paper , we reported results ( top-1 error on CIFAR-100 ) for 3 data portions : 10 % , 20 % , and 50 % . When data scarcity is not severe ( 50 % ) , CutMix ( 28.73 ) performs slightly better than Mixup ( 28.97 ) and is relatively close to PuzzleMix ( 28.04 ) . However , as the amount of data decreases , the performance of CutMix degrades rapidly , displaying the worst performance ( lags behind all augmentation methods ) among the augmentation methods as we hit 20 % and 10 % data ( On 10 % data , the top-1 error of CutMix is 55.71 while Mixup is 51.86 and PuzzleMix is 52.63 ) . We will clarify the statement in the revision . We suspect that this is because CutMix carries an innate flaw that is * * label mismatch * * , due to the saliency-agnostic nature of its scheme . A previous study shows that label noise is more destructive when data is scarce ( Rolnick et al.2017 [ 1 ] ) . We suspect that this is why CutMix performs worse in data scarcity situations . PuzzleMix and Mixup incompletely address this problem by generating incorrectly calibrated labels . However , our method produces well-calibrated ( correct ) labels which are shown to be more resistant to data scarcity , with our method displaying the best performance . [ 1 ] https : //arxiv.org/abs/1705.10694"}, {"review_id": "jjKzfD9vP9-1", "review_text": "The paper propose a novel method to address the lack of diversity in mixed samples created by the saliency-based Mixup strategies such as Puzzle Mix . To attain this goal , the proposed approach incorporates a random process in the saliency regions selection process to give all salient regions equal chance , and accordingly adjust the modeling target to match the saliency of the mixed image . The paper presents an interesting extension to the existing work . Nevertheless , I would like to see more evidence of why the existing methods lack of diversity and how it negatively impacts the model \u2019 s performance . I also think the experiments in the current form is weak . My major comments are as follows . Regarding the proposed method : 1 . The motivation of generating diverse mixed images through selecting diverse salient features is interesting to me . But I think the current justification is weak . It is true that the Puzzle Mix will select saliency regions with high intensity , but I wonder if the random mixing ratio \\lambda in PuzzleMix would help to generate diverse mixed samples due to the random sampling process . Another factor for the diversity here is that even using similar saliency regions of the input pair for the mixed images , the mixed labels are different due to the random selection of the mixing ratio . In this sense , I think how severe this lack of diversity issue is and how it negatively impacts the performance of the mode are not very clear to me . It would be very beneficial if the paper could show it somehow . 2.The way that the proposed method generates the mixed modeling target for the mixed input reminds me of this paper : nonlinear mixup : out-of-manifold data augmentation for text classification ( AAAI2020 ) . In the nonlinear Mixup method , the mixed label for a mixed input in Mixup is also computed based on the input pair , although the method is evaluated using text classification tasks . I think it would be useful to discuss the difference and adjust the content such as Table1 and Section3 to make the statements in the paper more accurate . 3.Some parts in the proposed method deserve further discussion and justification . For example , the Threshold for the normalized saliency map . How sensitive is the threshold value to the performance of the proposed model ? It would be useful to include some analysis either theoretically or experimentally . Regarding the experiments : 1 . I am surprised to see from Table2 that the PuzzleMix degraded the performance of the baseline model . I wonder why this happened . In this sense , I would like to see experiments with some other benchmarking datasets such as Cifar10 , SVHN , or TinyImagenet . Also , for these experiments , it would be useful to also provide the deviation of the 3 runs . 2.For Cifar100 using WRN28-10 ( also for ImageNet with ResNet-50 ) , the difference between the proposed method and the PuzzleMix is very small . This suggests that it would useful to experiment on other network architectures such as ResNet-18 or ResNet-50 , which are used in the original Mixup and the PuzzleMix papers . Minor issues : 1 . The paper discusses the harmful samples potentially generated by Mixup in several places without citation . I think it would be useful to include some citations such as the manifold intrusion issue raised in the AdaMixup paper ( AAAI2019 ) or the noise image issue as discussed in the PuzzleMix paper ( ICML2020 ) . In short , I think the paper would benefit from further justifying the problem of lack of mixed sample diversity in the PuzzleMix . Also , the current experimental results show minor improvement over PuzzleMix , which also makes the contribution of the paper less significant .", "rating": "5: Marginally below acceptance threshold", "reply_text": "# # # [ Q5.Experiments on other benchmarking datasets ] To address the reviewer 's concern , we evaluated our method on another benchmark dataset - TinyImageNet . We trained a ResNet-18 for 600 epochs with an SGD+momentum optimizer , following the one of TinyImageNet experimental settings in the PuzzleMix paper . Other data augmentation methods including PuzzleMix are evaluated using the released code and authors \u2019 hyperparameters . The obtained results are shown in Table C. Like the experiments contained in the main paper , Saliency Grafting consistently exhibits the best performance on this new benchmark dataset . However , PuzzleMix falls behind CutMix . Table C. Top-1/Top-5 errors on TinyImageNet for ResNet-18 in comparison to state-of-the-art data augmentation methods . | Method | Top-1 Err . | Top-5 Err . | |-| : - : | : - : | | Saliency Grafting | * * 34.80 ( \u00b10.095 ) * * | * * 16.13 ( \u00b10.077 ) * * | | PuzzleMix | 36.67 ( \u00b10.081 ) | 16.92 ( \u00b10.074 ) | | CutMix | 36.13 ( \u00b10.064 ) | 16.20 ( \u00b10.042 ) | | Mixup | 39.01 ( \u00b10.153 ) | 19.56 ( \u00b10.052 ) | # # # [ Q6 . CIFAR 100 experiment on other network architectures ] In response to the reviewer ` s constructive suggestion , we assess our method on CIFAR-100 using another model architecture , ResNet18 . We trained the network for 300 epochs with an initial learning rate of 0.1 and decayed the learning rate at 150 , 225 epoch . Other data augmentation methods including PuzzleMix are evaluated using the released code and authors \u2019 hyperparameters . The results are displayed in Table D. In this architecture setting , Saliency Grafting again shows the best performance . However , PuzzleMix falls behind Mixup and CutMix . Table D. Top-1/Top-5 errors on CIFAR-100 for ResNet-18 in comparison to state-of-the-art data augmentation methods . | Method | Top-1 Err . | Top-5 Err . | |-| : - : | : - : | | Saliency Grafting | * * 20.39 ( \u00b10.074 ) * * | * * 5.80 ( \u00b10.052 ) * * | | PuzzleMix | 22.51 ( \u00b10.092 ) | 6.39 ( \u00b10.101 ) | | CutMix | 20.97 ( \u00b10.060 ) | 5.89 ( \u00b10.026 ) | | Mixup | 22.04 ( \u00b10.124 ) | 6.96 ( \u00b10.101 ) | # # # [ Q7 . It would be useful to include some citations such as the manifold intrusion issue raised in the AdaMixup paper ( AAAI2019 ) ] Thank you for the helpful suggestion . This paper addressed the `` manifold intrusion '' problem , which is a mismatch problem between the mixup sample and the corresponding soft label . We agree that citing this interesting paper would be beneficial , and will add the references properly in the revision . * * Updated * * We 've cited this novel work in our introduction section ."}, {"review_id": "jjKzfD9vP9-2", "review_text": "The authors propose a novel sailency-guided data augmentation method that alleviates some of the drawbacks arising with recent Mixup-based augmentation approaches . Specifically , the authors propose sailency thresholding for region selection ( instead of maximum sailent region ) , stochastic sampling of sailent patches , and sailency-based label mixing . The results show clear improvements over competitors , and an ablation study shows the performance gains of each of the design choices discussed previously . The paper is pleasant to read and all decissions are properly motivated . The main contributions are fairly novel , the formulation sound and the experiments well designed . The claim of robustness of data corruption of the model seems like an over-statement . Even though the authors show that Sailency Grafting performs well under data corruption , one may attribute the robustness feature to the AugMix method that the grafting acts upon . The results of table 5. just show that grafting sailent corrupted image patches improves over just training on corrupted patches , which is something to expect given the previously reported improvements . Early in the paper the authors claim a difference wrt . previous work that is the uniform thresholding of the salient region rather than selecting the maximum , to mitigate selection bias . I missed a section in the ablation study demonstrating the specific improvement of this decision . Also the authors briefly mention the approach to select the thresholding as the mean of the normalized sailency map . I 'm curious to know if any other method of threshold selection ( or patch sampling ) has been explored .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thoughtful and constructive feedback . # # # [ Q1 : Claiming robustness ( Table 5 of the main paper ) is an overstatement ( it is due to AugMix ) . It just shows that grafting salient corrupted image patches improves over just training on corrupted patches , which is something to expect given the previously reported improvements . ] We conducted additional experiments to measure the performance of various methods without AugMix . In this setting , results show that all methods exhibit lackluster performance , including Saliency Grafting . In this sense , we agree with R1 that our method can not guarantee significant robustness on its own . However , the intention behind our statement is that our method can be used on top of AugMix with no repercussions while generating positive synergy . Augmix , on its own , provides noise robustness but shows lackluster performance when it comes to boosting performance on clean data . When Saliency Grafting is used alongside Augmix , both clean data performance and corrupted data performance is increased beyond the numbers obtained by applying them individually ( synergy ) . We will properly update our statements in the revision . # # # [ Q2 : Ablation study for employing uniform thresholding of the salient region rather than selecting the maximum , to mitigate selection bias . ] Please see Section 5.3 ( Ablation Study ) in our original submission where we presented the comparison of deterministic selection VS stochastic selection . In this section , we take AttentiveCutMix , which employs a deterministic strategy of selecting top-k pixels from a saliency map . While keeping everything else identical , we change this deterministic selection strategy of AttentiveCutMix to our stochastic selection strategy ( thresholding + stochastic sampling ) . To keep the number of selected pixels approximately equal , we calibrated the softmax temperature so that * * * k * * * pixels are selected on average . Results in Table 6 show that * * Stochastic+area labels * * ( 2nd entry ) show better performance compared to * * Deterministic+area labels * * ( 1st entry ) . # # # [ Q3 : Curious to know if any other method of threshold selection ( or patch sampling ) has been explored ? ] We considered that it was better to select the threshold value using data statistics rather than thresholding by a fixed constant , so we adopted the expectation ( mean ) . It is possible to adjust the number of saliency regions with a value larger than the expectation through temperature . It also is possible to control the number of saliency regions with higher values than the expectation by adjusting the shape of the softmax distribution through temperature $ T $ . Moreover , we plan to conduct an experiment that tweaks the $ \\alpha $ of the beta distribution , which is the current patch sampling distribution , to evaluate the performance difference by making the beta distribution shape concave or convex . We sincerely thank you for the helpful suggestion ."}], "0": {"review_id": "jjKzfD9vP9-0", "review_text": "This paper advances the line of research in data augmentation following Mixup paper . Cutmix is a image-specific variant of mixup that pastes a rectangular region from a donor image to a target image ; however , it does this in a completely random fashion not paying attention to whether the discriminative parts of either image are retained . Recent methods alleviate this problem by using saliency guided region selection either preserving the most discriminative parts of the donor image ( Attentive Cutmix ) or both images ( PuzzleMix ) . However , in doing so they replace the random nature of Cutmix with a deterministic approach potentially reducing sample diversity . The first contribution of the paper is a saliency guide stochastic augmentation method which combines the best of both worlds . The second contribution is in the generation of more meaningful labels for the newly created image by taking into account the saliency of the regions used from both images rather than blindly using the mixing ratio . The contributions of the paper are somewhat incremental but are reasonable . The validity of using saliency in determining the label of the new sample is clear . The experimental results are mixed . Most experiments demonstrate minor improvements over previous methods . On the other hand the top-5 performance of CutMix is better than the proposed method in Table 2 . With such small differences and no standard deviations on accuracy reported it is not clear whether the improvements are really significant . In the data scarcity experiments , I find the statement \u201c Note that the performance of CutMix deteriorates as the number of data per class decreases due to their randomness occurring label mismatching. \u201d misleading . This is true of every method in that table not just CutMix , and the detoriation amounts are very similar . Clarity : Overall I found the paper well written and easy to read . The authors have provided standard errors which improves the confidence that the improvements in the experiments are significant .", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # [ Q2 : Most experiments demonstrate minor improvements over previous methods . With such small differences and no standard deviations on accuracy reported it is not clear whether the improvements are really significant . ] Thank you for your feedback . We report the standard errors of CIFAR-100 for PyramidNet-200 ( Table 2 in the main paper ) in Table A ( below ) . Also , in addition to the experiments in the main paper , we conducted 2 additional experiments to benchmark Saliency Grafting : TinyImageNet for ResNet-18 ( Table B ) and CIFAR100 for ResNet-18 ( Table C ) , and also report standard errors for both of them . In these experiments , Saliency Grafting * consistently * outperforms other methods . This performance gap is a major gain for a data augmentation method , as the gain is achieved not by changing architectural designs , which are the main powerhouse of performance , but by tweaking data processing schemes , which are small but effective finishing touches . We also observe that for all of these experiments , the standard errors are far from overlapping , conveying statistical significance . In the main paper , we omitted standard errors to align our experiments with previous papers , as they have not reported any standard errors . However , we agree with R3 that standard errors are necessary . Since the results reported in previous papers do not contain standard errors , we reproduced them using author codes and guidelines for repetition . However , for some methods , we were unable to reproduce the performance stated in their respective papers , albeit using the author \u2019 s code/hyperparameters and following their guidelines . Table A. Top-1/Top-5 errors on CIFAR-100 for PyramidNet-200 in comparison to state-of-the-art data augmentation methods . | Method | Top-1 Err . | Top-5 Err . | |-| : - : | : - : | | Saliency Grafting | * * 13.59 ( \u00b10.062 ) * * | * * 3.01 ( \u00b10.021 ) * * | | PuzzleMix | 16.52 ( \u00b10.066 ) | 3.70 ( \u00b10.065 ) | | CutMix | 14.51 ( \u00b10.149 ) | 3.08 ( \u00b10.024 ) | | Attentive CutMix | 15.24 ( \u00b10.085 ) | 3.46 ( \u00b10.061 ) | Table B. Top-1/Top-5 errors on TinyImageNet for ResNet-18 in comparison to state-of-the-art data augmentation methods . | Method | Top-1 Err . | Top-5 Err . | |-| : - : | : - : | | Saliency Grafting | * * 34.80 ( \u00b10.095 ) * * | * * 16.13 ( \u00b10.077 ) * * | | PuzzleMix | 36.67 ( \u00b10.081 ) | 16.92 ( \u00b10.074 ) | | CutMix | 36.13 ( \u00b10.064 ) | 16.20 ( \u00b10.042 ) | | Mixup | 39.01 ( \u00b10.153 ) | 19.56 ( \u00b10.052 ) | Table C. Top-1/Top-5 errors on CIFAR-100 for ResNet-18 in comparison to state-of-the-art data augmentation methods . | Method | Top-1 Err . | Top-5 Err . | |-| : - : | : - : | | Saliency Grafting | * * 20.39 ( \u00b10.074 ) * * | * * 5.80 ( \u00b10.052 ) * * | | PuzzleMix | 22.51 ( \u00b10.092 ) | 6.39 ( \u00b10.101 ) | | CutMix | 20.97 ( \u00b10.060 ) | 5.89 ( \u00b10.026 ) | | Mixup | 22.04 ( \u00b10.124 ) | 6.96 ( \u00b10.101 ) | # # # [ Q3 : Not only CutMix but all methods struggle against data scarcity . ] The reviewer misunderstood our intentions . Indeed , the performances of all methods degrade as the number of data per class decreases . However , the meaning we tried to convey was that CutMix suffers the most from data scarcity . In Table 4 of the main paper , we reported results ( top-1 error on CIFAR-100 ) for 3 data portions : 10 % , 20 % , and 50 % . When data scarcity is not severe ( 50 % ) , CutMix ( 28.73 ) performs slightly better than Mixup ( 28.97 ) and is relatively close to PuzzleMix ( 28.04 ) . However , as the amount of data decreases , the performance of CutMix degrades rapidly , displaying the worst performance ( lags behind all augmentation methods ) among the augmentation methods as we hit 20 % and 10 % data ( On 10 % data , the top-1 error of CutMix is 55.71 while Mixup is 51.86 and PuzzleMix is 52.63 ) . We will clarify the statement in the revision . We suspect that this is because CutMix carries an innate flaw that is * * label mismatch * * , due to the saliency-agnostic nature of its scheme . A previous study shows that label noise is more destructive when data is scarce ( Rolnick et al.2017 [ 1 ] ) . We suspect that this is why CutMix performs worse in data scarcity situations . PuzzleMix and Mixup incompletely address this problem by generating incorrectly calibrated labels . However , our method produces well-calibrated ( correct ) labels which are shown to be more resistant to data scarcity , with our method displaying the best performance . [ 1 ] https : //arxiv.org/abs/1705.10694"}, "1": {"review_id": "jjKzfD9vP9-1", "review_text": "The paper propose a novel method to address the lack of diversity in mixed samples created by the saliency-based Mixup strategies such as Puzzle Mix . To attain this goal , the proposed approach incorporates a random process in the saliency regions selection process to give all salient regions equal chance , and accordingly adjust the modeling target to match the saliency of the mixed image . The paper presents an interesting extension to the existing work . Nevertheless , I would like to see more evidence of why the existing methods lack of diversity and how it negatively impacts the model \u2019 s performance . I also think the experiments in the current form is weak . My major comments are as follows . Regarding the proposed method : 1 . The motivation of generating diverse mixed images through selecting diverse salient features is interesting to me . But I think the current justification is weak . It is true that the Puzzle Mix will select saliency regions with high intensity , but I wonder if the random mixing ratio \\lambda in PuzzleMix would help to generate diverse mixed samples due to the random sampling process . Another factor for the diversity here is that even using similar saliency regions of the input pair for the mixed images , the mixed labels are different due to the random selection of the mixing ratio . In this sense , I think how severe this lack of diversity issue is and how it negatively impacts the performance of the mode are not very clear to me . It would be very beneficial if the paper could show it somehow . 2.The way that the proposed method generates the mixed modeling target for the mixed input reminds me of this paper : nonlinear mixup : out-of-manifold data augmentation for text classification ( AAAI2020 ) . In the nonlinear Mixup method , the mixed label for a mixed input in Mixup is also computed based on the input pair , although the method is evaluated using text classification tasks . I think it would be useful to discuss the difference and adjust the content such as Table1 and Section3 to make the statements in the paper more accurate . 3.Some parts in the proposed method deserve further discussion and justification . For example , the Threshold for the normalized saliency map . How sensitive is the threshold value to the performance of the proposed model ? It would be useful to include some analysis either theoretically or experimentally . Regarding the experiments : 1 . I am surprised to see from Table2 that the PuzzleMix degraded the performance of the baseline model . I wonder why this happened . In this sense , I would like to see experiments with some other benchmarking datasets such as Cifar10 , SVHN , or TinyImagenet . Also , for these experiments , it would be useful to also provide the deviation of the 3 runs . 2.For Cifar100 using WRN28-10 ( also for ImageNet with ResNet-50 ) , the difference between the proposed method and the PuzzleMix is very small . This suggests that it would useful to experiment on other network architectures such as ResNet-18 or ResNet-50 , which are used in the original Mixup and the PuzzleMix papers . Minor issues : 1 . The paper discusses the harmful samples potentially generated by Mixup in several places without citation . I think it would be useful to include some citations such as the manifold intrusion issue raised in the AdaMixup paper ( AAAI2019 ) or the noise image issue as discussed in the PuzzleMix paper ( ICML2020 ) . In short , I think the paper would benefit from further justifying the problem of lack of mixed sample diversity in the PuzzleMix . Also , the current experimental results show minor improvement over PuzzleMix , which also makes the contribution of the paper less significant .", "rating": "5: Marginally below acceptance threshold", "reply_text": "# # # [ Q5.Experiments on other benchmarking datasets ] To address the reviewer 's concern , we evaluated our method on another benchmark dataset - TinyImageNet . We trained a ResNet-18 for 600 epochs with an SGD+momentum optimizer , following the one of TinyImageNet experimental settings in the PuzzleMix paper . Other data augmentation methods including PuzzleMix are evaluated using the released code and authors \u2019 hyperparameters . The obtained results are shown in Table C. Like the experiments contained in the main paper , Saliency Grafting consistently exhibits the best performance on this new benchmark dataset . However , PuzzleMix falls behind CutMix . Table C. Top-1/Top-5 errors on TinyImageNet for ResNet-18 in comparison to state-of-the-art data augmentation methods . | Method | Top-1 Err . | Top-5 Err . | |-| : - : | : - : | | Saliency Grafting | * * 34.80 ( \u00b10.095 ) * * | * * 16.13 ( \u00b10.077 ) * * | | PuzzleMix | 36.67 ( \u00b10.081 ) | 16.92 ( \u00b10.074 ) | | CutMix | 36.13 ( \u00b10.064 ) | 16.20 ( \u00b10.042 ) | | Mixup | 39.01 ( \u00b10.153 ) | 19.56 ( \u00b10.052 ) | # # # [ Q6 . CIFAR 100 experiment on other network architectures ] In response to the reviewer ` s constructive suggestion , we assess our method on CIFAR-100 using another model architecture , ResNet18 . We trained the network for 300 epochs with an initial learning rate of 0.1 and decayed the learning rate at 150 , 225 epoch . Other data augmentation methods including PuzzleMix are evaluated using the released code and authors \u2019 hyperparameters . The results are displayed in Table D. In this architecture setting , Saliency Grafting again shows the best performance . However , PuzzleMix falls behind Mixup and CutMix . Table D. Top-1/Top-5 errors on CIFAR-100 for ResNet-18 in comparison to state-of-the-art data augmentation methods . | Method | Top-1 Err . | Top-5 Err . | |-| : - : | : - : | | Saliency Grafting | * * 20.39 ( \u00b10.074 ) * * | * * 5.80 ( \u00b10.052 ) * * | | PuzzleMix | 22.51 ( \u00b10.092 ) | 6.39 ( \u00b10.101 ) | | CutMix | 20.97 ( \u00b10.060 ) | 5.89 ( \u00b10.026 ) | | Mixup | 22.04 ( \u00b10.124 ) | 6.96 ( \u00b10.101 ) | # # # [ Q7 . It would be useful to include some citations such as the manifold intrusion issue raised in the AdaMixup paper ( AAAI2019 ) ] Thank you for the helpful suggestion . This paper addressed the `` manifold intrusion '' problem , which is a mismatch problem between the mixup sample and the corresponding soft label . We agree that citing this interesting paper would be beneficial , and will add the references properly in the revision . * * Updated * * We 've cited this novel work in our introduction section ."}, "2": {"review_id": "jjKzfD9vP9-2", "review_text": "The authors propose a novel sailency-guided data augmentation method that alleviates some of the drawbacks arising with recent Mixup-based augmentation approaches . Specifically , the authors propose sailency thresholding for region selection ( instead of maximum sailent region ) , stochastic sampling of sailent patches , and sailency-based label mixing . The results show clear improvements over competitors , and an ablation study shows the performance gains of each of the design choices discussed previously . The paper is pleasant to read and all decissions are properly motivated . The main contributions are fairly novel , the formulation sound and the experiments well designed . The claim of robustness of data corruption of the model seems like an over-statement . Even though the authors show that Sailency Grafting performs well under data corruption , one may attribute the robustness feature to the AugMix method that the grafting acts upon . The results of table 5. just show that grafting sailent corrupted image patches improves over just training on corrupted patches , which is something to expect given the previously reported improvements . Early in the paper the authors claim a difference wrt . previous work that is the uniform thresholding of the salient region rather than selecting the maximum , to mitigate selection bias . I missed a section in the ablation study demonstrating the specific improvement of this decision . Also the authors briefly mention the approach to select the thresholding as the mean of the normalized sailency map . I 'm curious to know if any other method of threshold selection ( or patch sampling ) has been explored .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thoughtful and constructive feedback . # # # [ Q1 : Claiming robustness ( Table 5 of the main paper ) is an overstatement ( it is due to AugMix ) . It just shows that grafting salient corrupted image patches improves over just training on corrupted patches , which is something to expect given the previously reported improvements . ] We conducted additional experiments to measure the performance of various methods without AugMix . In this setting , results show that all methods exhibit lackluster performance , including Saliency Grafting . In this sense , we agree with R1 that our method can not guarantee significant robustness on its own . However , the intention behind our statement is that our method can be used on top of AugMix with no repercussions while generating positive synergy . Augmix , on its own , provides noise robustness but shows lackluster performance when it comes to boosting performance on clean data . When Saliency Grafting is used alongside Augmix , both clean data performance and corrupted data performance is increased beyond the numbers obtained by applying them individually ( synergy ) . We will properly update our statements in the revision . # # # [ Q2 : Ablation study for employing uniform thresholding of the salient region rather than selecting the maximum , to mitigate selection bias . ] Please see Section 5.3 ( Ablation Study ) in our original submission where we presented the comparison of deterministic selection VS stochastic selection . In this section , we take AttentiveCutMix , which employs a deterministic strategy of selecting top-k pixels from a saliency map . While keeping everything else identical , we change this deterministic selection strategy of AttentiveCutMix to our stochastic selection strategy ( thresholding + stochastic sampling ) . To keep the number of selected pixels approximately equal , we calibrated the softmax temperature so that * * * k * * * pixels are selected on average . Results in Table 6 show that * * Stochastic+area labels * * ( 2nd entry ) show better performance compared to * * Deterministic+area labels * * ( 1st entry ) . # # # [ Q3 : Curious to know if any other method of threshold selection ( or patch sampling ) has been explored ? ] We considered that it was better to select the threshold value using data statistics rather than thresholding by a fixed constant , so we adopted the expectation ( mean ) . It is possible to adjust the number of saliency regions with a value larger than the expectation through temperature . It also is possible to control the number of saliency regions with higher values than the expectation by adjusting the shape of the softmax distribution through temperature $ T $ . Moreover , we plan to conduct an experiment that tweaks the $ \\alpha $ of the beta distribution , which is the current patch sampling distribution , to evaluate the performance difference by making the beta distribution shape concave or convex . We sincerely thank you for the helpful suggestion ."}}