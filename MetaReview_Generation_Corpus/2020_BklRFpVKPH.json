{"year": "2020", "forum": "BklRFpVKPH", "title": "Demonstration Actor Critic", "decision": "Reject", "meta_review": "The paper proposes to combine RL and Imitation Learning. It defines a regularized reward function that minimizes the KL distance between the policy and the expert action. The formulation is similar to the KL regularized MDPs, but with the difference that an additional indicator function based on the support of the expert\u2019s distribution is multiplied to the regularized term.\n\nSeveral issues have been brought up by the reviewers, including:\n* Comparison with pre-deep learning literature on the combination of RL and imitation learning\n* Similarity to regularized MDP framework\n* Assumption 1 requiring a stochastic expert policy, contradicting the policy invariance claim\n* Difficulty of learning the indicator function of the support of the expert\u2019s data distribution\n\nSome of these issues have been addressed, but at the end of the day, one of the expert reviewers was not convinced that the problem of learning an indicator function is going to be easy at all. The reviewer believes that learning such a function requires \"learning a harsh approximation of the density of visits of the expert for every state which is a quite hard task, especially in stochastic environments.\u201d \n\nAnother issue is related to the policy invariance under the optimal expert policy. In most MDPs, the optimal policy is not stochastic and does not satisfy Assumption 1, so the optimal policy invariance proof seems to contradict Assumption 1.\n\nOverall, it seems that even though this might become a good paper, it requires some improvements. I encourage the authors to address the reviewers\u2019 comments as much as possible.", "reviews": [{"review_id": "BklRFpVKPH-0", "review_text": "This paper presents a method for doing RL from demonstrations in continuous control tasks. It combines both an augmented reward for minimizing the KL between the policy and the expert actions as well as directly minimizing that KL in the policy. Results on 5 sparse reward mujoco tasks show that it out-performs other related methods. The motivation for the paper is difficult to follow. They claim that using demonstration data in a supervised manner \"cannot generalize supervision signal over those states unseen in the demonstrations,\" but most of these approaches are using neural networks and definitely are generalizing those signals to other states. Whether they're generalizing accurately or not is a different question. In contrast, they say that reward shaping approaches do not suffer that problem because they evaluate trajectories rather than states, but there will still be a problem of generalizing to new trajectories. The abstract is even more confusing as it tries to jump straight into the issues with these approaches without any explanation. I don't think there's enough space in the abstract to go into that level of detail. The description of DQfD and DDPGfD in the related work is not accurate. They're described as \"treating demonstration data as self-generated data,\" but in fact they both add supervised losses to more closely match the demonstrated data. https://ieeexplore.ieee.org/document/8794074 is another method built on DDPG that has both a critic and actor loss like yours and would make a useful comparison. The related work section should also discuss and compare/contrast to GAIL, I was surprised that wasn't in there, especially since you also use a discriminator to differentiate expert and agent actions. The end of the related work section is not very clear, you say these methods are problematic because \"the adopted shaping reward yields no direct dependence on the current policy\" but there's no explanation or motivation for why that would be a problem. Assumption 1 seems like a very strong assumption that would not be true for many human experts. For the experiments, I wonder about the impact of only using sparse reward tasks. Converting the tasks to sparse reward in this way makes them partially observable, and then potentially the expert demonstrations are required to overcome that partial observability. How do the methods compare on the unmodified tasks? There was nothing specific in your algorithm that meant it should specifically address sparse reward tasks. What about tasks that are naturally sparse reward? Overall, the algorithm is interesting and the results are nice. The motivation and related work need to be made clearer to situate this work with the other related works. And the experiments should go beyond these tasks that have been modified to have sparse rewards. The revised version of the paper addresses many of my concerns about the motivation, related works, and comparisons with GAIL, so I'm updating my score to Weak Accept. ", "rating": "6: Weak Accept", "reply_text": "Thanks so much for your comments and suggestions ! In the following context , we will give response to every question you have . Please let us know if you have any question on our response . Summary : our response includes : ( 1 ) Clarification on our motivation ; ( 2 ) Clarification on related works ; ( 3 ) Clarification on our assumption ; ( 3 ) Explanations of experiment environment . * * Clarification on our motivation * * We are sorry that we should give a clearer description of the motivation of our work . We give a new description here : We study the problem of RLfD , where both reward signals and expert demonstrations are given . One approach leverages demonstration data in a supervised manner . Though simple and direct , such approach can only provide accurate supervision signal over those states seen in the demonstrations [ 1 , 2 , 5 ] . To address this issue , another approach uses demonstration data for reward shaping , and can provide guidance on how to take actions , even for those states are not seen in the demonstrations . Specifically , such reward shaping approach trains an agent not only to imitate demonstrated actions when it encounters demonstrated states , but also to reach demonstrates states ( states visited by the expert strategy ) , when it confronts states that are not observed in the demonstration data [ 3 , 4 , 5 ] . This is the core idea behind such reshaping reward based approach . However , since the new adopted shaping reward yields no direct dependence on the current policy , such approach , updating policy over demonstrated states in the same way as other states by the reshaped value function , overlook the validity of such direct supervision for demonstrated states during training . In this paper , we delicately design a policy dependent shaping reward : $ 1_ { s \\in supp { \\rho_ { \\pi_E } ( s ) } } \\cdot ( M \u2013 D_ { KL } ( \\pi_ { \\theta } ( \\cdot|s ) , \\pi_ { E } ( \\cdot|s ) ) ) $ , in order to provide both guidance for all states , as well as direct supervision for demonstrated states . The purpose of designing the indicator function of $ supp { \\rho_ { \\pi_ { E } ( s ) } } $ and $ M $ is to assign positive reward for demonstrated states , while assign zero reward for non-demonstrated states , which will encourage the agent to reach demonstrated states . And the purpose of designing $ D_ { KL } ( \\pi_ { \\theta } ( \\cdot|s ) , \\pi_ { E } ( \\cdot|s ) ) $ is to encourage the agent to directly imitate the demonstrated actions over these demonstrated states . In particular , optimizing the objective with such policy dependent shaping reward will directly lead to a new update rule , as shown in Eqn . 8 in Section 4.1 , consisting two key parts : 1 ) encourage the agent to obtain more cumulative rewards and reach the demonstrated states as much as possible . 2 ) determine whether current state belongs to demonstrated states by the indicator function $ 1_ { s \\in supp { \\rho_ { \\pi_E } ( s ) } } $ , if so , leverage the direct supervision signals over current demonstrated state . Following your suggestion , we have updated the introduction and abstract section in our paper ."}, {"review_id": "BklRFpVKPH-1", "review_text": "Summary: This paper studied reinforcement learning from demonstration. Given a set of expert demonstrations, this work provides a policy-dependent reward shaping objective that can utilize demonstration information and preserves policy optimality, policy improvement, and the convergence of policy iteration at the same time, under the assumption that expert policy is optimal and stochastic. The main advantage of the proposed method is that the reward shaping function is related to the current policy. A practical algorithm based on theoretical derivation is provided. The authors conducted sufficient experimental results to demonstrate that the proposed method is effective, comparing with a set of advanced baselines. I recommend acceptance: Previous works on RLfD usually empirically incorporated a regularization to the RL objective, while those works didn't discuss whether this regularization will lead to sub-optimal policy or not. This paper discussed how to use the demonstration information to do exploration and maintain policy invariance at the same time, with a relatively strong assumption. Using the framework from SAC, the algorithm is shown to converge to the optimal via policy iteration, in tabular case. This work also developed a practical expert policy support estimation algorithm to measure the uncertainty of expert policy. Utilizing the adversarial training framework, the explicit computation of expert policy is avoided. The authors conducted sufficient experiments to demonstrate the effectiveness of the proposed method, compared with the state-of-the-art in RLfD. Technical concerns: The stochasticity assumption of expert policy in Asm. 1 can be contradicted with that expert policy is optimal in policy invariance proof. This paper works on a problem of infinite horizon discounted MDP. According to Puterman [1994], Theorem 6.2.7, there always exists a deterministic stationary policy \\pi that is optimal. Or intuitively, if we find the optimal value function via Bellman optimality equation, the optimal policy is acting greedily (deterministic). The provided theorems are not compatible with the MDP where only deterministic optimal policy exists. It is not clear that in what type of MDPs the optimal stochastic policy exists and it can satisfy Asm. 1. Could the authors clearly specify the applicable problem settings? If the asm 1 is satisfied, what is the necessity to incorporate the indicator function in Eq 4.? Since p(s) > 0 for all s, following strong stochasticity policy. For any trajectory \\tau, p(\\tau) = p(s)\\Pi_t p(s_{t+1}|s_{t}, a_t)\\pi_E(a_t|s_t) > 0. The proof of Theorem 2 is similar to the proof in Proposition 1, [1], though in a different context. It would be better to have a citation? Experiments: It would be more convincing to show the performance of behavior cloning policy using expert trajectories. [1] Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems. 2014. ", "rating": "6: Weak Accept", "reply_text": "Thank you so much for your supportive and constructive comments ! we will give response to every question you have . Please let us know if you have any question on our response . Summary : our response includes : ( 1 ) Discussion on stochasticity assumption . ( 2 ) Proof of theorem 2 ( 3 ) Performance of behavior cloning policy . First of all , we really appreciate your affirmation of our efforts to investigate the optimal policy invariance and others ! * * Discussion on stochasticity assumption * * Thanks a lot for your careful and insightful thought here . The provided theorems are indeed only compatible with the MDP where the optimal stochastic policy exists and satisfy Asm . 1 now , due to the stochasticity assumption . To better generalize our theory part into more general MDPs ( e.g.only deterministic optimal policy exists ) you mentioned , one potential answer we provided is to replace the KL divergence with some naturally-bounded divergences ( e.g.Jensen-Shannon divergence , satisfying that $ 0 \\le JSD ( p ||q ) \\le 1 $ given the base 2 logarithm ) , since essentially the primary purpose of stochasticity assumption is to ensure our policy-dependent shaping reward , which includes KL divergence term , is bounded . In this case , we will no longer need to introduce stochasticity assumption , and the theory part can be more close to the practical cases ( e.g.only deterministic optimal policy exists ) . Considering some potential further modifications to policy evaluation/policy improvement parts ( from KL towards JS ) , we leave this into our future work . It \u2019 s true that if the Asm . 1 is satisfied , the indicator function is unnecessary . But empirically , the indicator function is very useful , especially when demonstration states often only cover a part of the whole state space . Intuitively speaking , such indicator function aims to encourage the agent to visit the demonstrated states ( states visited by the expert strategy ) . * * Proof of theorem 2 * * Thanks for your kind reminder , the proof of our theorem 2 is indeed inspired by that of Proposition 1 in Generative Adversarial Nets ( GANs ) . We cited the GAN paper above the theorem 2 in Section 4.2 , and we are willing to refer to it again in our proof part for clearer illustration ( presented in the C.4 Theorem 2 part in Appendix ) . * * Performance of behavior cloning policy * * Thanks for your advice , and we will also add the performance of behavior cloning policy using expert trajectories in our camera-ready version . We welcome further discussion and are willing to answer any further questions . Thanks for your time and valuable feedbacks ."}, {"review_id": "BklRFpVKPH-2", "review_text": "This paper proposes to mix reinforcement learning and imitation learning to boost the learning of an actor critic architecture. The authors use a regularized reward function that minimizes the divergence between the policy of the expert and the one followed by the agent. They use demonstrations obtained from a trained agent and experiment their method on several mujoco tasks. I have many concerns about this paper. First, The state of the art is missing important pre-deep-learning references such as: 1. Direct Policy Iteration with Demonstrations: Chemali and Lazaric 2. Learning from limited demonstrations, Beomjoon et al. 3. Residual Minimization Handling Expert Demonstrations, Piot et al. Then, they make a mistake by saying that DQfD only considers transitions from the expert as self-generated and placed in the replay buffer. DQfD actually uses the same additional structured classification loss than Piot et al. [3] (except that they use boosted trees instead of deep networks, DQfD and Piot et al. are the same algorithm). Also, the proposed solution here is equivalent to regularizing the MDP with a KL divergence w.r.t. to an initial policy that would be the one of the expert. It is already studied in several works and more generally it comes with some assumptions on the policy update. It is generally studied in 4. A theory of regularized MDP, Geist et al They actually propose exactly the same framework as a special case in the appendix of that paper. In addition to not be very novel, I think the method has some flaws. The authors use demonstrations coming from a pre-trained network which is known to make the imitation learning part much easier. Especially if it comes from an RL agent using similar deep RL algorithms (which is the case here). Finally, they only test on mujoco tasks which are very specific tasks with deterministic dynamics and very dense rewards around states visited by the optimal strategy so initializing with an expert policy that is learned from demonstrations of a similar network of course helps. I would be more impressed by experiments on stochastic environments and sparse rewards. Finally, there is a concurrent work submitted to the same conference. Of course the authors could not know but I\u2019d like to have their impression about how their work is different. https://openreview.net/forum?id=BJg9hTNKPH&noteId=BJg9hTNKPH", "rating": "1: Reject", "reply_text": "Thanks so much for your review and detailed comments . In the following parts , we will give responses for each concern you have . Please let us know if you have any question on our response . Our response generally includes : ( 1 ) Missing important pre-deep-learning references ; ( 2 ) Clarification on DQfD method ; ( 3 ) The concern of novelty ; ( 4 ) Explanation of experiment details ; ( 5 ) Discussion on the concurrent work BRAC . * * Missing important pre-deep-learning references * * We are sorry that we missed some important pre-deep-learning works you mentioned . These methods are indeed relevant and we have added reference to them in our updated paper ( presented in the second paragraph of Section 2 ) . * * Clarification on DQfD method * * We apologize for our mistake in DQfD part . We have updated the previous statement , as shown in our updated paper ( presented in the second paragraph of Section 2 ) . Thanks for pointing it out ! * * The concern of novelty * * To clarify the novelty of our work , we start with our motivation : 1 . We study the problem of RLfD , where both reward signals and expert demonstrations are given . One approach leverages demonstration data in a supervised manner . Though simple and direct , such approach can only provide supervision signal over those states seen in the demonstrations . To address this issue , another approach uses demonstration data for reward shaping . Such approach trains an agent not only to imitate demonstrated actions over these demonstrated states , but also to reach the demonstrated states ( states visited by the expert strategy ) , when it confronts states that are unseen in the demonstration data [ 1 , 2 , 3 ] . This is the core idea behind such reshaping reward based approach . However , since the new adopted shaping reward yields no direct dependency on the current policy , such approach , updating policy over demonstrated states in the same way as other states by the reshaped value function , overlooks the validity of such direct supervision for demonstrated states during training . 2.In this paper , we delicately design a policy dependent shaping reward : $ 1_ { s \\in supp { \\rho_ { \\pi_E } ( s ) } } \\cdot ( M \u2013 D_ { KL } ( \\pi_ { \\theta } ( \\cdot|s ) , \\pi_ { E } ( \\cdot|s ) ) ) $ , in order to provide both guidance for all states , as well as direct supervision for demonstrated states . The purpose of designing the indicator function of $ supp { \\rho_ { \\pi_ { E } ( s ) } } $ and $ M $ is to assign positive reward for demonstrated states , while assign zero reward for non-demonstrated states , which will encourage the agent to reach demonstrated states . And the purpose of designing $ D_ { KL } ( \\pi_ { \\theta } ( \\cdot|s ) , \\pi_ { E } ( \\cdot|s ) ) $ is to encourage the agent to directly imitate the demonstrated actions over these demonstrated states . In particular , optimizing the objective with such policy dependent shaping reward will directly lead to a new update rule , as shown in Eqn . 8 in Section 4.1 , consisting two key parts : 1 ) encourage the agent to obtain more cumulative rewards and reach the demonstrated states as much as possible . 2 ) determine whether current state belongs to demonstrated states by the indicator function $ 1_ { s \\in supp { \\rho_ { \\pi_E } ( s ) } } $ , if so , leverage the direct supervision signals over current demonstrated state . By contrast , although very general , these Regularized MDP methods only regularize MDP with a KL divergence term , and are not very suitable for the RLfD problem . These methods do not encourage the agent to reach demonstrated states ( states visited by the expert strategy ) explicitly , but it is a very important unique property of RLfD problem itself [ 1 , 2 , 3 ] . Besides , most of Regularizing MDP works assume that the explicit expression of initial policy is available , but in our case , we can only access to expert demonstrations . To this end , we use the GAN technique to avoid the usage of explicit expression of $ \\pi_ { E } $ , and take advantage of support estimation techniques to estimate the indicator function of $ supp { \\rho_ { \\pi_ { E } ( s ) } } $ from expert demonstrations , which also leads to another obvious difference . Although there exists a significant difference between the scenario of our work and that of some Regularized MDP works , our method is indeed formally similar to these works . And we are very willing to add these Regularizing MDP works into our paper , and discuss their relationship and difference with our method ( please refer to the last paragraph of Section 2 ) ."}], "0": {"review_id": "BklRFpVKPH-0", "review_text": "This paper presents a method for doing RL from demonstrations in continuous control tasks. It combines both an augmented reward for minimizing the KL between the policy and the expert actions as well as directly minimizing that KL in the policy. Results on 5 sparse reward mujoco tasks show that it out-performs other related methods. The motivation for the paper is difficult to follow. They claim that using demonstration data in a supervised manner \"cannot generalize supervision signal over those states unseen in the demonstrations,\" but most of these approaches are using neural networks and definitely are generalizing those signals to other states. Whether they're generalizing accurately or not is a different question. In contrast, they say that reward shaping approaches do not suffer that problem because they evaluate trajectories rather than states, but there will still be a problem of generalizing to new trajectories. The abstract is even more confusing as it tries to jump straight into the issues with these approaches without any explanation. I don't think there's enough space in the abstract to go into that level of detail. The description of DQfD and DDPGfD in the related work is not accurate. They're described as \"treating demonstration data as self-generated data,\" but in fact they both add supervised losses to more closely match the demonstrated data. https://ieeexplore.ieee.org/document/8794074 is another method built on DDPG that has both a critic and actor loss like yours and would make a useful comparison. The related work section should also discuss and compare/contrast to GAIL, I was surprised that wasn't in there, especially since you also use a discriminator to differentiate expert and agent actions. The end of the related work section is not very clear, you say these methods are problematic because \"the adopted shaping reward yields no direct dependence on the current policy\" but there's no explanation or motivation for why that would be a problem. Assumption 1 seems like a very strong assumption that would not be true for many human experts. For the experiments, I wonder about the impact of only using sparse reward tasks. Converting the tasks to sparse reward in this way makes them partially observable, and then potentially the expert demonstrations are required to overcome that partial observability. How do the methods compare on the unmodified tasks? There was nothing specific in your algorithm that meant it should specifically address sparse reward tasks. What about tasks that are naturally sparse reward? Overall, the algorithm is interesting and the results are nice. The motivation and related work need to be made clearer to situate this work with the other related works. And the experiments should go beyond these tasks that have been modified to have sparse rewards. The revised version of the paper addresses many of my concerns about the motivation, related works, and comparisons with GAIL, so I'm updating my score to Weak Accept. ", "rating": "6: Weak Accept", "reply_text": "Thanks so much for your comments and suggestions ! In the following context , we will give response to every question you have . Please let us know if you have any question on our response . Summary : our response includes : ( 1 ) Clarification on our motivation ; ( 2 ) Clarification on related works ; ( 3 ) Clarification on our assumption ; ( 3 ) Explanations of experiment environment . * * Clarification on our motivation * * We are sorry that we should give a clearer description of the motivation of our work . We give a new description here : We study the problem of RLfD , where both reward signals and expert demonstrations are given . One approach leverages demonstration data in a supervised manner . Though simple and direct , such approach can only provide accurate supervision signal over those states seen in the demonstrations [ 1 , 2 , 5 ] . To address this issue , another approach uses demonstration data for reward shaping , and can provide guidance on how to take actions , even for those states are not seen in the demonstrations . Specifically , such reward shaping approach trains an agent not only to imitate demonstrated actions when it encounters demonstrated states , but also to reach demonstrates states ( states visited by the expert strategy ) , when it confronts states that are not observed in the demonstration data [ 3 , 4 , 5 ] . This is the core idea behind such reshaping reward based approach . However , since the new adopted shaping reward yields no direct dependence on the current policy , such approach , updating policy over demonstrated states in the same way as other states by the reshaped value function , overlook the validity of such direct supervision for demonstrated states during training . In this paper , we delicately design a policy dependent shaping reward : $ 1_ { s \\in supp { \\rho_ { \\pi_E } ( s ) } } \\cdot ( M \u2013 D_ { KL } ( \\pi_ { \\theta } ( \\cdot|s ) , \\pi_ { E } ( \\cdot|s ) ) ) $ , in order to provide both guidance for all states , as well as direct supervision for demonstrated states . The purpose of designing the indicator function of $ supp { \\rho_ { \\pi_ { E } ( s ) } } $ and $ M $ is to assign positive reward for demonstrated states , while assign zero reward for non-demonstrated states , which will encourage the agent to reach demonstrated states . And the purpose of designing $ D_ { KL } ( \\pi_ { \\theta } ( \\cdot|s ) , \\pi_ { E } ( \\cdot|s ) ) $ is to encourage the agent to directly imitate the demonstrated actions over these demonstrated states . In particular , optimizing the objective with such policy dependent shaping reward will directly lead to a new update rule , as shown in Eqn . 8 in Section 4.1 , consisting two key parts : 1 ) encourage the agent to obtain more cumulative rewards and reach the demonstrated states as much as possible . 2 ) determine whether current state belongs to demonstrated states by the indicator function $ 1_ { s \\in supp { \\rho_ { \\pi_E } ( s ) } } $ , if so , leverage the direct supervision signals over current demonstrated state . Following your suggestion , we have updated the introduction and abstract section in our paper ."}, "1": {"review_id": "BklRFpVKPH-1", "review_text": "Summary: This paper studied reinforcement learning from demonstration. Given a set of expert demonstrations, this work provides a policy-dependent reward shaping objective that can utilize demonstration information and preserves policy optimality, policy improvement, and the convergence of policy iteration at the same time, under the assumption that expert policy is optimal and stochastic. The main advantage of the proposed method is that the reward shaping function is related to the current policy. A practical algorithm based on theoretical derivation is provided. The authors conducted sufficient experimental results to demonstrate that the proposed method is effective, comparing with a set of advanced baselines. I recommend acceptance: Previous works on RLfD usually empirically incorporated a regularization to the RL objective, while those works didn't discuss whether this regularization will lead to sub-optimal policy or not. This paper discussed how to use the demonstration information to do exploration and maintain policy invariance at the same time, with a relatively strong assumption. Using the framework from SAC, the algorithm is shown to converge to the optimal via policy iteration, in tabular case. This work also developed a practical expert policy support estimation algorithm to measure the uncertainty of expert policy. Utilizing the adversarial training framework, the explicit computation of expert policy is avoided. The authors conducted sufficient experiments to demonstrate the effectiveness of the proposed method, compared with the state-of-the-art in RLfD. Technical concerns: The stochasticity assumption of expert policy in Asm. 1 can be contradicted with that expert policy is optimal in policy invariance proof. This paper works on a problem of infinite horizon discounted MDP. According to Puterman [1994], Theorem 6.2.7, there always exists a deterministic stationary policy \\pi that is optimal. Or intuitively, if we find the optimal value function via Bellman optimality equation, the optimal policy is acting greedily (deterministic). The provided theorems are not compatible with the MDP where only deterministic optimal policy exists. It is not clear that in what type of MDPs the optimal stochastic policy exists and it can satisfy Asm. 1. Could the authors clearly specify the applicable problem settings? If the asm 1 is satisfied, what is the necessity to incorporate the indicator function in Eq 4.? Since p(s) > 0 for all s, following strong stochasticity policy. For any trajectory \\tau, p(\\tau) = p(s)\\Pi_t p(s_{t+1}|s_{t}, a_t)\\pi_E(a_t|s_t) > 0. The proof of Theorem 2 is similar to the proof in Proposition 1, [1], though in a different context. It would be better to have a citation? Experiments: It would be more convincing to show the performance of behavior cloning policy using expert trajectories. [1] Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems. 2014. ", "rating": "6: Weak Accept", "reply_text": "Thank you so much for your supportive and constructive comments ! we will give response to every question you have . Please let us know if you have any question on our response . Summary : our response includes : ( 1 ) Discussion on stochasticity assumption . ( 2 ) Proof of theorem 2 ( 3 ) Performance of behavior cloning policy . First of all , we really appreciate your affirmation of our efforts to investigate the optimal policy invariance and others ! * * Discussion on stochasticity assumption * * Thanks a lot for your careful and insightful thought here . The provided theorems are indeed only compatible with the MDP where the optimal stochastic policy exists and satisfy Asm . 1 now , due to the stochasticity assumption . To better generalize our theory part into more general MDPs ( e.g.only deterministic optimal policy exists ) you mentioned , one potential answer we provided is to replace the KL divergence with some naturally-bounded divergences ( e.g.Jensen-Shannon divergence , satisfying that $ 0 \\le JSD ( p ||q ) \\le 1 $ given the base 2 logarithm ) , since essentially the primary purpose of stochasticity assumption is to ensure our policy-dependent shaping reward , which includes KL divergence term , is bounded . In this case , we will no longer need to introduce stochasticity assumption , and the theory part can be more close to the practical cases ( e.g.only deterministic optimal policy exists ) . Considering some potential further modifications to policy evaluation/policy improvement parts ( from KL towards JS ) , we leave this into our future work . It \u2019 s true that if the Asm . 1 is satisfied , the indicator function is unnecessary . But empirically , the indicator function is very useful , especially when demonstration states often only cover a part of the whole state space . Intuitively speaking , such indicator function aims to encourage the agent to visit the demonstrated states ( states visited by the expert strategy ) . * * Proof of theorem 2 * * Thanks for your kind reminder , the proof of our theorem 2 is indeed inspired by that of Proposition 1 in Generative Adversarial Nets ( GANs ) . We cited the GAN paper above the theorem 2 in Section 4.2 , and we are willing to refer to it again in our proof part for clearer illustration ( presented in the C.4 Theorem 2 part in Appendix ) . * * Performance of behavior cloning policy * * Thanks for your advice , and we will also add the performance of behavior cloning policy using expert trajectories in our camera-ready version . We welcome further discussion and are willing to answer any further questions . Thanks for your time and valuable feedbacks ."}, "2": {"review_id": "BklRFpVKPH-2", "review_text": "This paper proposes to mix reinforcement learning and imitation learning to boost the learning of an actor critic architecture. The authors use a regularized reward function that minimizes the divergence between the policy of the expert and the one followed by the agent. They use demonstrations obtained from a trained agent and experiment their method on several mujoco tasks. I have many concerns about this paper. First, The state of the art is missing important pre-deep-learning references such as: 1. Direct Policy Iteration with Demonstrations: Chemali and Lazaric 2. Learning from limited demonstrations, Beomjoon et al. 3. Residual Minimization Handling Expert Demonstrations, Piot et al. Then, they make a mistake by saying that DQfD only considers transitions from the expert as self-generated and placed in the replay buffer. DQfD actually uses the same additional structured classification loss than Piot et al. [3] (except that they use boosted trees instead of deep networks, DQfD and Piot et al. are the same algorithm). Also, the proposed solution here is equivalent to regularizing the MDP with a KL divergence w.r.t. to an initial policy that would be the one of the expert. It is already studied in several works and more generally it comes with some assumptions on the policy update. It is generally studied in 4. A theory of regularized MDP, Geist et al They actually propose exactly the same framework as a special case in the appendix of that paper. In addition to not be very novel, I think the method has some flaws. The authors use demonstrations coming from a pre-trained network which is known to make the imitation learning part much easier. Especially if it comes from an RL agent using similar deep RL algorithms (which is the case here). Finally, they only test on mujoco tasks which are very specific tasks with deterministic dynamics and very dense rewards around states visited by the optimal strategy so initializing with an expert policy that is learned from demonstrations of a similar network of course helps. I would be more impressed by experiments on stochastic environments and sparse rewards. Finally, there is a concurrent work submitted to the same conference. Of course the authors could not know but I\u2019d like to have their impression about how their work is different. https://openreview.net/forum?id=BJg9hTNKPH&noteId=BJg9hTNKPH", "rating": "1: Reject", "reply_text": "Thanks so much for your review and detailed comments . In the following parts , we will give responses for each concern you have . Please let us know if you have any question on our response . Our response generally includes : ( 1 ) Missing important pre-deep-learning references ; ( 2 ) Clarification on DQfD method ; ( 3 ) The concern of novelty ; ( 4 ) Explanation of experiment details ; ( 5 ) Discussion on the concurrent work BRAC . * * Missing important pre-deep-learning references * * We are sorry that we missed some important pre-deep-learning works you mentioned . These methods are indeed relevant and we have added reference to them in our updated paper ( presented in the second paragraph of Section 2 ) . * * Clarification on DQfD method * * We apologize for our mistake in DQfD part . We have updated the previous statement , as shown in our updated paper ( presented in the second paragraph of Section 2 ) . Thanks for pointing it out ! * * The concern of novelty * * To clarify the novelty of our work , we start with our motivation : 1 . We study the problem of RLfD , where both reward signals and expert demonstrations are given . One approach leverages demonstration data in a supervised manner . Though simple and direct , such approach can only provide supervision signal over those states seen in the demonstrations . To address this issue , another approach uses demonstration data for reward shaping . Such approach trains an agent not only to imitate demonstrated actions over these demonstrated states , but also to reach the demonstrated states ( states visited by the expert strategy ) , when it confronts states that are unseen in the demonstration data [ 1 , 2 , 3 ] . This is the core idea behind such reshaping reward based approach . However , since the new adopted shaping reward yields no direct dependency on the current policy , such approach , updating policy over demonstrated states in the same way as other states by the reshaped value function , overlooks the validity of such direct supervision for demonstrated states during training . 2.In this paper , we delicately design a policy dependent shaping reward : $ 1_ { s \\in supp { \\rho_ { \\pi_E } ( s ) } } \\cdot ( M \u2013 D_ { KL } ( \\pi_ { \\theta } ( \\cdot|s ) , \\pi_ { E } ( \\cdot|s ) ) ) $ , in order to provide both guidance for all states , as well as direct supervision for demonstrated states . The purpose of designing the indicator function of $ supp { \\rho_ { \\pi_ { E } ( s ) } } $ and $ M $ is to assign positive reward for demonstrated states , while assign zero reward for non-demonstrated states , which will encourage the agent to reach demonstrated states . And the purpose of designing $ D_ { KL } ( \\pi_ { \\theta } ( \\cdot|s ) , \\pi_ { E } ( \\cdot|s ) ) $ is to encourage the agent to directly imitate the demonstrated actions over these demonstrated states . In particular , optimizing the objective with such policy dependent shaping reward will directly lead to a new update rule , as shown in Eqn . 8 in Section 4.1 , consisting two key parts : 1 ) encourage the agent to obtain more cumulative rewards and reach the demonstrated states as much as possible . 2 ) determine whether current state belongs to demonstrated states by the indicator function $ 1_ { s \\in supp { \\rho_ { \\pi_E } ( s ) } } $ , if so , leverage the direct supervision signals over current demonstrated state . By contrast , although very general , these Regularized MDP methods only regularize MDP with a KL divergence term , and are not very suitable for the RLfD problem . These methods do not encourage the agent to reach demonstrated states ( states visited by the expert strategy ) explicitly , but it is a very important unique property of RLfD problem itself [ 1 , 2 , 3 ] . Besides , most of Regularizing MDP works assume that the explicit expression of initial policy is available , but in our case , we can only access to expert demonstrations . To this end , we use the GAN technique to avoid the usage of explicit expression of $ \\pi_ { E } $ , and take advantage of support estimation techniques to estimate the indicator function of $ supp { \\rho_ { \\pi_ { E } ( s ) } } $ from expert demonstrations , which also leads to another obvious difference . Although there exists a significant difference between the scenario of our work and that of some Regularized MDP works , our method is indeed formally similar to these works . And we are very willing to add these Regularizing MDP works into our paper , and discuss their relationship and difference with our method ( please refer to the last paragraph of Section 2 ) ."}}