{"year": "2020", "forum": "BJgcwh4FwS", "title": "Neural Maximum Common Subgraph Detection with Guided Subgraph Extraction", "decision": "Reject", "meta_review": "This paper proposed graph neural networks based approach for subgraph detection. The reviewers find that the overall the paper is interesting, however further improvements are needed to meet ICLR standard: \n1. Experiments on larger graph. Slight speedup in small graphs are less exciting.  \n2. It seems there's a mismatch between training and inference. \n3. The stopping criterion is quite heuristic. \n", "reviews": [{"review_id": "BJgcwh4FwS-0", "review_text": "This paper proposed a graph net based approach for subgraph matching. The general idea is based on the graph matching network (Li et.al, ICML 2019) that computes the node embeddings of two graphs with co-attentions. The training requires the supervision of ground truth matching. During inference an iterative method with heuristic stopping criteria is used. Experiments on tiny graphs show better results than learning based baselines, but worse results than MCS solver. Overall the paper is well motivated. However there are several major concerns with the paper: 1. Since it relies on the solver to provide training data, it might be hard to train on large graphs as there would be no cheap supervision. Also it seems that getting slightly faster but much worse results than the solver on small graphs is not that exciting. 2. It seems there's a mismatch between training and inference. The inference method is done iteratively, where the Eq (6) is somewhat not clear to me: as this ||w1-w2|| criteria is not trained during training, it seems quite heuristic by doing so. 3. I\u2019m not sure why the two stop conditions are needed. One can easily check (incrementally) whether the added nodes are isomorphic. 4. The graphs used in experiments are too small. Some other minor issues: It would be better to define Y with Eq (1) and Eq (2) in the paper. There seems to be no explicit definition of Y. ", "rating": "3: Weak Reject", "reply_text": "First of all , we would like to sincerely thank you for your feedback and suggestions . Regarding the concerns : 1a ) Regarding cheap supervision , we can indeed generate ground-truth MCS training pairs without using any exact MCS solver , as demonstrated in the new experiment described below . Also , we would like to point out that our model can be trained on smaller graph pairs with cheap supervision and tested on other ( larger ) graph pairs . The high level idea of generating ground-truth MCS training pairs without exact MCS solver is to create such pairs with a smart design instead of computing MCS for any given pair of graphs . One possible way to create such pair is to extract an induced subgraph from a given graph , and the ground-truth MCS of the two graphs is naturally the extracted subgraph . More concretely , our experimental setup is as follows : We generate training graph pairs by first using the Barab\u00e1si\u2013Albert model [ 1 ] to generate 1000 graphs of size 32 . For each generated graph , We randomly extract one connected 16-node subgraph from it . Each generated graph and extracted subgraph form one pair , giving a total of 1000 training graph pairs ( our training set ) . Notice , this generation procedure allows us to know the MCSs during generation . We follow a similar procedure for testing set , where we use the Barabasi-Albert model to generate 100 graphs of size 16 , 32 , 64 , and 128 ( denoted as \u201c Test Dataset Size \u201d in the table below ) . For each generated graph , we extract one connected 8- , 16- , 32- , and 64- node subgraph respectively . This gives us 5 test sets , each with 100 graph pairs . We use the following 5 metrics for thorough evaluation : Solved % : It measures the percentage of pairs that the model can successfully finish within 100 seconds . Soft % : It measures the fraction of the predicted MCS size over the true MCS size for the isomorphic extracted subgraphs ( Section D paragraph 5 ) . Iso % : Among the pairs that can be solved within the time budget , it measures the percentage of pairs whose extracted subgraphs are isomorphic . This is an important metric because subgraph isomorphism is a key constraint required by the definition of MCS . Dev in # nodes : among the pairs that can be solved within the time budget , it measures the average deviation of the number of nodes in the predicted MCS versus the number of nodes in the true MCS . The range of this metric is [ 0 , N ] where N is the number of nodes of the largest graph in a dataset . This metric gives a more intuitive understanding of the performance of a model compared to \u201c Soft % \u201d since it reports the number of nodes directly . ( Average ) Runtime ( msec ) : It measures the average running time per testing pairs that the model solves within the time budget . In other words , if a model fails at solving a pair within the time budget , the runtime will NOT be taken into account by this metric for fair comparison We set the time budget to 100 seconds and 500 seconds for McSplit ( state-of-the-art exact solver ) [ 2 ] respectively , and the results are as follows :"}, {"review_id": "BJgcwh4FwS-1", "review_text": "This paper proposes a novel algorithm NeuralMCS for maximum common subgraph (MCS) identification. The proposed algorithm consists of two components. One is a neural-network model based on Graph Matching Networks (GMN, Li et al, 2019) to learn a node-to-node matching matrix from examples of the ground-truth MCS results. Another is the algorithm called GSE (Guided Subgraph Extraction) to obtain an MCS by making an explicit assignment from the estimated matching matrix by the NN model. Experimental comparisons are made to other NN-based approaches combined with threshold-based assignments by the Hungarian algorithm (w.r.t the accuracy) and to a state-of-the-art exact algorithm MCSplit, and show the effectiveness of NeuralMCS. This paper proposes an interesting method for MCS detection which would have large application interests. Though the basic idea is nice, the reported performance gains would be a bit less convincing due to the following evaluation problem and its weak novelty. The algorithm has two parts, and the first NN part to learn a matching matrix is mostly based on the already existing algorithm of GMN (Li et al, 2019). The novel part would be primarily in post-processing normalization (described in 3.1) for the matching matrix and seems to also be applicable to other NNs (for example, GAT?). The second part GSE to get an explicit subgraph also seem to be applied independently to the first part. We can see that combining these two parts worked, but it is unclear how each component contributes to the performance gain compared to any possible alternatives of each part. I understand that MCS detection from a matching matrix (and node state vectors) is not exact if we just use Hungarian-like linear assignment problem (LAP) solvers for a submatrix obtained by a simple thresholding, but both post-processing normalization and GSE parts (which brings the novelty) can be more carefully evaluated through some 'ablation studies' using some simple alternative substitutes.", "rating": "3: Weak Reject", "reply_text": "Thank you for your helpful comments ! As pointed out , we first form a matching matrix then extract a subgraph guided by this matrix . To form the matching matrix , we utilize representation learning to make node embeddings ; compute X using similarity scores from node embeddings ( Section 3.1 , paragraphs 3 and 4 ) ; compute Y through normalization of X ( Section 3.1 ; Equations 1 and 2 ) . To perform extraction , we utilize the GSE method proposed ( Section 3.3 ) . Regarding the evaluation concern , we performed more in-depth ablation studies as shown below to show the importance of each component on the AIDs dataset ( the numbers are the Exact % defined in Section 4.1 ) : + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + | Matching Matrix | Subgraph Extraction Strategy | | Computation + -- -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -+ | | GSE | Threshold | Threshold + LAP | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -+ | GMN + Our Normalization ( NeuralMCS ) | 90.692 | 26.057 | 17.339 | | GAT + Our Normalization | 87.119 | 21.206 | 16.159 | | GMN + Sigmoid ( BasicGMN ) | 31.596 | 10.521 | 12.488 | | GMN + Sinkhorn Softmax | 21.861 | 12.488 | 0.197 | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 1 ) On the importance of GMN : GAT + Our Normalization uses GAT to encode the initial node embeddings and feed these embeddings into the NeuralMCS pipeline instead of the GMN node embeddings . As GAT does not perform inter-graph attention , the embeddings for one graph are not informed of the structure of the graph it is being matched to . This leads to lower performance . 2 ) On the importance of Normalization : GMN + Sigmoid ( BasicGMN ) uses sigmoid normalization , $ y=1/ ( 1+e^ { -x } ) $ , on each individual element of the X matrix , instead of our normalization scheme ( Section 3.1 ; Equations 1 and 2 ) to obtain Y . As sigmoid treats each node-node pair in Y as independent ( an incorrect assumption ) , we see that our normalization procedure performs the best . GMN + Sinkhorn Softmax uses successive row- and column-wise softmax normalization ( softmax to ensure that the matching matrix Y is in the range ( 0,1 ) ) on the X matrix ( similar to Image-PCA \u2019 s [ 1 ] sinkhorn algorithm [ 2 ] ) instead of our normalization scheme ( Section 3.1 ; Equations 1 and 2 ) . As softmax does not explicitly allow nodes to go unmatched ( Section 3.1 paragraph 4 ) , as dictated by the MCS definition , we see that our normalization procedure performs the best . 3 ) On the importance of GSE : For each Matching Matrix Computation method , we run 3 different subgraph extraction strategies : GSE ( Section 3.3 ) , thresholding , and thresholding + LAP ( i.e.Hungarian Algorithm , described in Appendix B paragraph 3 ) . For thresholding , for each of the two graphs , we select the nodes whose probabilities of being included in the MCS are greater than a tunable threshold , yielding two subgraphs . We calculate such probabilities by taking the summation of rows and columns of the matching matrix Y . More details can be found in Appendix B , the third paragraph . For thresholding + LAP , we ensure that the detected subgraphs are of equal size and have a one-to-one node-node mapping ( to validate their isomorphism ) by running the Hungarian algorithm [ 3 ] on the remaining rows and columns of Y after thresholding . We can not run LAP on the original Y since LAP would select all nodes in the smaller of the two graphs . Neither of these simpler subgraph extraction methods enforces the subgraph isomorphism constraint , which explains their worse performance compared with GSE detailed in Section 3.3 . We find that our major novelties ( GSE and normalization technique ) are the most important components in producing good performance . We appreciate your valuable feedback again ! [ 1 ] Wang , Runzhong , Junchi Yan , and Xiaokang Yang . `` Learning Combinatorial Embedding Networks for Deep Graph Matching . '' ICCV ( 2019 ) . [ 2 ] Knight , Philip A . `` The Sinkhorn\u2013Knopp algorithm : convergence and applications . '' SIAM Journal on Matrix Analysis and Applications 30.1 ( 2008 ) : 261-275 . [ 3 ] Kuhn , Harold W. `` The Hungarian method for the assignment problem . '' Naval research logistics quarterly 2.1\u20102 ( 1955 ) : 83-97 ."}, {"review_id": "BJgcwh4FwS-2", "review_text": "The authors proposed a novel method to find the Maximum Common Subgraph (MCS) of two graphs. I am familiar with the quadratic assignment problem (QAP) based graph matching and I am not very familiar with the MCS problem. The authors adopt Graph Matching Networks (GMN) for feature embedding, and then similarity matrix X can be generated by computing the similarities between the embeddings. The similarity matrix is then normalized using a similar way as the sinkhorn procedure in [1-3]. The Assignment matrix then can be given from X. Then a novel procedure, named Guided subgraph Extraction (GSE, which is considered as the main contribution of this paper), is used to get an MCS from assignment matrix. Here the authors may consider a simple baseline, which is to use QAP to give the assignment matrix, and then run GSE to obtain the MCS. Overall the paper is well written, and the experiment is good and solid. Some suggestions: The GCN based GMN might not be the best choice for graph embedding. The authors may consider stronger Graph Neural Networks such as DGCNN (used in [3]) or Message Passing Neural Network (used in [4] and [5]) as the graph embedding module in the future work. [1] Deep Learning of Graph Matching, CVPR18 [2] Learning Combinatorial Embedding Networks for Deep Graph Matching. ICCV19 [3] Deep Closest Point: Learning Representations for Point Cloud Registration. ICCV19 [4] Deep Graphical Feature Learning for the Feature Matching Problem, ICCV19 [5] Neural Message Passing for Quantum Chemistry, ICML17", "rating": "6: Weak Accept", "reply_text": "Thank you for your constructive feedback ! As you mentioned , the construction of matching matrix , Y ( Section 3.1 paragraph 2 ) , can be approached in 2 ways : - directly compute Y using some QAP-based approach - utilize representation learning to form node embeddings ; compute similarity matrix , X , using similarity scores from node embeddings ( Section 3.1 paragraphs 3 and 4 ) ; compute Y through normalization of X ( Section 3.1 ; Equations 1 and 2 ) . We consider alternative methods and suggestions of constructing the matching matrix , Y , whose results on AIDS are shown below : + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ | Matching Matrix Computation | Exact % | Soft % | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ | GMN + Our Normalization ( NeuralMCS ) | 90.692 | 97.899 | | DGCNN + Our Normalization | 86.169 | 96.770 | | GW-QAP | 23.894 | 56.150 | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + GMN + Our Normalization : It is the proposed NeuralMCS architecture . DGCNN + Our Normalization : It utilizes DGCNN [ 1 ] ( adopted in [ 2 ] ) to form node embeddings ; computation of X and Y from these node embeddings are the same as in NeuralMCS.We find that the representation learning approach significantly improves the quality of the Y matrix . While DGCNN achieves reasonable performance , we see that GMN outperforms it as GMN uses inter-graph message passing capturing the graph-graph matching relation . GW-QAP : It utilizes QAP [ 3 ] to directly obtain the Y matrix . This method produces an assignment matrix which is treated as Y by solving for the Gromov-Wasserstein ( GW ) discrepancy [ 4 ] for 2 graphs . The assignment matrix is soft , which is [ 3 ] is further turned into hard node-node mappings while we directly feed the soft matrix into the GSE step . The method is denoted as GW-QAP to be specific . Notice it directly solves the optimization problem without learnable components adaptive to the MCS ground-truth node-node mappings , so the result is relatively poor . In summary , we find that GMN outperforms DGCNN and QAP-based approaches for producing the matching matrix Y . We would like to thank you again for your helpful suggestions . [ 1 ] Wang , Yue , et al . `` Dynamic graph cnn for learning on point clouds . '' ACM Transactions on Graphics ( TOG ) 38.5 ( 2019 ) : 146 . [ 2 ] Wang , Yue , and Justin M. Solomon . `` Deep Closest Point : Learning Representations for Point Cloud Registration . '' ICCV ( 2019 ) . [ 3 ] Xu , Hongteng , Dixin Luo , and Lawrence Carin . `` Scalable Gromov-Wasserstein Learning for Graph Partitioning and Matching . '' NeurIPS ( 2019 ) . [ 4 ] M\u00e9moli , Facundo . `` Gromov\u2013Wasserstein distances and the metric approach to object matching . '' Foundations of computational mathematics 11.4 ( 2011 ) : 417-487 ."}], "0": {"review_id": "BJgcwh4FwS-0", "review_text": "This paper proposed a graph net based approach for subgraph matching. The general idea is based on the graph matching network (Li et.al, ICML 2019) that computes the node embeddings of two graphs with co-attentions. The training requires the supervision of ground truth matching. During inference an iterative method with heuristic stopping criteria is used. Experiments on tiny graphs show better results than learning based baselines, but worse results than MCS solver. Overall the paper is well motivated. However there are several major concerns with the paper: 1. Since it relies on the solver to provide training data, it might be hard to train on large graphs as there would be no cheap supervision. Also it seems that getting slightly faster but much worse results than the solver on small graphs is not that exciting. 2. It seems there's a mismatch between training and inference. The inference method is done iteratively, where the Eq (6) is somewhat not clear to me: as this ||w1-w2|| criteria is not trained during training, it seems quite heuristic by doing so. 3. I\u2019m not sure why the two stop conditions are needed. One can easily check (incrementally) whether the added nodes are isomorphic. 4. The graphs used in experiments are too small. Some other minor issues: It would be better to define Y with Eq (1) and Eq (2) in the paper. There seems to be no explicit definition of Y. ", "rating": "3: Weak Reject", "reply_text": "First of all , we would like to sincerely thank you for your feedback and suggestions . Regarding the concerns : 1a ) Regarding cheap supervision , we can indeed generate ground-truth MCS training pairs without using any exact MCS solver , as demonstrated in the new experiment described below . Also , we would like to point out that our model can be trained on smaller graph pairs with cheap supervision and tested on other ( larger ) graph pairs . The high level idea of generating ground-truth MCS training pairs without exact MCS solver is to create such pairs with a smart design instead of computing MCS for any given pair of graphs . One possible way to create such pair is to extract an induced subgraph from a given graph , and the ground-truth MCS of the two graphs is naturally the extracted subgraph . More concretely , our experimental setup is as follows : We generate training graph pairs by first using the Barab\u00e1si\u2013Albert model [ 1 ] to generate 1000 graphs of size 32 . For each generated graph , We randomly extract one connected 16-node subgraph from it . Each generated graph and extracted subgraph form one pair , giving a total of 1000 training graph pairs ( our training set ) . Notice , this generation procedure allows us to know the MCSs during generation . We follow a similar procedure for testing set , where we use the Barabasi-Albert model to generate 100 graphs of size 16 , 32 , 64 , and 128 ( denoted as \u201c Test Dataset Size \u201d in the table below ) . For each generated graph , we extract one connected 8- , 16- , 32- , and 64- node subgraph respectively . This gives us 5 test sets , each with 100 graph pairs . We use the following 5 metrics for thorough evaluation : Solved % : It measures the percentage of pairs that the model can successfully finish within 100 seconds . Soft % : It measures the fraction of the predicted MCS size over the true MCS size for the isomorphic extracted subgraphs ( Section D paragraph 5 ) . Iso % : Among the pairs that can be solved within the time budget , it measures the percentage of pairs whose extracted subgraphs are isomorphic . This is an important metric because subgraph isomorphism is a key constraint required by the definition of MCS . Dev in # nodes : among the pairs that can be solved within the time budget , it measures the average deviation of the number of nodes in the predicted MCS versus the number of nodes in the true MCS . The range of this metric is [ 0 , N ] where N is the number of nodes of the largest graph in a dataset . This metric gives a more intuitive understanding of the performance of a model compared to \u201c Soft % \u201d since it reports the number of nodes directly . ( Average ) Runtime ( msec ) : It measures the average running time per testing pairs that the model solves within the time budget . In other words , if a model fails at solving a pair within the time budget , the runtime will NOT be taken into account by this metric for fair comparison We set the time budget to 100 seconds and 500 seconds for McSplit ( state-of-the-art exact solver ) [ 2 ] respectively , and the results are as follows :"}, "1": {"review_id": "BJgcwh4FwS-1", "review_text": "This paper proposes a novel algorithm NeuralMCS for maximum common subgraph (MCS) identification. The proposed algorithm consists of two components. One is a neural-network model based on Graph Matching Networks (GMN, Li et al, 2019) to learn a node-to-node matching matrix from examples of the ground-truth MCS results. Another is the algorithm called GSE (Guided Subgraph Extraction) to obtain an MCS by making an explicit assignment from the estimated matching matrix by the NN model. Experimental comparisons are made to other NN-based approaches combined with threshold-based assignments by the Hungarian algorithm (w.r.t the accuracy) and to a state-of-the-art exact algorithm MCSplit, and show the effectiveness of NeuralMCS. This paper proposes an interesting method for MCS detection which would have large application interests. Though the basic idea is nice, the reported performance gains would be a bit less convincing due to the following evaluation problem and its weak novelty. The algorithm has two parts, and the first NN part to learn a matching matrix is mostly based on the already existing algorithm of GMN (Li et al, 2019). The novel part would be primarily in post-processing normalization (described in 3.1) for the matching matrix and seems to also be applicable to other NNs (for example, GAT?). The second part GSE to get an explicit subgraph also seem to be applied independently to the first part. We can see that combining these two parts worked, but it is unclear how each component contributes to the performance gain compared to any possible alternatives of each part. I understand that MCS detection from a matching matrix (and node state vectors) is not exact if we just use Hungarian-like linear assignment problem (LAP) solvers for a submatrix obtained by a simple thresholding, but both post-processing normalization and GSE parts (which brings the novelty) can be more carefully evaluated through some 'ablation studies' using some simple alternative substitutes.", "rating": "3: Weak Reject", "reply_text": "Thank you for your helpful comments ! As pointed out , we first form a matching matrix then extract a subgraph guided by this matrix . To form the matching matrix , we utilize representation learning to make node embeddings ; compute X using similarity scores from node embeddings ( Section 3.1 , paragraphs 3 and 4 ) ; compute Y through normalization of X ( Section 3.1 ; Equations 1 and 2 ) . To perform extraction , we utilize the GSE method proposed ( Section 3.3 ) . Regarding the evaluation concern , we performed more in-depth ablation studies as shown below to show the importance of each component on the AIDs dataset ( the numbers are the Exact % defined in Section 4.1 ) : + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + | Matching Matrix | Subgraph Extraction Strategy | | Computation + -- -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -+ | | GSE | Threshold | Threshold + LAP | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -+ | GMN + Our Normalization ( NeuralMCS ) | 90.692 | 26.057 | 17.339 | | GAT + Our Normalization | 87.119 | 21.206 | 16.159 | | GMN + Sigmoid ( BasicGMN ) | 31.596 | 10.521 | 12.488 | | GMN + Sinkhorn Softmax | 21.861 | 12.488 | 0.197 | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 1 ) On the importance of GMN : GAT + Our Normalization uses GAT to encode the initial node embeddings and feed these embeddings into the NeuralMCS pipeline instead of the GMN node embeddings . As GAT does not perform inter-graph attention , the embeddings for one graph are not informed of the structure of the graph it is being matched to . This leads to lower performance . 2 ) On the importance of Normalization : GMN + Sigmoid ( BasicGMN ) uses sigmoid normalization , $ y=1/ ( 1+e^ { -x } ) $ , on each individual element of the X matrix , instead of our normalization scheme ( Section 3.1 ; Equations 1 and 2 ) to obtain Y . As sigmoid treats each node-node pair in Y as independent ( an incorrect assumption ) , we see that our normalization procedure performs the best . GMN + Sinkhorn Softmax uses successive row- and column-wise softmax normalization ( softmax to ensure that the matching matrix Y is in the range ( 0,1 ) ) on the X matrix ( similar to Image-PCA \u2019 s [ 1 ] sinkhorn algorithm [ 2 ] ) instead of our normalization scheme ( Section 3.1 ; Equations 1 and 2 ) . As softmax does not explicitly allow nodes to go unmatched ( Section 3.1 paragraph 4 ) , as dictated by the MCS definition , we see that our normalization procedure performs the best . 3 ) On the importance of GSE : For each Matching Matrix Computation method , we run 3 different subgraph extraction strategies : GSE ( Section 3.3 ) , thresholding , and thresholding + LAP ( i.e.Hungarian Algorithm , described in Appendix B paragraph 3 ) . For thresholding , for each of the two graphs , we select the nodes whose probabilities of being included in the MCS are greater than a tunable threshold , yielding two subgraphs . We calculate such probabilities by taking the summation of rows and columns of the matching matrix Y . More details can be found in Appendix B , the third paragraph . For thresholding + LAP , we ensure that the detected subgraphs are of equal size and have a one-to-one node-node mapping ( to validate their isomorphism ) by running the Hungarian algorithm [ 3 ] on the remaining rows and columns of Y after thresholding . We can not run LAP on the original Y since LAP would select all nodes in the smaller of the two graphs . Neither of these simpler subgraph extraction methods enforces the subgraph isomorphism constraint , which explains their worse performance compared with GSE detailed in Section 3.3 . We find that our major novelties ( GSE and normalization technique ) are the most important components in producing good performance . We appreciate your valuable feedback again ! [ 1 ] Wang , Runzhong , Junchi Yan , and Xiaokang Yang . `` Learning Combinatorial Embedding Networks for Deep Graph Matching . '' ICCV ( 2019 ) . [ 2 ] Knight , Philip A . `` The Sinkhorn\u2013Knopp algorithm : convergence and applications . '' SIAM Journal on Matrix Analysis and Applications 30.1 ( 2008 ) : 261-275 . [ 3 ] Kuhn , Harold W. `` The Hungarian method for the assignment problem . '' Naval research logistics quarterly 2.1\u20102 ( 1955 ) : 83-97 ."}, "2": {"review_id": "BJgcwh4FwS-2", "review_text": "The authors proposed a novel method to find the Maximum Common Subgraph (MCS) of two graphs. I am familiar with the quadratic assignment problem (QAP) based graph matching and I am not very familiar with the MCS problem. The authors adopt Graph Matching Networks (GMN) for feature embedding, and then similarity matrix X can be generated by computing the similarities between the embeddings. The similarity matrix is then normalized using a similar way as the sinkhorn procedure in [1-3]. The Assignment matrix then can be given from X. Then a novel procedure, named Guided subgraph Extraction (GSE, which is considered as the main contribution of this paper), is used to get an MCS from assignment matrix. Here the authors may consider a simple baseline, which is to use QAP to give the assignment matrix, and then run GSE to obtain the MCS. Overall the paper is well written, and the experiment is good and solid. Some suggestions: The GCN based GMN might not be the best choice for graph embedding. The authors may consider stronger Graph Neural Networks such as DGCNN (used in [3]) or Message Passing Neural Network (used in [4] and [5]) as the graph embedding module in the future work. [1] Deep Learning of Graph Matching, CVPR18 [2] Learning Combinatorial Embedding Networks for Deep Graph Matching. ICCV19 [3] Deep Closest Point: Learning Representations for Point Cloud Registration. ICCV19 [4] Deep Graphical Feature Learning for the Feature Matching Problem, ICCV19 [5] Neural Message Passing for Quantum Chemistry, ICML17", "rating": "6: Weak Accept", "reply_text": "Thank you for your constructive feedback ! As you mentioned , the construction of matching matrix , Y ( Section 3.1 paragraph 2 ) , can be approached in 2 ways : - directly compute Y using some QAP-based approach - utilize representation learning to form node embeddings ; compute similarity matrix , X , using similarity scores from node embeddings ( Section 3.1 paragraphs 3 and 4 ) ; compute Y through normalization of X ( Section 3.1 ; Equations 1 and 2 ) . We consider alternative methods and suggestions of constructing the matching matrix , Y , whose results on AIDS are shown below : + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ | Matching Matrix Computation | Exact % | Soft % | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- -- -+ | GMN + Our Normalization ( NeuralMCS ) | 90.692 | 97.899 | | DGCNN + Our Normalization | 86.169 | 96.770 | | GW-QAP | 23.894 | 56.150 | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + GMN + Our Normalization : It is the proposed NeuralMCS architecture . DGCNN + Our Normalization : It utilizes DGCNN [ 1 ] ( adopted in [ 2 ] ) to form node embeddings ; computation of X and Y from these node embeddings are the same as in NeuralMCS.We find that the representation learning approach significantly improves the quality of the Y matrix . While DGCNN achieves reasonable performance , we see that GMN outperforms it as GMN uses inter-graph message passing capturing the graph-graph matching relation . GW-QAP : It utilizes QAP [ 3 ] to directly obtain the Y matrix . This method produces an assignment matrix which is treated as Y by solving for the Gromov-Wasserstein ( GW ) discrepancy [ 4 ] for 2 graphs . The assignment matrix is soft , which is [ 3 ] is further turned into hard node-node mappings while we directly feed the soft matrix into the GSE step . The method is denoted as GW-QAP to be specific . Notice it directly solves the optimization problem without learnable components adaptive to the MCS ground-truth node-node mappings , so the result is relatively poor . In summary , we find that GMN outperforms DGCNN and QAP-based approaches for producing the matching matrix Y . We would like to thank you again for your helpful suggestions . [ 1 ] Wang , Yue , et al . `` Dynamic graph cnn for learning on point clouds . '' ACM Transactions on Graphics ( TOG ) 38.5 ( 2019 ) : 146 . [ 2 ] Wang , Yue , and Justin M. Solomon . `` Deep Closest Point : Learning Representations for Point Cloud Registration . '' ICCV ( 2019 ) . [ 3 ] Xu , Hongteng , Dixin Luo , and Lawrence Carin . `` Scalable Gromov-Wasserstein Learning for Graph Partitioning and Matching . '' NeurIPS ( 2019 ) . [ 4 ] M\u00e9moli , Facundo . `` Gromov\u2013Wasserstein distances and the metric approach to object matching . '' Foundations of computational mathematics 11.4 ( 2011 ) : 417-487 ."}}