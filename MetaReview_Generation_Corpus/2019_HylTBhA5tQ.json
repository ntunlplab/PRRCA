{"year": "2019", "forum": "HylTBhA5tQ", "title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "decision": "Accept (Poster)", "meta_review": "Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.\n", "reviews": [{"review_id": "HylTBhA5tQ-0", "review_text": "In this paper, the authors associated with the generalization gap of robust adversarial training with the distance between the test point and the manifold of training data. A so-called 'blind-spot attack' is proposed to show the weakness of robust adversarial training. Although the paper contains interesting ideas and empirical results, I have several concerns about the current version. a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\". Can authors provide more details, e.g., empirical results, about it? What is its rationale? b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex generative models like in Song et al. (2018). For the MNIST dataset which Madry et al. (2018) demonstrate the strongest defense results so far, we propose a simple transformation to find the blind-spots in this model.\" Can authors provide empirical comparison between blind-spot attacks and the work by Song et al. (2018), e.g., attack success rate & distortion? c) The linear transformation x^\\prime = \\alpha x + \\beta yields a blind-spot attack which can defeat robust adversarial training. However, given the linear transformation, one can further modify the inner maximization (adv. example generation) in robust training framework so that the $\\ell_infty$ attack satisfies max_{\\alpha, \\beta} f(\\alpha x + \\beta) subject to \\| \\alpha x + \\beta \\|\\leq \\epsilon. In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training. d) \"Because we scale the image by a factor of \\alpha, we also set a stricter criterion of success, ..., perturbation must be less than \\alpha \\epsilon to be counted as a successful attack.\" I did not get the point. Even if you have a scaling factor in x^\\prime = \\alpha x + \\beta, the universal perturbation rule should still be | x - x^\\prime |_\\infty \\leq \\epsilon. The metric the authors used would result in a higher attack success rate, right? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer3 , Thank you for your insightful questions . They are very helpful for us to improve the paper . We would like to answer your 4 questions as below . a ) We added more figures with k=10 , 100 , 1000 in the appendix ( in main text , we used k=5 ) . Our main conclusion does not change regardless the value of k : there is a strong correlation between attack success rate and the distance between test examples to training dataset . A larger distance usually implies a higher attack success rate . The rational to use this metric is that it is simple , and nearest neighbour based methods are usually robust to hyper-parameter selection . We don \u2019 t want our observations depend on hyper-parameters during distance measurement . b ) Song et al . ( 2018 ) does not have ordinary metrics like distortion or ( ordinary ) attack success rates to compare with . In their attack , the input is a random noise for GAN , and they generate adversarial images from scratch . In typical adversarial attacks , people start from a specific reference ( natural ) image x and add adversarial distortion to obtain x_adv . In their paper , adversarial images are generated by GANs directly and there is no reference images at all , so distortion can not be calculated ( see definitions 1 and 2 in their paper ) . They have to conduct user study to determine what is the true class label for a generated image , and see if the model will misclassify it . The success rate is the model \u2019 s misclassification rate from user study . In our paper , our attacks first conduct slight transformations on a natural test image x to obtain x \u2019 , and then run ordinary gradient based adversarial attacks on x \u2019 to obtain x \u2019 _adv . We have a reference image x \u2019 , so we can compute the distortion between x \u2019 and x \u2019 _adv , and determine the success by a certain criterion on distortion . This setting is different from Song et al . ( 2018 ) so we can not directly compare distortion and success rates with them . c ) We want to emphasize that the \u201c blind-spot attack \u201d is a class of attacks , which exploits the gap between training and test data distributions ( see our definition in Section 3.3 ) . The linear transformation used in our paper is one of the simplest attacks in this class . If we know the details of this specific attack before training , it is possible defend against this specific simple attack . However , it is always possible to find some different blind-spot attacks ( for example , by using a generative model ) . Rather than starting a new arm race between attacks and defenses , our argument here is to show the fundamental limitations of adversarial training -- it is hard to cover all the blind-spots during training time because it is impossible to eliminate the gap between training and test data especially when data dimension is high . d ) The stricter criterion actually makes our attack success rates * lower * rather than higher . Finding adversarial examples with smaller distortions is harder than finding adversarial examples with large distortions . As an extreme case , if the criterion is distortion < =0 , the attack success rate will always be zero , since we can not fool the model using unmodified natural images . In Table 2 , the success rates under the column 0.27 are strictly lower than the numbers under the column 0.3 . We consider this additional stricter criterion because images after scaling are within a smaller range , so we also restrict the noise to be smaller , to keep the same signal-to-noise ratio and make an absolutely fair comparison . If we don \u2019 t use this stricter criterion , our attack success rates will look even better . In our updated revision , we also include additional experiments on GTS dataset , as long as two other state-of-the-art adversarial training methods by Wong et al.and Sinha et al .. We observe very similar results on all these methods and datasets , further confirming the conclusion of our paper . We hope our answers resolve all the doubts you had with our paper . We would like to further discuss with you if you have any unclear things or additional questions , and hope you can reconsider the rating of our paper . Thank you ! Paper 1584 Authors"}, {"review_id": "HylTBhA5tQ-1", "review_text": "This paper provides some insights on influence of data distribution on robustness of adversarial training. The paper demonstrates through a number of analysis that the distance between the training an test data sets plays an important role on the effectiveness of adversarial training. To show the latter, the paper proposes an approach to measure the distance between the two data sets using combination of nonlinear projection (e.g. t-SNE), KDE, and K-L divergence. The paper also shows that under simple transformation to the test dataset (e.g. scaling), performance of adversarial training reduces significantly due to the large gap between training and test data set. This tends to impact high dimensional data sets more than low dimensional data sets since it is much harder to cover the whole ground truth data distribution in the training dataset. Pros: - Provides insights on why adversarial training is less effective on some datasets. - Proposes a metric that seems to strongly correlate with the effectiveness of adversarial training. Cons: - Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution. - The marketing phrase \"the blind-spot attach\" falls short in delivering what one may expect from the paper after reading it. The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot. For some dataset, this is beyond a spot, it could actually be huge portion of the input space! Minor comments: - I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models. Though the paper is not suggesting that, it would help to clarify it in the paper. Furthermore, it would help if the paper elaborates why the distance between the test and training dataset is smaller in an adversarially trained network compared to a naturally trained network. - Are the results in Table 1 for an adversarially trained network or a naturally trained network? Either way, it could be also interesting to see the average K-L divergence between an adversarially and a naturally trained network on the same dataset. - Please provide more visualization similarly to those shown in Fig 4. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your insightful comments to help us improve our paper . First of all , we would like to mention that we add more experiments on two additional state-of-the-art strong and certified defense methods , and observe that they are also vulnerable to our proposed attacks . Please see our reply to all reviewers . Here are our responses to your concerns in \u201c Cons \u201d and \u201c Minor comments \u201d . Although we were not able to provide theoretical analysis in this paper , our proposed attacks are very effective on state-of-the-art adversarial training methods , and we believe our conclusions Currently , there is relatively few theoretical analysis in this field in general , and many analysis makes unpractical assumptions . We believe our results can inspire other researcher \u2019 s theoretical research . Regarding the \u201c blind-spot attack \u201d phrase , we are open to suggestions from the reviewers . Other phrases we considered including \u201c evasion attack \u201d , \u201c generalization gap attack \u201d and \u201c scaling attack \u201d . Which one do you think is a better option ? Regarding the distances in Figure 3 : Thanks for raising this concern . We have added a note to clarify this issue . The difference in distance can be partially explained by the sparsity in an adversarially trained model . As suggested in [ 1 ] , the adversarially trained model by Madry et al.tends to find sparse features ( see Figure 5 in [ 1 ] ) , where many components are zero . Thus , the distances tend to be overall smaller . Regarding the results in Table 1 : In our old version , we only used the adversarially trained network . In our revision , we added K-L divergence computed from both adversarially trained and naturally trained networks . Additionally , we also add a new distance metric proposed by AnonReviewer1 . The K-L divergences by both networks , as well as the newly added distance metric , show similar observations . Regarding adding more visualizations : We added some more visualizations in Fig 10 in the appendix . It is worth noting that the Linf distortion metric used in adversarial training is sometimes not a good metric to reflect visual differences . However , the test images under our proposed attack indeed have much smaller Linf distortions . We hope that we have answered all your questions , and we are glad to discuss with you if you have any further concerns about our paper . [ 1 ] Tsipras , Dimitris , et al . `` Robustness may be at odds with accuracy . '' arXiv preprint arXiv:1805.12152 ( 2018 ) . Thank you ! Paper 1584 Authors"}, {"review_id": "HylTBhA5tQ-2", "review_text": "The paper is well written and the main contribution, a methodology to find \u201cblind-spot attacks\u201d well motivated and differences to prior work stated clearly. The empirical results presented in Figure 1 and 2 are very convincing. The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms. Why for example not using a simple score based on the histogram, or even the mean distance? Of course providing a single measure would allow to leverage that information during training. However, in its current form this seems rather complicated and computationally expensive (KL-based). As stated later in the paper the histograms themselves are not informative enough to detect such blind-spot transformation. Intuitively this makes a lot of sense given that the distance is based on the network embedding and is therefore also susceptible to this kind of data. However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the encouraging comments . First of all , we would like to mention that we add more experiments on two additional state-of-the-art strong and certified defense methods , and observe that they are also vulnerable to blind-spot attacks . Please see our reply to all reviewers . We agree that the K-L based method is complicated and computationally extensive . Fortunately , we only need to compute it once per dataset . To the best of our knowledge , currently , there is no perfect metric to measure the distance between a training set and a test set . Ordinary statistical methods ( like kernel two-sample tests ) do not work well due to the high dimensionality and the complex nature of image data . So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem . As suggested by the reviewer , we added a new metric based on the mean of \\ell_2 distance on the histogram in Section 4.3 . The results are shown in Table 1 ( under column \u201c Avg.normalized l2 Distance \u201d ) . The results align well with our conclusion : the dataset with significant better attack success rates has noticeably larger distance . It further supports the conclusion of our paper and indicates that our conclusion is distance metric agnostic . We hope that we have made everything clear , and we again appreciate your comments . Let us know if you have any additional questions . Thank you ! Paper 1584 Authors"}], "0": {"review_id": "HylTBhA5tQ-0", "review_text": "In this paper, the authors associated with the generalization gap of robust adversarial training with the distance between the test point and the manifold of training data. A so-called 'blind-spot attack' is proposed to show the weakness of robust adversarial training. Although the paper contains interesting ideas and empirical results, I have several concerns about the current version. a) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\". Can authors provide more details, e.g., empirical results, about it? What is its rationale? b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex generative models like in Song et al. (2018). For the MNIST dataset which Madry et al. (2018) demonstrate the strongest defense results so far, we propose a simple transformation to find the blind-spots in this model.\" Can authors provide empirical comparison between blind-spot attacks and the work by Song et al. (2018), e.g., attack success rate & distortion? c) The linear transformation x^\\prime = \\alpha x + \\beta yields a blind-spot attack which can defeat robust adversarial training. However, given the linear transformation, one can further modify the inner maximization (adv. example generation) in robust training framework so that the $\\ell_infty$ attack satisfies max_{\\alpha, \\beta} f(\\alpha x + \\beta) subject to \\| \\alpha x + \\beta \\|\\leq \\epsilon. In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training. d) \"Because we scale the image by a factor of \\alpha, we also set a stricter criterion of success, ..., perturbation must be less than \\alpha \\epsilon to be counted as a successful attack.\" I did not get the point. Even if you have a scaling factor in x^\\prime = \\alpha x + \\beta, the universal perturbation rule should still be | x - x^\\prime |_\\infty \\leq \\epsilon. The metric the authors used would result in a higher attack success rate, right? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer3 , Thank you for your insightful questions . They are very helpful for us to improve the paper . We would like to answer your 4 questions as below . a ) We added more figures with k=10 , 100 , 1000 in the appendix ( in main text , we used k=5 ) . Our main conclusion does not change regardless the value of k : there is a strong correlation between attack success rate and the distance between test examples to training dataset . A larger distance usually implies a higher attack success rate . The rational to use this metric is that it is simple , and nearest neighbour based methods are usually robust to hyper-parameter selection . We don \u2019 t want our observations depend on hyper-parameters during distance measurement . b ) Song et al . ( 2018 ) does not have ordinary metrics like distortion or ( ordinary ) attack success rates to compare with . In their attack , the input is a random noise for GAN , and they generate adversarial images from scratch . In typical adversarial attacks , people start from a specific reference ( natural ) image x and add adversarial distortion to obtain x_adv . In their paper , adversarial images are generated by GANs directly and there is no reference images at all , so distortion can not be calculated ( see definitions 1 and 2 in their paper ) . They have to conduct user study to determine what is the true class label for a generated image , and see if the model will misclassify it . The success rate is the model \u2019 s misclassification rate from user study . In our paper , our attacks first conduct slight transformations on a natural test image x to obtain x \u2019 , and then run ordinary gradient based adversarial attacks on x \u2019 to obtain x \u2019 _adv . We have a reference image x \u2019 , so we can compute the distortion between x \u2019 and x \u2019 _adv , and determine the success by a certain criterion on distortion . This setting is different from Song et al . ( 2018 ) so we can not directly compare distortion and success rates with them . c ) We want to emphasize that the \u201c blind-spot attack \u201d is a class of attacks , which exploits the gap between training and test data distributions ( see our definition in Section 3.3 ) . The linear transformation used in our paper is one of the simplest attacks in this class . If we know the details of this specific attack before training , it is possible defend against this specific simple attack . However , it is always possible to find some different blind-spot attacks ( for example , by using a generative model ) . Rather than starting a new arm race between attacks and defenses , our argument here is to show the fundamental limitations of adversarial training -- it is hard to cover all the blind-spots during training time because it is impossible to eliminate the gap between training and test data especially when data dimension is high . d ) The stricter criterion actually makes our attack success rates * lower * rather than higher . Finding adversarial examples with smaller distortions is harder than finding adversarial examples with large distortions . As an extreme case , if the criterion is distortion < =0 , the attack success rate will always be zero , since we can not fool the model using unmodified natural images . In Table 2 , the success rates under the column 0.27 are strictly lower than the numbers under the column 0.3 . We consider this additional stricter criterion because images after scaling are within a smaller range , so we also restrict the noise to be smaller , to keep the same signal-to-noise ratio and make an absolutely fair comparison . If we don \u2019 t use this stricter criterion , our attack success rates will look even better . In our updated revision , we also include additional experiments on GTS dataset , as long as two other state-of-the-art adversarial training methods by Wong et al.and Sinha et al .. We observe very similar results on all these methods and datasets , further confirming the conclusion of our paper . We hope our answers resolve all the doubts you had with our paper . We would like to further discuss with you if you have any unclear things or additional questions , and hope you can reconsider the rating of our paper . Thank you ! Paper 1584 Authors"}, "1": {"review_id": "HylTBhA5tQ-1", "review_text": "This paper provides some insights on influence of data distribution on robustness of adversarial training. The paper demonstrates through a number of analysis that the distance between the training an test data sets plays an important role on the effectiveness of adversarial training. To show the latter, the paper proposes an approach to measure the distance between the two data sets using combination of nonlinear projection (e.g. t-SNE), KDE, and K-L divergence. The paper also shows that under simple transformation to the test dataset (e.g. scaling), performance of adversarial training reduces significantly due to the large gap between training and test data set. This tends to impact high dimensional data sets more than low dimensional data sets since it is much harder to cover the whole ground truth data distribution in the training dataset. Pros: - Provides insights on why adversarial training is less effective on some datasets. - Proposes a metric that seems to strongly correlate with the effectiveness of adversarial training. Cons: - Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution. - The marketing phrase \"the blind-spot attach\" falls short in delivering what one may expect from the paper after reading it. The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot. For some dataset, this is beyond a spot, it could actually be huge portion of the input space! Minor comments: - I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models. Though the paper is not suggesting that, it would help to clarify it in the paper. Furthermore, it would help if the paper elaborates why the distance between the test and training dataset is smaller in an adversarially trained network compared to a naturally trained network. - Are the results in Table 1 for an adversarially trained network or a naturally trained network? Either way, it could be also interesting to see the average K-L divergence between an adversarially and a naturally trained network on the same dataset. - Please provide more visualization similarly to those shown in Fig 4. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your insightful comments to help us improve our paper . First of all , we would like to mention that we add more experiments on two additional state-of-the-art strong and certified defense methods , and observe that they are also vulnerable to our proposed attacks . Please see our reply to all reviewers . Here are our responses to your concerns in \u201c Cons \u201d and \u201c Minor comments \u201d . Although we were not able to provide theoretical analysis in this paper , our proposed attacks are very effective on state-of-the-art adversarial training methods , and we believe our conclusions Currently , there is relatively few theoretical analysis in this field in general , and many analysis makes unpractical assumptions . We believe our results can inspire other researcher \u2019 s theoretical research . Regarding the \u201c blind-spot attack \u201d phrase , we are open to suggestions from the reviewers . Other phrases we considered including \u201c evasion attack \u201d , \u201c generalization gap attack \u201d and \u201c scaling attack \u201d . Which one do you think is a better option ? Regarding the distances in Figure 3 : Thanks for raising this concern . We have added a note to clarify this issue . The difference in distance can be partially explained by the sparsity in an adversarially trained model . As suggested in [ 1 ] , the adversarially trained model by Madry et al.tends to find sparse features ( see Figure 5 in [ 1 ] ) , where many components are zero . Thus , the distances tend to be overall smaller . Regarding the results in Table 1 : In our old version , we only used the adversarially trained network . In our revision , we added K-L divergence computed from both adversarially trained and naturally trained networks . Additionally , we also add a new distance metric proposed by AnonReviewer1 . The K-L divergences by both networks , as well as the newly added distance metric , show similar observations . Regarding adding more visualizations : We added some more visualizations in Fig 10 in the appendix . It is worth noting that the Linf distortion metric used in adversarial training is sometimes not a good metric to reflect visual differences . However , the test images under our proposed attack indeed have much smaller Linf distortions . We hope that we have answered all your questions , and we are glad to discuss with you if you have any further concerns about our paper . [ 1 ] Tsipras , Dimitris , et al . `` Robustness may be at odds with accuracy . '' arXiv preprint arXiv:1805.12152 ( 2018 ) . Thank you ! Paper 1584 Authors"}, "2": {"review_id": "HylTBhA5tQ-2", "review_text": "The paper is well written and the main contribution, a methodology to find \u201cblind-spot attacks\u201d well motivated and differences to prior work stated clearly. The empirical results presented in Figure 1 and 2 are very convincing. The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms. Why for example not using a simple score based on the histogram, or even the mean distance? Of course providing a single measure would allow to leverage that information during training. However, in its current form this seems rather complicated and computationally expensive (KL-based). As stated later in the paper the histograms themselves are not informative enough to detect such blind-spot transformation. Intuitively this makes a lot of sense given that the distance is based on the network embedding and is therefore also susceptible to this kind of data. However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the encouraging comments . First of all , we would like to mention that we add more experiments on two additional state-of-the-art strong and certified defense methods , and observe that they are also vulnerable to blind-spot attacks . Please see our reply to all reviewers . We agree that the K-L based method is complicated and computationally extensive . Fortunately , we only need to compute it once per dataset . To the best of our knowledge , currently , there is no perfect metric to measure the distance between a training set and a test set . Ordinary statistical methods ( like kernel two-sample tests ) do not work well due to the high dimensionality and the complex nature of image data . So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem . As suggested by the reviewer , we added a new metric based on the mean of \\ell_2 distance on the histogram in Section 4.3 . The results are shown in Table 1 ( under column \u201c Avg.normalized l2 Distance \u201d ) . The results align well with our conclusion : the dataset with significant better attack success rates has noticeably larger distance . It further supports the conclusion of our paper and indicates that our conclusion is distance metric agnostic . We hope that we have made everything clear , and we again appreciate your comments . Let us know if you have any additional questions . Thank you ! Paper 1584 Authors"}}