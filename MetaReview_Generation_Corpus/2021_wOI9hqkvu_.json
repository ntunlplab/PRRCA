{"year": "2021", "forum": "wOI9hqkvu_", "title": "A Text GAN for Language Generation with Non-Autoregressive Generator", "decision": "Reject", "meta_review": "This paper proposes GAN-training of a non-autoregressive generator for text. To circumvent the usual problems with non-differentiability of text GANs, the authors turn to Gumbel-Softmax parameterisation and straight-through estimation. \n\nThere are a number of aspects to this submission and they are not always clearly positioned. I will concentrate on the two aspects that seem most crucial:\n\n1. The authors position their generator as an implicit generator, but it really isn't. If we take the continuous interpretation of the output distributions: the Gumbel-Softmax transformation does correspond to a tractable density, the Concrete density of Maddison et al, with known parameter. If we take the discrete interpretation of the output distribution: Gumbel-argmax is just an alternative to sampling from a Categorical distribution with known parameter. In either case, the generator maps the noise source to a collection of conditionally independent distributions each of which has a known parameter and analytical density/mass function. The authors do, however, train the architecture using a GAN-type objective *as if* the generator were implicit.\n\n2. In the discussion phase the authors added that GAN training overcomes the independence assumptions made by the generator. Whereas that makes intuitive sense, it suddenly changes the emphasis of the contributions, from proposing an implicit generator (presumably powerful for it being implicit) to proposing a way to circumvent the strong independence assumptions of the generator with a mechanism other than more traditional approximate marginalisation of VAEs. In their rebuttal, the authors commented on the use of non-autoregressive VAEs in neural machine translation, and though those observations have indeed been made, they might well be specific to MT. The simplest and more satisfactory response would be to ablate the use of the GAN objective (that is, to train a non-autoregressive VAE, also note that, with the same choice of likelihood, posterior collapse is rather unlikely to happen).\n\nOther problems raised by reviewers were addressed in the rebuttal, and I would like to thank the authors for that. For example, ablating the non-autoregressive generator and comparing to REINFORCE. I believe these improved the submission. \n\nStill, I cannot recommend this version for publication. I would suggest that the authors consider careful ablations of the components they see as precisely important for the results (that currently seems to be the GAN-like objective despite the model not, strictly speaking, requiring it). \n\n\n\n\n", "reviews": [{"review_id": "wOI9hqkvu_-0", "review_text": "* * Summary * * This paper proposed a new text GAN framework by combining non-autoregressive text generator based on transformer , straight-through gradient approximation , and various regularization techniques such as gradient penalty and dropout . The paper demonstrates the superiority of non-autoregressive generator in the context of text GANs through various experiments including unconditional text generation , latent space manipulation and unsupervised decipherment . * * Pros * * - This work narrows the gap between image GAN and text GAN by leveraging recent advances on non-autoregressive text generators and a straight-through gradient approximation . While these components are well studied in previous works , I think this work presents a neat combination of them in order to solve a well-known problem . - The paper provides rich discussions in training text GAN and comprehensive experiments and ablations to demonstrate the usefulness of an implicit text generator in different contexts . * * Concerns & Questions to Answer during rebuttal * * - The original text GAN papers were mainly motivated to address the * exposure bias * problem in maximum likelihood estimation for autoregressive generators . In other words , when we use an autoregressive generator to sequentially generate tokens one by one , there is a distribution mismatch between training and test phase . In your case , now that you already have a non-autoregressive text generator , is there any * theoretical motivation/insights * for using the adversarial training framework ? We know that MLE is statistically efficient ( achieving Cram\u00b4er\u2013Rao Lower Bound ) and possesses many good properties , maybe training the non-autoregressive text generator with MLE ( e.g. , FlowSeq [ 1 ] or variational inference ) is a better choice ? - The non-autoregressive ( NA ) text generator have been well studied recently so the novelty of this work is more on the integration of NA generator with adversarial training . Thus the main challenge here is how to solve the non-differentiability problem . The paper directly leverages a traditional workaround , the straight through estimator , which is a biased gradient approximation . Is the bias going to be an issue and is there any better strategy ? I think the paper need to provide more discussions on this aspect . Overall the method section need to be polished with more details , as I feel this part is currently hard to follow . - In figure 1 , $ z_1 , \\ldots , z_L $ are sampled independently , which are sent to a transformer and later produced the sample . Is the independence between $ z_1 , \\ldots , z_L $ going to be a problem ? When we use transformer to do neural machine translation , the attention mechanism will capture the dependence in the input sentence ( $ z_1 , \\ldots , z_L $ in this context ) and then produce the output correspondingly . Hence will the independence in $ z_1 , \\ldots , z_L $ lead to a less expressive sample distribution in your text generator ( although this is not an issue in image GAN ) ? - The paper also propose to use the Max Gradient Penalty from image GAN domain . The Max Gradient Penalty was introduced under the framework of Wasserstein GAN or Lipschitz GAN framework , which aims to constraint the function space to be Lipschitz smooth . However this work uses the vanilla GAN objective ( eq 12 and eq 13 ) , which is not the WGAN or LGAN framework . Thus the regularization may not be theoretically correct . Also why not instead use the WGAN objective which is empirically more stable and theoretically sound ? - Experiments : In table 2 , all the results for NAGAN use the dropout with a positive ratio . How does NAGAN perform without dropout ? Also I wonder if the comparison in the table and figures are fair , since most previous methods.baselines such as MLE or SeqGAN only use a vanilla RNN/LSTM , while NAGAN has a more complicated structure with transformers , and additional regularization such as gradient penalty and dropout . Perhaps we should control at least the number of parameters to be in the same level . [ 1 ] FlowSeq : Non-Autoregressive Conditional Sequence Generation with Generative Flow Update after rebuttal : After seeing the author response below , no change to my score .", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # Question # 4 The vanilla GAN objective is a special case of Lipschitz GAN [ 6 ] , where $ \\phi ( x ) =\\varphi ( -x ) =\\psi ( -x ) =-\\log ( \\sigma ( -x ) ) $ in their Equation ( 10 ) . The formulation is mentioned just before Section 4.1 in their paper . Their experiments also show that vanilla objectives with the MaxGP penalty perform similarly with other objectives , including the WGAN objective . [ 6 ] Lipschitz Generative Adversarial Nets . ICML2019 # # Question # 5 # # # The problem of the dropout . We add two experiments : we set the dropout rate to 0 in inference ( the dropout rate in training is kept unchanged as 0.25 ) ; we remove the dropout in training . The results are added to Table 2 and Table 11 . Some results are shown here : | MODEL on COCO | LM Score | BLEU_F | BLEU_B | BLEU_HA | FED | |-|-| -- | -- ||-| | NAGAN ( dropout=0.25 ) | 3.69 | 0.315 | 0.257 | 0.283 | 0.108 | | NAGAN ( dropout=0.20 ) | 3.54 | 0.342 | 0.256 | 0.293 | 0.111 | | NAGAN ( dropout=0.10 ) | 3.30 | 0.391 | 0.245 | 0.301 | 0.128 | | NAGAN ( dropout=0.00 ) | 3.21 | 0.415 | 0.221 | 0.289 | 0.191 | | NAGAN ( no dropout in training ) | 3.90 | 0.267 | 0.192 | 0.223 | 0.160 | For NAGAN ( dropout=0 ) , the result agrees with our conclusion in Section 4.1 that the fluency continues to increase , and the diversity declines when the dropout rate becomes smaller . For NAGAN ( no dropout in training ) , we observe spiky losses and unstable training process , but it still outperforms some baselines without MLE pretraining . The phenomenon has been found in image GANs [ 7 , 8 ] , but not significant in autoregressive baselines . As an explanation , the dropout can be regarded as a method of introducing extra latent variables in GANs training . If you are interested , please refer to Appendix A.1.3 where more discussions are added . [ 7 ] Large Scale GAN Training for High Fidelity Natural Image Synthesis . ICLR2019 [ 8 ] A 9k-star repository on Github . https : //github.com/soumith/ganhacks . # # # The problem of the parameter size . We use a very small transformer ( 128-dim hidden , 5 layers , 4 attention heads ) , and NAGAN does not have much more parameters than baselines . We present the numbers of parameters in Table 12 , where * * NAGAN 's generator ( 2.0M parameters ) is smaller than the median size of baselines ' generator ( median : 3.1M parameters ) * * . ( ScratchGAN uses 2-layer LSTM with 512 cells , where we made a wrong statement previously.It has been fixed in Appendix A.4.3 . ) We do not use an unified architecture for our baselines because the GAN training is very sensitive to the generator 's architecture and hyper-parameters . A research confirms our observation and they find that the transformer is very unstable in RL training [ 9 ] . In Table 2 , we add an MLE-trained autoregressive transformer as our baseline , which also has 128-dim hidden states , 5 layers , and 4 attention heads . We also try a SeqGAN with the same generator , but it does not outperform the one with GRU . The results are shown below , and our conclusion does not change . | MODEL on COCO | LM Score | BLEU_F | BLEU_B | BLEU_HA | FED | | -- |-| -- | -- ||-| | GRU | 4.13 | 0.292 | 0.325 | 0.308 | 0.094 | | Transformer | 3.90 | 0.327 | 0.321 | 0.324 | 0.094 | | SeqGAN LSTM | 4.03 | 0.298 | 0.285 | 0.291 | 0.108 | | SeqGAN Transformer | 4.47 | 0.246 | 0.253 | 0.250 | 0.114 | [ 9 ] Stabilizing Transformers for Reinforcement Learning . Arxiv2019 # # # The problem of the gradient penalty . The gradient penalty is designed for gradient-based optimization , where many baselines are using RL . There is no theoretical motivation to adopt the gradient penalty on them . RelGAN and FMGAN are optimized by gradient-based method , where RelGAN uses the Gumbel-Softmax approximation , and FMGAN uses the Soft-Embedding approximation . In Section 4.2 , we conduct ablation studies using an autoregressive generator with the two approximation methods , where the gradient penalty are adopted in all models . As shown in Figure 3 and Table 3 , NAGAN remarkably outperform these two approximation methods . * * Please feel free to let us know if you still have some concerns or questions * * ."}, {"review_id": "wOI9hqkvu_-1", "review_text": "The paper creatively extends text GAN by introducing non-autoregressive generator , which is a well-known notion in translation and VAE like generation but not often applied in a GAN setting . The paper argues that a non-autoregressive generator brings more effective gradient-based optimization and also good latent representation learning capability . The comparison between NAGAN and other text GANS reads okay , but the reviewer concerns the limited scope and significance of this paper . 1 , Given very strong text generation capability of MLE learning and pre-training , NAGAN makes little contribution to push the generation SOTA . Audiences of this approach are also limited . In this paper , given a very old baseline of MLE and a bunch of text GANs , the overall performance of NAGAN is still not much leading . Let alone compare it to other strong pre-trained generators . 2.When claiming good latent representation learning capability , there should be a big gap between NAGAN and text VAEs in this aspect . If the author adds more control and manipulation experiments in text VAE , NAGAN will be not as shining as now . 3.Non-autoregressive generator has difficulties in generalizing to long text generation and conditional generation . How does the author consider such settings , instead of simple unconditional generation in toy datasets like COCO ? Overall , the reviewer thinks this is a well-written paper , but a boardline one considering its limited significance for the venue .", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # Question # 1 We agree that NAGAN does not push the general text generation SOTA , and it is still far behind the large-scale pretrained models like GPT3 . However , NAGAN tries to solve some challenging problems , and we have summarized NAGAN 's contributions on three different research directions in Part 1 of our response . We also mention that MLE still has weaknesses though it is very powerful . The exposure bias problem may lead to repetitions [ 6 ] . It is not convenient in some unsupervised settings where we do not have paired text data , such as unsupervised text style transfer . Adversarial training can address the two issues and more , so we believe that text GANs need further studied . [ 6 ] The Curious Case of Neural Text Degeneration . ICLR2020 # # # Question # 2 * * First , you mentioned that there should be a big gap between NAGAN and VAEs ' representations , but we do not find evidence that supports the statement . * * In our Application I , FMGAN is finetuned after the VAE pretraining . Comparing FMGAN and NAGAN , we can see similar performance on this task if we use the same method ( Offset Vector ) . Moreover , * * we add some experiments for exploring the latent space in Appendix A.2 * * , where we observe that NAGAN and VAEs behave similarly in sentence interpolation . * * Second , NAGAN can be extended to some application that VAE can not do . * * We have summarized our views from the perspective of generation applications : 1 . NAGAN supports back-propagation through discrete text data . 2.NAGAN can match two distributions in text space directly . More is discussed in Part 1 of our response . * * Third , we think VAEs are not in conflict with GANs . * * We can find many interesting models in image generations that combine GANs and VAEs [ 8 , 9 ] . These combined models have advantages from both methods and are convenient to be used in more applications such as image editing . [ 8 ] Autoencoding beyond pixels using a learned similarity metric . ICML2016 [ 9 ] Neural Photo Editing with Introspective Adversarial Networks . ICLR2017 # # # Question # 3 We admit that it will be harder to apply non-autoregressive generators in long text generation or conditional generation with a complex dataset . However , we have already shown NAGAN 's abilities on some challenging datasets . In addition to the small COCO dataset , we use the SNLI dataset , which contains 714,667 samples in the training set , and 42,981 words in the vocabulary . It is very challenging for GANs training because the search space increases exponentially according to the vocabulary size . Moreover , in Figure 3 , we show that our model is not very sensitive to the sentence length ( length varies from 10 to 25 ) , where autoregressive text GANs become worse rapidly as the length increases . We also test NAGAN on the unsupervised word decipherment task , which can be regarded as an unpaired sequence-to-sequence problem . * * NAGAN outperforms all SOTA models which use autoregressive generators . * * We show some cases below ( more in Table 14 ) , where we find the generated sentences have good fluency , even though some words are not correctly deciphered . | | | | -- | -- | | Generated | all cleaned up we enjoyed the area and * * searched * * for some drinks and * * sunglasses * * . | | Golden | all cleaned up we enjoyed the area and headed for some drinks and gaming . | | Generated | our server * * john * * was incredible * * - * * cute , patient , attentive and funny . | | Golden | our server james was incredible : cute , patient , attentive and funny . | * * Thanks for your attention and please let us know if we can address your concerns . * *"}, {"review_id": "wOI9hqkvu_-2", "review_text": "This paper introduces the non-autoregressive generator in the GAN-based text generation , making textGAN can be trained without pre-training and better utilize latent variables to control the style of generated text . Introducing non-autoregressive architectures into GAN-based text generator is a natural idea , and the modelling ability of non-autoregressive generator has been verified at BERT . I think that this paper is above the average because it provides comprehensive experiments ( including unconditional text generation , unsupervised decipherment and sentence manipulation ) , showing the significant improvement in various evaluation metrics compared to the text model without NAR and baselines . However , this paper should have more analysis of how non-autoregressive architectures work in the GAN-based text generation . 1.How latent variables in the non-autoregressive generator influence the content of the generated text ? Can you provide some analysis or examples about it ? ( e.g.change the value of some latent variable continually ( from 0 to 1 ? ) and give the generated text ) . 2.Can you give some analysis about the generation process of the non-autoregressive generator , ( e.g.attention map ) , which makes the generator more interpretable . 3.The user study is absent . 4.Is dropout necessary for the non-autoregressive generator ? What if the dropout rate is 0 , how the performance of generator changes ? 5.Can you give more details about experiments , such as model parameters , training time , inference time and GPU you used ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # Question # 4 We add two experiments : we set dropout rate to 0 in inference ( the dropout rate in training is kept unchanged as 0.25 ) ; we remove dropout in training . The results are added to Table 2 and Table 11 . Some results are shown here : | MODEL on COCO | LM Score | BLEU_F | BLEU_B | BLEU_HA | FED | |-|-| -- | -- ||-| | NAGAN ( dropout=0.25 ) | 3.69 | 0.315 | 0.257 | 0.283 | 0.108 | | NAGAN ( dropout=0.20 ) | 3.54 | 0.342 | 0.256 | 0.293 | 0.111 | | NAGAN ( dropout=0.10 ) | 3.30 | 0.391 | 0.245 | 0.301 | 0.128 | | NAGAN ( dropout=0.00 ) | 3.21 | 0.415 | 0.221 | 0.289 | 0.191 | | NAGAN ( no dropout in training ) | 3.90 | 0.267 | 0.192 | 0.223 | 0.160 | For NAGAN ( dropout=0 ) , it agrees with our conclusion in Section 4.1 that the fluency continues to increase , and the diversity declines when the dropout rate becomes smaller . For NAGAN ( no dropout in training ) , we observe spiky losses and unstable training process , but it still outperforms some baselines without MLE pretraining . The phenomenon has been found in image GANs [ 7 , 8 ] , but not significant in autoregressive baselines . As an explanation , the dropout can be regarded as a method of introducing extra latent variables in GANs training . If you are interested , please refer to Appendix A.1.3 where more discussions are added . [ 2 ] Large Scale GAN Training for High Fidelity Natural Image Synthesis . ICLR2019 [ 3 ] A 9k-star repository on Github . https : //github.com/soumith/ganhacks . # # # Question # 5 We add Table 12 to show the parameter size of all models . We use a very small transformer ( 128-dim hidden , 5 layers , 4 attention heads ) , where NAGAN 's generator ( 2.0M parameters ) is smaller than the median size of baselines ' generator ( 3.1M parameters ) . We add some training details in Appendix A.7 ( where we also fix some errors in the previous version ) . Some details are shown below . For the COCO and SNLI dataset , we train our model for 100 epochs ( each epoch contains 1500 batches of samples ) . Each training run used approximately 4 Intel Xeon E5-2690 v4 CPUs at 2.60GHz , and 1 Nvidia GeForce RTX 2080 Ti GPU . Although the results of 10-hour training are close to the reported performance ( with a gap of $ 0.01 $ in terms of $ BLEU_ { HA } $ ) , we finish the 100 epochs for around 28 hours . We also report the latency of one generator training step and the inference in the table below . Compared with an autoregressive Transformer of the same architecture , NAGAN is slower in training ( because our generator 's updates need gradients from the discriminator ) but faster in inference ( 6.66x speed up ) . The fast inference is brought by the parallel decoding . | Model | Generator Training Step | Inference Latency | | -- |-|-| | MLE-Transformer | 145ms ( 1.00x ) | 213ms ( 1.00x ) | | NAGAN | 210ms ( 0.69x ) | 32ms ( 6.66x ) | * * Please feel free to let us know if you still have some concerns or questions * * ."}], "0": {"review_id": "wOI9hqkvu_-0", "review_text": "* * Summary * * This paper proposed a new text GAN framework by combining non-autoregressive text generator based on transformer , straight-through gradient approximation , and various regularization techniques such as gradient penalty and dropout . The paper demonstrates the superiority of non-autoregressive generator in the context of text GANs through various experiments including unconditional text generation , latent space manipulation and unsupervised decipherment . * * Pros * * - This work narrows the gap between image GAN and text GAN by leveraging recent advances on non-autoregressive text generators and a straight-through gradient approximation . While these components are well studied in previous works , I think this work presents a neat combination of them in order to solve a well-known problem . - The paper provides rich discussions in training text GAN and comprehensive experiments and ablations to demonstrate the usefulness of an implicit text generator in different contexts . * * Concerns & Questions to Answer during rebuttal * * - The original text GAN papers were mainly motivated to address the * exposure bias * problem in maximum likelihood estimation for autoregressive generators . In other words , when we use an autoregressive generator to sequentially generate tokens one by one , there is a distribution mismatch between training and test phase . In your case , now that you already have a non-autoregressive text generator , is there any * theoretical motivation/insights * for using the adversarial training framework ? We know that MLE is statistically efficient ( achieving Cram\u00b4er\u2013Rao Lower Bound ) and possesses many good properties , maybe training the non-autoregressive text generator with MLE ( e.g. , FlowSeq [ 1 ] or variational inference ) is a better choice ? - The non-autoregressive ( NA ) text generator have been well studied recently so the novelty of this work is more on the integration of NA generator with adversarial training . Thus the main challenge here is how to solve the non-differentiability problem . The paper directly leverages a traditional workaround , the straight through estimator , which is a biased gradient approximation . Is the bias going to be an issue and is there any better strategy ? I think the paper need to provide more discussions on this aspect . Overall the method section need to be polished with more details , as I feel this part is currently hard to follow . - In figure 1 , $ z_1 , \\ldots , z_L $ are sampled independently , which are sent to a transformer and later produced the sample . Is the independence between $ z_1 , \\ldots , z_L $ going to be a problem ? When we use transformer to do neural machine translation , the attention mechanism will capture the dependence in the input sentence ( $ z_1 , \\ldots , z_L $ in this context ) and then produce the output correspondingly . Hence will the independence in $ z_1 , \\ldots , z_L $ lead to a less expressive sample distribution in your text generator ( although this is not an issue in image GAN ) ? - The paper also propose to use the Max Gradient Penalty from image GAN domain . The Max Gradient Penalty was introduced under the framework of Wasserstein GAN or Lipschitz GAN framework , which aims to constraint the function space to be Lipschitz smooth . However this work uses the vanilla GAN objective ( eq 12 and eq 13 ) , which is not the WGAN or LGAN framework . Thus the regularization may not be theoretically correct . Also why not instead use the WGAN objective which is empirically more stable and theoretically sound ? - Experiments : In table 2 , all the results for NAGAN use the dropout with a positive ratio . How does NAGAN perform without dropout ? Also I wonder if the comparison in the table and figures are fair , since most previous methods.baselines such as MLE or SeqGAN only use a vanilla RNN/LSTM , while NAGAN has a more complicated structure with transformers , and additional regularization such as gradient penalty and dropout . Perhaps we should control at least the number of parameters to be in the same level . [ 1 ] FlowSeq : Non-Autoregressive Conditional Sequence Generation with Generative Flow Update after rebuttal : After seeing the author response below , no change to my score .", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # Question # 4 The vanilla GAN objective is a special case of Lipschitz GAN [ 6 ] , where $ \\phi ( x ) =\\varphi ( -x ) =\\psi ( -x ) =-\\log ( \\sigma ( -x ) ) $ in their Equation ( 10 ) . The formulation is mentioned just before Section 4.1 in their paper . Their experiments also show that vanilla objectives with the MaxGP penalty perform similarly with other objectives , including the WGAN objective . [ 6 ] Lipschitz Generative Adversarial Nets . ICML2019 # # Question # 5 # # # The problem of the dropout . We add two experiments : we set the dropout rate to 0 in inference ( the dropout rate in training is kept unchanged as 0.25 ) ; we remove the dropout in training . The results are added to Table 2 and Table 11 . Some results are shown here : | MODEL on COCO | LM Score | BLEU_F | BLEU_B | BLEU_HA | FED | |-|-| -- | -- ||-| | NAGAN ( dropout=0.25 ) | 3.69 | 0.315 | 0.257 | 0.283 | 0.108 | | NAGAN ( dropout=0.20 ) | 3.54 | 0.342 | 0.256 | 0.293 | 0.111 | | NAGAN ( dropout=0.10 ) | 3.30 | 0.391 | 0.245 | 0.301 | 0.128 | | NAGAN ( dropout=0.00 ) | 3.21 | 0.415 | 0.221 | 0.289 | 0.191 | | NAGAN ( no dropout in training ) | 3.90 | 0.267 | 0.192 | 0.223 | 0.160 | For NAGAN ( dropout=0 ) , the result agrees with our conclusion in Section 4.1 that the fluency continues to increase , and the diversity declines when the dropout rate becomes smaller . For NAGAN ( no dropout in training ) , we observe spiky losses and unstable training process , but it still outperforms some baselines without MLE pretraining . The phenomenon has been found in image GANs [ 7 , 8 ] , but not significant in autoregressive baselines . As an explanation , the dropout can be regarded as a method of introducing extra latent variables in GANs training . If you are interested , please refer to Appendix A.1.3 where more discussions are added . [ 7 ] Large Scale GAN Training for High Fidelity Natural Image Synthesis . ICLR2019 [ 8 ] A 9k-star repository on Github . https : //github.com/soumith/ganhacks . # # # The problem of the parameter size . We use a very small transformer ( 128-dim hidden , 5 layers , 4 attention heads ) , and NAGAN does not have much more parameters than baselines . We present the numbers of parameters in Table 12 , where * * NAGAN 's generator ( 2.0M parameters ) is smaller than the median size of baselines ' generator ( median : 3.1M parameters ) * * . ( ScratchGAN uses 2-layer LSTM with 512 cells , where we made a wrong statement previously.It has been fixed in Appendix A.4.3 . ) We do not use an unified architecture for our baselines because the GAN training is very sensitive to the generator 's architecture and hyper-parameters . A research confirms our observation and they find that the transformer is very unstable in RL training [ 9 ] . In Table 2 , we add an MLE-trained autoregressive transformer as our baseline , which also has 128-dim hidden states , 5 layers , and 4 attention heads . We also try a SeqGAN with the same generator , but it does not outperform the one with GRU . The results are shown below , and our conclusion does not change . | MODEL on COCO | LM Score | BLEU_F | BLEU_B | BLEU_HA | FED | | -- |-| -- | -- ||-| | GRU | 4.13 | 0.292 | 0.325 | 0.308 | 0.094 | | Transformer | 3.90 | 0.327 | 0.321 | 0.324 | 0.094 | | SeqGAN LSTM | 4.03 | 0.298 | 0.285 | 0.291 | 0.108 | | SeqGAN Transformer | 4.47 | 0.246 | 0.253 | 0.250 | 0.114 | [ 9 ] Stabilizing Transformers for Reinforcement Learning . Arxiv2019 # # # The problem of the gradient penalty . The gradient penalty is designed for gradient-based optimization , where many baselines are using RL . There is no theoretical motivation to adopt the gradient penalty on them . RelGAN and FMGAN are optimized by gradient-based method , where RelGAN uses the Gumbel-Softmax approximation , and FMGAN uses the Soft-Embedding approximation . In Section 4.2 , we conduct ablation studies using an autoregressive generator with the two approximation methods , where the gradient penalty are adopted in all models . As shown in Figure 3 and Table 3 , NAGAN remarkably outperform these two approximation methods . * * Please feel free to let us know if you still have some concerns or questions * * ."}, "1": {"review_id": "wOI9hqkvu_-1", "review_text": "The paper creatively extends text GAN by introducing non-autoregressive generator , which is a well-known notion in translation and VAE like generation but not often applied in a GAN setting . The paper argues that a non-autoregressive generator brings more effective gradient-based optimization and also good latent representation learning capability . The comparison between NAGAN and other text GANS reads okay , but the reviewer concerns the limited scope and significance of this paper . 1 , Given very strong text generation capability of MLE learning and pre-training , NAGAN makes little contribution to push the generation SOTA . Audiences of this approach are also limited . In this paper , given a very old baseline of MLE and a bunch of text GANs , the overall performance of NAGAN is still not much leading . Let alone compare it to other strong pre-trained generators . 2.When claiming good latent representation learning capability , there should be a big gap between NAGAN and text VAEs in this aspect . If the author adds more control and manipulation experiments in text VAE , NAGAN will be not as shining as now . 3.Non-autoregressive generator has difficulties in generalizing to long text generation and conditional generation . How does the author consider such settings , instead of simple unconditional generation in toy datasets like COCO ? Overall , the reviewer thinks this is a well-written paper , but a boardline one considering its limited significance for the venue .", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # Question # 1 We agree that NAGAN does not push the general text generation SOTA , and it is still far behind the large-scale pretrained models like GPT3 . However , NAGAN tries to solve some challenging problems , and we have summarized NAGAN 's contributions on three different research directions in Part 1 of our response . We also mention that MLE still has weaknesses though it is very powerful . The exposure bias problem may lead to repetitions [ 6 ] . It is not convenient in some unsupervised settings where we do not have paired text data , such as unsupervised text style transfer . Adversarial training can address the two issues and more , so we believe that text GANs need further studied . [ 6 ] The Curious Case of Neural Text Degeneration . ICLR2020 # # # Question # 2 * * First , you mentioned that there should be a big gap between NAGAN and VAEs ' representations , but we do not find evidence that supports the statement . * * In our Application I , FMGAN is finetuned after the VAE pretraining . Comparing FMGAN and NAGAN , we can see similar performance on this task if we use the same method ( Offset Vector ) . Moreover , * * we add some experiments for exploring the latent space in Appendix A.2 * * , where we observe that NAGAN and VAEs behave similarly in sentence interpolation . * * Second , NAGAN can be extended to some application that VAE can not do . * * We have summarized our views from the perspective of generation applications : 1 . NAGAN supports back-propagation through discrete text data . 2.NAGAN can match two distributions in text space directly . More is discussed in Part 1 of our response . * * Third , we think VAEs are not in conflict with GANs . * * We can find many interesting models in image generations that combine GANs and VAEs [ 8 , 9 ] . These combined models have advantages from both methods and are convenient to be used in more applications such as image editing . [ 8 ] Autoencoding beyond pixels using a learned similarity metric . ICML2016 [ 9 ] Neural Photo Editing with Introspective Adversarial Networks . ICLR2017 # # # Question # 3 We admit that it will be harder to apply non-autoregressive generators in long text generation or conditional generation with a complex dataset . However , we have already shown NAGAN 's abilities on some challenging datasets . In addition to the small COCO dataset , we use the SNLI dataset , which contains 714,667 samples in the training set , and 42,981 words in the vocabulary . It is very challenging for GANs training because the search space increases exponentially according to the vocabulary size . Moreover , in Figure 3 , we show that our model is not very sensitive to the sentence length ( length varies from 10 to 25 ) , where autoregressive text GANs become worse rapidly as the length increases . We also test NAGAN on the unsupervised word decipherment task , which can be regarded as an unpaired sequence-to-sequence problem . * * NAGAN outperforms all SOTA models which use autoregressive generators . * * We show some cases below ( more in Table 14 ) , where we find the generated sentences have good fluency , even though some words are not correctly deciphered . | | | | -- | -- | | Generated | all cleaned up we enjoyed the area and * * searched * * for some drinks and * * sunglasses * * . | | Golden | all cleaned up we enjoyed the area and headed for some drinks and gaming . | | Generated | our server * * john * * was incredible * * - * * cute , patient , attentive and funny . | | Golden | our server james was incredible : cute , patient , attentive and funny . | * * Thanks for your attention and please let us know if we can address your concerns . * *"}, "2": {"review_id": "wOI9hqkvu_-2", "review_text": "This paper introduces the non-autoregressive generator in the GAN-based text generation , making textGAN can be trained without pre-training and better utilize latent variables to control the style of generated text . Introducing non-autoregressive architectures into GAN-based text generator is a natural idea , and the modelling ability of non-autoregressive generator has been verified at BERT . I think that this paper is above the average because it provides comprehensive experiments ( including unconditional text generation , unsupervised decipherment and sentence manipulation ) , showing the significant improvement in various evaluation metrics compared to the text model without NAR and baselines . However , this paper should have more analysis of how non-autoregressive architectures work in the GAN-based text generation . 1.How latent variables in the non-autoregressive generator influence the content of the generated text ? Can you provide some analysis or examples about it ? ( e.g.change the value of some latent variable continually ( from 0 to 1 ? ) and give the generated text ) . 2.Can you give some analysis about the generation process of the non-autoregressive generator , ( e.g.attention map ) , which makes the generator more interpretable . 3.The user study is absent . 4.Is dropout necessary for the non-autoregressive generator ? What if the dropout rate is 0 , how the performance of generator changes ? 5.Can you give more details about experiments , such as model parameters , training time , inference time and GPU you used ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # Question # 4 We add two experiments : we set dropout rate to 0 in inference ( the dropout rate in training is kept unchanged as 0.25 ) ; we remove dropout in training . The results are added to Table 2 and Table 11 . Some results are shown here : | MODEL on COCO | LM Score | BLEU_F | BLEU_B | BLEU_HA | FED | |-|-| -- | -- ||-| | NAGAN ( dropout=0.25 ) | 3.69 | 0.315 | 0.257 | 0.283 | 0.108 | | NAGAN ( dropout=0.20 ) | 3.54 | 0.342 | 0.256 | 0.293 | 0.111 | | NAGAN ( dropout=0.10 ) | 3.30 | 0.391 | 0.245 | 0.301 | 0.128 | | NAGAN ( dropout=0.00 ) | 3.21 | 0.415 | 0.221 | 0.289 | 0.191 | | NAGAN ( no dropout in training ) | 3.90 | 0.267 | 0.192 | 0.223 | 0.160 | For NAGAN ( dropout=0 ) , it agrees with our conclusion in Section 4.1 that the fluency continues to increase , and the diversity declines when the dropout rate becomes smaller . For NAGAN ( no dropout in training ) , we observe spiky losses and unstable training process , but it still outperforms some baselines without MLE pretraining . The phenomenon has been found in image GANs [ 7 , 8 ] , but not significant in autoregressive baselines . As an explanation , the dropout can be regarded as a method of introducing extra latent variables in GANs training . If you are interested , please refer to Appendix A.1.3 where more discussions are added . [ 2 ] Large Scale GAN Training for High Fidelity Natural Image Synthesis . ICLR2019 [ 3 ] A 9k-star repository on Github . https : //github.com/soumith/ganhacks . # # # Question # 5 We add Table 12 to show the parameter size of all models . We use a very small transformer ( 128-dim hidden , 5 layers , 4 attention heads ) , where NAGAN 's generator ( 2.0M parameters ) is smaller than the median size of baselines ' generator ( 3.1M parameters ) . We add some training details in Appendix A.7 ( where we also fix some errors in the previous version ) . Some details are shown below . For the COCO and SNLI dataset , we train our model for 100 epochs ( each epoch contains 1500 batches of samples ) . Each training run used approximately 4 Intel Xeon E5-2690 v4 CPUs at 2.60GHz , and 1 Nvidia GeForce RTX 2080 Ti GPU . Although the results of 10-hour training are close to the reported performance ( with a gap of $ 0.01 $ in terms of $ BLEU_ { HA } $ ) , we finish the 100 epochs for around 28 hours . We also report the latency of one generator training step and the inference in the table below . Compared with an autoregressive Transformer of the same architecture , NAGAN is slower in training ( because our generator 's updates need gradients from the discriminator ) but faster in inference ( 6.66x speed up ) . The fast inference is brought by the parallel decoding . | Model | Generator Training Step | Inference Latency | | -- |-|-| | MLE-Transformer | 145ms ( 1.00x ) | 213ms ( 1.00x ) | | NAGAN | 210ms ( 0.69x ) | 32ms ( 6.66x ) | * * Please feel free to let us know if you still have some concerns or questions * * ."}}