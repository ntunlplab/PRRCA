{"year": "2018", "forum": "Sy-dQG-Rb", "title": "Neural Speed Reading via Skim-RNN", "decision": "Accept (Poster)", "meta_review": "this submission proposes an efficient parametrization of a recurrent neural net by using two transition functions (one large and one small) to reduce the amount of computation (though, without actual improvement on GPU.) the reviewers found the submission very positive.\n\nplease, do not forget to include all the result and discussion on the proposed approach's relationship to VCRNN which was presented at the same conference just a year ago.", "reviews": [{"review_id": "Sy-dQG-Rb-0", "review_text": "The paper proposes a way to speed up the inference time of RNN via Skim mechanism where only a small part of hidden variable is updated once the model has decided a corresponding word token seems irrelevant w.r.t. a given task. While the proposed idea might be too simple, the authors show the importance of it via thorough experiments. It also seems to be easily integrated into existing RNN systems without heavy tuning as shown in the experiments. * One advantage of proposed idea claimed against the skip-RNN is that the Skim-RNN can generate the same length of output sequence given input sequence. It is not clear to me whether the output prediction on those skimmed tokens is made of the full hidden state (updated + copied) or a first few dimensions of the hidden state. I assume that the full hidden states are used for prediction. It is somehow interesting because it may mean the prediction heavily depends on small (d') part of the hidden state. In the second and third figures of Figure 10, the model made wrong decisions when the adjacent tokens were both skimmed although the target token was not skimmed, and it might be related to the above assumption. In this sense, it would be more beneficial if the skimming happens over consecutive tokens (focus on a region, not on an individual token). * This paper would gain more attention from practitioners because of its practical purpose. In a similar vein, it would be also good to have some comments on training time as well. In a general situation where there is no need of re-training, training time would be meaningless, however, if one requires updating the model on the fly, it would be also meaningful to have some intuition on training time. * One obvious way to reduce the computational complexity of RNN is to reduce the size of the hidden state. In this sense, it makes this manuscript more comprehensive if there are some comparisons with RNNs with limited-sized hidden dimensions (say 10 or 20). So that readers can check benefits of the skim RNN against skip-RNN and small-sized RNN. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your insightful and supportive comments ; we make a few clarifications and discuss additional experiments inspired by your suggestions . Clarification : - Output of skimmed tokens : The output of skimmed tokens is the full hidden state ( concatenating updated and copied parts ) . Suggestions : - Focusing on region : Thank you for your suggestion , and we will consider this approach in future work . - Training time : Since Skim-RNN needs to compute outputs for both RNNs ( big and small ) during training , it requires more time for the same number of training steps . For instance , in SQuAD , Skim-LSTM takes 8 hours of training whereas LSTM takes 5 hours until convergence . However , in terms of number of training steps , they both require approximately 18k steps . We will include this in any final version of the paper . - Comparison with small hidden size : When the hidden size becomes 10 ( from 100 ) for vanilla RNN , there is 3.4 % accuracy drop in SST ( 7.1x less FLOP ) and 6.1 % accuracy drop in Rotten Tomatoes ( 7.1x less FLOP ) . There is a clear trade-off between accuracy and FLOP when smaller hidden size is used . We are currently experimenting with other datasets and will include them in the next revision ."}, {"review_id": "Sy-dQG-Rb-1", "review_text": "Summary: The paper proposes a learnable skimming mechanism for RNN. The model decides whether to send the word to a larger heavy-weight RNN or a light-weight RNN. The heavy-weight and the light-weight RNN each controls a portion of the hidden state. The paper finds that with the proposed skimming method, they achieve a significant reduction in terms of FLOPS. Although it doesn\u2019t contribute to much speedup on modern GPU hardware, there is a good speedup on CPU, and it is more power efficient. Contribution: - The paper proposes to use a small RNN to read unimportant text. Unlike (Yu et al., 2017), which skips the text, here the model decides between small and large RNN. Pros: - Models that dynamically decide the amount of computation make intuitive sense and are of general interests. - The paper presents solid experimentation on various text classification and question answering datasets. - The proposed method has shown reasonable reduction in FLOPS and CPU speedup with no significant accuracy degradation (increase in accuracy in some tasks). - The paper is well written, and the presentation is good. Cons: - Each model component is not novel. The authors propose to use Gumbel softmax, but does compare other gradient estimators. It would be good to use REINFORCE to do a fair comparison with (Yu et al., 2017 ) to see the benefit of using small RNN. - The authors report that training from scratch results in unstable skim rate, while Half pretrain seems to always work better than fully pretrained ones. This makes the success of training a bit adhoc, as one need to actively tune the number of pretraining steps. - Although there is difference from (Yu et al., 2017), the contribution of this paper is still incremental. Questions: - Although it is out of the scope for this paper to achieve GPU level speedup, I am curious to know some numbers on GPU speedup. - One recommended task would probably be text summarization, in which the attended text can contribute to the output of the summary. Conclusion: - Based on the comments above, I recommend Accept", "rating": "7: Good paper, accept", "reply_text": "Thank you for your insightful and supportive comments ; we discuss additional experiments following your suggestions and make a few clarifications . Suggestions : - Other gradient methods : Thank you for your suggestion , and we found that REINFORCE substantially underperforms ( less than 20 % accuracy on SST ) Gumbel-Softmax within 50k steps of training . We suspect that this is due to the high variance of REINFORCE , which becomes even worse in our case where the sample space exponentially increases with the sequence length . We found that temperature annealing is not as bad as REINFORCE , but the accuracy is still ~0.5 % lower than Gumbel-Softmax and the convergence is slower . We will include this in any final version of the paper . - Text summarization : We agree that it is an appropriate application for Skim-RNN . We will consider it for potential future work . Clarifications : - Adhoc training due to required pretraining : while pretraining definitely helps in QA , we would like to emphasize that no-pretraining still performs well ( classification results are without pretraining ) , and there is no added cost of pretraining ; that is , pretraining + finetuning has a similar training time to training from scratch . - GPU speed up : Theoretically Skim-RNN could have speed up on GPU . However , because parallelization has log-time cost , this would be negligible compared to other costs ."}, {"review_id": "Sy-dQG-Rb-2", "review_text": "This paper proposes a skim-RNN, which skims unimportant inputs with a small RNN while normally processes important inputs with a standard RNN for fast inference. Pros. - The idea of switching small and standard RNNs for skimming and full reading respectively is quite simple and intuitive. - The paper is clearly written with enough explanations about the proposal method and the novelty. - One of the most difficult problems of this approach (non-differentiable) is elegantly solved by employing gumbel-softmax - The effectiveness (mainly inference speed improvement with CPU) is validated by various experiments. The examples (Table 3 and Figure 6) show that the skimming process is appropriately performed (skimmed unimportant words while fully read relevant words etc.) Cons. - The idea is quite simple and the novelty is incremental by considering the difference from skip-RNN. - No comments about computational costs during training with GPU (it would not increase the computational cost so much, but gumbel-softmax may require more iterations). Comments: - Section 1, Introduction, 2nd paragraph: \u2018peed\u2019 -> \u2018speed\u2019(?) - Equation (5): It would be better to explain why it uses the Gumbel distribution. To make (5) behave like argmax, only temperature parameter seems to be enough. - Section 4.1: What is \u201cglobal training step\u201d? - Section 4.2, \u201cWe also observe that the F1 score of Skim-LSTM is more stable across different configurations and computational cost.\u201d: This seems to be very interesting phenomena. Is there some discussion of why skim-LSTM is more stable? - Section 4.2, the last paragraph: \u201cTable 6 shows\u201d -> \u201cFigure 6 shows\u201d ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your insightful and supportive comments ; we discuss additional experiments inspired your suggestions and make a few clarifications . Suggestions : - Training cost with GPU : Thank you for the suggestion , and we report training cost in two dimensions : memory and time . Assuming d/d \u2019 =100/20 on SQuAD , memory consumption is only ~5 % more than vanilla RNN . Since Skim-RNN needs to compute outputs for both RNNs ( big and small ) during training , it requires more time for the same number of training steps . For instance , on SQuAD , Skim-LSTM takes 8 hours of training whereas LSTM takes 5 hours until convergence . However , in terms of number of training steps , they both require approximately 18k steps . - Why Gumbel-softmax and not just temperature : we used Gumbel-Softmax mainly due to its theoretical guarantee shown in Jang et al ( 2017 ) . We experimented with temperature annealing only , and found that the accuracy is ~0.5 % lower on SQuAD and convergence is a little slower . While there is some advantage of Gumbel-softmax , It seems temperature annealing is also an effective technique . - Typos : thank you for correcting them and we have fixed them in the current revision . Clarifications : - Sec 4.1 global training step : We meant just \u201c training step \u201d , and we fixed it in the most recent revision . - Sec 4.2 stableness : We actually meant that LSTM accuracy dips ( is unstable ) when we use smaller hidden state size to reduce FLOP , while Skim-LSTM accuracy does not dip even with high reduction in FLOP ."}], "0": {"review_id": "Sy-dQG-Rb-0", "review_text": "The paper proposes a way to speed up the inference time of RNN via Skim mechanism where only a small part of hidden variable is updated once the model has decided a corresponding word token seems irrelevant w.r.t. a given task. While the proposed idea might be too simple, the authors show the importance of it via thorough experiments. It also seems to be easily integrated into existing RNN systems without heavy tuning as shown in the experiments. * One advantage of proposed idea claimed against the skip-RNN is that the Skim-RNN can generate the same length of output sequence given input sequence. It is not clear to me whether the output prediction on those skimmed tokens is made of the full hidden state (updated + copied) or a first few dimensions of the hidden state. I assume that the full hidden states are used for prediction. It is somehow interesting because it may mean the prediction heavily depends on small (d') part of the hidden state. In the second and third figures of Figure 10, the model made wrong decisions when the adjacent tokens were both skimmed although the target token was not skimmed, and it might be related to the above assumption. In this sense, it would be more beneficial if the skimming happens over consecutive tokens (focus on a region, not on an individual token). * This paper would gain more attention from practitioners because of its practical purpose. In a similar vein, it would be also good to have some comments on training time as well. In a general situation where there is no need of re-training, training time would be meaningless, however, if one requires updating the model on the fly, it would be also meaningful to have some intuition on training time. * One obvious way to reduce the computational complexity of RNN is to reduce the size of the hidden state. In this sense, it makes this manuscript more comprehensive if there are some comparisons with RNNs with limited-sized hidden dimensions (say 10 or 20). So that readers can check benefits of the skim RNN against skip-RNN and small-sized RNN. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your insightful and supportive comments ; we make a few clarifications and discuss additional experiments inspired by your suggestions . Clarification : - Output of skimmed tokens : The output of skimmed tokens is the full hidden state ( concatenating updated and copied parts ) . Suggestions : - Focusing on region : Thank you for your suggestion , and we will consider this approach in future work . - Training time : Since Skim-RNN needs to compute outputs for both RNNs ( big and small ) during training , it requires more time for the same number of training steps . For instance , in SQuAD , Skim-LSTM takes 8 hours of training whereas LSTM takes 5 hours until convergence . However , in terms of number of training steps , they both require approximately 18k steps . We will include this in any final version of the paper . - Comparison with small hidden size : When the hidden size becomes 10 ( from 100 ) for vanilla RNN , there is 3.4 % accuracy drop in SST ( 7.1x less FLOP ) and 6.1 % accuracy drop in Rotten Tomatoes ( 7.1x less FLOP ) . There is a clear trade-off between accuracy and FLOP when smaller hidden size is used . We are currently experimenting with other datasets and will include them in the next revision ."}, "1": {"review_id": "Sy-dQG-Rb-1", "review_text": "Summary: The paper proposes a learnable skimming mechanism for RNN. The model decides whether to send the word to a larger heavy-weight RNN or a light-weight RNN. The heavy-weight and the light-weight RNN each controls a portion of the hidden state. The paper finds that with the proposed skimming method, they achieve a significant reduction in terms of FLOPS. Although it doesn\u2019t contribute to much speedup on modern GPU hardware, there is a good speedup on CPU, and it is more power efficient. Contribution: - The paper proposes to use a small RNN to read unimportant text. Unlike (Yu et al., 2017), which skips the text, here the model decides between small and large RNN. Pros: - Models that dynamically decide the amount of computation make intuitive sense and are of general interests. - The paper presents solid experimentation on various text classification and question answering datasets. - The proposed method has shown reasonable reduction in FLOPS and CPU speedup with no significant accuracy degradation (increase in accuracy in some tasks). - The paper is well written, and the presentation is good. Cons: - Each model component is not novel. The authors propose to use Gumbel softmax, but does compare other gradient estimators. It would be good to use REINFORCE to do a fair comparison with (Yu et al., 2017 ) to see the benefit of using small RNN. - The authors report that training from scratch results in unstable skim rate, while Half pretrain seems to always work better than fully pretrained ones. This makes the success of training a bit adhoc, as one need to actively tune the number of pretraining steps. - Although there is difference from (Yu et al., 2017), the contribution of this paper is still incremental. Questions: - Although it is out of the scope for this paper to achieve GPU level speedup, I am curious to know some numbers on GPU speedup. - One recommended task would probably be text summarization, in which the attended text can contribute to the output of the summary. Conclusion: - Based on the comments above, I recommend Accept", "rating": "7: Good paper, accept", "reply_text": "Thank you for your insightful and supportive comments ; we discuss additional experiments following your suggestions and make a few clarifications . Suggestions : - Other gradient methods : Thank you for your suggestion , and we found that REINFORCE substantially underperforms ( less than 20 % accuracy on SST ) Gumbel-Softmax within 50k steps of training . We suspect that this is due to the high variance of REINFORCE , which becomes even worse in our case where the sample space exponentially increases with the sequence length . We found that temperature annealing is not as bad as REINFORCE , but the accuracy is still ~0.5 % lower than Gumbel-Softmax and the convergence is slower . We will include this in any final version of the paper . - Text summarization : We agree that it is an appropriate application for Skim-RNN . We will consider it for potential future work . Clarifications : - Adhoc training due to required pretraining : while pretraining definitely helps in QA , we would like to emphasize that no-pretraining still performs well ( classification results are without pretraining ) , and there is no added cost of pretraining ; that is , pretraining + finetuning has a similar training time to training from scratch . - GPU speed up : Theoretically Skim-RNN could have speed up on GPU . However , because parallelization has log-time cost , this would be negligible compared to other costs ."}, "2": {"review_id": "Sy-dQG-Rb-2", "review_text": "This paper proposes a skim-RNN, which skims unimportant inputs with a small RNN while normally processes important inputs with a standard RNN for fast inference. Pros. - The idea of switching small and standard RNNs for skimming and full reading respectively is quite simple and intuitive. - The paper is clearly written with enough explanations about the proposal method and the novelty. - One of the most difficult problems of this approach (non-differentiable) is elegantly solved by employing gumbel-softmax - The effectiveness (mainly inference speed improvement with CPU) is validated by various experiments. The examples (Table 3 and Figure 6) show that the skimming process is appropriately performed (skimmed unimportant words while fully read relevant words etc.) Cons. - The idea is quite simple and the novelty is incremental by considering the difference from skip-RNN. - No comments about computational costs during training with GPU (it would not increase the computational cost so much, but gumbel-softmax may require more iterations). Comments: - Section 1, Introduction, 2nd paragraph: \u2018peed\u2019 -> \u2018speed\u2019(?) - Equation (5): It would be better to explain why it uses the Gumbel distribution. To make (5) behave like argmax, only temperature parameter seems to be enough. - Section 4.1: What is \u201cglobal training step\u201d? - Section 4.2, \u201cWe also observe that the F1 score of Skim-LSTM is more stable across different configurations and computational cost.\u201d: This seems to be very interesting phenomena. Is there some discussion of why skim-LSTM is more stable? - Section 4.2, the last paragraph: \u201cTable 6 shows\u201d -> \u201cFigure 6 shows\u201d ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your insightful and supportive comments ; we discuss additional experiments inspired your suggestions and make a few clarifications . Suggestions : - Training cost with GPU : Thank you for the suggestion , and we report training cost in two dimensions : memory and time . Assuming d/d \u2019 =100/20 on SQuAD , memory consumption is only ~5 % more than vanilla RNN . Since Skim-RNN needs to compute outputs for both RNNs ( big and small ) during training , it requires more time for the same number of training steps . For instance , on SQuAD , Skim-LSTM takes 8 hours of training whereas LSTM takes 5 hours until convergence . However , in terms of number of training steps , they both require approximately 18k steps . - Why Gumbel-softmax and not just temperature : we used Gumbel-Softmax mainly due to its theoretical guarantee shown in Jang et al ( 2017 ) . We experimented with temperature annealing only , and found that the accuracy is ~0.5 % lower on SQuAD and convergence is a little slower . While there is some advantage of Gumbel-softmax , It seems temperature annealing is also an effective technique . - Typos : thank you for correcting them and we have fixed them in the current revision . Clarifications : - Sec 4.1 global training step : We meant just \u201c training step \u201d , and we fixed it in the most recent revision . - Sec 4.2 stableness : We actually meant that LSTM accuracy dips ( is unstable ) when we use smaller hidden state size to reduce FLOP , while Skim-LSTM accuracy does not dip even with high reduction in FLOP ."}}