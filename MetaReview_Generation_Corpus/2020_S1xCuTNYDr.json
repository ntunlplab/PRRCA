{"year": "2020", "forum": "S1xCuTNYDr", "title": "Regularizing Black-box Models for Improved Interpretability", "decision": "Reject", "meta_review": "This paper investigates a promising direction on the important topic of interpretability; the reviewers find a variety of issues with the work, and I urge the authors to refine and extend their investigations.", "reviews": [{"review_id": "S1xCuTNYDr-0", "review_text": " The paper proposes a new type of regularizer to improve explainability in neural networks. The proposed regularizer is largely based on two metrics, namely fidelity and stability. It optimizes for fidelity and stability as a regularization objective in a differentiable manner. I would recommend for accept, as the paper shows in its experiments that the explainability of neural networks can be improved with the two proposed regularizer, which outperforms simple baselines of l1 and l2 regularizers. The paper's method is generic, and can be applied to almost all machine learning models with gradient-based optimization, making it helpful to building explainable machine learning systems. However, I would also like to note that the results in this paper are somewhat unsurprising. The ExpO-Fidelity and ExpO-Stability regularizers can be seen as (almost) directly optimizing the fidelity and stability metric for explainability, so one would naturally expect that models trained with these regularizers will do better on the two metrics above. In addition, I do not see much value in the derivations of Section 3.2. The conclusion that \"explainable models with smaller local variances ... are likely to have explanations of higher fidelity\" is unsurprising and almost a straightforward claim.", "rating": "6: Weak Accept", "reply_text": "The fidelity and stability metrics have been used to measure interpretability , for local explanations , in previous work . Although they are proxies for the qualitative concept of \u201c interpretability \u201d in the general sense , they are useful ; because a local explanation answers the question \u201c What could I have done differently to get a different outcome ? \u201d , a higher fidelity explanation will yield more accurate advice and a more stable explanation will be more trustworthy . A major contribution of our work is showing that these metrics can in fact be improved using a simple and general method . Moreover , we disagree with the sentiment that research needs to have a \u201c surprise factor \u201d in order to be impactful . The purpose of the generalization bound is to demonstrate theoretically that regularizing for fidelity on the training points should improve fidelity on new points . While not a main contribution of the work , and even though the result itself is not technically difficult to derive , we do think that it is a positive addition to the work ."}, {"review_id": "S1xCuTNYDr-1", "review_text": "This paper proposes an approach for local post-hoc explanation with introduction of a new regularization that helps regulate the \"fidelity\" and \"stability\" of the output explanation (in the style of LIME). The fidelity regularizer is essentially the squared error of the explainer as compared to the given model in the local neighborhood, whereas the stability regularizer measures the total squared differences between the explanation outcomes between the sample in question and other samples in the local neighborhood. In the experimental evaluation section, the authors evaluates the performance fo the proposed regularizers, used as part of both LIME and MAPLE, against three interpretability metrics: point fidelity, neighborhood fidelity and stability. The results verify that indeed the use of the regularizers improve the performance of both LIME and MAPLE over the unregularized versions, with respect to the corresponding metrics. This is in a way \"expected\", since the regularizer used in the method and that in the metric are closely related, and is an unsatisfying aspect of the work. Using image data, they also demonstrate that qualitatively the use of stability regularizer seems to significantly improve the saliency of the output explanation. The paper is well written, the proposal is reasonable, but the contribution is modest and experimental evaluation is not entirely convincing. ", "rating": "3: Weak Reject", "reply_text": "The fidelity and stability metrics have been used to measure interpretability , for local explanations , in previous work . They are proxies for the qualitative concept of \u201c interpretability \u201d in the general sense , and we would argue are quite useful ; because a local explanation answers the question \u201c What could I have done differently to get a different outcome ? \u201d , a higher fidelity explanation will yield more accurate advice and a more stable explanation will be more trustworthy . A major contribution of our work is showing that these metrics can in fact be improved using a simple and general method . Moreover , we disagree with the sentiment that research needs to have a \u201c surprise factor \u201d in order to be impactful ."}, {"review_id": "S1xCuTNYDr-2", "review_text": "The paper considers the problem of training black box models for improved interpretability, and proposes to penalize black-box models at training time using two regularizers that correspond to fidelity and stability explanation metrics. As computing the regularizers exactly is computationally intensive they propose two approximating algorithm. In addition, as the one for fidelity is still prohibitive, a randomized variant is proposed. Connections are established between the regularizers and the model's Lipschitz constant or total variation. A generalization bound is presented for local linear explanations. The proposed approach is evaluated on a variety of datasets. The paper deals with an important problem and the exposition is clear. While regularizing deep learning models is a pertinent direction, I feel the paper makes a couple of overstatements, and overall I am not fully convinced by the approach and empirical evaluation, as outlined below. - The paper states \"recent approaches that claim to overcome this apparent trade-off between prediction accuracy and explanation quality are in fact by-design proposals that impose certain constants on the underlying model families they consider\" and that they are addressing this shortcoming. But in fact, the proposed regularizations do exactly the same: they impose certain constraints. Indeed the regularizers encourages models with lower Lipschitz constant or with small total variation across neighborhoods. - I also find that it is unsurprising that regularizing via fidelity or stability will lead to models with better fidelity/stability so it's an artificial way to yield \"improved interpretability\" and this is more of an issue because fidelity and stability are kind of proxy metrics to evaluate interpretability. - It would be important to investigate further the difference between regularization and explanation neighborhoods. This might not be a bad thing which in fact help with generalization. - Proposition 1 supports algorithm 2, but it is not a given at all that Algorithm 1 will have smaller local variances across neightborhoods and hence might generalize well. It would be important to proceed with an empirical study of the local variance across neighborhoods for Algorithm 1. - Computational complexity remains an issue as ExpO-1D-fidelity is performing much worse. - Comparison against alternative approaches beyond SENN are lacking (e.g. Lee et al 2019, Wang and Rudin,2015 etc). Overall I feel that more work is needed to convincingly demonstrate the importante of the proposed approach. ", "rating": "1: Reject", "reply_text": "This is another by-design approach : `` - The paper states ... total variation across neighborhoods . '' The distinction being made here is that the other approaches enforce structural restrictions on the model architecture while our regularization is model agnostic . Further , we believe that you may have misunderstood the connection between our regularization and the Lipschitz constant or the total variation . The three are only equivalent after the part of the function that can be explained by a local linear explanation has been subtracted ; this is discussed when we discuss Figure 2 . As a result , our regularization does not necessarily encourage models with a lower Lipschitz constant or a smaller total variation . Results are Unsurprising : `` - I also find ... to evaluate interpretability . '' The fidelity and stability metrics have been used to measure interpretability , for local explanations , in previous work . Although they are proxies for the qualitative concept of \u201c interpretability \u201d in the general sense , they are useful ; because a local explanation answers the question \u201c What could I have done differently to get a different outcome ? \u201d , a higher fidelity explanation will yield more accurate advice and a more stable explanation will be more trustworthy . A major contribution of our work is showing that these metrics can in fact be improved using a simple and general method . Moreover , we disagree with the sentiment that research needs to have a \u201c surprise factor \u201d in order to be impactful . Need to explore different neighborhoods : `` - It would be important ... help with generalization. `` We agree that exploring this further is an interesting direction for future work and we said as much . However , we do not think it is necessary for the current results to be meaningful . What is the effect of ExpO on local variance ? : `` - Proposition 1 supports ... for Algorithm 1 . '' First , we would like to point out that if a regularized model and an ExpO regularized model have the same local variances across neighborhoods , then the fidelity-metric would generalize equally well . However , the results in the Github repo show that ExpO also reduces this variance . We excluded this result from the main paper because this variance is neither a measure of accuracy nor interpretability . Second , the purpose of the generalization bound is to demonstrate theoretically that regularizing for fidelity on the training points should improve fidelity on new points . The results we provide are on the testing data and consequently are empirical estimates of the true explanation fidelity . As a result , their importance does not depend on the fact that ExpO also reduced this variance . Computational Complexity is an issue : `` - Computational complexity ... performing much worse . '' We disagree with the characterization that \u201c ExpO-1D-fidelity is performing much worse \u201d . It still represents a significant improvement over the baseline models . Comparisons against baselines are missing : `` - Comparison against alternative ... Wang and Rudin,2015 etc ) . '' Lee et al 2019 was a concurrent work with ours and has no publicly available source code . As discussed in the related work , it differs significantly from our method . Wang and Rudin , 2015 is a global interpretability method which is fundamentally different from our approach which is related to local interpretability . These two areas use different metrics and making a direct comparison would be difficult. `` Overall I feel that more work is needed to convincingly demonstrate the importante of the proposed approach . '' The work demonstrates a simple approach for making post-hoc explanations systems work significantly better . To the best of our knowledge , there is no other model-agnostic regularization scheme that gives a practitioner the kind of control over local interpretability that we show in Figure 1 Left ."}], "0": {"review_id": "S1xCuTNYDr-0", "review_text": " The paper proposes a new type of regularizer to improve explainability in neural networks. The proposed regularizer is largely based on two metrics, namely fidelity and stability. It optimizes for fidelity and stability as a regularization objective in a differentiable manner. I would recommend for accept, as the paper shows in its experiments that the explainability of neural networks can be improved with the two proposed regularizer, which outperforms simple baselines of l1 and l2 regularizers. The paper's method is generic, and can be applied to almost all machine learning models with gradient-based optimization, making it helpful to building explainable machine learning systems. However, I would also like to note that the results in this paper are somewhat unsurprising. The ExpO-Fidelity and ExpO-Stability regularizers can be seen as (almost) directly optimizing the fidelity and stability metric for explainability, so one would naturally expect that models trained with these regularizers will do better on the two metrics above. In addition, I do not see much value in the derivations of Section 3.2. The conclusion that \"explainable models with smaller local variances ... are likely to have explanations of higher fidelity\" is unsurprising and almost a straightforward claim.", "rating": "6: Weak Accept", "reply_text": "The fidelity and stability metrics have been used to measure interpretability , for local explanations , in previous work . Although they are proxies for the qualitative concept of \u201c interpretability \u201d in the general sense , they are useful ; because a local explanation answers the question \u201c What could I have done differently to get a different outcome ? \u201d , a higher fidelity explanation will yield more accurate advice and a more stable explanation will be more trustworthy . A major contribution of our work is showing that these metrics can in fact be improved using a simple and general method . Moreover , we disagree with the sentiment that research needs to have a \u201c surprise factor \u201d in order to be impactful . The purpose of the generalization bound is to demonstrate theoretically that regularizing for fidelity on the training points should improve fidelity on new points . While not a main contribution of the work , and even though the result itself is not technically difficult to derive , we do think that it is a positive addition to the work ."}, "1": {"review_id": "S1xCuTNYDr-1", "review_text": "This paper proposes an approach for local post-hoc explanation with introduction of a new regularization that helps regulate the \"fidelity\" and \"stability\" of the output explanation (in the style of LIME). The fidelity regularizer is essentially the squared error of the explainer as compared to the given model in the local neighborhood, whereas the stability regularizer measures the total squared differences between the explanation outcomes between the sample in question and other samples in the local neighborhood. In the experimental evaluation section, the authors evaluates the performance fo the proposed regularizers, used as part of both LIME and MAPLE, against three interpretability metrics: point fidelity, neighborhood fidelity and stability. The results verify that indeed the use of the regularizers improve the performance of both LIME and MAPLE over the unregularized versions, with respect to the corresponding metrics. This is in a way \"expected\", since the regularizer used in the method and that in the metric are closely related, and is an unsatisfying aspect of the work. Using image data, they also demonstrate that qualitatively the use of stability regularizer seems to significantly improve the saliency of the output explanation. The paper is well written, the proposal is reasonable, but the contribution is modest and experimental evaluation is not entirely convincing. ", "rating": "3: Weak Reject", "reply_text": "The fidelity and stability metrics have been used to measure interpretability , for local explanations , in previous work . They are proxies for the qualitative concept of \u201c interpretability \u201d in the general sense , and we would argue are quite useful ; because a local explanation answers the question \u201c What could I have done differently to get a different outcome ? \u201d , a higher fidelity explanation will yield more accurate advice and a more stable explanation will be more trustworthy . A major contribution of our work is showing that these metrics can in fact be improved using a simple and general method . Moreover , we disagree with the sentiment that research needs to have a \u201c surprise factor \u201d in order to be impactful ."}, "2": {"review_id": "S1xCuTNYDr-2", "review_text": "The paper considers the problem of training black box models for improved interpretability, and proposes to penalize black-box models at training time using two regularizers that correspond to fidelity and stability explanation metrics. As computing the regularizers exactly is computationally intensive they propose two approximating algorithm. In addition, as the one for fidelity is still prohibitive, a randomized variant is proposed. Connections are established between the regularizers and the model's Lipschitz constant or total variation. A generalization bound is presented for local linear explanations. The proposed approach is evaluated on a variety of datasets. The paper deals with an important problem and the exposition is clear. While regularizing deep learning models is a pertinent direction, I feel the paper makes a couple of overstatements, and overall I am not fully convinced by the approach and empirical evaluation, as outlined below. - The paper states \"recent approaches that claim to overcome this apparent trade-off between prediction accuracy and explanation quality are in fact by-design proposals that impose certain constants on the underlying model families they consider\" and that they are addressing this shortcoming. But in fact, the proposed regularizations do exactly the same: they impose certain constraints. Indeed the regularizers encourages models with lower Lipschitz constant or with small total variation across neighborhoods. - I also find that it is unsurprising that regularizing via fidelity or stability will lead to models with better fidelity/stability so it's an artificial way to yield \"improved interpretability\" and this is more of an issue because fidelity and stability are kind of proxy metrics to evaluate interpretability. - It would be important to investigate further the difference between regularization and explanation neighborhoods. This might not be a bad thing which in fact help with generalization. - Proposition 1 supports algorithm 2, but it is not a given at all that Algorithm 1 will have smaller local variances across neightborhoods and hence might generalize well. It would be important to proceed with an empirical study of the local variance across neighborhoods for Algorithm 1. - Computational complexity remains an issue as ExpO-1D-fidelity is performing much worse. - Comparison against alternative approaches beyond SENN are lacking (e.g. Lee et al 2019, Wang and Rudin,2015 etc). Overall I feel that more work is needed to convincingly demonstrate the importante of the proposed approach. ", "rating": "1: Reject", "reply_text": "This is another by-design approach : `` - The paper states ... total variation across neighborhoods . '' The distinction being made here is that the other approaches enforce structural restrictions on the model architecture while our regularization is model agnostic . Further , we believe that you may have misunderstood the connection between our regularization and the Lipschitz constant or the total variation . The three are only equivalent after the part of the function that can be explained by a local linear explanation has been subtracted ; this is discussed when we discuss Figure 2 . As a result , our regularization does not necessarily encourage models with a lower Lipschitz constant or a smaller total variation . Results are Unsurprising : `` - I also find ... to evaluate interpretability . '' The fidelity and stability metrics have been used to measure interpretability , for local explanations , in previous work . Although they are proxies for the qualitative concept of \u201c interpretability \u201d in the general sense , they are useful ; because a local explanation answers the question \u201c What could I have done differently to get a different outcome ? \u201d , a higher fidelity explanation will yield more accurate advice and a more stable explanation will be more trustworthy . A major contribution of our work is showing that these metrics can in fact be improved using a simple and general method . Moreover , we disagree with the sentiment that research needs to have a \u201c surprise factor \u201d in order to be impactful . Need to explore different neighborhoods : `` - It would be important ... help with generalization. `` We agree that exploring this further is an interesting direction for future work and we said as much . However , we do not think it is necessary for the current results to be meaningful . What is the effect of ExpO on local variance ? : `` - Proposition 1 supports ... for Algorithm 1 . '' First , we would like to point out that if a regularized model and an ExpO regularized model have the same local variances across neighborhoods , then the fidelity-metric would generalize equally well . However , the results in the Github repo show that ExpO also reduces this variance . We excluded this result from the main paper because this variance is neither a measure of accuracy nor interpretability . Second , the purpose of the generalization bound is to demonstrate theoretically that regularizing for fidelity on the training points should improve fidelity on new points . The results we provide are on the testing data and consequently are empirical estimates of the true explanation fidelity . As a result , their importance does not depend on the fact that ExpO also reduced this variance . Computational Complexity is an issue : `` - Computational complexity ... performing much worse . '' We disagree with the characterization that \u201c ExpO-1D-fidelity is performing much worse \u201d . It still represents a significant improvement over the baseline models . Comparisons against baselines are missing : `` - Comparison against alternative ... Wang and Rudin,2015 etc ) . '' Lee et al 2019 was a concurrent work with ours and has no publicly available source code . As discussed in the related work , it differs significantly from our method . Wang and Rudin , 2015 is a global interpretability method which is fundamentally different from our approach which is related to local interpretability . These two areas use different metrics and making a direct comparison would be difficult. `` Overall I feel that more work is needed to convincingly demonstrate the importante of the proposed approach . '' The work demonstrates a simple approach for making post-hoc explanations systems work significantly better . To the best of our knowledge , there is no other model-agnostic regularization scheme that gives a practitioner the kind of control over local interpretability that we show in Figure 1 Left ."}}