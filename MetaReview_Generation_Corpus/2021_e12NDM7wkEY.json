{"year": "2021", "forum": "e12NDM7wkEY", "title": "Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation", "decision": "Accept (Poster)", "meta_review": "This paper received mostly positive reviews. The reviewers praised the strong performance when compared with previous work.\nAlso, the evaluation clearly shows the benefit of the proposed contributions in terms of performance.\nMost concerns raised by reviewers were properly addressed in the rebuttal.\n\nLack of comparison to several previous works has been noted in a comment, but the authors clarified this concern, stating that the current work is a \u201clarge deviation from prior works\u201d. The authors promised to include the missing references into the comparison.\n\nGiven the reviews, comments, and author's answers, I suggest acceptance.", "reviews": [{"review_id": "e12NDM7wkEY-0", "review_text": "One of the main contributions is this idea of feature decorrelation where they encourage the representation features to be independent / orthogonal . The other is instance discrimination . This aims to capture the similarity between individual data points . Both of these are interesting contributions to the field of 'deep clustering ' . Besides the stated contributions , I thought there were a number of other positive aspects of this . A ) I thought that the spectral clustering connection was nice and I am glad the authors included it . B ) The evaluation is fairly detailed . I particularly appreciate the fact that the authors used datasets that are somewhat larger than often used in the literature ( MNIST and CIFAR-10 vs CIFAR-100 and ImageNet-10 ) . The inclusion of the study of the temperature parameter also helped clarify a few questions I had when reading it . C ) Finally , the evaluation clearly shows the benefit of their contributions in terms of performance . There are a number of questions I have with the work as is . A ) Given the two methods proposed , IDFO , IDFD , neither of which outperforms the other on all tasks , and given this is unsupervised learning , how does one know which method to use ? B ) Why was the alpha parameter set to 1 for IDFD ? How does one know what to set this to for different datasets ? If it 's always 1 , why is it included at all ? This is particularly important to understand in unsupervised settings . C ) The impact of data augmentation is discussed in the supplementary but this is stated as being extremely important to the performance of the model . It is unclear to me whether the results in the main text include the augmentation process ? If so , then given this , I think it should be stated in the main text as it has an effect on both instance discrimination and feature decorrelation considering the addition of augmented images . The results in supplementary Table 4 include KNN and do n't match up with the main results in the main text which further confused me . D ) I was left wondering how well this method works on non-image data ? Other works in the literature have explored this . E ) For Fig.2 is this ACC calculated on the validation set or test set ? F ) What were the effects of resizing the ImageNet images ? Can this model handle larger images , and if so , how does this effect performance ? Minor A ) References are badly formatted in Table 3 . Overall , my questions above notwithstanding , I think this is an interesting contribution which shows the benefit of instance discrimination and feature decorrelation for deep clustering .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your comments . Please find below our responses to each of your comments . 1 . `` Given the two methods proposed , IDFO , IDFD , neither of which outperforms the other on all tasks , and given this is unsupervised learning , how does one know which method to use ? '' One of the advantages of IDFD compared with IDFO is performance stability . We plotted the accuracy values of IDFD and IDFO over the whole learning process in Figure 2 . The performance of IDFO is higher than the other methods in earlier epochs . However , the accuracy widely \ufb02uctuated over the learning process and dropped in later epochs . Totally , we recommend IDFD for its performance and stability . 2 . `` Why was the alpha parameter set to 1 for IDFD ? How does one know what to set this to for different datasets ? If it 's always 1 , why is it included at all ? This is particularly important to understand in unsupervised settings . '' According to the definition of ID and FD , two loss terms are resultantly of same order , and the $ \\alpha $ parameter was simply fixed to1 in our experiments . For the general formulation of loss function with two terms , we have prepared a weight term alpha . We think it is not easy to determine a certain value of alpha for different datasets , as for the weights of many other losses . 3 . `` It is unclear to me whether the results in the main text include the augmentation process ? If so , then given this , I think it should be stated in the main text\u2026 '' Yes , the augmentation process was included in the main results . We will state that in the main text of next revision . 4 . `` The results in supplementary Table 4 include KNN and do n't match up with the main results in the main text which further confused me . '' Original instance discriminative method ID ( original ) was only evaluated by KNN metric but clustering metrics ACC , NMI , and ARI . In order to illustrate the impacts of ID ( original ) from data augmentation , we reserved the KNN results in our paper . We can delete it to improve the readability and clarity . Difference between the results of Table 1 and 4 is due to different values for parameter $ \\tau_2 $ . We will execute these experiments with the same parameters in the revision . 5 . `` I was left wondering how well this method works on non-image data ? Other works in the literature have explored this . '' Up to now , we mainly attempted our method on image data including both open and our private data . It yielded good performance as expected . Further experiments on other data such as time-series data are undergoing . 6 . `` For Fig.2 is this ACC calculated on the validation set or test set ? '' ACC in Fig.2 is calculated on the train set . Because our method does not use the label information in training , we do not prepare train/valid/test sets and simply learn the representation from a set of data and evaluate our method with the set . We used the train set for training and evaluation rather than the whole dataset for the consistency with previous works . 7 . `` What were the effects of resizing the ImageNet images ? Can this model handle larger images , and if so , how does this effect performance ? '' One effect of resizing the ImageNet images is reducing the computation cost and memory consumption . Compared with CIFR-10 , ImageNet consists of bigger images . We referred to the previous work DCCM to resize images . We also executed the experiment on a smaller size 32x32 and got 87.6 % ACC values on ImageNet-10 , which is reduced by 7 % on 96x96 . From this result , we expect that the performance on larger images will be improved . 8. \u201c References are badly formatted in Table 3. \u201d We will fix it in the revision . Thank you very much ."}, {"review_id": "e12NDM7wkEY-1", "review_text": "This paper proposes a clustering-friendly representation learning method using instance discrimination and feature decorrelation . Instance discrimination loss and feature decorrelation loss are combined to optimize the network . The paper is well qritten and experimental results are good . I have some questions about this paper : 1 . There is no ablation analysis about the two loss terms in Eq . ( 6 ) . What about the contributions of the two loss terms ? 2.What is the motivation of Eq . ( 3 ) ? I.e. , why the `` = '' holds between the second and third expressions ?", "rating": "7: Good paper, accept", "reply_text": "Thank you very much . Please find below our responses to each of your questions . 1 . `` There is no ablation analysis about the two loss terms in Eq . ( 6 ) . What about the contributions of the two loss terms ? '' The results of ID ( tuned ) in Table 1 show the clustering performance only with the first term ID . The gaps between results of ID ( tuned ) and IDFD indicate the contribution of the second term FD . 2 . `` What is the motivation of Eq . ( 3 ) ? I.e. , why the `` = '' holds between the second and third expressions ? '' In our definition and notation , $ v $ and $ f $ have a relationship of $ v^T=f $ , which derives the equation of the second and third expressions in Eq . ( 3 ) ."}, {"review_id": "e12NDM7wkEY-2", "review_text": "The authors proposed an improved deep-learning-based representation learning method that provides more efficient features for clustering analysis . ( 1 ) According to the comparison experiments on several widely used datasets , the integration of a softmax-formulated orthogonal constraint is able to provide more stable latent feature representation . ( 2 ) As far as know , the widely-used deep clustering methods used to alternatively optimize the feature representation model parameters and update the anchors that provided by clustering method such as k-means , I am wondering if the proposed method in this study could integrate the two steps in a real end-to-end fashion . ( 3 ) I was deeply impressed by the far above state-of-the-art values of evaluation metric of this proposed representation learning method . Although the authors provide some distribution illustrations of latent features on CIFAR-10 dataset , what about the visualization on the ImageNet-10 ? Besides , adding some 'real ' visualization results existing in the original image space rather than the latent space could help to illustrate if the proposed method could mine visually meaningful concepts from the view of visual contents .", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your comments . Please find below our responses to each of your comments . 1. \u201c I am wondering if the proposed method in this study could integrate the two steps in a real end-to-end fashion. \u201d In this work , we focus on the task of representation learning which is essential for the performance of clustering . The insight on IDFD \u2019 s relation with spectral clustering , which performs construction of graph and its projection to lower dimension before clustering , is another motivation to focus on the representation learning . Our method can be integrated with other clustering methods ( including deep clustering methods ) . 2. \u201c what about the visualization on the ImageNet-10 ? \u201d We only gave the visualization on CIFAR-10 due to the limitation of space . The visualization on the ImageNet-10 can be shown in the next revision . \u201c adding some 'real ' visualization results existing in the original image space rather than the latent space \u201d It is not easy to visualize the high dimensional data in the original space . We can show several samples of original images corresponding to the points on the latent space in the next revision ."}], "0": {"review_id": "e12NDM7wkEY-0", "review_text": "One of the main contributions is this idea of feature decorrelation where they encourage the representation features to be independent / orthogonal . The other is instance discrimination . This aims to capture the similarity between individual data points . Both of these are interesting contributions to the field of 'deep clustering ' . Besides the stated contributions , I thought there were a number of other positive aspects of this . A ) I thought that the spectral clustering connection was nice and I am glad the authors included it . B ) The evaluation is fairly detailed . I particularly appreciate the fact that the authors used datasets that are somewhat larger than often used in the literature ( MNIST and CIFAR-10 vs CIFAR-100 and ImageNet-10 ) . The inclusion of the study of the temperature parameter also helped clarify a few questions I had when reading it . C ) Finally , the evaluation clearly shows the benefit of their contributions in terms of performance . There are a number of questions I have with the work as is . A ) Given the two methods proposed , IDFO , IDFD , neither of which outperforms the other on all tasks , and given this is unsupervised learning , how does one know which method to use ? B ) Why was the alpha parameter set to 1 for IDFD ? How does one know what to set this to for different datasets ? If it 's always 1 , why is it included at all ? This is particularly important to understand in unsupervised settings . C ) The impact of data augmentation is discussed in the supplementary but this is stated as being extremely important to the performance of the model . It is unclear to me whether the results in the main text include the augmentation process ? If so , then given this , I think it should be stated in the main text as it has an effect on both instance discrimination and feature decorrelation considering the addition of augmented images . The results in supplementary Table 4 include KNN and do n't match up with the main results in the main text which further confused me . D ) I was left wondering how well this method works on non-image data ? Other works in the literature have explored this . E ) For Fig.2 is this ACC calculated on the validation set or test set ? F ) What were the effects of resizing the ImageNet images ? Can this model handle larger images , and if so , how does this effect performance ? Minor A ) References are badly formatted in Table 3 . Overall , my questions above notwithstanding , I think this is an interesting contribution which shows the benefit of instance discrimination and feature decorrelation for deep clustering .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your comments . Please find below our responses to each of your comments . 1 . `` Given the two methods proposed , IDFO , IDFD , neither of which outperforms the other on all tasks , and given this is unsupervised learning , how does one know which method to use ? '' One of the advantages of IDFD compared with IDFO is performance stability . We plotted the accuracy values of IDFD and IDFO over the whole learning process in Figure 2 . The performance of IDFO is higher than the other methods in earlier epochs . However , the accuracy widely \ufb02uctuated over the learning process and dropped in later epochs . Totally , we recommend IDFD for its performance and stability . 2 . `` Why was the alpha parameter set to 1 for IDFD ? How does one know what to set this to for different datasets ? If it 's always 1 , why is it included at all ? This is particularly important to understand in unsupervised settings . '' According to the definition of ID and FD , two loss terms are resultantly of same order , and the $ \\alpha $ parameter was simply fixed to1 in our experiments . For the general formulation of loss function with two terms , we have prepared a weight term alpha . We think it is not easy to determine a certain value of alpha for different datasets , as for the weights of many other losses . 3 . `` It is unclear to me whether the results in the main text include the augmentation process ? If so , then given this , I think it should be stated in the main text\u2026 '' Yes , the augmentation process was included in the main results . We will state that in the main text of next revision . 4 . `` The results in supplementary Table 4 include KNN and do n't match up with the main results in the main text which further confused me . '' Original instance discriminative method ID ( original ) was only evaluated by KNN metric but clustering metrics ACC , NMI , and ARI . In order to illustrate the impacts of ID ( original ) from data augmentation , we reserved the KNN results in our paper . We can delete it to improve the readability and clarity . Difference between the results of Table 1 and 4 is due to different values for parameter $ \\tau_2 $ . We will execute these experiments with the same parameters in the revision . 5 . `` I was left wondering how well this method works on non-image data ? Other works in the literature have explored this . '' Up to now , we mainly attempted our method on image data including both open and our private data . It yielded good performance as expected . Further experiments on other data such as time-series data are undergoing . 6 . `` For Fig.2 is this ACC calculated on the validation set or test set ? '' ACC in Fig.2 is calculated on the train set . Because our method does not use the label information in training , we do not prepare train/valid/test sets and simply learn the representation from a set of data and evaluate our method with the set . We used the train set for training and evaluation rather than the whole dataset for the consistency with previous works . 7 . `` What were the effects of resizing the ImageNet images ? Can this model handle larger images , and if so , how does this effect performance ? '' One effect of resizing the ImageNet images is reducing the computation cost and memory consumption . Compared with CIFR-10 , ImageNet consists of bigger images . We referred to the previous work DCCM to resize images . We also executed the experiment on a smaller size 32x32 and got 87.6 % ACC values on ImageNet-10 , which is reduced by 7 % on 96x96 . From this result , we expect that the performance on larger images will be improved . 8. \u201c References are badly formatted in Table 3. \u201d We will fix it in the revision . Thank you very much ."}, "1": {"review_id": "e12NDM7wkEY-1", "review_text": "This paper proposes a clustering-friendly representation learning method using instance discrimination and feature decorrelation . Instance discrimination loss and feature decorrelation loss are combined to optimize the network . The paper is well qritten and experimental results are good . I have some questions about this paper : 1 . There is no ablation analysis about the two loss terms in Eq . ( 6 ) . What about the contributions of the two loss terms ? 2.What is the motivation of Eq . ( 3 ) ? I.e. , why the `` = '' holds between the second and third expressions ?", "rating": "7: Good paper, accept", "reply_text": "Thank you very much . Please find below our responses to each of your questions . 1 . `` There is no ablation analysis about the two loss terms in Eq . ( 6 ) . What about the contributions of the two loss terms ? '' The results of ID ( tuned ) in Table 1 show the clustering performance only with the first term ID . The gaps between results of ID ( tuned ) and IDFD indicate the contribution of the second term FD . 2 . `` What is the motivation of Eq . ( 3 ) ? I.e. , why the `` = '' holds between the second and third expressions ? '' In our definition and notation , $ v $ and $ f $ have a relationship of $ v^T=f $ , which derives the equation of the second and third expressions in Eq . ( 3 ) ."}, "2": {"review_id": "e12NDM7wkEY-2", "review_text": "The authors proposed an improved deep-learning-based representation learning method that provides more efficient features for clustering analysis . ( 1 ) According to the comparison experiments on several widely used datasets , the integration of a softmax-formulated orthogonal constraint is able to provide more stable latent feature representation . ( 2 ) As far as know , the widely-used deep clustering methods used to alternatively optimize the feature representation model parameters and update the anchors that provided by clustering method such as k-means , I am wondering if the proposed method in this study could integrate the two steps in a real end-to-end fashion . ( 3 ) I was deeply impressed by the far above state-of-the-art values of evaluation metric of this proposed representation learning method . Although the authors provide some distribution illustrations of latent features on CIFAR-10 dataset , what about the visualization on the ImageNet-10 ? Besides , adding some 'real ' visualization results existing in the original image space rather than the latent space could help to illustrate if the proposed method could mine visually meaningful concepts from the view of visual contents .", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your comments . Please find below our responses to each of your comments . 1. \u201c I am wondering if the proposed method in this study could integrate the two steps in a real end-to-end fashion. \u201d In this work , we focus on the task of representation learning which is essential for the performance of clustering . The insight on IDFD \u2019 s relation with spectral clustering , which performs construction of graph and its projection to lower dimension before clustering , is another motivation to focus on the representation learning . Our method can be integrated with other clustering methods ( including deep clustering methods ) . 2. \u201c what about the visualization on the ImageNet-10 ? \u201d We only gave the visualization on CIFAR-10 due to the limitation of space . The visualization on the ImageNet-10 can be shown in the next revision . \u201c adding some 'real ' visualization results existing in the original image space rather than the latent space \u201d It is not easy to visualize the high dimensional data in the original space . We can show several samples of original images corresponding to the points on the latent space in the next revision ."}}