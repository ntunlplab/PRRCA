{"year": "2019", "forum": "HygUOoC5KX", "title": "Are Generative Classifiers More Robust to Adversarial Attacks?", "decision": "Reject", "meta_review": "Adversarial defense is a tricky subject, and the authors are to be commended for their novel approach to this problem. The reviewers all agree that there is promise in this approach. However, after reviewing the discussion, they have all come to the conclusion that the robustness of your generative model needs to be more thoroughly explored. Regarding gradient masking, there are other attacks like a manifold attack that use gradients that can be explored as well. Regarding SPSA, it would be helpful perhaps to also include other numerical gradient attacks to ensure that SPSA is stronger and working as intended.\n\nEssentially, the reviewers would all like to see a more streamlined version of this paper that removes any doubt about the efficacy of the generative approach. Once that is established, additional properties and features can be explored.\n\nAlso note that for the purposes of these reviews and discussion, Schott et al. was considered as concurrent work and not prior work.\n", "reviews": [{"review_id": "HygUOoC5KX-0", "review_text": "This paper explores the potential of generative models for adversarial robustness. It presents some interesting and well formulated findings but is lacking in some meaningful ways. -It does a good job of summarizing the literature though it misses out on some very recent but relevant work -It introduces three detection methods and extensively evaluates them against several zero-knowledge attack types. Notably all the attack types are gradient based methods. -They present detailed performance metrics on the zero-knowledge attack, and introduce the perfect knowledge attack but do not present equivalently detailed results. -The results are presented in a confusing manor, and the paper is perhaps done a disservice by the inclusion of a very large number of tables both in the main text and the appendix without the necessary writing required to situate the reader. -The paper also briefly mentions the ongoing debate around \"off-manifold\" conjecture, only to them assume its correctness and based the entire work on the premise. ###Highlights *well written introduction and a good overview of generative modeling *very good visualizations and explanation of the detection methods *thorough results on zero-knowledge gradient based attack detection rates *interesting direction on adversarial examples ### Areas for improvement *more time should be spent on discussing the off manifold conjecture; the paper is based on the correctness of this conjecture though recent work has called its validity into question *only gradient based attacks are discussed though the paper title suggests robustness to all attacks *perfect-knowledge attacks should be explored in much more detail * the paper should streamline the results and findings, the large number of tables and results will confuse the reader and do not add to the argument *recent work on generative models for adversarial robustness should be discussed (https://arxiv.org/abs/1805.09190, https://arxiv.org/abs/1811.06969) *the two class cifar10 results are not particularly convincing as this is a substantial simplification of the cifar10 problem * the results on the full cifar10 data set make use a the extracted features of a pre-trained discriminative cifar10 network, this introduces the problem at layer activation adversarial attacks, but this is not mentioned in the paper (https://arxiv.org/pdf/1511.05122.pdf) Overall i think the work shows promise but is not yet ready for acceptance ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments . Short answers to your the major concerns : 1 . Our work is highly novel , the two `` prior work '' you mentioned are after or concurrent to our work . We have discussed this in section 5 . 2.We did tested score-based attack ( SPSA ) which is * * gradient-free * * . Results show that the robustness of generative classifiers is NOT due to gradient masking . 3.We did not pre-assume the correctness of the `` off-manifold '' conjecture , in fact our experiments serve as empirical verification of this conjecture . 4.The CIFAR-10 binary experiments are the best we can do to evaluate * * fully generative classifiers * * on * * natural images * * . 5.Observations on layer activation attacks against the * * fusion model * * can not directly transfer to robustness results of * * fully generative classifiers * * . We have discussed similar issues in section 5 . Detailed reply on your major questions : 1 . `` two previous work not discussed '' 1a ) Our work is at least concurrent to Schott et al . ( 2018 ) ( in fact , earlier , we have submitted evidence to the AC ) . 1b ) Schott et al . ( 2018 ) is also under submission to ICLR 2019 . 1c ) Frosst et al . ( 2018 ) was online on * * Nov 16 2018 * * , which is 1.5 months after ICLR submission deadline , and we were not aware of it . 1d ) We have discussed Schott et al . ( 2018 ) in revision ( see the second paragraph in section 5 ) . 1e ) In the below `` summary of the Q & A '' comment we extensively discussed the main difference of our work to Schott et al . ( 2018 ) , and we consider the difference to be very significant . 2 . `` Notably all the attack types are gradient based methods '' We did tested the SPSA attack ( Uesato et al.2018 ) which is based on evolutionary strategies and thus * * gradient-free * * . 2a ) This attack has been shown to be very successful against * * gradient masking based defences * * . 2b ) Our results show that SPSA worked * * worse * * than white-box attacks on generative classifiers . 2c ) Therefore , we conclude that the robustness of generative classifiers is NOT due to gradient masking . 3 . `` off-manifold conjecture pre-assumed '' We did not pre-assume the correctness of the `` off-manifold '' conjecture : 3a ) We were interested in verifying the `` off-manifold '' conjecture for both generative and discriminative classifiers . If for small epsilon the classifiers have high error , then this conjecture might not be true in practice . 3b ) The detection methods depend on the correctness of the `` off-manifold '' conjecture . Therefore , if empirically the detection methods do n't work even when we have a very good generative model ( which is the case for MNIST ) , then , this conjecture might not be true in practice . 3c ) Our empirical results clearly show that ( i ) generative classifiers are more robust than discriminative ones ( especially when epsilon is small ) ; ( ii ) the marginal/logit detection methods did work . 3d ) Observing the above , we are confident to say we have empirically verified the `` off-manifold '' conjecture on generative classifiers . 4 . `` two class cifar10 results are not particularly convincing '' We presented this example because : 4a ) Carlini & Wagner ( 2017b ) stated that MNIST results might not transfer to natural images . Therefore we wanted to see if our findings on MNIST is still true on natural images . By contrast Schott et al . ( 2018 ) only studied MNIST . 4b ) We did try full CIFAR-10 , but none of the existing generative models ( VAE/GAN/flow-based/autoregressive models ) can achieve satisfactory * * clean accuracy * * on it . 4c ) Therefore we instead studied this two-class CIFAR-10 problem in order to evaluate * * fully generative classifiers * * . We then presented the fusion model for full CIFAR-10 to address the scalability issue . 5 . `` the problem at layer activation adversarial attacks '' Thanks for pointing the relevant work which we shall cite . However : 5a ) The fusion model is not * * fully generative * * , and it contains * * discriminative features * * which might be vulnerable to attacks . 5b ) Therefore failures of the fusion model against layer activation attacks do not imply the failure of * * fully generative classifiers * * . 5c ) To be clear , we have no intension to oversell our results and claim generative classifiers are robust to * * all attacks * * . In fact we spent a full paragraph to discuss this issue , please see the third paragraph in section 5 ."}, {"review_id": "HygUOoC5KX-1", "review_text": "This work investigates an interesting direction of improving robustness of classifiers against adversarial attacks by using generative models. The authors propose the *deep Bayes classifier*, which is a deep LVM based extension of naive Bayes. Furthermore, the authors extensively explore 7 possible factorisations of the classifier. Thorough experiments are conducted to assess the capability of defending or detecting adversarial examples. Besides, the authors incorporate discriminative features to generative classifiers and demonstrate clear robustness gain. ### Highlights * This work proposes an attractive direction -- the use of generative model in defending or detecting adversarial attacks. I suggest this idea should be follower by more further studies. * The presented models are quite straight-forward but exhibit good robustness against attacks listed in the experiments. * Various structural possibilities of the graphical model are examined which is preferable and helps assess the effectiveness of generative classifiers. ### Minors * Although the major point here is robustness against adversarial attacks, as mentioned by the authors, the performance on clear cases (i.e. no attacks) is unsatisfactory. Also, experiments on CIFAR are too much simplified (only 2 very unlike classes) and therefore not very convincing. * For the combination of generative classifier and discriminative features, I\u2019m curious about the results on the clear CIFAR-10 multi-class problem. It should be a very positive plus if results are satisfactory. * The writing is sometimes hard to follow. For examples, many ad-hoc abbreviations are used across the paper causing difficulties of understanding the core idea and results. ### Conclusion In general, this paper brings our attention to a previously less investigated but seemingly promising research direction, i.e. robustness of generative model against adversarial attacks. The idea is insightful and proposed models are straight-forward. While only on small-scale problems (with the presence of attacks), extensive experimental results in this paper can assist further study on this field. Thus, I recommend this paper to be accepted. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review , we are glad that you liked our paper in general . We thank for your suggestions on better writing and will include them in revision . We clarify on our CIFAR experiments in below . 1.The test on CIFAR-binary is an important contribution : 1a ) Carlini & Wagner ( 2017b ) have shown that many defense techniques that works on MNIST didn \u2019 t work very well on natural images . Therefore it is important to have a natural image classification task , to see whether the robustness results on MNIST also extends to natural images . 1b ) We did try full generative classifiers on CIFAR-10 multi-class classification . Unfortunately it is still a research challenge to make full generative classifiers work beyond MNIST . In fact we have tried even more powerful architectures like PixelCNN++ to parameterise each of p ( x|y ) , and the clean accuracy in this case is 72.4 % . Therefore we don \u2019 t think it \u2019 s fair to compare this PixelCNN++ based classifier against e.g.VGG-16 ( clean accuracy > 93 % ) , since the gap on clean accuracy is huge . 1c ) Still due to the importance of testing natural image classification tasks , we derived from CIFAR-10 a binary classification task that the deep Bayes classifiers work reasonably well ( > 90 % accuracy ) , and tested the robustness of them on this task . The general conclusions here are consistent with MNIST experiments , providing evidences that the robustness results of generative classifiers do extend to natural images . 2.The results of the fusion model are indeed on * * full CIFAR-10 multi-class classification * * . 2a ) Since full generative classifiers don \u2019 t work very well on full CIFAR-10 , we decided to take discriminative features from VGG-16 and train generative classifiers on the features . We have the clean accuracy results reported in table D.2 for multi-class classification . In this case all classifiers have > 88 % accuracy on clean data , so we can have a reasonably fair comparison here . 2b ) The robustness tests ( Figure 10 ) still favours the fusion model , and the bottleneck discriminative classifiers ( DBX- ) didn \u2019 t improve robustness against PGD & MIM . To the best of our knowledge this is the first robustness test on this fusion model . Our results show that combining generative modelling and discriminative features is an exciting future research direction . 2c ) Importantly , we show that using lower-layer features for the fusion model ( i.e.~the model is less \u201c discriminative \u201d ) returns even better results . This is a clear evidence favouring generative classifiers , and we expect in future developments , a full generative classifier that can achieve > 90 % clean accuracy on CIFAR-10 will be even more robust . Thank you for your review again , let us know if you have more questions ."}, {"review_id": "HygUOoC5KX-2", "review_text": "In this work the authors propose and analyse generative models as defences against adversarial examples. In addition, three detection methods are introduced and an extension to deep features is suggested. My main concerns are as follows (details below): * Important prior work is not mentioned. * Evaluation with direct attacks is only based on (very few) gradient-based techniques, many results are not reliable. * There are signs of gradient masking (the common problem of robustness evaluation, in particular of only gradient-based techniques are used). * The way detection rates are taken into account in the perfect knowledge scenario is confusing. ### Style I like the idea of testing many different factorisation structures. However, that comes with the drawback that one needs to constantly check back what the abbreviations mean. Together with the three detection methods, the manuscript is quite confusing at times and should definitely be streamlined. One suggestion: remove the detection methods: I did not find any real conclusion about them but they are definitely side-tracking users away from the main results. ### Prior work There is at least one closely related prior work not mentioned here: the analysis by synthesis model [1]. This model uses a variational auto encoder to learn class conditional distributions and shows high robustness on MNIST. Please make clear what your contribution is over this paper (other than testing several other factorisations). ### Evaluation problems The robustness of models should be evaluated on different direct attacks ranging from gradient-based to score-based (e.g. NES [2]) to decision-based attacks [3]. Please take a look at [1] to see how a very extensive evaluation might look like. The results can be astonishingly different for different attacks, and so basing conclusion on only one or two attacks is dangerous (in particular if you only use gradient-based ones). One can also see that in your results, just check the variations you get between MIM and PGD. Also, rather then discussing (and showing in detail) results for individual attacks, the minimum adversarial distance for a given sample that can be found by any attack is much more comparable between models (which can also streamline the manuscript). One can see signs of gradient masking in your results. For example, in Figure 3 the MIM attacks levels out at 20% for the DBX model. That can happen for iterative attacks if the gradient is masked. Similarly, in Figure 5 DBX-ZK (zero knowledge) is better in both accuracy and detection rate than DBX-PKK (which takes the KL-detection method into account and should thus either be better in accuracy or detection rate). More generally, the perfect knowledge case, in which the attacker knows about the detector, should only count samples as adversarials which evade the detector and change the model decision. Thus, the detection rate should be zero. Otherwise I have no idea what trade-off between accuracy and detection rate you are actually targeting and how to compare the results. Also, some intermediate results are conflicting with each other. E.g. in 4.1 you state \u201cthe usage of bottleneck is beneficial for better robustness\u201d, but for L2 this is not true. Also, I am not sure how conclusive the grey-box and black-box scenarios really are: since the substitute is basically a DFX or DFZ, it\u2019s unsurprising that adversarials transfer best to those two models. ### Minor * In 4.1 you say \u201cas they fail to find near manifold adversarials\u201d, but I don\u2019t see how there can be L-infty adversarials on MNIST that are on-manifold (remember, MNIST pixel values are basically binary). Plus, in the zero-knowledge scenario there is nothing that enforces staying on this manifold. * Result presentation (Figure 3/5 & Table 1) is very different for different attack scenarios, which makes them hard to compare. Please unify. * Is the L2 distance you report in Table 1 the mean (or median) distance to adversarial examples. If so, GBZ (for which you state that C&W \u201cfailed on attacking\u201d has actually a smaller mean adversarial distance than some other models (for which C&W is actually quite successful). * Grey-box scenario doesn\u2019t make a lot of sense: since the substitute is basically a DFX or DFZ, it\u2019s unsurprising that adversarials transfer best to those two models. A similar confounder makes the black-box results difficult to interpret. * Also, taking into account that the paper is two pages longer and thus calls for higher standards Taken together, I find the general direction of the paper very interesting and I\u2019d definitely encourage the authors to go further. At the current stage, however, I feel that (1) contributions are not sufficiently delineated to prior work, (2) the evaluation is not convincingly supporting the claims and that (3) the manuscript needs to be streamlined (both in terms of text and figures). [1] Schott et al. (2018) \u201cTowards the first adversarially robust neural network model on MNIST\u201d (https://arxiv.org/abs/1805.09190) [2] Ilyas et al. (2018) \u201cBlack-box Adversarial Attacks with Limited Queries and Information\u201d ( [https://arxiv.org/abs/1804.08598)](https://arxiv.org/abs/1804.08598)) [3] Brendel et al. (2018) \u201cDecision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models\u201d (https://arxiv.org/abs/1712.04248)", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for pointing out the work in [ 1 ] to us . Our work is independent to [ 1 ] , and the majority of the experiments ( MNIST & CIFAR binary ) was done at the same time when v1 of [ 1 ] was released on ArXiv ( May 2018 ) . We have submitted evidences to the area chair . We were not aware of [ 1 ] until you kindly pointed it to us . We also found that [ 1 ] is currently under review at ICLR 2019 as well , so we feel that this somewhat excuses our lack of knowledge of a paper that can be at best described as concurrent work ! Since our work is concurrent to [ 1 ] , we believe our evaluation of generative classifiers on MNIST is a novel and significant contribution . * * In the revised version we have a paragraph discussing the difference between our work and the concurrent work [ 1 ] . * * We would like to explain the main differences in the following : 1 . We consider a greater number of discriminative and generative classifiers , showing how the factorization of the generative model is important . 2.We do not only evaluate on MNIST . We look at a CIFAR binary task . This is very important as Carlini and Wagner ( 2017b ) showed that robustness properties shown on MNIST often do not hold on more complicated dataset , e.g.natural images . Our results on CIFAR-binary are consistent with MNIST results , showing that generative classifiers indeed have robustness properties distinct from discriminative ones . 3.We provide ideas and experiments on how to scale robust generative classifiers to more representative color datasets such as CIFAR-10 , by building generative classifiers on pre-trained discriminative features . To the best of our knowledge , this is the first evaluation of the robustness on this fusion model , and we believe our results show that the combined approach offers an exciting research direction . 4.Our approach gives data log-likelihood meaning to the logits for free , and these logit values can be used for detecting adversarial examples . This is different from previous approaches that require training an extra detector or generative model . Also since the classifier and the detector share the same generative model , detected adversarial inputs are indeed far away from the classifier \u2019 s manifold , thus we can use both accuracies and detection rates to verify the \u201c off-manifold \u201d conjecture . 5.Throughout training and adversarial evaluation we amortize the cost of inference and use K=10 Monte Carlo samples for the latent variable z . By contrast [ 1 ] \u2019 s method is much much slower : to classify a single image , their method requires K=8000 ( ! ) initial z samples and then 50 gradient steps for z refinement . As these generative classifiers already require more computation over regular CNNs , this computation can often ill be afforded . Also the logit values in [ 1 ] \u2019 s classifier are ELBOs of log p ( x|y ) with a very simple Gaussian q distribution , which can be very different from the actual log p ( x|y ) . Instead the logit values of our generative classifier are importance sampling estimates of log p ( x|y ) which are more accurate . We thank you for your suggestion on the extra attacks to run . We agree that covering more attacks is always nice ! However , we argue that we have already covered a very strong arsenal of attacks . This is evidenced when comparing to [ 1 ] , as you can see that we evaluate the strongest attacks they find . Furthermore , we have also evaluated the C & W attack , which they have not . We believe this is important as it has been demonstrated to be a very powerful attack ( Carlini & Wagner , 2017b ) , which is also corroborated by our work . * * In the revised version we also show that SPSA as a score based attack fails to fool generative classifiers , which also indicates that the robustness results are unlikely to be caused by gradient masking . * *"}, {"review_id": "HygUOoC5KX-3", "review_text": "This paper aims to test the robustness of generative classifiers [1] w.r.t. adversarial examples, considering their use as a potentially more robust alternative to adversarial training of discriminative classifiers. To achieve this, *Deep Bayes*, a generalization of the Naive Bayes classifier using a latent variable model and trained in a fashion similar to variational autoencoders [2] is introduced, and 7 different latent variable models are compared, covering a spectrum of generative or discriminative classification models, with or without bottlenecks. Their DFX and DBX architectures in particular closely match traditional discriminative classifiers, without and with a latent bottleneck. These 7 models are compared against a large range of adversarial attacks, depending on the kind of noise added (l_2 or l_inf) and how much the adversary can access (the full gradients of the model, its output on training data, or only the model as a black-box). The performance of the models is assessed depending on two criteria: how the performance of the classifier resists to adversarial noise, and how quickly the model can detect adversarial samples. Three methods for detecting adversarial samples are compared: the first (only applicable to generative classifiers) discards samples with a low likelihood, according to the off-manifold assumption [3], the second discards samples for which the classifier has low confidence in its classification (p(y|x) is under some threshold), and the third compares the output probability vector of the classifier on a sample to the mean classification vector of this class over the train data, and discards the sample if the two vectors are too dissimilar (meaning the classifier is over-confident or under-confident). The main contribution of this paper is the extensive experiments that have been done to compare the models against the various adversarial attacks. While experiments were only done on small datasets like MNIST and CIFAR (generative classifiers don't scale as easily on large image datasets), they nonetheless give very interesting insights and the authors provided encouraging results on applying generative classifiers on features learned by discriminative classifiers. Theirs result shows that generative architecture are in general more robust to the current state-of-the-art adversarial attacks, and detect adversarial examples more easily. The authors also recognize that these results may be biased by the fact that current adversarial attacks have been specifically optimized towards discriminative classifier. This is a solid paper in my opinion. The experimental setup and motivations are clearly detailed, and the paper was easy to follow. Extensive results and description of the experimental protocol are provided in the appendices, giving me confidence that the results should be reproducible. The results of this paper give interesting insights regarding how to approach robustness to adversarial examples in classification tasks, and provide realistic ways to try and apply generative classifiers in real-worlds tasks, using pre-learned features from discriminative networks. [1] http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf [2] https://arxiv.org/abs/1312.6114 [3] https://arxiv.org/abs/1801.02774", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Glad to hear that you like our manuscript ! The motivation of this work is to test whether recently developed attacks can break generative classifiers , and our results show that generative classifiers are more robust than discriminative classifiers against recent attacks . Sure research in adversarial attacks and defenses is similar to a `` cat and mouse game '' , and we anticipate in the future an attack tailored to generative classifiers can break our models . But we have done the best to test mainstream attacks available now , and indeed we showed that generative classifiers have properties different from discriminative ones . We expect future work on developing attacks & defenses on generative models can make generative models more powerful and robust ! An important insight of our approach is that the generative classifiers and the proposed detection methods are based on the same generative model . This means detected adversarial inputs are indeed far away from the classifier 's manifold , and the classifier 's manifold is also an approximation to the data manifold . By contrast , previous approaches require training an extra copy of generative model/auto-encoder . Thus it 's very likely that the detector and the classifier do not have aligned manifold representations , thus the classifier can not enjoy many benefits from the generative model . We also wanted to encourage future research on combining generative and discriminative approaches . At least from our robustness test on CIFAR-10 , this fusion approach indeed is worth further investigations , and it offers an exciting venue for future research ."}], "0": {"review_id": "HygUOoC5KX-0", "review_text": "This paper explores the potential of generative models for adversarial robustness. It presents some interesting and well formulated findings but is lacking in some meaningful ways. -It does a good job of summarizing the literature though it misses out on some very recent but relevant work -It introduces three detection methods and extensively evaluates them against several zero-knowledge attack types. Notably all the attack types are gradient based methods. -They present detailed performance metrics on the zero-knowledge attack, and introduce the perfect knowledge attack but do not present equivalently detailed results. -The results are presented in a confusing manor, and the paper is perhaps done a disservice by the inclusion of a very large number of tables both in the main text and the appendix without the necessary writing required to situate the reader. -The paper also briefly mentions the ongoing debate around \"off-manifold\" conjecture, only to them assume its correctness and based the entire work on the premise. ###Highlights *well written introduction and a good overview of generative modeling *very good visualizations and explanation of the detection methods *thorough results on zero-knowledge gradient based attack detection rates *interesting direction on adversarial examples ### Areas for improvement *more time should be spent on discussing the off manifold conjecture; the paper is based on the correctness of this conjecture though recent work has called its validity into question *only gradient based attacks are discussed though the paper title suggests robustness to all attacks *perfect-knowledge attacks should be explored in much more detail * the paper should streamline the results and findings, the large number of tables and results will confuse the reader and do not add to the argument *recent work on generative models for adversarial robustness should be discussed (https://arxiv.org/abs/1805.09190, https://arxiv.org/abs/1811.06969) *the two class cifar10 results are not particularly convincing as this is a substantial simplification of the cifar10 problem * the results on the full cifar10 data set make use a the extracted features of a pre-trained discriminative cifar10 network, this introduces the problem at layer activation adversarial attacks, but this is not mentioned in the paper (https://arxiv.org/pdf/1511.05122.pdf) Overall i think the work shows promise but is not yet ready for acceptance ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments . Short answers to your the major concerns : 1 . Our work is highly novel , the two `` prior work '' you mentioned are after or concurrent to our work . We have discussed this in section 5 . 2.We did tested score-based attack ( SPSA ) which is * * gradient-free * * . Results show that the robustness of generative classifiers is NOT due to gradient masking . 3.We did not pre-assume the correctness of the `` off-manifold '' conjecture , in fact our experiments serve as empirical verification of this conjecture . 4.The CIFAR-10 binary experiments are the best we can do to evaluate * * fully generative classifiers * * on * * natural images * * . 5.Observations on layer activation attacks against the * * fusion model * * can not directly transfer to robustness results of * * fully generative classifiers * * . We have discussed similar issues in section 5 . Detailed reply on your major questions : 1 . `` two previous work not discussed '' 1a ) Our work is at least concurrent to Schott et al . ( 2018 ) ( in fact , earlier , we have submitted evidence to the AC ) . 1b ) Schott et al . ( 2018 ) is also under submission to ICLR 2019 . 1c ) Frosst et al . ( 2018 ) was online on * * Nov 16 2018 * * , which is 1.5 months after ICLR submission deadline , and we were not aware of it . 1d ) We have discussed Schott et al . ( 2018 ) in revision ( see the second paragraph in section 5 ) . 1e ) In the below `` summary of the Q & A '' comment we extensively discussed the main difference of our work to Schott et al . ( 2018 ) , and we consider the difference to be very significant . 2 . `` Notably all the attack types are gradient based methods '' We did tested the SPSA attack ( Uesato et al.2018 ) which is based on evolutionary strategies and thus * * gradient-free * * . 2a ) This attack has been shown to be very successful against * * gradient masking based defences * * . 2b ) Our results show that SPSA worked * * worse * * than white-box attacks on generative classifiers . 2c ) Therefore , we conclude that the robustness of generative classifiers is NOT due to gradient masking . 3 . `` off-manifold conjecture pre-assumed '' We did not pre-assume the correctness of the `` off-manifold '' conjecture : 3a ) We were interested in verifying the `` off-manifold '' conjecture for both generative and discriminative classifiers . If for small epsilon the classifiers have high error , then this conjecture might not be true in practice . 3b ) The detection methods depend on the correctness of the `` off-manifold '' conjecture . Therefore , if empirically the detection methods do n't work even when we have a very good generative model ( which is the case for MNIST ) , then , this conjecture might not be true in practice . 3c ) Our empirical results clearly show that ( i ) generative classifiers are more robust than discriminative ones ( especially when epsilon is small ) ; ( ii ) the marginal/logit detection methods did work . 3d ) Observing the above , we are confident to say we have empirically verified the `` off-manifold '' conjecture on generative classifiers . 4 . `` two class cifar10 results are not particularly convincing '' We presented this example because : 4a ) Carlini & Wagner ( 2017b ) stated that MNIST results might not transfer to natural images . Therefore we wanted to see if our findings on MNIST is still true on natural images . By contrast Schott et al . ( 2018 ) only studied MNIST . 4b ) We did try full CIFAR-10 , but none of the existing generative models ( VAE/GAN/flow-based/autoregressive models ) can achieve satisfactory * * clean accuracy * * on it . 4c ) Therefore we instead studied this two-class CIFAR-10 problem in order to evaluate * * fully generative classifiers * * . We then presented the fusion model for full CIFAR-10 to address the scalability issue . 5 . `` the problem at layer activation adversarial attacks '' Thanks for pointing the relevant work which we shall cite . However : 5a ) The fusion model is not * * fully generative * * , and it contains * * discriminative features * * which might be vulnerable to attacks . 5b ) Therefore failures of the fusion model against layer activation attacks do not imply the failure of * * fully generative classifiers * * . 5c ) To be clear , we have no intension to oversell our results and claim generative classifiers are robust to * * all attacks * * . In fact we spent a full paragraph to discuss this issue , please see the third paragraph in section 5 ."}, "1": {"review_id": "HygUOoC5KX-1", "review_text": "This work investigates an interesting direction of improving robustness of classifiers against adversarial attacks by using generative models. The authors propose the *deep Bayes classifier*, which is a deep LVM based extension of naive Bayes. Furthermore, the authors extensively explore 7 possible factorisations of the classifier. Thorough experiments are conducted to assess the capability of defending or detecting adversarial examples. Besides, the authors incorporate discriminative features to generative classifiers and demonstrate clear robustness gain. ### Highlights * This work proposes an attractive direction -- the use of generative model in defending or detecting adversarial attacks. I suggest this idea should be follower by more further studies. * The presented models are quite straight-forward but exhibit good robustness against attacks listed in the experiments. * Various structural possibilities of the graphical model are examined which is preferable and helps assess the effectiveness of generative classifiers. ### Minors * Although the major point here is robustness against adversarial attacks, as mentioned by the authors, the performance on clear cases (i.e. no attacks) is unsatisfactory. Also, experiments on CIFAR are too much simplified (only 2 very unlike classes) and therefore not very convincing. * For the combination of generative classifier and discriminative features, I\u2019m curious about the results on the clear CIFAR-10 multi-class problem. It should be a very positive plus if results are satisfactory. * The writing is sometimes hard to follow. For examples, many ad-hoc abbreviations are used across the paper causing difficulties of understanding the core idea and results. ### Conclusion In general, this paper brings our attention to a previously less investigated but seemingly promising research direction, i.e. robustness of generative model against adversarial attacks. The idea is insightful and proposed models are straight-forward. While only on small-scale problems (with the presence of attacks), extensive experimental results in this paper can assist further study on this field. Thus, I recommend this paper to be accepted. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review , we are glad that you liked our paper in general . We thank for your suggestions on better writing and will include them in revision . We clarify on our CIFAR experiments in below . 1.The test on CIFAR-binary is an important contribution : 1a ) Carlini & Wagner ( 2017b ) have shown that many defense techniques that works on MNIST didn \u2019 t work very well on natural images . Therefore it is important to have a natural image classification task , to see whether the robustness results on MNIST also extends to natural images . 1b ) We did try full generative classifiers on CIFAR-10 multi-class classification . Unfortunately it is still a research challenge to make full generative classifiers work beyond MNIST . In fact we have tried even more powerful architectures like PixelCNN++ to parameterise each of p ( x|y ) , and the clean accuracy in this case is 72.4 % . Therefore we don \u2019 t think it \u2019 s fair to compare this PixelCNN++ based classifier against e.g.VGG-16 ( clean accuracy > 93 % ) , since the gap on clean accuracy is huge . 1c ) Still due to the importance of testing natural image classification tasks , we derived from CIFAR-10 a binary classification task that the deep Bayes classifiers work reasonably well ( > 90 % accuracy ) , and tested the robustness of them on this task . The general conclusions here are consistent with MNIST experiments , providing evidences that the robustness results of generative classifiers do extend to natural images . 2.The results of the fusion model are indeed on * * full CIFAR-10 multi-class classification * * . 2a ) Since full generative classifiers don \u2019 t work very well on full CIFAR-10 , we decided to take discriminative features from VGG-16 and train generative classifiers on the features . We have the clean accuracy results reported in table D.2 for multi-class classification . In this case all classifiers have > 88 % accuracy on clean data , so we can have a reasonably fair comparison here . 2b ) The robustness tests ( Figure 10 ) still favours the fusion model , and the bottleneck discriminative classifiers ( DBX- ) didn \u2019 t improve robustness against PGD & MIM . To the best of our knowledge this is the first robustness test on this fusion model . Our results show that combining generative modelling and discriminative features is an exciting future research direction . 2c ) Importantly , we show that using lower-layer features for the fusion model ( i.e.~the model is less \u201c discriminative \u201d ) returns even better results . This is a clear evidence favouring generative classifiers , and we expect in future developments , a full generative classifier that can achieve > 90 % clean accuracy on CIFAR-10 will be even more robust . Thank you for your review again , let us know if you have more questions ."}, "2": {"review_id": "HygUOoC5KX-2", "review_text": "In this work the authors propose and analyse generative models as defences against adversarial examples. In addition, three detection methods are introduced and an extension to deep features is suggested. My main concerns are as follows (details below): * Important prior work is not mentioned. * Evaluation with direct attacks is only based on (very few) gradient-based techniques, many results are not reliable. * There are signs of gradient masking (the common problem of robustness evaluation, in particular of only gradient-based techniques are used). * The way detection rates are taken into account in the perfect knowledge scenario is confusing. ### Style I like the idea of testing many different factorisation structures. However, that comes with the drawback that one needs to constantly check back what the abbreviations mean. Together with the three detection methods, the manuscript is quite confusing at times and should definitely be streamlined. One suggestion: remove the detection methods: I did not find any real conclusion about them but they are definitely side-tracking users away from the main results. ### Prior work There is at least one closely related prior work not mentioned here: the analysis by synthesis model [1]. This model uses a variational auto encoder to learn class conditional distributions and shows high robustness on MNIST. Please make clear what your contribution is over this paper (other than testing several other factorisations). ### Evaluation problems The robustness of models should be evaluated on different direct attacks ranging from gradient-based to score-based (e.g. NES [2]) to decision-based attacks [3]. Please take a look at [1] to see how a very extensive evaluation might look like. The results can be astonishingly different for different attacks, and so basing conclusion on only one or two attacks is dangerous (in particular if you only use gradient-based ones). One can also see that in your results, just check the variations you get between MIM and PGD. Also, rather then discussing (and showing in detail) results for individual attacks, the minimum adversarial distance for a given sample that can be found by any attack is much more comparable between models (which can also streamline the manuscript). One can see signs of gradient masking in your results. For example, in Figure 3 the MIM attacks levels out at 20% for the DBX model. That can happen for iterative attacks if the gradient is masked. Similarly, in Figure 5 DBX-ZK (zero knowledge) is better in both accuracy and detection rate than DBX-PKK (which takes the KL-detection method into account and should thus either be better in accuracy or detection rate). More generally, the perfect knowledge case, in which the attacker knows about the detector, should only count samples as adversarials which evade the detector and change the model decision. Thus, the detection rate should be zero. Otherwise I have no idea what trade-off between accuracy and detection rate you are actually targeting and how to compare the results. Also, some intermediate results are conflicting with each other. E.g. in 4.1 you state \u201cthe usage of bottleneck is beneficial for better robustness\u201d, but for L2 this is not true. Also, I am not sure how conclusive the grey-box and black-box scenarios really are: since the substitute is basically a DFX or DFZ, it\u2019s unsurprising that adversarials transfer best to those two models. ### Minor * In 4.1 you say \u201cas they fail to find near manifold adversarials\u201d, but I don\u2019t see how there can be L-infty adversarials on MNIST that are on-manifold (remember, MNIST pixel values are basically binary). Plus, in the zero-knowledge scenario there is nothing that enforces staying on this manifold. * Result presentation (Figure 3/5 & Table 1) is very different for different attack scenarios, which makes them hard to compare. Please unify. * Is the L2 distance you report in Table 1 the mean (or median) distance to adversarial examples. If so, GBZ (for which you state that C&W \u201cfailed on attacking\u201d has actually a smaller mean adversarial distance than some other models (for which C&W is actually quite successful). * Grey-box scenario doesn\u2019t make a lot of sense: since the substitute is basically a DFX or DFZ, it\u2019s unsurprising that adversarials transfer best to those two models. A similar confounder makes the black-box results difficult to interpret. * Also, taking into account that the paper is two pages longer and thus calls for higher standards Taken together, I find the general direction of the paper very interesting and I\u2019d definitely encourage the authors to go further. At the current stage, however, I feel that (1) contributions are not sufficiently delineated to prior work, (2) the evaluation is not convincingly supporting the claims and that (3) the manuscript needs to be streamlined (both in terms of text and figures). [1] Schott et al. (2018) \u201cTowards the first adversarially robust neural network model on MNIST\u201d (https://arxiv.org/abs/1805.09190) [2] Ilyas et al. (2018) \u201cBlack-box Adversarial Attacks with Limited Queries and Information\u201d ( [https://arxiv.org/abs/1804.08598)](https://arxiv.org/abs/1804.08598)) [3] Brendel et al. (2018) \u201cDecision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models\u201d (https://arxiv.org/abs/1712.04248)", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for pointing out the work in [ 1 ] to us . Our work is independent to [ 1 ] , and the majority of the experiments ( MNIST & CIFAR binary ) was done at the same time when v1 of [ 1 ] was released on ArXiv ( May 2018 ) . We have submitted evidences to the area chair . We were not aware of [ 1 ] until you kindly pointed it to us . We also found that [ 1 ] is currently under review at ICLR 2019 as well , so we feel that this somewhat excuses our lack of knowledge of a paper that can be at best described as concurrent work ! Since our work is concurrent to [ 1 ] , we believe our evaluation of generative classifiers on MNIST is a novel and significant contribution . * * In the revised version we have a paragraph discussing the difference between our work and the concurrent work [ 1 ] . * * We would like to explain the main differences in the following : 1 . We consider a greater number of discriminative and generative classifiers , showing how the factorization of the generative model is important . 2.We do not only evaluate on MNIST . We look at a CIFAR binary task . This is very important as Carlini and Wagner ( 2017b ) showed that robustness properties shown on MNIST often do not hold on more complicated dataset , e.g.natural images . Our results on CIFAR-binary are consistent with MNIST results , showing that generative classifiers indeed have robustness properties distinct from discriminative ones . 3.We provide ideas and experiments on how to scale robust generative classifiers to more representative color datasets such as CIFAR-10 , by building generative classifiers on pre-trained discriminative features . To the best of our knowledge , this is the first evaluation of the robustness on this fusion model , and we believe our results show that the combined approach offers an exciting research direction . 4.Our approach gives data log-likelihood meaning to the logits for free , and these logit values can be used for detecting adversarial examples . This is different from previous approaches that require training an extra detector or generative model . Also since the classifier and the detector share the same generative model , detected adversarial inputs are indeed far away from the classifier \u2019 s manifold , thus we can use both accuracies and detection rates to verify the \u201c off-manifold \u201d conjecture . 5.Throughout training and adversarial evaluation we amortize the cost of inference and use K=10 Monte Carlo samples for the latent variable z . By contrast [ 1 ] \u2019 s method is much much slower : to classify a single image , their method requires K=8000 ( ! ) initial z samples and then 50 gradient steps for z refinement . As these generative classifiers already require more computation over regular CNNs , this computation can often ill be afforded . Also the logit values in [ 1 ] \u2019 s classifier are ELBOs of log p ( x|y ) with a very simple Gaussian q distribution , which can be very different from the actual log p ( x|y ) . Instead the logit values of our generative classifier are importance sampling estimates of log p ( x|y ) which are more accurate . We thank you for your suggestion on the extra attacks to run . We agree that covering more attacks is always nice ! However , we argue that we have already covered a very strong arsenal of attacks . This is evidenced when comparing to [ 1 ] , as you can see that we evaluate the strongest attacks they find . Furthermore , we have also evaluated the C & W attack , which they have not . We believe this is important as it has been demonstrated to be a very powerful attack ( Carlini & Wagner , 2017b ) , which is also corroborated by our work . * * In the revised version we also show that SPSA as a score based attack fails to fool generative classifiers , which also indicates that the robustness results are unlikely to be caused by gradient masking . * *"}, "3": {"review_id": "HygUOoC5KX-3", "review_text": "This paper aims to test the robustness of generative classifiers [1] w.r.t. adversarial examples, considering their use as a potentially more robust alternative to adversarial training of discriminative classifiers. To achieve this, *Deep Bayes*, a generalization of the Naive Bayes classifier using a latent variable model and trained in a fashion similar to variational autoencoders [2] is introduced, and 7 different latent variable models are compared, covering a spectrum of generative or discriminative classification models, with or without bottlenecks. Their DFX and DBX architectures in particular closely match traditional discriminative classifiers, without and with a latent bottleneck. These 7 models are compared against a large range of adversarial attacks, depending on the kind of noise added (l_2 or l_inf) and how much the adversary can access (the full gradients of the model, its output on training data, or only the model as a black-box). The performance of the models is assessed depending on two criteria: how the performance of the classifier resists to adversarial noise, and how quickly the model can detect adversarial samples. Three methods for detecting adversarial samples are compared: the first (only applicable to generative classifiers) discards samples with a low likelihood, according to the off-manifold assumption [3], the second discards samples for which the classifier has low confidence in its classification (p(y|x) is under some threshold), and the third compares the output probability vector of the classifier on a sample to the mean classification vector of this class over the train data, and discards the sample if the two vectors are too dissimilar (meaning the classifier is over-confident or under-confident). The main contribution of this paper is the extensive experiments that have been done to compare the models against the various adversarial attacks. While experiments were only done on small datasets like MNIST and CIFAR (generative classifiers don't scale as easily on large image datasets), they nonetheless give very interesting insights and the authors provided encouraging results on applying generative classifiers on features learned by discriminative classifiers. Theirs result shows that generative architecture are in general more robust to the current state-of-the-art adversarial attacks, and detect adversarial examples more easily. The authors also recognize that these results may be biased by the fact that current adversarial attacks have been specifically optimized towards discriminative classifier. This is a solid paper in my opinion. The experimental setup and motivations are clearly detailed, and the paper was easy to follow. Extensive results and description of the experimental protocol are provided in the appendices, giving me confidence that the results should be reproducible. The results of this paper give interesting insights regarding how to approach robustness to adversarial examples in classification tasks, and provide realistic ways to try and apply generative classifiers in real-worlds tasks, using pre-learned features from discriminative networks. [1] http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers-a-comparison-of-logistic-regression-and-naive-bayes.pdf [2] https://arxiv.org/abs/1312.6114 [3] https://arxiv.org/abs/1801.02774", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Glad to hear that you like our manuscript ! The motivation of this work is to test whether recently developed attacks can break generative classifiers , and our results show that generative classifiers are more robust than discriminative classifiers against recent attacks . Sure research in adversarial attacks and defenses is similar to a `` cat and mouse game '' , and we anticipate in the future an attack tailored to generative classifiers can break our models . But we have done the best to test mainstream attacks available now , and indeed we showed that generative classifiers have properties different from discriminative ones . We expect future work on developing attacks & defenses on generative models can make generative models more powerful and robust ! An important insight of our approach is that the generative classifiers and the proposed detection methods are based on the same generative model . This means detected adversarial inputs are indeed far away from the classifier 's manifold , and the classifier 's manifold is also an approximation to the data manifold . By contrast , previous approaches require training an extra copy of generative model/auto-encoder . Thus it 's very likely that the detector and the classifier do not have aligned manifold representations , thus the classifier can not enjoy many benefits from the generative model . We also wanted to encourage future research on combining generative and discriminative approaches . At least from our robustness test on CIFAR-10 , this fusion approach indeed is worth further investigations , and it offers an exciting venue for future research ."}}