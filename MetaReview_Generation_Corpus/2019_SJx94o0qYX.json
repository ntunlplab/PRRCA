{"year": "2019", "forum": "SJx94o0qYX", "title": "Precision Highway for Ultra Low-precision Quantization", "decision": "Reject", "meta_review": "The submission proposes a strategy for quantization of neural networks with skip connections that quantizes only the convolution paths, while leaving the skip paths at full precision.  The approach can save computation through compressing the convolution kernels, while spending more on the skip connections.\nEmpirical results show improved performance at 2-bit quantization compared to a handful of competing methods.  Figure 5 provides some interpretation of why the method might be working in terms of \"smoothness\" of the loss surface (term not used in the traditional mathematical sense).\n\nThe paper seems to focus too much on selling the name \"precision highway\" rather than providing proper definitions of their strategy (a definition block would be a good first step), and there is little mathematical analysis of the consequences of the chosen approach.\nThere are concerns about the novelty of the method, specifically compared to Liu et al. (2018) and Choi et al. (2018b), which propose approximately the same strategy.  Footnote 1 claims that these works were conducted in parallel with the current submission, but it is unambiguously the case that Choi et al appeared on arXiv in May, and Liu et al. appeared in ECCV 2018 and on arXiv more than 30 days before the ICLR deadline, and can fairly be considered prior work https://iclr.cc/Conferences/2019/Reviewer_Guidelines\n\nThe reviewer scores were on aggregate borderline for the ICLR acceptance threshold.  On the balance, the paper seems to fall under the threshold due to insufficient novelty and analysis of the method.\n", "reviews": [{"review_id": "SJx94o0qYX-0", "review_text": "This paper investigates the problem of neural network quantization. The main idea is to employ an end-to-end precision highway to reduce the accumulated quantization error and meanwhile enable ultra-low precision in deep neural networks. The experimental results on the 3- and 2-bit quantizations of ResNet-18/50 and 2-bit quantization of an LSTM model demonstrate the effectiveness of the proposed method. This paper is well written and organized. The idea of utilizing a high-precision information flow to reduce the accumulated quantization error is technically sound. The empirical studies on accumulated quantization error, loss surface analysis, model performance, and hardware cost are quite thorough and solid. The idea of precision highway, however, is quite similar to the skip connections used in Bi-Real Net. Therefore, it may be a good idea to provide a thorough discussion over these two different methods so as to make the distinction. In Table 2, the results of Bi-Real Net is based upon 1 bit activation/weight quantization, while the proposed method uses 2 bit activation/weight quantization. To give a fair comparison, it may be better to provide 1 bit activation/weight quantization results of the proposed method.", "rating": "6: Marginally above acceptance threshold", "reply_text": "1.The difference between the proposed method and Bi-Real Net We \u2019 d like to let you know that this study was conducted in parallel with Bi-Real Net . This work was submitted to another conference and re-submitted to ICLR2019 after adding additional extensive experiments including loss surface analysis and hardware cost estimation . We respect the research outcome of Bi-Real Net and refer to it in the original manuscript . As mentioned in the original manuscript , both Bi-Real Net and PACTv2 apply quantization on pre-activation style residual net , the basic module of which is composed of batch-normalization ( BN ) \u2013 ReLU \u2013 convolution . Meanwhile , the end-to-end precision highway is a generalized \u201c network-level structural \u201d method applicable to not only pre-activation style network but also post-activation style network , having the conv-BN-ReLU module as stated in the original manuscript as follows . \u201c in the case of feed-forward networks with identity path , our precision highway idea is applicable regardless of pre-activation or post-activation structure \u201d In addition , it is a general method also applicable to recurrent network including LSTM and GRU . We described how the precision highway can be applied to LSTM in Section 3.2 and showed it significantly outperforms the existing quantization method on a language model . Since it is a novel structural method , it raises new challenges for further improvements as mentioned in Section 3.3 of the original manuscript as follows . \u201c In the case of networks with multiple candidates for the precision highway , e.g. , DenseNet , which has multiple parallel skip connections ( Huang et al. , 2017 ) , we need to address a new problem of selecting skip connections to form a precision highway , which is left for future work. \u201d We think the precision highway opened a new space of mixed precision neural network design where the precision of data representation , previously ignored , can now be jointly optimized with that of computation for further improvements of quantized networks . 2.1-bit quantization result We performed 1-bit activation/weight quantization for the post-act style ResNet-18 . For a fair comparison , we didn \u2019 t apply the teacher-student and progressive quantization method and instead adopted BN-retraining proposed in Bi-Real Net . Our 1-bit activation/weight ResNet-18 gives 56.73 / 80.11 % of Top-1/Top-5 accuracy , which is by 0.33 / 0.61 % higher than the result of Bi-Real Net , respectively . According to our observation , the final accuracy is degraded when adopting a tight approximation of the derivative of the non-differentiable sign function proposed by Bi-Real Net . Instead , the conventional quantization method proposed by DoReFa-net can improve results . This difference seems to result from the difference of the network structure and activation quantization function . We will add this result and analysis to the revision ."}, {"review_id": "SJx94o0qYX-1", "review_text": "This paper studies methods to improve the performance of quantized neural networks. The paper is largely centered around the idea of \"precision highways\" (full-precision residual connections) that run in parallel to fully-quantized convolutions. However, the paper also throws in a toolbox of other methods like distillation from a teacher network, a quantization method based on the Laplace distribution, and a fine tuning scheme. The paper reports performance for the resulting networks that is impressive but still believable. They also do very extensive experiments, including an ablation study in Table 1 that I really liked, and a study of how the precision of the skip connections impacts overall performance. I also like the visualizations of how quantization impacts the loss surface. My main concern about this paper is that is has conceptual overlap with other approaches. The authors are not the first to quantize resnets, and other papers have looked at teacher training and distillation as a method of refinement. The authors are fairly upfront about this though, and I think this paper is the first to do a really thorough investigation of the impacts of skip connections in their own right. Realistically, fully binarizing neural nets without modification is unlikely to lead to good performance. The idea of leaving the skip connections with higher precision is a good compromise that achieves hardware friendliness along with strong performance, so I think it's worth having a paper like this that takes a closer look at this approach. A few questions I had: 1) I can't tell exactly what methods are being used in Table 1. When the \"highway\" box is unchecked, does this mean the skip connection is absent? Or that it exists but with full precision? Or maybe that the skip connection branched after the quantization instead of before? Also, what fine-tuning methods is used when the \"teacher\" box is un-checked? 2) You implemented your own version of Zhuang's method. However, I'd like to know how your numbers compare to the original reported numbers in Zhuang's paper. One other minor criticism - When you fine-tune a modified network, the activations and weights will change. It could be that the networks is modifying its parameters to account for (i.e., cancel out) the quantization errors. For this reason I don't interpret Figure 4 as evidence for accumulation of error. Perhaps this type of behavior would exists if you fine-tuned two full-precision networks using different random seeds, or different teacher networks.", "rating": "7: Good paper, accept", "reply_text": "1.The answer to question 1 When the \u201c highway \u201d box is unchecked , the skip connection is branched after the quantization . This case corresponds to the conventional quantization where the quantization is combined with ReLU and , thus , skip connection is quantized before the branch . When the \u201c teacher \u201d box is unchecked , we use the conventional cross-entropy loss for training . We will clarify this in the revision . 2.The answer to question 2 We re-implemented Zhuang \u2019 s baseline , but final accuracy is different due to the minor difference of implementation details on input augmentation and teacher-student methods . According to Zhuang \u2019 s paper , their implementation shows 70.8 /88.3 % of Top-1/Top-5 accuracy for 2-bit ResNet-50 , while our implementation shows 70.48 / 89.93 % of Top1-/Top-5 accuracy . 3.The comments about Figure 4 We appreciate the comments . We \u2019 d like to first explain how we had obtained Figure 4 and how we performed again new experiments to clarify the phenomenon of accumulated quantization error in the revision . In order to obtain Figure 4 , we first obtained a fully trained full-precision network . Then , we applied 4-bit weight/activation quantization to the network while having two cases of skip connection , 4-bit one ( Zhuang \u2019 s in the figure ) and 32-bit one ( Proposed in the figure ) . Since they are from the same fully trained full-precision network , we think that the difference between the two graphs in Figure 4 represents the effect of high-precision skip connection . In order to account for the reviewer \u2019 s comments and give a more direct comparison , we did new experiments where we prepared , from the same initial condition , two activation-quantized networks ( one with precision highway and the other with low precision skip connection ) where weights are not modified and only activations are quantized to 4 bits . The new experiments give a similar result to Figure 4 while the difference in accumulated quantization errors gets slightly reduced , possibly , due to the removal of the quantization error of weights . In order to clarify the phenomenon of accumulated quantization error , we will use the new experimental results in the revision ."}, {"review_id": "SJx94o0qYX-2", "review_text": "The proposed method is advantageous in that it only requires changes to some parts of the original ResNet or LSTM, without having to significantly change the network structure or training algorithm. It also reports empirical success of using high-precision skip connections in ResNet and cell/hidden state updates in LSTMs. However, it is unclear why it is necessary to keep a high-precision activation/gradient flow. What is the problem with existing quantized networks that do not have these high-precision-flow? Also, how does the high-precision flow interact with the rest of the network (with low-precision operations)? Moreover, the proposed method has limited novelty as the use of full-precision skip connections has been proposed in Bi-Real (Liu et al. 2018). Minor: - It is hard to tell that the weight histogram in Figure 3 is similar to a Laplacian distribution. It can also be approximated by other distributions (such as Gaussian or piecewise-linear distributions). - What kind of activation quantization is used? - In the experiments, when is the cosine similarity between the quantized and full-precision networks computed? after training or on an intermediate training step? - What are the axes in Figure 5? Why is there only one local minimum in Figure 5(d)? Why the training with PH converges even slower than without PH at the early stage of training?", "rating": "5: Marginally below acceptance threshold", "reply_text": "1.The importance of precision highway Precision highway helps reduce the accumulated quantization error . In ResNet , the difference between Equations ( 1 ) and ( 2 ) explains how the precision highway reduces quantization error . Without precision highway , the output of residual block has additional quantization error , \u2018 e \u2019 in Equation ( 1 ) while the precision highway removes it as shown in Equation ( 2 ) . Section 3.2 describes how the precision highway reduces the accumulation of quantization error in the LSTM as follows . \u201c Specifically , when calculating ct , the inputs are not quantized , which reduces the accumulation of quantization error on ct . The computation of ht can also reduce the accumulation of quantization error by utilizing high-precision inputs . The construction of such a precision highway allows us to propagate high-precision information , i.e. , cell states ct and outputs ht , across time steps \u201d The result of low-precision computation is in high precision before quantization . We perform elementwise operations ( additions in ResNet and multiplications in LSTM/GRU ) between the precision highway and the high-precision result of low-precision computation . In other words , the elementwise computations in ResNet and LSTM/GRU are performed in high precision as mentioned in the original manuscript as follows . \u201c We keep high-precision activation only on the skip connection and utilize it only for the element-wise addition . \u201d in Section 3.1 . \u201c In our proposed method , all of the element-wise multiplications in Equations 3e and 3f are performed in high precision. \u201d in Section 3.2 . 2.The novelty of the proposed method compared to Bi-Real Net Please refer to our response to reviewer 3 . 3.Laplace distribution approximation Please note that the y-axis is in log-scale while x-axis in linear scale . The histogram decreases linearly in the plot , which is well modeled by Laplace distribution . The jitter at the ends is due to the fact that the number of samples is small , and the range is in log-scale . We performed the same quantization adopting other distributions including Gaussian and triangle distributions , and the Laplace distribution showed marginally better results than the others . 4.What kind of activation quantization is used ? We use the conventional quantization method used in DoReFa-net , and the method is also adopted in Zhuang \u2019 s work . After clipping the activation to a pre-defined value , typically 1 , the linear quantization is applied to the activation . We will clarify this in the revision . 5. when is the cosine similarity between the quantized and full-precision networks computed ? Please refer to our response to reviewer 2 . 6.What are the axes in Figure 5 ? Why is there only one local minimum in Figure 5 ( d ) ? Why the training with PH converges even slower than without PH at the early stage of training ? We appreciate the comments . It helped clarify the loss surface analysis in Figure 5 . In order to obtain Figure 5 , we applied Hao Li \u2019 s method as mentioned in the paper . In short , each figure represents loss surface seen from the local minimum we obtained from the training , i.e. , the weight vector of the final trained model . In order to obtain two-dimensional view , we utilize two base vectors , u1 and u2 , each of which corresponds to the axis of the figure . The base vector is a randomly generated vector having the same dimension of the weight vector . According to ( Li et al. , 2017 ) , two randomly generated high-dimensional vectors tend to be orthogonal to each other . The origin of the figure at ( 0 , 0 ) corresponds to the weight vector of the local minimum . The z-axis corresponds to the loss . In order to obtain a point , e.g. , ( 0.25 , 0.5 ) in the figure , we scale the two base vectors , i.e. , 0.25 * u1 and 0.5 * u2 , and add them to the local minimum weight vector corresponding to the origin . Then , we obtain the loss for the new weight vector , which is depicted at the point , ( 0.25 , 0.5 ) on Figure 5 . Since the figure is a loss surface near the local minimum , we tend to have a single local minimum in the figure unless we have another local minimum near the obtained one . The figure does not represent the relationship between loss and training epochs . We will clarify how we obtained Figure 5 in the revision ."}], "0": {"review_id": "SJx94o0qYX-0", "review_text": "This paper investigates the problem of neural network quantization. The main idea is to employ an end-to-end precision highway to reduce the accumulated quantization error and meanwhile enable ultra-low precision in deep neural networks. The experimental results on the 3- and 2-bit quantizations of ResNet-18/50 and 2-bit quantization of an LSTM model demonstrate the effectiveness of the proposed method. This paper is well written and organized. The idea of utilizing a high-precision information flow to reduce the accumulated quantization error is technically sound. The empirical studies on accumulated quantization error, loss surface analysis, model performance, and hardware cost are quite thorough and solid. The idea of precision highway, however, is quite similar to the skip connections used in Bi-Real Net. Therefore, it may be a good idea to provide a thorough discussion over these two different methods so as to make the distinction. In Table 2, the results of Bi-Real Net is based upon 1 bit activation/weight quantization, while the proposed method uses 2 bit activation/weight quantization. To give a fair comparison, it may be better to provide 1 bit activation/weight quantization results of the proposed method.", "rating": "6: Marginally above acceptance threshold", "reply_text": "1.The difference between the proposed method and Bi-Real Net We \u2019 d like to let you know that this study was conducted in parallel with Bi-Real Net . This work was submitted to another conference and re-submitted to ICLR2019 after adding additional extensive experiments including loss surface analysis and hardware cost estimation . We respect the research outcome of Bi-Real Net and refer to it in the original manuscript . As mentioned in the original manuscript , both Bi-Real Net and PACTv2 apply quantization on pre-activation style residual net , the basic module of which is composed of batch-normalization ( BN ) \u2013 ReLU \u2013 convolution . Meanwhile , the end-to-end precision highway is a generalized \u201c network-level structural \u201d method applicable to not only pre-activation style network but also post-activation style network , having the conv-BN-ReLU module as stated in the original manuscript as follows . \u201c in the case of feed-forward networks with identity path , our precision highway idea is applicable regardless of pre-activation or post-activation structure \u201d In addition , it is a general method also applicable to recurrent network including LSTM and GRU . We described how the precision highway can be applied to LSTM in Section 3.2 and showed it significantly outperforms the existing quantization method on a language model . Since it is a novel structural method , it raises new challenges for further improvements as mentioned in Section 3.3 of the original manuscript as follows . \u201c In the case of networks with multiple candidates for the precision highway , e.g. , DenseNet , which has multiple parallel skip connections ( Huang et al. , 2017 ) , we need to address a new problem of selecting skip connections to form a precision highway , which is left for future work. \u201d We think the precision highway opened a new space of mixed precision neural network design where the precision of data representation , previously ignored , can now be jointly optimized with that of computation for further improvements of quantized networks . 2.1-bit quantization result We performed 1-bit activation/weight quantization for the post-act style ResNet-18 . For a fair comparison , we didn \u2019 t apply the teacher-student and progressive quantization method and instead adopted BN-retraining proposed in Bi-Real Net . Our 1-bit activation/weight ResNet-18 gives 56.73 / 80.11 % of Top-1/Top-5 accuracy , which is by 0.33 / 0.61 % higher than the result of Bi-Real Net , respectively . According to our observation , the final accuracy is degraded when adopting a tight approximation of the derivative of the non-differentiable sign function proposed by Bi-Real Net . Instead , the conventional quantization method proposed by DoReFa-net can improve results . This difference seems to result from the difference of the network structure and activation quantization function . We will add this result and analysis to the revision ."}, "1": {"review_id": "SJx94o0qYX-1", "review_text": "This paper studies methods to improve the performance of quantized neural networks. The paper is largely centered around the idea of \"precision highways\" (full-precision residual connections) that run in parallel to fully-quantized convolutions. However, the paper also throws in a toolbox of other methods like distillation from a teacher network, a quantization method based on the Laplace distribution, and a fine tuning scheme. The paper reports performance for the resulting networks that is impressive but still believable. They also do very extensive experiments, including an ablation study in Table 1 that I really liked, and a study of how the precision of the skip connections impacts overall performance. I also like the visualizations of how quantization impacts the loss surface. My main concern about this paper is that is has conceptual overlap with other approaches. The authors are not the first to quantize resnets, and other papers have looked at teacher training and distillation as a method of refinement. The authors are fairly upfront about this though, and I think this paper is the first to do a really thorough investigation of the impacts of skip connections in their own right. Realistically, fully binarizing neural nets without modification is unlikely to lead to good performance. The idea of leaving the skip connections with higher precision is a good compromise that achieves hardware friendliness along with strong performance, so I think it's worth having a paper like this that takes a closer look at this approach. A few questions I had: 1) I can't tell exactly what methods are being used in Table 1. When the \"highway\" box is unchecked, does this mean the skip connection is absent? Or that it exists but with full precision? Or maybe that the skip connection branched after the quantization instead of before? Also, what fine-tuning methods is used when the \"teacher\" box is un-checked? 2) You implemented your own version of Zhuang's method. However, I'd like to know how your numbers compare to the original reported numbers in Zhuang's paper. One other minor criticism - When you fine-tune a modified network, the activations and weights will change. It could be that the networks is modifying its parameters to account for (i.e., cancel out) the quantization errors. For this reason I don't interpret Figure 4 as evidence for accumulation of error. Perhaps this type of behavior would exists if you fine-tuned two full-precision networks using different random seeds, or different teacher networks.", "rating": "7: Good paper, accept", "reply_text": "1.The answer to question 1 When the \u201c highway \u201d box is unchecked , the skip connection is branched after the quantization . This case corresponds to the conventional quantization where the quantization is combined with ReLU and , thus , skip connection is quantized before the branch . When the \u201c teacher \u201d box is unchecked , we use the conventional cross-entropy loss for training . We will clarify this in the revision . 2.The answer to question 2 We re-implemented Zhuang \u2019 s baseline , but final accuracy is different due to the minor difference of implementation details on input augmentation and teacher-student methods . According to Zhuang \u2019 s paper , their implementation shows 70.8 /88.3 % of Top-1/Top-5 accuracy for 2-bit ResNet-50 , while our implementation shows 70.48 / 89.93 % of Top1-/Top-5 accuracy . 3.The comments about Figure 4 We appreciate the comments . We \u2019 d like to first explain how we had obtained Figure 4 and how we performed again new experiments to clarify the phenomenon of accumulated quantization error in the revision . In order to obtain Figure 4 , we first obtained a fully trained full-precision network . Then , we applied 4-bit weight/activation quantization to the network while having two cases of skip connection , 4-bit one ( Zhuang \u2019 s in the figure ) and 32-bit one ( Proposed in the figure ) . Since they are from the same fully trained full-precision network , we think that the difference between the two graphs in Figure 4 represents the effect of high-precision skip connection . In order to account for the reviewer \u2019 s comments and give a more direct comparison , we did new experiments where we prepared , from the same initial condition , two activation-quantized networks ( one with precision highway and the other with low precision skip connection ) where weights are not modified and only activations are quantized to 4 bits . The new experiments give a similar result to Figure 4 while the difference in accumulated quantization errors gets slightly reduced , possibly , due to the removal of the quantization error of weights . In order to clarify the phenomenon of accumulated quantization error , we will use the new experimental results in the revision ."}, "2": {"review_id": "SJx94o0qYX-2", "review_text": "The proposed method is advantageous in that it only requires changes to some parts of the original ResNet or LSTM, without having to significantly change the network structure or training algorithm. It also reports empirical success of using high-precision skip connections in ResNet and cell/hidden state updates in LSTMs. However, it is unclear why it is necessary to keep a high-precision activation/gradient flow. What is the problem with existing quantized networks that do not have these high-precision-flow? Also, how does the high-precision flow interact with the rest of the network (with low-precision operations)? Moreover, the proposed method has limited novelty as the use of full-precision skip connections has been proposed in Bi-Real (Liu et al. 2018). Minor: - It is hard to tell that the weight histogram in Figure 3 is similar to a Laplacian distribution. It can also be approximated by other distributions (such as Gaussian or piecewise-linear distributions). - What kind of activation quantization is used? - In the experiments, when is the cosine similarity between the quantized and full-precision networks computed? after training or on an intermediate training step? - What are the axes in Figure 5? Why is there only one local minimum in Figure 5(d)? Why the training with PH converges even slower than without PH at the early stage of training?", "rating": "5: Marginally below acceptance threshold", "reply_text": "1.The importance of precision highway Precision highway helps reduce the accumulated quantization error . In ResNet , the difference between Equations ( 1 ) and ( 2 ) explains how the precision highway reduces quantization error . Without precision highway , the output of residual block has additional quantization error , \u2018 e \u2019 in Equation ( 1 ) while the precision highway removes it as shown in Equation ( 2 ) . Section 3.2 describes how the precision highway reduces the accumulation of quantization error in the LSTM as follows . \u201c Specifically , when calculating ct , the inputs are not quantized , which reduces the accumulation of quantization error on ct . The computation of ht can also reduce the accumulation of quantization error by utilizing high-precision inputs . The construction of such a precision highway allows us to propagate high-precision information , i.e. , cell states ct and outputs ht , across time steps \u201d The result of low-precision computation is in high precision before quantization . We perform elementwise operations ( additions in ResNet and multiplications in LSTM/GRU ) between the precision highway and the high-precision result of low-precision computation . In other words , the elementwise computations in ResNet and LSTM/GRU are performed in high precision as mentioned in the original manuscript as follows . \u201c We keep high-precision activation only on the skip connection and utilize it only for the element-wise addition . \u201d in Section 3.1 . \u201c In our proposed method , all of the element-wise multiplications in Equations 3e and 3f are performed in high precision. \u201d in Section 3.2 . 2.The novelty of the proposed method compared to Bi-Real Net Please refer to our response to reviewer 3 . 3.Laplace distribution approximation Please note that the y-axis is in log-scale while x-axis in linear scale . The histogram decreases linearly in the plot , which is well modeled by Laplace distribution . The jitter at the ends is due to the fact that the number of samples is small , and the range is in log-scale . We performed the same quantization adopting other distributions including Gaussian and triangle distributions , and the Laplace distribution showed marginally better results than the others . 4.What kind of activation quantization is used ? We use the conventional quantization method used in DoReFa-net , and the method is also adopted in Zhuang \u2019 s work . After clipping the activation to a pre-defined value , typically 1 , the linear quantization is applied to the activation . We will clarify this in the revision . 5. when is the cosine similarity between the quantized and full-precision networks computed ? Please refer to our response to reviewer 2 . 6.What are the axes in Figure 5 ? Why is there only one local minimum in Figure 5 ( d ) ? Why the training with PH converges even slower than without PH at the early stage of training ? We appreciate the comments . It helped clarify the loss surface analysis in Figure 5 . In order to obtain Figure 5 , we applied Hao Li \u2019 s method as mentioned in the paper . In short , each figure represents loss surface seen from the local minimum we obtained from the training , i.e. , the weight vector of the final trained model . In order to obtain two-dimensional view , we utilize two base vectors , u1 and u2 , each of which corresponds to the axis of the figure . The base vector is a randomly generated vector having the same dimension of the weight vector . According to ( Li et al. , 2017 ) , two randomly generated high-dimensional vectors tend to be orthogonal to each other . The origin of the figure at ( 0 , 0 ) corresponds to the weight vector of the local minimum . The z-axis corresponds to the loss . In order to obtain a point , e.g. , ( 0.25 , 0.5 ) in the figure , we scale the two base vectors , i.e. , 0.25 * u1 and 0.5 * u2 , and add them to the local minimum weight vector corresponding to the origin . Then , we obtain the loss for the new weight vector , which is depicted at the point , ( 0.25 , 0.5 ) on Figure 5 . Since the figure is a loss surface near the local minimum , we tend to have a single local minimum in the figure unless we have another local minimum near the obtained one . The figure does not represent the relationship between loss and training epochs . We will clarify how we obtained Figure 5 in the revision ."}}