{"year": "2021", "forum": "F9sPTWSKznC", "title": "DiP Benchmark Tests: Evaluation Benchmarks for Discourse Phenomena in MT", "decision": "Reject", "meta_review": "This paper introduces a dataset and a trained evaluation metric for evaluating discourse phenomena for MT. Several context-aware MT models are compared against a sentence level baseline. The paper develops metrics which evaluate the models according to their performance on four discourse phenomena: anaphora, lexical consistency, coherence and readability, and discourse connective translation. Data is released for three language pairs (all using English as the target language). \n\nFirst, I\u2019d like to point out that creating datasets and benchmarks for analyzing/evaluating discourse-level errors in machine translation is an extremely valuable contribution. This paper is addressing a very relevant problem and even though there is no new model/method/algorithm being proposed, this work *fits* this conference - it is my opinion that the community should welcome and value more than it currently does the efforts spent in creating high quality datasets that can help make progress in the field.\n\nThere was substantial discussion among reviewers about this paper. \n\nThe main weaknesses raised by the reviewers were:\n- Limited information about the process to create the anaphora test, which was a contribution of prior work (Jwalapuram et al. (2019) - this was addressed in the updated version; but the anaphora challenge sets seem to be only a minor update over previous work.\n- All language pairs use English as the target language, and it is not simple to extend this approach beyond English target languages.\n- Lack of detail on how BLEU scores were computed (tokenised? true cased? My recommendation is to use sacrebleu) - this was clarified in the rebuttal.\n- The evaluated NMT models all date from 2018 or earlier.\n- Two of the 4 benchmarks (anaphora and coherence) are evaluated by neural models trained on WMT outputs, which makes the interpretation of scores is opaque, and their validity is unclear.\n\nWhile the creation of a benchmark for discourse evaluation of MT is a laudable effort as mentioned above, it is my opinion that due to some of the weaknesses above the current version of this work is not yet ready for publication. However, I strongly encourage the authors to improve upon these points and resubmit their work to another venue. I list some suggestions below to improve this paper.\n\nMy biggest concern with the current version is the last weakness above. As pointed out by a reviewer, the framework of Jwalapuram et al. (2019) provides empirical support for the model's sensitivity (if there is a pronoun error, does the metric pick it up?). But they don\u2019t necessarily capture model *specificity* (if the metric ranks one output higher, can we be confident that this is because of a pronoun translation error?). For the coherence metric, authors make an argument that their metric is sensitive to coherence issue, but concerns remain about whether it is sufficiently specific to these issues. In the rebuttal, authors argue that BLEURT is sentence-level, but they could easily aggregate sentence-level judgments and report correlation between BLEURT and human coherence  judgments to show that their metric correlates better with human coherence judgments than BLEURT or even just BLEU. Besides BLEURT, I would add there are other recently proposed metrics that may capture discourse phenomena (neural metrics trained against MQM annotations or sentence-level human assessments with document context) and should be compared against: check COMET [1] or PRISM [2] (the latter is sentence-based but could be adapted for paragraphs or documents).\n\nThere is also prior work comparing various context-aware machine translation approaches against a sentence-level baseline, some with negative findings [3,4,5]. I suggest the authors look at this related work in future iterations of their paper.\n\n[1] https://arxiv.org/pdf/2009.09025.pdf\n\n[2] https://arxiv.org/pdf/2004.14564.pdf \n\n[3] https://www.aclweb.org/anthology/2020.eamt-1.24.pdf\n\n[4] https://arxiv.org/pdf/1910.00294.pdf \n\n[5] https://www.aclweb.org/anthology/2020.emnlp-main.81.pdf \n", "reviews": [{"review_id": "F9sPTWSKznC-0", "review_text": "This paper presents four methods for creating benchmarking datasets , each focusing on a particular discourse phenomenon which is difficult/hard to solve with existing context-aware neural machine translation ( NMT ) models . It also evaluates several existing NMT models using the created data and makes a remark about their current status , such as the superiority of one method in one task . This work has the originality and I am happy to know that some researchers devote their efforts to address these issues . Moreover , I am sure that the field of NMT definitely benefits from this work , once the created data are made publicly available . However , I am very skeptical that the contribution of this paper fits for ICLR , since the methodology presented in this paper itself mainly consists of a pile of human efforts . In other words , if I understand the contents properly , only the technical advancement presented in this paper is TgtCon , a variation of the anaphora-centric method proposed by Voita et al . ( 2018b ) .However , it can not be a substantial merit to the community , since it does not necessarily perform better than existing methods and the reason of its deterioration is not analyzed . Considering the main focus of the paper , i.e. , creating benchmarking datasets , the paper should rather fit for a journal article in the field of natural language processing , where the authors can give more details in creating the datasets , such as procedure , tool , and attributes of annotators , not in appendices , and more careful analyses of the results , including WHY each method does ( not ) perform well on a particular discourse phenomenon . Indeed , when I was reading this paper , I suffered from the fact that the main part of the paper is not self-contained . For instance , the proposed TgtCon is not explained in Section 2 but the readers are advised to see an appendix . Other information that is indispensable in data creation but missing in this paper is the detail of human judgment , such as the proficiency of the evaluators , protocol , and judgment criteria . Each section reports on agreement ratio , but not Cohen 's kappa . With these reasons , I am not completely convinced of the quality of the resulted datasets and the portability of the proposed methods . Below shows some questions . Q1.Is there a reason to choose these four particular discourse phenomena ? Are they exclusive to each other ? Q2.Test sets for coherence/readability and discourse connectives are significantly smaller than those for the other two phenomena . I am not sure that a test set with 200 or less examples is enough . Q3.In Section 3.2.1 , the authors regard the inconsistency of named entities . However , in many error classification schemes , terminology errors are , irrespective of incorrect translations and inconsistency , distinguished from coherence errors . For instance , the Multidimensional Quality Metrics ( * 1 ) locate them in different branches . ( * 1 ) http : //www.qt21.eu/mqm-definition/ Q4 . What does `` Random translations '' in Section 3.3.1 mean ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for recognizing the scalability and originality of our work . We address your concerns here . * * Lack of technical advancement/Suitability of Venue : * * We humbly disagree with your view ! The main goal of the paper is to introduce extensive discourse based benchmarks for MT models and to motivate finer-grained evaluations of models across languages . To this end , we introduced several carefully constructed test sets and evaluation measures that were validated against human judgments . Although technical advancement is not the main contribution of this paper , it is the overarching objective of the paper . We aim to push for improvements in NMT models that go beyond merely optimizing word-overlap with reference texts . NMT models are evolving fast ( and are commonly part of ICLR submissions ) but there is a distinct lack of fine-grained testing . It is our contention that frameworks for evaluation of models must be introduced in the same venues as the models themselves , in order to ensure that better evaluation practices are adopted by the community that produces the models . For the same reason , papers on robustness of neural models have been well accepted by the ICLR community ( e.g. , [ 1 ] accepted as an oral ) . More closely related to ours , popular benchmarking frameworks such as GLUE [ 2 ] and benchmark for RL [ 3 ] were also introduced through ICLR . Similarly , the SuperGLUE [ 4 ] and XTREME [ 5 ] benchmarks were introduced in NeurIPS-2019 and ICML-2020 , respectively . In this regard , please allow us to quote a common reviewing oversight referred from the ICLR website ( https : //iclr.cc/Conferences/2021/ReviewerGuide ) : * \u201c \u201c This work does not fit topic X. \u201d \u2013 When reviewers come across a paper that does not fit the typical paper mold for a particular venue , one common response is to dismiss it for this very reason . If you find yourselves writing this in your review , stop and think whether you are being needlessly conservative . Is it possible that this new approach will actually move the field forward ? \u201d * We sincerely hope that our reviewer reconsiders their decision and gives our paper a fair chance . * * Agreements reported : * * As mentioned in the Appendix ( A1 ) , agreements reported in the paper are the AC1 Gamma coefficient proposed by Gwet ( 2008 ) . Due to the nature of the datasets used for the user studies ( essentially erroneous translations vs reference ) , annotators are more likely to choose the reference as the better candidate . This yields a skewed distribution of the annotations ; traditional correlation measures such as Cohen \u2019 s Kappa are not robust to this . Therefore we report the more appropriate Gwet \u2019 s AC1/Gamma coefficient . It is also the agreement reported by Jwalapuram et al. ( 2019 ) . * * Choice of discourse phenomena : * * Yes , they are mostly exclusive to each other ( although coherence has been used as a generic term to mean all the phenomena that contribute to document level understanding ) . In the literature , discourse phenomena have been broadly categorized into two : cohesion and coherence . Anaphora and lexical consistency fall under cohesion , while discourse relations ( e.g. , Contrast , Explanation ) and connectives fall under coherence . These are the four main discourse phenomena ; all other phenomena can be subsumed under one of these . Both Sennrich ( 2018 ) [ 6 ] and Hardmeier ( 2018 ) [ 7 ] list these four as the main discourse phenomena . -- [ 1 ] Synthetic and Natural Noise Both Break Neural Machine Translation . Yonatan Belinkov , Yonatan Bisk . In ICLR 2018 ( Oral presentation ) . [ 2 ] GLUE : A MultiTask Benchmark and Analysis Platform for Natural Language Understanding . Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy and Samuel R. Bowman . In ICLR 2019 . [ 3 ] Behaviour Suite for Reinforcement Learning . Ian Osband , Yotam Doron , Matteo Hessel , John Aslanides Eren Sezener , Andre Saraiva , Katrina McKinney , Tor Lattimore , Csaba Szepesvari Satinder Singh , Benjamin Van Roy , Richard Sutton , David Silver , Hado Van Hasselt . In ICLR 2020 . [ 4 ] SuperGLUE : A Stickier Benchmark for General-Purpose Language Understanding Systems . Alex Wang , Yada Pruksachatkun , Nikita Nangia , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy and Samuel R. Bowman . In NeurIPS 2019 . [ 5 ] XTREME : A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization . J. Hu , Sebastian Ruder , Aditya Siddhant , Graham Neubig , Orhan Firat and M. Johnson . In ICML 2020 . [ 6 ] http : //homepages.inf.ed.ac.uk/rsennric/wnmt2018.pdf [ 7 ] https : //ufal.mff.cuni.cz/mtm18/files/03-discourse-in-mt-christian-hardmeier.pdf"}, {"review_id": "F9sPTWSKznC-1", "review_text": "This paper presents a benchmark for discourse phenomena in machine translation . Its main novelty lies in the relatively large scale , spanning three translation directions , four discourse phenomena , and 150-5000 data points per language and phenomenon . A relatively large number of systems from previous work is benchmarked on each test set , and agreement with human judgments is measured . positives : - clearly , a lot of thought and effort has gone into the creation of the benchmarks , and this is the most diverse set of benchmarks for discourse phenomena in MT so far . - extensive experiments with models from previous work , along with human analysis . negatives : - two of the four benchmarks , anaphora and coherence , are evaluated by neural models trained on WMT outputs , so the interpretation of scores is opaque , and their validity is unclear . Specifically , Jwalapuram et al . ( 2019 ) train a neural network to distinguish references from MT output based on the ELMo representations of pronouns , but in principle , this model can use signals other than the correctness of pronouns translation to make this distinction . Similarly , the model by Moon et al . ( 2019 ) was originally trained to distinguish real documents from randomly shuffled ones , and I can see how their complex neural network would then learn to rely on coherence features . However , this submission uses reference translations as positive , MT output as negative examples , so it again may learn to use features other than coherence for its decisions . - also , I 'm not fully convinced about the validity of the automatic discourse connective evaluation . According to the manual analysis , there is a large proportion of false negatives ( synonymous translations flagged as errors ) , and rankings would change if synonyms were counted as correct . I was also not satisfied with the evidence that the omission of connectives is generally an error . The human study was a bit simplistic in that it just deleted connectives ( although more changes might be needed to ensure grammaticality ) or used noisy MT output rather than alternative human references . It also seems to have been monolingual . If the test set consists of examples with ambiguous or implicit discourse relations in the source , then it may actually be the right translation strategy to omit them . I 'm worried that a benchmark that rewards explicitation and punishes leaving discourse relations implicit may set the wrong incentives . - I was surprised by the low results ( already in terms of BLEU ) of some of the tested variants . Authors describe in great care their efforts to fairly reproduce previous work , including the use of original code and hyperparameters where possible , but I ca n't help but think that the models are suboptimally trained , and that statements about whether context-aware models consistently improve discourse phenomena are tainted by this . recommendation : I 'm leaning negative on the current version of the paper and benchmark . I think the test sets have been carefully assembled , and along with the various types of models evaluated on them , this work has value . But before I 'd recommend that the benchmarks actually be used in the field , authors would need to improve upon the evaluation scores used for anaphora , coherence , and discourse connectives and make sure they really are targeted towards the phenomena they claim to measure , and do not have large blind spots . further questions and minor problems : - did you perform early stopping ? What was your stopping criterion ? - do you have an explanation why anaphora scores differ wildly between systems for ZH-EN ( table 9 ) ? - the discourse connectives test sets are based on examples where the reference contains a connective , but MT output does not . What MT system was used ? Do the respective source segments generally contain explicit discourse connectives , or are the respective discourse relations generally implicit in the source ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for noting the effort and value of our work . We address your concerns here : * * Validity of Model Evaluation : * * 1 . Anaphora ( Jwalapuram et al. , 2019 ) : Although the model is trained on reference vs. MT output , their paper shows results for testing on two settings : * Reference vs. MT output , where the model is 90.69 % accurate , * Reference vs. noisy reference , where everything in the noisy version is identical except for the pronoun ; here the accuracy is 89.11 % . They also report high agreements ( > =0.8 ) with human judgments on similar noisy data . This convinced us that the model focuses on pronouns during evaluation . 2.Coherence ( Moon et al. , 2019 ) : Coherence is complex , and can include grammaticality , topic structures and inter-sentential relations , which the Moon et al . ( 2019 ) model claims to capture . We validated the model 's ability to distinguish coherence ( and readability ) by conducting a user study , where we compared the model 's ranking of texts against human rankings of texts and found strong agreements between the two ( > 0.8 ) . Since coherence is a subjective concept , there is no other way to validate the effectiveness of the model other than by comparing to human judgments , which we therefore do . We hope the reviewer can understand the challenges and appreciate our sincere effort to address them , especially considering the fact that currently there is no such established benchmark to measure progress . As mentioned in our response to AnonReviewer1 , there has been a recent trend towards model-based evaluation metrics like MoverScore ( Zhao et al , EMNLP-2019 ) , BERTScore ( Zhang et al , ICLR-2020 ) , BLEURT ( Sellam et al , ACL-2020 ) , etc . * * Validity of Discourse Connectives test set : * * * * Evaluation : * * Discourse connectives are complex , and can be quite hard to evaluate . It can be hard to perfectly define synonyms and distil them into a list , as they can convey different discourse relations depending on the context . In light of the fact that evaluating erroneous translations under such circumstances would be extremely difficult , we chose to instead concentrate on cases where MT systems omit the connective , since omissions are easier to detect and evaluate . We agree that synonyms should not be penalized ; this is why we included the percentages for when ANY candidate connective was produced by the MT output in order to give them the benefit of the doubt . * * User studies : * * Our decision to conduct two user studies for the connective test set validation stemmed from our discussion with the creators of the PDTB corpus . Note that both the studies provide context information to the users . 1.The first study uses the reference with the connective deleted , compared to the reference with the connective . This study serves to prove that connectives serve a purpose and omitting them is problematic . The high agreement ( > 0.9 ) results of this study preferring the reference with the connective indicate that connectives are necessary for grammaticality or for discourse relations to hold . 2.The second study uses the MT output with the missing connective , compared to the reference with the connective . This study serves to prove that the MT outputs are indeed making omission errors . If the MT outputs with missing connectives were structured in such a way as to have implicit discourse relations , the agreements that favoured the references should be significantly lower ; however we find strong ( > 0.8 ) agreements that favour the reference with the connective . In general , manual examination has shown that the source sentences do contain an explicit connective , and that reference translations very rarely introduce extra words that do not occur in the source ( we also detail a similar examination conducted for lexical consistency in the appendix ) . In summary , as the first of its kind benchmark for discourse level phenomena in MT , we left no stone unturned to make the evaluation as trustworthy as possible . * * MT systems used for testset generation : * * As mentioned in Section 3 , all testsets are based on multiple MT system outputs submitted to WMT , ensuring that there is no bias against particular types of models . For De-En , Ru-En and Zh-En , these consist of translation outputs from 68 , 41 and 47 unique systems respectively ."}, {"review_id": "F9sPTWSKznC-2", "review_text": "In this paper , the authors propose specific test sets for document-level NMT . They target anaphora , coherence/readability , lexical consistency and discourse connectives , and are available for multiple language pairs . The first two challenge sets rely on model-based evaluation . Strengths : The test sets , which target various discourse phenomena , directly evaluate the output of the models ( contrarily to some existing multiple-choice challenge sets ) . The types of mistakes made by NMT models are manually examined . The authors validate the quality of their metrics by comparing against human judgements ( although the number of samples is arguably small ) . Weaknesses : The evaluated NMT models all date from 2018 or earlier . The anaphora challenge sets are only a minor update over previous work . All language pairs use English as the target language . Other remarks and questions : For the anaphora and coherence/readability test sets , future work may `` cheat '' by using the evaluation models as part of the NMT systems . Is normalizing the scores actually useful ? The reference scores should be the same across all systems , so it only shifts all results by a constant and does n't affect relative performance . Whats steps would be needed to construct similar test sets for En- > X language pairs ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for recognizing our contributions . We address some of the questions and concerns here : * * All models from 2018 or earlier : * * Context-aware models are not as common as sentence-level models although many MT researchers have advocated for it . We chose some well-established , popularly cited work since we thought they would be representative . We aimed at testing original author codes for maximum credibility , so we chose papers with implementations . We also picked some older models due to their properties ( e.g.inclusion of target-side context ) . * * Future work may \u201c cheat \u201d : * * We agree that this is possible , but this would be true of any model-based evaluation . We can not let this preclude us from using all model-based evaluations , since that would mean we would be limited to simple automatic metrics like BLEU that can not produce finer-grained or nuanced evaluations ( moreover , BLEU has also been optimized ) . Discourse phenomena , especially coherence , are complex concepts that can not be captured by simple metrics , and require model-based evaluation . There has also been a recent trend towards model-based evaluation metrics like MoverScore ( Zhao et al , EMNLP-2019 ) , BERTScore ( Zhang et al , ICLR-2020 ) , BLEURT ( Sellam et al , ACL-2020 ) , etc . to name a few . We also think it would be non-trivial to use the evaluation models in NMT systems while balancing the translation objective . Moreover , for anaphora , the test set is independent of the evaluation model , so the error distributions are likely to be different . * * Normalization : * * The evaluation models are both pairwise ranking models , requiring reference and candidate translations as input together , and are trained to score the reference higher . Conceptually , it is not the absolute scores but the relative differences between pairs that are meaningful . Also , inputs with the same reference sometimes get slightly different scores due to some internal randomness stemming from ELMo embeddings . We added the normalization to account for these two issues ; we will add this explanation to the paper . * * Generalizability to other languages : * * Our test sets can be generated in other languages ( reproducing the answer above to AnonReviewer2 here for your convenience ) : 1 . * * Anaphora : * * the pronouns need to be separate morphemes ( and not attached to verbs etc . ) . If there are several equivalent pronoun translations , a list may be needed so they can be excluded from being considered translation errors . E.g.Miculicich Werlen and Popescu-Belis ( 2017 ) has such a list for French ; a list can also be collected through user studies as in Jwalapuram et al ( 2019 ) . 2 . * * Coherence : * * The coherence model ( Moon et al , 2019 ) used to find poorly translated texts was re-trained on reference vs. MT outputs , which is also certainly possible for other languages since WMT system outputs are available . The coherence model from Moon et al ( 2019 ) is an end-to-end neural model that does not rely on any language-specific features , and thus can be trained on any target language . However , language-specific or multilingual coherence models could also be used since Moon et al ( 2019 ) primarily train and test their model on English ( WSJ ) data . 3 . * * Lexical Consistency * * : A lemmatizer was used to reduce common suffixes for detecting lexical consistency ( e.g. \u201c box \u201d and \u201c boxes \u201d should not be detected as inconsistent words ) , so a similar tool will be needed for any other target language . CLTK provides a lemmatizer for several languages . 4 . * * Discourse Connectives : * * Discourse connectives also need to be separate morphemes . We trained a classifier trained on PDTB to identify connectives since they are ambiguous in English . Datasets analogous to PDTB in other languages e.g.PCC ( German ) and CDTB ( Chinese ) are available . We chose English as the target since we conducted extensive manual analysis and user studies , and it was the language that we had expertise available in ."}, {"review_id": "F9sPTWSKznC-3", "review_text": "This paper presents a dataset , a trained evaluation metric and a leaderboard for evaluating discourse phenomena for machine translation . They test this on a range of discourse level translation models and develop metrics which evaluate the models according to their performance on four discourse phenomena : anaphora , lexical consistency , coherence and readability , and discourse connective translation . Strengths : This paper delivers multiple contributions which could have significant impact on the field of discourse level machine translation . They release data for three language pairs and using their method one could extend it relatively easily into others . I like the thoughtful way that the authors find examples of hard discourse phenomena and each phenomena requires distinct handling . Weaknesses : They rely on previous work to create the Anaphora test set and evaluation model ( Jwalapuram et al . ( 2019 ) ) .They should have explained at a high level how the Jwalapuram evaluation model works and they should have given a general idea of the rules used to filter the anaphora test set : how many rules , an example , would this be possible to do for other languages or does it only work well for English ? I would have liked more discussion about the extensibility of their approach into languages other than English . They should have used sacrebleu or at least specified if the BLEU scores were tokenised and true cased . I think this paper should be accepted because of combined strength of the dataset/metric/leaderboard . I think fine grained evaluation of hard phenomenon is the way forward for improving already very good MT models .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for recognizing the strengths and impact of our contributions . We address some concerns here : * * General rules used to generate the anaphora test set : * * 1 . We aligned the references with various MT system outputs from WMT & IWSLT campaigns . 2.Given a list of pronouns ( a closed set ) , we find the cases where the pronoun translations in the references do not match the aligned , corresponding pronoun translations in the MT system outputs . 3.Jwalapuram et al ( 2019 ) provide a list of pronoun pairs that are not equivalent ( non-interchangeable ) translations of each other . We filter out the cases where the mismatched pronoun translations could be equivalent translations . 4.The corresponding source texts of the remaining wrongly translated cases are added to our test set . * * Generalizability to other languages : * * Our test sets can be generated in other languages , given certain simple conditions are met : 1 . * * Anaphora : * * the pronouns need to be separate morphemes ( and not attached to verbs etc . ) . If there are several equivalent pronoun translations , a list may be needed so they can be excluded from being considered translation errors . E.g.Miculicich Werlen and Popescu-Belis ( 2017 ) has such a list for French ; a list can also be collected through user studies as in Jwalapuram et al ( 2019 ) . 2 . * * Coherence : * * The coherence model ( Moon et al , 2019 ) used to find poorly translated texts was re-trained on reference vs. MT outputs , which is also certainly possible for other languages since WMT system outputs are available . The coherence model from Moon et al ( 2019 ) is an end-to-end neural model that does not rely on any language-specific features , and thus can be trained on any target language . However , language-specific or multilingual coherence models could also be used since Moon et al ( 2019 ) primarily train and test their model on English ( WSJ ) data . 3 . * * Lexical Consistency : * * A lemmatizer was used to reduce common suffixes for detecting lexical consistency ( e.g. \u201c box \u201d and \u201c boxes \u201d should not be detected as inconsistent words ) , so a similar tool will be needed for any other target language . CLTK provides a lemmatizer for several languages . 4 . * * Discourse Connectives : * * Discourse connectives also need to be separate morphemes . We trained a classifier trained on PDTB to identify connectives since they are ambiguous in English . Datasets analogous to PDTB in other languages e.g.PCC ( German ) and CDTB ( Chinese ) are available . * * BLEU scores : * * Our data was tokenized using standard Moses tokenization scripts ( tokenizer.perl , normalize-punctuation.perl , etc . ) for De/Ru/En and Jieba for Zh , with all text lowercased ( De/En ) ; the preprocessing steps are outlined in the Appendix ( A6 ) . The scores we reported were BLEU4 , either computed through fairseq \u2019 s evaluation code or NLTK , which we verified against each other ."}], "0": {"review_id": "F9sPTWSKznC-0", "review_text": "This paper presents four methods for creating benchmarking datasets , each focusing on a particular discourse phenomenon which is difficult/hard to solve with existing context-aware neural machine translation ( NMT ) models . It also evaluates several existing NMT models using the created data and makes a remark about their current status , such as the superiority of one method in one task . This work has the originality and I am happy to know that some researchers devote their efforts to address these issues . Moreover , I am sure that the field of NMT definitely benefits from this work , once the created data are made publicly available . However , I am very skeptical that the contribution of this paper fits for ICLR , since the methodology presented in this paper itself mainly consists of a pile of human efforts . In other words , if I understand the contents properly , only the technical advancement presented in this paper is TgtCon , a variation of the anaphora-centric method proposed by Voita et al . ( 2018b ) .However , it can not be a substantial merit to the community , since it does not necessarily perform better than existing methods and the reason of its deterioration is not analyzed . Considering the main focus of the paper , i.e. , creating benchmarking datasets , the paper should rather fit for a journal article in the field of natural language processing , where the authors can give more details in creating the datasets , such as procedure , tool , and attributes of annotators , not in appendices , and more careful analyses of the results , including WHY each method does ( not ) perform well on a particular discourse phenomenon . Indeed , when I was reading this paper , I suffered from the fact that the main part of the paper is not self-contained . For instance , the proposed TgtCon is not explained in Section 2 but the readers are advised to see an appendix . Other information that is indispensable in data creation but missing in this paper is the detail of human judgment , such as the proficiency of the evaluators , protocol , and judgment criteria . Each section reports on agreement ratio , but not Cohen 's kappa . With these reasons , I am not completely convinced of the quality of the resulted datasets and the portability of the proposed methods . Below shows some questions . Q1.Is there a reason to choose these four particular discourse phenomena ? Are they exclusive to each other ? Q2.Test sets for coherence/readability and discourse connectives are significantly smaller than those for the other two phenomena . I am not sure that a test set with 200 or less examples is enough . Q3.In Section 3.2.1 , the authors regard the inconsistency of named entities . However , in many error classification schemes , terminology errors are , irrespective of incorrect translations and inconsistency , distinguished from coherence errors . For instance , the Multidimensional Quality Metrics ( * 1 ) locate them in different branches . ( * 1 ) http : //www.qt21.eu/mqm-definition/ Q4 . What does `` Random translations '' in Section 3.3.1 mean ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for recognizing the scalability and originality of our work . We address your concerns here . * * Lack of technical advancement/Suitability of Venue : * * We humbly disagree with your view ! The main goal of the paper is to introduce extensive discourse based benchmarks for MT models and to motivate finer-grained evaluations of models across languages . To this end , we introduced several carefully constructed test sets and evaluation measures that were validated against human judgments . Although technical advancement is not the main contribution of this paper , it is the overarching objective of the paper . We aim to push for improvements in NMT models that go beyond merely optimizing word-overlap with reference texts . NMT models are evolving fast ( and are commonly part of ICLR submissions ) but there is a distinct lack of fine-grained testing . It is our contention that frameworks for evaluation of models must be introduced in the same venues as the models themselves , in order to ensure that better evaluation practices are adopted by the community that produces the models . For the same reason , papers on robustness of neural models have been well accepted by the ICLR community ( e.g. , [ 1 ] accepted as an oral ) . More closely related to ours , popular benchmarking frameworks such as GLUE [ 2 ] and benchmark for RL [ 3 ] were also introduced through ICLR . Similarly , the SuperGLUE [ 4 ] and XTREME [ 5 ] benchmarks were introduced in NeurIPS-2019 and ICML-2020 , respectively . In this regard , please allow us to quote a common reviewing oversight referred from the ICLR website ( https : //iclr.cc/Conferences/2021/ReviewerGuide ) : * \u201c \u201c This work does not fit topic X. \u201d \u2013 When reviewers come across a paper that does not fit the typical paper mold for a particular venue , one common response is to dismiss it for this very reason . If you find yourselves writing this in your review , stop and think whether you are being needlessly conservative . Is it possible that this new approach will actually move the field forward ? \u201d * We sincerely hope that our reviewer reconsiders their decision and gives our paper a fair chance . * * Agreements reported : * * As mentioned in the Appendix ( A1 ) , agreements reported in the paper are the AC1 Gamma coefficient proposed by Gwet ( 2008 ) . Due to the nature of the datasets used for the user studies ( essentially erroneous translations vs reference ) , annotators are more likely to choose the reference as the better candidate . This yields a skewed distribution of the annotations ; traditional correlation measures such as Cohen \u2019 s Kappa are not robust to this . Therefore we report the more appropriate Gwet \u2019 s AC1/Gamma coefficient . It is also the agreement reported by Jwalapuram et al. ( 2019 ) . * * Choice of discourse phenomena : * * Yes , they are mostly exclusive to each other ( although coherence has been used as a generic term to mean all the phenomena that contribute to document level understanding ) . In the literature , discourse phenomena have been broadly categorized into two : cohesion and coherence . Anaphora and lexical consistency fall under cohesion , while discourse relations ( e.g. , Contrast , Explanation ) and connectives fall under coherence . These are the four main discourse phenomena ; all other phenomena can be subsumed under one of these . Both Sennrich ( 2018 ) [ 6 ] and Hardmeier ( 2018 ) [ 7 ] list these four as the main discourse phenomena . -- [ 1 ] Synthetic and Natural Noise Both Break Neural Machine Translation . Yonatan Belinkov , Yonatan Bisk . In ICLR 2018 ( Oral presentation ) . [ 2 ] GLUE : A MultiTask Benchmark and Analysis Platform for Natural Language Understanding . Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy and Samuel R. Bowman . In ICLR 2019 . [ 3 ] Behaviour Suite for Reinforcement Learning . Ian Osband , Yotam Doron , Matteo Hessel , John Aslanides Eren Sezener , Andre Saraiva , Katrina McKinney , Tor Lattimore , Csaba Szepesvari Satinder Singh , Benjamin Van Roy , Richard Sutton , David Silver , Hado Van Hasselt . In ICLR 2020 . [ 4 ] SuperGLUE : A Stickier Benchmark for General-Purpose Language Understanding Systems . Alex Wang , Yada Pruksachatkun , Nikita Nangia , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy and Samuel R. Bowman . In NeurIPS 2019 . [ 5 ] XTREME : A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization . J. Hu , Sebastian Ruder , Aditya Siddhant , Graham Neubig , Orhan Firat and M. Johnson . In ICML 2020 . [ 6 ] http : //homepages.inf.ed.ac.uk/rsennric/wnmt2018.pdf [ 7 ] https : //ufal.mff.cuni.cz/mtm18/files/03-discourse-in-mt-christian-hardmeier.pdf"}, "1": {"review_id": "F9sPTWSKznC-1", "review_text": "This paper presents a benchmark for discourse phenomena in machine translation . Its main novelty lies in the relatively large scale , spanning three translation directions , four discourse phenomena , and 150-5000 data points per language and phenomenon . A relatively large number of systems from previous work is benchmarked on each test set , and agreement with human judgments is measured . positives : - clearly , a lot of thought and effort has gone into the creation of the benchmarks , and this is the most diverse set of benchmarks for discourse phenomena in MT so far . - extensive experiments with models from previous work , along with human analysis . negatives : - two of the four benchmarks , anaphora and coherence , are evaluated by neural models trained on WMT outputs , so the interpretation of scores is opaque , and their validity is unclear . Specifically , Jwalapuram et al . ( 2019 ) train a neural network to distinguish references from MT output based on the ELMo representations of pronouns , but in principle , this model can use signals other than the correctness of pronouns translation to make this distinction . Similarly , the model by Moon et al . ( 2019 ) was originally trained to distinguish real documents from randomly shuffled ones , and I can see how their complex neural network would then learn to rely on coherence features . However , this submission uses reference translations as positive , MT output as negative examples , so it again may learn to use features other than coherence for its decisions . - also , I 'm not fully convinced about the validity of the automatic discourse connective evaluation . According to the manual analysis , there is a large proportion of false negatives ( synonymous translations flagged as errors ) , and rankings would change if synonyms were counted as correct . I was also not satisfied with the evidence that the omission of connectives is generally an error . The human study was a bit simplistic in that it just deleted connectives ( although more changes might be needed to ensure grammaticality ) or used noisy MT output rather than alternative human references . It also seems to have been monolingual . If the test set consists of examples with ambiguous or implicit discourse relations in the source , then it may actually be the right translation strategy to omit them . I 'm worried that a benchmark that rewards explicitation and punishes leaving discourse relations implicit may set the wrong incentives . - I was surprised by the low results ( already in terms of BLEU ) of some of the tested variants . Authors describe in great care their efforts to fairly reproduce previous work , including the use of original code and hyperparameters where possible , but I ca n't help but think that the models are suboptimally trained , and that statements about whether context-aware models consistently improve discourse phenomena are tainted by this . recommendation : I 'm leaning negative on the current version of the paper and benchmark . I think the test sets have been carefully assembled , and along with the various types of models evaluated on them , this work has value . But before I 'd recommend that the benchmarks actually be used in the field , authors would need to improve upon the evaluation scores used for anaphora , coherence , and discourse connectives and make sure they really are targeted towards the phenomena they claim to measure , and do not have large blind spots . further questions and minor problems : - did you perform early stopping ? What was your stopping criterion ? - do you have an explanation why anaphora scores differ wildly between systems for ZH-EN ( table 9 ) ? - the discourse connectives test sets are based on examples where the reference contains a connective , but MT output does not . What MT system was used ? Do the respective source segments generally contain explicit discourse connectives , or are the respective discourse relations generally implicit in the source ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for noting the effort and value of our work . We address your concerns here : * * Validity of Model Evaluation : * * 1 . Anaphora ( Jwalapuram et al. , 2019 ) : Although the model is trained on reference vs. MT output , their paper shows results for testing on two settings : * Reference vs. MT output , where the model is 90.69 % accurate , * Reference vs. noisy reference , where everything in the noisy version is identical except for the pronoun ; here the accuracy is 89.11 % . They also report high agreements ( > =0.8 ) with human judgments on similar noisy data . This convinced us that the model focuses on pronouns during evaluation . 2.Coherence ( Moon et al. , 2019 ) : Coherence is complex , and can include grammaticality , topic structures and inter-sentential relations , which the Moon et al . ( 2019 ) model claims to capture . We validated the model 's ability to distinguish coherence ( and readability ) by conducting a user study , where we compared the model 's ranking of texts against human rankings of texts and found strong agreements between the two ( > 0.8 ) . Since coherence is a subjective concept , there is no other way to validate the effectiveness of the model other than by comparing to human judgments , which we therefore do . We hope the reviewer can understand the challenges and appreciate our sincere effort to address them , especially considering the fact that currently there is no such established benchmark to measure progress . As mentioned in our response to AnonReviewer1 , there has been a recent trend towards model-based evaluation metrics like MoverScore ( Zhao et al , EMNLP-2019 ) , BERTScore ( Zhang et al , ICLR-2020 ) , BLEURT ( Sellam et al , ACL-2020 ) , etc . * * Validity of Discourse Connectives test set : * * * * Evaluation : * * Discourse connectives are complex , and can be quite hard to evaluate . It can be hard to perfectly define synonyms and distil them into a list , as they can convey different discourse relations depending on the context . In light of the fact that evaluating erroneous translations under such circumstances would be extremely difficult , we chose to instead concentrate on cases where MT systems omit the connective , since omissions are easier to detect and evaluate . We agree that synonyms should not be penalized ; this is why we included the percentages for when ANY candidate connective was produced by the MT output in order to give them the benefit of the doubt . * * User studies : * * Our decision to conduct two user studies for the connective test set validation stemmed from our discussion with the creators of the PDTB corpus . Note that both the studies provide context information to the users . 1.The first study uses the reference with the connective deleted , compared to the reference with the connective . This study serves to prove that connectives serve a purpose and omitting them is problematic . The high agreement ( > 0.9 ) results of this study preferring the reference with the connective indicate that connectives are necessary for grammaticality or for discourse relations to hold . 2.The second study uses the MT output with the missing connective , compared to the reference with the connective . This study serves to prove that the MT outputs are indeed making omission errors . If the MT outputs with missing connectives were structured in such a way as to have implicit discourse relations , the agreements that favoured the references should be significantly lower ; however we find strong ( > 0.8 ) agreements that favour the reference with the connective . In general , manual examination has shown that the source sentences do contain an explicit connective , and that reference translations very rarely introduce extra words that do not occur in the source ( we also detail a similar examination conducted for lexical consistency in the appendix ) . In summary , as the first of its kind benchmark for discourse level phenomena in MT , we left no stone unturned to make the evaluation as trustworthy as possible . * * MT systems used for testset generation : * * As mentioned in Section 3 , all testsets are based on multiple MT system outputs submitted to WMT , ensuring that there is no bias against particular types of models . For De-En , Ru-En and Zh-En , these consist of translation outputs from 68 , 41 and 47 unique systems respectively ."}, "2": {"review_id": "F9sPTWSKznC-2", "review_text": "In this paper , the authors propose specific test sets for document-level NMT . They target anaphora , coherence/readability , lexical consistency and discourse connectives , and are available for multiple language pairs . The first two challenge sets rely on model-based evaluation . Strengths : The test sets , which target various discourse phenomena , directly evaluate the output of the models ( contrarily to some existing multiple-choice challenge sets ) . The types of mistakes made by NMT models are manually examined . The authors validate the quality of their metrics by comparing against human judgements ( although the number of samples is arguably small ) . Weaknesses : The evaluated NMT models all date from 2018 or earlier . The anaphora challenge sets are only a minor update over previous work . All language pairs use English as the target language . Other remarks and questions : For the anaphora and coherence/readability test sets , future work may `` cheat '' by using the evaluation models as part of the NMT systems . Is normalizing the scores actually useful ? The reference scores should be the same across all systems , so it only shifts all results by a constant and does n't affect relative performance . Whats steps would be needed to construct similar test sets for En- > X language pairs ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for recognizing our contributions . We address some of the questions and concerns here : * * All models from 2018 or earlier : * * Context-aware models are not as common as sentence-level models although many MT researchers have advocated for it . We chose some well-established , popularly cited work since we thought they would be representative . We aimed at testing original author codes for maximum credibility , so we chose papers with implementations . We also picked some older models due to their properties ( e.g.inclusion of target-side context ) . * * Future work may \u201c cheat \u201d : * * We agree that this is possible , but this would be true of any model-based evaluation . We can not let this preclude us from using all model-based evaluations , since that would mean we would be limited to simple automatic metrics like BLEU that can not produce finer-grained or nuanced evaluations ( moreover , BLEU has also been optimized ) . Discourse phenomena , especially coherence , are complex concepts that can not be captured by simple metrics , and require model-based evaluation . There has also been a recent trend towards model-based evaluation metrics like MoverScore ( Zhao et al , EMNLP-2019 ) , BERTScore ( Zhang et al , ICLR-2020 ) , BLEURT ( Sellam et al , ACL-2020 ) , etc . to name a few . We also think it would be non-trivial to use the evaluation models in NMT systems while balancing the translation objective . Moreover , for anaphora , the test set is independent of the evaluation model , so the error distributions are likely to be different . * * Normalization : * * The evaluation models are both pairwise ranking models , requiring reference and candidate translations as input together , and are trained to score the reference higher . Conceptually , it is not the absolute scores but the relative differences between pairs that are meaningful . Also , inputs with the same reference sometimes get slightly different scores due to some internal randomness stemming from ELMo embeddings . We added the normalization to account for these two issues ; we will add this explanation to the paper . * * Generalizability to other languages : * * Our test sets can be generated in other languages ( reproducing the answer above to AnonReviewer2 here for your convenience ) : 1 . * * Anaphora : * * the pronouns need to be separate morphemes ( and not attached to verbs etc . ) . If there are several equivalent pronoun translations , a list may be needed so they can be excluded from being considered translation errors . E.g.Miculicich Werlen and Popescu-Belis ( 2017 ) has such a list for French ; a list can also be collected through user studies as in Jwalapuram et al ( 2019 ) . 2 . * * Coherence : * * The coherence model ( Moon et al , 2019 ) used to find poorly translated texts was re-trained on reference vs. MT outputs , which is also certainly possible for other languages since WMT system outputs are available . The coherence model from Moon et al ( 2019 ) is an end-to-end neural model that does not rely on any language-specific features , and thus can be trained on any target language . However , language-specific or multilingual coherence models could also be used since Moon et al ( 2019 ) primarily train and test their model on English ( WSJ ) data . 3 . * * Lexical Consistency * * : A lemmatizer was used to reduce common suffixes for detecting lexical consistency ( e.g. \u201c box \u201d and \u201c boxes \u201d should not be detected as inconsistent words ) , so a similar tool will be needed for any other target language . CLTK provides a lemmatizer for several languages . 4 . * * Discourse Connectives : * * Discourse connectives also need to be separate morphemes . We trained a classifier trained on PDTB to identify connectives since they are ambiguous in English . Datasets analogous to PDTB in other languages e.g.PCC ( German ) and CDTB ( Chinese ) are available . We chose English as the target since we conducted extensive manual analysis and user studies , and it was the language that we had expertise available in ."}, "3": {"review_id": "F9sPTWSKznC-3", "review_text": "This paper presents a dataset , a trained evaluation metric and a leaderboard for evaluating discourse phenomena for machine translation . They test this on a range of discourse level translation models and develop metrics which evaluate the models according to their performance on four discourse phenomena : anaphora , lexical consistency , coherence and readability , and discourse connective translation . Strengths : This paper delivers multiple contributions which could have significant impact on the field of discourse level machine translation . They release data for three language pairs and using their method one could extend it relatively easily into others . I like the thoughtful way that the authors find examples of hard discourse phenomena and each phenomena requires distinct handling . Weaknesses : They rely on previous work to create the Anaphora test set and evaluation model ( Jwalapuram et al . ( 2019 ) ) .They should have explained at a high level how the Jwalapuram evaluation model works and they should have given a general idea of the rules used to filter the anaphora test set : how many rules , an example , would this be possible to do for other languages or does it only work well for English ? I would have liked more discussion about the extensibility of their approach into languages other than English . They should have used sacrebleu or at least specified if the BLEU scores were tokenised and true cased . I think this paper should be accepted because of combined strength of the dataset/metric/leaderboard . I think fine grained evaluation of hard phenomenon is the way forward for improving already very good MT models .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for recognizing the strengths and impact of our contributions . We address some concerns here : * * General rules used to generate the anaphora test set : * * 1 . We aligned the references with various MT system outputs from WMT & IWSLT campaigns . 2.Given a list of pronouns ( a closed set ) , we find the cases where the pronoun translations in the references do not match the aligned , corresponding pronoun translations in the MT system outputs . 3.Jwalapuram et al ( 2019 ) provide a list of pronoun pairs that are not equivalent ( non-interchangeable ) translations of each other . We filter out the cases where the mismatched pronoun translations could be equivalent translations . 4.The corresponding source texts of the remaining wrongly translated cases are added to our test set . * * Generalizability to other languages : * * Our test sets can be generated in other languages , given certain simple conditions are met : 1 . * * Anaphora : * * the pronouns need to be separate morphemes ( and not attached to verbs etc . ) . If there are several equivalent pronoun translations , a list may be needed so they can be excluded from being considered translation errors . E.g.Miculicich Werlen and Popescu-Belis ( 2017 ) has such a list for French ; a list can also be collected through user studies as in Jwalapuram et al ( 2019 ) . 2 . * * Coherence : * * The coherence model ( Moon et al , 2019 ) used to find poorly translated texts was re-trained on reference vs. MT outputs , which is also certainly possible for other languages since WMT system outputs are available . The coherence model from Moon et al ( 2019 ) is an end-to-end neural model that does not rely on any language-specific features , and thus can be trained on any target language . However , language-specific or multilingual coherence models could also be used since Moon et al ( 2019 ) primarily train and test their model on English ( WSJ ) data . 3 . * * Lexical Consistency : * * A lemmatizer was used to reduce common suffixes for detecting lexical consistency ( e.g. \u201c box \u201d and \u201c boxes \u201d should not be detected as inconsistent words ) , so a similar tool will be needed for any other target language . CLTK provides a lemmatizer for several languages . 4 . * * Discourse Connectives : * * Discourse connectives also need to be separate morphemes . We trained a classifier trained on PDTB to identify connectives since they are ambiguous in English . Datasets analogous to PDTB in other languages e.g.PCC ( German ) and CDTB ( Chinese ) are available . * * BLEU scores : * * Our data was tokenized using standard Moses tokenization scripts ( tokenizer.perl , normalize-punctuation.perl , etc . ) for De/Ru/En and Jieba for Zh , with all text lowercased ( De/En ) ; the preprocessing steps are outlined in the Appendix ( A6 ) . The scores we reported were BLEU4 , either computed through fairseq \u2019 s evaluation code or NLTK , which we verified against each other ."}}