{"year": "2019", "forum": "S1GkToR5tm", "title": "Discriminator Rejection Sampling", "decision": "Accept (Poster)", "meta_review": "The paper proposes a discriminator dependent rejection sampling scheme for improving the quality of samples from a trained GAN. The paper is clearly written, presents an interesting idea and the authors extended and improved the experimental analyses as suggested by the reviewers.", "reviews": [{"review_id": "S1GkToR5tm-0", "review_text": "This paper proposes a rejection sampling algorithm for sampling from the GAN generator. Authors establish a very clear connection between the optimal GAN discriminator and the rejection sampling acceptance probability. Then they explain very clearly that in practice the connection is not exact, and propose a practical algorithm. Experimental results suggest that the proposed algorithm helps the increase the accuracy of the generator, measured in terms of inception score and Frechet inception distance. It would be interesting though to see if the proposed algorithm buys anything over a trivial rejection scheme such as looking at the discriminator values and rejecting the samples if they fall below a certain threshold. This being said, I do understand that the proposed practical acceptance ratio in equation (8) is 'close' to the theoretically justified acceptance ratio. Since in practice the learnt discriminator is not exactly the ideal discriminator D*(x), I think it is super okay to add a constant and optimize it on a validation set. (Equation (7) is off anyways since in practice the things (e.g. the discriminator) are not ideal). But again, I do think it would make the paper much stronger to compare equation (8) with some other heuristic based rejection schemes. ", "rating": "7: Good paper, accept", "reply_text": "Thanks very much for the review , please see this comment : https : //openreview.net/forum ? id=S1GkToR5tm & noteId=SyxH1nd7R7 for some ablation experiments and comparisons with heuristic rejection schemes . Let us know if there 's anything else you think we can do to improve the work ."}, {"review_id": "S1GkToR5tm-1", "review_text": "his paper assumes that, in a GAN, the generator is not perfect and some information is left in the discriminator, so that it can be used to 'reject' some of the 'fake' examples produced by the generator. The introduction, problem statement and justification for rejection sampling are excellent, with a level of clarity that makes it understandable by non expert readers, and a wittiness that makes the paper fun to read. I assume this work is novel: the reviewer is more an expert in rejection than in GANs, and is aware how few publications rely on rejection. However, the authors fail to compare their algorithm to a much simpler rejection scheme, and a revised version should discuss this issue. Let's jump to equation (8): compared to a simple use of the dicriminator for rejection, it adds the term under the log. The basic rejection equation would read F(x) = D*(x) - gamma and one would adjust the threshold gamma to obtain the desired operating point. I am wondering why no comparison is provided with basic rejection? Let me try to understand the Gaussian mixture experiment, as the description is ambiguous: - GAN setting: 10K examples are generated and reported in figure 3? - DRS setting: 10K examples are generated, and submitted to algorithm in figure 1. For each batch, a line search sets gamma so that 95% of the examples are accepted. Thus only 9.5K are reported in figure 3. - What about basic rejection using F(x) = D*(x) - gamma: how does it compare to DRS at the same 95% accept? If this is my understanding, then the comparison in Figure 3 in unfair, as DRS is allowed to pick and choose. For completeness, basic rejection should also be added. Going back to Eq.(8), one realizes that the difference between DRS rejection and basic rejection may be negligible. First order Taylor expansion of log(1-x) that would apply to the case where the rejection probability is small yields: F(x) = (D*(x) - D*_M) + exp(D*(x) - D*_M) x+ exp(x) is monotonous, so thresholding over it is the same as thresholding over x: back to basic rejection!", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks very much for the review . We think that there have been two misunderstandings here , one about the Gaussian Mixture experiment and one about the purpose of the quantity F_hat ( x ) . These are our fault ; we should have made the paper more clear and we are modifying the draft to do so . In the meantime , we will address both issues here . We use > for quotes . GAUSSIAN MIXTURE EXPERIMENT : > - GAN setting : 10K examples are generated and reported in figure 3 ? This much is true . > - DRS setting : 10K examples are generated , and submitted to algorithm in figure 1 . For each batch , a line search sets gamma so that 95 % of the examples are accepted . Thus only 9.5K are reported in figure 3 . This part is not true . You probably got confused by the line 'We generate 10,000 samples from the generator with and without DRS . ' which we agree is unclear . First , we generate as many samples as needed to yield 10K acceptances , so both plots have 10k dots on them . Second , there is no line search . Each example is given an acceptance probability p that is generated from substituting F_hat from equation 8 for F in equation 6 . Then , a pseudo-random number in [ 0,1 ] is compared with p to determine acceptance . Thus , for any given batch , the number of examples accepted is non-deterministic . We think that this point also relates to the misunderstanding regarding the purpose of F_hat . Third , gamma is subtracted from F. So setting gamma equal to the 95th % -ile value of F means that an example where F ( x ) is at the 95th % -ile will have a 50 % chance of being accepted , because 1 / ( 1 + e^ ( -F_hat ( x ) ) ) = 1 / ( 1 + e^0 ) = 1 / 2 in this case . The result is that around 23 % of samples drawn from the generator made it into the final DRS plot , which means we had to draw a little less than 50k samples from the generator . > If this is my understanding , then the comparison in Figure 3 in unfair , as DRS is allowed to pick and choose . We 're unsure what you mean here . It 's true in some sense that DRS is allowed to pick and choose , but from our perspective this is part of the definition of rejection sampling ? The generator ca n't figure out how to stop yielding bad samples , but the discriminator can tell which samples are bad , so we can throw those out and get a distribution closer to the ground truth distribution at the cost of having to generate extra samples from the generator . PURPOSE OF F_HAT : > Let 's jump to equation ( 8 ) : compared to a simple use of the discriminator for rejection , it adds the term under the log We do n't think this is correct - the log already exists and we just add the gamma and epsilon terms . The discussion after eq 5 shows that the acceptance probability p ( x ) is exp ( D_tilde^ * ( x ) - D_tilde^ * ( x^ * ) ) . The tildes are important , because they mean that we are operating not on the sigmoid output of D but on the logit that is passed to the sigmoid output . Then we ask what F ( x ) would have to be s.t . 1 / ( 1 + e^ ( -F ( x ) ) ) = p ( x ) . This results in equation 7 , * which already has the log term * . The only difference between F_hat and F is that we introduce the epsilon for numerical stability and the gamma to modulate the acceptance probability . > First order Taylor expansion of ... What you say here is true , but we are not thresholding . We think this is the root of the misunderstanding . We do n't consider the hard thresholding algorithm here because it might deterministically reject certain samples for which D^ * is low , which means that we would never be able to actually draw samples from p_d , even in the idealized setting of section 3.1 Please let us know if this response answers all of your questions . We are happy to expand ."}, {"review_id": "S1GkToR5tm-2", "review_text": "This paper proposed a post-processing rejection sampling scheme for GANs, named Discriminator Rejection Sampling (DRS), to help filter \u2018good\u2019 samples from GANs\u2019 generator. More specifically, after training GANs\u2019 generator and discriminator are fixed; GANs\u2019 discriminator is further exploited to design a rejection sampler, which is used to reject the \u2018bad\u2019 samples generated from the fixed generator; accordingly, the accepted generated samples have good quality (better IS and FID results). Experiments of SAGAN model on GMM toys and ImageNet dataset show that DRS helps further increases the IS and reduces the FID. The paper is easy to follow, and the experimental results are convincing. However, I am curious about the follow questions. (1) Besides helping generate better samples, could you list several other applications where the proposed technique is useful? (2) In the last paragraph of Page 4, I don\u2019t think the presented Discriminator Rejection Sampling \u201caddresses\u201d the issues in Sec 3.2, especially the first paragraph of Page 5. (3) The hyperparameter gamma in Eq. (8) is of vital importance for the proposed DRS. Actually, it is believed the key to determining whether DRS works or not. Detailed analysis/experiments about hyperparameter gamma are considered missing. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for his/her time and feedback . We appreciate the kind words relating to the clarity and comprehensiveness of our submission , and hope to address any remaining concerns the reviewer has here . OTHER APPLICATIONS : ( a ) Suppose we \u2019 re designing molecules for drug discovery purposes using a generative model . At some point , we will have to physically test the molecules that we have designed , which could be costly . If the discriminator can throw out some obviously unrealistic molecule designs , this will save us money and time . ( b ) For text generation applications , a nonsensical generated sentence in a dialog system could be rejected by the discriminator , reducing the frequency of embarrassing mistakes . ( c ) In RL applications , if we are predicting future states with a generative model , we could use this technique to throw out silly predictions , reducing the risk of taking a silly action predicated on those predictions . ( d ) More generally , you could use DRS on models that are not GANs . ADDRESSING D * ISSUE You \u2019 re right about this - we will change the wording . We don \u2019 t do anything to * fix * the problem that we can \u2019 t actually compute D * , we just show that you don \u2019 t need to precisely recover D * to get good results . The first paragraph on page 5 speculates on why this might be so , and figures 4 and 5 provide evidence for this speculation . REGARDING GAMMA : We agree that gamma is an important hyperparameter , because it modulates the acceptance rate . We have already made the figure you propose and have updated the PDF to include it . It is now figure 6 . Please let us know if there are other experiments that you think would improve the quality of the work ."}], "0": {"review_id": "S1GkToR5tm-0", "review_text": "This paper proposes a rejection sampling algorithm for sampling from the GAN generator. Authors establish a very clear connection between the optimal GAN discriminator and the rejection sampling acceptance probability. Then they explain very clearly that in practice the connection is not exact, and propose a practical algorithm. Experimental results suggest that the proposed algorithm helps the increase the accuracy of the generator, measured in terms of inception score and Frechet inception distance. It would be interesting though to see if the proposed algorithm buys anything over a trivial rejection scheme such as looking at the discriminator values and rejecting the samples if they fall below a certain threshold. This being said, I do understand that the proposed practical acceptance ratio in equation (8) is 'close' to the theoretically justified acceptance ratio. Since in practice the learnt discriminator is not exactly the ideal discriminator D*(x), I think it is super okay to add a constant and optimize it on a validation set. (Equation (7) is off anyways since in practice the things (e.g. the discriminator) are not ideal). But again, I do think it would make the paper much stronger to compare equation (8) with some other heuristic based rejection schemes. ", "rating": "7: Good paper, accept", "reply_text": "Thanks very much for the review , please see this comment : https : //openreview.net/forum ? id=S1GkToR5tm & noteId=SyxH1nd7R7 for some ablation experiments and comparisons with heuristic rejection schemes . Let us know if there 's anything else you think we can do to improve the work ."}, "1": {"review_id": "S1GkToR5tm-1", "review_text": "his paper assumes that, in a GAN, the generator is not perfect and some information is left in the discriminator, so that it can be used to 'reject' some of the 'fake' examples produced by the generator. The introduction, problem statement and justification for rejection sampling are excellent, with a level of clarity that makes it understandable by non expert readers, and a wittiness that makes the paper fun to read. I assume this work is novel: the reviewer is more an expert in rejection than in GANs, and is aware how few publications rely on rejection. However, the authors fail to compare their algorithm to a much simpler rejection scheme, and a revised version should discuss this issue. Let's jump to equation (8): compared to a simple use of the dicriminator for rejection, it adds the term under the log. The basic rejection equation would read F(x) = D*(x) - gamma and one would adjust the threshold gamma to obtain the desired operating point. I am wondering why no comparison is provided with basic rejection? Let me try to understand the Gaussian mixture experiment, as the description is ambiguous: - GAN setting: 10K examples are generated and reported in figure 3? - DRS setting: 10K examples are generated, and submitted to algorithm in figure 1. For each batch, a line search sets gamma so that 95% of the examples are accepted. Thus only 9.5K are reported in figure 3. - What about basic rejection using F(x) = D*(x) - gamma: how does it compare to DRS at the same 95% accept? If this is my understanding, then the comparison in Figure 3 in unfair, as DRS is allowed to pick and choose. For completeness, basic rejection should also be added. Going back to Eq.(8), one realizes that the difference between DRS rejection and basic rejection may be negligible. First order Taylor expansion of log(1-x) that would apply to the case where the rejection probability is small yields: F(x) = (D*(x) - D*_M) + exp(D*(x) - D*_M) x+ exp(x) is monotonous, so thresholding over it is the same as thresholding over x: back to basic rejection!", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks very much for the review . We think that there have been two misunderstandings here , one about the Gaussian Mixture experiment and one about the purpose of the quantity F_hat ( x ) . These are our fault ; we should have made the paper more clear and we are modifying the draft to do so . In the meantime , we will address both issues here . We use > for quotes . GAUSSIAN MIXTURE EXPERIMENT : > - GAN setting : 10K examples are generated and reported in figure 3 ? This much is true . > - DRS setting : 10K examples are generated , and submitted to algorithm in figure 1 . For each batch , a line search sets gamma so that 95 % of the examples are accepted . Thus only 9.5K are reported in figure 3 . This part is not true . You probably got confused by the line 'We generate 10,000 samples from the generator with and without DRS . ' which we agree is unclear . First , we generate as many samples as needed to yield 10K acceptances , so both plots have 10k dots on them . Second , there is no line search . Each example is given an acceptance probability p that is generated from substituting F_hat from equation 8 for F in equation 6 . Then , a pseudo-random number in [ 0,1 ] is compared with p to determine acceptance . Thus , for any given batch , the number of examples accepted is non-deterministic . We think that this point also relates to the misunderstanding regarding the purpose of F_hat . Third , gamma is subtracted from F. So setting gamma equal to the 95th % -ile value of F means that an example where F ( x ) is at the 95th % -ile will have a 50 % chance of being accepted , because 1 / ( 1 + e^ ( -F_hat ( x ) ) ) = 1 / ( 1 + e^0 ) = 1 / 2 in this case . The result is that around 23 % of samples drawn from the generator made it into the final DRS plot , which means we had to draw a little less than 50k samples from the generator . > If this is my understanding , then the comparison in Figure 3 in unfair , as DRS is allowed to pick and choose . We 're unsure what you mean here . It 's true in some sense that DRS is allowed to pick and choose , but from our perspective this is part of the definition of rejection sampling ? The generator ca n't figure out how to stop yielding bad samples , but the discriminator can tell which samples are bad , so we can throw those out and get a distribution closer to the ground truth distribution at the cost of having to generate extra samples from the generator . PURPOSE OF F_HAT : > Let 's jump to equation ( 8 ) : compared to a simple use of the discriminator for rejection , it adds the term under the log We do n't think this is correct - the log already exists and we just add the gamma and epsilon terms . The discussion after eq 5 shows that the acceptance probability p ( x ) is exp ( D_tilde^ * ( x ) - D_tilde^ * ( x^ * ) ) . The tildes are important , because they mean that we are operating not on the sigmoid output of D but on the logit that is passed to the sigmoid output . Then we ask what F ( x ) would have to be s.t . 1 / ( 1 + e^ ( -F ( x ) ) ) = p ( x ) . This results in equation 7 , * which already has the log term * . The only difference between F_hat and F is that we introduce the epsilon for numerical stability and the gamma to modulate the acceptance probability . > First order Taylor expansion of ... What you say here is true , but we are not thresholding . We think this is the root of the misunderstanding . We do n't consider the hard thresholding algorithm here because it might deterministically reject certain samples for which D^ * is low , which means that we would never be able to actually draw samples from p_d , even in the idealized setting of section 3.1 Please let us know if this response answers all of your questions . We are happy to expand ."}, "2": {"review_id": "S1GkToR5tm-2", "review_text": "This paper proposed a post-processing rejection sampling scheme for GANs, named Discriminator Rejection Sampling (DRS), to help filter \u2018good\u2019 samples from GANs\u2019 generator. More specifically, after training GANs\u2019 generator and discriminator are fixed; GANs\u2019 discriminator is further exploited to design a rejection sampler, which is used to reject the \u2018bad\u2019 samples generated from the fixed generator; accordingly, the accepted generated samples have good quality (better IS and FID results). Experiments of SAGAN model on GMM toys and ImageNet dataset show that DRS helps further increases the IS and reduces the FID. The paper is easy to follow, and the experimental results are convincing. However, I am curious about the follow questions. (1) Besides helping generate better samples, could you list several other applications where the proposed technique is useful? (2) In the last paragraph of Page 4, I don\u2019t think the presented Discriminator Rejection Sampling \u201caddresses\u201d the issues in Sec 3.2, especially the first paragraph of Page 5. (3) The hyperparameter gamma in Eq. (8) is of vital importance for the proposed DRS. Actually, it is believed the key to determining whether DRS works or not. Detailed analysis/experiments about hyperparameter gamma are considered missing. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for his/her time and feedback . We appreciate the kind words relating to the clarity and comprehensiveness of our submission , and hope to address any remaining concerns the reviewer has here . OTHER APPLICATIONS : ( a ) Suppose we \u2019 re designing molecules for drug discovery purposes using a generative model . At some point , we will have to physically test the molecules that we have designed , which could be costly . If the discriminator can throw out some obviously unrealistic molecule designs , this will save us money and time . ( b ) For text generation applications , a nonsensical generated sentence in a dialog system could be rejected by the discriminator , reducing the frequency of embarrassing mistakes . ( c ) In RL applications , if we are predicting future states with a generative model , we could use this technique to throw out silly predictions , reducing the risk of taking a silly action predicated on those predictions . ( d ) More generally , you could use DRS on models that are not GANs . ADDRESSING D * ISSUE You \u2019 re right about this - we will change the wording . We don \u2019 t do anything to * fix * the problem that we can \u2019 t actually compute D * , we just show that you don \u2019 t need to precisely recover D * to get good results . The first paragraph on page 5 speculates on why this might be so , and figures 4 and 5 provide evidence for this speculation . REGARDING GAMMA : We agree that gamma is an important hyperparameter , because it modulates the acceptance rate . We have already made the figure you propose and have updated the PDF to include it . It is now figure 6 . Please let us know if there are other experiments that you think would improve the quality of the work ."}}