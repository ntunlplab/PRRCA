{"year": "2019", "forum": "rkgsvoA9K7", "title": "Dirichlet Variational Autoencoder", "decision": "Reject", "meta_review": "This paper applies Dirichlet distribution to the latent variables of a VAE in order to address the component collapsing issues for categorical probabilities. The method is clearly presented, and extensive experiments are carried out to prove the advantage against VAEs with other prior distributions.\n\nThe main concern of the paper is the limited novelty. The main methodology contribution of this paper is to combine the decomposition a Dirichlet distribution as Gamma distributions, and approximating Gamma component with inverse Gamma CDF, but both components are common practices. \n\nR3 also points out that the paper is distracted by two different messages the authors try to convey. The presentation and experiments are not designed to provide a cohesive message. The concern is not solved in the authors' feedback.\n\nBased on the current reviews, this paper does not meet the standard for ICLR publication. Despite the limited novelty in the proposed model, if the paper could be revised to show that a simple modification is good for solve one problem with general applications, it would make a good publication in a future venue.", "reviews": [{"review_id": "rkgsvoA9K7-0", "review_text": "In this paper, authors proposes an algorithm to use Dirichlet prior on the variational auto-encoder (VAE). They used this prior as natural conjugate to likelihood distributtion of multinomial (categorical). The paper proposes a way to use scalability power of VAE for data distributed by categorical distribution. In order to apply reparametrization trick, authors have used iid Gamma random variable to construct draw from Dirichlet distribution and have used approximation with inverse gamma CDF, it is discussed how this method has better performance than other approximations method for gamma distribution such as Weibull and logistic Gaussian. Authors pointed out, one of the weak points in competing models such as Guassian softmax prior or Griffith -Engen-McCloskey prior which has been used for Stick breaking VAE is to not encouraging of having multi-modal posteriori, while this prior empower having multi-modal posteriori distribution which give them advantage over previous papers. In experimental results, paper has used different datasets of MNIST, MNIST+rotation , OMNIGLOT , 20newsgroup and RCVI and used different measures to compare the existing method with the baselines. To summarize the contribution of this paper, following three points can be named as main contribution of this paper: - proposed a Dirichlet prior, for categorical likelihood which encourages having multi-modal posteriori. paper demonstrates couple of techniques to apply the reparametrization trick on Dirichlet distribution, by using sum of iid Gamma random variables. - used method of moments estimator to update the hyper parameter of the Dirichlet distribution which helps to have closer approximation of log likelihood. They update hyper-parameters after every few updates of VAE parameters. -discussed how to overcome Stick-breaking VAE \u201ccomponent collapse\u201d issue. Experiments show superior results on supervised and semi supervised, and authors claimed the main reason of this superiority being due to not having disadvantage of component collapse which happens in SBVAE. Quality and Novelty: claims in paper are supported by proofs and/or experimental results and there does not exist significant technical issues with the details of claims made in this paper and proofs provided. There are following issues with novelty and quality of paper that I would like discuss them under following three points: - Authors need to be clear about the motivation of the paper, if the motivation of the paper is to encourage the multi-modality in posteriori distribution, using Gaussian prior and methods like normalizing flow Rezende, Danilo Jimenez, and Shakir Mohamed. \"Variational inference with normalizing flows.\" arXiv preprint arXiv:1505.05770 (2015) or similar may be able to do the same work in which case paper should compare its results to those ideas which has not been done in this paper. - second appealing point that this paper can make is to use Dirichlet prior for the purposes like community detection, topic modeling and LDA etc etc. In this case, I did not find significant difference between the proposed method and what is found in Srivastava, Akash, and Charles Sutton. \"Autoencoding variational inference for topic models.\" arXiv preprint arXiv:1703.01488 (2017), but due to the encourages of multi-modality authors show in average DirVAE performs better in measures like perplexity and NPMI. Under this condition, my main concern is interpretablity of posteriori. That will be discussed under next point - Main motivation behind using Dirichlet prior, is to have posteriori with a few significant related topic and many unrelated topic for every word. By changing the concentration parameter in stick-breaking, it is possible that performance of stick-breaking method increase in perplexity and NPMI scores in cost of loosing interpretability of the model. So having higher concentration parameter can show better performance in the cost of interpretablity that put second point of the paper at risk Clarity: The paper is well written and previous relevant methods have been reviewed well. The organization of paper is good, experiments well explained and proofs and mathematical reasoning are clear. Significance of experiments: As discussed,in previous sections, the results show superior performance and compared to other methods on semi-supervised and supervised classification on different datasets. Also it has shown in average better perplexity and NPMI score for topic modeling, the only issue can be these scores come as cost of interpretablity of the model. Also it is possible that other competing models can be matching to this results if they do not aim for sparse posteriori.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . Currently , we are doing additional experiments to respond to your constructive comments . Sorry for the delay , but we will give you proper responds to your review with results as soon as possible . Up to the current status of experiments , the VAE with the normalizing flow still suffers from the decoder weight collapsing problem . Hence , the performance would not match to our approach , but we are going to make a certain on this premature result with the experiments , next few days . Sincerely ."}, {"review_id": "rkgsvoA9K7-1", "review_text": "This paper proposes DirVAE, a variational autoencoder with Dirichlet prior on latent variables. The advantage of using Dirichlet distribution is that due the nature of Dirichlet distribution the model does not suffer from decoder weight collapsing and latent value collapsing. Stochastic gradient variational Bayes with inverse CDF reparametrization of gamma distribution is presented. The motivation behind using Dirichlet instead of GEM makes sense, but other than that I fail to find any novelty in the paper. The authors should tone down the statement \"to our knowledge, combining the two statistical results is the first finding in the machine learning field\". Even though left unpublished, I've been using this combination of inverse CDF gamma reparametrization and transformation to Dirichlet all the time for my own problems. It's just trivial once we have both techniques. See also [2], where an improved way of reparametrizing gamma and Dirichlet distribution is presented. The observation that DirVAE does not suffer from latent value collapsing is interesting, but not really surprising. Minor question - What is the difference between negative LL's and reconstruction losses in experiments? - The approximation for inverse CDF of gamma works well only when alpha << 1. How did you treat the regime alpha > 1? References [1] Diederik P Kingma, Max Welling, Auto-encoding variational Bayes, ICLR, 2014. [2] Michael Figurnov, Shakir Mohamed, Andriy Mnih, Implicit reparametrization gradients, arXiv, 2018.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . Firstly , we would like to say thank you for introducing the paper [ 2 ] . Even though the paper [ 2 ] and our paper lie on the same path in terms of reparametrizing Gamma distribution , paper [ 2 ] deals with a general reparametrization trick on various probabilistic distributions while our paper focuses on the advantages and the applicabilities of Dirichlet prior in VAE . We believe that our contribution is not based on reparametrizing Gamma or Dirichlet distiribution , but introducing Dirichlet prior , which is a conjugate multi-modal prior of categorical distribution , on VAE which has better learned latent representation due to no component collapsing . To support this , we did extensive experiments including topic modeling experiments and experimentally showed that DirVAE with the Dirichlet prior does not have component collapsing for the first time in this field . This component collapsing was not experimented and discussed in the prior work of [ 2 ] . Moreover , our experiments on the topic modeling shows the consistent performance increases when we apply the DirVAE , which was not discusses in [ 2 ] . The below is the response to your questions . The log likelihood ( LL ) is the log-probability that a learner optimizes to train a model given an observed dataset . However , since the likelihood or log-likelihood function is intractable given a latent variable in VAE , so we use the evidence lower bound ( ELBO ) to optimize the LL . ELBO is a tractable alternative of LL , so the optimization on ELBO is feasible . ELBO term consists of two parts : Reconstruction Error ( or Reconstruction Loss , which you asked ) and KL divergence terms . Here , Reconstruction Error measures the error between the input and the output , which is an auto-encoder reconstructed input . Additionally , for your information , equation ( 1 ) in our paper , can be re-written as follows : Negative Log-likelihood < = Negative ELBO = Reconstruction Loss + KL Divergence . The author of paper [ 3 ] on the inverse Gamma recommends a finite difference approximation method when alpha > 1 . We only encountered such alpha > 1 cases when we updated alphas , and the topic modeling often sets the alpha to be in the range of [ 0,1 ] . In the cases of alpha > 1 , we approached this problem via approximating the inverse function of the Gamma CDF with a Newton method , but the learning performance was not satisfactory . Thus , we left the updated alpha parameters with values greater than one in the appendix . Sincerely . References [ 1 ] Diederik P Kingma , Max Welling , Auto-encoding variational Bayes , ICLR , 2014 . [ 2 ] Michael Figurnov , Shakir Mohamed , Andriy Mnih , Implicit reparametrization gradients , arXiv , 2018 . [ 3 ] David . A. Knowles . Stochastic gradient variational bayes for gamma approximating distributions . arXiv , 2015 ."}, {"review_id": "rkgsvoA9K7-2", "review_text": "Review: This paper proposes to change the typical Gaussian posterior distribution (and prior) for the latent features z associated to an image x that is used in Variational Autoencoders by a Dirichlet distribution. The work improves over previous attempts based on a soft-max + Gaussian distribution and the soft-max + Weibull distribution. The trick proposed to make feasible training the model includes approximating the inverse CDF of the gamma distribution and using the fact that the Dirichlet distribution can also be obtained as a normalized sum of gamma random variables. The method is compared in several problems. Some analysis of the reasons why it performs better is also carried out. Quality: I think the quality of the paper is high. It is a well written paper in which the choices made are well supported. It also has a strong experimental section. Clarity: The paper is well written and reads very smoothly. I have missed however a more clear statement in the introduction supporting the use of the Dirichlet for the prior and posterior of the latent variables, simply because it seems to give better results and the typical Gaussian choice. Originality: The paper is based on ideas already known. E.g., Dirichlet a normalized sum of gamma random variables and approximation of the inverse CDF of the gamma random variable. The combination of these two techniques is however novel. Significance: The results obtained indicate that the proposed approach improves over previous work on the Dirichlet VAE and on the Gaussian VAE. So I believe the significance of the paper is high. pros: - Good results. - Simple method proposed. - Extensive experiments. - Well written paper. cons: - The idea is a combination of already known techniques put in practice for the VAE. - A better motivation that the Dirichlet VAE gives good results should be given at the introduction. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . Firstly , as a motivation for the better result , we can state as the following , and if you are okay with the below sentences , we would like to add it to the introduction part . `` Due to the component collapsing issues , the existing VAEs have less meaningful latent values or could not effectively use its latent representation . Meanwhile , DirVAE does not have component collapsing due to the multi-modal prior which possibly leads to superior qualitative and quantitative performances . We experimentally showed that the DirVAE has more meaningful or disentangled latent representation by image generation and latent value visualizations . '' Secondly , although the techniques are already known , we 've rather wanted to focus our paper on the characteristic of Dirichlet prior on VAE such as better latent representation due to no component collapsing , or its applicability like topic modeling . Thanks again for your valuable comments . Sincerely ."}], "0": {"review_id": "rkgsvoA9K7-0", "review_text": "In this paper, authors proposes an algorithm to use Dirichlet prior on the variational auto-encoder (VAE). They used this prior as natural conjugate to likelihood distributtion of multinomial (categorical). The paper proposes a way to use scalability power of VAE for data distributed by categorical distribution. In order to apply reparametrization trick, authors have used iid Gamma random variable to construct draw from Dirichlet distribution and have used approximation with inverse gamma CDF, it is discussed how this method has better performance than other approximations method for gamma distribution such as Weibull and logistic Gaussian. Authors pointed out, one of the weak points in competing models such as Guassian softmax prior or Griffith -Engen-McCloskey prior which has been used for Stick breaking VAE is to not encouraging of having multi-modal posteriori, while this prior empower having multi-modal posteriori distribution which give them advantage over previous papers. In experimental results, paper has used different datasets of MNIST, MNIST+rotation , OMNIGLOT , 20newsgroup and RCVI and used different measures to compare the existing method with the baselines. To summarize the contribution of this paper, following three points can be named as main contribution of this paper: - proposed a Dirichlet prior, for categorical likelihood which encourages having multi-modal posteriori. paper demonstrates couple of techniques to apply the reparametrization trick on Dirichlet distribution, by using sum of iid Gamma random variables. - used method of moments estimator to update the hyper parameter of the Dirichlet distribution which helps to have closer approximation of log likelihood. They update hyper-parameters after every few updates of VAE parameters. -discussed how to overcome Stick-breaking VAE \u201ccomponent collapse\u201d issue. Experiments show superior results on supervised and semi supervised, and authors claimed the main reason of this superiority being due to not having disadvantage of component collapse which happens in SBVAE. Quality and Novelty: claims in paper are supported by proofs and/or experimental results and there does not exist significant technical issues with the details of claims made in this paper and proofs provided. There are following issues with novelty and quality of paper that I would like discuss them under following three points: - Authors need to be clear about the motivation of the paper, if the motivation of the paper is to encourage the multi-modality in posteriori distribution, using Gaussian prior and methods like normalizing flow Rezende, Danilo Jimenez, and Shakir Mohamed. \"Variational inference with normalizing flows.\" arXiv preprint arXiv:1505.05770 (2015) or similar may be able to do the same work in which case paper should compare its results to those ideas which has not been done in this paper. - second appealing point that this paper can make is to use Dirichlet prior for the purposes like community detection, topic modeling and LDA etc etc. In this case, I did not find significant difference between the proposed method and what is found in Srivastava, Akash, and Charles Sutton. \"Autoencoding variational inference for topic models.\" arXiv preprint arXiv:1703.01488 (2017), but due to the encourages of multi-modality authors show in average DirVAE performs better in measures like perplexity and NPMI. Under this condition, my main concern is interpretablity of posteriori. That will be discussed under next point - Main motivation behind using Dirichlet prior, is to have posteriori with a few significant related topic and many unrelated topic for every word. By changing the concentration parameter in stick-breaking, it is possible that performance of stick-breaking method increase in perplexity and NPMI scores in cost of loosing interpretability of the model. So having higher concentration parameter can show better performance in the cost of interpretablity that put second point of the paper at risk Clarity: The paper is well written and previous relevant methods have been reviewed well. The organization of paper is good, experiments well explained and proofs and mathematical reasoning are clear. Significance of experiments: As discussed,in previous sections, the results show superior performance and compared to other methods on semi-supervised and supervised classification on different datasets. Also it has shown in average better perplexity and NPMI score for topic modeling, the only issue can be these scores come as cost of interpretablity of the model. Also it is possible that other competing models can be matching to this results if they do not aim for sparse posteriori.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . Currently , we are doing additional experiments to respond to your constructive comments . Sorry for the delay , but we will give you proper responds to your review with results as soon as possible . Up to the current status of experiments , the VAE with the normalizing flow still suffers from the decoder weight collapsing problem . Hence , the performance would not match to our approach , but we are going to make a certain on this premature result with the experiments , next few days . Sincerely ."}, "1": {"review_id": "rkgsvoA9K7-1", "review_text": "This paper proposes DirVAE, a variational autoencoder with Dirichlet prior on latent variables. The advantage of using Dirichlet distribution is that due the nature of Dirichlet distribution the model does not suffer from decoder weight collapsing and latent value collapsing. Stochastic gradient variational Bayes with inverse CDF reparametrization of gamma distribution is presented. The motivation behind using Dirichlet instead of GEM makes sense, but other than that I fail to find any novelty in the paper. The authors should tone down the statement \"to our knowledge, combining the two statistical results is the first finding in the machine learning field\". Even though left unpublished, I've been using this combination of inverse CDF gamma reparametrization and transformation to Dirichlet all the time for my own problems. It's just trivial once we have both techniques. See also [2], where an improved way of reparametrizing gamma and Dirichlet distribution is presented. The observation that DirVAE does not suffer from latent value collapsing is interesting, but not really surprising. Minor question - What is the difference between negative LL's and reconstruction losses in experiments? - The approximation for inverse CDF of gamma works well only when alpha << 1. How did you treat the regime alpha > 1? References [1] Diederik P Kingma, Max Welling, Auto-encoding variational Bayes, ICLR, 2014. [2] Michael Figurnov, Shakir Mohamed, Andriy Mnih, Implicit reparametrization gradients, arXiv, 2018.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . Firstly , we would like to say thank you for introducing the paper [ 2 ] . Even though the paper [ 2 ] and our paper lie on the same path in terms of reparametrizing Gamma distribution , paper [ 2 ] deals with a general reparametrization trick on various probabilistic distributions while our paper focuses on the advantages and the applicabilities of Dirichlet prior in VAE . We believe that our contribution is not based on reparametrizing Gamma or Dirichlet distiribution , but introducing Dirichlet prior , which is a conjugate multi-modal prior of categorical distribution , on VAE which has better learned latent representation due to no component collapsing . To support this , we did extensive experiments including topic modeling experiments and experimentally showed that DirVAE with the Dirichlet prior does not have component collapsing for the first time in this field . This component collapsing was not experimented and discussed in the prior work of [ 2 ] . Moreover , our experiments on the topic modeling shows the consistent performance increases when we apply the DirVAE , which was not discusses in [ 2 ] . The below is the response to your questions . The log likelihood ( LL ) is the log-probability that a learner optimizes to train a model given an observed dataset . However , since the likelihood or log-likelihood function is intractable given a latent variable in VAE , so we use the evidence lower bound ( ELBO ) to optimize the LL . ELBO is a tractable alternative of LL , so the optimization on ELBO is feasible . ELBO term consists of two parts : Reconstruction Error ( or Reconstruction Loss , which you asked ) and KL divergence terms . Here , Reconstruction Error measures the error between the input and the output , which is an auto-encoder reconstructed input . Additionally , for your information , equation ( 1 ) in our paper , can be re-written as follows : Negative Log-likelihood < = Negative ELBO = Reconstruction Loss + KL Divergence . The author of paper [ 3 ] on the inverse Gamma recommends a finite difference approximation method when alpha > 1 . We only encountered such alpha > 1 cases when we updated alphas , and the topic modeling often sets the alpha to be in the range of [ 0,1 ] . In the cases of alpha > 1 , we approached this problem via approximating the inverse function of the Gamma CDF with a Newton method , but the learning performance was not satisfactory . Thus , we left the updated alpha parameters with values greater than one in the appendix . Sincerely . References [ 1 ] Diederik P Kingma , Max Welling , Auto-encoding variational Bayes , ICLR , 2014 . [ 2 ] Michael Figurnov , Shakir Mohamed , Andriy Mnih , Implicit reparametrization gradients , arXiv , 2018 . [ 3 ] David . A. Knowles . Stochastic gradient variational bayes for gamma approximating distributions . arXiv , 2015 ."}, "2": {"review_id": "rkgsvoA9K7-2", "review_text": "Review: This paper proposes to change the typical Gaussian posterior distribution (and prior) for the latent features z associated to an image x that is used in Variational Autoencoders by a Dirichlet distribution. The work improves over previous attempts based on a soft-max + Gaussian distribution and the soft-max + Weibull distribution. The trick proposed to make feasible training the model includes approximating the inverse CDF of the gamma distribution and using the fact that the Dirichlet distribution can also be obtained as a normalized sum of gamma random variables. The method is compared in several problems. Some analysis of the reasons why it performs better is also carried out. Quality: I think the quality of the paper is high. It is a well written paper in which the choices made are well supported. It also has a strong experimental section. Clarity: The paper is well written and reads very smoothly. I have missed however a more clear statement in the introduction supporting the use of the Dirichlet for the prior and posterior of the latent variables, simply because it seems to give better results and the typical Gaussian choice. Originality: The paper is based on ideas already known. E.g., Dirichlet a normalized sum of gamma random variables and approximation of the inverse CDF of the gamma random variable. The combination of these two techniques is however novel. Significance: The results obtained indicate that the proposed approach improves over previous work on the Dirichlet VAE and on the Gaussian VAE. So I believe the significance of the paper is high. pros: - Good results. - Simple method proposed. - Extensive experiments. - Well written paper. cons: - The idea is a combination of already known techniques put in practice for the VAE. - A better motivation that the Dirichlet VAE gives good results should be given at the introduction. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . Firstly , as a motivation for the better result , we can state as the following , and if you are okay with the below sentences , we would like to add it to the introduction part . `` Due to the component collapsing issues , the existing VAEs have less meaningful latent values or could not effectively use its latent representation . Meanwhile , DirVAE does not have component collapsing due to the multi-modal prior which possibly leads to superior qualitative and quantitative performances . We experimentally showed that the DirVAE has more meaningful or disentangled latent representation by image generation and latent value visualizations . '' Secondly , although the techniques are already known , we 've rather wanted to focus our paper on the characteristic of Dirichlet prior on VAE such as better latent representation due to no component collapsing , or its applicability like topic modeling . Thanks again for your valuable comments . Sincerely ."}}