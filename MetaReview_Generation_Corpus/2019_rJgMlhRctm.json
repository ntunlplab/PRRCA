{"year": "2019", "forum": "rJgMlhRctm", "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision", "decision": "Accept (Oral)", "meta_review": "Strong paper in an interesting new direction.\nMore work should be done in this area.", "reviews": [{"review_id": "rJgMlhRctm-0", "review_text": "To achieve the state-of-the-art on the CLEVR and the variations of this, the authors propose a method to use object-based visual representations and a differentiable quasi-symbolic executor. Since the semantic parser for a question input is not differentiable, they use REINFORCE algorithm and a technique to reduce its variance. Quality: The issue of invalid evaluation should be addressed. CLEVR dataset has train, validation, and test sets. Since the various hyper-parameters are determined with the validation set, the comparison of state-of-the-art should be done using test set. As the authors mentioned, REINFORCE algorithm may introduce high variance, this notion is critical to report valid results. However, the authors only report on the validation set in Table 2 including the main results, Table 4. For Table 5, they only specify train and test splits. Therefore, I firmly recommend the authors to report on the test set for the fair comparison with the other competitive models, and please describe how to determine the hyperparameters in all experimental settings. Clarity: As mentioned above, please specify the experimental details regarding setting hyperparameters. In Experiments section, the authors used less than 10% of CLEVR training images. How about to use 100% of the training examples? How about to use the same amount of training examples in the competitive models? The report is incomplete to see the differential evident from the efficient usage of training examples. Originality and significance: The authors argue that object-based visual representation and symbolic reasoning are the contributions of this work (excluding the recent work, NS-VQA < 1 month). However, bottom-up and top-down attention work [1] shows that attention networks using object-based visual representation significantly improve VQA and image captioning performances. If the object-based visual representation alone is the primary source of improvement, it severely weakens the argument of the neuro-symbolic concept learner. Since, considering the trend of gains, the contribution of the proposing method seems to be incremental, this concern is inevitable. To defend this critic, the additional experiment to see the improvement of the other attentional model (e.g, TbD, MAC) using object-based visual representations, without any other annotations, is needed. Pros: - To confirm the effective learning of visual concepts, words, and semantic parsing of sentences, they insightfully exploit the nature of the CLEVR dataset for visual reasoning diagnosis. Cons: - Invalid evaluation to report only on the validation set, not test set. - The unclear significance of the proposed method combining object-based visual representations and symbolic reasoning - In the original CLEVR dataset paper, the authors said \"we stress that accuracy on CLEVR is not an end goal in itself\" and \"..CLEVR should be used in conjunction with other VQA datasets in order to study the reasoning abilities of general VQA systems.\" Based on this suggestion, can this work generalize to real-world settings? This paper lacks to discuss its limitation and future direction toward the general problem settings. Minor comments: In 4.3, please fix the typos, \"born\" -> \"brown\" and \"convlutional\" -> \"convolutional\". [1] Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., & Zhang, L. (2018). Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. IEEE Computer Vision and Pattern Recognition (CVPR'18).", "rating": "6: Marginally above acceptance threshold", "reply_text": "4.Specific Questions - Choice of hyperparameters . We use the open-sourced implementation of Mask-RCNN [ 5 ] to generate object proposals . For all the training processes described in the rest of the paper , we used learning rate 1e-3 with a weight decay of 5e-4 . We decay the learning rate by a factor 0.1 after 60 % of the designated training epochs . The REINFORCE optimizer uses a discount factor of 0.95 . In the main text , the variance of REINFORCE means the variance of the gradient estimation but not the variance of the performance ( accuracy ) . We will also add the standard deviation of the model performance in the revision . - Data-efficiency Thanks for the very nice suggestion . We have conducted a more systematic study on the data efficiency and will include it the revision . The results are Trained on 10 % of the images : TbD : 54.2 % . MAC : 67.3 % . NS-CL : 98.9 % . Trained on 100 % of the images : TbD : 99.1 % . MAC : 98.9 % . NS-CL : 99.2 % . These results demonstrate that our model is more data-efficient . We have also listed all other planned changes in our general response above . Please don \u2019 t hesitate to let us know for any additional comments on the paper or on the planned changes . [ 1 ] Anderson et al . `` Bottom-up and top-down attention for image captioning and visual question answering . '' In CVPR , 2018 . [ 2 ] Baradel et al . `` Object Level Visual Reasoning in Videos . '' In ECCV , 2018 . [ 3 ] Artzi , Yoav , and Zettlemoyer . `` Weakly supervised learning of semantic parsers for mapping instructions to actions . '' TACL 1 ( 2013 ) : 49-62 . [ 4 ] Oh et al . `` Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning . '' In ICML , 2017 . [ 5 ] https : //github.com/roytseng-tw/Detectron.pytorch"}, {"review_id": "rJgMlhRctm-1", "review_text": " Summary: ========= The paper proposes a joint learning of visual representation and word and semantic parsing of the sentences given paired images and paired Q/A with a model called neuro-symbolic concept learner using curriculum learning. The paper reads well and is easy to follow. The idea of jointly learning visual concepts and language is an important task. Human reasoning involves learning and recall from multiple moralities. The authors use the CLEVR dataset for evaluation. Strength: ======== - Jointly learning the language parsing and visual representations indirectly from paired Q/A and paired images is interesting. Combining the visual learning with the visual questions answers by decomposing them into primitive symbolic operations and reasoning in symbolic space seems interesting. - End-to-end learning of the visual concepts, Q/A decomposition into primitives and program execution was shown to be competitive to baseline methods. Weakness: ========= - Although, the joint learning and composition is interesting, the visual task is simplistic and it is not obvious how this would generalize into other complex VQA tasks. - Experiments are not as rigorous as the discussion of the methods suggests. Evaluation on more datasets would have made the comparisons and drawn conclusions more stronger. Although CLEVR is suited for learning relational concepts from referential expressions, it is a toy dataset. Applicability of the proposed method on other realistic datasets would have made the paper more stronger.", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for the encouraging and constructive comments . We agree that generalizing to more complex visual domains would be essential for our task . In the revision , we will include the results of NS-CL on new datasets , including the VQA dataset of real-world images [ 1 ] and the Minecraft dataset used by Yi et al . [ 2 ] .We have also listed all other planned changes in our general response above . Please don \u2019 t hesitate to let us know for any additional comments on the paper or on the planned changes . [ 1 ] Antol , Stanislaw , Aishwarya Agrawal , Jiasen Lu , Margaret Mitchell , Dhruv Batra , C. Lawrence Zitnick , and Devi Parikh . `` Vqa : Visual question answering . '' In ICCV , 2015 . [ 2 ] Yi , Kexin , Jiajun Wu , Chuang Gan , Antonio Torralba , Pushmeet Kohli , and Joshua B. Tenenbaum . `` Neural-Symbolic VQA : Disentangling Reasoning from Vision and Language Understanding . '' In NIPS , 2018 ."}, {"review_id": "rJgMlhRctm-2", "review_text": "The paper is well written and flow well. The only thing I would like to see added is an elaboration of \"run a semantic parsing module to translate a question into an executable program\". How to do semantic parsing is far from obvious. This topic needs at least a paragraph of its own. This is not a requirement but an opportunity, can you explain how counting work? I think you have it at the standard level of the magic of DNN but some digging into the mechanism would be appreciated. In concluding maybe you can speculate how far this method can go. Compositionality? Implicit relations inferred from words and behavior? Application to video with words? ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you very much for the constructive comments . 1.Semantic parsing . In short , the semantic parsing module is a neural sequence-to-tree model . Given a natural language question , the module translates into an executable program with a hierarchy of primitive operation . We present an overview in Sec.3.1 ( last paragraph of Page 4 ) , with more implementation details in Appendix B. We \u2019 ll revise the text for better clarity . The module begins with encoding the question into a fixed-length embedding vector using a bidirectional GRU . The decoder , taking the sentence embedding as input , recovers the hierarchy of the operations in a top-down manner : It first predicts the root token ( the question type : query/count/\u2026 in the VQA case ) ; then , conditioned on the root token , it predicts the tokens of the root \u2019 s children . The decoding algorithm runs recursively . 2.Counting.We perform counting in a quasi-symbolic manner , based on the object-based scene representation . As an example , consider a simple program : Count ( Filter ( Red ) ) , which counts the number of red objects in the scene . The operation Filter ( Red ) assigns each object with a value p_i , as the confidence of classifying this object as a red one . Counting is performed as : $ \\sum_i p_i $ . During inference , we round this value to the nearest integer . More details can be found in Sec.3,1 . ( Page 5 ) and Appendix C. We will also revise the text for better clarity . Compared with alternatives , our method enjoys combinatorial generalization with the notion of ` objects \u2019 : for example , trained on scenes with < = 6 objects , our model can also perform counting on scenes with 10 objects . 3.Future direction We thank the reviewer for the suggestions on future directions and will include the following discussions in the revision : Compositionality . We currently view the scene as a collection of objects with latent representations . Building scene ( or video ) representations that also reflects the compositional nature of objects ( e.g. , an object is a combination of multiple primitives ) will be an interesting research direction . Infer relations from words and behavior . Modelling actions ( e.g. , push and pull ) as concepts is another interesting direction . People have studied the symbolic representation of skills [ 1 ] and learning word ( instruction ) meanings from interaction [ 2 ] . Videos and words . Our framework can also be extended to the video domain . Video techniques such as detection and tracking are needed to build the object-based representation [ 3 ] . Also , the semantic representation of sentences should be extended to include actions / interactions besides static spatial relations . We have also listed all other planned changes in our general response above . Please don \u2019 t hesitate to let us know for any additional comments on the paper or on the planned changes . [ 1 ] Konidaris , George , Leslie Pack Kaelbling , and Tomas Lozano-Perez . `` From skills to symbols : Learning symbolic representations for abstract high-level planning . '' Journal of Artificial Intelligence Research 61 ( 2018 ) : 215-289 . [ 2 ] Oh , Junhyuk , Satinder Singh , Honglak Lee , and Pushmeet Kohli . `` Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning . '' In ICML , 2017 . [ 3 ] Baradel , Fabien , Natalia Neverova , Christian Wolf , Julien Mille , and Greg Mori . `` Object Level Visual Reasoning in Videos . '' In ECCV , 2018 ."}], "0": {"review_id": "rJgMlhRctm-0", "review_text": "To achieve the state-of-the-art on the CLEVR and the variations of this, the authors propose a method to use object-based visual representations and a differentiable quasi-symbolic executor. Since the semantic parser for a question input is not differentiable, they use REINFORCE algorithm and a technique to reduce its variance. Quality: The issue of invalid evaluation should be addressed. CLEVR dataset has train, validation, and test sets. Since the various hyper-parameters are determined with the validation set, the comparison of state-of-the-art should be done using test set. As the authors mentioned, REINFORCE algorithm may introduce high variance, this notion is critical to report valid results. However, the authors only report on the validation set in Table 2 including the main results, Table 4. For Table 5, they only specify train and test splits. Therefore, I firmly recommend the authors to report on the test set for the fair comparison with the other competitive models, and please describe how to determine the hyperparameters in all experimental settings. Clarity: As mentioned above, please specify the experimental details regarding setting hyperparameters. In Experiments section, the authors used less than 10% of CLEVR training images. How about to use 100% of the training examples? How about to use the same amount of training examples in the competitive models? The report is incomplete to see the differential evident from the efficient usage of training examples. Originality and significance: The authors argue that object-based visual representation and symbolic reasoning are the contributions of this work (excluding the recent work, NS-VQA < 1 month). However, bottom-up and top-down attention work [1] shows that attention networks using object-based visual representation significantly improve VQA and image captioning performances. If the object-based visual representation alone is the primary source of improvement, it severely weakens the argument of the neuro-symbolic concept learner. Since, considering the trend of gains, the contribution of the proposing method seems to be incremental, this concern is inevitable. To defend this critic, the additional experiment to see the improvement of the other attentional model (e.g, TbD, MAC) using object-based visual representations, without any other annotations, is needed. Pros: - To confirm the effective learning of visual concepts, words, and semantic parsing of sentences, they insightfully exploit the nature of the CLEVR dataset for visual reasoning diagnosis. Cons: - Invalid evaluation to report only on the validation set, not test set. - The unclear significance of the proposed method combining object-based visual representations and symbolic reasoning - In the original CLEVR dataset paper, the authors said \"we stress that accuracy on CLEVR is not an end goal in itself\" and \"..CLEVR should be used in conjunction with other VQA datasets in order to study the reasoning abilities of general VQA systems.\" Based on this suggestion, can this work generalize to real-world settings? This paper lacks to discuss its limitation and future direction toward the general problem settings. Minor comments: In 4.3, please fix the typos, \"born\" -> \"brown\" and \"convlutional\" -> \"convolutional\". [1] Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., & Zhang, L. (2018). Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. IEEE Computer Vision and Pattern Recognition (CVPR'18).", "rating": "6: Marginally above acceptance threshold", "reply_text": "4.Specific Questions - Choice of hyperparameters . We use the open-sourced implementation of Mask-RCNN [ 5 ] to generate object proposals . For all the training processes described in the rest of the paper , we used learning rate 1e-3 with a weight decay of 5e-4 . We decay the learning rate by a factor 0.1 after 60 % of the designated training epochs . The REINFORCE optimizer uses a discount factor of 0.95 . In the main text , the variance of REINFORCE means the variance of the gradient estimation but not the variance of the performance ( accuracy ) . We will also add the standard deviation of the model performance in the revision . - Data-efficiency Thanks for the very nice suggestion . We have conducted a more systematic study on the data efficiency and will include it the revision . The results are Trained on 10 % of the images : TbD : 54.2 % . MAC : 67.3 % . NS-CL : 98.9 % . Trained on 100 % of the images : TbD : 99.1 % . MAC : 98.9 % . NS-CL : 99.2 % . These results demonstrate that our model is more data-efficient . We have also listed all other planned changes in our general response above . Please don \u2019 t hesitate to let us know for any additional comments on the paper or on the planned changes . [ 1 ] Anderson et al . `` Bottom-up and top-down attention for image captioning and visual question answering . '' In CVPR , 2018 . [ 2 ] Baradel et al . `` Object Level Visual Reasoning in Videos . '' In ECCV , 2018 . [ 3 ] Artzi , Yoav , and Zettlemoyer . `` Weakly supervised learning of semantic parsers for mapping instructions to actions . '' TACL 1 ( 2013 ) : 49-62 . [ 4 ] Oh et al . `` Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning . '' In ICML , 2017 . [ 5 ] https : //github.com/roytseng-tw/Detectron.pytorch"}, "1": {"review_id": "rJgMlhRctm-1", "review_text": " Summary: ========= The paper proposes a joint learning of visual representation and word and semantic parsing of the sentences given paired images and paired Q/A with a model called neuro-symbolic concept learner using curriculum learning. The paper reads well and is easy to follow. The idea of jointly learning visual concepts and language is an important task. Human reasoning involves learning and recall from multiple moralities. The authors use the CLEVR dataset for evaluation. Strength: ======== - Jointly learning the language parsing and visual representations indirectly from paired Q/A and paired images is interesting. Combining the visual learning with the visual questions answers by decomposing them into primitive symbolic operations and reasoning in symbolic space seems interesting. - End-to-end learning of the visual concepts, Q/A decomposition into primitives and program execution was shown to be competitive to baseline methods. Weakness: ========= - Although, the joint learning and composition is interesting, the visual task is simplistic and it is not obvious how this would generalize into other complex VQA tasks. - Experiments are not as rigorous as the discussion of the methods suggests. Evaluation on more datasets would have made the comparisons and drawn conclusions more stronger. Although CLEVR is suited for learning relational concepts from referential expressions, it is a toy dataset. Applicability of the proposed method on other realistic datasets would have made the paper more stronger.", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for the encouraging and constructive comments . We agree that generalizing to more complex visual domains would be essential for our task . In the revision , we will include the results of NS-CL on new datasets , including the VQA dataset of real-world images [ 1 ] and the Minecraft dataset used by Yi et al . [ 2 ] .We have also listed all other planned changes in our general response above . Please don \u2019 t hesitate to let us know for any additional comments on the paper or on the planned changes . [ 1 ] Antol , Stanislaw , Aishwarya Agrawal , Jiasen Lu , Margaret Mitchell , Dhruv Batra , C. Lawrence Zitnick , and Devi Parikh . `` Vqa : Visual question answering . '' In ICCV , 2015 . [ 2 ] Yi , Kexin , Jiajun Wu , Chuang Gan , Antonio Torralba , Pushmeet Kohli , and Joshua B. Tenenbaum . `` Neural-Symbolic VQA : Disentangling Reasoning from Vision and Language Understanding . '' In NIPS , 2018 ."}, "2": {"review_id": "rJgMlhRctm-2", "review_text": "The paper is well written and flow well. The only thing I would like to see added is an elaboration of \"run a semantic parsing module to translate a question into an executable program\". How to do semantic parsing is far from obvious. This topic needs at least a paragraph of its own. This is not a requirement but an opportunity, can you explain how counting work? I think you have it at the standard level of the magic of DNN but some digging into the mechanism would be appreciated. In concluding maybe you can speculate how far this method can go. Compositionality? Implicit relations inferred from words and behavior? Application to video with words? ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you very much for the constructive comments . 1.Semantic parsing . In short , the semantic parsing module is a neural sequence-to-tree model . Given a natural language question , the module translates into an executable program with a hierarchy of primitive operation . We present an overview in Sec.3.1 ( last paragraph of Page 4 ) , with more implementation details in Appendix B. We \u2019 ll revise the text for better clarity . The module begins with encoding the question into a fixed-length embedding vector using a bidirectional GRU . The decoder , taking the sentence embedding as input , recovers the hierarchy of the operations in a top-down manner : It first predicts the root token ( the question type : query/count/\u2026 in the VQA case ) ; then , conditioned on the root token , it predicts the tokens of the root \u2019 s children . The decoding algorithm runs recursively . 2.Counting.We perform counting in a quasi-symbolic manner , based on the object-based scene representation . As an example , consider a simple program : Count ( Filter ( Red ) ) , which counts the number of red objects in the scene . The operation Filter ( Red ) assigns each object with a value p_i , as the confidence of classifying this object as a red one . Counting is performed as : $ \\sum_i p_i $ . During inference , we round this value to the nearest integer . More details can be found in Sec.3,1 . ( Page 5 ) and Appendix C. We will also revise the text for better clarity . Compared with alternatives , our method enjoys combinatorial generalization with the notion of ` objects \u2019 : for example , trained on scenes with < = 6 objects , our model can also perform counting on scenes with 10 objects . 3.Future direction We thank the reviewer for the suggestions on future directions and will include the following discussions in the revision : Compositionality . We currently view the scene as a collection of objects with latent representations . Building scene ( or video ) representations that also reflects the compositional nature of objects ( e.g. , an object is a combination of multiple primitives ) will be an interesting research direction . Infer relations from words and behavior . Modelling actions ( e.g. , push and pull ) as concepts is another interesting direction . People have studied the symbolic representation of skills [ 1 ] and learning word ( instruction ) meanings from interaction [ 2 ] . Videos and words . Our framework can also be extended to the video domain . Video techniques such as detection and tracking are needed to build the object-based representation [ 3 ] . Also , the semantic representation of sentences should be extended to include actions / interactions besides static spatial relations . We have also listed all other planned changes in our general response above . Please don \u2019 t hesitate to let us know for any additional comments on the paper or on the planned changes . [ 1 ] Konidaris , George , Leslie Pack Kaelbling , and Tomas Lozano-Perez . `` From skills to symbols : Learning symbolic representations for abstract high-level planning . '' Journal of Artificial Intelligence Research 61 ( 2018 ) : 215-289 . [ 2 ] Oh , Junhyuk , Satinder Singh , Honglak Lee , and Pushmeet Kohli . `` Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning . '' In ICML , 2017 . [ 3 ] Baradel , Fabien , Natalia Neverova , Christian Wolf , Julien Mille , and Greg Mori . `` Object Level Visual Reasoning in Videos . '' In ECCV , 2018 ."}}