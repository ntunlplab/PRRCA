{"year": "2017", "forum": "S1HEBe_Jl", "title": "Learning to Protect Communications with Adversarial Neural Cryptography", "decision": "Reject", "meta_review": "Interesting paper but not over the accept bar.", "reviews": [{"review_id": "S1HEBe_Jl-0", "review_text": "The submission proposes to modify the typical GAN architecture slightly to include \"encrypt\" (Alice) and \"decrypt\" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve). Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code. Examples are given on toy data: \"As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64. Both plaintext and key values are uniformly distributed.\" The idea considered here is cute. If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption. In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part. While this is a nice thought experiment, there are significant barriers to this submission having a practical impact: 1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize. The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time). I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee. 2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere. The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point. Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We are glad about your comment that this paper presents \u201c an interesting thought experiment \u201d , as it is in line with how we regard this work . As for your points on the possible impact of this work : 1 ) We also agree that the techniques that we present are not likely to yield high-assurance security . They may however yield suitable protection against low-grade attackers ( much like spam filters ) or against our own actions . In particular , as suggested in the submission ( page 2 ) , they may be adequate in order to prevent one of our own neural-network components from using information that we want to keep from it because of concerns about privacy or discrimination . 2 ) An important contrast with readily available encryption systems is that , with our approach , one learns what needs to be \u201c scrambled \u201d for a given a protection goal ( as indicated in our reply of December 8 ) ."}, {"review_id": "S1HEBe_Jl-1", "review_text": "The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy. The concepts, ideas and previous literature are quite nicely and carefully presented. The only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3. In particular, I don't quite get the scenario. The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender). In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C. What is described in section 3, however, is that D and D-public are both reconstructed by Bob, but why would Bob reconstruct the latter (he is not public, in particular because he is allowed to reconstruct C, which is not tested here)? Also, Eve only tries to estimate C, thus rendering the scenario not different in any way to the scenario considered in section 2. I have two more minor concerns: 1) As raised in the pre-review, Eve should actually be stronger then Alice and Bob in order to be able to compensate for the missing key. The authors noted they have been doing these experiments and are going to add the results. 2) In any natural encryption case I would expect the length of the key to be much shorter then the length of the message. This, however, could potentially make the scenario much easier for Eve (although I doubt any of the results will change if the key is long enough). I like the creative application of adversarial training to a completely different domain, and I believe it could be the starting point of a very interesting direction in cryptographic systems or in privacy applications (although it is unclear whether the weak guarantees of neural network based approaches can ever be overcome). At the same time the application in the privacy setting leaves me quite confused, and the symmetric encryption example is not particularly strong either. I'd appreciate if the authors could address the major concern I raised above, and I will be quite happy to raise the score in case this confusion can be resolved.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your encouraging comments . Regarding the experiments of section 3 , the scenario is that Alice provides information so that anyone can observe an estimate of D without gaining information about C. The fact that C is hidden is formulated by introducing Eve , which attempts to reconstruct C. Alice also provides additional information so that Bob , with whom Alice shares a key , can do an even better job at estimating D , but may learn something about C. In sum , the main difference with section 2 is that here the \u201c communication goal \u201d and the \u201c hiding goal \u201d concern two different parts of the plaintext ( D and C respectively ) . We hope that this clarifies the matter ; we would certainly be happy to expand the discussion in section 3 ( perhaps adding diagrams which we have used in talks , but which seemed too long for the submission ) . Regarding minor concern ( 1 ) , we will indeed be glad to include the results indicated in our response and some updated ones in the event that the paper is accepted . We agree that it is interesting to think about stronger Eve networks , as suggested by the reviewer . On the other hand , we believe that the appropriate strength of Eve may sometimes be dictated by intended applications ; for example , if we are trying to hide information from one of our own neural-network components ( as suggested in page 2 ) , it makes sense that Eve be of a similar size and architecture as that component . Regarding minor concern ( 2 ) , we have two thoughts . First , it may often be appropriate to generate arbitrary-length key material using standard cryptographic key-generation techniques . ( These techniques generate an arbitrary-length stream of bits given a short initial seed , and parts of that stream of bits could be used as the keys passed into the neural cryptography ) . Second , as a generalization of our work , we agree that the treatment of larger messages could be interesting . It is natural to evaluate neural cryptography with messages larger than keys ; this should be a straightforward extension of our current work . As a more futuristic step , one could explore so-called \u201c modes of operation \u201d using RNNs or similar architectures ."}, {"review_id": "S1HEBe_Jl-2", "review_text": "This paper proposed to use GAN for encrypted communications. In section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system. In section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Decorrelating data is often attractive , as the review suggests , but it can be non-trivial . In our work , the role of the adversary is to expose the correlations , which may not otherwise be apparent . Moreover , decorrelating data may reduce utility , perhaps in completely unnecessary ways if the correlations are not relevant to the secrecy goals . Our work aims to show that there is an alternative , and that it can be learned : that neural networks can figure out which correlations matter given a set of secrecy goals , and can figure out how to hide them . We believe that this is an interesting capability , and that it may well be useful in building compositions of neural networks with confidentiality constraints ( as suggested on page 2 ) . On the other hand , we certainly do not expect to beat the state of the art in terms of cryptographic strength and assurance . It is conceivable that hybrid schemes would be viable , as discussed in our reply dated December 8 ."}], "0": {"review_id": "S1HEBe_Jl-0", "review_text": "The submission proposes to modify the typical GAN architecture slightly to include \"encrypt\" (Alice) and \"decrypt\" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve). Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code. Examples are given on toy data: \"As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64. Both plaintext and key values are uniformly distributed.\" The idea considered here is cute. If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption. In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part. While this is a nice thought experiment, there are significant barriers to this submission having a practical impact: 1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize. The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time). I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee. 2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere. The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point. Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We are glad about your comment that this paper presents \u201c an interesting thought experiment \u201d , as it is in line with how we regard this work . As for your points on the possible impact of this work : 1 ) We also agree that the techniques that we present are not likely to yield high-assurance security . They may however yield suitable protection against low-grade attackers ( much like spam filters ) or against our own actions . In particular , as suggested in the submission ( page 2 ) , they may be adequate in order to prevent one of our own neural-network components from using information that we want to keep from it because of concerns about privacy or discrimination . 2 ) An important contrast with readily available encryption systems is that , with our approach , one learns what needs to be \u201c scrambled \u201d for a given a protection goal ( as indicated in our reply of December 8 ) ."}, "1": {"review_id": "S1HEBe_Jl-1", "review_text": "The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy. The concepts, ideas and previous literature are quite nicely and carefully presented. The only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3. In particular, I don't quite get the scenario. The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender). In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C. What is described in section 3, however, is that D and D-public are both reconstructed by Bob, but why would Bob reconstruct the latter (he is not public, in particular because he is allowed to reconstruct C, which is not tested here)? Also, Eve only tries to estimate C, thus rendering the scenario not different in any way to the scenario considered in section 2. I have two more minor concerns: 1) As raised in the pre-review, Eve should actually be stronger then Alice and Bob in order to be able to compensate for the missing key. The authors noted they have been doing these experiments and are going to add the results. 2) In any natural encryption case I would expect the length of the key to be much shorter then the length of the message. This, however, could potentially make the scenario much easier for Eve (although I doubt any of the results will change if the key is long enough). I like the creative application of adversarial training to a completely different domain, and I believe it could be the starting point of a very interesting direction in cryptographic systems or in privacy applications (although it is unclear whether the weak guarantees of neural network based approaches can ever be overcome). At the same time the application in the privacy setting leaves me quite confused, and the symmetric encryption example is not particularly strong either. I'd appreciate if the authors could address the major concern I raised above, and I will be quite happy to raise the score in case this confusion can be resolved.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your encouraging comments . Regarding the experiments of section 3 , the scenario is that Alice provides information so that anyone can observe an estimate of D without gaining information about C. The fact that C is hidden is formulated by introducing Eve , which attempts to reconstruct C. Alice also provides additional information so that Bob , with whom Alice shares a key , can do an even better job at estimating D , but may learn something about C. In sum , the main difference with section 2 is that here the \u201c communication goal \u201d and the \u201c hiding goal \u201d concern two different parts of the plaintext ( D and C respectively ) . We hope that this clarifies the matter ; we would certainly be happy to expand the discussion in section 3 ( perhaps adding diagrams which we have used in talks , but which seemed too long for the submission ) . Regarding minor concern ( 1 ) , we will indeed be glad to include the results indicated in our response and some updated ones in the event that the paper is accepted . We agree that it is interesting to think about stronger Eve networks , as suggested by the reviewer . On the other hand , we believe that the appropriate strength of Eve may sometimes be dictated by intended applications ; for example , if we are trying to hide information from one of our own neural-network components ( as suggested in page 2 ) , it makes sense that Eve be of a similar size and architecture as that component . Regarding minor concern ( 2 ) , we have two thoughts . First , it may often be appropriate to generate arbitrary-length key material using standard cryptographic key-generation techniques . ( These techniques generate an arbitrary-length stream of bits given a short initial seed , and parts of that stream of bits could be used as the keys passed into the neural cryptography ) . Second , as a generalization of our work , we agree that the treatment of larger messages could be interesting . It is natural to evaluate neural cryptography with messages larger than keys ; this should be a straightforward extension of our current work . As a more futuristic step , one could explore so-called \u201c modes of operation \u201d using RNNs or similar architectures ."}, "2": {"review_id": "S1HEBe_Jl-2", "review_text": "This paper proposed to use GAN for encrypted communications. In section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system. In section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Decorrelating data is often attractive , as the review suggests , but it can be non-trivial . In our work , the role of the adversary is to expose the correlations , which may not otherwise be apparent . Moreover , decorrelating data may reduce utility , perhaps in completely unnecessary ways if the correlations are not relevant to the secrecy goals . Our work aims to show that there is an alternative , and that it can be learned : that neural networks can figure out which correlations matter given a set of secrecy goals , and can figure out how to hide them . We believe that this is an interesting capability , and that it may well be useful in building compositions of neural networks with confidentiality constraints ( as suggested on page 2 ) . On the other hand , we certainly do not expect to beat the state of the art in terms of cryptographic strength and assurance . It is conceivable that hybrid schemes would be viable , as discussed in our reply dated December 8 ."}}