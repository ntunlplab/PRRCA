{"year": "2020", "forum": "Syx33erYwH", "title": "ASYNCHRONOUS MULTI-AGENT GENERATIVE ADVERSARIAL IMITATION LEARNING", "decision": "Reject", "meta_review": "This paper extends multi-agent imitation learning to extensive-form games. There is a long discussion between reviewer #3 and the authors on the difference between Markov Games (MGs) and Extensive-Form Games (EFGs). The core of the discussion is on whether methods developed under the MG formalism (where agents take actions simultaneously) naturally can be applied to the EFG problem setting (where agents can take actions asynchronously). Despite the long discussion, the authors and reviewer did not come to an agreement on this point. Given that it is a crucial point for determining the significance of the contribution, my decision is to decline the paper. I suggest that the authors add a detailed discussion on why MG methods cannot be applied to EFGs in the way suggested by reviewer #3 in the next version of this work and then resubmit.", "reviews": [{"review_id": "Syx33erYwH-0", "review_text": " After reading Sections 1 and 2, I believe that the authors have a misunderstanding of the relationship between Markov Games and Extensive Form Games. The fundamental difference between these two formalisms is that Markov Games aka Stochastic Games (https://en.wikipedia.org/wiki/Stochastic_game) are fully observed while EFGs in general are not fully observed. EFGs are called Partially-Observed Markov Games in the RL literature. \"Go\" is actually a MG, in contradiction with the authors' statement in the intro. The authors make an artificial distinction that \"turn-based\" games cannot be handled under the MG formalism. In fact, turn-based games (even when the turn order is dynamic/stochastic) is easily handled by the MG formalism, by assigning \"no-op\" moves to players who are not active at this decision point. Therefore, I don't see why any additional mechanics are needed to apply MA-GAIL to turn-based games. Similarly, I don't understand the necessity of (t+1)-step constraints in order the achieve a SPE, which appears to be the central contribution of this work. The one-shot deviation principle still holds even if some players' actions are \"no-ops\" at certain decision points. I did not read the proofs or experiments closely since I believe there are flaws in the central idea of this work. I'd be happy to do a more thorough review if I am incorrect about these central points.", "rating": "1: Reject", "reply_text": "Thank you for your review and the time taken for it , below we address each of your questions in turn . Question # 1 : After reading Sections 1 and 2 , I believe that the authors have a misunderstanding of the relationship between Markov Games and Extensive Form Games . The fundamental difference between these two formalisms is that Markov Games aka Stochastic Games ( https : //en.wikipedia.org/wiki/Stochastic_game ) are fully observed while EFGs in general are not fully observed . EFGs are called Partially-Observed Markov Games in the RL literature . `` Go '' is actually a MG , in contradiction with the authors ' statement in the intro . Response # 1 : Thanks for pointing this out , but we do not agree with your statement . The difference between Markov games vs. Extensive-form games has nothing to do with full vs partial observability . Markov games ( aka stochastic game ) are normal-form games with multiple stages , and at each stage all agents need to make simultaneous decisions . In Extensive-form games ( or extensive games ) , agents make asynchronous decisions over game stages ( See Page 89 Chapter 6.1.1 , line 1 in [ R1 ] , and [ R3 ] ) . On the other hand , fully vs. partially observable games are also called perfect information games vs. imperfect information games ( See [ R1 ] ) . For a game , being a perfect or imperfect information game is totally orthogonal to whether it is a Markov game or an extensive-form game . For example , [ R2 ] discussed that Markov games can be perfect information or imperfect information ; [ R1 ] discussed that extensive games can be perfect information ( Part 2 in [ R1 ] ) or imperfect information ( Part 3 in [ R1 ] ) . Chess is an extensive game with perfect information ( Page 100 in [ R1 ] ) , so is Go . BTW , we checked the wiki link provided by the reviewer , which does not seem to support the statement of \u201c stochastic games are fully observed \u201d at all . [ R1 ] Osborne , Martin J. , and Ariel Rubinstein . A course in game theory . MIT press , 1994. http : //ebour.com.ar/pdfs/A % 20Course % 20in % 20Game % 20Theory.pdf [ R2 ] Hansen , Eric A. , Daniel S. Bernstein , and Shlomo Zilberstein . `` Dynamic programming for partially observable stochastic games . '' In AAAI , vol . 4 , pp.709-715.2004 . https : //www.aaai.org/Papers/Workshops/2004/WS-04-08/WS04-08-005.pdf [ R3 ] Introduction to Game , University of Maryland , https : //www.cs.umd.edu/users/nau/game-theory/8 % 20Stochastic % 20games.pdf Question # 2 : In fact , turn-based games ( even when the turn order is dynamic/stochastic ) is easily handled by the MG formalism , by assigning `` no-op '' moves to players who are not active at this decision point . Response # 2 : Good point , but you can not simply model \u201c no-op \u201d move ( i.e. , no-participation ) as an additional action that the agent can choose in AMA-GAIL problem , because \u201c no-op \u201d ( i.e. , no-participation ) itself is out of control of agents , and it is purely controlled/governed by the environment ( e , g. , in a stochastic turn-based game , the environment may by chance block some agents from participating in the game in certain rounds ) . In fact , when we implemented MA-GAIL in evaluations , we took the \u201c no-participation \u201d as an action for agents , and Fig 2 ( a ) - ( c ) show the comparison results with AMA-GAIL , and BC , and our AMA-GAIL outperforms the other baselines . Question # 3 : I do n't understand the necessity of ( t+1 ) -step constraints in order the achieve a SPE . Response # 3 : Thanks for pointing this out . Given the AMA-RL problem with Subgame Perfect Equilibrium ( SPE ) constraints defined in eq . ( 8 ) - ( 9 ) , we are not using ( t+1 ) -step constraints to achieve the SPE . Instead , we use ( t+1 ) -step constraints to find the corresponding AMA-IRL problem eq . ( 12 ) , in a consistent form as MA-IRL in MA-GAIL ( Song et al.2018 ) and IRL in GAIL ( Ho et al.2016 ) .With eq . ( 12 ) , we can further formulate the AMA-RL $ \\circ $ AMA-IRL problem in ( Theorem 3 and eq . ( 13 ) ) and employ the GAN framework to solve it ."}, {"review_id": "Syx33erYwH-1", "review_text": "In this work, a multi-agent imitation learning algorithm for extensive Markov Games is proposed. Compared to Markov Games (MGs), extensive Markov Games (eMGs) introduces indicator variables, which means whether agents will participate in the game at the specific time step or not, and player function, which is a probability distribution of indicator variables given histories and assumed to be governed by the environment, not by the agents. Such a model allows us to consider asynchronous participation of agents, whereas MGs only consider synchronous participation, which is assumed in the existing multi-agent imitation learning algorithms such as MA-GAIL and MA-AIRL. The contribution of this submission can be summarized as follows. From a theoretical perspective, the submission extends the theorems in MA-GAIL to those in eMGs, where most of them deal with Lagrange multiplier, its meaning, and properties. Followed by Theorem1 and 2, authors define an extensive occupancy measure, a natural extension of occupancy measures in MGs, and cast a multi-agent imitation learning problem into extensive occupancy measure matching problem in Theorem 3. For a practical algorithm, AMA-GAIL is proposed and shown to have a performance gain relative to BC and MA-GAIL. The submission is highly interesting, but I think section 4 (Practical Asynchronous Multi-Agent Imitation Learning) and section 5 (Experiments) should be much clearly written. The followings are comments regarding those sections: - It seems that the key difference between MA-GAIL and AMA-GAIL is whether we consider the cost function when the indicator is 0 or not, but it\u2019s difficult to figure out just by comparing (3) (MA-GAIL objective) and (14) (AMA-GAIL objective) at the first glance. - Similarly in Appendix B, it\u2019s difficult to figure out the difference between MA-GAIL algorithm and AMA-GAIL except the fact that eMGs are assumed. I think some additional explanation is needed. - How did you generate expert trajectories in eMGs setting? Suppose there is an agent taking an action \u201c1\u201d at time t, but it was not applied to the dynamics because the indicator variable of the agent is equal to 0 at time t. In this case, what would be stored in the expert trajectories? \u201cNull\u201d or \u201c1\u201d? If the agent cannot take an action in advance (before looking at its indicator variable), I think adding indicator variables in a condition of policy, e.g., $\\pi(a|s, i)=pi(a|s)$ if $i=1$, otherwise $\\mathbb{I}\\{a=\"Null\"\\}$, is mathematically rigorous. - Assuming that experts\u2019 trajectories include \u201cNull\u201d actions, how did you use MA-GAIL and BC with those trajectories? - The performance of BC seems weird to me since adding lots of training data reduces supervised learning errors and can also reduce covariate shift problems in BC since the theorem tells us that the regret is bounded by (error) * (episode length) ^ 2 in the worst case [Ross and Bagnell, \u201cEfficient reductions for imitation learning\u201d]. Such a tendency is empirically shown in MA-GAIL paper as well, i.e., performance of BC increases as the amount of expert trajectories increases. Is there any reason, BC shows poor performance in eMGs? There are some minor comments: - In 2.1., $\\eta$ (initial state distribution) is not explicitly defined. - In 2.1., MGs assume each agent\u2019s reward function depends on other agents\u2019 actions as well as agents\u2019 own actions, but in the submission, rewards only depend on agents\u2019 own actions. This may be due to the asynchronous setting, but I think it should be mentioned in the paper. - In Definition 1, null action is describe as $0$, whereas it was defined as $\\phi$ in 2.1. - In a sentence below Definition 1, $\\eta(i)=1$ -> $\\zeta(i)=1$. - Below (14), we don\u2019t have full knowledge of transition P, but we can sample from it (like black-box model). ", "rating": "6: Weak Accept", "reply_text": "We sincerely appreciate your careful review of our work and the precise summarization of the paper . We are also grateful for your comments and have tried very hard to address your and other reviewers \u2019 concerns , as detailed in our point-by-point responses below . Question # 1 : I think section 4 ( Practical Asynchronous Multi-Agent Imitation Learning ) and section 5 ( Experiments ) should be much clearly written . Response # 1 : Thank you for your suggestion . We are revising our section 4 and 5 to make them more clear . We will make the updated version available over the weekend . Question # 2 : It seems that the key difference between MA-GAIL and AMA-GAIL is whether we consider the cost function when the indicator is 0 or not , but it \u2019 s difficult to figure out just by comparing ( 3 ) ( MA-GAIL objective ) and ( 14 ) ( AMA-GAIL objective ) at the first glance . Similarly in Appendix B , it \u2019 s difficult to figure out the difference between MA-GAIL algorithm and AMA-GAIL except for the fact that eMGs are assumed . I think some additional explanation is needed . Response # 2 : Thank you for pointing out our carelessness . The proposed AMA-GAIL is a more general problem to MA-GAIL , where diverse player participation scenarios are modeled by the introduced player function $ Y $ . In eq . ( 14 ) , the two expectations $ \\mathbb { E } $ should both be with a subscript of $ Y $ as $ \\mathbb { E } _ { \\pi_ { \\theta } , Y } $ , and $ \\mathbb { E } _ { \\pi_ { E } , Y } $ , respectively . Such an expectation definition is clearly defined in Sec 2.1 ( Extensive Markov Games ) , i.e. , the last sentence in Sec 2.1 . We will fix this in the updated version . Question # 3 : How did you generate expert trajectories in eMGs setting ? Suppose there is an agent taking an action \u201c 1 \u201d at time t , but it was not applied to the dynamics because the indicator variable of the agent is equal to 0 at time t. In this case , what would be stored in the expert trajectories ? \u201c Null \u201d or \u201c 1 \u201d ? If the agent can not take an action in advance ( before looking at its indicator variable ) , I think adding indicator variables in a condition of policy , e.g. , $ \\pi ( a|s , i ) = \\pi ( a|s ) $ if $ i=1 $ , otherwise $ a= $ '' Null '' , is mathematically rigorous . Response # 3 : Thanks for your questions and suggestions . It is \u201c null \u201d . Yes.As you mentioned , the agent can not take an action in advance ( before looking at its indicator variable ) . Yes.Adding indicator variables in a condition of policy is absolutely correct mathematically . We still prefer to separate them 1 ) for the consistency with MA-GAIL paper ( Song et al.2018 ) , using the same form in $ \\pi $ , and 2 ) for highlighting the difference from MA-GAIL , by the player function $ Y $ . Question # 4 : Assuming that experts \u2019 trajectories include \u201c Null \u201d actions , how did you use MA-GAIL and BC with those trajectories ? Response # 4 : In the implementation , we added \u201c null \u201d ( no-participation ) as an additional action to each agent \u2019 s action set in MA-GAIL and BC experiments . The detailed results are shown in Figure 2 , Table 1 , and Appendix C.1 & C.2 . They all show that our AMA-GAIL outperforms other baselines , i.e. , MA-GAIL and BC . This is because \u201c no-participation \u201d itself is out of control of agents , and it is purely controlled/governed by the environment ( e , g. , in a stochastic turn-based game , the environment may by chance block some agents from participating in the game in certain rounds ) . Question # 5 : The performance of BC seems weird to me since adding lots of training data reduces supervised learning errors and can also reduce covariate shift problems in BC since the theorem tells us that the regret is bounded by ( error ) * ( episode length ) ^ 2 in the worst case [ Ross and Bagnell , \u201c Efficient reductions for imitation learning \u201d ] . Such a tendency is empirically shown in MA-GAIL paper as well , i.e. , the performance of BC increases as the amount of expert trajectories increases . Is there any reason , BC shows poor performance in eMGs ? Response # 5 : Thanks for your comments and for providing the reference link to us . We will cite it in our updated version . As for the performance of BC , in Figure 2 ( b ) the performance is poor at the beginning but increases rapidly and then converges at around 0.65 with 300 demonstrations . This is , in fact , consistent with MA-GAIL work , because in MA-GAIL , they only evaluated up to 400 demonstrations , where we evaluated from 200 to 1000 demonstrations . Moreover , in Figure 2 ( a ) , deterministic cooperative navigation is easier to learn compared with the stochastic cooperative navigation game shown in Figure 2 ( b ) , since there is no randomness in the player function . The performance from the beginning ( 200 demonstrations ) has already stabilized at 0.7 ."}, {"review_id": "Syx33erYwH-2", "review_text": "The submission extends the MARL\u25e6MAIR to the extensive Markov game case, where the decisions are made asynchronously. As a result, a stronger equilibrium SPE is becomes the target of the proposed method. To this end, the submission takes advantage of the previous game theory results, to formulate the problem, and transform the model to a MAGAIL form. The empirical performance of the proposed method is demonstrated using experiments. I believe the submission considers an interesting and challenging problem, and has extended the existing multi-agent IRL methods to the extensive Markov game case. ", "rating": "6: Weak Accept", "reply_text": "We really appreciate your precise summarization along with positive remarks about our paper . We are encouraged to work hard to improve the quality of the paper ."}], "0": {"review_id": "Syx33erYwH-0", "review_text": " After reading Sections 1 and 2, I believe that the authors have a misunderstanding of the relationship between Markov Games and Extensive Form Games. The fundamental difference between these two formalisms is that Markov Games aka Stochastic Games (https://en.wikipedia.org/wiki/Stochastic_game) are fully observed while EFGs in general are not fully observed. EFGs are called Partially-Observed Markov Games in the RL literature. \"Go\" is actually a MG, in contradiction with the authors' statement in the intro. The authors make an artificial distinction that \"turn-based\" games cannot be handled under the MG formalism. In fact, turn-based games (even when the turn order is dynamic/stochastic) is easily handled by the MG formalism, by assigning \"no-op\" moves to players who are not active at this decision point. Therefore, I don't see why any additional mechanics are needed to apply MA-GAIL to turn-based games. Similarly, I don't understand the necessity of (t+1)-step constraints in order the achieve a SPE, which appears to be the central contribution of this work. The one-shot deviation principle still holds even if some players' actions are \"no-ops\" at certain decision points. I did not read the proofs or experiments closely since I believe there are flaws in the central idea of this work. I'd be happy to do a more thorough review if I am incorrect about these central points.", "rating": "1: Reject", "reply_text": "Thank you for your review and the time taken for it , below we address each of your questions in turn . Question # 1 : After reading Sections 1 and 2 , I believe that the authors have a misunderstanding of the relationship between Markov Games and Extensive Form Games . The fundamental difference between these two formalisms is that Markov Games aka Stochastic Games ( https : //en.wikipedia.org/wiki/Stochastic_game ) are fully observed while EFGs in general are not fully observed . EFGs are called Partially-Observed Markov Games in the RL literature . `` Go '' is actually a MG , in contradiction with the authors ' statement in the intro . Response # 1 : Thanks for pointing this out , but we do not agree with your statement . The difference between Markov games vs. Extensive-form games has nothing to do with full vs partial observability . Markov games ( aka stochastic game ) are normal-form games with multiple stages , and at each stage all agents need to make simultaneous decisions . In Extensive-form games ( or extensive games ) , agents make asynchronous decisions over game stages ( See Page 89 Chapter 6.1.1 , line 1 in [ R1 ] , and [ R3 ] ) . On the other hand , fully vs. partially observable games are also called perfect information games vs. imperfect information games ( See [ R1 ] ) . For a game , being a perfect or imperfect information game is totally orthogonal to whether it is a Markov game or an extensive-form game . For example , [ R2 ] discussed that Markov games can be perfect information or imperfect information ; [ R1 ] discussed that extensive games can be perfect information ( Part 2 in [ R1 ] ) or imperfect information ( Part 3 in [ R1 ] ) . Chess is an extensive game with perfect information ( Page 100 in [ R1 ] ) , so is Go . BTW , we checked the wiki link provided by the reviewer , which does not seem to support the statement of \u201c stochastic games are fully observed \u201d at all . [ R1 ] Osborne , Martin J. , and Ariel Rubinstein . A course in game theory . MIT press , 1994. http : //ebour.com.ar/pdfs/A % 20Course % 20in % 20Game % 20Theory.pdf [ R2 ] Hansen , Eric A. , Daniel S. Bernstein , and Shlomo Zilberstein . `` Dynamic programming for partially observable stochastic games . '' In AAAI , vol . 4 , pp.709-715.2004 . https : //www.aaai.org/Papers/Workshops/2004/WS-04-08/WS04-08-005.pdf [ R3 ] Introduction to Game , University of Maryland , https : //www.cs.umd.edu/users/nau/game-theory/8 % 20Stochastic % 20games.pdf Question # 2 : In fact , turn-based games ( even when the turn order is dynamic/stochastic ) is easily handled by the MG formalism , by assigning `` no-op '' moves to players who are not active at this decision point . Response # 2 : Good point , but you can not simply model \u201c no-op \u201d move ( i.e. , no-participation ) as an additional action that the agent can choose in AMA-GAIL problem , because \u201c no-op \u201d ( i.e. , no-participation ) itself is out of control of agents , and it is purely controlled/governed by the environment ( e , g. , in a stochastic turn-based game , the environment may by chance block some agents from participating in the game in certain rounds ) . In fact , when we implemented MA-GAIL in evaluations , we took the \u201c no-participation \u201d as an action for agents , and Fig 2 ( a ) - ( c ) show the comparison results with AMA-GAIL , and BC , and our AMA-GAIL outperforms the other baselines . Question # 3 : I do n't understand the necessity of ( t+1 ) -step constraints in order the achieve a SPE . Response # 3 : Thanks for pointing this out . Given the AMA-RL problem with Subgame Perfect Equilibrium ( SPE ) constraints defined in eq . ( 8 ) - ( 9 ) , we are not using ( t+1 ) -step constraints to achieve the SPE . Instead , we use ( t+1 ) -step constraints to find the corresponding AMA-IRL problem eq . ( 12 ) , in a consistent form as MA-IRL in MA-GAIL ( Song et al.2018 ) and IRL in GAIL ( Ho et al.2016 ) .With eq . ( 12 ) , we can further formulate the AMA-RL $ \\circ $ AMA-IRL problem in ( Theorem 3 and eq . ( 13 ) ) and employ the GAN framework to solve it ."}, "1": {"review_id": "Syx33erYwH-1", "review_text": "In this work, a multi-agent imitation learning algorithm for extensive Markov Games is proposed. Compared to Markov Games (MGs), extensive Markov Games (eMGs) introduces indicator variables, which means whether agents will participate in the game at the specific time step or not, and player function, which is a probability distribution of indicator variables given histories and assumed to be governed by the environment, not by the agents. Such a model allows us to consider asynchronous participation of agents, whereas MGs only consider synchronous participation, which is assumed in the existing multi-agent imitation learning algorithms such as MA-GAIL and MA-AIRL. The contribution of this submission can be summarized as follows. From a theoretical perspective, the submission extends the theorems in MA-GAIL to those in eMGs, where most of them deal with Lagrange multiplier, its meaning, and properties. Followed by Theorem1 and 2, authors define an extensive occupancy measure, a natural extension of occupancy measures in MGs, and cast a multi-agent imitation learning problem into extensive occupancy measure matching problem in Theorem 3. For a practical algorithm, AMA-GAIL is proposed and shown to have a performance gain relative to BC and MA-GAIL. The submission is highly interesting, but I think section 4 (Practical Asynchronous Multi-Agent Imitation Learning) and section 5 (Experiments) should be much clearly written. The followings are comments regarding those sections: - It seems that the key difference between MA-GAIL and AMA-GAIL is whether we consider the cost function when the indicator is 0 or not, but it\u2019s difficult to figure out just by comparing (3) (MA-GAIL objective) and (14) (AMA-GAIL objective) at the first glance. - Similarly in Appendix B, it\u2019s difficult to figure out the difference between MA-GAIL algorithm and AMA-GAIL except the fact that eMGs are assumed. I think some additional explanation is needed. - How did you generate expert trajectories in eMGs setting? Suppose there is an agent taking an action \u201c1\u201d at time t, but it was not applied to the dynamics because the indicator variable of the agent is equal to 0 at time t. In this case, what would be stored in the expert trajectories? \u201cNull\u201d or \u201c1\u201d? If the agent cannot take an action in advance (before looking at its indicator variable), I think adding indicator variables in a condition of policy, e.g., $\\pi(a|s, i)=pi(a|s)$ if $i=1$, otherwise $\\mathbb{I}\\{a=\"Null\"\\}$, is mathematically rigorous. - Assuming that experts\u2019 trajectories include \u201cNull\u201d actions, how did you use MA-GAIL and BC with those trajectories? - The performance of BC seems weird to me since adding lots of training data reduces supervised learning errors and can also reduce covariate shift problems in BC since the theorem tells us that the regret is bounded by (error) * (episode length) ^ 2 in the worst case [Ross and Bagnell, \u201cEfficient reductions for imitation learning\u201d]. Such a tendency is empirically shown in MA-GAIL paper as well, i.e., performance of BC increases as the amount of expert trajectories increases. Is there any reason, BC shows poor performance in eMGs? There are some minor comments: - In 2.1., $\\eta$ (initial state distribution) is not explicitly defined. - In 2.1., MGs assume each agent\u2019s reward function depends on other agents\u2019 actions as well as agents\u2019 own actions, but in the submission, rewards only depend on agents\u2019 own actions. This may be due to the asynchronous setting, but I think it should be mentioned in the paper. - In Definition 1, null action is describe as $0$, whereas it was defined as $\\phi$ in 2.1. - In a sentence below Definition 1, $\\eta(i)=1$ -> $\\zeta(i)=1$. - Below (14), we don\u2019t have full knowledge of transition P, but we can sample from it (like black-box model). ", "rating": "6: Weak Accept", "reply_text": "We sincerely appreciate your careful review of our work and the precise summarization of the paper . We are also grateful for your comments and have tried very hard to address your and other reviewers \u2019 concerns , as detailed in our point-by-point responses below . Question # 1 : I think section 4 ( Practical Asynchronous Multi-Agent Imitation Learning ) and section 5 ( Experiments ) should be much clearly written . Response # 1 : Thank you for your suggestion . We are revising our section 4 and 5 to make them more clear . We will make the updated version available over the weekend . Question # 2 : It seems that the key difference between MA-GAIL and AMA-GAIL is whether we consider the cost function when the indicator is 0 or not , but it \u2019 s difficult to figure out just by comparing ( 3 ) ( MA-GAIL objective ) and ( 14 ) ( AMA-GAIL objective ) at the first glance . Similarly in Appendix B , it \u2019 s difficult to figure out the difference between MA-GAIL algorithm and AMA-GAIL except for the fact that eMGs are assumed . I think some additional explanation is needed . Response # 2 : Thank you for pointing out our carelessness . The proposed AMA-GAIL is a more general problem to MA-GAIL , where diverse player participation scenarios are modeled by the introduced player function $ Y $ . In eq . ( 14 ) , the two expectations $ \\mathbb { E } $ should both be with a subscript of $ Y $ as $ \\mathbb { E } _ { \\pi_ { \\theta } , Y } $ , and $ \\mathbb { E } _ { \\pi_ { E } , Y } $ , respectively . Such an expectation definition is clearly defined in Sec 2.1 ( Extensive Markov Games ) , i.e. , the last sentence in Sec 2.1 . We will fix this in the updated version . Question # 3 : How did you generate expert trajectories in eMGs setting ? Suppose there is an agent taking an action \u201c 1 \u201d at time t , but it was not applied to the dynamics because the indicator variable of the agent is equal to 0 at time t. In this case , what would be stored in the expert trajectories ? \u201c Null \u201d or \u201c 1 \u201d ? If the agent can not take an action in advance ( before looking at its indicator variable ) , I think adding indicator variables in a condition of policy , e.g. , $ \\pi ( a|s , i ) = \\pi ( a|s ) $ if $ i=1 $ , otherwise $ a= $ '' Null '' , is mathematically rigorous . Response # 3 : Thanks for your questions and suggestions . It is \u201c null \u201d . Yes.As you mentioned , the agent can not take an action in advance ( before looking at its indicator variable ) . Yes.Adding indicator variables in a condition of policy is absolutely correct mathematically . We still prefer to separate them 1 ) for the consistency with MA-GAIL paper ( Song et al.2018 ) , using the same form in $ \\pi $ , and 2 ) for highlighting the difference from MA-GAIL , by the player function $ Y $ . Question # 4 : Assuming that experts \u2019 trajectories include \u201c Null \u201d actions , how did you use MA-GAIL and BC with those trajectories ? Response # 4 : In the implementation , we added \u201c null \u201d ( no-participation ) as an additional action to each agent \u2019 s action set in MA-GAIL and BC experiments . The detailed results are shown in Figure 2 , Table 1 , and Appendix C.1 & C.2 . They all show that our AMA-GAIL outperforms other baselines , i.e. , MA-GAIL and BC . This is because \u201c no-participation \u201d itself is out of control of agents , and it is purely controlled/governed by the environment ( e , g. , in a stochastic turn-based game , the environment may by chance block some agents from participating in the game in certain rounds ) . Question # 5 : The performance of BC seems weird to me since adding lots of training data reduces supervised learning errors and can also reduce covariate shift problems in BC since the theorem tells us that the regret is bounded by ( error ) * ( episode length ) ^ 2 in the worst case [ Ross and Bagnell , \u201c Efficient reductions for imitation learning \u201d ] . Such a tendency is empirically shown in MA-GAIL paper as well , i.e. , the performance of BC increases as the amount of expert trajectories increases . Is there any reason , BC shows poor performance in eMGs ? Response # 5 : Thanks for your comments and for providing the reference link to us . We will cite it in our updated version . As for the performance of BC , in Figure 2 ( b ) the performance is poor at the beginning but increases rapidly and then converges at around 0.65 with 300 demonstrations . This is , in fact , consistent with MA-GAIL work , because in MA-GAIL , they only evaluated up to 400 demonstrations , where we evaluated from 200 to 1000 demonstrations . Moreover , in Figure 2 ( a ) , deterministic cooperative navigation is easier to learn compared with the stochastic cooperative navigation game shown in Figure 2 ( b ) , since there is no randomness in the player function . The performance from the beginning ( 200 demonstrations ) has already stabilized at 0.7 ."}, "2": {"review_id": "Syx33erYwH-2", "review_text": "The submission extends the MARL\u25e6MAIR to the extensive Markov game case, where the decisions are made asynchronously. As a result, a stronger equilibrium SPE is becomes the target of the proposed method. To this end, the submission takes advantage of the previous game theory results, to formulate the problem, and transform the model to a MAGAIL form. The empirical performance of the proposed method is demonstrated using experiments. I believe the submission considers an interesting and challenging problem, and has extended the existing multi-agent IRL methods to the extensive Markov game case. ", "rating": "6: Weak Accept", "reply_text": "We really appreciate your precise summarization along with positive remarks about our paper . We are encouraged to work hard to improve the quality of the paper ."}}