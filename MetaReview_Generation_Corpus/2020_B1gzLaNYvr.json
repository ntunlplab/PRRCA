{"year": "2020", "forum": "B1gzLaNYvr", "title": "TSInsight: A local-global attribution framework for interpretability in time-series data", "decision": "Reject", "meta_review": "Main content:\n\nBlind review #2 summarizes it well:\n\nThe aim of this work is to improve interpretability in time series prediction. To do so, they propose to use a relatively post-hoc procedure which learns a sparse representation informed by gradients of the prediction objective under a trained model. In particular, given a trained next-step classifier, they propose to train a sparse autoencoder with a combined objective of reconstruction and classification performance (while keeping the classifier fixed), so as to expose which features are useful for time series prediction.  Sparsity, and sparse auto-encoders, have been widely used for the end of interpretability. In this sense, the crux of the approach is very well motivated by the literature.\n\n--\n\nDiscussion:\n\nAll reviews had difficulties understanding the significance and novelty, which appears to have in large part arisen from the original submission not having sufficiently contextualized the motivation and strengths of the approach (especially for readers not already specialized in this exact subarea).\n\n--\n\nRecommendation and justification:\n\nThe reviews are uniformly low, probably due to the above factors, and while the authors' revisions during the rebuttal period have improved the objections, there are so many strong submissions that it would be difficult to justify override the very low reviewer scores.", "reviews": [{"review_id": "B1gzLaNYvr-0", "review_text": "In this paper, the authors proposed an algorithm for identifying important inputs for the time-series data as an explanation of the model's output. Given a fixed model, the authors proposed to put an auto-encoder to the input of the model, so that the input data is first transformed through the auto-encoder, and the transformed input is then fed to the model. In the proposed algorithm, the auto-encoder is trained so that (i) the prediction loss on the model's output to be small, (ii) the reconstruction loss of the auto-encoder to be small, and (iii) the transformed input of the auto-encoder to be sufficiently sparse (i.e. it has many zeros). I am not very sure if the proposed algorithm can generate reasonable explanation, for the following two reasons. First, the auto-encoder transforms the input into sparse, which can completely differ from any of the \"natural\" data, as shown in Fig1(b). I am not very sure whether studying the performance of the model for such an \"outlying\" input is informative. Second, it seems the authors implicitly assumed that zero input is irrelevant to the output of the model. However, zero input can have a certain meaning to the model, and thus naively introducing the sparsity to the model input may bias the model's output. I think the paper lacks deeper considerations on the use of sparsity. Thus, to me, the soundness of the proposed approach is not very clear to me. ### Updated after author response ### Through the communication with the author, I found that the author seems to be confusing the two different approaches *sparsifying the feature importance* and *sparsifying the input data*. This paper considers the latter approach, which can be biased as I mentioned above. The authors tried to justify the approach by raising related studies, which is however the mix of the two different approaches. I expect the author to clarify the distinction between the two approaches, and provide strong evidences that the sparsity is not a harm (if there is any).", "rating": "3: Weak Reject", "reply_text": "We would like to first thank the reviewer for spending his time on the perusal of our paper . Q : First , the auto-encoder transforms the input into sparse , which can completely differ from any of the `` natural '' data , as shown in Fig1 ( b ) . I am not very sure whether studying the performance of the model for such an `` outlying '' input is informative . R : The attribution methods have to attribute the importance to some particular locations while discarding the rest . So it is always assumed to be sparse . The sparsity that we induce is not a random one , but is very focused from the perspective of the classifier . In order to ensure that we don \u2019 t deviate from the natural distribution , we introduced reconstruction loss into the picture . Therefore , we think that the problem is perfectly justified and entirely conforms to the prior work in this direction . Q : Second , it seems the authors implicitly assumed that zero input is irrelevant to the output of the model . However , zero input can have a certain meaning to the model , and thus naively introducing the sparsity to the model input may bias the model 's output . R : We agree with the reviewer that the zero input can have certain meaning . Since there is no reliable test to quantify interpretability , this is very common in the literature [ 1 ] [ 2 ] . Therefore , this biasness exists in almost all of the methods that require a baseline input . [ 1 ] Fong , R. , Patrick , M. and Vedaldi , A. , 2019 . Understanding Deep Networks via Extremal Perturbations and Smooth Masks . In Proceedings of the IEEE International Conference on Computer Vision ( pp.2950-2958 ) . [ 2 ] Sundararajan , M. , Taly , A. and Yan , Q. , 2017 , August . Axiomatic attribution for deep networks . In Proceedings of the 34th International Conference on Machine Learning-Volume 70 ( pp.3319-3328 ) . JMLR.org.Q : I think the paper lacks deeper considerations on the use of sparsity . Thus , to me , the soundness of the proposed approach is not very clear to me . R : The reviewer should point out concrete instances where information is missing or incorrect so that we can clarify the confusion . \u201c I think it lack deeper considerations \u201d is not a well-defined argument which the authors can nullify ."}, {"review_id": "B1gzLaNYvr-1", "review_text": "The paper presents a new approach for improving the interpretability of deep learning methods used for time series. The is mainly concerned with classification tasks for time series. First, the classifier is learned in a usual way. Subsequently, a sparse auto-encoder is used that encodes the last layer of the classifier. For training the auto-encoder the classifier is fixed and there is a decoding loss as well as a sparsity loss. The sparse encoding of the last layer is supposed to increase the interpretability of the classification as it indicates which features are important for the classification. In general, I think this paper needs to be considerably improved in order to justify a publication. I am not convinced about the interpretability of the sparse extracted feature vector. It is not clear to me why this should be more interpretable then other methods. The paper contains many plots where the compare to other attributes methods, but it is not clear why the presented results should be any better as other methods (for example Fig 3). The paper is missing also a lot of motivation, the results are not well explained (e.g. Fig 3) and it needs to be improved in terms of writing. Equation 1 is not motivated (which is the main part of the paper) and it is not clear how Figure 2a has been generated and why this represented should be \" an interesting one, doesn\u2019t help with the interpretability of the model\". The authors have to improve the motivation part as well as the discussion of the results. More comments below: - The method seems to suffer from a severe parameter tuning problem, which makes it hard to use in practise. - It is unclear to me why the discriminator is fixed during training the encoder and decoder. Shouldnt it improve performance to also adapt the discriminator to the new representation. - Why can we not just add a layer with a sparsity constraint one layer before the \"encoded\" layer such that we have the same architecture and optimize that end to end? At least comparison to such an approach would be needed to justify something more complex. - The plots need to be better explained. While the comparisons seems to be exhaustive, it is already too many plots and it is very easy to get lost. Also, the quality of the plots need to be improved (e.g. font size) ", "rating": "1: Reject", "reply_text": "We would like to first thank the reviewer for spending his time on the perusal of our paper . Q : The method seems to suffer from a severe parameter tuning problem , which makes it hard to use in practise . R : We entirely disagree with the reviewer on this since this claim is totally unbacked by the reviewer . The only reason to opt for a range of different datasets is to show that the method is generic and applicable to a wide range of different datasets . Almost all of the interpretability methods with an optimization scheme rely on hyperparameters . Q1 : It is unclear to me why the discriminator is fixed during training the encoder and decoder . Should n't it improve performance to also adapt the discriminator to the new representation . Q2 : Why can we not just add a layer with a sparsity constraint one layer before the `` encoded '' layer such that we have the same architecture and optimize that end to end ? At least comparison to such an approach would be needed to justify something more complex . R : There are two major streams of research on interpretability which we discuss in the paper . The first stream focuses on explaining the decisions of pre-trained networks while the second one focuses on making the network itself interpretable . TSInsight is particularly focused on the first steam . Therefore , TSInsight takes a pre-trained model and tries to explain the decisions made by the network using an auto-encoder with sparsity inducing norm on top of it . The classifier remains intact since we just want to explain the decisions made by the classifier rather than coming up with an architecture that is itself explainable . However , it can be easily extended for the other case as the reviewer mentioned . But it wasn \u2019 t a focus of the current work and can be explored in detail in the future . Q : The plots need to be better explained . While the comparisons seems to be exhaustive , it is already too many plots and it is very easy to get lost . Also , the quality of the plots need to be improved ( e.g.font size ) R : We agree that the quality of the plots as well as the accompanying text can be improved . We will work on it to make everything clear . We also included high-resolution versions of these plots in the supplementary material . The reviewer can refer to them for now in case required ."}, {"review_id": "B1gzLaNYvr-2", "review_text": "The aim of this work is to improve interpretability in time series prediction. To do so, they propose to use a relatively post-hoc procedure which learns a sparse representation informed by gradients of the prediction objective under a trained model. In particular, given a trained next-step classifier, they propose to train a sparse autoencoder with a combined objective of reconstruction and classification performance (while keeping the classifier fixed), so as to expose which features are useful for time series prediction. Sparsity, and sparse auto-encoders, have been widely used for the end of interpretability. In this sense, the crux of the approach is very well motivated by the literature. * Pros * The work provides extensive comparison to a battery of other methods for model prediction interpretation. * The method is conceptually simple and is easy to implement. It is also general and can be applied to any prediction model (though this is more property of the sparse auto-encoder). * Despite its simplicity and generality, the method is shown to perform well on average, though it sometimes performs significantly worse than simple baselines. * Cons * The method itself is not explained very well. The authors use language such as \u201cattach the auto encoder to the classifier\u201d, which is a bit vague and could mean a number of things. It would be helpful if they provided either a formal definition of the model or a architectural diagram. * Though the quantitative evaluation is not entirely flattering, the authors should not be punished for providing experiments on many datasets. That said, if their contribution is then rather one of technical novelty, i.e. a sparse-autoencoder-based framework for time series interpretability, it would be helpful for them to * More formally define their framework / class of solutions * Provide a more in depth study of possible variants of the method (this is elaborated on in the \u201cQuestions\u201d section) * More strongly argue the novelty of their method * The authors provide a discussion on automatic hyper-parameter tuning that seems a bit out of place in the main method section, since it is not mentioned much thereafter and is claimed to not bring benefits. * The qualitative evaluation made by authors is rather vague: * \"Alongside the numbers, TSInsight was also able to produce the most plausible explanations\u201d * Additional Remarks * Why not train things jointly? Does this have to be done post-hoc? The authors state that they \u201cshould expect a drop in performance since the input distribution changes\u201d -> so why not at least try fine-tune and study the effect of training the classifier with sparse representations end-to-end? Exploring whether things can be trained jointly, or in other configurations, might allow the authors to frame their work as more of a general technical contribution. * It would be nice to have the simple baseline of a classifier with a sparsity constraint, i.e. * I.e. ablate the reconstruction loss I\u2019ve given a reject because 1) the explanation of the method is not very precise and could be greatly improved, 2) the quantitative evaluation is not sufficiently convincing, given the lack of technical novelty), and 3) the qualitative evaluation is hand-wavy. ", "rating": "1: Reject", "reply_text": "We would like to first thank the reviewer for spending his time on the perusal of our paper . Q : The method itself is not explained very well . The authors use language such as \u201c attach the auto encoder to the classifier \u201d , which is a bit vague and could mean a number of things . It would be helpful if they provided either a formal definition of the model or an architectural diagram . R : We tried to elaborate on what we meant by this throughout the paper . However , as the reviewer highlighted , a diagram is quite useful to avoid confusion . We have such a diagram for TSInsight which we moved to the supplementary material due to space constraints . We can adjust it back to the main text if the reviewers find it useful for the overall idea . Q : Though the quantitative evaluation is not entirely flattering , the authors should not be punished for providing experiments on many datasets . That said , if their contribution is then rather one of technical novelty , i.e.a sparse-autoencoder-based framework for time series interpretability , it would be helpful for them to \u2026 R : We spent more space than any other paper to clearly outline the previous work in this direction and how TSInsight is technically a novel solution to this problem . It is very easy to misguide the reviewer by just adding flattering cases . It is quite common in interpretability literature to selectively pick datasets where the method shines and avoid comparison against strong baselines . However , the reason for providing a detailed comparative study on a range of different datasets and almost all the commonly employed interpretability techniques is to show that although TSInsight provides the most plausible explanations on average , there is still a very big room for improvement . Q : The authors provide a discussion on automatic hyper-parameter tuning that seems a bit out of place in the main method section , since it is not mentioned much thereafter and is claimed to not bring benefits . R : Automated hyperparameter tuning is an important avenue for this work . However , we weren \u2019 t able to obtain any interesting results through it . It is mainly intended to provide a future direction for the work . Q : The qualitative evaluation made by authors is rather vague , `` Alongside the numbers , TSInsight was also able to produce the most plausible explanations \u201d . R : We didn \u2019 t intend to provide any accompanying text for that . The qualitative evaluation was based on the plots included for the user \u2019 s perusal as common in interpretability literature . Q : Why not train things jointly ? Does this have to be done post-hoc ? The authors state that they \u201c should expect a drop in performance since the input distribution changes \u201d - > so why not at least try fine-tune and study the effect of training the classifier with sparse representations end-to-end ? Exploring whether things can be trained jointly , or in other configurations , might allow the authors to frame their work as more of a general technical contribution . R : There are two major streams of research on interpretability which we discuss in the paper . The first stream focuses on explaining the decisions of pre-trained networks while the second one focuses on making the network itself interpretable . TSInsight is particularly focused on the first steam . Therefore , TSInsight takes a pre-trained model and tries to explain the decisions made by the network using an auto-encoder with sparsity inducing norm on top of it . The classifier remains intact since we just want to explain the decisions made by the classifier rather than coming up with an architecture that is itself explainable . However , it can be easily extended for the other case as the reviewer mentioned . But it wasn \u2019 t a focus of the current work and can be explored in detail in the future . Q : It would be nice to have the simple baseline of a classifier with a sparsity constraint , i.e.ablate the reconstruction loss R : We already show in the paper that removing the reconstruction loss destroys the method \u2019 s utility as an interpretability scheme . Since the aim of this work is to achieve interpretability , it doesn \u2019 t make sense to consider that as a baseline since the model won \u2019 t be providing attribution information . Overall : We agree that there is great room for improvement in terms of portraying the idea . However , we are quite disappointed with the reviewer \u2019 s comment on the lack of novelty and unconvincing quantitative evaluation . We compare against all the recent works in this direction and show that our method is the best on average . However , we would like to make it clear that there is no one method in the literature until this point that can provide reasonable explanations on any provided dataset . Since interpretability is itself a very tough domain to verify , we believe that our work provides one of the best coverage regarding the different attribution methods and their performance on a wide range of different datasets ."}], "0": {"review_id": "B1gzLaNYvr-0", "review_text": "In this paper, the authors proposed an algorithm for identifying important inputs for the time-series data as an explanation of the model's output. Given a fixed model, the authors proposed to put an auto-encoder to the input of the model, so that the input data is first transformed through the auto-encoder, and the transformed input is then fed to the model. In the proposed algorithm, the auto-encoder is trained so that (i) the prediction loss on the model's output to be small, (ii) the reconstruction loss of the auto-encoder to be small, and (iii) the transformed input of the auto-encoder to be sufficiently sparse (i.e. it has many zeros). I am not very sure if the proposed algorithm can generate reasonable explanation, for the following two reasons. First, the auto-encoder transforms the input into sparse, which can completely differ from any of the \"natural\" data, as shown in Fig1(b). I am not very sure whether studying the performance of the model for such an \"outlying\" input is informative. Second, it seems the authors implicitly assumed that zero input is irrelevant to the output of the model. However, zero input can have a certain meaning to the model, and thus naively introducing the sparsity to the model input may bias the model's output. I think the paper lacks deeper considerations on the use of sparsity. Thus, to me, the soundness of the proposed approach is not very clear to me. ### Updated after author response ### Through the communication with the author, I found that the author seems to be confusing the two different approaches *sparsifying the feature importance* and *sparsifying the input data*. This paper considers the latter approach, which can be biased as I mentioned above. The authors tried to justify the approach by raising related studies, which is however the mix of the two different approaches. I expect the author to clarify the distinction between the two approaches, and provide strong evidences that the sparsity is not a harm (if there is any).", "rating": "3: Weak Reject", "reply_text": "We would like to first thank the reviewer for spending his time on the perusal of our paper . Q : First , the auto-encoder transforms the input into sparse , which can completely differ from any of the `` natural '' data , as shown in Fig1 ( b ) . I am not very sure whether studying the performance of the model for such an `` outlying '' input is informative . R : The attribution methods have to attribute the importance to some particular locations while discarding the rest . So it is always assumed to be sparse . The sparsity that we induce is not a random one , but is very focused from the perspective of the classifier . In order to ensure that we don \u2019 t deviate from the natural distribution , we introduced reconstruction loss into the picture . Therefore , we think that the problem is perfectly justified and entirely conforms to the prior work in this direction . Q : Second , it seems the authors implicitly assumed that zero input is irrelevant to the output of the model . However , zero input can have a certain meaning to the model , and thus naively introducing the sparsity to the model input may bias the model 's output . R : We agree with the reviewer that the zero input can have certain meaning . Since there is no reliable test to quantify interpretability , this is very common in the literature [ 1 ] [ 2 ] . Therefore , this biasness exists in almost all of the methods that require a baseline input . [ 1 ] Fong , R. , Patrick , M. and Vedaldi , A. , 2019 . Understanding Deep Networks via Extremal Perturbations and Smooth Masks . In Proceedings of the IEEE International Conference on Computer Vision ( pp.2950-2958 ) . [ 2 ] Sundararajan , M. , Taly , A. and Yan , Q. , 2017 , August . Axiomatic attribution for deep networks . In Proceedings of the 34th International Conference on Machine Learning-Volume 70 ( pp.3319-3328 ) . JMLR.org.Q : I think the paper lacks deeper considerations on the use of sparsity . Thus , to me , the soundness of the proposed approach is not very clear to me . R : The reviewer should point out concrete instances where information is missing or incorrect so that we can clarify the confusion . \u201c I think it lack deeper considerations \u201d is not a well-defined argument which the authors can nullify ."}, "1": {"review_id": "B1gzLaNYvr-1", "review_text": "The paper presents a new approach for improving the interpretability of deep learning methods used for time series. The is mainly concerned with classification tasks for time series. First, the classifier is learned in a usual way. Subsequently, a sparse auto-encoder is used that encodes the last layer of the classifier. For training the auto-encoder the classifier is fixed and there is a decoding loss as well as a sparsity loss. The sparse encoding of the last layer is supposed to increase the interpretability of the classification as it indicates which features are important for the classification. In general, I think this paper needs to be considerably improved in order to justify a publication. I am not convinced about the interpretability of the sparse extracted feature vector. It is not clear to me why this should be more interpretable then other methods. The paper contains many plots where the compare to other attributes methods, but it is not clear why the presented results should be any better as other methods (for example Fig 3). The paper is missing also a lot of motivation, the results are not well explained (e.g. Fig 3) and it needs to be improved in terms of writing. Equation 1 is not motivated (which is the main part of the paper) and it is not clear how Figure 2a has been generated and why this represented should be \" an interesting one, doesn\u2019t help with the interpretability of the model\". The authors have to improve the motivation part as well as the discussion of the results. More comments below: - The method seems to suffer from a severe parameter tuning problem, which makes it hard to use in practise. - It is unclear to me why the discriminator is fixed during training the encoder and decoder. Shouldnt it improve performance to also adapt the discriminator to the new representation. - Why can we not just add a layer with a sparsity constraint one layer before the \"encoded\" layer such that we have the same architecture and optimize that end to end? At least comparison to such an approach would be needed to justify something more complex. - The plots need to be better explained. While the comparisons seems to be exhaustive, it is already too many plots and it is very easy to get lost. Also, the quality of the plots need to be improved (e.g. font size) ", "rating": "1: Reject", "reply_text": "We would like to first thank the reviewer for spending his time on the perusal of our paper . Q : The method seems to suffer from a severe parameter tuning problem , which makes it hard to use in practise . R : We entirely disagree with the reviewer on this since this claim is totally unbacked by the reviewer . The only reason to opt for a range of different datasets is to show that the method is generic and applicable to a wide range of different datasets . Almost all of the interpretability methods with an optimization scheme rely on hyperparameters . Q1 : It is unclear to me why the discriminator is fixed during training the encoder and decoder . Should n't it improve performance to also adapt the discriminator to the new representation . Q2 : Why can we not just add a layer with a sparsity constraint one layer before the `` encoded '' layer such that we have the same architecture and optimize that end to end ? At least comparison to such an approach would be needed to justify something more complex . R : There are two major streams of research on interpretability which we discuss in the paper . The first stream focuses on explaining the decisions of pre-trained networks while the second one focuses on making the network itself interpretable . TSInsight is particularly focused on the first steam . Therefore , TSInsight takes a pre-trained model and tries to explain the decisions made by the network using an auto-encoder with sparsity inducing norm on top of it . The classifier remains intact since we just want to explain the decisions made by the classifier rather than coming up with an architecture that is itself explainable . However , it can be easily extended for the other case as the reviewer mentioned . But it wasn \u2019 t a focus of the current work and can be explored in detail in the future . Q : The plots need to be better explained . While the comparisons seems to be exhaustive , it is already too many plots and it is very easy to get lost . Also , the quality of the plots need to be improved ( e.g.font size ) R : We agree that the quality of the plots as well as the accompanying text can be improved . We will work on it to make everything clear . We also included high-resolution versions of these plots in the supplementary material . The reviewer can refer to them for now in case required ."}, "2": {"review_id": "B1gzLaNYvr-2", "review_text": "The aim of this work is to improve interpretability in time series prediction. To do so, they propose to use a relatively post-hoc procedure which learns a sparse representation informed by gradients of the prediction objective under a trained model. In particular, given a trained next-step classifier, they propose to train a sparse autoencoder with a combined objective of reconstruction and classification performance (while keeping the classifier fixed), so as to expose which features are useful for time series prediction. Sparsity, and sparse auto-encoders, have been widely used for the end of interpretability. In this sense, the crux of the approach is very well motivated by the literature. * Pros * The work provides extensive comparison to a battery of other methods for model prediction interpretation. * The method is conceptually simple and is easy to implement. It is also general and can be applied to any prediction model (though this is more property of the sparse auto-encoder). * Despite its simplicity and generality, the method is shown to perform well on average, though it sometimes performs significantly worse than simple baselines. * Cons * The method itself is not explained very well. The authors use language such as \u201cattach the auto encoder to the classifier\u201d, which is a bit vague and could mean a number of things. It would be helpful if they provided either a formal definition of the model or a architectural diagram. * Though the quantitative evaluation is not entirely flattering, the authors should not be punished for providing experiments on many datasets. That said, if their contribution is then rather one of technical novelty, i.e. a sparse-autoencoder-based framework for time series interpretability, it would be helpful for them to * More formally define their framework / class of solutions * Provide a more in depth study of possible variants of the method (this is elaborated on in the \u201cQuestions\u201d section) * More strongly argue the novelty of their method * The authors provide a discussion on automatic hyper-parameter tuning that seems a bit out of place in the main method section, since it is not mentioned much thereafter and is claimed to not bring benefits. * The qualitative evaluation made by authors is rather vague: * \"Alongside the numbers, TSInsight was also able to produce the most plausible explanations\u201d * Additional Remarks * Why not train things jointly? Does this have to be done post-hoc? The authors state that they \u201cshould expect a drop in performance since the input distribution changes\u201d -> so why not at least try fine-tune and study the effect of training the classifier with sparse representations end-to-end? Exploring whether things can be trained jointly, or in other configurations, might allow the authors to frame their work as more of a general technical contribution. * It would be nice to have the simple baseline of a classifier with a sparsity constraint, i.e. * I.e. ablate the reconstruction loss I\u2019ve given a reject because 1) the explanation of the method is not very precise and could be greatly improved, 2) the quantitative evaluation is not sufficiently convincing, given the lack of technical novelty), and 3) the qualitative evaluation is hand-wavy. ", "rating": "1: Reject", "reply_text": "We would like to first thank the reviewer for spending his time on the perusal of our paper . Q : The method itself is not explained very well . The authors use language such as \u201c attach the auto encoder to the classifier \u201d , which is a bit vague and could mean a number of things . It would be helpful if they provided either a formal definition of the model or an architectural diagram . R : We tried to elaborate on what we meant by this throughout the paper . However , as the reviewer highlighted , a diagram is quite useful to avoid confusion . We have such a diagram for TSInsight which we moved to the supplementary material due to space constraints . We can adjust it back to the main text if the reviewers find it useful for the overall idea . Q : Though the quantitative evaluation is not entirely flattering , the authors should not be punished for providing experiments on many datasets . That said , if their contribution is then rather one of technical novelty , i.e.a sparse-autoencoder-based framework for time series interpretability , it would be helpful for them to \u2026 R : We spent more space than any other paper to clearly outline the previous work in this direction and how TSInsight is technically a novel solution to this problem . It is very easy to misguide the reviewer by just adding flattering cases . It is quite common in interpretability literature to selectively pick datasets where the method shines and avoid comparison against strong baselines . However , the reason for providing a detailed comparative study on a range of different datasets and almost all the commonly employed interpretability techniques is to show that although TSInsight provides the most plausible explanations on average , there is still a very big room for improvement . Q : The authors provide a discussion on automatic hyper-parameter tuning that seems a bit out of place in the main method section , since it is not mentioned much thereafter and is claimed to not bring benefits . R : Automated hyperparameter tuning is an important avenue for this work . However , we weren \u2019 t able to obtain any interesting results through it . It is mainly intended to provide a future direction for the work . Q : The qualitative evaluation made by authors is rather vague , `` Alongside the numbers , TSInsight was also able to produce the most plausible explanations \u201d . R : We didn \u2019 t intend to provide any accompanying text for that . The qualitative evaluation was based on the plots included for the user \u2019 s perusal as common in interpretability literature . Q : Why not train things jointly ? Does this have to be done post-hoc ? The authors state that they \u201c should expect a drop in performance since the input distribution changes \u201d - > so why not at least try fine-tune and study the effect of training the classifier with sparse representations end-to-end ? Exploring whether things can be trained jointly , or in other configurations , might allow the authors to frame their work as more of a general technical contribution . R : There are two major streams of research on interpretability which we discuss in the paper . The first stream focuses on explaining the decisions of pre-trained networks while the second one focuses on making the network itself interpretable . TSInsight is particularly focused on the first steam . Therefore , TSInsight takes a pre-trained model and tries to explain the decisions made by the network using an auto-encoder with sparsity inducing norm on top of it . The classifier remains intact since we just want to explain the decisions made by the classifier rather than coming up with an architecture that is itself explainable . However , it can be easily extended for the other case as the reviewer mentioned . But it wasn \u2019 t a focus of the current work and can be explored in detail in the future . Q : It would be nice to have the simple baseline of a classifier with a sparsity constraint , i.e.ablate the reconstruction loss R : We already show in the paper that removing the reconstruction loss destroys the method \u2019 s utility as an interpretability scheme . Since the aim of this work is to achieve interpretability , it doesn \u2019 t make sense to consider that as a baseline since the model won \u2019 t be providing attribution information . Overall : We agree that there is great room for improvement in terms of portraying the idea . However , we are quite disappointed with the reviewer \u2019 s comment on the lack of novelty and unconvincing quantitative evaluation . We compare against all the recent works in this direction and show that our method is the best on average . However , we would like to make it clear that there is no one method in the literature until this point that can provide reasonable explanations on any provided dataset . Since interpretability is itself a very tough domain to verify , we believe that our work provides one of the best coverage regarding the different attribution methods and their performance on a wide range of different datasets ."}}