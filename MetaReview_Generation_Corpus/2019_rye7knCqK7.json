{"year": "2019", "forum": "rye7knCqK7", "title": "Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks", "decision": "Accept (Poster)", "meta_review": "All reviewers agree that the proposed is interesting and innovative. One reviewer argues that  some additional baseline comparisons could be beneficial and the other two suggest inclusion of additional explanations and discussions of the results. The authors\u2019 rebuttal alleviated most of the concerns. All reviewers are very appreciative of the quality of the work overall and recommend probable acceptance. I agree with this score and recommend this work for poster presentation at ICLR.", "reviews": [{"review_id": "rye7knCqK7-0", "review_text": "The authors propose a new network architecture for multi-agent reinforcement learning. The new architecture addresses three issues: (1) the applicability of existing algorithms to semi-cooperative or competitive settings; (2) the ability to use local rewards during agent training; (3) the credit assignment problem with global multi-agent rewards. The authors address these issues with a new architecture that is comprised of several LSTM controllers with tied weights that transmit a continuous vector to each other, and that are augment with a gating mechanism that allows them to abstain from communicating. I think that this paper makes a solid contribution over the existing literature. My main comments are the following: * I feel like the paper can be strengthened by comparing to additional baselines. The authors compare mainly to Sukhbataar et al., but I think a more detailed comparison to other approaches (e.g. Foerster et al.) * One of the advantages of this method is that it can be used in non-cooperative settings. I am not familiar with this regime, and I would like a better explanation about why we would train competing agent with the same controller, rather than using a different controller for each team. * In several experimental results, the proposed method seems to have significantly higher variance than the baselines. I would like to see some discussion about why it is the case. * Also, in some places (e.g. Table 1), the method is highlighted in bold, even though it doesn\u2019t actually outperform the baseline. Please correct this and only highlight the best method (if several methods are tied, either highlight them one, or don\u2019t highlight any). * Also, in some cases when the error bars contain the previous best result, I am not sure if we can say that the proposed method is obviously better.", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer , We thank you for your insightful and helpful comments . We have responded to your comments , suggestions and concerns inline below . We hope our answers clarify your concerns . 1.I feel like the paper can be strengthened by comparing to additional baselines . The authors compare mainly to Sukhbataar et al. , but I think a more detailed comparison to other approaches ( e.g.Foerster et al . ) . CommNet and Foerster et . al.are similar approaches based on continuous communication . But different from our framework , Foerster et . al.is based on Q-learning , which makes it difficult to perform a fair comparison since our approach is based on policy gradient . This is due to the fact that there is no straightforward way to compare the sample complexity of Q-learning and policy gradients method because of the replay buffer . For the same reason , we didn \u2019 t compare with other approaches in StarCraft multiagent systems because those were defined for different environments than ours making fair comparison even harder . 2.One of the advantages of this method is that it can be used in non-cooperative settings . I am not familiar with this regime , and I would like a better explanation about why we would train competing agent with the same controller , rather than using a different controller for each team . Using same controller over different controller leads to shared weights which is reasonable given that all of the agents are performing the same task . Now , instead of training multiple weights , training single weights leads to faster training times and convergence which is beneficial . Further , we would like to point that our framework is independent of weight sharing and can even be used in scenarios where weights are not shared . 3.In several experimental results , the proposed method seems to have significantly higher variance than the baselines . I would like to see some discussion about why it is the case . We agree with the reviewer about the significant variance in IC3Net than the baselines . We would like to reiterate our response to R1 on a similar question about variance : We have performed a lot of experiments on StarCraft and can attribute the significant variance to stochasticity in the environment . There are a huge number of possible states in which agents can end up due to millions of possible interactions and their results in StarCraft and we believe it is hard to learn each one of them . This stochasticity variance can even be seen in simple heuristics baselines like \u201c attack closest \u201d ( Win ratio 76.6 +- 8 calculated over 5 runs ) and is in-fact an indicator of how difficult is it to learn real-world scenarios which also have same amount of stochasticity . Albeit , one can ask why don \u2019 t we see similar variance in CommNet and others . We believe that this might be due to the fact that adding gating action increases the action-state-space combinations which yields better results while being difficult to learn sometimes . Other point to be noted is that this variance is generally seen when the Win % ( requires learning more states ) is above some particular threshold which is close to nothing in baselines . We have added a section 6.2.2 on variance to the paper . 4.Also , in some places ( e.g.Table 1 ) , the method is highlighted in bold , even though it doesn \u2019 t actually outperform the baseline . Please correct this and only highlight the best method ( if several methods are tied , either highlight them one , or don \u2019 t highlight any ) . We thank reviewer for pointing this out . We have fixed this in the updated version ."}, {"review_id": "rye7knCqK7-1", "review_text": "From a methodological perspective, this paper describes a simple bu clever learning architecture with individual agents able to decide when to communicate through a learned gating mechanism. Each agent is an LSTM able to decide at each time point which aspects of its internal state should be exposed to other agents through this gating mechanism. The presentation of this method is clear to a level that should allows the reader to implement this him/herself. It would be great if the code associated to this could be released but the presentation allows for reproducibility. The experiments are interesting as well. Experimental results are presented on 3 problems and compared with known baselines from the academic community. The obtained results do show the merit of the approach. That being said, while the experimental results are extensive, there are places that could benefit from more clarity. For instance, I have found section 4.2 a bit dry. For instance, I had to read the plots caption and the text several times to map get at the deductions made in 4.2. Given the importance of gating in this work, I recommend expanding on this a bit (if space allows it). Small note: in the caption for Figure 3, on the fourth line, did you mean (f) instead of (d) when arguing that agents stop communicating once they reach the prey ( or am I missing something here)? Also, would it be possible to provide more insights on why IC3Net is doing better than CommNet except for the Combat-10Mv3Ze task (last table before the conclusion, what makes this task harder for IC3Net)? Another observation is on the variance terms that are reported for IC3Net. They are often (not always but definitely in the last table before the conclusion) quite higher when compared to the values associated with the baselines. Can this be explained? Another small thing: please add captions to your tables (at least a table number; I think that Table 2 does not have a caption). Overall, the paper is well written, interesting. Addressing the questions raised above would definitely help me and probably the eventual readers better appreciate its quality. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "6.Another observation is on the variance terms that are reported for IC3Net . They are often ( not always but definitely in the last table before the conclusion ) quite higher when compared to the values associated with the baselines . Can this be explained ? We agree with the reviewer about the significant variance in case of IC3Net for StarCraft . We have performed a lot of experiments on StarCraft and can attribute the significant variance to stochasticity in the environment . There are a huge number of possible states in which agents can end up due to millions of possible interactions and their results in StarCraft and we believe it is hard to learn each one of them . This stochasticity variance can even be seen in simple heuristics baselines like \u201c attack closest \u201d and is in-fact an indicator of how difficult is it to learn real-world scenarios which also have same amount of stochasticity . Albeit , one can ask why don \u2019 t we see similar variance in CommNet and others . We believe that this might be due to the fact that adding gating action increases the action-state-space combinations which yields better results while being difficult to learn sometimes . Other point to be noted is that this variance is generally seen when the Win % ( requires learning more states ) is above some particular threshold which is close to nothing in baselines . We have added a section 6.2.2 on variance to the paper . 7.Another small thing : please add captions to your tables ( at least a table number ; I think that Table 2 does not have a caption ) . We have updated the paper to better reflect captions for the tables . Thanks for pointing this out ."}, {"review_id": "rye7knCqK7-2", "review_text": "This work is an extension to the work of Sukbaatar et al. (2016) with two main differences: 1) Selective communication: agents are able to decide whether they want to communicate. 2) Individualized reward: Agents receive individual rewards; therefore, agents are aware of their contribution towards the goal. These two new extensions enable their model to work in either cooperative or a mix of competitive and competitive/collaborative settings. The authors also claim these two extensions enable their model to converge faster and better. The paper is well written, easy to follow, and everything has been explained quite well. The experiments are competent in the sense that the authors ran their model in four different environments (predator and prey, traffic junction, StarCraft explore, and StarCraft combat). The comparison between their model with three baselines was extensive; they reported the mean and variance over different runs. I have some concerns regarding their method and the experiments which are brought up in the following: Method: In a non-fully-cooperative environment, sharing hidden state entirely as the only option for communicate is not very reasonable; I think something like sending a message is a better option and more realistic (e.g., something like the work of Mordatch & Abbeel, 2017) Experiment: The experiment \"StarCraft explore\" is similar to predator-prey; therefore, instead of explaining StarCraft explore, I would like to see how the model works in StarCraft combat. Right now, the authors explain a bit about the model performance in Starcraft combat, but I found the explanation confusing. Authors provide 3 baselines: 1) no communication, but IR 2) no communication, no IR 3) global communication, no IR (commNet) I think having a baseline that has global communication with IR can show the effect of selective communication better. There are some questions in the experiment section that have not been addressed very well. For example: Is there any difference between the results of table 1, if we look at the cooperative setup? Does their model outperform a model which has global communication with IR? Why do IRIC and IC work worst in the medium in comparison to hard in TJ in table1? Why is CommNet work worse than IRIC and IC in table 2?", "rating": "6: Marginally above acceptance threshold", "reply_text": "5.Why do IRIC and IC work worst in the medium in comparison to hard in TJ in table1 ? Our visualizations suggest that this is due to high final add-rate in case of medium version compared to hard version . Collisions happen much more often in medium version leading to less success rate ( an episode is considered failure if a collision happens ) compared to hard where initial add-rate is low to accommodate curriculum learning for hard version \u2019 s big grid size . The final add-rate in case of hard level is comparatively low to make sure that it is possible to pass a junction without a collision as with more entry points it is easy to collide even with a small add-rate . 6.Why is CommNet work worse than IRIC and IC in table 2 ? First , we need to notice is that IRIC is better than IC also overall , which points to the fact that individualized reward are better than global rewards in case of exploration . This makes sense because if agents cover more area and know how much they covered through their own contribution ( individual reward ) , it should lead to overall more coverage , compared to global rewards where agents can \u2019 t figure out their own coverage but instead overall one . Second , in case of CommNet , it is easy to communicate and get together . We observe this pattern in CommNet where agents first get together at a point and then start exploring from there which leads to slow exploration , but IC is better in this respect because it is hard to gather at single point which inherently leads to faster exploration than CommNet .. Third , the reward structure in mixed scenario doesn \u2019 t appreciate searching together which is not directly visible to CommNet and IC due to global rewards . Note : You can observe the pattern for CommNet we just talked about at https : //gfycat.com/IllustriousMarvelousKagu . This video has been generated using trained CommNet model on PP-Hard . Red \u2018 X \u2019 are predators and \u2018 P \u2019 is the prey to be found . We can observe the pattern where the agents get together to find the prey leading to slack eventually . We have updated our paper to reflect the answers in Appendix . Thanks for providing us a detailed review and insightful questions ."}], "0": {"review_id": "rye7knCqK7-0", "review_text": "The authors propose a new network architecture for multi-agent reinforcement learning. The new architecture addresses three issues: (1) the applicability of existing algorithms to semi-cooperative or competitive settings; (2) the ability to use local rewards during agent training; (3) the credit assignment problem with global multi-agent rewards. The authors address these issues with a new architecture that is comprised of several LSTM controllers with tied weights that transmit a continuous vector to each other, and that are augment with a gating mechanism that allows them to abstain from communicating. I think that this paper makes a solid contribution over the existing literature. My main comments are the following: * I feel like the paper can be strengthened by comparing to additional baselines. The authors compare mainly to Sukhbataar et al., but I think a more detailed comparison to other approaches (e.g. Foerster et al.) * One of the advantages of this method is that it can be used in non-cooperative settings. I am not familiar with this regime, and I would like a better explanation about why we would train competing agent with the same controller, rather than using a different controller for each team. * In several experimental results, the proposed method seems to have significantly higher variance than the baselines. I would like to see some discussion about why it is the case. * Also, in some places (e.g. Table 1), the method is highlighted in bold, even though it doesn\u2019t actually outperform the baseline. Please correct this and only highlight the best method (if several methods are tied, either highlight them one, or don\u2019t highlight any). * Also, in some cases when the error bars contain the previous best result, I am not sure if we can say that the proposed method is obviously better.", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer , We thank you for your insightful and helpful comments . We have responded to your comments , suggestions and concerns inline below . We hope our answers clarify your concerns . 1.I feel like the paper can be strengthened by comparing to additional baselines . The authors compare mainly to Sukhbataar et al. , but I think a more detailed comparison to other approaches ( e.g.Foerster et al . ) . CommNet and Foerster et . al.are similar approaches based on continuous communication . But different from our framework , Foerster et . al.is based on Q-learning , which makes it difficult to perform a fair comparison since our approach is based on policy gradient . This is due to the fact that there is no straightforward way to compare the sample complexity of Q-learning and policy gradients method because of the replay buffer . For the same reason , we didn \u2019 t compare with other approaches in StarCraft multiagent systems because those were defined for different environments than ours making fair comparison even harder . 2.One of the advantages of this method is that it can be used in non-cooperative settings . I am not familiar with this regime , and I would like a better explanation about why we would train competing agent with the same controller , rather than using a different controller for each team . Using same controller over different controller leads to shared weights which is reasonable given that all of the agents are performing the same task . Now , instead of training multiple weights , training single weights leads to faster training times and convergence which is beneficial . Further , we would like to point that our framework is independent of weight sharing and can even be used in scenarios where weights are not shared . 3.In several experimental results , the proposed method seems to have significantly higher variance than the baselines . I would like to see some discussion about why it is the case . We agree with the reviewer about the significant variance in IC3Net than the baselines . We would like to reiterate our response to R1 on a similar question about variance : We have performed a lot of experiments on StarCraft and can attribute the significant variance to stochasticity in the environment . There are a huge number of possible states in which agents can end up due to millions of possible interactions and their results in StarCraft and we believe it is hard to learn each one of them . This stochasticity variance can even be seen in simple heuristics baselines like \u201c attack closest \u201d ( Win ratio 76.6 +- 8 calculated over 5 runs ) and is in-fact an indicator of how difficult is it to learn real-world scenarios which also have same amount of stochasticity . Albeit , one can ask why don \u2019 t we see similar variance in CommNet and others . We believe that this might be due to the fact that adding gating action increases the action-state-space combinations which yields better results while being difficult to learn sometimes . Other point to be noted is that this variance is generally seen when the Win % ( requires learning more states ) is above some particular threshold which is close to nothing in baselines . We have added a section 6.2.2 on variance to the paper . 4.Also , in some places ( e.g.Table 1 ) , the method is highlighted in bold , even though it doesn \u2019 t actually outperform the baseline . Please correct this and only highlight the best method ( if several methods are tied , either highlight them one , or don \u2019 t highlight any ) . We thank reviewer for pointing this out . We have fixed this in the updated version ."}, "1": {"review_id": "rye7knCqK7-1", "review_text": "From a methodological perspective, this paper describes a simple bu clever learning architecture with individual agents able to decide when to communicate through a learned gating mechanism. Each agent is an LSTM able to decide at each time point which aspects of its internal state should be exposed to other agents through this gating mechanism. The presentation of this method is clear to a level that should allows the reader to implement this him/herself. It would be great if the code associated to this could be released but the presentation allows for reproducibility. The experiments are interesting as well. Experimental results are presented on 3 problems and compared with known baselines from the academic community. The obtained results do show the merit of the approach. That being said, while the experimental results are extensive, there are places that could benefit from more clarity. For instance, I have found section 4.2 a bit dry. For instance, I had to read the plots caption and the text several times to map get at the deductions made in 4.2. Given the importance of gating in this work, I recommend expanding on this a bit (if space allows it). Small note: in the caption for Figure 3, on the fourth line, did you mean (f) instead of (d) when arguing that agents stop communicating once they reach the prey ( or am I missing something here)? Also, would it be possible to provide more insights on why IC3Net is doing better than CommNet except for the Combat-10Mv3Ze task (last table before the conclusion, what makes this task harder for IC3Net)? Another observation is on the variance terms that are reported for IC3Net. They are often (not always but definitely in the last table before the conclusion) quite higher when compared to the values associated with the baselines. Can this be explained? Another small thing: please add captions to your tables (at least a table number; I think that Table 2 does not have a caption). Overall, the paper is well written, interesting. Addressing the questions raised above would definitely help me and probably the eventual readers better appreciate its quality. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "6.Another observation is on the variance terms that are reported for IC3Net . They are often ( not always but definitely in the last table before the conclusion ) quite higher when compared to the values associated with the baselines . Can this be explained ? We agree with the reviewer about the significant variance in case of IC3Net for StarCraft . We have performed a lot of experiments on StarCraft and can attribute the significant variance to stochasticity in the environment . There are a huge number of possible states in which agents can end up due to millions of possible interactions and their results in StarCraft and we believe it is hard to learn each one of them . This stochasticity variance can even be seen in simple heuristics baselines like \u201c attack closest \u201d and is in-fact an indicator of how difficult is it to learn real-world scenarios which also have same amount of stochasticity . Albeit , one can ask why don \u2019 t we see similar variance in CommNet and others . We believe that this might be due to the fact that adding gating action increases the action-state-space combinations which yields better results while being difficult to learn sometimes . Other point to be noted is that this variance is generally seen when the Win % ( requires learning more states ) is above some particular threshold which is close to nothing in baselines . We have added a section 6.2.2 on variance to the paper . 7.Another small thing : please add captions to your tables ( at least a table number ; I think that Table 2 does not have a caption ) . We have updated the paper to better reflect captions for the tables . Thanks for pointing this out ."}, "2": {"review_id": "rye7knCqK7-2", "review_text": "This work is an extension to the work of Sukbaatar et al. (2016) with two main differences: 1) Selective communication: agents are able to decide whether they want to communicate. 2) Individualized reward: Agents receive individual rewards; therefore, agents are aware of their contribution towards the goal. These two new extensions enable their model to work in either cooperative or a mix of competitive and competitive/collaborative settings. The authors also claim these two extensions enable their model to converge faster and better. The paper is well written, easy to follow, and everything has been explained quite well. The experiments are competent in the sense that the authors ran their model in four different environments (predator and prey, traffic junction, StarCraft explore, and StarCraft combat). The comparison between their model with three baselines was extensive; they reported the mean and variance over different runs. I have some concerns regarding their method and the experiments which are brought up in the following: Method: In a non-fully-cooperative environment, sharing hidden state entirely as the only option for communicate is not very reasonable; I think something like sending a message is a better option and more realistic (e.g., something like the work of Mordatch & Abbeel, 2017) Experiment: The experiment \"StarCraft explore\" is similar to predator-prey; therefore, instead of explaining StarCraft explore, I would like to see how the model works in StarCraft combat. Right now, the authors explain a bit about the model performance in Starcraft combat, but I found the explanation confusing. Authors provide 3 baselines: 1) no communication, but IR 2) no communication, no IR 3) global communication, no IR (commNet) I think having a baseline that has global communication with IR can show the effect of selective communication better. There are some questions in the experiment section that have not been addressed very well. For example: Is there any difference between the results of table 1, if we look at the cooperative setup? Does their model outperform a model which has global communication with IR? Why do IRIC and IC work worst in the medium in comparison to hard in TJ in table1? Why is CommNet work worse than IRIC and IC in table 2?", "rating": "6: Marginally above acceptance threshold", "reply_text": "5.Why do IRIC and IC work worst in the medium in comparison to hard in TJ in table1 ? Our visualizations suggest that this is due to high final add-rate in case of medium version compared to hard version . Collisions happen much more often in medium version leading to less success rate ( an episode is considered failure if a collision happens ) compared to hard where initial add-rate is low to accommodate curriculum learning for hard version \u2019 s big grid size . The final add-rate in case of hard level is comparatively low to make sure that it is possible to pass a junction without a collision as with more entry points it is easy to collide even with a small add-rate . 6.Why is CommNet work worse than IRIC and IC in table 2 ? First , we need to notice is that IRIC is better than IC also overall , which points to the fact that individualized reward are better than global rewards in case of exploration . This makes sense because if agents cover more area and know how much they covered through their own contribution ( individual reward ) , it should lead to overall more coverage , compared to global rewards where agents can \u2019 t figure out their own coverage but instead overall one . Second , in case of CommNet , it is easy to communicate and get together . We observe this pattern in CommNet where agents first get together at a point and then start exploring from there which leads to slow exploration , but IC is better in this respect because it is hard to gather at single point which inherently leads to faster exploration than CommNet .. Third , the reward structure in mixed scenario doesn \u2019 t appreciate searching together which is not directly visible to CommNet and IC due to global rewards . Note : You can observe the pattern for CommNet we just talked about at https : //gfycat.com/IllustriousMarvelousKagu . This video has been generated using trained CommNet model on PP-Hard . Red \u2018 X \u2019 are predators and \u2018 P \u2019 is the prey to be found . We can observe the pattern where the agents get together to find the prey leading to slack eventually . We have updated our paper to reflect the answers in Appendix . Thanks for providing us a detailed review and insightful questions ."}}