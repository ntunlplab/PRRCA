{"year": "2019", "forum": "Hkx-ii05FQ", "title": "The Cakewalk Method", "decision": "Reject", "meta_review": "The paper investigates a variant of the \"cross-entropy method\" (CME) for heuristic combinatorial optimization, based on stochastically improving a search distribution via policy optimization in a surrogate objective.\n\nUnfortunately, the reviewers unanimously recommended rejection, noting that the significance of the contribution over CME remains far from clear and insufficiently supported by the given evidence.  The experimental evaluation was unconvincing to all of the reviewers, particularly since only one artificial problem (clique finding) was considered in the paper (with an additional problem, k-medoid clustering, briefly and incompletely considered in the appendix).  Several additional concerns were raised about the experimental evaluation, which triggered lengthy author responses but really need to be properly handled in the paper itself:\n\n- The sensitivity of performance to the optimization algorithm is a concern and requires more detailed understanding so that reasonable choices can be made in practice.\n\n- The independence assumption between search components is an extreme simplification that limits the appeal and applicability of the proposed approach.  Even after author response, it remains unconvincing that an independent search distribution over subcomponents can be effective in challenging combinatorial spaces.  Concrete evidence on challenging problems would be a more effective evidence than discussion.\n\n- The comparisons omitted any tailored algorithms for the specific problems.  Even if the authors insist on only comparing to more \"general purpose\" methods, there is a large space of evolutionary and Bayesian optimization strategies that have been neglected from the comparison.  A justification is needed for such an omission (if indeed it is even justifiable).", "reviews": [{"review_id": "Hkx-ii05FQ-0", "review_text": "The paper proposes an approach to construct surrogate objectives for the effective application of policy gradient methods to combinatorial optimization without known neighborhood structure. The surrogate is constructed with the goal of reducing the need of hyper-parameter tuning and evaluated on a clique finding task. The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper. The proposed sampling distributions assumes independence between the random variables over which the authors optimize \u2014 I find it surprising that this leads to good empirical results are relatively little structure can be captured using this distribution. Can the authors elaborate on this? However, as also observed by the authors, the sampling distribution can also be replaced by more sophisticated distributions. The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the evaluation . Please see our detailed response to several recurring issues at https : //openreview.net/forum ? id=Hkx-ii05FQ & noteId=HygFbNmL6X . In that response we address the following issues : ( 1 ) We emphasize fundamental differences between Cakewalk and CE . ( 2 ) How the sampling distribution should not be considered as a part of Cakewalk , and that it is mostly provided as an example , and a basis for the reported experiments . ( 3 ) The experiments include results two tasks . Nonetheless , it appears the paper doesn \u2019 t convey this clearly , and we suggest two possible ways how to update the paper in this regard . Next , we \u2019 ll try to provide some intuition as to why a sampling distribution that assumes independence between the different dimensions can be useful in some cases . The simple explanation is that in some problems the conditional expectation of the objective given that some x_i=j is much better than for other values x_i=k . In such cases , for each dimension the algorithm will tend to sample values which are useful to many possible solutions . In the clique problem for example , if some node i is part of a large clique , then sampling x_i=1 is likely to result in a good objective as there are many nodes that are connected to i , and the chance of not sampling any of them decreases with the clique size . In this way , over time the probability for sampling such nodes becomes higher , and the chance of sampling all of them together increases . Lastly , we note that these kind of factorized distributions have a long history of being useful in machine learning . In a similar context to the one studied in the paper , such distributions have been studied by Rubinstein in his paper which discusses CE as an algorithm for combinatorial optimization , and in the classical bandit papers Exp3 is applied independently to several dimensions to study game theoretic problems . In different contexts , such distributions have also been used as naive mean field approximations in variational inference ."}, {"review_id": "Hkx-ii05FQ-1", "review_text": "The authors argue that not knowing the distribution of rewards observed in the policy gradient algorithm hinders learning (and the tuning process). They propose to replace the reward term in the policy gradient algorithm with its centered empirical cumulative distribution, which has a fixed and known U[-1, 1] distribution. They test their methods on a toy task that consists in finding inclusion maximal cliques (which tests for local optimality) against REINFORCE (including their variants: centering the rewards with a mean baseline or normalizing them), the cross-entropy method and Exp3. I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method. The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult. The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer. I would like the method to be applied on other domains such as continuous non-convex optimization and reinforcement learning. Additionally, I find the motivation for caring about local optimality unconvincing. I take exception that people care more about local optimality than the actual objective. From a practical point of view, local optimality is a mean (that can be achieved via heuristic algorithms) to an end (the objective itself). This also holds for k-means, which is usually run multiple times with different starting conditions. Some comments: - Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample. e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse) - It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion). - The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.) - I would also encourage the authors to come up with a more descriptive name for the approach. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their evaluation . Please see our response at https : //openreview.net/forum ? id=Hkx-ii05FQ & noteId=HygFbNmL6X , where we also discuss our experimental framework . Even though we present results on two tasks , it appears the paper structure doesn \u2019 t convey this clearly , and we suggest two possible ways how to update the paper in this regard . We note in this context that even though we would also like to see Cakewalk evaluated on the domains mentioned by the reviewer , these are not part of our own research agenda , and accordingly our suggestions refer to other problems in combinatorial optimization . Next , we address other issues raised by the reviewer . First , we \u2019 d like to emphasize that the clique problem studied in the paper is far from being a toy problem . All the algorithms are evaluated on the DIMACS clique dataset which was published as part of the second DIMACS challenge which specifically focused on combinatorial optimization . Over the years , this dataset has become a standard benchmark for clique finding algorithms , and results on it are regularly published . In this respect , this dataset is an important benchmark for clique algorithms very much like CIFAR10 and CIFAR100 are for image classification methods . Notably , Cakewalk approaches the performance of the best clique finding algorithms that directly search a graph , and which are tailored to this specific task . Note that none of the tested methods were given enough samples even to recover the graph itself , as most graphs have more than 100 nodes , and we \u2019 ve allowed only for 100 |V| samples in each execution . To us this seems as a rather challenging setup , not just for the algorithms we \u2019 ve tested in this paper , but for any clique finding algorithm . Next , we wonder how would the reviewer correct the confounds mentioned with regard to the clique experiment . Providing a controlled experiment is always challenging , though the elements mentioned by the reviewer were specifically selected as to reduce various confounds . The main research question we try to address is whether algorithms that only rely on function evaluations can recover locally optimal solutions . Since the objective is the only source of information for such algorithms , an all-or-none kind of objective would not be very useful . Instead , the objective is designed in a manner that provides information even for partial solutions , thus allowing the tested algorithms to gradually improve the objective . In terms of the sampling distribution , as our focus is on the update step , we decided to use the simplest possible sampling distribution we can think of . In such a regime , we can attribute any performance gains to the algorithms themselves , and not to any prior knowledge that is reflected by the structure of some complex sampling distribution . Next , we agree that local optimality is a mean rather than a goal ( the objective itself ) . Nonetheless , as in the problems we seek to address the global optimum can not be found in polynomial time , the second best approach is first to design a method that can recover locally optimal solutions . Once such a method is available , repeated applications of that method can allow one to select a good solution , very much like the standard practice of repeated applications of k-means which the reviewer mentions . This reasoning however is dependent on a method \u2019 s capability of recovering locally optimal solutions , and therefore studying this ability makes for a worthwhile effort . Answers to the last comments : - Table 3 is indeed confusing , this is a good point . We will correct it . - Methods that apply a surrogate objective work best with AdaGrad . In this case , our data is a classical use which is explored in the AdaGrad paper uses as a motivating example . Not surprisingly , both Cakewalk and OCE work best with it . REINFORCE however is sensitive to the objective values , and it appears that Adam somewhat mitigates this problem . However , this is not as effective as applying a surrogate objective , and REINF_Z with Adam is outperformed by OCE_0.1 with AdaGrad in all measures . - Our frame of reference were algorithms that could be applied to any combinatorial problem , and which only rely on function evaluations . Control variates and reward shaping methods are mostly useful when tied to the particularities of a given objective , and thus do not fall into this category . In neural combinatorial optimization the study is focused on designing a sampling distribution that reflects some prior knowledge about a problem , and thus , we consider this line of work as orthogonal to ours . Having said that , we see how these areas might seem related , and we will revise the related work section to better emphasize the aforementioned differences . - We selected the name \u2018 Cakewalk \u2019 after consulting with a few colleagues . Following a joint discussion , we concluded that this name has the best chance for increasing our work \u2019 s impact ."}, {"review_id": "Hkx-ii05FQ-2", "review_text": "## Summary ## The authors apply policy gradients to combinatorial optimization problems. They suggest a surrogate reward function that mitigates the variance in the reward, and hence the update size. They demonstrate performance on a clique-finding problem. ## Assessment ## I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR. I also have concerns about the independence assumption in their sampling distribution (Section 3.2), and the fact that their experiments use the same set of (untuned) hyperparameters for each method. They both approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF. ## Specific Comments and Questions ## 1. Cakewalk is *very* closely related to the cross-entropy method. The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it. Both Cakewalk and CE approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF. 2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems. Consider $x$ a binary vector and reward equal to the parity $S(x) = \\sum{x_j} % 2$. 3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4). Is there any explanation for this? 4. How were the hyperparameters (learning rate, AdaGrad $\\delta$, Adam $\\beta_1, \\beta_2$) chosen? It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function. I would suggest tuning these values for each method independently. 5. It would be nice to see experimental results on more than one problem. The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet. 6. In Table 3, the figure in bold is not the lowest (best) in the table. The reason for this is only given in a single sentence at the end of Section 6, so it is a little confusing. I would replace these values with N/A or something similar.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Except for the learning rate , all the hyper-parameters were chosen according to the values suggested by the authors of AdaGrad , and Adam . The learning rate was chosen as 1/K , with K=100 being the number of examples used to estimate the CDF . As our stated goal is to present an algorithm which can be blindly applied with some fixed set of hyper-parameters to any possible objective , one of the goals of the experiments is to show that in such a setting some methods will work , while others will fail . Thus , as a controlled experiment for this hypothesis , we first fixed the set of all hyper-parameters for all methods , and then proceeded to apply them to various problems . In this setting therefore , tuning the learning rate or any other hyper-parameter for that matter will compromise the validity of our results . Regarding table 3 , we accept the reviewer \u2019 s suggestion , this is a good point . We particularly like the suggestion of writing NA or some such value , and we will use it to correct the paper ."}], "0": {"review_id": "Hkx-ii05FQ-0", "review_text": "The paper proposes an approach to construct surrogate objectives for the effective application of policy gradient methods to combinatorial optimization without known neighborhood structure. The surrogate is constructed with the goal of reducing the need of hyper-parameter tuning and evaluated on a clique finding task. The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper. The proposed sampling distributions assumes independence between the random variables over which the authors optimize \u2014 I find it surprising that this leads to good empirical results are relatively little structure can be captured using this distribution. Can the authors elaborate on this? However, as also observed by the authors, the sampling distribution can also be replaced by more sophisticated distributions. The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the evaluation . Please see our detailed response to several recurring issues at https : //openreview.net/forum ? id=Hkx-ii05FQ & noteId=HygFbNmL6X . In that response we address the following issues : ( 1 ) We emphasize fundamental differences between Cakewalk and CE . ( 2 ) How the sampling distribution should not be considered as a part of Cakewalk , and that it is mostly provided as an example , and a basis for the reported experiments . ( 3 ) The experiments include results two tasks . Nonetheless , it appears the paper doesn \u2019 t convey this clearly , and we suggest two possible ways how to update the paper in this regard . Next , we \u2019 ll try to provide some intuition as to why a sampling distribution that assumes independence between the different dimensions can be useful in some cases . The simple explanation is that in some problems the conditional expectation of the objective given that some x_i=j is much better than for other values x_i=k . In such cases , for each dimension the algorithm will tend to sample values which are useful to many possible solutions . In the clique problem for example , if some node i is part of a large clique , then sampling x_i=1 is likely to result in a good objective as there are many nodes that are connected to i , and the chance of not sampling any of them decreases with the clique size . In this way , over time the probability for sampling such nodes becomes higher , and the chance of sampling all of them together increases . Lastly , we note that these kind of factorized distributions have a long history of being useful in machine learning . In a similar context to the one studied in the paper , such distributions have been studied by Rubinstein in his paper which discusses CE as an algorithm for combinatorial optimization , and in the classical bandit papers Exp3 is applied independently to several dimensions to study game theoretic problems . In different contexts , such distributions have also been used as naive mean field approximations in variational inference ."}, "1": {"review_id": "Hkx-ii05FQ-1", "review_text": "The authors argue that not knowing the distribution of rewards observed in the policy gradient algorithm hinders learning (and the tuning process). They propose to replace the reward term in the policy gradient algorithm with its centered empirical cumulative distribution, which has a fixed and known U[-1, 1] distribution. They test their methods on a toy task that consists in finding inclusion maximal cliques (which tests for local optimality) against REINFORCE (including their variants: centering the rewards with a mean baseline or normalizing them), the cross-entropy method and Exp3. I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method. The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult. The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer. I would like the method to be applied on other domains such as continuous non-convex optimization and reinforcement learning. Additionally, I find the motivation for caring about local optimality unconvincing. I take exception that people care more about local optimality than the actual objective. From a practical point of view, local optimality is a mean (that can be achieved via heuristic algorithms) to an end (the objective itself). This also holds for k-means, which is usually run multiple times with different starting conditions. Some comments: - Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample. e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse) - It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion). - The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.) - I would also encourage the authors to come up with a more descriptive name for the approach. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their evaluation . Please see our response at https : //openreview.net/forum ? id=Hkx-ii05FQ & noteId=HygFbNmL6X , where we also discuss our experimental framework . Even though we present results on two tasks , it appears the paper structure doesn \u2019 t convey this clearly , and we suggest two possible ways how to update the paper in this regard . We note in this context that even though we would also like to see Cakewalk evaluated on the domains mentioned by the reviewer , these are not part of our own research agenda , and accordingly our suggestions refer to other problems in combinatorial optimization . Next , we address other issues raised by the reviewer . First , we \u2019 d like to emphasize that the clique problem studied in the paper is far from being a toy problem . All the algorithms are evaluated on the DIMACS clique dataset which was published as part of the second DIMACS challenge which specifically focused on combinatorial optimization . Over the years , this dataset has become a standard benchmark for clique finding algorithms , and results on it are regularly published . In this respect , this dataset is an important benchmark for clique algorithms very much like CIFAR10 and CIFAR100 are for image classification methods . Notably , Cakewalk approaches the performance of the best clique finding algorithms that directly search a graph , and which are tailored to this specific task . Note that none of the tested methods were given enough samples even to recover the graph itself , as most graphs have more than 100 nodes , and we \u2019 ve allowed only for 100 |V| samples in each execution . To us this seems as a rather challenging setup , not just for the algorithms we \u2019 ve tested in this paper , but for any clique finding algorithm . Next , we wonder how would the reviewer correct the confounds mentioned with regard to the clique experiment . Providing a controlled experiment is always challenging , though the elements mentioned by the reviewer were specifically selected as to reduce various confounds . The main research question we try to address is whether algorithms that only rely on function evaluations can recover locally optimal solutions . Since the objective is the only source of information for such algorithms , an all-or-none kind of objective would not be very useful . Instead , the objective is designed in a manner that provides information even for partial solutions , thus allowing the tested algorithms to gradually improve the objective . In terms of the sampling distribution , as our focus is on the update step , we decided to use the simplest possible sampling distribution we can think of . In such a regime , we can attribute any performance gains to the algorithms themselves , and not to any prior knowledge that is reflected by the structure of some complex sampling distribution . Next , we agree that local optimality is a mean rather than a goal ( the objective itself ) . Nonetheless , as in the problems we seek to address the global optimum can not be found in polynomial time , the second best approach is first to design a method that can recover locally optimal solutions . Once such a method is available , repeated applications of that method can allow one to select a good solution , very much like the standard practice of repeated applications of k-means which the reviewer mentions . This reasoning however is dependent on a method \u2019 s capability of recovering locally optimal solutions , and therefore studying this ability makes for a worthwhile effort . Answers to the last comments : - Table 3 is indeed confusing , this is a good point . We will correct it . - Methods that apply a surrogate objective work best with AdaGrad . In this case , our data is a classical use which is explored in the AdaGrad paper uses as a motivating example . Not surprisingly , both Cakewalk and OCE work best with it . REINFORCE however is sensitive to the objective values , and it appears that Adam somewhat mitigates this problem . However , this is not as effective as applying a surrogate objective , and REINF_Z with Adam is outperformed by OCE_0.1 with AdaGrad in all measures . - Our frame of reference were algorithms that could be applied to any combinatorial problem , and which only rely on function evaluations . Control variates and reward shaping methods are mostly useful when tied to the particularities of a given objective , and thus do not fall into this category . In neural combinatorial optimization the study is focused on designing a sampling distribution that reflects some prior knowledge about a problem , and thus , we consider this line of work as orthogonal to ours . Having said that , we see how these areas might seem related , and we will revise the related work section to better emphasize the aforementioned differences . - We selected the name \u2018 Cakewalk \u2019 after consulting with a few colleagues . Following a joint discussion , we concluded that this name has the best chance for increasing our work \u2019 s impact ."}, "2": {"review_id": "Hkx-ii05FQ-2", "review_text": "## Summary ## The authors apply policy gradients to combinatorial optimization problems. They suggest a surrogate reward function that mitigates the variance in the reward, and hence the update size. They demonstrate performance on a clique-finding problem. ## Assessment ## I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR. I also have concerns about the independence assumption in their sampling distribution (Section 3.2), and the fact that their experiments use the same set of (untuned) hyperparameters for each method. They both approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF. ## Specific Comments and Questions ## 1. Cakewalk is *very* closely related to the cross-entropy method. The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it. Both Cakewalk and CE approximate the reward CDF from K samples and use this to construct a surrogate reward. The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF. 2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems. Consider $x$ a binary vector and reward equal to the parity $S(x) = \\sum{x_j} % 2$. 3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4). Is there any explanation for this? 4. How were the hyperparameters (learning rate, AdaGrad $\\delta$, Adam $\\beta_1, \\beta_2$) chosen? It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function. I would suggest tuning these values for each method independently. 5. It would be nice to see experimental results on more than one problem. The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet. 6. In Table 3, the figure in bold is not the lowest (best) in the table. The reason for this is only given in a single sentence at the end of Section 6, so it is a little confusing. I would replace these values with N/A or something similar.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Except for the learning rate , all the hyper-parameters were chosen according to the values suggested by the authors of AdaGrad , and Adam . The learning rate was chosen as 1/K , with K=100 being the number of examples used to estimate the CDF . As our stated goal is to present an algorithm which can be blindly applied with some fixed set of hyper-parameters to any possible objective , one of the goals of the experiments is to show that in such a setting some methods will work , while others will fail . Thus , as a controlled experiment for this hypothesis , we first fixed the set of all hyper-parameters for all methods , and then proceeded to apply them to various problems . In this setting therefore , tuning the learning rate or any other hyper-parameter for that matter will compromise the validity of our results . Regarding table 3 , we accept the reviewer \u2019 s suggestion , this is a good point . We particularly like the suggestion of writing NA or some such value , and we will use it to correct the paper ."}}