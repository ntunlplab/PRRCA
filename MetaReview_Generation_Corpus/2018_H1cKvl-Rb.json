{"year": "2018", "forum": "H1cKvl-Rb", "title": "UCB EXPLORATION VIA Q-ENSEMBLES", "decision": "Reject", "meta_review": "The idea studied here is interesting, if incremental. The empirical results are not particularly stellar, but it's clear that the authors have done their best to provide reproducible and defensible results. A few sticking points: a) The use of the term 'UCB', as mentioned in an anonymous comment, is somewhat misleading. \"Approximate Confidence Interval\" might be less controversial; b) there are a number of recent research results on exploration that are worth paying attention to (Plappert et al, O'Donoghue et al.) and worth comparing to, and c) the theoretical results are not always justified or useful (e.g. Equation 9: the bound is trivial, posterior >= 0 or 1). ", "reviews": [{"review_id": "H1cKvl-Rb-0", "review_text": "This paper paper uses an ensemble of networks to represent the uncertainty in deep reinforcement learning. The algorithm then chooses optimistically over the distribution induced by the ensemble. This leads to improved learning / exploration, notably better than the similar approach bootstrapped DQN. There are several things to like about this paper: - It is a clear paper, with a simple message and experiments that back up the claims. - The proposed algorithm is simple and could be practical in a lot of settings and even non-DQN variants. - It is interesting that Bootstrapped DQN gets such poor performance, this suggests that it is very important in the original paper https://arxiv.org/abs/1602.04621 that \"ensemble voting\" is applied to the test evaluation... (why do you think this is by the way, do you think it has something to do with the data being *more* off-policy / diverse under a TS vs UCB scheme?) On the other hand: - The novelty/scope of this work is somewhat limited... this is more likely (valuable) incremental work than a game-changer. - Something feels wrong/hacky/incomplete about just doing \"ensemble\" for uncertainty without bootstrapping/randomization... if we had access to more powerful optimization techniques then this certainly wouldn't be sensible - I think that you should mention that you are heavily reliant on \"random initialization + SGD/Adam + specific network architecture\" to maintain this idea of uncertainty. For example, this wouldn't work for linear value functions! - I think the original bootstrapped DQN used \"ensemble voting\" at test time, so maybe you should change the labels or the way this is introduced/discussed. It's definitely very interesting that *essentially* the learning benefit is coming from ensembling (rather than \"raw\" bootstrapped DQN) and UCB still looks like it does better. - I'm not convinced that page 4 and the \"Bayesian\" derivation really add too much value to this paper... alternatively, maybe you could introduce the actual algorithm first (train K models in parallel) and then say \"this is similar to particle filter\" and add the mathematical derivation after, rather than as if it was some complex formula derived. If you want to reference some justification/theory for ensemble-based uncertainty approximation you might consider https://arxiv.org/pdf/1705.07347.pdf instead. - I think this paper might miss the point of the \"bigger\" problem of efficient exploration in RL... or even how to get \"deep\" exploration with deep RL. Yes this algorithm sees improvements across Atari, but it's not clear why/if this is a step change versus simply increasing the amount of replay or tuning the learning rate. (Actually I do believe this algorithm can demonstrate deep exploration... but it looks like we're not seeing the big improvements on the \"sub-human\" games you might hope.) Overall I do think this is a pretty good short paper/evaluation of UCB-ensembles on Atari. The scope/insight of the paper isn't groundbreaking, but I think it delivers a clear short message on the Atari benchmark. Perhaps this will encourage people to dig deeper into some of these issues... I vote accept. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer \u2019 s comments and address in the following : 1 . Bootstrapped DQN samples one Q network from the ensemble applies it for a whole episode for exploration . We hypothesize that this intuition of deep exploration , by consistently using one Q function in each episode , does not guarantee that each Q function \u2019 s exploration is beneficial nor efficient . For example , each Q function deviates from the ensembled Q and accumulates inefficiency in a long episode . Although our proposed methods use the same network structure of bootstrapped DQN , the goal is very different : we build exploration bonus based on the uncertainty or discrepancy of Q ensembles . In UCB exploration , exploration bonus is based on the uncertainty of Q ensembles and encourages the agent to reduce the uncertainty in Q values . 2.Ensemble uncertainty is due to Q networks being parametrized with deep neural networks , which introduces nonconvexity in Bellman update . Thus , even though the Q networks are trained with the same samples , their parameters do not converge to the same . We also experimented with training each Q network with independently sampled transitions from the reply buffer , and did not observe improved performance . We don \u2019 t think the optimization method ( SGD/Adam ) plays a key role . This phenomenon that bagging worsens the performance of deep ensembles is also observed in supervised training setting . [ Lee et al , 2015 ] observed that supervised learning trained with deep ensembles with random initializations perform better than bagging for deep ensembles . [ Balaji et al , 2017 ] used deep ensembles for uncertainty estimates and also observed that bagging deteriorated performance in their experiments . We will revise and clarify the source of uncertainty from the ensembles . 3.We will modify/shorten the derivation on pages 3 and 4 . 4.On efficient exploration in RL , our proposed two algorithms use the Q functions directly while prior works construct exploration bonus using state-visitation counts , which are not tied to the rewards that agents seek to maximize . Our goal is to construct methods that reduce the inefficiency of prior algorithms where learning can be wasted on visiting irrelevant states . Thus , by improving upon bootstrapped DQN and comparing with state-visitation count-based methods such as A3C+ , we demonstrate that this direction of exploration based on Q-values is promising , and different from hyperparameter tuning . Due to compute constraint , we trained the proposed algorithms on each game with 40 million frames , less than 200 million frames used in prior works . Thus games that typically require more frames to learn do not show big improvement in our experiments . References : S. Lee , S. Purushwalkam , M. Cogswell , D. Crandall , and D. Batra . Why M heads are better than one : Training a diverse ensemble of deep networks . arXiv preprint arXiv:1511.06314 , 2015 . Lakshminarayanan , Balaji , Alexander Pritzel , and Charles Blundell . `` Simple and scalable predictive uncertainty estimation using deep ensembles . '' Advances in Neural Information Processing Systems . 2017 ."}, {"review_id": "H1cKvl-Rb-1", "review_text": "The authors propose a new exploration algorithm for Deep RL. They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style. There is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I\u2019m not sure this is justified and necessary here. Moreover, the UCB strategy is generally not considered a Bayesian strategy, so I wasn\u2019t convinced by the link to Bayesian RL in this paper. I liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper). Some questions about the results: -How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise? -How does the distribution of Q values look like during different phases of learning? -Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2. -What\u2019s different between Alg 1 and bootstrapped DQN (other than the action selection)? Minor things: -Missing propto in Eq 7? -Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere\u2026 -it looks more a Bellman residual update as written in (11). ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer \u2019 s comment and address in the following : 1 . We will modify/shorten the derivation on pages 3 and 4 . 2.We observed the Q values for actions chosen according to Alg 1 and Alg 2 . These Q values correspond to good actions . During learning , the Q values for such good actions gradually increase . The discrepancies between the Q values also increase in absolute values . But normalized by the mean Q value from different Q networks , the discrepancies gradually decrease . 3.In Alg 1 and Alg 2 , epsilon-greedy is not used , such that we can isolate the effects of exploration using Ensemble Voting or UCB exploration only . We did not experiment with adding epsilon-greedy on top of Alg 1 , but agree that it will be an interesting experiment to see whether epsilon-greedy helps or hurts exploration on top of Alg 1 . 4.Besides action selection , bootstrapped DQN allows each Q network to be trained with different samples ( using a masking mechanism ) , even though in bootstrapped DQN \u2019 s Atari experiments , all Q networks are trained using the same samples . In Alg 1 , Q networks only use random initialization and trained with the same samples . We also experimented with training Q networks with independently drawn samples , which deteriorated the performance ."}, {"review_id": "H1cKvl-Rb-2", "review_text": "This paper introduces a number of different techniques for improving exploration in deep Q learning. The main technique is to use UCB (upper confidence bound) to speedup exploration. The authors also introduces \"Ensemble voting\" facilitate exploitation. This paper shows improvement over baselines. But does not seem to offer significant insight or dramatic improvement. The techniques introduced are a small permutation of previous results. The baselines are not particularly strong either. The paper appeared to have be rushed. The presentation is not always clear. I also have the following questions I hope the authors could help me with: 1. I failed to understand how Eqn (5). Could you please clarify. 2. What is the significance of the math introduced in section 3? All that was proposed was: (1) Majority voting, (2) UCB exploration. 3. Why comparing to A3C+ which is not necessarily better than A3C in final performance? 4. Why not comparing to Bootstrapped DQN since the proposed method is based on it? 5. Why is the proposed method better than Bootstrapped DQN, since UCB does not necessarily outperform Thompson sampling in the case of bandits? 6. If there is a section on INFOGAIN exploration, why not mention it in the main text?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer \u2019 s comments . We address in the following : 1 . We first comment that the improvement from our proposed methods is significant . We used a strong Double DQN baseline , which achieves competitive or better learning results trained with 40 million frames , compared with prior published results [ Van Hasselt , et al , 2016 ] trained with 200 million frames . Improvement of proposed methods is significant over this strong Double DQN baseline . Table 2 in Appendix B shows that Ensemble Voting performs better than Double DQN in 37 out of 49 games evaluated , and UCB Exploration performs better than Double DQN in 38 out of 49 games evaluated . In addition , UCB Exploration performs better than Ensemble Voting in 35 out of 49 games evaluated . We will include such comparison in the results section . 2.We also compared to bootstrapped DQN as shown in Figure 1 , Figure 2 , and the results Table 2 in Appendix B . 3.A3C+ represents one line of research comprised of multiple works where the agent constructs exploration bonus based on state visitation counts . As discussed in Section 2.2 , the exploration bonus from these methods does not depend on the reward , thus the exploration may focus on irrelevant aspects of the environment . In comparison , our exploration bonus depend on the Q values directly . We chose A3C+ to compare our method of reward-based exploration bonus against count-based exploration bonus and demonstrate that this reward/Q values-based approach of constructing exploration bonus is promising . 4.Bootstrapped DQN samples one Q network from the ensemble applies it for a whole episode for exploration . We hypothesize that this intuition of deep exploration , by consistently using one Q function in each episode , does not guarantee that exploration is beneficial nor efficient . For example , each Q function deviates from the ensembled-Q . Although our proposed methods use the same network structure of bootstrapped DQN , the goal is very different : we build exploration bonus based on the uncertainty or discrepancy of Q ensembles . UCB exploration bonus is based on the uncertainty of Q ensembles and encourages the agent to reduce the uncertainty in Q values . 5.The INFOGAIN section attempts another approach of exploration using Q-ensembles . However , the improvement of this method is less consistently across the board . This could be due to the approximations we made in constructing the INFOGAIN exploration bonus . We document the results of the experiment in the Appendix for potential future interest in this direction . 6.We will modify/shorten the derivation on pages 3 and 4 ."}], "0": {"review_id": "H1cKvl-Rb-0", "review_text": "This paper paper uses an ensemble of networks to represent the uncertainty in deep reinforcement learning. The algorithm then chooses optimistically over the distribution induced by the ensemble. This leads to improved learning / exploration, notably better than the similar approach bootstrapped DQN. There are several things to like about this paper: - It is a clear paper, with a simple message and experiments that back up the claims. - The proposed algorithm is simple and could be practical in a lot of settings and even non-DQN variants. - It is interesting that Bootstrapped DQN gets such poor performance, this suggests that it is very important in the original paper https://arxiv.org/abs/1602.04621 that \"ensemble voting\" is applied to the test evaluation... (why do you think this is by the way, do you think it has something to do with the data being *more* off-policy / diverse under a TS vs UCB scheme?) On the other hand: - The novelty/scope of this work is somewhat limited... this is more likely (valuable) incremental work than a game-changer. - Something feels wrong/hacky/incomplete about just doing \"ensemble\" for uncertainty without bootstrapping/randomization... if we had access to more powerful optimization techniques then this certainly wouldn't be sensible - I think that you should mention that you are heavily reliant on \"random initialization + SGD/Adam + specific network architecture\" to maintain this idea of uncertainty. For example, this wouldn't work for linear value functions! - I think the original bootstrapped DQN used \"ensemble voting\" at test time, so maybe you should change the labels or the way this is introduced/discussed. It's definitely very interesting that *essentially* the learning benefit is coming from ensembling (rather than \"raw\" bootstrapped DQN) and UCB still looks like it does better. - I'm not convinced that page 4 and the \"Bayesian\" derivation really add too much value to this paper... alternatively, maybe you could introduce the actual algorithm first (train K models in parallel) and then say \"this is similar to particle filter\" and add the mathematical derivation after, rather than as if it was some complex formula derived. If you want to reference some justification/theory for ensemble-based uncertainty approximation you might consider https://arxiv.org/pdf/1705.07347.pdf instead. - I think this paper might miss the point of the \"bigger\" problem of efficient exploration in RL... or even how to get \"deep\" exploration with deep RL. Yes this algorithm sees improvements across Atari, but it's not clear why/if this is a step change versus simply increasing the amount of replay or tuning the learning rate. (Actually I do believe this algorithm can demonstrate deep exploration... but it looks like we're not seeing the big improvements on the \"sub-human\" games you might hope.) Overall I do think this is a pretty good short paper/evaluation of UCB-ensembles on Atari. The scope/insight of the paper isn't groundbreaking, but I think it delivers a clear short message on the Atari benchmark. Perhaps this will encourage people to dig deeper into some of these issues... I vote accept. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer \u2019 s comments and address in the following : 1 . Bootstrapped DQN samples one Q network from the ensemble applies it for a whole episode for exploration . We hypothesize that this intuition of deep exploration , by consistently using one Q function in each episode , does not guarantee that each Q function \u2019 s exploration is beneficial nor efficient . For example , each Q function deviates from the ensembled Q and accumulates inefficiency in a long episode . Although our proposed methods use the same network structure of bootstrapped DQN , the goal is very different : we build exploration bonus based on the uncertainty or discrepancy of Q ensembles . In UCB exploration , exploration bonus is based on the uncertainty of Q ensembles and encourages the agent to reduce the uncertainty in Q values . 2.Ensemble uncertainty is due to Q networks being parametrized with deep neural networks , which introduces nonconvexity in Bellman update . Thus , even though the Q networks are trained with the same samples , their parameters do not converge to the same . We also experimented with training each Q network with independently sampled transitions from the reply buffer , and did not observe improved performance . We don \u2019 t think the optimization method ( SGD/Adam ) plays a key role . This phenomenon that bagging worsens the performance of deep ensembles is also observed in supervised training setting . [ Lee et al , 2015 ] observed that supervised learning trained with deep ensembles with random initializations perform better than bagging for deep ensembles . [ Balaji et al , 2017 ] used deep ensembles for uncertainty estimates and also observed that bagging deteriorated performance in their experiments . We will revise and clarify the source of uncertainty from the ensembles . 3.We will modify/shorten the derivation on pages 3 and 4 . 4.On efficient exploration in RL , our proposed two algorithms use the Q functions directly while prior works construct exploration bonus using state-visitation counts , which are not tied to the rewards that agents seek to maximize . Our goal is to construct methods that reduce the inefficiency of prior algorithms where learning can be wasted on visiting irrelevant states . Thus , by improving upon bootstrapped DQN and comparing with state-visitation count-based methods such as A3C+ , we demonstrate that this direction of exploration based on Q-values is promising , and different from hyperparameter tuning . Due to compute constraint , we trained the proposed algorithms on each game with 40 million frames , less than 200 million frames used in prior works . Thus games that typically require more frames to learn do not show big improvement in our experiments . References : S. Lee , S. Purushwalkam , M. Cogswell , D. Crandall , and D. Batra . Why M heads are better than one : Training a diverse ensemble of deep networks . arXiv preprint arXiv:1511.06314 , 2015 . Lakshminarayanan , Balaji , Alexander Pritzel , and Charles Blundell . `` Simple and scalable predictive uncertainty estimation using deep ensembles . '' Advances in Neural Information Processing Systems . 2017 ."}, "1": {"review_id": "H1cKvl-Rb-1", "review_text": "The authors propose a new exploration algorithm for Deep RL. They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style. There is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I\u2019m not sure this is justified and necessary here. Moreover, the UCB strategy is generally not considered a Bayesian strategy, so I wasn\u2019t convinced by the link to Bayesian RL in this paper. I liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper). Some questions about the results: -How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise? -How does the distribution of Q values look like during different phases of learning? -Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2. -What\u2019s different between Alg 1 and bootstrapped DQN (other than the action selection)? Minor things: -Missing propto in Eq 7? -Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere\u2026 -it looks more a Bellman residual update as written in (11). ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer \u2019 s comment and address in the following : 1 . We will modify/shorten the derivation on pages 3 and 4 . 2.We observed the Q values for actions chosen according to Alg 1 and Alg 2 . These Q values correspond to good actions . During learning , the Q values for such good actions gradually increase . The discrepancies between the Q values also increase in absolute values . But normalized by the mean Q value from different Q networks , the discrepancies gradually decrease . 3.In Alg 1 and Alg 2 , epsilon-greedy is not used , such that we can isolate the effects of exploration using Ensemble Voting or UCB exploration only . We did not experiment with adding epsilon-greedy on top of Alg 1 , but agree that it will be an interesting experiment to see whether epsilon-greedy helps or hurts exploration on top of Alg 1 . 4.Besides action selection , bootstrapped DQN allows each Q network to be trained with different samples ( using a masking mechanism ) , even though in bootstrapped DQN \u2019 s Atari experiments , all Q networks are trained using the same samples . In Alg 1 , Q networks only use random initialization and trained with the same samples . We also experimented with training Q networks with independently drawn samples , which deteriorated the performance ."}, "2": {"review_id": "H1cKvl-Rb-2", "review_text": "This paper introduces a number of different techniques for improving exploration in deep Q learning. The main technique is to use UCB (upper confidence bound) to speedup exploration. The authors also introduces \"Ensemble voting\" facilitate exploitation. This paper shows improvement over baselines. But does not seem to offer significant insight or dramatic improvement. The techniques introduced are a small permutation of previous results. The baselines are not particularly strong either. The paper appeared to have be rushed. The presentation is not always clear. I also have the following questions I hope the authors could help me with: 1. I failed to understand how Eqn (5). Could you please clarify. 2. What is the significance of the math introduced in section 3? All that was proposed was: (1) Majority voting, (2) UCB exploration. 3. Why comparing to A3C+ which is not necessarily better than A3C in final performance? 4. Why not comparing to Bootstrapped DQN since the proposed method is based on it? 5. Why is the proposed method better than Bootstrapped DQN, since UCB does not necessarily outperform Thompson sampling in the case of bandits? 6. If there is a section on INFOGAIN exploration, why not mention it in the main text?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer \u2019 s comments . We address in the following : 1 . We first comment that the improvement from our proposed methods is significant . We used a strong Double DQN baseline , which achieves competitive or better learning results trained with 40 million frames , compared with prior published results [ Van Hasselt , et al , 2016 ] trained with 200 million frames . Improvement of proposed methods is significant over this strong Double DQN baseline . Table 2 in Appendix B shows that Ensemble Voting performs better than Double DQN in 37 out of 49 games evaluated , and UCB Exploration performs better than Double DQN in 38 out of 49 games evaluated . In addition , UCB Exploration performs better than Ensemble Voting in 35 out of 49 games evaluated . We will include such comparison in the results section . 2.We also compared to bootstrapped DQN as shown in Figure 1 , Figure 2 , and the results Table 2 in Appendix B . 3.A3C+ represents one line of research comprised of multiple works where the agent constructs exploration bonus based on state visitation counts . As discussed in Section 2.2 , the exploration bonus from these methods does not depend on the reward , thus the exploration may focus on irrelevant aspects of the environment . In comparison , our exploration bonus depend on the Q values directly . We chose A3C+ to compare our method of reward-based exploration bonus against count-based exploration bonus and demonstrate that this reward/Q values-based approach of constructing exploration bonus is promising . 4.Bootstrapped DQN samples one Q network from the ensemble applies it for a whole episode for exploration . We hypothesize that this intuition of deep exploration , by consistently using one Q function in each episode , does not guarantee that exploration is beneficial nor efficient . For example , each Q function deviates from the ensembled-Q . Although our proposed methods use the same network structure of bootstrapped DQN , the goal is very different : we build exploration bonus based on the uncertainty or discrepancy of Q ensembles . UCB exploration bonus is based on the uncertainty of Q ensembles and encourages the agent to reduce the uncertainty in Q values . 5.The INFOGAIN section attempts another approach of exploration using Q-ensembles . However , the improvement of this method is less consistently across the board . This could be due to the approximations we made in constructing the INFOGAIN exploration bonus . We document the results of the experiment in the Appendix for potential future interest in this direction . 6.We will modify/shorten the derivation on pages 3 and 4 ."}}