{"year": "2019", "forum": "r1xyx3R9tQ", "title": "Prototypical Examples in Deep Learning: Metrics, Characteristics, and Utility", "decision": "Reject", "meta_review": "This paper considers \"prototypes\" in machine learning, in which a small subset of a dataset is selected as representative of the behavior of the models. The authors propose a number of desiderata, and outline the connections to existing approaches. Further, they carry out evaluation with user studies to compare them with human intuition, and empirical experiments to compare them to each other. The reviewers agreed that the search for more concrete definitions of prototypes is a worthy one, and they appreciated the user studies.\n\nThe reviewers and AC note the following potential weaknesses: (1) the specific description of prototypes that the authors are using is not provided precisely, (2) the desiderata was found to be informal, leading to considerable confusion regarding the choices that are made and their compatibility with each other, (3) concerns in the evaluation regarding the practicality and the appropriateness of the user study for the goals of the paper.\n\nAlthough the authors provided detailed responses to these concerns, most of them still remained. Both reviewer 1 and reviewer 2 encourage the authors to define the prototypes defined more precisely, providing motivation for the various choices therein. Even though some of the concerns raised by reviewer 3 were addressed, it still remains to be seen how scalable the approach is for real-world applications.\n\nFor these reasons, the reviewers and the AC feel that the authors would need to make substantial improvements for the paper to be accepted.", "reviews": [{"review_id": "r1xyx3R9tQ-0", "review_text": "## Strength This paper explores ways of identifying prototypes with extensive qualitative and quantitative empirical attempts. ## Weakness ### Not practical The authors report that \u201cremoving individual training examples did not have a measurable impact on model performance\u201d. However, this seems not to be supported by experiments. First, it is not clear what exactly models do they use in Section 4, e.g. ResnetV2 with how many layers? Learning rate schedules? Second, why is the baseline models on CIFAR-10 perform so bad (<90%) even with 100% data? Third, with `\"adv\" metric, we need to perform adversarial-example attacks before training, which has little value in practice. ### Datasets They only conduct quantitative experiments (section 4) on relatively small datasets (i.e. MNIST, Fashion-MNIST and CIFAR-10). It is not clear how it will generalize to more realistic settings. ## Most confusing typos 1. Section 4, paragraph 5, \"However, we find that training only on the most prototypical examples gives extremely high accuracy on the other prototypical examples.\" Is there a missing \"than\"? It's confused. 2. The description of Figure 6 is not clear enough. Especially there is no explanation to (d, e, f). ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We are unsure whether the reviewer 's concerns about practicality are with respect to our methods or our results . Our methods are practical and are simple to implement with less than ten lines of code ( either by querying the pre-trained model , making calls to CleverHans for generating adversarial examples , or reusing the existing training code for retraining ) . The exception to this is our prototypicality metric based on privacy , which we agree is a state-of-the-art technique . We will release our code for this once we have completed the experiments . Our results have several practical benefits : 1 . We provide a practical technique for identifying mislabeled training examples ( see for example Appendix H ) which can help automate the collection of training data . 2.We provide a practical technique for identifying inherently ambiguous training examples such as the boot-like sneakers in the Fashion-MNIST dataset ( see Figure 12 in Appendix F ) , which can be used in training corpus data constructions and class definitions . For example in Appendix J that we have added in response , we show that our techniques for finding memorized exceptions apply to ImageNet . Our methods can automatically find many classes that are inherently ambiguous or are overlapping on ImageNet ( \u201c tusker \u201d vs. \u201c elephant \u201d ; \u201c sunglass \u201d vs. \u201c sunglasses , dark glasses , shades \u201d ; \u201c projectile , missile \u201d vs. \u201c missile \u201d ; \u201c maillot \u201d vs. \u201c maillot , tank suit \u201d ; \u201c breastplate , aegis , egis \u201d vs. \u201c cuirass \u201d ) . 3.We provide a practical technique for identifying uncommon sub-modes ( e.g. , 1s written in both serif and non-serif style ) , see Figure 11 Appendix F , which can be used to understand and balance a training data set . 4.We demonstrate four new practical metrics for identifying prototypical examples that correspond to human intuition ( as well as confirming the prior work of Stock and Cisse ( 2018 ) ) , See Appendix G. 5 . We find that training on prototypical examples is a quick and efficient method ( requiring only 2 % -10 % of the original training data ) for constructing models that perform well on test prototypical examples ."}, {"review_id": "r1xyx3R9tQ-1", "review_text": "Summary: The paper proposes methods for identifying prototypes. Unfortunately, a formal definition of a prototype is lacking, and the authors instead present a set of heuristics for sorting data points that purport to measure 'prototypicality', although different heuristics have different (and possibly conflicting) notions of what this means. The experiments are not very convincing, and often present results that are either inconclusive or negative, i.e. seem to demonstrate that prototypes are not very useful. Pros: - The notion of prototypes is used in various papers, but a formal definition is lacking, and the usefulness of prototypes is not demonstrated. The fact that this paper sets out to do both is laudable, although the paper needs work before it can be accepted for publications. Detailed comments / cons: *Defining prototypes: - The authors list desirable properties before defining (even informally) what a prototype is, and what its purposes are. Taking the first property as an example, is it reasonable to expect a metric for prototypes to be useful for image classification AND image generation? The answer completely depends on what one expects from a prototype, what its purpose is, etc. - The second property seems to indicate that prototypes are model-independent, i.e. two models trained on the same dataset will have the same prototypes. This is confusing as the metrics proposed are clearly model-dependent (e.g. adv completely depends on the trained model's decision boundary, conf obviously depends on the model providing the confidence score) - The third and fourth property are poorly defined. Human intuition presupposes that humans agree on what a prototype means. Using 'modes of prototypical examples' in trying to define a metric for prototypes is circular, as a mode of prototypical example depends on a working notion of prototypical examples. - The last property is completely dependent on which models are trained, and how they are trained. If a model has high label complexity, maybe it does not achieve high accuracy even when trained on high quality prototypes. In any case, this property is at odds with the first two properties. In sum: it's not clear what prototypes are, so it becomes hard to judge if the list of desiderata is reasonable. The list is in any case ill-defined, and contains contradictions. * Metrics for prototypicality - The second paragraph in this section is unnecessary - All of the metrics proposed are heuristics with little to no justification. Specific comments below. - Adversarial robustness is a property of a trained model, not of prototypical examples, unless prototypes are supposed to be model dependent (contra property 1). In any case, it is not clear why examples that are robust to adversarial noise are good 'prototypes'. Using facial recognition as an example, a 'mean face' may be very robust to adversarial noise but not prototypical at all under common definitions. A face with a particular type of facial hair (e.g. nose hair) may be very representative of a class of faces (i.e. a prototype), but very susceptible to adversarial noise. In fact, any examples in the boundary of the decision function will be more susceptible to adversaries, but that does not make them 'less prototypical'. - Holdout retraining is again completely model dependent. Why should we expect a model to treat a prototype the same regardless of whether or not it is trained on it? This basically means that we expect the model to always be accurate on prototypes. - Ensemble agreement proposes a notion of prototypes that is based on prediction 'hardness'. It is clear that such a notion depends completely on which models are being considered, which features are being used, and etc, much more than on notions of prototypicality inherent in the data. The same criticism applies to model confidence. - Privacy preserving training assumes prototypicality has to do with the model being able to learn with some robustness to noise (related to Adversarial Robustness, but different). This assumes a definition of prototypes that is not congruent with the other metrics. In sum: the proposed metrics are basically heuristics with little justification, and different metrics assume different notions of what a prototype is. * Evaluation - Section 3.1 claims that the metrics are strongly correlated, but that is not true for MNIST or CIFAR, and is somewhat true for fashion-mnist. In any case, since the metrics are so model-dependent, it is not clear if these results would hold if other models were used. - Section 3.2 - The question asked of turkers in the study is too vague, and borderline irrelevant for the task at hand - what does the 'best image' of an airplane mean, and how does this translate to it being a prototype? All that the study demonstrates is that the proposed metrics score malformed images with low score. The results in Table 1 are very spread out, and seem to indicate a low agreement between the metrics and human evaluation - although Table 1 is almost irrelevant given the question that was asked of users. - The results in Section 4 are very discouraging: sometimes it is better to train on most prototypical examples according to the metrics, sometimes it is worse, sometimes it's better to take examples in the middle. That is, prototypes don't seem to help at all. 'Prototype percentile' is uncorrelated with robustness for MNIST in Appendix E, while being correlated for other datasets. It is clear why this would be the case for metrics such as confidence, but in general models trained on less examples are less robust than models trained on the whole dataset (again, as expected). As a whole, the results do not provide any help for a user who wants to produce a more robust model, other than 'ignore prototypes and use the whole dataset'. ", "rating": "3: Clear rejection", "reply_text": "* Evaluation > - Section 3.1 claims that the metrics are strongly correlated , but that is not true for MNIST or CIFAR , and is somewhat true for fashion-mnist . In any case , since the metrics are so model-dependent , it is not clear if these results would hold if other models were used . Almost all pairs have a correlation coefficient higher than .5 , and many have correlation coefficients higher than .7 . Again , for the reasons stated above , our metrics are not model-dependent . > - Section 3.2 - The question asked of turkers in the study is too vague , and borderline irrelevant for the task at hand - what does the 'best image ' of an airplane mean , and how does this translate to it being a prototype ? All that the study demonstrates is that the proposed metrics score malformed images with low score . The results in Table 1 are very spread out , and seem to indicate a low agreement between the metrics and human evaluation - although Table 1 is almost irrelevant given the question that was asked of users . We completely reject the reviewer \u2019 s statements here . The results show a very strong correlation with human intuition ; indeed , the correlation with the top decile in the \u201c pick best \u201d table is quite unexpectedly strong for some metrics . The correlation varies with metrics , and for some metrics the strongest correlation is indeed on finding the non-prototypes ( i.e. , outliers ) , but this can only be expected for datasets where the majority of examples seem \u201c good \u201d to humans . Regarding the study design , it follows best practice in evaluating whether human intuition of what are \u201c good \u201d examples for a class matches our metrics \u2019 rankings . The question must be generic : if we taught humans what a prototype is based on the properties we used to design our metrics , this would result in a circular argument and unsound results . Our experiments with Turkers instead show that our metrics identify examples in the training and test data that are consistent with what a human would intuitively consider as a \u201c good example \u201d without priming them with what we consider as a good example . > - The results in Section 4 are very discouraging : sometimes it is better to train on most prototypical examples according to the metrics , sometimes it is worse , sometimes it 's better to take examples in the middle . That is , prototypes do n't seem to help at all . 'Prototype percentile ' is uncorrelated with robustness for MNIST in Appendix E , while being correlated for other datasets . It is clear why this would be the case for metrics such as confidence , but in general models trained on less examples are less robust than models trained on the whole dataset ( again , as expected ) . As a whole , the results do not provide any help for a user who wants to produce a more robust model , other than 'ignore prototypes and use the whole dataset ' . As explained in the text of our paper , the results are not discouraging but rather show that the metrics are able to capture the subtleties of different datasets . MNIST is a simple task where almost all training points are correctly classified and therefore models trained on this dataset learn more from outlier examples . Instead , FashionMNIST is a more complex task where some points are either mislabeled or ambiguous ( it is unclear to which class they belong ) : therefore models trained on the least prototypical examples do not perform well ( because these training points are mislabeled ) but models trained on the next slices of data according to the prototypicality metric perform in a similar way to the MNIST dataset . Furthermore , the truth is rarely black-and-white . While much prior work has either argued it is better to train on the easy prototypical examples , or conversely argued it is better to train on the harder ( more outlier ) examples , we find that both statements can be true depending on the exact details of the metric used , dataset , or learning task . We do not find our results to be discouraging , but even if they were , a true but discouraging result is worth reporting even when it does not match one \u2019 s expectations ."}, {"review_id": "r1xyx3R9tQ-2", "review_text": "Summary: This paper attempts to better understand the notion of prototypes and in some sense create a taxonomy for characterizing various prototypicality metrics. While the idea of thinking about such a taxonomy is novel, I think the paper falls in clearly justifying certain design choices such as why are the properties outlined at the beginning of Section 2 desirable. I also felt that the paper is resorting to rather informal ways of describing various properties and metrics without precisely quantifying them. Pros: 1. Novel attempt at understanding prototypes. Two specific contributions: a) outlining the properties desirable in prototypicality metrics b) proposing new prototypicality metrics and demonstrating the relevance of the various prototypicality metrics. 2. Detailed experimental analysis along with some user studies Cons: 1. An important drawback of this paper is that the notion of prototype is not very clearly contextualized and explained. There is often a purpose associated with identifying prototypes - are we summarizing a dataset? are we thinking about helping humans understand the behavior of a specific learning model? Answers to these questions guide the process of choosing prototypes. However, this paper seems to approach the problem of choosing prototypes via the \"one approach fits all\" strategy which I am not sure is even possible. 2. The choice of desirable properties is not clearly justified (Beginning of Section 2). For instance, why should prototypes be independent of learning tasks? 3. Lack of rigor in defining prototypicality metrics as well as properties in Section 2. For example, wouldn't it be possible to theoretically prove that the metrics outlined in Section 2 satisfy the desired properties? Detailed Comments: 1. I would strongly encourage the authors to illustrate using examples in the introduction the significance of finding prototypes. What are the end goals for which these prototypes would be used? Why do you think the metric for chooosing prototypes should be independent of the learning task or model? 2. Along the same lines as the comment above, please provide detailed justifications for the list of properties provided in the beginning of Section 2. It would be even better if you could formalize these a bit more. 3. Would it be possible to theoretically show that the metrics defined in Section 2 satisfy any of the desirable properties highlighted in Section 2? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "> Cons : > 1 . An important drawback of this paper is that the notion of prototype is not very clearly contextualized and explained . There is often a purpose associated with identifying prototypes - are we summarizing a dataset ? are we thinking about helping humans understand the behavior of a specific learning model ? Answers to these questions guide the process of choosing prototypes . However , this paper seems to approach the problem of choosing prototypes via the `` one approach fits all '' strategy which I am not sure is even possible . We tried to offer more clear definitions than what we found in earlier work . Unlike that work , we did not start off with a specific goal ; rather , we wanted to see if five new-and-old techniques for ranking training and test examples would give insights into ML model training processes and data corpora . We explicitly did not believe that \u201c one approach fits all , \u201d and hence we evaluated five different techniques ( actually more , but others were less informative ) . This was fortunate , because the differences between the metrics actually proved to be more informative than the metrics themselves , e.g. , in finding memorized exceptions or mislabeled and inherently-ambiguous training data . > 2.The choice of desirable properties is not clearly justified ( Beginning of Section 2 ) . For instance , why should prototypes be independent of learning tasks ? See our comment above on that list . On this specific property , it seemed preferable to us if a single measure ( i.e.mechanism for evaluating a metric ) could be applied equally to classification models , generative sequences models , etc . That way , a single technique could be used to find prototypical examples in a range of different modes and for many different types of tasks . This is not a property that holds for all of our five metrics , e.g. , since a notion of \u201c confidence \u201d or \u201c adversarial class \u201d simply isn \u2019 t defined in all learning tasks ( e.g. , training an embedding ) . But for both our retraining-distance and privacy-based metrics , it should be possible to apply the metric to nearly all learning tasks . Again , this seemed preferable . > 3.Lack of rigor in defining prototypicality metrics as well as properties in Section 2 . For example , would n't it be possible to theoretically prove that the metrics outlined in Section 2 satisfy the desired properties ? As said above , we tried to give a more precise definition for \u201c prototype \u201d than what we could find in the existing literature . We agree that our definitions are still less rigorous than would be ideal . We do not think it would be feasible to theoretically prove that our metrics satisfy our properties , since they quantitatively depend on data corpora and ML models and tasks , and some such combinations ( e.g. , artificially constructed ones ) may surely fail the properties . However , for some existing concrete data corpus and ML model/taks , like MNIST , CIFAR-10 , ImageNet , etc. , we can try to empirically validate how the properties apply to our metrics , for example as we do with our human studies . In the final revision of the paper , we will add an appendix showing how each property is supported for each of our metric . > Detailed Comments : > 1 . I would strongly encourage the authors to illustrate using examples in the introduction the significance of finding prototypes . What are the end goals for which these prototypes would be used ? In addition to performance benefits , via curriculum learning etc. , discussed in Section 4 , our end goals are to understand aspects of training data and tasks such as those shown in Figure 5 . In particular , we know of no other technique for finding memorized exceptions or mislabeled and inherently-ambiguous training data that works in the same way and equally well . We will move that figure earlier in the paper , into the introduction . > Why do you think the metric for chooosing prototypes should be independent of the learning task or model ? See detailed answer above . > 2.Along the same lines as the comment above , please provide detailed justifications for the list of properties provided in the beginning of Section 2 . It would be even better if you could formalize these a bit more . See answer above . > 3.Would it be possible to theoretically show that the metrics defined in Section 2 satisfy any of the desirable properties highlighted in Section 2 ? See answer above ."}], "0": {"review_id": "r1xyx3R9tQ-0", "review_text": "## Strength This paper explores ways of identifying prototypes with extensive qualitative and quantitative empirical attempts. ## Weakness ### Not practical The authors report that \u201cremoving individual training examples did not have a measurable impact on model performance\u201d. However, this seems not to be supported by experiments. First, it is not clear what exactly models do they use in Section 4, e.g. ResnetV2 with how many layers? Learning rate schedules? Second, why is the baseline models on CIFAR-10 perform so bad (<90%) even with 100% data? Third, with `\"adv\" metric, we need to perform adversarial-example attacks before training, which has little value in practice. ### Datasets They only conduct quantitative experiments (section 4) on relatively small datasets (i.e. MNIST, Fashion-MNIST and CIFAR-10). It is not clear how it will generalize to more realistic settings. ## Most confusing typos 1. Section 4, paragraph 5, \"However, we find that training only on the most prototypical examples gives extremely high accuracy on the other prototypical examples.\" Is there a missing \"than\"? It's confused. 2. The description of Figure 6 is not clear enough. Especially there is no explanation to (d, e, f). ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We are unsure whether the reviewer 's concerns about practicality are with respect to our methods or our results . Our methods are practical and are simple to implement with less than ten lines of code ( either by querying the pre-trained model , making calls to CleverHans for generating adversarial examples , or reusing the existing training code for retraining ) . The exception to this is our prototypicality metric based on privacy , which we agree is a state-of-the-art technique . We will release our code for this once we have completed the experiments . Our results have several practical benefits : 1 . We provide a practical technique for identifying mislabeled training examples ( see for example Appendix H ) which can help automate the collection of training data . 2.We provide a practical technique for identifying inherently ambiguous training examples such as the boot-like sneakers in the Fashion-MNIST dataset ( see Figure 12 in Appendix F ) , which can be used in training corpus data constructions and class definitions . For example in Appendix J that we have added in response , we show that our techniques for finding memorized exceptions apply to ImageNet . Our methods can automatically find many classes that are inherently ambiguous or are overlapping on ImageNet ( \u201c tusker \u201d vs. \u201c elephant \u201d ; \u201c sunglass \u201d vs. \u201c sunglasses , dark glasses , shades \u201d ; \u201c projectile , missile \u201d vs. \u201c missile \u201d ; \u201c maillot \u201d vs. \u201c maillot , tank suit \u201d ; \u201c breastplate , aegis , egis \u201d vs. \u201c cuirass \u201d ) . 3.We provide a practical technique for identifying uncommon sub-modes ( e.g. , 1s written in both serif and non-serif style ) , see Figure 11 Appendix F , which can be used to understand and balance a training data set . 4.We demonstrate four new practical metrics for identifying prototypical examples that correspond to human intuition ( as well as confirming the prior work of Stock and Cisse ( 2018 ) ) , See Appendix G. 5 . We find that training on prototypical examples is a quick and efficient method ( requiring only 2 % -10 % of the original training data ) for constructing models that perform well on test prototypical examples ."}, "1": {"review_id": "r1xyx3R9tQ-1", "review_text": "Summary: The paper proposes methods for identifying prototypes. Unfortunately, a formal definition of a prototype is lacking, and the authors instead present a set of heuristics for sorting data points that purport to measure 'prototypicality', although different heuristics have different (and possibly conflicting) notions of what this means. The experiments are not very convincing, and often present results that are either inconclusive or negative, i.e. seem to demonstrate that prototypes are not very useful. Pros: - The notion of prototypes is used in various papers, but a formal definition is lacking, and the usefulness of prototypes is not demonstrated. The fact that this paper sets out to do both is laudable, although the paper needs work before it can be accepted for publications. Detailed comments / cons: *Defining prototypes: - The authors list desirable properties before defining (even informally) what a prototype is, and what its purposes are. Taking the first property as an example, is it reasonable to expect a metric for prototypes to be useful for image classification AND image generation? The answer completely depends on what one expects from a prototype, what its purpose is, etc. - The second property seems to indicate that prototypes are model-independent, i.e. two models trained on the same dataset will have the same prototypes. This is confusing as the metrics proposed are clearly model-dependent (e.g. adv completely depends on the trained model's decision boundary, conf obviously depends on the model providing the confidence score) - The third and fourth property are poorly defined. Human intuition presupposes that humans agree on what a prototype means. Using 'modes of prototypical examples' in trying to define a metric for prototypes is circular, as a mode of prototypical example depends on a working notion of prototypical examples. - The last property is completely dependent on which models are trained, and how they are trained. If a model has high label complexity, maybe it does not achieve high accuracy even when trained on high quality prototypes. In any case, this property is at odds with the first two properties. In sum: it's not clear what prototypes are, so it becomes hard to judge if the list of desiderata is reasonable. The list is in any case ill-defined, and contains contradictions. * Metrics for prototypicality - The second paragraph in this section is unnecessary - All of the metrics proposed are heuristics with little to no justification. Specific comments below. - Adversarial robustness is a property of a trained model, not of prototypical examples, unless prototypes are supposed to be model dependent (contra property 1). In any case, it is not clear why examples that are robust to adversarial noise are good 'prototypes'. Using facial recognition as an example, a 'mean face' may be very robust to adversarial noise but not prototypical at all under common definitions. A face with a particular type of facial hair (e.g. nose hair) may be very representative of a class of faces (i.e. a prototype), but very susceptible to adversarial noise. In fact, any examples in the boundary of the decision function will be more susceptible to adversaries, but that does not make them 'less prototypical'. - Holdout retraining is again completely model dependent. Why should we expect a model to treat a prototype the same regardless of whether or not it is trained on it? This basically means that we expect the model to always be accurate on prototypes. - Ensemble agreement proposes a notion of prototypes that is based on prediction 'hardness'. It is clear that such a notion depends completely on which models are being considered, which features are being used, and etc, much more than on notions of prototypicality inherent in the data. The same criticism applies to model confidence. - Privacy preserving training assumes prototypicality has to do with the model being able to learn with some robustness to noise (related to Adversarial Robustness, but different). This assumes a definition of prototypes that is not congruent with the other metrics. In sum: the proposed metrics are basically heuristics with little justification, and different metrics assume different notions of what a prototype is. * Evaluation - Section 3.1 claims that the metrics are strongly correlated, but that is not true for MNIST or CIFAR, and is somewhat true for fashion-mnist. In any case, since the metrics are so model-dependent, it is not clear if these results would hold if other models were used. - Section 3.2 - The question asked of turkers in the study is too vague, and borderline irrelevant for the task at hand - what does the 'best image' of an airplane mean, and how does this translate to it being a prototype? All that the study demonstrates is that the proposed metrics score malformed images with low score. The results in Table 1 are very spread out, and seem to indicate a low agreement between the metrics and human evaluation - although Table 1 is almost irrelevant given the question that was asked of users. - The results in Section 4 are very discouraging: sometimes it is better to train on most prototypical examples according to the metrics, sometimes it is worse, sometimes it's better to take examples in the middle. That is, prototypes don't seem to help at all. 'Prototype percentile' is uncorrelated with robustness for MNIST in Appendix E, while being correlated for other datasets. It is clear why this would be the case for metrics such as confidence, but in general models trained on less examples are less robust than models trained on the whole dataset (again, as expected). As a whole, the results do not provide any help for a user who wants to produce a more robust model, other than 'ignore prototypes and use the whole dataset'. ", "rating": "3: Clear rejection", "reply_text": "* Evaluation > - Section 3.1 claims that the metrics are strongly correlated , but that is not true for MNIST or CIFAR , and is somewhat true for fashion-mnist . In any case , since the metrics are so model-dependent , it is not clear if these results would hold if other models were used . Almost all pairs have a correlation coefficient higher than .5 , and many have correlation coefficients higher than .7 . Again , for the reasons stated above , our metrics are not model-dependent . > - Section 3.2 - The question asked of turkers in the study is too vague , and borderline irrelevant for the task at hand - what does the 'best image ' of an airplane mean , and how does this translate to it being a prototype ? All that the study demonstrates is that the proposed metrics score malformed images with low score . The results in Table 1 are very spread out , and seem to indicate a low agreement between the metrics and human evaluation - although Table 1 is almost irrelevant given the question that was asked of users . We completely reject the reviewer \u2019 s statements here . The results show a very strong correlation with human intuition ; indeed , the correlation with the top decile in the \u201c pick best \u201d table is quite unexpectedly strong for some metrics . The correlation varies with metrics , and for some metrics the strongest correlation is indeed on finding the non-prototypes ( i.e. , outliers ) , but this can only be expected for datasets where the majority of examples seem \u201c good \u201d to humans . Regarding the study design , it follows best practice in evaluating whether human intuition of what are \u201c good \u201d examples for a class matches our metrics \u2019 rankings . The question must be generic : if we taught humans what a prototype is based on the properties we used to design our metrics , this would result in a circular argument and unsound results . Our experiments with Turkers instead show that our metrics identify examples in the training and test data that are consistent with what a human would intuitively consider as a \u201c good example \u201d without priming them with what we consider as a good example . > - The results in Section 4 are very discouraging : sometimes it is better to train on most prototypical examples according to the metrics , sometimes it is worse , sometimes it 's better to take examples in the middle . That is , prototypes do n't seem to help at all . 'Prototype percentile ' is uncorrelated with robustness for MNIST in Appendix E , while being correlated for other datasets . It is clear why this would be the case for metrics such as confidence , but in general models trained on less examples are less robust than models trained on the whole dataset ( again , as expected ) . As a whole , the results do not provide any help for a user who wants to produce a more robust model , other than 'ignore prototypes and use the whole dataset ' . As explained in the text of our paper , the results are not discouraging but rather show that the metrics are able to capture the subtleties of different datasets . MNIST is a simple task where almost all training points are correctly classified and therefore models trained on this dataset learn more from outlier examples . Instead , FashionMNIST is a more complex task where some points are either mislabeled or ambiguous ( it is unclear to which class they belong ) : therefore models trained on the least prototypical examples do not perform well ( because these training points are mislabeled ) but models trained on the next slices of data according to the prototypicality metric perform in a similar way to the MNIST dataset . Furthermore , the truth is rarely black-and-white . While much prior work has either argued it is better to train on the easy prototypical examples , or conversely argued it is better to train on the harder ( more outlier ) examples , we find that both statements can be true depending on the exact details of the metric used , dataset , or learning task . We do not find our results to be discouraging , but even if they were , a true but discouraging result is worth reporting even when it does not match one \u2019 s expectations ."}, "2": {"review_id": "r1xyx3R9tQ-2", "review_text": "Summary: This paper attempts to better understand the notion of prototypes and in some sense create a taxonomy for characterizing various prototypicality metrics. While the idea of thinking about such a taxonomy is novel, I think the paper falls in clearly justifying certain design choices such as why are the properties outlined at the beginning of Section 2 desirable. I also felt that the paper is resorting to rather informal ways of describing various properties and metrics without precisely quantifying them. Pros: 1. Novel attempt at understanding prototypes. Two specific contributions: a) outlining the properties desirable in prototypicality metrics b) proposing new prototypicality metrics and demonstrating the relevance of the various prototypicality metrics. 2. Detailed experimental analysis along with some user studies Cons: 1. An important drawback of this paper is that the notion of prototype is not very clearly contextualized and explained. There is often a purpose associated with identifying prototypes - are we summarizing a dataset? are we thinking about helping humans understand the behavior of a specific learning model? Answers to these questions guide the process of choosing prototypes. However, this paper seems to approach the problem of choosing prototypes via the \"one approach fits all\" strategy which I am not sure is even possible. 2. The choice of desirable properties is not clearly justified (Beginning of Section 2). For instance, why should prototypes be independent of learning tasks? 3. Lack of rigor in defining prototypicality metrics as well as properties in Section 2. For example, wouldn't it be possible to theoretically prove that the metrics outlined in Section 2 satisfy the desired properties? Detailed Comments: 1. I would strongly encourage the authors to illustrate using examples in the introduction the significance of finding prototypes. What are the end goals for which these prototypes would be used? Why do you think the metric for chooosing prototypes should be independent of the learning task or model? 2. Along the same lines as the comment above, please provide detailed justifications for the list of properties provided in the beginning of Section 2. It would be even better if you could formalize these a bit more. 3. Would it be possible to theoretically show that the metrics defined in Section 2 satisfy any of the desirable properties highlighted in Section 2? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "> Cons : > 1 . An important drawback of this paper is that the notion of prototype is not very clearly contextualized and explained . There is often a purpose associated with identifying prototypes - are we summarizing a dataset ? are we thinking about helping humans understand the behavior of a specific learning model ? Answers to these questions guide the process of choosing prototypes . However , this paper seems to approach the problem of choosing prototypes via the `` one approach fits all '' strategy which I am not sure is even possible . We tried to offer more clear definitions than what we found in earlier work . Unlike that work , we did not start off with a specific goal ; rather , we wanted to see if five new-and-old techniques for ranking training and test examples would give insights into ML model training processes and data corpora . We explicitly did not believe that \u201c one approach fits all , \u201d and hence we evaluated five different techniques ( actually more , but others were less informative ) . This was fortunate , because the differences between the metrics actually proved to be more informative than the metrics themselves , e.g. , in finding memorized exceptions or mislabeled and inherently-ambiguous training data . > 2.The choice of desirable properties is not clearly justified ( Beginning of Section 2 ) . For instance , why should prototypes be independent of learning tasks ? See our comment above on that list . On this specific property , it seemed preferable to us if a single measure ( i.e.mechanism for evaluating a metric ) could be applied equally to classification models , generative sequences models , etc . That way , a single technique could be used to find prototypical examples in a range of different modes and for many different types of tasks . This is not a property that holds for all of our five metrics , e.g. , since a notion of \u201c confidence \u201d or \u201c adversarial class \u201d simply isn \u2019 t defined in all learning tasks ( e.g. , training an embedding ) . But for both our retraining-distance and privacy-based metrics , it should be possible to apply the metric to nearly all learning tasks . Again , this seemed preferable . > 3.Lack of rigor in defining prototypicality metrics as well as properties in Section 2 . For example , would n't it be possible to theoretically prove that the metrics outlined in Section 2 satisfy the desired properties ? As said above , we tried to give a more precise definition for \u201c prototype \u201d than what we could find in the existing literature . We agree that our definitions are still less rigorous than would be ideal . We do not think it would be feasible to theoretically prove that our metrics satisfy our properties , since they quantitatively depend on data corpora and ML models and tasks , and some such combinations ( e.g. , artificially constructed ones ) may surely fail the properties . However , for some existing concrete data corpus and ML model/taks , like MNIST , CIFAR-10 , ImageNet , etc. , we can try to empirically validate how the properties apply to our metrics , for example as we do with our human studies . In the final revision of the paper , we will add an appendix showing how each property is supported for each of our metric . > Detailed Comments : > 1 . I would strongly encourage the authors to illustrate using examples in the introduction the significance of finding prototypes . What are the end goals for which these prototypes would be used ? In addition to performance benefits , via curriculum learning etc. , discussed in Section 4 , our end goals are to understand aspects of training data and tasks such as those shown in Figure 5 . In particular , we know of no other technique for finding memorized exceptions or mislabeled and inherently-ambiguous training data that works in the same way and equally well . We will move that figure earlier in the paper , into the introduction . > Why do you think the metric for chooosing prototypes should be independent of the learning task or model ? See detailed answer above . > 2.Along the same lines as the comment above , please provide detailed justifications for the list of properties provided in the beginning of Section 2 . It would be even better if you could formalize these a bit more . See answer above . > 3.Would it be possible to theoretically show that the metrics defined in Section 2 satisfy any of the desirable properties highlighted in Section 2 ? See answer above ."}}