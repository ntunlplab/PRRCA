{"year": "2021", "forum": "J3OUycKwz-", "title": "Mapping the Timescale Organization of Neural Language Models", "decision": "Accept (Poster)", "meta_review": "This paper applies methods inspired by neuroscience to analyze the inner workings of LSTM language models. In particular, a simple and clever approach is proposed, in which a sentence is presented in its observed context vs. a random one. The time for a unit activation to become similar in the two contexts is used as a probe of the timescale of contextual effects. The main results are that timescales increase with layer and that there are two classes of long-timescale units with different graph-theoretical properties. The functionality of syntax-sensitive units previously identified in the literature is confirmed. Finally, the analysis is replicated for a character-level model.\n\nThe paper received detailed and insightful reviews, and there was a lively (but always respectful) discussion between authors and reviewers.\n\nOverall, the reviewers liked the topic of the paper and the overall methodology, however they had several issues with it. One of the issue pertained to the \"holistic\" approach to time in the paper, which is measured in number of tokens, rather than in terms of syntactic distance. More in general, there was a feeling that the paper was somewhat short on actual insights on the exact functional role of units in a linguistic context. The reviewer who assigned the most severe score was mostly concerned about one specific instance of this, namely the fact that the authors focus on syntax-tracking and number agreement units whose scope should not really extend across sentences. Moreover, the reviewer was surprised that the syntax-tracking units maintain information across longer distances than the number-agreement units, that should, by definition, keep track of long-distance relations.\n\nI am divided. I welcome work that focuses on novel qualitative and quantitative analyses of an existing model. I wished there were clearer take-home messages on how LSTMs process language, but I recognize that our knowledge of deep-learning models is very preliminary, and I am thus not surprised that the conclusions are not entirely clear.  The reviewers raised important concerns, but I would not confidently claim that we know enough about the relevant units to be genuinely surprised by some of the results. For example, can we really say that number-agreement units are only limited to clause-internal agreement tracking? Couldn't it be, say, that we will discover in the future they also play a role in tracking discourse-determined pronominal number (going out on a random limb, here, of course)?\n\nOverall, I would like to see this at least as a poster at the conference, but I am assigning low confidence to my recommendation as I respect the reviewers' point of view.\n", "reviews": [{"review_id": "J3OUycKwz--0", "review_text": "This paper applies tools from neuroscience to understand how language models integrate across time . The basic approach is to present a phrase , preceded by two different context phrases : one that is natural ( i.e.the phrase that actually preceded it in the corpus ) and one that is randomly selected . The authors then measure how long it takes for the unit activations to become similar for the two different contexts , which provides a measure for how long the context impacts the representation . They find that ( 1 ) timescales increase at later layers of the language model ( 2 ) that only a small fraction of units exhibit long timescales ( 3 ) that long/medium-timescale units appear to come in two forms which they try and characterize using graph-style analyses . -- Pros : How language models integrate across time is clearly important , and this paper describes interesting first steps in characterizing the analysis of time using relevant tools from the neuroscience literature . The method presented is simple and broadly applicable . The graph-style results seem intriguing if a little hard to make sense of . I also think that the sparsity of the long-timescale units is cool and interesting . -- Limitations and questions : 1 . It \u2019 s not clear to me if the notion of time is a meaningful one in a language model . For example , the duration of contextual effects on a unit that codes syntactic number will presumably be highly variable and depend upon the details of the particular sentence being encoded . Thus a natural question is how variable are these timescales from moment-to-moment ? What \u2019 s being plotted is the average across a bunch of sentences , segmented at a particular moment ( a conjunction ) . How robust are these results if one examines a different point in a sentence ? Are the timescales of some units more variable than others ? -- Update : the authors have repeated their analysis for a different sentence point ( after the 10th word ) and report similar results . This analysis is helpful , though of course the 10th word is not a very principled break point , and there presumably is a lot of variation in timescales that are being averaged across . I continue to wonder how meaningful the notion of an absolute timescale is . -- 2.None of the steps in the graph analyses seemed particularly natural or well-motivated to me . Why were the graph edges thresholded at z > 5 and why was k-core analysis performed ? I find it hard to make sense of what this analysis tells us about how language information is processed . Is there some reason why medium timescale \u201c controller \u201d units and long-timescale \u201c integrator \u201d units should help with language processing ? If these results are purely exploratory and lack a clear interpretation , then perhaps the authors could help the reader by explaining the thought process behind the exploration . Perhaps starting with the MDS plot would be useful rather than the k-core analysis , because the MDS plot clearly shows some interesting structure . -- The authors have motivated some of their analyses by discussing brain research reporting that longer-timescale regions are more densely connected . Of course , the relationship between connectivity between large-scale brain regions and the units in a LSTM remains highly speculative . But having some motivation is helpful . -- 3.It would be interesting to know how dependent these findings are on the model \u2019 s architecture . Would similar results be found for a Transformer or a simpler GRU-style RNN ? -- The authors have attempted to address this point , but with limited time were not able to train a network to a high level of performance . Minor points : In Figure 4 , it would be helpful if the absolute timescale was labeled in all plots rather than the rank of the unit or the \u201c normalized timescale \u201d . The absolute timescale seems much more meaningful to me ( and the units can of course still be ranked , just the axis labels changed or augmented ) . The legend for Figure 4c is incorrect .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your valuable feedback ! Please see our point-by-point response below : - * It \u2019 s not clear to me if the notion of time is a meaningful one in a language model . For example , the duration of contextual effects on a unit that codes syntactic number will presumably be highly variable and depend upon the details of the particular sentence being encoded . Thus a natural question is how variable are these timescales from moment-to-moment ? What \u2019 s being plotted is the average across a bunch of sentences , segmented at a particular moment ( a conjunction ) . How robust are these results if one examines a different point in a sentence ? Are the timescales of some units more variable than others ? * Thank you for raising the point . Indeed , the timescale of individual units could vary based on the syntactic distance , which can be different from the token distance measured in our study . As pointed out by reviewer 3 , one would expect that functional units such as syntax or number units should \u201c reset \u201d their timescale at certain points of the sentence ( please refer to our response to the figures generated for Reviewer 3 regarding the reset process ) . Thank you for the suggestion about testing other locations in sentences , rather than just the specific \u201c , and \u201d conjunction . We conducted a new analysis in which we segmented the context and shared segments at a fixed distance from the sentence onset ( i.e.we pick the 10th token as the segmentation point ) . We found that the timescale organization was largely preserved , regardless of whether we measured the cross-context effect at the \u201c , and \u201d boundary [ as in our original analysis ] or simply before and after the 10th token [ as in this new analysis ] . ( Please refer to the figure here : https : //anonymous.4open.science/repository/ef3696c5-e97e-4bd8-b12a-94795a038b8c/timescale_corr_commaAnd_middle.png ) . At the same time , we did find that there was a small subset of units ( fewer than 10 units ) whose timescales were clearly shorter following the \u201c , and \u201d conjunction . This is intriguing , as it suggests that some units were \u201c resetting \u201d their context following the \u201c , and \u201d conjunction . In future work , we will also seek to identify whether these units . In all , we agree with the reviewer that the timescales measured using \u201c tokens \u201d may be flexible , and can vary according to the syntactic context , for example . To address how syntactic distance affects context encoding in individual unit , carefully controlled context conditions are needed , which is beyond the scope of the current study . However , we would like to stress the importance of token distance for RNNs in general , since it serves as an important parameter for RNNs to predict the next token in any kind of sequences , before it learns to flexibly integrate information based on syntactic distance in language specifically . We will revise the manuscript to include the analysis suggested by the reviewer , which suggests that the timescales measured at the level of tokens are robust across different choices of the location of the prior context segment ."}, {"review_id": "J3OUycKwz--1", "review_text": "_ * * Update after author response * * _ : I think this is a very promising paper , and I am really excited about seeing techniques from neuroscience employed to answer questions about neural network models . The authors have further conducted several additional experiments after reviewer comments , which I appreciate . However , my most fundamental concern -- the mismatch between the method and the way that it is validated -- unfortunately still stands , which is why I would encourage the authors to further pursue this line of work , but recommend to reject it for ICLR . * * Summary * * This paper proposes to apply time-scale methods from neuroscience to investigate the timescale organisation in neural language models . More specifically , the authors test the timescale of individual units in a word- and character-level LSTM by comparing the units ' activations values on the same sentence , but with different contexts . Using this method , the authors first show that the higher layers on average have longer timescales . They then , for all units , they fit a logistic function to the `` recovery '' curves and use the half-times of this curves as an indication of the time scale of these units . They test the syntax unit and two long-distance units found by Lakretz et al and show that the number units have similar time-scales , while the syntax unit have a longer time scale . Lastly , the authors analyse the connectivity between the longer time scale units and find that the units with longer processing timescales make a larger number of strong projections . Within these units , the authors identify two sets of units in the word-level LSTM : `` controller units '' , that play a role in how the connectivity of the network is updated , and `` integrator units '' , that instead integrate information . * * Strong points * * - Neuroscience has long been asking questions about the brain that are very similar to the questions we now ask about neural networks , cross-pollination between these fields is extremely important , and this paper contributes to this - Aside from the main technique , the paper introduces some interesting and useful methods , such as projectivity analysis and k-core analysis . I think these methods can be useful for other researchers as well - Time scale analysis of LSTMs is a very relevant and interesting topic , that deserves more attention than it is currently getting * Concerns * - My main concern is that there seems to be a mismatch between the `` language time scales '' on which the authors operate : their experiment is designed to investigate the impact of extra-sentential context , but the Lakretz et al results they keep coming back to concern syntactic phenomena that are only relevant * within * a sentence , which is a different scale . In other words , the units found by the authors of this paper are long-distance when it comes to integrating context , but the syntax and number units found by Lakretz et al are not really related to that : they model relationships * within * sentences . Theoretically speaking , they should be reset at the beginning of every new sentence and they should thus be completely independent from the content . That the authors find this to be untrue is interesting , but inconsistent with what Lakretz et al describe these unit do . Since this is not addressed at all in the paper , it makes the results in general a bit difficult to interpret . _ * * Update after author response * * : In their response the authors clarified that the they have only analysed single sentences , where two distinct subsentences are combined with a conjunction . This , unfortunately , does not make a difference for the argument : whether two sentences are split by a full stop or instead concatenated with `` and '' does not make any difference for the argument above , since the subject-verb agreement relationships that the units the authors look at model do not cross these boundaries either . Furthermore , in their response the authors state that the find that the context representations of units was 'reset ' at sentence boundaries , as I asked before . I appreciate that the authors did these additional experiments , but I find the result somewhat worrisome : since the units they are looking at are syntactic units that encode number across long distance subject verb relationships , they should be reset both when a new sentence starts , as well as when a new conjunct with a new relationship starts . In terms of SV relationships , there should be no difference between `` The boy kicked the ball and the girl caught it '' and `` The boy kicked the ball . The girl caught it . '' That the authors do find a difference points to a potential flaw in methodology._ - Relatedly , the authors say that their result that the syntax unit is a long distance unit , while the number units are not . This is not consistent with what they say in the related work of the section , but also not with the results reported by Lakretz et al , who hypothesise that the syntax units represent the depth of the syntactic dependency . This is something that changes with every new incoming word , whereas the number units are the ones that have to keep their activation constant across time . - While , as I said before , I think it is great that the authors try to use methods from neuroscience into the field , I do think that in this case the main method they propose is only very marginally different from earlier work ( in particular Khandelwal et al.Perhaps it would make more sense to put a bit more stress on the rest of the methods as well ( btw , also Lakretz et al do connectivity analysis ) . - The results are a bit underexplained , and understanding them requires many back and forths to the appendix . I would have appreciated a bit more motivated interpretation of several aspects . For instance : why is there such a large difference in activation differences in different units in the `` pre-shared segment '' part , and is this related to the half-time ( it seems so from the plots ) ? What is the difference between character and word-level models in terms of expectations ( we 'd expect there to be an additional level of time-hierarchy , perhaps ? ) How do assessing activation differences and correlations differ in terms of conclusions ? These things should , in my opinion , all be worked out a bit better . - Lastly , there are a few unsupported claims , the most important of which that their method recovers the previously discovered units of Lakretz et al , while ( as far as I understand ) , they actually only * use * their method to analyse those neurons , but did not find them independently . ( for other suggestions and comments , see below ) . To summarise , while I think the idea is very nice and definitely worth working out further , I do think that some work is needed to make this a publishable paper . * Suggestions/comments for authors * _Typographic_ : - If you use quotes in latex , you should use different ones for left ( ` ) and right ( ' ) , for them to appear correctly ( check for instance line three in the introduction ) - To prevent additional spaces after abbreviations like e.g.and i.e. , put a backslash : `` e.g.\\ `` - Lerner et al -- > put all references within parenthesis - Introduction switches from present tense to paste tense in the last paragraph - `` we measure the time-taken for the effect of this prior context to \u201d decay \u201d ( see Methods ) '' -- > I do n't really understand what this means , you measure how long it takes for these changes to not be measurable anymore ? - Try to avoid double parethesis with abbreviations , e.g . : ( WLSTM Gulordava et al . ( 2018 ) ) should be : ( WLSTM , Gulordava et al ; 2018 ) . You can do this with \\citep [ text before ] [ text after ] { citation } . - `` has an 650-dimensional '' -- > `` has a 650-dimensional '' - `` without fine-tuning to the novel '' -- > I first thought this sentence was unfinished until I read back and realised that `` the novel '' is your corpus . This is a bit confusing perhaps you could rephrase . - `` how the cell state activation differ '' -- > `` how the cell state activations differ '' - `` we will see that the activation difference drop quickly ' -- > drops quickly / see the activation difference drop quickly - There are several references that were published at ACL * conferences that are listed as arxiv papers in the reference list ( Lakretz et al , Gulordava et al , Khandelwal et al ) _Content_ - I would say that the conclusion that `` Overall , prior works suggests that a small subset of units track long-range dependencies '' is rather overstated : Lakretz et al found that the units representing long distance number information were sparse , but this does not imply that long range information in general is represented sparsely . Their method also focusses quite exclusively on finding sparsely distributed properties , as more distributed properties can not be found with ablation . Furthermore , this is just one study , focusing on one syntactic aspect . I would suggest to rephrase this a bit . - Lakretz at all actually identified several syntax units , but only one of them was interpretable . - I find it a bit confusing that in 3.2 , second paragraph , you first talk about comparing cell state activation , then say that you compare hidden state activations and then talk again about the cell state activation - Figure 1 C & D : I do n't think these figures add much to the paper , for the following reasons i ) They show only individual units and no average , making it difficult to interpret the values ii ) while , as pointed out in 5.1 , the * rate * of decay is the most important , the cut-off point is not indicated in the figure , which puts a stress on irrelevant aspects : the actual difference between the two lines . - I would appreciate to have Figure A.1 in the main text , it is important for the story .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for acknowledging the potential contribution of this paper , including the importance of transferring methods from brain science to understand neural network AI models , and the importance of analyzing timescales in neural language models . Here are the point-by-point responses to the concerns raised by the reviewer : - * My main concern is that there seems to be a mismatch between the `` language time scales '' on which the authors operate : their experiment is designed to investigate the impact of extra-sentential context , but the Lakretz et al results they keep coming back to concern syntactic phenomena that are only relevant within a sentence , which is a different scale . In other words , the units found by the authors of this paper are long-distance when it comes to integrating context , but the syntax and number units found by Lakretz et al are not really related to that : they model relationships within sentences . Theoretically speaking , they should be reset at the beginning of every new sentence and they should thus be completely independent from the content . That the authors find this to be untrue is interesting , but inconsistent with what Lakretz et al describe these unit do . Since this is not addressed at all in the paper , it makes the results in general a bit difficult to interpret . * We apologize for the confusion here . Crucially , our work only examines phenomena within an individual sentence , just as in the Lakretz study . The reviewer is correct that context representations are \u2018 reset \u2019 at sentence boundaries in the Gulordava et al.model ( see below , where we confirm this in our own data ) . For this reason in an early draft of our manuscript , we had only analyzed single sentences which combined two distinct sub-sentences in the following way , e.g . : \u201c The boy kicked the ball , and the girl caught it. \u201d Since the segment before the conjunction \u201c , and \u201d can be read as a self-contained sentence , our original paper contained text such as : \u201c the preceding sentence differed across the two conditions \u201d ( in Section 3.2 ) . However , to be clear , these preceding segments were always part of the same sentence . Thus , in all of the data reported in our paper , we only examined context effects within a single sentence . We apologize again for the confusion . For improved clarity , we have now revised the manuscript so that the word \u201c segment \u201d is used consistently throughout to refer to the prior context segment and to the shared input segment . Consistent with the reviewer \u2019 s statement , we did find that the context representation of units was \u201c reset \u201d at sentence boundaries . To demonstrate this , we examined the timescales of each unit when the context segment and shared input segment were separated by a \u201c full stop \u201d symbol , which signals the end of a sentence . ( Please refer to the result figure here : https : //anonymous.4open.science/repository/ef3696c5-e97e-4bd8-b12a-94795a038b8c/fullstop_reset.png ) . When the first and second segments were separated \u201c full stop \u201d symbol , we found that the timescales inferred for most of the units became extremely short , compared to when the first and second segments were separated with a \u201c comma \u201d symbol as in our original paper . This \u201c reset \u201d phenomenon at the beginning of every new sentence is consistent with what the reviewer predicts and with the results of Lakretz et al ."}, {"review_id": "J3OUycKwz--2", "review_text": "This paper explores the application of innovative methods to track the flow of linguistic information in LSTM language models . In particular , the overarching question is how contextual information might be encoded in the network at the level of single units , and how context disruption might alter the LSTM dynamics and thus impact its predictive ability . The paper is clear and it tackles an interesting question . The approach is well motivated , and the authors give a brief survey of the most recent applications of this kind of methodology in linguistics and cognitive neuroscience studies . The methodology is generally appropriate , though some details and parameters ( e.g. , numerical thresholds ) seem to be chosen arbitrarily . Also , the analysis could be improved by applying statistical testing in order to better quantify the strength of the observed effects . Overall , I think this is a nice paper , though it might be especially relevant to the linguistics community rather than to the ICLR community . Moreover , I think that further analyses are required in order to better clarify some important aspects . In particular , I think that ablation studies should be performed in order to better identify the functional role of the \u201c controller \u201d and \u201c integrator \u201d units , whose actual functional role remains a bit speculative ( and mostly based on structural / connectivity information ) . It would also strengthen the paper to have some more controlled simulations , where the contextual information is defined according to specific linguistic constraints , in order to better characterize what the target units are actually encoding . Indeed , as also noted by the authors almost \u201c all the long timescale units are of unknown function \u201d . Finally , I think that it would be important to establish whether these findings are generally applicable to LSTM models , regardless of the specific architecture under investigation ( e.g. , What happens if we force the LSTM to rely on fewer units ? Does the hierarchical organization of the context improve by adding more layers ? ) . Other comments : - Why did the author choose to test the model on a different corpus ( Anna Karenina novel ) rather than considering a test set from the same corpus from which the training set was derived ? The Tolstoy book might have a quite different linguistic structure from that of the corpora used to train the LSTMs . - It might be informative to also include a third condition in-between \u201c Intact \u201d and \u201c Random \u201d context , where the same context words are maintained with scrambled order . This would allow to better understand the role of individual words in shaping context representation and activating the LSTM units . - In Fig.1D , it is interesting to note that the Unit 823 ( green line ) actually exhibits a sharp increase in difference after the shared segment starts . Do the authors have a possible explanation for this kind of phenomena ? Was it observed systematically in other units ? - In relation to the results shown in Fig.3A , I did not understand how the thresholds and parameters for the k-core analysis were chosen . - Pg.3 : there is a typo regarding the size of the output layer ( 5,0000 ) - In Fig.A1 , error bars would help in better understanding the actual difference between the curves . - In order to improve reproducibility , it would be very helpful to share the source code used for these analyses .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your positive comments ! Please see our responses to the points you raised below : - * I think that further analyses are required in order to better clarify some important aspects . In particular , I think that ablation studies should be performed in order to better identify the functional role of the \u201c controller \u201d and \u201c integrator \u201d units , whose actual functional role remains a bit speculative ( and mostly based on structural / connectivity information ) . It would also strengthen the paper to have some more controlled simulations , where the contextual information is defined according to specific linguistic constraints , in order to better characterize what the target units are actually encoding . Indeed , as also noted by the authors almost \u201c all the long timescale units are of unknown function \u201d . * Thank you for the positive comments and valuable suggestions . We agree that it is important to further investigate the functional roles of the units we identified using the current analyses . Each putative \u201c controller unit \u201d may serve a different functions due to its distinctive positions in the MDS space ( Figure 3D ) . It will requiring targeted linguistic experiments to probe each functions . Nonetheless , we performed a preliminary group ablation analysis to look at how ablating the controller units influences model performance , relative to the intact original model , and relative to the ablation of a random set of units . We evaluated model performance by examining the difference of probabilities assigned to the target words in the ablated model and in the intact model , and comparing the conditions when controller units ( N=10 ) /integrator units ( N=5 ) were ablated vs. same number of random units from layer 2were ablated . We used the test corpus used by Gulordava et al.and measured the average performance of each model across 100 text-batches , randomly sampled from the Wikipedia test dataset . Each text-batch was composed of 1000 tokens start from the beginning of a sentence . We found that ablating controller units reduced the probabilities the model assigned to the target words , more so than ablating random units ( controller vs. random across 100 text batches : Cohen \u2019 s d = 4.96 , t=-35.12 , p < 0.001 ) . Ablating integrator units did not show significant difference compared to ablating random units ( Cohen \u2019 s d = 0.8 , t=1.77 , p=0.09 ) . It may be that the integrator units mostly influence the model performance on predicting tokens in cases where long-range information is especially relevant ( e.g.in the later portions of long clauses ) . Overall , these abalation results support the non-trivial functional role for the controller units , which are only 10 amongst 650 units in total . ( Please refer to the figures here : https : //anonymous.4open.science/repository/ef3696c5-e97e-4bd8-b12a-94795a038b8c/ablation_analysis.png ) However , as we mentioned , this only provides a rough sense of the importance of these units compared to other units in the network . Also , as Reviewer 4 commented , ablation analysis has its limitation , and one should be careful interpreting the results ( https : //doi.org/10.1007/s42113-020-00081-z ) . Analyses targeted on specific linguistic properties will be required to further understand the exact functional role of each unit in the controller network , in the vein of Lakretz et al . ( 2019 ) , and we hope that the timescale and network mapping methods we have introduced here can help to target future investigations of this kind . - * Finally , I think that it would be important to establish whether these findings are generally applicable to LSTM models , regardless of the specific architecture under investigation ( e.g. , What happens if we force the LSTM to rely on fewer units ? Does the hierarchical organization of the context improve by adding more layers ? ) . * Reviewer 2 has also raised the concern of whether the results are applicable to other architectures . To address this concern , we trained a GRU language model , mapped the timescales of each layer , mapped the timescales of individual units , and explored the relationship between timescale and connectivity patterns . We found similarities and differences regarding the results between GRU and LSTM ; the distribution of timescales across nodes was similar across the GRU model and the LSTM . The GRU did not show the same timescale-connectivity relationship as the LSTM . These findings remain preliminary because of the limited time available for training the GRU model . For further details , please refer to our response to the third point raised by Reviewer 2 ."}, {"review_id": "J3OUycKwz--3", "review_text": "This paper looks at LSTMs with the intention of understanding their functional connectivity . I am not sure exactly what the relationship between the brain and LSTMs is being assumed or proposed herein \u2014 however I understand the need to understand complex neural networks regardless of their relationship to biological systems . I would have liked to have a discussion with respect to what the hierarchical organisation is due to . Is this merely a repercussion of the connectivity , for example ? What do the authors think ? In terms of work that looks at ablation ( i.e. , damage ) , it might be useful to bear in mind limitations of such work if various ( seemingly , perhaps ) extraneous factors are not taken into account , see : https : //doi.org/10.1007/s42113-020-00081-z I think this paper can be polished to the level of a solidly good paper if the authors can sketch out a bit more their rationale and syllogisms with respect to my above questions . Minor : * Figures are very hard to read , is it possible to redesign them slightly to make the text bigger ? * In LaTeX to open double quotes you need to use two backticks . Also the \\cite and \\citep commands should be used appropriately in terms of places where \\citep is needed as well as use of optional arguments to avoid double parentheses .", "rating": "7: Good paper, accept", "reply_text": "First of all , thank you for your positive comments on the paper . Please see our response to the specific questions/comments below : - * I would have liked to have a discussion with respect to what the hierarchical organisation is due to . Is this merely a repercussion of the connectivity , for example ? What do the authors think ? * Is the hierarchical timescale phenomenon a direct result of the connectivity ? At the level of layers ( i.e.the fact that Layer 1 showed a shorter timescale overall , relative to Layer 2 ) , this effect is likely due to connectivity . If unit B in Layer 2 receives an input from unit A in Layer 1 , and if Unit A is sensitive to changes in the input from N words earlier , then that context-sensitive activation is passed as an input to Unit B . As a result , Unit B will most likely show at least some sensitivity to changes in the input from N words earlier . Thus , on average , when an entire population of units is downstream from another population , we should expect the downstream layer to have longer timescales , on average . The link to connectivity is less straightforward for within-layer connections . Models from the neuroscience literature indicate that higher-degree nodes in dynamical systems will ( under some conditions ) tend to exhibit slower dynamics than lower-degree nodes ( Baria et al. , 2013 , https : //doi.org/10.1016/j.neuroimage.2013.01.072 ) , so one might expect that higher-degree nodes ( simply by virtue of changing state more slowly ) should also have longer context-dependence . However , this is not always the case , as shown in the GRU model that we trained to test the generality of the timescale-connectivity findings ( Please see our Response to Reviewer 2 ) . Moreover , it is certainly theoretically possible for a small subset of very long-timescale nodes to operate through a connection bottleneck . We will revise the manuscript to note these connectivity-timescale relationships , which were also of interest to Reviewer 2 . - * In terms of work that looks at ablation ( i.e. , damage ) , it might be useful to bear in mind limitations of such work if various ( seemingly , perhaps ) extraneous factors are not taken into account , see : https : //doi.org/10.1007/s42113-020-00081-z * Thank you for sharing the interesting article . We agree that ablation methods have some limitations and therefore in the current study we proposed this model-free method to investigate the functional property of individual units without lesioning the model . We will cite this paper in relation to our own new ablation findings in the revised paper . - * Minor : Figures are very hard to read , is it possible to redesign them slightly to make the text bigger ? * We will get feedback from colleagues and attempt to improve the readability and scaling of the figure panels . - * In LaTeX to open double quotes you need to use two backticks . Also the \\cite and \\citep commands should be used appropriately in terms of places where \\citep is needed as well as use of optional arguments to avoid double parentheses . * We will revise the quotation and citation format . Thank you for the tips !"}], "0": {"review_id": "J3OUycKwz--0", "review_text": "This paper applies tools from neuroscience to understand how language models integrate across time . The basic approach is to present a phrase , preceded by two different context phrases : one that is natural ( i.e.the phrase that actually preceded it in the corpus ) and one that is randomly selected . The authors then measure how long it takes for the unit activations to become similar for the two different contexts , which provides a measure for how long the context impacts the representation . They find that ( 1 ) timescales increase at later layers of the language model ( 2 ) that only a small fraction of units exhibit long timescales ( 3 ) that long/medium-timescale units appear to come in two forms which they try and characterize using graph-style analyses . -- Pros : How language models integrate across time is clearly important , and this paper describes interesting first steps in characterizing the analysis of time using relevant tools from the neuroscience literature . The method presented is simple and broadly applicable . The graph-style results seem intriguing if a little hard to make sense of . I also think that the sparsity of the long-timescale units is cool and interesting . -- Limitations and questions : 1 . It \u2019 s not clear to me if the notion of time is a meaningful one in a language model . For example , the duration of contextual effects on a unit that codes syntactic number will presumably be highly variable and depend upon the details of the particular sentence being encoded . Thus a natural question is how variable are these timescales from moment-to-moment ? What \u2019 s being plotted is the average across a bunch of sentences , segmented at a particular moment ( a conjunction ) . How robust are these results if one examines a different point in a sentence ? Are the timescales of some units more variable than others ? -- Update : the authors have repeated their analysis for a different sentence point ( after the 10th word ) and report similar results . This analysis is helpful , though of course the 10th word is not a very principled break point , and there presumably is a lot of variation in timescales that are being averaged across . I continue to wonder how meaningful the notion of an absolute timescale is . -- 2.None of the steps in the graph analyses seemed particularly natural or well-motivated to me . Why were the graph edges thresholded at z > 5 and why was k-core analysis performed ? I find it hard to make sense of what this analysis tells us about how language information is processed . Is there some reason why medium timescale \u201c controller \u201d units and long-timescale \u201c integrator \u201d units should help with language processing ? If these results are purely exploratory and lack a clear interpretation , then perhaps the authors could help the reader by explaining the thought process behind the exploration . Perhaps starting with the MDS plot would be useful rather than the k-core analysis , because the MDS plot clearly shows some interesting structure . -- The authors have motivated some of their analyses by discussing brain research reporting that longer-timescale regions are more densely connected . Of course , the relationship between connectivity between large-scale brain regions and the units in a LSTM remains highly speculative . But having some motivation is helpful . -- 3.It would be interesting to know how dependent these findings are on the model \u2019 s architecture . Would similar results be found for a Transformer or a simpler GRU-style RNN ? -- The authors have attempted to address this point , but with limited time were not able to train a network to a high level of performance . Minor points : In Figure 4 , it would be helpful if the absolute timescale was labeled in all plots rather than the rank of the unit or the \u201c normalized timescale \u201d . The absolute timescale seems much more meaningful to me ( and the units can of course still be ranked , just the axis labels changed or augmented ) . The legend for Figure 4c is incorrect .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your valuable feedback ! Please see our point-by-point response below : - * It \u2019 s not clear to me if the notion of time is a meaningful one in a language model . For example , the duration of contextual effects on a unit that codes syntactic number will presumably be highly variable and depend upon the details of the particular sentence being encoded . Thus a natural question is how variable are these timescales from moment-to-moment ? What \u2019 s being plotted is the average across a bunch of sentences , segmented at a particular moment ( a conjunction ) . How robust are these results if one examines a different point in a sentence ? Are the timescales of some units more variable than others ? * Thank you for raising the point . Indeed , the timescale of individual units could vary based on the syntactic distance , which can be different from the token distance measured in our study . As pointed out by reviewer 3 , one would expect that functional units such as syntax or number units should \u201c reset \u201d their timescale at certain points of the sentence ( please refer to our response to the figures generated for Reviewer 3 regarding the reset process ) . Thank you for the suggestion about testing other locations in sentences , rather than just the specific \u201c , and \u201d conjunction . We conducted a new analysis in which we segmented the context and shared segments at a fixed distance from the sentence onset ( i.e.we pick the 10th token as the segmentation point ) . We found that the timescale organization was largely preserved , regardless of whether we measured the cross-context effect at the \u201c , and \u201d boundary [ as in our original analysis ] or simply before and after the 10th token [ as in this new analysis ] . ( Please refer to the figure here : https : //anonymous.4open.science/repository/ef3696c5-e97e-4bd8-b12a-94795a038b8c/timescale_corr_commaAnd_middle.png ) . At the same time , we did find that there was a small subset of units ( fewer than 10 units ) whose timescales were clearly shorter following the \u201c , and \u201d conjunction . This is intriguing , as it suggests that some units were \u201c resetting \u201d their context following the \u201c , and \u201d conjunction . In future work , we will also seek to identify whether these units . In all , we agree with the reviewer that the timescales measured using \u201c tokens \u201d may be flexible , and can vary according to the syntactic context , for example . To address how syntactic distance affects context encoding in individual unit , carefully controlled context conditions are needed , which is beyond the scope of the current study . However , we would like to stress the importance of token distance for RNNs in general , since it serves as an important parameter for RNNs to predict the next token in any kind of sequences , before it learns to flexibly integrate information based on syntactic distance in language specifically . We will revise the manuscript to include the analysis suggested by the reviewer , which suggests that the timescales measured at the level of tokens are robust across different choices of the location of the prior context segment ."}, "1": {"review_id": "J3OUycKwz--1", "review_text": "_ * * Update after author response * * _ : I think this is a very promising paper , and I am really excited about seeing techniques from neuroscience employed to answer questions about neural network models . The authors have further conducted several additional experiments after reviewer comments , which I appreciate . However , my most fundamental concern -- the mismatch between the method and the way that it is validated -- unfortunately still stands , which is why I would encourage the authors to further pursue this line of work , but recommend to reject it for ICLR . * * Summary * * This paper proposes to apply time-scale methods from neuroscience to investigate the timescale organisation in neural language models . More specifically , the authors test the timescale of individual units in a word- and character-level LSTM by comparing the units ' activations values on the same sentence , but with different contexts . Using this method , the authors first show that the higher layers on average have longer timescales . They then , for all units , they fit a logistic function to the `` recovery '' curves and use the half-times of this curves as an indication of the time scale of these units . They test the syntax unit and two long-distance units found by Lakretz et al and show that the number units have similar time-scales , while the syntax unit have a longer time scale . Lastly , the authors analyse the connectivity between the longer time scale units and find that the units with longer processing timescales make a larger number of strong projections . Within these units , the authors identify two sets of units in the word-level LSTM : `` controller units '' , that play a role in how the connectivity of the network is updated , and `` integrator units '' , that instead integrate information . * * Strong points * * - Neuroscience has long been asking questions about the brain that are very similar to the questions we now ask about neural networks , cross-pollination between these fields is extremely important , and this paper contributes to this - Aside from the main technique , the paper introduces some interesting and useful methods , such as projectivity analysis and k-core analysis . I think these methods can be useful for other researchers as well - Time scale analysis of LSTMs is a very relevant and interesting topic , that deserves more attention than it is currently getting * Concerns * - My main concern is that there seems to be a mismatch between the `` language time scales '' on which the authors operate : their experiment is designed to investigate the impact of extra-sentential context , but the Lakretz et al results they keep coming back to concern syntactic phenomena that are only relevant * within * a sentence , which is a different scale . In other words , the units found by the authors of this paper are long-distance when it comes to integrating context , but the syntax and number units found by Lakretz et al are not really related to that : they model relationships * within * sentences . Theoretically speaking , they should be reset at the beginning of every new sentence and they should thus be completely independent from the content . That the authors find this to be untrue is interesting , but inconsistent with what Lakretz et al describe these unit do . Since this is not addressed at all in the paper , it makes the results in general a bit difficult to interpret . _ * * Update after author response * * : In their response the authors clarified that the they have only analysed single sentences , where two distinct subsentences are combined with a conjunction . This , unfortunately , does not make a difference for the argument : whether two sentences are split by a full stop or instead concatenated with `` and '' does not make any difference for the argument above , since the subject-verb agreement relationships that the units the authors look at model do not cross these boundaries either . Furthermore , in their response the authors state that the find that the context representations of units was 'reset ' at sentence boundaries , as I asked before . I appreciate that the authors did these additional experiments , but I find the result somewhat worrisome : since the units they are looking at are syntactic units that encode number across long distance subject verb relationships , they should be reset both when a new sentence starts , as well as when a new conjunct with a new relationship starts . In terms of SV relationships , there should be no difference between `` The boy kicked the ball and the girl caught it '' and `` The boy kicked the ball . The girl caught it . '' That the authors do find a difference points to a potential flaw in methodology._ - Relatedly , the authors say that their result that the syntax unit is a long distance unit , while the number units are not . This is not consistent with what they say in the related work of the section , but also not with the results reported by Lakretz et al , who hypothesise that the syntax units represent the depth of the syntactic dependency . This is something that changes with every new incoming word , whereas the number units are the ones that have to keep their activation constant across time . - While , as I said before , I think it is great that the authors try to use methods from neuroscience into the field , I do think that in this case the main method they propose is only very marginally different from earlier work ( in particular Khandelwal et al.Perhaps it would make more sense to put a bit more stress on the rest of the methods as well ( btw , also Lakretz et al do connectivity analysis ) . - The results are a bit underexplained , and understanding them requires many back and forths to the appendix . I would have appreciated a bit more motivated interpretation of several aspects . For instance : why is there such a large difference in activation differences in different units in the `` pre-shared segment '' part , and is this related to the half-time ( it seems so from the plots ) ? What is the difference between character and word-level models in terms of expectations ( we 'd expect there to be an additional level of time-hierarchy , perhaps ? ) How do assessing activation differences and correlations differ in terms of conclusions ? These things should , in my opinion , all be worked out a bit better . - Lastly , there are a few unsupported claims , the most important of which that their method recovers the previously discovered units of Lakretz et al , while ( as far as I understand ) , they actually only * use * their method to analyse those neurons , but did not find them independently . ( for other suggestions and comments , see below ) . To summarise , while I think the idea is very nice and definitely worth working out further , I do think that some work is needed to make this a publishable paper . * Suggestions/comments for authors * _Typographic_ : - If you use quotes in latex , you should use different ones for left ( ` ) and right ( ' ) , for them to appear correctly ( check for instance line three in the introduction ) - To prevent additional spaces after abbreviations like e.g.and i.e. , put a backslash : `` e.g.\\ `` - Lerner et al -- > put all references within parenthesis - Introduction switches from present tense to paste tense in the last paragraph - `` we measure the time-taken for the effect of this prior context to \u201d decay \u201d ( see Methods ) '' -- > I do n't really understand what this means , you measure how long it takes for these changes to not be measurable anymore ? - Try to avoid double parethesis with abbreviations , e.g . : ( WLSTM Gulordava et al . ( 2018 ) ) should be : ( WLSTM , Gulordava et al ; 2018 ) . You can do this with \\citep [ text before ] [ text after ] { citation } . - `` has an 650-dimensional '' -- > `` has a 650-dimensional '' - `` without fine-tuning to the novel '' -- > I first thought this sentence was unfinished until I read back and realised that `` the novel '' is your corpus . This is a bit confusing perhaps you could rephrase . - `` how the cell state activation differ '' -- > `` how the cell state activations differ '' - `` we will see that the activation difference drop quickly ' -- > drops quickly / see the activation difference drop quickly - There are several references that were published at ACL * conferences that are listed as arxiv papers in the reference list ( Lakretz et al , Gulordava et al , Khandelwal et al ) _Content_ - I would say that the conclusion that `` Overall , prior works suggests that a small subset of units track long-range dependencies '' is rather overstated : Lakretz et al found that the units representing long distance number information were sparse , but this does not imply that long range information in general is represented sparsely . Their method also focusses quite exclusively on finding sparsely distributed properties , as more distributed properties can not be found with ablation . Furthermore , this is just one study , focusing on one syntactic aspect . I would suggest to rephrase this a bit . - Lakretz at all actually identified several syntax units , but only one of them was interpretable . - I find it a bit confusing that in 3.2 , second paragraph , you first talk about comparing cell state activation , then say that you compare hidden state activations and then talk again about the cell state activation - Figure 1 C & D : I do n't think these figures add much to the paper , for the following reasons i ) They show only individual units and no average , making it difficult to interpret the values ii ) while , as pointed out in 5.1 , the * rate * of decay is the most important , the cut-off point is not indicated in the figure , which puts a stress on irrelevant aspects : the actual difference between the two lines . - I would appreciate to have Figure A.1 in the main text , it is important for the story .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for acknowledging the potential contribution of this paper , including the importance of transferring methods from brain science to understand neural network AI models , and the importance of analyzing timescales in neural language models . Here are the point-by-point responses to the concerns raised by the reviewer : - * My main concern is that there seems to be a mismatch between the `` language time scales '' on which the authors operate : their experiment is designed to investigate the impact of extra-sentential context , but the Lakretz et al results they keep coming back to concern syntactic phenomena that are only relevant within a sentence , which is a different scale . In other words , the units found by the authors of this paper are long-distance when it comes to integrating context , but the syntax and number units found by Lakretz et al are not really related to that : they model relationships within sentences . Theoretically speaking , they should be reset at the beginning of every new sentence and they should thus be completely independent from the content . That the authors find this to be untrue is interesting , but inconsistent with what Lakretz et al describe these unit do . Since this is not addressed at all in the paper , it makes the results in general a bit difficult to interpret . * We apologize for the confusion here . Crucially , our work only examines phenomena within an individual sentence , just as in the Lakretz study . The reviewer is correct that context representations are \u2018 reset \u2019 at sentence boundaries in the Gulordava et al.model ( see below , where we confirm this in our own data ) . For this reason in an early draft of our manuscript , we had only analyzed single sentences which combined two distinct sub-sentences in the following way , e.g . : \u201c The boy kicked the ball , and the girl caught it. \u201d Since the segment before the conjunction \u201c , and \u201d can be read as a self-contained sentence , our original paper contained text such as : \u201c the preceding sentence differed across the two conditions \u201d ( in Section 3.2 ) . However , to be clear , these preceding segments were always part of the same sentence . Thus , in all of the data reported in our paper , we only examined context effects within a single sentence . We apologize again for the confusion . For improved clarity , we have now revised the manuscript so that the word \u201c segment \u201d is used consistently throughout to refer to the prior context segment and to the shared input segment . Consistent with the reviewer \u2019 s statement , we did find that the context representation of units was \u201c reset \u201d at sentence boundaries . To demonstrate this , we examined the timescales of each unit when the context segment and shared input segment were separated by a \u201c full stop \u201d symbol , which signals the end of a sentence . ( Please refer to the result figure here : https : //anonymous.4open.science/repository/ef3696c5-e97e-4bd8-b12a-94795a038b8c/fullstop_reset.png ) . When the first and second segments were separated \u201c full stop \u201d symbol , we found that the timescales inferred for most of the units became extremely short , compared to when the first and second segments were separated with a \u201c comma \u201d symbol as in our original paper . This \u201c reset \u201d phenomenon at the beginning of every new sentence is consistent with what the reviewer predicts and with the results of Lakretz et al ."}, "2": {"review_id": "J3OUycKwz--2", "review_text": "This paper explores the application of innovative methods to track the flow of linguistic information in LSTM language models . In particular , the overarching question is how contextual information might be encoded in the network at the level of single units , and how context disruption might alter the LSTM dynamics and thus impact its predictive ability . The paper is clear and it tackles an interesting question . The approach is well motivated , and the authors give a brief survey of the most recent applications of this kind of methodology in linguistics and cognitive neuroscience studies . The methodology is generally appropriate , though some details and parameters ( e.g. , numerical thresholds ) seem to be chosen arbitrarily . Also , the analysis could be improved by applying statistical testing in order to better quantify the strength of the observed effects . Overall , I think this is a nice paper , though it might be especially relevant to the linguistics community rather than to the ICLR community . Moreover , I think that further analyses are required in order to better clarify some important aspects . In particular , I think that ablation studies should be performed in order to better identify the functional role of the \u201c controller \u201d and \u201c integrator \u201d units , whose actual functional role remains a bit speculative ( and mostly based on structural / connectivity information ) . It would also strengthen the paper to have some more controlled simulations , where the contextual information is defined according to specific linguistic constraints , in order to better characterize what the target units are actually encoding . Indeed , as also noted by the authors almost \u201c all the long timescale units are of unknown function \u201d . Finally , I think that it would be important to establish whether these findings are generally applicable to LSTM models , regardless of the specific architecture under investigation ( e.g. , What happens if we force the LSTM to rely on fewer units ? Does the hierarchical organization of the context improve by adding more layers ? ) . Other comments : - Why did the author choose to test the model on a different corpus ( Anna Karenina novel ) rather than considering a test set from the same corpus from which the training set was derived ? The Tolstoy book might have a quite different linguistic structure from that of the corpora used to train the LSTMs . - It might be informative to also include a third condition in-between \u201c Intact \u201d and \u201c Random \u201d context , where the same context words are maintained with scrambled order . This would allow to better understand the role of individual words in shaping context representation and activating the LSTM units . - In Fig.1D , it is interesting to note that the Unit 823 ( green line ) actually exhibits a sharp increase in difference after the shared segment starts . Do the authors have a possible explanation for this kind of phenomena ? Was it observed systematically in other units ? - In relation to the results shown in Fig.3A , I did not understand how the thresholds and parameters for the k-core analysis were chosen . - Pg.3 : there is a typo regarding the size of the output layer ( 5,0000 ) - In Fig.A1 , error bars would help in better understanding the actual difference between the curves . - In order to improve reproducibility , it would be very helpful to share the source code used for these analyses .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your positive comments ! Please see our responses to the points you raised below : - * I think that further analyses are required in order to better clarify some important aspects . In particular , I think that ablation studies should be performed in order to better identify the functional role of the \u201c controller \u201d and \u201c integrator \u201d units , whose actual functional role remains a bit speculative ( and mostly based on structural / connectivity information ) . It would also strengthen the paper to have some more controlled simulations , where the contextual information is defined according to specific linguistic constraints , in order to better characterize what the target units are actually encoding . Indeed , as also noted by the authors almost \u201c all the long timescale units are of unknown function \u201d . * Thank you for the positive comments and valuable suggestions . We agree that it is important to further investigate the functional roles of the units we identified using the current analyses . Each putative \u201c controller unit \u201d may serve a different functions due to its distinctive positions in the MDS space ( Figure 3D ) . It will requiring targeted linguistic experiments to probe each functions . Nonetheless , we performed a preliminary group ablation analysis to look at how ablating the controller units influences model performance , relative to the intact original model , and relative to the ablation of a random set of units . We evaluated model performance by examining the difference of probabilities assigned to the target words in the ablated model and in the intact model , and comparing the conditions when controller units ( N=10 ) /integrator units ( N=5 ) were ablated vs. same number of random units from layer 2were ablated . We used the test corpus used by Gulordava et al.and measured the average performance of each model across 100 text-batches , randomly sampled from the Wikipedia test dataset . Each text-batch was composed of 1000 tokens start from the beginning of a sentence . We found that ablating controller units reduced the probabilities the model assigned to the target words , more so than ablating random units ( controller vs. random across 100 text batches : Cohen \u2019 s d = 4.96 , t=-35.12 , p < 0.001 ) . Ablating integrator units did not show significant difference compared to ablating random units ( Cohen \u2019 s d = 0.8 , t=1.77 , p=0.09 ) . It may be that the integrator units mostly influence the model performance on predicting tokens in cases where long-range information is especially relevant ( e.g.in the later portions of long clauses ) . Overall , these abalation results support the non-trivial functional role for the controller units , which are only 10 amongst 650 units in total . ( Please refer to the figures here : https : //anonymous.4open.science/repository/ef3696c5-e97e-4bd8-b12a-94795a038b8c/ablation_analysis.png ) However , as we mentioned , this only provides a rough sense of the importance of these units compared to other units in the network . Also , as Reviewer 4 commented , ablation analysis has its limitation , and one should be careful interpreting the results ( https : //doi.org/10.1007/s42113-020-00081-z ) . Analyses targeted on specific linguistic properties will be required to further understand the exact functional role of each unit in the controller network , in the vein of Lakretz et al . ( 2019 ) , and we hope that the timescale and network mapping methods we have introduced here can help to target future investigations of this kind . - * Finally , I think that it would be important to establish whether these findings are generally applicable to LSTM models , regardless of the specific architecture under investigation ( e.g. , What happens if we force the LSTM to rely on fewer units ? Does the hierarchical organization of the context improve by adding more layers ? ) . * Reviewer 2 has also raised the concern of whether the results are applicable to other architectures . To address this concern , we trained a GRU language model , mapped the timescales of each layer , mapped the timescales of individual units , and explored the relationship between timescale and connectivity patterns . We found similarities and differences regarding the results between GRU and LSTM ; the distribution of timescales across nodes was similar across the GRU model and the LSTM . The GRU did not show the same timescale-connectivity relationship as the LSTM . These findings remain preliminary because of the limited time available for training the GRU model . For further details , please refer to our response to the third point raised by Reviewer 2 ."}, "3": {"review_id": "J3OUycKwz--3", "review_text": "This paper looks at LSTMs with the intention of understanding their functional connectivity . I am not sure exactly what the relationship between the brain and LSTMs is being assumed or proposed herein \u2014 however I understand the need to understand complex neural networks regardless of their relationship to biological systems . I would have liked to have a discussion with respect to what the hierarchical organisation is due to . Is this merely a repercussion of the connectivity , for example ? What do the authors think ? In terms of work that looks at ablation ( i.e. , damage ) , it might be useful to bear in mind limitations of such work if various ( seemingly , perhaps ) extraneous factors are not taken into account , see : https : //doi.org/10.1007/s42113-020-00081-z I think this paper can be polished to the level of a solidly good paper if the authors can sketch out a bit more their rationale and syllogisms with respect to my above questions . Minor : * Figures are very hard to read , is it possible to redesign them slightly to make the text bigger ? * In LaTeX to open double quotes you need to use two backticks . Also the \\cite and \\citep commands should be used appropriately in terms of places where \\citep is needed as well as use of optional arguments to avoid double parentheses .", "rating": "7: Good paper, accept", "reply_text": "First of all , thank you for your positive comments on the paper . Please see our response to the specific questions/comments below : - * I would have liked to have a discussion with respect to what the hierarchical organisation is due to . Is this merely a repercussion of the connectivity , for example ? What do the authors think ? * Is the hierarchical timescale phenomenon a direct result of the connectivity ? At the level of layers ( i.e.the fact that Layer 1 showed a shorter timescale overall , relative to Layer 2 ) , this effect is likely due to connectivity . If unit B in Layer 2 receives an input from unit A in Layer 1 , and if Unit A is sensitive to changes in the input from N words earlier , then that context-sensitive activation is passed as an input to Unit B . As a result , Unit B will most likely show at least some sensitivity to changes in the input from N words earlier . Thus , on average , when an entire population of units is downstream from another population , we should expect the downstream layer to have longer timescales , on average . The link to connectivity is less straightforward for within-layer connections . Models from the neuroscience literature indicate that higher-degree nodes in dynamical systems will ( under some conditions ) tend to exhibit slower dynamics than lower-degree nodes ( Baria et al. , 2013 , https : //doi.org/10.1016/j.neuroimage.2013.01.072 ) , so one might expect that higher-degree nodes ( simply by virtue of changing state more slowly ) should also have longer context-dependence . However , this is not always the case , as shown in the GRU model that we trained to test the generality of the timescale-connectivity findings ( Please see our Response to Reviewer 2 ) . Moreover , it is certainly theoretically possible for a small subset of very long-timescale nodes to operate through a connection bottleneck . We will revise the manuscript to note these connectivity-timescale relationships , which were also of interest to Reviewer 2 . - * In terms of work that looks at ablation ( i.e. , damage ) , it might be useful to bear in mind limitations of such work if various ( seemingly , perhaps ) extraneous factors are not taken into account , see : https : //doi.org/10.1007/s42113-020-00081-z * Thank you for sharing the interesting article . We agree that ablation methods have some limitations and therefore in the current study we proposed this model-free method to investigate the functional property of individual units without lesioning the model . We will cite this paper in relation to our own new ablation findings in the revised paper . - * Minor : Figures are very hard to read , is it possible to redesign them slightly to make the text bigger ? * We will get feedback from colleagues and attempt to improve the readability and scaling of the figure panels . - * In LaTeX to open double quotes you need to use two backticks . Also the \\cite and \\citep commands should be used appropriately in terms of places where \\citep is needed as well as use of optional arguments to avoid double parentheses . * We will revise the quotation and citation format . Thank you for the tips !"}}