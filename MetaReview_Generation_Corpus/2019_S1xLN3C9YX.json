{"year": "2019", "forum": "S1xLN3C9YX", "title": "Learnable Embedding Space for Efficient Neural Architecture Compression", "decision": "Accept (Poster)", "meta_review": "The authors propose a method to learn a neural network architecture which achieves the same accuracy as a reference network, with fewer parameters through Bayesian Optimization. The search is carried out on embeddings of the neural network architecture using a train bi-directional LSTM. The reviewers generally found the work to be clearly written, and well motivated, with thorough experimentation, particularly in the revised version. Given the generally positive reviews from the authors, the AC recommends that the paper be accepted.\n", "reviews": [{"review_id": "S1xLN3C9YX-0", "review_text": "Review: This paper proposes a method for finding optimal architectures for deep neural networks based on a teacher network. The optimal network is found by removing or shrinking layers or adding skip connections. A Bayesian Optimization approach is used by employing a Gaussian Process to guide the search and the acquisition function expected improvement. A special kernel is used in the GP to model the space of network architectures. The method proposed is compared to a random search strategy and a method based on reinforcement learning. Quality: The quality of the paper is high in the sense that it is very well written and contains exhaustive experiments with respect to other related methods Clarity: The paper is well written in general with a few typos, e.g., \"The weights of the Bi-LSTM \u03b8, is learned during the search process. The weights \u03b8 determines\" Originality: The proposed method is not very original in the sense that it is a combination of several known techniques. May be the most original contribution is the proposal of a kernel for network architectures based on recurrent neural networks. Another original idea is the use of sampling to avoid the problem of doing kernel over-fitting. Something that can be questioned, however, in this regard is the fact that instead of averaging over kernels the GP prediction to account for uncertainty in the kernel parameters, the authors have suggested to optimize a different acquisition function per each kernel. This can be problematic since for each kernel over-fitting can indeed occur, although the experimental results suggest that this is not happening. Significance: Why N2N does not appear in all the CIRFAR-10 and CIFAR-100 experiments? This may question the significance of the results. It also seems that the authors have not repeated the experiments several times since there are no error bars in the results. This may also question the significance of the results. An average over several repetitions is needed to account for the randomness in for example the sampling of the network architectures to learn the kernels. Besides this, the authors may want to cite this paper Hern\u00e1ndez-Lobato, D., Hernandez-Lobato, J., Shah, A., & Adams, R. (2016, June). Predictive entropy search for multi-objective Bayesian optimization. In International Conference on Machine Learning (pp. 1492-1501). which does multi-objective Bayesian optimization of deep neural networks (the objectives are accuracy and prediction time). Pros: - Well written paper. - Simply idea. - Extensive experiments. Cons: - The proposed approach is a combination of well known methods. - The significance of the results is in question since the authors do not include error bars in the experiments.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the feedback and suggestions . We have addressed all the questions here : * * * Response to questions about the performance of N2N : * * * All the numbers of N2N are from their original paper but N2N did not test their method to compress ShuffleNet so we do not have the performance of N2N on ShuffleNet . N2N did not test their method under the setting VGG-19 on CIFAR-100 either . For ResNet-34 on CIFAR-100 , N2N only provides results of layer removal ( indicated by \u2018 N2N - removal \u2019 in Table 1 in our paper ) so for fair comparison , we compare \u2018 N2N - removal \u2019 with \u2018 Ours - removal \u2019 , which refers to only considering the layer removal operation in the search space . \u2018 Ours - removal \u2019 also significantly outperforms \u2018 N2N - removal \u2019 in terms of both the accuracy and the compression ratio . * * * Response to questions about experiment results : * * * We re-run the experiments for 3 times and update the results in the paper ( please check the PDF ) . In Table 1 , we show the mean and standard deviation of the results for \u2018 Ours \u2019 and \u2018 Random Search \u2019 . We observe that after multiple runs , the average performance of our method also outperforms all the baselines as before . * * * Response to questions about the related work : * * * We have updated the paper and added this paper in related work . Also in the conclusion section , we think it \u2019 s an interesting future direction to combine their method with our proposed embedding space to identify the Pareto set of the architectures that are both small and accurate . Thanks for suggesting the related work ! * * * Response to questions about the originality of our work : * * * We would like to emphasize that our key contribution is a novel method that incrementally learns an embedding space for the architecture domain , i.e. , a unified representation for the configuration of architectures . The learned embedding space can be used to compare architectures with complex skip connections and multiple branches and we can combine it with any Sequential Model-Based Optimization ( SMBO ) method ( we choose GP based BO algorithms in this work ) to search for desired architectures . Based the learned embedding space , we present a framework of searching for compressed network architectures with Bayesian optimization ( BO ) . The learned embedding provides a feature space over which the kernel function of BO is defined . Under this framework , we propose a set of architecture operators for generating architectures for search and a multiple kernel strategy to encourage the search algorithm to explore more diverse architectures . We demonstrate that our method can significantly outperform various baseline methods , such as random search and N2N ( Ashok et al.,2018 ) . The compressed architectures found by our method are also better than the state-of-the-art manually-designed compact architecture ShuffleNet ( Zhang et al. , 2018 ) . We also demonstrate that the learned embedding space can be transferred to new settings for architecture search , such as a larger teacher network or a teacher network in a different architecture family , without any training ."}, {"review_id": "S1xLN3C9YX-1", "review_text": "================ Post-Rebuttal ================ I thank the authors for the larger amount of additional work they put into the rebuttal. Since the authors addressed my main concerns, i e. comparison to existing methods, clarifications of the proposed approach, adding references to related work, I will increase my score and suggest to accept the paper. The paper describes a new neural architecture search strategy based on Bayesian optimization to find a compressed version of a teacher network. The main contribution of the paper is to learn an embedding that maps from a discrete encoding of an architecture to a continuous latent vector such that standard Bayesian optimization can be applied. The new proposed method improves in terms of compressing the teacher network with just a small drop in accuracy upon an existing neural architecture search method based on reinforcement learning and random sampling. Overall, the paper presents an interesting idea to use Bayesian optimization on high dimensional discrete problems such as neural architecture search. I think a particular strength of this methods is that the embedding is fairly general and can be combined with various recent advances in Bayesian optimization, such as, for instance, multi-fidelity modelling. It also shows on some compression experiments superior performance to other state-of-the-art methods. However, in its current state I do not think that the paper is read for acceptance: - Since the problem is basically just a high dimensional, discrete optimization problem, the paper misses comparison to other existing Bayesian optimization methods such as TPE [1] / SMAC [2] that can also handle these kind of input spaces. Both of these methods have been applied to neural architecture search [3][4] before. Furthermore, since the method is highly related to NASBOT [5], it would be great to also see a comparison to it. - I assume that in order to learn a good embedding, similar architectures need to be mapped to latent vector that are close in euclidean space, such that the Gaussian process kernel can model any correlation[7]. How do you make sure that the LSTM learns a meaningful embedding space? It is also a bit unclear why the performance f is not used directly instead of p(f|D). Using f instead of p(f|D) would probably also make continual training of the LSTM easier, since function values do not change. - The experiment section misses some details: - Do the tables report mean performances or the performance of single runs? It would also be more convincing if the table contains error bars on the reported numbers. - How are the hyperparameters of the Gaussian process treated? - The related work section misses some references to Lu et al.[6] and Gomez-Bombarelli et al.[7] which are highly related. - What do you mean with the sentence \"works on BO for NAS can only tune feed-forward structures\" in the related work section? There is no reason why other Bayesian optimization should not be able to also optimize recurrent architectures (see for instance Snoek et al.[8]). - Section 3.3 is a bit confusing and to be honest I do not get the motivation for the usage of multiple kernels. Why do the first architectures biasing the LSTM? Since Bayesian optimization with expected improvement samples around the global optimum, should not later evaluated, well-performing architectures more present in the training dataset for the LSTM? [1] Algorithms for Hyper-Parameter Optimization J. Bergstra and R. Bardenet and Y. Bengio and B. Kegl Proceedings of the 25th International Conference on Advances in Neural Information Processing Systems (NIPS'11) [2] Sequential Model-Based Optimization for General Algorithm Configuration F. Hutter and H. Hoos and K. Leyton-Brown Proceedings of the Fifth International Conference on Learning and Intelligent Optimization (LION'11) [3] Towards Automatically-Tuned Neural Networks H. Mendoza and A. Klein and M. Feurer and J. Springenberg and F. Hutter ICML 2016 AutoML Workshop [4] Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures J. Bergstra and D. Yamins and D. Cox Proceedings of the 30th International Conference on Machine Learning (ICML'13) [5] Neural Architecture Search with Bayesian Optimisation and Optimal Transport K. Kandasamy and W. Neiswanger and J. Schneider and B. P{\\'{o}}czos and E. Xing abs/1802.07191 [6] Structured Variationally Auto-encoded Optimization X. Lu and J. Gonzalez and Z. Dai and N. Lawrence Proceedings of the 35th International Conference on Machine Learning [7] Automatic chemical design using a data-driven continuous representation of molecules R. G\u00f3mez-Bombarelli and J. Wei and D. Duvenaud and J. Hern\u00e1ndez-Lobato and B. S\u00e1nchez-Lengeling and D. Sheberla and J. Aguilera-Iparraguirre and T. Hirzel. and R. Adams and A. Aspuru-Guzik American Chemical Society Central Science [8] Scalable {B}ayesian Optimization Using Deep Neural Networks J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams Proceedings of the 32nd International Conference on Machine Learning (ICML'15)", "rating": "7: Good paper, accept", "reply_text": "Here is our response to other questions : * * * Response to questions about experimental details : * * * We re-run the experiments for 3 times and update the results in the paper ( please check the PDF ) . In Table 1 , we show the mean and standard deviation of the results for \u2018 Ours \u2019 and \u2018 Random Search \u2019 . We observe that after multiple runs , the average performance of our method also outperforms all the baselines as before . The mean of the Gaussian process prior is set to zero . The Gaussian noise variance is set to 0.05 . The kernel width parameter $ \\sigma $ ( defined in Eq 4 ) in the RBF kernel is set as $ \\sigma^2=0.01 $ . * * * Response to questions about related work : * * * Thanks for suggesting the related work . We have updated the paper and added [ 6 ] and [ 7 ] in the related work section . For your convenience , here is the text about [ 6 ] and [ 7 ] in the paper : \u201c Our work can also be viewed as carrying out optimization in the latent space of a high dimensional and structured space , which shares a similar idea with previous literature [ 6 ] [ 7 ] . For example , [ 6 ] presents a new variational auto-encoder to map kernel combinations produced by a context-free grammar into a continuous and low-dimensional latent space. \u201d * * * Response to \u201c What do you mean with the sentence `` works on BO for NAS can only tune feed-forward structures '' in the related work section ? \u201d : * * * We are sorry for the confusion of the term \u2018 feed-forward structures \u2019 in this sentence . We have corrected the sentence to \u201c However , most existing works on BO for NAS only show results on tuning network architectures where the connections between network layers are fixed , i.e. , most of them do not optimize how the layers are connected to each other. \u201d For example , [ 8 ] tunes the hidden size , the embedding size and other architectural parameters in the language model but it does NOT change how the layers in the model are connected to each other . Our results ( Table 5 in Appendix 6.3 ) show that optimizing how the layers are connected ( in this work , by adding skip connections ) is crucial to the performance of the compressed network architecture . The fundamental reason why previous works on BO for NAS do not optimize how the layers are connected is that there lacked a principled way to quantify the similarity between two architectures with complex skip connections , which is addressed by our proposed learnable embedding space . They can benefit our proposed method to be extended to optimize how the layers are connected . * * * Response to questions about the motivation of using multiple kernels : * * * Sorry for the confusion in Sec 3.3 . We have edited Sec 3.3 to make the motivation more clear . The main motivation of training multiple kernels is to encourage the search algorithm to explore more diverse architectures . We only evaluate 160 architectures during the whole search process so it is possible the learned kernel is overfitted to the training samples and bias the following sampled architectures for evaluation . To encourage the search algorithm to explore more diverse architectures , we propose the usage of multiple kernels , motivated by the bagging algorithm , which is usually employed to avoid overfitting . Regarding the statement about the first architecture biasing the LSTM , this statement is invalid in the current context and we have removed it from the paper . This was a conjecture at the early development stage of this work and we mistakenly put it here ."}, {"review_id": "S1xLN3C9YX-2", "review_text": "In this work, the authors propose a new strategy to compress a teacher neural network. Briefly, the authors propose using Bayesian optimization (BO) where the accuracy of the networks is modelled using a Gaussian Process function with a squared exponential kernel on continuous neural network (NN) embeddings. Such embeddings are the output of a bidirectional LSTM taking as input the \u201craw\u201d (discrete) NN representations (when regarded as a covariance function of the \u201craw\u201d (discrete) NN representations, the kernel is a deep kernel). The authors apply this framework for model compression. In this application, the search space is the space of networks obtained by sampling reducing operations on a teacher network. In applications to CIFAR-10 and CIFAR-100 the authors show that the accuracies of the compressed network obtained through their method exceeds accuracies obtained through other methods for compression, manually compressed networks and random sampling. I have the following concerns/questions: 1) The authors motivate their work in the introduction by discussing the importance of learning a good embedding space over network architectures to \u201cgenerate a priority ordering of architectures for evaluation\u201d. Within the proposed BO framework, this would require the optimization of the expected improvement in a high-dimensional and discrete space (the space of NN architectures), which \u201cis non-trivial\u201d. In this work, the authors do not try to solve this general problem, but specialize their work to model compression, which has a much lower dimensional search space (space of networks obtained by sampling reducing operations on a teacher network). For this reason, I believe the presentation and motivation of this work is not presented clearly. Specifically, while I agree that the methods and results in this paper can be relevant to the problem of getting NN embeddings for a larger search space, this should be discussed in the conclusion/discussion as future direction, rather than as motivating example. Generally, I think the method should be described in the context of model compression rather than as a general method for neural architecture search (NAS) method (in my understanding, its use for NAS would be unfeasible). 2) I have been wondering why the authors optimize the kernel parameters by maximizing the predictive GP posterior rather than maximizing the GP log marginal likelihood as in standard GP regression? 3) The sampling procedure should be explained in greater detail. How many reducing operations are sampled? This would be important to fully understand the random search method the authors consider for comparison in their experiments. I expect that the results from that method will strongly depend on the sampling procedure and different choices should probably be explored for a fair comparison. Do the authors have any comment on this? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the useful feedback ! Here is our response : * * * Response to the question about the motivation and the presentation of this paper : * * * We thank the reviewer for the suggestion about the presentation of the paper . We have edited the introduction to motivate our method more in the context of model compression . We also include exploring its application to the general NAS problem as our future work in the conclusion section . * * * Response to the question about the using the log marginal likelihood as the objective function : * * * We agree that the log marginal likelihood is the standard objective function in previous works on kernel learning . However , we do not use the log marginal likelihood for the following two reasons : ( 1 ) We empirically find that maximizing the log marginal likelihood yields worse results than maximizing the predictive GP posterior . Here are the results : CIFAR-100 Accuracy # Params Ratio Times f ( x ) VGG-19 Log Marginal 69.90 % \u00b10.69 % 1.50M\u00b10.68M 0.9254\u00b10.3382 16.14x\u00b19.22x 0.9422\u00b10.0071 Ours 71.41 % \u00b10.75 % 2.61M\u00b10.61M 0.8699\u00b10.0306 7.99x\u00b11.99x 0.9518\u00b10.0158 ResNet-18 Log Marginal 72.80 % \u00b11.11 % 1.72M\u00b10.18M 0.8467\u00b10.0160 6.57x\u00b10.67x 0.9033\u00b10.0094 Ours 73.83 % \u00b11.11 % 1.87M\u00b10.08M 0.8335\u00b10.0073 6.01x\u00b10.26x 0.9123\u00b10.0151 ResNet-34 Log Marginal 73.11 % \u00b10.57 % 3.34M\u00b10.48M 0.8435\u00b10.0224 6.47x\u00b10.89x 0.9059\u00b10.0134 Ours 73.68 % \u00b10.57 % 2.36M\u00b10.15M 0.8895\u00b10.0069 9.08x\u00b10.59x 0.9246\u00b10.0076 'Log Marginal ' refers to training the LSTM by maximizing the log marginal likelihood . 'Ours ' refers to maximizing p ( f|D ) . ( 2 ) Also , when using the log marginal likelihood , we observe the loss is numerically unstable due to the log determinant of the covariance matrix in the log likelihood . The training objective usually goes to infinity when the dimension of the covariance matrix is larger than 50 , even with smaller learning rates , which may harm the search performance . Therefore , we train the LSTM parameters by maximizing the predictive GP posterior . * * * Response to questions about the sampling procedure : * * * Here are the details about how we sample one compressed architecture . This sampling procedure is used in both the \u2018 Random Search \u2019 baseline and the optimization of the acquisition function in our method . ( 1 ) For layer removal , only layers whose input dimension and output dimension are the same are allowed to be removed . Each removable layer can be removed with probability p_1 . However , if the probability is fixed , the diversity of sampled architectures would be reduced . For example , if we fix p_1 to 0.5 , a compressed architecture with over 70 % layers removed can hardly be generated . To encourage the diversity of random samples , p_1 is first randomly drawn from the set P_1= { 0.3 , 0.4 , 0.5 , 0.6 , 0.7 } at the beginning of generating a new compressed architecture . ( 2 ) For layer shrinkage , we divide layers into groups and for layers in the same group , the number of channels are always shrunken with the same ratio . The layers are grouped according to their input and output dimension . This is to make sure the network is still valid after the layer shrinkage . The shrinkage ratio for each group is drawn from the uniform distribution U ( 0.0 , 1.0 ) . ( 3 ) For adding skip connections , only when the output dimension of one layer is the same as the input dimension of another layer , the two layers can be connected . When there are multiple incoming connections for one layer , the outputs of source layers are added up to form the input for that layer . For each pair of connectable layers , a connection can be added between them with probability p_3 . Similar to p_1 in layer removal , p_3 is not fixed but randomly drawn from the set P_3= { 0.003 , 0.005 , 0.01 , 0.03 , 0.05 } at the beginning of generating a compressed architecture . Values in P_3 are relatively small , because we found in experiments that adding too many skip connections empirically harm the performance of compressed architectures . Combining all these three kinds of randomly sampled operations , a compressed architecture is generated from the teacher architecture . We have tried to include more values in the set P_1 and P_3 but that does not yield any improvement in the performance ."}], "0": {"review_id": "S1xLN3C9YX-0", "review_text": "Review: This paper proposes a method for finding optimal architectures for deep neural networks based on a teacher network. The optimal network is found by removing or shrinking layers or adding skip connections. A Bayesian Optimization approach is used by employing a Gaussian Process to guide the search and the acquisition function expected improvement. A special kernel is used in the GP to model the space of network architectures. The method proposed is compared to a random search strategy and a method based on reinforcement learning. Quality: The quality of the paper is high in the sense that it is very well written and contains exhaustive experiments with respect to other related methods Clarity: The paper is well written in general with a few typos, e.g., \"The weights of the Bi-LSTM \u03b8, is learned during the search process. The weights \u03b8 determines\" Originality: The proposed method is not very original in the sense that it is a combination of several known techniques. May be the most original contribution is the proposal of a kernel for network architectures based on recurrent neural networks. Another original idea is the use of sampling to avoid the problem of doing kernel over-fitting. Something that can be questioned, however, in this regard is the fact that instead of averaging over kernels the GP prediction to account for uncertainty in the kernel parameters, the authors have suggested to optimize a different acquisition function per each kernel. This can be problematic since for each kernel over-fitting can indeed occur, although the experimental results suggest that this is not happening. Significance: Why N2N does not appear in all the CIRFAR-10 and CIFAR-100 experiments? This may question the significance of the results. It also seems that the authors have not repeated the experiments several times since there are no error bars in the results. This may also question the significance of the results. An average over several repetitions is needed to account for the randomness in for example the sampling of the network architectures to learn the kernels. Besides this, the authors may want to cite this paper Hern\u00e1ndez-Lobato, D., Hernandez-Lobato, J., Shah, A., & Adams, R. (2016, June). Predictive entropy search for multi-objective Bayesian optimization. In International Conference on Machine Learning (pp. 1492-1501). which does multi-objective Bayesian optimization of deep neural networks (the objectives are accuracy and prediction time). Pros: - Well written paper. - Simply idea. - Extensive experiments. Cons: - The proposed approach is a combination of well known methods. - The significance of the results is in question since the authors do not include error bars in the experiments.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the feedback and suggestions . We have addressed all the questions here : * * * Response to questions about the performance of N2N : * * * All the numbers of N2N are from their original paper but N2N did not test their method to compress ShuffleNet so we do not have the performance of N2N on ShuffleNet . N2N did not test their method under the setting VGG-19 on CIFAR-100 either . For ResNet-34 on CIFAR-100 , N2N only provides results of layer removal ( indicated by \u2018 N2N - removal \u2019 in Table 1 in our paper ) so for fair comparison , we compare \u2018 N2N - removal \u2019 with \u2018 Ours - removal \u2019 , which refers to only considering the layer removal operation in the search space . \u2018 Ours - removal \u2019 also significantly outperforms \u2018 N2N - removal \u2019 in terms of both the accuracy and the compression ratio . * * * Response to questions about experiment results : * * * We re-run the experiments for 3 times and update the results in the paper ( please check the PDF ) . In Table 1 , we show the mean and standard deviation of the results for \u2018 Ours \u2019 and \u2018 Random Search \u2019 . We observe that after multiple runs , the average performance of our method also outperforms all the baselines as before . * * * Response to questions about the related work : * * * We have updated the paper and added this paper in related work . Also in the conclusion section , we think it \u2019 s an interesting future direction to combine their method with our proposed embedding space to identify the Pareto set of the architectures that are both small and accurate . Thanks for suggesting the related work ! * * * Response to questions about the originality of our work : * * * We would like to emphasize that our key contribution is a novel method that incrementally learns an embedding space for the architecture domain , i.e. , a unified representation for the configuration of architectures . The learned embedding space can be used to compare architectures with complex skip connections and multiple branches and we can combine it with any Sequential Model-Based Optimization ( SMBO ) method ( we choose GP based BO algorithms in this work ) to search for desired architectures . Based the learned embedding space , we present a framework of searching for compressed network architectures with Bayesian optimization ( BO ) . The learned embedding provides a feature space over which the kernel function of BO is defined . Under this framework , we propose a set of architecture operators for generating architectures for search and a multiple kernel strategy to encourage the search algorithm to explore more diverse architectures . We demonstrate that our method can significantly outperform various baseline methods , such as random search and N2N ( Ashok et al.,2018 ) . The compressed architectures found by our method are also better than the state-of-the-art manually-designed compact architecture ShuffleNet ( Zhang et al. , 2018 ) . We also demonstrate that the learned embedding space can be transferred to new settings for architecture search , such as a larger teacher network or a teacher network in a different architecture family , without any training ."}, "1": {"review_id": "S1xLN3C9YX-1", "review_text": "================ Post-Rebuttal ================ I thank the authors for the larger amount of additional work they put into the rebuttal. Since the authors addressed my main concerns, i e. comparison to existing methods, clarifications of the proposed approach, adding references to related work, I will increase my score and suggest to accept the paper. The paper describes a new neural architecture search strategy based on Bayesian optimization to find a compressed version of a teacher network. The main contribution of the paper is to learn an embedding that maps from a discrete encoding of an architecture to a continuous latent vector such that standard Bayesian optimization can be applied. The new proposed method improves in terms of compressing the teacher network with just a small drop in accuracy upon an existing neural architecture search method based on reinforcement learning and random sampling. Overall, the paper presents an interesting idea to use Bayesian optimization on high dimensional discrete problems such as neural architecture search. I think a particular strength of this methods is that the embedding is fairly general and can be combined with various recent advances in Bayesian optimization, such as, for instance, multi-fidelity modelling. It also shows on some compression experiments superior performance to other state-of-the-art methods. However, in its current state I do not think that the paper is read for acceptance: - Since the problem is basically just a high dimensional, discrete optimization problem, the paper misses comparison to other existing Bayesian optimization methods such as TPE [1] / SMAC [2] that can also handle these kind of input spaces. Both of these methods have been applied to neural architecture search [3][4] before. Furthermore, since the method is highly related to NASBOT [5], it would be great to also see a comparison to it. - I assume that in order to learn a good embedding, similar architectures need to be mapped to latent vector that are close in euclidean space, such that the Gaussian process kernel can model any correlation[7]. How do you make sure that the LSTM learns a meaningful embedding space? It is also a bit unclear why the performance f is not used directly instead of p(f|D). Using f instead of p(f|D) would probably also make continual training of the LSTM easier, since function values do not change. - The experiment section misses some details: - Do the tables report mean performances or the performance of single runs? It would also be more convincing if the table contains error bars on the reported numbers. - How are the hyperparameters of the Gaussian process treated? - The related work section misses some references to Lu et al.[6] and Gomez-Bombarelli et al.[7] which are highly related. - What do you mean with the sentence \"works on BO for NAS can only tune feed-forward structures\" in the related work section? There is no reason why other Bayesian optimization should not be able to also optimize recurrent architectures (see for instance Snoek et al.[8]). - Section 3.3 is a bit confusing and to be honest I do not get the motivation for the usage of multiple kernels. Why do the first architectures biasing the LSTM? Since Bayesian optimization with expected improvement samples around the global optimum, should not later evaluated, well-performing architectures more present in the training dataset for the LSTM? [1] Algorithms for Hyper-Parameter Optimization J. Bergstra and R. Bardenet and Y. Bengio and B. Kegl Proceedings of the 25th International Conference on Advances in Neural Information Processing Systems (NIPS'11) [2] Sequential Model-Based Optimization for General Algorithm Configuration F. Hutter and H. Hoos and K. Leyton-Brown Proceedings of the Fifth International Conference on Learning and Intelligent Optimization (LION'11) [3] Towards Automatically-Tuned Neural Networks H. Mendoza and A. Klein and M. Feurer and J. Springenberg and F. Hutter ICML 2016 AutoML Workshop [4] Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures J. Bergstra and D. Yamins and D. Cox Proceedings of the 30th International Conference on Machine Learning (ICML'13) [5] Neural Architecture Search with Bayesian Optimisation and Optimal Transport K. Kandasamy and W. Neiswanger and J. Schneider and B. P{\\'{o}}czos and E. Xing abs/1802.07191 [6] Structured Variationally Auto-encoded Optimization X. Lu and J. Gonzalez and Z. Dai and N. Lawrence Proceedings of the 35th International Conference on Machine Learning [7] Automatic chemical design using a data-driven continuous representation of molecules R. G\u00f3mez-Bombarelli and J. Wei and D. Duvenaud and J. Hern\u00e1ndez-Lobato and B. S\u00e1nchez-Lengeling and D. Sheberla and J. Aguilera-Iparraguirre and T. Hirzel. and R. Adams and A. Aspuru-Guzik American Chemical Society Central Science [8] Scalable {B}ayesian Optimization Using Deep Neural Networks J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams Proceedings of the 32nd International Conference on Machine Learning (ICML'15)", "rating": "7: Good paper, accept", "reply_text": "Here is our response to other questions : * * * Response to questions about experimental details : * * * We re-run the experiments for 3 times and update the results in the paper ( please check the PDF ) . In Table 1 , we show the mean and standard deviation of the results for \u2018 Ours \u2019 and \u2018 Random Search \u2019 . We observe that after multiple runs , the average performance of our method also outperforms all the baselines as before . The mean of the Gaussian process prior is set to zero . The Gaussian noise variance is set to 0.05 . The kernel width parameter $ \\sigma $ ( defined in Eq 4 ) in the RBF kernel is set as $ \\sigma^2=0.01 $ . * * * Response to questions about related work : * * * Thanks for suggesting the related work . We have updated the paper and added [ 6 ] and [ 7 ] in the related work section . For your convenience , here is the text about [ 6 ] and [ 7 ] in the paper : \u201c Our work can also be viewed as carrying out optimization in the latent space of a high dimensional and structured space , which shares a similar idea with previous literature [ 6 ] [ 7 ] . For example , [ 6 ] presents a new variational auto-encoder to map kernel combinations produced by a context-free grammar into a continuous and low-dimensional latent space. \u201d * * * Response to \u201c What do you mean with the sentence `` works on BO for NAS can only tune feed-forward structures '' in the related work section ? \u201d : * * * We are sorry for the confusion of the term \u2018 feed-forward structures \u2019 in this sentence . We have corrected the sentence to \u201c However , most existing works on BO for NAS only show results on tuning network architectures where the connections between network layers are fixed , i.e. , most of them do not optimize how the layers are connected to each other. \u201d For example , [ 8 ] tunes the hidden size , the embedding size and other architectural parameters in the language model but it does NOT change how the layers in the model are connected to each other . Our results ( Table 5 in Appendix 6.3 ) show that optimizing how the layers are connected ( in this work , by adding skip connections ) is crucial to the performance of the compressed network architecture . The fundamental reason why previous works on BO for NAS do not optimize how the layers are connected is that there lacked a principled way to quantify the similarity between two architectures with complex skip connections , which is addressed by our proposed learnable embedding space . They can benefit our proposed method to be extended to optimize how the layers are connected . * * * Response to questions about the motivation of using multiple kernels : * * * Sorry for the confusion in Sec 3.3 . We have edited Sec 3.3 to make the motivation more clear . The main motivation of training multiple kernels is to encourage the search algorithm to explore more diverse architectures . We only evaluate 160 architectures during the whole search process so it is possible the learned kernel is overfitted to the training samples and bias the following sampled architectures for evaluation . To encourage the search algorithm to explore more diverse architectures , we propose the usage of multiple kernels , motivated by the bagging algorithm , which is usually employed to avoid overfitting . Regarding the statement about the first architecture biasing the LSTM , this statement is invalid in the current context and we have removed it from the paper . This was a conjecture at the early development stage of this work and we mistakenly put it here ."}, "2": {"review_id": "S1xLN3C9YX-2", "review_text": "In this work, the authors propose a new strategy to compress a teacher neural network. Briefly, the authors propose using Bayesian optimization (BO) where the accuracy of the networks is modelled using a Gaussian Process function with a squared exponential kernel on continuous neural network (NN) embeddings. Such embeddings are the output of a bidirectional LSTM taking as input the \u201craw\u201d (discrete) NN representations (when regarded as a covariance function of the \u201craw\u201d (discrete) NN representations, the kernel is a deep kernel). The authors apply this framework for model compression. In this application, the search space is the space of networks obtained by sampling reducing operations on a teacher network. In applications to CIFAR-10 and CIFAR-100 the authors show that the accuracies of the compressed network obtained through their method exceeds accuracies obtained through other methods for compression, manually compressed networks and random sampling. I have the following concerns/questions: 1) The authors motivate their work in the introduction by discussing the importance of learning a good embedding space over network architectures to \u201cgenerate a priority ordering of architectures for evaluation\u201d. Within the proposed BO framework, this would require the optimization of the expected improvement in a high-dimensional and discrete space (the space of NN architectures), which \u201cis non-trivial\u201d. In this work, the authors do not try to solve this general problem, but specialize their work to model compression, which has a much lower dimensional search space (space of networks obtained by sampling reducing operations on a teacher network). For this reason, I believe the presentation and motivation of this work is not presented clearly. Specifically, while I agree that the methods and results in this paper can be relevant to the problem of getting NN embeddings for a larger search space, this should be discussed in the conclusion/discussion as future direction, rather than as motivating example. Generally, I think the method should be described in the context of model compression rather than as a general method for neural architecture search (NAS) method (in my understanding, its use for NAS would be unfeasible). 2) I have been wondering why the authors optimize the kernel parameters by maximizing the predictive GP posterior rather than maximizing the GP log marginal likelihood as in standard GP regression? 3) The sampling procedure should be explained in greater detail. How many reducing operations are sampled? This would be important to fully understand the random search method the authors consider for comparison in their experiments. I expect that the results from that method will strongly depend on the sampling procedure and different choices should probably be explored for a fair comparison. Do the authors have any comment on this? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the useful feedback ! Here is our response : * * * Response to the question about the motivation and the presentation of this paper : * * * We thank the reviewer for the suggestion about the presentation of the paper . We have edited the introduction to motivate our method more in the context of model compression . We also include exploring its application to the general NAS problem as our future work in the conclusion section . * * * Response to the question about the using the log marginal likelihood as the objective function : * * * We agree that the log marginal likelihood is the standard objective function in previous works on kernel learning . However , we do not use the log marginal likelihood for the following two reasons : ( 1 ) We empirically find that maximizing the log marginal likelihood yields worse results than maximizing the predictive GP posterior . Here are the results : CIFAR-100 Accuracy # Params Ratio Times f ( x ) VGG-19 Log Marginal 69.90 % \u00b10.69 % 1.50M\u00b10.68M 0.9254\u00b10.3382 16.14x\u00b19.22x 0.9422\u00b10.0071 Ours 71.41 % \u00b10.75 % 2.61M\u00b10.61M 0.8699\u00b10.0306 7.99x\u00b11.99x 0.9518\u00b10.0158 ResNet-18 Log Marginal 72.80 % \u00b11.11 % 1.72M\u00b10.18M 0.8467\u00b10.0160 6.57x\u00b10.67x 0.9033\u00b10.0094 Ours 73.83 % \u00b11.11 % 1.87M\u00b10.08M 0.8335\u00b10.0073 6.01x\u00b10.26x 0.9123\u00b10.0151 ResNet-34 Log Marginal 73.11 % \u00b10.57 % 3.34M\u00b10.48M 0.8435\u00b10.0224 6.47x\u00b10.89x 0.9059\u00b10.0134 Ours 73.68 % \u00b10.57 % 2.36M\u00b10.15M 0.8895\u00b10.0069 9.08x\u00b10.59x 0.9246\u00b10.0076 'Log Marginal ' refers to training the LSTM by maximizing the log marginal likelihood . 'Ours ' refers to maximizing p ( f|D ) . ( 2 ) Also , when using the log marginal likelihood , we observe the loss is numerically unstable due to the log determinant of the covariance matrix in the log likelihood . The training objective usually goes to infinity when the dimension of the covariance matrix is larger than 50 , even with smaller learning rates , which may harm the search performance . Therefore , we train the LSTM parameters by maximizing the predictive GP posterior . * * * Response to questions about the sampling procedure : * * * Here are the details about how we sample one compressed architecture . This sampling procedure is used in both the \u2018 Random Search \u2019 baseline and the optimization of the acquisition function in our method . ( 1 ) For layer removal , only layers whose input dimension and output dimension are the same are allowed to be removed . Each removable layer can be removed with probability p_1 . However , if the probability is fixed , the diversity of sampled architectures would be reduced . For example , if we fix p_1 to 0.5 , a compressed architecture with over 70 % layers removed can hardly be generated . To encourage the diversity of random samples , p_1 is first randomly drawn from the set P_1= { 0.3 , 0.4 , 0.5 , 0.6 , 0.7 } at the beginning of generating a new compressed architecture . ( 2 ) For layer shrinkage , we divide layers into groups and for layers in the same group , the number of channels are always shrunken with the same ratio . The layers are grouped according to their input and output dimension . This is to make sure the network is still valid after the layer shrinkage . The shrinkage ratio for each group is drawn from the uniform distribution U ( 0.0 , 1.0 ) . ( 3 ) For adding skip connections , only when the output dimension of one layer is the same as the input dimension of another layer , the two layers can be connected . When there are multiple incoming connections for one layer , the outputs of source layers are added up to form the input for that layer . For each pair of connectable layers , a connection can be added between them with probability p_3 . Similar to p_1 in layer removal , p_3 is not fixed but randomly drawn from the set P_3= { 0.003 , 0.005 , 0.01 , 0.03 , 0.05 } at the beginning of generating a compressed architecture . Values in P_3 are relatively small , because we found in experiments that adding too many skip connections empirically harm the performance of compressed architectures . Combining all these three kinds of randomly sampled operations , a compressed architecture is generated from the teacher architecture . We have tried to include more values in the set P_1 and P_3 but that does not yield any improvement in the performance ."}}