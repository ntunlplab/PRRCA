{"year": "2017", "forum": "B1YfAfcgl", "title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "decision": "Accept (Poster)", "meta_review": "This paper presents both an analysis of neural net optimization landscapes, and an optimization algorithm that encourages movement in directions of high entropy. The motivation is based on intuitions from physics.\n \n Pros\n  - the main idea is well-motivated, from a non-standard perspective.\n  - There are lots of side-experiments supporting the claims for the motivation.\n Cons\n  - The propose method is very complicated, and it sounds like good performance depended on adding and annealing yet another hyperparameter, referred to as 'scoping'.\n  - The motivating intuition has been around for a long time in different forms. In particular, the proposed method is very closely related to stochastic variational inference, or MCMC methods. Appendix C makes it clear that the two methods aren't identical, but I wish the authors had simply run SVI with their proposed modification, instead of appearing to re-invent the idea of maximizing local volume from scratch. The intuition that good generalization comes from regions of high volume is also exactly what Bayes rule says.\n \n In summary, while there is improvement for the paper, the idea is well-motivated and the experimental results are sound.", "reviews": [{"review_id": "B1YfAfcgl-0", "review_text": "The paper introduces a new regularization term which encourages the optimizer to search for a flat local minimum of reasonably low loss instead of seeking a sharp region of a low loss. This is motivated by some empirical observations that local minima of good generalization performance tend to have flat shape. To achieve this, a regularization term based on the free local energy is proposed and the gradient of this term, which do not have tractable closed-form solution, is obtained by performing Monte Carlo estimation using SGLD sampler. In the experiments, the authors show some evidence of the flatness of good local minima, and also the performance of the proposed method in comparison to the Adam optimizer. The paper is well and clearly written. I enjoyed reading the paper. The connection to the concept of free energy in optimization framework seems interesting. The motivation of pursuing flatness is also well analyzed with a few experiments. I'm wondering if the first term in eqn. (8) is correct. I guess it should be f(x') not f(x)? Also, I'm wondering why the authors did not add the experiment results on RNN in the evaluation of the performance because char-lstm for text generation was already used for the flatness experiments. I think adding more experiments on various models and applications of deep architectures (e.g., RNN, seq2seq, etc.) will make the author's claim more persuasive. I also found the mixed usage of the terminology, e.g., free energy and free entropy, a bit confusing. ", "rating": "7: Good paper, accept", "reply_text": "> > Eqn.8 should have f ( x ' ) Thanks , it is fixed now . > > experiments on RNNs Thanks for suggesting this . We have included results for word-level and character-level text prediction on PTB and char-LSTM with War and Peace . We not only obtain a better test perplexity than a competitive baseline with SGD , but also train in about half as much wall-clock time . Please see the `` updates to the paper '' comment above for more details ."}, {"review_id": "B1YfAfcgl-1", "review_text": "Overview: This paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error. This comes at a computational cost of estimating the gradient of the biasing term for each iteration through stochastic gradient Langevin dynamics, approximating an MCMC sample of the log partition function of a modified Gibbs distribution. The cost is equivalent to adding an inner for-loop to the standard SGD algorithm for each minibatch. Pros: - Reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks - Generally very well written and well presented results, with interesting discussion of eigenvalues of Hessian as a way to characterize \u201cflat\u201d minima - Promising mathematical arguments suggest that E-SGD has generalization error bounded below by SGD, motivating further research in the area Cons / points suggested for a rebuttal: (1) One claim of the paper given in the abstract is \u201dexperiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training.\u201c This does not appear to be supported by the current set of experiments. As the authors comment in the discussion section, \u201cIn our experiments, Entropy-SGD results in a comparable generalization error as SGD, but always has a lower cross-entropy loss.\u201d It's not clear to me how to reconcile those two claims. (2) Similarly, the claim of accelerated training is not convincingly supported in the present version of the paper. Vanilla SGD requires a single forward pass through all M minibatches during one epoch for a parameter update, but the new method, E-SGD requires, L*M forward passes during one epoch where L is the number of Langevin updates, which require a minibatch sample each. This could in fact mean that E-SGD has worse computational complexity to reach the same point. In a remark on p.9, the authors note that a single epoch is defined to be \u201cthe number of parameter updates required to run through the dataset once.\u201d It\u2019s not clear to me how this answers the objection to a factor of L additional computations required for the inner-loop SGLD iterations. SGLD appears to introduces a potentially costly tradeoff that must be carefully managed by a user of E-SGD. (3) As the previous two points suggest, the paper could use some attention to the magnitude of the claims. For example, the introduction reads \u201cActively biasing towards wide valleys aids generalization, in fact, we can optimize solely the free energy term to obtain similar generalization error as SGD on the original loss function.\u201c According the the values reported on pp.9-10, only on MNIST is the generalization error, using only the free energy term (the log partition function of the modified Gibbs distribution), equivalent to using only the SGD loss function. This corresponds to setting rho to 0 in equation (6). On CIFAR-10, rho = 0.01 is used. (4) Another contribution of this paper, the characterization of the optimization landscape in terms of the eigenvalues of the Hessian and low generalization error being associated with flat local extrema, is helpful and interesting. I found the plots clear and useful. As another reviewer has already pointed out, there are high-level similarities to \u201cFlat Minima\u201d by Hochreiter and Schmidhuber (1997). The authors have responded already by adding a paragraph that helpfully explores some differences with H&S 1997. However, the similarities should also be carefully identified and mentioned. H&S 1997 includes detailed theoretical analysis that could be helpful for future work in this area, and has independently discovered a similar approach to training generalizable networks. (5) It's not clear how the assumption about the eigenvalues that were made in section 4.4 / Appendix B affect the application of this result to real-world problems. What magnitude of c>0 needs to be chosen? Does this correspond to a measurable characteristic of the dataset? It's a little mysterious in the current version of the paper. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Please see the `` updates to the paper '' comment above for the response to the questions regarding ( i ) the eigenvalue assumption and , ( ii ) thorough experimental results . > > how to reconcile `` Entropy-SGD obtains better generalization but results in lower cross-entropy loss '' The confusion is probably caused by our omission , it should read `` ... lower cross-entropy loss on the training set '' ( fixed now ) . This experimental observation suggests that there exist wide valleys that are deeper in the energy landscape that also generalize well ; Entropy-SGD manages to find them while SGD gets stuck at a higher loss . > > \\rho = 0.01 on CIFAR-10 but \\rho = 0 on MNIST We have modified the algorithm to only train with the local entropy objective , i.e. , \\rho = 0 always ( cf.Eqn.6 ) .Using scoping we can obtain better results on all our networks both in terms of generalization error and wall-clock time . > > discuss similarities to Hochreiter and Schmidhuber '97 [ HS97 ] We have expanded our discussion of [ HS97 ] in Sec.2 to include this discussion : While the motivations are exactly the same , similarities with their exact formulation are only conceptual in nature , e.g. , in a flat minimum , the local entropy is a direct measure of the width of the valley which is similar to their usage of Hessian . The Gibbs variant to averaging in weight space in their analysis ( Eqn.33 , pg.22 of [ HS97 ] ) is similar to the averaging in Eqn . 7 of our paper . We agree with the reviewer that the elaborate analysis of generalization of [ HS97 ] using the Gibbs formalism is a promising direction . In our case , we benefit from similar elaborate and technical results introduced for uniform stability [ BE02 ] . > > experiments are on a toy example Please note that our results on CIFAR-10 are without any data augmentation , the best result for this is 6.55 % error using ELU units by [ CUH15 ] . To our knowledge , our baseline on CIFAR-10 ( 7.71 \\pm 0.19 % error with SGD ) , is the best reported result for the popular All-CNN-C network [ S14 ] , which is a medium-sized model with about 1.6 million weights . The largest model we have experimented with is an LSTM on the PTB dataset with 66 million weights and we achieve better test perplexity than the original authors [ ZSV14 ] in half as much wall-clock time . [ HS97 ] Hochreiter , S. and Schmidhuber , J . ( 1997 ) , Flat Minima . [ BE02 ] Olivier Bousquet and Andre Elisseeff . Stability and generalization . JMLR , 2002 . [ CUH15 ] Clevert , Djork-Arn\u00e9 , Thomas Unterthiner , and Sepp Hochreiter . `` Fast and accurate deep network learning by exponential linear units ( elus ) . '' arXiv:1511.07289 ( 2015 ) . [ S14 ] Springenberg , Jost Tobias , et al . `` Striving for simplicity : The all convolutional net . '' arXiv:1412.6806 ( 2014 ) . [ ZSV14 ] Zaremba , Wojciech , Ilya Sutskever , and Oriol Vinyals . `` Recurrent neural network regularization . '' arXiv:1409.2329 ( 2014 ) ."}, {"review_id": "B1YfAfcgl-2", "review_text": "This paper presents a principled approach to finding flat minima. The motivation to seek such minima is due to their better generalization ability. The idea is to add to the original loss function a new term that exploits both width and depth of the objective function. In fact, the regularization term can be interpreted as Gaussian convolution of the exponentiated loss. Therefore, the introduced regularization term is essentially Gaussian smoothed version of the exponentiated loss. The smoothing obviously tends to suppress sharp minima. Overall, developing such regularization term based on thermodynamics concepts is very interesting. I have a couple of concerns that the authors may want to clarify in the rebuttal. 1. When reporting the generalization performance, the experiments report the number of epochs; showing the proposed algorithm reaches better generalization in fewer epochs than plain SGD. Is this the number of epochs it takes by line 7 of your algorithm, or it is the total number of epochs (line 3 and 7 all combined)? If the former, it is not a fair comparison. If you multiply the number of epochs of SGD (line 7) by the number iterations it takes to approximate Langevin dynamics, it seems you obtain little gain against plain SGD. 2. The proposed algorithm approximates the smoothed \"exponentiated\" loss (by smoothing I refer to convolution with the Gaussian). I am wondering how it compares against simpler idea of smoothing the original loss (dropping exponentiation)? Is the difference only in the motivation (e.g. thermodynamics interpretation) or it is deeper, e.g. the proposed scheme lends itself to more accurate approximation and/or achieves better generalization bound (in terms of the attained smoothness)? Smoothing the cost function without exponentiation allows simpler approximation (Monte Carlo integration instead of MCMC), e.g. see section 5.3 of https://arxiv.org/pdf/1601.04114 3. Section 4.4. Thank you for revising the statements related to the eigenvalues of the Hessian. However, even in the revised version, there seems to be some discrepancy. You \"assume no eigenvalue of the Hessian lies in the set [\u22122\u03b3 \u2212c, c] for some small c > 0\". This essentially says the eigenvalues are far from zero. Such assumption seems to be in the opposite direction of the reality: the plots of eigenvalues (Figure 1) show most eigenvalues are indeed close to zero. 4. Theorem 3 from Hardt 2015: The way you quote it differs from the original paper. Are you referring to the Theorem 3.12 of Hardt's paper? If so, why the difference, including elimination of dependency on constant c in the exponent of T? ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "> > Thm.3 does not have the constant c in Hardt et . al . '15 We set the constant c of Hardt et . al. , to 1 in our statement to avoid confusion with our constant , with is also called c ( our statement has the learning rate \\eta_t \\leq 1/t instead of their original \\eta_t \\leq c/t ) . This does not change the result qualitatively . Please see the `` updates to the paper '' comment above for the response to the questions regarding ( i ) the eigenvalue assumption , ( ii ) thorough experimental results and , ( iii ) smoothing the original loss vs. local entropy ."}], "0": {"review_id": "B1YfAfcgl-0", "review_text": "The paper introduces a new regularization term which encourages the optimizer to search for a flat local minimum of reasonably low loss instead of seeking a sharp region of a low loss. This is motivated by some empirical observations that local minima of good generalization performance tend to have flat shape. To achieve this, a regularization term based on the free local energy is proposed and the gradient of this term, which do not have tractable closed-form solution, is obtained by performing Monte Carlo estimation using SGLD sampler. In the experiments, the authors show some evidence of the flatness of good local minima, and also the performance of the proposed method in comparison to the Adam optimizer. The paper is well and clearly written. I enjoyed reading the paper. The connection to the concept of free energy in optimization framework seems interesting. The motivation of pursuing flatness is also well analyzed with a few experiments. I'm wondering if the first term in eqn. (8) is correct. I guess it should be f(x') not f(x)? Also, I'm wondering why the authors did not add the experiment results on RNN in the evaluation of the performance because char-lstm for text generation was already used for the flatness experiments. I think adding more experiments on various models and applications of deep architectures (e.g., RNN, seq2seq, etc.) will make the author's claim more persuasive. I also found the mixed usage of the terminology, e.g., free energy and free entropy, a bit confusing. ", "rating": "7: Good paper, accept", "reply_text": "> > Eqn.8 should have f ( x ' ) Thanks , it is fixed now . > > experiments on RNNs Thanks for suggesting this . We have included results for word-level and character-level text prediction on PTB and char-LSTM with War and Peace . We not only obtain a better test perplexity than a competitive baseline with SGD , but also train in about half as much wall-clock time . Please see the `` updates to the paper '' comment above for more details ."}, "1": {"review_id": "B1YfAfcgl-1", "review_text": "Overview: This paper introduces a biasing term for SGD that, in theoretical results and a toy example, yields solutions with an approximately equal or lower generalization error. This comes at a computational cost of estimating the gradient of the biasing term for each iteration through stochastic gradient Langevin dynamics, approximating an MCMC sample of the log partition function of a modified Gibbs distribution. The cost is equivalent to adding an inner for-loop to the standard SGD algorithm for each minibatch. Pros: - Reviews and distills many results and theorems from past 2 decades that suggest a promising way forward for increasing the generalizability of deep neural networks - Generally very well written and well presented results, with interesting discussion of eigenvalues of Hessian as a way to characterize \u201cflat\u201d minima - Promising mathematical arguments suggest that E-SGD has generalization error bounded below by SGD, motivating further research in the area Cons / points suggested for a rebuttal: (1) One claim of the paper given in the abstract is \u201dexperiments on competitive baselines demonstrate that Entropy-SGD leads to improved generalization and has the potential to accelerate training.\u201c This does not appear to be supported by the current set of experiments. As the authors comment in the discussion section, \u201cIn our experiments, Entropy-SGD results in a comparable generalization error as SGD, but always has a lower cross-entropy loss.\u201d It's not clear to me how to reconcile those two claims. (2) Similarly, the claim of accelerated training is not convincingly supported in the present version of the paper. Vanilla SGD requires a single forward pass through all M minibatches during one epoch for a parameter update, but the new method, E-SGD requires, L*M forward passes during one epoch where L is the number of Langevin updates, which require a minibatch sample each. This could in fact mean that E-SGD has worse computational complexity to reach the same point. In a remark on p.9, the authors note that a single epoch is defined to be \u201cthe number of parameter updates required to run through the dataset once.\u201d It\u2019s not clear to me how this answers the objection to a factor of L additional computations required for the inner-loop SGLD iterations. SGLD appears to introduces a potentially costly tradeoff that must be carefully managed by a user of E-SGD. (3) As the previous two points suggest, the paper could use some attention to the magnitude of the claims. For example, the introduction reads \u201cActively biasing towards wide valleys aids generalization, in fact, we can optimize solely the free energy term to obtain similar generalization error as SGD on the original loss function.\u201c According the the values reported on pp.9-10, only on MNIST is the generalization error, using only the free energy term (the log partition function of the modified Gibbs distribution), equivalent to using only the SGD loss function. This corresponds to setting rho to 0 in equation (6). On CIFAR-10, rho = 0.01 is used. (4) Another contribution of this paper, the characterization of the optimization landscape in terms of the eigenvalues of the Hessian and low generalization error being associated with flat local extrema, is helpful and interesting. I found the plots clear and useful. As another reviewer has already pointed out, there are high-level similarities to \u201cFlat Minima\u201d by Hochreiter and Schmidhuber (1997). The authors have responded already by adding a paragraph that helpfully explores some differences with H&S 1997. However, the similarities should also be carefully identified and mentioned. H&S 1997 includes detailed theoretical analysis that could be helpful for future work in this area, and has independently discovered a similar approach to training generalizable networks. (5) It's not clear how the assumption about the eigenvalues that were made in section 4.4 / Appendix B affect the application of this result to real-world problems. What magnitude of c>0 needs to be chosen? Does this correspond to a measurable characteristic of the dataset? It's a little mysterious in the current version of the paper. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Please see the `` updates to the paper '' comment above for the response to the questions regarding ( i ) the eigenvalue assumption and , ( ii ) thorough experimental results . > > how to reconcile `` Entropy-SGD obtains better generalization but results in lower cross-entropy loss '' The confusion is probably caused by our omission , it should read `` ... lower cross-entropy loss on the training set '' ( fixed now ) . This experimental observation suggests that there exist wide valleys that are deeper in the energy landscape that also generalize well ; Entropy-SGD manages to find them while SGD gets stuck at a higher loss . > > \\rho = 0.01 on CIFAR-10 but \\rho = 0 on MNIST We have modified the algorithm to only train with the local entropy objective , i.e. , \\rho = 0 always ( cf.Eqn.6 ) .Using scoping we can obtain better results on all our networks both in terms of generalization error and wall-clock time . > > discuss similarities to Hochreiter and Schmidhuber '97 [ HS97 ] We have expanded our discussion of [ HS97 ] in Sec.2 to include this discussion : While the motivations are exactly the same , similarities with their exact formulation are only conceptual in nature , e.g. , in a flat minimum , the local entropy is a direct measure of the width of the valley which is similar to their usage of Hessian . The Gibbs variant to averaging in weight space in their analysis ( Eqn.33 , pg.22 of [ HS97 ] ) is similar to the averaging in Eqn . 7 of our paper . We agree with the reviewer that the elaborate analysis of generalization of [ HS97 ] using the Gibbs formalism is a promising direction . In our case , we benefit from similar elaborate and technical results introduced for uniform stability [ BE02 ] . > > experiments are on a toy example Please note that our results on CIFAR-10 are without any data augmentation , the best result for this is 6.55 % error using ELU units by [ CUH15 ] . To our knowledge , our baseline on CIFAR-10 ( 7.71 \\pm 0.19 % error with SGD ) , is the best reported result for the popular All-CNN-C network [ S14 ] , which is a medium-sized model with about 1.6 million weights . The largest model we have experimented with is an LSTM on the PTB dataset with 66 million weights and we achieve better test perplexity than the original authors [ ZSV14 ] in half as much wall-clock time . [ HS97 ] Hochreiter , S. and Schmidhuber , J . ( 1997 ) , Flat Minima . [ BE02 ] Olivier Bousquet and Andre Elisseeff . Stability and generalization . JMLR , 2002 . [ CUH15 ] Clevert , Djork-Arn\u00e9 , Thomas Unterthiner , and Sepp Hochreiter . `` Fast and accurate deep network learning by exponential linear units ( elus ) . '' arXiv:1511.07289 ( 2015 ) . [ S14 ] Springenberg , Jost Tobias , et al . `` Striving for simplicity : The all convolutional net . '' arXiv:1412.6806 ( 2014 ) . [ ZSV14 ] Zaremba , Wojciech , Ilya Sutskever , and Oriol Vinyals . `` Recurrent neural network regularization . '' arXiv:1409.2329 ( 2014 ) ."}, "2": {"review_id": "B1YfAfcgl-2", "review_text": "This paper presents a principled approach to finding flat minima. The motivation to seek such minima is due to their better generalization ability. The idea is to add to the original loss function a new term that exploits both width and depth of the objective function. In fact, the regularization term can be interpreted as Gaussian convolution of the exponentiated loss. Therefore, the introduced regularization term is essentially Gaussian smoothed version of the exponentiated loss. The smoothing obviously tends to suppress sharp minima. Overall, developing such regularization term based on thermodynamics concepts is very interesting. I have a couple of concerns that the authors may want to clarify in the rebuttal. 1. When reporting the generalization performance, the experiments report the number of epochs; showing the proposed algorithm reaches better generalization in fewer epochs than plain SGD. Is this the number of epochs it takes by line 7 of your algorithm, or it is the total number of epochs (line 3 and 7 all combined)? If the former, it is not a fair comparison. If you multiply the number of epochs of SGD (line 7) by the number iterations it takes to approximate Langevin dynamics, it seems you obtain little gain against plain SGD. 2. The proposed algorithm approximates the smoothed \"exponentiated\" loss (by smoothing I refer to convolution with the Gaussian). I am wondering how it compares against simpler idea of smoothing the original loss (dropping exponentiation)? Is the difference only in the motivation (e.g. thermodynamics interpretation) or it is deeper, e.g. the proposed scheme lends itself to more accurate approximation and/or achieves better generalization bound (in terms of the attained smoothness)? Smoothing the cost function without exponentiation allows simpler approximation (Monte Carlo integration instead of MCMC), e.g. see section 5.3 of https://arxiv.org/pdf/1601.04114 3. Section 4.4. Thank you for revising the statements related to the eigenvalues of the Hessian. However, even in the revised version, there seems to be some discrepancy. You \"assume no eigenvalue of the Hessian lies in the set [\u22122\u03b3 \u2212c, c] for some small c > 0\". This essentially says the eigenvalues are far from zero. Such assumption seems to be in the opposite direction of the reality: the plots of eigenvalues (Figure 1) show most eigenvalues are indeed close to zero. 4. Theorem 3 from Hardt 2015: The way you quote it differs from the original paper. Are you referring to the Theorem 3.12 of Hardt's paper? If so, why the difference, including elimination of dependency on constant c in the exponent of T? ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "> > Thm.3 does not have the constant c in Hardt et . al . '15 We set the constant c of Hardt et . al. , to 1 in our statement to avoid confusion with our constant , with is also called c ( our statement has the learning rate \\eta_t \\leq 1/t instead of their original \\eta_t \\leq c/t ) . This does not change the result qualitatively . Please see the `` updates to the paper '' comment above for the response to the questions regarding ( i ) the eigenvalue assumption , ( ii ) thorough experimental results and , ( iii ) smoothing the original loss vs. local entropy ."}}