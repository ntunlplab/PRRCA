{"year": "2021", "forum": "9l9WD4ahJgs", "title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "decision": "Reject", "meta_review": "As of now, automatic data augmentation methods have mostly been proposed for supervised learning tasks, especially classification. This paper introduces automatic data augmentation to deep (image-based) reinforcement learning agents, aiming to make the agents generalize better to new environments. A new algorithm called data-regularized actor-critic (DrAC) is proposed, with three variants that correspond to different methods for automatically finding a useful augmentation: UCB-DrAC, RL2-DrAC, and Meta-DrAC. Promising results are reported on OpenAI\u2019s Procgen generalization benchmark which consists of 16 procedurally generated environments (games) with visual observations. Further experiments have been added in the revised version.\n\n**Strengths:**\n  * This work is among the first attempts that propose an automatic data augmentation scheme for reinforcement learning.\n  * The paper articulates well the problem of data augmentation for reinforcement learning.\n  * The experiment results are generally promising.\n\n**Weaknesses:**\n  * Although the experiment results reported seem promising, there are missing pieces in order to help the readers gain a deeper understanding to justify more thoroughly why the proposed regularization-based scheme works.\n  * Theoretical justification is lacking.\n\nThis is a borderline paper. While it presents some interesting ideas supported empirically by experiment results, the paper in its current form is premature for acceptance since a more thorough, scientific treatment is lacking before drawing conclusions. Moreover, considering that there are many competitive submissions to ICLR, I do not recommend that the paper be accepted. Nevertheless, the authors are encouraged to address the concerns raised to fill the gaps when revising their paper for resubmission in the future.\n", "reviews": [{"review_id": "9l9WD4ahJgs-0", "review_text": "Summary after Discussion Period : -- After reading the other reviewer 's comments and corresponding with the authors , I have become convinced that the author 's proposed regularization method is novel and effective , and would recommend this avenue of research be further explored . Yet it has also become clear to me that the author 's claims on why their method works are not yet supported by evidence . Further , I do n't believe the author 's proposed further ablation studies would fix the theory , since such experiments do n't address whether their method works by fixing problems with Laskin \u2019 s work ( as the author 's claim ) or because it provides a more direct way of enforcing invariance to transformation ( as I claim ) . So we 're left with a difficult situation , the method and the experiments are good while the theory is lacking . In such a situation both acceptance or rejection seem reasonable . Yet , as per ICLR reviewer guidelines , one should answer three questions for oneself : - What is the specific question and/or problem tackled by the paper ? - is the approach well motivated , including being well-placed in the literature ? - Does the paper support the claims ? This includes determining if results , whether theoretical or empirical , are correct and if they are scientifically rigorous . Since the theory is lacking and the approach is not well motivated , and since the theoretical claims have n't been rigorously supported , I feel as per ICLR guidelines the paper is not yet ready for acceptance . Initial Review : - Summary In this paper , the authors introduce three approaches for automatically finding an augmentations for any RL task , and a novel regularization scheme to make such augmentations work effectively . Positive aspects : -- The paper \u2019 s language is clear and the authors provide a good overview of the problem of data augmentation for reinforcement learning . Furthermore , they nicely explain why data augmentation for RL isn \u2019 t as straightforward as augmenting data for supervised learning learning . I believe that data augmentation could be a nice tool in the Reinforcement Learner \u2019 s toolbox , and I \u2019 m glad to see a paper advancing the idea . Major Concerns : -- This paper has not provided sufficient evidence that the author \u2019 s proposed way of doing data augmentation is effective . In this paper , there are two main novel methods for doing data augmentation / insights in RL . I will discuss my concerns with both methods separately . Policy and Value function Regularization . -- The authors criticize the naive application of transformers in the PPO \u2019 s buffer , as done in Laskin et al . ( 2020 ) , saying that this changes the PPO objective . While I agree that such a naive transformation as in Eq.2 is problematic , I fail to understand why application of transformation to states in the buffer would result in the Eq.2 , as the transformation would happen to the states being fed into both $ \\pi_\\theta $ and $ \\pi_ { \\theta_\\text { old } } $ , resulting in an equation different from Eq.2.One just needs to save the old policies ( and old Advantage function ) , so that $ \\pi_ { \\theta_\\text { old } } $ can be applied to transformed states , and not just use the actions from the buffer . This would seem like a straightforward fix . Yet the authors have proposed a different regularization fix which judging the experiments does seem to work , as shown in Figure 2 . I suspect it works for another reason : since the regularization forces $ V ( s ) = V ( f ( s ) ) $ and $ \\pi ( \\cdot | s ) = \\pi ( \\cdot | f ( s ) ) $ I wonder if this isn \u2019 t a method to allow prior knowledge to flow into the policy and value estimation . If the transformation ( s ) $ f_i $ have been chosen such that one can be reasonably sure that true value and policy functions should be invariant to said transformations , then by enforcing $ V ( s ) = V ( f ( s ) ) $ and $ \\pi ( \\cdot | s ) = \\pi ( \\cdot | f ( s ) ) $ one is constraining V and \\pi to fit the prior knowledge contained in $ f_i $ . So now we \u2019 re comparing apples and oranges , since your method ( DrAC ) gets to incorporate this prior knowledge , while PPO and RAD don \u2019 t , and DrAC \u2019 s good performance isn \u2019 t surprising . Automatic Data Augmentation - Here , given some candidates for data augmentation , the authors propose three methods to discover which candidate work well . Unfortunately , I don \u2019 t fully understand the approach . The authors are examining a Meta-Reinforcement Learning setting , where one wishes to find a policy which performs well not just on one MDP , but on a whole distribution of MDPs . This leads to an inner-and-outer for-loop like setting , in the inner for loop , the agent does multiple episodes with a single environment , in the outer loop the agent gets new environments . I had expected this inner-outer for-loop structure to pe present in the meta learning of augmentations , and for the authors to clearly describe how knowledge about the effectiveness of augmentations is transferred from past episodes on one environment to future episodes in the same environment , and how it \u2019 s transferred from past environments to future environments . The only support given for these methods is the experimental results . Yet we see that the performance of DrAC and UCB-DrAC lie within a standard deviation of one another . So the evidence of the effectiveness of UCB-DrAC over the simpler DrAC is weak at best . Minor Comments : - $ T_m ( s \u2019 |s , a ) $ is the transition function , $ R_m ( s , a ) $ is the reward function : Here , $ T $ and $ R $ are generally distributions and not functions Eq.6 is confusing , since $ f_ * $ can refer to both $ f_t $ ( function at timestep $ t $ ) and $ f_i $ ( the $ i $ -th transformation function ) . Do the update with something like $ N_t ( f ) \\leftarrow N_ { t-1 } ( f ) + 1 $ is a bit clearer , since then it \u2019 s the number of times $ f $ has been pulled before timestep $ t $", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their feedback and we especially appreciate that the reviewer finds our writing clear and believes that data augmentation for reinforcement learning is an important research direction . We respond to the reviewer \u2019 s concerns below . Policy and Value Function Regularization C1 : \u201c One just needs to save the old policies ( and old Advantage function ) , so that \u03c0\u03b8old can be applied to transformed states , and not just use the actions from the buffer . This would seem like a straightforward fix. \u201d A1 : We respectfully disagree with this comment as we believe such a solution would be incorrect and would not solve the problem we emphasize . Please note that the correct estimate of the policy gradient objective used in PPO is the one in equation ( 1 ) which does not use the augmented observations at all since we are estimating advantages for the actual observations A ( s , a ) . The probability distribution used to sample advantages is \\pi_ { old } ( a | s ) ( rather than \\pi_ { old } ( a | f ( s ) ) since we can only interact with the environment via the true observations and not the augmented ones ( because the reward and transition functions are not defined in this case ) . Hence , the correct importance sampling estimate uses \\pi ( a|s ) / \\pi_ { old } ( a | s ) . If we understood correctly , what you propose is to use \\pi ( a | f ( s ) ) / \\pi_ { old } ( a | f ( s ) ) , but this is incorrect for the reasons mentioned above . What we argue is that , in the case of RAD , the only way to use the augmented observations f ( s ) is in the policy gradient objective , whether by \\pi ( a | f ( s ) ) / \\pi_ { old } ( a | f ( s ) ) as you propose or \\pi ( a | f ( s ) ) / \\pi_ { old } ( a | s ) as RAD uses , but both are incorrect . In contrast , DrAC does not change the policy gradient objective at all which remains the one in equation ( 2 ) and instead uses the augmented observations in the additional regularization losses , as shown in equations ( 3 ) , ( 4 ) , and ( 5 ) . C2 : \u201c So now we \u2019 re comparing apples and oranges , since your method ( DrAC ) gets to incorporate this prior knowledge , while PPO and RAD don \u2019 t , and DrAC \u2019 s good performance isn \u2019 t surprising. \u201d A2 : We respectfully disagree with this interpretation of our work . DrAC does not use any extra knowledge compared to RAD since they both use augmented observations to regularize RL agents . The only difference is that DrAC makes better use of this additional data by explicitly regularizing the policy and value function , while RAD attempts to do this implicitly . Please note that , even if RAD uses additional data , its performance can still be worse than PPO \u2019 s , while our method is comparable or better than PPO when using the same extra data as RAD ( as shown in Figure 2 ) . Automatic Data Augmentation C3 : \u201c I had expected this inner-outer for-loop structure to pe present in the meta learning of augmentations , and for the authors to clearly describe how knowledge about the effectiveness of augmentations is transferred from past episodes on one environment to future episodes in the same environment , and how it \u2019 s transferred from past environments to future environments. \u201d A3 : We will add more details about the meta-learning training . Please note that we have also included the code in our submission and we plan to open source it for full transparency . C4 : \u201c Yet we see that the performance of DrAC and UCB-DrAC lie within a standard deviation of one another . So the evidence of the effectiveness of UCB-DrAC over the simpler DrAC is weak at best. \u201d A4 : We never claim that the performance of UCB-DrAC is better than that of DrAC . In fact , DrAC with the best augmentation is an upper bound to UCB-DrAC since DrAC uses the best augmentation during the entire training process while UCB-DrAC needs to first find the best augmentation from a given set . The advantage of UCB-DrAC over DrAC is not superior performance but rather the fact that it provides an automatic way of selecting a good augmentation for a given task . As explained in section 3.3 , UCB-DrAC reduces the computational resources needed to train an RL agent with data augmentation since it only needs to train a model once , while DrAC needs to train models with each type of augmentation in order to select the best one . Our point is that UCB-DrAC still achieves comparable performance with DrAC trained with the best augmentation , as shown in Figure 3 . C5 : Minor Comments . A5 : Thank you for pointing these out . We will change our notation accordingly . In light of these clarifications , we would appreciate it if the reviewer confirmed that all their concerns have been addressed and , if so , reconsider their assessment ."}, {"review_id": "9l9WD4ahJgs-1", "review_text": "This paper presents a method that utilizes data augmentation for image-based reinforcement learning . The data augmentation is used to regularize the policy and function approximation in the proposed method . In addition , a method for automatically identifying effective ways of data augmentation is proposed . The experimental results show that the proposed method outperforms the baseline methods . The study shows that the regularization of policy and function approximation using the transformed images is more effective than training a policy by using the transformed image as a state . Regarding identification of effective ways of data augmentation , the proposed method seems improve the performance of image-based RL methods , although the proposed approach is simple . I have some suggestions for improving the paper . - The term for regularizing the policy is given in Eq . ( 3 ) .I think that the first \\pi ( a|s ) in the KL divergence should also have the subscript \\theta because it seems that the two policies are based on the same model . Likewise , the first term in Eq . ( 4 ) should have the same subscript as the second term . - It would be good to show the Jensen-Shannon divergence and the cycle consistency of RAD in Table 2 . == comments after discussions and the paper update == I appreciate the authors ' efforts to improve the clarity and provide additional results . I believe that the proposed method is now clearly presented and the claims are properly supported by experiments . I raise the score to `` accept '' .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their encouraging and constructive feedback . Below we would like to respond to the reviewer \u2019 s suggestions for improving our work . C1 : \u201c The term for regularizing the policy is given in Eq . ( 3 ) .I think that the first \\pi ( a|s ) in the KL divergence should also have the subscript \\theta because it seems that the two policies are based on the same model . Likewise , the first term in Eq . ( 4 ) should have the same subscript as the second term. \u201d A1 : It is true that the two policies in the equation , \\pi ( a|s ) and \\pi ( a|f ( s , \\nu ) ) have the same parameters \\theta . The reason we left out the \\theta subscript from \\pi ( a|s ) is to indicate that we are only backpropagating gradients through \\pi ( a|f ( s , \\nu ) ) . This is mentioned right below equation ( 5 ) . But we appreciate the suggestion and we will make our notation more clear . C2 : \u201c It would be good to show the Jensen-Shannon divergence and the cycle consistency of RAD in Table 2. \u201d A2 : We agree this would be valuable and we will add these results in the updated draft . In light of the promised changes , we would appreciate it if the reviewer would be willing to reconsider their assessment and/or provide us with further feedback ."}, {"review_id": "9l9WD4ahJgs-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper tackles the problem of generalization in deep RL via data augmentation . It provides a framework for automatic data augmentation based on UCB , RL^2 , or MAML . When UCB is combined with regularization of the policy and value function so that their outputs are invariant to transformations ( such as rotation , crop , etc . ) , it shows improvements over similar algorithms on the ProcGen benchmark . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . The paper is well written and the proposed algorithms are straightforward to understand . 2.UCB-DrAC is shown to be statistically superior to the tested baselines . Surprisingly , when all ProcGen games are taken into account , some existing methods such as Rand-FM are actually worse than PPO . 3.Ablation studies are able to show that both ingredients of UCB-DrAC are important . UCB is shown to find the best augmentation asymptotically , and the DrAC is shown to be better than PPO and RAD . Cons : 1.The experiments do not compare the proposed algorithms to DrQ , the algorithm proposed in Kostrikov et al . ( 2020 ) .Since it also tackles generalization in deep RL through data augmentation , it seems that it should be included as a baseline . Can the authors explain why it was not included ? 2.The proposed algorithms are only combined with PPO . It would be good to have some results for other actor-critic algorithms , such as SAC , to verify if the SOTA behavior holds . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Overall : I would vote for accepting this paper , due to the strength of its experimental results . The proposed approaches are novel in the data augmentation for generalization in deep RL subfield , although AutoAugment ( cited in the paper ) also uses RL for choosing data augmentations in the supervised learning case . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Further comments and questions : 1 . It may be helpful to rewrite some of the algorithms in Section 3.3 in pseudocode , as text is harder to read . 2.Is there intuition for why only UCB-DrAC leads to statistically significant improvements over PPO , and not RL2-DrAC and Meta-DrAC ? Can it be shown that the former has lower sample complexity than the latter two ? 3.Why is the cycle consistency percentage in Table 2 always low ? Is it some idiosyncrasy of the metric or due to the inherent variance in the trajectories ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Update after reading other reviews and author responses : I am happy to keep my score and support accepting this paper . I agree with Reviewer 4 that conducting a more detailed ablation study of the impact of regularizing both the policy and value function ( i.e.a comparison with an algorithm like that of Kostrikov et al . ( 2020 ) ) would improve the paper and hope that it will be included in the final paper .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their positive and constructive feedback . We were delighted to hear they found our paper to be well written , our experimental results strong , and our approach novel for generalization in RL . We address your concerns below . C1 : \u201c The experiments do not compare the proposed algorithms to DrQ , the algorithm proposed in Kostrikov et al . ( 2020 ) .Since it also tackles generalization in deep RL through data augmentation , it seems that it should be included as a baseline . Can the authors explain why it was not included ? \u201d A1 : The reason we do not compare with DrQ is because DrQ uses SAC as a base algorithm , while our method is based on PPO . We chose to use PPO as a base algorithm because it is the state-of-the-art on the Procgen benchmark , which we use to test generalization . In addition , TD ( 0 ) methods such as SAC or DrQ have not been shown yet to achieve decent performance on Procgen . Please also note that DrQ was designed for improving sample complexity rather than generalization . C2 : \u201c The proposed algorithms are only combined with PPO . It would be good to have some results for other actor-critic algorithms , such as SAC , to verify if the SOTA behavior holds. \u201d A2 : As mentioned above , to the best of our knowledge SAC is not well suited for the Procgen benchmark . While we are happy to run more experiments with other actor-critic algorithms , we would like to point out that our paper already contains extensive empirical results , with 25 models on 16 different environments , 10 seeds each , accounting to 4000 experiments in total , each taking at least 4 hours on a GPU ( after hyperparameter search ) . Please note that this is far in excess of most other published papers that use Procgen for evaluation , which showed results only on a subset of the environments ( e.g.3 in Laskin et al.2020 , 1 in Igl et al.2019 and Lee et al.2020 ) .C3 : \u201c It may be helpful to rewrite some of the algorithms in Section 3.3 in pseudocode , as text is harder to read. \u201d A3 : Thank you for the suggestion . We will add a pseudocode in the paper . C4 : \u201c Is there intuition for why only UCB-DrAC leads to statistically significant improvements over PPO , and not RL2-DrAC and Meta-DrAC ? Can it be shown that the former has lower sample complexity than the latter two ? \u201d A4 : We are not entirely sure but UCB does have theoretical finite-time regret guarantees for multi-armed bandit problems such as this one ( Auer , 2002 ) while we are not aware of a comparable result for RL^2 or MAML . We also think it is possible that UCB is better suited for our problem formulation while RL^2 and MAML start showing their benefits on more complex problems such as contextual bandits or sequential decision making . We think this is an interesting question for future work . C5 : `` Why is the cycle consistency percentage in Table 2 always low ? Is it some idiosyncrasy of the metric or due to the inherent variance in the trajectories ? '' A5 : We believe this might indeed be due to the variance in the trajectories . In some cases , there can be more than a single path for maximizing reward and the agent might be prone to choose different paths for different backgrounds of the same level ( which could be exacerbated by the fact that many of the backgrounds are highly structured rather uniform ) . In light of these clarifications , we would appreciate it if the reviewer confirmed that all their concerns have been addressed and , if so , reconsider their assessment ."}, {"review_id": "9l9WD4ahJgs-3", "review_text": "Summary : This paper proposes an automatic data augmentation approach for RL tasks . Specifically , it takes UCB for data selection and introduces two regularization terms for actor-critic algorithms ' policy and value function . Then this paper evaluated the approach based on the Procgen benchmark and demonstrated that it outperforms existing methods . It is also shown that the learned policies and representations are robust to irrelevant factors . Reasons for score : On the one hand , I feel that the proposed approach is incremental ( UCB for data selection with the actor-critic algorithm as the RL algorithm plus additional regularization terms ) . It would also be ideal if more experiments can be included to prove that the proposed approach is effective in most RL tasks . However , on the other hand , based on the current experimentation results , the proposed method seems promising and can be useful for generalization in reinforcement learning . Pros 1.The proposal of the automatic data augmentation/selection approach is useful , given that the existing primary methods either rely on expert knowledge or separately evaluate a large number of transformations to find the best one , both of which are expensive . 2.The proposed approach achieves SOTA performance in the Procgen benchmark . 3.It shows that the proposed method is robust to irrelevant factors . Cons 1.I think the primary difference between the proposed approach and existing approaches is the additional regularization losses . This would make the new approach seem incremental . 2.There is a lack of performance comparison between the automatic data augmentation method and non-automatic approaches . 3.The data augmentation approach is only evaluated in the Procgen benchmark . It would be great if this paper includes more experiments to demonstrate the performance , especially since the proposed method is intended for any RL task .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank their reviewer for their positive and constructive comments . We particularly appreciate that the reviewer finds our approach to be useful , effective and robust to irrelevant factors . We respond to the reviewer \u2019 s comments below . C1 : \u201c I think the primary difference between the proposed approach and existing approaches is the additional regularization losses . This would make the new approach seem incremental. \u201d A1 : We respectfully disagree with the characterization that our approach is \u201c incremental \u201d . On the contrary , we believe that the simplicity of our approach is one of its main strengths since it makes it easier build upon for other members of the community . In addition , even if our method is simple , it shows significant improvements over the previous state-of-the-art on a challenging benchmark for generalization in RL . Moreover , we provide theoretical intuition and empirical evidence for the importance of the additional regularization losses we introduce . As shown in Figure 2 , not using these additional losses , the methods may completely fail to learn in certain cases . C2 : \u201c There is a lack of performance comparison between the automatic data augmentation method and non-automatic approaches. \u201d A2 : Figure 6 and 7 in the Appendix compare our proposed automatic approaches ( UCB-DrAC , RL2-DrAC , and Meta-DrAC ) with the non-automatic ones ( DrAC with the best augmentation ) . C3 : \u201c The data augmentation approach is only evaluated in the Procgen benchmark . It would be great if this paper includes more experiments to demonstrate the performance , especially since the proposed method is intended for any RL task. \u201d A3 : We agree with the reviewer that it would be valuable to include experiments on other domains and we will do our best to report back on this by the end of the discussion period . However , we would like to point out that our paper already contains extensive empirical results , with 25 models on 16 different environments , 10 seeds each , accounting to 4000 experiments in total , each taking at least 4 hours on a GPU ( after hyperparameter search ) . Please note that this is far in excess of most other published papers that use Procgen for evaluation , which showed results only on a subset of the environments ( e.g.3 in Laskin et al.2020 , 1 in Igl et al.2019 and Lee et al.2020 ) .While we aim to include experiments on another domain by the end of the discussion period , due to computational and time constraints , we expect the comparisons to be more limited in scope ( i.e.they will likely contain a smaller number of environments and baselines ) . In light of these clarifications , we would appreciate it if the reviewer would be willing to reconsider their assessment and/or provide us with further feedback ."}], "0": {"review_id": "9l9WD4ahJgs-0", "review_text": "Summary after Discussion Period : -- After reading the other reviewer 's comments and corresponding with the authors , I have become convinced that the author 's proposed regularization method is novel and effective , and would recommend this avenue of research be further explored . Yet it has also become clear to me that the author 's claims on why their method works are not yet supported by evidence . Further , I do n't believe the author 's proposed further ablation studies would fix the theory , since such experiments do n't address whether their method works by fixing problems with Laskin \u2019 s work ( as the author 's claim ) or because it provides a more direct way of enforcing invariance to transformation ( as I claim ) . So we 're left with a difficult situation , the method and the experiments are good while the theory is lacking . In such a situation both acceptance or rejection seem reasonable . Yet , as per ICLR reviewer guidelines , one should answer three questions for oneself : - What is the specific question and/or problem tackled by the paper ? - is the approach well motivated , including being well-placed in the literature ? - Does the paper support the claims ? This includes determining if results , whether theoretical or empirical , are correct and if they are scientifically rigorous . Since the theory is lacking and the approach is not well motivated , and since the theoretical claims have n't been rigorously supported , I feel as per ICLR guidelines the paper is not yet ready for acceptance . Initial Review : - Summary In this paper , the authors introduce three approaches for automatically finding an augmentations for any RL task , and a novel regularization scheme to make such augmentations work effectively . Positive aspects : -- The paper \u2019 s language is clear and the authors provide a good overview of the problem of data augmentation for reinforcement learning . Furthermore , they nicely explain why data augmentation for RL isn \u2019 t as straightforward as augmenting data for supervised learning learning . I believe that data augmentation could be a nice tool in the Reinforcement Learner \u2019 s toolbox , and I \u2019 m glad to see a paper advancing the idea . Major Concerns : -- This paper has not provided sufficient evidence that the author \u2019 s proposed way of doing data augmentation is effective . In this paper , there are two main novel methods for doing data augmentation / insights in RL . I will discuss my concerns with both methods separately . Policy and Value function Regularization . -- The authors criticize the naive application of transformers in the PPO \u2019 s buffer , as done in Laskin et al . ( 2020 ) , saying that this changes the PPO objective . While I agree that such a naive transformation as in Eq.2 is problematic , I fail to understand why application of transformation to states in the buffer would result in the Eq.2 , as the transformation would happen to the states being fed into both $ \\pi_\\theta $ and $ \\pi_ { \\theta_\\text { old } } $ , resulting in an equation different from Eq.2.One just needs to save the old policies ( and old Advantage function ) , so that $ \\pi_ { \\theta_\\text { old } } $ can be applied to transformed states , and not just use the actions from the buffer . This would seem like a straightforward fix . Yet the authors have proposed a different regularization fix which judging the experiments does seem to work , as shown in Figure 2 . I suspect it works for another reason : since the regularization forces $ V ( s ) = V ( f ( s ) ) $ and $ \\pi ( \\cdot | s ) = \\pi ( \\cdot | f ( s ) ) $ I wonder if this isn \u2019 t a method to allow prior knowledge to flow into the policy and value estimation . If the transformation ( s ) $ f_i $ have been chosen such that one can be reasonably sure that true value and policy functions should be invariant to said transformations , then by enforcing $ V ( s ) = V ( f ( s ) ) $ and $ \\pi ( \\cdot | s ) = \\pi ( \\cdot | f ( s ) ) $ one is constraining V and \\pi to fit the prior knowledge contained in $ f_i $ . So now we \u2019 re comparing apples and oranges , since your method ( DrAC ) gets to incorporate this prior knowledge , while PPO and RAD don \u2019 t , and DrAC \u2019 s good performance isn \u2019 t surprising . Automatic Data Augmentation - Here , given some candidates for data augmentation , the authors propose three methods to discover which candidate work well . Unfortunately , I don \u2019 t fully understand the approach . The authors are examining a Meta-Reinforcement Learning setting , where one wishes to find a policy which performs well not just on one MDP , but on a whole distribution of MDPs . This leads to an inner-and-outer for-loop like setting , in the inner for loop , the agent does multiple episodes with a single environment , in the outer loop the agent gets new environments . I had expected this inner-outer for-loop structure to pe present in the meta learning of augmentations , and for the authors to clearly describe how knowledge about the effectiveness of augmentations is transferred from past episodes on one environment to future episodes in the same environment , and how it \u2019 s transferred from past environments to future environments . The only support given for these methods is the experimental results . Yet we see that the performance of DrAC and UCB-DrAC lie within a standard deviation of one another . So the evidence of the effectiveness of UCB-DrAC over the simpler DrAC is weak at best . Minor Comments : - $ T_m ( s \u2019 |s , a ) $ is the transition function , $ R_m ( s , a ) $ is the reward function : Here , $ T $ and $ R $ are generally distributions and not functions Eq.6 is confusing , since $ f_ * $ can refer to both $ f_t $ ( function at timestep $ t $ ) and $ f_i $ ( the $ i $ -th transformation function ) . Do the update with something like $ N_t ( f ) \\leftarrow N_ { t-1 } ( f ) + 1 $ is a bit clearer , since then it \u2019 s the number of times $ f $ has been pulled before timestep $ t $", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their feedback and we especially appreciate that the reviewer finds our writing clear and believes that data augmentation for reinforcement learning is an important research direction . We respond to the reviewer \u2019 s concerns below . Policy and Value Function Regularization C1 : \u201c One just needs to save the old policies ( and old Advantage function ) , so that \u03c0\u03b8old can be applied to transformed states , and not just use the actions from the buffer . This would seem like a straightforward fix. \u201d A1 : We respectfully disagree with this comment as we believe such a solution would be incorrect and would not solve the problem we emphasize . Please note that the correct estimate of the policy gradient objective used in PPO is the one in equation ( 1 ) which does not use the augmented observations at all since we are estimating advantages for the actual observations A ( s , a ) . The probability distribution used to sample advantages is \\pi_ { old } ( a | s ) ( rather than \\pi_ { old } ( a | f ( s ) ) since we can only interact with the environment via the true observations and not the augmented ones ( because the reward and transition functions are not defined in this case ) . Hence , the correct importance sampling estimate uses \\pi ( a|s ) / \\pi_ { old } ( a | s ) . If we understood correctly , what you propose is to use \\pi ( a | f ( s ) ) / \\pi_ { old } ( a | f ( s ) ) , but this is incorrect for the reasons mentioned above . What we argue is that , in the case of RAD , the only way to use the augmented observations f ( s ) is in the policy gradient objective , whether by \\pi ( a | f ( s ) ) / \\pi_ { old } ( a | f ( s ) ) as you propose or \\pi ( a | f ( s ) ) / \\pi_ { old } ( a | s ) as RAD uses , but both are incorrect . In contrast , DrAC does not change the policy gradient objective at all which remains the one in equation ( 2 ) and instead uses the augmented observations in the additional regularization losses , as shown in equations ( 3 ) , ( 4 ) , and ( 5 ) . C2 : \u201c So now we \u2019 re comparing apples and oranges , since your method ( DrAC ) gets to incorporate this prior knowledge , while PPO and RAD don \u2019 t , and DrAC \u2019 s good performance isn \u2019 t surprising. \u201d A2 : We respectfully disagree with this interpretation of our work . DrAC does not use any extra knowledge compared to RAD since they both use augmented observations to regularize RL agents . The only difference is that DrAC makes better use of this additional data by explicitly regularizing the policy and value function , while RAD attempts to do this implicitly . Please note that , even if RAD uses additional data , its performance can still be worse than PPO \u2019 s , while our method is comparable or better than PPO when using the same extra data as RAD ( as shown in Figure 2 ) . Automatic Data Augmentation C3 : \u201c I had expected this inner-outer for-loop structure to pe present in the meta learning of augmentations , and for the authors to clearly describe how knowledge about the effectiveness of augmentations is transferred from past episodes on one environment to future episodes in the same environment , and how it \u2019 s transferred from past environments to future environments. \u201d A3 : We will add more details about the meta-learning training . Please note that we have also included the code in our submission and we plan to open source it for full transparency . C4 : \u201c Yet we see that the performance of DrAC and UCB-DrAC lie within a standard deviation of one another . So the evidence of the effectiveness of UCB-DrAC over the simpler DrAC is weak at best. \u201d A4 : We never claim that the performance of UCB-DrAC is better than that of DrAC . In fact , DrAC with the best augmentation is an upper bound to UCB-DrAC since DrAC uses the best augmentation during the entire training process while UCB-DrAC needs to first find the best augmentation from a given set . The advantage of UCB-DrAC over DrAC is not superior performance but rather the fact that it provides an automatic way of selecting a good augmentation for a given task . As explained in section 3.3 , UCB-DrAC reduces the computational resources needed to train an RL agent with data augmentation since it only needs to train a model once , while DrAC needs to train models with each type of augmentation in order to select the best one . Our point is that UCB-DrAC still achieves comparable performance with DrAC trained with the best augmentation , as shown in Figure 3 . C5 : Minor Comments . A5 : Thank you for pointing these out . We will change our notation accordingly . In light of these clarifications , we would appreciate it if the reviewer confirmed that all their concerns have been addressed and , if so , reconsider their assessment ."}, "1": {"review_id": "9l9WD4ahJgs-1", "review_text": "This paper presents a method that utilizes data augmentation for image-based reinforcement learning . The data augmentation is used to regularize the policy and function approximation in the proposed method . In addition , a method for automatically identifying effective ways of data augmentation is proposed . The experimental results show that the proposed method outperforms the baseline methods . The study shows that the regularization of policy and function approximation using the transformed images is more effective than training a policy by using the transformed image as a state . Regarding identification of effective ways of data augmentation , the proposed method seems improve the performance of image-based RL methods , although the proposed approach is simple . I have some suggestions for improving the paper . - The term for regularizing the policy is given in Eq . ( 3 ) .I think that the first \\pi ( a|s ) in the KL divergence should also have the subscript \\theta because it seems that the two policies are based on the same model . Likewise , the first term in Eq . ( 4 ) should have the same subscript as the second term . - It would be good to show the Jensen-Shannon divergence and the cycle consistency of RAD in Table 2 . == comments after discussions and the paper update == I appreciate the authors ' efforts to improve the clarity and provide additional results . I believe that the proposed method is now clearly presented and the claims are properly supported by experiments . I raise the score to `` accept '' .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their encouraging and constructive feedback . Below we would like to respond to the reviewer \u2019 s suggestions for improving our work . C1 : \u201c The term for regularizing the policy is given in Eq . ( 3 ) .I think that the first \\pi ( a|s ) in the KL divergence should also have the subscript \\theta because it seems that the two policies are based on the same model . Likewise , the first term in Eq . ( 4 ) should have the same subscript as the second term. \u201d A1 : It is true that the two policies in the equation , \\pi ( a|s ) and \\pi ( a|f ( s , \\nu ) ) have the same parameters \\theta . The reason we left out the \\theta subscript from \\pi ( a|s ) is to indicate that we are only backpropagating gradients through \\pi ( a|f ( s , \\nu ) ) . This is mentioned right below equation ( 5 ) . But we appreciate the suggestion and we will make our notation more clear . C2 : \u201c It would be good to show the Jensen-Shannon divergence and the cycle consistency of RAD in Table 2. \u201d A2 : We agree this would be valuable and we will add these results in the updated draft . In light of the promised changes , we would appreciate it if the reviewer would be willing to reconsider their assessment and/or provide us with further feedback ."}, "2": {"review_id": "9l9WD4ahJgs-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper tackles the problem of generalization in deep RL via data augmentation . It provides a framework for automatic data augmentation based on UCB , RL^2 , or MAML . When UCB is combined with regularization of the policy and value function so that their outputs are invariant to transformations ( such as rotation , crop , etc . ) , it shows improvements over similar algorithms on the ProcGen benchmark . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . The paper is well written and the proposed algorithms are straightforward to understand . 2.UCB-DrAC is shown to be statistically superior to the tested baselines . Surprisingly , when all ProcGen games are taken into account , some existing methods such as Rand-FM are actually worse than PPO . 3.Ablation studies are able to show that both ingredients of UCB-DrAC are important . UCB is shown to find the best augmentation asymptotically , and the DrAC is shown to be better than PPO and RAD . Cons : 1.The experiments do not compare the proposed algorithms to DrQ , the algorithm proposed in Kostrikov et al . ( 2020 ) .Since it also tackles generalization in deep RL through data augmentation , it seems that it should be included as a baseline . Can the authors explain why it was not included ? 2.The proposed algorithms are only combined with PPO . It would be good to have some results for other actor-critic algorithms , such as SAC , to verify if the SOTA behavior holds . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Overall : I would vote for accepting this paper , due to the strength of its experimental results . The proposed approaches are novel in the data augmentation for generalization in deep RL subfield , although AutoAugment ( cited in the paper ) also uses RL for choosing data augmentations in the supervised learning case . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Further comments and questions : 1 . It may be helpful to rewrite some of the algorithms in Section 3.3 in pseudocode , as text is harder to read . 2.Is there intuition for why only UCB-DrAC leads to statistically significant improvements over PPO , and not RL2-DrAC and Meta-DrAC ? Can it be shown that the former has lower sample complexity than the latter two ? 3.Why is the cycle consistency percentage in Table 2 always low ? Is it some idiosyncrasy of the metric or due to the inherent variance in the trajectories ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Update after reading other reviews and author responses : I am happy to keep my score and support accepting this paper . I agree with Reviewer 4 that conducting a more detailed ablation study of the impact of regularizing both the policy and value function ( i.e.a comparison with an algorithm like that of Kostrikov et al . ( 2020 ) ) would improve the paper and hope that it will be included in the final paper .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their positive and constructive feedback . We were delighted to hear they found our paper to be well written , our experimental results strong , and our approach novel for generalization in RL . We address your concerns below . C1 : \u201c The experiments do not compare the proposed algorithms to DrQ , the algorithm proposed in Kostrikov et al . ( 2020 ) .Since it also tackles generalization in deep RL through data augmentation , it seems that it should be included as a baseline . Can the authors explain why it was not included ? \u201d A1 : The reason we do not compare with DrQ is because DrQ uses SAC as a base algorithm , while our method is based on PPO . We chose to use PPO as a base algorithm because it is the state-of-the-art on the Procgen benchmark , which we use to test generalization . In addition , TD ( 0 ) methods such as SAC or DrQ have not been shown yet to achieve decent performance on Procgen . Please also note that DrQ was designed for improving sample complexity rather than generalization . C2 : \u201c The proposed algorithms are only combined with PPO . It would be good to have some results for other actor-critic algorithms , such as SAC , to verify if the SOTA behavior holds. \u201d A2 : As mentioned above , to the best of our knowledge SAC is not well suited for the Procgen benchmark . While we are happy to run more experiments with other actor-critic algorithms , we would like to point out that our paper already contains extensive empirical results , with 25 models on 16 different environments , 10 seeds each , accounting to 4000 experiments in total , each taking at least 4 hours on a GPU ( after hyperparameter search ) . Please note that this is far in excess of most other published papers that use Procgen for evaluation , which showed results only on a subset of the environments ( e.g.3 in Laskin et al.2020 , 1 in Igl et al.2019 and Lee et al.2020 ) .C3 : \u201c It may be helpful to rewrite some of the algorithms in Section 3.3 in pseudocode , as text is harder to read. \u201d A3 : Thank you for the suggestion . We will add a pseudocode in the paper . C4 : \u201c Is there intuition for why only UCB-DrAC leads to statistically significant improvements over PPO , and not RL2-DrAC and Meta-DrAC ? Can it be shown that the former has lower sample complexity than the latter two ? \u201d A4 : We are not entirely sure but UCB does have theoretical finite-time regret guarantees for multi-armed bandit problems such as this one ( Auer , 2002 ) while we are not aware of a comparable result for RL^2 or MAML . We also think it is possible that UCB is better suited for our problem formulation while RL^2 and MAML start showing their benefits on more complex problems such as contextual bandits or sequential decision making . We think this is an interesting question for future work . C5 : `` Why is the cycle consistency percentage in Table 2 always low ? Is it some idiosyncrasy of the metric or due to the inherent variance in the trajectories ? '' A5 : We believe this might indeed be due to the variance in the trajectories . In some cases , there can be more than a single path for maximizing reward and the agent might be prone to choose different paths for different backgrounds of the same level ( which could be exacerbated by the fact that many of the backgrounds are highly structured rather uniform ) . In light of these clarifications , we would appreciate it if the reviewer confirmed that all their concerns have been addressed and , if so , reconsider their assessment ."}, "3": {"review_id": "9l9WD4ahJgs-3", "review_text": "Summary : This paper proposes an automatic data augmentation approach for RL tasks . Specifically , it takes UCB for data selection and introduces two regularization terms for actor-critic algorithms ' policy and value function . Then this paper evaluated the approach based on the Procgen benchmark and demonstrated that it outperforms existing methods . It is also shown that the learned policies and representations are robust to irrelevant factors . Reasons for score : On the one hand , I feel that the proposed approach is incremental ( UCB for data selection with the actor-critic algorithm as the RL algorithm plus additional regularization terms ) . It would also be ideal if more experiments can be included to prove that the proposed approach is effective in most RL tasks . However , on the other hand , based on the current experimentation results , the proposed method seems promising and can be useful for generalization in reinforcement learning . Pros 1.The proposal of the automatic data augmentation/selection approach is useful , given that the existing primary methods either rely on expert knowledge or separately evaluate a large number of transformations to find the best one , both of which are expensive . 2.The proposed approach achieves SOTA performance in the Procgen benchmark . 3.It shows that the proposed method is robust to irrelevant factors . Cons 1.I think the primary difference between the proposed approach and existing approaches is the additional regularization losses . This would make the new approach seem incremental . 2.There is a lack of performance comparison between the automatic data augmentation method and non-automatic approaches . 3.The data augmentation approach is only evaluated in the Procgen benchmark . It would be great if this paper includes more experiments to demonstrate the performance , especially since the proposed method is intended for any RL task .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank their reviewer for their positive and constructive comments . We particularly appreciate that the reviewer finds our approach to be useful , effective and robust to irrelevant factors . We respond to the reviewer \u2019 s comments below . C1 : \u201c I think the primary difference between the proposed approach and existing approaches is the additional regularization losses . This would make the new approach seem incremental. \u201d A1 : We respectfully disagree with the characterization that our approach is \u201c incremental \u201d . On the contrary , we believe that the simplicity of our approach is one of its main strengths since it makes it easier build upon for other members of the community . In addition , even if our method is simple , it shows significant improvements over the previous state-of-the-art on a challenging benchmark for generalization in RL . Moreover , we provide theoretical intuition and empirical evidence for the importance of the additional regularization losses we introduce . As shown in Figure 2 , not using these additional losses , the methods may completely fail to learn in certain cases . C2 : \u201c There is a lack of performance comparison between the automatic data augmentation method and non-automatic approaches. \u201d A2 : Figure 6 and 7 in the Appendix compare our proposed automatic approaches ( UCB-DrAC , RL2-DrAC , and Meta-DrAC ) with the non-automatic ones ( DrAC with the best augmentation ) . C3 : \u201c The data augmentation approach is only evaluated in the Procgen benchmark . It would be great if this paper includes more experiments to demonstrate the performance , especially since the proposed method is intended for any RL task. \u201d A3 : We agree with the reviewer that it would be valuable to include experiments on other domains and we will do our best to report back on this by the end of the discussion period . However , we would like to point out that our paper already contains extensive empirical results , with 25 models on 16 different environments , 10 seeds each , accounting to 4000 experiments in total , each taking at least 4 hours on a GPU ( after hyperparameter search ) . Please note that this is far in excess of most other published papers that use Procgen for evaluation , which showed results only on a subset of the environments ( e.g.3 in Laskin et al.2020 , 1 in Igl et al.2019 and Lee et al.2020 ) .While we aim to include experiments on another domain by the end of the discussion period , due to computational and time constraints , we expect the comparisons to be more limited in scope ( i.e.they will likely contain a smaller number of environments and baselines ) . In light of these clarifications , we would appreciate it if the reviewer would be willing to reconsider their assessment and/or provide us with further feedback ."}}