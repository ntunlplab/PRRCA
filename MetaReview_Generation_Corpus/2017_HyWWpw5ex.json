{"year": "2017", "forum": "HyWWpw5ex", "title": "Recurrent Coevolutionary Feature Embedding Processes for Recommendation", "decision": "Reject", "meta_review": "A nice paper, with sufficient experimental validation, and the idea of incorporating a form of change point detection is good. However, the technical contribution relative to the NIPS paper by the same authors is not significant, in that it primarily involves using an RNN instead of a Hawkes process to model the temporal dynamics. The results are significantly better than this earlier paper -- the authors should explore if this is due only to the RNN, or to the optimization method.", "reviews": [{"review_id": "HyWWpw5ex-0", "review_text": "The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled \u2013 autoregressive processes \u2013 i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines. There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation. The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers. Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation? A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods. Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors. I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your reviews . A.Comparison between ( Wang et.al 2016 ) Our work is significantly different in both modeling and learning : 1 . ( Wang et.al 2016 ) uses a specific and simple process ( Hawkes process ) to model the evolution of features , which corresponds to a * linear * summation of history features . Our work is more expressive and general since we use the * nonlinear * recurrent framework . Specifically , we use a latent state to capture the histories , and update it using RNN . The RNN aspect makes our work able to capture the nonlinearity in the data hence our work has consistent improvements over ( Wang et al 2016 ) in the prediction tasks . A recent work ( Recurrent Marked Temporal Point Process , Nan et.al 2016 ) shows that the RNN is quite flexible to approximate many point process models . This work systematically compared the RNN with many other point process models , such as Hawkes and self-correcting processes . Thus using RNN to capture the coevolutionary feature process gives us a powerful and flexible model . 2.Regarding the # parameters , In ( Wang et.al 2016 ) , # parameters are O ( # user \\times # item ) . However , our work only has O ( # user + # item ) regardless of RNN related parameters . 3.In terms of learning : ( Wang et.al 2016 ) uses a convex optimization which is a batch method . However , we develop a novel stochastic training technique which is * never * explored in this co-evolution dynamic graph scenario before . Our method can also be potentially applied to online setting . 4.In terms of the time prediciton task : ( Wang et.al 2016 ) needs to simulate future events to predict the next event time , which more sampling computation and less accurate . However , we assume the history is encoded in the hidden states of RNN and we can use a parametric distribution which has the closed form of the expected value of next event time . According to the universal approximation theorem , the network with sigmoid ( or tanh ) function used in our paper is already powerful enough [ Cybenko. , G. ( 1989 ) ] . And also , it is straightforward to incorporate more complicated RNN cells , such as GRU or LSTM , to further strengthen the representation power . We also tried different parameterizations of the intensity functions ( e.g. , the version used in Nan et.al 2016 ) , different dependency of multidimensional point process ( e.g. , user-centric multidimensional one used in ( Wang et.al 2016 ) , versus our symmetric NxM dimensional one ) , different activation functions ( ReLU has poor performance in this case ) , and used the best paramertrization choice in our paper . Please check http : //www.cc.gatech.edu/~ywang/papers/WanDuTriSon16.pdf for the NIPS paper ( Wang et al 2016 ) . B.Evaluation metric : 1 . To test the item prediction performance , given a triple of the upcoming test event ( user U , item I , time T ) , we ask the question : what is the item the user U will interact at time T ? Since we know the true answer is item I , we evaluate the model \u2019 s prediction against this groundtruth . 2.Accurate prediction of the returning time of a customer to a specific service is also very useful for the service provider . Although the following important applications are from different domains , they can be well captured by the proposed model : I . As a web company , like Google and Facebook , time-sensitive recommendations can have potential impact first to display ads . If we can predict when our users will come back next , we can make the existing ads bidding much more economical , allowing marketers to bid on time slots . After all , marketers do not need to blindly bid all time slots indiscriminately . II.For most online stores , accurate prediction of the returning time of customers can help to improve stock management and products display and arrangement . III.For mainstream personal assistants , like Google Now , because people tend to have different activities dependent on the temporal/spatial contexts like morning vs. evening , weekdays vs. weekend , making recommendations on the right thing and at the right moment can make such services more relevant and usable . IV.In modern electronic health record data , patients may have several diseases that have complicated dependencies on each other . The occurrence of one disease can trigger the progression of others . Predicting the returning time on certain disease can effectively help to take proactive steps to reduce the potential risks . C. complexity 1 . # parameters : If we use one-hot representation of user and items , then the basic feature embedding would take ( # user + # item ) \\times embedding_size parameters ; the interaction features are independent to # user or # item ( e.g. , bag of words features ) ; other model parameters are independent of the dataset size , so we won \u2019 t have many more parameters than basic MF models . 2.Training complexity : we use stochastic training to handle large datasets , each time we focus on consecutive K samples . When a new event happens , two embeddings gets updated , and # user + # item dimensions will update their constant intensity functions . So ideally we need O ( ( # user + # item ) \\times K ) to forward the intensity and its integration ( survival probability , which has closed form of integration ) . However , as we mentioned in the paper , we use NCE to sample C \u2018 \u2019 negative \u2019 \u2019 ( i.e. , the dimensions survives from last event to current event ) dimensions , in order to further reduce the terms of survival probabilities , analogous to NCE used in language model to deal with large vocabulary size . So finally , we update 2K embeddings , and correspondingly compute C * K survival terms . 3.Testing task complexity : a ) for item recommendation , we need O ( N ) time to compare with each item for the current user ; this is unimprovable . b ) for time prediction , it takes O ( 1 ) since we have closed form of expectation calculation ; c ) we use RNN to update two embeddings corresponds to the current user and item . It still takes O ( 1 ) . In summary , our method is scalable in both training and testing . During testing , it has same complexity as traditional matrix factorization based methods ."}, {"review_id": "HyWWpw5ex-1", "review_text": "This paper proposes a method to model time changing dynamics in collaborative filtering. Comments: 1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN 2) The author describes a BPTT technique to train the model 3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair. 4) It would be interesting to consider other metrics, for example - The switching time where a user changes his/her to another item - Jointly predict the next item and switching time. In summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your reviews . Your suggestions on the new metric are very interesting , and we will definitely incorporate them in our experiments . The first switching time metric is actually similar to our time prediction task . To do this , we can predict the time for each item , then report the one with earliest time as next event . As for the second metric , since P ( item , time | history ) can be factorized as P ( item | history ) P ( time | item , history ) , we can make the joint prediction according to this formula ."}, {"review_id": "HyWWpw5ex-2", "review_text": "The paper seeks to predict user events (interactions with items at a particular point in time). Roughly speaking the contributions are as follows: (a) the paper models the co-evolutionary process of users' preferences toward items (b) the paper is able to incorporate external sources of information, such as user and item features (c) the process proposed is generative, so is able to estimate specific time-points at which events occur (d) the model is able to account for non-linearities in the above Following the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper. A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is). Other than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar. I hadn't quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times). This is a slightly unusual setting compared to most temporal recommender systems work. I question to some extent whether this problem setting isn't a bit restrictive. That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with \"hundreds\" of events means that you're left with a very biased sample of the user base. Other than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your reviews . Our work focus on the implicit feedback problem . It is motivated by the real world applications where users watch TV programs or visit different restaurants multiple times . Hence it is nature that we require each * user * would have multiple events , but we do not require that each user-item pair has multiple events . We are now doing experiments on a larger Yelp dataset , and will further improve the experiment section ."}], "0": {"review_id": "HyWWpw5ex-0", "review_text": "The paper introduces a time dependent recommender system based on point processes parametrized by time dependent user and item latent representations. The later are modeled as coupled \u2013 autoregressive processes \u2013 i.e. the representation of a user/item changes when he interacts with an item/user, and is a function of both the user and the item representations before time t. This is called coevolution here and the autoregressive process is called recurrent NN. The model may also incorporate heterogeneous inputs. Experiments are performed on several datasets, and the model is compared with different baselines. There are several contributions in the paper: 1) modeling recommendation via parametrized point processes where the parameter dynamics are modeled by latent user/item representations, 2) an optimization algorithm for maximizing the likelihood of this process, with different technical tricks that seem to break its intrinsic complexity, 3) evaluation experiments for time dependent recommendation. The paper by the same authors (NIPS 2016) describes a similar model of continuous time coevolution, and a similar evaluation. The difference lies in the details of the model: the point process model is not the same and of the latent factor dynamic model is slightly different, but the modeling approach and the arguments are exactly the same. By the end, one does not know what makes this model perform better than the one proposed in NIPS, is it the choice for the process, the new parametrization? Both are quite similar. There is no justification on the choice of the specific form of the point process in the two papers. Did the authors tried other forms as well? The same remark applies for the form of the dynamical process: the non-linearity used for the modeling of the latent user/item vectors here is limited to a sigmoid function, which probably does not change much w.r.t. a linear model, but there is no evidence of the role of this non linearity in the paper. Note that there are some inconsistencies between the results in the two papers. Concerning the evaluation, the authors introduce two criteria. I did not get exactly how they evaluate the item recommendation: it is mentioned that at each time t, the model predicts the item the user will interact with. Do you mean, the next item the user will interact with after time t? For the time prediction, why is it a relevant metric for recommendation? A comparison of the complexity, or execution time of the different methods would be helpful. The complexity of your method is apparently proportional to #items*#users, what are the complexity limits of your methods. Overall, the paper is quite nice and looks technically sound, albeit many details are missing. On the other hand, I have a mixed feeling because of the similarity with NIPS paper. The authors should have make a better work at convincing us that this is not a marginal extension of previous work by the authors. I was not convinced either by the evaluation criteria and there is no evidence that the model can be used for large datasets. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your reviews . A.Comparison between ( Wang et.al 2016 ) Our work is significantly different in both modeling and learning : 1 . ( Wang et.al 2016 ) uses a specific and simple process ( Hawkes process ) to model the evolution of features , which corresponds to a * linear * summation of history features . Our work is more expressive and general since we use the * nonlinear * recurrent framework . Specifically , we use a latent state to capture the histories , and update it using RNN . The RNN aspect makes our work able to capture the nonlinearity in the data hence our work has consistent improvements over ( Wang et al 2016 ) in the prediction tasks . A recent work ( Recurrent Marked Temporal Point Process , Nan et.al 2016 ) shows that the RNN is quite flexible to approximate many point process models . This work systematically compared the RNN with many other point process models , such as Hawkes and self-correcting processes . Thus using RNN to capture the coevolutionary feature process gives us a powerful and flexible model . 2.Regarding the # parameters , In ( Wang et.al 2016 ) , # parameters are O ( # user \\times # item ) . However , our work only has O ( # user + # item ) regardless of RNN related parameters . 3.In terms of learning : ( Wang et.al 2016 ) uses a convex optimization which is a batch method . However , we develop a novel stochastic training technique which is * never * explored in this co-evolution dynamic graph scenario before . Our method can also be potentially applied to online setting . 4.In terms of the time prediciton task : ( Wang et.al 2016 ) needs to simulate future events to predict the next event time , which more sampling computation and less accurate . However , we assume the history is encoded in the hidden states of RNN and we can use a parametric distribution which has the closed form of the expected value of next event time . According to the universal approximation theorem , the network with sigmoid ( or tanh ) function used in our paper is already powerful enough [ Cybenko. , G. ( 1989 ) ] . And also , it is straightforward to incorporate more complicated RNN cells , such as GRU or LSTM , to further strengthen the representation power . We also tried different parameterizations of the intensity functions ( e.g. , the version used in Nan et.al 2016 ) , different dependency of multidimensional point process ( e.g. , user-centric multidimensional one used in ( Wang et.al 2016 ) , versus our symmetric NxM dimensional one ) , different activation functions ( ReLU has poor performance in this case ) , and used the best paramertrization choice in our paper . Please check http : //www.cc.gatech.edu/~ywang/papers/WanDuTriSon16.pdf for the NIPS paper ( Wang et al 2016 ) . B.Evaluation metric : 1 . To test the item prediction performance , given a triple of the upcoming test event ( user U , item I , time T ) , we ask the question : what is the item the user U will interact at time T ? Since we know the true answer is item I , we evaluate the model \u2019 s prediction against this groundtruth . 2.Accurate prediction of the returning time of a customer to a specific service is also very useful for the service provider . Although the following important applications are from different domains , they can be well captured by the proposed model : I . As a web company , like Google and Facebook , time-sensitive recommendations can have potential impact first to display ads . If we can predict when our users will come back next , we can make the existing ads bidding much more economical , allowing marketers to bid on time slots . After all , marketers do not need to blindly bid all time slots indiscriminately . II.For most online stores , accurate prediction of the returning time of customers can help to improve stock management and products display and arrangement . III.For mainstream personal assistants , like Google Now , because people tend to have different activities dependent on the temporal/spatial contexts like morning vs. evening , weekdays vs. weekend , making recommendations on the right thing and at the right moment can make such services more relevant and usable . IV.In modern electronic health record data , patients may have several diseases that have complicated dependencies on each other . The occurrence of one disease can trigger the progression of others . Predicting the returning time on certain disease can effectively help to take proactive steps to reduce the potential risks . C. complexity 1 . # parameters : If we use one-hot representation of user and items , then the basic feature embedding would take ( # user + # item ) \\times embedding_size parameters ; the interaction features are independent to # user or # item ( e.g. , bag of words features ) ; other model parameters are independent of the dataset size , so we won \u2019 t have many more parameters than basic MF models . 2.Training complexity : we use stochastic training to handle large datasets , each time we focus on consecutive K samples . When a new event happens , two embeddings gets updated , and # user + # item dimensions will update their constant intensity functions . So ideally we need O ( ( # user + # item ) \\times K ) to forward the intensity and its integration ( survival probability , which has closed form of integration ) . However , as we mentioned in the paper , we use NCE to sample C \u2018 \u2019 negative \u2019 \u2019 ( i.e. , the dimensions survives from last event to current event ) dimensions , in order to further reduce the terms of survival probabilities , analogous to NCE used in language model to deal with large vocabulary size . So finally , we update 2K embeddings , and correspondingly compute C * K survival terms . 3.Testing task complexity : a ) for item recommendation , we need O ( N ) time to compare with each item for the current user ; this is unimprovable . b ) for time prediction , it takes O ( 1 ) since we have closed form of expectation calculation ; c ) we use RNN to update two embeddings corresponds to the current user and item . It still takes O ( 1 ) . In summary , our method is scalable in both training and testing . During testing , it has same complexity as traditional matrix factorization based methods ."}, "1": {"review_id": "HyWWpw5ex-1", "review_text": "This paper proposes a method to model time changing dynamics in collaborative filtering. Comments: 1) The main idea of the paper is build upon similar to a previous work by the same group of author (Wang et.al KDD), the major difference appears to be change some of the latent factors to be RNN 2) The author describes a BPTT technique to train the model 3) The author introduced time prediction as a new metric to evaluate the effectiveness of time dependent model. However, this need to be condition on a given user-item pair. 4) It would be interesting to consider other metrics, for example - The switching time where a user changes his/her to another item - Jointly predict the next item and switching time. In summary, this is a paper that improves over an existing work on time dynamics model in recommender system. The time prediction metric is interesting and opens up interesting discussion on how we should evaluate recommender systems when time is involved (see also comments). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your reviews . Your suggestions on the new metric are very interesting , and we will definitely incorporate them in our experiments . The first switching time metric is actually similar to our time prediction task . To do this , we can predict the time for each item , then report the one with earliest time as next event . As for the second metric , since P ( item , time | history ) can be factorized as P ( item | history ) P ( time | item , history ) , we can make the joint prediction according to this formula ."}, "2": {"review_id": "HyWWpw5ex-2", "review_text": "The paper seeks to predict user events (interactions with items at a particular point in time). Roughly speaking the contributions are as follows: (a) the paper models the co-evolutionary process of users' preferences toward items (b) the paper is able to incorporate external sources of information, such as user and item features (c) the process proposed is generative, so is able to estimate specific time-points at which events occur (d) the model is able to account for non-linearities in the above Following the pre-review questions, I understand that it is the combination of (a) and (c) that is the most novel aspect of the paper. A fully generative process which can be sampled is certainly nice (though of course, non-generative processes like regular old regression can estimate specific time points and such too, so not sure in practice how relevant this distinction is). Other than that the above parts have all appeared in some combination in previous work, though the combination of parts here certainly passes the novelty bar. I hadn't quite followed the issue mentioned in the pre-review discussion that the model requires multiple interactions per userXitem pair in order to fit the model (e.g. a user interacts with the same business multiple times). This is a slightly unusual setting compared to most temporal recommender systems work. I question to some extent whether this problem setting isn't a bit restrictive. That being said I take the point about why the authors had to subsample the Yelp data, but keeping only users with \"hundreds\" of events means that you're left with a very biased sample of the user base. Other than the above issues, the paper is technically nice, and the experiments include strong baselines and reports good performance. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your reviews . Our work focus on the implicit feedback problem . It is motivated by the real world applications where users watch TV programs or visit different restaurants multiple times . Hence it is nature that we require each * user * would have multiple events , but we do not require that each user-item pair has multiple events . We are now doing experiments on a larger Yelp dataset , and will further improve the experiment section ."}}