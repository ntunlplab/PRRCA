{"year": "2017", "forum": "BkGakb9lx", "title": "RenderGAN: Generating Realistic Labeled Data", "decision": "Invite to Workshop Track", "meta_review": "This paper was fairly well received by the reviewers in terms of the underling idea but the fact that a very specialized problem was the focus of the paper held back reviewers from giving stronger ratings. The question of what sorts of baselines would be reasonable was discussed extensively as the reviewers felt that other credible baselines should be included. The authors argue certain baselines are not appropriate but they were not able to clearly sway the reviewers to a more positive rating based on their response to this issue. We recommend a workshop invitation for this paper.", "reviews": [{"review_id": "BkGakb9lx-0", "review_text": "This paper addresses the problem of decoding barcode-like markers depicted in an image. The main insight is to train a CNN from generated data produced from a GAN. The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background). The parameters for the image transformations are trained such that it confuses a GAN discriminator. A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images. The proposed method out-performs both baselines on decoding the barcode markers. The proposed GAN architecture could potentially be interesting. However, I won\u2019t champion the paper as the evaluation could be improved. A critical missing baseline is a comparison against a generic GAN. Without this it\u2019s hard to judge the benefit of the more structured GAN. Also, it would be worth seeing the result when one combines generated and real images for the final task. A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes): [A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015. [B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016. The problem domain (decoding barcode markers on bees) is limited. It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed. I found the writing to be somewhat vague throughout. For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper. Minor comments: Fig 3 - Are these really renders from a 3D model? The images look like 2D images, perhaps spatially warped via a homography. Page 3: \"chapter\" => \"section\". In Table 2, what is the loss used for the DCNN? Fig 9 (a) - The last four images look like they have strange artifacts. Can you explain these? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your review . Your comments have helped to improve the paper . We think your main point of critique was that we did not evaluate our system sufficiently : > \u201c However , I won \u2019 t champion the paper as the evaluation could be improved . > A critical missing baseline is a comparison against a generic GAN . Without this > it \u2019 s hard to judge the benefit of the more structured GAN . \u201c Since similar comments were made by the other reviewers , we formulated a general response to this point . Please refer to the \u201c general rebuttal \u201d in which we give a detailed rationale of why a generic GAN can not be used as a baseline as you ( and others ) suggested . We think this is a crucial point representing the central motivation of the RenderGAN method . > \u201c Also , it would be worth seeing the result when one combines generated and > real images for the final task. \u201d In previous experiments , we used a mixture of real vs. generated data ( 5/95 and 50/50 ) . The real data was augmented to prevent overfitting as described in Appendix C in the paper . Both experiments did not result in significant improvements , and we therefore decided not to include them in the paper . We agree that this might be interesting and we will update the manuscript accordingly . Thank you for pointing out additional references . We will include both of them in our next update . > \u201c The problem domain ( decoding barcode markers on bees ) is limited . It would > be great to see this applied to another problem domain , e.g. , object detection > from 3D models as shown in paper reference [ A ] , where direct comparison against > prior work could be performed . \u201c We are currently working on more popular problem domains . We , however , feel that , although the problem has limited applicability for the general reader , it nonetheless represents a complex problem with analogies to other , potentially more popular problems , like e.g.pose estimation for cars or faces . > \u201c I found the writing to be somewhat vague throughout . For instance , on first > reading of the introduction it is not clear what exactly is the contribution of > the paper. \u201d Thank you for this feedback . We are currently revising the manuscript to improve overall clarity and will update the paper in the following days . Thank you also for your minor comments , the manuscript will be updated with appropriate changes . > \u201c Fig 3 - Are these really renders from a 3D model ? The images look like 2D > images , perhaps spatially warped via a homography. \u201d Yes , the bee marker is represented by a 3D mesh which is rotated in 3D space according to the parameters and then projected to the image plane . Here is a link to the code ( https : //goo.gl/9UWuhi ) . This is a very simple 3D model and we believe this is one of the advantages of the RenderGAN method . Only a rudimentary object and camera model are required . All the rest of the imaging process , lighting , shadows , blur , reflections , compression artifacts , etc . is learned by the RenderGAN . > \u201c Page 3 : `` chapter '' = > `` section '' . \u201d Changed accordingly . > \u201c In Table 2 , what is the loss used for the DCNN ? \u201d The loss is the mean log-loss for the classification of the individual bits of the barcode marker . We will include this in the caption of the figure in the next revision . > \u201c Fig 9 ( a ) - The last four images look like they have strange artifacts . > Can you explain these ? \u201d We mentioned them briefly in the text ( Section 3 , Training ) : > In some cases , the generator creates artifacts that destroy the images . As > these bad images are scored unrealistic by the discriminator without exception , > we can discard them for the training of the supervised algorithm . We suspect that during training the generator can deceive the discriminator by producing out of distribution samples i.e.very unrealistic images . The discriminator learns quickly to score them as fake , and now the gradient for the generator might be too small to escape this local minimum . However , this is only a hypothesis which we did not investigate further . The images with the strange artifacts are rated unrealistic by the discriminator without exception . Therefore , we can sort them out before training the DCNN ."}, {"review_id": "BkGakb9lx-1", "review_text": "The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. The topic of the paper \u2014 using machine learning (in particular, adversarial training) for generating realistic synthetic training data \u2014 is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets. I appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper. As Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios. The authors should tone down their claims such as \u201cOur method is an improvement over previous work <...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. \u201c. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your review . You summarized our work well , and your feedback contributed significantly to improving our paper . > As Reviewer3 points out , it would be interesting to analyze if restricting GAN > to a fixed set of transformations is necessary here , [ ... ] . Please see our general rebuttal for a detailed explanation why the transformations are necessary and can not be left out . We will revise the manuscript to make this point clearer . > The downside is that experiments are limited to a fairly simple and > not-widely-known domain of honeybee marker classification . We agree , there are more complex datasets e.g.ImageNet.However , our problem , i.e.extraction of the marker \u2019 s ID and rotation in space , was unsolvable with state-of-the-art supervised learning methods due to the time-consuming nature of manual labeling . We are currently working on more popular problem domains . We , however , feel that , although the problem has limited applicability for the general reader , it nonetheless represents a complex problem with analogies to other , potentially more popular problems , like pose estimation for cars or faces . > The authors should tone down their claims such as \u201c Our method is an improvement > over previous work < ... > Whereas previous work relied on real data for training > using pre-trained models or mixing real and generated data , we were able to > train a DCNN from scratch with generated data that performed well when tested on > real data . \u201c .This is not a fair comparison : the domain studied by authors in > this work is much simpler than what was studied in these previous works , so this > comparison is not appropriate . We fully agree and will change the paragraph accordingly ."}, {"review_id": "BkGakb9lx-2", "review_text": "The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture. The main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance. several points were raised during the discussion: 1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks. The answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty. 2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator) The authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty. 3. How do the different stages (\\phis) effect performance? which are the most important ones? The authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps. While there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your review . Your pre-review questions already helped to improve the paper . However , we disagree with several points raised in your review . > The main novelty are parametric modules that emulate different transformations > and artefact that allow to match the natural appearance . [ ... ] the proposed > method is more model driven that previous GAN models . But does it pay off ? how > would a traditional GAN approach perform ? [ ... ] The key proposal of the > submission seems parameterised modules that can be trained to match the real > data distribution . but it remains unclear why not a more generic > parameterisation can also do the job . E.g.a neural network - as done in regular > GANs . We disagree with your summary that our main contribution are augmentation functions that allow a GAN to learn the real data distribution . In fact , the augmentation functions are specifically designed to constrain the generative capabilities of the model . A generic GAN can not be used as a baseline for our method . Please refer to our general rebuttal for a more thorough reply . > Using a render engine to generate the initial sample appearance if of limited novelty . The point of using a render engine is not to generate an initial appearance but to allow collecting a _labeled_ sample that subsequently is rendered more realistic by a constrained GAN . To the best of our knowledge , this has never been done before . > The answers of the authors only partially addresses the point . This is incorrect . We explained in our pre-review reply that a generic GAN can not be used as a baseline . Furthermore , we elaborated on this point in our general rebuttal . Could you please clarify your point if you feel that we still didn \u2019 t address your questions completely ? > The authors reply that plenty of such augmentation was used and more details are > going to be provided in the appendix . it would have been appreciated if such > information was directly included in the revision - so that the procedure could > be directly checked . right now - this remains a point of uncertainty . It was already included in the revision from 9 Dec 2016 which is based on your pre-review feedback ( see appendix C page 14 ) . > The authors do evaluate the effect of hand tuning the transformation stages vs. > learning them . it would be great to also include results of including/excluding > stages completely We agree , excluding different transformation stages would shed light on the relative importance of each stage , and this would be an interesting aspect . We will try to include this in the manuscript by the submission deadline . For now , the differences in performance obtained by hand-designed and learned augmentations can be used as a rough proxy for the importance of the individual augmentations . For example , there is a large improvement in validation performance when replacing the hand-designed lighting augmentation with a learned one . We added a respective remark to the latest revision ( 9 Dec ) . > - and also reporting how much the initial jittering of the data helps . It is not clear to us what you mean with initial jittering of the data . Would you mind clarifying this point ?"}], "0": {"review_id": "BkGakb9lx-0", "review_text": "This paper addresses the problem of decoding barcode-like markers depicted in an image. The main insight is to train a CNN from generated data produced from a GAN. The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background). The parameters for the image transformations are trained such that it confuses a GAN discriminator. A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images. The proposed method out-performs both baselines on decoding the barcode markers. The proposed GAN architecture could potentially be interesting. However, I won\u2019t champion the paper as the evaluation could be improved. A critical missing baseline is a comparison against a generic GAN. Without this it\u2019s hard to judge the benefit of the more structured GAN. Also, it would be worth seeing the result when one combines generated and real images for the final task. A couple of references that are relevant to this work (for object detection using rendered views of 3D shapes): [A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015. [B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016. The problem domain (decoding barcode markers on bees) is limited. It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed. I found the writing to be somewhat vague throughout. For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper. Minor comments: Fig 3 - Are these really renders from a 3D model? The images look like 2D images, perhaps spatially warped via a homography. Page 3: \"chapter\" => \"section\". In Table 2, what is the loss used for the DCNN? Fig 9 (a) - The last four images look like they have strange artifacts. Can you explain these? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your review . Your comments have helped to improve the paper . We think your main point of critique was that we did not evaluate our system sufficiently : > \u201c However , I won \u2019 t champion the paper as the evaluation could be improved . > A critical missing baseline is a comparison against a generic GAN . Without this > it \u2019 s hard to judge the benefit of the more structured GAN . \u201c Since similar comments were made by the other reviewers , we formulated a general response to this point . Please refer to the \u201c general rebuttal \u201d in which we give a detailed rationale of why a generic GAN can not be used as a baseline as you ( and others ) suggested . We think this is a crucial point representing the central motivation of the RenderGAN method . > \u201c Also , it would be worth seeing the result when one combines generated and > real images for the final task. \u201d In previous experiments , we used a mixture of real vs. generated data ( 5/95 and 50/50 ) . The real data was augmented to prevent overfitting as described in Appendix C in the paper . Both experiments did not result in significant improvements , and we therefore decided not to include them in the paper . We agree that this might be interesting and we will update the manuscript accordingly . Thank you for pointing out additional references . We will include both of them in our next update . > \u201c The problem domain ( decoding barcode markers on bees ) is limited . It would > be great to see this applied to another problem domain , e.g. , object detection > from 3D models as shown in paper reference [ A ] , where direct comparison against > prior work could be performed . \u201c We are currently working on more popular problem domains . We , however , feel that , although the problem has limited applicability for the general reader , it nonetheless represents a complex problem with analogies to other , potentially more popular problems , like e.g.pose estimation for cars or faces . > \u201c I found the writing to be somewhat vague throughout . For instance , on first > reading of the introduction it is not clear what exactly is the contribution of > the paper. \u201d Thank you for this feedback . We are currently revising the manuscript to improve overall clarity and will update the paper in the following days . Thank you also for your minor comments , the manuscript will be updated with appropriate changes . > \u201c Fig 3 - Are these really renders from a 3D model ? The images look like 2D > images , perhaps spatially warped via a homography. \u201d Yes , the bee marker is represented by a 3D mesh which is rotated in 3D space according to the parameters and then projected to the image plane . Here is a link to the code ( https : //goo.gl/9UWuhi ) . This is a very simple 3D model and we believe this is one of the advantages of the RenderGAN method . Only a rudimentary object and camera model are required . All the rest of the imaging process , lighting , shadows , blur , reflections , compression artifacts , etc . is learned by the RenderGAN . > \u201c Page 3 : `` chapter '' = > `` section '' . \u201d Changed accordingly . > \u201c In Table 2 , what is the loss used for the DCNN ? \u201d The loss is the mean log-loss for the classification of the individual bits of the barcode marker . We will include this in the caption of the figure in the next revision . > \u201c Fig 9 ( a ) - The last four images look like they have strange artifacts . > Can you explain these ? \u201d We mentioned them briefly in the text ( Section 3 , Training ) : > In some cases , the generator creates artifacts that destroy the images . As > these bad images are scored unrealistic by the discriminator without exception , > we can discard them for the training of the supervised algorithm . We suspect that during training the generator can deceive the discriminator by producing out of distribution samples i.e.very unrealistic images . The discriminator learns quickly to score them as fake , and now the gradient for the generator might be too small to escape this local minimum . However , this is only a hypothesis which we did not investigate further . The images with the strange artifacts are rated unrealistic by the discriminator without exception . Therefore , we can sort them out before training the DCNN ."}, "1": {"review_id": "BkGakb9lx-1", "review_text": "The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. The topic of the paper \u2014 using machine learning (in particular, adversarial training) for generating realistic synthetic training data \u2014 is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets. I appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper. As Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios. The authors should tone down their claims such as \u201cOur method is an improvement over previous work <...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. \u201c. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your review . You summarized our work well , and your feedback contributed significantly to improving our paper . > As Reviewer3 points out , it would be interesting to analyze if restricting GAN > to a fixed set of transformations is necessary here , [ ... ] . Please see our general rebuttal for a detailed explanation why the transformations are necessary and can not be left out . We will revise the manuscript to make this point clearer . > The downside is that experiments are limited to a fairly simple and > not-widely-known domain of honeybee marker classification . We agree , there are more complex datasets e.g.ImageNet.However , our problem , i.e.extraction of the marker \u2019 s ID and rotation in space , was unsolvable with state-of-the-art supervised learning methods due to the time-consuming nature of manual labeling . We are currently working on more popular problem domains . We , however , feel that , although the problem has limited applicability for the general reader , it nonetheless represents a complex problem with analogies to other , potentially more popular problems , like pose estimation for cars or faces . > The authors should tone down their claims such as \u201c Our method is an improvement > over previous work < ... > Whereas previous work relied on real data for training > using pre-trained models or mixing real and generated data , we were able to > train a DCNN from scratch with generated data that performed well when tested on > real data . \u201c .This is not a fair comparison : the domain studied by authors in > this work is much simpler than what was studied in these previous works , so this > comparison is not appropriate . We fully agree and will change the paragraph accordingly ."}, "2": {"review_id": "BkGakb9lx-2", "review_text": "The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture. The main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance. several points were raised during the discussion: 1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks. The answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty. 2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator) The authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty. 3. How do the different stages (\\phis) effect performance? which are the most important ones? The authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps. While there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your review . Your pre-review questions already helped to improve the paper . However , we disagree with several points raised in your review . > The main novelty are parametric modules that emulate different transformations > and artefact that allow to match the natural appearance . [ ... ] the proposed > method is more model driven that previous GAN models . But does it pay off ? how > would a traditional GAN approach perform ? [ ... ] The key proposal of the > submission seems parameterised modules that can be trained to match the real > data distribution . but it remains unclear why not a more generic > parameterisation can also do the job . E.g.a neural network - as done in regular > GANs . We disagree with your summary that our main contribution are augmentation functions that allow a GAN to learn the real data distribution . In fact , the augmentation functions are specifically designed to constrain the generative capabilities of the model . A generic GAN can not be used as a baseline for our method . Please refer to our general rebuttal for a more thorough reply . > Using a render engine to generate the initial sample appearance if of limited novelty . The point of using a render engine is not to generate an initial appearance but to allow collecting a _labeled_ sample that subsequently is rendered more realistic by a constrained GAN . To the best of our knowledge , this has never been done before . > The answers of the authors only partially addresses the point . This is incorrect . We explained in our pre-review reply that a generic GAN can not be used as a baseline . Furthermore , we elaborated on this point in our general rebuttal . Could you please clarify your point if you feel that we still didn \u2019 t address your questions completely ? > The authors reply that plenty of such augmentation was used and more details are > going to be provided in the appendix . it would have been appreciated if such > information was directly included in the revision - so that the procedure could > be directly checked . right now - this remains a point of uncertainty . It was already included in the revision from 9 Dec 2016 which is based on your pre-review feedback ( see appendix C page 14 ) . > The authors do evaluate the effect of hand tuning the transformation stages vs. > learning them . it would be great to also include results of including/excluding > stages completely We agree , excluding different transformation stages would shed light on the relative importance of each stage , and this would be an interesting aspect . We will try to include this in the manuscript by the submission deadline . For now , the differences in performance obtained by hand-designed and learned augmentations can be used as a rough proxy for the importance of the individual augmentations . For example , there is a large improvement in validation performance when replacing the hand-designed lighting augmentation with a learned one . We added a respective remark to the latest revision ( 9 Dec ) . > - and also reporting how much the initial jittering of the data helps . It is not clear to us what you mean with initial jittering of the data . Would you mind clarifying this point ?"}}