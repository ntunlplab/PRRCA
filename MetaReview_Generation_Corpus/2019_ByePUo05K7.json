{"year": "2019", "forum": "ByePUo05K7", "title": "What a difference a pixel makes: An empirical examination of features used by CNNs for categorisation", "decision": "Reject", "meta_review": "This paper claims to demonstrate that CNNs, unlike human vision, do not have a bias towards reliance on shape for object recognition. Both AnonReviewer1 and AnonReviewer2 point to fundamental flaws in the paper's argument, which the rebuttal fails to resolve. (AnonReviewer1's criticisms are unfortunately conflated with AnonReviewer1's reluctance to view neuroscience or biological vision as an appropriate topic for ICLR; nonetheless AnonReviewer1's technical criticism stands).\n\nThese observations are:\n\nAnonReviewer2:\n\n\"Authors have carefully designed a set of experiments which shows CNNs will [overfit] to non-shape features that they added to training images. However, this outcome is not surprising.\"\n\nAnonReviewer1:\n\n\"The experiments don't seem to effectively demonstrate the main claim of the paper that categorization CNNs do not have inductive shape bias\"\n\n\"The best way to demonstrate this would have been to subject a trained image-categorization CNN to test data with object shapes in a way that the appearance information couldn\u2019t be used to predict the object label. The paper doesn\u2019t do this. None of the experiments logically imply that with an unaltered training regime, a trained network would not be predictive of the category label if shapes corresponding to that category are presented.\"\n\nThe AC agrees with both of these observations. CNN behavior is partially a product of the training regime. To examine the scientific question of whether CNNs have similar biases as human vision, the training regimes should be similar. Conversely, if human vision evolved in an environment in which shortcut recognition cues were available via indicator pixels, perhaps it would not have a shape bias.\n\nThis paper appears fundamentally flawed in its approach. The results are not informative about differences between human vision and CNNs, nor are they surprising to machine learning practitioners.", "reviews": [{"review_id": "ByePUo05K7-0", "review_text": "The paper seeks to establish via a series of well-designed experiments that CNNs trained for image classification differ in a fundamental way from human vision \u2013 they don\u2019t encode shape-bias like human vision. Towards this goal, the authors modified the training data with \u2018shortcut\u2019 features to be functions of the category label using single diagnostic pixels and their placements, noise masks (salt and pepper, additive) and their parameters and demonstrate that image categorization CNNs learn whatever statistical features are there in the data most relevant to the learning task. Investigation of the properties of neural architectures like CNNs and using the understanding thus developed to create better neural architectures, learning algorithms and training paradigms are good directions for the community and from that perspective, the direction explored in the paper is of great relevance and interest to the community. The paper presents careful experimentation to establish that image categorization CNNs learn the statistical features most relevant to the learning task. And, it seems to satisfactorily demonstrate this. It shows that such features could be single pixels, noise masks and even parameters of stochastic distributions which randomly produce these features, as long as the parameters are predictive of the image category. The experiments are well designed and they demonstrate this point quite well. They also demonstrate the well-known problem of catastrophic forgetting. Nonetheless, there are significant drawbacks in the presented work: 1. The experiments don't seem to effectively demonstrate the main claim of the paper that categorization CNNs do not have inductive shape bias (encode shape information). (Let\u2019s make this claim more concrete: categorization CNNs when trained via supervised learning with paired training data of {(image, category_label)} do not have inductive shape bias.) The best way to demonstrate this would have been to subject a trained image-categorization CNN to test data with object shapes in a way that the appearance information couldn\u2019t be used to predict the object label. The paper doesn\u2019t do this. None of the experiments logically imply that with an unaltered training regime, a trained network would not be predictive of the category label if shapes corresponding to that category are presented. 2. Due to the surprising results (especially the intensity of observed effects), we tried to reproduce some results from the paper in our lab and faced difficulties in doing so: a. We tried to replicate Figure 4(a) 'nopix' and 'same' cases on a standard setting (VGG-12-BN on CIFAR-10). The results deviated significantly (33%-72% margin) on \u2018nopix\u2019 case from the results reported in the paper on a much stronger setting (1/3072 pixels vs 1/50176 as in the paper). Please let me know any crucial settings (see below) that we might have missed. Details: We used the vgg-cifar10 repository by chengyangfu. The only additions was fixing the pixel values while sending in the data. The code is anonymized and hosted here: https://file.io/qiziAK. The pixel values in CIFAR-10 using the pytorch dataloader are between [-0.45, 0.45] theoretically, typically much smaller. We set the (0,0) RGB pixels categorically spacing it uniformly from [-0.25, 0.25), [-0.025, 0.025), [-0.0025, 0.0025) as a simple experiment. The third case did not suffer any decrease in the nopix case or any increase in the pix at all. The first case showed significant deviations from the claimed results with the no-pix resulting in ~43% accuracy which is 33% off vis-\u00e0-vis the results in the paper. The \u2018same\u2019 setting didn\u2019t achieve 100% either though it got close - achieving 98.4%. Summary: The paper presents an important line of investigation to understand the properties of CNNs. However, it fails to effectively demonstrate its main claim. Further, we had difficulties in reproducing the results. As it stands, the submission is not of publishable quality. I encourage the authors to do more careful experimentation to demonstrate their main claim and perhaps work on strategies to encourage CNNs to learn more meaningful features, including \u2018shape\u2019-features and submit to a future conference. Revision: Updated my rating to acknowledge that the reproducibility issue is addressed.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for some really useful feedback and especially the effort put in to replicating our study . We are pleased that this reviewer found our findings sufficiently surprising that he/she ran a replication study based on Figure 4 ( a ) and provided us a link to determine exactly what was done . We were of course concerned to hear that our findings did not replicate . However , it turns out that this is due to the way in which the dataset has been generated . When generated in the correct manner , our results indeed replicate using the code provided by the reviewer . The other major concern the reviewer had was whether our experiments correctly examine our main claim . We address both these concerns in detail below . [ Replication ] There are two crucial differences between our code and the reviewer \u2019 s in the way the pixel was inserted into any image . Firstly , we inserted the pixel at a different ( x , y ) location for each category . This made the pixel location diagnostic of the category of each image . In contrast , the reviewer inserted the pixel at the same location ( 0,0 ) for all categories , making the location of the pixel non-diagnostic . Secondly , the reviewer assumes that the pixel values in CIFAR-10 using the pytorch dataloader are between [ -0.45 , 0.45 ] . This is not the case in the code provided by the reviewer . Due to normalisation , these values are , in fact , approximately between [ -2.5 , 2.5 ] . Therefore , pixel values used by the reviewer to test the results ( [ -0.25 , 0.25 ] / [ -0.025 , 0.025 ] / [ -0.0025 , 0.0025 ] ) provide a very weak diagnostic signal to the network . To check whether these settings make a difference , we modified the code provided by the reviewer in two ways : ( i ) we picked the ( x , y ) location of the pixel for each category randomly from a uniform distribution [ 0 , 32 ) ( but kept it constant for all images within a category ) , and ( ii ) we picked each of the RGB values for the inserted pixel from a uniform distribution in the range [ -2.0 , 2.0 ) , [ -1.0 , 1.0 ) or [ -0.01 , 0.01 ) . Other than these changes the code provided by the reviewer remains the same ( only changes are around lines 60-70 and then 175-180 ) . When the pixel values were in the interval [ -2.0 , 2.0 ) , the accuracy is ~100 % in the Same condition and between 10-20 % in the NoPix condition , which is very close to what we find . The small remaining difference could be due to a difference in learning algorithm used ( we used RMSProp while the reviewer used SGD ) or due to pretraining ( we used a VGG-16 network pretrained on ImageNet while the reviewer used a VGG-11 network that was trained from scratch on the modified dataset ) . When pixel values are in the range [ -1.0 , 1.0 ) we again get an accuracy of ~100 % in the Same condition , which drops to ~20 % in the NoPix condition . Even when the inserted pixel provides a very weak diagnostic signal , with pixel values nearly at the mean [ -0.01 , 0.01 ) , the network shows a large drop in performance from ~100 % in the Same condition to ~50 % in the NoPix condition , clearly demonstrating the reliance on this diagnostic pixel . Furthermore , it should also be noted that even under the conditions used by the reviewer , where all the categories had the pixel inserted at the same location and pixel values were grayscale and in the small range [ -0.25 , 0.25 ) , performance dropped from 98 % to 42 % when the pixel was removed . It is inconceivable that this will happen for human participants and provides additional support for our observation that the model simply picks up whatever statistical structure is most relevant to learning the training set , with shape playing no special role . We have uploaded the modified code and log files here : http : //s000.tinyupload.com/ ? file_id=24861367338244091333 . However , we do understand that some of these settings may not have been completely obvious in the previous version of the manuscript . To facilitate future replication , we have moved the description of some of these settings from the Appendix to the main text . Furthermore , in order to check if the results replicated on other networks , we have now run the key simulation using ResNet-101 and found that we get a similar pattern of results . We have updated the key figures ( Figure 2 & Figure 6 ) to show these results ."}, {"review_id": "ByePUo05K7-1", "review_text": "Humans leverage shape information to recognize objects. Shape prior information helps human object recognition ability to generalize well to different scenarios. This paper aims to highlight the fact that CNNs will not necessarily learn to recognize objects based on their shape. Authors modified training images by changing a value of a pixel where its location is correlated with object category or by adding noise-like (additive or Salt-and-pepper) masks to training images. Parameters of such noise-like masks are correlated with object category. In other words if one learns noise parameters or location of altered pixel for each object category, they can categorize all images in the training set. This paper shows that CNNs will overfeat to these noise based features and fail to correctly classify images at test time when these noise based features are changed or not added to the test images. Dataset bias is a very important factor in designing a dataset (Torralba et al,. 2011). Consider the case where we have a dataset of birds and cats. The task is image classification. All birds' images have the same background which is different than cats' background. As a result the network that is trained on these images will learn to categorize training images based on their background. Because extracting object based features such as shape of a bird and bird's texture is more difficult than extracting background features which is the same for all training images. Authors have carefully designed a set of experiments which shows CNNs will overfeat to non-shape features that they added to training images. However, this outcome is not surprising. Similar to dataset design example, if you add a noise pattern correlated with object categories to training images, you are adding a significant bias to your dataset. As a result networks that are trained on this dataset will overfeat to these noise patterns. Because it is easier to extract these noise parameters than to extract object based features which are different for each image due to different viewpoints or illumination and so on. This paper would have been a stronger paper if authors had suggested mechanisms or solutions which could have reduced dataset bias or geared CNNs towards extracting shape like features. Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR '11).", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for taking time to read the paper and for their feedback . The reviewer \u2019 s key concern was that he/she did not find our results sufficiently novel or surprising as it is already well established that bias in datasets can lead to incorrect generalisation , as shown by Torralba & Efros ( 2011 ) . We agree that we are not the first to make this point . Our contribution is to highlight the surprising extent to which non-shape features can drive performance , and indeed , even the slightest bias ( even a single pixel ) is enough for CNNs to ignore shape and rely on the diagnostic signal . Also , unlike previous studies , we systematically manipulated the conditions in which non-shape information impacted performance : We varied the type of non-shape noise , we varied the timing at which the noise was introduced ( and found that the non-shape information overwrote shape information through catastrophic interference ) , manipulated the percentage of images in which the noise was embedded , and varied the form of regularization in order to see whether the effects are related to overfitting ( we found no effect ) . Most importantly , we manipulated the degree to which the noise biased the training set ( e.g. , we manipulate the fraction of images containing the noise mask and the variability in the mask from one image to another ) , and showed that CNNs are strongly impacted across all levels of noise ( and types of noise ) . This is important given that all image datasets undoubtedly included uncontrolled noise that is correlated with the output categories ( as pointed by Torralba & Efros ) . Our findings highlight that this may have a larger impact on performance than previously assumed ( and indeed , may help explain how single-pixel attacks can be successful ) . Furthermore , our findings with CNNs contrast with human visual perception where the extraction of shape occurs quickly and automatically and shape holds a privileged status compared to other diagnostic features , such as size , colour or texture . These features may allow humans to overcome biases present in their own environments . As Torralba & Efros point out : \u201c a human learns about vision by living in a reduced environment with many potential local biases and yet the visual system is robust enough to overcome this. \u201d Being biased towards finding shape may be a way in which the visual system overcomes one type of dataset biases ( ones due to non-shape features present within the environment ) , and our results show that this shape-bias is missing from CNNs . We disagree with Reviewer 1 that our findings are due to overfitting . Rather than overfitting , we have shown that CNNs are happy to fit to non-shape data . This is why regularisation methods such as weight-decay , batch normalisation and dropout have no impact on the results ( Figure 3 ) . In light of the reviewer \u2019 s comments , we have revised the Introduction and Discussion to make these issues clear ."}, {"review_id": "ByePUo05K7-2", "review_text": "This paper adds to a growing body of literature which suggests that modern CNNs use qualitatively different visual strategies for object recognition compared to human observers. More specifically, the authors create shapeless object features (by adding noise masks in various forms or single pixels that are predictive of categorization to object images) to study how much CNNs rely on shape information (as humans would) as opposed to shapeless arbitrary statistical dependencies between pixels. The hypotheses tested are straightforward and the experiments cleverly answer these questions. On the negative side, there is nothing groundbreaking in this study. As acknowledged by the authors, the results are not all that novel in light of recent work that has already shown that one could conduct adversarial attacks by corrupting a single pixel as well as work that has shown that CNNs do not generalize to noise degradations they have not seen. Still, there is value in the work presented as the empirical tests described address the role of shape in object recognition with CNNs. In a sense, the present study offers a null result and obviously, the work would have been much more significant had the authors offered a mechanism to get CNNs to learn to prioritize \"shape\" features (then verifying that such network would work on CIFAR, but performed poorly on the shapeless images). Additional analysis involving visualization methods to further explain why shape features were ignored would have been a plus\u2013 with bonus points for providing a heuristic to determine the \"shapelessness\" of a convolution kernel.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for his/her positive comments . We agree with the reviewer that the results may not be ground-breaking as a demonstration of the limitations of CNNs , but as the reviewer correctly identifies , the key novelty of our work lies in contrasting how shape is treated very differently by humans and CNNs . In this sense , our study is more constructive than a typical adversarial attack , as it points in a direction where deep learning research could benefit from understanding the representations and processes underlying human vision . ( Please see the revised Introduction , which highlights this contrast ) . Another contrast with adversarial studies ( such as single pixel attacks ) is that we manually insert non-shape features in the training set , rather than exploring susceptibilities of CNNs trained on well-known datasets . By creating these biases in the dataset , we were able to understand how such biases affect performance and the surprising extent to which non-shape features can drive performance ( even in cases when such non-shape features would be nearly imperceptible to a human being ) . Also , our study is the first to systematically manipulate the conditions in which non-shape information impacted performance : We varied the type of non-shape noise , we varied the timing at which the noise was introduced ( and found that the non-shape information overwrote shape information through catastrophic interference ) , manipulated the percentage of images in which the noise was embedded , and varied the form of regularization in order to see whether the effects are related to overfitting ( we found no effect ) . This is a very different approach than past papers that search for adversarial images , but our findings may help explain why some adversarial images are effective . We completely agree that it is an important question about how to make CNNs focus more on shape ( that is , how to induce a shape bias in CNNs ) . This might help make these models less susceptible to adversarial images , and , make the models more informative about human vision ."}], "0": {"review_id": "ByePUo05K7-0", "review_text": "The paper seeks to establish via a series of well-designed experiments that CNNs trained for image classification differ in a fundamental way from human vision \u2013 they don\u2019t encode shape-bias like human vision. Towards this goal, the authors modified the training data with \u2018shortcut\u2019 features to be functions of the category label using single diagnostic pixels and their placements, noise masks (salt and pepper, additive) and their parameters and demonstrate that image categorization CNNs learn whatever statistical features are there in the data most relevant to the learning task. Investigation of the properties of neural architectures like CNNs and using the understanding thus developed to create better neural architectures, learning algorithms and training paradigms are good directions for the community and from that perspective, the direction explored in the paper is of great relevance and interest to the community. The paper presents careful experimentation to establish that image categorization CNNs learn the statistical features most relevant to the learning task. And, it seems to satisfactorily demonstrate this. It shows that such features could be single pixels, noise masks and even parameters of stochastic distributions which randomly produce these features, as long as the parameters are predictive of the image category. The experiments are well designed and they demonstrate this point quite well. They also demonstrate the well-known problem of catastrophic forgetting. Nonetheless, there are significant drawbacks in the presented work: 1. The experiments don't seem to effectively demonstrate the main claim of the paper that categorization CNNs do not have inductive shape bias (encode shape information). (Let\u2019s make this claim more concrete: categorization CNNs when trained via supervised learning with paired training data of {(image, category_label)} do not have inductive shape bias.) The best way to demonstrate this would have been to subject a trained image-categorization CNN to test data with object shapes in a way that the appearance information couldn\u2019t be used to predict the object label. The paper doesn\u2019t do this. None of the experiments logically imply that with an unaltered training regime, a trained network would not be predictive of the category label if shapes corresponding to that category are presented. 2. Due to the surprising results (especially the intensity of observed effects), we tried to reproduce some results from the paper in our lab and faced difficulties in doing so: a. We tried to replicate Figure 4(a) 'nopix' and 'same' cases on a standard setting (VGG-12-BN on CIFAR-10). The results deviated significantly (33%-72% margin) on \u2018nopix\u2019 case from the results reported in the paper on a much stronger setting (1/3072 pixels vs 1/50176 as in the paper). Please let me know any crucial settings (see below) that we might have missed. Details: We used the vgg-cifar10 repository by chengyangfu. The only additions was fixing the pixel values while sending in the data. The code is anonymized and hosted here: https://file.io/qiziAK. The pixel values in CIFAR-10 using the pytorch dataloader are between [-0.45, 0.45] theoretically, typically much smaller. We set the (0,0) RGB pixels categorically spacing it uniformly from [-0.25, 0.25), [-0.025, 0.025), [-0.0025, 0.0025) as a simple experiment. The third case did not suffer any decrease in the nopix case or any increase in the pix at all. The first case showed significant deviations from the claimed results with the no-pix resulting in ~43% accuracy which is 33% off vis-\u00e0-vis the results in the paper. The \u2018same\u2019 setting didn\u2019t achieve 100% either though it got close - achieving 98.4%. Summary: The paper presents an important line of investigation to understand the properties of CNNs. However, it fails to effectively demonstrate its main claim. Further, we had difficulties in reproducing the results. As it stands, the submission is not of publishable quality. I encourage the authors to do more careful experimentation to demonstrate their main claim and perhaps work on strategies to encourage CNNs to learn more meaningful features, including \u2018shape\u2019-features and submit to a future conference. Revision: Updated my rating to acknowledge that the reproducibility issue is addressed.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for some really useful feedback and especially the effort put in to replicating our study . We are pleased that this reviewer found our findings sufficiently surprising that he/she ran a replication study based on Figure 4 ( a ) and provided us a link to determine exactly what was done . We were of course concerned to hear that our findings did not replicate . However , it turns out that this is due to the way in which the dataset has been generated . When generated in the correct manner , our results indeed replicate using the code provided by the reviewer . The other major concern the reviewer had was whether our experiments correctly examine our main claim . We address both these concerns in detail below . [ Replication ] There are two crucial differences between our code and the reviewer \u2019 s in the way the pixel was inserted into any image . Firstly , we inserted the pixel at a different ( x , y ) location for each category . This made the pixel location diagnostic of the category of each image . In contrast , the reviewer inserted the pixel at the same location ( 0,0 ) for all categories , making the location of the pixel non-diagnostic . Secondly , the reviewer assumes that the pixel values in CIFAR-10 using the pytorch dataloader are between [ -0.45 , 0.45 ] . This is not the case in the code provided by the reviewer . Due to normalisation , these values are , in fact , approximately between [ -2.5 , 2.5 ] . Therefore , pixel values used by the reviewer to test the results ( [ -0.25 , 0.25 ] / [ -0.025 , 0.025 ] / [ -0.0025 , 0.0025 ] ) provide a very weak diagnostic signal to the network . To check whether these settings make a difference , we modified the code provided by the reviewer in two ways : ( i ) we picked the ( x , y ) location of the pixel for each category randomly from a uniform distribution [ 0 , 32 ) ( but kept it constant for all images within a category ) , and ( ii ) we picked each of the RGB values for the inserted pixel from a uniform distribution in the range [ -2.0 , 2.0 ) , [ -1.0 , 1.0 ) or [ -0.01 , 0.01 ) . Other than these changes the code provided by the reviewer remains the same ( only changes are around lines 60-70 and then 175-180 ) . When the pixel values were in the interval [ -2.0 , 2.0 ) , the accuracy is ~100 % in the Same condition and between 10-20 % in the NoPix condition , which is very close to what we find . The small remaining difference could be due to a difference in learning algorithm used ( we used RMSProp while the reviewer used SGD ) or due to pretraining ( we used a VGG-16 network pretrained on ImageNet while the reviewer used a VGG-11 network that was trained from scratch on the modified dataset ) . When pixel values are in the range [ -1.0 , 1.0 ) we again get an accuracy of ~100 % in the Same condition , which drops to ~20 % in the NoPix condition . Even when the inserted pixel provides a very weak diagnostic signal , with pixel values nearly at the mean [ -0.01 , 0.01 ) , the network shows a large drop in performance from ~100 % in the Same condition to ~50 % in the NoPix condition , clearly demonstrating the reliance on this diagnostic pixel . Furthermore , it should also be noted that even under the conditions used by the reviewer , where all the categories had the pixel inserted at the same location and pixel values were grayscale and in the small range [ -0.25 , 0.25 ) , performance dropped from 98 % to 42 % when the pixel was removed . It is inconceivable that this will happen for human participants and provides additional support for our observation that the model simply picks up whatever statistical structure is most relevant to learning the training set , with shape playing no special role . We have uploaded the modified code and log files here : http : //s000.tinyupload.com/ ? file_id=24861367338244091333 . However , we do understand that some of these settings may not have been completely obvious in the previous version of the manuscript . To facilitate future replication , we have moved the description of some of these settings from the Appendix to the main text . Furthermore , in order to check if the results replicated on other networks , we have now run the key simulation using ResNet-101 and found that we get a similar pattern of results . We have updated the key figures ( Figure 2 & Figure 6 ) to show these results ."}, "1": {"review_id": "ByePUo05K7-1", "review_text": "Humans leverage shape information to recognize objects. Shape prior information helps human object recognition ability to generalize well to different scenarios. This paper aims to highlight the fact that CNNs will not necessarily learn to recognize objects based on their shape. Authors modified training images by changing a value of a pixel where its location is correlated with object category or by adding noise-like (additive or Salt-and-pepper) masks to training images. Parameters of such noise-like masks are correlated with object category. In other words if one learns noise parameters or location of altered pixel for each object category, they can categorize all images in the training set. This paper shows that CNNs will overfeat to these noise based features and fail to correctly classify images at test time when these noise based features are changed or not added to the test images. Dataset bias is a very important factor in designing a dataset (Torralba et al,. 2011). Consider the case where we have a dataset of birds and cats. The task is image classification. All birds' images have the same background which is different than cats' background. As a result the network that is trained on these images will learn to categorize training images based on their background. Because extracting object based features such as shape of a bird and bird's texture is more difficult than extracting background features which is the same for all training images. Authors have carefully designed a set of experiments which shows CNNs will overfeat to non-shape features that they added to training images. However, this outcome is not surprising. Similar to dataset design example, if you add a noise pattern correlated with object categories to training images, you are adding a significant bias to your dataset. As a result networks that are trained on this dataset will overfeat to these noise patterns. Because it is easier to extract these noise parameters than to extract object based features which are different for each image due to different viewpoints or illumination and so on. This paper would have been a stronger paper if authors had suggested mechanisms or solutions which could have reduced dataset bias or geared CNNs towards extracting shape like features. Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR '11).", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for taking time to read the paper and for their feedback . The reviewer \u2019 s key concern was that he/she did not find our results sufficiently novel or surprising as it is already well established that bias in datasets can lead to incorrect generalisation , as shown by Torralba & Efros ( 2011 ) . We agree that we are not the first to make this point . Our contribution is to highlight the surprising extent to which non-shape features can drive performance , and indeed , even the slightest bias ( even a single pixel ) is enough for CNNs to ignore shape and rely on the diagnostic signal . Also , unlike previous studies , we systematically manipulated the conditions in which non-shape information impacted performance : We varied the type of non-shape noise , we varied the timing at which the noise was introduced ( and found that the non-shape information overwrote shape information through catastrophic interference ) , manipulated the percentage of images in which the noise was embedded , and varied the form of regularization in order to see whether the effects are related to overfitting ( we found no effect ) . Most importantly , we manipulated the degree to which the noise biased the training set ( e.g. , we manipulate the fraction of images containing the noise mask and the variability in the mask from one image to another ) , and showed that CNNs are strongly impacted across all levels of noise ( and types of noise ) . This is important given that all image datasets undoubtedly included uncontrolled noise that is correlated with the output categories ( as pointed by Torralba & Efros ) . Our findings highlight that this may have a larger impact on performance than previously assumed ( and indeed , may help explain how single-pixel attacks can be successful ) . Furthermore , our findings with CNNs contrast with human visual perception where the extraction of shape occurs quickly and automatically and shape holds a privileged status compared to other diagnostic features , such as size , colour or texture . These features may allow humans to overcome biases present in their own environments . As Torralba & Efros point out : \u201c a human learns about vision by living in a reduced environment with many potential local biases and yet the visual system is robust enough to overcome this. \u201d Being biased towards finding shape may be a way in which the visual system overcomes one type of dataset biases ( ones due to non-shape features present within the environment ) , and our results show that this shape-bias is missing from CNNs . We disagree with Reviewer 1 that our findings are due to overfitting . Rather than overfitting , we have shown that CNNs are happy to fit to non-shape data . This is why regularisation methods such as weight-decay , batch normalisation and dropout have no impact on the results ( Figure 3 ) . In light of the reviewer \u2019 s comments , we have revised the Introduction and Discussion to make these issues clear ."}, "2": {"review_id": "ByePUo05K7-2", "review_text": "This paper adds to a growing body of literature which suggests that modern CNNs use qualitatively different visual strategies for object recognition compared to human observers. More specifically, the authors create shapeless object features (by adding noise masks in various forms or single pixels that are predictive of categorization to object images) to study how much CNNs rely on shape information (as humans would) as opposed to shapeless arbitrary statistical dependencies between pixels. The hypotheses tested are straightforward and the experiments cleverly answer these questions. On the negative side, there is nothing groundbreaking in this study. As acknowledged by the authors, the results are not all that novel in light of recent work that has already shown that one could conduct adversarial attacks by corrupting a single pixel as well as work that has shown that CNNs do not generalize to noise degradations they have not seen. Still, there is value in the work presented as the empirical tests described address the role of shape in object recognition with CNNs. In a sense, the present study offers a null result and obviously, the work would have been much more significant had the authors offered a mechanism to get CNNs to learn to prioritize \"shape\" features (then verifying that such network would work on CIFAR, but performed poorly on the shapeless images). Additional analysis involving visualization methods to further explain why shape features were ignored would have been a plus\u2013 with bonus points for providing a heuristic to determine the \"shapelessness\" of a convolution kernel.", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for his/her positive comments . We agree with the reviewer that the results may not be ground-breaking as a demonstration of the limitations of CNNs , but as the reviewer correctly identifies , the key novelty of our work lies in contrasting how shape is treated very differently by humans and CNNs . In this sense , our study is more constructive than a typical adversarial attack , as it points in a direction where deep learning research could benefit from understanding the representations and processes underlying human vision . ( Please see the revised Introduction , which highlights this contrast ) . Another contrast with adversarial studies ( such as single pixel attacks ) is that we manually insert non-shape features in the training set , rather than exploring susceptibilities of CNNs trained on well-known datasets . By creating these biases in the dataset , we were able to understand how such biases affect performance and the surprising extent to which non-shape features can drive performance ( even in cases when such non-shape features would be nearly imperceptible to a human being ) . Also , our study is the first to systematically manipulate the conditions in which non-shape information impacted performance : We varied the type of non-shape noise , we varied the timing at which the noise was introduced ( and found that the non-shape information overwrote shape information through catastrophic interference ) , manipulated the percentage of images in which the noise was embedded , and varied the form of regularization in order to see whether the effects are related to overfitting ( we found no effect ) . This is a very different approach than past papers that search for adversarial images , but our findings may help explain why some adversarial images are effective . We completely agree that it is an important question about how to make CNNs focus more on shape ( that is , how to induce a shape bias in CNNs ) . This might help make these models less susceptible to adversarial images , and , make the models more informative about human vision ."}}