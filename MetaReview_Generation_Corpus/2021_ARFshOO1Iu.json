{"year": "2021", "forum": "ARFshOO1Iu", "title": "Adaptive Self-training for Neural Sequence Labeling with Few Labels", "decision": "Reject", "meta_review": "This paper introduces a self-training strategy for semi-supervised learning for few shot sequence learning.   It builds on ideas from an existing work on robust deep learning that adaptively reweights examples for learning to reduce impact of noisy examples, here the noisy examples are introduced to the student network training by the teacher network.  Two main novel points, one is on  selectively constructing the validation set used for adaptive reweighting.  Another idea is to move from the sentence level reweighting to token level reweighting.   The paper shows strong results suggesting the proposed method can effectively learn under few-shot learning. \nA primary concern from the reviewers is that the paper has limited novelty given that it primarily applies existing ideas to a slightly different problem.  Another concern is that the system consists of many components, each of the choices could have other viable options.  The ablation studies indicate these components are useful compared to when removed, but fail to explore possible alternative choices. One of the questions is whether token-level reweighting is necessary. It would have been nice to see an ablation study comparing against a baseline using sentence-level reweighting. \n\n", "reviews": [{"review_id": "ARFshOO1Iu-0", "review_text": "SUMMARY The paper presents a series of strategies for self-supervised learning for sequence labeling tasks . The proposed model is a teacher-student network . A teacher model is trained on a small set of labeled data , and then is used to pseudo-annotate a lot of unlabeled data . The student model is trained on the pseudo-labeled data . This paper introduces a strategy to select informative labeled examples to use as dev set for the student model , and adapts an existing re-weighting mechanism for pseudo-labeled examples to the sequence labeling setting . The overall approach is tested on different sequence labeling datasets with different characteristics . In low resources scenarios , the proposed model significantly outperforms previous models . REVIEW Please , make clear that the model is thought for a low resource setting . It would be interesting to see an experiment where the entire dataset is used for training the teacher model and an external unlabeled data is pseudo labeled . Following the experiments in table 5 , this should not work as good as in a low resource . The authors should give a intuition , or an answer on why this is the case . There is an important experiment missing . The impact of adaptive label data acquisition is not tested . What happens if you keep all the examples ? The argument in the introduction about self learning not suitable for sequence learning model is a bit weak . Self learning has been extensively studied and successfully applied in sequence labeling tasks ( for example , https : //arxiv.org/pdf/1804.09530.pdf ) . Why do you need to sum in equation 8 ? do you have same instances and same tokens in different batches ? why do you need that ? In general section 3.2 could be a bit more clear . - Presentation Some sentences are very long and hard to read , i.e. , `` To address such issues stemming from noisy labels and training set biases , learning to re-weight noisy examples ( Ren et al. , 2018 ) lever- ages a meta objective with the basic assumption that the best weighting strategy should minimize the loss on a held-out clean labeled validation set . '' Ren et al.2018 has been published at ICML 2018 , please update the references . -- UPDATE Thanks for the clear and exhaustive response . Minor , regarding the ablation study in A.1 , with S=3 you get the best results , why not try with more ?", "rating": "7: Good paper, accept", "reply_text": "Thanks for your constructive review and we are glad to incorporate your suggestions to improve our paper . Please refer to the general response for the task setup and the key objective . In this paper , we study the problem of sequence labeling with few annotated/labeled examples for each slot . More specifically , we leverage a small set of labeled data and a large amount of in-domain unlabeled data to improve training efficiency for a given task . While prior work in transfer learning and domain shift / adaptation ( e.g.adapting tasks from a high resource source domain to a low resource target domain ) are related to our problem , this is not our focus . Our setting is useful for scenarios where it is difficult to obtain large-scale manual annotations for a task either due to cost or compliance concerns ( e.g. , task-oriented dialog systems with sensitive user information that we can not manually access and annotate ) . To the best of our knowledge , this is the first work to develop a few-shot neural sequence labeling model to obtain significant performance improvements over several state-of-the-art baselines over a wide range of tasks with multiple shots , slots , domains , and languages . We provide detailed responses to other concerns as follows . > _1. \u201c Please , make clear that the model is thought for a low resource setting . It would be interesting to see an experiment where the entire dataset is used for training the teacher model and an external unlabeled data is pseudo labeled . Following the experiments in table 5 , this should not work as good as in a low resource . The authors should give a intuition , or an answer on why this is the case. \u201d _ Thanks for your constructive suggestions . Methods like self-training and meta-learning have been shown to be more useful for tasks with limited labeled data , and the impact/improvement decreases with an increasing amount of labeled examples [ 2 ] . Particularly , for self-training , given the ceiling performance for every task and the improved performance of the teacher with more training labels , there is less room for ( relative ) improvement for the student over the teacher model . Consider the SNIPS dataset for example . Our model obtains 12 % and 2 % improvement over the few-shot BERT model for the 10-shot and 100-shot setting with F1-scores as 88.22 % and 95.39 % , respectively . The ceiling performance for this task is 95.8 % on training BERT on the entire dataset with 13K labeled examples . Please note that there is no additional unlabeled data over that reported in Table 1 . For each task/dataset , we randomly sampled K labeled examples from the corresponding Train set in Table 1 , while treating the remaining as in-domain unlabeled data by disregarding their labels -- following standard setups for semi-supervised learning . We repeatedly sample K labeled instances three times for multiple runs and report average performance with standard deviation across the runs . > 2._ \u201c There is an important experiment missing . The impact of adaptive label data acquisition is not tested . What happens if you keep all the examples ? \u201d _ * * We have reported this important ablation study result in Table 6 * * . We remove the adaptive labeled data acquisition from MetaST ( denoted as \u201c MetaST w/o Labeled Data Acq. \u201d ) that leads to around 2 % performance drop on average . To emphasize the importance of these results , we will add one corresponding analysis in the experimental section for better illustration . For additional discussions regarding adaptive data acquisition , please refer to our corresponding response to Reviewer 2 ."}, {"review_id": "ARFshOO1Iu-1", "review_text": "Summary This paper proposes an adaptive self-training framework , called MetaST , for tackling few-shot sequence labeling tasks . The framework consists of several components : a teacher model that finetunes with the few-shot training data and generates noisy labels for the unlabeled examples ; a student model that learns from re-weighted noisy labels ( at the token level ) , and an iterative process to update the teacher with the trained student . It also uses a meta-learning mechanism to adjust the token-level weights based on a subsampled set of clean data . This subset is sampled based on the student model \u2019 s uncertainty to improve learning efficiency . The proposed system is evaluated on a few sequence tagging tasks for slot filling or named entity recognition . It outperforms previous semi-supervised learning systems across all the evaluated tasks . Strengths - Very solid experimental results on both English and multilingual datasets . The comparison against previous systems ( including ones using BERT , similar to the proposed model ) seems quite thorough . - Ablation studies that showcase the effectiveness of each model component . Weaknesses - The framework is quite complex with many subcomponents : uncertainty-based data acquisition , meta-learning based token-level re-weighting , iterative updates , etc . It is nice that the paper contains a fairly complete ablation study to analyze the effectiveness of each of these components , though . Other questions/comments : - In Algorithm 1 , how do you check convergence ? How many steps does that usually take ? - In the last paragraph of section 4 , \u201c Analysis of pseudo-labeled data re-weighting \u201d , does \u201c step 100 \u201d refer to the step of the student model or the outer loop ( teacher re-initialization ) ?", "rating": "7: Good paper, accept", "reply_text": "Thanks a lot for your constructive feedback and acknowledgement of our extensive experiments and ablation studies . To the best of our knowledge , this is the first work to extensively study few-shot neural sequence labeling with significant improvements over state-of-the-art baselines on several benchmark datasets including multiple shots , slots , domains , and languages . We summarize some additional findings in our experiments complementing those outlined in previous responses . * __The necessity of adaptive data re-weighting__ . In Table 6 , MetaST-Easy sampling strategy outperforms MetaST-Difficult one significantly on CoNLL03 ( EN ) but achieves slightly lower performance on SNIPS . This depicts that sampling strategies may need to vary for different datasets , thereby demonstrating the necessity of adaptive data re-weighting as in our framework MetaST . Similar observation has been noted in prior work [ 1 ] in the context of self-training under domain shift . * __Soft labels v.s . Hard labels.__ In Table 6 , while replacing hard pseudo-labels with soft ones ( denoted as \u201c MetaST w/ Soft Pseudo-Labels \u201d ) , we observe performance drop on SNIPS and CoNLL ( EN ) . This suggests hard or sharpened labels are more informative than soft ones especially with an effective data selection strategy . Similar observation has been noted in prior work [ 2 ] in the context of self-training for image classification . > _1. \u201c In Algorithm 1 , how do you check convergence ? How many steps does that usually take ? \u201d _ In this work , we train for 3000 steps per self-training iteration with 20000 as max-steps overall . Based on our observations , the models converge to stable performance within the max-steps . > _2. \u201d In the last paragraph of section 4 , \u201c Analysis of pseudo-labeled data re-weighting \u201d , does \u201c step 100 \u201d refer to the step of the student model or the outer loop ( teacher re-initialization ) ? \u201d _ Sorry for the confusion . \u201c Step 100 \u201d here refers to the step of the student model . We will make it clear in the revised version . [ 1 ] Sebastian Ruder , Barbara Plank . Strong Baselines for Neural Semi-supervised Learning under Domain Shift . ACL 2018 [ 2 ] Ananya Kumar , Tengyu Ma , Percy Liang . Understanding Self-Training for Gradual Domain Adaptation ICML 2020"}, {"review_id": "ARFshOO1Iu-2", "review_text": "The authors propose adaptive self-training that uses self-training + meta-learning for few-shot training of neural sequence taggers . Specifically the authors focus on reducing noisy training data for student models and reweighting them . ADAPTIVE LABELED DATA ACQUISITION : * The motivation was not clear to me . * The authors proposed a way to select example based on teacher loss , but it 's unclear to me why this was done in this way . I did not find sufficient discussion around this . What happens if we do n't do this ? * Overall , It would 've been nice to see a deeper discussion around whether this is needed in the first place , what benefits does it give , and if so what are all the ways of solving this problem and why the particular approach is the right one . RE-WEIGHTING PSEUDO-LABELED DATA * I could not find sufficient novelty about the `` token '' aspect . What 's special about perturbing weight for each token vs instance . * the entire section was a bit unconvincing , I could not find the motivation for diversity , nor sufficient rigor for the choices made . Experimental Results * It 's nice to see several experiments . * Have the authors considered introducing a pretraining objective on the unlabeled data ? I wonder how much of the `` gap '' from pretrain+finetune would go away if the teacher models were pretrained on the in-domain unlabeled data . * Have the authors compared with an explicit distillation step of a teacher ( E.g.BERT+finetune + distill ) ? I am asking because this might really show whether we need 'reweighting ' , noise reduction etc .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We want to thank the reviewer for thoughtful comments and we would be glad to incorporate your suggestions to improve our paper . Please refer to the general response for the task setup and the key objective . In this paper , we focus on the problem of sequence labeling with few annotated examples ( e.g. , K=10 ) for each slot for each task/dataset . This few shot setting is particularly challenging for structured prediction tasks , where self-training has not been effective in the past [ 1 ] . The main challenge is error propagation due to noisy pseudo-labels over iterative training . We address this error propagation/amplification by leveraging meta-learning with adaptive labeled data acquisition and re-weighting pseudo-labeled data as follows . > ADAPTIVE LABELED DATA ACQUISITION : > 1 . _ \u201c Overall , It would 've been nice to see a deeper discussion around whether this is needed in the first place , what benefits does it give , and if so what are all the ways of solving this problem and why the particular approach is the right one. \u201d _ * * Why do we need labeled data acquisition ? * * Given N slots for each task and K labels for each slot , the model has access to K\\ * N labeled examples . The role of labeled data acquisition is to construct an informative held-out validation set from K\\ * N labeled examples for the pseudo-labeled data re-weighting mechanism . This held-out data is necessary for the meta-learning setup . Previous work on using meta-learning for classification tasks [ 2 , 3 ] constructs this validation set by random sampling . In contrast to instance-level classification tasks , sequence labeling tasks require labels for each token in the sequence where the tokens and correspondingly its labels have inter-dependencies . Therefore this validation set generation can benefit from a better exploration strategy instead of random sampling . * * Our labeled data acquisition strategy : * * We leverage uncertainty sampling to select samples that the model is confused about and correspondingly can benefit from knowing their labels ( similar strategies have been used in active learning settings ) . We use stochastic loss decay from the model as a proxy for the model uncertainty to generate validation set on the fly that is used for weight-estimation and re-weighting pseudo labeled data in the next step . We empirically demonstrate its impact via ablation study , where MetaST w/o Labeled Data Acq \u201d in Table 6 demonstrates 2 % average degradation on replacing this component by random sampling in MetaST . * * Other acquisition strategies : * * Data acquisition strategies can be divided into random [ 2,3 ] , easy [ 4 ] and hard example mining [ 5 ] or uncertainty-based methods [ 6 ] . Easy and hard example mining methods work well in different scenarios , where prior works [ 6 , 12 ] show uncertainty-based methods to have better generalizability across diverse settings . There are several approaches to uncertainty estimation including error decay [ 13 , 14 ] , Monte Carlo dropouts [ 12 ] and predictive variance [ 6 ] . > RE-WEIGHTING PSEUDO-LABELED DATA : > 2 . _ \u201c I could not find sufficient novelty about the `` token '' aspect . What 's special about perturbing weight for each token vs instance . The entire section was a bit unconvincing , I could not find the motivation for diversity , nor sufficient rigor for the choices made. \u201d _ * * Token-level vs. instance-level re-weighting : * * The teacher model assigns pseudo-labels to each token in a sequence . Since token-level pseudo-labels are noisy , we want to re-weight them . Instance-level re-weighting assumes the quality of all token-level pseudo-labels in the sequence to be similar which is not valid given the variable performance of the model for different slot types . * * Token-level weight perturbation * * follows a similar principle to that of an instance . However , the key challenge is to construct an informative held-out validation set for the re-weighting procedure . This is where the adaptive labeled data acquisition strategy from the previous step comes into play . This selects the most informative ( given by model uncertainty ) labeled data samples to construct a small held-out validation set . The token weights are estimated based on the loss changes on this validation set ."}], "0": {"review_id": "ARFshOO1Iu-0", "review_text": "SUMMARY The paper presents a series of strategies for self-supervised learning for sequence labeling tasks . The proposed model is a teacher-student network . A teacher model is trained on a small set of labeled data , and then is used to pseudo-annotate a lot of unlabeled data . The student model is trained on the pseudo-labeled data . This paper introduces a strategy to select informative labeled examples to use as dev set for the student model , and adapts an existing re-weighting mechanism for pseudo-labeled examples to the sequence labeling setting . The overall approach is tested on different sequence labeling datasets with different characteristics . In low resources scenarios , the proposed model significantly outperforms previous models . REVIEW Please , make clear that the model is thought for a low resource setting . It would be interesting to see an experiment where the entire dataset is used for training the teacher model and an external unlabeled data is pseudo labeled . Following the experiments in table 5 , this should not work as good as in a low resource . The authors should give a intuition , or an answer on why this is the case . There is an important experiment missing . The impact of adaptive label data acquisition is not tested . What happens if you keep all the examples ? The argument in the introduction about self learning not suitable for sequence learning model is a bit weak . Self learning has been extensively studied and successfully applied in sequence labeling tasks ( for example , https : //arxiv.org/pdf/1804.09530.pdf ) . Why do you need to sum in equation 8 ? do you have same instances and same tokens in different batches ? why do you need that ? In general section 3.2 could be a bit more clear . - Presentation Some sentences are very long and hard to read , i.e. , `` To address such issues stemming from noisy labels and training set biases , learning to re-weight noisy examples ( Ren et al. , 2018 ) lever- ages a meta objective with the basic assumption that the best weighting strategy should minimize the loss on a held-out clean labeled validation set . '' Ren et al.2018 has been published at ICML 2018 , please update the references . -- UPDATE Thanks for the clear and exhaustive response . Minor , regarding the ablation study in A.1 , with S=3 you get the best results , why not try with more ?", "rating": "7: Good paper, accept", "reply_text": "Thanks for your constructive review and we are glad to incorporate your suggestions to improve our paper . Please refer to the general response for the task setup and the key objective . In this paper , we study the problem of sequence labeling with few annotated/labeled examples for each slot . More specifically , we leverage a small set of labeled data and a large amount of in-domain unlabeled data to improve training efficiency for a given task . While prior work in transfer learning and domain shift / adaptation ( e.g.adapting tasks from a high resource source domain to a low resource target domain ) are related to our problem , this is not our focus . Our setting is useful for scenarios where it is difficult to obtain large-scale manual annotations for a task either due to cost or compliance concerns ( e.g. , task-oriented dialog systems with sensitive user information that we can not manually access and annotate ) . To the best of our knowledge , this is the first work to develop a few-shot neural sequence labeling model to obtain significant performance improvements over several state-of-the-art baselines over a wide range of tasks with multiple shots , slots , domains , and languages . We provide detailed responses to other concerns as follows . > _1. \u201c Please , make clear that the model is thought for a low resource setting . It would be interesting to see an experiment where the entire dataset is used for training the teacher model and an external unlabeled data is pseudo labeled . Following the experiments in table 5 , this should not work as good as in a low resource . The authors should give a intuition , or an answer on why this is the case. \u201d _ Thanks for your constructive suggestions . Methods like self-training and meta-learning have been shown to be more useful for tasks with limited labeled data , and the impact/improvement decreases with an increasing amount of labeled examples [ 2 ] . Particularly , for self-training , given the ceiling performance for every task and the improved performance of the teacher with more training labels , there is less room for ( relative ) improvement for the student over the teacher model . Consider the SNIPS dataset for example . Our model obtains 12 % and 2 % improvement over the few-shot BERT model for the 10-shot and 100-shot setting with F1-scores as 88.22 % and 95.39 % , respectively . The ceiling performance for this task is 95.8 % on training BERT on the entire dataset with 13K labeled examples . Please note that there is no additional unlabeled data over that reported in Table 1 . For each task/dataset , we randomly sampled K labeled examples from the corresponding Train set in Table 1 , while treating the remaining as in-domain unlabeled data by disregarding their labels -- following standard setups for semi-supervised learning . We repeatedly sample K labeled instances three times for multiple runs and report average performance with standard deviation across the runs . > 2._ \u201c There is an important experiment missing . The impact of adaptive label data acquisition is not tested . What happens if you keep all the examples ? \u201d _ * * We have reported this important ablation study result in Table 6 * * . We remove the adaptive labeled data acquisition from MetaST ( denoted as \u201c MetaST w/o Labeled Data Acq. \u201d ) that leads to around 2 % performance drop on average . To emphasize the importance of these results , we will add one corresponding analysis in the experimental section for better illustration . For additional discussions regarding adaptive data acquisition , please refer to our corresponding response to Reviewer 2 ."}, "1": {"review_id": "ARFshOO1Iu-1", "review_text": "Summary This paper proposes an adaptive self-training framework , called MetaST , for tackling few-shot sequence labeling tasks . The framework consists of several components : a teacher model that finetunes with the few-shot training data and generates noisy labels for the unlabeled examples ; a student model that learns from re-weighted noisy labels ( at the token level ) , and an iterative process to update the teacher with the trained student . It also uses a meta-learning mechanism to adjust the token-level weights based on a subsampled set of clean data . This subset is sampled based on the student model \u2019 s uncertainty to improve learning efficiency . The proposed system is evaluated on a few sequence tagging tasks for slot filling or named entity recognition . It outperforms previous semi-supervised learning systems across all the evaluated tasks . Strengths - Very solid experimental results on both English and multilingual datasets . The comparison against previous systems ( including ones using BERT , similar to the proposed model ) seems quite thorough . - Ablation studies that showcase the effectiveness of each model component . Weaknesses - The framework is quite complex with many subcomponents : uncertainty-based data acquisition , meta-learning based token-level re-weighting , iterative updates , etc . It is nice that the paper contains a fairly complete ablation study to analyze the effectiveness of each of these components , though . Other questions/comments : - In Algorithm 1 , how do you check convergence ? How many steps does that usually take ? - In the last paragraph of section 4 , \u201c Analysis of pseudo-labeled data re-weighting \u201d , does \u201c step 100 \u201d refer to the step of the student model or the outer loop ( teacher re-initialization ) ?", "rating": "7: Good paper, accept", "reply_text": "Thanks a lot for your constructive feedback and acknowledgement of our extensive experiments and ablation studies . To the best of our knowledge , this is the first work to extensively study few-shot neural sequence labeling with significant improvements over state-of-the-art baselines on several benchmark datasets including multiple shots , slots , domains , and languages . We summarize some additional findings in our experiments complementing those outlined in previous responses . * __The necessity of adaptive data re-weighting__ . In Table 6 , MetaST-Easy sampling strategy outperforms MetaST-Difficult one significantly on CoNLL03 ( EN ) but achieves slightly lower performance on SNIPS . This depicts that sampling strategies may need to vary for different datasets , thereby demonstrating the necessity of adaptive data re-weighting as in our framework MetaST . Similar observation has been noted in prior work [ 1 ] in the context of self-training under domain shift . * __Soft labels v.s . Hard labels.__ In Table 6 , while replacing hard pseudo-labels with soft ones ( denoted as \u201c MetaST w/ Soft Pseudo-Labels \u201d ) , we observe performance drop on SNIPS and CoNLL ( EN ) . This suggests hard or sharpened labels are more informative than soft ones especially with an effective data selection strategy . Similar observation has been noted in prior work [ 2 ] in the context of self-training for image classification . > _1. \u201c In Algorithm 1 , how do you check convergence ? How many steps does that usually take ? \u201d _ In this work , we train for 3000 steps per self-training iteration with 20000 as max-steps overall . Based on our observations , the models converge to stable performance within the max-steps . > _2. \u201d In the last paragraph of section 4 , \u201c Analysis of pseudo-labeled data re-weighting \u201d , does \u201c step 100 \u201d refer to the step of the student model or the outer loop ( teacher re-initialization ) ? \u201d _ Sorry for the confusion . \u201c Step 100 \u201d here refers to the step of the student model . We will make it clear in the revised version . [ 1 ] Sebastian Ruder , Barbara Plank . Strong Baselines for Neural Semi-supervised Learning under Domain Shift . ACL 2018 [ 2 ] Ananya Kumar , Tengyu Ma , Percy Liang . Understanding Self-Training for Gradual Domain Adaptation ICML 2020"}, "2": {"review_id": "ARFshOO1Iu-2", "review_text": "The authors propose adaptive self-training that uses self-training + meta-learning for few-shot training of neural sequence taggers . Specifically the authors focus on reducing noisy training data for student models and reweighting them . ADAPTIVE LABELED DATA ACQUISITION : * The motivation was not clear to me . * The authors proposed a way to select example based on teacher loss , but it 's unclear to me why this was done in this way . I did not find sufficient discussion around this . What happens if we do n't do this ? * Overall , It would 've been nice to see a deeper discussion around whether this is needed in the first place , what benefits does it give , and if so what are all the ways of solving this problem and why the particular approach is the right one . RE-WEIGHTING PSEUDO-LABELED DATA * I could not find sufficient novelty about the `` token '' aspect . What 's special about perturbing weight for each token vs instance . * the entire section was a bit unconvincing , I could not find the motivation for diversity , nor sufficient rigor for the choices made . Experimental Results * It 's nice to see several experiments . * Have the authors considered introducing a pretraining objective on the unlabeled data ? I wonder how much of the `` gap '' from pretrain+finetune would go away if the teacher models were pretrained on the in-domain unlabeled data . * Have the authors compared with an explicit distillation step of a teacher ( E.g.BERT+finetune + distill ) ? I am asking because this might really show whether we need 'reweighting ' , noise reduction etc .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We want to thank the reviewer for thoughtful comments and we would be glad to incorporate your suggestions to improve our paper . Please refer to the general response for the task setup and the key objective . In this paper , we focus on the problem of sequence labeling with few annotated examples ( e.g. , K=10 ) for each slot for each task/dataset . This few shot setting is particularly challenging for structured prediction tasks , where self-training has not been effective in the past [ 1 ] . The main challenge is error propagation due to noisy pseudo-labels over iterative training . We address this error propagation/amplification by leveraging meta-learning with adaptive labeled data acquisition and re-weighting pseudo-labeled data as follows . > ADAPTIVE LABELED DATA ACQUISITION : > 1 . _ \u201c Overall , It would 've been nice to see a deeper discussion around whether this is needed in the first place , what benefits does it give , and if so what are all the ways of solving this problem and why the particular approach is the right one. \u201d _ * * Why do we need labeled data acquisition ? * * Given N slots for each task and K labels for each slot , the model has access to K\\ * N labeled examples . The role of labeled data acquisition is to construct an informative held-out validation set from K\\ * N labeled examples for the pseudo-labeled data re-weighting mechanism . This held-out data is necessary for the meta-learning setup . Previous work on using meta-learning for classification tasks [ 2 , 3 ] constructs this validation set by random sampling . In contrast to instance-level classification tasks , sequence labeling tasks require labels for each token in the sequence where the tokens and correspondingly its labels have inter-dependencies . Therefore this validation set generation can benefit from a better exploration strategy instead of random sampling . * * Our labeled data acquisition strategy : * * We leverage uncertainty sampling to select samples that the model is confused about and correspondingly can benefit from knowing their labels ( similar strategies have been used in active learning settings ) . We use stochastic loss decay from the model as a proxy for the model uncertainty to generate validation set on the fly that is used for weight-estimation and re-weighting pseudo labeled data in the next step . We empirically demonstrate its impact via ablation study , where MetaST w/o Labeled Data Acq \u201d in Table 6 demonstrates 2 % average degradation on replacing this component by random sampling in MetaST . * * Other acquisition strategies : * * Data acquisition strategies can be divided into random [ 2,3 ] , easy [ 4 ] and hard example mining [ 5 ] or uncertainty-based methods [ 6 ] . Easy and hard example mining methods work well in different scenarios , where prior works [ 6 , 12 ] show uncertainty-based methods to have better generalizability across diverse settings . There are several approaches to uncertainty estimation including error decay [ 13 , 14 ] , Monte Carlo dropouts [ 12 ] and predictive variance [ 6 ] . > RE-WEIGHTING PSEUDO-LABELED DATA : > 2 . _ \u201c I could not find sufficient novelty about the `` token '' aspect . What 's special about perturbing weight for each token vs instance . The entire section was a bit unconvincing , I could not find the motivation for diversity , nor sufficient rigor for the choices made. \u201d _ * * Token-level vs. instance-level re-weighting : * * The teacher model assigns pseudo-labels to each token in a sequence . Since token-level pseudo-labels are noisy , we want to re-weight them . Instance-level re-weighting assumes the quality of all token-level pseudo-labels in the sequence to be similar which is not valid given the variable performance of the model for different slot types . * * Token-level weight perturbation * * follows a similar principle to that of an instance . However , the key challenge is to construct an informative held-out validation set for the re-weighting procedure . This is where the adaptive labeled data acquisition strategy from the previous step comes into play . This selects the most informative ( given by model uncertainty ) labeled data samples to construct a small held-out validation set . The token weights are estimated based on the loss changes on this validation set ."}}