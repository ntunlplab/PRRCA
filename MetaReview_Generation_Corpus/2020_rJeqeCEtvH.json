{"year": "2020", "forum": "rJeqeCEtvH", "title": "Semi-Supervised Generative Modeling for Controllable Speech Synthesis", "decision": "Accept (Poster)", "meta_review": "The authors propose to enforce interpretability and controllability on latent variables, like affect and speaking rate, in a speech synthesis model by training in a semi-supervised way, with a small amount of labeled data with the variables of interest labeled.   The idea is sensible and the results are very encouraging, and the authors have addressed the initial concerns brought up by the reviewers.", "reviews": [{"review_id": "rJeqeCEtvH-0", "review_text": "The authors propose to do neural text to speech, conditioned on attributes such as valence and arousal and speech rate. A seq-to-seq network is trained using stochastic gradient variational Bayes. The idea is interesting and new. The method section could be made clearer by giving first some intuition, explaining the formulas in the prose and introducing the terms used. Regarding the crowd sourced MOS: how were the ratings obtained? how many subjects were used for the rating? How were they selected? Was each rater presented with samples from each method? Or was each method assessed by different groups of raters? The experimental setting is a little weak, relying mostly on the Mean Opinion Score. It would be useful to include more evalution, for instance: * run a speech recognition method on the generated speech and measure the error rate * examples could be included in the supplementary (e.g. spectrograms) * For the evaluation of emotional speech to be meaningful, the proposed classifier should be tested, and more detailed given (hyper-parameters, training setting, validation/test splits?, etc). In particular, it would be useful to compare the proposed method for affect classification to an existing (state-of-the-art) methodology. A state-of-the-art method should be ideally be directly used to classify the generated sequences into emotional classes. The authors collected data in studio conditions, as such it is hard to compare. Will the data be released? How much hdata was collected? It seems that what the authors effectively do is condition on discrete emotion classes, not valence and arousal, which are continuous measures of affect.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for their time and comments . For reasons outlined in detail below we do not fully agree with the statement that our initial evaluation was weak or relied primarily on MOS . However to help further improve our evaluation , we have extended the results by adding : 1 ) Automatic speech recognition word accuracy on our generations . 2 ) subjective human evaluation of affect controllability ( see table 1 in the paper ) 3 ) sample spectrograms in addition to the audio examples we previously provided . 4 ) results on an open data set ( libriTTS ) We hope this addresses the reviewer \u2019 s concern about our evaluations and that the reviewer may reconsider their score because of these improvements ."}, {"review_id": "rJeqeCEtvH-1", "review_text": "This paper proposes to use a semi-supervised VAE based text-to-speech (TTS) for expressive speech synthesis. The main contribution of this paper is that it can provide more explicit interpretation for the latent variable with the help of the supervised learning component. The formulations and implementations in the main body are quite high-level and we may not easily understand the technical/implementation details only with the main body but, in other words, the paper is well written to convey their main messages and of course some details are described in the appendix. The experiments show the effectiveness in terms of subjective (MOS) and objective (cepstral distance etc.) with a lot of audio examples on the demo page. My concern for this paper is a lack of reproducibility. The paper uses the in-house data to perform their experiments and the code does not seem to be publically available. Also, the paper misses several detailed information (e.g., detailed configurations of the Wave RNN vocoder, what kind of neural network toolkits and libraries). The high computational cost (\"distributed across 32 Google Cloud TPU chips\") would also make the reproducibility difficult. I also would like to see whether this method can have some experimental comparisons with (Hsu et al., 2018) with their postprocessing to show the distinction in terms of the performance in addition to the functional difference. Comments: - In general, the font size in the figures is too small - Figure 1: it's better to have an explanation of \"CBHG\". People outside the end-to-end TTS community cannot understand it. - Can you also control the noise level as shown in (Hsu et al., 2018) but more explicitly within this framework? Controlling the noise level is quite important for end-to-end TTS, and I think this method can fit this direction because we can easily obtain the noise attribute (supervision) by data simulation or annotate the noise. - Section 2, second paragraph y_{1...t} --> y_{1...k} (?) - equation (6), classification loss: I think this part requires more clarifications in this timing, e.g., by giving an example of classification tasks. - I think it's better to add what kind of (neural) vocoder is used in the main body (not in the appendix) to asses the sound quality for their experiments. ", "rating": "8: Accept", "reply_text": "We thank the reviewer for their feedback . We agree with the reviewer that reproducibility is of vital importance . To help address the reviewers concerns we have : 1 ) Added experiments on an open data-set ( libriTTS ) to the paper . 2 ) Added much more detail about our wave-RRN vocoder 3 ) Added a table summarising all hyper-parameters to aid reproduction . 4 ) Added much more detail to the description of the CHBG text encoder We would also like to point out that very similar models have been successfully reproduced in the past . See here : https : //github.com/NVIDIA/tacotron2 or here : https : //github.com/Rayhane-mamah/Tacotron-2 , Our model requires only a very modest increase in compute resources over these methods . We hope that our changes have helped to assure the reviewer of the reproducibility of our work and that they may revise their score because of this ."}, {"review_id": "rJeqeCEtvH-2", "review_text": "Overview: This paper proposes to use semi-supervised learning to enforce interpretability on latent variables corresponding to properties like affect and speaking rate for text-to-speech synthesis. During training, only a few training items are annotated for these types of properties; for items where these labels are not given, the variables are marginalised out. TTS experiments are performed and the approach is evaluated objectively by training classifiers on top of the synthesised speech and subjectively in terms of mean opinion score. I should note that, although I am a speech researchers, I am not a TTS expert, and my review can be weighed accordingly. Strengths: The proposed approach is interesting. I think it differs from standard semi-supervised training in that at test time we aren't explicitly interested in predicting labels from the semi-supervised labelled classes; rather, we feed in these labels as input to affect the generated model output. I agree that this is a principled way to impart interpretability on latent spaces which are obtained through unsupervised modelling aiming to disentangle properties like affect and speaking rate. Weaknesses: This work misses some essential baselines, specifically a baseline that only makes use of the (small number of) labelled instances. In the experiments, the best performance is achieved when gamma is set very high, which (I think) correspond to the purely supervised case (I might be wrong). Nevertheless, I think a model that uses only the small amount of labelled data (i.e. without semi-supervised learning incorporating unlabelled data) should also be considered. As a minor weakness, the evaluation seems lacking in that human evaluations are only performed on the audio quality, not any of the target properties that are being changed. For affect specifically, it would be helpful to know whether the changes can be perceived by humans. As a second minor weakness, some aspects of the paper's presentation can be improved (see below). Overall assessment: The paper currently does not contain some very relevant baselines, and I therefore assign a \"weak reject\". Questions, suggestions, typos, grammar and style: - p. 1: \"control high level attributes *of of* speech\" - p. 2: It would be more helpful to state the absolute amount of labelled data (since 1% is somewhat meaningless). - p. 2: I am not a TTS expert, but I believe the last of your contributions have already been achieved in other work. - Figure 2: It would be helpful if these figures are vectorised. - p. 4: \"*where* summation would again ...\" - Figure 4: Is there a reason for the gamma=1000 experiment, which performs best in (a), not to be included in (b) to (d)? - Section 5: Table 1 is not references in the text. - Section 5.1: \"P(x|y,z_s,z_u)\" -> \"p(x|y,z_s,z_u)\" - In a number of places, I think the paper meant to cite [1] but instead cited the older Kingma & Welling (2013) paper; for instance before equation (6) (this additional loss did not appear in the original VAE paper). References: [1] https://arxiv.org/abs/1406.5298 Edit: Based on the author's response, I am changing my rating from a 'weak reject' to a 'weak accept'.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for their feedback . To address the reviewers concerns about baselines and subjective metrics we have added to the paper : 1 ) Word-error and MOS results for fully supervised models . Shown in the table below and added to the paper . 2 ) We have added subjective metrics of controllability in addition to classification accuracy . These show that humans are able to perceive the affect control . ( see table 1 in the paper for details ) Size of training set | 27 min ( 1 % ) | 54 min ( 2 % ) | 108 min ( 4 % ) | 135 min ( 5 % ) | 270 min ( 10 % ) | 45 hours ( 100 % ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- MOS \u2502 unintelligible | unintelligible | 3.20 +- 0.13 | 3.52 +- 0.11 | 4.03 +- 0.08 | 4.08 +- 0.09 Word Error Rate % \u2502 91.95 | 96.31 | 19.83 | 7.55 | 5.56 | 4.93 Character Error Rate % \u2502 74.57 | 78.9 | 12.54 | 4.46 | 2.79 | 2.22 We hope that this addresses the reviewer 's main concern and they may reconsider their score because of this . Our experiments show that , the minimum time required to train an intelligible multi-speaker TTS system is certainly greater than 54 minutes . After 270 minutes , MOS improves only slowly with more training data . Since we are able to achieve control with 3 minutes of labels for continuous attributes and 30 minutes for discrete attributes , this strongly motivates our semi-supervised approach in the context of controllability ."}], "0": {"review_id": "rJeqeCEtvH-0", "review_text": "The authors propose to do neural text to speech, conditioned on attributes such as valence and arousal and speech rate. A seq-to-seq network is trained using stochastic gradient variational Bayes. The idea is interesting and new. The method section could be made clearer by giving first some intuition, explaining the formulas in the prose and introducing the terms used. Regarding the crowd sourced MOS: how were the ratings obtained? how many subjects were used for the rating? How were they selected? Was each rater presented with samples from each method? Or was each method assessed by different groups of raters? The experimental setting is a little weak, relying mostly on the Mean Opinion Score. It would be useful to include more evalution, for instance: * run a speech recognition method on the generated speech and measure the error rate * examples could be included in the supplementary (e.g. spectrograms) * For the evaluation of emotional speech to be meaningful, the proposed classifier should be tested, and more detailed given (hyper-parameters, training setting, validation/test splits?, etc). In particular, it would be useful to compare the proposed method for affect classification to an existing (state-of-the-art) methodology. A state-of-the-art method should be ideally be directly used to classify the generated sequences into emotional classes. The authors collected data in studio conditions, as such it is hard to compare. Will the data be released? How much hdata was collected? It seems that what the authors effectively do is condition on discrete emotion classes, not valence and arousal, which are continuous measures of affect.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for their time and comments . For reasons outlined in detail below we do not fully agree with the statement that our initial evaluation was weak or relied primarily on MOS . However to help further improve our evaluation , we have extended the results by adding : 1 ) Automatic speech recognition word accuracy on our generations . 2 ) subjective human evaluation of affect controllability ( see table 1 in the paper ) 3 ) sample spectrograms in addition to the audio examples we previously provided . 4 ) results on an open data set ( libriTTS ) We hope this addresses the reviewer \u2019 s concern about our evaluations and that the reviewer may reconsider their score because of these improvements ."}, "1": {"review_id": "rJeqeCEtvH-1", "review_text": "This paper proposes to use a semi-supervised VAE based text-to-speech (TTS) for expressive speech synthesis. The main contribution of this paper is that it can provide more explicit interpretation for the latent variable with the help of the supervised learning component. The formulations and implementations in the main body are quite high-level and we may not easily understand the technical/implementation details only with the main body but, in other words, the paper is well written to convey their main messages and of course some details are described in the appendix. The experiments show the effectiveness in terms of subjective (MOS) and objective (cepstral distance etc.) with a lot of audio examples on the demo page. My concern for this paper is a lack of reproducibility. The paper uses the in-house data to perform their experiments and the code does not seem to be publically available. Also, the paper misses several detailed information (e.g., detailed configurations of the Wave RNN vocoder, what kind of neural network toolkits and libraries). The high computational cost (\"distributed across 32 Google Cloud TPU chips\") would also make the reproducibility difficult. I also would like to see whether this method can have some experimental comparisons with (Hsu et al., 2018) with their postprocessing to show the distinction in terms of the performance in addition to the functional difference. Comments: - In general, the font size in the figures is too small - Figure 1: it's better to have an explanation of \"CBHG\". People outside the end-to-end TTS community cannot understand it. - Can you also control the noise level as shown in (Hsu et al., 2018) but more explicitly within this framework? Controlling the noise level is quite important for end-to-end TTS, and I think this method can fit this direction because we can easily obtain the noise attribute (supervision) by data simulation or annotate the noise. - Section 2, second paragraph y_{1...t} --> y_{1...k} (?) - equation (6), classification loss: I think this part requires more clarifications in this timing, e.g., by giving an example of classification tasks. - I think it's better to add what kind of (neural) vocoder is used in the main body (not in the appendix) to asses the sound quality for their experiments. ", "rating": "8: Accept", "reply_text": "We thank the reviewer for their feedback . We agree with the reviewer that reproducibility is of vital importance . To help address the reviewers concerns we have : 1 ) Added experiments on an open data-set ( libriTTS ) to the paper . 2 ) Added much more detail about our wave-RRN vocoder 3 ) Added a table summarising all hyper-parameters to aid reproduction . 4 ) Added much more detail to the description of the CHBG text encoder We would also like to point out that very similar models have been successfully reproduced in the past . See here : https : //github.com/NVIDIA/tacotron2 or here : https : //github.com/Rayhane-mamah/Tacotron-2 , Our model requires only a very modest increase in compute resources over these methods . We hope that our changes have helped to assure the reviewer of the reproducibility of our work and that they may revise their score because of this ."}, "2": {"review_id": "rJeqeCEtvH-2", "review_text": "Overview: This paper proposes to use semi-supervised learning to enforce interpretability on latent variables corresponding to properties like affect and speaking rate for text-to-speech synthesis. During training, only a few training items are annotated for these types of properties; for items where these labels are not given, the variables are marginalised out. TTS experiments are performed and the approach is evaluated objectively by training classifiers on top of the synthesised speech and subjectively in terms of mean opinion score. I should note that, although I am a speech researchers, I am not a TTS expert, and my review can be weighed accordingly. Strengths: The proposed approach is interesting. I think it differs from standard semi-supervised training in that at test time we aren't explicitly interested in predicting labels from the semi-supervised labelled classes; rather, we feed in these labels as input to affect the generated model output. I agree that this is a principled way to impart interpretability on latent spaces which are obtained through unsupervised modelling aiming to disentangle properties like affect and speaking rate. Weaknesses: This work misses some essential baselines, specifically a baseline that only makes use of the (small number of) labelled instances. In the experiments, the best performance is achieved when gamma is set very high, which (I think) correspond to the purely supervised case (I might be wrong). Nevertheless, I think a model that uses only the small amount of labelled data (i.e. without semi-supervised learning incorporating unlabelled data) should also be considered. As a minor weakness, the evaluation seems lacking in that human evaluations are only performed on the audio quality, not any of the target properties that are being changed. For affect specifically, it would be helpful to know whether the changes can be perceived by humans. As a second minor weakness, some aspects of the paper's presentation can be improved (see below). Overall assessment: The paper currently does not contain some very relevant baselines, and I therefore assign a \"weak reject\". Questions, suggestions, typos, grammar and style: - p. 1: \"control high level attributes *of of* speech\" - p. 2: It would be more helpful to state the absolute amount of labelled data (since 1% is somewhat meaningless). - p. 2: I am not a TTS expert, but I believe the last of your contributions have already been achieved in other work. - Figure 2: It would be helpful if these figures are vectorised. - p. 4: \"*where* summation would again ...\" - Figure 4: Is there a reason for the gamma=1000 experiment, which performs best in (a), not to be included in (b) to (d)? - Section 5: Table 1 is not references in the text. - Section 5.1: \"P(x|y,z_s,z_u)\" -> \"p(x|y,z_s,z_u)\" - In a number of places, I think the paper meant to cite [1] but instead cited the older Kingma & Welling (2013) paper; for instance before equation (6) (this additional loss did not appear in the original VAE paper). References: [1] https://arxiv.org/abs/1406.5298 Edit: Based on the author's response, I am changing my rating from a 'weak reject' to a 'weak accept'.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for their feedback . To address the reviewers concerns about baselines and subjective metrics we have added to the paper : 1 ) Word-error and MOS results for fully supervised models . Shown in the table below and added to the paper . 2 ) We have added subjective metrics of controllability in addition to classification accuracy . These show that humans are able to perceive the affect control . ( see table 1 in the paper for details ) Size of training set | 27 min ( 1 % ) | 54 min ( 2 % ) | 108 min ( 4 % ) | 135 min ( 5 % ) | 270 min ( 10 % ) | 45 hours ( 100 % ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- MOS \u2502 unintelligible | unintelligible | 3.20 +- 0.13 | 3.52 +- 0.11 | 4.03 +- 0.08 | 4.08 +- 0.09 Word Error Rate % \u2502 91.95 | 96.31 | 19.83 | 7.55 | 5.56 | 4.93 Character Error Rate % \u2502 74.57 | 78.9 | 12.54 | 4.46 | 2.79 | 2.22 We hope that this addresses the reviewer 's main concern and they may reconsider their score because of this . Our experiments show that , the minimum time required to train an intelligible multi-speaker TTS system is certainly greater than 54 minutes . After 270 minutes , MOS improves only slowly with more training data . Since we are able to achieve control with 3 minutes of labels for continuous attributes and 30 minutes for discrete attributes , this strongly motivates our semi-supervised approach in the context of controllability ."}}