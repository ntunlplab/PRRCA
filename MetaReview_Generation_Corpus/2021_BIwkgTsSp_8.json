{"year": "2021", "forum": "BIwkgTsSp_8", "title": "Learning to Noise: Application-Agnostic Data Sharing with Local Differential Privacy", "decision": "Reject", "meta_review": "The paper considers the problem of private data sharing under local differential privacy. \n\n(1) it assumes having access to a public unlabeled dataset for learning a VAE, so it reduces the dimensionality in a more meaningful way than simply running PCA. (2) the LDP guarantee is coming from the standard Laplace mechanism and Randomized Responses. (3) then the authors propose how to learn a model based on the privately released (encoded) data which exploits the knowledge of the noise distribution.\n\nNone of these components are new as far as I know, nor were they new in the context of differential privacy. For example, the use of a publicly available data for DP was considered in: \n\n- Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure\nvs. approximate differential privacy. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 363\u2013378. Springer, 2013.\n\n(they called it Semi-Private Learning...)\n\n- Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., & Talwar, K. (2017). Semi-supervised knowledge transfer for deep learning from private training data. In ICLR-17.\n\nThe idea of integrating out the noise by leveraging the known noise structure were considered in:\n\n- Williams, O., & McSherry, F. (2010). Probabilistic inference and differential privacy. Advances in Neural Information Processing Systems, 23, 2451-2459.\n\n- Balle, B., & Wang, Y. X. (2018). Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising. In International Conference on Machine Learning (pp. 394-403).\n\nAnd many subsequent work.\n\nThe contribution of this work is in combining these known pieces (without citing some of the earlier work) to achieve a reasonably strong set of experimental results (for LDP standard).  I believe this is the first experimental study that uses VAE for the dimension reduction, however, this alone is not sufficient to carry the paper in my opinion; especially since the setting is now much easier, with access to a public dataset.\n\nThe reviewers question the experiments are baselines are usually not using a public dataset as well as the practicality of the proposed method.   Also, connections to some of the existing work on private data release (a.k.a., private synthetic data generation) were note clarified. For these reasons, there were not sufficient support among the reviewers to push the paper through. \n\nThe authors are encouraged to revise the paper according to the suggestions and resubmit in the next appropriate venue.", "reviews": [{"review_id": "BIwkgTsSp_8-0", "review_text": "Summary : This paper presents a new privatization mechanism for Local Differential Privacy based on representation learning . The proposed VAE-based method is used for the low-dimensional latent representation of the data and uses the Laplace mechanism to satisfy Local DP . The paper shows this mechanism can be used across various applications such as private data collection , private novel-class classification , data joining , etc . The paper is clear and easy to follow . The proposed method provides a great solution for data sharing with the local DP and can be used in many real-world applications . However , some clarification is needed . The experimental results can be improved by adding more baselines . Some important/recent references are also missing . Major Comments : - It would be better if the authors add a related work section or extend the literature review paragraph in Introduction and include more recent work in this area and point out how the proposed work advances the state-of-the-art . There are several works on DP for high-dimensional data such as : AutoGAN-based Dimension Reduction for Privacy Preservation by Nguyena et al.P3GM : Private High-Dimensional Data Release via Privacy Preserving Phased Generative Model by Takagi et al.There are also several existing works on LDP based on VAE . The authors are expected to state the differences between the existing work and the proposed work . - The authors mentioned the DP Synthetic data models need large data , this is also the case for training the VAE in this work . Also , they mentioned these techniques need labeled data . DP-GAN models need access to real data for training ( no label is required ) and then it can be used indefinitely for generating synthetic data . Please clarify this . - In an existing work ( P3GM paper mentioned above ) , it is shown that VAE \u2019 s objective function is too sensitive to the noise of DP-SGD , how the authors tackle this problem ? And how it affects the final results ? - The baseline methods are limited to `` direct noise features '' only . It would be better if the authors use other techniques such as some recent work on LDP on high dimensional data or other general DP classifiers , DP-SGD , PATE , etc . as the baselines for this experiment . - It would be better if the authors also showed the performance of the proposed method on higher dimensional data ( e.g. , Lending club has only 23 features ) Minor comments : - In the introduction , please list the contribution of this work . - Please define all variables in Eq 5-6 . - Please add more up-to-date references .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their response , and have addressed each of their points in order below . * * Major Point 1 : * * We have updated the introduction to include a more in depth discussion of the related work , including the papers cited in this review . * * Major Point 2 : * * Thank you for pointing this out , we have updated this literature review to reflect the fact that not all work in synthetic generation requires labels for training , and more importantly to outline how our work differs from existing work in the literature . In short , our method is solving a very different problem to the work on synthetic generation , and we believe synthetic generation to be limited in its applications . As mentioned in our response to AnonReviewer1 point 4 , we learn an LDP mechanism for privatizing individual datapoints or subsets of features , for use in downstream tasks ; this opens up possibilities to solve a much broader range of problems . Synthetic generation approaches create datasets that are not related to individuals , but are simply likely under the learnt probability distribution . One has no way of collecting / privatizing new data with these approaches , as is required for the tasks in Sections 4.1 , 4.2 or 5 . * * Major Point 3 : * * In P3GM , it is suggested that training a VAE end to end with DP-SGD is challenging , and this is something we also found . We found that the simple 2-stage approach outlined at the end of Section 3.1 was sufficient for learning good representations . P3GM also adopts a technique whereby the training of the VAE is split into steps , however their approach seems more complicated than ours . It should be stated that we are attempting to solve a completely different problem to P3GM as described in our rebuttal to Major Point 2 above . * * Major Point 4 : * * As discussed in our response to AnonReviewer1 point 4 , we do not see an obvious way to benchmark against DP classifiers due the nature of the tasks solved by our model in this paper . The literature on local differential privacy for high dimensional data is still in its infancy , and much of the existing work is tailored to specific datasets rather than a general model such as ours . For example [ 1 , 2 ] focus on the collection of data over time , whilst [ 2 ] note that for one time collection \u201c direct randomization on the true client \u2019 s value is sufficient to provide strong privacy protection \u201d . Indeed this direct randomization approach is in line with our benchmark . These papers have been discussed in the literature review within Section 1 . * * Major Point 5 : * * We tested our algorithm on MNIST , which contains 784 features , and do not consider this to be a low-dimensional problem . Even if , in the context of SOTA non-DP computer vision work , one might categorise MNIST as low dimensional , this is certainly not the case in the context of LDP research . By applying the algorithm to both MNIST and Lending Club , we demonstrate the versatility of the approach . It should theoretically work on any data type for which you can train a VAE to learn good representations . This allows us to make use of a vast body of work that has already been achieved in the generative modelling community . * * Minor Points : * * All three minor points have been addressed in the updated manuscript . We hope that these clarifications as well as the updates to our manuscript have addressed the reviewer 's comments , and that the reviewer will consider revising their score accordingly . [ 1 ] Ding et al.- Collecting telemetry data privately , 2017 [ 2 ] Erlingsson et al.- RAPPOR : Randomized aggregatable privacy-preservingordinal response , 2014 ."}, {"review_id": "BIwkgTsSp_8-1", "review_text": "For LDP , when applying noise directly to high-dimensional data , the required noise entirely destroys data utility . In this paper , authors introduce a novel , application-agnostic privatization mechanism that leverages representation learning to overcome the prohibitive noise requirements of direct methods . They further demonstrate that this privatization mechanism can be used to train machine learning algorithms across a range of applications . They achieve significant gains in performance for high-dimensional data . In this paper , authors have benchmarked results against such a mechanism , in which add Laplace noise to all continuous features , and flip each of the categorical features with some probability . For high-dimensional datasets , features are often highly correlated ; consequently , noising features independently is wasteful towards privatizing the information content in each datapoint . A more effective approach to privatization involves noising a learned lower-dimensional representation of each datapoint using a generic noising mechanism . Applying the Laplace mechanism thus ensures the encoded latents , as well as reconstructed datapoints satisfy LDP . They focus on classification tasks . At inference time , they show that this classififier can act on either clean or privatized datapoints , . For the writing , it \u2019 s better to give a clear algorithm . For the experiment , when epsilon < =10 , the accuracy is not very good . The related work and comparisons are not enough . They are quite a few work about LDP learning can be literature reviewed . By the way , we usually use private data rather than privatized data .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the valuable feedback ; we hope that points below adequately address your concerns . Re : \u201c it \u2019 s better to give a clear algorithm \u201d AnonReviewer3 also sought clarification in this regard . We agree this could have been clearer and have significantly edited Section 3.1 to clarify the privatization procedure . Please let us know if you still believe it is unclear . Re : \u201c the accuracy is not very good \u201d LDP is an extremely strict criterion , and training models under LDP involves a privacy-utility trade off . Equation 9 and Figure 4 outline a strict upper bound on what the model can achieve , and demonstrate how one can not directly compare an LDP model against a classifier trained without privacy guarantees . Even in comparison to models trained with CDP , such as those from ( Abadi et al.2016 ) , the LDP criterion is incredibly strict , as it protects the privacy of individual datapoints . Very little work has been done on LDP mechanisms for high dimensional data . Many applications ( including those that were studied in our paper ) are impossible without LDP . Therefore , our results should be compared against the best technique available to the application , rather than the standard techniques that we are used to in less private applications . Re : \u201c The related work and comparisons are not enough \u201d We have extended our literature review to give a greater overview of this research in this field ."}, {"review_id": "BIwkgTsSp_8-2", "review_text": "In this paper , the authors present a generative-model-based Laplace mechanism . By training the VAE on some dataset , the trained encoder can be used to privatize raw data towards epsilon , delta-LDP . Though the method is novel , the privacy guarantee of the proposed method is not clearly stated and proved . Related experiments are not convincing , either . * * Strength * * The paper is well written with a clear motivation , explanation of methodology . To my knowledge , I believe the work is useful for the privacy research community . The proposed method is also novel . * * Weakness * * - The motivation to use the Laplace mechanism is not very clear . At the beginning of Sec.2 , the authors reason the usage by `` as it provides strong theoretical privacy guarantees '' . This is not convincing for readers especially for those who are not familiar with LDP . Since the Laplace mechanism directly comes from the CDP , I would wonder how does the Gaussian mechanism works . How does the Laplace mechanism guarantee privacy better than the Gaussian mechanism ? Reference or proof is essential here . - In page 3 , the authors briefly mention that the local version of the Laplace mechanism can be epsilon-LDP if the sensitivity is accordingly defined . This really lacks rigorousness . In the following sections , the authors refer to ( Dwork and Roth , 2014 ) for the post-processing theorem . Since the work ( Dwork and Roth , 2014 ) is mainly about CDP , I am not sure how the post-processing theorem can be adopted for LDP . Either reference or clear proof is required . - Meanwhile , there lacks an end-to-end proof of the privacy guarantee of the VAE . I am not sure if the proposed VLM training guarantees privacy . Either , the privacy of encoding is not very clear . Especially , there involves a non-private training on stage 1 . - The experiments are run with pretty week baselines . Through this paper , the authors actively use the same conclusion from CDP ( Dwork and Roth , 2014 ) . Thus , I suppose the state-of-the-art CDP algorithms should also be applicable to the experimented tasks , e.g , classification . For the specific task , how well is the proposed compared to the SOTA CDP private learning algorithms ? For example , ( Abadi , et al. , 2016 ) , or ( Phan , et al.2017 ) .Especially , ( Phan , et al.2017 ) also proposed an adaptive Laplace mechanism without depending on pre-training of the mechanism . - In page 4 , the DP-Adam mentioned in Stage 2 is not stated or proved in ( Abadi et al. , 2016 ) . Only DP-SGD was discussed . A strict proof is required for the DP-Adam which intensively re-uses private results to help improve the gradients . Thus , the privacy guarantee is not straightforward . - Seems the VLM training is using a non-DP optimizer at stage 1 . Then how the whole training could guarantee privacy on the VLM training set . In experiments , the VLM training set is directly extracted from the private dataset ( MNIST ) . Even though the author experiments with diverse D_1 D_2 distribution for VLM train/test in Sec 4.2 , the two datasets are still from the same dataset . In practice , when such a D_2 is private , it is hard to find a D_1 to be non-private . I am afraid this could cause serious privacy leakage . Therefore , I doubt if the experimental results are useful for proving the effectiveness of a private algorithm . More realistic dates should be used . - In Sec 4.1 , the authors run the experiments in two steps . First , the VLM is trained with ' a DP encoder using D_1 ' . I am not clear how the DP encoder comes from . Does the VLM is also trained with DP ? The setting has to be clarified . - The experiment comparison seems not fair for baselines . For VLM , there are two datasets for training VLM and encoding classification train data . However , the baseline only has classification training data . The VLM encoder has additional information about the data distribution or the noise ( by back-propagation in VLM training ) . The unfairness in the information could be the core reason for the difference in performance . How does the baseline perform if it is pre-trained and tuned ( hyper-parameters ) on another dataset ? ( Phan , et al.2017 ) .Adaptive Laplace Mechanism : Differential Privacy Preservation in Deep Learning", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for their detailed feedback . Point-by-point responses are provided below : * * Points 1 & 2 : * * We chose the Local Laplace mechanism as it guarantees ( epsilon , delta=0 ) -LDP , while the Gaussian mechanism guarantees ( epsilon , delta > 0 ) -LDP which is not as strict . We believe that the method should work using the Gaussian mechanism ; this would require a minimal change to the model ( changing the prior and approximate posterior to be Gaussian ) . A paragraph has been added to Section 2 to discuss the choice of Laplace versus Gaussian mechanisms . We have also added a formal definition of the local Laplace mechanism in Section 2 , and a proof that this guarantees LDP in Appendix A . We have added a proof that LDP is immune to post-processing in Appendix B . * * Points 3 & 7 : * * There are two separate types of data in our approach : data which is transferred between entities and data which is only used for training the VLM . The latter must satisfy CDP so that the VLM parameters can be shared , while the former must satisfy LDP so that it can be safely shared . Because these two requirements are completely independent , keeping track of privacy is relatively straightforward : * DP training ( e.g.DP-Adam ) must be used when training the parameters of the VLM that are to be shared . This is done in stage 2 of VLM training as outlined in Section 3.1 . * LDP is required for transferring the data . This is achieved when the VLM adds noise in the latent space . With this clarification of the two-stage approach , and the proofs that have now been added to Appendix A and B , the LDP and CDP guarantees are proved . * * Point 4 : * * Our approach aims to tackle problems in which data must be privately shared between entities , thus requiring a LDP mechanism . The experiments in our paper require data sharing , and thus CDP approaches such as those referenced , can not be used to tackle this problem . Consequently , they can not be applied as benchmarks in our work . * * Point 5 : * * Thank you for pointing this out . We had missed a citation for [ Gylberth et al.- Differentially Private Optimization Algorithms for Deep Neural Networks ] who show that DP-Adam satisfies CDP . * * Point 6 : * * With respect to privacy leakage , we have clarified this concern in our response to points 3 & 7 above . The reviewer also comments on the relevance of our data set choices . We describe in Sections 4.1-4.3 and Section 5 multiple different scenarios in which the dataset assumptions we have made are highly relevant . * * Point 8 : * * We agree that with access to a similar , pre-training dataset $ D_1 $ , we are at an advantage over our baseline , and this is the motivation for the paper . We are demonstrating that , with some knowledge about the structure of our data distribution , one can learn a mechanism to privatise the data which is more effective than a fixed mechanism . This setup is a common scenario for organisations looking to privately exploit data , as we have articulated in our applications . In summary , we have made significant improvements to the manuscript in response to some of the reviewer 's comments ( e.g.extensive detail added to address the privacy guarantees ) , and hopefully provided helpful clarification in cases where perhaps the reviewer misunderstood our work ( e.g.the applicability of central DP ) . With this , we hope the reviewer will reconsider their score ."}, {"review_id": "BIwkgTsSp_8-3", "review_text": "Strong point 1 : The idea of putting noise insertion ( via noisy data-generation models ) and optimization of good representations together to obtain LDP representations and/or synthetic data seems to be effective . While ( 6 ) relies on some independency assumptions , it might be fine in most cases and empirical evidence is reported to support it Strong point 2 : It is an application-agnostic approach and theoretically any downstream tasks and models can be supported ... When there is a label , the privacy budget is split and random perturbation is used on labels Strong point 3 : It outperforms naive LDP baselines ( with noise added directly to features ) a lot in experiments Weak point 1 : The proof of the most important result is missing : It is said that `` sampling from $ q_\\phi ( z|x ) $ produces a representation $ \\tilde z $ of $ x $ that satisfies $ \\epsilon $ -LDP . I do n't think it is a trivial result and the author needs to everything together ( including the analysis of sensitivity , the optimization algorithm , and so on ) to formally prove it Weak point 2 : A minor issue : in figures of experiments , by `` clean accuracy '' , do you actually mean `` accuracy '' ( for some algorithms in the figures , it is privacy accuracy ? ) W1 is the main reason for the rating of 6 but not higher ones - highly encourage the authors to fix it before the publication", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their valuable feedback , and respond to their comments below . * * Weak point 1 : * * The reviewers primary concern was the lack of explicit detail around proving that our approach satisfies the requirements of LDP . This was an oversight on our part , and we have now added multiple appendices with proofs as well as commentary within the body of the paper to further clarify . In particular , we have added an explicit definition for the local Laplace mechanism ( see Equation 5 ) , as well as the accompanying proof that the local Laplace mechanism satisfies epsilon-LDP in Appendix A ( the proof that LDP is immune to post-processing is additionally provided in Appendix B ) . We have also updated Section 3.1 to clarify that sampling from $ q_\\phi ( z|x ) $ is equivalent to a Laplace mechanism $ \\mathcal { M } ^\\text { ( local ) } \\left ( x , \\mu_ { \\phi } ( \\cdot ) , \\epsilon_x \\right ) $ , since sampling from $ q_\\phi ( z|x ) $ involves passing a datapoint through the mean function $ \\mu_\\phi ( x ) $ and adding laplace noise of scale $ b = \\Delta\\mu_\\phi / \\epsilon_x $ , where $ \\Delta\\mu_\\phi $ is the sensitivity of $ \\mu_\\phi ( . ) $ , as determined by the clipping function . We now have all formal results in the paper needed to fully address the reviewers main concern , and hope that consequently the reviewer will consider raising their score . * * Weak point 2 : * * For clarification , clean accuracy refers to the accuracy of our classifier when applied to a clean datapoint at inference time , while private accuracy refers to the accuracy of our classifier when applied to an LDP datapoint . In every case training is done on LDP data . In order to clarify these definitions to readers , the second paragraph of Section 4 has been updated significantly . With the proofs of privacy added and clarifications of the mechanism made in the text , we feel that we have addressed the primary concern of the reviewer . We hope they agree and will consider revising their score accordingly ."}], "0": {"review_id": "BIwkgTsSp_8-0", "review_text": "Summary : This paper presents a new privatization mechanism for Local Differential Privacy based on representation learning . The proposed VAE-based method is used for the low-dimensional latent representation of the data and uses the Laplace mechanism to satisfy Local DP . The paper shows this mechanism can be used across various applications such as private data collection , private novel-class classification , data joining , etc . The paper is clear and easy to follow . The proposed method provides a great solution for data sharing with the local DP and can be used in many real-world applications . However , some clarification is needed . The experimental results can be improved by adding more baselines . Some important/recent references are also missing . Major Comments : - It would be better if the authors add a related work section or extend the literature review paragraph in Introduction and include more recent work in this area and point out how the proposed work advances the state-of-the-art . There are several works on DP for high-dimensional data such as : AutoGAN-based Dimension Reduction for Privacy Preservation by Nguyena et al.P3GM : Private High-Dimensional Data Release via Privacy Preserving Phased Generative Model by Takagi et al.There are also several existing works on LDP based on VAE . The authors are expected to state the differences between the existing work and the proposed work . - The authors mentioned the DP Synthetic data models need large data , this is also the case for training the VAE in this work . Also , they mentioned these techniques need labeled data . DP-GAN models need access to real data for training ( no label is required ) and then it can be used indefinitely for generating synthetic data . Please clarify this . - In an existing work ( P3GM paper mentioned above ) , it is shown that VAE \u2019 s objective function is too sensitive to the noise of DP-SGD , how the authors tackle this problem ? And how it affects the final results ? - The baseline methods are limited to `` direct noise features '' only . It would be better if the authors use other techniques such as some recent work on LDP on high dimensional data or other general DP classifiers , DP-SGD , PATE , etc . as the baselines for this experiment . - It would be better if the authors also showed the performance of the proposed method on higher dimensional data ( e.g. , Lending club has only 23 features ) Minor comments : - In the introduction , please list the contribution of this work . - Please define all variables in Eq 5-6 . - Please add more up-to-date references .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their response , and have addressed each of their points in order below . * * Major Point 1 : * * We have updated the introduction to include a more in depth discussion of the related work , including the papers cited in this review . * * Major Point 2 : * * Thank you for pointing this out , we have updated this literature review to reflect the fact that not all work in synthetic generation requires labels for training , and more importantly to outline how our work differs from existing work in the literature . In short , our method is solving a very different problem to the work on synthetic generation , and we believe synthetic generation to be limited in its applications . As mentioned in our response to AnonReviewer1 point 4 , we learn an LDP mechanism for privatizing individual datapoints or subsets of features , for use in downstream tasks ; this opens up possibilities to solve a much broader range of problems . Synthetic generation approaches create datasets that are not related to individuals , but are simply likely under the learnt probability distribution . One has no way of collecting / privatizing new data with these approaches , as is required for the tasks in Sections 4.1 , 4.2 or 5 . * * Major Point 3 : * * In P3GM , it is suggested that training a VAE end to end with DP-SGD is challenging , and this is something we also found . We found that the simple 2-stage approach outlined at the end of Section 3.1 was sufficient for learning good representations . P3GM also adopts a technique whereby the training of the VAE is split into steps , however their approach seems more complicated than ours . It should be stated that we are attempting to solve a completely different problem to P3GM as described in our rebuttal to Major Point 2 above . * * Major Point 4 : * * As discussed in our response to AnonReviewer1 point 4 , we do not see an obvious way to benchmark against DP classifiers due the nature of the tasks solved by our model in this paper . The literature on local differential privacy for high dimensional data is still in its infancy , and much of the existing work is tailored to specific datasets rather than a general model such as ours . For example [ 1 , 2 ] focus on the collection of data over time , whilst [ 2 ] note that for one time collection \u201c direct randomization on the true client \u2019 s value is sufficient to provide strong privacy protection \u201d . Indeed this direct randomization approach is in line with our benchmark . These papers have been discussed in the literature review within Section 1 . * * Major Point 5 : * * We tested our algorithm on MNIST , which contains 784 features , and do not consider this to be a low-dimensional problem . Even if , in the context of SOTA non-DP computer vision work , one might categorise MNIST as low dimensional , this is certainly not the case in the context of LDP research . By applying the algorithm to both MNIST and Lending Club , we demonstrate the versatility of the approach . It should theoretically work on any data type for which you can train a VAE to learn good representations . This allows us to make use of a vast body of work that has already been achieved in the generative modelling community . * * Minor Points : * * All three minor points have been addressed in the updated manuscript . We hope that these clarifications as well as the updates to our manuscript have addressed the reviewer 's comments , and that the reviewer will consider revising their score accordingly . [ 1 ] Ding et al.- Collecting telemetry data privately , 2017 [ 2 ] Erlingsson et al.- RAPPOR : Randomized aggregatable privacy-preservingordinal response , 2014 ."}, "1": {"review_id": "BIwkgTsSp_8-1", "review_text": "For LDP , when applying noise directly to high-dimensional data , the required noise entirely destroys data utility . In this paper , authors introduce a novel , application-agnostic privatization mechanism that leverages representation learning to overcome the prohibitive noise requirements of direct methods . They further demonstrate that this privatization mechanism can be used to train machine learning algorithms across a range of applications . They achieve significant gains in performance for high-dimensional data . In this paper , authors have benchmarked results against such a mechanism , in which add Laplace noise to all continuous features , and flip each of the categorical features with some probability . For high-dimensional datasets , features are often highly correlated ; consequently , noising features independently is wasteful towards privatizing the information content in each datapoint . A more effective approach to privatization involves noising a learned lower-dimensional representation of each datapoint using a generic noising mechanism . Applying the Laplace mechanism thus ensures the encoded latents , as well as reconstructed datapoints satisfy LDP . They focus on classification tasks . At inference time , they show that this classififier can act on either clean or privatized datapoints , . For the writing , it \u2019 s better to give a clear algorithm . For the experiment , when epsilon < =10 , the accuracy is not very good . The related work and comparisons are not enough . They are quite a few work about LDP learning can be literature reviewed . By the way , we usually use private data rather than privatized data .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the valuable feedback ; we hope that points below adequately address your concerns . Re : \u201c it \u2019 s better to give a clear algorithm \u201d AnonReviewer3 also sought clarification in this regard . We agree this could have been clearer and have significantly edited Section 3.1 to clarify the privatization procedure . Please let us know if you still believe it is unclear . Re : \u201c the accuracy is not very good \u201d LDP is an extremely strict criterion , and training models under LDP involves a privacy-utility trade off . Equation 9 and Figure 4 outline a strict upper bound on what the model can achieve , and demonstrate how one can not directly compare an LDP model against a classifier trained without privacy guarantees . Even in comparison to models trained with CDP , such as those from ( Abadi et al.2016 ) , the LDP criterion is incredibly strict , as it protects the privacy of individual datapoints . Very little work has been done on LDP mechanisms for high dimensional data . Many applications ( including those that were studied in our paper ) are impossible without LDP . Therefore , our results should be compared against the best technique available to the application , rather than the standard techniques that we are used to in less private applications . Re : \u201c The related work and comparisons are not enough \u201d We have extended our literature review to give a greater overview of this research in this field ."}, "2": {"review_id": "BIwkgTsSp_8-2", "review_text": "In this paper , the authors present a generative-model-based Laplace mechanism . By training the VAE on some dataset , the trained encoder can be used to privatize raw data towards epsilon , delta-LDP . Though the method is novel , the privacy guarantee of the proposed method is not clearly stated and proved . Related experiments are not convincing , either . * * Strength * * The paper is well written with a clear motivation , explanation of methodology . To my knowledge , I believe the work is useful for the privacy research community . The proposed method is also novel . * * Weakness * * - The motivation to use the Laplace mechanism is not very clear . At the beginning of Sec.2 , the authors reason the usage by `` as it provides strong theoretical privacy guarantees '' . This is not convincing for readers especially for those who are not familiar with LDP . Since the Laplace mechanism directly comes from the CDP , I would wonder how does the Gaussian mechanism works . How does the Laplace mechanism guarantee privacy better than the Gaussian mechanism ? Reference or proof is essential here . - In page 3 , the authors briefly mention that the local version of the Laplace mechanism can be epsilon-LDP if the sensitivity is accordingly defined . This really lacks rigorousness . In the following sections , the authors refer to ( Dwork and Roth , 2014 ) for the post-processing theorem . Since the work ( Dwork and Roth , 2014 ) is mainly about CDP , I am not sure how the post-processing theorem can be adopted for LDP . Either reference or clear proof is required . - Meanwhile , there lacks an end-to-end proof of the privacy guarantee of the VAE . I am not sure if the proposed VLM training guarantees privacy . Either , the privacy of encoding is not very clear . Especially , there involves a non-private training on stage 1 . - The experiments are run with pretty week baselines . Through this paper , the authors actively use the same conclusion from CDP ( Dwork and Roth , 2014 ) . Thus , I suppose the state-of-the-art CDP algorithms should also be applicable to the experimented tasks , e.g , classification . For the specific task , how well is the proposed compared to the SOTA CDP private learning algorithms ? For example , ( Abadi , et al. , 2016 ) , or ( Phan , et al.2017 ) .Especially , ( Phan , et al.2017 ) also proposed an adaptive Laplace mechanism without depending on pre-training of the mechanism . - In page 4 , the DP-Adam mentioned in Stage 2 is not stated or proved in ( Abadi et al. , 2016 ) . Only DP-SGD was discussed . A strict proof is required for the DP-Adam which intensively re-uses private results to help improve the gradients . Thus , the privacy guarantee is not straightforward . - Seems the VLM training is using a non-DP optimizer at stage 1 . Then how the whole training could guarantee privacy on the VLM training set . In experiments , the VLM training set is directly extracted from the private dataset ( MNIST ) . Even though the author experiments with diverse D_1 D_2 distribution for VLM train/test in Sec 4.2 , the two datasets are still from the same dataset . In practice , when such a D_2 is private , it is hard to find a D_1 to be non-private . I am afraid this could cause serious privacy leakage . Therefore , I doubt if the experimental results are useful for proving the effectiveness of a private algorithm . More realistic dates should be used . - In Sec 4.1 , the authors run the experiments in two steps . First , the VLM is trained with ' a DP encoder using D_1 ' . I am not clear how the DP encoder comes from . Does the VLM is also trained with DP ? The setting has to be clarified . - The experiment comparison seems not fair for baselines . For VLM , there are two datasets for training VLM and encoding classification train data . However , the baseline only has classification training data . The VLM encoder has additional information about the data distribution or the noise ( by back-propagation in VLM training ) . The unfairness in the information could be the core reason for the difference in performance . How does the baseline perform if it is pre-trained and tuned ( hyper-parameters ) on another dataset ? ( Phan , et al.2017 ) .Adaptive Laplace Mechanism : Differential Privacy Preservation in Deep Learning", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for their detailed feedback . Point-by-point responses are provided below : * * Points 1 & 2 : * * We chose the Local Laplace mechanism as it guarantees ( epsilon , delta=0 ) -LDP , while the Gaussian mechanism guarantees ( epsilon , delta > 0 ) -LDP which is not as strict . We believe that the method should work using the Gaussian mechanism ; this would require a minimal change to the model ( changing the prior and approximate posterior to be Gaussian ) . A paragraph has been added to Section 2 to discuss the choice of Laplace versus Gaussian mechanisms . We have also added a formal definition of the local Laplace mechanism in Section 2 , and a proof that this guarantees LDP in Appendix A . We have added a proof that LDP is immune to post-processing in Appendix B . * * Points 3 & 7 : * * There are two separate types of data in our approach : data which is transferred between entities and data which is only used for training the VLM . The latter must satisfy CDP so that the VLM parameters can be shared , while the former must satisfy LDP so that it can be safely shared . Because these two requirements are completely independent , keeping track of privacy is relatively straightforward : * DP training ( e.g.DP-Adam ) must be used when training the parameters of the VLM that are to be shared . This is done in stage 2 of VLM training as outlined in Section 3.1 . * LDP is required for transferring the data . This is achieved when the VLM adds noise in the latent space . With this clarification of the two-stage approach , and the proofs that have now been added to Appendix A and B , the LDP and CDP guarantees are proved . * * Point 4 : * * Our approach aims to tackle problems in which data must be privately shared between entities , thus requiring a LDP mechanism . The experiments in our paper require data sharing , and thus CDP approaches such as those referenced , can not be used to tackle this problem . Consequently , they can not be applied as benchmarks in our work . * * Point 5 : * * Thank you for pointing this out . We had missed a citation for [ Gylberth et al.- Differentially Private Optimization Algorithms for Deep Neural Networks ] who show that DP-Adam satisfies CDP . * * Point 6 : * * With respect to privacy leakage , we have clarified this concern in our response to points 3 & 7 above . The reviewer also comments on the relevance of our data set choices . We describe in Sections 4.1-4.3 and Section 5 multiple different scenarios in which the dataset assumptions we have made are highly relevant . * * Point 8 : * * We agree that with access to a similar , pre-training dataset $ D_1 $ , we are at an advantage over our baseline , and this is the motivation for the paper . We are demonstrating that , with some knowledge about the structure of our data distribution , one can learn a mechanism to privatise the data which is more effective than a fixed mechanism . This setup is a common scenario for organisations looking to privately exploit data , as we have articulated in our applications . In summary , we have made significant improvements to the manuscript in response to some of the reviewer 's comments ( e.g.extensive detail added to address the privacy guarantees ) , and hopefully provided helpful clarification in cases where perhaps the reviewer misunderstood our work ( e.g.the applicability of central DP ) . With this , we hope the reviewer will reconsider their score ."}, "3": {"review_id": "BIwkgTsSp_8-3", "review_text": "Strong point 1 : The idea of putting noise insertion ( via noisy data-generation models ) and optimization of good representations together to obtain LDP representations and/or synthetic data seems to be effective . While ( 6 ) relies on some independency assumptions , it might be fine in most cases and empirical evidence is reported to support it Strong point 2 : It is an application-agnostic approach and theoretically any downstream tasks and models can be supported ... When there is a label , the privacy budget is split and random perturbation is used on labels Strong point 3 : It outperforms naive LDP baselines ( with noise added directly to features ) a lot in experiments Weak point 1 : The proof of the most important result is missing : It is said that `` sampling from $ q_\\phi ( z|x ) $ produces a representation $ \\tilde z $ of $ x $ that satisfies $ \\epsilon $ -LDP . I do n't think it is a trivial result and the author needs to everything together ( including the analysis of sensitivity , the optimization algorithm , and so on ) to formally prove it Weak point 2 : A minor issue : in figures of experiments , by `` clean accuracy '' , do you actually mean `` accuracy '' ( for some algorithms in the figures , it is privacy accuracy ? ) W1 is the main reason for the rating of 6 but not higher ones - highly encourage the authors to fix it before the publication", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their valuable feedback , and respond to their comments below . * * Weak point 1 : * * The reviewers primary concern was the lack of explicit detail around proving that our approach satisfies the requirements of LDP . This was an oversight on our part , and we have now added multiple appendices with proofs as well as commentary within the body of the paper to further clarify . In particular , we have added an explicit definition for the local Laplace mechanism ( see Equation 5 ) , as well as the accompanying proof that the local Laplace mechanism satisfies epsilon-LDP in Appendix A ( the proof that LDP is immune to post-processing is additionally provided in Appendix B ) . We have also updated Section 3.1 to clarify that sampling from $ q_\\phi ( z|x ) $ is equivalent to a Laplace mechanism $ \\mathcal { M } ^\\text { ( local ) } \\left ( x , \\mu_ { \\phi } ( \\cdot ) , \\epsilon_x \\right ) $ , since sampling from $ q_\\phi ( z|x ) $ involves passing a datapoint through the mean function $ \\mu_\\phi ( x ) $ and adding laplace noise of scale $ b = \\Delta\\mu_\\phi / \\epsilon_x $ , where $ \\Delta\\mu_\\phi $ is the sensitivity of $ \\mu_\\phi ( . ) $ , as determined by the clipping function . We now have all formal results in the paper needed to fully address the reviewers main concern , and hope that consequently the reviewer will consider raising their score . * * Weak point 2 : * * For clarification , clean accuracy refers to the accuracy of our classifier when applied to a clean datapoint at inference time , while private accuracy refers to the accuracy of our classifier when applied to an LDP datapoint . In every case training is done on LDP data . In order to clarify these definitions to readers , the second paragraph of Section 4 has been updated significantly . With the proofs of privacy added and clarifications of the mechanism made in the text , we feel that we have addressed the primary concern of the reviewer . We hope they agree and will consider revising their score accordingly ."}}