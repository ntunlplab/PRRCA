{"year": "2020", "forum": "Hyez1CVYvr", "title": "Simultaneous Classification and Out-of-Distribution Detection Using Deep Neural Networks", "decision": "Reject", "meta_review": "The paper proposes a method for out-of-distribution (OOD) detection for neural network classifiers.\n\nThe reviewers raised several concerns about novelty, choice of baselines and the experimental evaluation. While the author rebuttal addressed some of these concerns, I think the paper is still not ready for acceptance as is. \n\nI encourage the authors to revise the paper and resubmit to a different venue.", "reviews": [{"review_id": "Hyez1CVYvr-0", "review_text": "This paper proposes to tackle the problems of out-of-distribution (OOD) detection and model calibration by adapting the loss function of the Outlier Exposure (OE) technique [1]. In OE, model softmax outputs are encouraged to be uniform for OOD samples, which is enforced through the use of a KL divergence loss function. The first proposed modification in this paper is to replace KL divergence term with an L1 penalty. The second change is the addition of an L2 penalty between the maximum softmax probability and the model accuracy. Experimental results demonstrate that adding these two components increases performance over OE on standard OOD benchmarks for both vision and text domains, and also improves model calibration. Although this paper presents some good quantitative results, I tend towards rejection in its current state. This is mainly due to the limited comparison to alternative methods, and the lack of ablation study. If these were addressed I would consider increasing my score. Things to improve the paper: 1) Currently, one of the most commonly used benchmark methods for OOD detection is the Mahalanobis distance based confidence score (MD) [2], which, as far as I am aware, is state-of-the-art among published works. The authors claim that they do not compare to this work because it is a post-training method, and, presumably, the techniques should be doubly effective when combined. However, we do not have any proof that this is actually the case. Therefore, I think it is important to verify that the two techniques are indeed compatible, and if not, then direct comparison with MD would still be necessary. 2) In the case of confidence calibration, there is no comparison made with other calibration techniques, such as temperature scaling [3]. I think it would be good to included these for reference. 3) Since two distinct components are being added to the loss function, I think it is important to include an ablation study to identify how much each component contributes to improvements in OOD detection and confidence calibration. Minor things to improve the paper that did not impact the score: 4) With regards to the confidence calibration loss, there is similar work by [4] which also optimizes the output of the model to make sure confidence predictions are close to the true accuracy. It may be worth citing if you think it is relevant. Additional questions: 5) How are lambda1 and lambda2 tuned? I could not find this information in the paper. 6) How sensitive is model performance to the setting of the lambda hyperparameters? It would be nice to see a plot of lambda versus the OOD detection metrics. 7) Have you evaluated how this method performs at detecting adversarial attacks? I do not think the paper will suffer without these results, but they are certainly relevant and of interest to practitioners in this area. References: [1] Hendrycks, Dan, Mantas Mazeika, and Thomas G. Dietterich. \"Deep anomaly detection with outlier exposure.\" ICLR (2018). [2] Lee, Kimin, Kibok Lee, Honglak Lee, and Jinwoo Shin. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" NeurIPS, (2018). [3] Guo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. \"On calibration of modern neural networks.\" In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1321-1330. JMLR. org, 2017. [4] Corbi\u00e8re, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick P\u00e9rez. \"Addressing Failure Prediction by Learning Model Confidence.\" NeurIPS (2019). ### Post-Rebuttal Comments ### I would like to thank the authors for their hard work during the rebuttal period. I think the current version of the paper is much improved over the previous version. The choice to remove the claims about calibration definitely improves the focus of the paper. The addition of the Mahalanobis distance experiments and the ablation study also significantly strengthen the paper. However, as the other reviewers have pointed out, the novelty of the paper is quite limited since the majority of the gains come from simply swapping the KL-divergence penalty with an L1 penalty. Despite simplicity, this single change yields a significant improvement in performance for the OE algorithm, which is noteworthy. As a result, I will increase my score from a weak reject (3) to a very, very weak accept (more like a 5 than a 6). ", "rating": "6: Weak Accept", "reply_text": "Thank you for your valuable comments and for taking the time and effort to review our paper . Your comments helped us to significantly improve the quality of our paper . We addressed all of them ! Please check our answers below : 1 ) Thank you for your valuable comment . In the initial version of our paper , we had mentioned that Mahalanobis distance-based confidence score [ 1 ] is a post-training method . At this point , we would also like to mention that that in the OOD detection task , we could classify the methods into three categories : 1 ) If someone assumes access to knowledge of the test distribution , then Mahalanobis distance-based classifier proposed by [ 1 ] achieves state-of-the-art results in the OOD detection task , 2 ) If someone does not make this assumption then the results of [ 2 ] are considered state-of-the-art , 3 ) if someone assumes access to no extra data during training , then the maximum softmax probability+rotation prediction is the best ( please check the same argument that was made by the Anonymous Reviewer # 3 of the ICLR 2020 paper in the following link : https : //openreview.net/forum ? id=r1g6MCEtwr ) . In our initially submitted version , we had proposed a loss function that beat the results of [ 2 ] in both image and text classification tasks achieving state-of-the-art results in the second category of methods , which together with the novelty of our loss function showed the contribution of our technique . Understandably , since all of the esteemed reviewers asked us to run more simulation experiments , we followed your suggestion and we tried to verify the compatibility of our method and Mahalanobis method . Therefore , in the revised version of the paper , we carried out additional image classification experiments where we trained the DNN with our proposed method and then we used the Mahalanobis method . We compared the results we obtained with those obtained by the original Mahalanobis distance-based classifier in [ 1 ] . We have added a whole section in our paper ( please see Section 4.2 ) for this . The experimental results are presented in Table 4 of the paper where it is shown that our method combined with the Mahalanobis method outperforms the original Mahalanobis method in most of the experiments achieving state-of-the-art results even in the first category of methods for OOD detection . Furthermore , we added a discussion section ( just before Section 5 ) where we explain these experimental results . In our opinion , these results are significant because they present a very effective and adaptable OOD detection method . 2 ) Thank you for your valuable comment . Regarding the calibration term , we firmly believe that the contribution of our paper is in the field of OOD detection . The idea of calibration was one of our inspirations ( but not the main one ! ) when we added the corresponding regularization term in our loss function . Our intent was to show that our proposed loss function can also achieve this . However , taking into account your comment and the fact that the contribution of our work is in the OOD detection field , we decided to remove both the calibration experiments and the statement of calibration improvement from the contribution of our work in the revised version of the paper . In its current state , the contribution of our paper is : a ) We propose a novel loss function for the OOD detection task consisting of two regularization terms , we try to go through all the theoretical derivation of those and we show experimentally what is the contribution of each term in both the OOD evaluation metrics as well as in the test accuracy on examples generated by $ D_ { in } $ as can be shown in the Table 2 in the revised version of our paper . b ) We achieve state-of-the-art results in the OOD detection task for the class of methods that do not make any assumption about access to the test distribution by consistently outperforming the results of [ 2 ] in both image and text classification tasks ( see Tables 1,3,5 and 6 in the revised version of our paper ) . c ) We show the adaptability and the effectiveness of our method by combining it with the Mahalanobis method and by outperforming the original Mahalanobis method proposed in [ 1 ] , achieving state-of-the-art results in the OOD detection task even for the first category of methods for OOD ( see Section 4.2 and Table 4 in the revised version of our paper ) . 3 ) Thank you for your valuable comment . We added a section ( at the end of Section 4.1.2 in the revised version of our paper ) and carried out additional experiments to demonstrate the effect of each regularization term both in the OOD evaluation metrics as well as in the test accuracy on examples generated by $ D_ { in } $ . The experimental results can be found in Table 2 in the revised version of our paper . 4 ) The initial arXiv version of [ 3 ] was uploaded on October 1st which is later than the submission deadline of September 25th for the ICLR paper . However , we found this work relevant and we cited it accordingly ."}, {"review_id": "Hyez1CVYvr-1", "review_text": "This work proposes a new loss function to train the network with Outlier Exposure(OE) [1] which leads to better OOD detection compared to simple loss function that uses KL divergence as the regularizer for OOD detection. The new loss function is the cross entropy plus two more regularizers which are : 1) Average ECE (Expected Calibration Error) function to calibrate the model and 2) absolute difference of the network output to $1/K$ where $K$ is the number of tasks. The second regularizer keeps the softmax output of the network uniform for the OE samples. They show adding these new regularizers to the cross-entropy loss function will improve the Out-distribution detection capability of networks more than OE method proposed in [1] and the baseline proposed in [2]. Pros: The paper is written clearly and the motivation of designed loss functions are explained well. Cons: 1- The level of contributions is limited. 2- The variety of comparison is not enough. The authors did not show how the approach is working in compared to the other OOD methods like ODIN[3] and the proposed method in [4]. 3- The experiments are not supporting the idea. First, the paper claims that the KL is not a good regularizer for OOD detection as it is not a distance metric. But there is no experiment or justification in the paper that supports why this claim is true. Then the second contribution claims that the calibration term that is added to the loss function improves the OOD detection as well as calibration in the network, but the experiments are not designed to show the impact of each regularizer term separately in improving the OOD detection rate. Figure 2 also does not depict any significant conclusion. It only shows that the new loss function makes the network more calibrated than the naive network. This phenomenon was reported before in [1]. It would be better if the paper investigated the relation between the calibration and OOD detection by designing more specific experiments for calibration section. Overall, I think the paper should be rejected as the contributions are limited and are not aligned with the experiments. References [1]Hendrycks, Dan, Mantas Mazeika, and Thomas G. Dietterich. \"Deep Anomaly Detection with Outlier Exposure.\" arXiv preprint arXiv:1812.04606 (2018). [2] A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks, ICLR2016. [3] Liang, Shiyu, Yixuan Li, and Rayadurgam Srikant. \"Enhancing the reliability of out-of-distribution image detection in neural networks.\" arXiv preprint arXiv:1706.02690 (2017). [4] Lee, Kimin, et al. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" Advances in Neural Information Processing Systems. 2018.", "rating": "1: Reject", "reply_text": "Thank you for your valuable comments since they helped us significantly improve the quality of our paper . We addressed all of your concerns and comments . Please see our answer below : 1 ) At first , we would like to mention that in the OOD detection task , we could classify the methods into three categories : 1 ) If someone assumes access to knowledge of the test distribution , then Mahalanobis distance-based classifier proposed by [ 1 ] achieves state-of-the-art results in the OOD detection task , 2 ) If someone does not make this assumption then the results of [ 2 ] are considered state-of-the-art , 3 ) if someone assumes access to no extra data during training , then the maximum softmax probability+rotation prediction is the best ( please check the same argument that was made by the Anonymous Reviewer # 3 of the ICLR 2020 paper in the following link : https : //openreview.net/forum ? id=r1g6MCEtwr ) . In our initially submitted version , we had proposed a loss function that beat the results of [ 2 ] in both image and text classification tasks achieving state-of-the-results in the second category of methods which together with the novelty of our loss function showed the contribution of our technique . Understandably , all of the esteemed reviewers asked us to run more simulation experiments and compare our results with more methods , which resulted in significant improvements in the paper and proved the merits of our method . In the initial version of our paper , we had cited the seminal work of [ 1 ] stating that Mahalanobis distance-based classifier is a post-training method and it is not directly comparable to ours . Fortunately , the Anonymous Reviewer # 1 of our paper brought to our attention that we should verify whether the two methods are indeed compatible . Therefore , in the revised version of the paper , we ran additional image classification experiments where we trained the DNN with our proposed method and then we used the Mahalanobis method . We compared the results we obtained with those obtained by the original Mahalanobis distance-based classifier in [ 1 ] . We have dedicated a whole section in our paper ( please see Section 4.2 ) to this combination . The experimental results are presented in Table 4 of the paper where it is shown that our method combined with the Mahalanobis method outperforms the original Mahalanobis method in most of the experiments achieving state-of-the-art results even in the first category of methods for OOD detection . Furthermore , we added a discussion section ( just before Section 5 ) where we explain these experimental results . In our opinion , these results are significant because they present a very effective and flexible OOD detection method . Additionally , in order to further demonstrate the insights of our method we ran some additional experiments for CIFAR-10 and CIFAR-100 datasets where we show what the contribution of each regularization term in the OOD evaluation metrics is , as well as in the test accuracy of the DNN in examples generated by $ D_ { in } $ ( Please check the results of these experiments in Table 2 . ) . In its current state , the contributions of our paper are : 1 ) We propose a novel loss function for the OOD detection task consisting of two regularization terms , we thoroughly explain the theoretical and mathematical foundations of those regularization terms and we show experimentally the contribution of each term in both the OOD evaluation metrics as well as in the test accuracy on examples generated by $ D_ { in } $ . 2 ) We achieve state-of-the-art results in the OOD detection task for the class of methods that do not make any assumption about access to the test distribution by consistently outperforming the results of [ 2 ] in both image and text classification tasks ( see Tables 1,3,5 and 6 in the revised version of our paper ) . 3 ) We show the adaptability and the effectiveness of our method by combining it with the Mahalanobis method and by outperforming the original Mahalanobis method proposed in [ 1 ] , achieving state-of-the-art results in the OOD detection task even for the first category of methods for OOD ( see Section 4.2 and Table 4 in the revised version of our paper ) . We believe that the aforementioned arguments explain the novelty and the contribution of our paper in the field of OOD detection ."}, {"review_id": "Hyez1CVYvr-2", "review_text": "The paper considers the problem of out-of-distribution detection in the context of image and text classification with deep neural networks. The proposed method is based on [1], where cross-entropy between a uniform distribution and the predictive distribution is maximized on the out-of-distribution data during training. The authors propose two simple modifications to the objective. The first modification is to replace cross-entropy between predictive and uniform distributions with an l1-norm. The second modification is to add a separate loss term that encourages the average confidence on training data to be close to the training accuracy. The authors show that these modifications improve results compared to [1] on image and text classification. There are a few concerns I have for this paper. The main issue is that I am not sure if the level of novelty is sufficient for ICLR. The contributions of the paper consist of a new loss term and a modification of the other loss term in OE [1]. At the same time, the paper achieves an improvement over OE consistently on all the considered problems. Given the limited novelty, experiments aimed at understanding the proposed modification would strengthen the paper. Right now I am leaning towards rejecting the paper, but if authors add more insight into why the proposed modifications help, or provide a strong rebuttal, I may update my score. I discuss the other less general concerns below. 1. I believe the presentation of the method in Section 3 is suboptimal. The authors start with presenting a constrained minimization problem, then convert it to a problem with Lagrange multipliers, then modify the problem in an ad hoc way (adding a square and a norm without any mathematical reason), to get the standard form of loss with regularizers. The presentation would be much cleaner, and wouldn\u2019t lose anything if the authors directly presented the loss with regularizers. Furthermore, in the Lagrange multiplier view the Lagrange multipliers are not hyper-parameters, they are dual variables, and the optimization problem shouldn\u2019t be just with respect to theta, we need to find a stationary point with respect to both lambda and theta. 2. The motivation for changing the distance measure between the predictive distribution on outlier data and the uniform distribution is unclear. The authors state multiple times that KL is not a distance metric, but it isn\u2019t clear why this is important. KL is commonly used as a measure of distance between distributions. One could also use symmetrized KL in order to get a distance metric that is similar to KL. I am not opposed to just using l1-norm because it performs better, but if the switch of distance measures is listed as one of the two main methodological contributions, I believe more insight needs to be provided for why it helps. 3. The motivation for the other loss term which is enforcing calibration of uncertainty on train data is also not very clear. At least in image classification, strong networks typically achieve perfect accuracy (or close to that) on the train set, and then the proposed loss term would basically push the predictive confidence on all training data to 1, which is already enforced by the standard cross-entropy loss. Does outlier exposure prevent the classifier from getting close to 100% accuracy on train? What is the train accuracy for the experiments on CIFAR-10, CIFAR-100 and SVHN? 4. While the authors report accuracy of out-of-distribution detection in the experiments, they don\u2019t report the accuracy of the actual classifier on in-distribution data. Is this accuracy similar for the proposed method and OE? Is the accuracy also similar for the proposed method and baseline for the experiment in section 4.4? 5. The method is only being compared to the OE method of [1]. Why is this comparison important, and are there other methods that the authors could compare against? [1] Deep Anomaly Detection with Outlier Exposure. Dan Hendrycks, Mantas Mazeika, Thomas Dietterich", "rating": "3: Weak Reject", "reply_text": "Thank you for your response and your valuable feedback since we firmly believe that it helped us to improve the quality of our paper . We took into consideration all of your comments . Since you initially wrote a general comment , let us first answer to this one and subsequently , we are going to answer all the other less general concerns . Please see our answer below : In the revised version of the paper , we tried to better explain the insights of our method both theoretically and experimentally . At first , we would like to mention that in the OOD detection task , we could classify the methods into three categories : 1 ) If someone assumes access to knowledge of the test distribution , then Mahalanobis distance-based classifier proposed by [ 1 ] achieves state-of-the-art results in the OOD detection task , 2 ) If someone does not make this assumption then the results of [ 2 ] are considered state-of-the-art , 3 ) if someone assumes access to no extra data during training , then the maximum softmax probability+rotation prediction is the best ( please check the same argument that was made by the Anonymous Reviewer # 3 of the ICLR 2020 paper in the following link : https : //openreview.net/forum ? id=r1g6MCEtwr ) . In our initially submitted version , we had proposed a loss function that beat the results of [ 2 ] in both image and text classification tasks achieving state-of-the-results in the second category of methods which together with the novelty of our loss function showed the contribution of our technique . Understandably , all of the esteemed reviewers asked us to run more simulation experiments and compare our results with more methods , which resulted in significant improvements in the paper and proved the merits of our method . In the initial version of our paper , we had cited the seminal work of [ 1 ] stating that Mahalanobis distance-based classifier is a post-training method and it is not directly comparable to ours . Fortunately , the Anonymous Reviewer # 1 of our paper brought to our attention that we should verify whether the two methods are indeed compatible . Therefore , in the revised version of the paper , we ran additional image classification experiments where we trained the DNN with our proposed method and then we used the Mahalanobis method . We compared the results we obtained with those obtained by the original Mahalanobis distance-based classifier in [ 1 ] . We have dedicated a whole section in our paper ( please see Section 4.2 ) to this combination . The experimental results are presented in Table 4 of the paper where it is shown that our method combined with the Mahalanobis method outperforms the original Mahalanobis method in most of the experiments achieving state-of-the-art results even in the first category of methods for OOD detection . Furthermore , we added a discussion section ( just before Section 5 ) where we explain these experimental results . In our opinion , these results are significant because they present a very effective and flexible OOD detection method . Additionally , in order to further demonstrate the insights of our method we ran some additional experiments for CIFAR-10 and CIFAR-100 datasets where we show what the contribution of each regularization term in the OOD evaluation metrics is , as well as in the test accuracy of the DNN in examples generated by $ D_ { in } $ ( Please check the results of these experiments in Table 2 . ) . In its current state , the contributions of our paper are : 1 ) We propose a novel loss function for the OOD detection task consisting of two regularization terms , we thoroughly explain the theoretical and mathematical foundations of those regularization terms and we show experimentally the contribution of each term in both the OOD evaluation metrics as well as in the test accuracy on examples generated by $ D_ { in } $ . 2 ) We achieve state-of-the-art results in the OOD detection task for the class of methods that do not make any assumption about access to the test distribution by consistently outperforming the results of [ 2 ] in both image and text classification tasks ( see Tables 1,3,5 and 6 in the revised version of our paper ) . 3 ) We show the adaptability and the effectiveness of our method by combining it with the Mahalanobis method and by outperforming the original Mahalanobis method proposed in [ 1 ] , achieving state-of-the-art results in the OOD detection task even for the first category of methods for OOD ( see Section 4.2 and Table 4 in the revised version of our paper ) . We believe that the aforementioned arguments explain the novelty and the contribution of our paper in the field of OOD detection ."}], "0": {"review_id": "Hyez1CVYvr-0", "review_text": "This paper proposes to tackle the problems of out-of-distribution (OOD) detection and model calibration by adapting the loss function of the Outlier Exposure (OE) technique [1]. In OE, model softmax outputs are encouraged to be uniform for OOD samples, which is enforced through the use of a KL divergence loss function. The first proposed modification in this paper is to replace KL divergence term with an L1 penalty. The second change is the addition of an L2 penalty between the maximum softmax probability and the model accuracy. Experimental results demonstrate that adding these two components increases performance over OE on standard OOD benchmarks for both vision and text domains, and also improves model calibration. Although this paper presents some good quantitative results, I tend towards rejection in its current state. This is mainly due to the limited comparison to alternative methods, and the lack of ablation study. If these were addressed I would consider increasing my score. Things to improve the paper: 1) Currently, one of the most commonly used benchmark methods for OOD detection is the Mahalanobis distance based confidence score (MD) [2], which, as far as I am aware, is state-of-the-art among published works. The authors claim that they do not compare to this work because it is a post-training method, and, presumably, the techniques should be doubly effective when combined. However, we do not have any proof that this is actually the case. Therefore, I think it is important to verify that the two techniques are indeed compatible, and if not, then direct comparison with MD would still be necessary. 2) In the case of confidence calibration, there is no comparison made with other calibration techniques, such as temperature scaling [3]. I think it would be good to included these for reference. 3) Since two distinct components are being added to the loss function, I think it is important to include an ablation study to identify how much each component contributes to improvements in OOD detection and confidence calibration. Minor things to improve the paper that did not impact the score: 4) With regards to the confidence calibration loss, there is similar work by [4] which also optimizes the output of the model to make sure confidence predictions are close to the true accuracy. It may be worth citing if you think it is relevant. Additional questions: 5) How are lambda1 and lambda2 tuned? I could not find this information in the paper. 6) How sensitive is model performance to the setting of the lambda hyperparameters? It would be nice to see a plot of lambda versus the OOD detection metrics. 7) Have you evaluated how this method performs at detecting adversarial attacks? I do not think the paper will suffer without these results, but they are certainly relevant and of interest to practitioners in this area. References: [1] Hendrycks, Dan, Mantas Mazeika, and Thomas G. Dietterich. \"Deep anomaly detection with outlier exposure.\" ICLR (2018). [2] Lee, Kimin, Kibok Lee, Honglak Lee, and Jinwoo Shin. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" NeurIPS, (2018). [3] Guo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. \"On calibration of modern neural networks.\" In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1321-1330. JMLR. org, 2017. [4] Corbi\u00e8re, Charles, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick P\u00e9rez. \"Addressing Failure Prediction by Learning Model Confidence.\" NeurIPS (2019). ### Post-Rebuttal Comments ### I would like to thank the authors for their hard work during the rebuttal period. I think the current version of the paper is much improved over the previous version. The choice to remove the claims about calibration definitely improves the focus of the paper. The addition of the Mahalanobis distance experiments and the ablation study also significantly strengthen the paper. However, as the other reviewers have pointed out, the novelty of the paper is quite limited since the majority of the gains come from simply swapping the KL-divergence penalty with an L1 penalty. Despite simplicity, this single change yields a significant improvement in performance for the OE algorithm, which is noteworthy. As a result, I will increase my score from a weak reject (3) to a very, very weak accept (more like a 5 than a 6). ", "rating": "6: Weak Accept", "reply_text": "Thank you for your valuable comments and for taking the time and effort to review our paper . Your comments helped us to significantly improve the quality of our paper . We addressed all of them ! Please check our answers below : 1 ) Thank you for your valuable comment . In the initial version of our paper , we had mentioned that Mahalanobis distance-based confidence score [ 1 ] is a post-training method . At this point , we would also like to mention that that in the OOD detection task , we could classify the methods into three categories : 1 ) If someone assumes access to knowledge of the test distribution , then Mahalanobis distance-based classifier proposed by [ 1 ] achieves state-of-the-art results in the OOD detection task , 2 ) If someone does not make this assumption then the results of [ 2 ] are considered state-of-the-art , 3 ) if someone assumes access to no extra data during training , then the maximum softmax probability+rotation prediction is the best ( please check the same argument that was made by the Anonymous Reviewer # 3 of the ICLR 2020 paper in the following link : https : //openreview.net/forum ? id=r1g6MCEtwr ) . In our initially submitted version , we had proposed a loss function that beat the results of [ 2 ] in both image and text classification tasks achieving state-of-the-art results in the second category of methods , which together with the novelty of our loss function showed the contribution of our technique . Understandably , since all of the esteemed reviewers asked us to run more simulation experiments , we followed your suggestion and we tried to verify the compatibility of our method and Mahalanobis method . Therefore , in the revised version of the paper , we carried out additional image classification experiments where we trained the DNN with our proposed method and then we used the Mahalanobis method . We compared the results we obtained with those obtained by the original Mahalanobis distance-based classifier in [ 1 ] . We have added a whole section in our paper ( please see Section 4.2 ) for this . The experimental results are presented in Table 4 of the paper where it is shown that our method combined with the Mahalanobis method outperforms the original Mahalanobis method in most of the experiments achieving state-of-the-art results even in the first category of methods for OOD detection . Furthermore , we added a discussion section ( just before Section 5 ) where we explain these experimental results . In our opinion , these results are significant because they present a very effective and adaptable OOD detection method . 2 ) Thank you for your valuable comment . Regarding the calibration term , we firmly believe that the contribution of our paper is in the field of OOD detection . The idea of calibration was one of our inspirations ( but not the main one ! ) when we added the corresponding regularization term in our loss function . Our intent was to show that our proposed loss function can also achieve this . However , taking into account your comment and the fact that the contribution of our work is in the OOD detection field , we decided to remove both the calibration experiments and the statement of calibration improvement from the contribution of our work in the revised version of the paper . In its current state , the contribution of our paper is : a ) We propose a novel loss function for the OOD detection task consisting of two regularization terms , we try to go through all the theoretical derivation of those and we show experimentally what is the contribution of each term in both the OOD evaluation metrics as well as in the test accuracy on examples generated by $ D_ { in } $ as can be shown in the Table 2 in the revised version of our paper . b ) We achieve state-of-the-art results in the OOD detection task for the class of methods that do not make any assumption about access to the test distribution by consistently outperforming the results of [ 2 ] in both image and text classification tasks ( see Tables 1,3,5 and 6 in the revised version of our paper ) . c ) We show the adaptability and the effectiveness of our method by combining it with the Mahalanobis method and by outperforming the original Mahalanobis method proposed in [ 1 ] , achieving state-of-the-art results in the OOD detection task even for the first category of methods for OOD ( see Section 4.2 and Table 4 in the revised version of our paper ) . 3 ) Thank you for your valuable comment . We added a section ( at the end of Section 4.1.2 in the revised version of our paper ) and carried out additional experiments to demonstrate the effect of each regularization term both in the OOD evaluation metrics as well as in the test accuracy on examples generated by $ D_ { in } $ . The experimental results can be found in Table 2 in the revised version of our paper . 4 ) The initial arXiv version of [ 3 ] was uploaded on October 1st which is later than the submission deadline of September 25th for the ICLR paper . However , we found this work relevant and we cited it accordingly ."}, "1": {"review_id": "Hyez1CVYvr-1", "review_text": "This work proposes a new loss function to train the network with Outlier Exposure(OE) [1] which leads to better OOD detection compared to simple loss function that uses KL divergence as the regularizer for OOD detection. The new loss function is the cross entropy plus two more regularizers which are : 1) Average ECE (Expected Calibration Error) function to calibrate the model and 2) absolute difference of the network output to $1/K$ where $K$ is the number of tasks. The second regularizer keeps the softmax output of the network uniform for the OE samples. They show adding these new regularizers to the cross-entropy loss function will improve the Out-distribution detection capability of networks more than OE method proposed in [1] and the baseline proposed in [2]. Pros: The paper is written clearly and the motivation of designed loss functions are explained well. Cons: 1- The level of contributions is limited. 2- The variety of comparison is not enough. The authors did not show how the approach is working in compared to the other OOD methods like ODIN[3] and the proposed method in [4]. 3- The experiments are not supporting the idea. First, the paper claims that the KL is not a good regularizer for OOD detection as it is not a distance metric. But there is no experiment or justification in the paper that supports why this claim is true. Then the second contribution claims that the calibration term that is added to the loss function improves the OOD detection as well as calibration in the network, but the experiments are not designed to show the impact of each regularizer term separately in improving the OOD detection rate. Figure 2 also does not depict any significant conclusion. It only shows that the new loss function makes the network more calibrated than the naive network. This phenomenon was reported before in [1]. It would be better if the paper investigated the relation between the calibration and OOD detection by designing more specific experiments for calibration section. Overall, I think the paper should be rejected as the contributions are limited and are not aligned with the experiments. References [1]Hendrycks, Dan, Mantas Mazeika, and Thomas G. Dietterich. \"Deep Anomaly Detection with Outlier Exposure.\" arXiv preprint arXiv:1812.04606 (2018). [2] A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks, ICLR2016. [3] Liang, Shiyu, Yixuan Li, and Rayadurgam Srikant. \"Enhancing the reliability of out-of-distribution image detection in neural networks.\" arXiv preprint arXiv:1706.02690 (2017). [4] Lee, Kimin, et al. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" Advances in Neural Information Processing Systems. 2018.", "rating": "1: Reject", "reply_text": "Thank you for your valuable comments since they helped us significantly improve the quality of our paper . We addressed all of your concerns and comments . Please see our answer below : 1 ) At first , we would like to mention that in the OOD detection task , we could classify the methods into three categories : 1 ) If someone assumes access to knowledge of the test distribution , then Mahalanobis distance-based classifier proposed by [ 1 ] achieves state-of-the-art results in the OOD detection task , 2 ) If someone does not make this assumption then the results of [ 2 ] are considered state-of-the-art , 3 ) if someone assumes access to no extra data during training , then the maximum softmax probability+rotation prediction is the best ( please check the same argument that was made by the Anonymous Reviewer # 3 of the ICLR 2020 paper in the following link : https : //openreview.net/forum ? id=r1g6MCEtwr ) . In our initially submitted version , we had proposed a loss function that beat the results of [ 2 ] in both image and text classification tasks achieving state-of-the-results in the second category of methods which together with the novelty of our loss function showed the contribution of our technique . Understandably , all of the esteemed reviewers asked us to run more simulation experiments and compare our results with more methods , which resulted in significant improvements in the paper and proved the merits of our method . In the initial version of our paper , we had cited the seminal work of [ 1 ] stating that Mahalanobis distance-based classifier is a post-training method and it is not directly comparable to ours . Fortunately , the Anonymous Reviewer # 1 of our paper brought to our attention that we should verify whether the two methods are indeed compatible . Therefore , in the revised version of the paper , we ran additional image classification experiments where we trained the DNN with our proposed method and then we used the Mahalanobis method . We compared the results we obtained with those obtained by the original Mahalanobis distance-based classifier in [ 1 ] . We have dedicated a whole section in our paper ( please see Section 4.2 ) to this combination . The experimental results are presented in Table 4 of the paper where it is shown that our method combined with the Mahalanobis method outperforms the original Mahalanobis method in most of the experiments achieving state-of-the-art results even in the first category of methods for OOD detection . Furthermore , we added a discussion section ( just before Section 5 ) where we explain these experimental results . In our opinion , these results are significant because they present a very effective and flexible OOD detection method . Additionally , in order to further demonstrate the insights of our method we ran some additional experiments for CIFAR-10 and CIFAR-100 datasets where we show what the contribution of each regularization term in the OOD evaluation metrics is , as well as in the test accuracy of the DNN in examples generated by $ D_ { in } $ ( Please check the results of these experiments in Table 2 . ) . In its current state , the contributions of our paper are : 1 ) We propose a novel loss function for the OOD detection task consisting of two regularization terms , we thoroughly explain the theoretical and mathematical foundations of those regularization terms and we show experimentally the contribution of each term in both the OOD evaluation metrics as well as in the test accuracy on examples generated by $ D_ { in } $ . 2 ) We achieve state-of-the-art results in the OOD detection task for the class of methods that do not make any assumption about access to the test distribution by consistently outperforming the results of [ 2 ] in both image and text classification tasks ( see Tables 1,3,5 and 6 in the revised version of our paper ) . 3 ) We show the adaptability and the effectiveness of our method by combining it with the Mahalanobis method and by outperforming the original Mahalanobis method proposed in [ 1 ] , achieving state-of-the-art results in the OOD detection task even for the first category of methods for OOD ( see Section 4.2 and Table 4 in the revised version of our paper ) . We believe that the aforementioned arguments explain the novelty and the contribution of our paper in the field of OOD detection ."}, "2": {"review_id": "Hyez1CVYvr-2", "review_text": "The paper considers the problem of out-of-distribution detection in the context of image and text classification with deep neural networks. The proposed method is based on [1], where cross-entropy between a uniform distribution and the predictive distribution is maximized on the out-of-distribution data during training. The authors propose two simple modifications to the objective. The first modification is to replace cross-entropy between predictive and uniform distributions with an l1-norm. The second modification is to add a separate loss term that encourages the average confidence on training data to be close to the training accuracy. The authors show that these modifications improve results compared to [1] on image and text classification. There are a few concerns I have for this paper. The main issue is that I am not sure if the level of novelty is sufficient for ICLR. The contributions of the paper consist of a new loss term and a modification of the other loss term in OE [1]. At the same time, the paper achieves an improvement over OE consistently on all the considered problems. Given the limited novelty, experiments aimed at understanding the proposed modification would strengthen the paper. Right now I am leaning towards rejecting the paper, but if authors add more insight into why the proposed modifications help, or provide a strong rebuttal, I may update my score. I discuss the other less general concerns below. 1. I believe the presentation of the method in Section 3 is suboptimal. The authors start with presenting a constrained minimization problem, then convert it to a problem with Lagrange multipliers, then modify the problem in an ad hoc way (adding a square and a norm without any mathematical reason), to get the standard form of loss with regularizers. The presentation would be much cleaner, and wouldn\u2019t lose anything if the authors directly presented the loss with regularizers. Furthermore, in the Lagrange multiplier view the Lagrange multipliers are not hyper-parameters, they are dual variables, and the optimization problem shouldn\u2019t be just with respect to theta, we need to find a stationary point with respect to both lambda and theta. 2. The motivation for changing the distance measure between the predictive distribution on outlier data and the uniform distribution is unclear. The authors state multiple times that KL is not a distance metric, but it isn\u2019t clear why this is important. KL is commonly used as a measure of distance between distributions. One could also use symmetrized KL in order to get a distance metric that is similar to KL. I am not opposed to just using l1-norm because it performs better, but if the switch of distance measures is listed as one of the two main methodological contributions, I believe more insight needs to be provided for why it helps. 3. The motivation for the other loss term which is enforcing calibration of uncertainty on train data is also not very clear. At least in image classification, strong networks typically achieve perfect accuracy (or close to that) on the train set, and then the proposed loss term would basically push the predictive confidence on all training data to 1, which is already enforced by the standard cross-entropy loss. Does outlier exposure prevent the classifier from getting close to 100% accuracy on train? What is the train accuracy for the experiments on CIFAR-10, CIFAR-100 and SVHN? 4. While the authors report accuracy of out-of-distribution detection in the experiments, they don\u2019t report the accuracy of the actual classifier on in-distribution data. Is this accuracy similar for the proposed method and OE? Is the accuracy also similar for the proposed method and baseline for the experiment in section 4.4? 5. The method is only being compared to the OE method of [1]. Why is this comparison important, and are there other methods that the authors could compare against? [1] Deep Anomaly Detection with Outlier Exposure. Dan Hendrycks, Mantas Mazeika, Thomas Dietterich", "rating": "3: Weak Reject", "reply_text": "Thank you for your response and your valuable feedback since we firmly believe that it helped us to improve the quality of our paper . We took into consideration all of your comments . Since you initially wrote a general comment , let us first answer to this one and subsequently , we are going to answer all the other less general concerns . Please see our answer below : In the revised version of the paper , we tried to better explain the insights of our method both theoretically and experimentally . At first , we would like to mention that in the OOD detection task , we could classify the methods into three categories : 1 ) If someone assumes access to knowledge of the test distribution , then Mahalanobis distance-based classifier proposed by [ 1 ] achieves state-of-the-art results in the OOD detection task , 2 ) If someone does not make this assumption then the results of [ 2 ] are considered state-of-the-art , 3 ) if someone assumes access to no extra data during training , then the maximum softmax probability+rotation prediction is the best ( please check the same argument that was made by the Anonymous Reviewer # 3 of the ICLR 2020 paper in the following link : https : //openreview.net/forum ? id=r1g6MCEtwr ) . In our initially submitted version , we had proposed a loss function that beat the results of [ 2 ] in both image and text classification tasks achieving state-of-the-results in the second category of methods which together with the novelty of our loss function showed the contribution of our technique . Understandably , all of the esteemed reviewers asked us to run more simulation experiments and compare our results with more methods , which resulted in significant improvements in the paper and proved the merits of our method . In the initial version of our paper , we had cited the seminal work of [ 1 ] stating that Mahalanobis distance-based classifier is a post-training method and it is not directly comparable to ours . Fortunately , the Anonymous Reviewer # 1 of our paper brought to our attention that we should verify whether the two methods are indeed compatible . Therefore , in the revised version of the paper , we ran additional image classification experiments where we trained the DNN with our proposed method and then we used the Mahalanobis method . We compared the results we obtained with those obtained by the original Mahalanobis distance-based classifier in [ 1 ] . We have dedicated a whole section in our paper ( please see Section 4.2 ) to this combination . The experimental results are presented in Table 4 of the paper where it is shown that our method combined with the Mahalanobis method outperforms the original Mahalanobis method in most of the experiments achieving state-of-the-art results even in the first category of methods for OOD detection . Furthermore , we added a discussion section ( just before Section 5 ) where we explain these experimental results . In our opinion , these results are significant because they present a very effective and flexible OOD detection method . Additionally , in order to further demonstrate the insights of our method we ran some additional experiments for CIFAR-10 and CIFAR-100 datasets where we show what the contribution of each regularization term in the OOD evaluation metrics is , as well as in the test accuracy of the DNN in examples generated by $ D_ { in } $ ( Please check the results of these experiments in Table 2 . ) . In its current state , the contributions of our paper are : 1 ) We propose a novel loss function for the OOD detection task consisting of two regularization terms , we thoroughly explain the theoretical and mathematical foundations of those regularization terms and we show experimentally the contribution of each term in both the OOD evaluation metrics as well as in the test accuracy on examples generated by $ D_ { in } $ . 2 ) We achieve state-of-the-art results in the OOD detection task for the class of methods that do not make any assumption about access to the test distribution by consistently outperforming the results of [ 2 ] in both image and text classification tasks ( see Tables 1,3,5 and 6 in the revised version of our paper ) . 3 ) We show the adaptability and the effectiveness of our method by combining it with the Mahalanobis method and by outperforming the original Mahalanobis method proposed in [ 1 ] , achieving state-of-the-art results in the OOD detection task even for the first category of methods for OOD ( see Section 4.2 and Table 4 in the revised version of our paper ) . We believe that the aforementioned arguments explain the novelty and the contribution of our paper in the field of OOD detection ."}}