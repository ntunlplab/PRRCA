{"year": "2021", "forum": "-N7PBXqOUJZ", "title": "Lipschitz Recurrent Neural Networks", "decision": "Accept (Poster)", "meta_review": "Solid work on extending AntisymmetricRNN and expanding its expressivity while controlling the global stability of the recurrent dynamics. It contributes to the growing interest in continuous-time RNN formulations that can deal with exploding gradient problem, and worthy of ICLR poster presentation. Three reviewers were positive and one was slightly negative. Authors added additional experiments and strengthened the manuscript significantly during the review process.", "reviews": [{"review_id": "-N7PBXqOUJZ-0", "review_text": "Considering a continuous time RNN with Lipschitz-continuous nonlinearity , the authors formulate sufficient conditions on the parameter matrices for the network to be globally stable , in the sense of a globally attracting fixed point . They provide a specific parameterization for the hidden-to-hidden weight matrices to control global stability and error gradients , consisting of a weighted combination of a symmetric and a skew-symmetric matrix ( and some diagonal offset ) . The authors discuss numerical integration by forward-Euler and RK2 , and thoroughly benchmark their approach against a large set of other state-of-the-art RNNs on various tasks including versions of MNIST and TIMIT . Finally , they highlight improved stability of their RNN against parameter and input perturbations . What I like about this paper is that it provides a solid theoretical basis and a principled , insightful parameterization that let \u2019 s one control separately the size of the real and the imaginary parts of the eigenvalues of A and W. The paper contains a number of interesting thoughts , and a really extensive comparison to other state-of-the-art models on several benchmarks . In general , I feel however that this work is relatively close to the 2019 ICLR paper by Chang et al . ; in several ways it feels like a more or less straightforward extension of this previous work . It also remains a bit unclear to me how it \u2019 s ensured in practice that the matrices obey to the required conditions in the training process . Sect.5 is not really about training , but just about numerically solving the ODE . From Appendix C it seems the scalar parameters $ \\beta , \\gamma $ controlling the influence of the symmetric vs. skew-symmetric parts and the offset are not learned at all but just fixed after grid-search ? The component matrices B , C , on the other hand it seems are not restricted at all but just initialized such that the theoretical conditions are likely , but not necessarily , met ? This seems somewhat unsatisfying as there are in fact no guarantees that the global stability conditions will be met in practice , and tuning the model may require ( potentially extensive ) meta-parameter search ? Another drawback in my mind is that enforcing global fixed point dynamics is quite restrictive . For instance , this rules out cycles and many other interesting dynamics in the model \u2019 s intrinsic behavior . Apparently , from the authors \u2019 empirical tests , this seems not to be required for solving this particular set of tasks . Which is somewhat puzzling to me as it appears this assumption should strongly curtail the model \u2019 s expressiveness . In the tables and figures I missed statistics . No standard errors or confidence bands were provided , or how many runs were performed . If it \u2019 s all from a single run , can I be certain the numbers are not just lucky draws ? Nevertheless , given the overall convincing and extensive empirical results , I \u2019 m slightly leaning toward acceptance . Minor issues : - The authors sell the additional linear term in their RNN as a novelty , while in fact it \u2019 s rather standard in continuous RNN ( older papers by Barak Pearlmutter , Song & Wang 2016 , arxiv.org/abs/2006.02427 , arxiv.org/abs/1910.03471 ) - RK2 is generally not sufficient for more involved ( stiff ) dynamical problems ; so the reason it works well here may lie in the fact that the model \u2019 s intrinsic dynamic is indeed very simple - What is an unstable unit ? I guess the authors mean that the RNN is not globally stable ? - Sect.2 , dynamical systems inspired RNN : It may be important to note that formulating a RNN as ODE does not solve the exploding/vanishing gradient or stability problem per se ( nor is it immediately clear to me why it should actually make it easier ) . - Theorem 1 : The $ \\sigma $ refer to the matrix eigenvalues in this case ? - What is a superset of skew-symmetric matrices ? - Second-to-last pg . of Sect.7 was unclear to me , i.e.what exactly was done and evaluated here , maybe cos I \u2019 m not familiar with some of the cited methods .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the detailed feedback and insights . We are pleased to see that the reviewer likes the paper and recognizes the contributions to the literature . There are a few comments raised that we would like to further comment on and hope to address . First , we acknowledge that our work took primary inspiration from Chang et al . ( 2019 ) .We found their particular choice of parameterization to be far too restrictive in practice , and our primary objective was to improve upon their ideas through a less restrictive parameterization that aligns with theory . * * The reviewer raises three good points in particular that we would like to comment on : * * ( 1 ) it can not be ensured that the hidden-to-hidden matrices remain in the \u201c stable \u201d region during training ; ( 2 ) enforcing global stability could be too restrictive ; and ( 3 ) the success of RK2 in practice seems to suggest that the resulting ODE is not stiff , and therefore can not have differentials with eigenvalues with large negative real parts . All are astute observations . We agree with ( 2 ) , strong global stability is indeed problematic and to be avoided . Indeed , strong global stability is linked to the vanishing gradient problem . However , when the eigenvalues have only small negative real parts ( sometimes called the edge of chaos ) , approximate cycles and other interesting behavior can still occur . Strong global stability would reveal stiff ODEs , so from ( 3 ) we know this does not occur during training . Indeed , regarding ( 1 ) , once beta and gamma have been appropriately chosen such that beta is relatively close to one and gamma ensures we lie in the stability region initially , experiments show that the real parts of the eigenvalues remain small and negative . Our newly added lemma also sheds some light on this , showing that taking beta close to one ensures that the real parts of the eigenvalues can not change by too much , even though the training and test losses do change significantly . In the extreme case where $ \\beta = 1 $ , there is no risk of leaving the stability region , and we revert to the work of Chang et al . ( 2019 ) but this significantly impacts expressivity . Achieving a good balance is still a matter of manually tuning these parameters . One could try to also optimize these parameters , but this proves too difficult in practice , and we have found it unnecessary . However , the strategy mentioned above prevents any search from becoming too extensive . * * Regarding the other comments : * * * Standard errors over multiple runs will be provided ; we can confirm the runs are not simply lucky . * An RNN is considered unstable if it is not globally stable * The continuous-time approach offers ( arguably ) simpler analysis and often larger stability regions . * The superset of skew-symmetric matrices considered here is the set of all matrices . * * References * * Chang , B. , Chen , M. , Haber , E. , & Chi , E. H. ( 2019 ) . AntisymmetricRNN : A Dynamical System View on Recurrent Neural Networks . In International Conference on Learning Representations ."}, {"review_id": "-N7PBXqOUJZ-1", "review_text": "The authors proposed a new continuous-time RNNs which appears to be extremely similar to CT-RNN ( Funahashi et al.1993 ) .They then constraint the network representation to account for learning long-term dependencies . Positive : The formulation of the constraint is very clear and sound . Positive : The analysis of the stability of the model and other properties is rigor enough and to the best of my knowledge sound and correct . Negative : From the experimental setting , reported values in tables , and code , it seems like the authors tuned the hyperparameters on the test set which I consider a bad practice that violates the code of conduct ! I suspected this , and therefore , ran the code myself . With a few changes to the hyperparameters from the tuned one reported in the table the performance of the proposed model dropped significantly ! I would suggest the authors to create a fair testing scheme for all baselines and report the experimental results more accurately .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for their comments . We appreciate that the reviewer agrees that the proposed formulation of the constraint is clear and sound and that the analysis of the stability of the model and other properties is rigorous and correct . We would also like to thank the reviewer for running our provided code and verifying that a reader is able to reproduce the experiments . However , we strongly disagree with the reviewer that our experiments violate the code of ethics please see the discussion of our training procedure below . * * CT-RNN formulations * * There is a broad class of continuous-time RNN formulations in the theoretical literature ; see our related work section and in particular Zhang et al . ( 2014 ) for a comprehensive survey of continuous-time RNNs and their stability properties . The CT-RNN model in Funahashi et al. \u2018 93 is of the Hopfield form , and therefore can be recast into the Cohen-Grossberg form , which we discuss in Section 3 . We have provided a direct stability analysis of our particular formulation , and relate this to the Cohen-Grossberg form in Lemma 1 . * * Training procedure * * We utilized a standard training procedure using a non-exhaustive random search within plausible ranges . We will add the ranges used in the search to the existing table of final values in the appendix . To ensure fair comparisons , we have adopted the experimental procedure in the open source implementations of published papers : For MNIST and TIMIT we adapted the code provided by [ 1 ] using the following repository https : //github.com/Lezcano/expRNN . For the language modeling tasks we adapted code by [ 2 ] using the following repository https : //github.com/KyleGoyette/nnRNN . For tasks such as TIMIT and the language modeling tasks , our implementation uses the provided validation set . Many accepted papers , including the implementation of [ 1 ] which we used as a baseline , often do not use a validation set for MNIST since it is known that there is no distribution shift between the training and test sets . Furthermore , we do not believe the optimization procedure is prone to overfitting as we are using a fixed stopping scheme ; i.e. , we reported the results of the model obtained after a fixed number of epochs dictated by a standard learning rate schedule . Nevertheless , we are quite happy to create a validation set for MNIST from the training data . * * Sensitivity to hyperparameters * * We are glad that the reviewer was able to run our implementation and reproduce the results . We welcome the reviewer to elaborate on the specific changes during the discussion period so that we may better address these . However , we are confident that our model is not any more sensitive than any other . With respect to the hyperparameters that are introduced by our architecture , Figure 3b shows a detailed ablation study for \\beta showing the model performance across the range of valid values , as bounded in the derivation . * * References * * [ 1 ] Lezcano-Casado , M. , and Mart\u0131\u0301nez-Rubio , D. `` Cheap Orthogonal Constraints in Neural Networks : A Simple Parametrization of the Orthogonal and Unitary Group . '' International Conference on Machine Learning . 2019 . [ 2 ] Kerg , G. , Goyette , K. , Puelma Touzel , M. , Gidel , G. , Vorontsov , E. , Bengio , Y. , & Lajoie , G. `` Non-normal Recurrent Neural Network ( nnRNN ) : learning long time dependencies while improving expressivity with transient dynamics . '' Advances in Neural Information Processing Systems . 2019 ."}, {"review_id": "-N7PBXqOUJZ-2", "review_text": "I have increased my score to reflect the revisions This paper presents yet another architecture for fully connected RNNs or infinitely deep networks based on the integration of a continuous time dynamical system , where a projection of the weights is used to guarantee stability , hence a fixed point and a finite Lipschitz constant . Positives : The maths presented in the paper is correct and their results are nicer than the considered ( not exhaustive ) baselines . Negatives : 1 ) This idea has been widely explored and exploited by now . Adding a linear term and using a different solver is not enough in my opinion to make an innovative contribution . If you wish to present this as an ablation study , then perhaps you need to benchmark against existing solutions . For instance , in this ( missing ) reference a very similar network is presented and analysed . @ incollection { NIPS2018_7566 , title = { NAIS-Net : Stable Deep Networks from Non-Autonomous Differential Equations } , author = { Ciccone , Marco and Gallieri , Marco and Masci , Jonathan and Osendorfer , Christian and Gomez , Faustino } , booktitle = { Advances in Neural Information Processing Systems 31 } , pages = { 3025 -- 3035 } , year = { 2018 } , publisher = { Curran Associates , Inc. } , url = { http : //papers.nips.cc/paper/7566-nais-net-stable-deep-networks-from-non-autonomous-differential-equations.pdf } } I will refer to this as [ 1 ] . While this paper was about unrolling the stable RNN to generate a deep Lipschitz classifier , and was not used for sequence to sequence task , the architecture you propose and the claims are so much similar to [ 1 ] that this demands for a direct comparison . In the above paper , stability projections are presented for an architecture that is essentially the same , minus the additional linear component here . Paper [ 1 ] should be included as a baseline . You have it already implemented for fully connected layers . 2 ) It is not very clear how this additional linear component would help in practise , as your architecture is fundamentally discretised with Euler which results in yet another generalised res-net . It has been shown that ResNet works much better than their predecessor , Highway networks , because of the direct skip connection and better gradient flow . While the missing reference [ 1 ] ( NAIS-Net ) preserves that connection , It feels like your linear term would get rid of the skip connection and prevent the technique from being used in very deep networks or very long sequences due to vanishing gradients . 3 ) What is the motivation for using a continuos-time approach ? Your results are invalidated by the forward Euler , unless a very small hyperparameter epsilon is introduced . Why not just compute the projection in discrete time as done in [ 1 ] . Your stability will not hold for sparse in time data-points , because the Euler step would become too big and this is effectively an RNN that can not handle different sampling time while preserving stability . 4 ) It seems strange to compare to NODEs as they are meant to be used for something else . In particular , NODEs are designed to work without inputs but just by estimating the initial condition for the ODE and then `` unrolling '' . If you add input signals to the NODE , which I guess is what you mean by `` NODE RNN '' , how do you train it with the adjoint method ? This should be made very clear in the paper . 5 ) Your results are limited to a fully connected architecture , while [ 1 ] has shown a method to have a Lipschitz RNN for convolutional layers . Can you generalize to that as well ? I do n't feel the contribution here is relevant enough to be included in the conference . If the above points are clarified in a convincing way and both the theoretical justification and the ablations performed with respect to [ 1 ] , then I could consider improving my score .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for reading the paper and providing a detailed review . Regarding the reviewer \u2019 s summary of our work : \u201c This paper presents yet another architecture for fully connected RNNs or infinitely deep networks based on the integration of a continuous time dynamical system , where a projection of the weights is used to guarantee stability , hence a fixed point and a finite Lipschitz constant. \u201d * There have been many papers ( indeed many published long before the reference [ 1 ] ) that have investigated RNNs from the continuous-time perspective ( see our references ) , so it seems unfair to dismiss our work based solely on this fact without mentioning our contributions . We feel that reviewer # 2 gave an especially accurate summary of our work . * We emphasize that we do not consider an arbitrary projection of the weights . In particular , we argue in the text that arbitrary projections will inevitably negatively impact expressivity . Instead , we consider a parameterization that better supports stability during training while being flexible enough so that highly expressive models are attained . Our choice of parametrization makes it one of the primary features of our computational model . 1 . * * What are the innovative contributions ? * * We would like to stress that adding a linear term and using a different solver are not our contributions . Among others , our key contributions are the derivation of sufficient conditions for stability of our model and a novel skew-symmetric scheme that can be used to control the vanishing and exploding gradients problems . The additional linear term acts as a stabilizer , and its presence is valuable for establishing global stability criteria for the system without sacrificing expressivity . The fully connected NAIS-Net model reads $ x ( k+1 ) = x ( k ) + h \\sigma ( A x ( k ) + B u + b ) $ , and incorporates a particular projection scheme to maintain stability . In the original paper [ 1 ] , this model is not formulated to deal with sequential data . ( They considered whole-image tasks , as opposed to pixel-by-pixel tasks . ) If we were to adapt their formulation in the obvious way to allow for sequential input ( so that $ x ( k+1 ) = x ( k ) + h \\sigma ( A x ( k ) + B u ( k ) + b ) $ ) , then it would be equivalent to an Euler discretization of our general model without specific parameterization of the hidden-to-hidden matrices and without the linear term . Training this model with the same projection scheme in [ 1 ] , we have not been able to achieve above 90 % accuracy on pixel-by-pixel MNIST . Hence , we do not feel it is as good a baseline as LSTMs . However , if the reviewer still believes the comparison is merited , we are happy to include it for the two MNIST tasks . Also , we are happy to include a reference to the paper in the related work section ."}, {"review_id": "-N7PBXqOUJZ-3", "review_text": "The paper presents some novel contributions regarding recurrent neural networks . Building on the work of Chang et al . ( 2019 ) , the authors provide a global convergence result for the hidden representation of a family of recurrent neural networks using standard techniques from the Lyapunov analysis of dynamical systems . The requirements of the theorem are met ( within the limits of discretization ) by their proposed algorithmic scheme . Numerical evaluation on a variety of benchmarks shows that the proposed algorithm yields systematic improvement over other RNN approaches . For all of the above reasons , I recommend the acceptance of the paper . Some concerns to be addressed : - The connection between stability and trainability or refer to Chang et al . ( 2019 ) if their analysis applies here . - specify the functions \\sigma_min and \\sigma_max used in Theorem 1 - specify the meaning of the one-arg function f ( h^ * ) as opposed to the 2-arg f ( h , t ) appearing in Definition 1 .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for this positive appraisal of our work . Regarding the mentioned issue of stability during training , the analyses of Chang et al . ( 2019 ) do not apply because we can not guarantee ( with probability one ) that the process will remain stable during training . However , we will include an additional lemma that shows as $ \\beta \\to 1 $ , regardless of the integration scheme , the process will remain stable with higher probability during training , provided the initial parameters lie within the stable region . For more details on this , please see our response to Reviewer # 2 . Regarding the other comments : - Definitions for sigma_min and sigma_max as minimum and maximum spectral values will be provided in the updated manuscript . - The one-arg function $ f ( h^ * ) $ in Definition 1 is a typo , thank you for pointing this out . It should read $ f ( h^ * , t ) = 0 $ ."}], "0": {"review_id": "-N7PBXqOUJZ-0", "review_text": "Considering a continuous time RNN with Lipschitz-continuous nonlinearity , the authors formulate sufficient conditions on the parameter matrices for the network to be globally stable , in the sense of a globally attracting fixed point . They provide a specific parameterization for the hidden-to-hidden weight matrices to control global stability and error gradients , consisting of a weighted combination of a symmetric and a skew-symmetric matrix ( and some diagonal offset ) . The authors discuss numerical integration by forward-Euler and RK2 , and thoroughly benchmark their approach against a large set of other state-of-the-art RNNs on various tasks including versions of MNIST and TIMIT . Finally , they highlight improved stability of their RNN against parameter and input perturbations . What I like about this paper is that it provides a solid theoretical basis and a principled , insightful parameterization that let \u2019 s one control separately the size of the real and the imaginary parts of the eigenvalues of A and W. The paper contains a number of interesting thoughts , and a really extensive comparison to other state-of-the-art models on several benchmarks . In general , I feel however that this work is relatively close to the 2019 ICLR paper by Chang et al . ; in several ways it feels like a more or less straightforward extension of this previous work . It also remains a bit unclear to me how it \u2019 s ensured in practice that the matrices obey to the required conditions in the training process . Sect.5 is not really about training , but just about numerically solving the ODE . From Appendix C it seems the scalar parameters $ \\beta , \\gamma $ controlling the influence of the symmetric vs. skew-symmetric parts and the offset are not learned at all but just fixed after grid-search ? The component matrices B , C , on the other hand it seems are not restricted at all but just initialized such that the theoretical conditions are likely , but not necessarily , met ? This seems somewhat unsatisfying as there are in fact no guarantees that the global stability conditions will be met in practice , and tuning the model may require ( potentially extensive ) meta-parameter search ? Another drawback in my mind is that enforcing global fixed point dynamics is quite restrictive . For instance , this rules out cycles and many other interesting dynamics in the model \u2019 s intrinsic behavior . Apparently , from the authors \u2019 empirical tests , this seems not to be required for solving this particular set of tasks . Which is somewhat puzzling to me as it appears this assumption should strongly curtail the model \u2019 s expressiveness . In the tables and figures I missed statistics . No standard errors or confidence bands were provided , or how many runs were performed . If it \u2019 s all from a single run , can I be certain the numbers are not just lucky draws ? Nevertheless , given the overall convincing and extensive empirical results , I \u2019 m slightly leaning toward acceptance . Minor issues : - The authors sell the additional linear term in their RNN as a novelty , while in fact it \u2019 s rather standard in continuous RNN ( older papers by Barak Pearlmutter , Song & Wang 2016 , arxiv.org/abs/2006.02427 , arxiv.org/abs/1910.03471 ) - RK2 is generally not sufficient for more involved ( stiff ) dynamical problems ; so the reason it works well here may lie in the fact that the model \u2019 s intrinsic dynamic is indeed very simple - What is an unstable unit ? I guess the authors mean that the RNN is not globally stable ? - Sect.2 , dynamical systems inspired RNN : It may be important to note that formulating a RNN as ODE does not solve the exploding/vanishing gradient or stability problem per se ( nor is it immediately clear to me why it should actually make it easier ) . - Theorem 1 : The $ \\sigma $ refer to the matrix eigenvalues in this case ? - What is a superset of skew-symmetric matrices ? - Second-to-last pg . of Sect.7 was unclear to me , i.e.what exactly was done and evaluated here , maybe cos I \u2019 m not familiar with some of the cited methods .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the detailed feedback and insights . We are pleased to see that the reviewer likes the paper and recognizes the contributions to the literature . There are a few comments raised that we would like to further comment on and hope to address . First , we acknowledge that our work took primary inspiration from Chang et al . ( 2019 ) .We found their particular choice of parameterization to be far too restrictive in practice , and our primary objective was to improve upon their ideas through a less restrictive parameterization that aligns with theory . * * The reviewer raises three good points in particular that we would like to comment on : * * ( 1 ) it can not be ensured that the hidden-to-hidden matrices remain in the \u201c stable \u201d region during training ; ( 2 ) enforcing global stability could be too restrictive ; and ( 3 ) the success of RK2 in practice seems to suggest that the resulting ODE is not stiff , and therefore can not have differentials with eigenvalues with large negative real parts . All are astute observations . We agree with ( 2 ) , strong global stability is indeed problematic and to be avoided . Indeed , strong global stability is linked to the vanishing gradient problem . However , when the eigenvalues have only small negative real parts ( sometimes called the edge of chaos ) , approximate cycles and other interesting behavior can still occur . Strong global stability would reveal stiff ODEs , so from ( 3 ) we know this does not occur during training . Indeed , regarding ( 1 ) , once beta and gamma have been appropriately chosen such that beta is relatively close to one and gamma ensures we lie in the stability region initially , experiments show that the real parts of the eigenvalues remain small and negative . Our newly added lemma also sheds some light on this , showing that taking beta close to one ensures that the real parts of the eigenvalues can not change by too much , even though the training and test losses do change significantly . In the extreme case where $ \\beta = 1 $ , there is no risk of leaving the stability region , and we revert to the work of Chang et al . ( 2019 ) but this significantly impacts expressivity . Achieving a good balance is still a matter of manually tuning these parameters . One could try to also optimize these parameters , but this proves too difficult in practice , and we have found it unnecessary . However , the strategy mentioned above prevents any search from becoming too extensive . * * Regarding the other comments : * * * Standard errors over multiple runs will be provided ; we can confirm the runs are not simply lucky . * An RNN is considered unstable if it is not globally stable * The continuous-time approach offers ( arguably ) simpler analysis and often larger stability regions . * The superset of skew-symmetric matrices considered here is the set of all matrices . * * References * * Chang , B. , Chen , M. , Haber , E. , & Chi , E. H. ( 2019 ) . AntisymmetricRNN : A Dynamical System View on Recurrent Neural Networks . In International Conference on Learning Representations ."}, "1": {"review_id": "-N7PBXqOUJZ-1", "review_text": "The authors proposed a new continuous-time RNNs which appears to be extremely similar to CT-RNN ( Funahashi et al.1993 ) .They then constraint the network representation to account for learning long-term dependencies . Positive : The formulation of the constraint is very clear and sound . Positive : The analysis of the stability of the model and other properties is rigor enough and to the best of my knowledge sound and correct . Negative : From the experimental setting , reported values in tables , and code , it seems like the authors tuned the hyperparameters on the test set which I consider a bad practice that violates the code of conduct ! I suspected this , and therefore , ran the code myself . With a few changes to the hyperparameters from the tuned one reported in the table the performance of the proposed model dropped significantly ! I would suggest the authors to create a fair testing scheme for all baselines and report the experimental results more accurately .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for their comments . We appreciate that the reviewer agrees that the proposed formulation of the constraint is clear and sound and that the analysis of the stability of the model and other properties is rigorous and correct . We would also like to thank the reviewer for running our provided code and verifying that a reader is able to reproduce the experiments . However , we strongly disagree with the reviewer that our experiments violate the code of ethics please see the discussion of our training procedure below . * * CT-RNN formulations * * There is a broad class of continuous-time RNN formulations in the theoretical literature ; see our related work section and in particular Zhang et al . ( 2014 ) for a comprehensive survey of continuous-time RNNs and their stability properties . The CT-RNN model in Funahashi et al. \u2018 93 is of the Hopfield form , and therefore can be recast into the Cohen-Grossberg form , which we discuss in Section 3 . We have provided a direct stability analysis of our particular formulation , and relate this to the Cohen-Grossberg form in Lemma 1 . * * Training procedure * * We utilized a standard training procedure using a non-exhaustive random search within plausible ranges . We will add the ranges used in the search to the existing table of final values in the appendix . To ensure fair comparisons , we have adopted the experimental procedure in the open source implementations of published papers : For MNIST and TIMIT we adapted the code provided by [ 1 ] using the following repository https : //github.com/Lezcano/expRNN . For the language modeling tasks we adapted code by [ 2 ] using the following repository https : //github.com/KyleGoyette/nnRNN . For tasks such as TIMIT and the language modeling tasks , our implementation uses the provided validation set . Many accepted papers , including the implementation of [ 1 ] which we used as a baseline , often do not use a validation set for MNIST since it is known that there is no distribution shift between the training and test sets . Furthermore , we do not believe the optimization procedure is prone to overfitting as we are using a fixed stopping scheme ; i.e. , we reported the results of the model obtained after a fixed number of epochs dictated by a standard learning rate schedule . Nevertheless , we are quite happy to create a validation set for MNIST from the training data . * * Sensitivity to hyperparameters * * We are glad that the reviewer was able to run our implementation and reproduce the results . We welcome the reviewer to elaborate on the specific changes during the discussion period so that we may better address these . However , we are confident that our model is not any more sensitive than any other . With respect to the hyperparameters that are introduced by our architecture , Figure 3b shows a detailed ablation study for \\beta showing the model performance across the range of valid values , as bounded in the derivation . * * References * * [ 1 ] Lezcano-Casado , M. , and Mart\u0131\u0301nez-Rubio , D. `` Cheap Orthogonal Constraints in Neural Networks : A Simple Parametrization of the Orthogonal and Unitary Group . '' International Conference on Machine Learning . 2019 . [ 2 ] Kerg , G. , Goyette , K. , Puelma Touzel , M. , Gidel , G. , Vorontsov , E. , Bengio , Y. , & Lajoie , G. `` Non-normal Recurrent Neural Network ( nnRNN ) : learning long time dependencies while improving expressivity with transient dynamics . '' Advances in Neural Information Processing Systems . 2019 ."}, "2": {"review_id": "-N7PBXqOUJZ-2", "review_text": "I have increased my score to reflect the revisions This paper presents yet another architecture for fully connected RNNs or infinitely deep networks based on the integration of a continuous time dynamical system , where a projection of the weights is used to guarantee stability , hence a fixed point and a finite Lipschitz constant . Positives : The maths presented in the paper is correct and their results are nicer than the considered ( not exhaustive ) baselines . Negatives : 1 ) This idea has been widely explored and exploited by now . Adding a linear term and using a different solver is not enough in my opinion to make an innovative contribution . If you wish to present this as an ablation study , then perhaps you need to benchmark against existing solutions . For instance , in this ( missing ) reference a very similar network is presented and analysed . @ incollection { NIPS2018_7566 , title = { NAIS-Net : Stable Deep Networks from Non-Autonomous Differential Equations } , author = { Ciccone , Marco and Gallieri , Marco and Masci , Jonathan and Osendorfer , Christian and Gomez , Faustino } , booktitle = { Advances in Neural Information Processing Systems 31 } , pages = { 3025 -- 3035 } , year = { 2018 } , publisher = { Curran Associates , Inc. } , url = { http : //papers.nips.cc/paper/7566-nais-net-stable-deep-networks-from-non-autonomous-differential-equations.pdf } } I will refer to this as [ 1 ] . While this paper was about unrolling the stable RNN to generate a deep Lipschitz classifier , and was not used for sequence to sequence task , the architecture you propose and the claims are so much similar to [ 1 ] that this demands for a direct comparison . In the above paper , stability projections are presented for an architecture that is essentially the same , minus the additional linear component here . Paper [ 1 ] should be included as a baseline . You have it already implemented for fully connected layers . 2 ) It is not very clear how this additional linear component would help in practise , as your architecture is fundamentally discretised with Euler which results in yet another generalised res-net . It has been shown that ResNet works much better than their predecessor , Highway networks , because of the direct skip connection and better gradient flow . While the missing reference [ 1 ] ( NAIS-Net ) preserves that connection , It feels like your linear term would get rid of the skip connection and prevent the technique from being used in very deep networks or very long sequences due to vanishing gradients . 3 ) What is the motivation for using a continuos-time approach ? Your results are invalidated by the forward Euler , unless a very small hyperparameter epsilon is introduced . Why not just compute the projection in discrete time as done in [ 1 ] . Your stability will not hold for sparse in time data-points , because the Euler step would become too big and this is effectively an RNN that can not handle different sampling time while preserving stability . 4 ) It seems strange to compare to NODEs as they are meant to be used for something else . In particular , NODEs are designed to work without inputs but just by estimating the initial condition for the ODE and then `` unrolling '' . If you add input signals to the NODE , which I guess is what you mean by `` NODE RNN '' , how do you train it with the adjoint method ? This should be made very clear in the paper . 5 ) Your results are limited to a fully connected architecture , while [ 1 ] has shown a method to have a Lipschitz RNN for convolutional layers . Can you generalize to that as well ? I do n't feel the contribution here is relevant enough to be included in the conference . If the above points are clarified in a convincing way and both the theoretical justification and the ablations performed with respect to [ 1 ] , then I could consider improving my score .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for reading the paper and providing a detailed review . Regarding the reviewer \u2019 s summary of our work : \u201c This paper presents yet another architecture for fully connected RNNs or infinitely deep networks based on the integration of a continuous time dynamical system , where a projection of the weights is used to guarantee stability , hence a fixed point and a finite Lipschitz constant. \u201d * There have been many papers ( indeed many published long before the reference [ 1 ] ) that have investigated RNNs from the continuous-time perspective ( see our references ) , so it seems unfair to dismiss our work based solely on this fact without mentioning our contributions . We feel that reviewer # 2 gave an especially accurate summary of our work . * We emphasize that we do not consider an arbitrary projection of the weights . In particular , we argue in the text that arbitrary projections will inevitably negatively impact expressivity . Instead , we consider a parameterization that better supports stability during training while being flexible enough so that highly expressive models are attained . Our choice of parametrization makes it one of the primary features of our computational model . 1 . * * What are the innovative contributions ? * * We would like to stress that adding a linear term and using a different solver are not our contributions . Among others , our key contributions are the derivation of sufficient conditions for stability of our model and a novel skew-symmetric scheme that can be used to control the vanishing and exploding gradients problems . The additional linear term acts as a stabilizer , and its presence is valuable for establishing global stability criteria for the system without sacrificing expressivity . The fully connected NAIS-Net model reads $ x ( k+1 ) = x ( k ) + h \\sigma ( A x ( k ) + B u + b ) $ , and incorporates a particular projection scheme to maintain stability . In the original paper [ 1 ] , this model is not formulated to deal with sequential data . ( They considered whole-image tasks , as opposed to pixel-by-pixel tasks . ) If we were to adapt their formulation in the obvious way to allow for sequential input ( so that $ x ( k+1 ) = x ( k ) + h \\sigma ( A x ( k ) + B u ( k ) + b ) $ ) , then it would be equivalent to an Euler discretization of our general model without specific parameterization of the hidden-to-hidden matrices and without the linear term . Training this model with the same projection scheme in [ 1 ] , we have not been able to achieve above 90 % accuracy on pixel-by-pixel MNIST . Hence , we do not feel it is as good a baseline as LSTMs . However , if the reviewer still believes the comparison is merited , we are happy to include it for the two MNIST tasks . Also , we are happy to include a reference to the paper in the related work section ."}, "3": {"review_id": "-N7PBXqOUJZ-3", "review_text": "The paper presents some novel contributions regarding recurrent neural networks . Building on the work of Chang et al . ( 2019 ) , the authors provide a global convergence result for the hidden representation of a family of recurrent neural networks using standard techniques from the Lyapunov analysis of dynamical systems . The requirements of the theorem are met ( within the limits of discretization ) by their proposed algorithmic scheme . Numerical evaluation on a variety of benchmarks shows that the proposed algorithm yields systematic improvement over other RNN approaches . For all of the above reasons , I recommend the acceptance of the paper . Some concerns to be addressed : - The connection between stability and trainability or refer to Chang et al . ( 2019 ) if their analysis applies here . - specify the functions \\sigma_min and \\sigma_max used in Theorem 1 - specify the meaning of the one-arg function f ( h^ * ) as opposed to the 2-arg f ( h , t ) appearing in Definition 1 .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for this positive appraisal of our work . Regarding the mentioned issue of stability during training , the analyses of Chang et al . ( 2019 ) do not apply because we can not guarantee ( with probability one ) that the process will remain stable during training . However , we will include an additional lemma that shows as $ \\beta \\to 1 $ , regardless of the integration scheme , the process will remain stable with higher probability during training , provided the initial parameters lie within the stable region . For more details on this , please see our response to Reviewer # 2 . Regarding the other comments : - Definitions for sigma_min and sigma_max as minimum and maximum spectral values will be provided in the updated manuscript . - The one-arg function $ f ( h^ * ) $ in Definition 1 is a typo , thank you for pointing this out . It should read $ f ( h^ * , t ) = 0 $ ."}}