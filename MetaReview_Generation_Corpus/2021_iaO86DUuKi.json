{"year": "2021", "forum": "iaO86DUuKi", "title": "Conservative Safety Critics for Exploration", "decision": "Accept (Poster)", "meta_review": "Summary: \nThis paper introduces a different, interesting definition of safety in RL. The paper does a nice job of showing success with empirical results and providing bounds. I think it provides a nice contribution to the field.\n\nDiscussion:\nThe reviewers agree this paper should be accepted. The initial points brought up against the paper have been successfully addressed or mitigated. ", "reviews": [{"review_id": "iaO86DUuKi-0", "review_text": "This paper would like to address the problem of `` `` safe exploration '' with a conservative estimation of the environment . Although the problem seems reasonable , I have the following several concerns on this paper : - Will the safety constraints be revealed to the agent ? Standard RL assumes the reward is not revealed to the agent , and I feel the safety constraints can not be revealed to the agent in prior as well . - If the safety constraints is not revealed to the agent , then how to train the safety critic ? In my opinion , when the agent collect the data that fail catastrophically , then the exploration is already not safe . - I feel the derivation part in Section 3 is not so clear , as several notations have been introduced without explanation , though it is not hard to understand . - I don \u2019 t understand what Lemma 1 and Theorem 1 can indicate in practice . When we estimate V_C , we already satisfies several catastrophic failures , otherwise we will know nothing about the possible failure . This can also be seen from the traditional exploration analysis . And traditional exploration analysis also says when we have sufficient number of data , the probability that we suffer from catastrophic failure will decrease . In one word , I don \u2019 t see any significant idea of `` safe exploration \u2019 \u2019 from Lemma 1 and Theorem 1 . - Also , from the traditional exploration analysis , we have the regret lower bound , which says we must suffer from failure to get the best policy . We also have algorithms to match these regret lower bound on some simplified MDP settings , and even stronger guarantee that with high probability we will make small mistake when we have sufficient samples ( see the mistake version of IPOC bound in [ 1 ] ) . I don \u2019 t feel the current paper present such kinds of guarantee . A summarize of point 4 & 5 : to me I don \u2019 t feel the authors well-defined the ` `` safe exploration '' and give a rigorous analysis on the `` the safety guarantee . For me , I prefer to define the `` safe exploration '' as minimizing the total failure when finding the optimal policy , but no matter how to define it , I feel what the authors have done is not `` safe '' . I feel Theorem 2 is a straightforward adaption of the existing results in [ 2 ] . Though seems correct , the authors argue that Theorem 2 shows a tradeoff between safety and convergence , which I can not agree . If we want the policy to converge to the global optimum , then we need to collect samples from the dangerous region ( otherwise we can not know if that \u2019 s dangerous and probably we can get some improvement at that region ) , how to keep `` safe '' at that time ? Also , a slow convergence will need to run for a longer time and thus need more samples , which can enlarge the probability of catastrophic failure , right ? Overall , though the empirical performance on some benchmark shows the proposed method is better than some of the previous methods , I don \u2019 t think the proposed methods really address the issue of the safe exploration . The authors does not formally define what is the safe exploration ( and even little about exploration in fact ) , and the theoretical analysis also doesn \u2019 t address any of the safety issues . From my point of view , to achieve good performance , one must suffer from some failures , and our goal should be minimizing the number of failures , rather than minimizing the failure probability of each turn . Unfortunately , the authors does not characterize all of these issues I care about . I would like to say , considering the unsatisfactory of the problem description and theoretical analysis , I don \u2019 t think this paper is suitable for publication . [ 1 ] Dann , Christoph , et al . `` Policy certificates : Towards accountable reinforcement learning . '' International Conference on Machine Learning . PMLR , 2019 . [ 2 ] Agarwal , Alekh , et al . `` Optimality and approximation with policy gradient methods in markov decision processes . '' arXiv preprint arXiv:1908.00261 ( 2019 ) . The authors answers one of the most important concerns , I have raised score to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "due to space constraint , we are creating another comment . * * Though seems correct , the authors argue that Theorem 2 shows a tradeoff between safety and convergence , which I can not agree . If we want the policy to converge to the global optimum , then we need to collect samples from the dangerous region ( otherwise we can not know if that \u2019 s dangerous and probably we can get some improvement at that region ) , how to keep `` safe '' at that time ? Also , a slow convergence will need to run for a longer time and thus need more samples , which can enlarge the probability of catastrophic failure , right ? * * Since we bound the probability of failure after every policy update ( Theorem 1 ) , the total failures until convergence is also bounded . In contrast , although the unconstrained problem might converge to the optimal policy with respect to task performance faster , it incurs significantly more failures as there is no bound on the probability of failure during training.The slower convergence rate of the task value function ( Theorem 2 relates to how quickly the policy converges to the optimal policy in terms of task performance ) is in relation to the unconstrained RL algorithm that does not take safety constraints into account during training . We also verify this empirically in Fig 3 where we see that although the unconstrained RL algorithm ( Base ) converges quickly in terms of task reward , it incurs more failures . [ 1 ] Altman , Eitan . Constrained Markov decision processes . Vol.7.CRC Press , 1999 . [ 2 ] Achiam , J. , Held , D. , Tamar , A. and Abbeel , P. , 2017 . Constrained policy optimization . arXiv preprint arXiv:1705.10528 . [ 3 ] Wachi , A. and Sui , Y. , 2020 . Safe reinforcement learning in constrained markov decision processes . arXiv preprint arXiv:2008.06626 . [ 4 ] Chow , Y. , Nachum , O. , Faust , A. , Duenez-Guzman , E. , and Ghavamzadeh , M. Lyapunov-based safe policy optimization for continuous control . arXiv preprint arXiv:1901.10031 , 2019 . [ 5 ] Garc\u0131a , J. and Fern\u00e1ndez , F. , 2015 . A comprehensive survey on safe reinforcement learning . Journal of Machine Learning Research , 16 ( 1 ) , pp.1437-1480 ."}, {"review_id": "iaO86DUuKi-1", "review_text": "In this submission the authors are trying to tackle the very important problem of safe RL with safety guarantees . The problem formulation is rather clear , and the paper is overall well written . The main idea is to formulate the safe RL problem as a CMDP problem , but with worst-case bounds to ensure that the safety constrained is guaranteed throughout the learning . The general idea is fine , albeit not new ( please check/compare against recent works on robust-CMDPs ) , however , I do have several issues with the lack of rigor in the mathematical proofs , as well as the rather amplified statements about safety guarantees 'throughout learning ' . Indeed , the CMDP problem ( or its worst-case bound ) is solved using a Lagrangian formulation , which is well-know as a soft constraints formulation , i.e. , you do not have guaranteed safety during learning , as claimed in the Introduction ( paragraph 3 ) , and throughout the paper . In that case , the dramatic drone motivational example used in the Introduction ( paragraph 1 ) is and exaggeration , i.e. , this method will lead to a crash too . Besides , mathematically , many of your variables are not defined , not even in the appendix , e.g. , in equation ( 2 ) $ \\hat { B } $ is not defined ; $ \\alpha $ which seems a key tuning parameter in Theorem 1 is not defined , etc . Most importantly , the authors keep referring to the paper about CPO ( Achiam et al 17 ) to support their proofs and technical derivations , whereas that paper is about continuous constraints costs . This paper on the other hand , is clearly using discontinuous constraints , not even $ C^ { 0 } $ , and thus one can not just use parts of the results in ( Achiam et al 17 ) without further carful examination of the technical challenges introduced with a discontinuous cost function , e.g. , you are using some Taylor developments throughout , while these only make sense for analytic functions , etc . Furthermore , the probability bounds proposed in Theorem 1 and 2 seem to be rater weak bounds , since $ \\xi $ is bounded by a term inversely proportional to the confidence parameter $ \\omega $ , which means the probability of being safe is high when the safety bound is loose , i.e. , $ \\chi+\\xi $ , for $ \\xi\\rightarrow +\\infty $ . Similarly , for tight safety bound , i.e. , $ \\chi+\\xi $ , for $ \\xi\\rightarrow 0 $ , the safety probability drops to zero . The authors seem to minimize this point by the 'proper tuning ' of $ \\alpha $ which remains a mysterious parameter ( even after checking the proofs in Appendix ) . This is all to say that the authors have to tone down their statements about guaranteed provable safety bound , quite a bit . Finally , the numerical simulations are interesting , but only confirm my point about the fact that the obtained safety is asymptotic only , i.e. , in steady state , and absolutely not during learning . This is clear from the plots in Figure 3 ( bottom ) where we see that in average the proposed algorithm necessitates more than 1000 iteration before reaching a truly safe behavior . One can also note that the CPO algorithm behaves almost similarly to the proposed algorithm , when tested on the car navigation example , and the laikago robot ( Fig.3 , bottom number 3 and 4 ) . One also wonders why in the first set of tests , in Fig.3- top number 2 , one can reach a performance cost of 20 with the proposed method , while it seems to plato at 10 in the second set of experiments , in Fig.4-top number 2 . Another point that is worth clarifying is that the tests in Fig.4-top number 1 , we see that the performance cost reaches 10 for the proposed method with large safety constraint bound ( 0.2 ) , which is intuitively an almost unconstrained case . However , the unconstrained algorithm 'Base ' in the first tests , Fig.3-top number 1 , the Base algorithm does not achieve a similar performance , could it be better tuned in that case ? In summary , I found the paper well written , but it needs to be carefully revised for technical rigor , and toned down in terms of what is really achieved here ( maybe somehow safer CMDP algorithm but definitely not safe during exploration ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Author response ( part 2/2 ) : Response to other comments * * * * In that case , the dramatic drone motivational example used in the Introduction ( paragraph 1 ) is and exaggeration , i.e. , this method will lead to a crash too . * * We have now replaced this example in the revised paper . Thank you for pointing this out . * * Besides , mathematically , many of your variables are not defined , not even in the appendix , e.g. , $ \\hat { B } ^ { \\pi_\\phi } $ in equation ( 2 ) is not defined ; $ \\alpha $ which seems a key tuning parameter in Theorem 1 is not defined , etc . * * Thank you for spotting this - we have defined the Bellman operator $ \\hat { B } ^ { \\pi_\\phi } $ in the revised paper . Kindly note that we have the minimum theoretically necessary value of $ \\alpha $ defined in equation 27 of the Appendix . We have now included a line about $ \\alpha $ being a parameter that weighs the first term in equation ( 2 ) . It is inherited from the CQL paper ( Kumar et al.2020 ) . * * The authors seem to minimize this point by the 'proper tuning ' of which remains a mysterious parameter ( even after checking the proofs in Appendix ) . * * We request you to kindly refer equation ( 27 ) in the Appendix where we had derived the minimum theoretically specified value of $ \\alpha $ for obtaining a tight bound . * * Finally , the numerical simulations are interesting , but only confirm my point about the fact that the obtained safety is asymptotic only , i.e. , in steady state , and absolutely not during learning . This is clear from the plots in Figure 3 ( bottom ) where we see that in average the proposed algorithm necessitates more than 1000 iteration before reaching a truly safe behavior . * * Yes , we agree with you that it is not possible to obtain a good , near-optimal policy without hitting some failures during training . The amount of safety violation during training depends on the need of the task , and is not an absolute number . We kindly refer you to the fact that even during training , after around 500 iterations , the avg . failure rate drops to about $ 10\\ % $ for our method CSC , and becomes almost $ 0 $ after about 1000 iterations . We do not claim to have $ 0 $ failures during training , but have low average failures . * * One can also note that the CPO algorithm behaves almost similarly to the proposed algorithm , when tested on the car navigation example , and the laikago robot ( Fig.3 , bottom number 3 and 4 ) . * * Please note that there is a large gap ( the gap is more than Mean $ \\pm $ S.D ) between the average failures of CSC and CPO in the car Navigation and Laikago environment , although eventually CPO also achieves nearly $ 0 $ average failures , which is expected since all the methods succeed in satisfying constraints at convergence . The differences are pronounced in the early stages of training . We kindly request you to also look at the plots of total cumulative failures in section A.8 of the Appendix where the differences are more clearly seen - CSC has around $ 50 $ failures during the entire training process , while CPO has $ > 100 $ . * * One also wonders why in the first set of tests , in Fig.3- top number 2 , one can reach a performance cost of 20 with the proposed method , while it seems to plato at 10 in the second set of experiments , in Fig.4-top number 2 . * * Kindly note that the reward functions for the two environments are defined differently , and the size of the workspace are different ( Section A.5 of the Appendix ) , so the absolute numbers on y-axis for the two plots can not be directly compared ."}, {"review_id": "iaO86DUuKi-2", "review_text": "( some difficulty following all of the proofs lowered the confidence of my evaluation ) # # Summary The authors lay out a new technique for safety-constrained exploration which reduces the likelihood of catastrophic failure . This is shown via extensive proofs providing theoretical guarantees on both convergence and the likelihood of failure as well as experimental results in a number of compelling tasks . # # Quality & Clarity The authors provide regular and clear comparisons of their approach to related techniques described in other works . They provide extensive proofs and experimental results justifying their technique \u2019 s advantages . # # Originality & Significance The work provides original techniques with both theoretical and empirical improvements over previous techniques . # # Suggestions In the experimental section , the positive reward is clear but what is missing is how receiving negative reward for failure might compare to this approach . A natural comparison against methods that do not utilize the safety constraint features is to include a very large negative reward for failure and see if the algorithms can avoid failure via this signal instead of the separate safety signal . The authors emphasize the value of only accessing a binary signal to signify safety from the environment , so this could easily be considered a slight modification of the baseline algorithms : they induce a large negative intrinsic reward when they receive a failure signal . The trap experiment provides an especially clear setting for negative rewards : the agents could receive a lesser negative reward each time they lose a health counter before reaching catastrophic failure . During policy evaluation , you resample actions when the likelihood of failure for a given action exceeds a certain threshold . I wonder how alternative approaches might perform , such as using the failure likelihood instead as a probability ( perhaps scaled ) of resampling ; reweighting the action probabilities based on the failure likelihoods ( maybe another action had a nearly identical expected return but a much lower chance of failure ) ; or even utilizing the failure likelihood as an input to the policy ( maybe in some scenarios the model stays as far away as possible from any risk but in others might choose to take a dangerous route that could lead to high reward ) . These alternatives also provide an answer to \u201c what do you do when every action from a given state is above the threshold ? \u201d which is not specified in Section 3 \u2019 s \u201c Executing rollouts ( i.e. , safe exploration ) \u201d subsection nor in Algorithm 1 . Alternative approaches to the sampling procedure could potentially improve the tradeoff between safety and convergence . Where other methods might waste time exploring dangerous parts of the space , your method might be able to focus more time on the healthier parts of the states and converge to an effective policy sooner . At least , this may be true after the initial period of learning an accurate safety classifier , which as you point out takes some time for the agent to figure out . An experiment that could add to the paper would be to pretrain a safety estimator and then restart training with a newly initialized policy -- how does this affect the convergence with different thresholds ? Finally , I would be interested to see how this work could be extended with non-binary safety constraints . The trap task provides a clear example of incremental danger ( the agent is being \u201c injured \u201d before being finally destroyed ) and being able to effectively utilize earlier hints signalling impending catastrophic failure would be a valuable skill .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the detailed review of our paper and the encouraging comments . The main comments in the review were about additional experiments to better understand the approach , which we have addressed in the responses and below , and in the revised paper . In particular , we have * * added a new experiment with a baseline `` BaseShaped '' * * modified from the unconstrained baseline `` Base '' that incorporates safety violations as negative rewards , as proposed by the reviewer , clarified an existing result with offline data for pre-training the safety estimator , and added results for continuous safety indicator in the car navigation with traps environment . We have also provided more intuitions for the theorems and proofs . Please find our detailed responses pointwise below , with reviewer comments in bold and our replies in plaintext : * * In the experimental section , the positive reward is clear but what is missing is how receiving negative reward for failure might compare to this approach . A natural comparison against methods that do not utilize the safety constraint features is to include a very large negative reward for failure and see if the algorithms can avoid failure via this signal instead of the separate safety signal . The authors emphasize the value of only accessing a binary signal to signify safety from the environment , so this could easily be considered a slight modification of the baseline algorithms : they induce a large negative intrinsic reward when they receive a failure signal . * * Thank you for suggesting this baseline . We have implemented this now , and added the results in the revised paper for Fig.3.This new baseline is named `` BaseShaped '' and it performs better than `` Base '' in terms of minimizing the number of failures . It also achieves 0 failures asymptotically , unlike Base . * * An experiment that could add to the paper would be to pretrain a safety estimator and then restart training with a newly initialized policy -- how does this affect the convergence with different thresholds ? * * Thank you for suggesting this . Kindly note that we had experimented with a very similar setting by seeding the replay buffer with some offline data for training the critic ( section A.10 ) . The main issue with a completely offline pre-training of the critic ( the safety estimator ) is how to collect the data in the first place without being unsafe . For the car navigation with traps environment in section A.10 Figure 8 , we consider a `` small '' offline dataset where we manually marked 1000 states as being safe/unsafe , but in the general case and with a large dataset , this is intractable . We are happy to experiment with any other suggested scheme that the reviewer might suggest . * * Finally , I would be interested to see how this work could be extended with non-binary safety constraints . The trap task provides a clear example of incremental danger ( the agent is being \u201c injured \u201d before being finally destroyed ) and being able to effectively utilize earlier hints signalling impending catastrophic failure would be a valuable skill . * * Yes , we had results for this non-binary safety indicator in the trap task , which we did not include in the paper to avoid confusion . We have now added these plots in section A.12 Figure 9 . We believe that this setting of a continuous safety indicator requires more domain knowledge to design a shaped function indicating impending danger , but where available easily - for example in the trap task - should be used for training the safety critic . We hope that the results for non-binary safety indicator shows that our method is applicable to this setting as well ."}, {"review_id": "iaO86DUuKi-3", "review_text": "This paper introduces a method for performing safe exploration in RL . It addresses the problem of ensuring that partially-trained policies do not visit unsafe regions of the state space , while still being exploratory enough to collect useful training experiences . The proposed technique is based on learning conservative estimates of the probability of a catastrophic failure occurring at different states . Based on these , the authors show that it is possible to upper bound the likelihood of reaching an unsafe state at every training step , thereby guaranteeing that all safety constraints are satisfied with high probability . Importantly , the authors also show that ( at least asymptotically ) , the method is no worse than standard unsafe reinforcement learning algorithms . Overall , this is a well-written paper with sound mathematical arguments . The authors present a thorough review of related work , such as constrained MDPs , and argue that the proposed method requires fewer assumptions . For example , the proposed technique assumes access only to a sparse ( binary ) indicator of whether entering a particular state would result in catastrophic failure . The authors formally show that it is possible to upper bound the expected probability of failure during every policy update iteration ( Thm1 ) , which is a non-trivial result . All update equations are carefully derived and discussed in the appendix . The experiments are well-designed and convincing , and include evaluations of the proposed method ( CSC ) in 4 different domains , including an 18-DoF quadruped robot . I believe that this paper introduces an important contribution to the RL community that is concerned with safety . It presents a principled method and introduces non-trivial bounds . In my opinion , this conference 's community would benefit from having this paper accepted to its proceedings . I have just a few questions for the authors : 1 ) safe exploration in RL is often expressed in terms of ensuring that , after every policy update , the new policy is no worse than the current one ( or is worse by a bounded amount ) . In your paper , by contrast , safe exploration is related to the ability to avoid particular states . Can any of the bounds derived in this paper be used to ensure the former type of safety ? 2 ) during policy evaluation , the proposed algorithm uses the safety critic , Q_C ( s , . ) , to estimate how unsafe a particular state is . If an action sampled by the policy is deemed unsafe , the algorithm repeatedly re-samples new actions . Is this type of rejection sampling guaranteed to always stop ? What happens if all actions available in s are unsafe ? What if safe actions do exist but have zero probability under the current policy ? Could the algorithm get stuck and stop exploring ? 3 ) in Eq2 , what is \\hat { Beta } ^pi ? 4 ) the bound presented in Theorem 2 depends on an additional term K , which ( so you argue ) can be made small by picking alpha appropriately . How do you pick alpha , in practice , so that the algorithm trades-off convergence rate and safety ? 5 ) when deriving the upper bound on V_C^pi_new ( Eq26 ) , you point out the epsilon_C can not be computed exactly before the update , since it depends on the new optimized policy pi_new . How loose does the upper bound on V get by placing a trivial upper bound epsilon_C ( epsilon_C < = 2 ) ? Also , given that policy updates are constrained to keep the stationary distribution of successive policies similar , could this be exploited to achieve a tighter bound on epsilon_C ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for the very detailed review of our paper and the encouraging comments about the approach and results . We have responded to your questions pointwise below : - * * safe exploration in RL is often expressed in terms of ensuring that , after every policy update , the new policy is no worse than the current one ( or is worse by a bounded amount ) . In your paper , by contrast , safe exploration is related to the ability to avoid particular states . Can any of the bounds derived in this paper be used to ensure the former type of safety ? * * The notion of safe exploration that the reviewer is referring to , also referred to as safe policy improvement , is certainly an interesting one . The bounds derived in this paper pertain to a different notion of safety violations -- i.e , the agent satisfies an upper bound on the probability of failure for an environment-specified constraint , while safe policy improvement pertains to a monotonic policy improvement trend . These two aspects are orthogonal and our analysis tools can potentially be used for safe policy improvement as well . - * * during policy evaluation , the proposed algorithm uses the safety critic , $ Q_C ( s , . ) $ , to estimate how unsafe a particular state is . If an action sampled by the policy is deemed unsafe , the algorithm repeatedly re-samples new actions . Is this type of rejection sampling guaranteed to always stop ? What happens if all actions available in s are unsafe ? What if safe actions do exist but have zero probability under the current policy ? Could the algorithm get stuck and stop exploring ? * * Thank you for pointing this out . We discuss this in Section A.4 of the Appendix . It can indeed happen that the rejection sampling procedure does not yield an action with a non-zero probability , especially in the early stages of training . So , in order to avoid getting stuck completely , we loop over for a maximum of 100 iterations . Kindly note that if no action is found such that $ Q ( s , a ) < \\epsilon $ is ensured , it does not necessarily mean that all actions in state s , under the current policy will yield a failure . It could be that the learned critic is not completely trained . In this case , we just choose the action for which $ Q ( s , a ) $ is minimum - * * in Eq2 , what is $ \\hat { B } ^\\pi $ ? * * Thank you for noticing this . We missed explaining this term in the submission . That is the Bellman operator . We have explained it in the revised manuscript . - * * the bound presented in Theorem 2 depends on an additional term K , which ( so you argue ) can be made small by picking $ \\alpha $ appropriately . How do you pick $ \\alpha $ , in practice , so that the algorithm trades-off convergence rate and safety ? * * Yes , K can be made small by choosing alpha appropriately as shown in equation 27 of the Appendix . We have the term $ \\alpha $ from the CQL bound . We have discussed the value of $ \\alpha $ in Section A.6 of the Appendix ."}], "0": {"review_id": "iaO86DUuKi-0", "review_text": "This paper would like to address the problem of `` `` safe exploration '' with a conservative estimation of the environment . Although the problem seems reasonable , I have the following several concerns on this paper : - Will the safety constraints be revealed to the agent ? Standard RL assumes the reward is not revealed to the agent , and I feel the safety constraints can not be revealed to the agent in prior as well . - If the safety constraints is not revealed to the agent , then how to train the safety critic ? In my opinion , when the agent collect the data that fail catastrophically , then the exploration is already not safe . - I feel the derivation part in Section 3 is not so clear , as several notations have been introduced without explanation , though it is not hard to understand . - I don \u2019 t understand what Lemma 1 and Theorem 1 can indicate in practice . When we estimate V_C , we already satisfies several catastrophic failures , otherwise we will know nothing about the possible failure . This can also be seen from the traditional exploration analysis . And traditional exploration analysis also says when we have sufficient number of data , the probability that we suffer from catastrophic failure will decrease . In one word , I don \u2019 t see any significant idea of `` safe exploration \u2019 \u2019 from Lemma 1 and Theorem 1 . - Also , from the traditional exploration analysis , we have the regret lower bound , which says we must suffer from failure to get the best policy . We also have algorithms to match these regret lower bound on some simplified MDP settings , and even stronger guarantee that with high probability we will make small mistake when we have sufficient samples ( see the mistake version of IPOC bound in [ 1 ] ) . I don \u2019 t feel the current paper present such kinds of guarantee . A summarize of point 4 & 5 : to me I don \u2019 t feel the authors well-defined the ` `` safe exploration '' and give a rigorous analysis on the `` the safety guarantee . For me , I prefer to define the `` safe exploration '' as minimizing the total failure when finding the optimal policy , but no matter how to define it , I feel what the authors have done is not `` safe '' . I feel Theorem 2 is a straightforward adaption of the existing results in [ 2 ] . Though seems correct , the authors argue that Theorem 2 shows a tradeoff between safety and convergence , which I can not agree . If we want the policy to converge to the global optimum , then we need to collect samples from the dangerous region ( otherwise we can not know if that \u2019 s dangerous and probably we can get some improvement at that region ) , how to keep `` safe '' at that time ? Also , a slow convergence will need to run for a longer time and thus need more samples , which can enlarge the probability of catastrophic failure , right ? Overall , though the empirical performance on some benchmark shows the proposed method is better than some of the previous methods , I don \u2019 t think the proposed methods really address the issue of the safe exploration . The authors does not formally define what is the safe exploration ( and even little about exploration in fact ) , and the theoretical analysis also doesn \u2019 t address any of the safety issues . From my point of view , to achieve good performance , one must suffer from some failures , and our goal should be minimizing the number of failures , rather than minimizing the failure probability of each turn . Unfortunately , the authors does not characterize all of these issues I care about . I would like to say , considering the unsatisfactory of the problem description and theoretical analysis , I don \u2019 t think this paper is suitable for publication . [ 1 ] Dann , Christoph , et al . `` Policy certificates : Towards accountable reinforcement learning . '' International Conference on Machine Learning . PMLR , 2019 . [ 2 ] Agarwal , Alekh , et al . `` Optimality and approximation with policy gradient methods in markov decision processes . '' arXiv preprint arXiv:1908.00261 ( 2019 ) . The authors answers one of the most important concerns , I have raised score to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "due to space constraint , we are creating another comment . * * Though seems correct , the authors argue that Theorem 2 shows a tradeoff between safety and convergence , which I can not agree . If we want the policy to converge to the global optimum , then we need to collect samples from the dangerous region ( otherwise we can not know if that \u2019 s dangerous and probably we can get some improvement at that region ) , how to keep `` safe '' at that time ? Also , a slow convergence will need to run for a longer time and thus need more samples , which can enlarge the probability of catastrophic failure , right ? * * Since we bound the probability of failure after every policy update ( Theorem 1 ) , the total failures until convergence is also bounded . In contrast , although the unconstrained problem might converge to the optimal policy with respect to task performance faster , it incurs significantly more failures as there is no bound on the probability of failure during training.The slower convergence rate of the task value function ( Theorem 2 relates to how quickly the policy converges to the optimal policy in terms of task performance ) is in relation to the unconstrained RL algorithm that does not take safety constraints into account during training . We also verify this empirically in Fig 3 where we see that although the unconstrained RL algorithm ( Base ) converges quickly in terms of task reward , it incurs more failures . [ 1 ] Altman , Eitan . Constrained Markov decision processes . Vol.7.CRC Press , 1999 . [ 2 ] Achiam , J. , Held , D. , Tamar , A. and Abbeel , P. , 2017 . Constrained policy optimization . arXiv preprint arXiv:1705.10528 . [ 3 ] Wachi , A. and Sui , Y. , 2020 . Safe reinforcement learning in constrained markov decision processes . arXiv preprint arXiv:2008.06626 . [ 4 ] Chow , Y. , Nachum , O. , Faust , A. , Duenez-Guzman , E. , and Ghavamzadeh , M. Lyapunov-based safe policy optimization for continuous control . arXiv preprint arXiv:1901.10031 , 2019 . [ 5 ] Garc\u0131a , J. and Fern\u00e1ndez , F. , 2015 . A comprehensive survey on safe reinforcement learning . Journal of Machine Learning Research , 16 ( 1 ) , pp.1437-1480 ."}, "1": {"review_id": "iaO86DUuKi-1", "review_text": "In this submission the authors are trying to tackle the very important problem of safe RL with safety guarantees . The problem formulation is rather clear , and the paper is overall well written . The main idea is to formulate the safe RL problem as a CMDP problem , but with worst-case bounds to ensure that the safety constrained is guaranteed throughout the learning . The general idea is fine , albeit not new ( please check/compare against recent works on robust-CMDPs ) , however , I do have several issues with the lack of rigor in the mathematical proofs , as well as the rather amplified statements about safety guarantees 'throughout learning ' . Indeed , the CMDP problem ( or its worst-case bound ) is solved using a Lagrangian formulation , which is well-know as a soft constraints formulation , i.e. , you do not have guaranteed safety during learning , as claimed in the Introduction ( paragraph 3 ) , and throughout the paper . In that case , the dramatic drone motivational example used in the Introduction ( paragraph 1 ) is and exaggeration , i.e. , this method will lead to a crash too . Besides , mathematically , many of your variables are not defined , not even in the appendix , e.g. , in equation ( 2 ) $ \\hat { B } $ is not defined ; $ \\alpha $ which seems a key tuning parameter in Theorem 1 is not defined , etc . Most importantly , the authors keep referring to the paper about CPO ( Achiam et al 17 ) to support their proofs and technical derivations , whereas that paper is about continuous constraints costs . This paper on the other hand , is clearly using discontinuous constraints , not even $ C^ { 0 } $ , and thus one can not just use parts of the results in ( Achiam et al 17 ) without further carful examination of the technical challenges introduced with a discontinuous cost function , e.g. , you are using some Taylor developments throughout , while these only make sense for analytic functions , etc . Furthermore , the probability bounds proposed in Theorem 1 and 2 seem to be rater weak bounds , since $ \\xi $ is bounded by a term inversely proportional to the confidence parameter $ \\omega $ , which means the probability of being safe is high when the safety bound is loose , i.e. , $ \\chi+\\xi $ , for $ \\xi\\rightarrow +\\infty $ . Similarly , for tight safety bound , i.e. , $ \\chi+\\xi $ , for $ \\xi\\rightarrow 0 $ , the safety probability drops to zero . The authors seem to minimize this point by the 'proper tuning ' of $ \\alpha $ which remains a mysterious parameter ( even after checking the proofs in Appendix ) . This is all to say that the authors have to tone down their statements about guaranteed provable safety bound , quite a bit . Finally , the numerical simulations are interesting , but only confirm my point about the fact that the obtained safety is asymptotic only , i.e. , in steady state , and absolutely not during learning . This is clear from the plots in Figure 3 ( bottom ) where we see that in average the proposed algorithm necessitates more than 1000 iteration before reaching a truly safe behavior . One can also note that the CPO algorithm behaves almost similarly to the proposed algorithm , when tested on the car navigation example , and the laikago robot ( Fig.3 , bottom number 3 and 4 ) . One also wonders why in the first set of tests , in Fig.3- top number 2 , one can reach a performance cost of 20 with the proposed method , while it seems to plato at 10 in the second set of experiments , in Fig.4-top number 2 . Another point that is worth clarifying is that the tests in Fig.4-top number 1 , we see that the performance cost reaches 10 for the proposed method with large safety constraint bound ( 0.2 ) , which is intuitively an almost unconstrained case . However , the unconstrained algorithm 'Base ' in the first tests , Fig.3-top number 1 , the Base algorithm does not achieve a similar performance , could it be better tuned in that case ? In summary , I found the paper well written , but it needs to be carefully revised for technical rigor , and toned down in terms of what is really achieved here ( maybe somehow safer CMDP algorithm but definitely not safe during exploration ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Author response ( part 2/2 ) : Response to other comments * * * * In that case , the dramatic drone motivational example used in the Introduction ( paragraph 1 ) is and exaggeration , i.e. , this method will lead to a crash too . * * We have now replaced this example in the revised paper . Thank you for pointing this out . * * Besides , mathematically , many of your variables are not defined , not even in the appendix , e.g. , $ \\hat { B } ^ { \\pi_\\phi } $ in equation ( 2 ) is not defined ; $ \\alpha $ which seems a key tuning parameter in Theorem 1 is not defined , etc . * * Thank you for spotting this - we have defined the Bellman operator $ \\hat { B } ^ { \\pi_\\phi } $ in the revised paper . Kindly note that we have the minimum theoretically necessary value of $ \\alpha $ defined in equation 27 of the Appendix . We have now included a line about $ \\alpha $ being a parameter that weighs the first term in equation ( 2 ) . It is inherited from the CQL paper ( Kumar et al.2020 ) . * * The authors seem to minimize this point by the 'proper tuning ' of which remains a mysterious parameter ( even after checking the proofs in Appendix ) . * * We request you to kindly refer equation ( 27 ) in the Appendix where we had derived the minimum theoretically specified value of $ \\alpha $ for obtaining a tight bound . * * Finally , the numerical simulations are interesting , but only confirm my point about the fact that the obtained safety is asymptotic only , i.e. , in steady state , and absolutely not during learning . This is clear from the plots in Figure 3 ( bottom ) where we see that in average the proposed algorithm necessitates more than 1000 iteration before reaching a truly safe behavior . * * Yes , we agree with you that it is not possible to obtain a good , near-optimal policy without hitting some failures during training . The amount of safety violation during training depends on the need of the task , and is not an absolute number . We kindly refer you to the fact that even during training , after around 500 iterations , the avg . failure rate drops to about $ 10\\ % $ for our method CSC , and becomes almost $ 0 $ after about 1000 iterations . We do not claim to have $ 0 $ failures during training , but have low average failures . * * One can also note that the CPO algorithm behaves almost similarly to the proposed algorithm , when tested on the car navigation example , and the laikago robot ( Fig.3 , bottom number 3 and 4 ) . * * Please note that there is a large gap ( the gap is more than Mean $ \\pm $ S.D ) between the average failures of CSC and CPO in the car Navigation and Laikago environment , although eventually CPO also achieves nearly $ 0 $ average failures , which is expected since all the methods succeed in satisfying constraints at convergence . The differences are pronounced in the early stages of training . We kindly request you to also look at the plots of total cumulative failures in section A.8 of the Appendix where the differences are more clearly seen - CSC has around $ 50 $ failures during the entire training process , while CPO has $ > 100 $ . * * One also wonders why in the first set of tests , in Fig.3- top number 2 , one can reach a performance cost of 20 with the proposed method , while it seems to plato at 10 in the second set of experiments , in Fig.4-top number 2 . * * Kindly note that the reward functions for the two environments are defined differently , and the size of the workspace are different ( Section A.5 of the Appendix ) , so the absolute numbers on y-axis for the two plots can not be directly compared ."}, "2": {"review_id": "iaO86DUuKi-2", "review_text": "( some difficulty following all of the proofs lowered the confidence of my evaluation ) # # Summary The authors lay out a new technique for safety-constrained exploration which reduces the likelihood of catastrophic failure . This is shown via extensive proofs providing theoretical guarantees on both convergence and the likelihood of failure as well as experimental results in a number of compelling tasks . # # Quality & Clarity The authors provide regular and clear comparisons of their approach to related techniques described in other works . They provide extensive proofs and experimental results justifying their technique \u2019 s advantages . # # Originality & Significance The work provides original techniques with both theoretical and empirical improvements over previous techniques . # # Suggestions In the experimental section , the positive reward is clear but what is missing is how receiving negative reward for failure might compare to this approach . A natural comparison against methods that do not utilize the safety constraint features is to include a very large negative reward for failure and see if the algorithms can avoid failure via this signal instead of the separate safety signal . The authors emphasize the value of only accessing a binary signal to signify safety from the environment , so this could easily be considered a slight modification of the baseline algorithms : they induce a large negative intrinsic reward when they receive a failure signal . The trap experiment provides an especially clear setting for negative rewards : the agents could receive a lesser negative reward each time they lose a health counter before reaching catastrophic failure . During policy evaluation , you resample actions when the likelihood of failure for a given action exceeds a certain threshold . I wonder how alternative approaches might perform , such as using the failure likelihood instead as a probability ( perhaps scaled ) of resampling ; reweighting the action probabilities based on the failure likelihoods ( maybe another action had a nearly identical expected return but a much lower chance of failure ) ; or even utilizing the failure likelihood as an input to the policy ( maybe in some scenarios the model stays as far away as possible from any risk but in others might choose to take a dangerous route that could lead to high reward ) . These alternatives also provide an answer to \u201c what do you do when every action from a given state is above the threshold ? \u201d which is not specified in Section 3 \u2019 s \u201c Executing rollouts ( i.e. , safe exploration ) \u201d subsection nor in Algorithm 1 . Alternative approaches to the sampling procedure could potentially improve the tradeoff between safety and convergence . Where other methods might waste time exploring dangerous parts of the space , your method might be able to focus more time on the healthier parts of the states and converge to an effective policy sooner . At least , this may be true after the initial period of learning an accurate safety classifier , which as you point out takes some time for the agent to figure out . An experiment that could add to the paper would be to pretrain a safety estimator and then restart training with a newly initialized policy -- how does this affect the convergence with different thresholds ? Finally , I would be interested to see how this work could be extended with non-binary safety constraints . The trap task provides a clear example of incremental danger ( the agent is being \u201c injured \u201d before being finally destroyed ) and being able to effectively utilize earlier hints signalling impending catastrophic failure would be a valuable skill .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the detailed review of our paper and the encouraging comments . The main comments in the review were about additional experiments to better understand the approach , which we have addressed in the responses and below , and in the revised paper . In particular , we have * * added a new experiment with a baseline `` BaseShaped '' * * modified from the unconstrained baseline `` Base '' that incorporates safety violations as negative rewards , as proposed by the reviewer , clarified an existing result with offline data for pre-training the safety estimator , and added results for continuous safety indicator in the car navigation with traps environment . We have also provided more intuitions for the theorems and proofs . Please find our detailed responses pointwise below , with reviewer comments in bold and our replies in plaintext : * * In the experimental section , the positive reward is clear but what is missing is how receiving negative reward for failure might compare to this approach . A natural comparison against methods that do not utilize the safety constraint features is to include a very large negative reward for failure and see if the algorithms can avoid failure via this signal instead of the separate safety signal . The authors emphasize the value of only accessing a binary signal to signify safety from the environment , so this could easily be considered a slight modification of the baseline algorithms : they induce a large negative intrinsic reward when they receive a failure signal . * * Thank you for suggesting this baseline . We have implemented this now , and added the results in the revised paper for Fig.3.This new baseline is named `` BaseShaped '' and it performs better than `` Base '' in terms of minimizing the number of failures . It also achieves 0 failures asymptotically , unlike Base . * * An experiment that could add to the paper would be to pretrain a safety estimator and then restart training with a newly initialized policy -- how does this affect the convergence with different thresholds ? * * Thank you for suggesting this . Kindly note that we had experimented with a very similar setting by seeding the replay buffer with some offline data for training the critic ( section A.10 ) . The main issue with a completely offline pre-training of the critic ( the safety estimator ) is how to collect the data in the first place without being unsafe . For the car navigation with traps environment in section A.10 Figure 8 , we consider a `` small '' offline dataset where we manually marked 1000 states as being safe/unsafe , but in the general case and with a large dataset , this is intractable . We are happy to experiment with any other suggested scheme that the reviewer might suggest . * * Finally , I would be interested to see how this work could be extended with non-binary safety constraints . The trap task provides a clear example of incremental danger ( the agent is being \u201c injured \u201d before being finally destroyed ) and being able to effectively utilize earlier hints signalling impending catastrophic failure would be a valuable skill . * * Yes , we had results for this non-binary safety indicator in the trap task , which we did not include in the paper to avoid confusion . We have now added these plots in section A.12 Figure 9 . We believe that this setting of a continuous safety indicator requires more domain knowledge to design a shaped function indicating impending danger , but where available easily - for example in the trap task - should be used for training the safety critic . We hope that the results for non-binary safety indicator shows that our method is applicable to this setting as well ."}, "3": {"review_id": "iaO86DUuKi-3", "review_text": "This paper introduces a method for performing safe exploration in RL . It addresses the problem of ensuring that partially-trained policies do not visit unsafe regions of the state space , while still being exploratory enough to collect useful training experiences . The proposed technique is based on learning conservative estimates of the probability of a catastrophic failure occurring at different states . Based on these , the authors show that it is possible to upper bound the likelihood of reaching an unsafe state at every training step , thereby guaranteeing that all safety constraints are satisfied with high probability . Importantly , the authors also show that ( at least asymptotically ) , the method is no worse than standard unsafe reinforcement learning algorithms . Overall , this is a well-written paper with sound mathematical arguments . The authors present a thorough review of related work , such as constrained MDPs , and argue that the proposed method requires fewer assumptions . For example , the proposed technique assumes access only to a sparse ( binary ) indicator of whether entering a particular state would result in catastrophic failure . The authors formally show that it is possible to upper bound the expected probability of failure during every policy update iteration ( Thm1 ) , which is a non-trivial result . All update equations are carefully derived and discussed in the appendix . The experiments are well-designed and convincing , and include evaluations of the proposed method ( CSC ) in 4 different domains , including an 18-DoF quadruped robot . I believe that this paper introduces an important contribution to the RL community that is concerned with safety . It presents a principled method and introduces non-trivial bounds . In my opinion , this conference 's community would benefit from having this paper accepted to its proceedings . I have just a few questions for the authors : 1 ) safe exploration in RL is often expressed in terms of ensuring that , after every policy update , the new policy is no worse than the current one ( or is worse by a bounded amount ) . In your paper , by contrast , safe exploration is related to the ability to avoid particular states . Can any of the bounds derived in this paper be used to ensure the former type of safety ? 2 ) during policy evaluation , the proposed algorithm uses the safety critic , Q_C ( s , . ) , to estimate how unsafe a particular state is . If an action sampled by the policy is deemed unsafe , the algorithm repeatedly re-samples new actions . Is this type of rejection sampling guaranteed to always stop ? What happens if all actions available in s are unsafe ? What if safe actions do exist but have zero probability under the current policy ? Could the algorithm get stuck and stop exploring ? 3 ) in Eq2 , what is \\hat { Beta } ^pi ? 4 ) the bound presented in Theorem 2 depends on an additional term K , which ( so you argue ) can be made small by picking alpha appropriately . How do you pick alpha , in practice , so that the algorithm trades-off convergence rate and safety ? 5 ) when deriving the upper bound on V_C^pi_new ( Eq26 ) , you point out the epsilon_C can not be computed exactly before the update , since it depends on the new optimized policy pi_new . How loose does the upper bound on V get by placing a trivial upper bound epsilon_C ( epsilon_C < = 2 ) ? Also , given that policy updates are constrained to keep the stationary distribution of successive policies similar , could this be exploited to achieve a tighter bound on epsilon_C ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for the very detailed review of our paper and the encouraging comments about the approach and results . We have responded to your questions pointwise below : - * * safe exploration in RL is often expressed in terms of ensuring that , after every policy update , the new policy is no worse than the current one ( or is worse by a bounded amount ) . In your paper , by contrast , safe exploration is related to the ability to avoid particular states . Can any of the bounds derived in this paper be used to ensure the former type of safety ? * * The notion of safe exploration that the reviewer is referring to , also referred to as safe policy improvement , is certainly an interesting one . The bounds derived in this paper pertain to a different notion of safety violations -- i.e , the agent satisfies an upper bound on the probability of failure for an environment-specified constraint , while safe policy improvement pertains to a monotonic policy improvement trend . These two aspects are orthogonal and our analysis tools can potentially be used for safe policy improvement as well . - * * during policy evaluation , the proposed algorithm uses the safety critic , $ Q_C ( s , . ) $ , to estimate how unsafe a particular state is . If an action sampled by the policy is deemed unsafe , the algorithm repeatedly re-samples new actions . Is this type of rejection sampling guaranteed to always stop ? What happens if all actions available in s are unsafe ? What if safe actions do exist but have zero probability under the current policy ? Could the algorithm get stuck and stop exploring ? * * Thank you for pointing this out . We discuss this in Section A.4 of the Appendix . It can indeed happen that the rejection sampling procedure does not yield an action with a non-zero probability , especially in the early stages of training . So , in order to avoid getting stuck completely , we loop over for a maximum of 100 iterations . Kindly note that if no action is found such that $ Q ( s , a ) < \\epsilon $ is ensured , it does not necessarily mean that all actions in state s , under the current policy will yield a failure . It could be that the learned critic is not completely trained . In this case , we just choose the action for which $ Q ( s , a ) $ is minimum - * * in Eq2 , what is $ \\hat { B } ^\\pi $ ? * * Thank you for noticing this . We missed explaining this term in the submission . That is the Bellman operator . We have explained it in the revised manuscript . - * * the bound presented in Theorem 2 depends on an additional term K , which ( so you argue ) can be made small by picking $ \\alpha $ appropriately . How do you pick $ \\alpha $ , in practice , so that the algorithm trades-off convergence rate and safety ? * * Yes , K can be made small by choosing alpha appropriately as shown in equation 27 of the Appendix . We have the term $ \\alpha $ from the CQL bound . We have discussed the value of $ \\alpha $ in Section A.6 of the Appendix ."}}