{"year": "2021", "forum": "_qoQkWNEhS", "title": "Ricci-GNN: Defending Against Structural Attacks Through a Geometric Approach", "decision": "Reject", "meta_review": "The paper proposes a new defense against adversarial attacks on graphs using a reweighting scheme based on Ricci-flow. Reviewers highlighted that the paper introduces interesting ideas and that the use of Ricci-curvature/flow is a novel and promising contribution. Reviewers also recognized that the paper has significantly improved after rebuttal and clarified some aspects of their initial reviews.\n\nHowever, there exist still concerns around the current version of the manuscript. In particular, important aspects of the method and algorithm, as well as some design choices are currently unclear. This includes evaluating and discussing robustness, training method, and practicality/improvements in real-world scenarios. I agree with the majority of the reviewers that the current version requires an additional revision to iron out the aforementioned issues. However, I also agree with the reviewers that the overall idea is promising and I'd encourage the authors to revise and resubmit their work with considering the feedback from this round of reviews.", "reviews": [{"review_id": "_qoQkWNEhS-0", "review_text": "Summary : In Ricci-GCN new graphs are resampled in each iteration of the training phase based on the Ricci flow metric . The Ricci flow incorporates curvature information and captures the intrinsic geometry of the graph . Compared to e.g.spectral embedding it is more robust to structural perturbations . This leads to improved robustness against adversarial attacks on the graph structure . Reasons for score : Overall , I vote for accepting . The idea is well motivated , the paper is well written , and the experiments show a clear increase in robustness on real data . My major concern is not using an adaptive attack to evaluate robustness ( see weak points ) . Strong points : * The main idea of using Ricci flow is interesting , well motivated and well executed . * Evaluation on SBM graphs helps with better understanding why the proposed approach works . * The comparison to other metrics ( Spectral and HC ) is appreciated . Weak points : * It is not clear whether META ( the meta-learning attack ) is computed w.r.t.the vanilla GCN or the Ricci-GCN . If META was run on the original GCN it is not clear whether the attack is not successful because Ricci-GCN is more robust or because the adversarial edges found for GCN are not transferable to Ricci-GCN . Since the proposed defense is only heuristic ( not certifiable ) in order to show robustness it has to be evaluated against an adaptive attacker that takes the defense into account [ 1 ] . Otherwise , an adaptive attacker may easily break the defense in the future . For example , META can be adapted to account for the Ricci flow . If META was indeed run on Ricci-GCN , the author should discuss the details , e.g.whether they use the reparametrization trick to compute the gradients through the sampling . * The gain in robustness is only significant for large perturbation rates ( > 0.1 ) which might not correspond to realistic threat models in practice , e.g.for perturbation rates < 0.1 GCN-SVD is on par with Ricci-GCN . * One interpretation of the proposed approach is that Ricci-GCN is doing ( a specific type of ) data augmentation which is known to improve generalization and by extension the clean and the adversarial accuracy . For example , one augmentation in [ 2 ] is to randomly sample edges to add or remove , and in [ 3 ] edges are randomly dropped . Even though [ 2 ] and [ 3 ] are not motivated by robustness they are a relevant baselines since similar to Ricci-GCN they generated different graphs during training . * A big drawback of the proposed approach is the large number of hyperparameters : \\gamma=0.5 , p=2 , k=2 , \\sigma , \\beta , etc . It this is not clear how sensitive is the method to these choices or to the definition of the `` probability measure of each neighborhood '' . * Ignoring the random attack which extremely weak , the evaluation is limited to a single attack ( META ) . Evaluating against other attacks ( see [ 4 ] and [ 5 ] ) would help to better evaluate the robustness of the model . Question for the authors : 1 . How robust is Ricci-GCN to adaptive attacks ? ( see weak points ) 2 . How does Ricci-GCN compare to other data augmentation techniques ( see weak points ) 3 . Is sampling performed only during training ? If so are there any benefits to also sampling during inference ( and aggregating the predictions ) ? Can sampling during inference help defend against evasion attacks such as Nettack ? 4.Is there any improvement if we are willing to pay the price of decreased sparsity , take k larger than 2 ? 5.Why is the row for perturbation rate of 0 omitted from Table 3 ? Where the hyperparameters tuned separately for Spectral and HC ? Additional feedback that did not affect the decision : * The related work should also discuss the difference between certifiable and heuristic defenses and how the proposed approach fits in this context . * It would be nice to quantitatively show `` The probability of edges in the original community structure are higher than the attack edges '' * It would be beneficial to provide a reference or evidence for the claim `` generally lacks descriptive power to provide desirable resolution and differentiation '' . While the results in Table 3 provide indirect evidence , it is not clear that this is due to `` lack of descriptive power '' . * It would be interesting to see whether Ricci-GCN is also more certifiably robust than vanilla GCN , e.g.by computing model agnostic certificates such as [ 6 ] . Typos : * Figure 5 Captions : Purterbation Rate # # After Rebuttal The authors ' response clarified some of the issues and partially addressed some of my concerns . Based on this and the remaining reviews I have decided to keep the score unchanged . One additional comment regarding the evaluation : In the authors ' response they state `` Finally , we would like to point out that it is common practice to use GCN as a subroutine for Meta-attack against different defense methods . This was shown in the original Meta-Attack paper , as well as multiple follow-up defense papers . '' I would like to again point out that the fact that this is a common practice is not ideal , even though multiple follow-up defense papers use the same strategy . We have already learned the lesson in the computer vision literature that adaptive attacks are the least we can do to evaluate heuristic defenses ( see [ 1 ] ) and even that might not provide strong evidence . References : 1 . Tramer , Florian , Nicholas Carlini , Wieland Brendel , and Aleksander Madry . `` On adaptive attacks to adversarial example defenses . '' 2.Wang , Yiwei , Wei Wang , Yuxuan Liang , Yujun Cai , Juncheng Liu , and Bryan Hooi . `` NodeAug : Semi-Supervised Node Classification with Data Augmentation . '' 3.Rong , Yu , Wenbing Huang , Tingyang Xu , and Junzhou Huang . `` Dropedge : Towards deep graph convolutional networks on node classification . '' 4. https : //github.com/gitgiter/Graph-Adversarial-Learning 5. https : //github.com/safe-graph/graph-adversarial-learning-literature 6 . Bojchevski , Aleksandar , Johannes Klicpera , and Stephan G\u00fcnnemann . `` Efficient robustness certificates for discrete data : Sparsity-aware randomized smoothing for graphs , images and more . ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "Weak points : * * [ 1 ] ( It is not clear ... ) * * * * Ans * * : META learning uses GCN as a subroutine . We apply Ricci-GCN on the graph containing the contaminated/adversarial edges . * * [ 2 ] ( The gain in\u2026 ) * * * * Ans * * : You are absolutely right . Since the attacker \u2019 s power is strictly limited when the perturbation rate is low . The difference between different defense methods and even the base method ( GCN and GAT ) is not large , but our method still outperforms the other baselines on almost all perturbation rates . * * [ 3 ] ( One interpretation\u2026 ) * * * * Ans * * : Thank you for pointing it out . We add the DropEdge as another baseline and show the new results in Appendix ( B.3.2 ) . * * [ 4 ] ( A big drawback\u2026 ) * * * * Ans * * : \\sigma and \\beta : for each parameter , we tested about five values . In our experiments , we did not run parameter search on parameters \\gamma , p , k. Below is the rationale for the choice of these parameters . The choice of k=2 : we resample edges among the 2-hop neighborhood in the original graph . We fix k=2 for two reasons : 1 ) due to the small world property of most random graphs and real-world graphs the diameter is a small constant ; 2 ) two nodes of three or more hops away are expected to have a long geodesic distance , and thus the chance of being sampled is low . \\gamma and p are parameters used in the computation of Ricci flow . In prior work [ Ni 2019 ] , the influence of these parameters on the computed curvature has been thoroughly evaluated . In short , the choice of different \\gamma mainly introduces a global shift and scaling of the curvature values . The choice of p influences the convergence rate . In our experiments , we fixed the value of \\gamma and p as the suggested values as in [ Ni 2019 ] . We also include a few new figures with different values of \\gamma and p , which shows that there is no significant difference in a qualitative manner . Similarly , the choice of probability measure used in the curvature definition has been fully investigated in the prior work [ Ni 2018 ] and [ Ni 2019 ] , which suggested to use the exponential distribution . This choice is also fixed in the experiments . * * [ 5 ] ( Ignoring the random ... ) * * * * Ans * * : We add another SOTA attack algorithm Topological Attack-MinMax ( from paper \u201c Topology Attack and Defense for Graph Neural Networks : An Optimization Perspective \u201d ) . The results are shown in the Appendix ( Table 5 ) . Our Ricci-GNN achieves the best performance on all datasets under this new attack . Question for the authors : * * [ 1 ] ( How robust is\u2026 ) * * * * Ans * * : Ricci curvature , defined on an edge , can be considered as a measure of how much the removal of one edge changes the network connectivity . Edges of negative curvature ( e.g. , a critical edge connecting two communities ) , if deleted , can possibly completely change the global network layout . Ricci flow ( which is used in this paper ) is to modify the edge weights such that all edges have the same curvature . The new weights are observed in [ Ni 2018 ] to be more robust to * random * edge insertion/deletion , compared to alternative network metrics . This inspired us to apply Ricci flow metric to improve robustness to * adversarial attacks * in graph learning . * * [ 2 ] ( How does\u2026 ) * * * * Ans * * : Ricci-GNN is more robust than other data augmentation techniques in most cases . * * [ 3 ] ( Is sampling\u2026 ) * * * * Ans * * : No , it \u2019 s sampled every time an adjacency matrix is needed in both training and testing stages . * * [ 4 ] ( Is there any\u2026 ) * * * * Ans * * : If we increase k too much , there will be an over smoothing problem . And the computation and memory cost will be significantly increased . * * [ 5 ] ( Why is the row ... ) * * * * Ans * * : Thank you for pointing it out . Originally , we think that the main purpose is to compare the robustness of different approaches . We will add the 0 perturbation rate to the paper . Ricci flow metric is better than both hop count metric ( HC ) and metric from spectral embedding ( Spectral ) ."}, {"review_id": "_qoQkWNEhS-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The paper proses a new adversarial ( poisoning ) defense based on a known graph reweighting scheme known as the ricci curvature . The ricci curvature assigns a weight to each edge that captures the graph structure , i.e.the value reflects whether the edge is an inter-community connection or an intracommunity connection . Empirically , the ricci curvature is known to be more robust w.r.t.random edge insertions/deletions . The authors propose a new sampling method based on the ricci curvature and use it within their novel training scheme . Empirically , the effectiveness of their approach is shown via experiments on synthetic SBM graphs . Moreover , the authors use a random attack and Metattack on various datasets . They show superior performance to multiple baseline architectures/defenses . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for rating : Overall it is an interesting work and the empirical performance seems to be good . However , neglecting the very weak random attack and the experiment on synthetic data , the authors effectively only evaluate against one strong attack . Hence , the question arises if the defense is solely effective against the characteristic of Metattack ? I would recommend to add at least one further strong attack . Furthermore , it is not clear if a transfer attack is used ( a surrogate used for Metattack ) . It would be very interesting to see if the ricci curvature calculation itself is adversarially robust . In the chosen setup this fact is obfuscated . Last , the authors cite but do not compare to the Curvature Graph Network which also uses ricci curvature instead of the widely used symmetric normalization of the adjacency matrix ( e.g.as a GCN ) . Hence , it is not clear to the reader if the sampling scheme/training scheme or the ricci curvature is the major reason for adversarial robustness . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : + Interesting and promising approach + Consistently improved performance over the baselines + Interesting analysis of their defense via SBM graphs Cons : - Only one strong attack on real-world graphs is used to benchmark to other architectures . - Is the ricci curvature itself robust w.r.t.adversarial attacks ? The authors seem to use transfer attacks and the referenced literature claims only robustness against random attacks and this is also only evaluated empirically . - Curvature Graph Network should be added as a baseline . - The authors do not discuss the space and time complexity . Only the time cost for the ricci curvature is discussed . Moreover , the authors use the two-hop neighborhood for adding potential edges \u2014 this can still be very expensive specifically for power-law graphs . A discussion would be appreciated . Further points : - The proposed sampling based training scheme seems to be highly related to adversarial training . The authors should add a corresponding discussion . - The paper lacks clarity at some points and has inconsistencies in notation . For example , in Section 2 `` F '' is not introduced . S denotes the geodesic distance ( aka length of shortest path ) which is denoted by d ( x , y ) in Section A . - At some points the authors say that a graph is sampled in each `` iteration '' ( epoch ) and sometimes for every layer . - Figure 2 : What is H_0 , H_1 , L ? - Section 1.1 : The authors should make clear that Figure 3 is an example and does not imply superior robustness in general . - What are the limits of ricci curvature ? Beyond some level of perturbation , there should be a tipping point ( i.e.communities can not be distinguished anymore ) . - Related work : There are many more ( relevant ) attacks/defenses . Please add them and/or make clear that your discussion is not exhaustive .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the comments . We address the main concerns one-by-one . Cons : * * [ 1 ] ( Only one strong attack ) * * * * Ans * * : Per your request , we added a new attack called Topological attack-MinMax from the paper : \u201c Topology Attack and Defense for Graph Neural Networks : An Optimization Perspective \u201d . The result has been added to the appendix ( Table 5 ) . Our Ricci-GNN achieves the best performance on all datasets under this new attack . * * [ 2 ] ( Is the ricci curvature itself robust ) * * * * Ans * * : No . Ricci curvature itself is not robust . Ricci curvature , defined on an edge , can be considered as a measure of how much one edge changes the network connectivity . Edges of negative curvature ( e.g. , a critical edge connecting two communities ) , if deleted , may completely change the global network layout . Ricci flow ( which is used in this paper ) is to modify the edge weights such that all edges have the same curvature . The new weights are observed in [ Ni 2018 ] to be robust to * random * edge insertion/deletion . Inspired by this observation , we hypothesized that Ricci flow metric can improve robustness against * adversarial attacks * . This is validated empirically . We added additional discussion to provide the intuition ( see \u201c robustness of Ricci flow metric \u201d in Section 2.2 ) . * * [ 3 ] ( Curvature Graph Network ) * * * * Ans * * : Thanks for the suggestion . We added curvature graph network ( CurvGN ) as a baseline method . The design of CurvGN is to use curvature as additional structural information in the training . It does not consider robustness and from our experiments there does not seem to be extra benefits of robustness against adversarial attacks . The new results are shown in our Appendix B 3.2 . * * [ 4 ] ( The authors do not discuss the space and time complexity ) * * * * Ans * * : The time complexity of computing Ricci curvature on an edge using Sinkhorn distances is O^tilde ( d^2 ) where d is the maximum degree . So running one Ricci flow iteration requires O^tilde ( |E| * d^2 ) . Ricci flow is only computed once at the beginning of the GNN algorithm . The space complexity is O ( |V|^2 ) . Further points : * * [ 1 ] ( The proposed sampling\u2026 ) * * * * Ans * * : The adversarial training method is trying to introduce adversarial examples and increase the model \u2019 s robustness with regularization terms that neutralize the effect of the introduced adversarial samples . However , our method directly changes the graph \u2019 s structure to improve the model \u2019 s robustness . * * [ 2 ] ( The paper lacks ... ) * * * * Ans * * : Thanks for pointing this out . We have updated the paper accordingly . Recall that in Section A.2 , we use Ricci flow ( Eq A.3 same as Eq 2.3 ) to iteratively update the edge $ w^ { ( t+1 ) } ( x , y ) $ . So F is the edge weight matrix for this resultant weight after convergence : $ F_ { ij } = w^ * ( i , j ) $ . And S is the matrix for the geodesic distance by running the all-pair shortest paths on weights in $ F $ . Therefore $ F ( x , y ) = 0 $ if there is no edge $ xy $ . But $ S ( x , y ) $ is non-zero as long as there is a path connecting x and y . * * [ 3 ] ( At some points\u2026 ) * * * * Ans * * : For each epoch , for each layer , we sample a new graph . Therefore , if we run for 100 epochs on a GNN with 2 layers , we sample $ 2\\times 100 $ graphs in total . * * [ 4 ] ( Figure 2 ... ) * * * * Ans * * : H_0 is the initial feature matrix ( H_0 = H in Sec 2.1 ) with dimension |V| ( number of nodes ) * |D| ( feature dimension ) . H_1 is the transformed feature after applying the first layer of GNN on H_0 . L is the final output of GNN . If it is a binary node classification task , L is of dimension |V| ( predicted score for each node ) . It is explained in the third paragraph of section 2.1 . We also change the subscript of H into a superscript to make it consistent with the context . * * [ 5 ] ( Section 1.1 ... ) * * * * Ans * * : It \u2019 s correct that Figure 3 serves as a motivating example to visually compare different graph metrics . It is observed in the literature ( such as Ni et al. , 2015 ; Sandhu et al. , 2015 ) that Ricci curvature is more robust for several real-world graph datasets and is a useful prior for many clustering/classification tasks on such datasets . * * [ 6 ] ( What are the limits\u2026 ) * * * * Ans * * : We agree that when the perturbation rate increases , eventually all learning methods fail as the input graph connectivity becomes completely different from the ground truth . In our experiments ( and also in prior literature ) the highest perturbation rate we tested is 25 % -- which is already significantly high for any practical settings . It seems that 25 % perturbation rate is the tipping point for all other defense methods ( for Polblogs ) , while our method can still provide reasonable results . * * [ 7 ] ( Related work\u2026 ) * * * * Ans * * : Thank you ! We edited our related work as suggested ."}, {"review_id": "_qoQkWNEhS-2", "review_text": "Strengths : The paper is well written and clean . Weaknesses : I have several concerns regarding this paper . \u2022 Novelty . The authors propose to use Ricci flow to compute the distance between nodes so that to sample edges with respect to that distance . Using Ricci flow for distance computation is a well-studied area ( as indicated in related work ) . The only novel part is that each layer gets a new graph ; however , this choice is not motivated ( why not to train all layers of GNN on different graphs instead ? ) and has problems ( see next ) . \u2022 Approach . Computing optimal transport distance is generally an expensive procedure . While authors indicated that it takes seconds to compute it on 36 cores machine , it \u2019 s not clear how scalable this method is . I would like to see whether it scales on normal machines with a couple of cores . Moreover , how do you compute exactly optimal transport , because the Sinkhorn method gives you a doubly stochastic matrix ( how do you go from it to optimal transport ? ) . \u2022 Algorithm . This is the most obscure part of the paper . First , it \u2019 s not indicated how many layers do you use in experiments . This is a major part of your algorithm because you claim that if an edge appears in several layers it means that it \u2019 s not adversarial ( or that it does not harm your algorithm ) . In most of the baselines , there are at most 2-3 layers . There are theoretical limitations why GNN with many layers may not work in practice ( see , the literature on \u201c GNN oversmoothing \u201d ) . Considering that you didn \u2019 t provide the code ( can you provide an anonymized version of the code ? ) and that your baselines ( GCN , GAT , etc . ) have similar ( or the same ) performance as in the original papers ( where the number of layers is 2-3 ) , I deduce that your model Ricci-GNN also has this number of layers . With that said , I doubt that it \u2019 s possible to make any conclusive results about whether an edge is adversarial or not with 2-3 graphs . Moreover , I would expect to see an experiment on how your approach varies depending on the number of layers . This is a crucial part of your algorithm and not seeing discussion of it in the paper , raises concerns about the validity of experiments . \u2022 Design choices . Another potential problem of your algorithm is that the sampled graphs can become dense . There are hyperparameters \\sigma and \\beta that control the probabilities and also you limit the sampling only for 2-hop neighborhoods ( \u201c To keep graph sparsity , we only sample edges between pairs that are within k hops of each other in G ( we always take k = 2 in the experiments ) . \u201d This is arbitrary and the effect of it on the performance is not clear . How did you select parameters \\sigma and \\beta ? Why k=2 ? How do you ensure that the sampled graphs are similar to the original one ? Does it matter that sampled graphs should have similar statistics to the original graph ? I guess , this crucially affects the performance of your algorithm , so I would like to see more experiments on this . \u2022 Datasets . Since this paper is mostly experimental , I would like to see a comparison of this model on more datasets ( 5-7 in total ) . Verifying on realistic but small datasets such as Cora and Citeseer limits our intuition about performance . For example , Cora is a single graph of 2.7K nodes . As indicated in [ 1 ] , \u201c Although small datasets are useful as sanity checks for new ideas , they can become a liability in the long run as new GNN models will be designed to overfit the small test sets instead of searching for more generalizable architectures. \u201d There are many sources of real graphs , you can consider OGB [ 2 ] or [ 3 ] . \u2022 Weak baselines . Another major concern of the validity of the experiments is the choice of the baselines . Neither of GNN baselines ( GCN , GAT , etc . ) was designed for the defense of adversarial attacks , so choosing them for comparison is not fair . A comparison with previous works ( indicated in \u201c Adversarial attack on graphs. \u201d in related work section ) is necessary . Moreover , an experiment where you randomly sample edges ( instead of using Ricci distance ) is desirable to compare the performance against random sampling . \u2022 Ablation . Since you use GCN , why the performance of Ricci-GCN is so different from GCN when there 0 perturbations ? For Citeseer the absolute difference is 2 % which is quite high for the same models . Also , an experiment with different choices of GNN is desirable . \u2022 Training . Since experiments play important role in this paper , it \u2019 s important to give a fair setup for the models in comparison . You write \u201c For each training procedure , we run 100 epochs and use the model trained at 100-th epoch. \u201d . This can disadvantageous for many models . A better way would be to run each model setup until convergence on the training set , selecting the epoch using the validation set . Otherwise , your baselines could suffer from either underfitting or overfitting . [ 1 ] https : //arxiv.org/pdf/2003.00982.pdf [ 2 ] https : //ogb.stanford.edu/ [ 3 ] https : //paperswithcode.com/task/node-classification = After reading the authors comments . I applaud the authors for greatly improving their paper via the revision . Now the number of layers is specified and the explanation of having many sampled graphs during training is added , which was missing in the original text and was preventing a full understanding of the reasons why the proposed approach works . Overall , I am leaning toward increasing the score . I still have several concerns about the practicality of Ricci-GNN . In simple words , the proposed approach uses some metric S ( Ricci flow ) that dictates how to sample graphs for training . The motivation for using Ricci flow is \u201c that Ricci flow is a global process that tries to uncover the underlying metric space supported by the graph topology and thus embraces redundancy \u201d . This claim cites previous papers , which in turn do not discuss what exactly is meant by \u201c a global process that tries to uncover the underlying metric space \u201d . Spectral embeddings also can be considered as a global metric , so some analysis on what properties of Ricci flow makes it more robust to attacks would be appreciated . Also including random sampling in comparison would confirm that the effect is coming not from the fact that you use more graphs during the training , but from how you sample those graphs . In addition , as the paper is empirical and relies on the properties of Ricci flow which was discussed in previous works and was not addressed in the context of adversarial attacks , having more datasets ( especially larger ones ) in the experiments would improve the paper .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comment . First , we would like to point out that there seem to be quite a few misunderstandings , including Sinkhorn , graph sampling , defense baselines , etc . Below we respond to each comment in detail : Weaknesses : * * [ 1 ] ( Novelty ) * * * * Ans * * : \u201c Using Ricci flow for distance computation is a well-studied area ( as indicated in related work ) . The only novel part is that each layer gets a new graph. \u201d Ans : We do not think this paper is incremental just because we are using the existing Ricci flow metric . We are the first to apply the Ricci flow metric to GNN . Our major novelty is to customize this principled tool to address the robustness of GNN to adversarial attacks problems in an effective manner . The idea is built on deep insights into the connection between the Ricci flow theory and the GNN robustness and has demonstrated great performance improvement over prior work . * * [ 2 ] ( Approach ) * * * * Ans * * : \u201c because the Sinkhorn method gives you a doubly stochastic matrix ( how do you go from it to optimal transport ? \u201d Ans : It seems the reviewer has a misunderstanding of the \u201c Sinkhorn \u201d method.The Sinkhorn distance is a standard method to approximate the optimal transport distance by adding entropy regularization . In the paper , we have provided the reference of the Sinkhorn method ( Cuturi ( 2013 ) ) for optimal transport . In our case , we did not compute the exact optimal transport but used the \u201c Sinkhorn distance \u201d based on the \u201c Sinkhorn method \u201d as an approximation instead to compute Ricci flow for the graph . This approximation to the exact optimal transport distance in the Ricci flow computation is shown to give similar results in the referenced paper \u201c Community Detection on Networks with Ricci Flow. \u201d The running time to compute Ricci flow ( https : //github.com/saibalmars/GraphRicciCurvature ) is inversely proportional to the number of cores . In all cases we tested , the computation of Ricci flow is insignificant compared with the GPU training time , and can be handled easily even on a standard laptop machine . For example , it takes 18.9 s \u00b1 73.5 ms on average to compute Ricci flow on the Cora graph with 2485 vertices and 5069 edges for 20 iterations on an Intel ( R ) Xeon ( R ) Gold 6140 CPU @ 2.30GHz 36 cores machine . On a 2018 Macbook Pro with 2.6 GHz 6-Core Intel Core i7 it takes 1min 6s \u00b1 2.91 s on average . * * [ 3 ] ( Algorithm , graph sampling ) * * * * Ans * * : \u201c I doubt that it \u2019 s possible to make any conclusive results about whether an edge is adversarial or not with 2-3 graphs. \u201d Ans : All baseline algorithms ( e.g. , GCN , GAT ) and ours use the same architecture of 2 layers , for a fair comparison . We add these missing experimental details in the revised paper and also share our anonymized codes at \u201c https : //anonymous.4open.science/r/43ab93f7-8b8b-4f76-a7c1-50ebc9bb0f6a/ \u201d . But we are not using only 2 graphs , because in the graph sampling phase we sample different graphs for each layer and each epoch . If our final run has 100 epochs , then we have 2 * 100=200 sampled graphs . The reviewer has a misunderstanding of graph sampling . Graph sampling is commonly used in the graph learning community to make the GNN scalable , such as GraphSAINT [ Zeng et al.ICLR2020 ] , LDS [ Franceschi et al.ICML2019 ] . We randomly sample different graphs for every epoch . Please note * * the sampling ( Line 2 of Algorithm 1 ) is inside the training loop * * . Different layers of the network use different sample graphs . The graph sampling technique does not require the sampled graphs to be similar to the original graph . To simply explain it , the final graph embedding is an expectation of the embeddings from all sample graphs . It has been fully understood and experimentally demonstrated in previous works . In our case , we use a probability matrix to sample the graph , which is similar to LDS ."}], "0": {"review_id": "_qoQkWNEhS-0", "review_text": "Summary : In Ricci-GCN new graphs are resampled in each iteration of the training phase based on the Ricci flow metric . The Ricci flow incorporates curvature information and captures the intrinsic geometry of the graph . Compared to e.g.spectral embedding it is more robust to structural perturbations . This leads to improved robustness against adversarial attacks on the graph structure . Reasons for score : Overall , I vote for accepting . The idea is well motivated , the paper is well written , and the experiments show a clear increase in robustness on real data . My major concern is not using an adaptive attack to evaluate robustness ( see weak points ) . Strong points : * The main idea of using Ricci flow is interesting , well motivated and well executed . * Evaluation on SBM graphs helps with better understanding why the proposed approach works . * The comparison to other metrics ( Spectral and HC ) is appreciated . Weak points : * It is not clear whether META ( the meta-learning attack ) is computed w.r.t.the vanilla GCN or the Ricci-GCN . If META was run on the original GCN it is not clear whether the attack is not successful because Ricci-GCN is more robust or because the adversarial edges found for GCN are not transferable to Ricci-GCN . Since the proposed defense is only heuristic ( not certifiable ) in order to show robustness it has to be evaluated against an adaptive attacker that takes the defense into account [ 1 ] . Otherwise , an adaptive attacker may easily break the defense in the future . For example , META can be adapted to account for the Ricci flow . If META was indeed run on Ricci-GCN , the author should discuss the details , e.g.whether they use the reparametrization trick to compute the gradients through the sampling . * The gain in robustness is only significant for large perturbation rates ( > 0.1 ) which might not correspond to realistic threat models in practice , e.g.for perturbation rates < 0.1 GCN-SVD is on par with Ricci-GCN . * One interpretation of the proposed approach is that Ricci-GCN is doing ( a specific type of ) data augmentation which is known to improve generalization and by extension the clean and the adversarial accuracy . For example , one augmentation in [ 2 ] is to randomly sample edges to add or remove , and in [ 3 ] edges are randomly dropped . Even though [ 2 ] and [ 3 ] are not motivated by robustness they are a relevant baselines since similar to Ricci-GCN they generated different graphs during training . * A big drawback of the proposed approach is the large number of hyperparameters : \\gamma=0.5 , p=2 , k=2 , \\sigma , \\beta , etc . It this is not clear how sensitive is the method to these choices or to the definition of the `` probability measure of each neighborhood '' . * Ignoring the random attack which extremely weak , the evaluation is limited to a single attack ( META ) . Evaluating against other attacks ( see [ 4 ] and [ 5 ] ) would help to better evaluate the robustness of the model . Question for the authors : 1 . How robust is Ricci-GCN to adaptive attacks ? ( see weak points ) 2 . How does Ricci-GCN compare to other data augmentation techniques ( see weak points ) 3 . Is sampling performed only during training ? If so are there any benefits to also sampling during inference ( and aggregating the predictions ) ? Can sampling during inference help defend against evasion attacks such as Nettack ? 4.Is there any improvement if we are willing to pay the price of decreased sparsity , take k larger than 2 ? 5.Why is the row for perturbation rate of 0 omitted from Table 3 ? Where the hyperparameters tuned separately for Spectral and HC ? Additional feedback that did not affect the decision : * The related work should also discuss the difference between certifiable and heuristic defenses and how the proposed approach fits in this context . * It would be nice to quantitatively show `` The probability of edges in the original community structure are higher than the attack edges '' * It would be beneficial to provide a reference or evidence for the claim `` generally lacks descriptive power to provide desirable resolution and differentiation '' . While the results in Table 3 provide indirect evidence , it is not clear that this is due to `` lack of descriptive power '' . * It would be interesting to see whether Ricci-GCN is also more certifiably robust than vanilla GCN , e.g.by computing model agnostic certificates such as [ 6 ] . Typos : * Figure 5 Captions : Purterbation Rate # # After Rebuttal The authors ' response clarified some of the issues and partially addressed some of my concerns . Based on this and the remaining reviews I have decided to keep the score unchanged . One additional comment regarding the evaluation : In the authors ' response they state `` Finally , we would like to point out that it is common practice to use GCN as a subroutine for Meta-attack against different defense methods . This was shown in the original Meta-Attack paper , as well as multiple follow-up defense papers . '' I would like to again point out that the fact that this is a common practice is not ideal , even though multiple follow-up defense papers use the same strategy . We have already learned the lesson in the computer vision literature that adaptive attacks are the least we can do to evaluate heuristic defenses ( see [ 1 ] ) and even that might not provide strong evidence . References : 1 . Tramer , Florian , Nicholas Carlini , Wieland Brendel , and Aleksander Madry . `` On adaptive attacks to adversarial example defenses . '' 2.Wang , Yiwei , Wei Wang , Yuxuan Liang , Yujun Cai , Juncheng Liu , and Bryan Hooi . `` NodeAug : Semi-Supervised Node Classification with Data Augmentation . '' 3.Rong , Yu , Wenbing Huang , Tingyang Xu , and Junzhou Huang . `` Dropedge : Towards deep graph convolutional networks on node classification . '' 4. https : //github.com/gitgiter/Graph-Adversarial-Learning 5. https : //github.com/safe-graph/graph-adversarial-learning-literature 6 . Bojchevski , Aleksandar , Johannes Klicpera , and Stephan G\u00fcnnemann . `` Efficient robustness certificates for discrete data : Sparsity-aware randomized smoothing for graphs , images and more . ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "Weak points : * * [ 1 ] ( It is not clear ... ) * * * * Ans * * : META learning uses GCN as a subroutine . We apply Ricci-GCN on the graph containing the contaminated/adversarial edges . * * [ 2 ] ( The gain in\u2026 ) * * * * Ans * * : You are absolutely right . Since the attacker \u2019 s power is strictly limited when the perturbation rate is low . The difference between different defense methods and even the base method ( GCN and GAT ) is not large , but our method still outperforms the other baselines on almost all perturbation rates . * * [ 3 ] ( One interpretation\u2026 ) * * * * Ans * * : Thank you for pointing it out . We add the DropEdge as another baseline and show the new results in Appendix ( B.3.2 ) . * * [ 4 ] ( A big drawback\u2026 ) * * * * Ans * * : \\sigma and \\beta : for each parameter , we tested about five values . In our experiments , we did not run parameter search on parameters \\gamma , p , k. Below is the rationale for the choice of these parameters . The choice of k=2 : we resample edges among the 2-hop neighborhood in the original graph . We fix k=2 for two reasons : 1 ) due to the small world property of most random graphs and real-world graphs the diameter is a small constant ; 2 ) two nodes of three or more hops away are expected to have a long geodesic distance , and thus the chance of being sampled is low . \\gamma and p are parameters used in the computation of Ricci flow . In prior work [ Ni 2019 ] , the influence of these parameters on the computed curvature has been thoroughly evaluated . In short , the choice of different \\gamma mainly introduces a global shift and scaling of the curvature values . The choice of p influences the convergence rate . In our experiments , we fixed the value of \\gamma and p as the suggested values as in [ Ni 2019 ] . We also include a few new figures with different values of \\gamma and p , which shows that there is no significant difference in a qualitative manner . Similarly , the choice of probability measure used in the curvature definition has been fully investigated in the prior work [ Ni 2018 ] and [ Ni 2019 ] , which suggested to use the exponential distribution . This choice is also fixed in the experiments . * * [ 5 ] ( Ignoring the random ... ) * * * * Ans * * : We add another SOTA attack algorithm Topological Attack-MinMax ( from paper \u201c Topology Attack and Defense for Graph Neural Networks : An Optimization Perspective \u201d ) . The results are shown in the Appendix ( Table 5 ) . Our Ricci-GNN achieves the best performance on all datasets under this new attack . Question for the authors : * * [ 1 ] ( How robust is\u2026 ) * * * * Ans * * : Ricci curvature , defined on an edge , can be considered as a measure of how much the removal of one edge changes the network connectivity . Edges of negative curvature ( e.g. , a critical edge connecting two communities ) , if deleted , can possibly completely change the global network layout . Ricci flow ( which is used in this paper ) is to modify the edge weights such that all edges have the same curvature . The new weights are observed in [ Ni 2018 ] to be more robust to * random * edge insertion/deletion , compared to alternative network metrics . This inspired us to apply Ricci flow metric to improve robustness to * adversarial attacks * in graph learning . * * [ 2 ] ( How does\u2026 ) * * * * Ans * * : Ricci-GNN is more robust than other data augmentation techniques in most cases . * * [ 3 ] ( Is sampling\u2026 ) * * * * Ans * * : No , it \u2019 s sampled every time an adjacency matrix is needed in both training and testing stages . * * [ 4 ] ( Is there any\u2026 ) * * * * Ans * * : If we increase k too much , there will be an over smoothing problem . And the computation and memory cost will be significantly increased . * * [ 5 ] ( Why is the row ... ) * * * * Ans * * : Thank you for pointing it out . Originally , we think that the main purpose is to compare the robustness of different approaches . We will add the 0 perturbation rate to the paper . Ricci flow metric is better than both hop count metric ( HC ) and metric from spectral embedding ( Spectral ) ."}, "1": {"review_id": "_qoQkWNEhS-1", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The paper proses a new adversarial ( poisoning ) defense based on a known graph reweighting scheme known as the ricci curvature . The ricci curvature assigns a weight to each edge that captures the graph structure , i.e.the value reflects whether the edge is an inter-community connection or an intracommunity connection . Empirically , the ricci curvature is known to be more robust w.r.t.random edge insertions/deletions . The authors propose a new sampling method based on the ricci curvature and use it within their novel training scheme . Empirically , the effectiveness of their approach is shown via experiments on synthetic SBM graphs . Moreover , the authors use a random attack and Metattack on various datasets . They show superior performance to multiple baseline architectures/defenses . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for rating : Overall it is an interesting work and the empirical performance seems to be good . However , neglecting the very weak random attack and the experiment on synthetic data , the authors effectively only evaluate against one strong attack . Hence , the question arises if the defense is solely effective against the characteristic of Metattack ? I would recommend to add at least one further strong attack . Furthermore , it is not clear if a transfer attack is used ( a surrogate used for Metattack ) . It would be very interesting to see if the ricci curvature calculation itself is adversarially robust . In the chosen setup this fact is obfuscated . Last , the authors cite but do not compare to the Curvature Graph Network which also uses ricci curvature instead of the widely used symmetric normalization of the adjacency matrix ( e.g.as a GCN ) . Hence , it is not clear to the reader if the sampling scheme/training scheme or the ricci curvature is the major reason for adversarial robustness . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : + Interesting and promising approach + Consistently improved performance over the baselines + Interesting analysis of their defense via SBM graphs Cons : - Only one strong attack on real-world graphs is used to benchmark to other architectures . - Is the ricci curvature itself robust w.r.t.adversarial attacks ? The authors seem to use transfer attacks and the referenced literature claims only robustness against random attacks and this is also only evaluated empirically . - Curvature Graph Network should be added as a baseline . - The authors do not discuss the space and time complexity . Only the time cost for the ricci curvature is discussed . Moreover , the authors use the two-hop neighborhood for adding potential edges \u2014 this can still be very expensive specifically for power-law graphs . A discussion would be appreciated . Further points : - The proposed sampling based training scheme seems to be highly related to adversarial training . The authors should add a corresponding discussion . - The paper lacks clarity at some points and has inconsistencies in notation . For example , in Section 2 `` F '' is not introduced . S denotes the geodesic distance ( aka length of shortest path ) which is denoted by d ( x , y ) in Section A . - At some points the authors say that a graph is sampled in each `` iteration '' ( epoch ) and sometimes for every layer . - Figure 2 : What is H_0 , H_1 , L ? - Section 1.1 : The authors should make clear that Figure 3 is an example and does not imply superior robustness in general . - What are the limits of ricci curvature ? Beyond some level of perturbation , there should be a tipping point ( i.e.communities can not be distinguished anymore ) . - Related work : There are many more ( relevant ) attacks/defenses . Please add them and/or make clear that your discussion is not exhaustive .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the comments . We address the main concerns one-by-one . Cons : * * [ 1 ] ( Only one strong attack ) * * * * Ans * * : Per your request , we added a new attack called Topological attack-MinMax from the paper : \u201c Topology Attack and Defense for Graph Neural Networks : An Optimization Perspective \u201d . The result has been added to the appendix ( Table 5 ) . Our Ricci-GNN achieves the best performance on all datasets under this new attack . * * [ 2 ] ( Is the ricci curvature itself robust ) * * * * Ans * * : No . Ricci curvature itself is not robust . Ricci curvature , defined on an edge , can be considered as a measure of how much one edge changes the network connectivity . Edges of negative curvature ( e.g. , a critical edge connecting two communities ) , if deleted , may completely change the global network layout . Ricci flow ( which is used in this paper ) is to modify the edge weights such that all edges have the same curvature . The new weights are observed in [ Ni 2018 ] to be robust to * random * edge insertion/deletion . Inspired by this observation , we hypothesized that Ricci flow metric can improve robustness against * adversarial attacks * . This is validated empirically . We added additional discussion to provide the intuition ( see \u201c robustness of Ricci flow metric \u201d in Section 2.2 ) . * * [ 3 ] ( Curvature Graph Network ) * * * * Ans * * : Thanks for the suggestion . We added curvature graph network ( CurvGN ) as a baseline method . The design of CurvGN is to use curvature as additional structural information in the training . It does not consider robustness and from our experiments there does not seem to be extra benefits of robustness against adversarial attacks . The new results are shown in our Appendix B 3.2 . * * [ 4 ] ( The authors do not discuss the space and time complexity ) * * * * Ans * * : The time complexity of computing Ricci curvature on an edge using Sinkhorn distances is O^tilde ( d^2 ) where d is the maximum degree . So running one Ricci flow iteration requires O^tilde ( |E| * d^2 ) . Ricci flow is only computed once at the beginning of the GNN algorithm . The space complexity is O ( |V|^2 ) . Further points : * * [ 1 ] ( The proposed sampling\u2026 ) * * * * Ans * * : The adversarial training method is trying to introduce adversarial examples and increase the model \u2019 s robustness with regularization terms that neutralize the effect of the introduced adversarial samples . However , our method directly changes the graph \u2019 s structure to improve the model \u2019 s robustness . * * [ 2 ] ( The paper lacks ... ) * * * * Ans * * : Thanks for pointing this out . We have updated the paper accordingly . Recall that in Section A.2 , we use Ricci flow ( Eq A.3 same as Eq 2.3 ) to iteratively update the edge $ w^ { ( t+1 ) } ( x , y ) $ . So F is the edge weight matrix for this resultant weight after convergence : $ F_ { ij } = w^ * ( i , j ) $ . And S is the matrix for the geodesic distance by running the all-pair shortest paths on weights in $ F $ . Therefore $ F ( x , y ) = 0 $ if there is no edge $ xy $ . But $ S ( x , y ) $ is non-zero as long as there is a path connecting x and y . * * [ 3 ] ( At some points\u2026 ) * * * * Ans * * : For each epoch , for each layer , we sample a new graph . Therefore , if we run for 100 epochs on a GNN with 2 layers , we sample $ 2\\times 100 $ graphs in total . * * [ 4 ] ( Figure 2 ... ) * * * * Ans * * : H_0 is the initial feature matrix ( H_0 = H in Sec 2.1 ) with dimension |V| ( number of nodes ) * |D| ( feature dimension ) . H_1 is the transformed feature after applying the first layer of GNN on H_0 . L is the final output of GNN . If it is a binary node classification task , L is of dimension |V| ( predicted score for each node ) . It is explained in the third paragraph of section 2.1 . We also change the subscript of H into a superscript to make it consistent with the context . * * [ 5 ] ( Section 1.1 ... ) * * * * Ans * * : It \u2019 s correct that Figure 3 serves as a motivating example to visually compare different graph metrics . It is observed in the literature ( such as Ni et al. , 2015 ; Sandhu et al. , 2015 ) that Ricci curvature is more robust for several real-world graph datasets and is a useful prior for many clustering/classification tasks on such datasets . * * [ 6 ] ( What are the limits\u2026 ) * * * * Ans * * : We agree that when the perturbation rate increases , eventually all learning methods fail as the input graph connectivity becomes completely different from the ground truth . In our experiments ( and also in prior literature ) the highest perturbation rate we tested is 25 % -- which is already significantly high for any practical settings . It seems that 25 % perturbation rate is the tipping point for all other defense methods ( for Polblogs ) , while our method can still provide reasonable results . * * [ 7 ] ( Related work\u2026 ) * * * * Ans * * : Thank you ! We edited our related work as suggested ."}, "2": {"review_id": "_qoQkWNEhS-2", "review_text": "Strengths : The paper is well written and clean . Weaknesses : I have several concerns regarding this paper . \u2022 Novelty . The authors propose to use Ricci flow to compute the distance between nodes so that to sample edges with respect to that distance . Using Ricci flow for distance computation is a well-studied area ( as indicated in related work ) . The only novel part is that each layer gets a new graph ; however , this choice is not motivated ( why not to train all layers of GNN on different graphs instead ? ) and has problems ( see next ) . \u2022 Approach . Computing optimal transport distance is generally an expensive procedure . While authors indicated that it takes seconds to compute it on 36 cores machine , it \u2019 s not clear how scalable this method is . I would like to see whether it scales on normal machines with a couple of cores . Moreover , how do you compute exactly optimal transport , because the Sinkhorn method gives you a doubly stochastic matrix ( how do you go from it to optimal transport ? ) . \u2022 Algorithm . This is the most obscure part of the paper . First , it \u2019 s not indicated how many layers do you use in experiments . This is a major part of your algorithm because you claim that if an edge appears in several layers it means that it \u2019 s not adversarial ( or that it does not harm your algorithm ) . In most of the baselines , there are at most 2-3 layers . There are theoretical limitations why GNN with many layers may not work in practice ( see , the literature on \u201c GNN oversmoothing \u201d ) . Considering that you didn \u2019 t provide the code ( can you provide an anonymized version of the code ? ) and that your baselines ( GCN , GAT , etc . ) have similar ( or the same ) performance as in the original papers ( where the number of layers is 2-3 ) , I deduce that your model Ricci-GNN also has this number of layers . With that said , I doubt that it \u2019 s possible to make any conclusive results about whether an edge is adversarial or not with 2-3 graphs . Moreover , I would expect to see an experiment on how your approach varies depending on the number of layers . This is a crucial part of your algorithm and not seeing discussion of it in the paper , raises concerns about the validity of experiments . \u2022 Design choices . Another potential problem of your algorithm is that the sampled graphs can become dense . There are hyperparameters \\sigma and \\beta that control the probabilities and also you limit the sampling only for 2-hop neighborhoods ( \u201c To keep graph sparsity , we only sample edges between pairs that are within k hops of each other in G ( we always take k = 2 in the experiments ) . \u201d This is arbitrary and the effect of it on the performance is not clear . How did you select parameters \\sigma and \\beta ? Why k=2 ? How do you ensure that the sampled graphs are similar to the original one ? Does it matter that sampled graphs should have similar statistics to the original graph ? I guess , this crucially affects the performance of your algorithm , so I would like to see more experiments on this . \u2022 Datasets . Since this paper is mostly experimental , I would like to see a comparison of this model on more datasets ( 5-7 in total ) . Verifying on realistic but small datasets such as Cora and Citeseer limits our intuition about performance . For example , Cora is a single graph of 2.7K nodes . As indicated in [ 1 ] , \u201c Although small datasets are useful as sanity checks for new ideas , they can become a liability in the long run as new GNN models will be designed to overfit the small test sets instead of searching for more generalizable architectures. \u201d There are many sources of real graphs , you can consider OGB [ 2 ] or [ 3 ] . \u2022 Weak baselines . Another major concern of the validity of the experiments is the choice of the baselines . Neither of GNN baselines ( GCN , GAT , etc . ) was designed for the defense of adversarial attacks , so choosing them for comparison is not fair . A comparison with previous works ( indicated in \u201c Adversarial attack on graphs. \u201d in related work section ) is necessary . Moreover , an experiment where you randomly sample edges ( instead of using Ricci distance ) is desirable to compare the performance against random sampling . \u2022 Ablation . Since you use GCN , why the performance of Ricci-GCN is so different from GCN when there 0 perturbations ? For Citeseer the absolute difference is 2 % which is quite high for the same models . Also , an experiment with different choices of GNN is desirable . \u2022 Training . Since experiments play important role in this paper , it \u2019 s important to give a fair setup for the models in comparison . You write \u201c For each training procedure , we run 100 epochs and use the model trained at 100-th epoch. \u201d . This can disadvantageous for many models . A better way would be to run each model setup until convergence on the training set , selecting the epoch using the validation set . Otherwise , your baselines could suffer from either underfitting or overfitting . [ 1 ] https : //arxiv.org/pdf/2003.00982.pdf [ 2 ] https : //ogb.stanford.edu/ [ 3 ] https : //paperswithcode.com/task/node-classification = After reading the authors comments . I applaud the authors for greatly improving their paper via the revision . Now the number of layers is specified and the explanation of having many sampled graphs during training is added , which was missing in the original text and was preventing a full understanding of the reasons why the proposed approach works . Overall , I am leaning toward increasing the score . I still have several concerns about the practicality of Ricci-GNN . In simple words , the proposed approach uses some metric S ( Ricci flow ) that dictates how to sample graphs for training . The motivation for using Ricci flow is \u201c that Ricci flow is a global process that tries to uncover the underlying metric space supported by the graph topology and thus embraces redundancy \u201d . This claim cites previous papers , which in turn do not discuss what exactly is meant by \u201c a global process that tries to uncover the underlying metric space \u201d . Spectral embeddings also can be considered as a global metric , so some analysis on what properties of Ricci flow makes it more robust to attacks would be appreciated . Also including random sampling in comparison would confirm that the effect is coming not from the fact that you use more graphs during the training , but from how you sample those graphs . In addition , as the paper is empirical and relies on the properties of Ricci flow which was discussed in previous works and was not addressed in the context of adversarial attacks , having more datasets ( especially larger ones ) in the experiments would improve the paper .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comment . First , we would like to point out that there seem to be quite a few misunderstandings , including Sinkhorn , graph sampling , defense baselines , etc . Below we respond to each comment in detail : Weaknesses : * * [ 1 ] ( Novelty ) * * * * Ans * * : \u201c Using Ricci flow for distance computation is a well-studied area ( as indicated in related work ) . The only novel part is that each layer gets a new graph. \u201d Ans : We do not think this paper is incremental just because we are using the existing Ricci flow metric . We are the first to apply the Ricci flow metric to GNN . Our major novelty is to customize this principled tool to address the robustness of GNN to adversarial attacks problems in an effective manner . The idea is built on deep insights into the connection between the Ricci flow theory and the GNN robustness and has demonstrated great performance improvement over prior work . * * [ 2 ] ( Approach ) * * * * Ans * * : \u201c because the Sinkhorn method gives you a doubly stochastic matrix ( how do you go from it to optimal transport ? \u201d Ans : It seems the reviewer has a misunderstanding of the \u201c Sinkhorn \u201d method.The Sinkhorn distance is a standard method to approximate the optimal transport distance by adding entropy regularization . In the paper , we have provided the reference of the Sinkhorn method ( Cuturi ( 2013 ) ) for optimal transport . In our case , we did not compute the exact optimal transport but used the \u201c Sinkhorn distance \u201d based on the \u201c Sinkhorn method \u201d as an approximation instead to compute Ricci flow for the graph . This approximation to the exact optimal transport distance in the Ricci flow computation is shown to give similar results in the referenced paper \u201c Community Detection on Networks with Ricci Flow. \u201d The running time to compute Ricci flow ( https : //github.com/saibalmars/GraphRicciCurvature ) is inversely proportional to the number of cores . In all cases we tested , the computation of Ricci flow is insignificant compared with the GPU training time , and can be handled easily even on a standard laptop machine . For example , it takes 18.9 s \u00b1 73.5 ms on average to compute Ricci flow on the Cora graph with 2485 vertices and 5069 edges for 20 iterations on an Intel ( R ) Xeon ( R ) Gold 6140 CPU @ 2.30GHz 36 cores machine . On a 2018 Macbook Pro with 2.6 GHz 6-Core Intel Core i7 it takes 1min 6s \u00b1 2.91 s on average . * * [ 3 ] ( Algorithm , graph sampling ) * * * * Ans * * : \u201c I doubt that it \u2019 s possible to make any conclusive results about whether an edge is adversarial or not with 2-3 graphs. \u201d Ans : All baseline algorithms ( e.g. , GCN , GAT ) and ours use the same architecture of 2 layers , for a fair comparison . We add these missing experimental details in the revised paper and also share our anonymized codes at \u201c https : //anonymous.4open.science/r/43ab93f7-8b8b-4f76-a7c1-50ebc9bb0f6a/ \u201d . But we are not using only 2 graphs , because in the graph sampling phase we sample different graphs for each layer and each epoch . If our final run has 100 epochs , then we have 2 * 100=200 sampled graphs . The reviewer has a misunderstanding of graph sampling . Graph sampling is commonly used in the graph learning community to make the GNN scalable , such as GraphSAINT [ Zeng et al.ICLR2020 ] , LDS [ Franceschi et al.ICML2019 ] . We randomly sample different graphs for every epoch . Please note * * the sampling ( Line 2 of Algorithm 1 ) is inside the training loop * * . Different layers of the network use different sample graphs . The graph sampling technique does not require the sampled graphs to be similar to the original graph . To simply explain it , the final graph embedding is an expectation of the embeddings from all sample graphs . It has been fully understood and experimentally demonstrated in previous works . In our case , we use a probability matrix to sample the graph , which is similar to LDS ."}}