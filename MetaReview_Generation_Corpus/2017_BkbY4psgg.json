{"year": "2017", "forum": "BkbY4psgg", "title": "Making Neural Programming Architectures Generalize via Recursion", "decision": "Accept (Oral)", "meta_review": "The reviewers were very favourable, and the paper is on a highly-relevant topic and explores a useful practical trick.", "reviews": [{"review_id": "BkbY4psgg-0", "review_text": "This paper argues that being able to handle recursion is very important for neural programming architectures \u2014 that handling recursion allows for strong generalization to out of domain test cases and learning from smaller amounts of training data. Most of the paper is a riff on the Reed & de Freitas paper on Neural Programmer Interpreters from ICLR 2016 which learns from program traces \u2014 this paper trains NPI models on traces that have recursive calls. The authors show how to verify correctness by evaluating the learned program on only a small set of base cases and reduction rules and impressively, show that the NPI architecture is able to perfectly infer Bubblesort and the Tower of Hanoi problems. What I like is that the idea is super simple and as the authors even mention, the only change is to the execution traces that the training pipeline gets to see. I\u2019m actually not sure what the right take-away is \u2014 does this mean that we have effectively solved the neural programming problem when the execution traces are available? (and was the problem too easy to begin with?). For example, a larger input domain (as one of the reviewers also mentions) is MNIST digits and we can imagine a problem where the NPI must infer how to sort MNIST digits from highest to lowest. In this setting, having execution traces would effectively decouple the problem of recognizing the digits from that of inferring the program logic \u2014 and so the problem would be no harder than learning to recognize MNIST digits and learning to bubble sort from symbols. What is a problem where we have access to execution traces but cannot infer it using the proposed method? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your comments ! We do not see any obvious limitations to the proposed model \u2019 s ability to infer programs from execution traces , as long as the program semantics is well defined and an adequate training set is used . The model \u2019 s ability to learn the program depends heavily on the training set -- -if the training set is not comprehensive enough , the learned program will not be correct , as the model would lack information about how to handle plausible situations unseen during training . Execution traces provide very detailed supervision for the model , which makes learning from execution traces much easier than input-output pairs . As we mention in the paper , for future work , an important direction is to reduce the amount of supervision in the training data and to create models that incorporate recursion into the architectures themselves ."}, {"review_id": "BkbY4psgg-1", "review_text": "This is a very interesting and fairly easy to read paper. The authors present a small, yet nifty approach to make Neural Programming Interpreters significantly more powerful. By allowing recursion, NPI generalizes better from fewer execution traces. It's an interesting example of how a small but non-trivial extension can make a machine learning method significantly more practical. I also appreciate that the same notation was used in this paper and the original Deepmind paper. As a non-expert on this topic, it was easy to read the original paper in tandem. My one point of critique is that the generalization proves are a bit vague. For the numerical examples in the paper, you can iterate over all possible execution paths until the next recursive call. However, how would this approach generalize a continuous input space (e.g. the 3D car example in the original paper). It seems that a prove of generalization will still be intractable in the continuous case? Are you planning on releasing the source code?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your comments ! Your question about proof over continuous space is similar to one of the questions below , so we refer you to our comment \u201c Feasibility of Verification Procedure \u201d . We plan to clean up our source code and release it in the near future ."}, {"review_id": "BkbY4psgg-2", "review_text": "This paper improves significantly upon the original NPI work, showing that the model generalizes far better when trained on traces in recursive form. The authors show better sample complexity and generalization results for addition and bubblesort programs, and add two new and more interesting tasks - topological sort and quicksort (added based on reviewer discussion). Furthermore, they actually *prove* that the algorithms learned by the model generalize perfectly, which to my knowledge is the first time this has been done in neural program induction.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "I like this paper as well , and think it is a valuable improvement over the NPI baseline . However , even on first reading , I was annoyed by the use of the term `` prove '' . The paper proves that under certain circumstances , _testing_ on a finite set of values is sufficient to establish correctness for whole classes of inputs . However , this does not amount to a generally applicable proof strategy . It is only feasible because the input domain is quite simple , and only a handful of cases need to be considered . However , this quickly becomes problematic when the input domain expands , e.g. , when one-hot encodings of digits are replaced by MNIST digits . I would expect NPI ( and the recursive extension presented here ) to have no problem to handle this with an appropriately structured domain-specific encoder , but the `` proof '' strategy described in this paper would not work anymore . Thus , I am not convinced of the practical value of this part of the contribution , as it seems to be unfeasible for everything but toy paper examples ."}], "0": {"review_id": "BkbY4psgg-0", "review_text": "This paper argues that being able to handle recursion is very important for neural programming architectures \u2014 that handling recursion allows for strong generalization to out of domain test cases and learning from smaller amounts of training data. Most of the paper is a riff on the Reed & de Freitas paper on Neural Programmer Interpreters from ICLR 2016 which learns from program traces \u2014 this paper trains NPI models on traces that have recursive calls. The authors show how to verify correctness by evaluating the learned program on only a small set of base cases and reduction rules and impressively, show that the NPI architecture is able to perfectly infer Bubblesort and the Tower of Hanoi problems. What I like is that the idea is super simple and as the authors even mention, the only change is to the execution traces that the training pipeline gets to see. I\u2019m actually not sure what the right take-away is \u2014 does this mean that we have effectively solved the neural programming problem when the execution traces are available? (and was the problem too easy to begin with?). For example, a larger input domain (as one of the reviewers also mentions) is MNIST digits and we can imagine a problem where the NPI must infer how to sort MNIST digits from highest to lowest. In this setting, having execution traces would effectively decouple the problem of recognizing the digits from that of inferring the program logic \u2014 and so the problem would be no harder than learning to recognize MNIST digits and learning to bubble sort from symbols. What is a problem where we have access to execution traces but cannot infer it using the proposed method? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your comments ! We do not see any obvious limitations to the proposed model \u2019 s ability to infer programs from execution traces , as long as the program semantics is well defined and an adequate training set is used . The model \u2019 s ability to learn the program depends heavily on the training set -- -if the training set is not comprehensive enough , the learned program will not be correct , as the model would lack information about how to handle plausible situations unseen during training . Execution traces provide very detailed supervision for the model , which makes learning from execution traces much easier than input-output pairs . As we mention in the paper , for future work , an important direction is to reduce the amount of supervision in the training data and to create models that incorporate recursion into the architectures themselves ."}, "1": {"review_id": "BkbY4psgg-1", "review_text": "This is a very interesting and fairly easy to read paper. The authors present a small, yet nifty approach to make Neural Programming Interpreters significantly more powerful. By allowing recursion, NPI generalizes better from fewer execution traces. It's an interesting example of how a small but non-trivial extension can make a machine learning method significantly more practical. I also appreciate that the same notation was used in this paper and the original Deepmind paper. As a non-expert on this topic, it was easy to read the original paper in tandem. My one point of critique is that the generalization proves are a bit vague. For the numerical examples in the paper, you can iterate over all possible execution paths until the next recursive call. However, how would this approach generalize a continuous input space (e.g. the 3D car example in the original paper). It seems that a prove of generalization will still be intractable in the continuous case? Are you planning on releasing the source code?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your comments ! Your question about proof over continuous space is similar to one of the questions below , so we refer you to our comment \u201c Feasibility of Verification Procedure \u201d . We plan to clean up our source code and release it in the near future ."}, "2": {"review_id": "BkbY4psgg-2", "review_text": "This paper improves significantly upon the original NPI work, showing that the model generalizes far better when trained on traces in recursive form. The authors show better sample complexity and generalization results for addition and bubblesort programs, and add two new and more interesting tasks - topological sort and quicksort (added based on reviewer discussion). Furthermore, they actually *prove* that the algorithms learned by the model generalize perfectly, which to my knowledge is the first time this has been done in neural program induction.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "I like this paper as well , and think it is a valuable improvement over the NPI baseline . However , even on first reading , I was annoyed by the use of the term `` prove '' . The paper proves that under certain circumstances , _testing_ on a finite set of values is sufficient to establish correctness for whole classes of inputs . However , this does not amount to a generally applicable proof strategy . It is only feasible because the input domain is quite simple , and only a handful of cases need to be considered . However , this quickly becomes problematic when the input domain expands , e.g. , when one-hot encodings of digits are replaced by MNIST digits . I would expect NPI ( and the recursive extension presented here ) to have no problem to handle this with an appropriately structured domain-specific encoder , but the `` proof '' strategy described in this paper would not work anymore . Thus , I am not convinced of the practical value of this part of the contribution , as it seems to be unfeasible for everything but toy paper examples ."}}