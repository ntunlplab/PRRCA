{"year": "2020", "forum": "rklMnyBtPB", "title": "Adversarial Robustness Against the Union of Multiple Perturbation Models", "decision": "Reject", "meta_review": "Thanks to the authors for submitting the paper and providing further explanations and experiments. This paper aims to ensure robustness against several perturbation models simultaneously. While the authors' response has addressed several issues raised by the reviewers, the concern on the lack of novelty remains. Overall, there is not enough support among the reviewers for the paper to be accepted.", "reviews": [{"review_id": "rklMnyBtPB-0", "review_text": "The paper proposes to do adversarial training on multiple L_p norm perturbation models simultaneously, to make the model robust against various types of attacks. [Novelty] I feel this is just a natural extension of adversarial training. If we define the perturbation set in PGD to be S, then in general S can be union of perturbation set of several L_p norm, and the resulting algorithm will be MSD (everytime you do a gradient update and then find the worst case projection in S). It would be interesting to study the convergence of this kind of algorithms, since S is no longer convex, the projection is trickier to define. Unfortunately this is not discussed in the paper. In terms of experiments, this is an interesting data point to show that we can have a model that is (weakly) robust to L1, L2 and Linf norms simultaneously. However, the results are not surprising since there's more than 10% performance decreases compared to the original adversarial training under each particular attack. So it's still not clear whether we can get a model that simultaneously achieves L1, L2, Linf robust error comparable to original PGD training. [Performance] - It seems MSD is not always better than others (worst PGD and PGD Aug). For MNIST, MSD performs poorly on Linf norm and it's not clear why. - There's significant performance drop in clean accuracy, especially MSD on MNIST data. [Suggestions] - As mentioned before, studying the convergence properties of the proposed methods will be interesting. - It will be interesting if you can train on a set of perturbation models and make it also robust to another perturbation not in the training phase. For instance, can we apply the proposed method to L{1,inf} in training and generalize to L2 perturbation? ===== Thanks for the response. I still have concerns about novelty so would like to keep my rating unchanged. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your feedback . We definitely aimed for this to be a \u201c natural \u201d extension of adversarial , and we view the simplicity of the approach to be an advantage of the approach over relying on more complex methods . As a minor note , the extension goes beyond finding the worst-case projection : it is important to also consider the individual steepest descent directions for each threat model , so there is no singular gradient step like in PGD . On convergence : We do agree that studying the convergence properties could be interesting , however , this is not our focus and is out of the scope of this paper . This is actually a fairly complex problem : the convergence properties of steepest descent for a * single * norm ( to our knowledge ) for deep networks is not quite known . On performance : The main point is that while comparing individual threat models leads to an unclear conclusion about risk tradeoffs as you pointed out , the conclusion for the reader is clear when measuring performance in the \u201c all attacks \u201d mode . This is the metric that makes the most sense , since this is exactly the robust optimization objective being minimized by all the algorithms , and has a simple interpretation as measuring performance when a failure in even a single threat model is unacceptable . We will adjust the paper accordingly to make this more obvious . Please see our general comment here for a more detailed discussion on comparing performances , as well as on generalizing outside the threat model used during training : https : //openreview.net/forum ? id=rklMnyBtPB & noteId=rJgRBxZ-iB"}, {"review_id": "rklMnyBtPB-1", "review_text": "Summary of the paper: The paper describes adversarial training aiming to build models that are robust to multiple adversarial attacks - with L_1, L_2 and L_inf norms. The method is a based on adversarial training against a union of adversaries. That union is created by taking (projected) gradient steps like PGD (Kurakin 2017), but choosing the maximal loss over GD steps for L1, L2, L_inf at each step. Strengths: The topic is trendy and interesting. The proposed algorithm is simple and easy to implement. The experimental results demonstrate improvement over several baselines. Weaknesses: -- I am missing a more systematic comparisons to baseline defenses in the experiments. Figures 2 and 3 should have shown the accuracy as a function of radius also for PGD-aug, PGD-worst, Schott et al. Also what about comparisons to the latest SoTA defenses, e.g. recent baselines from from www.robust-ml.org/defenses/. -- An implicit expectation from this paper is that it addresses the key issue of \"Defend against one attack but face a different attack\". The paper could have done more to advance our understanding of this issue. Specifically: The approach improves over baselines for the \"all attacks\" mode, but under-performs compared with PGDaug and PGDworst when attacked with a single norm (Tab 1). While this is expected and probably cannot be avoided, it leaves the reader with an unclear conclusion about risk tradeoffs. It would have been useful to clarify the regime of mixtures of attacks where the various approaches are best. For instance, if one uses a of mix attack samples from the three norms, what mixtures would it be best to defend using MSD, wand what mixtures would it be best to use PGD-aug? or ABS? ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and the provided suggestions . On the comparison to baseline defenses : Combining the robustness curves is a great suggestion , thank you . We do in fact have the accuracies as a function of radius for PGD-aug and PGD-worst ( we had put them in the appendix as Figures 4-7 for lack of space ) , but we can certainly combine them into a single plot for MNIST and CIFAR10 . As for the baselines at robust-ml.org , since the setting we study is the union of multiple threat models , we focus on baselines which also study defending against multiple threat models . To our knowledge , the only baseline on robust-ml.org which does this is the ABS model by Schott et al. , which we explicitly compare to in our paper . On comparing the performances against individual attacks and the corresponding risk tradeoffs : The main point is that while comparing individual threat models leads to an unclear conclusion about risk tradeoffs as you pointed out , the conclusion for the reader is quite clear when measuring performance in the \u201c all attacks \u201d mode . This is the metric that makes the most sense , since this is exactly the robust optimization objective being minimized by all the algorithms , and has a simple interpretation as measuring performance when failure in even a single threat model is unacceptable . If one wishes to instead defend against a different mixture of attacks , then it makes more sense to change the robust optimization objective to reflect the different mixture of attacks using MSD , rather than trying to obtain it ad-hoc with PGD-aug or PGD-worst using a different threat model . Please see our general comment here for a more detailed discussion on measuring and comparing performance : https : //openreview.net/forum ? id=rklMnyBtPB & noteId=rJgRBxZ-iB"}, {"review_id": "rklMnyBtPB-2", "review_text": "This paper adversarially trains models against l_p norms where p is of there different values. They then propose a method which does somewhat better than the obvious way of adversarially training against more than one l_p perturbation. The motivation for the paper is limited, in that they suggest previous works have suggested adversarial training itself \"overfits\" to the given l_p norm. This isn't surprising that it works, since the straightforward baseline works. They make it seem surprising by suggesting that ABS suggested adversarial training is doomed and cannot provide robustness to l_1, l_2, l_\\infty norms simultaneously. The other motivation is that this is a step toward studying an expanded threat model, but the authors have not demonstrated that the learned representations are any bit more robust to common corruptions (could the authors show the generalization performance on CIFAR-10-C or generalization to unforeseen corruptions?). Without further evidence, we are left to believe this only helps for this narrow threat model. Overall the paper is deficient in creativity and generality, so I vote for rejection. Small comments: > take more time than a single norm, it is a step closer towards the end goal of truly robust models, with adversarial robustness against all perturbations. Please show model performance on CIFAR-10-C since if the model is more robust, it should hopefully be more robust to stochastic adversaries. > has claimed that adversarial training \u201coverfits\u201d to the particular type of perturbation used to generate the adversarial examples Wouldn't this be that l_\\infty training fits specifically to l_\\infty examples, not that robust optimization cannot handle more than one norm at a time? Who is claiming that? > First, we show that even simple aggregations of different adversarial attacks can achieve competitive universal robustness against multiple perturbations models without resorting to complex architectures. I am not sure this was in doubt. The phrase \"universal robustness\" is misleading. How were the budgets chosen for l_2 and l_1? Those values seem small.", "rating": "1: Reject", "reply_text": "Thank you for your feedback . Clarification on \u201c overfitting \u201d : First , we would like to clarify that we took the language of \u201c adversarial training overfits to the L-infinity norm \u201d directly from \u201c Towards the first adversarially robust neural network model on MNIST \u201d by Schott et al. , which was published at last year \u2019 s ICLR , and is where the claim comes from ( you can see this in the abstract ) . Of course , this is by no means a central point of the paper , and we merely wished to contextualize the result with relevant research on the same topic . We are quite willing to adjust the wording ( e.g.the referenced phrasing with respect to overfitting and universal robustness ) . On the motivation and significance of MSD : While it is correct that the straightforward baselines work , they only work to some degree and are suboptimal when measuring their performance * with respect to the robust performance metric at which they attempt to minimize * , namely the robust optimization objective which is the performance against the union of threat models . On both MNIST and CIFAR10 , we see a substantial increase in robust performance ( 5 % and 6 % respectively ) on the union threat model from MSD over the baselines . This shows that the baselines , while they work to some extent , make various implicit tradeoffs that don \u2019 t actually minimize the robust objective that they are trying to minimize , and so MSD is a more direct , explicit way of minimizing the robust loss over the union adversary . The baselines themselves are also not consistent across the datasets : PGD-Aug performs poorly on MNIST , while PGD-Worst performs poorly on CIFAR10 , whereas MSD is consistent across both problems . It is unfortunate that you think the approach is deficient in creativity and generality . Rather , we believe that the simplicity of the method adds to its strength , showing that even simple approaches can perform quite well without resorting to complex procedures . MSD is also general in that it can utilize any first-order iterative method for adversarial generation , and is not an image-specific defense ( it is at least as generally applicable as the standard adversarial training approach for a single threat model ) . On generalizing to unforeseen corruptions : Defending against attacks outside of the threat model has never been a goal of adversarial training , and has little theoretical justification for why this would be the case . As such , performance comparisons on out-of-threat-model attacks like CIFAR-10-C , while potentially interesting , are completely orthogonal to the point of the paper . See our general comment here for a more detailed discussion : https : //openreview.net/forum ? id=rklMnyBtPB & noteId=rJgRBxZ-iB However , despite this , since CIFAR-10-C isn \u2019 t too expensive to evaluate , we ran this anyways just to see what happens and got the following mean accuracies : Standard model : 66.52 % PGD-Worst : 70.8 % PGD-Aug : 76.84 % MSD : 74.22 % So indeed , all of the approaches appear to improve model performance on CIFAR-10-C in comparison to standard training to some degree . However , because none of the models were explicitly trained to minimize these sorts of corruptions , we refrain from making any further conclusions . On the budgets chosen for L1 and L2 : The chosen budgets all come from the literature . For MNIST , we chose the same budget as that used in \u201c Towards the First Adversarially Robust Neural Network Model on MNIST \u201d [ Schott et al.2019 ] in order to be directly comparable and most fair in the comparison . For CIFAR10 budgets comes from \u201c Towards Evaluating the Robustness of Neural Networks \u201d [ Carlini & Wagner 2017 ] , though we used a smaller L1 budget to account for the difference from L0 to L1 and to not entirely subsume the other threat models ."}], "0": {"review_id": "rklMnyBtPB-0", "review_text": "The paper proposes to do adversarial training on multiple L_p norm perturbation models simultaneously, to make the model robust against various types of attacks. [Novelty] I feel this is just a natural extension of adversarial training. If we define the perturbation set in PGD to be S, then in general S can be union of perturbation set of several L_p norm, and the resulting algorithm will be MSD (everytime you do a gradient update and then find the worst case projection in S). It would be interesting to study the convergence of this kind of algorithms, since S is no longer convex, the projection is trickier to define. Unfortunately this is not discussed in the paper. In terms of experiments, this is an interesting data point to show that we can have a model that is (weakly) robust to L1, L2 and Linf norms simultaneously. However, the results are not surprising since there's more than 10% performance decreases compared to the original adversarial training under each particular attack. So it's still not clear whether we can get a model that simultaneously achieves L1, L2, Linf robust error comparable to original PGD training. [Performance] - It seems MSD is not always better than others (worst PGD and PGD Aug). For MNIST, MSD performs poorly on Linf norm and it's not clear why. - There's significant performance drop in clean accuracy, especially MSD on MNIST data. [Suggestions] - As mentioned before, studying the convergence properties of the proposed methods will be interesting. - It will be interesting if you can train on a set of perturbation models and make it also robust to another perturbation not in the training phase. For instance, can we apply the proposed method to L{1,inf} in training and generalize to L2 perturbation? ===== Thanks for the response. I still have concerns about novelty so would like to keep my rating unchanged. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your feedback . We definitely aimed for this to be a \u201c natural \u201d extension of adversarial , and we view the simplicity of the approach to be an advantage of the approach over relying on more complex methods . As a minor note , the extension goes beyond finding the worst-case projection : it is important to also consider the individual steepest descent directions for each threat model , so there is no singular gradient step like in PGD . On convergence : We do agree that studying the convergence properties could be interesting , however , this is not our focus and is out of the scope of this paper . This is actually a fairly complex problem : the convergence properties of steepest descent for a * single * norm ( to our knowledge ) for deep networks is not quite known . On performance : The main point is that while comparing individual threat models leads to an unclear conclusion about risk tradeoffs as you pointed out , the conclusion for the reader is clear when measuring performance in the \u201c all attacks \u201d mode . This is the metric that makes the most sense , since this is exactly the robust optimization objective being minimized by all the algorithms , and has a simple interpretation as measuring performance when a failure in even a single threat model is unacceptable . We will adjust the paper accordingly to make this more obvious . Please see our general comment here for a more detailed discussion on comparing performances , as well as on generalizing outside the threat model used during training : https : //openreview.net/forum ? id=rklMnyBtPB & noteId=rJgRBxZ-iB"}, "1": {"review_id": "rklMnyBtPB-1", "review_text": "Summary of the paper: The paper describes adversarial training aiming to build models that are robust to multiple adversarial attacks - with L_1, L_2 and L_inf norms. The method is a based on adversarial training against a union of adversaries. That union is created by taking (projected) gradient steps like PGD (Kurakin 2017), but choosing the maximal loss over GD steps for L1, L2, L_inf at each step. Strengths: The topic is trendy and interesting. The proposed algorithm is simple and easy to implement. The experimental results demonstrate improvement over several baselines. Weaknesses: -- I am missing a more systematic comparisons to baseline defenses in the experiments. Figures 2 and 3 should have shown the accuracy as a function of radius also for PGD-aug, PGD-worst, Schott et al. Also what about comparisons to the latest SoTA defenses, e.g. recent baselines from from www.robust-ml.org/defenses/. -- An implicit expectation from this paper is that it addresses the key issue of \"Defend against one attack but face a different attack\". The paper could have done more to advance our understanding of this issue. Specifically: The approach improves over baselines for the \"all attacks\" mode, but under-performs compared with PGDaug and PGDworst when attacked with a single norm (Tab 1). While this is expected and probably cannot be avoided, it leaves the reader with an unclear conclusion about risk tradeoffs. It would have been useful to clarify the regime of mixtures of attacks where the various approaches are best. For instance, if one uses a of mix attack samples from the three norms, what mixtures would it be best to defend using MSD, wand what mixtures would it be best to use PGD-aug? or ABS? ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and the provided suggestions . On the comparison to baseline defenses : Combining the robustness curves is a great suggestion , thank you . We do in fact have the accuracies as a function of radius for PGD-aug and PGD-worst ( we had put them in the appendix as Figures 4-7 for lack of space ) , but we can certainly combine them into a single plot for MNIST and CIFAR10 . As for the baselines at robust-ml.org , since the setting we study is the union of multiple threat models , we focus on baselines which also study defending against multiple threat models . To our knowledge , the only baseline on robust-ml.org which does this is the ABS model by Schott et al. , which we explicitly compare to in our paper . On comparing the performances against individual attacks and the corresponding risk tradeoffs : The main point is that while comparing individual threat models leads to an unclear conclusion about risk tradeoffs as you pointed out , the conclusion for the reader is quite clear when measuring performance in the \u201c all attacks \u201d mode . This is the metric that makes the most sense , since this is exactly the robust optimization objective being minimized by all the algorithms , and has a simple interpretation as measuring performance when failure in even a single threat model is unacceptable . If one wishes to instead defend against a different mixture of attacks , then it makes more sense to change the robust optimization objective to reflect the different mixture of attacks using MSD , rather than trying to obtain it ad-hoc with PGD-aug or PGD-worst using a different threat model . Please see our general comment here for a more detailed discussion on measuring and comparing performance : https : //openreview.net/forum ? id=rklMnyBtPB & noteId=rJgRBxZ-iB"}, "2": {"review_id": "rklMnyBtPB-2", "review_text": "This paper adversarially trains models against l_p norms where p is of there different values. They then propose a method which does somewhat better than the obvious way of adversarially training against more than one l_p perturbation. The motivation for the paper is limited, in that they suggest previous works have suggested adversarial training itself \"overfits\" to the given l_p norm. This isn't surprising that it works, since the straightforward baseline works. They make it seem surprising by suggesting that ABS suggested adversarial training is doomed and cannot provide robustness to l_1, l_2, l_\\infty norms simultaneously. The other motivation is that this is a step toward studying an expanded threat model, but the authors have not demonstrated that the learned representations are any bit more robust to common corruptions (could the authors show the generalization performance on CIFAR-10-C or generalization to unforeseen corruptions?). Without further evidence, we are left to believe this only helps for this narrow threat model. Overall the paper is deficient in creativity and generality, so I vote for rejection. Small comments: > take more time than a single norm, it is a step closer towards the end goal of truly robust models, with adversarial robustness against all perturbations. Please show model performance on CIFAR-10-C since if the model is more robust, it should hopefully be more robust to stochastic adversaries. > has claimed that adversarial training \u201coverfits\u201d to the particular type of perturbation used to generate the adversarial examples Wouldn't this be that l_\\infty training fits specifically to l_\\infty examples, not that robust optimization cannot handle more than one norm at a time? Who is claiming that? > First, we show that even simple aggregations of different adversarial attacks can achieve competitive universal robustness against multiple perturbations models without resorting to complex architectures. I am not sure this was in doubt. The phrase \"universal robustness\" is misleading. How were the budgets chosen for l_2 and l_1? Those values seem small.", "rating": "1: Reject", "reply_text": "Thank you for your feedback . Clarification on \u201c overfitting \u201d : First , we would like to clarify that we took the language of \u201c adversarial training overfits to the L-infinity norm \u201d directly from \u201c Towards the first adversarially robust neural network model on MNIST \u201d by Schott et al. , which was published at last year \u2019 s ICLR , and is where the claim comes from ( you can see this in the abstract ) . Of course , this is by no means a central point of the paper , and we merely wished to contextualize the result with relevant research on the same topic . We are quite willing to adjust the wording ( e.g.the referenced phrasing with respect to overfitting and universal robustness ) . On the motivation and significance of MSD : While it is correct that the straightforward baselines work , they only work to some degree and are suboptimal when measuring their performance * with respect to the robust performance metric at which they attempt to minimize * , namely the robust optimization objective which is the performance against the union of threat models . On both MNIST and CIFAR10 , we see a substantial increase in robust performance ( 5 % and 6 % respectively ) on the union threat model from MSD over the baselines . This shows that the baselines , while they work to some extent , make various implicit tradeoffs that don \u2019 t actually minimize the robust objective that they are trying to minimize , and so MSD is a more direct , explicit way of minimizing the robust loss over the union adversary . The baselines themselves are also not consistent across the datasets : PGD-Aug performs poorly on MNIST , while PGD-Worst performs poorly on CIFAR10 , whereas MSD is consistent across both problems . It is unfortunate that you think the approach is deficient in creativity and generality . Rather , we believe that the simplicity of the method adds to its strength , showing that even simple approaches can perform quite well without resorting to complex procedures . MSD is also general in that it can utilize any first-order iterative method for adversarial generation , and is not an image-specific defense ( it is at least as generally applicable as the standard adversarial training approach for a single threat model ) . On generalizing to unforeseen corruptions : Defending against attacks outside of the threat model has never been a goal of adversarial training , and has little theoretical justification for why this would be the case . As such , performance comparisons on out-of-threat-model attacks like CIFAR-10-C , while potentially interesting , are completely orthogonal to the point of the paper . See our general comment here for a more detailed discussion : https : //openreview.net/forum ? id=rklMnyBtPB & noteId=rJgRBxZ-iB However , despite this , since CIFAR-10-C isn \u2019 t too expensive to evaluate , we ran this anyways just to see what happens and got the following mean accuracies : Standard model : 66.52 % PGD-Worst : 70.8 % PGD-Aug : 76.84 % MSD : 74.22 % So indeed , all of the approaches appear to improve model performance on CIFAR-10-C in comparison to standard training to some degree . However , because none of the models were explicitly trained to minimize these sorts of corruptions , we refrain from making any further conclusions . On the budgets chosen for L1 and L2 : The chosen budgets all come from the literature . For MNIST , we chose the same budget as that used in \u201c Towards the First Adversarially Robust Neural Network Model on MNIST \u201d [ Schott et al.2019 ] in order to be directly comparable and most fair in the comparison . For CIFAR10 budgets comes from \u201c Towards Evaluating the Robustness of Neural Networks \u201d [ Carlini & Wagner 2017 ] , though we used a smaller L1 budget to account for the difference from L0 to L1 and to not entirely subsume the other threat models ."}}