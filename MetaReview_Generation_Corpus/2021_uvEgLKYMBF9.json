{"year": "2021", "forum": "uvEgLKYMBF9", "title": "Variance Reduction in Hierarchical Variational Autoencoders", "decision": "Reject", "meta_review": "This paper develops a smoothing procedure to avoid the problem of posterior collapse in VAEs. The method is interesting and novel, the experiments are well executed, and the authors answered satisfactorily to most of the reviewers' concerns. However, there is one remaining issue that would require additional discussion. As identified by Reviewer 1, the analysis in Section 3 is only valid when the number of layers is 2. Above that value, \"it is possible to construct models where the ELBO has a reasonable value, but the smoothed objective behaves catastrophically\". Thus, the scope of the analysis in Section 3 deserves further discussion. Given the large number of ICLR submissions, this paper unfortunately does not meet the acceptance bar. That said, I encourage the authors to address this point and resubmit the paper to another (or the same) venue.", "reviews": [{"review_id": "uvEgLKYMBF9-0", "review_text": "This paper proposes a Hermite variational auto-encoder which use Ornstein Uhlenbeck Semi-group to p ( z_i|z_i+1 ) which i denotes the latent layer number . It has clear theoretical inspiration and had solid analysis on variance reduction . Pros : Quality : The paper 's generic theoretical motivation and analysis is with high quality . Clarify : The paper 's presentation is clear . Originality : This paper provides a new perspective and used mathematically tools of Hermite expansion etc to inspire and proposed new method for variance reduction which prevent dying unity problem in Hierarchical VAE . Although motived by advanced tools , but the application of the method in vanilla version of hierarchical VAE ( in term of implementation ) seems very simple . Thus , the method looks easy to adopt . Cons and questions : Significance : 1 ) Does the method work for vanilla VAE with only one level of z ? It seems that it is only applicable to the hierarchical version as the operator is applied in p ( z_i|z_i+1 ) and if it is one level , it lost the point due to single Normal prior . This may limit the application impact as VAE is much more widely adapted in different applications comparing to hierarchical VAE . 2 ) I am not sure making unit not dying at all is desired ( such as shown in last column of table one or Figure 3.Being Bayesian with the prior , there is a natural model selection behavior ( implicit Occam 's Razor ) , thus , behavor such as the method with active unints ( 40,40,40,24 ) in table one may not be desired and rather a bit weird as only the last layer have dying units . Behavior such as VAE+KL ( 2,3 , 11 , 37 ) looks more natural to me as simpler model is needed in high hierarchy . 3 ) Experiments only compared to Ladder VAE in number of dying unit but not in term of ELBO for performance . As LVAE is one of the most known work in this domain and also mentioned first in the related work section ) , this is weird . In LVAE paper , the MNIST performance is reported as -81-82 while in the paper it is about -85 which is significantly worse . Although there is a chance due to minior setting differences , I doubt the method 's performance can match LVAE . ( Again with 2 ) , I do n't think that puring comparing the number of units without reporting performance makes sense ) . 4 ) in term of performance of ELBO , most of the time , it does not match simple KL annealing either . 5 ) there are more highly related work anaylsis the variance-bias trade off such as Tighter Variational Bounds are Not Necessarily Better are not discussed in the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for appreciating the contribution . We hope to address the concerns below . > Cons and questions : Significance : > Does the method work for vanilla VAE with only one level of z ? It seems that it is only applicable to the hierarchical version as the operator is applied in p ( z_i|z_i+1 ) and if it is one level , it lost the point due to single Normal prior . This may limit the application impact as VAE is much more widely adapted in different applications comparing to hierarchical VAE . Posterior collapse is most clearly observed in bigger ( hierarchical ) stochastic depths in VAEs , with strong autoregressive decoders . Or , as Figure 3 in the paper shows , in shallower but wider VAE architectures . Existing methods for reducing collapse already work quite well with shallow models ( as long as the decoder is not too powerful ) . Although the focus of our paper in posterior collapse with increasing stochastic depth , the proposed model works just as well for a single stochastic layer . We show this in Table 2 in A.2 in the Appendix , where Hermite VAE improves latent activity ( compared to regular VAE ) on single stochastic layer architecture on MNIST ( 18 active units for VAE v. 31 for HVAE out of 64 ) . We conclude that the proposed Hermite VAE can also work with single stochastic layers . > I am not sure making unit not dying at all is desired ( such as shown in last column of table one or Figure 3.Being Bayesian with the prior , there is a natural model selection behavior ( implicit Occam 's Razor ) , thus , behavor such as the method with active unints ( 40,40,40,24 ) in table one may not be desired and rather a bit weird as only the last layer have dying units . Behavior such as VAE+KL ( 2,3 , 11 , 37 ) looks more natural to me as simpler model is needed in high hierarchy . We test the hypothesis of whether posterior collapse does some sort of feature selection pruning in higher layers as follows . We train multiple 4-layer VAEs+KL annealing with latent dimensions : { 40,40,40,40 } , { 30,30,30,30 } , { 20,20,20,20 } , { 10,10,10,10 } , { 5,5,5,5 } . If posterior collapse is a means for feature selection pruning , we would expect a good balance between ( A ) the validation ELBO while ( B ) attaining larger relative top layer activity ( active/total units ) for smaller networks . The results are ( see Appendix A.4 for details ) : |Model | V. ELBO | Active units | Relative activity | | : | : :| : - : | : - : | |40-40-40-40 | -84.73 | 1,6,15,40 | 2.5 % , 15 % , 37.5 % , 100 % | |30-30-30-30 | -88.7 |1,6,13,30 | 2.5 % , 15 % , 32.5 % , 75 % | |20-20-20-20 | -85.4 | 1,5,12,20 | 2.5 % , 12.5 % , 30 % , 50 % | |10-10-10-10 | -90.47 | 2,3,9,10 | 5 % , 7.5 % , 22.5 % , 25 % | |5-5-5-5 |-104.36 | 1,3,5,5 | 2.5 % , 7.5 % , 12.5 % , 12.5 % | In summary , with smaller latent dimension the validation ELBO becomes worse . At the same time , the relative top layer activity remains the same . If the model was performing feature selection , we would expect not to have worse validation ELBO alongside low top layer activity . Especially for the smaller models . We conclude that , according to experiments , there is no sign that posterior collapse is the way of the model to do feature selection pruning in the higher layers ."}, {"review_id": "uvEgLKYMBF9-1", "review_text": "Post-rebuttal update Thank you for your response . Now I understand that the algorithm works by smoothing the Gaussian parameters $ \\mu_i , \\sigma_i $ w.r.t.the centered Gaussian rv ( as described in my last reply , second part of bullet point ( 1 ) ) , so my original concern regarding the bias _in the Gaussian parameters_ does not hold . However , I still can not recommend acceptance at this point , because of a newly discovered issue in the theoretical analysis : The analysis in Section 3 does not take into account the impact of smoothing on the `` downstream '' nonlinear layers . The text only considers two layers of stochastic latents and the KL part of ELBO , but in the deeper case , the smoothing of $ \\mu_i ( z_ { i+1 } ) $ will additionally have influence on the layers below $ i $ , through the nonlinear functions $ \\mu_ { i ' } , \\sigma_ { i ' } $ for $ i ' < i $ . More concretely , consider the following scenario : $ \\mu_i ( z_ { i+1 } ) \\equiv z_ { i+1 } , \\sigma_i ( z_ { i+1 } ) \\equiv \\epsilon $ which is very small . Further assume that $ z_ { i+1 } $ is high-dimensional and approximately follows $ \\mathcal { N } ( 0 , I ) $ , so $ \\\\|z_i\\\\|_2 = \\\\|\\mu_i ( z_ { i+1 } ) +\\sigma_i\\varepsilon_i\\\\|_2 \\approx \\\\| z_ { i+1 } \\\\|_2 > 100 $ with probability $ 1-\\epsilon_1 $ , where $ \\epsilon_1 $ is also very small . In this case , it is possible to achieve a low KL in the original ELBO , by using a $ \\mu_ { i-1 } $ which only has sensible values in the region $ B : = \\\\ { z_i : \\\\|z_i\\\\| > 100\\\\ } $ ; in the complement set $ B^c $ , $ \\mu_ { i-1 } $ can be `` arbitrarily '' bad so long as its impact on the ELBO does not outweigh $ \\epsilon_1 $ , the probability its input falls there . However , in the smoothed estimator with $ \\rho=0 $ , the input to $ \\mu_ { i-1 } $ only have norm $ O_p ( \\sigma_i ( z_ { i+1 } ) ) =O_p ( \\epsilon ) $ , so the value of $ \\mu_ { i-1 } $ on $ B^c $ will have a far higher impact , easily exceeding the original by $ O ( 1/\\epsilon_1 ) $ . To summarize , * it is possible to construct models where the ELBO has a reasonable value , but the smoothed objective behaves catastrophically * . Moreover , even in the shallow case , $ z_i $ will be fed into a final decoder block to generate the reconstruction image , so a similar issue exists , although it will be in the reconstruction likelihood part of the ELBO as opposed to the KL part . A less important issue is that parts of the analysis are written in a confusing way . Apart from the abuse of notation $ U_\\rho $ which leads to my original confusion , in Section 3 the $ \\hat { \\mu } _p $ 's should have a suffix of $ z_1 $ , to signify the fact that they are coefficients of a function that depends on $ z_1 $ ( see the last response from he authors ) . Also it is unclear to me why there is no mention of $ \\mu_p^4 $ , in the analysis of the variance of an estimator for $ \\mu_p^2 $ . But given the aforementioned issue , I do n't think it is necessary to look further into this case . Summary - This work proposes to smooth the mean and variance parameters in the decoder of hierarchical VAEs with the O-U process . It is shown that the smoothing procedure reduces variance of the ELBO , alleviates posterior collapse , and improves on model likelihood on CIFAR-10 under a fixed number-of-parameter budget . The idea to investigate the impact of ELBO variance in hierarchical VAE performance is sensible , and the experiments seem to show improvements . However , I have concerns regarding the theoretical claims , and the empirical results also seem to need clarification . Major Concerns -- - The claim that the smoothing does n't change the expectation ( of functions acting on the latents ) does n't seem correct . Prop.1 and 2 only holds when the expectation is taken wrt the standard normal distribution , while all but the top-level latents ( i.e. , $ z_i $ for $ i < L $ ) come from a mixture of Gaussian . Intuitively it also seems incorrect : what if $ \\rho=0 $ ? - The variance analysis works by assuming $ \\sigma_q $ , the decoder variance , is constant . This ignores the problem of unbounded likelihood [ 1 ] , where posterior variance goes to zero , thus driving the ELBO and its variance to infinity . It would be helpful to include a plot of the decoder variances in the most realistic model , to see if this issue is relevant in modern hierarchical VAEs ( and thus whether the analysis here provides a complete picture ) . - The conclusion of the analysis does not seem helpful : the bias is $ O ( 1-\\rho^2 ) $ and the variance is $ O ( \\rho^2 ) $ , so it is unclear from the bound whether there will be a $ \\rho $ that decreases the overall MSE . Minor -- - It is worth mentioning that there are several types of posterior collapse and not all of them are undesirable [ 2 ] : sometimes it is superfluous units rightfully pruned [ 3 , 4 ] . This also implies that the number of active units is not a good measure of model quality ; it is helpful to include reconstruction error in Section 5.1 . - The observed phase transition of KLD connects to the fact that ELBO-trained VAE acts like a thresholding operator ; see [ 2 ] . - Why did n't Table 3 mention NVAE [ 5 ] and IAF-VAE [ 6 ] , both of which have better BPD values ? Seeing where those models are on the # parameters-BPD curve helps to put the results here in perspective . References - [ 1 ] : Mattei and Frellsen , Leveraging the Exact Likelihood of Deep Latent Variable Models , in NeurIPS 18 . [ 2 ] : Dai et al , The Usual Suspects ? Reassessing Blame for VAE Posterior Collapse , in ICML 20 . [ 3 ] : Lucas et al , Do n't Blame the ELBO ! A Linear VAE Perspective on Posterior Collapse , in NeurIPS 19 . [ 4 ] : Dai and Wipf , Diagnosing and Enhancing VAE Models , in ICLR 19 . [ 5 ] : Vahdat and Kautz , NVAE : A Deep Hierarchical Variational Autoencoder , in NeurIPS 20 . [ 6 ] : Kingma et al , Improved variational inference with inverse autoregressive flow , in ICLR 16 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We appreciate the comments and attempt to address the concerns below . # # Major > The claim that the smoothing does n't change the expectation ( of functions acting on the latents ) does n't seem correct . Prop.1 and 2 only holds when the expectation is taken wrt the standard normal distribution , while all but the top-level latents ( i.e. , ... for ... ) come from a mixture of Gaussian . Intuitively it also seems incorrect : what if ... ? The reviewer is correct in that the marginal distribution , $ p ( z_i ) $ , of the incoming latent variables is a mixture of Gaussians . However , in the analysis we only work with conditional distributions , $ p ( z_i|z_ { i+1 } ) $ . To conclude , the smoothing by $ \\rho $ does not change the expectation and the propositions hold . In that case for $ \\rho=0 $ , the output is the expected value of the function . We have made a clarification in the text on page 4 paragraph 1 . > The variance analysis works by assuming ... , the decoder variance , is constant . This ignores the problem of unbounded likelihood [ 1 ] , where posterior variance goes to zero , thus driving the ELBO and its variance to infinity . It would be helpful to include a plot of the decoder variances in the most realistic model , to see if this issue is relevant in modern hierarchical VAEs ( and thus whether the analysis here provides a complete picture ) . There is a miscommunication here . To understand posterior collapse , our analysis focuses only on the KL divergence term of the ELBO . It makes no reference to the output model of the decoder of the generative model . To address the core of the question , we note that the problem of unbounded likelihood depends on the output model for the decoder . As mentioned in the paper cited by the reviewer , for instance , Gaussian output models face the problem of unbounded likelihoods while Bernoulli output models do not . Either case does not affect our analysis . To conclude , the bearing of the decoder on unbounded likelihoods is definitely important for any VAEs but we focus on posterior collapse in the KL term specifically . In VAE implementation it is common practice to constrain the variance when using Gaussian decoders . That constraining the variance prevents unbounded likelihoods is justified by proposition 2 of the cited paper ( Mattei and Frellsen , 2018 ) . Nevertheless to check whether realistic VAEs might suffer from this problem when variance is left unconstrained , we used a ResNet decoder VAE on CIFAR with Gaussian output with an unconstrained variance and track the minimum variance over dimensions during training . We add a plot of the minimum decoder output variance alongside training and validation ELBO with the Gaussian decoder on CIFAR on Appendix A.5 . We observe that even when unconstrained the variance remains bounded away from 0 and that training and validation ELBOs are quite similar . > The conclusion of the analysis does not seem helpful : the bias is .. and the variance is .. , so it is unclear from the bound whether there will be a that decreases the overall MSE . The usefulness of the analysis is in justifying the smoothing operation of OU sampling , showing that it reduces variance at the cost of bias . The analysis does not focus on the output likelihood , only the KL term to analyze the effect of our method in addressing posterior collapse . The analysis does not derive , therefore , a single value of the parameter $ \\rho $ that provably gives the best bound . Rather our intent is to take the analysis in conjunction with the experimental results . Experiments indeed show a consistent behavior and phase transitioning as we change $ \\rho $ . Specifically , experiments corroborate that there exists a single value of $ \\rho $ for which models consistently undergoes posterior collapse for different datasets given the same architecture . To derive an optimal $ \\rho $ that provably gives prevents collapse we would require analysis of the decoder . This is a good suggestion . As it is out of scope of the current paper , which focuses on posterior collapse , we leave the suggestion for future work ."}, {"review_id": "uvEgLKYMBF9-2", "review_text": "1.Summary This paper studies the training of deep hierarchical VAEs and focuses on the problem of posterior collapse . It is argued that reducing the variance of the gradient estimate may help to overcome posterior collapse . The authors focus on reducing the variance of the functions parameterizing the variational distribution of each layer using a layer-wise smoothing operator based on the Ornstein-Uhlenbeck semigroup ( parameterized by a parameter $ \\rho $ ) . The operator requires additional Monte-Carlo samples . The authors provide an analytical analysis of bias and variance . Last they train multiple VAEs models , measure the posterior collapse and observe a phase transition behaviour depending on the parameter $ \\rho $ . 2. a Strong Points This paper introduced a theoretically grounded solution to the problem of posterior collapse . In particular , it is discussed that variance may be an issue . Great efforts were invested to study the behaviour of the Hermite VAE in theoretical terms and the authors provide analytical results on the Bias and Variance for this estimator . 2. b Weak Points * Complexity In the main text , it is written that `` * experiments show that 5 or 10 samples suffice * '' . This is a major drawback for Hermite VAEs and the complexity of the algorithm is not discussed , nor it is studied empirically . Given 5 MC samples , I interpret that HVAE is 5 times more expensive than other approaches -- please clarify this point . * Empirical study of the variance The problem of the variance is discussed in the paper but left apart in the experimental section . I would expect the authors to measure the variance ( and/or SNR ) of the gradients for the HVAE objective , the VAE , for advanced estimators such as STL and DReG . A study is required to corroborate the claim that reducing variance overcomes posterior collapse . * Experiments on posterior collapse I am surprised to see that none of the existing methods ( KL warmup and freebits ) allows overcoming posterior collapse ( Figure 1 ) . At least using the right amount of freebits should improve the results ( the number of freebits is not reported ) . Furthermore , the authors should report the KL divergence in the benchmark experiment . * Experimental protocol I do n't understand why VAE models trained in section 5 only have 2 layers whereas HVAE uses 4 layers : this is not a fair comparison . Furthermore , LVAE should be studied on the basis of posterior collapse -- not only in terms of likelihood . 3.Recommendation Unfortunately , based on the current form of the paper , I recommend rejecting this paper . 4.Recommendation Arguments Despite the good theoretical contributions , I do not find the experimental section to be strong enough to support the claims . In particular , the cost induced by the additional MC samples is not discussed and methods are hence not compared on the same basis . 5.Questions to the Author - What is the complexity of HVAE ? Do the VAE models use multiple MC samples as well ? - Why using only 2 layers for the VAE models ? - How are the freebits and KL-warmup applied in figure 1 ? 6.Feedback Your work is very relevant and the theoretical insights are very interesting , this work would greatly benefit from an improved experimental section . In the first page , two typos : - you defined $ q ( z | x ) $ and not $ q ( x , z ) $ - The KL divergence in equation 1 should depend on $ q ( z_i | z_ { i-1 } ) $ and $ p ( z_i | z_ { i+1 } ) $", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the review . We try to address the concerns below . > Complexity In the main text , it is written that `` experiments show that 5 or 10 samples suffice '' . This is a major drawback for Hermite VAEs and the complexity of the algorithm is not discussed , nor it is studied empirically . Given 5 MC samples , I interpret that HVAE is 5 times more expensive than other approaches -- please clarify this point . We do provide a timing comparison in the appendix ( A.3 ) for a 4 layer model on static MNIST . Comparisons show that our method bears a small only a extra cost in time per epoch ( 4.6 sec for our model , 4.2 sec for VAE , 4.4 sec for IWAE ) . The complexity of the method is approximately on-par with IWAE , which actually has more parallel computations than us . We will add a reference in the main paper . The reason for the smaller added complexity is that the OU sampling operation is only applied in chosen parts of the model . In particular , we do not apply the sampling operation in the inference network . Secondly , we do not apply the operation in the last stochastic layer of the decoder . The overall complexity ( for 5 MC samples ) is significantly less than if we were to repeat 5 samples with the original complexity . > Empirical study of the variance The problem of the variance is discussed in the paper but left apart in the experimental section . I would expect the authors to measure the variance ( and/or SNR ) of the gradients for the HVAE objective , the VAE , for advanced estimators such as STL and DReG . A study is required to corroborate the claim that reducing variance overcomes posterior collapse . We have added a gradient variance comparison between VAE and HVAE in the Appendix A.5 . Experiments indeed show that variance is reduced with our Hermite VAEs . However , there are a few points which need to be emphasized about our method . First , since we have a variance-bias trade-off we can not conclusively say variance reduction ( at least on its own ) is responsible for elimination of posterior collapse . We do claim , however , that OU smoothing , which also reduces variance , does mitigate posterior collapse . We will clarify this in the introduction . Furthermore , since the smoothing operator introduces a bias-variance trade-off , a direct comparison of variance with other unbiased methods is not meaningful . Also , it is not sufficient to establish the requisite link between smoothing and collapse . Last , we include a study on the relation between smoothing ( and variance reduction thereafter ) and posterior collapse in figure 2 . The parameter $ \\rho $ controls the amount of smoothing ( and variance reduction ) . By increasing $ \\rho $ ( thus less smoothing , more variance ) leads to posterior collapse . Taken together with the theory , this study establishes a link between smoothing and posterior collapse . > Experiments on posterior collapse I am surprised to see that none of the existing methods ( KL warmup and freebits ) allows overcoming posterior collapse ( Figure 1 ) . At least using the right amount of freebits should improve the results ( the number of freebits is not reported ) . Furthermore , the authors should report the KL divergence in the benchmark experiment . We disagree with the conclusion . KL annealing and free bits do help in mitigating posterior collapse for shallow hierarchies of stochastic variables , annealing being more effective . For instance , in tables 1 and 2 ( updated version ) KL annealing and free bits do lead to more active units , especially in the lower levels , and a somewhat larger top layer KL divergence than a standard VAE . However these techniques are not very effective at overcoming collapse in deeper hierarchies , which is the motivation of our work . [ We assume the reference here is to table 1 and not figure 1 where we plot KLD and validation ELBO curves for the vanilla VAE . ] > Experimental protocol I do n't understand why VAE models trained in section 5 only have 2 layers whereas HVAE uses 4 layers : this is not a fair comparison . Furthermore , LVAE should be studied on the basis of posterior collapse -- not only in terms of likelihood . The VAE models showed in section 5 in figures 1 & 2 and tables 1 & 2 ( updated version ) all were trained with 4 stochastic layers . The only place we show a 2-layer VAE is in table 4 where we compare against models reported in the literature . We were unable to find a case of a vanilla VAE deeper than 2 layers with better performance on this dataset in the literature . We will clarify this in the text . We have now also added a comparison to the 5 layer LVAE in table 5 . Hermite VAE outperforms all baselines ."}], "0": {"review_id": "uvEgLKYMBF9-0", "review_text": "This paper proposes a Hermite variational auto-encoder which use Ornstein Uhlenbeck Semi-group to p ( z_i|z_i+1 ) which i denotes the latent layer number . It has clear theoretical inspiration and had solid analysis on variance reduction . Pros : Quality : The paper 's generic theoretical motivation and analysis is with high quality . Clarify : The paper 's presentation is clear . Originality : This paper provides a new perspective and used mathematically tools of Hermite expansion etc to inspire and proposed new method for variance reduction which prevent dying unity problem in Hierarchical VAE . Although motived by advanced tools , but the application of the method in vanilla version of hierarchical VAE ( in term of implementation ) seems very simple . Thus , the method looks easy to adopt . Cons and questions : Significance : 1 ) Does the method work for vanilla VAE with only one level of z ? It seems that it is only applicable to the hierarchical version as the operator is applied in p ( z_i|z_i+1 ) and if it is one level , it lost the point due to single Normal prior . This may limit the application impact as VAE is much more widely adapted in different applications comparing to hierarchical VAE . 2 ) I am not sure making unit not dying at all is desired ( such as shown in last column of table one or Figure 3.Being Bayesian with the prior , there is a natural model selection behavior ( implicit Occam 's Razor ) , thus , behavor such as the method with active unints ( 40,40,40,24 ) in table one may not be desired and rather a bit weird as only the last layer have dying units . Behavior such as VAE+KL ( 2,3 , 11 , 37 ) looks more natural to me as simpler model is needed in high hierarchy . 3 ) Experiments only compared to Ladder VAE in number of dying unit but not in term of ELBO for performance . As LVAE is one of the most known work in this domain and also mentioned first in the related work section ) , this is weird . In LVAE paper , the MNIST performance is reported as -81-82 while in the paper it is about -85 which is significantly worse . Although there is a chance due to minior setting differences , I doubt the method 's performance can match LVAE . ( Again with 2 ) , I do n't think that puring comparing the number of units without reporting performance makes sense ) . 4 ) in term of performance of ELBO , most of the time , it does not match simple KL annealing either . 5 ) there are more highly related work anaylsis the variance-bias trade off such as Tighter Variational Bounds are Not Necessarily Better are not discussed in the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for appreciating the contribution . We hope to address the concerns below . > Cons and questions : Significance : > Does the method work for vanilla VAE with only one level of z ? It seems that it is only applicable to the hierarchical version as the operator is applied in p ( z_i|z_i+1 ) and if it is one level , it lost the point due to single Normal prior . This may limit the application impact as VAE is much more widely adapted in different applications comparing to hierarchical VAE . Posterior collapse is most clearly observed in bigger ( hierarchical ) stochastic depths in VAEs , with strong autoregressive decoders . Or , as Figure 3 in the paper shows , in shallower but wider VAE architectures . Existing methods for reducing collapse already work quite well with shallow models ( as long as the decoder is not too powerful ) . Although the focus of our paper in posterior collapse with increasing stochastic depth , the proposed model works just as well for a single stochastic layer . We show this in Table 2 in A.2 in the Appendix , where Hermite VAE improves latent activity ( compared to regular VAE ) on single stochastic layer architecture on MNIST ( 18 active units for VAE v. 31 for HVAE out of 64 ) . We conclude that the proposed Hermite VAE can also work with single stochastic layers . > I am not sure making unit not dying at all is desired ( such as shown in last column of table one or Figure 3.Being Bayesian with the prior , there is a natural model selection behavior ( implicit Occam 's Razor ) , thus , behavor such as the method with active unints ( 40,40,40,24 ) in table one may not be desired and rather a bit weird as only the last layer have dying units . Behavior such as VAE+KL ( 2,3 , 11 , 37 ) looks more natural to me as simpler model is needed in high hierarchy . We test the hypothesis of whether posterior collapse does some sort of feature selection pruning in higher layers as follows . We train multiple 4-layer VAEs+KL annealing with latent dimensions : { 40,40,40,40 } , { 30,30,30,30 } , { 20,20,20,20 } , { 10,10,10,10 } , { 5,5,5,5 } . If posterior collapse is a means for feature selection pruning , we would expect a good balance between ( A ) the validation ELBO while ( B ) attaining larger relative top layer activity ( active/total units ) for smaller networks . The results are ( see Appendix A.4 for details ) : |Model | V. ELBO | Active units | Relative activity | | : | : :| : - : | : - : | |40-40-40-40 | -84.73 | 1,6,15,40 | 2.5 % , 15 % , 37.5 % , 100 % | |30-30-30-30 | -88.7 |1,6,13,30 | 2.5 % , 15 % , 32.5 % , 75 % | |20-20-20-20 | -85.4 | 1,5,12,20 | 2.5 % , 12.5 % , 30 % , 50 % | |10-10-10-10 | -90.47 | 2,3,9,10 | 5 % , 7.5 % , 22.5 % , 25 % | |5-5-5-5 |-104.36 | 1,3,5,5 | 2.5 % , 7.5 % , 12.5 % , 12.5 % | In summary , with smaller latent dimension the validation ELBO becomes worse . At the same time , the relative top layer activity remains the same . If the model was performing feature selection , we would expect not to have worse validation ELBO alongside low top layer activity . Especially for the smaller models . We conclude that , according to experiments , there is no sign that posterior collapse is the way of the model to do feature selection pruning in the higher layers ."}, "1": {"review_id": "uvEgLKYMBF9-1", "review_text": "Post-rebuttal update Thank you for your response . Now I understand that the algorithm works by smoothing the Gaussian parameters $ \\mu_i , \\sigma_i $ w.r.t.the centered Gaussian rv ( as described in my last reply , second part of bullet point ( 1 ) ) , so my original concern regarding the bias _in the Gaussian parameters_ does not hold . However , I still can not recommend acceptance at this point , because of a newly discovered issue in the theoretical analysis : The analysis in Section 3 does not take into account the impact of smoothing on the `` downstream '' nonlinear layers . The text only considers two layers of stochastic latents and the KL part of ELBO , but in the deeper case , the smoothing of $ \\mu_i ( z_ { i+1 } ) $ will additionally have influence on the layers below $ i $ , through the nonlinear functions $ \\mu_ { i ' } , \\sigma_ { i ' } $ for $ i ' < i $ . More concretely , consider the following scenario : $ \\mu_i ( z_ { i+1 } ) \\equiv z_ { i+1 } , \\sigma_i ( z_ { i+1 } ) \\equiv \\epsilon $ which is very small . Further assume that $ z_ { i+1 } $ is high-dimensional and approximately follows $ \\mathcal { N } ( 0 , I ) $ , so $ \\\\|z_i\\\\|_2 = \\\\|\\mu_i ( z_ { i+1 } ) +\\sigma_i\\varepsilon_i\\\\|_2 \\approx \\\\| z_ { i+1 } \\\\|_2 > 100 $ with probability $ 1-\\epsilon_1 $ , where $ \\epsilon_1 $ is also very small . In this case , it is possible to achieve a low KL in the original ELBO , by using a $ \\mu_ { i-1 } $ which only has sensible values in the region $ B : = \\\\ { z_i : \\\\|z_i\\\\| > 100\\\\ } $ ; in the complement set $ B^c $ , $ \\mu_ { i-1 } $ can be `` arbitrarily '' bad so long as its impact on the ELBO does not outweigh $ \\epsilon_1 $ , the probability its input falls there . However , in the smoothed estimator with $ \\rho=0 $ , the input to $ \\mu_ { i-1 } $ only have norm $ O_p ( \\sigma_i ( z_ { i+1 } ) ) =O_p ( \\epsilon ) $ , so the value of $ \\mu_ { i-1 } $ on $ B^c $ will have a far higher impact , easily exceeding the original by $ O ( 1/\\epsilon_1 ) $ . To summarize , * it is possible to construct models where the ELBO has a reasonable value , but the smoothed objective behaves catastrophically * . Moreover , even in the shallow case , $ z_i $ will be fed into a final decoder block to generate the reconstruction image , so a similar issue exists , although it will be in the reconstruction likelihood part of the ELBO as opposed to the KL part . A less important issue is that parts of the analysis are written in a confusing way . Apart from the abuse of notation $ U_\\rho $ which leads to my original confusion , in Section 3 the $ \\hat { \\mu } _p $ 's should have a suffix of $ z_1 $ , to signify the fact that they are coefficients of a function that depends on $ z_1 $ ( see the last response from he authors ) . Also it is unclear to me why there is no mention of $ \\mu_p^4 $ , in the analysis of the variance of an estimator for $ \\mu_p^2 $ . But given the aforementioned issue , I do n't think it is necessary to look further into this case . Summary - This work proposes to smooth the mean and variance parameters in the decoder of hierarchical VAEs with the O-U process . It is shown that the smoothing procedure reduces variance of the ELBO , alleviates posterior collapse , and improves on model likelihood on CIFAR-10 under a fixed number-of-parameter budget . The idea to investigate the impact of ELBO variance in hierarchical VAE performance is sensible , and the experiments seem to show improvements . However , I have concerns regarding the theoretical claims , and the empirical results also seem to need clarification . Major Concerns -- - The claim that the smoothing does n't change the expectation ( of functions acting on the latents ) does n't seem correct . Prop.1 and 2 only holds when the expectation is taken wrt the standard normal distribution , while all but the top-level latents ( i.e. , $ z_i $ for $ i < L $ ) come from a mixture of Gaussian . Intuitively it also seems incorrect : what if $ \\rho=0 $ ? - The variance analysis works by assuming $ \\sigma_q $ , the decoder variance , is constant . This ignores the problem of unbounded likelihood [ 1 ] , where posterior variance goes to zero , thus driving the ELBO and its variance to infinity . It would be helpful to include a plot of the decoder variances in the most realistic model , to see if this issue is relevant in modern hierarchical VAEs ( and thus whether the analysis here provides a complete picture ) . - The conclusion of the analysis does not seem helpful : the bias is $ O ( 1-\\rho^2 ) $ and the variance is $ O ( \\rho^2 ) $ , so it is unclear from the bound whether there will be a $ \\rho $ that decreases the overall MSE . Minor -- - It is worth mentioning that there are several types of posterior collapse and not all of them are undesirable [ 2 ] : sometimes it is superfluous units rightfully pruned [ 3 , 4 ] . This also implies that the number of active units is not a good measure of model quality ; it is helpful to include reconstruction error in Section 5.1 . - The observed phase transition of KLD connects to the fact that ELBO-trained VAE acts like a thresholding operator ; see [ 2 ] . - Why did n't Table 3 mention NVAE [ 5 ] and IAF-VAE [ 6 ] , both of which have better BPD values ? Seeing where those models are on the # parameters-BPD curve helps to put the results here in perspective . References - [ 1 ] : Mattei and Frellsen , Leveraging the Exact Likelihood of Deep Latent Variable Models , in NeurIPS 18 . [ 2 ] : Dai et al , The Usual Suspects ? Reassessing Blame for VAE Posterior Collapse , in ICML 20 . [ 3 ] : Lucas et al , Do n't Blame the ELBO ! A Linear VAE Perspective on Posterior Collapse , in NeurIPS 19 . [ 4 ] : Dai and Wipf , Diagnosing and Enhancing VAE Models , in ICLR 19 . [ 5 ] : Vahdat and Kautz , NVAE : A Deep Hierarchical Variational Autoencoder , in NeurIPS 20 . [ 6 ] : Kingma et al , Improved variational inference with inverse autoregressive flow , in ICLR 16 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We appreciate the comments and attempt to address the concerns below . # # Major > The claim that the smoothing does n't change the expectation ( of functions acting on the latents ) does n't seem correct . Prop.1 and 2 only holds when the expectation is taken wrt the standard normal distribution , while all but the top-level latents ( i.e. , ... for ... ) come from a mixture of Gaussian . Intuitively it also seems incorrect : what if ... ? The reviewer is correct in that the marginal distribution , $ p ( z_i ) $ , of the incoming latent variables is a mixture of Gaussians . However , in the analysis we only work with conditional distributions , $ p ( z_i|z_ { i+1 } ) $ . To conclude , the smoothing by $ \\rho $ does not change the expectation and the propositions hold . In that case for $ \\rho=0 $ , the output is the expected value of the function . We have made a clarification in the text on page 4 paragraph 1 . > The variance analysis works by assuming ... , the decoder variance , is constant . This ignores the problem of unbounded likelihood [ 1 ] , where posterior variance goes to zero , thus driving the ELBO and its variance to infinity . It would be helpful to include a plot of the decoder variances in the most realistic model , to see if this issue is relevant in modern hierarchical VAEs ( and thus whether the analysis here provides a complete picture ) . There is a miscommunication here . To understand posterior collapse , our analysis focuses only on the KL divergence term of the ELBO . It makes no reference to the output model of the decoder of the generative model . To address the core of the question , we note that the problem of unbounded likelihood depends on the output model for the decoder . As mentioned in the paper cited by the reviewer , for instance , Gaussian output models face the problem of unbounded likelihoods while Bernoulli output models do not . Either case does not affect our analysis . To conclude , the bearing of the decoder on unbounded likelihoods is definitely important for any VAEs but we focus on posterior collapse in the KL term specifically . In VAE implementation it is common practice to constrain the variance when using Gaussian decoders . That constraining the variance prevents unbounded likelihoods is justified by proposition 2 of the cited paper ( Mattei and Frellsen , 2018 ) . Nevertheless to check whether realistic VAEs might suffer from this problem when variance is left unconstrained , we used a ResNet decoder VAE on CIFAR with Gaussian output with an unconstrained variance and track the minimum variance over dimensions during training . We add a plot of the minimum decoder output variance alongside training and validation ELBO with the Gaussian decoder on CIFAR on Appendix A.5 . We observe that even when unconstrained the variance remains bounded away from 0 and that training and validation ELBOs are quite similar . > The conclusion of the analysis does not seem helpful : the bias is .. and the variance is .. , so it is unclear from the bound whether there will be a that decreases the overall MSE . The usefulness of the analysis is in justifying the smoothing operation of OU sampling , showing that it reduces variance at the cost of bias . The analysis does not focus on the output likelihood , only the KL term to analyze the effect of our method in addressing posterior collapse . The analysis does not derive , therefore , a single value of the parameter $ \\rho $ that provably gives the best bound . Rather our intent is to take the analysis in conjunction with the experimental results . Experiments indeed show a consistent behavior and phase transitioning as we change $ \\rho $ . Specifically , experiments corroborate that there exists a single value of $ \\rho $ for which models consistently undergoes posterior collapse for different datasets given the same architecture . To derive an optimal $ \\rho $ that provably gives prevents collapse we would require analysis of the decoder . This is a good suggestion . As it is out of scope of the current paper , which focuses on posterior collapse , we leave the suggestion for future work ."}, "2": {"review_id": "uvEgLKYMBF9-2", "review_text": "1.Summary This paper studies the training of deep hierarchical VAEs and focuses on the problem of posterior collapse . It is argued that reducing the variance of the gradient estimate may help to overcome posterior collapse . The authors focus on reducing the variance of the functions parameterizing the variational distribution of each layer using a layer-wise smoothing operator based on the Ornstein-Uhlenbeck semigroup ( parameterized by a parameter $ \\rho $ ) . The operator requires additional Monte-Carlo samples . The authors provide an analytical analysis of bias and variance . Last they train multiple VAEs models , measure the posterior collapse and observe a phase transition behaviour depending on the parameter $ \\rho $ . 2. a Strong Points This paper introduced a theoretically grounded solution to the problem of posterior collapse . In particular , it is discussed that variance may be an issue . Great efforts were invested to study the behaviour of the Hermite VAE in theoretical terms and the authors provide analytical results on the Bias and Variance for this estimator . 2. b Weak Points * Complexity In the main text , it is written that `` * experiments show that 5 or 10 samples suffice * '' . This is a major drawback for Hermite VAEs and the complexity of the algorithm is not discussed , nor it is studied empirically . Given 5 MC samples , I interpret that HVAE is 5 times more expensive than other approaches -- please clarify this point . * Empirical study of the variance The problem of the variance is discussed in the paper but left apart in the experimental section . I would expect the authors to measure the variance ( and/or SNR ) of the gradients for the HVAE objective , the VAE , for advanced estimators such as STL and DReG . A study is required to corroborate the claim that reducing variance overcomes posterior collapse . * Experiments on posterior collapse I am surprised to see that none of the existing methods ( KL warmup and freebits ) allows overcoming posterior collapse ( Figure 1 ) . At least using the right amount of freebits should improve the results ( the number of freebits is not reported ) . Furthermore , the authors should report the KL divergence in the benchmark experiment . * Experimental protocol I do n't understand why VAE models trained in section 5 only have 2 layers whereas HVAE uses 4 layers : this is not a fair comparison . Furthermore , LVAE should be studied on the basis of posterior collapse -- not only in terms of likelihood . 3.Recommendation Unfortunately , based on the current form of the paper , I recommend rejecting this paper . 4.Recommendation Arguments Despite the good theoretical contributions , I do not find the experimental section to be strong enough to support the claims . In particular , the cost induced by the additional MC samples is not discussed and methods are hence not compared on the same basis . 5.Questions to the Author - What is the complexity of HVAE ? Do the VAE models use multiple MC samples as well ? - Why using only 2 layers for the VAE models ? - How are the freebits and KL-warmup applied in figure 1 ? 6.Feedback Your work is very relevant and the theoretical insights are very interesting , this work would greatly benefit from an improved experimental section . In the first page , two typos : - you defined $ q ( z | x ) $ and not $ q ( x , z ) $ - The KL divergence in equation 1 should depend on $ q ( z_i | z_ { i-1 } ) $ and $ p ( z_i | z_ { i+1 } ) $", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the review . We try to address the concerns below . > Complexity In the main text , it is written that `` experiments show that 5 or 10 samples suffice '' . This is a major drawback for Hermite VAEs and the complexity of the algorithm is not discussed , nor it is studied empirically . Given 5 MC samples , I interpret that HVAE is 5 times more expensive than other approaches -- please clarify this point . We do provide a timing comparison in the appendix ( A.3 ) for a 4 layer model on static MNIST . Comparisons show that our method bears a small only a extra cost in time per epoch ( 4.6 sec for our model , 4.2 sec for VAE , 4.4 sec for IWAE ) . The complexity of the method is approximately on-par with IWAE , which actually has more parallel computations than us . We will add a reference in the main paper . The reason for the smaller added complexity is that the OU sampling operation is only applied in chosen parts of the model . In particular , we do not apply the sampling operation in the inference network . Secondly , we do not apply the operation in the last stochastic layer of the decoder . The overall complexity ( for 5 MC samples ) is significantly less than if we were to repeat 5 samples with the original complexity . > Empirical study of the variance The problem of the variance is discussed in the paper but left apart in the experimental section . I would expect the authors to measure the variance ( and/or SNR ) of the gradients for the HVAE objective , the VAE , for advanced estimators such as STL and DReG . A study is required to corroborate the claim that reducing variance overcomes posterior collapse . We have added a gradient variance comparison between VAE and HVAE in the Appendix A.5 . Experiments indeed show that variance is reduced with our Hermite VAEs . However , there are a few points which need to be emphasized about our method . First , since we have a variance-bias trade-off we can not conclusively say variance reduction ( at least on its own ) is responsible for elimination of posterior collapse . We do claim , however , that OU smoothing , which also reduces variance , does mitigate posterior collapse . We will clarify this in the introduction . Furthermore , since the smoothing operator introduces a bias-variance trade-off , a direct comparison of variance with other unbiased methods is not meaningful . Also , it is not sufficient to establish the requisite link between smoothing and collapse . Last , we include a study on the relation between smoothing ( and variance reduction thereafter ) and posterior collapse in figure 2 . The parameter $ \\rho $ controls the amount of smoothing ( and variance reduction ) . By increasing $ \\rho $ ( thus less smoothing , more variance ) leads to posterior collapse . Taken together with the theory , this study establishes a link between smoothing and posterior collapse . > Experiments on posterior collapse I am surprised to see that none of the existing methods ( KL warmup and freebits ) allows overcoming posterior collapse ( Figure 1 ) . At least using the right amount of freebits should improve the results ( the number of freebits is not reported ) . Furthermore , the authors should report the KL divergence in the benchmark experiment . We disagree with the conclusion . KL annealing and free bits do help in mitigating posterior collapse for shallow hierarchies of stochastic variables , annealing being more effective . For instance , in tables 1 and 2 ( updated version ) KL annealing and free bits do lead to more active units , especially in the lower levels , and a somewhat larger top layer KL divergence than a standard VAE . However these techniques are not very effective at overcoming collapse in deeper hierarchies , which is the motivation of our work . [ We assume the reference here is to table 1 and not figure 1 where we plot KLD and validation ELBO curves for the vanilla VAE . ] > Experimental protocol I do n't understand why VAE models trained in section 5 only have 2 layers whereas HVAE uses 4 layers : this is not a fair comparison . Furthermore , LVAE should be studied on the basis of posterior collapse -- not only in terms of likelihood . The VAE models showed in section 5 in figures 1 & 2 and tables 1 & 2 ( updated version ) all were trained with 4 stochastic layers . The only place we show a 2-layer VAE is in table 4 where we compare against models reported in the literature . We were unable to find a case of a vanilla VAE deeper than 2 layers with better performance on this dataset in the literature . We will clarify this in the text . We have now also added a comparison to the 5 layer LVAE in table 5 . Hermite VAE outperforms all baselines ."}}