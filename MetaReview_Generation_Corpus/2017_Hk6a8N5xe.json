{"year": "2017", "forum": "Hk6a8N5xe", "title": "Classify or Select: Neural Architectures for Extractive Document Summarization", "decision": "Reject", "meta_review": "Reviewers found this paper clear to read, but leaned negative on in terms of impact and originality of the work. Main complaint is that the paper is neither significantly novel in terms of modeling (pointing to Cheng & Lapata), nor significantly more performative on this task (\"only slightly better\"). One reviewer also has a side complaint that the task itself is also somewhat simplistic and simplified, and suggests other tasks. This comment is perhaps harsh, but reflects a mandate for revisiting \"old\" problems to provide significant improvements in accuracy or novel modeling.", "reviews": [{"review_id": "Hk6a8N5xe-0", "review_text": "This paper presents two models for extractive document summarization: the classifier architecture and the selector architecture. These two models basically use either classification or ranking in a sequential order to pick the candidate sentences for summarization. Experiments in this paper show the results are either better or close to the SOTA. Technical comments: - In equation (1), there is a position-relevant component call \"positional importance\". I am wondering how important this component is? Is it possible to show the performance without this component? Especially, for the discussion on impact of document structure, when the model is trained on the shuffled order but tested on the original order. - A similar question about equation (1), is the content-richness component really necessary? Since the score function already has salience part, which could measure how important of $h_j$ with respect to the whole document. - For the dynamic summary representation in equation (3), why not use the same updating equation for both training and test procedures? During test time, the model actually knows the decisions that have been made so far by the decoder. In this way, the model will be more consistent during training and test. - I think section 5 is the most interesting part of this paper, and it is also convincing on the difference between the two architectures. - It is a little disappointing that the decoding algorithm used in this paper is too simple. In a minimal case, both of them could use beam search and the results could be better.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer3 , Thanks for your feedback . * Regarding your questions/comments on the positional features : We have displayed the importance weights of the various abstract features of the deep Selector model on both original and shuffled data in Table 4 in the Appendix section . The results show that the positional features have high absolute weight ( 31.09 ) in the original data but close to zero weight ( 0.20 ) in the shuffled data , confirming our intuition that the document structure is quite important in modeling summaries . We also performed ablation experiments removing one feature at a time for both Classifier and Selector architectures as reported in Table 5 . These results show that removing positional features hurts the Selector the most . Removing these features also hurts the Classifier , but not as much , since the architecture of the sequence classifier already captures the structural properties of the document . * Regarding your questions/comments on the content-richness feature : The importance weights of the features in Table 4 show that content richness is not as important as salience , position or redundancy , but the weights are not insignificant showing that they indeed add some value . The ablation tests in Table 5 also demonstrate the removing the content features hurts performance of the Classifier as well as the Selector . * Regarding your question on inconsistency between training and testing for Eq ( 3 ) : The update for summary representation is consistent between training and testing for the Selector architecture . For the Classifier architecture , we use hard-updates at training time and soft-updates at testing time for the following reason : since the classifier model makes binary decision for each sentence , it may end up selecting too few or too many sentences for summary ( this is not the case for Selector because we stop selecting sentences once the desired summary length is reached ) . Therefore we run the classifier on all sentences , and sort them in the decreasing order of their probability of being in summary , and chose the top sentences in that order until the desired summary length is reached . Since we use soft probabilities in making decisions , we also use soft updates of the summary representation at test time so that it is consistent with our sentence selection mechanism . * Regarding your comment on beam search : We agree beam search will improve the performance of both models . However , so far as the comparison between the two architectures is concerned , greedy algorithm is used on both of them , so it is fair . We will definitely update the numbers with beam search upon acceptance ."}, {"review_id": "Hk6a8N5xe-1", "review_text": "This paper presents two RNN architectures for extractive document summarization. The first one, Classifier, takes into account the order in which sentences appear in the original document, whereas the second one, Selector, chooses sentences in an arbitrary order. For each architecture, the concatenated RNN hidden state from a sentence forward and backward pass is used as features to compute a score that captures content richness, salience, positional importance, and redundancy. Both models are trained in a supervised manner, so the authors used \"pseudo-ground truth generation\" to create training data from abstractive summaries. Experiments show that the Classifier model performs better, and it achieves near state-of-the-art performance for some evaluation metrics. The proposed model is in general an extension of Cheng and Lapata, 2016. Unfortunately, the performance is only slightly better or sometimes even worse. The authors mentioned that one key difference how they transform abstractive summaries to become gold labels for the supervised method. However, in the experiment results, the authors described that one potential reason their models do not consistently outperform the extractive model of Cheng & Lapata, 2016 is that the unsupervised greedy approximation may generate noisier ground truth labels than Cheng & Lapata. Is there a reason to construct the training data similar to Cheng & Lapata, if that turns out to be a better method? In order for the proposed models to be convincing, they need to outperform this baseline that's very similar to the proposed methods more consistently, since the main contribution is improved neural architectures for extractive document summarization. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear AnonReviewer2 , Thanks for your comments . We agree our models do not consistently outperform the model of Cheng and Lapata in all settings . Where they fail to outpeform , they are still more or less statistically indistinguishable from their system . We would like to gently reiterate that the main contribution of our work is comparison and analysis of the Classifier and Selector architectures , and establishing the conditions under which one may outperform the other . Cheng and Lapata 's model is subsumed under the Classifier architecture , and therefore outperforming this system was not the main goal of this work ."}, {"review_id": "Hk6a8N5xe-2", "review_text": "This paper provides two RNN-based architectures for extractive document summarization. The first, \"Classify\", reads in the whole document and traverses the sentences a second time to decide whether to include them or not (0/1 decisions). The second, \"Select\", reads in the whole document and picks the most relevant sentence one at the time. The models assume that oracle extractive summaries exist, and a pseudo ground-truth generation procedure is used, which mimics Svore et al. (2007) among others. Overall, this paper seems a small increment over Cheng & Lapata (2016) and performance is similar or worse to that paper. The problem of single document extractive summarization is not particularly exciting since in DUC 2002 (14 years ago) existing models could not beat the lead baseline (which selects the first sentences of the document). It's a pity that this paper doesn't address the most interesting problems of abstractive summarization or apply the proposed approach to multi-document summarization. It's also a little disappointing that the maximum sentence length had to be capped to 50, which suggests the model has some trouble to scale.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear AnonReviewer1 , Thanks for your comments . * Regarding your comment on single document extractive summarization not being exciting : We believe single document extractive summarization is still interesting because the summaries produced by an extractive system are always grammatically and semantically correct and coherent , and therefore may deliver better user experience than abstractive systems which tend to be brittle in terms of both grammar and semantics . Modern datasets such as CNN/DailyMail open up new opportunities to train deep learning models for extractive summarization compared to traditional DUC datasets that have very little training data , and it is now more possible than ever to substantially outperform the LEAD baseline ( which our models as well as that of Cheng and Lapata do ) . Hence we think research on using deep learning models for extractive single document summarization is both desirable and necessary . * Regarding your comment on multi-document summarization : We are still not aware of any large scale datasets for this problem on which we could train our deep learning models . The newly released MS-MARCO dataset is the only one that comes close but it was released in parallel to our paper . * Regarding your comment on sentence length capped to 50 : This is more of an approximation to increase training speed than any real limitation . More than 90 % of the sentences have fewer than 50 words , so this approximation has very little effect in terms of accuracy , but increases training speed by restricting the tensor size of each batch ."}], "0": {"review_id": "Hk6a8N5xe-0", "review_text": "This paper presents two models for extractive document summarization: the classifier architecture and the selector architecture. These two models basically use either classification or ranking in a sequential order to pick the candidate sentences for summarization. Experiments in this paper show the results are either better or close to the SOTA. Technical comments: - In equation (1), there is a position-relevant component call \"positional importance\". I am wondering how important this component is? Is it possible to show the performance without this component? Especially, for the discussion on impact of document structure, when the model is trained on the shuffled order but tested on the original order. - A similar question about equation (1), is the content-richness component really necessary? Since the score function already has salience part, which could measure how important of $h_j$ with respect to the whole document. - For the dynamic summary representation in equation (3), why not use the same updating equation for both training and test procedures? During test time, the model actually knows the decisions that have been made so far by the decoder. In this way, the model will be more consistent during training and test. - I think section 5 is the most interesting part of this paper, and it is also convincing on the difference between the two architectures. - It is a little disappointing that the decoding algorithm used in this paper is too simple. In a minimal case, both of them could use beam search and the results could be better.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer3 , Thanks for your feedback . * Regarding your questions/comments on the positional features : We have displayed the importance weights of the various abstract features of the deep Selector model on both original and shuffled data in Table 4 in the Appendix section . The results show that the positional features have high absolute weight ( 31.09 ) in the original data but close to zero weight ( 0.20 ) in the shuffled data , confirming our intuition that the document structure is quite important in modeling summaries . We also performed ablation experiments removing one feature at a time for both Classifier and Selector architectures as reported in Table 5 . These results show that removing positional features hurts the Selector the most . Removing these features also hurts the Classifier , but not as much , since the architecture of the sequence classifier already captures the structural properties of the document . * Regarding your questions/comments on the content-richness feature : The importance weights of the features in Table 4 show that content richness is not as important as salience , position or redundancy , but the weights are not insignificant showing that they indeed add some value . The ablation tests in Table 5 also demonstrate the removing the content features hurts performance of the Classifier as well as the Selector . * Regarding your question on inconsistency between training and testing for Eq ( 3 ) : The update for summary representation is consistent between training and testing for the Selector architecture . For the Classifier architecture , we use hard-updates at training time and soft-updates at testing time for the following reason : since the classifier model makes binary decision for each sentence , it may end up selecting too few or too many sentences for summary ( this is not the case for Selector because we stop selecting sentences once the desired summary length is reached ) . Therefore we run the classifier on all sentences , and sort them in the decreasing order of their probability of being in summary , and chose the top sentences in that order until the desired summary length is reached . Since we use soft probabilities in making decisions , we also use soft updates of the summary representation at test time so that it is consistent with our sentence selection mechanism . * Regarding your comment on beam search : We agree beam search will improve the performance of both models . However , so far as the comparison between the two architectures is concerned , greedy algorithm is used on both of them , so it is fair . We will definitely update the numbers with beam search upon acceptance ."}, "1": {"review_id": "Hk6a8N5xe-1", "review_text": "This paper presents two RNN architectures for extractive document summarization. The first one, Classifier, takes into account the order in which sentences appear in the original document, whereas the second one, Selector, chooses sentences in an arbitrary order. For each architecture, the concatenated RNN hidden state from a sentence forward and backward pass is used as features to compute a score that captures content richness, salience, positional importance, and redundancy. Both models are trained in a supervised manner, so the authors used \"pseudo-ground truth generation\" to create training data from abstractive summaries. Experiments show that the Classifier model performs better, and it achieves near state-of-the-art performance for some evaluation metrics. The proposed model is in general an extension of Cheng and Lapata, 2016. Unfortunately, the performance is only slightly better or sometimes even worse. The authors mentioned that one key difference how they transform abstractive summaries to become gold labels for the supervised method. However, in the experiment results, the authors described that one potential reason their models do not consistently outperform the extractive model of Cheng & Lapata, 2016 is that the unsupervised greedy approximation may generate noisier ground truth labels than Cheng & Lapata. Is there a reason to construct the training data similar to Cheng & Lapata, if that turns out to be a better method? In order for the proposed models to be convincing, they need to outperform this baseline that's very similar to the proposed methods more consistently, since the main contribution is improved neural architectures for extractive document summarization. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear AnonReviewer2 , Thanks for your comments . We agree our models do not consistently outperform the model of Cheng and Lapata in all settings . Where they fail to outpeform , they are still more or less statistically indistinguishable from their system . We would like to gently reiterate that the main contribution of our work is comparison and analysis of the Classifier and Selector architectures , and establishing the conditions under which one may outperform the other . Cheng and Lapata 's model is subsumed under the Classifier architecture , and therefore outperforming this system was not the main goal of this work ."}, "2": {"review_id": "Hk6a8N5xe-2", "review_text": "This paper provides two RNN-based architectures for extractive document summarization. The first, \"Classify\", reads in the whole document and traverses the sentences a second time to decide whether to include them or not (0/1 decisions). The second, \"Select\", reads in the whole document and picks the most relevant sentence one at the time. The models assume that oracle extractive summaries exist, and a pseudo ground-truth generation procedure is used, which mimics Svore et al. (2007) among others. Overall, this paper seems a small increment over Cheng & Lapata (2016) and performance is similar or worse to that paper. The problem of single document extractive summarization is not particularly exciting since in DUC 2002 (14 years ago) existing models could not beat the lead baseline (which selects the first sentences of the document). It's a pity that this paper doesn't address the most interesting problems of abstractive summarization or apply the proposed approach to multi-document summarization. It's also a little disappointing that the maximum sentence length had to be capped to 50, which suggests the model has some trouble to scale.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear AnonReviewer1 , Thanks for your comments . * Regarding your comment on single document extractive summarization not being exciting : We believe single document extractive summarization is still interesting because the summaries produced by an extractive system are always grammatically and semantically correct and coherent , and therefore may deliver better user experience than abstractive systems which tend to be brittle in terms of both grammar and semantics . Modern datasets such as CNN/DailyMail open up new opportunities to train deep learning models for extractive summarization compared to traditional DUC datasets that have very little training data , and it is now more possible than ever to substantially outperform the LEAD baseline ( which our models as well as that of Cheng and Lapata do ) . Hence we think research on using deep learning models for extractive single document summarization is both desirable and necessary . * Regarding your comment on multi-document summarization : We are still not aware of any large scale datasets for this problem on which we could train our deep learning models . The newly released MS-MARCO dataset is the only one that comes close but it was released in parallel to our paper . * Regarding your comment on sentence length capped to 50 : This is more of an approximation to increase training speed than any real limitation . More than 90 % of the sentences have fewer than 50 words , so this approximation has very little effect in terms of accuracy , but increases training speed by restricting the tensor size of each batch ."}}