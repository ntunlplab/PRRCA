{"year": "2018", "forum": "SyVOjfbRb", "title": "LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION", "decision": "Invite to Workshop Track", "meta_review": "The reviewers think that the theoretical contribution is not significant on its own. The reviewers find the empirical aspect of the paper interesting, but more analysis of the empirical behavior is required, especially for large datasets. Even for small datasets with input augmentation (e.g. random crops in CIFAR-10) the pre-processing can become prohibitive. I recommend improving the manuscript for a re-submission to another venue and an ICLR workshop presentation.", "reviews": [{"review_id": "SyVOjfbRb-0", "review_text": "Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH. I found the paper relatively creative and generally well-founded and well-argued. Nice clear example with least squares linear regression, though a little hard to tell how generalizable the given ideas are to other loss functions/function classes, given the authors seem to be taking heavy advantage of the inner product. Experiments: appreciated the wall clock timings. SGD comparison: \u201cfixed learning rate.\u201d Didn't see how the initial (well constant here) step size was tuned? Why not use the more standard 1/t decay? Fig 1: Suspicious CIFAR100 that test objective is so much better than train objective? Legend backwards? Why were so many of the chosen datasets have so few training examples? Paper is mostly very clearly written, though a bit too redundant and some sentences are oddly ungrammatical as if a word is missing - just needs a careful read-through. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the encouraging comment . We are happy to get your support and hope you will clarify the misconception of other reviewers in the subsequent discussions . To avoid any bells and whistles , we show plain SGD as well as adagrad which adaptively chooses the step size based on the previous gradients estimates . We did not tune anything to nullify the effect of any tuning and ensure an apples-to-apples comparison . Better gradient estimate leads to improvements despite SGD or adagrad . Inner product naturally goes for linear regression as well as logistic ( exp^ { inner product } ) . A natural next step is to look at popular loss function as well as existing LSH to see if there are other sweet spots . Other than CIFAR , we chose high dimensional regression datasets ( not classification ) from UCI . https : //archive.ics.uci.edu/ml/datasets.html unfortunately , all high dimensional regressions datasets are small . Let us know if you have any suggestions on that ."}, {"review_id": "SyVOjfbRb-1", "review_text": " The main idea in the paper is fairly simple: The paper considers SGD over an objective of the form of a sum over examples of a quadratic loss. The basic form of SGD selects an example uniformly. Instead, one can use any probability distribution over examples and apply inverse probability weighting to retain unbiasedness of the gradient. A good method (that builds on classic pps sampling) is to select examples with higher normed gradients with higher probability [Alain et al 2015]. With quadratic loss, the gradient increases with the inner product of the parameter vector (concatenated with -1) and the example vector x_i (concatenated with the label y_i). For the current parameter vector \\theta, we would like to sample examples so that the probability of sampling larger inner products is larger. The paper uses LSH structures, computed over the set of examples, to quickly sample examples with large inner products with the current parameter vector \\theta. Essentially, two vectors are hashed to the same bucket with probability that increases with their cosine similarity. So we select examples in the same LSH bucket as \\theta (for rubstness, we use multiple LSH mappings). strengths: simple idea that can work well in the context of sampling examples for SGD weaknesses: The novelty in the paper is limited. The use of LSH for sampling is a common technique to sample more similar vectors with higher probability. There are theorems, but they are trivial, straightforward applications of importance sampling. The paper is not well written. The presentation is much more complex that need be. References to classic weighted sampling are The application is limited to certain loss functions for which we can compute LSH structures. This excludes NN models and even the addition of regularization to the quadratic loss can affect the effectiveness. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "The novelty in the paper is limited . The use of LSH for sampling is a common technique to sample more similar vectors with higher probability . There are theorems , but they are trivial , straightforward applications of importance sampling . > > LSH as sampling was first used very recently ( early 2016 ) . Note the importance weighting factor in the algorithm of 1 - ( 1-p^K ) ^L . It is about the unbiased estimation of gradients rather than a simple heuristic . We challenge the reviewer to show one paper which shows the use of LSH as sampling for unbiased estimation of the gradient in SGD . Simplicity is not bad , especially when it beats a fundamental barrier . * * * * * * * * * * * * * * The paper uses LSH structures , computed over the set of examples , to quickly sample examples with large inner products with the current parameter vector \\theta . Essentially , two vectors are hashed to the same bucket with probability that increases with their cosine similarity . So we select examples in the same LSH bucket as \\theta ( for robustness , we use multiple LSH mappings ) . > > Not really , the process is about unbiased estimation ( mentioned in the paper at several places ) . Again you are missing the importance style weights . And the sampling is correlated and not normalized , so it is something never seen before . Due to the simplicity of our proposal , it might be easy to overlook the subtlety of the methods . We reiterate , this not yet another heuristic here . For the first time , we see some hope of beating SGD in running time using better estimator , and this does not happen often . We hope these comments will lead to a healthy discussion and correction of any misconceptions on either side : ) Thanks for taking time in trying to improve our paper ."}, {"review_id": "SyVOjfbRb-2", "review_text": "The main contribution of this work is just a combination of LSH schemes and SGD updates. Since hashing schemes essentially reduce the dimension, LSH brings computational benefits to the SGD operation. The targeted issue is fundamentally important, and the proposed approach (exploiting LSH schemes) seems to be sound. Specifically, LSH schemes fit into the SGD schemes since they hash two vectors to the same bucket with probability in proportional to their distance (here, inner product or Cosine similarity). Strengths: a sound approach; a simple and straightforward idea that is shown to work well in evaluations. Weaknesses: 1. The phrase of \"computational chicken-and-egg loop\" in the title and also in the main body is misleading and not accurate. The so-called \"chicken-and-egg\u201d issue concerns the causality dilemma: two causally related things, which comes the first. In the paper, the authors concerned \"more accurate gradients\" and \"faster convergence\"; their causality is very clear (the first leads to the second), and there is no causality dilemma. Even from a computational perspective, \"SDG schemes aim for computational efficiency\" and \"stochastic makes the convergence slow down\" are not a causality dilemma. The reason behind is that the latter is the cost of the first one, just the old saying that \"there is no such thing as a free lunch\". Therefore, this disordered logic makes the title very misleading, and all the corresponding descriptions in the main body are obscured by \"twisted\" and unnatural logics. 2. The depth is so limited. Besides a good observation that LSH fits well into SDG, there are no more in-depth results provided. The theorems (Theorems 1~3) are trivial, with loose relations with LSH. 3. The LSH schemes are not correctly referred to. Since the similarity metric is inner-product, the authors are expected to refer to Cosine similarity and inner-product based LSHs, which were published recently in NIPS. It is not in depth to assume \"any known LSH scheme\" in Alg. 2. Accordingly again, Theorems 1~3 are unrelated with this specific kind of similarity metric (Cosine similarity). 4. As the authors tried hard to stick to the unnecessary (a bit bragging) phrase \"computational chicken-and-egg loop\", the organization and presentation of the whole manuscript are poor. 5. Occasionally, there are typos, and it is not good to use words in formulas. Please proof-read carefully. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Since hashing schemes essentially reduce the dimension , LSH brings computational benefits to the SGD operation > > NO .... Not at all . It has nothing to do with dimensionality reduction at all . It is about efficient sampling using hash tables . ( Also see response to AnonReviewer1 ) We are afraid that the reviewer is mistaken as to what the method it , despite this being mentioned at several placed very explicitly . We still try our best to respond to concerns . 1 ) SGD reduces the costly iteration ( O ( 1 ) per iteration ) but increases the number of iterations . Any known adaptive scheme to reduce the number of iterations leads to very costly O ( N ) per iteration . We refer this inherent tradeoff as chicken and egg loop . If this is a big issue , we can easily change it ? 2 ) See response to AnonReviewer1 . Missing the subtlety of the algorithm is easy . Simplicity that beats a fundamental barrier is rare and most exciting . 3 ) The theorems are valid for any LSH irrespective of the choice of similarity , similar to why importance sampling is unbiased for any proposal . So we do n't really see what the issue is . 4 ) see 1 5 ) We will proofread the paper . Thanks for pointing out . We hope that our comments will change the opinion of the reviewer . We are happy to have any more suggestions . Thanks for the time in providing feedback ."}], "0": {"review_id": "SyVOjfbRb-0", "review_text": "Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH. I found the paper relatively creative and generally well-founded and well-argued. Nice clear example with least squares linear regression, though a little hard to tell how generalizable the given ideas are to other loss functions/function classes, given the authors seem to be taking heavy advantage of the inner product. Experiments: appreciated the wall clock timings. SGD comparison: \u201cfixed learning rate.\u201d Didn't see how the initial (well constant here) step size was tuned? Why not use the more standard 1/t decay? Fig 1: Suspicious CIFAR100 that test objective is so much better than train objective? Legend backwards? Why were so many of the chosen datasets have so few training examples? Paper is mostly very clearly written, though a bit too redundant and some sentences are oddly ungrammatical as if a word is missing - just needs a careful read-through. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the encouraging comment . We are happy to get your support and hope you will clarify the misconception of other reviewers in the subsequent discussions . To avoid any bells and whistles , we show plain SGD as well as adagrad which adaptively chooses the step size based on the previous gradients estimates . We did not tune anything to nullify the effect of any tuning and ensure an apples-to-apples comparison . Better gradient estimate leads to improvements despite SGD or adagrad . Inner product naturally goes for linear regression as well as logistic ( exp^ { inner product } ) . A natural next step is to look at popular loss function as well as existing LSH to see if there are other sweet spots . Other than CIFAR , we chose high dimensional regression datasets ( not classification ) from UCI . https : //archive.ics.uci.edu/ml/datasets.html unfortunately , all high dimensional regressions datasets are small . Let us know if you have any suggestions on that ."}, "1": {"review_id": "SyVOjfbRb-1", "review_text": " The main idea in the paper is fairly simple: The paper considers SGD over an objective of the form of a sum over examples of a quadratic loss. The basic form of SGD selects an example uniformly. Instead, one can use any probability distribution over examples and apply inverse probability weighting to retain unbiasedness of the gradient. A good method (that builds on classic pps sampling) is to select examples with higher normed gradients with higher probability [Alain et al 2015]. With quadratic loss, the gradient increases with the inner product of the parameter vector (concatenated with -1) and the example vector x_i (concatenated with the label y_i). For the current parameter vector \\theta, we would like to sample examples so that the probability of sampling larger inner products is larger. The paper uses LSH structures, computed over the set of examples, to quickly sample examples with large inner products with the current parameter vector \\theta. Essentially, two vectors are hashed to the same bucket with probability that increases with their cosine similarity. So we select examples in the same LSH bucket as \\theta (for rubstness, we use multiple LSH mappings). strengths: simple idea that can work well in the context of sampling examples for SGD weaknesses: The novelty in the paper is limited. The use of LSH for sampling is a common technique to sample more similar vectors with higher probability. There are theorems, but they are trivial, straightforward applications of importance sampling. The paper is not well written. The presentation is much more complex that need be. References to classic weighted sampling are The application is limited to certain loss functions for which we can compute LSH structures. This excludes NN models and even the addition of regularization to the quadratic loss can affect the effectiveness. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "The novelty in the paper is limited . The use of LSH for sampling is a common technique to sample more similar vectors with higher probability . There are theorems , but they are trivial , straightforward applications of importance sampling . > > LSH as sampling was first used very recently ( early 2016 ) . Note the importance weighting factor in the algorithm of 1 - ( 1-p^K ) ^L . It is about the unbiased estimation of gradients rather than a simple heuristic . We challenge the reviewer to show one paper which shows the use of LSH as sampling for unbiased estimation of the gradient in SGD . Simplicity is not bad , especially when it beats a fundamental barrier . * * * * * * * * * * * * * * The paper uses LSH structures , computed over the set of examples , to quickly sample examples with large inner products with the current parameter vector \\theta . Essentially , two vectors are hashed to the same bucket with probability that increases with their cosine similarity . So we select examples in the same LSH bucket as \\theta ( for robustness , we use multiple LSH mappings ) . > > Not really , the process is about unbiased estimation ( mentioned in the paper at several places ) . Again you are missing the importance style weights . And the sampling is correlated and not normalized , so it is something never seen before . Due to the simplicity of our proposal , it might be easy to overlook the subtlety of the methods . We reiterate , this not yet another heuristic here . For the first time , we see some hope of beating SGD in running time using better estimator , and this does not happen often . We hope these comments will lead to a healthy discussion and correction of any misconceptions on either side : ) Thanks for taking time in trying to improve our paper ."}, "2": {"review_id": "SyVOjfbRb-2", "review_text": "The main contribution of this work is just a combination of LSH schemes and SGD updates. Since hashing schemes essentially reduce the dimension, LSH brings computational benefits to the SGD operation. The targeted issue is fundamentally important, and the proposed approach (exploiting LSH schemes) seems to be sound. Specifically, LSH schemes fit into the SGD schemes since they hash two vectors to the same bucket with probability in proportional to their distance (here, inner product or Cosine similarity). Strengths: a sound approach; a simple and straightforward idea that is shown to work well in evaluations. Weaknesses: 1. The phrase of \"computational chicken-and-egg loop\" in the title and also in the main body is misleading and not accurate. The so-called \"chicken-and-egg\u201d issue concerns the causality dilemma: two causally related things, which comes the first. In the paper, the authors concerned \"more accurate gradients\" and \"faster convergence\"; their causality is very clear (the first leads to the second), and there is no causality dilemma. Even from a computational perspective, \"SDG schemes aim for computational efficiency\" and \"stochastic makes the convergence slow down\" are not a causality dilemma. The reason behind is that the latter is the cost of the first one, just the old saying that \"there is no such thing as a free lunch\". Therefore, this disordered logic makes the title very misleading, and all the corresponding descriptions in the main body are obscured by \"twisted\" and unnatural logics. 2. The depth is so limited. Besides a good observation that LSH fits well into SDG, there are no more in-depth results provided. The theorems (Theorems 1~3) are trivial, with loose relations with LSH. 3. The LSH schemes are not correctly referred to. Since the similarity metric is inner-product, the authors are expected to refer to Cosine similarity and inner-product based LSHs, which were published recently in NIPS. It is not in depth to assume \"any known LSH scheme\" in Alg. 2. Accordingly again, Theorems 1~3 are unrelated with this specific kind of similarity metric (Cosine similarity). 4. As the authors tried hard to stick to the unnecessary (a bit bragging) phrase \"computational chicken-and-egg loop\", the organization and presentation of the whole manuscript are poor. 5. Occasionally, there are typos, and it is not good to use words in formulas. Please proof-read carefully. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Since hashing schemes essentially reduce the dimension , LSH brings computational benefits to the SGD operation > > NO .... Not at all . It has nothing to do with dimensionality reduction at all . It is about efficient sampling using hash tables . ( Also see response to AnonReviewer1 ) We are afraid that the reviewer is mistaken as to what the method it , despite this being mentioned at several placed very explicitly . We still try our best to respond to concerns . 1 ) SGD reduces the costly iteration ( O ( 1 ) per iteration ) but increases the number of iterations . Any known adaptive scheme to reduce the number of iterations leads to very costly O ( N ) per iteration . We refer this inherent tradeoff as chicken and egg loop . If this is a big issue , we can easily change it ? 2 ) See response to AnonReviewer1 . Missing the subtlety of the algorithm is easy . Simplicity that beats a fundamental barrier is rare and most exciting . 3 ) The theorems are valid for any LSH irrespective of the choice of similarity , similar to why importance sampling is unbiased for any proposal . So we do n't really see what the issue is . 4 ) see 1 5 ) We will proofread the paper . Thanks for pointing out . We hope that our comments will change the opinion of the reviewer . We are happy to have any more suggestions . Thanks for the time in providing feedback ."}}