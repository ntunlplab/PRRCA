{"year": "2019", "forum": "rJlDnoA5Y7", "title": "Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs", "decision": "Accept (Poster)", "meta_review": "this is a meta-review with the recommendation, but i will ultimately leave the final call to the programme chairs, as this submission has a number of valid concerns.\n\nthe proposed approach is one of the early, principled one to using (fixed) dense vectors for computing the predictive probability without resorting to softmax, that scales better than and work almost as well as softmax in neural sequence modelling. the reviewers as well as public commentators have noticed some (potentially significant) short comings, such as instability of learning due to numerical precision and the inability of using beam search (perhaps due to the sub-optimal calibration of probabilities under vMF.) however, i believe these two issues should be addressed as separate follow-up work not necessarily by the authors themselves but by a broader community who would find this approach appealing for their own work, which would only be possible if the authors presented this work and had a chance to discuss it with the community at the conference. therefore, i recommend it be accepted. ", "reviews": [{"review_id": "rJlDnoA5Y7-0", "review_text": " [clarity] This paper is basically well written. The motivation is clear and reasonable. However, I have some points that I need to confirm for review (Please see the significance part). [originality] The idea of taking advantage of von Mises-Fisher distributions is not novel in the context of DL/DNN research community. E.g., von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification. However, as described in the paper, the incorporation of von Mises-Fisher for calculating loss function seems to be novel, to the best of my knowledge. [significance] Unfortunately, the experiments in this paper do not fully support the effectiveness of the proposed method. See below for more detailed comments. *weak baseline (comparison) As an anonymous reviewer pointed out, the author should run baseline method with beam search if the authors aim to convince readers (including reviewers) for the effectiveness of the proposed method. I understand that it is important to investigate the effectiveness of the proposed method in the identical settings. However, it is also important to compare the proposed method with strong baseline to reveal the relative effectiveness of the proposed method comparing with the current state-of-the-art methods. * open vocabulary setting I am confused whether the experimental setting for the proposed method is really in an open vocabulary setting or not. If my understanding is correct, the vocabulary sizes used for the proposed method were 50,000 (iwslt2016) and 300,000 (wmt16), which cannot be an open vocabulary setting. If this is correct, the applicability of the proposed method is potentially limited comparing with the subword-based approach. Is there any comment for this question? * convergence speed I think the claim of faster convergence of the proposed method in terms of iteration may be misleading. This might be true, but it is empirically proven only by single dataset and single run. The authors should show more empirical results on several datasets or provide a theoretical justification for this claim. Overall, basically I like the idea of the proposed method. I also aim to remove the large computational cost of softmax in neural encoder-decoder approach. In my feeling, the proposed method should be a bit more improved for a recommendation of clear acceptance. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your detailed feedback . Here are our responses to your comments : Weak Baseline : As you pointed out , we show results for only greedy decoding to investigate its effectiveness in identical settings . We have since updated the paper with beam search results with the baseline model . The translation quality in our models is still on par or only slightly lower than the results with beam search . Closed Vocabulary : In IWSLT2016 datasets , the target vocabulary size is around 55000 , and around 800,000 in WMT16 . The vocabulary sizes we have chosen are not arbitrary but reflect the overlap of the target vocabulary with the pre-trained embedding vocabulary . In principle , it is possible to train the embeddings on a larger monolingual corpora to increase the overlap . But the words for which embeddings could not be found in the embedding table are likely very rare words such as named entities which we handle by using a copy mechanism . Moreover , although subword methods theoretically gives us open-vocabulary setting , they still perform poorly which translating such rare words ( see Table 5 ) because it breaks those rare words into very small units that lose meaning . Convergence Speed : Due to space limitations , we only report the convergence speed ( Figure 1 ) on one dataset . But the reported results are generated and averaged from multiple runs , and we have achieved consistent performance on all the datasets . We have updated those figures in the appendix . We also report total training time for all the datasets in Table 3 which are also averaged results across multiple runs ."}, {"review_id": "rJlDnoA5Y7-1", "review_text": "This paper proposes to replace the softmax over the vocab in the decoder with a single embedding layer using the Von Mises-Fisher distribution, which speeds up training 2.5x compared to a standard softmax+cross entropy decoder. The goal is admirable, as the softmax during training is a huge time sink (the proposed approach does not speed up inference due to requiring a nearest neighbor computation over the whole vocab). The approach is evaluated on machine translation (De/F>En and En>F), and the results indicate that there is minor quality loss (measured by BLEU) when using vMF. One huge limitation of the approach is the lack of a beam search-like algorithm; as such, the model is compared to greedy softmax+CE decoders (I would like to see numbers with a standard beam search model as well just to emphasize the quality drop from the state-of-the-art systems). With that said, I found this approach quite exciting and it has potential to be further improved, so I'm a weak accept. comments: - is convergence time the right thing to measure when you're comparing the two different types of models? i'd like to see something like flops as in the transformer paper. - relatedly, it's great that you can use a bigger batch size! this could be very important especially for non-MT tasks that require producing longer output sequences (e.g., summarization). - it looks like the choice of pretrained embedding makes a very significant difference in BLEU. i wonder if contextualized embeddings such as ELMo or CoVE could be somehow incorporated into this framework, since they generally outperform static word embeddings. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback ! Here are our responses : Beam Search : As pointed in one of our earlier comments ( https : //openreview.net/forum ? id=rJlDnoA5Y7 & noteId=HkxNHeoR57 ) , beam search is not impossible to do but with our proposed model but is not trivial as to just using k nearest neighbors as candidates . It requires substantial investigation due to which we leave it as future work . We included BLEU scores with just greedy search for fair comparison with baselines and keeping in line with earlier work with similar motivation to ours ( https : //arxiv.org/abs/1704.06918 ) . But thank you for your suggestion , we have now updated the draft to include results with beam search as well . Note that translation quality in our models is still on par or only slightly lower than the results with beam search . Since the beam search is known to slow down decoding and there has been work in the past to get rid of it from softmax based architectures ( https : //openreview.net/forum ? id=rJZlKFkvM , https : //arxiv.org/pdf/1701.02854.pdf ) . The latter paper , for example , proposes a deterministic alternative to decoding where instead of sampling from softmax output at each step , you feed the entire softmax distribution to the next step . We plan to explore a similar approach in the future where the output vector is fed directly to the next step as opposed to finding it \u2019 s nearest neighbor and feeding that to the next step . Convergence Time : Convergence time , which is reflective of total training time is crucial factor in machine translation systems some of which can take weeks to train and is reported by the transformer paper as well ( https : //arxiv.org/abs/1706.03762 ) . We report number of samples processed per second ( Figure 1 ) instead of FLOPs ( floating point operations ) per second , as was reported in the transformer paper . The metrics correlate . We believe that FLOPs measure is more noisy because it 's hard to keep GPUs utilized at 100 % . Use of ELMo or CoVe : This is a great suggestion , thank you . But as you pointed out , they are contextual embeddings and it \u2019 s not clear how to directly incorporate them . But it would an exciting future direction for this work ."}, {"review_id": "rJlDnoA5Y7-2", "review_text": "This paper describes a technique for replacing the softmax layer in sequence-to-sequence models with one that attempts to predict a continuous word embedding, which will then be mapped into a (potentially huge) pre-trained embedding vector via nearest neighbor search. The obvious choice for building a loss around such a prediction (squared error) is shown to be inappropriate empirically, and instead a von Mises-Fisher loss is proposed. Experiments conducted on small-data, small-model, greedy-search German->English, French->English and English->French scenarios demonstrate translation quality on par with BPE, and superior performance to a number of other continuous vector losses. They also provide convincing arguments that this new objective is more efficient in terms of both time and number of learned parameters. This is a nice innovation for sequence-to-sequence modeling. The technical contribution required to make it work is non-trivial, and the authors have demonstrated promising results on a small system. I\u2019m not sure whether this has any chance of supplanting BPE as the go-to solution for large vocabulary models, but I think it\u2019s very healthy to add this method to the discussion. Other than the aforementioned small baseline systems, this paper has few issues, so I\u2019ll take some of my usual \u2018problems with the paper\u2019 space to discuss some downsides with this method. First: the need to use pre-trained word embeddings may be a step backward. It\u2019s always a little scary to introduce more steps into the pipeline, and it\u2019s uncomfortable to hear the authors state that they may be able to improve performance by changing the word embedding objective. As we move to large training sets, having pre-trained embeddings is likely to stop being an advantage and start being a hindrance. Second: though this can drastically increase vocabulary sizes, it is still a closed vocabulary model, which is a weakness when compared to BPE (though I suppose you could do both). Smaller issues: First paragraph after equation (1): \u201cthe hidden state \u2026 t, h.\u201d -> \u201cthe hidden state h \u2026 t.\u201d Equation (2): it might help your readers to spell out how setting \\kappa to ||\\hat{e}|| allows you to ignore the unit-norm assumption of \\mu. \u201cthe negative log-likelihood of the vMF\u2026\u201d - missing capital Unnumbered equation immediately before \u201cRegularization of NLLvMF\u201d: C_m||\\hat{e}|| is missing round brackets around ||\\hat{e}|| to make it an argument of the C_m function. Is predicting the word vector whose target embedding has the highest value of vMF probability any more expensive than nearest neighbor search? Does it preclude the use of very fast nearest neighbor searches? It might be a good idea to make it clear in 4.3 that you see an extension to beam search for your method to be non-trivial (and that you aren\u2019t simply leaving out beam search for comparability to the various empirical loss functions). This didn\u2019t become clear to me until the Future Work section. In Table 5, I don\u2019t fully understand F1 in terms of word-level translation accuracy. Recall is easy to understand (does the reference word appear in the system output?) but precision is harder to conceptualize. It might help to define the metric more carefully.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thorough feedback , we have updated the paper addressing your comments ! Here are our replies to address your comments : Small Dataset : We show extensive analysis on smaller machine translation datasets ( IWSLT ) because they take short time to train and hence easier to experiment with . But with our best model we show results on par with softmax based baselines on a much larger WMT German to English dataset with 4.5 million training instances showing the effectiveness of our proposed model in a much broader setting Pre-trained Embeddings being a hindrance in large datasets : We share your concern on this matter . One of our ongoing projects involves being able to update these output embeddings as part of the training as well . It is possible to do this directly with max-margin loss ( which is a contrastive loss ) by making the output embeddings trainable , but with other ( pairwise ) losses , it \u2019 ll lead to a degenerate solution ( with all outputs as zeroes ) . We are currently exploring a wake-sleep-like algorithm to tackle this problem . Closed vocabulary : This is a good point but the vocabulary can always be increased by training the embeddings on a larger monolingual corpora . Additionally , the words which wouldn \u2019 t exist in the vocabulary are most likely ( 1 ) proper nouns like named entities which can be handled by the copy mechanism we used in the paper , or ( 2 ) rare words . For the latter , although theoretically BPE allows open vocabulary decoding , in practice we see that our model performs much better than BPE baselines in particular on rare words ( Table 5 ) . It would be interesting to explore a combination of BPE and our proposed model in future work . No , Predicting the word with highest probability using vMF has the same computational complexity as nearest neighbor search We choose F1 to control for noise in the predicted sentence . Recall will only measure a reference word is produced . But by including precision , we measure what fraction of the predicted words are actually in reference . So a sentence producing all the words in reference but also a lot of garbage will be given less score ."}], "0": {"review_id": "rJlDnoA5Y7-0", "review_text": " [clarity] This paper is basically well written. The motivation is clear and reasonable. However, I have some points that I need to confirm for review (Please see the significance part). [originality] The idea of taking advantage of von Mises-Fisher distributions is not novel in the context of DL/DNN research community. E.g., von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification. However, as described in the paper, the incorporation of von Mises-Fisher for calculating loss function seems to be novel, to the best of my knowledge. [significance] Unfortunately, the experiments in this paper do not fully support the effectiveness of the proposed method. See below for more detailed comments. *weak baseline (comparison) As an anonymous reviewer pointed out, the author should run baseline method with beam search if the authors aim to convince readers (including reviewers) for the effectiveness of the proposed method. I understand that it is important to investigate the effectiveness of the proposed method in the identical settings. However, it is also important to compare the proposed method with strong baseline to reveal the relative effectiveness of the proposed method comparing with the current state-of-the-art methods. * open vocabulary setting I am confused whether the experimental setting for the proposed method is really in an open vocabulary setting or not. If my understanding is correct, the vocabulary sizes used for the proposed method were 50,000 (iwslt2016) and 300,000 (wmt16), which cannot be an open vocabulary setting. If this is correct, the applicability of the proposed method is potentially limited comparing with the subword-based approach. Is there any comment for this question? * convergence speed I think the claim of faster convergence of the proposed method in terms of iteration may be misleading. This might be true, but it is empirically proven only by single dataset and single run. The authors should show more empirical results on several datasets or provide a theoretical justification for this claim. Overall, basically I like the idea of the proposed method. I also aim to remove the large computational cost of softmax in neural encoder-decoder approach. In my feeling, the proposed method should be a bit more improved for a recommendation of clear acceptance. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your detailed feedback . Here are our responses to your comments : Weak Baseline : As you pointed out , we show results for only greedy decoding to investigate its effectiveness in identical settings . We have since updated the paper with beam search results with the baseline model . The translation quality in our models is still on par or only slightly lower than the results with beam search . Closed Vocabulary : In IWSLT2016 datasets , the target vocabulary size is around 55000 , and around 800,000 in WMT16 . The vocabulary sizes we have chosen are not arbitrary but reflect the overlap of the target vocabulary with the pre-trained embedding vocabulary . In principle , it is possible to train the embeddings on a larger monolingual corpora to increase the overlap . But the words for which embeddings could not be found in the embedding table are likely very rare words such as named entities which we handle by using a copy mechanism . Moreover , although subword methods theoretically gives us open-vocabulary setting , they still perform poorly which translating such rare words ( see Table 5 ) because it breaks those rare words into very small units that lose meaning . Convergence Speed : Due to space limitations , we only report the convergence speed ( Figure 1 ) on one dataset . But the reported results are generated and averaged from multiple runs , and we have achieved consistent performance on all the datasets . We have updated those figures in the appendix . We also report total training time for all the datasets in Table 3 which are also averaged results across multiple runs ."}, "1": {"review_id": "rJlDnoA5Y7-1", "review_text": "This paper proposes to replace the softmax over the vocab in the decoder with a single embedding layer using the Von Mises-Fisher distribution, which speeds up training 2.5x compared to a standard softmax+cross entropy decoder. The goal is admirable, as the softmax during training is a huge time sink (the proposed approach does not speed up inference due to requiring a nearest neighbor computation over the whole vocab). The approach is evaluated on machine translation (De/F>En and En>F), and the results indicate that there is minor quality loss (measured by BLEU) when using vMF. One huge limitation of the approach is the lack of a beam search-like algorithm; as such, the model is compared to greedy softmax+CE decoders (I would like to see numbers with a standard beam search model as well just to emphasize the quality drop from the state-of-the-art systems). With that said, I found this approach quite exciting and it has potential to be further improved, so I'm a weak accept. comments: - is convergence time the right thing to measure when you're comparing the two different types of models? i'd like to see something like flops as in the transformer paper. - relatedly, it's great that you can use a bigger batch size! this could be very important especially for non-MT tasks that require producing longer output sequences (e.g., summarization). - it looks like the choice of pretrained embedding makes a very significant difference in BLEU. i wonder if contextualized embeddings such as ELMo or CoVE could be somehow incorporated into this framework, since they generally outperform static word embeddings. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback ! Here are our responses : Beam Search : As pointed in one of our earlier comments ( https : //openreview.net/forum ? id=rJlDnoA5Y7 & noteId=HkxNHeoR57 ) , beam search is not impossible to do but with our proposed model but is not trivial as to just using k nearest neighbors as candidates . It requires substantial investigation due to which we leave it as future work . We included BLEU scores with just greedy search for fair comparison with baselines and keeping in line with earlier work with similar motivation to ours ( https : //arxiv.org/abs/1704.06918 ) . But thank you for your suggestion , we have now updated the draft to include results with beam search as well . Note that translation quality in our models is still on par or only slightly lower than the results with beam search . Since the beam search is known to slow down decoding and there has been work in the past to get rid of it from softmax based architectures ( https : //openreview.net/forum ? id=rJZlKFkvM , https : //arxiv.org/pdf/1701.02854.pdf ) . The latter paper , for example , proposes a deterministic alternative to decoding where instead of sampling from softmax output at each step , you feed the entire softmax distribution to the next step . We plan to explore a similar approach in the future where the output vector is fed directly to the next step as opposed to finding it \u2019 s nearest neighbor and feeding that to the next step . Convergence Time : Convergence time , which is reflective of total training time is crucial factor in machine translation systems some of which can take weeks to train and is reported by the transformer paper as well ( https : //arxiv.org/abs/1706.03762 ) . We report number of samples processed per second ( Figure 1 ) instead of FLOPs ( floating point operations ) per second , as was reported in the transformer paper . The metrics correlate . We believe that FLOPs measure is more noisy because it 's hard to keep GPUs utilized at 100 % . Use of ELMo or CoVe : This is a great suggestion , thank you . But as you pointed out , they are contextual embeddings and it \u2019 s not clear how to directly incorporate them . But it would an exciting future direction for this work ."}, "2": {"review_id": "rJlDnoA5Y7-2", "review_text": "This paper describes a technique for replacing the softmax layer in sequence-to-sequence models with one that attempts to predict a continuous word embedding, which will then be mapped into a (potentially huge) pre-trained embedding vector via nearest neighbor search. The obvious choice for building a loss around such a prediction (squared error) is shown to be inappropriate empirically, and instead a von Mises-Fisher loss is proposed. Experiments conducted on small-data, small-model, greedy-search German->English, French->English and English->French scenarios demonstrate translation quality on par with BPE, and superior performance to a number of other continuous vector losses. They also provide convincing arguments that this new objective is more efficient in terms of both time and number of learned parameters. This is a nice innovation for sequence-to-sequence modeling. The technical contribution required to make it work is non-trivial, and the authors have demonstrated promising results on a small system. I\u2019m not sure whether this has any chance of supplanting BPE as the go-to solution for large vocabulary models, but I think it\u2019s very healthy to add this method to the discussion. Other than the aforementioned small baseline systems, this paper has few issues, so I\u2019ll take some of my usual \u2018problems with the paper\u2019 space to discuss some downsides with this method. First: the need to use pre-trained word embeddings may be a step backward. It\u2019s always a little scary to introduce more steps into the pipeline, and it\u2019s uncomfortable to hear the authors state that they may be able to improve performance by changing the word embedding objective. As we move to large training sets, having pre-trained embeddings is likely to stop being an advantage and start being a hindrance. Second: though this can drastically increase vocabulary sizes, it is still a closed vocabulary model, which is a weakness when compared to BPE (though I suppose you could do both). Smaller issues: First paragraph after equation (1): \u201cthe hidden state \u2026 t, h.\u201d -> \u201cthe hidden state h \u2026 t.\u201d Equation (2): it might help your readers to spell out how setting \\kappa to ||\\hat{e}|| allows you to ignore the unit-norm assumption of \\mu. \u201cthe negative log-likelihood of the vMF\u2026\u201d - missing capital Unnumbered equation immediately before \u201cRegularization of NLLvMF\u201d: C_m||\\hat{e}|| is missing round brackets around ||\\hat{e}|| to make it an argument of the C_m function. Is predicting the word vector whose target embedding has the highest value of vMF probability any more expensive than nearest neighbor search? Does it preclude the use of very fast nearest neighbor searches? It might be a good idea to make it clear in 4.3 that you see an extension to beam search for your method to be non-trivial (and that you aren\u2019t simply leaving out beam search for comparability to the various empirical loss functions). This didn\u2019t become clear to me until the Future Work section. In Table 5, I don\u2019t fully understand F1 in terms of word-level translation accuracy. Recall is easy to understand (does the reference word appear in the system output?) but precision is harder to conceptualize. It might help to define the metric more carefully.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thorough feedback , we have updated the paper addressing your comments ! Here are our replies to address your comments : Small Dataset : We show extensive analysis on smaller machine translation datasets ( IWSLT ) because they take short time to train and hence easier to experiment with . But with our best model we show results on par with softmax based baselines on a much larger WMT German to English dataset with 4.5 million training instances showing the effectiveness of our proposed model in a much broader setting Pre-trained Embeddings being a hindrance in large datasets : We share your concern on this matter . One of our ongoing projects involves being able to update these output embeddings as part of the training as well . It is possible to do this directly with max-margin loss ( which is a contrastive loss ) by making the output embeddings trainable , but with other ( pairwise ) losses , it \u2019 ll lead to a degenerate solution ( with all outputs as zeroes ) . We are currently exploring a wake-sleep-like algorithm to tackle this problem . Closed vocabulary : This is a good point but the vocabulary can always be increased by training the embeddings on a larger monolingual corpora . Additionally , the words which wouldn \u2019 t exist in the vocabulary are most likely ( 1 ) proper nouns like named entities which can be handled by the copy mechanism we used in the paper , or ( 2 ) rare words . For the latter , although theoretically BPE allows open vocabulary decoding , in practice we see that our model performs much better than BPE baselines in particular on rare words ( Table 5 ) . It would be interesting to explore a combination of BPE and our proposed model in future work . No , Predicting the word with highest probability using vMF has the same computational complexity as nearest neighbor search We choose F1 to control for noise in the predicted sentence . Recall will only measure a reference word is produced . But by including precision , we measure what fraction of the predicted words are actually in reference . So a sentence producing all the words in reference but also a lot of garbage will be given less score ."}}