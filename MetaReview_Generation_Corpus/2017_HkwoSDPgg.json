{"year": "2017", "forum": "HkwoSDPgg", "title": "Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data", "decision": "Accept (Oral)", "meta_review": "The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written.", "reviews": [{"review_id": "HkwoSDPgg-0", "review_text": "This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for your encouraging review . In the revised version of our paper posted today , we added more context to the bound on the moment generating function provided in Theorem 1 . Our analysis is rather conservative in that it pessimistically assumes that , even if just one example in the training set for one teacher changes , the classifier produced by that teacher may change arbitrarily . Therefore , our analysis does not require any assumptions about the workings of the teachers ; we regard this point as an advantage of our approach . Nevertheless , we expect that stronger privacy guarantees may perhaps be established . We will add a discussion of this subject to the paper ."}, {"review_id": "HkwoSDPgg-1", "review_text": "This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as \"teachers\" model, which will train a \"student\" model to predict an output chosen by noisy voting among all of the teachers. The theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not. The experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your time and for your review . As in other work on differential privacy , we provide probabilistic guarantees , and these are parameterized to make trade-offs explicit . Our method provides a lower probability of privacy failure than other methods at comparable utility . ( You may find a detailed explanation of the different parameters of our analysis in our answer dated from November 22nd on OpenReview . ) An important advantage of MNIST and SVHN , at this stage of development of the field , is that they enable rapid experimentation and benchmarking -- -no dataset that contains sensitive data or that imitates sensitive data seems to be a standard `` fruit fly '' to the same degree . However , to further demonstrate the applicability of our approach to other natural data types , like the ones encountered in medical applications , we have followed your excellent suggestion of looking at specific applications using medical data . For this purpose we have performed additional experiments on the UCI Diabetes dataset , and have added the results to the new , revised version of our submission . The results show that our approach can apply well to medical data , and provide strong , meaningful privacy guarantees without reducing the accuracy of key medical-domain tasks , such as diagnostic classification and relapse prediction [ https : //arxiv.org/pdf/1602.04257.pdf ] . ( You may find additional details in our response to another review . )"}, {"review_id": "HkwoSDPgg-2", "review_text": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough. One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance. Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work. Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used. Other comments: Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble. G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013. Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. The paper is extremely well-written, for the most part. Some places needing clarification include: - Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database. - 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution. - Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for your time and for your review . Current deep learning models do not come with any accuracy guarantees , unfortunately . In particular , for tasks such as MNIST and SVHN , none of the state-of-the-art methods , even those not concerned about privacy , provide accuracy guarantees . With our current level of theoretical understanding , performance in terms of accuracy has to be established empirically , as is current practice in ML research , and as we have done here . In order to further demonstrate the general applicability of our approach , we performed experiments on two additional datasets . The UCI Adult dataset is made up of census data , and the task is to predict when individuals make over $ 50k per year . The UCI Diabetes dataset includes de-identified records of diabetic patients and corresponding hospital outcomes , which we use to predict whether diabetic patients where readmitted less than 30 days after their hospital release . We looked at the UCI datasets because some of them appear in previous papers on privacy ; among the many UCI datasets , in the limited time available , we quickly decided to study two that looked interesting . These UCI datasets seemed at least reasonable , and not random , as they had been used for similar ML applications , so they were the first ( and only ) datasets that we tried for our new experiments . They turned out to be well-suited to our techniques , even though we had no idea whether this was the case before running our experiments . While our experiments on MNIST and SVHN used convolutional neural networks and generative adversarial networks , we instead used random forests to train our teacher and student models for both of the UCI datasets . Our new results on these datasets show that despite the differing data types and architectures , we are able to provide meaningful privacy guarantees -- -with epsilon bounds below 2.7 -- -at modest costs in accuracy -- -with reductions of at most 2 % between a non-private model and our privacy-preserving student . We added them to the revised draft of our paper posted today . In a real deployment , noise needs to be added to the differential privacy guarantees themselves before releasing them . In order to allow for a meaningful comparison with related work , we provided these additional values in our updated draft . We note that doing so does not impact our results \u2019 competitiveness with prior work . We added a discussion of prior work by Jagannathan et al.in the revision of our paper . A key difference is that their approach is tailored to decision trees . Our approach is applicable to any machine learning algorithm , including more complex ones . In the specific case of decision trees , our approach can also work well , as demonstrated by our new , added results for the Adult and Diabetes datasets performed with random forests . Another key difference is that their approach modifies the classic model of a decision tree to include the Laplace mechanism . Thus , the privacy guarantee does not come from the disjoint sets of training data analyzed by different decision trees in the random forest , but rather from the modified architecture . In contrast , partitioning is essential for our approach 's privacy guarantees . We know of no other paper that is similarly comparable to our work , and could not find any via a reverse citation search based on Jagannathan et al.Regarding the comparison of Appendix C with GANs , we made it clearer that our goal was to reduce the number of labels required from the teacher ensemble . This makes the comparison more straightforward . We thank you for all remaining comments not discussed in this response : we agree with all of them and modified the draft accordingly ."}], "0": {"review_id": "HkwoSDPgg-0", "review_text": "This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for your encouraging review . In the revised version of our paper posted today , we added more context to the bound on the moment generating function provided in Theorem 1 . Our analysis is rather conservative in that it pessimistically assumes that , even if just one example in the training set for one teacher changes , the classifier produced by that teacher may change arbitrarily . Therefore , our analysis does not require any assumptions about the workings of the teachers ; we regard this point as an advantage of our approach . Nevertheless , we expect that stronger privacy guarantees may perhaps be established . We will add a discussion of this subject to the paper ."}, "1": {"review_id": "HkwoSDPgg-1", "review_text": "This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as \"teachers\" model, which will train a \"student\" model to predict an output chosen by noisy voting among all of the teachers. The theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not. The experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your time and for your review . As in other work on differential privacy , we provide probabilistic guarantees , and these are parameterized to make trade-offs explicit . Our method provides a lower probability of privacy failure than other methods at comparable utility . ( You may find a detailed explanation of the different parameters of our analysis in our answer dated from November 22nd on OpenReview . ) An important advantage of MNIST and SVHN , at this stage of development of the field , is that they enable rapid experimentation and benchmarking -- -no dataset that contains sensitive data or that imitates sensitive data seems to be a standard `` fruit fly '' to the same degree . However , to further demonstrate the applicability of our approach to other natural data types , like the ones encountered in medical applications , we have followed your excellent suggestion of looking at specific applications using medical data . For this purpose we have performed additional experiments on the UCI Diabetes dataset , and have added the results to the new , revised version of our submission . The results show that our approach can apply well to medical data , and provide strong , meaningful privacy guarantees without reducing the accuracy of key medical-domain tasks , such as diagnostic classification and relapse prediction [ https : //arxiv.org/pdf/1602.04257.pdf ] . ( You may find additional details in our response to another review . )"}, "2": {"review_id": "HkwoSDPgg-2", "review_text": "Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough. One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance. Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work. Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used. Other comments: Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as \u201cteacher-learner\u201d). The only time the private labeled data is used is when learning the \u201cprimary ensemble.\u201d A \"secondary ensemble\" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble. G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013. Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. The paper is extremely well-written, for the most part. Some places needing clarification include: - Last paragraph of 3.1. \u201call teachers\u2026.get the same training data\u2026.\u201d This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database. - 4.1: The authors state: \u201cThe number n of teachers is limited by a trade-off between the classification task\u2019s complexity and the available data.\u201d However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution. - Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for your time and for your review . Current deep learning models do not come with any accuracy guarantees , unfortunately . In particular , for tasks such as MNIST and SVHN , none of the state-of-the-art methods , even those not concerned about privacy , provide accuracy guarantees . With our current level of theoretical understanding , performance in terms of accuracy has to be established empirically , as is current practice in ML research , and as we have done here . In order to further demonstrate the general applicability of our approach , we performed experiments on two additional datasets . The UCI Adult dataset is made up of census data , and the task is to predict when individuals make over $ 50k per year . The UCI Diabetes dataset includes de-identified records of diabetic patients and corresponding hospital outcomes , which we use to predict whether diabetic patients where readmitted less than 30 days after their hospital release . We looked at the UCI datasets because some of them appear in previous papers on privacy ; among the many UCI datasets , in the limited time available , we quickly decided to study two that looked interesting . These UCI datasets seemed at least reasonable , and not random , as they had been used for similar ML applications , so they were the first ( and only ) datasets that we tried for our new experiments . They turned out to be well-suited to our techniques , even though we had no idea whether this was the case before running our experiments . While our experiments on MNIST and SVHN used convolutional neural networks and generative adversarial networks , we instead used random forests to train our teacher and student models for both of the UCI datasets . Our new results on these datasets show that despite the differing data types and architectures , we are able to provide meaningful privacy guarantees -- -with epsilon bounds below 2.7 -- -at modest costs in accuracy -- -with reductions of at most 2 % between a non-private model and our privacy-preserving student . We added them to the revised draft of our paper posted today . In a real deployment , noise needs to be added to the differential privacy guarantees themselves before releasing them . In order to allow for a meaningful comparison with related work , we provided these additional values in our updated draft . We note that doing so does not impact our results \u2019 competitiveness with prior work . We added a discussion of prior work by Jagannathan et al.in the revision of our paper . A key difference is that their approach is tailored to decision trees . Our approach is applicable to any machine learning algorithm , including more complex ones . In the specific case of decision trees , our approach can also work well , as demonstrated by our new , added results for the Adult and Diabetes datasets performed with random forests . Another key difference is that their approach modifies the classic model of a decision tree to include the Laplace mechanism . Thus , the privacy guarantee does not come from the disjoint sets of training data analyzed by different decision trees in the random forest , but rather from the modified architecture . In contrast , partitioning is essential for our approach 's privacy guarantees . We know of no other paper that is similarly comparable to our work , and could not find any via a reverse citation search based on Jagannathan et al.Regarding the comparison of Appendix C with GANs , we made it clearer that our goal was to reduce the number of labels required from the teacher ensemble . This makes the comparison more straightforward . We thank you for all remaining comments not discussed in this response : we agree with all of them and modified the draft accordingly ."}}