{"year": "2017", "forum": "B1-q5Pqxl", "title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "decision": "Accept (Poster)", "meta_review": "This paper provides two approaches to question answering: pointing to spans, and use of match-LSTM. The models are evaluated on SQuAD and MSMARCO. The reviewers we satisfied that, with the provision of additional comparisons and ablation studies submitted during discussion, the paper was acceptable to the conference, albeit marginally so.", "reviews": [{"review_id": "B1-q5Pqxl-0", "review_text": "Summary: The paper presents a deep neural network for the task of machine comprehension on the SQuAD dataset. The proposed model is based on two previous works -- match-LSTM and Pointer Net. Match-LSTM produces attention over each word in the given question for each word in the given passage, and sequentially aggregates this matching of each word in the passage with the words in the question. The pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage. The experimental results show that both the variants of the proposed model outperform the baseline presented in the SQuAD paper. The paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types. Strengths: 1. A novel end-to-end model for the task of machine comprehension rather than using hand-crafted features. 2. Significant performance boost over the baseline presented in the SQuAD paper. 3. Some insightful analyses of the results such as performance is better when answers are short, \"why\" questions are difficult to answer. Weaknesses/Questions/Suggestions: 1. The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer. 2. It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer. 3. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required. 4. Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across dimension of Q. Why not learn different activations for each dimension? 5. I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1. 6. Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail? Review Summary: The paper presents a reasonable end-to-end model for the task of machine comprehension on the SQuAD dataset, which outperforms the baseline model significantly. However, it would be good if more analyses / ablation studies / insights are included regarding -- how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult.", "rating": "7: Good paper, accept", "reply_text": "We thank you for your valuable comments ! Q : The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps . So , it would be insightful if authors could compare the model performance with and without attention in match-LSTM , and with and without attention in answer pointer layer . A : We have added these baselines in Table 1 with the model name `` LSTM with Ans-Ptr '' . We use the final state of the question LSTM to replace the representation computed by the attention mechanism and the performance drop heavily without attention . Q : It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer . A : The sequence model has a problem of early stop prediction , while the boundary model can somehow overcome this . We did another experiment on the `` MS MARCO : A Human Generated MAchine Reading Comprehension Dataset '' , where the question is a real submission to the Bing search engine . As the average length of the answer in this dataset is 16 words which is much longer than the 3 words in average for the SQuAD , our boundary model can significantly outperform the sequence model . The average length of the answers generated by the sequence model is 7 words , while the boundary model is 21 words . We further show comparison between the predictions of the boundary and the sequence models in the appendix B. Q : I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning ( table 3 in SQuAD paper ) . This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required . A : We show the predictions of different models on different types of questions in Appendix B . Our model ca n't solve the questions that need several sentences reasoning . Q : Could authors please explain why the activations resulting from { h^p } _i and { h^r } _ { i-1 } in G_i in equation 2 are being repeated across dimension of Q . Why not learn different activations for each dimension ? A : We think that it is somehow introducing the word position information of the question when computing the attention weights , if each dimension learns a diffenrt { W^p } . We think the semantic meaning of the word is more important than the position information for the attention mechanism . We will explore it in the future . Thank you for the suggestion ! Q : I wonder why Bi-Ans-Ptr is not used in the ensemble model ( last row in table 2 ) when it is shown that Bi-Ans-Ptr improves performance by 1.2 % in F1 . A : I 'm sorry about it . Our model still runs on CPUs and that takes time to train a single model . We haven \u2019 t successfully built an ensemble model on it yet . We will further explore it and update it . Q : Could authors please discuss and compare the DCR model ( in table 2 ) in the paper in more detail ? A : Our model was first posted in August 2017 through arXiv , two months earlier than the DCR model and other similar work was posted . Our model has some similarities with DCR , such as the attention parts inspired by match-LSTM . While they maximize the correct span from all the candidate spans , we predict the start and the end points of the span . We 've added this in the revision ."}, {"review_id": "B1-q5Pqxl-1", "review_text": "The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text. Strength: - The suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results) - The paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work. Weaknesses: 1. It is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text. 2. Experimental evaluation 2.1. It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance. 2.2. It would be interested if this approach generalizes to other datasets. Other (minor/discussion points) - The task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction. - I am wondering how much this task can be seen as a \u201cguided extractive summarization\u201d, i.e. where the question guides the summarization process. - Page 6, last paragraph: missing \u201c.\u201d: \u201c\u2026 searching This\u2026\u201d Summary: While the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for your valuable comments ! Q : It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance . A : I 'm sorry about it . Our model still runs on CPUs and that takes time to train a single model . We haven \u2019 t successfully built an ensemble model on it yet . We will further explore it and update it . Q : It would be interested if this approach generalizes to other datasets . A : We further explored the dataset `` MS MARCO : A Human Generated MAchine Reading COmprehension Dataset '' , where the question is a real submission to the Bing search . Our experiment is shown in Table 2 . Our boundary model can outperform the `` Golden Passage '' baseline , which uses the human selected passage from the 10 candidate passages as the answer . We only predict one span in these passages . Our performance on the hidden test data is still in submission and we will update the paper when we get feedback . Q : The task and approach seem to have some similarity of locating queries in images and visual question answering . The authors might want to consider pointing to related works in this direction . A : We added the analysis of the difference between our work and VQA problem in the related work . Thank you for the suggestion ! Q : I am wondering how much this task can be seen as a \u201c guided extractive summarization \u201d , i.e.where the question guides the summarization process . A : It 's an quite interesting problem ! We will further explore it ! Thank you for the suggestion ! Q : Page 6 , last paragraph : missing \u201c . \u201d : \u201c \u2026 searching This\u2026 \u201d A : We have fixed the typo in the updated version . Thank you !"}, {"review_id": "B1-q5Pqxl-2", "review_text": "SUMMARY. This paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage. The proposed model combines two well-know neural network architectures match-lstm and pointer nets. First the passage and the questions are encoded with a unidirectional LSTM. Then the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question. For each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm. The same process is done in the opposite direction with a backward lstm. The final representation is a concatenation of the two lstms. As a decoded a pointer network is used. The authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer. The proposed model is tested on the Stanford Question Answering Dataset. An ensemble of the proposed model achieves performance close to state-of-the-art models. ---------- OVERALL JUDGMENT I think the model is interesting mainly because of the use of pointer networks as a decoder. One thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm. The analysis of the model is interesting and insightful. The sharing of the code is good.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for your valuable comments ! As our models on the SQuAD dataset was released through arXiv in August 2016 , around two months earlier than the other studies on the SQuAD dataset , we believe we were the first to bring the following two useful insights into this task : ( 1 ) Using attention mechanism to make each word of the passage to have a certain degree of compatibility with the question is important for the SQuAD dataset . ( 2 ) Only predicting the start and the end points of the answer span is better than predicting the answer span word by word . The other studies on the SQuAD dataset ( Bidirectional Attention Flow for Machine Comprehension ( https : //openreview.net/forum ? id=HJ0UKP9ge ) and Dynamic Co-attention Networks For Question Answering ( https : //openreview.net/forum ? id=rJeKjwvclx ) ) further explored these two insights and achieved the state-of-art performance . In the latest version of the paper , we further added experiments on the dataset \u201c MS MARCO : A Human Generated Machine Reading Comprehension Dataset \u201d . We believe we are the first to explore this dataset where the question is a real submission to the Bing search engine . As the average length of the answer in this dataset is 16 words , which is much longer than the average ( 3 words ) for the SQuAD dataset , we further showed that our boundary model could significantly outperform the sequence model . The sequence model will always stop the prediction early , while the boundary model can somehow overcome this . Our boundary model can outperform the `` Golden Passage '' baseline , which uses the human selected passage from the 10 candidate passages as the answer . We only predict one span in these passages . We 've updated the paper by adding this experiment and the analysis , and several prediction cases in the appendix . We will further explore deep match-LSTM . Thank you for the suggestion !"}], "0": {"review_id": "B1-q5Pqxl-0", "review_text": "Summary: The paper presents a deep neural network for the task of machine comprehension on the SQuAD dataset. The proposed model is based on two previous works -- match-LSTM and Pointer Net. Match-LSTM produces attention over each word in the given question for each word in the given passage, and sequentially aggregates this matching of each word in the passage with the words in the question. The pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage. The experimental results show that both the variants of the proposed model outperform the baseline presented in the SQuAD paper. The paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types. Strengths: 1. A novel end-to-end model for the task of machine comprehension rather than using hand-crafted features. 2. Significant performance boost over the baseline presented in the SQuAD paper. 3. Some insightful analyses of the results such as performance is better when answers are short, \"why\" questions are difficult to answer. Weaknesses/Questions/Suggestions: 1. The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer. 2. It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer. 3. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required. 4. Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across dimension of Q. Why not learn different activations for each dimension? 5. I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1. 6. Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail? Review Summary: The paper presents a reasonable end-to-end model for the task of machine comprehension on the SQuAD dataset, which outperforms the baseline model significantly. However, it would be good if more analyses / ablation studies / insights are included regarding -- how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult.", "rating": "7: Good paper, accept", "reply_text": "We thank you for your valuable comments ! Q : The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps . So , it would be insightful if authors could compare the model performance with and without attention in match-LSTM , and with and without attention in answer pointer layer . A : We have added these baselines in Table 1 with the model name `` LSTM with Ans-Ptr '' . We use the final state of the question LSTM to replace the representation computed by the attention mechanism and the performance drop heavily without attention . Q : It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer . A : The sequence model has a problem of early stop prediction , while the boundary model can somehow overcome this . We did another experiment on the `` MS MARCO : A Human Generated MAchine Reading Comprehension Dataset '' , where the question is a real submission to the Bing search engine . As the average length of the answer in this dataset is 16 words which is much longer than the 3 words in average for the SQuAD , our boundary model can significantly outperform the sequence model . The average length of the answers generated by the sequence model is 7 words , while the boundary model is 21 words . We further show comparison between the predictions of the boundary and the sequence models in the appendix B. Q : I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning ( table 3 in SQuAD paper ) . This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required . A : We show the predictions of different models on different types of questions in Appendix B . Our model ca n't solve the questions that need several sentences reasoning . Q : Could authors please explain why the activations resulting from { h^p } _i and { h^r } _ { i-1 } in G_i in equation 2 are being repeated across dimension of Q . Why not learn different activations for each dimension ? A : We think that it is somehow introducing the word position information of the question when computing the attention weights , if each dimension learns a diffenrt { W^p } . We think the semantic meaning of the word is more important than the position information for the attention mechanism . We will explore it in the future . Thank you for the suggestion ! Q : I wonder why Bi-Ans-Ptr is not used in the ensemble model ( last row in table 2 ) when it is shown that Bi-Ans-Ptr improves performance by 1.2 % in F1 . A : I 'm sorry about it . Our model still runs on CPUs and that takes time to train a single model . We haven \u2019 t successfully built an ensemble model on it yet . We will further explore it and update it . Q : Could authors please discuss and compare the DCR model ( in table 2 ) in the paper in more detail ? A : Our model was first posted in August 2017 through arXiv , two months earlier than the DCR model and other similar work was posted . Our model has some similarities with DCR , such as the attention parts inspired by match-LSTM . While they maximize the correct span from all the candidate spans , we predict the start and the end points of the span . We 've added this in the revision ."}, "1": {"review_id": "B1-q5Pqxl-1", "review_text": "The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text. Strength: - The suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results) - The paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work. Weaknesses: 1. It is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text. 2. Experimental evaluation 2.1. It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance. 2.2. It would be interested if this approach generalizes to other datasets. Other (minor/discussion points) - The task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction. - I am wondering how much this task can be seen as a \u201cguided extractive summarization\u201d, i.e. where the question guides the summarization process. - Page 6, last paragraph: missing \u201c.\u201d: \u201c\u2026 searching This\u2026\u201d Summary: While the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for your valuable comments ! Q : It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance . A : I 'm sorry about it . Our model still runs on CPUs and that takes time to train a single model . We haven \u2019 t successfully built an ensemble model on it yet . We will further explore it and update it . Q : It would be interested if this approach generalizes to other datasets . A : We further explored the dataset `` MS MARCO : A Human Generated MAchine Reading COmprehension Dataset '' , where the question is a real submission to the Bing search . Our experiment is shown in Table 2 . Our boundary model can outperform the `` Golden Passage '' baseline , which uses the human selected passage from the 10 candidate passages as the answer . We only predict one span in these passages . Our performance on the hidden test data is still in submission and we will update the paper when we get feedback . Q : The task and approach seem to have some similarity of locating queries in images and visual question answering . The authors might want to consider pointing to related works in this direction . A : We added the analysis of the difference between our work and VQA problem in the related work . Thank you for the suggestion ! Q : I am wondering how much this task can be seen as a \u201c guided extractive summarization \u201d , i.e.where the question guides the summarization process . A : It 's an quite interesting problem ! We will further explore it ! Thank you for the suggestion ! Q : Page 6 , last paragraph : missing \u201c . \u201d : \u201c \u2026 searching This\u2026 \u201d A : We have fixed the typo in the updated version . Thank you !"}, "2": {"review_id": "B1-q5Pqxl-2", "review_text": "SUMMARY. This paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage. The proposed model combines two well-know neural network architectures match-lstm and pointer nets. First the passage and the questions are encoded with a unidirectional LSTM. Then the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question. For each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm. The same process is done in the opposite direction with a backward lstm. The final representation is a concatenation of the two lstms. As a decoded a pointer network is used. The authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer. The proposed model is tested on the Stanford Question Answering Dataset. An ensemble of the proposed model achieves performance close to state-of-the-art models. ---------- OVERALL JUDGMENT I think the model is interesting mainly because of the use of pointer networks as a decoder. One thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm. The analysis of the model is interesting and insightful. The sharing of the code is good.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for your valuable comments ! As our models on the SQuAD dataset was released through arXiv in August 2016 , around two months earlier than the other studies on the SQuAD dataset , we believe we were the first to bring the following two useful insights into this task : ( 1 ) Using attention mechanism to make each word of the passage to have a certain degree of compatibility with the question is important for the SQuAD dataset . ( 2 ) Only predicting the start and the end points of the answer span is better than predicting the answer span word by word . The other studies on the SQuAD dataset ( Bidirectional Attention Flow for Machine Comprehension ( https : //openreview.net/forum ? id=HJ0UKP9ge ) and Dynamic Co-attention Networks For Question Answering ( https : //openreview.net/forum ? id=rJeKjwvclx ) ) further explored these two insights and achieved the state-of-art performance . In the latest version of the paper , we further added experiments on the dataset \u201c MS MARCO : A Human Generated Machine Reading Comprehension Dataset \u201d . We believe we are the first to explore this dataset where the question is a real submission to the Bing search engine . As the average length of the answer in this dataset is 16 words , which is much longer than the average ( 3 words ) for the SQuAD dataset , we further showed that our boundary model could significantly outperform the sequence model . The sequence model will always stop the prediction early , while the boundary model can somehow overcome this . Our boundary model can outperform the `` Golden Passage '' baseline , which uses the human selected passage from the 10 candidate passages as the answer . We only predict one span in these passages . We 've updated the paper by adding this experiment and the analysis , and several prediction cases in the appendix . We will further explore deep match-LSTM . Thank you for the suggestion !"}}