{"year": "2018", "forum": "rkfbLilAb", "title": "Improving Search Through A3C Reinforcement Learning Based Conversational Agent", "decision": "Reject", "meta_review": "meta score: 4\n\nThis paper is primarily an application paper applying known RL techniques to dialogue.    Very little reference to the extensive literature in this area.\n\nPros:\n - interesting application (digital search)\n - revised version contains subjective evaluation of experiments\n\nCons:\n - limited technical novelty\n - very weak links to the state-of-the-art, missing many key aspects of the research domain\n", "reviews": [{"review_id": "rkfbLilAb-0", "review_text": "The paper \"IMPROVING SEARCH THROUGH A3C REINFORCEMENT LEARNING BASED CONVERSATIONAL AGENT\" proposes to define an agent to guide users in information retrieval tasks. By proposing refinements of the query, categorizations of the results or some other bookmarking actions, the agent is supposed to help the user in achieving his search. The proposed agent is learned via reinforcement learning. My concern with this paper is about the experiments that are only based on simulated agents, as it is the case for learning. While it can be questionable for learning (but we understand why it is difficult to overcome), it is very problematic for the experiments to not have anything that demonstrates the usability of the approach in a real-world scenario. I have serious doubts about the performances of such an artificially learned approach for achieving real-world search tasks. Also, for me the experimental section is not sufficiently detailed, which lead to not reproducible results. Moreover, authors should have considered baselines (only the two proposed agents are compared which is clearly not sufficient). Also, both models have some issues from my point of view. First, the Q-learning methods looks very complex: how could we expect to get an accurate model with 10^7 states ? No generalization about the situations is done here, examples of trajectories have to be collected for each individual considered state, which looks very huge (especially if we think about the number of possible trajectories in such an MDP). The second model is able to generalize from similar situations thanks to the neural architecture that is proposed. However, I have some concerns about it: why keeping the history of actions in the inputs since it is captured by the LSTM cell ? It is a redondant information that might disturb the process. Secondly, the proposed loss looks very heuristic for me, it is difficult to understand what is really optimized here. Particularly, the loss entropy function looks strange to me. Is it classical ? Are there some references of such a method to maintain some exploration ability. I understand the need of exploration, but including it in the loss function reduces the interpretability of the objective (wouldn't it be preferable to use a more classical loss but with an epsilon greedy policy?). Other remarks: - In the begining of \"varying memory capacity\" section, what is \"100, 150 and 250\" ? Time steps ? What is the unit ? Seconds ? - I did not understand the \"Capturing seach context at local and global level\" at all - In the loss entropy formula, the two negation signs could be removed ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We evaluated our system through real humans and added the results in section 4.3 . Please refer to appendix ( section 6.2 ) for some conversations between actual users and trained agent . For performing experiments with humans , we developed chat interface where an actual user can interact with the agent during their search . The implementation details of the chat interface have been discussed in the appendix ( section 6.1.1 ) . User action is obtained from user utterance using a rule-based Natural language unit ( NLU ) which uses dependency tree based syntactic parsing , stop words and pre-defined rules ( as described in appendix , section 6.1.2 ) . You may refer to supplementary material ( footnote-2 , page-9 ) which contains a video demonstrating search on our conversational search interface . In order to evaluate our system with the virtual user , we simulate validation episodes between the agent and the virtual user after every training episode . This simulation comprises of sequence of alternate actions between the user and the agent . The user action is sampled using the user model while the agent action is sampled using the policy learned till that point . Corresponding to a single validation episode , we determine two performance metrics . First is total reward obtained at the end of the episode . The values of the states observed in the episode is obtained using the model , average of states values observed during the validation episode is determined and used as the second performance metric . Average of these values over different validation episodes is taken and depicted in figures 3,4,5 and 6 ."}, {"review_id": "rkfbLilAb-1", "review_text": "This paper proposes to use RL (Q-learning and A3C) to optimize the interaction strategy of a search assistant. The method is trained against a simulated user to bootstrap the learning process. The algorithm is tested on some search base of assets such as images or videos. My first concern is about the proposed reward function which is composed of different terms. These are very engineered and cannot easily transfer to other tasks. Then the different algorithms are assessed according to their performance w.r.t. to these rewards. They of course improve with training since this is the purpose of RL to optimize these numbers. Assessment of a dialogue system should be done according to metrics obtained through actual interactions with users, not according to auxiliary tasks etc. But above all, this paper incredibly lacks of context in both RL and dialogue systems. The authors cite a 2014 paper when it comes to refer to Q-learning (Q-learning was first published in 1989 by Watkins). The first time dialogue has been casted into a RL problem is in 1997 by E. Levin and R. Pieraccini (although it has been suggested before by M. Walker). User simulation has been proposed at the same time and further developed in the early 2000 by Schatzmann, Young, Pietquin etc. Using LSTMs to build user models has been proposed in 2016 (Interspeech) by El Asri et al. Buiding efficient reward functions for RL-based conversational systems has also been studied for more than 20 years with early work by M. Walker on PARADISE (@ACL 1997) but also via inverse RL by Chandramohan et al (2011). A2C (which is a single-agent version of A3C) has been used by Strub et al (@ IJCAI 2017) to optimize visually grounded dialogue systems. RL-based recommender systems have also been studied before (e.g. Shani in JMLR 2005). I think the authors should first read the state of the art in the domain before they suggest new solutions. ", "rating": "2: Strong rejection", "reply_text": "Thanks for your reviews . We have modeled rewards specifically for the domain of digital assets search in order to obtain a bootstrapped agent which performs reasonably well in assisting humans in their search so that it can be fine tuned further based on interaction with humans . As our problem caters to a subjective task of searching digital assets which is different from more common objective tasks such as reservation , it is difficult to determine generic rewards based on whether the agent has been able to provide exact information to the user unlike objective search tasks where rewards are measured based on required information has been provided to the user . This makes rewards transferability between subjective and objective search difficult . Though our modeled rewards are easily transferable to search tasks such as e-commerce sites where search tasks comprises of a subjective component ( in addition to objective preferences such as price ) . Since we aim to optimise dialogue strategy and do not generate dialogue utterances , we assign the rewards corresponding to the appropriateness of the action performed by the agent considering the state and history of the search . We have used some rewards such as task success ( based on implicit and explicit feedback from the user during the search ) which is also used in PARADISE framework [ 1 ] . At the same time several metrics used by PARADISE can not be used for modelling rewards . For instance , time required ( number of turns ) for user to search desired results can not be penalised since it can be possible that user is finding the system engaging and helpful in refining the results better which may increase number of turns in the search . We evaluated our system through humans and added the results to the paper , please refer to section 4.3 in the updated paper . You may refer to appendix ( section 6.2 ) for some conversations between actual users and the trained agent . Thanks for suggesting related references , we have updated our paper based on the suggestions . Kindly suggest any other further improvements . [ 1 ] Walker , Marilyn A. , et al . `` PARADISE : A framework for evaluating spoken dialogue agents . '' Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics . Association for Computational Linguistics , 1997 ."}, {"review_id": "rkfbLilAb-2", "review_text": "The paper describes reinforcement learning techniques for digital asset search. The RL techniques consist of A3C and DQN. This is an application paper since the techniques described already exist. Unfortunately, there is a lack of detail throughout the paper and therefore it is not possible for someone to reproduce the results if desired. Since there is no corpus of message response pairs to train the model, the paper trains a simulator from logs to emulate user behaviours. Unfortunately, there is no description of the algorithm used to obtain the simulator. The paper explains that the simulator is obtained from log data, but this is not sufficient. The RL problem is described at a very high level in the sense that abstract states and actions are listed, but there is no explanation about how those abstract states are recognized from the raw text and there is no explanation about how the actions are turned into text. There seems to be some confusion in the notion of state. After describing the abstract states, it is explained that actions are selected based on a history of states. This suggests that the abstract states are really abstract observations. In fact, this becomes obvious when the paper introduces the RNN where a hidden belief is computed by combining the observations. The rewards are also described at a hiogh level, but it is not clear how exactly they are computed. The digital search application is interesting, however a detailed description with comprehensive experiments are needed for the publication of an application paper.", "rating": "3: Clear rejection", "reply_text": "Due to legal issues , we can not not share the query session logs data . We have tried to provide details of our algorithm which can be used for obtaining user model from any given session logs data . The mapping between interactions in session log data and user actions which the agent can understand has been discussed in table 3 . Using these mapping , we obtain a probabilistic user model ( algorithm has been described in section 3.5 ) . Figure 1 in the paper demonstrates how interactions in a session can be mapped to user actions . Kindly mention the sections which are lacking details and missing information in the algorithm for user model which will help us in improving our paper ."}], "0": {"review_id": "rkfbLilAb-0", "review_text": "The paper \"IMPROVING SEARCH THROUGH A3C REINFORCEMENT LEARNING BASED CONVERSATIONAL AGENT\" proposes to define an agent to guide users in information retrieval tasks. By proposing refinements of the query, categorizations of the results or some other bookmarking actions, the agent is supposed to help the user in achieving his search. The proposed agent is learned via reinforcement learning. My concern with this paper is about the experiments that are only based on simulated agents, as it is the case for learning. While it can be questionable for learning (but we understand why it is difficult to overcome), it is very problematic for the experiments to not have anything that demonstrates the usability of the approach in a real-world scenario. I have serious doubts about the performances of such an artificially learned approach for achieving real-world search tasks. Also, for me the experimental section is not sufficiently detailed, which lead to not reproducible results. Moreover, authors should have considered baselines (only the two proposed agents are compared which is clearly not sufficient). Also, both models have some issues from my point of view. First, the Q-learning methods looks very complex: how could we expect to get an accurate model with 10^7 states ? No generalization about the situations is done here, examples of trajectories have to be collected for each individual considered state, which looks very huge (especially if we think about the number of possible trajectories in such an MDP). The second model is able to generalize from similar situations thanks to the neural architecture that is proposed. However, I have some concerns about it: why keeping the history of actions in the inputs since it is captured by the LSTM cell ? It is a redondant information that might disturb the process. Secondly, the proposed loss looks very heuristic for me, it is difficult to understand what is really optimized here. Particularly, the loss entropy function looks strange to me. Is it classical ? Are there some references of such a method to maintain some exploration ability. I understand the need of exploration, but including it in the loss function reduces the interpretability of the objective (wouldn't it be preferable to use a more classical loss but with an epsilon greedy policy?). Other remarks: - In the begining of \"varying memory capacity\" section, what is \"100, 150 and 250\" ? Time steps ? What is the unit ? Seconds ? - I did not understand the \"Capturing seach context at local and global level\" at all - In the loss entropy formula, the two negation signs could be removed ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We evaluated our system through real humans and added the results in section 4.3 . Please refer to appendix ( section 6.2 ) for some conversations between actual users and trained agent . For performing experiments with humans , we developed chat interface where an actual user can interact with the agent during their search . The implementation details of the chat interface have been discussed in the appendix ( section 6.1.1 ) . User action is obtained from user utterance using a rule-based Natural language unit ( NLU ) which uses dependency tree based syntactic parsing , stop words and pre-defined rules ( as described in appendix , section 6.1.2 ) . You may refer to supplementary material ( footnote-2 , page-9 ) which contains a video demonstrating search on our conversational search interface . In order to evaluate our system with the virtual user , we simulate validation episodes between the agent and the virtual user after every training episode . This simulation comprises of sequence of alternate actions between the user and the agent . The user action is sampled using the user model while the agent action is sampled using the policy learned till that point . Corresponding to a single validation episode , we determine two performance metrics . First is total reward obtained at the end of the episode . The values of the states observed in the episode is obtained using the model , average of states values observed during the validation episode is determined and used as the second performance metric . Average of these values over different validation episodes is taken and depicted in figures 3,4,5 and 6 ."}, "1": {"review_id": "rkfbLilAb-1", "review_text": "This paper proposes to use RL (Q-learning and A3C) to optimize the interaction strategy of a search assistant. The method is trained against a simulated user to bootstrap the learning process. The algorithm is tested on some search base of assets such as images or videos. My first concern is about the proposed reward function which is composed of different terms. These are very engineered and cannot easily transfer to other tasks. Then the different algorithms are assessed according to their performance w.r.t. to these rewards. They of course improve with training since this is the purpose of RL to optimize these numbers. Assessment of a dialogue system should be done according to metrics obtained through actual interactions with users, not according to auxiliary tasks etc. But above all, this paper incredibly lacks of context in both RL and dialogue systems. The authors cite a 2014 paper when it comes to refer to Q-learning (Q-learning was first published in 1989 by Watkins). The first time dialogue has been casted into a RL problem is in 1997 by E. Levin and R. Pieraccini (although it has been suggested before by M. Walker). User simulation has been proposed at the same time and further developed in the early 2000 by Schatzmann, Young, Pietquin etc. Using LSTMs to build user models has been proposed in 2016 (Interspeech) by El Asri et al. Buiding efficient reward functions for RL-based conversational systems has also been studied for more than 20 years with early work by M. Walker on PARADISE (@ACL 1997) but also via inverse RL by Chandramohan et al (2011). A2C (which is a single-agent version of A3C) has been used by Strub et al (@ IJCAI 2017) to optimize visually grounded dialogue systems. RL-based recommender systems have also been studied before (e.g. Shani in JMLR 2005). I think the authors should first read the state of the art in the domain before they suggest new solutions. ", "rating": "2: Strong rejection", "reply_text": "Thanks for your reviews . We have modeled rewards specifically for the domain of digital assets search in order to obtain a bootstrapped agent which performs reasonably well in assisting humans in their search so that it can be fine tuned further based on interaction with humans . As our problem caters to a subjective task of searching digital assets which is different from more common objective tasks such as reservation , it is difficult to determine generic rewards based on whether the agent has been able to provide exact information to the user unlike objective search tasks where rewards are measured based on required information has been provided to the user . This makes rewards transferability between subjective and objective search difficult . Though our modeled rewards are easily transferable to search tasks such as e-commerce sites where search tasks comprises of a subjective component ( in addition to objective preferences such as price ) . Since we aim to optimise dialogue strategy and do not generate dialogue utterances , we assign the rewards corresponding to the appropriateness of the action performed by the agent considering the state and history of the search . We have used some rewards such as task success ( based on implicit and explicit feedback from the user during the search ) which is also used in PARADISE framework [ 1 ] . At the same time several metrics used by PARADISE can not be used for modelling rewards . For instance , time required ( number of turns ) for user to search desired results can not be penalised since it can be possible that user is finding the system engaging and helpful in refining the results better which may increase number of turns in the search . We evaluated our system through humans and added the results to the paper , please refer to section 4.3 in the updated paper . You may refer to appendix ( section 6.2 ) for some conversations between actual users and the trained agent . Thanks for suggesting related references , we have updated our paper based on the suggestions . Kindly suggest any other further improvements . [ 1 ] Walker , Marilyn A. , et al . `` PARADISE : A framework for evaluating spoken dialogue agents . '' Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics . Association for Computational Linguistics , 1997 ."}, "2": {"review_id": "rkfbLilAb-2", "review_text": "The paper describes reinforcement learning techniques for digital asset search. The RL techniques consist of A3C and DQN. This is an application paper since the techniques described already exist. Unfortunately, there is a lack of detail throughout the paper and therefore it is not possible for someone to reproduce the results if desired. Since there is no corpus of message response pairs to train the model, the paper trains a simulator from logs to emulate user behaviours. Unfortunately, there is no description of the algorithm used to obtain the simulator. The paper explains that the simulator is obtained from log data, but this is not sufficient. The RL problem is described at a very high level in the sense that abstract states and actions are listed, but there is no explanation about how those abstract states are recognized from the raw text and there is no explanation about how the actions are turned into text. There seems to be some confusion in the notion of state. After describing the abstract states, it is explained that actions are selected based on a history of states. This suggests that the abstract states are really abstract observations. In fact, this becomes obvious when the paper introduces the RNN where a hidden belief is computed by combining the observations. The rewards are also described at a hiogh level, but it is not clear how exactly they are computed. The digital search application is interesting, however a detailed description with comprehensive experiments are needed for the publication of an application paper.", "rating": "3: Clear rejection", "reply_text": "Due to legal issues , we can not not share the query session logs data . We have tried to provide details of our algorithm which can be used for obtaining user model from any given session logs data . The mapping between interactions in session log data and user actions which the agent can understand has been discussed in table 3 . Using these mapping , we obtain a probabilistic user model ( algorithm has been described in section 3.5 ) . Figure 1 in the paper demonstrates how interactions in a session can be mapped to user actions . Kindly mention the sections which are lacking details and missing information in the algorithm for user model which will help us in improving our paper ."}}