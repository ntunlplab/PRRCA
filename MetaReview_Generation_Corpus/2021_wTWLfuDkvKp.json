{"year": "2021", "forum": "wTWLfuDkvKp", "title": "Should Ensemble Members Be Calibrated?", "decision": "Reject", "meta_review": "This paper studies ensemble calibration and the relationship between the calibration of individual ensemble member models with the calibration of the resulting ensemble prediction.  The main theoretical result is that individual ensemble members should not be individually calibrated in order to have a well-calibrated ensemble prediction.  While other recent work has found this to be the case in empirical results, this paper substantiates the empirical results through theoretical results. \n\nPros:\n* Theoretical study of ensemble calibration with meaningful insights\n\nCons:\n* Contributions limited to theoretical study of known observation and dynamic temperature scaling.\n* Dynamic temperature scaling is not shown to outperform baseline methods.\n* Limited experimental validation: CIFAR-10/CIFAR-100.\n\nThe authors engaged in a extensive discussion with reviewers and made changes to their paper, including adding standard deviation results over multiple runs and the SKCE calibration measure.\n\nOverall this is solid work and could be accepted to the conference; however, reviewers agree that parts of the work are lacking, in particular: 1. limited experimental evaluation (one type of task, one/two datasets only), and 2. given known literature the benefit of the derived theoretical results to practioners is not clear.  The discussions have been unable to resolve this disagreement.\n", "reviews": [{"review_id": "wTWLfuDkvKp-0", "review_text": "Update after the author response : I 've read the other reviews , and agree with R2 and R3 . I think the paper is useful ( emphasizes you need to calibrate the final ensemble , not enough to calibrate members ) , and has some nice conceptual contributions ( explaining that if ensemble accuracy > average member accuracy ( which is usually the case ) , and the ensemble is calibrated even in just a global/weak sense , then the members must be uncalibrated ) . This could spur more research into conceptually analyzing ensembles , and seems interesting . But I understand the other reviewer 's concerns that it 's not clear what practical impact this will have , so I 'm keeping my score at a 6 ( instead of raising to a 7 ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper tackles the problem of calibrating an ensemble . They show experimentally that calibrating all members of an ensemble is often not enough to calibrate the combined ensemble , so instead we need to calibrate the final predictions of the ensemble . Additionally , they show that using a different temperature parameter for different regions of outputs can improve calibration . They explain why if the ensemble members are top-label calibrated ( even in a very weak sense they call \u201c global \u201d calibration \u201d ) , and the ensemble is calibrated , then the ensemble is less accurate than the average member of the ensemble . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : They make interesting observations about calibration of ensembles that could guide practitioners . For example , that it \u2019 s not enough to calibrate the members of the ensemble . They also raise an intriguing connection between calibration of ensemble members and ensemble accuracy , one would not expect a priori that if both are calibrated the ensemble would do worse than the average member . I could see this result being interesting to people who study ensembles as well . There are some weaknesses in writing and execution , but overall this paper is probably worth publishing if edited . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : - I think it \u2019 s a nice observation that calibrating the members of an ensemble may not yield a calibrated ensemble . It \u2019 s easy to come up with toy examples where this is the case , but it \u2019 s interesting that this seems to be the case in practice . - They make an intriguing observation that if the ensemble members are in fact calibrated and the ensemble is calibrated , then the ensemble accuracy is at most the average member accuracy # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : - I believe the writing can be substantially simplified . The core ideas are simple and nice , but it takes a lot of effort to get to them , and I believe the authors should put in more work into making this understandable . - Some of the results seem unrealistic and can be omitted . For example in the start of section 4.1 , the first couple of results require that the ensemble member regions and ensemble regions are the same . This seems rather unrealistic . The assumptions in prop 1 seem too strong to me . I \u2019 d remove the mentions of regions and I \u2019 d instead mention the other results ( prop 2 , 3 , 4 ) in the main paper , Section 4.1 . You could just move the propositions , and give some intuition for why the results are true . Removing regions should also considerably simplify the notation and setup . - I \u2019 m not quite sure what you mean in the intro when you say \u201c Eq . ( 1 ) doesn \u2019 t explicitly reflect the relation between \u2026 and the underlying data distribution p ( x , y ) \u201d . The definition in Equation ( 1 ) uses p ( x , y ) . I \u2019 m not sure why all the definitions in 3.1 and 3.3 are defined in a way different from the standard ways in the calibration literature e.g.in Kull et al 2019 or Kumar et al 2019 . - Temperature scaling is performed on logits , not on the actual probabilities . From equations 24 , 25 , and 26 it looks like you might be doing temperature scaling on the probability space ( in equation 24 , 25 the first argument to f is the probability , not the logit ) , which looks a bit odd . - Prop 4 should also hold when K = 2 ( 2 ensemble members ) I believe . Happy to provide an example . - Some symbols are undefined . For example , \\delta ( y^ { ( i ) } , \\omega_j ) , I don \u2019 t believe \\delta is defined . I think it should be 1 if they are equal and 0 otherwise ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions and things to improve : - Please answer the cons above . - Ensembles are particularly useful because they tend to be more calibrated out of domain ( Lakshminarayanan et al 2017 ) . It could be useful to see which of these methods ( calibrating the members , or the entire ensemble ) is better calibrated when we have domain shift ( e.g.training data = CIFAR-10 , test data = CIFAR-10C , Hendrycks et al 2019 ) . - Having confidence intervals for the calibration errors would be nice ( and also using more modern , debiased estimators to estimate the calibration error ) e.g.in Kumar et al 2019 . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # All cites mentioned are already in the paper , except : Benchmarking Neural Network Robustness to Common Corruptions and Perturbations . Dan Hendrycks , Thomas Dietterich . ICLR 2019 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q1 : \u2022 I believe the writing can be substantially simplified . The core ideas are simple and nice , but it takes a lot of effort to get to them , and I believe the authors should put in more work into making this understandable . \u2022 Some of the results seem unrealistic and can be omitted . For example in the start of section 4.1 , the first couple of results require that the ensemble member regions and ensemble regions are the same . This seems rather unrealistic . The assumptions in prop 1 seem too strong to me . I \u2019 d remove the mentions of regions and I \u2019 d instead mention the other results ( prop 2 , 3 , 4 ) in the main paper , Section 4.1 . You could just move the propositions , and give some intuition for why the results are true . Removing regions should also considerably simplify the notation and setup . Response : Thanks for the comments , we will update the text to make it clear . Q2 : I \u2019 m not quite sure what you mean in the intro when you say \u201c Eq . ( 1 ) doesn \u2019 t explicitly reflect the relation between \u2026 and the underlying data distribution p ( x , y ) \u201d . The definition in Equation ( 1 ) uses p ( x , y ) . I \u2019 m not sure why all the definitions in 3.1 and 3.3 are defined in a way different from the standard ways in the calibration literature e.g.in Kull et al 2019 or Kumar et al 2019 . Response : As shown by equation ( 14 ) , our definition of ECE is equivalent to ECE defined in Guo et al.2017 and TCE in Kumar et al.2019.Actually , the definitions in Guo et al.2017 and Kumar et al.2019 are using a sample-based definition . However , we are defining the ECE based on the true distribution p ( x , y ) , where the samples are drawn . We adopted this definition , based on the true distribution , as it allows more fundamental definitions of the attributes of calibration and ensemble calibrstion . We also show in Appendix A.3 Example 2 that with finite number of samples , the calibration error calculated based on the sample-based definition can be different from the calibration error obtained via the true distribution . = Q3 : Temperature scaling is performed on logits , not on the actual probabilities . From equations 24 , 25 , and 26 it looks like you might be doing temperature scaling on the probability space ( in equation 24 , 25 the first argument to f is the probability , not the logit ) , which looks a bit odd . Response : Thanks for the comment , the function f in Eq . ( 24 ) and ( 25 ) is actually referring to a general mapping from one probability to another probability . The function f in Eq . ( 26 ) is a special case of the general mapping . We have updated the text to make it clear . = Q4 : Prop 4 should also hold when K = 2 ( 2 ensemble members ) I believe . Happy to provide an example . Response : No , when K=2 , if all members are globally top-label calibrated , then the ensemble is 'always ' global top-label calibrated . Because when K=2 , top-label calibration is equivalent to all-label calibration . According to proposition 2 , we know the above holds . = Q5 : Some symbols are undefined . For example , \\delta ( y^ { ( i ) } , \\omega_j ) , I don \u2019 t believe \\delta is defined . I think it should be 1 if they are equal and 0 otherwise ? Response : Thanks for the comment , we have added the definition for \\delta function . = Q6 : \u2022 Ensembles are particularly useful because they tend to be more calibrated out of domain ( Lakshminarayanan et al 2017 ) . It could be useful to see which of these methods ( calibrating the members , or the entire ensemble ) is better calibrated when we have domain shift ( e.g.training data = CIFAR-10 , test data = CIFAR-10C , Hendrycks et al 2019 ) . \u2022 Having confidence intervals for the calibration errors would be nice ( and also using more modern , debiased estimators to estimate the calibration error ) e.g.in Kumar et al 2019 . All cites mentioned are already in the paper , except : Benchmarking Neural Network Robustness to Common Corruptions and Perturbations . Dan Hendrycks , Thomas Dietterich . ICLR 2019 . Response : Thanks for these constructive comments . We will improve our experiments accordingly and add the reference ."}, {"review_id": "wTWLfuDkvKp-1", "review_text": "- In general , my opinion is aligned with AnonReviewer1 the theory and the empirical contribution do not feel sufficient . - I also agree with AnonReviewer3 and AnonReviewer4 but feel less excited about the prons and more worried about the cons . At this point , I 'm not against the acceptance of the paper , although I 'm still staying on the rejection side . I 'm increasing my score because we are at least talking about a borderline . -- Summary : The paper study calibration of ensembles of DNNs and its relation to the calibration of individual members of ensembles . The work demonstrates that i ) members of an ensemble should not be calibrated , but the final ensemble may require calibration ( especially if members of an ensemble are calibrated ) ii ) provide theoretical results to support the statement iii ) propose an adaptive calibration scheme ( dynamic temperature scaling ) that uses different temperatures based on the confidence of a model . Concerns : 1 ) The main question of the paper `` Should ensemble members be calibrated ? '' feels trivial , because the community is aware of the simple example that provides an answer . The Deep Ensembles [ Lakshminarayanan2017 ] have miscalibrated membersconventional DNNs , but the predictions of an ensemble are , in-most-cases , calibrated . Thus the answer is `` No '' . 2 ) The paper mostly is clearly written , but section 4.1 `` Theoretical Analysis '' is * extremely * hard to follow . Even though I re-read it many times , I 'm still not sure if I understood it correctly . The most confusing part is the conclusion `` In practice , there is no constraint that the ensemble prediction should be calibrated , thus ensemble prediction calibration is required even for top-label calibrated members . `` . It seems that no listed results were used to produce this statement . 3 ) The calibration of ensemble has been proposed in [ Ashukha2020 , 5 Discussion & Conclusion ] . ( `` The resulting ensemble predictions ... , requiring calibration functions to be optimized for the ensemble prediction , rather than ensemble members . '' ) 4 ) The two main contributions ( 4.1 Theoretical Analysis , 4.2 Temperature Annealing for Ensemble Calibration ) feels not related , they are basically two independent topics packed in the one paper . 5 ) The empirical comparison exploits the calibrations score ( e.g. , ECE ) . ECE is a biased estimate of true calibration with a different bias for each model , so it is not a valid metric to compare different models ( see Vaicenavicius2019 ) . The fact is even mentioned in the current paper ( `` It should be noted that for finite number of samples ... `` ) but still is ignored in the empirical study . What I suggest is to use the squared kernel calibration error ( SKCE ) proposed in [ Widmann2019 ] along with de facto standard , but biased ECE . The SKCE is an unbiased estimate of calibration . There might be some pitfalls of this metric that I 'm not aware of , but the paper looks solid and convincing . Also , please put attention to Figure 83 in the ar\u0425iv version . Yes , ECE is the standard in the field , but it is the wrong standard that prevents us from meaningful scientific progress , so we should stop using it . 6 ) The results provided in Table 1 seem to be close values ( 0.6119 vs 0.6129 , etc . ) , so at least standard deviations need to be reported . Also , there is no mentioning of several runs per results in the text . The paper toches the nice topics but , overall it feels like `` ok , but not enough '' . The theory is interesting but it does not give us a lot of insides ( maybe it 's very subjective ) . The dynamic temperature scaling is not proofed to outperform the basslines . The contributions feel disconnected . The writing quality needs to be improved . Comments : 1 ) As far as I can tell , the citation `` The weights assigned to the probabilities are either optimized using AUC as in ( Ashukha et al. , 2020 ) ... '' is incorrect , as there is no mentioning of optimizing weights using AUC in the paper . 2 ) typo : It should be noted that for A finite number of sampleS [ Lakshminarayanan2017 ] Lakshminarayanan B , Pritzel A , Blundell C. Simple and scalable predictive uncertainty estimation using deep ensembles . In Advances in neural information processing systems 2017 ( pp.6402-6413 ) . [ Ashukha2020 ] Ashukha A , Lyzhov A , Molchanov D , Vetrov D. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning . ICLR , 2020 . [ Vaicenavicius2019 ] Juozas Vaicenavicius , David Widmann , Carl Andersson , Fredrik Lindsten , Jacob Roll , and Thomas B Schon . Evaluating model calibration in classification . AISTATS , 2019 . [ Widmann2019 ] Widmann D , Lindsten F , Zachariah D. Calibration tests in multi-class classification : A unifying framework . In Advances in Neural Information Processing Systems 2019 ( pp.12257-12267 ) . https : //arxiv.org/pdf/1910.11385.pdf", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q1 : The main question of the paper `` Should ensemble members be calibrated ? '' feels trivial , because the community is aware of the simple example that provides an answer . The Deep Ensembles [ Lakshminarayanan2017 ] have miscalibrated membersconventional DNNs , but the predictions of an ensemble are , in-most-cases , calibrated . Thus the answer is `` No '' . Response : We think the question is non-trivial as ensembles are a standard approach for both improving the performance of deep learning systems , and deriving confidence measures associated with these predictions . Well calibrated systems are important for bot appropriate system combination and uncertainty . The observation of mis-calibrated members lead to better calibrated ensemble doesn \u2019 t answer the question of \u201c what if the members are calibrated \u201d , does it lead to better calibrated ensemble ? How should we treat the members , calibrate them or make them worse calibrated , in order to achieve better calibrated ensemble ? This paper attempts to address these questions . Second , the deep ensemble predictions are not always well calibrated . This paper confirms this statement both empirically and theoretically . Third , the empirical observation of \u201c calibrating members leads to ensemble calibration degradation \u201d has been reported in several previous papers , however , to the authors knowledge this is the first time a theoretical analysis of this observation has been given . Fourth , we do not only focus on this observation , but also discuss the calibration of non-prediction probabilities . The analysis leads to the answer that is not simply \u201c No \u201d . Just to clarify this statement . The answer is \u201c No , \u201d when considering top-label calibration . The answer is \u201c Yes \u201d , if we are talking about all-label global calibration , when it is safe to say that \u201c calibrated members lead to calibrated ensemble \u201d ( Proposition 2 ) . We will try to be clearer in the text . Q2 : The paper mostly is clearly written , but section 4.1 `` Theoretical Analysis '' is extremely hard to follow . Response : We will update the text to make it clear ."}, {"review_id": "wTWLfuDkvKp-2", "review_text": "The paper makes an analysis of calibration in ensembles of deep learning models . Through some theoretical developments , the paper supports that a given ensemble can not be more confident than the average individual members for regions where the ensemble is well calibrated . Empirical results , on CIFAR-100 and three different deep models , report a comparison of ensemble calibration , where calibration is done over all members in order to achieved a calibrated ensemble decision , over individual calibration of members with no feedback from the ensemble decisions . Results show that individual member calibration does not lead to calibrated ensembles , and as such calibrating directly on the ensemble output is required for obtained a proper evaluation of its uncertainty . Different ensemble calibration approaches are also compared . Pros : - Overall well-written paper . - Straightforward proposal , simple yet meaningful on several aspects for better understanding of the link between calibration and ensembles . - Rely on theory to support some claims , which strengthen the proposal . Cons : - The proposal is somewhat trivial , although I do not have knowledge that it has been investigated in detail elsewhere . Before reading the paper , I expected the results ( i.e.calibration of individual members will not lead to calibrated ensemble decisions ; calibration at the ensemble level is required ) , the paper is somewhat confirming this in a more explicit manner . - Evaluation on only one dataset ( CIFAR-100 ) in the main paper , with another dataset for the appendix ( CIFAR-10 ) . - Results on CIFAR-10 in the appendix are not very compelling . - It is hard to make sense of the results in Table 1 and similar . Differences are small and difficult to interpret . - The explanations and organization of the paper are hard to following in some specific part . Although the paper is making a well-founded analysis of a hot topic in the last few years ( i.e. , ensembles are a way to evaluate uncertainty on decisions ) , I found it having some relatively trivial developments . And the conclusion is intuitive and expected . However , it is the first time I see this point well articulated , and the authors have made a good effort to develop theoretically backed explanations to support this .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Response : The empirical observation of \u201c ensemble of calibrated or uncalibrated members leads to calibration degradation \u201d has been reported in several previous papers , however , to the authors knowledge this is the first time that a theoretical analysis of this observation has been given . Note we do not only focus on this observation , where only the predictions from the members and ensemble are considered . The analysis also leads to interesting conclusions about all class calibration . Thank the reviewer for the valuable comments !"}, {"review_id": "wTWLfuDkvKp-3", "review_text": "- * * Summary * * : This paper investigates the impact of different calibration strategy ( pre-combination , post-combination and its dynamic variant ) on the performance of a deep ensemble . It presents both theoretical and empirical proof to show that well-calibrated ensemble member does guarantee calibration in the final ensemble . - * * Strength * * : * A coherent theoretical account for the issue of calibrating Deep ensembles . Accompanied by empirical evidence from CIFAR datasets . * Although not stated explicitly , a new calibration approach ( dynamic calibration ) is introduced , which empirically leads to better performance . - * * Weakness * * * Novelty may be limited : one central contribution of this paper is to provide a mathematical derivation to confirm the observation made in Rahaman and Thiery ( 2020 ) and Wen et al . ( 2020 ) .Although I appreciate author 's work on providing mathematical explanation for recent empirical findings , I 'm not sure if the submission in its current form is contributing significant novel theoretical insight beyond the fact that ensemble prediction is less confidence , since max of the mean probability is no greater than mean of the max probabilities . On the other hand , the empirical investigation is conducted on a single vision task ( CIFAR-10/-100 ) . This paper can be made stronger by investigating synthetic situation where the ground truth is known , or extend experiment to also other data modalities ( like Guo et al. ( 2017 ) ) . * Organization : Given the place of the new approach ( dynamic temperature scaling ) in the experiment , it might be worthwhile to devote some paragraph to introduce the procedure in more detail . - * * Recommendation * * : Based on reason stated in weakness , I recommend rejection since the either theoretical or the empirical contribution of this paper does not seem to be substantive enough for ICLR .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q1 : Novelty may be limited : one central contribution of this paper is to provide a mathematical derivation to confirm the observation made in Rahaman and Thiery ( 2020 ) and Wen et al . ( 2020 ) .Although I appreciate author 's work on providing mathematical explanation for recent empirical findings , I 'm not sure if the submission in its current form is contributing significant novel theoretical insight beyond the fact that ensemble prediction is less confidence , since max of the mean probability is no greater than mean of the max probabilities . On the other hand , the empirical investigation is conducted on a single vision task ( CIFAR-10/-100 ) . This paper can be made stronger by investigating synthetic situation where the ground truth is known , or extend experiment to also other data modalities ( like Guo et al . ( 2017 ) ) .Response : ( 1 ) In this work we are not only talking about the ensemble prediction , but examining the attributes of two type of calibration performance metrics : top-label ( prediction ) calibration ; and all-label ( prediction and non-prediction ) calibration . We show that for top-label global calibration , the calibrated ensemble will be less accurate than the mean accuracies of calibrated members , which implies that we should leave the members uncalibrated in order to achieve ensemble accuracy that is better than the mean accuracy of members . We also show that for all-label global calibration , then it is possible to say that \u201c calibrated members lead to global calibrated ensemble \u201d ( Proposition 2 ) . ( 2 ) \u201c This paper can be made stronger by investigating synthetic situation where the ground truth is known \u201d \u2014We actually have the experiments on synthetic dataset where ground truth confidence is known in the Appendix A.3 Example 2 . ( 3 ) Extended to other modalities . Thanks for the comment . Yes , it is desirable to verify our performance of dynamic temperature calibration on more modalities . We will finish this in our future work . The experiments are undergoing . == Q2 : Organization : Given the place of the new approach ( dynamic temperature scaling ) in the experiment , it might be worthwhile to devote some paragraph to introduce the procedure in more detail . Response : We have the equation for dynamic temperature scaling ( 26 ) . But due to the lack of space , we didn \u2019 t reveal much detail about dynamic temperature scaling . We have added the discussion of region numbers in Section 4.3 Figure 3 . == Thank the reviewer for the valuable comments !"}], "0": {"review_id": "wTWLfuDkvKp-0", "review_text": "Update after the author response : I 've read the other reviews , and agree with R2 and R3 . I think the paper is useful ( emphasizes you need to calibrate the final ensemble , not enough to calibrate members ) , and has some nice conceptual contributions ( explaining that if ensemble accuracy > average member accuracy ( which is usually the case ) , and the ensemble is calibrated even in just a global/weak sense , then the members must be uncalibrated ) . This could spur more research into conceptually analyzing ensembles , and seems interesting . But I understand the other reviewer 's concerns that it 's not clear what practical impact this will have , so I 'm keeping my score at a 6 ( instead of raising to a 7 ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper tackles the problem of calibrating an ensemble . They show experimentally that calibrating all members of an ensemble is often not enough to calibrate the combined ensemble , so instead we need to calibrate the final predictions of the ensemble . Additionally , they show that using a different temperature parameter for different regions of outputs can improve calibration . They explain why if the ensemble members are top-label calibrated ( even in a very weak sense they call \u201c global \u201d calibration \u201d ) , and the ensemble is calibrated , then the ensemble is less accurate than the average member of the ensemble . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : They make interesting observations about calibration of ensembles that could guide practitioners . For example , that it \u2019 s not enough to calibrate the members of the ensemble . They also raise an intriguing connection between calibration of ensemble members and ensemble accuracy , one would not expect a priori that if both are calibrated the ensemble would do worse than the average member . I could see this result being interesting to people who study ensembles as well . There are some weaknesses in writing and execution , but overall this paper is probably worth publishing if edited . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : - I think it \u2019 s a nice observation that calibrating the members of an ensemble may not yield a calibrated ensemble . It \u2019 s easy to come up with toy examples where this is the case , but it \u2019 s interesting that this seems to be the case in practice . - They make an intriguing observation that if the ensemble members are in fact calibrated and the ensemble is calibrated , then the ensemble accuracy is at most the average member accuracy # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : - I believe the writing can be substantially simplified . The core ideas are simple and nice , but it takes a lot of effort to get to them , and I believe the authors should put in more work into making this understandable . - Some of the results seem unrealistic and can be omitted . For example in the start of section 4.1 , the first couple of results require that the ensemble member regions and ensemble regions are the same . This seems rather unrealistic . The assumptions in prop 1 seem too strong to me . I \u2019 d remove the mentions of regions and I \u2019 d instead mention the other results ( prop 2 , 3 , 4 ) in the main paper , Section 4.1 . You could just move the propositions , and give some intuition for why the results are true . Removing regions should also considerably simplify the notation and setup . - I \u2019 m not quite sure what you mean in the intro when you say \u201c Eq . ( 1 ) doesn \u2019 t explicitly reflect the relation between \u2026 and the underlying data distribution p ( x , y ) \u201d . The definition in Equation ( 1 ) uses p ( x , y ) . I \u2019 m not sure why all the definitions in 3.1 and 3.3 are defined in a way different from the standard ways in the calibration literature e.g.in Kull et al 2019 or Kumar et al 2019 . - Temperature scaling is performed on logits , not on the actual probabilities . From equations 24 , 25 , and 26 it looks like you might be doing temperature scaling on the probability space ( in equation 24 , 25 the first argument to f is the probability , not the logit ) , which looks a bit odd . - Prop 4 should also hold when K = 2 ( 2 ensemble members ) I believe . Happy to provide an example . - Some symbols are undefined . For example , \\delta ( y^ { ( i ) } , \\omega_j ) , I don \u2019 t believe \\delta is defined . I think it should be 1 if they are equal and 0 otherwise ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions and things to improve : - Please answer the cons above . - Ensembles are particularly useful because they tend to be more calibrated out of domain ( Lakshminarayanan et al 2017 ) . It could be useful to see which of these methods ( calibrating the members , or the entire ensemble ) is better calibrated when we have domain shift ( e.g.training data = CIFAR-10 , test data = CIFAR-10C , Hendrycks et al 2019 ) . - Having confidence intervals for the calibration errors would be nice ( and also using more modern , debiased estimators to estimate the calibration error ) e.g.in Kumar et al 2019 . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # All cites mentioned are already in the paper , except : Benchmarking Neural Network Robustness to Common Corruptions and Perturbations . Dan Hendrycks , Thomas Dietterich . ICLR 2019 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q1 : \u2022 I believe the writing can be substantially simplified . The core ideas are simple and nice , but it takes a lot of effort to get to them , and I believe the authors should put in more work into making this understandable . \u2022 Some of the results seem unrealistic and can be omitted . For example in the start of section 4.1 , the first couple of results require that the ensemble member regions and ensemble regions are the same . This seems rather unrealistic . The assumptions in prop 1 seem too strong to me . I \u2019 d remove the mentions of regions and I \u2019 d instead mention the other results ( prop 2 , 3 , 4 ) in the main paper , Section 4.1 . You could just move the propositions , and give some intuition for why the results are true . Removing regions should also considerably simplify the notation and setup . Response : Thanks for the comments , we will update the text to make it clear . Q2 : I \u2019 m not quite sure what you mean in the intro when you say \u201c Eq . ( 1 ) doesn \u2019 t explicitly reflect the relation between \u2026 and the underlying data distribution p ( x , y ) \u201d . The definition in Equation ( 1 ) uses p ( x , y ) . I \u2019 m not sure why all the definitions in 3.1 and 3.3 are defined in a way different from the standard ways in the calibration literature e.g.in Kull et al 2019 or Kumar et al 2019 . Response : As shown by equation ( 14 ) , our definition of ECE is equivalent to ECE defined in Guo et al.2017 and TCE in Kumar et al.2019.Actually , the definitions in Guo et al.2017 and Kumar et al.2019 are using a sample-based definition . However , we are defining the ECE based on the true distribution p ( x , y ) , where the samples are drawn . We adopted this definition , based on the true distribution , as it allows more fundamental definitions of the attributes of calibration and ensemble calibrstion . We also show in Appendix A.3 Example 2 that with finite number of samples , the calibration error calculated based on the sample-based definition can be different from the calibration error obtained via the true distribution . = Q3 : Temperature scaling is performed on logits , not on the actual probabilities . From equations 24 , 25 , and 26 it looks like you might be doing temperature scaling on the probability space ( in equation 24 , 25 the first argument to f is the probability , not the logit ) , which looks a bit odd . Response : Thanks for the comment , the function f in Eq . ( 24 ) and ( 25 ) is actually referring to a general mapping from one probability to another probability . The function f in Eq . ( 26 ) is a special case of the general mapping . We have updated the text to make it clear . = Q4 : Prop 4 should also hold when K = 2 ( 2 ensemble members ) I believe . Happy to provide an example . Response : No , when K=2 , if all members are globally top-label calibrated , then the ensemble is 'always ' global top-label calibrated . Because when K=2 , top-label calibration is equivalent to all-label calibration . According to proposition 2 , we know the above holds . = Q5 : Some symbols are undefined . For example , \\delta ( y^ { ( i ) } , \\omega_j ) , I don \u2019 t believe \\delta is defined . I think it should be 1 if they are equal and 0 otherwise ? Response : Thanks for the comment , we have added the definition for \\delta function . = Q6 : \u2022 Ensembles are particularly useful because they tend to be more calibrated out of domain ( Lakshminarayanan et al 2017 ) . It could be useful to see which of these methods ( calibrating the members , or the entire ensemble ) is better calibrated when we have domain shift ( e.g.training data = CIFAR-10 , test data = CIFAR-10C , Hendrycks et al 2019 ) . \u2022 Having confidence intervals for the calibration errors would be nice ( and also using more modern , debiased estimators to estimate the calibration error ) e.g.in Kumar et al 2019 . All cites mentioned are already in the paper , except : Benchmarking Neural Network Robustness to Common Corruptions and Perturbations . Dan Hendrycks , Thomas Dietterich . ICLR 2019 . Response : Thanks for these constructive comments . We will improve our experiments accordingly and add the reference ."}, "1": {"review_id": "wTWLfuDkvKp-1", "review_text": "- In general , my opinion is aligned with AnonReviewer1 the theory and the empirical contribution do not feel sufficient . - I also agree with AnonReviewer3 and AnonReviewer4 but feel less excited about the prons and more worried about the cons . At this point , I 'm not against the acceptance of the paper , although I 'm still staying on the rejection side . I 'm increasing my score because we are at least talking about a borderline . -- Summary : The paper study calibration of ensembles of DNNs and its relation to the calibration of individual members of ensembles . The work demonstrates that i ) members of an ensemble should not be calibrated , but the final ensemble may require calibration ( especially if members of an ensemble are calibrated ) ii ) provide theoretical results to support the statement iii ) propose an adaptive calibration scheme ( dynamic temperature scaling ) that uses different temperatures based on the confidence of a model . Concerns : 1 ) The main question of the paper `` Should ensemble members be calibrated ? '' feels trivial , because the community is aware of the simple example that provides an answer . The Deep Ensembles [ Lakshminarayanan2017 ] have miscalibrated membersconventional DNNs , but the predictions of an ensemble are , in-most-cases , calibrated . Thus the answer is `` No '' . 2 ) The paper mostly is clearly written , but section 4.1 `` Theoretical Analysis '' is * extremely * hard to follow . Even though I re-read it many times , I 'm still not sure if I understood it correctly . The most confusing part is the conclusion `` In practice , there is no constraint that the ensemble prediction should be calibrated , thus ensemble prediction calibration is required even for top-label calibrated members . `` . It seems that no listed results were used to produce this statement . 3 ) The calibration of ensemble has been proposed in [ Ashukha2020 , 5 Discussion & Conclusion ] . ( `` The resulting ensemble predictions ... , requiring calibration functions to be optimized for the ensemble prediction , rather than ensemble members . '' ) 4 ) The two main contributions ( 4.1 Theoretical Analysis , 4.2 Temperature Annealing for Ensemble Calibration ) feels not related , they are basically two independent topics packed in the one paper . 5 ) The empirical comparison exploits the calibrations score ( e.g. , ECE ) . ECE is a biased estimate of true calibration with a different bias for each model , so it is not a valid metric to compare different models ( see Vaicenavicius2019 ) . The fact is even mentioned in the current paper ( `` It should be noted that for finite number of samples ... `` ) but still is ignored in the empirical study . What I suggest is to use the squared kernel calibration error ( SKCE ) proposed in [ Widmann2019 ] along with de facto standard , but biased ECE . The SKCE is an unbiased estimate of calibration . There might be some pitfalls of this metric that I 'm not aware of , but the paper looks solid and convincing . Also , please put attention to Figure 83 in the ar\u0425iv version . Yes , ECE is the standard in the field , but it is the wrong standard that prevents us from meaningful scientific progress , so we should stop using it . 6 ) The results provided in Table 1 seem to be close values ( 0.6119 vs 0.6129 , etc . ) , so at least standard deviations need to be reported . Also , there is no mentioning of several runs per results in the text . The paper toches the nice topics but , overall it feels like `` ok , but not enough '' . The theory is interesting but it does not give us a lot of insides ( maybe it 's very subjective ) . The dynamic temperature scaling is not proofed to outperform the basslines . The contributions feel disconnected . The writing quality needs to be improved . Comments : 1 ) As far as I can tell , the citation `` The weights assigned to the probabilities are either optimized using AUC as in ( Ashukha et al. , 2020 ) ... '' is incorrect , as there is no mentioning of optimizing weights using AUC in the paper . 2 ) typo : It should be noted that for A finite number of sampleS [ Lakshminarayanan2017 ] Lakshminarayanan B , Pritzel A , Blundell C. Simple and scalable predictive uncertainty estimation using deep ensembles . In Advances in neural information processing systems 2017 ( pp.6402-6413 ) . [ Ashukha2020 ] Ashukha A , Lyzhov A , Molchanov D , Vetrov D. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning . ICLR , 2020 . [ Vaicenavicius2019 ] Juozas Vaicenavicius , David Widmann , Carl Andersson , Fredrik Lindsten , Jacob Roll , and Thomas B Schon . Evaluating model calibration in classification . AISTATS , 2019 . [ Widmann2019 ] Widmann D , Lindsten F , Zachariah D. Calibration tests in multi-class classification : A unifying framework . In Advances in Neural Information Processing Systems 2019 ( pp.12257-12267 ) . https : //arxiv.org/pdf/1910.11385.pdf", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q1 : The main question of the paper `` Should ensemble members be calibrated ? '' feels trivial , because the community is aware of the simple example that provides an answer . The Deep Ensembles [ Lakshminarayanan2017 ] have miscalibrated membersconventional DNNs , but the predictions of an ensemble are , in-most-cases , calibrated . Thus the answer is `` No '' . Response : We think the question is non-trivial as ensembles are a standard approach for both improving the performance of deep learning systems , and deriving confidence measures associated with these predictions . Well calibrated systems are important for bot appropriate system combination and uncertainty . The observation of mis-calibrated members lead to better calibrated ensemble doesn \u2019 t answer the question of \u201c what if the members are calibrated \u201d , does it lead to better calibrated ensemble ? How should we treat the members , calibrate them or make them worse calibrated , in order to achieve better calibrated ensemble ? This paper attempts to address these questions . Second , the deep ensemble predictions are not always well calibrated . This paper confirms this statement both empirically and theoretically . Third , the empirical observation of \u201c calibrating members leads to ensemble calibration degradation \u201d has been reported in several previous papers , however , to the authors knowledge this is the first time a theoretical analysis of this observation has been given . Fourth , we do not only focus on this observation , but also discuss the calibration of non-prediction probabilities . The analysis leads to the answer that is not simply \u201c No \u201d . Just to clarify this statement . The answer is \u201c No , \u201d when considering top-label calibration . The answer is \u201c Yes \u201d , if we are talking about all-label global calibration , when it is safe to say that \u201c calibrated members lead to calibrated ensemble \u201d ( Proposition 2 ) . We will try to be clearer in the text . Q2 : The paper mostly is clearly written , but section 4.1 `` Theoretical Analysis '' is extremely hard to follow . Response : We will update the text to make it clear ."}, "2": {"review_id": "wTWLfuDkvKp-2", "review_text": "The paper makes an analysis of calibration in ensembles of deep learning models . Through some theoretical developments , the paper supports that a given ensemble can not be more confident than the average individual members for regions where the ensemble is well calibrated . Empirical results , on CIFAR-100 and three different deep models , report a comparison of ensemble calibration , where calibration is done over all members in order to achieved a calibrated ensemble decision , over individual calibration of members with no feedback from the ensemble decisions . Results show that individual member calibration does not lead to calibrated ensembles , and as such calibrating directly on the ensemble output is required for obtained a proper evaluation of its uncertainty . Different ensemble calibration approaches are also compared . Pros : - Overall well-written paper . - Straightforward proposal , simple yet meaningful on several aspects for better understanding of the link between calibration and ensembles . - Rely on theory to support some claims , which strengthen the proposal . Cons : - The proposal is somewhat trivial , although I do not have knowledge that it has been investigated in detail elsewhere . Before reading the paper , I expected the results ( i.e.calibration of individual members will not lead to calibrated ensemble decisions ; calibration at the ensemble level is required ) , the paper is somewhat confirming this in a more explicit manner . - Evaluation on only one dataset ( CIFAR-100 ) in the main paper , with another dataset for the appendix ( CIFAR-10 ) . - Results on CIFAR-10 in the appendix are not very compelling . - It is hard to make sense of the results in Table 1 and similar . Differences are small and difficult to interpret . - The explanations and organization of the paper are hard to following in some specific part . Although the paper is making a well-founded analysis of a hot topic in the last few years ( i.e. , ensembles are a way to evaluate uncertainty on decisions ) , I found it having some relatively trivial developments . And the conclusion is intuitive and expected . However , it is the first time I see this point well articulated , and the authors have made a good effort to develop theoretically backed explanations to support this .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Response : The empirical observation of \u201c ensemble of calibrated or uncalibrated members leads to calibration degradation \u201d has been reported in several previous papers , however , to the authors knowledge this is the first time that a theoretical analysis of this observation has been given . Note we do not only focus on this observation , where only the predictions from the members and ensemble are considered . The analysis also leads to interesting conclusions about all class calibration . Thank the reviewer for the valuable comments !"}, "3": {"review_id": "wTWLfuDkvKp-3", "review_text": "- * * Summary * * : This paper investigates the impact of different calibration strategy ( pre-combination , post-combination and its dynamic variant ) on the performance of a deep ensemble . It presents both theoretical and empirical proof to show that well-calibrated ensemble member does guarantee calibration in the final ensemble . - * * Strength * * : * A coherent theoretical account for the issue of calibrating Deep ensembles . Accompanied by empirical evidence from CIFAR datasets . * Although not stated explicitly , a new calibration approach ( dynamic calibration ) is introduced , which empirically leads to better performance . - * * Weakness * * * Novelty may be limited : one central contribution of this paper is to provide a mathematical derivation to confirm the observation made in Rahaman and Thiery ( 2020 ) and Wen et al . ( 2020 ) .Although I appreciate author 's work on providing mathematical explanation for recent empirical findings , I 'm not sure if the submission in its current form is contributing significant novel theoretical insight beyond the fact that ensemble prediction is less confidence , since max of the mean probability is no greater than mean of the max probabilities . On the other hand , the empirical investigation is conducted on a single vision task ( CIFAR-10/-100 ) . This paper can be made stronger by investigating synthetic situation where the ground truth is known , or extend experiment to also other data modalities ( like Guo et al. ( 2017 ) ) . * Organization : Given the place of the new approach ( dynamic temperature scaling ) in the experiment , it might be worthwhile to devote some paragraph to introduce the procedure in more detail . - * * Recommendation * * : Based on reason stated in weakness , I recommend rejection since the either theoretical or the empirical contribution of this paper does not seem to be substantive enough for ICLR .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q1 : Novelty may be limited : one central contribution of this paper is to provide a mathematical derivation to confirm the observation made in Rahaman and Thiery ( 2020 ) and Wen et al . ( 2020 ) .Although I appreciate author 's work on providing mathematical explanation for recent empirical findings , I 'm not sure if the submission in its current form is contributing significant novel theoretical insight beyond the fact that ensemble prediction is less confidence , since max of the mean probability is no greater than mean of the max probabilities . On the other hand , the empirical investigation is conducted on a single vision task ( CIFAR-10/-100 ) . This paper can be made stronger by investigating synthetic situation where the ground truth is known , or extend experiment to also other data modalities ( like Guo et al . ( 2017 ) ) .Response : ( 1 ) In this work we are not only talking about the ensemble prediction , but examining the attributes of two type of calibration performance metrics : top-label ( prediction ) calibration ; and all-label ( prediction and non-prediction ) calibration . We show that for top-label global calibration , the calibrated ensemble will be less accurate than the mean accuracies of calibrated members , which implies that we should leave the members uncalibrated in order to achieve ensemble accuracy that is better than the mean accuracy of members . We also show that for all-label global calibration , then it is possible to say that \u201c calibrated members lead to global calibrated ensemble \u201d ( Proposition 2 ) . ( 2 ) \u201c This paper can be made stronger by investigating synthetic situation where the ground truth is known \u201d \u2014We actually have the experiments on synthetic dataset where ground truth confidence is known in the Appendix A.3 Example 2 . ( 3 ) Extended to other modalities . Thanks for the comment . Yes , it is desirable to verify our performance of dynamic temperature calibration on more modalities . We will finish this in our future work . The experiments are undergoing . == Q2 : Organization : Given the place of the new approach ( dynamic temperature scaling ) in the experiment , it might be worthwhile to devote some paragraph to introduce the procedure in more detail . Response : We have the equation for dynamic temperature scaling ( 26 ) . But due to the lack of space , we didn \u2019 t reveal much detail about dynamic temperature scaling . We have added the discussion of region numbers in Section 4.3 Figure 3 . == Thank the reviewer for the valuable comments !"}}