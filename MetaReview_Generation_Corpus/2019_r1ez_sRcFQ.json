{"year": "2019", "forum": "r1ez_sRcFQ", "title": "Pixel Redrawn For A Robust Adversarial Defense", "decision": "Reject", "meta_review": "Based on the majority of reviewers with reject (ratings: 4,6,3), the current version of paper is proposed as reject. ", "reviews": [{"review_id": "r1ez_sRcFQ-0", "review_text": "This paper proposed the pixel redrawing approach to generate distorted training images to improve the performance of the deep networks, which hopefully can be used to prevent future attacks. The key idea is to randomly perturb the pixel values according pre-defined range and probabilities. The proposed method is quite simple and is similar to denoising autoencoders in flavor. My concern is that a pre-defined noisy perturbation may not be general enough to tackle various types of attacks. Ideally the perturbation should take into account properties of the input images, both pixel-wise and structure-wise. Unfortunately the proposed method ignores such information. The performance improvement seems quite limited judging from the results. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review and comments . Based on our knowledge , most state-of-the-art attack techniques usually are pixel-wise perturbation . Hence , we focus more on this issue and we came out with a distorted image generated by PR method . From Table 2 , the PR method can eliminate most adversarial pixels from the strongest attack technique , L2-CW attack in Case A and Case D ( newly added in Section 3.4 ) due to the mapping method ( newly added in Section 2.2 ) . From Table 3 ( newly added in Section 4.2 ) , the neural network that has trained with PR images increases the robustness of the neural network ."}, {"review_id": "r1ez_sRcFQ-1", "review_text": "The authors propose a defense technique to make the NN model more robust to adversarial events by redrawing the images and use them for training the model so that the model can prevent future attacks. The idea itself is simple but seems to be effective as shown in Tables 2 and 3. What I\u2019m missing here is a simple experiment to see the difference in accuracy performance between 1) when using PR for training the model (which is Case B in Table 2) against attacks versus 2) when not using PR for training the model against other attacks (similar to Table 2, Testing phase but using other attack models not PR as an attack model). It is not quite clear to me why using PR as a defense mechanism helps the NN model. I see its utility when training the NN model but using it as a defense mechanism is not quite clear why it works. Minor: It is not quite clear how the author chose the hyperparameters, maybe by changing those hyperparameter the attacker could have much clever ways to attack the NN model.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and comments . Following your comment , we have added the experiments in Section 4.2 . The experiment results are shown in Table 3 . From the results , the neural network that has trained with PR image increases the robustness of the neural network although the accuracy of legitimate data has slightly decreased compare with the normal neural network . During the training phase , PR method generates PR images ( distorted images ) as the training dataset to increase the robustness of the neural network . During the testing phase , however , PR method distorts the adversarial image by eliminating most adversarial pixels of the adversarial image to defend the adversarial attack ."}, {"review_id": "r1ez_sRcFQ-2", "review_text": "This paper presents a new adversarial defense method. I found the paper difficult to understand, but as far as I understand, the method involves randomly perturbing the pixels of images in a dataset, and retraining the classifier to correctly classify these perturbed images as well. The perturbations are done independently per pixel by quantizing the pixel value, and then using a perceptron model to generate the full pixel value from just this quantized value. The perceptron is trained on randomly generated data mapping quantized pixel values to full pixel values (this random generation does not use the dataset statistics at all). They use both partially converged perceptron models as well as fully converged perceptron models. Pros: 1. The defense technique does not require knowledge of the attack method Cons: 1. The paper is incredibly difficult to understand due to the writing. 2. The performed experiments are insufficient to determine whether or not their technique works because: a. They don't compare against any other defense techniques. In addition to at least adversarial training, I would like to also see comparisons to feature squeezing which I would expect to have a very similar effect. b. They do not show the results of an attack by an adversary that is aware of their technique. (i.e. F(PR(A(F,PR,x)))) Many alternative defense techniques will work much better if we assume the adversary does not know about the technique. c. Their comparison against random noise is not an apples-to-apples comparison. Instead of perturbing uniformly within a range, they perturb according to a normal distribution. The random noise perturbations also have a much larger L2 distance than the perturbations from their technique. To believe that their method is actually better than training with random noise I'd like to see an apples-to-apples comparison where these values are hyperparameter tuned as they presumably did for their method. d. They only show results for one value of epsilon and one value of \"# of colors\" for their technique. Presumably if there is a mismatch between these values then the results will be much worse (i.e. if they choose a large \"# of colors\"/small range per color and the attacker chooses a large epsilon). 3. Their use of machine learning models is quite ad-hoc. In particular they use a perceptron trained on algorithmically generated data. And their justification for using a perceptron, instead of just using the known underlying generation algorithm is that they also use a partially converged version of the perceptron as part of their model. 4. The authors partly deanonymize the paper through a github link 5. The main paper is 10 pages long, and the quality of the paper does not justify the additional length. ", "rating": "3: Clear rejection", "reply_text": "Thank you for your review and comments . 1.We have proof-read and added preliminaries ( Section 2.1 ) as well as the clear explanation of our method in the figure format ( Figure 1 ) in this latest version . We believe that the readers could understand our method easily . 2. a ) Following your comment , we have added the comparison with the adversarial training in Appendix section ( Section A.3 ) . The comparison results are shown in Table 11 . In Table 11 , our PR method outperforms the adversarial training which has applied with momentum iterative method ( MIM ) to generate the adversarial images as the training dataset . We would like to include feature squeezing next time . 2. b ) Following your comment , we have included F ( PR ( A ( F , PR , X ) ) ) as Case D in Section 3.4 . The experimental results are shown in Table 2 . We have also included the full version of the results with different number of epochs in Table 6 , Section A.1 . 2. c ) We are sorry for the mistake that we have made in the previous version of our manuscript . The distribution that we have used for the random noise injection was a uniform distribution . It is not from a normal distribution . 2. d ) Following your comments , we have operated some experiments with different \u201c # of colors \u201d which we have defined as k in the latest manuscript . In the previous manuscript , we used k=3 . In the latest manuscript , we include k=4 and k=10 which the results are shown in Table 7 and 8 respectively , in Section A.1 . 3.There are two reasons we use the perceptron in the paper . The perceptron can be used in parallel way during the training or testing phase . We can replace the perceptron with a similar convergence rate but with different weights ( and biases ) if the perceptron has been revealed by the attacker . In other words , the perceptron can be used as a \u201c key \u201d to protect the neural networks from the attacker by replacing a new perceptron . A paper with similar idea can be found in the following link . https : //openreview.net/forum ? id=HkElFj0qYQ & noteId=HJedRiS5hX 4 . Thanks for reminding us . We have changed the github name to a different name ."}], "0": {"review_id": "r1ez_sRcFQ-0", "review_text": "This paper proposed the pixel redrawing approach to generate distorted training images to improve the performance of the deep networks, which hopefully can be used to prevent future attacks. The key idea is to randomly perturb the pixel values according pre-defined range and probabilities. The proposed method is quite simple and is similar to denoising autoencoders in flavor. My concern is that a pre-defined noisy perturbation may not be general enough to tackle various types of attacks. Ideally the perturbation should take into account properties of the input images, both pixel-wise and structure-wise. Unfortunately the proposed method ignores such information. The performance improvement seems quite limited judging from the results. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review and comments . Based on our knowledge , most state-of-the-art attack techniques usually are pixel-wise perturbation . Hence , we focus more on this issue and we came out with a distorted image generated by PR method . From Table 2 , the PR method can eliminate most adversarial pixels from the strongest attack technique , L2-CW attack in Case A and Case D ( newly added in Section 3.4 ) due to the mapping method ( newly added in Section 2.2 ) . From Table 3 ( newly added in Section 4.2 ) , the neural network that has trained with PR images increases the robustness of the neural network ."}, "1": {"review_id": "r1ez_sRcFQ-1", "review_text": "The authors propose a defense technique to make the NN model more robust to adversarial events by redrawing the images and use them for training the model so that the model can prevent future attacks. The idea itself is simple but seems to be effective as shown in Tables 2 and 3. What I\u2019m missing here is a simple experiment to see the difference in accuracy performance between 1) when using PR for training the model (which is Case B in Table 2) against attacks versus 2) when not using PR for training the model against other attacks (similar to Table 2, Testing phase but using other attack models not PR as an attack model). It is not quite clear to me why using PR as a defense mechanism helps the NN model. I see its utility when training the NN model but using it as a defense mechanism is not quite clear why it works. Minor: It is not quite clear how the author chose the hyperparameters, maybe by changing those hyperparameter the attacker could have much clever ways to attack the NN model.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and comments . Following your comment , we have added the experiments in Section 4.2 . The experiment results are shown in Table 3 . From the results , the neural network that has trained with PR image increases the robustness of the neural network although the accuracy of legitimate data has slightly decreased compare with the normal neural network . During the training phase , PR method generates PR images ( distorted images ) as the training dataset to increase the robustness of the neural network . During the testing phase , however , PR method distorts the adversarial image by eliminating most adversarial pixels of the adversarial image to defend the adversarial attack ."}, "2": {"review_id": "r1ez_sRcFQ-2", "review_text": "This paper presents a new adversarial defense method. I found the paper difficult to understand, but as far as I understand, the method involves randomly perturbing the pixels of images in a dataset, and retraining the classifier to correctly classify these perturbed images as well. The perturbations are done independently per pixel by quantizing the pixel value, and then using a perceptron model to generate the full pixel value from just this quantized value. The perceptron is trained on randomly generated data mapping quantized pixel values to full pixel values (this random generation does not use the dataset statistics at all). They use both partially converged perceptron models as well as fully converged perceptron models. Pros: 1. The defense technique does not require knowledge of the attack method Cons: 1. The paper is incredibly difficult to understand due to the writing. 2. The performed experiments are insufficient to determine whether or not their technique works because: a. They don't compare against any other defense techniques. In addition to at least adversarial training, I would like to also see comparisons to feature squeezing which I would expect to have a very similar effect. b. They do not show the results of an attack by an adversary that is aware of their technique. (i.e. F(PR(A(F,PR,x)))) Many alternative defense techniques will work much better if we assume the adversary does not know about the technique. c. Their comparison against random noise is not an apples-to-apples comparison. Instead of perturbing uniformly within a range, they perturb according to a normal distribution. The random noise perturbations also have a much larger L2 distance than the perturbations from their technique. To believe that their method is actually better than training with random noise I'd like to see an apples-to-apples comparison where these values are hyperparameter tuned as they presumably did for their method. d. They only show results for one value of epsilon and one value of \"# of colors\" for their technique. Presumably if there is a mismatch between these values then the results will be much worse (i.e. if they choose a large \"# of colors\"/small range per color and the attacker chooses a large epsilon). 3. Their use of machine learning models is quite ad-hoc. In particular they use a perceptron trained on algorithmically generated data. And their justification for using a perceptron, instead of just using the known underlying generation algorithm is that they also use a partially converged version of the perceptron as part of their model. 4. The authors partly deanonymize the paper through a github link 5. The main paper is 10 pages long, and the quality of the paper does not justify the additional length. ", "rating": "3: Clear rejection", "reply_text": "Thank you for your review and comments . 1.We have proof-read and added preliminaries ( Section 2.1 ) as well as the clear explanation of our method in the figure format ( Figure 1 ) in this latest version . We believe that the readers could understand our method easily . 2. a ) Following your comment , we have added the comparison with the adversarial training in Appendix section ( Section A.3 ) . The comparison results are shown in Table 11 . In Table 11 , our PR method outperforms the adversarial training which has applied with momentum iterative method ( MIM ) to generate the adversarial images as the training dataset . We would like to include feature squeezing next time . 2. b ) Following your comment , we have included F ( PR ( A ( F , PR , X ) ) ) as Case D in Section 3.4 . The experimental results are shown in Table 2 . We have also included the full version of the results with different number of epochs in Table 6 , Section A.1 . 2. c ) We are sorry for the mistake that we have made in the previous version of our manuscript . The distribution that we have used for the random noise injection was a uniform distribution . It is not from a normal distribution . 2. d ) Following your comments , we have operated some experiments with different \u201c # of colors \u201d which we have defined as k in the latest manuscript . In the previous manuscript , we used k=3 . In the latest manuscript , we include k=4 and k=10 which the results are shown in Table 7 and 8 respectively , in Section A.1 . 3.There are two reasons we use the perceptron in the paper . The perceptron can be used in parallel way during the training or testing phase . We can replace the perceptron with a similar convergence rate but with different weights ( and biases ) if the perceptron has been revealed by the attacker . In other words , the perceptron can be used as a \u201c key \u201d to protect the neural networks from the attacker by replacing a new perceptron . A paper with similar idea can be found in the following link . https : //openreview.net/forum ? id=HkElFj0qYQ & noteId=HJedRiS5hX 4 . Thanks for reminding us . We have changed the github name to a different name ."}}