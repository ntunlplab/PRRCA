{"year": "2019", "forum": "HyM8V2A9Km", "title": "ACTRCE: Augmenting Experience via Teacher\u2019s Advice", "decision": "Reject", "meta_review": "This paper was reviewed by three experts (I assure the authors R3 is indeed familiar with RL and this area). Initially, the reviews were mixed with several concerns raised. After the author response, R2 and R3 recommend rejecting the paper, and R1 is unwilling to defend/champion/support it (not visible to the authors). The AC agrees with the concerns raised (in particular by R2) and finds no basis for overruling this recommendation. We encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future venue. ", "reviews": [{"review_id": "HyM8V2A9Km-0", "review_text": "Paper Summary: The idea of the paper is to improve Hindsight Experience Replay by providing natural language instructions as intermediate goals. Paper Strengths: Unfortunately, there is not many positive points about the paper except that it explores an interesting direction. Paper Weaknesses: I vote for rejection of the paper due to the following issues: - It is not clear how a description for a point along the way is provided (when the agent is not at a target). It is not clear how those feedback sentences are generated. That is the main claim of the paper and it is not clear at all. - The result of DQN is surprising (it is always zero). DQN is not that bad. Probably, there is a bug in the implementation. There should be comments on this in the rebuttal. - According to several recent works, algorithms like A3C work much better than DQN. Does the proposed method provide improvements over A3C as well? - The only measure that is reported is success rate. The episode length should be reported as well. I suggest using the SPL metric proposed by Anderson et al. in \"On Evaluation of Embodied Navigation Agents\". - Replacing one word with its synonym is considered as zero-shot. That is not really a zero-shot setting. Please refer to the following paper, which is missing in the related work: Interactive Grounded Language Acquisition and Generalization in a 2D World, ICLR 2018 - The environments are toy environments. The experiments should be carried out in more complex environments such as THOR or House3D that include more semantics. - What is the difference between this method and providing a large negative reward at a non-target object? - The paper discusses the advantages of word embeddings over one-hot vectors. That is obvious and not the goal of this paper. - It seems the same environment is used for train and test. ------------------------ Post rebuttal comments: Most of my concerns have been addressed. My new rating is 5. I like the idea of having a compact representation for the hindsight experience replay, but there are still a few issues: - I expected more complexity in vision and language. I do not agree with the rebuttal that AI2-THOR or House3D are not suitable. This level of complexity would be ok if this paper was among the first ones to explore this domain, but there are already several works. The zero-shot setting (changing the word with its synonym) is also so simplistic. - The proposed method uses much more annotations than the baselines so the comparisons are not really fair. This information should have been added to the baseline to see how this additional information changes the performance. Basically, it is not clear if the improvement should be attributed to the extra annotation or the way the advice is given. - The writing is still confusing. For instance, it is mentioned that \"Concretely, for each state s \u2208 S, we define T as a teacher that gives an advice T(s)\", while that is not true since later it is mentioned that \"the teacher give advice based solely on the terminal state\". These statements are contradictory, and it is not trivial at all to provide an advice for each state. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "> \u201c Replacing one word with its synonym is considered as zero-shot . That is not really a zero-shot setting . Please refer to the following paper , which is missing in the related work : Interactive Grounded Language Acquisition and Generalization in a 2D World , ICLR 2018 \u201d Thank you for bringing Yu et al.2018 \u2019 s work to our attention , we will cite their paper in the related work section . In their terminology , our zero-shot results is equivalent to their ZS1 ( \u201c interpolation \u201d to new combinations of previously seen words for the same use case ) . Our one word synonym experiment was applied to the ZS1 task ( i.e.the testing instructions ) as well as to training instructions . We feel that this is closer to the ZS2 in Yu et al.2018 \u2019 s work , as it is extrapolating to new words transferred from other use cases and models -- in this case , a pre-trained sentence embedding model . Our work differs in that we are only training the agent on the navigation task , while Yu et al.2018 has both navigation task and a question-answering task . Their ZS2 sentences contain a word that does not appear in the Navigation task training sentence but * does * appear in their Question-Answering training answer . > \u201c What is the difference between this method and providing a large negative reward at a non-target object ? \u201d Providing large negative reward at a non-target object can lead to undesired behaviour of avoiding reaching any of the objects , only hitting walls or wandering around until the episode terminates after a maximum number of timesteps . This is because the expected return of not hitting anything is zero , compared to when the agent randomly reaches one of the ( mostly likely ) incorrect object and receiving a large negative reward . In comparison , our method only rewards the agent when it reaches the target object for the instruction . References : [ 1 ] Chaplot et al. , \u201c Gated-Attention Architectures for Task-Oriented Language Grounding \u201d . https : //arxiv.org/abs/1706.07230 . ArXiv , 2017 [ 2 ] https : //github.com/devendrachaplot/DeepRL-Grounding [ 3 ] Zhu et al , \u201c Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning \u201d . https : //arxiv.org/abs/1609.05143 . ArXiv , 2018 [ 4 ] Yu el al. , \u201c Building Generalizable Agents with a realistic and rich 3D environment \u201d . https : //arxiv.org/abs/1801.02209 . ICLR , 2018 [ 5 ] Hermann et al. \u201c Grounded language learning in a simulated 3d world. \u201d https : //arxiv.org/abs/1706.06551 . ArXiv , 2017 [ 6 ] Misra et al .. Mapping instructions and visual observations to actions with reinforcement learning . EMNLP , 2017 [ 7 ] Das et al. , \u201c Embodied Question Answering \u201d . https : //arxiv.org/abs/1711.11543 . ArXiv , 2017"}, {"review_id": "HyM8V2A9Km-1", "review_text": "This paper considers the assumption implicit in hindsight experience replay (HER), namely that we have access to a mapping from states to goals. Rather than satisfying this requirement by defining goals as states, which involves great redundancy, the paper proposes a natural language goal representation. Concretely, for every state a teacher is used to provide a natural language description of the goal achieved in that state, which can be used to directly relabel the goal so the episode can be used as a positive experience. Strengths: - the proposed idea is simple and intuitively appealing, and shows much better results than the DQN baseline. Weaknesses: - In the VizDoom task, the goal specification is already in (templated) language. Given that this is the case, and the mapping from states to goals can be extracted from the environment anyway, it seems like the method that is applied really just reduces to a vanilla implementation of HER. There seems to be little novelty in this. From reading the introduction and method, I expected the ACTRCE approach to be applied to a task where the goal was not originally specified in language, perhaps by collecting language from human teachers. This would be a much more interesting experiment, addressing the question of whether human feedback in natural language can help the agent learn more quickly. - Even leaving aside the previous concern, it seems very difficult to put this work in the context of previous work on the same tasks. For example, it is not clear why there are no comparisons to the previous work on instruction following in VizDoom, as the setting appears to be exactly like Chaplot et al. 2017. It would seem like a natural comparison would be to take the model from Chaplot (leaving the task and architecture etc unchanged) and train it using ACTRCE. Is there any reason why this can\u2019t be done? There is already so much existing work in this space, it seems quite unusual that the proposed new method is not compared to any existing work on an existing task. Summary: This is a simple and intuitively appealing idea, but I find the evaluation to be quite lacking because the tasks already use a language specification (such that ACTRCE seems to be vanilla HER in application) and there are no comparisons to previous work. These two concerns seem quite substantial to me and make it difficult to recommend acceptance. Smaller issues: - ACTRCE - possibly the most tortured acronym in recent memory! How should it be pronounced?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Concern 2b : > \u201c I expected the ACTRCE approach to be applied to a task where the goal was not originally specified in language , perhaps by collecting language from human teachers . This would be a much more interesting experiment , addressing the question of whether human feedback in natural language can help the agent learn more quickly. \u201d We agree that applying the ACTRCE to a problem which the goal was not originally specified in language by collecting language from human teachers would be an interesting approach . In our work , we took a different approach where we converted a task originally specified in language and remove the language aspect , turning an instruction into one hot vector . In this setting , we were able to show that as we transitioned from single to composition task , using language representation helped the agent learn better than using a one-hot approach for each instruction Smaller concern : > '' ACTRCE - possibly the most tortured acronym in recent memory ! How should it be pronounced ? '' ACTRCE is pronounced as `` actress ''"}, {"review_id": "HyM8V2A9Km-2", "review_text": "This submission presents a method to improve the sample-efficiency of instruction-following models by leveraging the Hindsight Experience Replay framework with natural language goals. Here are my comments/questions: - The paper is well written and easy to follow, it introduces a simple idea which achieves very good results. - In addition to improving the performance as compared to the baselines, the authors perform a wide variety of experiments such as analysis of language representations, visualization of embeddings, etc. which lead several insightful results such as ability of sentence embeddings to generalize to unseen lexicon, ability of the model to perform well with just 1% advice. - It is important to note that as compared to the baselines, the proposed method requires access to the set of goals and extra information about which goal was reached in each episode. - In Table 1, how many frames were DQN and ACTRCE trained for? I am wondering why the MT performance for DQN is so low. Did the DQN have Gated-Attention? - The composition task is very interesting, did the agent receive intermediate rewards for completing a part of the instruction in this task? - Some implementation details questions: - In Appendix D Training details, what do you mean by 'chosen from the range {1000, 10000, 10000}'? - In Appendix D Training details, it is mentioned that you reproduce training using Asynchronous Advantage Actor Critic (A3C), where is A3C used in the experiments?", "rating": "7: Good paper, accept", "reply_text": "Thank you for taking the time to reviewing our paper and we appreciate the positive feedback . We will address your clarification questions below : > \u201c In Table 1 , how many frames were DQN and ACTRCE trained for ? I am wondering why the MT performance for DQN is so low . Did the DQN have Gated-Attention ? \u201d Both DQN and ACTRCE were trained with 40 million frames , and both had identical architecture which uses Gated-Attention as in Chaplot et al.2017 [ 1 ] .The difference with Chaplot et al \u2019 s set up is that we increased the number of objects from 5 to 7 , and we also increased the size of the room by 50 % . This made the reward in the environment even more sparse . We have tried experimenting on the easy and hard task using 5 objects ( similar to [ 1 ] ) , and found that in those cases , the baseline DQN was in fact able to learn ( see the next point below ) . > \u201c In Appendix D Training details , it is mentioned that you reproduce training using Asynchronous Advantage Actor Critic ( A3C ) , where is A3C used in the experiments ? \u201d Thank you for reviewer pointing this out . We had mistakenly left out this result in our initial submission . We had attempted to reproduce the A3C results from Chaplot et al.2017 [ 1 ] based on their available implementation online [ 2 ] for the single target task with 5 objects , but was not able to achieve the same performance in the hard mode as published , given our computation budget . + -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | | MT | ZSL | + -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | # of frame s | 8M | 16M | 150M | N/A | 16M | 150M | N/A | | A3C [ 1 ] | - | - | - | 0.83 | - | - | 0.73 | | A3C ( Reprod ) | 0.10 +/- 0.01 | 0.09 +/- 0.04 | 0.73 +/- 0.01 | - | - | 0.71 +/- 0.02| - | | DQN | 0.4 +/- 0.2 * | 0.73 +/- 0.08 | - | - | 0.75 +/- 0.05 | - | - | | ACTRCE ( Ours ) | * 0.69 +/- 0.04 * | * 0.83 +/- 0.02 * | - | - | * 0.77 +/- 0.02 * | - | - | + -- We will include these results for the 5 objects ( easy and hard mode ) in the next revision in Appendix G. > \u201c The composition task is very interesting , did the agent receive intermediate rewards for completing a part of the instruction in this task ? \u201d In the composition task , the reward is still sparse -- the agent only receives a reward of 1 if the agent reached both of the desired objects during the episode , in any order . A reward of 0 is given otherwise , i.e.when the agent reached only none/one object , or two objects that are not both desired ones . > \u201c In Appendix D Training details , what do you mean by 'chosen from the range { 1000 , 10000 , 10000 } ' ? \u201d The third number was a typo ( 100000 ) . We simply wanted to denote that we tried those 3 replay buffer sizes when performing hyperparameter tuning . > \u201c It is important to note that as compared to the baselines , the proposed method requires access to the set of goals and extra information about which goal was reached in each episode. \u201d We will note this in our revision of the paper ! [ 1 ] Chaplot et al. , \u201c Gated-Attention Architectures for Task-Oriented Language Grounding \u201d . https : //arxiv.org/abs/1706.07230 [ 2 ] https : //github.com/devendrachaplot/DeepRL-Grounding"}], "0": {"review_id": "HyM8V2A9Km-0", "review_text": "Paper Summary: The idea of the paper is to improve Hindsight Experience Replay by providing natural language instructions as intermediate goals. Paper Strengths: Unfortunately, there is not many positive points about the paper except that it explores an interesting direction. Paper Weaknesses: I vote for rejection of the paper due to the following issues: - It is not clear how a description for a point along the way is provided (when the agent is not at a target). It is not clear how those feedback sentences are generated. That is the main claim of the paper and it is not clear at all. - The result of DQN is surprising (it is always zero). DQN is not that bad. Probably, there is a bug in the implementation. There should be comments on this in the rebuttal. - According to several recent works, algorithms like A3C work much better than DQN. Does the proposed method provide improvements over A3C as well? - The only measure that is reported is success rate. The episode length should be reported as well. I suggest using the SPL metric proposed by Anderson et al. in \"On Evaluation of Embodied Navigation Agents\". - Replacing one word with its synonym is considered as zero-shot. That is not really a zero-shot setting. Please refer to the following paper, which is missing in the related work: Interactive Grounded Language Acquisition and Generalization in a 2D World, ICLR 2018 - The environments are toy environments. The experiments should be carried out in more complex environments such as THOR or House3D that include more semantics. - What is the difference between this method and providing a large negative reward at a non-target object? - The paper discusses the advantages of word embeddings over one-hot vectors. That is obvious and not the goal of this paper. - It seems the same environment is used for train and test. ------------------------ Post rebuttal comments: Most of my concerns have been addressed. My new rating is 5. I like the idea of having a compact representation for the hindsight experience replay, but there are still a few issues: - I expected more complexity in vision and language. I do not agree with the rebuttal that AI2-THOR or House3D are not suitable. This level of complexity would be ok if this paper was among the first ones to explore this domain, but there are already several works. The zero-shot setting (changing the word with its synonym) is also so simplistic. - The proposed method uses much more annotations than the baselines so the comparisons are not really fair. This information should have been added to the baseline to see how this additional information changes the performance. Basically, it is not clear if the improvement should be attributed to the extra annotation or the way the advice is given. - The writing is still confusing. For instance, it is mentioned that \"Concretely, for each state s \u2208 S, we define T as a teacher that gives an advice T(s)\", while that is not true since later it is mentioned that \"the teacher give advice based solely on the terminal state\". These statements are contradictory, and it is not trivial at all to provide an advice for each state. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "> \u201c Replacing one word with its synonym is considered as zero-shot . That is not really a zero-shot setting . Please refer to the following paper , which is missing in the related work : Interactive Grounded Language Acquisition and Generalization in a 2D World , ICLR 2018 \u201d Thank you for bringing Yu et al.2018 \u2019 s work to our attention , we will cite their paper in the related work section . In their terminology , our zero-shot results is equivalent to their ZS1 ( \u201c interpolation \u201d to new combinations of previously seen words for the same use case ) . Our one word synonym experiment was applied to the ZS1 task ( i.e.the testing instructions ) as well as to training instructions . We feel that this is closer to the ZS2 in Yu et al.2018 \u2019 s work , as it is extrapolating to new words transferred from other use cases and models -- in this case , a pre-trained sentence embedding model . Our work differs in that we are only training the agent on the navigation task , while Yu et al.2018 has both navigation task and a question-answering task . Their ZS2 sentences contain a word that does not appear in the Navigation task training sentence but * does * appear in their Question-Answering training answer . > \u201c What is the difference between this method and providing a large negative reward at a non-target object ? \u201d Providing large negative reward at a non-target object can lead to undesired behaviour of avoiding reaching any of the objects , only hitting walls or wandering around until the episode terminates after a maximum number of timesteps . This is because the expected return of not hitting anything is zero , compared to when the agent randomly reaches one of the ( mostly likely ) incorrect object and receiving a large negative reward . In comparison , our method only rewards the agent when it reaches the target object for the instruction . References : [ 1 ] Chaplot et al. , \u201c Gated-Attention Architectures for Task-Oriented Language Grounding \u201d . https : //arxiv.org/abs/1706.07230 . ArXiv , 2017 [ 2 ] https : //github.com/devendrachaplot/DeepRL-Grounding [ 3 ] Zhu et al , \u201c Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning \u201d . https : //arxiv.org/abs/1609.05143 . ArXiv , 2018 [ 4 ] Yu el al. , \u201c Building Generalizable Agents with a realistic and rich 3D environment \u201d . https : //arxiv.org/abs/1801.02209 . ICLR , 2018 [ 5 ] Hermann et al. \u201c Grounded language learning in a simulated 3d world. \u201d https : //arxiv.org/abs/1706.06551 . ArXiv , 2017 [ 6 ] Misra et al .. Mapping instructions and visual observations to actions with reinforcement learning . EMNLP , 2017 [ 7 ] Das et al. , \u201c Embodied Question Answering \u201d . https : //arxiv.org/abs/1711.11543 . ArXiv , 2017"}, "1": {"review_id": "HyM8V2A9Km-1", "review_text": "This paper considers the assumption implicit in hindsight experience replay (HER), namely that we have access to a mapping from states to goals. Rather than satisfying this requirement by defining goals as states, which involves great redundancy, the paper proposes a natural language goal representation. Concretely, for every state a teacher is used to provide a natural language description of the goal achieved in that state, which can be used to directly relabel the goal so the episode can be used as a positive experience. Strengths: - the proposed idea is simple and intuitively appealing, and shows much better results than the DQN baseline. Weaknesses: - In the VizDoom task, the goal specification is already in (templated) language. Given that this is the case, and the mapping from states to goals can be extracted from the environment anyway, it seems like the method that is applied really just reduces to a vanilla implementation of HER. There seems to be little novelty in this. From reading the introduction and method, I expected the ACTRCE approach to be applied to a task where the goal was not originally specified in language, perhaps by collecting language from human teachers. This would be a much more interesting experiment, addressing the question of whether human feedback in natural language can help the agent learn more quickly. - Even leaving aside the previous concern, it seems very difficult to put this work in the context of previous work on the same tasks. For example, it is not clear why there are no comparisons to the previous work on instruction following in VizDoom, as the setting appears to be exactly like Chaplot et al. 2017. It would seem like a natural comparison would be to take the model from Chaplot (leaving the task and architecture etc unchanged) and train it using ACTRCE. Is there any reason why this can\u2019t be done? There is already so much existing work in this space, it seems quite unusual that the proposed new method is not compared to any existing work on an existing task. Summary: This is a simple and intuitively appealing idea, but I find the evaluation to be quite lacking because the tasks already use a language specification (such that ACTRCE seems to be vanilla HER in application) and there are no comparisons to previous work. These two concerns seem quite substantial to me and make it difficult to recommend acceptance. Smaller issues: - ACTRCE - possibly the most tortured acronym in recent memory! How should it be pronounced?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Concern 2b : > \u201c I expected the ACTRCE approach to be applied to a task where the goal was not originally specified in language , perhaps by collecting language from human teachers . This would be a much more interesting experiment , addressing the question of whether human feedback in natural language can help the agent learn more quickly. \u201d We agree that applying the ACTRCE to a problem which the goal was not originally specified in language by collecting language from human teachers would be an interesting approach . In our work , we took a different approach where we converted a task originally specified in language and remove the language aspect , turning an instruction into one hot vector . In this setting , we were able to show that as we transitioned from single to composition task , using language representation helped the agent learn better than using a one-hot approach for each instruction Smaller concern : > '' ACTRCE - possibly the most tortured acronym in recent memory ! How should it be pronounced ? '' ACTRCE is pronounced as `` actress ''"}, "2": {"review_id": "HyM8V2A9Km-2", "review_text": "This submission presents a method to improve the sample-efficiency of instruction-following models by leveraging the Hindsight Experience Replay framework with natural language goals. Here are my comments/questions: - The paper is well written and easy to follow, it introduces a simple idea which achieves very good results. - In addition to improving the performance as compared to the baselines, the authors perform a wide variety of experiments such as analysis of language representations, visualization of embeddings, etc. which lead several insightful results such as ability of sentence embeddings to generalize to unseen lexicon, ability of the model to perform well with just 1% advice. - It is important to note that as compared to the baselines, the proposed method requires access to the set of goals and extra information about which goal was reached in each episode. - In Table 1, how many frames were DQN and ACTRCE trained for? I am wondering why the MT performance for DQN is so low. Did the DQN have Gated-Attention? - The composition task is very interesting, did the agent receive intermediate rewards for completing a part of the instruction in this task? - Some implementation details questions: - In Appendix D Training details, what do you mean by 'chosen from the range {1000, 10000, 10000}'? - In Appendix D Training details, it is mentioned that you reproduce training using Asynchronous Advantage Actor Critic (A3C), where is A3C used in the experiments?", "rating": "7: Good paper, accept", "reply_text": "Thank you for taking the time to reviewing our paper and we appreciate the positive feedback . We will address your clarification questions below : > \u201c In Table 1 , how many frames were DQN and ACTRCE trained for ? I am wondering why the MT performance for DQN is so low . Did the DQN have Gated-Attention ? \u201d Both DQN and ACTRCE were trained with 40 million frames , and both had identical architecture which uses Gated-Attention as in Chaplot et al.2017 [ 1 ] .The difference with Chaplot et al \u2019 s set up is that we increased the number of objects from 5 to 7 , and we also increased the size of the room by 50 % . This made the reward in the environment even more sparse . We have tried experimenting on the easy and hard task using 5 objects ( similar to [ 1 ] ) , and found that in those cases , the baseline DQN was in fact able to learn ( see the next point below ) . > \u201c In Appendix D Training details , it is mentioned that you reproduce training using Asynchronous Advantage Actor Critic ( A3C ) , where is A3C used in the experiments ? \u201d Thank you for reviewer pointing this out . We had mistakenly left out this result in our initial submission . We had attempted to reproduce the A3C results from Chaplot et al.2017 [ 1 ] based on their available implementation online [ 2 ] for the single target task with 5 objects , but was not able to achieve the same performance in the hard mode as published , given our computation budget . + -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | | MT | ZSL | + -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | # of frame s | 8M | 16M | 150M | N/A | 16M | 150M | N/A | | A3C [ 1 ] | - | - | - | 0.83 | - | - | 0.73 | | A3C ( Reprod ) | 0.10 +/- 0.01 | 0.09 +/- 0.04 | 0.73 +/- 0.01 | - | - | 0.71 +/- 0.02| - | | DQN | 0.4 +/- 0.2 * | 0.73 +/- 0.08 | - | - | 0.75 +/- 0.05 | - | - | | ACTRCE ( Ours ) | * 0.69 +/- 0.04 * | * 0.83 +/- 0.02 * | - | - | * 0.77 +/- 0.02 * | - | - | + -- We will include these results for the 5 objects ( easy and hard mode ) in the next revision in Appendix G. > \u201c The composition task is very interesting , did the agent receive intermediate rewards for completing a part of the instruction in this task ? \u201d In the composition task , the reward is still sparse -- the agent only receives a reward of 1 if the agent reached both of the desired objects during the episode , in any order . A reward of 0 is given otherwise , i.e.when the agent reached only none/one object , or two objects that are not both desired ones . > \u201c In Appendix D Training details , what do you mean by 'chosen from the range { 1000 , 10000 , 10000 } ' ? \u201d The third number was a typo ( 100000 ) . We simply wanted to denote that we tried those 3 replay buffer sizes when performing hyperparameter tuning . > \u201c It is important to note that as compared to the baselines , the proposed method requires access to the set of goals and extra information about which goal was reached in each episode. \u201d We will note this in our revision of the paper ! [ 1 ] Chaplot et al. , \u201c Gated-Attention Architectures for Task-Oriented Language Grounding \u201d . https : //arxiv.org/abs/1706.07230 [ 2 ] https : //github.com/devendrachaplot/DeepRL-Grounding"}}