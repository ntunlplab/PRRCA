{"year": "2020", "forum": "r1l7E1HFPH", "title": "Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning", "decision": "Reject", "meta_review": "This paper extends recent multi-step dynamic programming algorithms to reinforcement learning with function approximation.  In particular, the paper extends h-step optimal Bellman operators (and associated k-PI and k-VI algorithms) to deep reinforcement learning.  The paper describes new extensions to DQN and TRPO algorithms.  This approach is claimed to reduce the instability of model-free algorithms, and the approach is tested on Atari and Mujoco domains. \n\nThe reviewers noticed several limitations of the work.  The reviewers found little theoretical contribution in this work and they were unsatisfied with the empirical contributions.  The reviewers were unconvinced of the strength and clarity of the empirical results with the Atari and Mujoco domains along with the deep learning network architectures.  The reviewers suggested that simpler domains with a simpler function approximation scheme could enable more through experiments and more conclusive results.  The claim in the abstract of addressing the instabilities was also not adequately studied in the paper.\n\nThis paper is not ready for publication.  The primary contribution of this work is the empirical evaluation, and the evaluation is not sufficiently clear for the reviewers.", "reviews": [{"review_id": "r1l7E1HFPH-0", "review_text": "This paper focuses on the implementation and some empirical evaluations of a class of algorithms designed to find optimal strategies/values of large MDP. The basic idea of these algorithm (called \\kappa-PI or \\kappa-VI) is to combine two type of classical approaches: - policy/value iteration - k-step ahead computation (instead of just 1-ahead, and actually, k should be quite big or even infinite with an auxiliary appropriate discount rate). The theoretical formulation of \\kappa-PI and \\kappa-VI involves solving, at each iteration, another auxiliary MDP problem (where the discount rate is of order \\kappa\\gamma). This is basically what this paper suggests to do, and implements. The experiments are a bit difficult for me to read, as the baselines (\\kappa=0 and =1, say) are compared with \"the best \\kappa\" which seems to be problem dependent, so I do not know if there is a clear message.", "rating": "3: Weak Reject", "reply_text": "* * * * * Response to the Reviewer \u2019 s Comments * * * * * At the beginning , we would like to bring it to the reviewer \u2019 s attention that $ \\kappa $ is a parameter in the range [ 0,1 ] and can not go to infinity . $ \\kappa $ =0 corresponds to 1-step greedy and $ \\kappa $ =1 corresponds to solving the entire MDP . This fact makes the resulting algorithms easy to implement , unlike an approach that uses finite lookahead policies . We now provide a summary of our experiments and the lessons one can learn from them . Hope this helps the reviewer with reading the experiments , and clarifies the messages we would like to deliver in this work . The goal behind our experiments is to compare against the DQN and TRPO baselines , which are special cases of our algorithm by setting $ \\kappa $ =1 . The first takeaway message is that there are non-trivial $ \\kappa $ values for which we could observe better performance than DQN and TRPO . These $ \\kappa $ values are different for different environments . Here are some results revealed through our work : - We can categorize each environment with a certain range of \u2018 ideal \u2019 $ \\kappa $ values , e.g. , either lower or higher $ \\kappa $ values . - Our results also show that in TRPO , although previous work , such as GAE , concluded to have a fixed $ \\lambda $ parameter across all environments , this is certainly not true . A $ \\kappa $ or a $ \\lambda $ value that works well for one environment is not guaranteed to be working well for another . Therefore , the natural next step , that we are currently working on , is to build methods that can adapt the value of $ \\kappa $ based on the problem at hand . Secondly , since our methods have been derived from Policy/Value Iteration schemes , it makes sense to check how well they work when the policy evaluation and improvement steps are separated , i.e. , improving for multiple time steps before evaluating the policy . We do this through the \u2018 naive \u2019 baseline comparison which improves the policy for a single time-step . The results consistently show that doing a multi-step update is better . This is the second takeaway message from our work . Thirdly , one can also wonder what effect does lowering the discount factor have on the problem , since the $ \\kappa $ -PI algorithm advocates for solving a more discounted MDP ( i.e. , the $ \\gamma\\kappa $ MDP , instead of the $ \\gamma $ MDP , at each time step ) . Our results show that the comparison is non-trivial , as we achieve consistently better performance with $ \\kappa $ PI/VI , while lowering the discount factor actually hurts the baseline performance in most cases . This forms the third take-away of our work ."}, {"review_id": "r1l7E1HFPH-1", "review_text": "The main contributions of this paper are k-PI-DQN and k-VI-DQN, which are model-free versions of dynamic programming (DP) methods k-PI and k-VI from another paper (Efroni et al., 2018). The deep architecture of the two algorithms follows that of DQN. Efroni et al. (2018b) already gave a stochastic online (model-free) version of k-PI in the tabular setting. Although this paper is going one step further extending from tabular to function approximation, I feel that the paper just combined known results, the shaped reward from Efroni et al (2018a) and DQN. The extension seems straightforward. Mentioning previous results from Efroni et al (2018a) and (2018b) does not justify the extension would possess the same property or behaviour. The experiments were only comparing their methods with different hyperparameters, with only a brief comparison to DQN. ", "rating": "3: Weak Reject", "reply_text": "\u201c no mention of our TRPO work in the review \u201d The review only mentions our DQN work and does not talk about our TRPO algorithms and experiments . We are sorry if we did not present this part of our work clearly enough . We will improve the presentation of this part in the final version of the paper . To clarify , we experimented with both DQN and TRPO extensions of our approach . As stated in the paper , the TRPO extension resembles the practically used GAE ( Generalized Advantage Estimation ) algorithm , with a crucial difference that in GAE the value and policy are concurrently updated ( each policy improvement step is followed by a policy evaluation step ) , while in our work , we emphasize the need to do multiple step improvement before evaluating the policy . The theoretical results of Efroni et al . ( 2018b ) suggest that the concurrent update approach used by GAE does not necessarily result in an improving algorithm , which hints that using this approach might be problematic . We conjecture that the reason that this issue does not lead to significant performance deterioration in GAE is that most MuJoCo continuous control tasks are inherently of short horizon . In fact , our experiments show that in the Atari domains concurrently learning the policy and value leads to inferior performance ."}, {"review_id": "r1l7E1HFPH-2", "review_text": "===== Summary ===== The paper proposes an extension of multi-step dynamic programming algorithms from Efroni, Dalal, Scherrer, and Mannor (2018a, 2018b) to the reinforcement learning setting with function approximation. The multi-step dynamic programming algorithms proposed by Efroni et. al. (2018a) find the solution of the h-step optimal Bellman operator, which applies the maximum over the next h sequence of actions. Moreover, Efroni et. al. (2018a) also showed an equivalence between h-step optimal Bellman operators and k-Policy Iteration (k-PI) and k-Value Iteration (k-VI) algorithms, which, similar to TD( \ud835\udf06 ) but for policy improvement, take a geometric average of all future h-step returns weighted by k. The paper extends the work from Efroni et. al. (2018a, 2018b) to the deep reinforcement learning setting by proposing an approximate k-PI and k-VI algorithm based on DQN and TRPO. Finally, the paper provides empirical evaluations of k-PI and k-VI with DQN in several Atari games and of k-PI and k-VI with TRPO in several MuJoCo environments with continuous actions paces. Contributions: 1. The paper proposes a non-trivial extension for k-PI and k-VI to use function approximation via the DQN algorithm. 2. Similarly, the paper proposes a non-trivial extension for k-PI and k-VI to use function approximation with continuous action spaces via the TRPO algorithm. 3. The paper provides empirical evaluations of the four proposed algorithms and, at least for the k-PI algorithm with DQN and TRPO, demonstrates an improvement over the baselines. ===== Decision ===== The paper represents a natural next step to the work of Efroni et. al. (2018a, 2018b). The paper extends the applicability of multi-step greedy policies to more complex environments and shows a statistically significant improvement in performance compared to the methods that it builds upon. Additionally, the ideas are presented clearly and incrementally throughout the paper, which makes it flow nicely until the part where k-PI and k-VI DQN and TRPO are introduced. This is my main complaint about the paper, the lack of simple and intuitive understanding about k-PI and k-VI with function approximation due to the complicated architectures associated with DQN and TRPO. For this reason, my rating of the paper is weak accept. ===== Detailed Comments about Decision ===== All of these are comments for which I would consider increasing my score if they were addressed. === Empirical Evaluations === First, my main complaint is the complicated architectures and complex domains used to gain insights about k-PI and k-VI with function approximation. Big demonstrations in Atari and MuJoCo are important, but in the case of very new algorithms such as these ones, I consider it to be more important to gain insight through small domains that allow us to dig deep into the algorithms. Any small domain that would allow for big sample sizes for ablation and parameter studies would be more insightful than big demonstrations with very small sample sizes. I do not mean to be dismissive about what has been done in the paper, but it would be a great source of insight and a big improvement to what has already been done if a simple demonstration was presented in the paper. My suggestion would be to use a simple approximation method, such as Tile Coding with linear function approximation, in small a domain such as mountain car. This would allow for a bigger sample size and a parameter study that could provide more insight about the role of the parameters k and C_{FA} on the performance of k-PI and k-VI. Additionally, one of the claims in the conclusions was never emphasized in the results: \u201cimportantly, the performance of the algorithms was shown to be \u2018smooth\u2019 in the parameter k.\u201d This was not completely obvious until I spent some time looking closely at the graph. It eventually became clear, but I think a simpler way to emphasize this is to show a plot of the cumulative reward over the whole training period with the values of k on the x-axis. Based on the top right pane of FIgure 1, this type of plot would show a smooth increase from k=0.99 to k=0.68 followed by a smooth decrease from k=0.68 to k=0. Finally, I have some questions about some of the choices made in the experiments and results sections: 1. Why choose 50% confidence intervals? 50% confidence intervals with a sample size of 4 in the case of DQN and 5 in the case of TRPO is equivalent to multiplying the standard error by a factor of approximately 0.7, which is narrower than using the standard error on its own. Thus, it seems that some of the conclusions would change based on using a 95% confidence interval compared to a 50% confidence interval in Tables 1 and 2. I insist in showing the performance in a small domain with a simple form of function approximation. This would complement the Atari and MuJoCo experiments by showing improvements in performance with a higher confidence. 2. In remark one, it is pointed out that another target network \\tilde Q should be used to obtain \\pi_{t-1}, but this was not done to reduce the space complexity of the algorithm. How big were the networks that you used for k-PI DQN? If the network was not prohibitively big, why not implement \\tilde Q instead of using an alternative that further deviates from the original k-PI algorithm? 3. Line 19 of Algorithm 5 in Appendix A.1 is supposed to be the off-policy TD(0) update. However, it is not clear how this update is off-policy TD(0) since it based on Q and it does not have any importance sampling to correct for the difference in policies. Am I missing something? It seems that it should be off-policy Sarsa(0), but even then it would still be missing an importance sampling term (see Sutton & Barto, 2018, Equation 7.11, or Algorithm 1 of Precup, Sutton, and Singh, 2000, for more information). === Contradictory Claims in the Results === There are a few claims that contradict with what is shown in Table 1 and 2. In the last paragraph of Section 5.1.1 it says that \u201c[the table 1] show[s] that setting N(k) = T leads to a clear degradation of the final training performance on all the domains except Enduro.\u201d This is only true in two out of four games presented in Table 1. In Seaquest the lower confidence bound of the performance of k-PI with k=0.68 is 4643, whereas the upper confidence bound of the performance of k-PI with N(k) = T is 4837; the intervals clearly overlap. Similarly, in the game of Enduro, where k-PI with N(k) = T is said to have better performance, the lower confidence bound of k-PI with N(k) =T is 530, whereas for k-PI with k=0.84 the upper confidence bound is 575; again, the confidence intervals overlap. Hence, neither of these two claims are fully justified, and it is certainly not a \u201cclear degradation of the final training performance.\u201d Similarly, in Section 5.2.2, k-PI is said to have a better performance than N(k) = T based on the results of Table 2. However, similar calculations show that this is only true for the Ant domain. ===== Minor Comments ===== 1. I believe there is a typo in the last column of Table 1, it should be a \\kappa instead of a \ud835\udf06. 2. In the second paragraph above Equation 7, the convergence of PI and VI are said to converge to the optimal value with linear rate, but the rate of convergence is O( \\gamma^N ), i.e., exponential. Similarly, for the k-PI and k-VI their rate of convergence is O( \\ksi ( \\kappa )^{N( \\kappa )} ), which is also exponential. ===== References ===== Precup, Doina; Sutton, Richard S.; and Singh, Satinder, \"Eligibility Traces for Off-Policy Policy Evaluation\" (2000).ICML '00 Proceedings of the Seventeenth International Conference on Machine Learning. 80.Retrieved fromhttps://scholarworks.umass.edu/cs_faculty_pubs/80 R. Sutton and A. Barto. Reinforcement learning: An introduction. 2018. Y. Efroni, G. Dalal, B. Scherrer, and S. Mannor. Beyond the one step greedy approach in reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, 2018a. Y. Efroni, G. Dalal, B. Scherrer, and S. Mannor. Multiple-step greedy policies in approximate and online reinforcement learning. In Advances in Neural Information Processing Systems, pp. 5238\u20135247, 2018b. ", "rating": "6: Weak Accept", "reply_text": "We would like to thank the reviewer for the detailed review and useful comments . \u201c using simpler domains to better explain the algorithms \u201d We agree with the reviewer that it would be better to describe very new algorithms using small/simple domains . The reason that we did not initially include our experiments in simple domains is that the difference between the performance of the algorithms is not very clear in such problems . To address the reviewer \u2019 s concern , we have now added a new section to the paper ( Appendix C ) , where we report results on simpler environments , such as CartPole and Mountain Car . For the experiments on CartPole , we focus on the $ \\kappa $ -PI TRPO algorithm solely , since the results for the other versions ( $ \\kappa $ -VI TRPO , DQN , and $ \\kappa $ -PI DQN ) follow similarly . As pointed out by the reviewer , the purpose of this section is to gather a more intuitive understanding of the algorithms . Please refer to Appendix C in the updated paper for a more detailed discussion . \u201c a simpler way to emphasize this is to show a plot of the cumulative reward ... \u201d We have added a couple of bar plots to address this in Appendix C.4 . These correspond to the $ \\kappa $ -PI training plots for HalfCheetah and Ant domains . It is clear from the bar plots that the performance is smooth in $ \\kappa $ . \u201c 1.Why choose 50 % confidence intervals ? \u201d The results reported in Table 1 and 2 are the empirical mean $ \\pm $ the empirical standard deviation , which for the sample size of 4 or 5 runs is roughly equal to the 95 % confidence interval bound . Moreover , in the plots , we show results describing the empirical mean $ \\pm $ 0.5 * empirical standard deviation , which is , again for the sample size of 4 or 5 runs , roughly equal to a 60 % to 70 % confidence interval . This is done so that there is less overlap in the graphs and they are more readable . The 50 % value actually corresponds to these plots . We apologize for the lack of clarity here and have updated the paper with the correct confidence values . All conclusions are made with respect to the Table data eventually , which remains unchanged and still corresponds to the 95 % confidence bound . \u201c 2.How big were the networks that you used for k-PI DQN ? \u201d The DQN network sizes are the same as used in Mnih et al . [ 1 ] , i.e. , 3 convolutional layers followed by 2 fully connected layers . \u201c 3.Line 19 of Algorithm 5 in Appendix A.1 \u2026 \u201d The update in Line 9 resembles the expected SARSA update in Van Seijen et al . [ 2 ] .Also , this is the exact update as in DDPG ( Equation 5 in Lillicrap et al . [ 3 ] ) . \u201c Contradictory Claims in the Results \u201d Our claim is essentially saying that the mean values of the best performing $ \\kappa $ are consistently better than the mean values of the N_kappa = T baseline . Since the data here corresponds to the 95 % confidence interval bound for 4-5 sample runs , increasing the number of sample runs would decrease the width of the 95 % confidence interval , which essentially would ensure no overlap between the upper confidence limit of the baseline and the lower confidence limit of the best $ \\kappa $ value . For example , comparing the two versions for 10 sample runs in the Ant domain , results in the lower confidence limit of the best $ \\kappa $ value to be around 1230 , while the upper confidence limit of the baseline to be around 1180 , hence ensuring no overlap . Please note that due to the inherent variability in final training performance because of random seeding , the mean values for both cases , although relatively consistent , are also slightly changed . However , taking more samples always ensures that any x % confidence bound is narrowed . \u201c Typo in the last column of Table 1 \u201d Thank you for pointing this out . We have fixed this in the updated version . \u201c Linear convergence of PI and VI \u201d We apologize for the confusion here . Many of the works in the optimization literature refer to such an exponential rate as linear in the parameter N , and we borrowed the same definition . We have fixed this in the updated version . References : 1 . Mnih , Volodymyr , et al . `` Human-level control through deep reinforcement learning . '' Nature 518.7540 ( 2015 ) : 529 . 2.Van Seijen , Harm , et al . `` A theoretical and empirical analysis of Expected Sarsa . '' 2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning . IEEE , 2009 . 3.Lillicrap , Timothy P. , et al . `` Continuous control with deep reinforcement learning . '' arXiv preprint arXiv:1509.02971 ( 2015 ) ."}], "0": {"review_id": "r1l7E1HFPH-0", "review_text": "This paper focuses on the implementation and some empirical evaluations of a class of algorithms designed to find optimal strategies/values of large MDP. The basic idea of these algorithm (called \\kappa-PI or \\kappa-VI) is to combine two type of classical approaches: - policy/value iteration - k-step ahead computation (instead of just 1-ahead, and actually, k should be quite big or even infinite with an auxiliary appropriate discount rate). The theoretical formulation of \\kappa-PI and \\kappa-VI involves solving, at each iteration, another auxiliary MDP problem (where the discount rate is of order \\kappa\\gamma). This is basically what this paper suggests to do, and implements. The experiments are a bit difficult for me to read, as the baselines (\\kappa=0 and =1, say) are compared with \"the best \\kappa\" which seems to be problem dependent, so I do not know if there is a clear message.", "rating": "3: Weak Reject", "reply_text": "* * * * * Response to the Reviewer \u2019 s Comments * * * * * At the beginning , we would like to bring it to the reviewer \u2019 s attention that $ \\kappa $ is a parameter in the range [ 0,1 ] and can not go to infinity . $ \\kappa $ =0 corresponds to 1-step greedy and $ \\kappa $ =1 corresponds to solving the entire MDP . This fact makes the resulting algorithms easy to implement , unlike an approach that uses finite lookahead policies . We now provide a summary of our experiments and the lessons one can learn from them . Hope this helps the reviewer with reading the experiments , and clarifies the messages we would like to deliver in this work . The goal behind our experiments is to compare against the DQN and TRPO baselines , which are special cases of our algorithm by setting $ \\kappa $ =1 . The first takeaway message is that there are non-trivial $ \\kappa $ values for which we could observe better performance than DQN and TRPO . These $ \\kappa $ values are different for different environments . Here are some results revealed through our work : - We can categorize each environment with a certain range of \u2018 ideal \u2019 $ \\kappa $ values , e.g. , either lower or higher $ \\kappa $ values . - Our results also show that in TRPO , although previous work , such as GAE , concluded to have a fixed $ \\lambda $ parameter across all environments , this is certainly not true . A $ \\kappa $ or a $ \\lambda $ value that works well for one environment is not guaranteed to be working well for another . Therefore , the natural next step , that we are currently working on , is to build methods that can adapt the value of $ \\kappa $ based on the problem at hand . Secondly , since our methods have been derived from Policy/Value Iteration schemes , it makes sense to check how well they work when the policy evaluation and improvement steps are separated , i.e. , improving for multiple time steps before evaluating the policy . We do this through the \u2018 naive \u2019 baseline comparison which improves the policy for a single time-step . The results consistently show that doing a multi-step update is better . This is the second takeaway message from our work . Thirdly , one can also wonder what effect does lowering the discount factor have on the problem , since the $ \\kappa $ -PI algorithm advocates for solving a more discounted MDP ( i.e. , the $ \\gamma\\kappa $ MDP , instead of the $ \\gamma $ MDP , at each time step ) . Our results show that the comparison is non-trivial , as we achieve consistently better performance with $ \\kappa $ PI/VI , while lowering the discount factor actually hurts the baseline performance in most cases . This forms the third take-away of our work ."}, "1": {"review_id": "r1l7E1HFPH-1", "review_text": "The main contributions of this paper are k-PI-DQN and k-VI-DQN, which are model-free versions of dynamic programming (DP) methods k-PI and k-VI from another paper (Efroni et al., 2018). The deep architecture of the two algorithms follows that of DQN. Efroni et al. (2018b) already gave a stochastic online (model-free) version of k-PI in the tabular setting. Although this paper is going one step further extending from tabular to function approximation, I feel that the paper just combined known results, the shaped reward from Efroni et al (2018a) and DQN. The extension seems straightforward. Mentioning previous results from Efroni et al (2018a) and (2018b) does not justify the extension would possess the same property or behaviour. The experiments were only comparing their methods with different hyperparameters, with only a brief comparison to DQN. ", "rating": "3: Weak Reject", "reply_text": "\u201c no mention of our TRPO work in the review \u201d The review only mentions our DQN work and does not talk about our TRPO algorithms and experiments . We are sorry if we did not present this part of our work clearly enough . We will improve the presentation of this part in the final version of the paper . To clarify , we experimented with both DQN and TRPO extensions of our approach . As stated in the paper , the TRPO extension resembles the practically used GAE ( Generalized Advantage Estimation ) algorithm , with a crucial difference that in GAE the value and policy are concurrently updated ( each policy improvement step is followed by a policy evaluation step ) , while in our work , we emphasize the need to do multiple step improvement before evaluating the policy . The theoretical results of Efroni et al . ( 2018b ) suggest that the concurrent update approach used by GAE does not necessarily result in an improving algorithm , which hints that using this approach might be problematic . We conjecture that the reason that this issue does not lead to significant performance deterioration in GAE is that most MuJoCo continuous control tasks are inherently of short horizon . In fact , our experiments show that in the Atari domains concurrently learning the policy and value leads to inferior performance ."}, "2": {"review_id": "r1l7E1HFPH-2", "review_text": "===== Summary ===== The paper proposes an extension of multi-step dynamic programming algorithms from Efroni, Dalal, Scherrer, and Mannor (2018a, 2018b) to the reinforcement learning setting with function approximation. The multi-step dynamic programming algorithms proposed by Efroni et. al. (2018a) find the solution of the h-step optimal Bellman operator, which applies the maximum over the next h sequence of actions. Moreover, Efroni et. al. (2018a) also showed an equivalence between h-step optimal Bellman operators and k-Policy Iteration (k-PI) and k-Value Iteration (k-VI) algorithms, which, similar to TD( \ud835\udf06 ) but for policy improvement, take a geometric average of all future h-step returns weighted by k. The paper extends the work from Efroni et. al. (2018a, 2018b) to the deep reinforcement learning setting by proposing an approximate k-PI and k-VI algorithm based on DQN and TRPO. Finally, the paper provides empirical evaluations of k-PI and k-VI with DQN in several Atari games and of k-PI and k-VI with TRPO in several MuJoCo environments with continuous actions paces. Contributions: 1. The paper proposes a non-trivial extension for k-PI and k-VI to use function approximation via the DQN algorithm. 2. Similarly, the paper proposes a non-trivial extension for k-PI and k-VI to use function approximation with continuous action spaces via the TRPO algorithm. 3. The paper provides empirical evaluations of the four proposed algorithms and, at least for the k-PI algorithm with DQN and TRPO, demonstrates an improvement over the baselines. ===== Decision ===== The paper represents a natural next step to the work of Efroni et. al. (2018a, 2018b). The paper extends the applicability of multi-step greedy policies to more complex environments and shows a statistically significant improvement in performance compared to the methods that it builds upon. Additionally, the ideas are presented clearly and incrementally throughout the paper, which makes it flow nicely until the part where k-PI and k-VI DQN and TRPO are introduced. This is my main complaint about the paper, the lack of simple and intuitive understanding about k-PI and k-VI with function approximation due to the complicated architectures associated with DQN and TRPO. For this reason, my rating of the paper is weak accept. ===== Detailed Comments about Decision ===== All of these are comments for which I would consider increasing my score if they were addressed. === Empirical Evaluations === First, my main complaint is the complicated architectures and complex domains used to gain insights about k-PI and k-VI with function approximation. Big demonstrations in Atari and MuJoCo are important, but in the case of very new algorithms such as these ones, I consider it to be more important to gain insight through small domains that allow us to dig deep into the algorithms. Any small domain that would allow for big sample sizes for ablation and parameter studies would be more insightful than big demonstrations with very small sample sizes. I do not mean to be dismissive about what has been done in the paper, but it would be a great source of insight and a big improvement to what has already been done if a simple demonstration was presented in the paper. My suggestion would be to use a simple approximation method, such as Tile Coding with linear function approximation, in small a domain such as mountain car. This would allow for a bigger sample size and a parameter study that could provide more insight about the role of the parameters k and C_{FA} on the performance of k-PI and k-VI. Additionally, one of the claims in the conclusions was never emphasized in the results: \u201cimportantly, the performance of the algorithms was shown to be \u2018smooth\u2019 in the parameter k.\u201d This was not completely obvious until I spent some time looking closely at the graph. It eventually became clear, but I think a simpler way to emphasize this is to show a plot of the cumulative reward over the whole training period with the values of k on the x-axis. Based on the top right pane of FIgure 1, this type of plot would show a smooth increase from k=0.99 to k=0.68 followed by a smooth decrease from k=0.68 to k=0. Finally, I have some questions about some of the choices made in the experiments and results sections: 1. Why choose 50% confidence intervals? 50% confidence intervals with a sample size of 4 in the case of DQN and 5 in the case of TRPO is equivalent to multiplying the standard error by a factor of approximately 0.7, which is narrower than using the standard error on its own. Thus, it seems that some of the conclusions would change based on using a 95% confidence interval compared to a 50% confidence interval in Tables 1 and 2. I insist in showing the performance in a small domain with a simple form of function approximation. This would complement the Atari and MuJoCo experiments by showing improvements in performance with a higher confidence. 2. In remark one, it is pointed out that another target network \\tilde Q should be used to obtain \\pi_{t-1}, but this was not done to reduce the space complexity of the algorithm. How big were the networks that you used for k-PI DQN? If the network was not prohibitively big, why not implement \\tilde Q instead of using an alternative that further deviates from the original k-PI algorithm? 3. Line 19 of Algorithm 5 in Appendix A.1 is supposed to be the off-policy TD(0) update. However, it is not clear how this update is off-policy TD(0) since it based on Q and it does not have any importance sampling to correct for the difference in policies. Am I missing something? It seems that it should be off-policy Sarsa(0), but even then it would still be missing an importance sampling term (see Sutton & Barto, 2018, Equation 7.11, or Algorithm 1 of Precup, Sutton, and Singh, 2000, for more information). === Contradictory Claims in the Results === There are a few claims that contradict with what is shown in Table 1 and 2. In the last paragraph of Section 5.1.1 it says that \u201c[the table 1] show[s] that setting N(k) = T leads to a clear degradation of the final training performance on all the domains except Enduro.\u201d This is only true in two out of four games presented in Table 1. In Seaquest the lower confidence bound of the performance of k-PI with k=0.68 is 4643, whereas the upper confidence bound of the performance of k-PI with N(k) = T is 4837; the intervals clearly overlap. Similarly, in the game of Enduro, where k-PI with N(k) = T is said to have better performance, the lower confidence bound of k-PI with N(k) =T is 530, whereas for k-PI with k=0.84 the upper confidence bound is 575; again, the confidence intervals overlap. Hence, neither of these two claims are fully justified, and it is certainly not a \u201cclear degradation of the final training performance.\u201d Similarly, in Section 5.2.2, k-PI is said to have a better performance than N(k) = T based on the results of Table 2. However, similar calculations show that this is only true for the Ant domain. ===== Minor Comments ===== 1. I believe there is a typo in the last column of Table 1, it should be a \\kappa instead of a \ud835\udf06. 2. In the second paragraph above Equation 7, the convergence of PI and VI are said to converge to the optimal value with linear rate, but the rate of convergence is O( \\gamma^N ), i.e., exponential. Similarly, for the k-PI and k-VI their rate of convergence is O( \\ksi ( \\kappa )^{N( \\kappa )} ), which is also exponential. ===== References ===== Precup, Doina; Sutton, Richard S.; and Singh, Satinder, \"Eligibility Traces for Off-Policy Policy Evaluation\" (2000).ICML '00 Proceedings of the Seventeenth International Conference on Machine Learning. 80.Retrieved fromhttps://scholarworks.umass.edu/cs_faculty_pubs/80 R. Sutton and A. Barto. Reinforcement learning: An introduction. 2018. Y. Efroni, G. Dalal, B. Scherrer, and S. Mannor. Beyond the one step greedy approach in reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, 2018a. Y. Efroni, G. Dalal, B. Scherrer, and S. Mannor. Multiple-step greedy policies in approximate and online reinforcement learning. In Advances in Neural Information Processing Systems, pp. 5238\u20135247, 2018b. ", "rating": "6: Weak Accept", "reply_text": "We would like to thank the reviewer for the detailed review and useful comments . \u201c using simpler domains to better explain the algorithms \u201d We agree with the reviewer that it would be better to describe very new algorithms using small/simple domains . The reason that we did not initially include our experiments in simple domains is that the difference between the performance of the algorithms is not very clear in such problems . To address the reviewer \u2019 s concern , we have now added a new section to the paper ( Appendix C ) , where we report results on simpler environments , such as CartPole and Mountain Car . For the experiments on CartPole , we focus on the $ \\kappa $ -PI TRPO algorithm solely , since the results for the other versions ( $ \\kappa $ -VI TRPO , DQN , and $ \\kappa $ -PI DQN ) follow similarly . As pointed out by the reviewer , the purpose of this section is to gather a more intuitive understanding of the algorithms . Please refer to Appendix C in the updated paper for a more detailed discussion . \u201c a simpler way to emphasize this is to show a plot of the cumulative reward ... \u201d We have added a couple of bar plots to address this in Appendix C.4 . These correspond to the $ \\kappa $ -PI training plots for HalfCheetah and Ant domains . It is clear from the bar plots that the performance is smooth in $ \\kappa $ . \u201c 1.Why choose 50 % confidence intervals ? \u201d The results reported in Table 1 and 2 are the empirical mean $ \\pm $ the empirical standard deviation , which for the sample size of 4 or 5 runs is roughly equal to the 95 % confidence interval bound . Moreover , in the plots , we show results describing the empirical mean $ \\pm $ 0.5 * empirical standard deviation , which is , again for the sample size of 4 or 5 runs , roughly equal to a 60 % to 70 % confidence interval . This is done so that there is less overlap in the graphs and they are more readable . The 50 % value actually corresponds to these plots . We apologize for the lack of clarity here and have updated the paper with the correct confidence values . All conclusions are made with respect to the Table data eventually , which remains unchanged and still corresponds to the 95 % confidence bound . \u201c 2.How big were the networks that you used for k-PI DQN ? \u201d The DQN network sizes are the same as used in Mnih et al . [ 1 ] , i.e. , 3 convolutional layers followed by 2 fully connected layers . \u201c 3.Line 19 of Algorithm 5 in Appendix A.1 \u2026 \u201d The update in Line 9 resembles the expected SARSA update in Van Seijen et al . [ 2 ] .Also , this is the exact update as in DDPG ( Equation 5 in Lillicrap et al . [ 3 ] ) . \u201c Contradictory Claims in the Results \u201d Our claim is essentially saying that the mean values of the best performing $ \\kappa $ are consistently better than the mean values of the N_kappa = T baseline . Since the data here corresponds to the 95 % confidence interval bound for 4-5 sample runs , increasing the number of sample runs would decrease the width of the 95 % confidence interval , which essentially would ensure no overlap between the upper confidence limit of the baseline and the lower confidence limit of the best $ \\kappa $ value . For example , comparing the two versions for 10 sample runs in the Ant domain , results in the lower confidence limit of the best $ \\kappa $ value to be around 1230 , while the upper confidence limit of the baseline to be around 1180 , hence ensuring no overlap . Please note that due to the inherent variability in final training performance because of random seeding , the mean values for both cases , although relatively consistent , are also slightly changed . However , taking more samples always ensures that any x % confidence bound is narrowed . \u201c Typo in the last column of Table 1 \u201d Thank you for pointing this out . We have fixed this in the updated version . \u201c Linear convergence of PI and VI \u201d We apologize for the confusion here . Many of the works in the optimization literature refer to such an exponential rate as linear in the parameter N , and we borrowed the same definition . We have fixed this in the updated version . References : 1 . Mnih , Volodymyr , et al . `` Human-level control through deep reinforcement learning . '' Nature 518.7540 ( 2015 ) : 529 . 2.Van Seijen , Harm , et al . `` A theoretical and empirical analysis of Expected Sarsa . '' 2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning . IEEE , 2009 . 3.Lillicrap , Timothy P. , et al . `` Continuous control with deep reinforcement learning . '' arXiv preprint arXiv:1509.02971 ( 2015 ) ."}}