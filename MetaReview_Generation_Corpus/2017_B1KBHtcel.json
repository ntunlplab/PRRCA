{"year": "2017", "forum": "B1KBHtcel", "title": "Here's My Point: Argumentation Mining with Pointer Networks", "decision": "Reject", "meta_review": "The paper presents an interesting application of pointer networks to the argumentation mining task, and the reviewers found it generally solid. The reviewers generally agree (and I share their concerns) that the contribution on the machine learning side (i.e. the modification to PNs) is not sufficient to warrant publication at ICLR. Moreover, the task is not as standard and extensively studied in NLP; so strong results on the benchmark should not automatically warrant publication at ICLR (e.g., I would probably treat differently standard benchmarks in syntactic parsing of English or machine translation). If judged as an NLP paper, as pointed out by one of reviewers, the lack of qualitative evaluation / error analysis seems also problematic.", "reviews": [{"review_id": "B1KBHtcel-0", "review_text": "This paper addresses automated argumentation mining using pointer network. Although the task and the discussion is interesting, the contribution and the novelty is marginal because this is a single-task application of PN among many potential tasks.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Looking at Table 1 , I 'm worried it is not clear that our proposed joint model is called 'PN ' in the table . This is the model that achieves state-of-the-art . Conversely , 'PN No Type ' is a standard PN applied to this task , which achieves substantially worse results . We try to explain in Section 5 which model is which , but this might not be clear . I understand that as a reviewer , its your job to judge the overall contribution of the paper , but I want it to be clear what the results are : Pointer Network performs poorly compared to our proposed joint model ."}, {"review_id": "B1KBHtcel-1", "review_text": "This paper addresses the problem of argument mining, which consists of finding argument types and predicting the relationships between the arguments. The authors proposed a pointer network structure to recover the argument relations. They also propose modifications on pointer network to perform joint training on both type and link prediction tasks. Overall the model is reasonable, but I am not sure if ICLR is the best venue for this work. My first concern of the paper is on the novelty of the model. Pointer network has been proposed before. The proposed multi-task learning method is interesting, but the authors only verified it on one task. This makes me feel that maybe the submission is more for a NLP conference rather than ICLR. The authors stated that the pointer network is less restrictive compared to some of the existing tree predicting method. However, the datasets seem to only contain single trees or forests, and the stack-based method can be used for forest prediction by adding a virtual root node to each example (as done in the dependency parsing tasks). Therefore, I think the experiments right now cannot reflect the advantages of pointer network models unfortunately. My second concern of the paper is on the target task. Given that the authors want to analyze the structures between sentences, is the argumentation mining the best dataset? For example, authors could verify their model by applying it to the other tasks that require tree structures such as dependency parsing. As for NLP applications, I found that the assumption that the boundaries of AC are given is a very strong constraint, and could potentially limit the usefulness of the proposed model. Overall, in terms of ML, I also feel that baseline methods the authors compared to are probably strong for the argument mining task, but not necessary strong enough for the general tree/forest prediction tasks (as there are other tree/forest prediction methods). In terms of NLP applications, I think the assumption of having AC boundaries is too restrictive, and maybe ICLR is not the best venture for this submission. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for reviewing our paper . We have responded to specific points in the review below . \u201c Therefore , I think the experiments right now can not reflect the advantages of pointer network models unfortunately. \u201d The ILP Joint model uses 18 different hand-engineered features , often requiring external tools such as dependency parsing and POS tagging . Our model outperforms it with minimal feature extraction . The MP+p model assumes single-tree structure explicitly for links ( which is unique to the Microtext corpus ) . Our model outperforms it for link extraction without any explicit single-tree constraint . \u201c Pointer network has been proposed before. \u201d As is evident in Table 1 , a direct application of a pointer network does not achieve state-of-the-art . Nor does it accomplish the type prediction task . In fact , it performs substantially worse than the joint model on link prediction , which does achieve state-of-the-art on both tasks in the persuasive essay corpus , as well as link prediction in the Microtext corpus . \u201c The proposed multi-task learning method is interesting , but the authors only verified it on one task. \u201d Although we do focus on a single task , we test the model on two different datasets , each with its own characteristics . For example , the Microtext corpus has only 100 training examples , and our model is still able to achieve state-of-the-art on link prediction . Furthermore , the Microtext corpus is much more standardized ; all examples are trees and there is exactly one claim in each example . Therefore , even though we focus on a single task , the datasets we test on have varying characteristics , which highlights the generalizability of the model . \u201c My second concern of the paper is on the target task. \u201d One novel aspect of our work is the application of a PN-based model to discourse parsing . Our work is the first to use a sequence-to-sequence , attention-based model for discourse parsing . \u201c ... I found that the assumption that the boundaries of AC are given is a very strong constraint , and could potentially limit the usefulness of the proposed model. \u201d A model proposed by Stab and Gurevych , the authors of the persuasive essay corpus , achieves near human-level performance in identifying argument components : .867 f1 score compared to .886 f1 score . Alternatively , for the models they propose , the difference for link prediction is much larger : .751 f1 score compared to .854 f1 score . For type prediction , the gap is also larger : .826 f1 score compared to .868 f1 score ."}, {"review_id": "B1KBHtcel-2", "review_text": "This paper proposes a model for the task of argumentation mining (labeling the set of relationships between statements expressed as sentence-sized spans in a short text). The model combines a pointer network component that identifies links between statements and a classifier that predicts the roles of these statements. The resulting model works well: It outperforms strong baselines, even on datasets with fewer than 100 training examples. I don't see any major technical issues with this paper, and the results are strong. I am concerned, though, that the paper doesn't make a substantial novel contribution to representation learning. It focuses on ways to adapt reasonably mature techniques to a novel NLP problem. I think that one of the ACL conferences would be a better fit for this work. The choice of a pointer network for this problem seems reasonable, though (as noted by other commenters) the paper does not make any substantial comparison with other possible ways of producing trees. The paper does a solid job at breaking down the results quantitatively, but I would appreciate some examples of model output and some qualitative error analysis. Detail notes: - Figure 2 appears to have an error. You report that the decoder produces a distribution over input indices only, but you show an example of the network pointing to an output index in one case. - I don't think \"Wei12\" is a name.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for reviewing our paper . Below we have responded to specific points in the review . \u201c I am concerned , though , that the paper does n't make a substantial novel contribution to representation learning. \u201d Although the problem may appear to be a straightforward application of a PN , the results of Table 1 show that a PN alone does not achieve high performance . We extend the PN through joint modeling , as well as adding a FC-layer for LSTM input . As we note in our discussion , these modifications are crucial for achieving state-of-the-art results . The argument for the joint model is that the representations learned in the encoding phase are able to be shared between the dual tasks . For example , certain component types are likelier to have incoming/outgoing links , therefore information encoded for type prediction can also be useful for link prediction . Also , we address the fact that this problem can be solved by a standard recurrent model , as opposed to a sequence-to-sequence model . However , as our results show , while a BLSTM model can match previous state-of-the-art , the PN model performs measurably better . We argue that the better performance of the PN model is due to the separate representations learned during encoding/decoding . This allows components to be modeled separately as incoming/outgoing components . \u201c ... the paper does not make any substantial comparison with other possible ways of producing trees. \u201d We would like to reiterate that only in the Microtext corpus are all the examples trees ; in the persuasive essay corpus some examples are forests ( we will work to clarify this in the paper ) . With this in mind , for the Microtext corpus , we compare against a model , MP+p , that uses a minimum spanning tree parser . We apologize for not disambiguating the abbreviation \u2018 MSTParser \u2019 . \u201c Figure 2 appears to have an error . You report that the decoder produces a distribution over input indices only , but you show an example of the network pointing to an output index in one case. \u201d We chose to construct the figure in this way to illustrate that , by pointing to its own index , the model predicts no outgoing link for this components . By pointing to an input component in the Figure , this implies that the component has an outgoing link . We will clarify this in the paper ."}], "0": {"review_id": "B1KBHtcel-0", "review_text": "This paper addresses automated argumentation mining using pointer network. Although the task and the discussion is interesting, the contribution and the novelty is marginal because this is a single-task application of PN among many potential tasks.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Looking at Table 1 , I 'm worried it is not clear that our proposed joint model is called 'PN ' in the table . This is the model that achieves state-of-the-art . Conversely , 'PN No Type ' is a standard PN applied to this task , which achieves substantially worse results . We try to explain in Section 5 which model is which , but this might not be clear . I understand that as a reviewer , its your job to judge the overall contribution of the paper , but I want it to be clear what the results are : Pointer Network performs poorly compared to our proposed joint model ."}, "1": {"review_id": "B1KBHtcel-1", "review_text": "This paper addresses the problem of argument mining, which consists of finding argument types and predicting the relationships between the arguments. The authors proposed a pointer network structure to recover the argument relations. They also propose modifications on pointer network to perform joint training on both type and link prediction tasks. Overall the model is reasonable, but I am not sure if ICLR is the best venue for this work. My first concern of the paper is on the novelty of the model. Pointer network has been proposed before. The proposed multi-task learning method is interesting, but the authors only verified it on one task. This makes me feel that maybe the submission is more for a NLP conference rather than ICLR. The authors stated that the pointer network is less restrictive compared to some of the existing tree predicting method. However, the datasets seem to only contain single trees or forests, and the stack-based method can be used for forest prediction by adding a virtual root node to each example (as done in the dependency parsing tasks). Therefore, I think the experiments right now cannot reflect the advantages of pointer network models unfortunately. My second concern of the paper is on the target task. Given that the authors want to analyze the structures between sentences, is the argumentation mining the best dataset? For example, authors could verify their model by applying it to the other tasks that require tree structures such as dependency parsing. As for NLP applications, I found that the assumption that the boundaries of AC are given is a very strong constraint, and could potentially limit the usefulness of the proposed model. Overall, in terms of ML, I also feel that baseline methods the authors compared to are probably strong for the argument mining task, but not necessary strong enough for the general tree/forest prediction tasks (as there are other tree/forest prediction methods). In terms of NLP applications, I think the assumption of having AC boundaries is too restrictive, and maybe ICLR is not the best venture for this submission. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for reviewing our paper . We have responded to specific points in the review below . \u201c Therefore , I think the experiments right now can not reflect the advantages of pointer network models unfortunately. \u201d The ILP Joint model uses 18 different hand-engineered features , often requiring external tools such as dependency parsing and POS tagging . Our model outperforms it with minimal feature extraction . The MP+p model assumes single-tree structure explicitly for links ( which is unique to the Microtext corpus ) . Our model outperforms it for link extraction without any explicit single-tree constraint . \u201c Pointer network has been proposed before. \u201d As is evident in Table 1 , a direct application of a pointer network does not achieve state-of-the-art . Nor does it accomplish the type prediction task . In fact , it performs substantially worse than the joint model on link prediction , which does achieve state-of-the-art on both tasks in the persuasive essay corpus , as well as link prediction in the Microtext corpus . \u201c The proposed multi-task learning method is interesting , but the authors only verified it on one task. \u201d Although we do focus on a single task , we test the model on two different datasets , each with its own characteristics . For example , the Microtext corpus has only 100 training examples , and our model is still able to achieve state-of-the-art on link prediction . Furthermore , the Microtext corpus is much more standardized ; all examples are trees and there is exactly one claim in each example . Therefore , even though we focus on a single task , the datasets we test on have varying characteristics , which highlights the generalizability of the model . \u201c My second concern of the paper is on the target task. \u201d One novel aspect of our work is the application of a PN-based model to discourse parsing . Our work is the first to use a sequence-to-sequence , attention-based model for discourse parsing . \u201c ... I found that the assumption that the boundaries of AC are given is a very strong constraint , and could potentially limit the usefulness of the proposed model. \u201d A model proposed by Stab and Gurevych , the authors of the persuasive essay corpus , achieves near human-level performance in identifying argument components : .867 f1 score compared to .886 f1 score . Alternatively , for the models they propose , the difference for link prediction is much larger : .751 f1 score compared to .854 f1 score . For type prediction , the gap is also larger : .826 f1 score compared to .868 f1 score ."}, "2": {"review_id": "B1KBHtcel-2", "review_text": "This paper proposes a model for the task of argumentation mining (labeling the set of relationships between statements expressed as sentence-sized spans in a short text). The model combines a pointer network component that identifies links between statements and a classifier that predicts the roles of these statements. The resulting model works well: It outperforms strong baselines, even on datasets with fewer than 100 training examples. I don't see any major technical issues with this paper, and the results are strong. I am concerned, though, that the paper doesn't make a substantial novel contribution to representation learning. It focuses on ways to adapt reasonably mature techniques to a novel NLP problem. I think that one of the ACL conferences would be a better fit for this work. The choice of a pointer network for this problem seems reasonable, though (as noted by other commenters) the paper does not make any substantial comparison with other possible ways of producing trees. The paper does a solid job at breaking down the results quantitatively, but I would appreciate some examples of model output and some qualitative error analysis. Detail notes: - Figure 2 appears to have an error. You report that the decoder produces a distribution over input indices only, but you show an example of the network pointing to an output index in one case. - I don't think \"Wei12\" is a name.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for reviewing our paper . Below we have responded to specific points in the review . \u201c I am concerned , though , that the paper does n't make a substantial novel contribution to representation learning. \u201d Although the problem may appear to be a straightforward application of a PN , the results of Table 1 show that a PN alone does not achieve high performance . We extend the PN through joint modeling , as well as adding a FC-layer for LSTM input . As we note in our discussion , these modifications are crucial for achieving state-of-the-art results . The argument for the joint model is that the representations learned in the encoding phase are able to be shared between the dual tasks . For example , certain component types are likelier to have incoming/outgoing links , therefore information encoded for type prediction can also be useful for link prediction . Also , we address the fact that this problem can be solved by a standard recurrent model , as opposed to a sequence-to-sequence model . However , as our results show , while a BLSTM model can match previous state-of-the-art , the PN model performs measurably better . We argue that the better performance of the PN model is due to the separate representations learned during encoding/decoding . This allows components to be modeled separately as incoming/outgoing components . \u201c ... the paper does not make any substantial comparison with other possible ways of producing trees. \u201d We would like to reiterate that only in the Microtext corpus are all the examples trees ; in the persuasive essay corpus some examples are forests ( we will work to clarify this in the paper ) . With this in mind , for the Microtext corpus , we compare against a model , MP+p , that uses a minimum spanning tree parser . We apologize for not disambiguating the abbreviation \u2018 MSTParser \u2019 . \u201c Figure 2 appears to have an error . You report that the decoder produces a distribution over input indices only , but you show an example of the network pointing to an output index in one case. \u201d We chose to construct the figure in this way to illustrate that , by pointing to its own index , the model predicts no outgoing link for this components . By pointing to an input component in the Figure , this implies that the component has an outgoing link . We will clarify this in the paper ."}}