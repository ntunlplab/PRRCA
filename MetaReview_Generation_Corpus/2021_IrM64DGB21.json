{"year": "2021", "forum": "IrM64DGB21", "title": "On the role of planning in model-based deep reinforcement learning", "decision": "Accept (Poster)", "meta_review": "The reviewers appreciated the author replies, the additional experiments (more runs but also more ablations/baselines), and the updated paper. Also R2 is now largely satisfied (but seems to have been too late to post a public reply or to raise the score of the review).\n\nThe paper provides important insights in model-based RL and its connections to planning, by studying MuZero with systematic ablations. Hence a valuable contribution to the community. All (major) cons have been addressed in the revision.", "reviews": [{"review_id": "IrM64DGB21-0", "review_text": "Overview of the paper : This paper studies three empirically questions about the planning part in Muzero , an algorithm integrating direct learning , model-learning , and planning . These three questions , as written in the paper , are that 1 ) for what purposes is planning most useful ? 2 ) what design choices in the search procedure contribute most to the learning process 3 ) does planning assist in generalization across variations of the environment . The paper answers all these three questions using experiments : 1 ) the major reason for the performance improvement using planning is maintaining a policy that approximates the policy found by the MCTS algorithm , 2 ) simpler and shallower planning is often as performant as more complex planning , 3 ) search at evaluation time only slightly improves zero-shot generalization . Comments : Overall I think the paper is not ready to publish . While the 3 questions asked in the paper are quite general and apply to many model-based algorithms , the paper provided general answers to them using empirical results only for the Muzero algorithm under deterministic environments , which does n't look appropriate to me . The Muzero algorithm is different from many other planning algorithms , such as Dyna-style planning , MPC , value iteration , etc . And deterministic environments are easy cases compared with stochastic or even partial observable environments . I would suggest the authors rephrase the questions and answers to make them more specific so that the results in the paper can support conclusions well . While there are multiple things making me confused , I would like to highlight the following one as an example because that almost makes me think their answer to their first question is incorrect . The answer is drawn from the results shown in figure 3 , which illustrates how important three design choices ( following MCTS policy in both training and testing , following MCTS policy in training and prior policy in testing , and following prior policy in both training and testing ) are in planning . These results could tell us how much more performance the algorithm achieves by following MCTS policy in training or testing , but they can not tell us how much more performance the algorithm achieves by updating the prior policy towards the MCTS policy compared with other approaches . That is , in all cases , the algorithm updates its prior policy towards the MCTS policy . The other thing I feel not appropriate is , while the paper claimed that `` we systematically study the role of planning and its algorithmic design choices in a recent state-of-the-art MBRL algorithm , MuZero '' , the planning part is not the only part that varies across different design choices . In particular , because the model parameters are learnable and all the variations of algorithms they tested have different updates to model parameters . The resulting learned models are different in these variations . Thus it is not appropriate to conclude that the performance difference between different variations is solely the result of planning . It might also come from differences in learned models . Thanks for the author 's detailed response . In terms of the first question , I do appreciate the value of the paper as a nice empirical study of Muzero and other similar MBRL algorithms . Meanwhile , many MBRL algorithms are not like Muzero . For example , value-based planning algorithms do n't maintain an explicitly parameterized policy . Therefore the conclusions here may not apply to all cases . Making its conclusions more precisely will not undermine the value of the paper , instead , it provides readers clearer results . I do see in the revised version , the authors changed their language in the discussion about the result . But maybe clearer results themselves are better . My second concern is addressed in the updated version of the paper , with additional experiments . Cool ! In terms of the third question , after reading your response , I think there is a very interesting question . When we test planning algorithms , should we give the agent a fixed model and a fixed representation or fixed algorithms learning the model and the representation ? After thinking for a while , I can see the advantages and disadvantages of both cases . So I would change my mind and agree with the authors that their choice of testing is valid . But I do hope this choice being mentioned in the paper because people like me would typically consider the other one . I would like to raise my score to 5 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your time in reviewing our paper . We hope that we can convince you of the merits of our work ; we believe there may be some misunderstandings in what our paper aims to do , which we attempt to address here . We also note that the other three reviewers all found our results to be insightful and thorough , which we think is an indicator of their potential impact in the wider community . We would like to begin by clarifying that the point of our paper is not to explain why MuZero is better than other algorithms , but to understand how MuZero \u2019 s performance is affected when using planning in different ways and with different design choices . This type of analysis is important because while MuZero is state-of-the-art in MBRL in terms of final reward obtained , it is a very complex algorithm and the roles and interactions of its different components are not well understood . Our experiments allow us to better understand the role of planning in its design , which we think is important both for understanding MuZero itself and building intuition about the role of planning in MBRL more generally . We believe that our conclusionsfor example , that standard environments may not be really measuring the capacity of agents to \u201c reason \u201d are important takeaways that should inform future research in MBRL . Responding to your specific points : 1 . Regarding the generality of MuZero and the types of environments studied , please see our response to all reviewers . MuZero has important connections to all the algorithms you listed ( and in fact uses MPC as a subroutine ) , and the environments on which we evaluated are also very standard in the MBRL literature . However , we acknowledge that we haven \u2019 t directly tested all possibilities , and are therefore working to adjust the language in the paper to be more precise regarding our claims . 2.While it is true that all our experiments in Figure 3 use planning to update the policy , we disagree that this invalidates our claims . However , we do agree our claims could be strengthened by the addition of further baselines as suggested by R3 , which we are working on implementing . Please see our response to all reviewers ( \u201c Contributions of planning in computing policy updates \u201d ) . 3.MuZero is a complex , integrated agent with many working parts , and like many other MBRL algorithms ( MBPO , Dreamer , etc . ) , the model is learned online and therefore affected by any changes which affect learning . As we are interested in understanding the role of planning on the behavior of the whole integrated MBRL system ( i.e. , planning , learning , and the interactions between them ) , we believe it is appropriate to experiment on the planning procedure while keeping the rest of the agent untouched . It is not a problem if modifications to planning affect model learning , because the model is part of the whole system and we are interested in the whole system \u2019 s behavior . Consider that even some model-free algorithms have this property : for example , in actor-critic agents , changes to the behavior policy will affect learning of the critic which will affect performance . But this property does not invalidate hypotheses or claims about , for example , the effect of exploration on the overall agent \u2019 s performance . Similarly , just because changes to the planning procedure result in different models does not mean that we can not draw conclusions about the effect of those changes on overall behavior . Your comment makes it clear , however , that we were not clear enough in the paper that we are interested in the role of planning within integrated MBRL agents . We will clarify this and add some discussion about how the various components in agents like MuZero interact . We also think that your point about the effect of the model quality on agent performance suggests two very interesting follow-up questions , which we hope that future work will address by building on our paper ! First , it would be interesting to perform a separate analysis of how changes to the planning procedure affect the quality of the learned model . Second , it would also be interesting to understand the contributions of planning when given a pretrained , fixed model ( though it \u2019 s not immediately clear the best way to do this in MuZero , given that the policy , value , and dynamics share a common torso ) . By using a pretrained model , this would allow a deeper understanding of how the model quality influences planning performance . ( We do note that other recent work has already looked at this question to some extent ( e.g.Janner et al. , 2019 ) , and some of our generalization experiments touch on it as well ( Figure 5b , 5c , 6 ) ) . We are happy to clarify any remaining questions you may have !"}, {"review_id": "IrM64DGB21-1", "review_text": "The paper investigates how and why planning might be beneficial in model-based reinforcement learning settings . To that end , the authors ask three questions on planning in MBRL : ( 1 ) How does planning benefit MBRL agents ? ( 2 ) Within planning , what choices drive performance ? ( 3 ) To what extent does planning improve generalization ? In order to answer these questions , the authors investigate the performance of MuZero in a variety of learning challenges while systematically ablating the algorithm to find how each part of the algorithm effects the overall performance . In its current form the paper is marginally below the acceptance threshold . The reasoning for my judgement is as follows : The paper makes strong claims about how planning effects model-based RL algorithms based on their experimental results . However , the results are only based on five different seeds . Given the fundamental conclusions the authors formulate , the results are not statistically relevant enough . ( For further intuition about the impact of a small number of seeds in DRL , please refer to \u2018 Deep Reinforcement Learning that Matters \u2019 , Figure 5 by Henderson 2017 ) . The strong language in combination with the limited statistical relevance result in the experimental design not being sufficient . Having that said , the paper itself is well written and provides important insights to the community . I would strongly encourage the authors to either adjust the language or ( better ) run additional experiments to strengthen the paper . With more runs for each of the experiments , I would recommend a clear accept . Minor comment : At the beginning of the second paragraph , the authors state \u2018 Many have suggested that models will play a key role in generally intelligent artificial agents \u2019 . The supporting papers are essentially just two different author-sets , making this another instance of too strong of a statement with too little foundation . -- After reading the authors response to all reviewers , I believe all questions to be sufficiently addressed . I will therefore ( happily ) raise my score .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . We are very glad you found our paper to be clear and its insights to be important ! As discussed in our response to all reviewers ( see \u201c Strength of claims \u201d ) , we are working on adjusting the language in the text to be more precise regarding the claims being made . We agree that statistical power is an important requirement for making empirical claims , and that using 5 seeds to perform a t-test is insufficient , as in the example from Henderson et al . ( 2017 ) .However , we note that our conclusions are based mostly on Spearman rank correlations which pool across multiple parameter settings or on comparisons that pool across multiple environments . For example , the result that the \u201c Learn \u201d variant of MuZero achieves 69.4 % of vanilla MuZero is computed from 40 datapoints , and the individual rank correlations computed for each environment ( e.g.for D_tree , D_UCT ) use 25 datapoints . Please see Tables 11-18 in the Appendix for statistical tests , which we will also update to include sample sizes . Overall , we trained 660 unique agents and ran a further 1060 test-time experiments , which we believe is already quite extensive . Of course , it can never hurt to increase statistical power and as you have pointed out , doing so will strengthen our results . Thus , we are running 5 more seeds for each experiment ( for a total of 10 ) . We do not anticipate the results will change our overall findings , however , for the reasons described above . We will change \u201c Many \u201d to \u201c Some \u201d in the second paragraph , and will also add a few more references from other authors ."}, {"review_id": "IrM64DGB21-2", "review_text": "Summary : This paper tries to disentangle the role of planning in model-based reinforcement learning with a number of different ablations and modifications to MuZero . Specifically , the authors analyze the overall contribution of planning by omitting planning from which it is originally used in MuZero , and investigate different planner settings that can drive performance . In addition , they check the generalization advantage of MBRL . Overall , the paper is well-written , and experiments are conducted appropriately . The results provide some insights that other researchers in the MBRL community can leverage for their future work . My major concern is the lack of direct ablation study that can clearly show the advantage of planning in providing a good learning signal . See the detailed comments below . Comments : * To bolster the argument that planning contributes the most in providing a good learning signal , \u201c ( Data ) \u201d or \u201c ( Data+Eval ) \u201d ablation can be done : training update is done with targets generated with MCTS of $ D_ { \\text { UCT } } = D_ { \\text { tree } } = 1 $ while the agent uses full MCTS during training rollouts . Since the current experiments do not ablate planning in training , the effect of planning is only implicitly examined . * In the extreme , model-free version of \u201c ( Data ) \u201d could be implemented in which $ q $ is trained instead of $ v $ while $ v^ { \\text { MCTS } } $ and $ \\pi^ { \\text { MCTS } } $ is replaced with $ \\max q $ and $ \\arg\\max q $ . If this change is viable , it would also provide a chance to confirm the effect of planning for exploration only . * BFS is a too weak planner to compare with . How about random shooting methods ? Styles & Typo : * Use \u201c \\citet \u201d when appropriate ; when a citation is used as a subject in the sentence . * Line 3 of Algorithm 2 in Appendix : $ k=1 \\dots S $ \u2192 $ k=1 \\dots B $ ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thoughtful review ; we are glad that you liked the paper and that you found it insightful ! 1.That \u2019 s a great suggestion to perform updates with $ D_\\mathrm { tree } =D_\\mathrm { UCT } =1 $ while acting according to the full search . We have implemented this and are in the process of running experiments . Please also see our longer discussion of this in the response to all reviewers ( see \u201c Contributions of planning in computing policy updates \u201d ) . 2.The model-free version of \u201c ( Data ) \u201d implemented via Q-functions is also an interesting idea , and is quite similar to what was explored by Hamrick et al . ( 2020 ) \u201c Combining Q-learning and search with amortized value estimates \u201d . In their work , they found that this version was actually quite unstable ( see Figure 3 , \u201c SAVE w/o AL \u201d ) . We will add some discussion of this to the text . 3.We chose to compare to BFS not because of its strength or weakness as a planner but to understand how the model and value function behave when they are used differently than during training ( though we note that with large amounts of search , BFS is the strongest possible planner , as it amounts to exhaustive search ) . We actually find the BFS results to be some of the most interesting results in the paper , because they highlight the substantial mismatch between the actions recommended by the value function and the policy prior . For example , in Minipacman and Sokoban , the branching factor is 5 ; therefore 5 steps of BFS search means that all actions at the root node are expanded and the action will be selected as the one with the highest value . Yet , the performance of this agent is substantially worse than just relying on the policy prior . For sparse reward environments , any planner that relies on the value function without the policy prior will suffer from this effect , including random shooting . We will clarify the significance of this comparison in the text . Thank you for catching the typos and style issues , we will fix these in the text as well ."}, {"review_id": "IrM64DGB21-3", "review_text": "summary : This paper analyzes the role of planning in the model-based reinforcement learning agent , based on evaluating MuZero on eight tasks ( i.e.Ms.Pacman , Hero , Minipacman , Sokoban , 9x9Go , Acrobot , Cheetah , and Humanoid ) , which have discrete action spaces . The conducted experiments show three major implications : ( 1 ) Of the three parts in which search is used ( i.e.search at evaluation time , search at training time for exploration , and using search result as a policy target ) , the role of serving as a policy improvement target was most substantial . ( 2 ) Deep tree search did not make a significant contribution to performance , and a simple Monte-Carlo rollout could be performant enough for MBRL . Also , a too small or too large search budget can be harmful to the performance of the MBRL agent . ( 3 ) Search at evaluation time was helpful for zero-shot generalization especially when the model is accurate . pros : - The paper is well-written and well-organized . Hypotheses and the experiments are well-designed and seem thorough . cons : - The analyses are limited to MuZero that deals with discrete action space and to the deterministic environments . Since MuZero is a particular instance of MBRL , where only the reward and value prediction are performed in the latent-state space , it is unclear that the conclusions of the work can be applied to other instances of MBRL ( e.g.MBRL methods dealing with dynamics model that operates on the original state space ) . I am not fully convinced that the results here can be generalized to other classes of MBRL . some questions and comments : - It is interesting that the main benefit of the search is serving as a policy target . What are the advantages of policy improvement through search in MuZero compared to directly optimizing policy using analytic gradients in latent space like Dreamer ( Hafner et al , 2020 ) ? - In Figure 4c , why does the performance get worse as the number of simulations increases in Acrobot and Cheetah ? Or in Figure 6 ( Simulator , Green line ) , why the performance of ( # Simulations = 3125 ) is worse than ( # Simulations = 125 ) ? This may not be explained as compounding model errors since the agent is using the exact simulator . - It would be have been great to see the learning curve even for the results of Section 4.3 , similarly to Appendix D2 that presents the learning curve for the results of Section 4.2 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review ! We are glad you find the paper to be well-written and the experiments to be well-designed and thorough . With regards to your main question about applicability to MBRL more generally , please see our response to all reviewers . It \u2019 s a great question how our results might change when learning dynamics over the original state space ! In the course of developing our experiments for this paper , we actually performed a preliminary test of this question ( by adding a pixel reconstruction loss to MuZero ) but did not find that reconstructing observations made much of a difference . We are working on more formally re-running some of our experiments in Sokoban using the pixel reconstruction loss and will include the results in the Appendix . Regarding the comparison of search versus gradient-based planning ( i.e.in SVG-style methods like Dreamer ) , we agree that it would be very interesting to test this . We believe that this question highlights one of the strengths of our work , which is that it brings into focus a number of follow-up questions about how planning can best serve MBRL agents such as \u201c what types of planning lead to the most reliable policy and value targets ? \u201d . Additionally , please see our discussion of some further connections to SVG in our response to all reviewers ( see \u201c Generality of MuZero \u201d ) . In Figure 4c , the performance of some agents decreases with large search budgets due to compounding model errors . This is an instance of the same problem discussed by a number of other papers , such as MBPO ( Janner et al. , 2019 ) , and can be also seen in Figure 3b of the MuZero paper ( Schrittwieser et al , 2019 ) . The models learned for different environments will be of varying qualities , which is why we do not see this effect in all environments . If we were to continue increasing the search budget , we would likely eventually see a drop-off everywhere . In Figure 6 , the performance decreases even with the simulator due to off-policy errors in the value function , which get worse with larger searches . In fact , this is the same effect as what can be seen in Figure 5c ; the only reason it is less drastic than with BFS is because the policy prior used by MCTS keeps the search more on-policy . Note that Figure 5b does not demonstrate this effect because the training and testing distributions are the same ( though we expect with large enough search budgets , we would see decreases here too ) ; in contrast , in Figure 6 , the errors in the value function are more severe because the agent is being asked to generalize to out-of-distribution scenes . We will clarify these points in the text . We will update the Appendix to include learning curves for the results in Section 4.3 ."}], "0": {"review_id": "IrM64DGB21-0", "review_text": "Overview of the paper : This paper studies three empirically questions about the planning part in Muzero , an algorithm integrating direct learning , model-learning , and planning . These three questions , as written in the paper , are that 1 ) for what purposes is planning most useful ? 2 ) what design choices in the search procedure contribute most to the learning process 3 ) does planning assist in generalization across variations of the environment . The paper answers all these three questions using experiments : 1 ) the major reason for the performance improvement using planning is maintaining a policy that approximates the policy found by the MCTS algorithm , 2 ) simpler and shallower planning is often as performant as more complex planning , 3 ) search at evaluation time only slightly improves zero-shot generalization . Comments : Overall I think the paper is not ready to publish . While the 3 questions asked in the paper are quite general and apply to many model-based algorithms , the paper provided general answers to them using empirical results only for the Muzero algorithm under deterministic environments , which does n't look appropriate to me . The Muzero algorithm is different from many other planning algorithms , such as Dyna-style planning , MPC , value iteration , etc . And deterministic environments are easy cases compared with stochastic or even partial observable environments . I would suggest the authors rephrase the questions and answers to make them more specific so that the results in the paper can support conclusions well . While there are multiple things making me confused , I would like to highlight the following one as an example because that almost makes me think their answer to their first question is incorrect . The answer is drawn from the results shown in figure 3 , which illustrates how important three design choices ( following MCTS policy in both training and testing , following MCTS policy in training and prior policy in testing , and following prior policy in both training and testing ) are in planning . These results could tell us how much more performance the algorithm achieves by following MCTS policy in training or testing , but they can not tell us how much more performance the algorithm achieves by updating the prior policy towards the MCTS policy compared with other approaches . That is , in all cases , the algorithm updates its prior policy towards the MCTS policy . The other thing I feel not appropriate is , while the paper claimed that `` we systematically study the role of planning and its algorithmic design choices in a recent state-of-the-art MBRL algorithm , MuZero '' , the planning part is not the only part that varies across different design choices . In particular , because the model parameters are learnable and all the variations of algorithms they tested have different updates to model parameters . The resulting learned models are different in these variations . Thus it is not appropriate to conclude that the performance difference between different variations is solely the result of planning . It might also come from differences in learned models . Thanks for the author 's detailed response . In terms of the first question , I do appreciate the value of the paper as a nice empirical study of Muzero and other similar MBRL algorithms . Meanwhile , many MBRL algorithms are not like Muzero . For example , value-based planning algorithms do n't maintain an explicitly parameterized policy . Therefore the conclusions here may not apply to all cases . Making its conclusions more precisely will not undermine the value of the paper , instead , it provides readers clearer results . I do see in the revised version , the authors changed their language in the discussion about the result . But maybe clearer results themselves are better . My second concern is addressed in the updated version of the paper , with additional experiments . Cool ! In terms of the third question , after reading your response , I think there is a very interesting question . When we test planning algorithms , should we give the agent a fixed model and a fixed representation or fixed algorithms learning the model and the representation ? After thinking for a while , I can see the advantages and disadvantages of both cases . So I would change my mind and agree with the authors that their choice of testing is valid . But I do hope this choice being mentioned in the paper because people like me would typically consider the other one . I would like to raise my score to 5 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your time in reviewing our paper . We hope that we can convince you of the merits of our work ; we believe there may be some misunderstandings in what our paper aims to do , which we attempt to address here . We also note that the other three reviewers all found our results to be insightful and thorough , which we think is an indicator of their potential impact in the wider community . We would like to begin by clarifying that the point of our paper is not to explain why MuZero is better than other algorithms , but to understand how MuZero \u2019 s performance is affected when using planning in different ways and with different design choices . This type of analysis is important because while MuZero is state-of-the-art in MBRL in terms of final reward obtained , it is a very complex algorithm and the roles and interactions of its different components are not well understood . Our experiments allow us to better understand the role of planning in its design , which we think is important both for understanding MuZero itself and building intuition about the role of planning in MBRL more generally . We believe that our conclusionsfor example , that standard environments may not be really measuring the capacity of agents to \u201c reason \u201d are important takeaways that should inform future research in MBRL . Responding to your specific points : 1 . Regarding the generality of MuZero and the types of environments studied , please see our response to all reviewers . MuZero has important connections to all the algorithms you listed ( and in fact uses MPC as a subroutine ) , and the environments on which we evaluated are also very standard in the MBRL literature . However , we acknowledge that we haven \u2019 t directly tested all possibilities , and are therefore working to adjust the language in the paper to be more precise regarding our claims . 2.While it is true that all our experiments in Figure 3 use planning to update the policy , we disagree that this invalidates our claims . However , we do agree our claims could be strengthened by the addition of further baselines as suggested by R3 , which we are working on implementing . Please see our response to all reviewers ( \u201c Contributions of planning in computing policy updates \u201d ) . 3.MuZero is a complex , integrated agent with many working parts , and like many other MBRL algorithms ( MBPO , Dreamer , etc . ) , the model is learned online and therefore affected by any changes which affect learning . As we are interested in understanding the role of planning on the behavior of the whole integrated MBRL system ( i.e. , planning , learning , and the interactions between them ) , we believe it is appropriate to experiment on the planning procedure while keeping the rest of the agent untouched . It is not a problem if modifications to planning affect model learning , because the model is part of the whole system and we are interested in the whole system \u2019 s behavior . Consider that even some model-free algorithms have this property : for example , in actor-critic agents , changes to the behavior policy will affect learning of the critic which will affect performance . But this property does not invalidate hypotheses or claims about , for example , the effect of exploration on the overall agent \u2019 s performance . Similarly , just because changes to the planning procedure result in different models does not mean that we can not draw conclusions about the effect of those changes on overall behavior . Your comment makes it clear , however , that we were not clear enough in the paper that we are interested in the role of planning within integrated MBRL agents . We will clarify this and add some discussion about how the various components in agents like MuZero interact . We also think that your point about the effect of the model quality on agent performance suggests two very interesting follow-up questions , which we hope that future work will address by building on our paper ! First , it would be interesting to perform a separate analysis of how changes to the planning procedure affect the quality of the learned model . Second , it would also be interesting to understand the contributions of planning when given a pretrained , fixed model ( though it \u2019 s not immediately clear the best way to do this in MuZero , given that the policy , value , and dynamics share a common torso ) . By using a pretrained model , this would allow a deeper understanding of how the model quality influences planning performance . ( We do note that other recent work has already looked at this question to some extent ( e.g.Janner et al. , 2019 ) , and some of our generalization experiments touch on it as well ( Figure 5b , 5c , 6 ) ) . We are happy to clarify any remaining questions you may have !"}, "1": {"review_id": "IrM64DGB21-1", "review_text": "The paper investigates how and why planning might be beneficial in model-based reinforcement learning settings . To that end , the authors ask three questions on planning in MBRL : ( 1 ) How does planning benefit MBRL agents ? ( 2 ) Within planning , what choices drive performance ? ( 3 ) To what extent does planning improve generalization ? In order to answer these questions , the authors investigate the performance of MuZero in a variety of learning challenges while systematically ablating the algorithm to find how each part of the algorithm effects the overall performance . In its current form the paper is marginally below the acceptance threshold . The reasoning for my judgement is as follows : The paper makes strong claims about how planning effects model-based RL algorithms based on their experimental results . However , the results are only based on five different seeds . Given the fundamental conclusions the authors formulate , the results are not statistically relevant enough . ( For further intuition about the impact of a small number of seeds in DRL , please refer to \u2018 Deep Reinforcement Learning that Matters \u2019 , Figure 5 by Henderson 2017 ) . The strong language in combination with the limited statistical relevance result in the experimental design not being sufficient . Having that said , the paper itself is well written and provides important insights to the community . I would strongly encourage the authors to either adjust the language or ( better ) run additional experiments to strengthen the paper . With more runs for each of the experiments , I would recommend a clear accept . Minor comment : At the beginning of the second paragraph , the authors state \u2018 Many have suggested that models will play a key role in generally intelligent artificial agents \u2019 . The supporting papers are essentially just two different author-sets , making this another instance of too strong of a statement with too little foundation . -- After reading the authors response to all reviewers , I believe all questions to be sufficiently addressed . I will therefore ( happily ) raise my score .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . We are very glad you found our paper to be clear and its insights to be important ! As discussed in our response to all reviewers ( see \u201c Strength of claims \u201d ) , we are working on adjusting the language in the text to be more precise regarding the claims being made . We agree that statistical power is an important requirement for making empirical claims , and that using 5 seeds to perform a t-test is insufficient , as in the example from Henderson et al . ( 2017 ) .However , we note that our conclusions are based mostly on Spearman rank correlations which pool across multiple parameter settings or on comparisons that pool across multiple environments . For example , the result that the \u201c Learn \u201d variant of MuZero achieves 69.4 % of vanilla MuZero is computed from 40 datapoints , and the individual rank correlations computed for each environment ( e.g.for D_tree , D_UCT ) use 25 datapoints . Please see Tables 11-18 in the Appendix for statistical tests , which we will also update to include sample sizes . Overall , we trained 660 unique agents and ran a further 1060 test-time experiments , which we believe is already quite extensive . Of course , it can never hurt to increase statistical power and as you have pointed out , doing so will strengthen our results . Thus , we are running 5 more seeds for each experiment ( for a total of 10 ) . We do not anticipate the results will change our overall findings , however , for the reasons described above . We will change \u201c Many \u201d to \u201c Some \u201d in the second paragraph , and will also add a few more references from other authors ."}, "2": {"review_id": "IrM64DGB21-2", "review_text": "Summary : This paper tries to disentangle the role of planning in model-based reinforcement learning with a number of different ablations and modifications to MuZero . Specifically , the authors analyze the overall contribution of planning by omitting planning from which it is originally used in MuZero , and investigate different planner settings that can drive performance . In addition , they check the generalization advantage of MBRL . Overall , the paper is well-written , and experiments are conducted appropriately . The results provide some insights that other researchers in the MBRL community can leverage for their future work . My major concern is the lack of direct ablation study that can clearly show the advantage of planning in providing a good learning signal . See the detailed comments below . Comments : * To bolster the argument that planning contributes the most in providing a good learning signal , \u201c ( Data ) \u201d or \u201c ( Data+Eval ) \u201d ablation can be done : training update is done with targets generated with MCTS of $ D_ { \\text { UCT } } = D_ { \\text { tree } } = 1 $ while the agent uses full MCTS during training rollouts . Since the current experiments do not ablate planning in training , the effect of planning is only implicitly examined . * In the extreme , model-free version of \u201c ( Data ) \u201d could be implemented in which $ q $ is trained instead of $ v $ while $ v^ { \\text { MCTS } } $ and $ \\pi^ { \\text { MCTS } } $ is replaced with $ \\max q $ and $ \\arg\\max q $ . If this change is viable , it would also provide a chance to confirm the effect of planning for exploration only . * BFS is a too weak planner to compare with . How about random shooting methods ? Styles & Typo : * Use \u201c \\citet \u201d when appropriate ; when a citation is used as a subject in the sentence . * Line 3 of Algorithm 2 in Appendix : $ k=1 \\dots S $ \u2192 $ k=1 \\dots B $ ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thoughtful review ; we are glad that you liked the paper and that you found it insightful ! 1.That \u2019 s a great suggestion to perform updates with $ D_\\mathrm { tree } =D_\\mathrm { UCT } =1 $ while acting according to the full search . We have implemented this and are in the process of running experiments . Please also see our longer discussion of this in the response to all reviewers ( see \u201c Contributions of planning in computing policy updates \u201d ) . 2.The model-free version of \u201c ( Data ) \u201d implemented via Q-functions is also an interesting idea , and is quite similar to what was explored by Hamrick et al . ( 2020 ) \u201c Combining Q-learning and search with amortized value estimates \u201d . In their work , they found that this version was actually quite unstable ( see Figure 3 , \u201c SAVE w/o AL \u201d ) . We will add some discussion of this to the text . 3.We chose to compare to BFS not because of its strength or weakness as a planner but to understand how the model and value function behave when they are used differently than during training ( though we note that with large amounts of search , BFS is the strongest possible planner , as it amounts to exhaustive search ) . We actually find the BFS results to be some of the most interesting results in the paper , because they highlight the substantial mismatch between the actions recommended by the value function and the policy prior . For example , in Minipacman and Sokoban , the branching factor is 5 ; therefore 5 steps of BFS search means that all actions at the root node are expanded and the action will be selected as the one with the highest value . Yet , the performance of this agent is substantially worse than just relying on the policy prior . For sparse reward environments , any planner that relies on the value function without the policy prior will suffer from this effect , including random shooting . We will clarify the significance of this comparison in the text . Thank you for catching the typos and style issues , we will fix these in the text as well ."}, "3": {"review_id": "IrM64DGB21-3", "review_text": "summary : This paper analyzes the role of planning in the model-based reinforcement learning agent , based on evaluating MuZero on eight tasks ( i.e.Ms.Pacman , Hero , Minipacman , Sokoban , 9x9Go , Acrobot , Cheetah , and Humanoid ) , which have discrete action spaces . The conducted experiments show three major implications : ( 1 ) Of the three parts in which search is used ( i.e.search at evaluation time , search at training time for exploration , and using search result as a policy target ) , the role of serving as a policy improvement target was most substantial . ( 2 ) Deep tree search did not make a significant contribution to performance , and a simple Monte-Carlo rollout could be performant enough for MBRL . Also , a too small or too large search budget can be harmful to the performance of the MBRL agent . ( 3 ) Search at evaluation time was helpful for zero-shot generalization especially when the model is accurate . pros : - The paper is well-written and well-organized . Hypotheses and the experiments are well-designed and seem thorough . cons : - The analyses are limited to MuZero that deals with discrete action space and to the deterministic environments . Since MuZero is a particular instance of MBRL , where only the reward and value prediction are performed in the latent-state space , it is unclear that the conclusions of the work can be applied to other instances of MBRL ( e.g.MBRL methods dealing with dynamics model that operates on the original state space ) . I am not fully convinced that the results here can be generalized to other classes of MBRL . some questions and comments : - It is interesting that the main benefit of the search is serving as a policy target . What are the advantages of policy improvement through search in MuZero compared to directly optimizing policy using analytic gradients in latent space like Dreamer ( Hafner et al , 2020 ) ? - In Figure 4c , why does the performance get worse as the number of simulations increases in Acrobot and Cheetah ? Or in Figure 6 ( Simulator , Green line ) , why the performance of ( # Simulations = 3125 ) is worse than ( # Simulations = 125 ) ? This may not be explained as compounding model errors since the agent is using the exact simulator . - It would be have been great to see the learning curve even for the results of Section 4.3 , similarly to Appendix D2 that presents the learning curve for the results of Section 4.2 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review ! We are glad you find the paper to be well-written and the experiments to be well-designed and thorough . With regards to your main question about applicability to MBRL more generally , please see our response to all reviewers . It \u2019 s a great question how our results might change when learning dynamics over the original state space ! In the course of developing our experiments for this paper , we actually performed a preliminary test of this question ( by adding a pixel reconstruction loss to MuZero ) but did not find that reconstructing observations made much of a difference . We are working on more formally re-running some of our experiments in Sokoban using the pixel reconstruction loss and will include the results in the Appendix . Regarding the comparison of search versus gradient-based planning ( i.e.in SVG-style methods like Dreamer ) , we agree that it would be very interesting to test this . We believe that this question highlights one of the strengths of our work , which is that it brings into focus a number of follow-up questions about how planning can best serve MBRL agents such as \u201c what types of planning lead to the most reliable policy and value targets ? \u201d . Additionally , please see our discussion of some further connections to SVG in our response to all reviewers ( see \u201c Generality of MuZero \u201d ) . In Figure 4c , the performance of some agents decreases with large search budgets due to compounding model errors . This is an instance of the same problem discussed by a number of other papers , such as MBPO ( Janner et al. , 2019 ) , and can be also seen in Figure 3b of the MuZero paper ( Schrittwieser et al , 2019 ) . The models learned for different environments will be of varying qualities , which is why we do not see this effect in all environments . If we were to continue increasing the search budget , we would likely eventually see a drop-off everywhere . In Figure 6 , the performance decreases even with the simulator due to off-policy errors in the value function , which get worse with larger searches . In fact , this is the same effect as what can be seen in Figure 5c ; the only reason it is less drastic than with BFS is because the policy prior used by MCTS keeps the search more on-policy . Note that Figure 5b does not demonstrate this effect because the training and testing distributions are the same ( though we expect with large enough search budgets , we would see decreases here too ) ; in contrast , in Figure 6 , the errors in the value function are more severe because the agent is being asked to generalize to out-of-distribution scenes . We will clarify these points in the text . We will update the Appendix to include learning curves for the results in Section 4.3 ."}}