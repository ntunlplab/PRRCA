{"year": "2017", "forum": "B1gtu5ilg", "title": "Transfer of View-manifold Learning to Similarity Perception of Novel Objects", "decision": "Accept (Poster)", "meta_review": "The paper proposes a model for multi-view learning that uses a triplet loss to encourage different views of the same object to be closer together then the views of two different objects. The technical novelty of the model is somewhat limited, and the reviewers are concerned that experimental evaluations are done exclusively on synthetic data. The connections to human perception appear interesting. Earlier issues with missing references to prior work and comparisons with baseline models appear to have been substantially addressed in revisions of the paper. We strongly encourage the authors to further revise their paper to address the remaining outstanding issues mentioned above.", "reviews": [{"review_id": "B1gtu5ilg-0", "review_text": "This paper proposes a model to learn across different views of objects. The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object. The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance. Furthermore, a comparison against human perception on the \"Tenenbaum objects\u201d is shown. Positives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below). The paper is reasonably written. Negatives: The paper is missing relevant references of related work in this space and should compare against an existing approach. More details: The \u201cimage purification\u201d paper is very related to this work: [A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015. There they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval. If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]). It appears that code and data is available online (http://shapenet.github.io/JointEmbedding/). Somewhat related to the proposed method is recent work on multi-view 3D object retrieval: [B] Multi-View 3D Object Retrieval With Deep Embedding Network. Haiyun Guo, Jinqiao Wang, Yue Gao, Jianqiang Li, and Hanqing Lu. IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 25, NO. 12, DECEMBER 2016. There they developed a triplet loss as well, but for multi-view retrieval (given multiple images of the same object). Given the similarity of the developed approach, it somewhat limits the novelty of the proposed approach in my view. Also related are approaches that predict a volumetric representation of an input 2D image (going from image to canonical orientation of 3D shape): [C] R. Girdhar, D. Fouhey, M. Rodriguez, A. Gupta. Learning a Predictable and Generative Vector Representation for Objects. ECCV 2016. [D] Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling. Jiajun Wu*, Chengkai Zhang*, Tianfan Xue, William T. Freeman, and Joshua B. Tenenbaum. NIPS 2016. For the experiments, I would like to see a comparison using different feature layers (e.g., conv4, conv5, pool4, pool5) and feature comparison (dot product, Eucllidean). It has been shown that different layers and feature comparisons perform differently for a given task, e.g., [E] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [F] Understanding Deep Features with Computer-Generated Imagery. Mathieu Aubry and Bryan C. Russell. IEEE International Conference on Computer Vision (ICCV), 2015. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank reviewer 1 for pointing us to many related papers in the field . We have considered those papers and acted on them , and made three revisions per your suggestions . 1.We compare our approach with the joint embedding approach discussed in Su et al \u2019 s SIGGRAPH paper , using a model pre-trained on the chair category , provided by the authors . We test this on the chair category of ShapeNet and found that our OPNet outperformed this model on the instance retrieval task by a large margin . See Figure 2b in the paper . There may be two reasons that the joint embedding approach does not perform as well as the OPNet . 1 ) Su et al.used the Light Field Descriptor ( LFD ) as anchor points for the joint embedding and used CNN only for image purification . So the distance among the LFD may not serve well for good similarity judgment ; 2 ) Their application is more about recognizing 3D shapes from cluttered background , and we test the images without background . When Su et al.did compare the joint embedding approach with the Siamese network , the models were trained and tested with backgrounds which caused confusion to the Siamese network . However , an important distinction between OPNet and Su et al \u2019 s work is that our goal is to understand transferability of view-manifold learning to novel objects and novel categories that the system has not seen before . When tested with novel categories , such as synthesized objects or Pokemon ( Figure 2c , d , e ) , the joint embedding approach does not perform well . The joint embedding approach needs to train the network for each specific category while our network can transfer view-manifold learning from learned categories to completely novel categories . Thus the joint embedding approach performs much worse on novel categories than our approach . We would like to emphasize that the most novel contribution of our work is the transferability aspect of the learned view-manifold . Prior to our work , it is not clear that training with objects in one class will necessarily transfer the similarity judgment of objects to other classes , particularly to classes of unreal objects . The fact , on instance retrieval of \u201c novel objects \u201d , i.e.given a novel object , find similar novel objects ( assuming views of the same object , or objects of the same class would be considered similar ) , OPNet can generalize much better than AlexNet and Joint Embedding , suggesting strong transferability of the learned view-manifold across object classes , which we attributed to the learning of some underlying universal parts . 2.We have also evaluated OPNet in the instance retrieval task using features from different layers , as suggested by reviewer 1 . We have explored both Euclidean distance and cosine distance for the AlexNet for comparison and found cosine distance worked better than Euclidean distance , so we compared the performance of the different layers using cosine distance . The results are shown in Table 1 as well as in the Appendix B in the paper . We found that feature representation in deeper layers give better results , except for the fc8 layer of AlexNet which indicated object categories . 3.We have added references on related works in learning a generative 3D representation . Although all the references are related to our work in some technical aspects and goals , there is an important distinction . Even though we used instance retrieval as a metric for evaluation technically , and might appear to be just like other computer vision work , the motivation of our project is to investigate what will take for a deep network to develop the ability of human-like similarity judgment of NOVEL objects or novel classes of objects not in the training set as in Tenanbaum \u2019 s Science paper . The other works while related , did not investigate the transferability of view-manifold learning to different novel objects or novel classes of objects . Obviously , Siamese network and triplet cost is not new , and image retrieval of different views of 3D objects has also been intensely worked on . The novelty of our contribution is in the transferability of the view-manifold learning based on object persistence principle that allows our network to relate one novel object to other novel objects that we have never seen before . Image retrieval is just a method and precision-recall a metrics we used to measure this similarity judgment ability more systematically and comprehensively ."}, {"review_id": "B1gtu5ilg-1", "review_text": "I think learning a deep feature representation that is supervised to group dissimilar views of the same object is interesting. The paper isn't technically especially novel but that doesn't bother me at all. It does a good job exploring a new form of supervision with a new dataset. I'm also not bothered that the dataset is synthetic, but it would be good to have more experiments with real data, as well. I think the paper goes too far in linking itself to human vision. I would prefer the intro not have as much cognitive science or neuroscience. The second to fourth paragraphs of the intro in particular feels like it over-stating the contribution of this paper as somehow revealing some truth about human vision. Really, the narrative is much simpler -- \"we often want deep feature representations that are viewpoint invariant. We supervise a deep network accordingly. Humans also have some capability to be viewpoint invariant which has been widely studied [citations]\". I am skeptical of any claimed connections bigger than that. I think 3.1 should not be based on tree-to-tree distance comparisons but instead based on the entire matrix of instance-to-instance similarity assessments. Why do the lossy conversion to trees first? I don't think \"Remarkably\" is justified in the statement \"Remarkably, we found that OPnets similarity judgement matches a set of data on human similarity judgement, significantly better than AlexNet\" I'm not an expert on human vision, but from browsing online and from what I've learned before it seems that \"object persistence\" frequently relates to the concept of occlusion. Occlusion is never mentioned in this paper. I feel like the use of human vision terms might be misleading or overclaiming. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your helpful comments and suggestions . First , the reason we use tree-to-tree distance comparison in 3.1 is that it is hard to obtain the ground truth of human judgment based on the pair-wise similarity judgment . A hierarchical clustering is easier and more accurate for a human to give his judgment . Next , we would like to address reviewer \u2019 s suggestion of tuning down the connection between our work and human vision . Quoting reviewer 2 : > \u201c I think the paper goes too far in linking itself to human vision . I would prefer the intro not have as much cognitive science or neuroscience . The second to fourth paragraphs of the intro in particular feels like it over-stating the contribution of this paper as somehow revealing some truth about human vision . Really , the narrative is much simpler -- `` we often want deep feature representations that are viewpoint invariant . We supervise a deep network accordingly . Humans also have some capability to be viewpoint invariant which has been widely studied [ citations ] '' . I am skeptical of any claimed connections bigger than that. \u201d > \u201c I do n't think `` Remarkably '' is justified in the statement `` Remarkably , we found that OPnets similarity judgment matches a set of data on human similarity judgment , significantly better than AlexNet '' The motivation of our work articulated in the Introduction ( including paragraph 2 and 4 ) is truly our motivation for starting and doing this project . Our objective is to investigate what will take for a deep network to develop the ability of similarity grouping and judgment of NOVEL objects or classes of objects not in the training set as in Tenanbaum \u2019 s Science paper , and then we hit on the idea of using Siamnese triplet to evaluate whether the object continuity / object persistence constraint hypothesized by theoretical neuroscientists can lead to network that better approximates the performance of human . That view-manifold training will lead to similarity perception close to human is a conjecture that we seek to empirically investigate . Thus , we do find it remarkable that this view-invariant learning allows the network to \u201c see \u201d the similarity of novel objects in a way that is similar to human ( as in Tenenbaum \u2019 s results ) , which we must say it is rather unexpected . As we moved on this project , we realized there were similar works in the computer vision works , including those pointed out by reviewer 1 that we were not familiar with at the time , and our work became more technically driven and phrased . Even though we used the image retrieval task and precision-recall metrics for evaluation , we should not lose sight that our goal is to understand how systems can learn to see similarity in novel objects like human . That said , we do agree with reviewer 3 , the main evidence that directly supported our conjecture is the correlation with Tenenbaum \u2019 s results on human perceptual similarity grouping , though we assumed different views of the same object , and between objects of the same categories are considered more similar at semantic level ( e.g.Erdogen et al.2014 , Goldstone and Day 2013 see references below ) . Nevertheless , we thank the reviewer \u2019 s cautionary note and accordingly tuned down our claim on the possible implications of this work on human similarity judgment particularly in the Final Discussion . However , we wish to keep our motivation in the introduction , because ( 1 ) that is the motivation of our project , ( 2 ) we believe there are some deep links between deep network and human visual system , so it is important to discuss them and explore their interesting connections , particularly in a deep learning conference like ICLR . [ 1 ] Quian Quiroga , R. , Reddy , L. , Kreiman , G. , Koch , C. & Fried , I. Invariant visual representation by single neurons in the human brain . Nature 435 , 1102\u20131107 ( 2005 ) . [ 2 ] Erdogen G , Yildirim and Jacobs R. ( 2014 ) Transfer of object shape knowledge across visual and haptic modalities . http : //www.mit.edu/~ilkery/papers/ErdoganYildirimJacobs_CogSci.pdf [ 3 ] Goldstone , R. L. , & Day , S. B . ( 2013 ) .Similarity . In H. Pashler ( Ed . ) The Encyclopedia of Mind . SAGE Reference : Thousand Oaks , CA . ( pp.696-699 )"}, {"review_id": "B1gtu5ilg-2", "review_text": "On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI http://aloi.science.uva.nl . In summary, the mechanics of the proposed approach are not new, but the findings about the transfer of similarity judgement to novel object classes are interesting.", "rating": "7: Good paper, accept", "reply_text": "We thank reviewer 3 for his positive reviews . Upon reviewer 3 \u2019 s advice , we evaluated the performance of our network on the real images in ALOI viewpoint dataset . The dataset contains 1000 different objects , each with different views . In the instance retrieval task , both OPNet and AlexNet exhibited transfer learning , but the improvement of our OPNet relative to AlexNet , is only in the order of 0.5 % in the mean average precision . A problem of this dataset is that it does not have category labels so for the instance retrieval task we have to retrieve different views of the same object from all other 999 objects . OPNet \u2019 s superiority over AlexNet lies in its capability of better discriminating among objects of the same category ( e.g.discrimination between chair A and chair B , instead of between a chair and a cup ) . OPnet performs better when the distractors are more similar to the reference object , which is a harder situation . In the ALOI dataset , since the 1000 objects significantly differ from each other , this advantage can not be brought out and thus OPnet achieves a comparable results with AlexNet . In addition , real world images also have other properties such as lighting changes , reflectance and scale changes that are not considered in our training , but object persistence principle can generalize to take care of those nuisance variables . We added comments on this issue in the Final Discussion section of the revised paper . We would indeed want to explore manifold learning to discount varieties of nuisance variables present in real world scenes in future work ."}], "0": {"review_id": "B1gtu5ilg-0", "review_text": "This paper proposes a model to learn across different views of objects. The key insight is to use a triplet loss that encourages two different views of the same object to be closer than an image of a different object. The approach is evaluated on object instance and category retrieval and compared against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance. Furthermore, a comparison against human perception on the \"Tenenbaum objects\u201d is shown. Positives: Leveraging a triplet loss for this problem may have some novelty (although it may be somewhat limited given some concurrent work; see below). The paper is reasonably written. Negatives: The paper is missing relevant references of related work in this space and should compare against an existing approach. More details: The \u201cimage purification\u201d paper is very related to this work: [A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su*, Yangyan Li*, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015. There they learn to map CNN features to (hand-designed) light field descriptors of 3D shapes for view-invariant object retrieval. If possible, it would be good to compare directly against this approach (e.g., the cross-view retrieval experiment in Table 1 of [A]). It appears that code and data is available online (http://shapenet.github.io/JointEmbedding/). Somewhat related to the proposed method is recent work on multi-view 3D object retrieval: [B] Multi-View 3D Object Retrieval With Deep Embedding Network. Haiyun Guo, Jinqiao Wang, Yue Gao, Jianqiang Li, and Hanqing Lu. IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 25, NO. 12, DECEMBER 2016. There they developed a triplet loss as well, but for multi-view retrieval (given multiple images of the same object). Given the similarity of the developed approach, it somewhat limits the novelty of the proposed approach in my view. Also related are approaches that predict a volumetric representation of an input 2D image (going from image to canonical orientation of 3D shape): [C] R. Girdhar, D. Fouhey, M. Rodriguez, A. Gupta. Learning a Predictable and Generative Vector Representation for Objects. ECCV 2016. [D] Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling. Jiajun Wu*, Chengkai Zhang*, Tianfan Xue, William T. Freeman, and Joshua B. Tenenbaum. NIPS 2016. For the experiments, I would like to see a comparison using different feature layers (e.g., conv4, conv5, pool4, pool5) and feature comparison (dot product, Eucllidean). It has been shown that different layers and feature comparisons perform differently for a given task, e.g., [E] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [F] Understanding Deep Features with Computer-Generated Imagery. Mathieu Aubry and Bryan C. Russell. IEEE International Conference on Computer Vision (ICCV), 2015. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank reviewer 1 for pointing us to many related papers in the field . We have considered those papers and acted on them , and made three revisions per your suggestions . 1.We compare our approach with the joint embedding approach discussed in Su et al \u2019 s SIGGRAPH paper , using a model pre-trained on the chair category , provided by the authors . We test this on the chair category of ShapeNet and found that our OPNet outperformed this model on the instance retrieval task by a large margin . See Figure 2b in the paper . There may be two reasons that the joint embedding approach does not perform as well as the OPNet . 1 ) Su et al.used the Light Field Descriptor ( LFD ) as anchor points for the joint embedding and used CNN only for image purification . So the distance among the LFD may not serve well for good similarity judgment ; 2 ) Their application is more about recognizing 3D shapes from cluttered background , and we test the images without background . When Su et al.did compare the joint embedding approach with the Siamese network , the models were trained and tested with backgrounds which caused confusion to the Siamese network . However , an important distinction between OPNet and Su et al \u2019 s work is that our goal is to understand transferability of view-manifold learning to novel objects and novel categories that the system has not seen before . When tested with novel categories , such as synthesized objects or Pokemon ( Figure 2c , d , e ) , the joint embedding approach does not perform well . The joint embedding approach needs to train the network for each specific category while our network can transfer view-manifold learning from learned categories to completely novel categories . Thus the joint embedding approach performs much worse on novel categories than our approach . We would like to emphasize that the most novel contribution of our work is the transferability aspect of the learned view-manifold . Prior to our work , it is not clear that training with objects in one class will necessarily transfer the similarity judgment of objects to other classes , particularly to classes of unreal objects . The fact , on instance retrieval of \u201c novel objects \u201d , i.e.given a novel object , find similar novel objects ( assuming views of the same object , or objects of the same class would be considered similar ) , OPNet can generalize much better than AlexNet and Joint Embedding , suggesting strong transferability of the learned view-manifold across object classes , which we attributed to the learning of some underlying universal parts . 2.We have also evaluated OPNet in the instance retrieval task using features from different layers , as suggested by reviewer 1 . We have explored both Euclidean distance and cosine distance for the AlexNet for comparison and found cosine distance worked better than Euclidean distance , so we compared the performance of the different layers using cosine distance . The results are shown in Table 1 as well as in the Appendix B in the paper . We found that feature representation in deeper layers give better results , except for the fc8 layer of AlexNet which indicated object categories . 3.We have added references on related works in learning a generative 3D representation . Although all the references are related to our work in some technical aspects and goals , there is an important distinction . Even though we used instance retrieval as a metric for evaluation technically , and might appear to be just like other computer vision work , the motivation of our project is to investigate what will take for a deep network to develop the ability of human-like similarity judgment of NOVEL objects or novel classes of objects not in the training set as in Tenanbaum \u2019 s Science paper . The other works while related , did not investigate the transferability of view-manifold learning to different novel objects or novel classes of objects . Obviously , Siamese network and triplet cost is not new , and image retrieval of different views of 3D objects has also been intensely worked on . The novelty of our contribution is in the transferability of the view-manifold learning based on object persistence principle that allows our network to relate one novel object to other novel objects that we have never seen before . Image retrieval is just a method and precision-recall a metrics we used to measure this similarity judgment ability more systematically and comprehensively ."}, "1": {"review_id": "B1gtu5ilg-1", "review_text": "I think learning a deep feature representation that is supervised to group dissimilar views of the same object is interesting. The paper isn't technically especially novel but that doesn't bother me at all. It does a good job exploring a new form of supervision with a new dataset. I'm also not bothered that the dataset is synthetic, but it would be good to have more experiments with real data, as well. I think the paper goes too far in linking itself to human vision. I would prefer the intro not have as much cognitive science or neuroscience. The second to fourth paragraphs of the intro in particular feels like it over-stating the contribution of this paper as somehow revealing some truth about human vision. Really, the narrative is much simpler -- \"we often want deep feature representations that are viewpoint invariant. We supervise a deep network accordingly. Humans also have some capability to be viewpoint invariant which has been widely studied [citations]\". I am skeptical of any claimed connections bigger than that. I think 3.1 should not be based on tree-to-tree distance comparisons but instead based on the entire matrix of instance-to-instance similarity assessments. Why do the lossy conversion to trees first? I don't think \"Remarkably\" is justified in the statement \"Remarkably, we found that OPnets similarity judgement matches a set of data on human similarity judgement, significantly better than AlexNet\" I'm not an expert on human vision, but from browsing online and from what I've learned before it seems that \"object persistence\" frequently relates to the concept of occlusion. Occlusion is never mentioned in this paper. I feel like the use of human vision terms might be misleading or overclaiming. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your helpful comments and suggestions . First , the reason we use tree-to-tree distance comparison in 3.1 is that it is hard to obtain the ground truth of human judgment based on the pair-wise similarity judgment . A hierarchical clustering is easier and more accurate for a human to give his judgment . Next , we would like to address reviewer \u2019 s suggestion of tuning down the connection between our work and human vision . Quoting reviewer 2 : > \u201c I think the paper goes too far in linking itself to human vision . I would prefer the intro not have as much cognitive science or neuroscience . The second to fourth paragraphs of the intro in particular feels like it over-stating the contribution of this paper as somehow revealing some truth about human vision . Really , the narrative is much simpler -- `` we often want deep feature representations that are viewpoint invariant . We supervise a deep network accordingly . Humans also have some capability to be viewpoint invariant which has been widely studied [ citations ] '' . I am skeptical of any claimed connections bigger than that. \u201d > \u201c I do n't think `` Remarkably '' is justified in the statement `` Remarkably , we found that OPnets similarity judgment matches a set of data on human similarity judgment , significantly better than AlexNet '' The motivation of our work articulated in the Introduction ( including paragraph 2 and 4 ) is truly our motivation for starting and doing this project . Our objective is to investigate what will take for a deep network to develop the ability of similarity grouping and judgment of NOVEL objects or classes of objects not in the training set as in Tenanbaum \u2019 s Science paper , and then we hit on the idea of using Siamnese triplet to evaluate whether the object continuity / object persistence constraint hypothesized by theoretical neuroscientists can lead to network that better approximates the performance of human . That view-manifold training will lead to similarity perception close to human is a conjecture that we seek to empirically investigate . Thus , we do find it remarkable that this view-invariant learning allows the network to \u201c see \u201d the similarity of novel objects in a way that is similar to human ( as in Tenenbaum \u2019 s results ) , which we must say it is rather unexpected . As we moved on this project , we realized there were similar works in the computer vision works , including those pointed out by reviewer 1 that we were not familiar with at the time , and our work became more technically driven and phrased . Even though we used the image retrieval task and precision-recall metrics for evaluation , we should not lose sight that our goal is to understand how systems can learn to see similarity in novel objects like human . That said , we do agree with reviewer 3 , the main evidence that directly supported our conjecture is the correlation with Tenenbaum \u2019 s results on human perceptual similarity grouping , though we assumed different views of the same object , and between objects of the same categories are considered more similar at semantic level ( e.g.Erdogen et al.2014 , Goldstone and Day 2013 see references below ) . Nevertheless , we thank the reviewer \u2019 s cautionary note and accordingly tuned down our claim on the possible implications of this work on human similarity judgment particularly in the Final Discussion . However , we wish to keep our motivation in the introduction , because ( 1 ) that is the motivation of our project , ( 2 ) we believe there are some deep links between deep network and human visual system , so it is important to discuss them and explore their interesting connections , particularly in a deep learning conference like ICLR . [ 1 ] Quian Quiroga , R. , Reddy , L. , Kreiman , G. , Koch , C. & Fried , I. Invariant visual representation by single neurons in the human brain . Nature 435 , 1102\u20131107 ( 2005 ) . [ 2 ] Erdogen G , Yildirim and Jacobs R. ( 2014 ) Transfer of object shape knowledge across visual and haptic modalities . http : //www.mit.edu/~ilkery/papers/ErdoganYildirimJacobs_CogSci.pdf [ 3 ] Goldstone , R. L. , & Day , S. B . ( 2013 ) .Similarity . In H. Pashler ( Ed . ) The Encyclopedia of Mind . SAGE Reference : Thousand Oaks , CA . ( pp.696-699 )"}, "2": {"review_id": "B1gtu5ilg-2", "review_text": "On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI http://aloi.science.uva.nl . In summary, the mechanics of the proposed approach are not new, but the findings about the transfer of similarity judgement to novel object classes are interesting.", "rating": "7: Good paper, accept", "reply_text": "We thank reviewer 3 for his positive reviews . Upon reviewer 3 \u2019 s advice , we evaluated the performance of our network on the real images in ALOI viewpoint dataset . The dataset contains 1000 different objects , each with different views . In the instance retrieval task , both OPNet and AlexNet exhibited transfer learning , but the improvement of our OPNet relative to AlexNet , is only in the order of 0.5 % in the mean average precision . A problem of this dataset is that it does not have category labels so for the instance retrieval task we have to retrieve different views of the same object from all other 999 objects . OPNet \u2019 s superiority over AlexNet lies in its capability of better discriminating among objects of the same category ( e.g.discrimination between chair A and chair B , instead of between a chair and a cup ) . OPnet performs better when the distractors are more similar to the reference object , which is a harder situation . In the ALOI dataset , since the 1000 objects significantly differ from each other , this advantage can not be brought out and thus OPnet achieves a comparable results with AlexNet . In addition , real world images also have other properties such as lighting changes , reflectance and scale changes that are not considered in our training , but object persistence principle can generalize to take care of those nuisance variables . We added comments on this issue in the Final Discussion section of the revised paper . We would indeed want to explore manifold learning to discount varieties of nuisance variables present in real world scenes in future work ."}}