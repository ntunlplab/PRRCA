{"year": "2019", "forum": "SkeJ6iR9Km", "title": "Variational Sparse Coding", "decision": "Reject", "meta_review": "The paper develops and investigates the use of a spike-and-slab prior and approximate posterior for a VAE. It uses a continuous relaxation for the discrete binary component in the reconstruction term of the ELBO, and an analytic expression for the KL term between the spike-and-slab prior and approximate posterior. Experiments on MNIS, Fashion-MNIST and CelebA convincingly show that the approach works to learn sparse representations with improved interpretability that also yield more robust classification \n\nAll reviewers agreed that this approach to sparsity in VAEs is well motivated and sound, that the paper is well written and clear, and the experiments interesting.\nOne reviewer noted that the accuracy on MNIST remains really poor, so the approach does not cure VAEs yielding subpar representations for classification (although not the goal of this research).\n\nThe reviewers and the AC however all judged that it currently constitutes a too limited contribution because a) the approach is a straightforward application of vanilla VAEs with a different prior/posterior, and is thus rather incremental. b) the scope of the paper is rather limited, in particular as it does not sufficiently discuss and does not empirically compare with other (VAE-related) approaches from the literature that were developed for sparse latent representations.\n", "reviews": [{"review_id": "SkeJ6iR9Km-0", "review_text": "in this work the authors propose to replace Gaussian distribution over latent variables in standard variational autoencoders (VAEs) with a sparsity inducing spike-and-slab distribution. While taking the slab to be Gaussian, the authors approximate the spike with a scaled sigmoid function, which is then reparameterized through a uniform random variable. The authors derive an extension of the VAE lower bound to accommodate KL penalty terms associated with spikes. The variational lower bound (VLB) is optimized stochastically using SGD (with KL-divergence computed in closed form). Results on benchmarks show that as compared to standard VAE, the proposed method achieves better VLB for higher number of latent dimensions. Classification results on latent embeddings show that the proposed method achieves stable classification accuracy with increasing number of latent dimensions. Lastly the authors visualize sampled data to hint that different latent dimensions may encode interpretable properties of input data. Originality and significance: In my opinion, the approach taken in this work does not constitute a major methodological advancement; the VLB authors derive is a relatively straight-forward extension of VAE's lower bound. Pros: The paper is well-written and easy to follow. The idea of having a sparse prior in latent space is indeed relevant, The approximation and reparameterization of the spike variable is however functionally appealing. Potentially useful for semi-supervised learning or conditional generative modeling. Concerns: The authors show various empirical results to highlight the performance of their approach, but I am still not sure where it is best to use sparse embeddings that are induced by the proposed approach vs. those of standard VAE (or other of its sparse variants e.g., rectified Gaussian priors by Tim Salimans). For instance in all experiments VAE seems to be competitive or better for low-dimensional latent space, so one may ask, why is it necessary to go to a higher number of latent variables? In a VAE setup, one can simply tune the number of latent dimensions through cross-validation, as one would probably need to do to tune the prior sparsity parameter in the proposed method. I am also wondering if the disparity between VAE and proposed method w.r.t. classification performance for increasing number of latent dimensions vanishes as more labeled data is used for training? Fig. 11 in appendix seems to indicate that. Lastly I am not sure how we can expect to always converge to interpretable encodings since there is nothing explicit in the objective function to encourage interpretable solutions. Perhaps samples such as those shown in the paper can also be generated by modulating VAE embeddings? Maybe the proposed approach offers potential for tasks such as semi-supervised learning or conditional generative modeling, but the current set of empirical results does not allow one to draw any conclusions there. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the comments and we are glad to find that some main points and advantages of our proposed method were recognised ; inducing sparsity in the latent space of a VAE in order to find non-linear sparse codes that can constitute useful inputs in semi-supervised learning and allow for interpretable control in the generation of data . The novelty concern is addressed in the general reply , while below we address each individual concern : * Need for Cross-Validation * When finding useful latent representations for controlled generation and classification tasks there is no need to cross validate the sparsity parameter of the prior . In our experiments we set this parameter to a sufficiently low value ( 0.01 ) such that the regularisation term of the ELBO essentially induces the latent variables to be always zero and the reconstruction term induces only the variables it needs to reconstruct samples to be active . This effect occurs for any sufficiently low value of the prior sparsity parameter . This is shown in Fig.11 in the appendix , where for values of alpha lower than 0.1 the classification accuracy is steadily high . * Advantage with More Labels * The advantage is more pronounced at lower number of available labels and is especially useful in semi-supervised settings . However , the advantage is still present at higher regimes of labelled data ; In figure 11 the blue line is the classification performance as a function of latent prior sparsity alpha for 20,000 labelled examples ( 1/3 of the examples used to train the VSC ) . At alpha=1 , approximately corresponding to a standard VAE , the classification accuracy is ~81 % for MNIST and ~72 % for Fashion-MNIST , while it is ~88 % and ~80 % at alpha < 0.1 . We will clarify this point in the revised version of the paper . * Interpretation * The discussion on this aspect of sparse latent spaces is particularly interesting and we hope to initiate a conversation on it as well as study it formally in future work . We do not explicitly induce interpretation . However , sparsity in the latent space does result into a higher expectation of interpretability in large latent spaces , provided that the sources of variations in the observed data can be considered sparse ( many possible features are present in the ensemble but only small subsets of them are present in each individual example ) . Consider a VAE with a large dimensionality of latent space . The model will cluster distinct objects in different regions of the latent space and controlled generation is possible by interpolating between the regions of an aggregate posterior . However , given the encoding for one single example , the direction in which to move to modify interpretable aspects of the generation is difficult to find ; there are many normally distributed latent variables and interpretable changes may or may not be caused by altering any combination of these . Of course , it is possible to improve the expected interpretation of altering elements by lowering the dimensionality of the latent space , but this also reduces the capacity of the model and hiders the ability of modelling data that may present a large number of features in its aggregate . VSC aims at modelling data which presents few features in individual examples , but many in the data aggregate . When encoding a single example the vector we obtain only has a small subset of active features and we can expect these few dimensions to control the continuous variables that represent relevant sources of variation for this example and similar objects , while ignoring others by setting them to zero . In such a way the sub-space of smoothly variable features relevant to each encoded example is defined by the encoding itself . At the same time , the model retains the capacity to describe complicated data ensembles by being able to use different sparse elements for different examples . We realise this may not be very clear in the current version of the paper and we will make such theme a central point in the discussion of section 4.3 ."}, {"review_id": "SkeJ6iR9Km-1", "review_text": "This paper proposes an extension of VAEs with sparse priors and posteriors to learn sparse interpretable representations. Training is made tractable by computing the analytic KL for spike and slab distributions, and using a continuous relaxation for the spike variable. The technique is evaluated on MNIST, Fashion MNIST, and CelebA where it learns sparse representations with reasonable log-likelihood compared to Gaussian priors/posteriors, but improved classification accuracy and interpretability of the representation. While this paper is clear and well written, the novelty of the approach is limited. In particular, this is a straightforward application of vanilla VAEs with a different prior/posterior. The authors missed a bunch of related work and their main theoretical contributions are known in the literature (KL for spike and slab distributions, effective continuous relaxations for Bernoulli variables). The experiments are interesting but the authors should compare to more baselines with alternative priors (e.g. stick breaking VAEs, VampPrior, epitomic VAEs, discrete VAEs). Strengths + Well written, clear, and self-contained paper. Figures are nice and polished. + Thorough experiments studying the effect of sparsity on the representation Weaknesses - No discussion/comparison to other VAE approaches that incorporate sparsity into the latents: Eptimoic VAEs (2017), discrete VAEs with binary or categorical latents are sparse (see: Discrete VAEs, Concrete/Gumbel-Softmax, VQ-VAE, output-interpretable VAEs), stick breaking VAEs, structured VAEs for the Beta-Bernoulli process (Singh, Ling, et al., 2017). Missing citation to foundational work on sparse coding from Olshausen and Field (1996). - Lack of novelty: The analytic KL term for spike and slab priors has been derived before in Discrete VAEs (Rolfe, 2017) and in work on weight uncertainty (Yarin Gal's thesis, Blundell et al. 2016). Continous relaxations like the one used for the spike variable has been presented in earlier work (Concrete distributon, Gumbel-Softmax, Discrete VAEs). Minor comments: - Eq. 1, shape for B should be MxJ - Cite Rezende & Mohamed for VAEs along w/ Kingma & Welling - Definition of VAE is overly-restrictive. Typically a VAE is the combo of variational inference with an amortized inference network (and optionally reparameterization gradients). Saying that VAE implies Gaussian prior and Gaussian posterior is far too restrictive. - VLB is a non-standard acronym, use ELBO for evidence lower bound - I'm surprised that VAEs perform so poorly as latent dim increases. I'd expect it to just prune latent dimensions. Do you have an explanation for why performance drops for VAEs? Are they overfitting? - VAEs with Gaussian p(x|z) are typically harder to train and more sensitive to hyperparameters than Bernoulli p(x|z). Could you repeat your experiments using the more common binarized MNIST so that numbers are comparable to prior work? - If the goal is to learn representations with high information, then beta-VAEs or InfoVAEs should be compared (see analysis in Alemi et al., 2017). The number of dimensions may matter less for classification than the rate of the VAE. To analyze this further, you could plot the rate (KL(q(z|x) || p(z)) vs. the classification accuracy for all your models. - Fig 4: consider adding in plots of continuous interpolation of the latent dimension (as in beta-VAE, TC-VAE, etc.) - Would be interested to see how much class information is stored in the value vs. the pattern of non-zeroes in the latent representation (as done in Understanding Locally Competitive networks from Srivasta et al. 2014). - Not at all expected as this came out after your submission, but would be nice to compare to a similar recent paper: https://www.biorxiv.org/content/early/2018/08/23/399246", "rating": "5: Marginally below acceptance threshold", "reply_text": "* Contextualisation * We thank the reviewer for pointing out the related work . We agree that a better contextualisation is needed to appreciate the contribution . We will therefore modify the introduction and related work sections to incorporate relevant papers to the existing VAEs methods and different latent space priors . * Novelty * We do not claim novelty of the re-parametrisation trick for binary variables alone and we will cite the appropriate work as advised . However , we are unable to find in the literature referenced by reviewer 2 ( and in general ) the derivation of an analytic form for the general discrete mixture-Spike and Slab KL divergence ( reported in section 3.1 and derived in appendix B of our paper ) . As we may be missing the relevant sections of the cited literature , we kindly ask reviewer 2 if he/she could refer to the specific pages or equations that detail an analytic form for the discrete mixture-Spike and Slab KL divergence we present in our paper ? In the mentioned works , we observe the following : - In \u201c Discrete Variational Auto-encoders \u201d by Rolfe , the KL divergence term of the ELBO for a recognition function that models dependences between continuous and discrete variables is estimated and derived stochastically as detailed in appendix F. In our work , we derive directly an exact analytic discrete mixture-Spike and Slab KL divergence that induces sparse regularisation which does not require stochastic sampling to be estimated . - In Yarin Gal \u2019 s thesis , the approximate posterior distribution q is the product of an approximation to an optimal posterior component obtained by moment matching and the prior itself ( see p.124 ) . The KL divergence between such approximate posterior and a Spike and Slab prior is then reported in appendix C. Because the approximate posterior q contains the prior p , this KL divergence is different and arguably simpler to compute analytically than the one we present in our paper ; the prior simplifies inside the logarithm leaving the cross entropy between the approximate posterior ( which contains a Spike and Slab ) and the moment matched Gaussian ( see p.159 ) . In our work we derive a general discrete mixture-Spike and Slab KL divergence that works for any discrete-continuous mixture distribution recognition function . - In \u201c Weighted Uncertainty in Neural Networks \u201d by Blundell et al . ( if this is the paper reviewer 2 is referring to ) the proposed prior is a scale mixture of two gaussians which resembles the Spike and Slab distribution ( section 3.3 ) and the KL divergence is computed stochastically along with the rest of the ELBO ( equation 2 ) . While in this work the KL divergence is estimated by sampling from a general posterior q , we derive an exact analytic form for the KL divergence between a discrete mixture recognition function and a Spike and Slab prior . * Comparison with other VAE models * Experimental comparison with other priors presented in previous work would indeed be interesting . However , we point out that in our evaluation we aim to study the effect of sparsity in the latent space of a VAEs and show the characteristics of sparse representations rather than demonstrate a new method that performs better than previous ones in some settings . The comparison is drawn with respect to the standard VAE to clearly show how sparse latent representations differ from normally regularised ones and give the reader a clear intuition of what effects may be expected when inducing sparsity in the latent space and where it might be useful to do so in other models ."}, {"review_id": "SkeJ6iR9Km-2", "review_text": "This paper presents variational sparse coding (VSC). VSC combines variational autoencoder (VAE) with sparse coding by putting a sparse-inducing prior -- the spike and slap prior -- on the latent code z. In doing so, VSC is capable of producing sparse latent code, utilizing the latent representation more efficiently regardless of the total dimension of the latent code, meanwhile offers better interpretability. To perform traceable inference, a recognition model with the same mixture structure as the spike and slap prior is used to produce the approximate posterior. Experimental results on both MNIST and Fashion-MNIST show that even though VSC performs comparably worse than VAE in terms of ELBO, the representation it learns is more robust in terms of the total latent dimension in a downstream classification task. Additionally, the authors show that VSC provides better interpretability by interpolating the latent code and find that some dimensions correspond to certain characteristics of the data. Overall, the paper is clearly written and easy to follow. VSC is reasonably motivated and the idea behind it is quite straightforward. Technical-wise, the paper is relatively incremental -- all of the building blocks for performing tractable inference are standard: Since the posterior is intractable for nonlinear sparse coding, a recognition network is used; the prior is spike and slap, thus the recognition network will output parameters in a similar mixture structure with both a spike and a slap component; to apply reparametrization trick on the non-differentiable latent code, a continuous relaxation, similar to the one used in concrete distribution/Gamble trick, is applied to approximate the step selection function with a controllable \"temperature\" parameter. Overall, the novelty is not the strong suit of the paper. I do like the idea of VSC and its ability to learn interpretable latent features for complex non-linear models though. I have two major comments regarding the execution of the experiment that I hope the authors could address: 1. It is understandable that VSC is not able to achieve the same level of ELBO with VAE, as is quite common in models which trade off performance with interpretability. However, one attractive property of VAE is its ability to produce relatively realistic samples right from the prior, since its latent space is fairly smooth. It is not clear to me if VSC has the same property -- my guess is probably not, judging from the interpolation results currently presented in the paper. It would be interesting if the authors could comment on this and maybe include some examples to illustrate it. 2. As is known in some recent literature, e.g. Alemi et al. Fixing a broken ELBO (2018), VAE can be easily trained to simply ignore the latent representation, hence produce terrible performance on a downstream classification task. I don't know exactly how the data is processed, but on MNIST, an accuracy of less than 90% means it is quite bad (I can get >90% with PCA + logistic regression). I wonder if the authors have explored the idea of learning better representation by including a scalar in front of the KL term -- or if VSC is more robust to this problem of ignoring latent code. Minor comments: A potential relevant reference: Ainsworth et al., Interpretable VAEs for nonlinear group factor analysis (2018). ", "rating": "5: Marginally below acceptance threshold", "reply_text": "1 \u2013 * Sampling from the Prior * Sampling straight from the prior is not expected to produce as good quality samples as a standard VAE , since data samples are not well represented by just any combination of sparse features . However , samples from a PDF which has the Spike distribution of a conditional posterior ( encoding from one observation ) and slab distribution of the prior , not only produces good synthetic samples , but it does so conditioned on the features present in the particular encoded observation . Through the conditional activation of only certain variables , VSC defines a sort of \u201c sub-generative model \u201d for a given observation that models only the continuous sources of variations identified in the specific object and similar ones . For example , consider a VSC trained on fashion-MNIST ; if we sample from the prior Gaussian , but only along the dimensions activated by the encoding of a t-shirt , we have a sub-generative model for t-shirts . We partially discuss this in section 4.3 , however approaching it from a modification of encodings prospective rather than a sampling one . We realise the connection is not clear and we will add a discussion and experimental results either in the main body or appendix to clarify this important aspect . 2 \u2013 * Increasing the KL * Indeed representations found with VAEs do suffer from this known problem of ignoring latent representations . VSC does in part counteract this effect due to the discretisation from the spike variables , but is similarly affected by it . In our experiments we do not aim to obtain the best representations or classification accuracy achievable with our model , but rather compare to the standard VAE in order to highlight the difference between sparse and normally regularised latent vectors and the advantage in robustness when increasing the number of latent dimensions . The overall representation quality , and consequentially classification performance , can be improved at the expense of the ELBO value by increasing the coefficient of KL regularisation , as in beta-VAEs . By doing so , we get classification accuracies for MNIST above 90 % for 5,000 labelled examples . We will add an experimental section in either the main body or the appendix where we compare this beta-VAE strategy for VAEs and VSCs and discuss how the VSC advantage varies as the beta coefficient is changed ."}], "0": {"review_id": "SkeJ6iR9Km-0", "review_text": "in this work the authors propose to replace Gaussian distribution over latent variables in standard variational autoencoders (VAEs) with a sparsity inducing spike-and-slab distribution. While taking the slab to be Gaussian, the authors approximate the spike with a scaled sigmoid function, which is then reparameterized through a uniform random variable. The authors derive an extension of the VAE lower bound to accommodate KL penalty terms associated with spikes. The variational lower bound (VLB) is optimized stochastically using SGD (with KL-divergence computed in closed form). Results on benchmarks show that as compared to standard VAE, the proposed method achieves better VLB for higher number of latent dimensions. Classification results on latent embeddings show that the proposed method achieves stable classification accuracy with increasing number of latent dimensions. Lastly the authors visualize sampled data to hint that different latent dimensions may encode interpretable properties of input data. Originality and significance: In my opinion, the approach taken in this work does not constitute a major methodological advancement; the VLB authors derive is a relatively straight-forward extension of VAE's lower bound. Pros: The paper is well-written and easy to follow. The idea of having a sparse prior in latent space is indeed relevant, The approximation and reparameterization of the spike variable is however functionally appealing. Potentially useful for semi-supervised learning or conditional generative modeling. Concerns: The authors show various empirical results to highlight the performance of their approach, but I am still not sure where it is best to use sparse embeddings that are induced by the proposed approach vs. those of standard VAE (or other of its sparse variants e.g., rectified Gaussian priors by Tim Salimans). For instance in all experiments VAE seems to be competitive or better for low-dimensional latent space, so one may ask, why is it necessary to go to a higher number of latent variables? In a VAE setup, one can simply tune the number of latent dimensions through cross-validation, as one would probably need to do to tune the prior sparsity parameter in the proposed method. I am also wondering if the disparity between VAE and proposed method w.r.t. classification performance for increasing number of latent dimensions vanishes as more labeled data is used for training? Fig. 11 in appendix seems to indicate that. Lastly I am not sure how we can expect to always converge to interpretable encodings since there is nothing explicit in the objective function to encourage interpretable solutions. Perhaps samples such as those shown in the paper can also be generated by modulating VAE embeddings? Maybe the proposed approach offers potential for tasks such as semi-supervised learning or conditional generative modeling, but the current set of empirical results does not allow one to draw any conclusions there. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the comments and we are glad to find that some main points and advantages of our proposed method were recognised ; inducing sparsity in the latent space of a VAE in order to find non-linear sparse codes that can constitute useful inputs in semi-supervised learning and allow for interpretable control in the generation of data . The novelty concern is addressed in the general reply , while below we address each individual concern : * Need for Cross-Validation * When finding useful latent representations for controlled generation and classification tasks there is no need to cross validate the sparsity parameter of the prior . In our experiments we set this parameter to a sufficiently low value ( 0.01 ) such that the regularisation term of the ELBO essentially induces the latent variables to be always zero and the reconstruction term induces only the variables it needs to reconstruct samples to be active . This effect occurs for any sufficiently low value of the prior sparsity parameter . This is shown in Fig.11 in the appendix , where for values of alpha lower than 0.1 the classification accuracy is steadily high . * Advantage with More Labels * The advantage is more pronounced at lower number of available labels and is especially useful in semi-supervised settings . However , the advantage is still present at higher regimes of labelled data ; In figure 11 the blue line is the classification performance as a function of latent prior sparsity alpha for 20,000 labelled examples ( 1/3 of the examples used to train the VSC ) . At alpha=1 , approximately corresponding to a standard VAE , the classification accuracy is ~81 % for MNIST and ~72 % for Fashion-MNIST , while it is ~88 % and ~80 % at alpha < 0.1 . We will clarify this point in the revised version of the paper . * Interpretation * The discussion on this aspect of sparse latent spaces is particularly interesting and we hope to initiate a conversation on it as well as study it formally in future work . We do not explicitly induce interpretation . However , sparsity in the latent space does result into a higher expectation of interpretability in large latent spaces , provided that the sources of variations in the observed data can be considered sparse ( many possible features are present in the ensemble but only small subsets of them are present in each individual example ) . Consider a VAE with a large dimensionality of latent space . The model will cluster distinct objects in different regions of the latent space and controlled generation is possible by interpolating between the regions of an aggregate posterior . However , given the encoding for one single example , the direction in which to move to modify interpretable aspects of the generation is difficult to find ; there are many normally distributed latent variables and interpretable changes may or may not be caused by altering any combination of these . Of course , it is possible to improve the expected interpretation of altering elements by lowering the dimensionality of the latent space , but this also reduces the capacity of the model and hiders the ability of modelling data that may present a large number of features in its aggregate . VSC aims at modelling data which presents few features in individual examples , but many in the data aggregate . When encoding a single example the vector we obtain only has a small subset of active features and we can expect these few dimensions to control the continuous variables that represent relevant sources of variation for this example and similar objects , while ignoring others by setting them to zero . In such a way the sub-space of smoothly variable features relevant to each encoded example is defined by the encoding itself . At the same time , the model retains the capacity to describe complicated data ensembles by being able to use different sparse elements for different examples . We realise this may not be very clear in the current version of the paper and we will make such theme a central point in the discussion of section 4.3 ."}, "1": {"review_id": "SkeJ6iR9Km-1", "review_text": "This paper proposes an extension of VAEs with sparse priors and posteriors to learn sparse interpretable representations. Training is made tractable by computing the analytic KL for spike and slab distributions, and using a continuous relaxation for the spike variable. The technique is evaluated on MNIST, Fashion MNIST, and CelebA where it learns sparse representations with reasonable log-likelihood compared to Gaussian priors/posteriors, but improved classification accuracy and interpretability of the representation. While this paper is clear and well written, the novelty of the approach is limited. In particular, this is a straightforward application of vanilla VAEs with a different prior/posterior. The authors missed a bunch of related work and their main theoretical contributions are known in the literature (KL for spike and slab distributions, effective continuous relaxations for Bernoulli variables). The experiments are interesting but the authors should compare to more baselines with alternative priors (e.g. stick breaking VAEs, VampPrior, epitomic VAEs, discrete VAEs). Strengths + Well written, clear, and self-contained paper. Figures are nice and polished. + Thorough experiments studying the effect of sparsity on the representation Weaknesses - No discussion/comparison to other VAE approaches that incorporate sparsity into the latents: Eptimoic VAEs (2017), discrete VAEs with binary or categorical latents are sparse (see: Discrete VAEs, Concrete/Gumbel-Softmax, VQ-VAE, output-interpretable VAEs), stick breaking VAEs, structured VAEs for the Beta-Bernoulli process (Singh, Ling, et al., 2017). Missing citation to foundational work on sparse coding from Olshausen and Field (1996). - Lack of novelty: The analytic KL term for spike and slab priors has been derived before in Discrete VAEs (Rolfe, 2017) and in work on weight uncertainty (Yarin Gal's thesis, Blundell et al. 2016). Continous relaxations like the one used for the spike variable has been presented in earlier work (Concrete distributon, Gumbel-Softmax, Discrete VAEs). Minor comments: - Eq. 1, shape for B should be MxJ - Cite Rezende & Mohamed for VAEs along w/ Kingma & Welling - Definition of VAE is overly-restrictive. Typically a VAE is the combo of variational inference with an amortized inference network (and optionally reparameterization gradients). Saying that VAE implies Gaussian prior and Gaussian posterior is far too restrictive. - VLB is a non-standard acronym, use ELBO for evidence lower bound - I'm surprised that VAEs perform so poorly as latent dim increases. I'd expect it to just prune latent dimensions. Do you have an explanation for why performance drops for VAEs? Are they overfitting? - VAEs with Gaussian p(x|z) are typically harder to train and more sensitive to hyperparameters than Bernoulli p(x|z). Could you repeat your experiments using the more common binarized MNIST so that numbers are comparable to prior work? - If the goal is to learn representations with high information, then beta-VAEs or InfoVAEs should be compared (see analysis in Alemi et al., 2017). The number of dimensions may matter less for classification than the rate of the VAE. To analyze this further, you could plot the rate (KL(q(z|x) || p(z)) vs. the classification accuracy for all your models. - Fig 4: consider adding in plots of continuous interpolation of the latent dimension (as in beta-VAE, TC-VAE, etc.) - Would be interested to see how much class information is stored in the value vs. the pattern of non-zeroes in the latent representation (as done in Understanding Locally Competitive networks from Srivasta et al. 2014). - Not at all expected as this came out after your submission, but would be nice to compare to a similar recent paper: https://www.biorxiv.org/content/early/2018/08/23/399246", "rating": "5: Marginally below acceptance threshold", "reply_text": "* Contextualisation * We thank the reviewer for pointing out the related work . We agree that a better contextualisation is needed to appreciate the contribution . We will therefore modify the introduction and related work sections to incorporate relevant papers to the existing VAEs methods and different latent space priors . * Novelty * We do not claim novelty of the re-parametrisation trick for binary variables alone and we will cite the appropriate work as advised . However , we are unable to find in the literature referenced by reviewer 2 ( and in general ) the derivation of an analytic form for the general discrete mixture-Spike and Slab KL divergence ( reported in section 3.1 and derived in appendix B of our paper ) . As we may be missing the relevant sections of the cited literature , we kindly ask reviewer 2 if he/she could refer to the specific pages or equations that detail an analytic form for the discrete mixture-Spike and Slab KL divergence we present in our paper ? In the mentioned works , we observe the following : - In \u201c Discrete Variational Auto-encoders \u201d by Rolfe , the KL divergence term of the ELBO for a recognition function that models dependences between continuous and discrete variables is estimated and derived stochastically as detailed in appendix F. In our work , we derive directly an exact analytic discrete mixture-Spike and Slab KL divergence that induces sparse regularisation which does not require stochastic sampling to be estimated . - In Yarin Gal \u2019 s thesis , the approximate posterior distribution q is the product of an approximation to an optimal posterior component obtained by moment matching and the prior itself ( see p.124 ) . The KL divergence between such approximate posterior and a Spike and Slab prior is then reported in appendix C. Because the approximate posterior q contains the prior p , this KL divergence is different and arguably simpler to compute analytically than the one we present in our paper ; the prior simplifies inside the logarithm leaving the cross entropy between the approximate posterior ( which contains a Spike and Slab ) and the moment matched Gaussian ( see p.159 ) . In our work we derive a general discrete mixture-Spike and Slab KL divergence that works for any discrete-continuous mixture distribution recognition function . - In \u201c Weighted Uncertainty in Neural Networks \u201d by Blundell et al . ( if this is the paper reviewer 2 is referring to ) the proposed prior is a scale mixture of two gaussians which resembles the Spike and Slab distribution ( section 3.3 ) and the KL divergence is computed stochastically along with the rest of the ELBO ( equation 2 ) . While in this work the KL divergence is estimated by sampling from a general posterior q , we derive an exact analytic form for the KL divergence between a discrete mixture recognition function and a Spike and Slab prior . * Comparison with other VAE models * Experimental comparison with other priors presented in previous work would indeed be interesting . However , we point out that in our evaluation we aim to study the effect of sparsity in the latent space of a VAEs and show the characteristics of sparse representations rather than demonstrate a new method that performs better than previous ones in some settings . The comparison is drawn with respect to the standard VAE to clearly show how sparse latent representations differ from normally regularised ones and give the reader a clear intuition of what effects may be expected when inducing sparsity in the latent space and where it might be useful to do so in other models ."}, "2": {"review_id": "SkeJ6iR9Km-2", "review_text": "This paper presents variational sparse coding (VSC). VSC combines variational autoencoder (VAE) with sparse coding by putting a sparse-inducing prior -- the spike and slap prior -- on the latent code z. In doing so, VSC is capable of producing sparse latent code, utilizing the latent representation more efficiently regardless of the total dimension of the latent code, meanwhile offers better interpretability. To perform traceable inference, a recognition model with the same mixture structure as the spike and slap prior is used to produce the approximate posterior. Experimental results on both MNIST and Fashion-MNIST show that even though VSC performs comparably worse than VAE in terms of ELBO, the representation it learns is more robust in terms of the total latent dimension in a downstream classification task. Additionally, the authors show that VSC provides better interpretability by interpolating the latent code and find that some dimensions correspond to certain characteristics of the data. Overall, the paper is clearly written and easy to follow. VSC is reasonably motivated and the idea behind it is quite straightforward. Technical-wise, the paper is relatively incremental -- all of the building blocks for performing tractable inference are standard: Since the posterior is intractable for nonlinear sparse coding, a recognition network is used; the prior is spike and slap, thus the recognition network will output parameters in a similar mixture structure with both a spike and a slap component; to apply reparametrization trick on the non-differentiable latent code, a continuous relaxation, similar to the one used in concrete distribution/Gamble trick, is applied to approximate the step selection function with a controllable \"temperature\" parameter. Overall, the novelty is not the strong suit of the paper. I do like the idea of VSC and its ability to learn interpretable latent features for complex non-linear models though. I have two major comments regarding the execution of the experiment that I hope the authors could address: 1. It is understandable that VSC is not able to achieve the same level of ELBO with VAE, as is quite common in models which trade off performance with interpretability. However, one attractive property of VAE is its ability to produce relatively realistic samples right from the prior, since its latent space is fairly smooth. It is not clear to me if VSC has the same property -- my guess is probably not, judging from the interpolation results currently presented in the paper. It would be interesting if the authors could comment on this and maybe include some examples to illustrate it. 2. As is known in some recent literature, e.g. Alemi et al. Fixing a broken ELBO (2018), VAE can be easily trained to simply ignore the latent representation, hence produce terrible performance on a downstream classification task. I don't know exactly how the data is processed, but on MNIST, an accuracy of less than 90% means it is quite bad (I can get >90% with PCA + logistic regression). I wonder if the authors have explored the idea of learning better representation by including a scalar in front of the KL term -- or if VSC is more robust to this problem of ignoring latent code. Minor comments: A potential relevant reference: Ainsworth et al., Interpretable VAEs for nonlinear group factor analysis (2018). ", "rating": "5: Marginally below acceptance threshold", "reply_text": "1 \u2013 * Sampling from the Prior * Sampling straight from the prior is not expected to produce as good quality samples as a standard VAE , since data samples are not well represented by just any combination of sparse features . However , samples from a PDF which has the Spike distribution of a conditional posterior ( encoding from one observation ) and slab distribution of the prior , not only produces good synthetic samples , but it does so conditioned on the features present in the particular encoded observation . Through the conditional activation of only certain variables , VSC defines a sort of \u201c sub-generative model \u201d for a given observation that models only the continuous sources of variations identified in the specific object and similar ones . For example , consider a VSC trained on fashion-MNIST ; if we sample from the prior Gaussian , but only along the dimensions activated by the encoding of a t-shirt , we have a sub-generative model for t-shirts . We partially discuss this in section 4.3 , however approaching it from a modification of encodings prospective rather than a sampling one . We realise the connection is not clear and we will add a discussion and experimental results either in the main body or appendix to clarify this important aspect . 2 \u2013 * Increasing the KL * Indeed representations found with VAEs do suffer from this known problem of ignoring latent representations . VSC does in part counteract this effect due to the discretisation from the spike variables , but is similarly affected by it . In our experiments we do not aim to obtain the best representations or classification accuracy achievable with our model , but rather compare to the standard VAE in order to highlight the difference between sparse and normally regularised latent vectors and the advantage in robustness when increasing the number of latent dimensions . The overall representation quality , and consequentially classification performance , can be improved at the expense of the ELBO value by increasing the coefficient of KL regularisation , as in beta-VAEs . By doing so , we get classification accuracies for MNIST above 90 % for 5,000 labelled examples . We will add an experimental section in either the main body or the appendix where we compare this beta-VAE strategy for VAEs and VSCs and discuss how the VSC advantage varies as the beta coefficient is changed ."}}