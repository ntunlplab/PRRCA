{"year": "2020", "forum": "rkgz2aEKDr", "title": "On the Variance of the Adaptive Learning Rate and Beyond", "decision": "Accept (Poster)", "meta_review": "The paper considers an important topic of the warmup in deep learning, and investigates the problem of the adaptive learning rate. While the paper is somewhat borderline, the reviewers agree that it might be useful to present it to the  ICLR community.", "reviews": [{"review_id": "rkgz2aEKDr-0", "review_text": "Authors propose a way to rectify the variance of the adaptive learning rate (RAdam) and apply the optimizer to applications in image classification, language modeling and neural machine translation. The experiments demonstrate not only a strong results over baseline Adam with warmup learning rate but the robustness of the optimizer. The authors additionally demonstrate the theoretical justification behind their optimizer, however I am not very qualified to make the judgement on the theory. Overall judging from the authors description of approach and experimental results, I recommend acceptance. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and feedback . Although the warmup technology has been demonstrated to be useful in several applications and domains , it is not regarded as a common practice , partially because it is unclear why we need such technologies . In this study , we aim to uncover its underpinnings and identify an important yet long-overlooked issue : the adaptive learning rate has a problematically large variance in the early stage of training , due to the lack of enough samples . Based on our analysis , we further propose a rectification term to address this issue . In our experiments , we show that it works well on various tasks/domains ."}, {"review_id": "rkgz2aEKDr-1", "review_text": "I haven't worked in this area before, and my knowledge to the topic of designing optimizers for improving stochastic gradient descent is limited to the level of advanced ML courses at graduate school. Nevertheless, below I try my best to evaluate the technicality of this paper ==================== In this work the authors studied the variance issue of the adaptive learning rate and aim to justify the warm-start heuristic. They also demonstrate that the convergence issue of many of the stochastic gradient descent algorithms is due to large variance induced by the adaptive learning rate in the early stage of training. To tackle this issue, they proposed a variant of ADAM, which is known as rectified ADAM, whose learning rate not only takes the momentum into the account, but it also adapts to the variance of the previous gradient updates. In the first part of the paper, the authors analyzed the variance issue exists in the existing ADAM algorithm, such that with limited samples in the early stage of training, the variance of the adaptive learning rate becomes rather large and it induces high variance to the gradient update to ADAM. In general I found this theoretical justification on the observation of variance issue in ADAM sound, and quite intuitive. In the second part, they proposed the algorithm, namely rectified ADAM, where the difference here to take the second moment of the gradient into account when updating the adaptive learning rate. They showed that the variance of the adaptive learning rate with such rectification is more numerically stable (especially when variance is intractable in vanilla ADAM), and under some regularity assumption it decreases in the order or O(1/\\rho_t). In extensive numerical studies of supervised learning, the authors showed that RADAM achieves a better accuracy than ADAM (although in Table 1, I am a bit puzzled why the best accuracy is indeed from SGD, if so, what's the point of all adaptive learning rates, is that because SGD requires extensive lr tuning?) Because the superiority in accuracy they also showed that RADAM manages to have more stable training and achieves lower training loss, which is quite interesting. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and feedback . We understand your concern and hope that our response will alleviate it . More specifically , > On the comparison between SGDM and adaptive optimization algorithms It is true that for datasets like ImageNet , the state of the art resnet performance is usually achieved by SGD with momentum ( SGDM ) . In our case , we observe a similar phenomenon , and we suggest it because the hyper-parameters are tuned for the SGDM and may be sub-optimal for other algorithms . Comparing to SGDM , the adaptive optimization algorithms usually converge faster and are more robust to the choice of hyper-parameters , thus have been viewed as the default choice in many applications [ 1,2 ] . Based on our experience , it requires non-trivial efforts to make SGDM achieve similar performance for cases like training Transformers . 1.Reimers , Nils , and Iryna Gurevych . `` Optimal hyperparameters for deep lstm-networks for sequence labeling tasks . '' arXiv preprint arXiv:1707.06799 ( 2017 ) . 2.Popel , Martin , and Ond\u0159ej Bojar . `` Training tips for the transformer model . '' The Prague Bulletin of Mathematical Linguistics 110.1 ( 2018 ) : 43-70 ."}, {"review_id": "rkgz2aEKDr-2", "review_text": "In this work, authors show that the bad performance of Adam is from the large variance of adaptive learning rate at the beginning of the training. Pros: 1. Authors demonstrate that the variance of the first few stages is large, which may interpret the degradation in the performance of Adam. 2. The empirical study supports the claim about the large variance. Cons: 1. Theoretically, authors didn\u2019t illustrate why the large variance can result in the bad performance in terms of, e.g., convergence rate, generalization error, etc. 2. The performance of the proposed algorithm is still worse than SGD and it makes the analysis less attractive. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and feedback . We understand your concern and hope that our response will alleviate them . More specifically , > On theoretic analysis of the impact of the large variance In our study , we focus on exploring the underlying mechanism of the warmup technology , and find that it is non-trivial to theoretically identify the existence of the variance issue . Due to the complicated nature of neural networks , we believe that it would be even more challenging to establish a general and direct connection between large variance and the neural network behavior \u2014 in fact , a theoretical analysis of the neural network behavior by itself is a big challenge . Therefore , we believe that these questions deserve a more in-depth analysis , and we would like to leave it to future work . Here , we want to borrow some insights from recently proposed theories to intuitively illustrate why large variance of the adaptive learning rate is harmful in a simplified case . It is worth mentioning that these results are based on simple model structures ( e.g.two-layer CNN/ResNet ) and strong data distribution assumptions [ 1 , 2 , 3 ] . It has been shown that there are bad local optima when optimizing neural networks [ 2 , 3 ] ; and it requires the learning rate of the gradient descent algorithm ( not SGD ) to be controlled within a range , in order to avoid being trapped in the regions of bad local optima and converging to the global optimum [ 2 , 3 ] . In other words , the learning rate can not be too large and has to be set within a range . Such condition might be compromised by the large variance of the adaptive learning rate \u2014 as variance is a measure of variability ( i.e. , lack of consistency or fixed pattern ) , the adaptive learning rate with a larger variance is less likely to be held within the desired range . 1.Ge , Rong , et al . `` Learning two-layer neural networks with symmetric inputs . '' International Conference on Learning Representations ( ICLR ) , 2019 . 2.Du , Simon S. , et al . `` Gradient descent learns one-hidden-layer cnn : Do n't be afraid of spurious local minima . '' International Conference on Machine Learning ( ICML ) 2019 . 3.Liu , Tianyi , et al . `` Towards Understanding the Importance of Shortcut Connections in Residual Networks . `` Annual Conference on Neural Information Processing Systems ( NeurIPS ) , 2019 . > On the accuracy of SGDM and adaptive optimization algorithms It is true that for datasets like ImageNet , the state of the art resnet performance is usually achieved by SGD with momentum ( SGDM ) . In our case , we observe a similar phenomenon , and we suggest it because the hyper-parameters are tuned for SGDM and may be sub-optimal for other algorithms . At the same time , our proposed RAdam algorithm shows more robustness towards learning rate changes . It is also worth mentioning that , although RAdam fails to outperform SGD in terms of test accuracy , it results in faster convergence , lower training loss and better training performance ( e.g. , the training accuracy of SGD , Adam , and RAdam on ImageNet are 69.57 , 69.12 and 70.30 respectively ) ."}], "0": {"review_id": "rkgz2aEKDr-0", "review_text": "Authors propose a way to rectify the variance of the adaptive learning rate (RAdam) and apply the optimizer to applications in image classification, language modeling and neural machine translation. The experiments demonstrate not only a strong results over baseline Adam with warmup learning rate but the robustness of the optimizer. The authors additionally demonstrate the theoretical justification behind their optimizer, however I am not very qualified to make the judgement on the theory. Overall judging from the authors description of approach and experimental results, I recommend acceptance. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and feedback . Although the warmup technology has been demonstrated to be useful in several applications and domains , it is not regarded as a common practice , partially because it is unclear why we need such technologies . In this study , we aim to uncover its underpinnings and identify an important yet long-overlooked issue : the adaptive learning rate has a problematically large variance in the early stage of training , due to the lack of enough samples . Based on our analysis , we further propose a rectification term to address this issue . In our experiments , we show that it works well on various tasks/domains ."}, "1": {"review_id": "rkgz2aEKDr-1", "review_text": "I haven't worked in this area before, and my knowledge to the topic of designing optimizers for improving stochastic gradient descent is limited to the level of advanced ML courses at graduate school. Nevertheless, below I try my best to evaluate the technicality of this paper ==================== In this work the authors studied the variance issue of the adaptive learning rate and aim to justify the warm-start heuristic. They also demonstrate that the convergence issue of many of the stochastic gradient descent algorithms is due to large variance induced by the adaptive learning rate in the early stage of training. To tackle this issue, they proposed a variant of ADAM, which is known as rectified ADAM, whose learning rate not only takes the momentum into the account, but it also adapts to the variance of the previous gradient updates. In the first part of the paper, the authors analyzed the variance issue exists in the existing ADAM algorithm, such that with limited samples in the early stage of training, the variance of the adaptive learning rate becomes rather large and it induces high variance to the gradient update to ADAM. In general I found this theoretical justification on the observation of variance issue in ADAM sound, and quite intuitive. In the second part, they proposed the algorithm, namely rectified ADAM, where the difference here to take the second moment of the gradient into account when updating the adaptive learning rate. They showed that the variance of the adaptive learning rate with such rectification is more numerically stable (especially when variance is intractable in vanilla ADAM), and under some regularity assumption it decreases in the order or O(1/\\rho_t). In extensive numerical studies of supervised learning, the authors showed that RADAM achieves a better accuracy than ADAM (although in Table 1, I am a bit puzzled why the best accuracy is indeed from SGD, if so, what's the point of all adaptive learning rates, is that because SGD requires extensive lr tuning?) Because the superiority in accuracy they also showed that RADAM manages to have more stable training and achieves lower training loss, which is quite interesting. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and feedback . We understand your concern and hope that our response will alleviate it . More specifically , > On the comparison between SGDM and adaptive optimization algorithms It is true that for datasets like ImageNet , the state of the art resnet performance is usually achieved by SGD with momentum ( SGDM ) . In our case , we observe a similar phenomenon , and we suggest it because the hyper-parameters are tuned for the SGDM and may be sub-optimal for other algorithms . Comparing to SGDM , the adaptive optimization algorithms usually converge faster and are more robust to the choice of hyper-parameters , thus have been viewed as the default choice in many applications [ 1,2 ] . Based on our experience , it requires non-trivial efforts to make SGDM achieve similar performance for cases like training Transformers . 1.Reimers , Nils , and Iryna Gurevych . `` Optimal hyperparameters for deep lstm-networks for sequence labeling tasks . '' arXiv preprint arXiv:1707.06799 ( 2017 ) . 2.Popel , Martin , and Ond\u0159ej Bojar . `` Training tips for the transformer model . '' The Prague Bulletin of Mathematical Linguistics 110.1 ( 2018 ) : 43-70 ."}, "2": {"review_id": "rkgz2aEKDr-2", "review_text": "In this work, authors show that the bad performance of Adam is from the large variance of adaptive learning rate at the beginning of the training. Pros: 1. Authors demonstrate that the variance of the first few stages is large, which may interpret the degradation in the performance of Adam. 2. The empirical study supports the claim about the large variance. Cons: 1. Theoretically, authors didn\u2019t illustrate why the large variance can result in the bad performance in terms of, e.g., convergence rate, generalization error, etc. 2. The performance of the proposed algorithm is still worse than SGD and it makes the analysis less attractive. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and feedback . We understand your concern and hope that our response will alleviate them . More specifically , > On theoretic analysis of the impact of the large variance In our study , we focus on exploring the underlying mechanism of the warmup technology , and find that it is non-trivial to theoretically identify the existence of the variance issue . Due to the complicated nature of neural networks , we believe that it would be even more challenging to establish a general and direct connection between large variance and the neural network behavior \u2014 in fact , a theoretical analysis of the neural network behavior by itself is a big challenge . Therefore , we believe that these questions deserve a more in-depth analysis , and we would like to leave it to future work . Here , we want to borrow some insights from recently proposed theories to intuitively illustrate why large variance of the adaptive learning rate is harmful in a simplified case . It is worth mentioning that these results are based on simple model structures ( e.g.two-layer CNN/ResNet ) and strong data distribution assumptions [ 1 , 2 , 3 ] . It has been shown that there are bad local optima when optimizing neural networks [ 2 , 3 ] ; and it requires the learning rate of the gradient descent algorithm ( not SGD ) to be controlled within a range , in order to avoid being trapped in the regions of bad local optima and converging to the global optimum [ 2 , 3 ] . In other words , the learning rate can not be too large and has to be set within a range . Such condition might be compromised by the large variance of the adaptive learning rate \u2014 as variance is a measure of variability ( i.e. , lack of consistency or fixed pattern ) , the adaptive learning rate with a larger variance is less likely to be held within the desired range . 1.Ge , Rong , et al . `` Learning two-layer neural networks with symmetric inputs . '' International Conference on Learning Representations ( ICLR ) , 2019 . 2.Du , Simon S. , et al . `` Gradient descent learns one-hidden-layer cnn : Do n't be afraid of spurious local minima . '' International Conference on Machine Learning ( ICML ) 2019 . 3.Liu , Tianyi , et al . `` Towards Understanding the Importance of Shortcut Connections in Residual Networks . `` Annual Conference on Neural Information Processing Systems ( NeurIPS ) , 2019 . > On the accuracy of SGDM and adaptive optimization algorithms It is true that for datasets like ImageNet , the state of the art resnet performance is usually achieved by SGD with momentum ( SGDM ) . In our case , we observe a similar phenomenon , and we suggest it because the hyper-parameters are tuned for SGDM and may be sub-optimal for other algorithms . At the same time , our proposed RAdam algorithm shows more robustness towards learning rate changes . It is also worth mentioning that , although RAdam fails to outperform SGD in terms of test accuracy , it results in faster convergence , lower training loss and better training performance ( e.g. , the training accuracy of SGD , Adam , and RAdam on ImageNet are 69.57 , 69.12 and 70.30 respectively ) ."}}