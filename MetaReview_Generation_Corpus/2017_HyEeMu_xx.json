{"year": "2017", "forum": "HyEeMu_xx", "title": "Progressive Attention Networks for Visual Attribute Prediction", "decision": "Reject", "meta_review": "The program committee appreciates the authors' response to concerns raised in the reviews. Authors have conducted additional experiments and provided comparisons to other existing models. However, reviewer scores are not leaning sufficiently towards acceptance.\n \n The effectiveness of this approach on realistic data still remains unclear in the context of existing approaches. I agree that the reported improvement on Visual Genome over the baseline is non-trivial. But evaluating an existing state-of-the-art VQA approach (for instance) would help better place the performance of this approach in perspective relative to state-of-the-art. \n \n Incorporating reviewer comments, and more convincing demonstration of the model's capabilities on realistic data will help make the paper stronger.", "reviews": [{"review_id": "HyEeMu_xx-0", "review_text": "The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only in the last layer. Instead, the features are reweighted in each layer with the predicted attention. 1. Contribution of approach: The approach to use attention in this way is to my knowledge novel and interesting. 2. Qualitative results: 2.1. I like the large number of qualitative results; however, I would have wished the focus would have been less on the \u201cnumber\u201d dataset and more on the Visual Genome dataset. 2.2. The qualitative results for the Genome dataset unfortunately does not provide the predicted attributes. It would be interesting to see e.g. the highest predicted attributes for a given query. So far the results only show the intermediate results. 3. Qualitative results: 3.1. The paper presents results on two datasets, one simulated dataset as well as Visual Genome. On both it shows moderate but significant improvements over related approaches. 3.2. For the visual genome dataset, it would be interesting to include a quantitative evaluation how good the localization performance is of the attention approach. 3.3. It would be interesting to get a more detailed understanding of the model by providing results for different CNN layers where the attention is applied. 4. It would be interesting to see results on more established tasks, e.g. VQA, where the model should similarly apply. In fact, the task on the numbers seems to be identical to the VQA task (input/output), so most/all state-of-the-art VQA approaches should be applicable. Other (minor/discussion points) - Something seems wrong in the last two columns in Figure 11: the query \u201c7\u201d is blue not green. Either the query or the answer seem wrong. - Section 3: \u201cIn each layer, the each attended feature map\u201d -> \u201cIn each layer, each attended feature map\u201d - I think Appendix A would be clearer if it would be stated that is the attention mechanism used in SAN and which work it is based on. Summary: While the experimental evaluation could be improved with more detailed evaluation, comparisons, and qualitative results, the presented evaluation is sufficient to validate the approach. The approach itself is novel and interesting to my knowledge and speaks for acceptance. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and constructive comments . The updates of the paper and the responses are summarized below . 1.More results for VG We added true-positive ratio ( TPR ) of the attentions on VG to Table 2 . TPR of each model is measured with the GT bounding boxes because we can not obtain the GT segmentation masks . The results show that the attention quality of the proposed method is better than the other baseline models . As suggested , we also added more qualitative results of VG in Appendix D. In the qualitative results of VG , we now present the GT attribute of each example and the predicted probability of the GT attribute for each model in addition to the attention maps . 2.Experiments on VQA We wrote about this in the above response . Please refer to the second part of our response to AnonReviewer1 \u2019 s review . 3.Other minor comments We have edited the typo , Figure 11 and Appendix A based on your comments ."}, {"review_id": "HyEeMu_xx-1", "review_text": "This paper proposes an attention mechanism which is essentially a gating on every spatial feature. Though they claim novelty through the attention being progressive, progressive attention has been done before [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections], and the element-wise multiplicative gates are very similar to convolutional LSTMs and Highway Nets. There is a lack of novelty and no significant results. Pros: - The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections] - Good visualisations. Cons: - No progressive baselines were evaluated, e.g. STN and HAN at every layer acting on featuremaps. - Not clear how the query is fed into the localisation networks of baselines. - The difference in performance between author-made synthetic data and the Visual Genome datasets between baselines and PAN is very different. Why is this? There is no significant performance gain on any standard datasets. - No real novelty.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review and comments . Here are our responses to your comments . 1. comparison to STN with multiple transformer layers ( STN-M ) The proposed progressive attention model can attend to the precise region of the target object by predicting attention maps with fine-grained shapes . In contrast , the shape of the attended regions in STN is constrained by a predefined transformation type such as affine transformation . The same limitation exists even with multiple transformation layers ( STN-M ) . Note that the ability to attend to precise spatial support of object is crucial for predicting some attributes e.g.colors.In addition , the proposed method can also attend to a variable number of isolated regions ( i.e.multi-modal target distribution ) simultaneously whereas the number of attended regions of STN is fixed when the network is designed . Also , STN is developed in the context of classification tasks while our model is designed for attribute prediction . As you suggested , we conducted experiments on using STN with multiple transformer layers ( STN-M ) for our task and the results are reported in Table . 1a.Although STN-M performs better on simple settings such as MREF than other baselines ( reported in our paper ) , but is still far from the accuracy of the proposed method . Moreover , the performance of STN-M drops significantly as the task becomes more difficult and is even lower than the other baselines on MBG . Finally , STN-M could not learn proper spatial attention process on Visual Genome but learned a coding system of the query to fit the query-specific biases through transformations . The transformer layers generated padded images of different sizes and rotations to encode the query vector . We included these experiments related to STN-M in the updated paper . 2. comparison to dasNet [ Deep Networks with Internal Selective Attention through Feedback Connections ] While a channel-wise attention process is applied to multiple layers of the network in dasNet , there is no \u201c spatial \u201d attention . Our task requires to have an attention to local regions specified by the query . More importantly , extending dasNet to our tasks is not trivial for the following reasons : First , to build a dasNet for our target task , we need to assume that a pretrained attribute classifier is available prior to learning attention . Second , query module needs to be integrated in the network for feature manipulation . Finally , the network is not end-to-end trainable while the task itself involves the attention process . We added this paper in the related work section . 3. localization networks of baselines The attention functions in our experiments are depicted in Figure 5b for MNIST Reference and in Appendix Figure 9c for VG and are shared among all the networks including baselines . Note that the local context of $ \\mathcal { F } ^l_ { i , j } $ is only used in PAN-CTX . 4. performance improvements on Visual Genome The performance improvement is much larger on the MNIST Reference experiment than the VG experiment as the former benefits more directly from accurate attention maps while the latter has higher uncertainty on attribute prediction even if the resulting attention map is reasonable . Note that as shown in Table . 2 , the proposed method achieves 29.38 % mAP while the baseline ( SAN ) achieves 27.62 % on VG . It appears to be a small improvement , but it is still 6.37 % relative improvement on mAP ."}, {"review_id": "HyEeMu_xx-2", "review_text": "This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset in addition to doing attribute prediction on the Visual Genome dataset. Overall I think this is a well executed paper, with good experimental results and nice qualitative visualizations. The main thing I believe it is missing would be experiments on a dataset like VQA which would help better place the significance of this work in context of other approaches. An important missing citation is Graves 2013 which had an early version of the attention model. Minor typo: \"It confins possible attributes..\" -> It confines.. \"ImageNet (Deng et al., 2009), is used, and three additional\" -> \".., are used,\"", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and constructive comments . Below , we summarize our responses to the review . 1.Missing citation and minor typos As suggested , we added ( Graves , 2013 ) to the references and introduced it in the related work section and edited the typos . 2.Experiments on VQA VQA takes a free-form question as an input , so it is different from the attribute prediction task where the input is just an object and the output is designed to be its attribute . Although attribute prediction task can be regarded as a special VQA question type , but general VQA problems contain a lot more variety of questions . On the other hand , for the attribute prediction task , the target objects are often small in size , so precise attention with detailed spatial extent on the target objects are essential for correctly predicting their attributes . However , for many types of questions in VQA , precise attention covering an object extent is often not required . Even with these differences , we have conducted additional experiments on VQA with a na\u00efve extension of PAN ( using skip-thought vector to encode questions and treat it as an attention target ) but this na\u00efve extension did not give much performance gain . In order to apply the proposed model more appropriately for VQA , we need to use natural language processing to parse and understand questions and model multi-step attention processing pipeline such as attention for a target object , attention combination , attention transformation as in the work of \u2018 Neural Module Networks \u2019 , which we can explore as a future work ."}], "0": {"review_id": "HyEeMu_xx-0", "review_text": "The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only in the last layer. Instead, the features are reweighted in each layer with the predicted attention. 1. Contribution of approach: The approach to use attention in this way is to my knowledge novel and interesting. 2. Qualitative results: 2.1. I like the large number of qualitative results; however, I would have wished the focus would have been less on the \u201cnumber\u201d dataset and more on the Visual Genome dataset. 2.2. The qualitative results for the Genome dataset unfortunately does not provide the predicted attributes. It would be interesting to see e.g. the highest predicted attributes for a given query. So far the results only show the intermediate results. 3. Qualitative results: 3.1. The paper presents results on two datasets, one simulated dataset as well as Visual Genome. On both it shows moderate but significant improvements over related approaches. 3.2. For the visual genome dataset, it would be interesting to include a quantitative evaluation how good the localization performance is of the attention approach. 3.3. It would be interesting to get a more detailed understanding of the model by providing results for different CNN layers where the attention is applied. 4. It would be interesting to see results on more established tasks, e.g. VQA, where the model should similarly apply. In fact, the task on the numbers seems to be identical to the VQA task (input/output), so most/all state-of-the-art VQA approaches should be applicable. Other (minor/discussion points) - Something seems wrong in the last two columns in Figure 11: the query \u201c7\u201d is blue not green. Either the query or the answer seem wrong. - Section 3: \u201cIn each layer, the each attended feature map\u201d -> \u201cIn each layer, each attended feature map\u201d - I think Appendix A would be clearer if it would be stated that is the attention mechanism used in SAN and which work it is based on. Summary: While the experimental evaluation could be improved with more detailed evaluation, comparisons, and qualitative results, the presented evaluation is sufficient to validate the approach. The approach itself is novel and interesting to my knowledge and speaks for acceptance. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and constructive comments . The updates of the paper and the responses are summarized below . 1.More results for VG We added true-positive ratio ( TPR ) of the attentions on VG to Table 2 . TPR of each model is measured with the GT bounding boxes because we can not obtain the GT segmentation masks . The results show that the attention quality of the proposed method is better than the other baseline models . As suggested , we also added more qualitative results of VG in Appendix D. In the qualitative results of VG , we now present the GT attribute of each example and the predicted probability of the GT attribute for each model in addition to the attention maps . 2.Experiments on VQA We wrote about this in the above response . Please refer to the second part of our response to AnonReviewer1 \u2019 s review . 3.Other minor comments We have edited the typo , Figure 11 and Appendix A based on your comments ."}, "1": {"review_id": "HyEeMu_xx-1", "review_text": "This paper proposes an attention mechanism which is essentially a gating on every spatial feature. Though they claim novelty through the attention being progressive, progressive attention has been done before [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections], and the element-wise multiplicative gates are very similar to convolutional LSTMs and Highway Nets. There is a lack of novelty and no significant results. Pros: - The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections] - Good visualisations. Cons: - No progressive baselines were evaluated, e.g. STN and HAN at every layer acting on featuremaps. - Not clear how the query is fed into the localisation networks of baselines. - The difference in performance between author-made synthetic data and the Visual Genome datasets between baselines and PAN is very different. Why is this? There is no significant performance gain on any standard datasets. - No real novelty.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review and comments . Here are our responses to your comments . 1. comparison to STN with multiple transformer layers ( STN-M ) The proposed progressive attention model can attend to the precise region of the target object by predicting attention maps with fine-grained shapes . In contrast , the shape of the attended regions in STN is constrained by a predefined transformation type such as affine transformation . The same limitation exists even with multiple transformation layers ( STN-M ) . Note that the ability to attend to precise spatial support of object is crucial for predicting some attributes e.g.colors.In addition , the proposed method can also attend to a variable number of isolated regions ( i.e.multi-modal target distribution ) simultaneously whereas the number of attended regions of STN is fixed when the network is designed . Also , STN is developed in the context of classification tasks while our model is designed for attribute prediction . As you suggested , we conducted experiments on using STN with multiple transformer layers ( STN-M ) for our task and the results are reported in Table . 1a.Although STN-M performs better on simple settings such as MREF than other baselines ( reported in our paper ) , but is still far from the accuracy of the proposed method . Moreover , the performance of STN-M drops significantly as the task becomes more difficult and is even lower than the other baselines on MBG . Finally , STN-M could not learn proper spatial attention process on Visual Genome but learned a coding system of the query to fit the query-specific biases through transformations . The transformer layers generated padded images of different sizes and rotations to encode the query vector . We included these experiments related to STN-M in the updated paper . 2. comparison to dasNet [ Deep Networks with Internal Selective Attention through Feedback Connections ] While a channel-wise attention process is applied to multiple layers of the network in dasNet , there is no \u201c spatial \u201d attention . Our task requires to have an attention to local regions specified by the query . More importantly , extending dasNet to our tasks is not trivial for the following reasons : First , to build a dasNet for our target task , we need to assume that a pretrained attribute classifier is available prior to learning attention . Second , query module needs to be integrated in the network for feature manipulation . Finally , the network is not end-to-end trainable while the task itself involves the attention process . We added this paper in the related work section . 3. localization networks of baselines The attention functions in our experiments are depicted in Figure 5b for MNIST Reference and in Appendix Figure 9c for VG and are shared among all the networks including baselines . Note that the local context of $ \\mathcal { F } ^l_ { i , j } $ is only used in PAN-CTX . 4. performance improvements on Visual Genome The performance improvement is much larger on the MNIST Reference experiment than the VG experiment as the former benefits more directly from accurate attention maps while the latter has higher uncertainty on attribute prediction even if the resulting attention map is reasonable . Note that as shown in Table . 2 , the proposed method achieves 29.38 % mAP while the baseline ( SAN ) achieves 27.62 % on VG . It appears to be a small improvement , but it is still 6.37 % relative improvement on mAP ."}, "2": {"review_id": "HyEeMu_xx-2", "review_text": "This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset in addition to doing attribute prediction on the Visual Genome dataset. Overall I think this is a well executed paper, with good experimental results and nice qualitative visualizations. The main thing I believe it is missing would be experiments on a dataset like VQA which would help better place the significance of this work in context of other approaches. An important missing citation is Graves 2013 which had an early version of the attention model. Minor typo: \"It confins possible attributes..\" -> It confines.. \"ImageNet (Deng et al., 2009), is used, and three additional\" -> \".., are used,\"", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and constructive comments . Below , we summarize our responses to the review . 1.Missing citation and minor typos As suggested , we added ( Graves , 2013 ) to the references and introduced it in the related work section and edited the typos . 2.Experiments on VQA VQA takes a free-form question as an input , so it is different from the attribute prediction task where the input is just an object and the output is designed to be its attribute . Although attribute prediction task can be regarded as a special VQA question type , but general VQA problems contain a lot more variety of questions . On the other hand , for the attribute prediction task , the target objects are often small in size , so precise attention with detailed spatial extent on the target objects are essential for correctly predicting their attributes . However , for many types of questions in VQA , precise attention covering an object extent is often not required . Even with these differences , we have conducted additional experiments on VQA with a na\u00efve extension of PAN ( using skip-thought vector to encode questions and treat it as an attention target ) but this na\u00efve extension did not give much performance gain . In order to apply the proposed model more appropriately for VQA , we need to use natural language processing to parse and understand questions and model multi-step attention processing pipeline such as attention for a target object , attention combination , attention transformation as in the work of \u2018 Neural Module Networks \u2019 , which we can explore as a future work ."}}