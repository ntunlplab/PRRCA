{"year": "2018", "forum": "r1kP7vlRb", "title": "Toward learning better metrics for sequence generation training with policy gradient", "decision": "Reject", "meta_review": "The pros and cons of this paper can be summarized as follows:\n\nPros:\n* It seems that the method has very good intuitions: consideration of partial rewards, estimation of rewards from modified sequences, etc.\n\nCons:\n* The writing of the paper is scattered and not very well structured, which makes it difficult to follow exactly what the method is doing. If I were to give advice, I would flip the order of the sections to 4, 3, 2 (first describe the overall method, then describe the method for partial rewards, and finally describe the relationship with SeqGAN)\n* It is strange that the proposed method does not consider subsequences that do not contain y_{t+1}. This seems to go contrary to the idea of using RL or similar methods to optimize the global coherence of the generated sequence.\n* For some of the key elements of the paper, there are similar (widely used) methods that are not cited, and it is a bit difficult  to understand the relationship between them:\n** Partial rewards: this is similar to \"reward shaping\" which is widely used in RL, for example in the actor-critic method of Bahdanau et al.\n** Making modifications of the reference into a modified reference: this is done in, for example, the scheduled sampling method of Bengio et al.\n** Weighting modifications by their reward: A similar idea is presented in \"Reward Augmented Maximum Likelihood for Neural Structured Prediction\" by Norouzi et al.\n\nThe approach in this paper is potentially promising, as it definitely contains a lot of promising insights, but the clarity issues and fact that many of the key insights already exist in other approaches to which no empirical analysis is provided makes the contribution of the paper at the current time feel a bit weak. I am not recommending for acceptance at this time, but would certainly encourage the authors to do clean up the exposition, perhaps add a comparison to other methods such as RL with reward shaping, scheduled sampling, and RAML, and re-submit to another venue.", "reviews": [{"review_id": "r1kP7vlRb-0", "review_text": "This article is a follow-up from recent publications (especially the one on \"seqGAN\" by Yu et al. @ AAAI 2017) which tends to assimilate Generative Adversarial Networks as an Inverse Reinforcement Learning task in order to obtain a better stability. The adversarial learning is replaced here by a combination of policy gradient and a learned reward function. If we except the introduction which is tainted with a few typos and English mistakes, the paper is clear and well written. The experiments made on both synthetic and real text data seems solid. Being not expert in GANs I found it pleasant to read and instructive. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review . I am glad that you enjoyed reading our paper . About the mistakes of English in the introduction part , we will get native check and revise it ."}, {"review_id": "r1kP7vlRb-1", "review_text": "This paper describes an approach to generating time sequences by learning state-action values, where the state is the sequence generated so far, and the action is the choice of the next value. Local and global reward functions are learned from existing data sequences and then the Q-function learned from a policy gradient. Unfortunately, this description is a little vague, because the paper's details are quite difficult to understand. Though the approach is interesting, and the experiments are promising, important explanation is missing or muddled. Perhaps most confusing is the loss function in equation 7, which is quite inadequately explained. This paper could be interesting, but substantial editing is needed before it is sufficient for publication.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the review . From the title and the first paragraph of your review , we assume that you might not get our paper , maybe due to our poor writing . We are not sure how you understand our paper , so we firstly try to correct your misunderstandings . This paper is introducing the two techniques to learn better reward function , partial reward function and expert-based reward function training , rather than introducing new RL approach . From your review , it can be assumed that you think our paper argues about q-learning , but our paper uses policy-based RL approach ( it has been firstly done by Ranzato et al.and it is not our novelty ) and does not argue about q-learning at all . A policy ( or a sequence generator ) is learned by a policy gradient , and Q-function is NOT learned by a policy gradient . In REINFORCE , Q-value is estimated by Monte-Carlo samplings . I think the first paragraph of reviewer3 well summarizes our paper . We would appreciate if you could tell us which parts of our paper actually caused your misunderstandings so that we can revise these parts . Q.Explain about equation 7 specifically . A.The motivation of equation 7 is , when the produced fake sequence is not quite different from the true sequence ( for example , only one token in the sequence of length 20 is changed ) , we thought it would be effective to decrease the weight of the objective function , binary cross entropy ( BCE ) , because this fake sequence is actually not so bad sequence . The benefit of decreasing the weight for such sequence is that the learned reward function would become easier to be maximized by a policy gradient , because learned reward function would return some reward to a generated sequence that has some mistakes . In our paper , we describe it as \u201c smooth '' reward function . The parameter \\tau in quality function directly affects the weight of BCE . When \\tau is large , the fake sequence that is little edited from expert one get a large value of quality function , resulting in making ( 1 - q ) / ( 1 + q ) lower than 1 , and it decreases the weight of the second term in the right hand side of equation ( 7 ) . On the other hand , when \\tau is small , the fake sequence that is little edited from expert one gets a near 0 value of quality function , resulting in ( 1 - q ) / ( 1 + q ) ~= 1 , and equation ( 7 ) becomes the conventional BCE . The term ( 1 - q ) / ( 1 + q ) is heuristic and there is no theoretical background for it , but it enables to control the strictness of the learned reward function by changing the parameter \\tau ( \u201c strict \u201d means that only realistic sequence gets the reward close to 1 , and others get the reward close to 0.A strict reward function is accurate , but it is considered to be difficult to maximize by a policy gradient because this reward function might be binary-like peaky function ) . In the experiment , we show that when the partial reward function has long scale , easing the conventional BCE by using \\tau=1.5 is effective . Please give us more specific parts that you are still confused , and we are willing to give answers . Best ,"}, {"review_id": "r1kP7vlRb-2", "review_text": "This paper considers the problem of improving sequence generation by learning better metrics. Specifically, it focuses on addressing the exposure bias problem, where traditional methods such as SeqGAN uses GAN framework and reinforcement learning. Different from these work, this paper does not use GAN framework. Instead, it proposed an expert-based reward function training, which trains the reward function (the discriminator) from data that are generated by randomly modifying parts of the expert trajectories. Furthermore, it also introduces partial reward function that measures the quality of the subsequences of different lengths in the generated data. This is similar to the idea of hierarchical RL, which divide the problem into potential subtasks, which could alleviate the difficulty of reinforcement learning from sparse rewards. The idea of the paper is novel. However, there are a few points to be clarified. In Section 3.2 and in (4) and (5), the authors explains how the action value Q_{D_i} is modeled and estimated for the partial reward function D_i of length L_{D_i}. But the authors do not explain how the rewards (or action value functions) of different lengths are aggregated together to update the model using policy gradient. Is it a simple sum of all of them? It is not clear why the future subsequences that do not contain y_{t+1} are ignored for estimating the action value function Q in (4) and (5). The authors stated that it is for reducing the computation complexity. But it is not clear why specifically dropping the sequences that do not contain y_{t+1}. Please clarify more on this point. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for the review . Your first paragraph of the review well summarizes our paper . Our paper is seemingly well understood by you . Q.How are the action-state values of different length aggregated ? A.We simply add the Q values of different scales . To balance the importance of different scales , we also introduce hyper parameter alpha . Q.Why are the future subsequences that do not contain y_ { t+1 } ignored ? A2.In some setting such as Go or Atari games , the final state of the agent is important ( e.g.win or lose ) , and future states affect the Q-value a lot . So , it is important to see further future state after the certain action at t to estimate Q-value in those setting . In our setting , however , the importance of states ( or subsequences ) does not depend on the timesteps . The partial reward functions treat every subsequences at a time step equally . So , we think the subsequences that contain y_ { t+1 } are enough samples ( and they should depend on q-value of y_ { t+1 } a lot because y_ { t_1 } itself is in the subsequences ) to estimate q-value . In equation ( 4 ) , the subsequences that do not contain y_ { t+1 } are not ignored ."}], "0": {"review_id": "r1kP7vlRb-0", "review_text": "This article is a follow-up from recent publications (especially the one on \"seqGAN\" by Yu et al. @ AAAI 2017) which tends to assimilate Generative Adversarial Networks as an Inverse Reinforcement Learning task in order to obtain a better stability. The adversarial learning is replaced here by a combination of policy gradient and a learned reward function. If we except the introduction which is tainted with a few typos and English mistakes, the paper is clear and well written. The experiments made on both synthetic and real text data seems solid. Being not expert in GANs I found it pleasant to read and instructive. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review . I am glad that you enjoyed reading our paper . About the mistakes of English in the introduction part , we will get native check and revise it ."}, "1": {"review_id": "r1kP7vlRb-1", "review_text": "This paper describes an approach to generating time sequences by learning state-action values, where the state is the sequence generated so far, and the action is the choice of the next value. Local and global reward functions are learned from existing data sequences and then the Q-function learned from a policy gradient. Unfortunately, this description is a little vague, because the paper's details are quite difficult to understand. Though the approach is interesting, and the experiments are promising, important explanation is missing or muddled. Perhaps most confusing is the loss function in equation 7, which is quite inadequately explained. This paper could be interesting, but substantial editing is needed before it is sufficient for publication.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the review . From the title and the first paragraph of your review , we assume that you might not get our paper , maybe due to our poor writing . We are not sure how you understand our paper , so we firstly try to correct your misunderstandings . This paper is introducing the two techniques to learn better reward function , partial reward function and expert-based reward function training , rather than introducing new RL approach . From your review , it can be assumed that you think our paper argues about q-learning , but our paper uses policy-based RL approach ( it has been firstly done by Ranzato et al.and it is not our novelty ) and does not argue about q-learning at all . A policy ( or a sequence generator ) is learned by a policy gradient , and Q-function is NOT learned by a policy gradient . In REINFORCE , Q-value is estimated by Monte-Carlo samplings . I think the first paragraph of reviewer3 well summarizes our paper . We would appreciate if you could tell us which parts of our paper actually caused your misunderstandings so that we can revise these parts . Q.Explain about equation 7 specifically . A.The motivation of equation 7 is , when the produced fake sequence is not quite different from the true sequence ( for example , only one token in the sequence of length 20 is changed ) , we thought it would be effective to decrease the weight of the objective function , binary cross entropy ( BCE ) , because this fake sequence is actually not so bad sequence . The benefit of decreasing the weight for such sequence is that the learned reward function would become easier to be maximized by a policy gradient , because learned reward function would return some reward to a generated sequence that has some mistakes . In our paper , we describe it as \u201c smooth '' reward function . The parameter \\tau in quality function directly affects the weight of BCE . When \\tau is large , the fake sequence that is little edited from expert one get a large value of quality function , resulting in making ( 1 - q ) / ( 1 + q ) lower than 1 , and it decreases the weight of the second term in the right hand side of equation ( 7 ) . On the other hand , when \\tau is small , the fake sequence that is little edited from expert one gets a near 0 value of quality function , resulting in ( 1 - q ) / ( 1 + q ) ~= 1 , and equation ( 7 ) becomes the conventional BCE . The term ( 1 - q ) / ( 1 + q ) is heuristic and there is no theoretical background for it , but it enables to control the strictness of the learned reward function by changing the parameter \\tau ( \u201c strict \u201d means that only realistic sequence gets the reward close to 1 , and others get the reward close to 0.A strict reward function is accurate , but it is considered to be difficult to maximize by a policy gradient because this reward function might be binary-like peaky function ) . In the experiment , we show that when the partial reward function has long scale , easing the conventional BCE by using \\tau=1.5 is effective . Please give us more specific parts that you are still confused , and we are willing to give answers . Best ,"}, "2": {"review_id": "r1kP7vlRb-2", "review_text": "This paper considers the problem of improving sequence generation by learning better metrics. Specifically, it focuses on addressing the exposure bias problem, where traditional methods such as SeqGAN uses GAN framework and reinforcement learning. Different from these work, this paper does not use GAN framework. Instead, it proposed an expert-based reward function training, which trains the reward function (the discriminator) from data that are generated by randomly modifying parts of the expert trajectories. Furthermore, it also introduces partial reward function that measures the quality of the subsequences of different lengths in the generated data. This is similar to the idea of hierarchical RL, which divide the problem into potential subtasks, which could alleviate the difficulty of reinforcement learning from sparse rewards. The idea of the paper is novel. However, there are a few points to be clarified. In Section 3.2 and in (4) and (5), the authors explains how the action value Q_{D_i} is modeled and estimated for the partial reward function D_i of length L_{D_i}. But the authors do not explain how the rewards (or action value functions) of different lengths are aggregated together to update the model using policy gradient. Is it a simple sum of all of them? It is not clear why the future subsequences that do not contain y_{t+1} are ignored for estimating the action value function Q in (4) and (5). The authors stated that it is for reducing the computation complexity. But it is not clear why specifically dropping the sequences that do not contain y_{t+1}. Please clarify more on this point. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for the review . Your first paragraph of the review well summarizes our paper . Our paper is seemingly well understood by you . Q.How are the action-state values of different length aggregated ? A.We simply add the Q values of different scales . To balance the importance of different scales , we also introduce hyper parameter alpha . Q.Why are the future subsequences that do not contain y_ { t+1 } ignored ? A2.In some setting such as Go or Atari games , the final state of the agent is important ( e.g.win or lose ) , and future states affect the Q-value a lot . So , it is important to see further future state after the certain action at t to estimate Q-value in those setting . In our setting , however , the importance of states ( or subsequences ) does not depend on the timesteps . The partial reward functions treat every subsequences at a time step equally . So , we think the subsequences that contain y_ { t+1 } are enough samples ( and they should depend on q-value of y_ { t+1 } a lot because y_ { t_1 } itself is in the subsequences ) to estimate q-value . In equation ( 4 ) , the subsequences that do not contain y_ { t+1 } are not ignored ."}}