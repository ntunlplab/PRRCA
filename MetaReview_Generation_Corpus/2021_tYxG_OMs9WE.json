{"year": "2021", "forum": "tYxG_OMs9WE", "title": "Property Controllable Variational Autoencoder via Invertible Mutual Dependence", "decision": "Accept (Poster)", "meta_review": "This paper proposes a new approach to learning deep generative models with induced structure in the latent representation. All four reviewers gave the same score of 6 to this paper, showing a consensus that the paper is above the bar for acceptance. The authors did a commendable job of detailed replies to reviewer comments, which as R1, R3, and R4 all note has improved the clarity and quality of the paper, addressing their concerns.", "reviews": [{"review_id": "tYxG_OMs9WE-0", "review_text": "* * Update * * I appreciate the effort by the authors to clarify some of the issues , most of which are addressed in the rebuttal , so I will raise my score to 6 . I still feel like the $ I ( w , y ) $ part needs to be dealt with a bit carefully , especially there is a invertible mapping between the two on the generative side . The simple graphical model seems like $ w \\leftarrow x \\rightarrow y $ , where left is encoder and right is data generation procedure . * * Summary * * The paper proposes property controllable VAE , with the aim to learn certain latent variables correlated to the property and are disentangled . This is done by variational inference + total-variation based disentanglement terms , with the aim to `` control the properties '' . * * Strengths * * Empirical improvements over similar VAE approaches , such as Semi-VAE and CSVAE , showing the ability to learn and control the properties . * * Weakness * * - The paper has several important details that are missing , and seems not self-contained enough to reproduce if the reader is not familiar with the disentanglement literature ( see questions below ) . - I do n't quite understand the motivation behind the method . If you want to control the value y ( which is assumed to exist for all x ) , then why not replace w with y entirely ( i.e.invertible network is identity ) and operate directly on y ? On inference side , predict $ q ( y | x ) $ , on generation side use empirical distribution ( or some simple KDE ) on y to get $ p ( y ) $ . - Granted , it is possible that $ y_i $ s can be dependent , whereas you want $ w_i $ s to be independent ; but it seems that Section 3.2.4 adds the assumption that $ y_i $ are independent because you can group dependent variables ? * * Questions * * - How are the total correlation objectives evaluated ? For example , the explicit log-likelihood values for either $ \\log q ( w ) $ or $ \\log q ( w_i ) $ can not be efficiently computed since that is the aggregate posterior ? Is this implemented as an adversarial objective ? - What is the difference between PCVAE and PCVAE_tc ( write down the equations ) ? Performance-wise they seem relatively similar . - How does the invertible network work when y is discrete ? If I made any error in w , then it maps to some y value outside of the sample space ? - How do you evaluate $ I ( w , y ) $ , and for which distribution is this defined ? This confusion is because in reality there are two `` worlds '' concerning both variables ( generative and inference directions ) and because you have a invertible function between w and y ( so it seems that optimizing MI is useless since you do n't lose information anyways ? ) . - Why do we use avgMI as a metric ? Do we expect the optimal MI value to be 1 ? If that is the case , then how do we get an avgMI of 1.004 for Semi-VAE in the first place ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your valuable time and comments . Please find our answers to your comments below . We have updated our paper based on suggestions from you . The summary of updates based on your comments in the paper is listed as follows . To help reproduce the proposed method , we have also provided the codes in the supplemental materials . If you have any further comments or suggestions on the updated version of our paper , we will be glad to improve on them . We hope that we have addressed your concerns and questions satisfactorily . # # # # Summary of the updates : 1 . We have provided the codes of this paper in the supplemental materials . 2.We have mentioned the tool and methods to calculate the avgMI in Footnote 1 on Page 7 . 3.We have added the detailed description of the calculation of the density $ q ( z ) $ in Appendix B.3 4 . We have added a simple variation of our method in handling discrete property in Appendix A.2 . 5.We have added a detailed description of avgMI in Appendix B.4 . * * * * * Comment # 1 : * * * I do n't quite understand the motivation behind the method . If you want to control the value y ( which is assumed to exist for all x ) , then why not replace w with y entirely ( i.e.invertible network is identity ) and operate directly on y ? On inference side , predict , on generation side use empirical distribution ( or some simple KDE ) on y to get . * * * Response # 1 : * * 1 . The alternative method mentioned by the reviewer is the same as semi-VAE [ 1 ] ( one of the comparison models in the experiment ) , which directly defines properties as latent variables in the model , as discussed in the second paragraph of Section 2 . However , this operation has two main drawbacks : ( a ) Difficult to assume the distribution of $ y $ . It requires to assume a prescribed distribution of $ y $ ( e.g. , Gaussian in [ 1 ] ) which is very difficult as most of the time the true distribution of the real-world properties in $ y $ is unknown and too sophisticated to be predefined by simple distribution . However , our method can automatically approximate the sophisticated distribution on y by a learned mapping from $ w $ to $ y $ . The mapping can be highly expressive leverage due to universal approximation theory , so very sophisticated distribution on $ y $ can be learned without redefining ; ( b ) Simply replacing $ w $ with $ y $ can not handle the correlated properties . In many real-world applications the properties are correlated , hence if we discard w and directly use the properties in $ y $ to control $ x $ then , changing each property in $ y $ will also lead to the change of the other correlated properties . So , the disentanglement among the ( latent ) variables , which is a widely desired goal in VAE , can not be achieved . 2.Our model outperformed the alternative method mentioned by the reviewer ( i.e. , semi-VAE ) in our experiment . The superiority of the proposed PCVAE over the semi-VAE is obviously observed in Tables 1 and 2 . The limitation of semi-VAE is more obvious when dealing with complex real-world data . For example , regarding the control on Molweight property of molecules , the MSE score of semi-VAE is about 28 % larger than the proposed PCVAE in Table 2 . [ 1 ] Francesco Locatello , Michael Tschannen , Stefan Bauer , Gunnar Ratsch , Bernhard Sch \u00a8 olkopf , and \u00a8Olivier Bachem . Disentangling factors of variations using few labels . In International Conference on Learning Representations , 2019b . * * * * * Comment # 2 : * * * Granted , it is possible that y_i ( s ) can be dependent , whereas you want w_i ( s ) to be independent ; but it seems that Section 3.2.4 adds the assumption that y_i are independent because you can group dependent variables ? * * * Response # 2 : * * 1 . In the domain of disentangled representation learning , all the existing works conventionally focus on the independent ground-truth factors $ y_i $ ( s ) ( i.e.properties ) that to be captured by the latent variables $ w_i $ ( s ) , which is the same as our PCVAE . 2.Beyond that , one of our contributions is that we are the first to break this conventional constrained assumption and extend our model to handle the dependent ( correlated ) properties , as stated in Section 3.2.4 . * * * * * Question # 1 : * * * How are the total correlation objectives evaluated ? For example , the explicit log-likelihood values for either $ log ( q ( w ) ) $ or $ log ( q ( w_i ) ) $ can not be efficiently computed since that is the aggregate posterior ? * * * Answer # 1 : * * 1 . We briefly mentioned this in the first paragraph in Section 3.3 in the original version . We utilize the Na\u00efve Monte Carlo approximation based on a mini-batch of samples to underestimate $ q ( z ) $ , $ q ( w ) $ , and $ q ( w_k ) $ , which is commonly used ( i.e.Chen et al . [ 2 ] ) .2.Since the calculation of total correlation is not the focus of our paper , but just the utilization of existing well-recognized methods , we have added the detailed description in Appendix B.3 in the revised paper . [ 2 ] Ricky TQ Chen , et al.Isolating sources of disentanglement in variational autoencoders . NeurIPS , 2018 ."}, {"review_id": "tYxG_OMs9WE-1", "review_text": "To encourage disentanglement in the latent space of a variational autoencoder ( VAE ) , the authors propose to learn two sets of latent z and w : the dimensions of w are independent of each other and each dimension w_i maps to a known ground truth generating factor y_i . Latent z captures all the other factors . The well studied Total Correlation regularisation is used to enforce the independence of z and w , and the same is used to enforce the independence of the dimensions of w. Each dimension is learned to predict a corresponding ground truth factor . The key difference from the previous approach is the use of invertible and Lipschitz smooth mapping to learn monotonic mappings from w to y. Pros : + The proposed regularization is shown to be useful for controlled manipulation for dsprites and QM9 dataset and improves upon comparable baselines . + Can be applied when the ground truth factors are continuous-valued . + Ability to handle the case where the ground truth factors are correlated . Cons + Questions : - Ablation studies on the use of spectral normalization will be helpful . - The choice of Gaussian distribution for modeling the relationship between latent and ground truth factors could be elaborated upon . - A more complex dataset at least in the case of natural images with large number of ground truth factors with correlated latent structure will be useful . In such cases how does the choice of dimensions of z and w impact the learning and their independence structure ? Typos : Sec 2 : earning - > learning Sec 3.2.1 likely-hood - > likelihood Fig 1 ( d ) Y is missing from the model . \\phi is missing from RHS of eq 4 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your valuable time and comments . Please find our answers to your comments below . We have updated our paper based on suggestions from you . The summary of all the updates in the paper is listed in a separate comment on top of the page . If you have any further comments or suggestions on the updated version of our paper , we will be glad to improve on them . We also sincerely hope that the revised version and responses could help with an increase in the score . # # # # Summary of updates based on comments from Reviewer2 : 1 . We added the experiment on a new dataset : 3Dshapes . 2.We added the ablation study experiment for spectral normalization . 3.We have provided the codes of this paper in the supplemental materials . 4.We have added a simple variation of our method in handling discrete property in Appendix A . 2.5.We have modified the typos and rephrased some hard-parsing sentences . * * * * * Comment # 1 : * * * Ablation studies on the use of spectral normalization will be helpful . * * * Response # 1 * * Thanks for your suggestion . We have added the ablation study results of spectral normalization . The model without spectral normalization is denoted as \u201c PCVAE_nsp \u201d , as shown in Table 1 , 2 , and 3 on Page 7 . As we expected , the results show that the spectral normalization has little influence on the disentanglement learning and prediction performance , but have a critical influence on property controllable generation , especially when dealing with the complex real-world dataset ( i.e.QM9 ) , as shown in Table 2 . For example , the MSE of PCVAE_nsp is around 39 % larger than that of PCVAE averagely , as shown below : | Model | cLogP | Molweights | | : - : | : - : | : - : | |semi-VAE|1.40|122.34| |CSVAE|4.69|180.92| |PCVAE_tc|5.02|131.15| |PCVAE_nsp|1.81|176.94| |PCVAE| * * 1.29 * * | * * 87.62 * * | More results can be found in the revised paper . * * * * * Comment # 2 : * * * The choice of Gaussian distribution for modeling the relationship between latent and ground truth factors could be elaborated upon . * * * Response # 2 : * * 1 . In Equation 9 , the ground truth factors are assumed to be continuous-valued data . Thus , the prediction of continuous-valued $ y $ from $ w $ can be formalized as a regression problem , so Gaussian is widely used by default . Specifically , as we know for the regression problem , the prediction target is typically given by a deterministic function with additive Gaussian noise , which is equal to a Gaussian whose mean is the deterministic function ( as specified in Section 3.3.2 ( Page 156 ) in the book by Bishop [ 1 ] ) . 2.Our model is not limited to continuous-valued ground truth factors and Gaussian distribution . In the modified version , we have added the operation in handling the dependence between the discrete-valued factors and latent variables by formalizing it as a classification problem by utilizing the categorical distribution in Appendix A . 2 . [ 1 ] Bishop , C. M. ( 2006 ) . Pattern recognition and machine learning . springer . * * * * * Comment # 3 : * * * \u201c A more complex dataset at least in the case of natural images with large number of ground truth factors with correlated latent structure will be useful . In such cases how does the choice of dimensions of z and w impact the learning and their independence structure ? * * * Response # 3 : * * Thanks for the suggestion . 1.We have added a new dataset : 3Dshapes in the revised version , which has the ground truth factors and is commonly utilized in the domain of disentangled representation learning . 2.It is difficult for us to build and evaluate a more complex natural dataset at the current stage . On one hand , the problem of \u201c dealing with continuous correlated properties \u201d is a very novel problem , thus there are few available datasets to test , especially the one with a large number of correlated factors . On the other hand , large efforts of annotations will be required when building the natural image dataset for supervision . Considering the intensive efforts needed and the significance in this topic , we are interested in exploring further down this line in future work . * * * * * Comment # 4 * * * Typos issues * * * Response # 4 : * * Thanks for pointing out the typos . We have already modified all of them in the revised version except the third one . For the third typo raised by the reviewer , it is correct that there is no $ y $ in the left-hand side of Fig 1 ( d ) . Since $ y $ is only explicitly modeled in the generative model ( right-hand-side ) , as consistent with the objective L1 in Equation 3 on Page 4 ."}, {"review_id": "tYxG_OMs9WE-2", "review_text": "The paper presents the new Property-controllable VAE ( PCVAE ) to inductively bias the latent representation to capture explicit data properties . Towards this , the paper proposes group-wise and property-wise disentanglement terms . The group-wise disentanglement term separates two subsets of the latent representation . In contrast , the property-wise disentanglement term promotes the disentanglement of an individual component of one of the subsets in the latent space . Furthermore , the model enforces individually disentangled latent subset to account for the given property of the dataset via invertible constraint . The presented work is evaluated on two datasets from two domains . Below I present some pros , cons , suggestions , and clarifying questions for the authors . - The paper is clear to understand . The presented experimental studies are clear and demonstrate the efficacy of the model . For instance , the precision with which the proposed model controls the property generation is impressive . Qualitatively this is demonstrated by Fig 4 . Are these demonstrated on the train or the test set ? - For both group-wise and property-wise disentanglement terms , the TC term is considered . Can the authors clarify why they claim them to be novel ? Can authors present training statistics to give insight into the model 's optimization in relation to these up-weighted TC terms ? - dSprites is considered to be a relatively simpler dataset for disentanglement . If authors wanted a dataset with ground truth generative factors in the dataset , they could consider 3DShapes ( Burgess & Kim , 2018 ) , which would have more generative factors and is more challenging than dSprites . Also , 3DShapes is a widely considered dataset beside dSprites for disentanglement related study . Burgess , C , & Kim , H. ( 2018 ) . 3D Shapes Dataset . https : //github.com/deepmind/3dshapes-dataset/ . - The literature review in the paper seems broad . I would encourage authors to discuss some recent approaches ( e.g. , Gyawali et al. , 2019 ) that disentangle latent space into two subsets with one being related to certain property of the dataset . Gyawali , P. K. , Horacek , B. M. , Sapp , J. L. , & Wang , L. ( 2019 ) . Sequential factorized autoencoder for localizing the origin of ventricular activation from 12-lead electrocardiograms . IEEE Transactions on Biomedical Engineering , 67 ( 5 ) , 1505-1516 . Minor comments : - Please proofread your manuscript carefully to avoid grammatical errors . e.g. , `` latent presentation '' .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your valuable time and insightful comments . Please find our answers point by point below . We have updated our paper based on your suggestions . The summary of all the updates in the paper is listed in a separate comment on top of the webpage . If you have any further comments/suggestions on the updated version of our paper , we will be glad to improve on them . We also sincerely hope that the revised version and responses could be helpful with an increase in the final score . # # # # Summary of updated based on comments from Reviewer 3 : 1 . We have added the experiment on a new dataset : 3Dshapes . 2.We have provided the codes of this paper in the supplemental materials . 3.We have added additional related work analysis in the second paragraph in Section 2 on Page 2 . 4.We have modified the typos and rephrased some hard-parsing sentences . * * * * * Comment # 1 : * * * The paper is clear to understand\u2026 ... Qualitatively this is demonstrated by Fig 4 . Are these demonstrated on the train or the test set ? * * * Response # 1 : * * Thanks for your comments . All the evaluation results displayed are measured based on the test set . * * * * * Comment # 2 : * * * For both group-wise and property-wise disentanglement terms , the TC term is considered . Can the authors clarify why they claim them to be novel ? Can authors present training statistics to give insight into the model 's optimization in relation to these up-weighted TC terms ? * * * Response # 2 : * * Thanks for this comment . 1.The novelty of our method is nontrivial . In our problem , we have three requirements in the disentanglement : variables in $ w $ are disentangled ( property-wise ) , $ w $ is independent of $ z $ ( group-wise ) and variables in $ z $ are not required to be disentangled . The existing TC term can not handle all of them . We propose new terms by decoupling and generalizing the existing TC term in a principled way . The proposed new terms can jointly handle group-wise and property-wise disentanglements but do not over-restrict on $ q ( z ) $ . Our model is more powerful since the trade-off among the group-wise and property-wise terms can all be adjustable , which is preferable in real applications . 2.Our model outperforms the TC term-based model ( i.e.PVCAE_tc ) in the ablation study experiments . For example , in the experiment on Q9 molecule dataset , the model with proposed terms ( i.e. , PCVAE ) achieved a better performance than TC-term based model . This validated the advantage of the group-wise and property-wise disentanglement terms over the TC term . * * * * * Comment # 3 : * * * If authors wanted a dataset with ground truth generative factors in the dataset , they could consider 3DShapes ( Burgess & Kim , 2018 ) , which would have more generative factors and is more challenging than dSprites . * * * Response # 3 : * * Thanks for this suggestion . We have followed your suggestion and already added the evaluation results on the 3DShapes dataset , as shown in Table 1 , and Figure 5 in the main text , and Figure 7 in the Appendix . * * * * * Comment # 4 : * * * I would encourage authors to discuss some recent approaches ( e.g. , Gyawali et al. , 2019 ) that disentangle latent space into two subsets with one being related to certain property of the dataset . * * * Response # 4 : * * Thanks for the recommendation . We have added the analysis of this work in the second paragraph in Section 2 as : \u201c Some researchers have addressed this problem by introducing the architecture bias through a two-way factored autoencoder and realize the supervision based on a pair-wise contrastive loss ( Gyawali et al.2019 ) . `` * * * * * Minor comments # 1 : * * * Please proofread your manuscript carefully to avoid grammatical errors . e.g. , `` latent presentation '' . * * Response : * * Thanks for this suggestion . We have fully proofread our paper and modified the typos in the revised version ."}, {"review_id": "tYxG_OMs9WE-3", "review_text": "* * Property controllable variational autoencoder via invertible mutual dependence * * The paper proposes an approach for learning disentangled latent representation in the VAEs where parts of the latent space should correspond to observed properties of interest of the data so that these could be controlled at generation . They replace the total correlation regularization term of Chen2018 with more appropriate group- and property-wise KL terms promoting independence between the * relevant * ( w ) and the * remaining * ( z ) part of the latent space and inner independence between dimensions of the w part of the space ( dropping the term promoting independence in the z part . ) More importantly , they link the data property y with the latent space w by a learned mapping . The is another contribution to the rather extensive literature on disentanglement of latent representation of VAEs . It builds on previous papers in the area , mainly Chen2018 and Klys2018 . The change in the ELBO as compared to Chen2018 ( TC vs group/property-wise ) is well explained and motivated though seems rather trivial . The introduction of the invertible mapping between y and w is in my view more interesting and important . Though somewhat contrived at first reading ( some improvements in the text could help , see comments ) , it seems to correspond well to what the model shall achieve . This as is also demonstrated in the experimental section focusing mainly on the dSprites and QM9 . The paper would benefit from a more extensive experimental evaluation ( another standard dataset such as CelebA ) though I understand this may be difficult to do given the deadlines and computational resources and therefore do not consider as critical . The paper is generally well written though there are places which are difficult to parse and comprehend and suggest last minute drafting . These shall be made clear for the final version ( see comments below ) . The problem of correlated properties is addressed by a rather trivial extension of the basic model and requires the user to provide to the model groups of correlated properties . I feel this deserves a lot more attention and search for better solutions is a possible direction for future work . I recommend to accept the paper as I find it interesting for the community , addressing a lively area of research via a new approach which , though not dramatically innovative , seems to yield the desired results . I do have a few comments as to the clarity of the statements in the paper which I hope will be addressed by the authors during the rebuttal period and in the final version . * Comments / questions : * * I find calling the mapping between y and w * invertible * as misleading . The mapping $ w \\to y $ is defined as stochastic via the learned generative distribution $ p ( y | w ) $ . Each w thus corresponds to multiple possible values of $ y $ . * eq ( 2 ) : please state clearly ( in equations ) the independence and conditional independence assumptions leading to this result * eq ( 4 ) holds only in expectation over $ p ( x ) $ , right ? ( $ E_ { p ( x ) } $ in front of the DKL in the left hand side . ) * p4 before eq ( 5 ) : `` Roughly disentangling all pairs of latent variables without emphasis could lead to poor convergence and incur exponential number of pairs among properties and latent variables for such enforcement . '' What do you mean ? Please explain / elaborate / rephrase . * p4 , `` ... no strict assumptions of parameters for $ p ( y_k ) $ and $ q ( w_k |y_k ) $ '' . $ y $ does not exist in the inference model so what does $ q ( w_k |y_k ) $ refer to ? * p4 `` The most straightforward way to do this is to model both the mutual mapping between $ y_k $ and its relevant latent variable $ w_k $ . '' Both mappings $ y_k \\to w_k $ and its inverse $ w_k \\to y_k $ ? Or what do you mean ? * p5 , para5 `` Thus , we utilize the Na\u0131\u0308ve Monte Carlo approximation based on a mini-batch of samples to * * underestimate * * $ q ( z ) , q ( w ) $ and $ q ( w_k ) $ , as described by Chen et al . ( 2018 ) '' Underestimate ? What do you mean ? * p6 : you say you use the normalized mutual information between $ w_k $ and $ y_k $ . Please explain how you define this and how you calculate in practice . * p7 , fig3 `` ... when traversing three latent variables in subset z ... '' . There are only 3 latent variables z ? If more , how did you decide which shall be traversed ? * p8 , tab3 : How do you predict the property $ y_k $ from a molecule ? You first infer w and z and then use the mean of p ( y | w ) as prediction ? Or you sample y from p ( y | w ) ? Please elaborate . * p7 , tab2 : Do I read correctly that MSE of cLogP is bigger from the PCVAE ( corr ) then from the standard PCVAE ? What does this say about the advantages of PCVAE ( corr ) ? * experiments over QM9 : are all your evaluation metrics calculated only over valid molecules ? Usually , generative models for molecules are evaluated using the validity , unicity and novelty scores . It would be instructive to complement your results with these . * Minor comments / questions * * Please provide references to models displayed in Fig1 ( a ) - ( c ) . * You use the term * correletad * a lot . I 'm guessing here you mean any sort of dependence , not just linear ? * p2 : `` Directly enforcing such mutual independence inherently between all pairs of latent variables incurs exponential number of pairs among properties and latent variables for such enforcement . '' Exponential number of pairs ? What do you mean ? * p2 , para3 : `` ... as these have been shown to be relatively resilient with respect to the complex variants involved ( Bengio et al. , 2013 ) . '' Complex variants ? What do you mean ? * p3 , para2 : $ y = { y_k \\in R } _ { k=1 } ^K $ . Is this the same as $ y \\in R^K $ ? * Typos or phrasing improvements needed : * * p1 , para1 : `` * * Knowing such properties is crucial * * for many applications that depend on being able to interpret the data and control the data generation * * to yield the desired properties * * . '' ? ? ? * p2.para1 : `` Also , many cases require to generate data with properties of which the values * * are ? * * unseen during training process . '' * p3 just before eq ( 2 ) : $ \\log P_ { \u03b8 , \u03b3 } ( x , y , w , z ) $ ( capital P ? ) * p5 , para3 : `` As stated in the third challenge in Section 1 , there are usually several groups of properties involved in * * formatting * * ? data x ... '' * p6 , para5 : `` ... each encoded latent variable $ w_k $ and the property $ y_k $ * * that is ? * * assigned to it ... '' * p7 , caption Tab2 : `` ... ( PCVAE ( cor ) denotes the * * extensive * * ? model for correlated propertie ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your detailed summarization and insightful comments . Please find our answers to your comments/questions below . We have updated our paper based on your suggestions . The summary of updates in the paper are listed in a separate comment on top of the webpage . If you have any further comments/suggestions on the updated version of our paper , we will be glad to improve on them . We also sincerely hope that the revised version and responses could help with an increase in the score . * * A summary of updates based on comments from Reviewer4 : * * 1 . We have provided the codes of this paper in the supplemental materials . 2.We added the experiment on the new dataset : 3Dshapes . 3.We have modified the typos and rephrased some hard-parsing sentences . 4.We have added the references in the caption of Figure 1 . 5.We have added a clarification about invertible dependence in the first line after Equation 9 on Page 5 . 6.We added the detailed derivation of Equation 2 on Page 4 . 7.We added the annotation of expectation in Eq ( 4 ) and detailed derivation of Eq ( 4 ) in Appendix A.1 . 8.We have added the detailed description of the calculation of the density $ q ( z ) $ in Appendix B.3 9 . We have added the evaluation results on validity , unicity , and novelty of QM9 dataset in Appendix C.1 * * * * * Comment # 1 * * : * The paper would benefit from a more extensive experimental evaluation ( another standard dataset such as CelebA ) * ` * * Response # 1 : * * That \u2019 s a good point . We have added the experiment of a new dataset named \u201c 3Dshapes \u201d in the revised paper . The results are shown in Table 1 and Figure 5 . We do not select CelebA since it does not contain the annotated ground-truth labels for supervision . * * * * * Comment # 2 : * * * I find calling the mapping between $ y $ and $ w $ invertible as misleading . The mapping is defined as stochastic via the learned generative distribution . Each w thus corresponds to multiple possible values of y . * ` * * Response # 2 : * * Thanks for this suggestion . 1.In fact , it is the mapping function $ f_k ( w_k ) $ that is invertible . The stochasticity is added over function $ f_k ( w_k ) $ , via $ \\mathcal { N } ( y_k=m|f_k ( w_k ; \\gamma ) , \\sigma_k ) $ , where $ \\sigma_k $ means standard deviation , as shown in Equation 9 . We have added the clarification in the revised paper in the first line after Equation 9 on Page 5 . 2.To eliminate the misleading issue , we modified two places in the paper that mentions \u201c invertible mapping \u201d and change them into the \u201c invertible dependence \u201d in the revised version . * * * * * Comment # 3 : * * * eq ( 2 ) : please state clearly ( in equations ) the independence and conditional independence assumptions leading to this result . * * * Response # 3 : * * We have added the detailed derivation as well as the independence assumptions in equations in the revised version before Equation 2 on Page 4 as : \u201c The joint likelihood $ \\log p ( x , y , w , z ) $ can be decomposed as $ \\log p ( x , y|z , w ) +\\log p ( z , w ) $ . First , by assuming $ w $ only encode the information from $ y $ , namely , $ x $ and $ y $ are conditionally independent given $ w $ ( i.e. , $ x\\perp y|w $ ) , we can have $ \\log p ( x , y|z , w ) =\\log p ( x|z , w ) +\\log p ( y|z , w ) $ . Next.by assuming that $ z $ is independent from $ w $ and $ y $ , namely $ z \\perp w $ and $ z \\perp y $ , we can have $ \\log p ( y|z , w ) =\\log p ( y|w ) $ . Then we get $ \\log p ( x , y|z , w ) =\\log p ( x|z , w ) +\\log p ( y|w ) $ . To explicitly represent the dependence between $ x $ and $ ( z , w ) $ as well as the dependence between $ y $ and $ w $ , we can parameterize the joint log-likelihood as $ \\log p_ { \\theta , \\gamma } ( x , y , w , z ) $ with $ \\theta $ and $ \\gamma $ as : $ \\log p_ { \\theta , \\gamma } ( x , y , w , z ) =\\log p_ { \\theta } ( x|z , w ) +\\log p ( z , w ) +\\log p_ { \\gamma } ( y|w ) . $ * * * * * Comment # 4 : * * * eq ( 4 ) holds only in expectation over p ( x ) right ? ( E_ { p ( x ) } in front of the DKL in the left-hand side . ) * * * Response # 4 : * * Yes , you are right . Thanks for pointing this . We added the annotation of expectation in Eq ( 4 ) in the revised version . To make the derivation process clearer , we have also added the detailed derivation of Eq ( 3 ) and Eq ( 4 ) in Appendix A.1 . * * * * * Comment # 5 : * * * p4 before eq ( 5 ) : \u2018 Roughly disentangling all pairs of latent variables without emphasis could lead to poor convergence and incur exponential number of pairs among properties and latent variables for such enforcement. \u2019 What do you mean ? Please explain / elaborate / rephrase . * * * Response # 5 : * * ( 1 ) We revised the sentence as : \u201c Roughly enforcing the disentanglement between all pairs of latent variables in $ w $ and $ z $ , as done by the existing TC term , can incur at least quadratic number of redundant optimization efforts and could lead to poor convergence. \u201d ( 2 ) To explain it , suppose we have $ N $ variables in $ z $ and $ M $ variables in $ w $ . The traditional TC term enforces disentanglement among $ ( M+N ) ^2 $ pairs of variables , while only $ ( N^2+N ) $ pairs of variables are necessarily needed to be disentangled in our problem . Thus , the TC term includes lots of redundant enforcement components which adds much burden on the optimization ."}], "0": {"review_id": "tYxG_OMs9WE-0", "review_text": "* * Update * * I appreciate the effort by the authors to clarify some of the issues , most of which are addressed in the rebuttal , so I will raise my score to 6 . I still feel like the $ I ( w , y ) $ part needs to be dealt with a bit carefully , especially there is a invertible mapping between the two on the generative side . The simple graphical model seems like $ w \\leftarrow x \\rightarrow y $ , where left is encoder and right is data generation procedure . * * Summary * * The paper proposes property controllable VAE , with the aim to learn certain latent variables correlated to the property and are disentangled . This is done by variational inference + total-variation based disentanglement terms , with the aim to `` control the properties '' . * * Strengths * * Empirical improvements over similar VAE approaches , such as Semi-VAE and CSVAE , showing the ability to learn and control the properties . * * Weakness * * - The paper has several important details that are missing , and seems not self-contained enough to reproduce if the reader is not familiar with the disentanglement literature ( see questions below ) . - I do n't quite understand the motivation behind the method . If you want to control the value y ( which is assumed to exist for all x ) , then why not replace w with y entirely ( i.e.invertible network is identity ) and operate directly on y ? On inference side , predict $ q ( y | x ) $ , on generation side use empirical distribution ( or some simple KDE ) on y to get $ p ( y ) $ . - Granted , it is possible that $ y_i $ s can be dependent , whereas you want $ w_i $ s to be independent ; but it seems that Section 3.2.4 adds the assumption that $ y_i $ are independent because you can group dependent variables ? * * Questions * * - How are the total correlation objectives evaluated ? For example , the explicit log-likelihood values for either $ \\log q ( w ) $ or $ \\log q ( w_i ) $ can not be efficiently computed since that is the aggregate posterior ? Is this implemented as an adversarial objective ? - What is the difference between PCVAE and PCVAE_tc ( write down the equations ) ? Performance-wise they seem relatively similar . - How does the invertible network work when y is discrete ? If I made any error in w , then it maps to some y value outside of the sample space ? - How do you evaluate $ I ( w , y ) $ , and for which distribution is this defined ? This confusion is because in reality there are two `` worlds '' concerning both variables ( generative and inference directions ) and because you have a invertible function between w and y ( so it seems that optimizing MI is useless since you do n't lose information anyways ? ) . - Why do we use avgMI as a metric ? Do we expect the optimal MI value to be 1 ? If that is the case , then how do we get an avgMI of 1.004 for Semi-VAE in the first place ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your valuable time and comments . Please find our answers to your comments below . We have updated our paper based on suggestions from you . The summary of updates based on your comments in the paper is listed as follows . To help reproduce the proposed method , we have also provided the codes in the supplemental materials . If you have any further comments or suggestions on the updated version of our paper , we will be glad to improve on them . We hope that we have addressed your concerns and questions satisfactorily . # # # # Summary of the updates : 1 . We have provided the codes of this paper in the supplemental materials . 2.We have mentioned the tool and methods to calculate the avgMI in Footnote 1 on Page 7 . 3.We have added the detailed description of the calculation of the density $ q ( z ) $ in Appendix B.3 4 . We have added a simple variation of our method in handling discrete property in Appendix A.2 . 5.We have added a detailed description of avgMI in Appendix B.4 . * * * * * Comment # 1 : * * * I do n't quite understand the motivation behind the method . If you want to control the value y ( which is assumed to exist for all x ) , then why not replace w with y entirely ( i.e.invertible network is identity ) and operate directly on y ? On inference side , predict , on generation side use empirical distribution ( or some simple KDE ) on y to get . * * * Response # 1 : * * 1 . The alternative method mentioned by the reviewer is the same as semi-VAE [ 1 ] ( one of the comparison models in the experiment ) , which directly defines properties as latent variables in the model , as discussed in the second paragraph of Section 2 . However , this operation has two main drawbacks : ( a ) Difficult to assume the distribution of $ y $ . It requires to assume a prescribed distribution of $ y $ ( e.g. , Gaussian in [ 1 ] ) which is very difficult as most of the time the true distribution of the real-world properties in $ y $ is unknown and too sophisticated to be predefined by simple distribution . However , our method can automatically approximate the sophisticated distribution on y by a learned mapping from $ w $ to $ y $ . The mapping can be highly expressive leverage due to universal approximation theory , so very sophisticated distribution on $ y $ can be learned without redefining ; ( b ) Simply replacing $ w $ with $ y $ can not handle the correlated properties . In many real-world applications the properties are correlated , hence if we discard w and directly use the properties in $ y $ to control $ x $ then , changing each property in $ y $ will also lead to the change of the other correlated properties . So , the disentanglement among the ( latent ) variables , which is a widely desired goal in VAE , can not be achieved . 2.Our model outperformed the alternative method mentioned by the reviewer ( i.e. , semi-VAE ) in our experiment . The superiority of the proposed PCVAE over the semi-VAE is obviously observed in Tables 1 and 2 . The limitation of semi-VAE is more obvious when dealing with complex real-world data . For example , regarding the control on Molweight property of molecules , the MSE score of semi-VAE is about 28 % larger than the proposed PCVAE in Table 2 . [ 1 ] Francesco Locatello , Michael Tschannen , Stefan Bauer , Gunnar Ratsch , Bernhard Sch \u00a8 olkopf , and \u00a8Olivier Bachem . Disentangling factors of variations using few labels . In International Conference on Learning Representations , 2019b . * * * * * Comment # 2 : * * * Granted , it is possible that y_i ( s ) can be dependent , whereas you want w_i ( s ) to be independent ; but it seems that Section 3.2.4 adds the assumption that y_i are independent because you can group dependent variables ? * * * Response # 2 : * * 1 . In the domain of disentangled representation learning , all the existing works conventionally focus on the independent ground-truth factors $ y_i $ ( s ) ( i.e.properties ) that to be captured by the latent variables $ w_i $ ( s ) , which is the same as our PCVAE . 2.Beyond that , one of our contributions is that we are the first to break this conventional constrained assumption and extend our model to handle the dependent ( correlated ) properties , as stated in Section 3.2.4 . * * * * * Question # 1 : * * * How are the total correlation objectives evaluated ? For example , the explicit log-likelihood values for either $ log ( q ( w ) ) $ or $ log ( q ( w_i ) ) $ can not be efficiently computed since that is the aggregate posterior ? * * * Answer # 1 : * * 1 . We briefly mentioned this in the first paragraph in Section 3.3 in the original version . We utilize the Na\u00efve Monte Carlo approximation based on a mini-batch of samples to underestimate $ q ( z ) $ , $ q ( w ) $ , and $ q ( w_k ) $ , which is commonly used ( i.e.Chen et al . [ 2 ] ) .2.Since the calculation of total correlation is not the focus of our paper , but just the utilization of existing well-recognized methods , we have added the detailed description in Appendix B.3 in the revised paper . [ 2 ] Ricky TQ Chen , et al.Isolating sources of disentanglement in variational autoencoders . NeurIPS , 2018 ."}, "1": {"review_id": "tYxG_OMs9WE-1", "review_text": "To encourage disentanglement in the latent space of a variational autoencoder ( VAE ) , the authors propose to learn two sets of latent z and w : the dimensions of w are independent of each other and each dimension w_i maps to a known ground truth generating factor y_i . Latent z captures all the other factors . The well studied Total Correlation regularisation is used to enforce the independence of z and w , and the same is used to enforce the independence of the dimensions of w. Each dimension is learned to predict a corresponding ground truth factor . The key difference from the previous approach is the use of invertible and Lipschitz smooth mapping to learn monotonic mappings from w to y. Pros : + The proposed regularization is shown to be useful for controlled manipulation for dsprites and QM9 dataset and improves upon comparable baselines . + Can be applied when the ground truth factors are continuous-valued . + Ability to handle the case where the ground truth factors are correlated . Cons + Questions : - Ablation studies on the use of spectral normalization will be helpful . - The choice of Gaussian distribution for modeling the relationship between latent and ground truth factors could be elaborated upon . - A more complex dataset at least in the case of natural images with large number of ground truth factors with correlated latent structure will be useful . In such cases how does the choice of dimensions of z and w impact the learning and their independence structure ? Typos : Sec 2 : earning - > learning Sec 3.2.1 likely-hood - > likelihood Fig 1 ( d ) Y is missing from the model . \\phi is missing from RHS of eq 4 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your valuable time and comments . Please find our answers to your comments below . We have updated our paper based on suggestions from you . The summary of all the updates in the paper is listed in a separate comment on top of the page . If you have any further comments or suggestions on the updated version of our paper , we will be glad to improve on them . We also sincerely hope that the revised version and responses could help with an increase in the score . # # # # Summary of updates based on comments from Reviewer2 : 1 . We added the experiment on a new dataset : 3Dshapes . 2.We added the ablation study experiment for spectral normalization . 3.We have provided the codes of this paper in the supplemental materials . 4.We have added a simple variation of our method in handling discrete property in Appendix A . 2.5.We have modified the typos and rephrased some hard-parsing sentences . * * * * * Comment # 1 : * * * Ablation studies on the use of spectral normalization will be helpful . * * * Response # 1 * * Thanks for your suggestion . We have added the ablation study results of spectral normalization . The model without spectral normalization is denoted as \u201c PCVAE_nsp \u201d , as shown in Table 1 , 2 , and 3 on Page 7 . As we expected , the results show that the spectral normalization has little influence on the disentanglement learning and prediction performance , but have a critical influence on property controllable generation , especially when dealing with the complex real-world dataset ( i.e.QM9 ) , as shown in Table 2 . For example , the MSE of PCVAE_nsp is around 39 % larger than that of PCVAE averagely , as shown below : | Model | cLogP | Molweights | | : - : | : - : | : - : | |semi-VAE|1.40|122.34| |CSVAE|4.69|180.92| |PCVAE_tc|5.02|131.15| |PCVAE_nsp|1.81|176.94| |PCVAE| * * 1.29 * * | * * 87.62 * * | More results can be found in the revised paper . * * * * * Comment # 2 : * * * The choice of Gaussian distribution for modeling the relationship between latent and ground truth factors could be elaborated upon . * * * Response # 2 : * * 1 . In Equation 9 , the ground truth factors are assumed to be continuous-valued data . Thus , the prediction of continuous-valued $ y $ from $ w $ can be formalized as a regression problem , so Gaussian is widely used by default . Specifically , as we know for the regression problem , the prediction target is typically given by a deterministic function with additive Gaussian noise , which is equal to a Gaussian whose mean is the deterministic function ( as specified in Section 3.3.2 ( Page 156 ) in the book by Bishop [ 1 ] ) . 2.Our model is not limited to continuous-valued ground truth factors and Gaussian distribution . In the modified version , we have added the operation in handling the dependence between the discrete-valued factors and latent variables by formalizing it as a classification problem by utilizing the categorical distribution in Appendix A . 2 . [ 1 ] Bishop , C. M. ( 2006 ) . Pattern recognition and machine learning . springer . * * * * * Comment # 3 : * * * \u201c A more complex dataset at least in the case of natural images with large number of ground truth factors with correlated latent structure will be useful . In such cases how does the choice of dimensions of z and w impact the learning and their independence structure ? * * * Response # 3 : * * Thanks for the suggestion . 1.We have added a new dataset : 3Dshapes in the revised version , which has the ground truth factors and is commonly utilized in the domain of disentangled representation learning . 2.It is difficult for us to build and evaluate a more complex natural dataset at the current stage . On one hand , the problem of \u201c dealing with continuous correlated properties \u201d is a very novel problem , thus there are few available datasets to test , especially the one with a large number of correlated factors . On the other hand , large efforts of annotations will be required when building the natural image dataset for supervision . Considering the intensive efforts needed and the significance in this topic , we are interested in exploring further down this line in future work . * * * * * Comment # 4 * * * Typos issues * * * Response # 4 : * * Thanks for pointing out the typos . We have already modified all of them in the revised version except the third one . For the third typo raised by the reviewer , it is correct that there is no $ y $ in the left-hand side of Fig 1 ( d ) . Since $ y $ is only explicitly modeled in the generative model ( right-hand-side ) , as consistent with the objective L1 in Equation 3 on Page 4 ."}, "2": {"review_id": "tYxG_OMs9WE-2", "review_text": "The paper presents the new Property-controllable VAE ( PCVAE ) to inductively bias the latent representation to capture explicit data properties . Towards this , the paper proposes group-wise and property-wise disentanglement terms . The group-wise disentanglement term separates two subsets of the latent representation . In contrast , the property-wise disentanglement term promotes the disentanglement of an individual component of one of the subsets in the latent space . Furthermore , the model enforces individually disentangled latent subset to account for the given property of the dataset via invertible constraint . The presented work is evaluated on two datasets from two domains . Below I present some pros , cons , suggestions , and clarifying questions for the authors . - The paper is clear to understand . The presented experimental studies are clear and demonstrate the efficacy of the model . For instance , the precision with which the proposed model controls the property generation is impressive . Qualitatively this is demonstrated by Fig 4 . Are these demonstrated on the train or the test set ? - For both group-wise and property-wise disentanglement terms , the TC term is considered . Can the authors clarify why they claim them to be novel ? Can authors present training statistics to give insight into the model 's optimization in relation to these up-weighted TC terms ? - dSprites is considered to be a relatively simpler dataset for disentanglement . If authors wanted a dataset with ground truth generative factors in the dataset , they could consider 3DShapes ( Burgess & Kim , 2018 ) , which would have more generative factors and is more challenging than dSprites . Also , 3DShapes is a widely considered dataset beside dSprites for disentanglement related study . Burgess , C , & Kim , H. ( 2018 ) . 3D Shapes Dataset . https : //github.com/deepmind/3dshapes-dataset/ . - The literature review in the paper seems broad . I would encourage authors to discuss some recent approaches ( e.g. , Gyawali et al. , 2019 ) that disentangle latent space into two subsets with one being related to certain property of the dataset . Gyawali , P. K. , Horacek , B. M. , Sapp , J. L. , & Wang , L. ( 2019 ) . Sequential factorized autoencoder for localizing the origin of ventricular activation from 12-lead electrocardiograms . IEEE Transactions on Biomedical Engineering , 67 ( 5 ) , 1505-1516 . Minor comments : - Please proofread your manuscript carefully to avoid grammatical errors . e.g. , `` latent presentation '' .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your valuable time and insightful comments . Please find our answers point by point below . We have updated our paper based on your suggestions . The summary of all the updates in the paper is listed in a separate comment on top of the webpage . If you have any further comments/suggestions on the updated version of our paper , we will be glad to improve on them . We also sincerely hope that the revised version and responses could be helpful with an increase in the final score . # # # # Summary of updated based on comments from Reviewer 3 : 1 . We have added the experiment on a new dataset : 3Dshapes . 2.We have provided the codes of this paper in the supplemental materials . 3.We have added additional related work analysis in the second paragraph in Section 2 on Page 2 . 4.We have modified the typos and rephrased some hard-parsing sentences . * * * * * Comment # 1 : * * * The paper is clear to understand\u2026 ... Qualitatively this is demonstrated by Fig 4 . Are these demonstrated on the train or the test set ? * * * Response # 1 : * * Thanks for your comments . All the evaluation results displayed are measured based on the test set . * * * * * Comment # 2 : * * * For both group-wise and property-wise disentanglement terms , the TC term is considered . Can the authors clarify why they claim them to be novel ? Can authors present training statistics to give insight into the model 's optimization in relation to these up-weighted TC terms ? * * * Response # 2 : * * Thanks for this comment . 1.The novelty of our method is nontrivial . In our problem , we have three requirements in the disentanglement : variables in $ w $ are disentangled ( property-wise ) , $ w $ is independent of $ z $ ( group-wise ) and variables in $ z $ are not required to be disentangled . The existing TC term can not handle all of them . We propose new terms by decoupling and generalizing the existing TC term in a principled way . The proposed new terms can jointly handle group-wise and property-wise disentanglements but do not over-restrict on $ q ( z ) $ . Our model is more powerful since the trade-off among the group-wise and property-wise terms can all be adjustable , which is preferable in real applications . 2.Our model outperforms the TC term-based model ( i.e.PVCAE_tc ) in the ablation study experiments . For example , in the experiment on Q9 molecule dataset , the model with proposed terms ( i.e. , PCVAE ) achieved a better performance than TC-term based model . This validated the advantage of the group-wise and property-wise disentanglement terms over the TC term . * * * * * Comment # 3 : * * * If authors wanted a dataset with ground truth generative factors in the dataset , they could consider 3DShapes ( Burgess & Kim , 2018 ) , which would have more generative factors and is more challenging than dSprites . * * * Response # 3 : * * Thanks for this suggestion . We have followed your suggestion and already added the evaluation results on the 3DShapes dataset , as shown in Table 1 , and Figure 5 in the main text , and Figure 7 in the Appendix . * * * * * Comment # 4 : * * * I would encourage authors to discuss some recent approaches ( e.g. , Gyawali et al. , 2019 ) that disentangle latent space into two subsets with one being related to certain property of the dataset . * * * Response # 4 : * * Thanks for the recommendation . We have added the analysis of this work in the second paragraph in Section 2 as : \u201c Some researchers have addressed this problem by introducing the architecture bias through a two-way factored autoencoder and realize the supervision based on a pair-wise contrastive loss ( Gyawali et al.2019 ) . `` * * * * * Minor comments # 1 : * * * Please proofread your manuscript carefully to avoid grammatical errors . e.g. , `` latent presentation '' . * * Response : * * Thanks for this suggestion . We have fully proofread our paper and modified the typos in the revised version ."}, "3": {"review_id": "tYxG_OMs9WE-3", "review_text": "* * Property controllable variational autoencoder via invertible mutual dependence * * The paper proposes an approach for learning disentangled latent representation in the VAEs where parts of the latent space should correspond to observed properties of interest of the data so that these could be controlled at generation . They replace the total correlation regularization term of Chen2018 with more appropriate group- and property-wise KL terms promoting independence between the * relevant * ( w ) and the * remaining * ( z ) part of the latent space and inner independence between dimensions of the w part of the space ( dropping the term promoting independence in the z part . ) More importantly , they link the data property y with the latent space w by a learned mapping . The is another contribution to the rather extensive literature on disentanglement of latent representation of VAEs . It builds on previous papers in the area , mainly Chen2018 and Klys2018 . The change in the ELBO as compared to Chen2018 ( TC vs group/property-wise ) is well explained and motivated though seems rather trivial . The introduction of the invertible mapping between y and w is in my view more interesting and important . Though somewhat contrived at first reading ( some improvements in the text could help , see comments ) , it seems to correspond well to what the model shall achieve . This as is also demonstrated in the experimental section focusing mainly on the dSprites and QM9 . The paper would benefit from a more extensive experimental evaluation ( another standard dataset such as CelebA ) though I understand this may be difficult to do given the deadlines and computational resources and therefore do not consider as critical . The paper is generally well written though there are places which are difficult to parse and comprehend and suggest last minute drafting . These shall be made clear for the final version ( see comments below ) . The problem of correlated properties is addressed by a rather trivial extension of the basic model and requires the user to provide to the model groups of correlated properties . I feel this deserves a lot more attention and search for better solutions is a possible direction for future work . I recommend to accept the paper as I find it interesting for the community , addressing a lively area of research via a new approach which , though not dramatically innovative , seems to yield the desired results . I do have a few comments as to the clarity of the statements in the paper which I hope will be addressed by the authors during the rebuttal period and in the final version . * Comments / questions : * * I find calling the mapping between y and w * invertible * as misleading . The mapping $ w \\to y $ is defined as stochastic via the learned generative distribution $ p ( y | w ) $ . Each w thus corresponds to multiple possible values of $ y $ . * eq ( 2 ) : please state clearly ( in equations ) the independence and conditional independence assumptions leading to this result * eq ( 4 ) holds only in expectation over $ p ( x ) $ , right ? ( $ E_ { p ( x ) } $ in front of the DKL in the left hand side . ) * p4 before eq ( 5 ) : `` Roughly disentangling all pairs of latent variables without emphasis could lead to poor convergence and incur exponential number of pairs among properties and latent variables for such enforcement . '' What do you mean ? Please explain / elaborate / rephrase . * p4 , `` ... no strict assumptions of parameters for $ p ( y_k ) $ and $ q ( w_k |y_k ) $ '' . $ y $ does not exist in the inference model so what does $ q ( w_k |y_k ) $ refer to ? * p4 `` The most straightforward way to do this is to model both the mutual mapping between $ y_k $ and its relevant latent variable $ w_k $ . '' Both mappings $ y_k \\to w_k $ and its inverse $ w_k \\to y_k $ ? Or what do you mean ? * p5 , para5 `` Thus , we utilize the Na\u0131\u0308ve Monte Carlo approximation based on a mini-batch of samples to * * underestimate * * $ q ( z ) , q ( w ) $ and $ q ( w_k ) $ , as described by Chen et al . ( 2018 ) '' Underestimate ? What do you mean ? * p6 : you say you use the normalized mutual information between $ w_k $ and $ y_k $ . Please explain how you define this and how you calculate in practice . * p7 , fig3 `` ... when traversing three latent variables in subset z ... '' . There are only 3 latent variables z ? If more , how did you decide which shall be traversed ? * p8 , tab3 : How do you predict the property $ y_k $ from a molecule ? You first infer w and z and then use the mean of p ( y | w ) as prediction ? Or you sample y from p ( y | w ) ? Please elaborate . * p7 , tab2 : Do I read correctly that MSE of cLogP is bigger from the PCVAE ( corr ) then from the standard PCVAE ? What does this say about the advantages of PCVAE ( corr ) ? * experiments over QM9 : are all your evaluation metrics calculated only over valid molecules ? Usually , generative models for molecules are evaluated using the validity , unicity and novelty scores . It would be instructive to complement your results with these . * Minor comments / questions * * Please provide references to models displayed in Fig1 ( a ) - ( c ) . * You use the term * correletad * a lot . I 'm guessing here you mean any sort of dependence , not just linear ? * p2 : `` Directly enforcing such mutual independence inherently between all pairs of latent variables incurs exponential number of pairs among properties and latent variables for such enforcement . '' Exponential number of pairs ? What do you mean ? * p2 , para3 : `` ... as these have been shown to be relatively resilient with respect to the complex variants involved ( Bengio et al. , 2013 ) . '' Complex variants ? What do you mean ? * p3 , para2 : $ y = { y_k \\in R } _ { k=1 } ^K $ . Is this the same as $ y \\in R^K $ ? * Typos or phrasing improvements needed : * * p1 , para1 : `` * * Knowing such properties is crucial * * for many applications that depend on being able to interpret the data and control the data generation * * to yield the desired properties * * . '' ? ? ? * p2.para1 : `` Also , many cases require to generate data with properties of which the values * * are ? * * unseen during training process . '' * p3 just before eq ( 2 ) : $ \\log P_ { \u03b8 , \u03b3 } ( x , y , w , z ) $ ( capital P ? ) * p5 , para3 : `` As stated in the third challenge in Section 1 , there are usually several groups of properties involved in * * formatting * * ? data x ... '' * p6 , para5 : `` ... each encoded latent variable $ w_k $ and the property $ y_k $ * * that is ? * * assigned to it ... '' * p7 , caption Tab2 : `` ... ( PCVAE ( cor ) denotes the * * extensive * * ? model for correlated propertie ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your detailed summarization and insightful comments . Please find our answers to your comments/questions below . We have updated our paper based on your suggestions . The summary of updates in the paper are listed in a separate comment on top of the webpage . If you have any further comments/suggestions on the updated version of our paper , we will be glad to improve on them . We also sincerely hope that the revised version and responses could help with an increase in the score . * * A summary of updates based on comments from Reviewer4 : * * 1 . We have provided the codes of this paper in the supplemental materials . 2.We added the experiment on the new dataset : 3Dshapes . 3.We have modified the typos and rephrased some hard-parsing sentences . 4.We have added the references in the caption of Figure 1 . 5.We have added a clarification about invertible dependence in the first line after Equation 9 on Page 5 . 6.We added the detailed derivation of Equation 2 on Page 4 . 7.We added the annotation of expectation in Eq ( 4 ) and detailed derivation of Eq ( 4 ) in Appendix A.1 . 8.We have added the detailed description of the calculation of the density $ q ( z ) $ in Appendix B.3 9 . We have added the evaluation results on validity , unicity , and novelty of QM9 dataset in Appendix C.1 * * * * * Comment # 1 * * : * The paper would benefit from a more extensive experimental evaluation ( another standard dataset such as CelebA ) * ` * * Response # 1 : * * That \u2019 s a good point . We have added the experiment of a new dataset named \u201c 3Dshapes \u201d in the revised paper . The results are shown in Table 1 and Figure 5 . We do not select CelebA since it does not contain the annotated ground-truth labels for supervision . * * * * * Comment # 2 : * * * I find calling the mapping between $ y $ and $ w $ invertible as misleading . The mapping is defined as stochastic via the learned generative distribution . Each w thus corresponds to multiple possible values of y . * ` * * Response # 2 : * * Thanks for this suggestion . 1.In fact , it is the mapping function $ f_k ( w_k ) $ that is invertible . The stochasticity is added over function $ f_k ( w_k ) $ , via $ \\mathcal { N } ( y_k=m|f_k ( w_k ; \\gamma ) , \\sigma_k ) $ , where $ \\sigma_k $ means standard deviation , as shown in Equation 9 . We have added the clarification in the revised paper in the first line after Equation 9 on Page 5 . 2.To eliminate the misleading issue , we modified two places in the paper that mentions \u201c invertible mapping \u201d and change them into the \u201c invertible dependence \u201d in the revised version . * * * * * Comment # 3 : * * * eq ( 2 ) : please state clearly ( in equations ) the independence and conditional independence assumptions leading to this result . * * * Response # 3 : * * We have added the detailed derivation as well as the independence assumptions in equations in the revised version before Equation 2 on Page 4 as : \u201c The joint likelihood $ \\log p ( x , y , w , z ) $ can be decomposed as $ \\log p ( x , y|z , w ) +\\log p ( z , w ) $ . First , by assuming $ w $ only encode the information from $ y $ , namely , $ x $ and $ y $ are conditionally independent given $ w $ ( i.e. , $ x\\perp y|w $ ) , we can have $ \\log p ( x , y|z , w ) =\\log p ( x|z , w ) +\\log p ( y|z , w ) $ . Next.by assuming that $ z $ is independent from $ w $ and $ y $ , namely $ z \\perp w $ and $ z \\perp y $ , we can have $ \\log p ( y|z , w ) =\\log p ( y|w ) $ . Then we get $ \\log p ( x , y|z , w ) =\\log p ( x|z , w ) +\\log p ( y|w ) $ . To explicitly represent the dependence between $ x $ and $ ( z , w ) $ as well as the dependence between $ y $ and $ w $ , we can parameterize the joint log-likelihood as $ \\log p_ { \\theta , \\gamma } ( x , y , w , z ) $ with $ \\theta $ and $ \\gamma $ as : $ \\log p_ { \\theta , \\gamma } ( x , y , w , z ) =\\log p_ { \\theta } ( x|z , w ) +\\log p ( z , w ) +\\log p_ { \\gamma } ( y|w ) . $ * * * * * Comment # 4 : * * * eq ( 4 ) holds only in expectation over p ( x ) right ? ( E_ { p ( x ) } in front of the DKL in the left-hand side . ) * * * Response # 4 : * * Yes , you are right . Thanks for pointing this . We added the annotation of expectation in Eq ( 4 ) in the revised version . To make the derivation process clearer , we have also added the detailed derivation of Eq ( 3 ) and Eq ( 4 ) in Appendix A.1 . * * * * * Comment # 5 : * * * p4 before eq ( 5 ) : \u2018 Roughly disentangling all pairs of latent variables without emphasis could lead to poor convergence and incur exponential number of pairs among properties and latent variables for such enforcement. \u2019 What do you mean ? Please explain / elaborate / rephrase . * * * Response # 5 : * * ( 1 ) We revised the sentence as : \u201c Roughly enforcing the disentanglement between all pairs of latent variables in $ w $ and $ z $ , as done by the existing TC term , can incur at least quadratic number of redundant optimization efforts and could lead to poor convergence. \u201d ( 2 ) To explain it , suppose we have $ N $ variables in $ z $ and $ M $ variables in $ w $ . The traditional TC term enforces disentanglement among $ ( M+N ) ^2 $ pairs of variables , while only $ ( N^2+N ) $ pairs of variables are necessarily needed to be disentangled in our problem . Thus , the TC term includes lots of redundant enforcement components which adds much burden on the optimization ."}}