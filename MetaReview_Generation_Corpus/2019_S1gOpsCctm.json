{"year": "2019", "forum": "S1gOpsCctm", "title": "Learning Finite State Representations of Recurrent Policy Networks", "decision": "Accept (Poster)", "meta_review": "The paper addresses the problem of interpreting recurrent neural networks by quantizing their states an mapping them onto a Moore Machine. The paper presents some interesting results on reinforcement learning and other tasks. I believe the experiments could have been more informative if the proposed technique was compared against a simple quantization baseline (e.g. based on k-means) so that one can get a better understanding of the difficulty of these task.\n\nThis paper is clearly above the acceptance threshold at ICLR. ", "reviews": [{"review_id": "S1gOpsCctm-0", "review_text": "This paper proposes a method to learn a quantization of both observations and hidden states in an RNN. Its findings suggest that many problems can be reduced to relatively simple Moore Machines, even for complex environments such as Atari games. The method works by pretraing an RNN to learn a policy (e.g. through the A3C algorithm), and then training pairs of encoder/decoder networks with a quantizing forward pass and a straight-through backpropagation. The learned quantizations can then be used to build a Moore Machine, which itself can be reduced with FSM reduction algorithms, yielding a discrete, symbolic approximation of the inner workings of RNNs, that could in principle be interpreted more easily than latent embedding spaces. One downside of this paper is that it promises an exciting method to analyse the inner workings of RNNs, but then postpones this analysis to later work. Understandably, the synthetic experiments take some space and shows that the proposed method works as expected when the problem is amenable to discretization; maybe some parts of this could be in the appendix? Another downside is that there is little indication of the computational implications of the method. The method was evaluated on a fairly small set of hyperparameters, and there are no indication of how long the optimization and finetuning takes. Presumably, minimizing a Moore Machine has been studied for decades, but how long does minimizing the 1000s of states in Atari games take? A second or an hour? The paper is fairly well written and easy to understand. The method seems well grounded, although I'm not familiar enough with the quantization literature to detect if something important is missing. I think this is a great tool that hopefully will be used to try to understand the memory mechanisms of RNNs. I think the proposed method (and the fact that it works in simple cases) warrants acceptance, but I think more experimental work would make this a great contribution. Since there is no reason for quantization to improve performance if it is done after training, then more emphasis should be put on the interpretability of the discretization; yet it is lacking in the current work. Some Atari games are known to require various amounts of memory, this could be analysed. Some other Atari games are known to be hard to solve, what happens to the RNN when the agent fails to achieve an optimal policy might also show up in the subsequent discretization and be interesting to analyse. Comments: - In atari, you can have access to the RAM and from it, using exactly the same mechanisms and maybe a bit of tabular MDPs, you should be able to recover the optimal MM. - It is good that the authors report their failure to train MMNs from scratch; IMO this says something about the straight through estimators' limits. Measuring how sensible these things are to change in their target distribution and comparing to previous uses of ST in quantization works could be interesting. - in Section 8 (appendix) \"Grammer\" should be \"Grammar\" - All the (PO)MDPs that you analyse arguably have finite state spaces, and you set the ALE to be deterministic. What happens in continuous stochastic environments? - Do you think a similar technique could be used to recover a (possibly stochastic) MDP instead of a Moore Machine? It would be interesting to see MDP reduction methods applied to a learned MDP. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for the comments . Below we pull quotes from the review followed by responses . `` One downside of this paper is that it promises an exciting method to analyse the inner workings of RNNs , but then postpones this analysis to later work . '' RE : First , we do want to point out that while we were not able to fully analyze the Moore Machines resulting from Atari games , we were able to make some interesting observations about the memory use , which we 've never seen done before ( last part of Section 6 ) . In particular , some policies did not even use observations and others did not really use memory . We do n't know of any prior technique that could be employed to uncover these surprising aspects of the policies . We agree that it would have been ideal to provide a full analysis of the Atari policies using additional visualization tools . However , even without that , the work has taken the big step of showing how to create quantized representations that would allow such an analysis . It was very surprising to us that this was able to work for Atari games at all . `` little indication of the computational implications of the method \u2026 how long does minimizing 1000s of states in Atari games take '' RE : We will provide some information on optimization times in the revised paper ( Will be uploaded soon . ) We can say now that training the quantized autoencoders + fine tuning is faster than training a Moore Machine Network from scratch ( which failed for Atari games ) and much faster than training the original RNN policy . Minimization is quite fast . For the largest numbers of states in Atari the minimization took a couple of minutes , noting that this was highly unoptimized minimization code . The synthetic problems took seconds to minimize . `` In atari , you can have access to the RAM and from it , using exactly the same mechanisms and maybe a bit of tabular MDPs , you should be able to recover the optimal MM . '' RE : Not sure what you have in mind here . It seems unlikely that we would be able to get an optimal MM for an Atari game even with the use of RAM . The problem is just too large to solve optimally . `` All the ( PO ) MDPs that you analyse arguably have finite state spaces , and you set the ALE to be deterministic . What happens in continuous stochastic environments ? '' RE : The POMDPs we use actually have continuous observations ( but yes finite states ) with some stochastic behavior in the observation generation . We have n't used policies trained on stochastic variants of ALE , but agree this is an interesting direction to consider . `` Do you think a similar technique could be used to recover a ( possibly stochastic ) MDP instead of a Moore Machine ? It would be interesting to see MDP reduction methods applied to a learned MDP . '' RE : This is an interesting idea and it does seem possible . The `` bottleneck insertion '' approach is quite general and can be plugged into a network at any point where quantization seems useful to introduce ."}, {"review_id": "S1gOpsCctm-1", "review_text": "RNNs are difficult to explain, understand and analyze due to the continuous-valued memory vectors and observations features they use. Thus, this paper attempts to extract finite representation from RNNs so as to better interpret or understand RNNs. They introduce a new technique called Quantized Bottleneck Insertion to extract Moore Machines (MM). The extracted MM can be analyzed to improve the understanding of memory use and general behavior on the policies. The experiments on synthetic datasets and six Atari games validate the effectiveness of the proposal. Here are my detailed comments: Interpreting or understanding RNNs is a very interesting and important topic since RNNs and their variants like LSTM, GRU are widely used in different domains such as reinforcement learning, sentiment analysis, stock market prediction, natural language processing, etc. The more understandable on RNNs, the more trustful on them. In this paper, the authors try to extract more interpretable representation of RNNs, namely Moore Machines (MM). MM is actually a classical finite state automaton. The authors mention that (Zeng et al., 1993) is the most similar work to theirs. In fact a series of works have been proposed to extract finite state automaton, which is similar to (Zeng et al., 1993) such as [1], [2], [3], etc. I think the authors could make the related works more complete by incorporating these literatures I mentioned. Besides, I think this work is a good application of the idea of extraction of RNNs on reinforcement learning since no works have introduced this idea into this domain as far as I know. The authors use the autoencoder named as QBN to quantize the space of hidden states. This is a good operation of clustering or quantizing the space of hidden states since it can be tuned to make the final performance better. The authors also incorporate the minimization of MM to show the probability of shrinking memory which can also make the extracted MM more interpretable. As a result, the policy represented by MM is intuitive and vivid. Nevertheless, there is an obvious weak point in this paper. Specifically, the authors claim that the main contribution of this paper is to introduce an approach for transforming RNNs to finite state representations. But I do not see any comparisons between the proposed methods and other relative methods such as the method proposed by (Zeng et al., 1993) to show the effectiveness or improvement of the proposed method. I suggest the authors could incorporate comparisons to make the results more convincing. [1] C. W. Omlin and C. L. Giles, \"Extraction of rules from discrete-time recurrent neural networks,\" Neural Networks, vol. 9, no. 1, pp. 41\u201352, 1996. [2] C. W. Omlin and C. L. Giles, \"Constructing deterministic finite-state automata in recurrent neural networks,\" Journal of the ACM, vol. 43, no. 6, pp. 937-972, 1996. [3] A. Cleeremans, D. Servan-Schreiber, and J. L. McClelland. \"Finite state automata and simple recurrent networks.\" Neural computation, vol. 1, no. 3, pp. 372-381, 1989.", "rating": "7: Good paper, accept", "reply_text": "Thanks for the comments . Below we pull quotes from the review followed by responses . `` the authors could make the related works more complete by incorporating these literatures I mentioned . '' RE : Indeed the literature on FSM extraction is quite vast and we tried to include representative papers from the different classes of approaches . We will be happy to include the papers you pointed to , noting that these are just a few from this class of approaches that have appeared over the years . `` obvious weak point \u2026 do not see any comparisons between the proposed methods and other relative methods such as the method proposed by ( Zeng et al. , 1993 ) '' RE : As we mentioned in the related work , there is no prior work that we are aware of that attempts to learn to transform RNNs into Moore Machines . We would be happy to get pointers to related work that we can compare with . We included a discussion of work on learning FSMs in the related work , because those techniques are related to our problem . But NONE of the approaches that we are aware of can be applied to our problems without significant innovation . This is due to two reasons : 1 ) Our inputs are complex objects ( images or real numbers ) compared to FSM learning where the inputs are from a discrete alphabet , and 2 ) FSMs are different from Moore Machines , since Moore Machines must output an action/symbol at each time step , rather than just accepting/rejecting entire strings as is the case for FSMs . So FSM approaches are not directly applicable . For the Grammar Learning benchmarks , prior FSM methods can apply ( since actions are just accept/reject ) . However , here we achieve nearly perfect performance , so a comparison would not shed additional light ."}, {"review_id": "S1gOpsCctm-2", "review_text": "Approximation of RNNs is a hot and important topic in term of interpretability and control of nets. The related work section is good but in my opinion miss to give a position with respect to the work dedicated to extract rules from a net which are also way to \"interpret\" a RNNs - as an example https://arxiv.org/abs/1702.02540 from ICLR'17. pros: - important practical topic - The papers includes a variety of ideas/tricks which seems to bring performance as the 3 stage procedure and the gradient backpropagation over quantization. - Makes \"interpretable\" observations of some no so easy to understand nets on Atari games - Reach state of the art performance on artificial set of task cons: - The impact of each step is not always assessed by an experiment (especially ones introduced in section 4.1) - The method is never benchmarked against an other one. Neither in terms of performance of the approximation nor in terms of interpretability (thought other techniques are cited in the paper). I understand that this is because this pursue the two goals at the same time but I'd be interested this tradeoff to be more investigated. - Performance on Atari games is usually reported in term of % wrt human performance which helps understanding where we stand. It would be good also to discuss the performance of the RNN on the game wrt other nets. As an example in this paper on space invaders the performance of the RNN is slightly better human but very far from state of the art yielded by prioritized duelling which is almost 10x higher in terms of score. While on breakout they are very good (see https://arxiv.org/pdf/1806.06923.pdf to have a recent list of score on Atari). - I'd been interested in having an artificial task where to proposed algorithm does not succeed (an ideally some discussion on what make the structure recoverable or not). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your time and comments . Below we pull quotes from the review followed by responses . `` miss to give a position with respect to the work dedicated to extract rules \u2026 . example https : //arxiv.org/abs/1702.02540 '' RE : We will add rule-extraction techniques as a related class of methods to related work . ( Revision will be coming . ) However , we have not seen a rule-extraction approach that can be easily adapted to our target problems where the recurrent policies consume complex inputs such as images ( Atari experiments ) or real-valued inputs ( mode counter experiments ) . So while related they are not really competing approaches without significant innovation . `` Impact of each step is not always assessed \u2026 . especially ones introduced in section 4.1 '' RE : The two choices not assessed were : 1 ) The level of quantization , where we use three levels { -1 , 0 , 1 } , and 2 ) The impact of using the `` flattened tanh '' function . For ( 1 ) we have not done an in depth comparison of the impact of # of quantization levels . This is primarily due to the expense of producing the comparison . As mentioned in the paper , this is an interesting point to explore in the future , but was not really central to our main goal of demonstrating the potential for this new approach . For ( 2 ) during early experimentation we did some comparison to using pure tanh and observed a small advantage to using the flattened version . At that point , due to experimental cost , we needed to stick to one or the other and chose the flattened tanh . We may be able to include limited comparisons in the revision , but it will need to be in the appendix for space reasons . `` never benchmarked against an other one . Neither in terms of performance of the approximation nor in terms of interpretability '' RE : As we mentioned in the related work , there is no prior work that we are aware of that attempts to learn to transform RNNs into Moore Machines . We would be happy to get pointers to related work that we can compare with . We included a discussion of work on learning FSMs in the related work , because those techniques are related to our problem . But NONE of the approaches that we are aware of can be applied to our problems without significant innovation . This is due to two reasons : 1 ) Our inputs are complex objects ( images or real numbers ) compared to FSM learning where the inputs are from a discrete alphabet , and 2 ) FSMs are different from Moore Machines , since Moore Machines must output an action/symbol at each time step , rather than just accepting/rejecting entire strings as is the case for FSMs . So FSM approaches are not directly applicable . For the Grammar Learning benchmarks , prior FSM methods can apply ( since actions are just accept/reject ) . However , here we achieve nearly perfect performance , so a comparison would not shed additional light . `` Performance on Atari games is usually reported in term of % wrt human performance \u2026 discuss the performance of the RNN on the game wrt other nets '' RE : It is important to recall the primary goals of this paper . We are NOT trying to train the best Atari playing policies . Rather , our aim is to study how to create finite state representations for problems as complex as Atari games . In this sense , normalizing scores and/or comparing performance to other Atari playing policies is orthogonal to our goals . Rather it is primarily important to indicate that we are dealing with policies that are achieving reasonable performance . To address your concern and give a point of reference for our policy qualities , we will report the Nature DQN and current SOTA scores for those games in the next draft . Although some researchers tend to report scores w.r.t.human baselines , there is a fair bit of disagreement about where those baselines should lie ( Figure 1 in http : //gershmanlab.webfactional.com/pubs/Tsividis17.pdf seems to indicate , for example , that the DeepMind baselines are too low ) . So we prefer just to report raw scores for now . `` interested in an artificial task where the algorithm does not succeed '' RE : The third paragraph on page 9 gives 2 examples from Atari where the approach results in a decrease in performance after discretization . We give our best explanation for why this happens there . We agree that it will be interesting future work to design classes of artificial problems , where different complexity parameters can be modified for testing the limits of our approach ."}], "0": {"review_id": "S1gOpsCctm-0", "review_text": "This paper proposes a method to learn a quantization of both observations and hidden states in an RNN. Its findings suggest that many problems can be reduced to relatively simple Moore Machines, even for complex environments such as Atari games. The method works by pretraing an RNN to learn a policy (e.g. through the A3C algorithm), and then training pairs of encoder/decoder networks with a quantizing forward pass and a straight-through backpropagation. The learned quantizations can then be used to build a Moore Machine, which itself can be reduced with FSM reduction algorithms, yielding a discrete, symbolic approximation of the inner workings of RNNs, that could in principle be interpreted more easily than latent embedding spaces. One downside of this paper is that it promises an exciting method to analyse the inner workings of RNNs, but then postpones this analysis to later work. Understandably, the synthetic experiments take some space and shows that the proposed method works as expected when the problem is amenable to discretization; maybe some parts of this could be in the appendix? Another downside is that there is little indication of the computational implications of the method. The method was evaluated on a fairly small set of hyperparameters, and there are no indication of how long the optimization and finetuning takes. Presumably, minimizing a Moore Machine has been studied for decades, but how long does minimizing the 1000s of states in Atari games take? A second or an hour? The paper is fairly well written and easy to understand. The method seems well grounded, although I'm not familiar enough with the quantization literature to detect if something important is missing. I think this is a great tool that hopefully will be used to try to understand the memory mechanisms of RNNs. I think the proposed method (and the fact that it works in simple cases) warrants acceptance, but I think more experimental work would make this a great contribution. Since there is no reason for quantization to improve performance if it is done after training, then more emphasis should be put on the interpretability of the discretization; yet it is lacking in the current work. Some Atari games are known to require various amounts of memory, this could be analysed. Some other Atari games are known to be hard to solve, what happens to the RNN when the agent fails to achieve an optimal policy might also show up in the subsequent discretization and be interesting to analyse. Comments: - In atari, you can have access to the RAM and from it, using exactly the same mechanisms and maybe a bit of tabular MDPs, you should be able to recover the optimal MM. - It is good that the authors report their failure to train MMNs from scratch; IMO this says something about the straight through estimators' limits. Measuring how sensible these things are to change in their target distribution and comparing to previous uses of ST in quantization works could be interesting. - in Section 8 (appendix) \"Grammer\" should be \"Grammar\" - All the (PO)MDPs that you analyse arguably have finite state spaces, and you set the ALE to be deterministic. What happens in continuous stochastic environments? - Do you think a similar technique could be used to recover a (possibly stochastic) MDP instead of a Moore Machine? It would be interesting to see MDP reduction methods applied to a learned MDP. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for the comments . Below we pull quotes from the review followed by responses . `` One downside of this paper is that it promises an exciting method to analyse the inner workings of RNNs , but then postpones this analysis to later work . '' RE : First , we do want to point out that while we were not able to fully analyze the Moore Machines resulting from Atari games , we were able to make some interesting observations about the memory use , which we 've never seen done before ( last part of Section 6 ) . In particular , some policies did not even use observations and others did not really use memory . We do n't know of any prior technique that could be employed to uncover these surprising aspects of the policies . We agree that it would have been ideal to provide a full analysis of the Atari policies using additional visualization tools . However , even without that , the work has taken the big step of showing how to create quantized representations that would allow such an analysis . It was very surprising to us that this was able to work for Atari games at all . `` little indication of the computational implications of the method \u2026 how long does minimizing 1000s of states in Atari games take '' RE : We will provide some information on optimization times in the revised paper ( Will be uploaded soon . ) We can say now that training the quantized autoencoders + fine tuning is faster than training a Moore Machine Network from scratch ( which failed for Atari games ) and much faster than training the original RNN policy . Minimization is quite fast . For the largest numbers of states in Atari the minimization took a couple of minutes , noting that this was highly unoptimized minimization code . The synthetic problems took seconds to minimize . `` In atari , you can have access to the RAM and from it , using exactly the same mechanisms and maybe a bit of tabular MDPs , you should be able to recover the optimal MM . '' RE : Not sure what you have in mind here . It seems unlikely that we would be able to get an optimal MM for an Atari game even with the use of RAM . The problem is just too large to solve optimally . `` All the ( PO ) MDPs that you analyse arguably have finite state spaces , and you set the ALE to be deterministic . What happens in continuous stochastic environments ? '' RE : The POMDPs we use actually have continuous observations ( but yes finite states ) with some stochastic behavior in the observation generation . We have n't used policies trained on stochastic variants of ALE , but agree this is an interesting direction to consider . `` Do you think a similar technique could be used to recover a ( possibly stochastic ) MDP instead of a Moore Machine ? It would be interesting to see MDP reduction methods applied to a learned MDP . '' RE : This is an interesting idea and it does seem possible . The `` bottleneck insertion '' approach is quite general and can be plugged into a network at any point where quantization seems useful to introduce ."}, "1": {"review_id": "S1gOpsCctm-1", "review_text": "RNNs are difficult to explain, understand and analyze due to the continuous-valued memory vectors and observations features they use. Thus, this paper attempts to extract finite representation from RNNs so as to better interpret or understand RNNs. They introduce a new technique called Quantized Bottleneck Insertion to extract Moore Machines (MM). The extracted MM can be analyzed to improve the understanding of memory use and general behavior on the policies. The experiments on synthetic datasets and six Atari games validate the effectiveness of the proposal. Here are my detailed comments: Interpreting or understanding RNNs is a very interesting and important topic since RNNs and their variants like LSTM, GRU are widely used in different domains such as reinforcement learning, sentiment analysis, stock market prediction, natural language processing, etc. The more understandable on RNNs, the more trustful on them. In this paper, the authors try to extract more interpretable representation of RNNs, namely Moore Machines (MM). MM is actually a classical finite state automaton. The authors mention that (Zeng et al., 1993) is the most similar work to theirs. In fact a series of works have been proposed to extract finite state automaton, which is similar to (Zeng et al., 1993) such as [1], [2], [3], etc. I think the authors could make the related works more complete by incorporating these literatures I mentioned. Besides, I think this work is a good application of the idea of extraction of RNNs on reinforcement learning since no works have introduced this idea into this domain as far as I know. The authors use the autoencoder named as QBN to quantize the space of hidden states. This is a good operation of clustering or quantizing the space of hidden states since it can be tuned to make the final performance better. The authors also incorporate the minimization of MM to show the probability of shrinking memory which can also make the extracted MM more interpretable. As a result, the policy represented by MM is intuitive and vivid. Nevertheless, there is an obvious weak point in this paper. Specifically, the authors claim that the main contribution of this paper is to introduce an approach for transforming RNNs to finite state representations. But I do not see any comparisons between the proposed methods and other relative methods such as the method proposed by (Zeng et al., 1993) to show the effectiveness or improvement of the proposed method. I suggest the authors could incorporate comparisons to make the results more convincing. [1] C. W. Omlin and C. L. Giles, \"Extraction of rules from discrete-time recurrent neural networks,\" Neural Networks, vol. 9, no. 1, pp. 41\u201352, 1996. [2] C. W. Omlin and C. L. Giles, \"Constructing deterministic finite-state automata in recurrent neural networks,\" Journal of the ACM, vol. 43, no. 6, pp. 937-972, 1996. [3] A. Cleeremans, D. Servan-Schreiber, and J. L. McClelland. \"Finite state automata and simple recurrent networks.\" Neural computation, vol. 1, no. 3, pp. 372-381, 1989.", "rating": "7: Good paper, accept", "reply_text": "Thanks for the comments . Below we pull quotes from the review followed by responses . `` the authors could make the related works more complete by incorporating these literatures I mentioned . '' RE : Indeed the literature on FSM extraction is quite vast and we tried to include representative papers from the different classes of approaches . We will be happy to include the papers you pointed to , noting that these are just a few from this class of approaches that have appeared over the years . `` obvious weak point \u2026 do not see any comparisons between the proposed methods and other relative methods such as the method proposed by ( Zeng et al. , 1993 ) '' RE : As we mentioned in the related work , there is no prior work that we are aware of that attempts to learn to transform RNNs into Moore Machines . We would be happy to get pointers to related work that we can compare with . We included a discussion of work on learning FSMs in the related work , because those techniques are related to our problem . But NONE of the approaches that we are aware of can be applied to our problems without significant innovation . This is due to two reasons : 1 ) Our inputs are complex objects ( images or real numbers ) compared to FSM learning where the inputs are from a discrete alphabet , and 2 ) FSMs are different from Moore Machines , since Moore Machines must output an action/symbol at each time step , rather than just accepting/rejecting entire strings as is the case for FSMs . So FSM approaches are not directly applicable . For the Grammar Learning benchmarks , prior FSM methods can apply ( since actions are just accept/reject ) . However , here we achieve nearly perfect performance , so a comparison would not shed additional light ."}, "2": {"review_id": "S1gOpsCctm-2", "review_text": "Approximation of RNNs is a hot and important topic in term of interpretability and control of nets. The related work section is good but in my opinion miss to give a position with respect to the work dedicated to extract rules from a net which are also way to \"interpret\" a RNNs - as an example https://arxiv.org/abs/1702.02540 from ICLR'17. pros: - important practical topic - The papers includes a variety of ideas/tricks which seems to bring performance as the 3 stage procedure and the gradient backpropagation over quantization. - Makes \"interpretable\" observations of some no so easy to understand nets on Atari games - Reach state of the art performance on artificial set of task cons: - The impact of each step is not always assessed by an experiment (especially ones introduced in section 4.1) - The method is never benchmarked against an other one. Neither in terms of performance of the approximation nor in terms of interpretability (thought other techniques are cited in the paper). I understand that this is because this pursue the two goals at the same time but I'd be interested this tradeoff to be more investigated. - Performance on Atari games is usually reported in term of % wrt human performance which helps understanding where we stand. It would be good also to discuss the performance of the RNN on the game wrt other nets. As an example in this paper on space invaders the performance of the RNN is slightly better human but very far from state of the art yielded by prioritized duelling which is almost 10x higher in terms of score. While on breakout they are very good (see https://arxiv.org/pdf/1806.06923.pdf to have a recent list of score on Atari). - I'd been interested in having an artificial task where to proposed algorithm does not succeed (an ideally some discussion on what make the structure recoverable or not). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your time and comments . Below we pull quotes from the review followed by responses . `` miss to give a position with respect to the work dedicated to extract rules \u2026 . example https : //arxiv.org/abs/1702.02540 '' RE : We will add rule-extraction techniques as a related class of methods to related work . ( Revision will be coming . ) However , we have not seen a rule-extraction approach that can be easily adapted to our target problems where the recurrent policies consume complex inputs such as images ( Atari experiments ) or real-valued inputs ( mode counter experiments ) . So while related they are not really competing approaches without significant innovation . `` Impact of each step is not always assessed \u2026 . especially ones introduced in section 4.1 '' RE : The two choices not assessed were : 1 ) The level of quantization , where we use three levels { -1 , 0 , 1 } , and 2 ) The impact of using the `` flattened tanh '' function . For ( 1 ) we have not done an in depth comparison of the impact of # of quantization levels . This is primarily due to the expense of producing the comparison . As mentioned in the paper , this is an interesting point to explore in the future , but was not really central to our main goal of demonstrating the potential for this new approach . For ( 2 ) during early experimentation we did some comparison to using pure tanh and observed a small advantage to using the flattened version . At that point , due to experimental cost , we needed to stick to one or the other and chose the flattened tanh . We may be able to include limited comparisons in the revision , but it will need to be in the appendix for space reasons . `` never benchmarked against an other one . Neither in terms of performance of the approximation nor in terms of interpretability '' RE : As we mentioned in the related work , there is no prior work that we are aware of that attempts to learn to transform RNNs into Moore Machines . We would be happy to get pointers to related work that we can compare with . We included a discussion of work on learning FSMs in the related work , because those techniques are related to our problem . But NONE of the approaches that we are aware of can be applied to our problems without significant innovation . This is due to two reasons : 1 ) Our inputs are complex objects ( images or real numbers ) compared to FSM learning where the inputs are from a discrete alphabet , and 2 ) FSMs are different from Moore Machines , since Moore Machines must output an action/symbol at each time step , rather than just accepting/rejecting entire strings as is the case for FSMs . So FSM approaches are not directly applicable . For the Grammar Learning benchmarks , prior FSM methods can apply ( since actions are just accept/reject ) . However , here we achieve nearly perfect performance , so a comparison would not shed additional light . `` Performance on Atari games is usually reported in term of % wrt human performance \u2026 discuss the performance of the RNN on the game wrt other nets '' RE : It is important to recall the primary goals of this paper . We are NOT trying to train the best Atari playing policies . Rather , our aim is to study how to create finite state representations for problems as complex as Atari games . In this sense , normalizing scores and/or comparing performance to other Atari playing policies is orthogonal to our goals . Rather it is primarily important to indicate that we are dealing with policies that are achieving reasonable performance . To address your concern and give a point of reference for our policy qualities , we will report the Nature DQN and current SOTA scores for those games in the next draft . Although some researchers tend to report scores w.r.t.human baselines , there is a fair bit of disagreement about where those baselines should lie ( Figure 1 in http : //gershmanlab.webfactional.com/pubs/Tsividis17.pdf seems to indicate , for example , that the DeepMind baselines are too low ) . So we prefer just to report raw scores for now . `` interested in an artificial task where the algorithm does not succeed '' RE : The third paragraph on page 9 gives 2 examples from Atari where the approach results in a decrease in performance after discretization . We give our best explanation for why this happens there . We agree that it will be interesting future work to design classes of artificial problems , where different complexity parameters can be modified for testing the limits of our approach ."}}