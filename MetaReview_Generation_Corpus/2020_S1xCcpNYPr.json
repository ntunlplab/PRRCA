{"year": "2020", "forum": "S1xCcpNYPr", "title": "Cost-Effective Testing of a Deep Learning Model through Input Reduction", "decision": "Reject", "meta_review": "This paper presents a method which creates a representative subset of testing examples so that the model can be tested quickly during the training. The procedure makes use of the famous HGS selection algorithm which identifies and then eliminates the redundant and obsolete test cases based on two criteria: (1) structural coverage as measured by the number of neurons activated beyond a certain threshold, and (2) distribution mismatch (as measured by KL divergence) of the last layer activations. The algorithm has two-phases: (1) a greedy subset selection based on the coverage, and (2) an iterative phase were additional test examples are added until the KL divergence (as defined above) falls below some threshold. \nThis approach is incremental in nature -- the resulting multi-objective optimisation problem is not a significant improvement over BOT. After the discussion phase, we believe that the advantages over BOT were not clearly demonstrated and that the main drawback of BOT (requiring the number of samples) is not hindering practical applications. Finally, the empirical evaluation is performed on very small data sets and I do not see an efficient way to apply it to larger data sets where this reduction could be significant. Hence, I will recommend the rejection of this paper. To merit acceptance to ICLR the authors need to provide a cleaner presentation (especially of the algorithms), with a focus on the incremental improvements over BOT, an empirical analysis on larger datasets, and a detailed look into the computational aspects of the proposed approach.\n", "reviews": [{"review_id": "S1xCcpNYPr-0", "review_text": "The paper presents a new approach to create subsets of the testing examples that are representative of the entire test set so that the model can be tested quickly during the training and leaving the check on the full test set only at the end to validate its validity. The key idea is to create the smaller possible subset with the same or similar coverage (in the paper the neurons coverage is considered) and output distribution, maintaining the difference below a (small) threshold. In particular, the output distribution is approximated by dividing the output range of each neuron into K intervals. In this way, an estimate of the output distribution is considered during the extraction of the representative subset of the testing data. The whole process is divided into two phases, the first is to create a first subset using HGS, the second refines this subset in order to achieve the desired precision in the output distribution. The paper also presents a good experimental campaign that shows good performances. The paper is really well written and enjoyable, clear in its description and in the objectives it aims to achieve. To improve the readability a bit further, I would suggest trying to move equation 1 after or close to its reference, or at least to describe before it what KL is. Moreover, a similar problem is present for TI, which is introduced in Algorithm 1 and described later. I would add, for example in the input section of the algorithm, a sentence stating that TI is used in getCandidate and described later. The first sentence on page 5 seems to be incomplete as it is written. I would suggest rephrasing the sentence. In Algorithm 2, about the two \"for\" cycles for i in 1,m and foreach k in 2,K, I suggest unifying them and use the same cycle (only for reasons of readability). Moreover, I think that using braces instead of parentheses would be more correct in these cycles. It is interesting to note that in the first line for VGG19, last column of Table 3, the accuracy is lower with KL < 0.001 than with KL < 0.005. I would expect monotonicity here. Do the authors have any idea of the reasons for this? Finally, it would be interesting to know the runtime to obtain the subsets of the test data of Table 2 required by the considered systems. Typos: On page 6, penultimate paragraph, the word \"the\" is repeated twice in the parentheses. On page 8, at the beginning of the first sentence after Table 3, there is a comma that seems to be useless after the word \"that\". On page 8, \"When the termination crite gets stricter\", crite should be corrected in criterion. There is a typo in the README of the github project linked in the paper: \"coveraeg data\" instead of \"coverage data\".", "rating": "8: Accept", "reply_text": "We thank the reviewer for your helpful and encouraging comments . We have addressed the comments and revised the paper . Please see the detailed replies below : 1 . \u2019 ' ... the accuracy is lower with KL < 0.001 than with KL < 0.005 . I would expect monotonicity here . ' Response : We agree that there is monotonicity among the results with different KL values . The monotonicity can be generally observed in Table 3 . The only exception is on VGG19 when KL < 0.005 and NC=0.25 . This is because our approach is a greedy heuristic , the monotonicity may not be very strict when the absolute difference in accuracy is small . For VGG19 , when KL < 0.005 and NC=0.25 , the absolute difference in accuracy is less than 0.004 , which could be caused by just one or two test data points . 2.About the runtime to obtain the subsets of the test data of Table 2 Response : The runtime to obtain the subsets varies for different models . For example , the runtime for three LeNet models ranges from 8.93 to 16.27 seconds . More details will be given on our website . 3.About the presentation and typos : Response : Thanks for your suggestions ! We have improved the presentation of the paper and fixed the typos ."}, {"review_id": "S1xCcpNYPr-1", "review_text": "The paper develops methods to reduce the test data size while maintaining the coverage and the effectiveness on large test data. The paper proposes a two-phase reduction approach to select representative samples based on heuristics. Extensive experiments have shown the proposed methods can reduce 95% test samples while still obtaining similar measurements. The paper targets a very important problem in practice. Effectively selecting small, representative test sets can save many computational resources and greatly accelerate the research and development. Although the developed technique is quite simple, they are meaningful in practice. Overall, the work can be much improved if a theoretical framework is proposed. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for your helpful and encouraging comments . Please see the detailed replies below : 1 . Regarding the comment on theoretical framework . Response : Thanks for your valuable suggestion ! As you point out , our paper investigates the input reduction problem from a practical perspective . It would be interesting to investigate the theoretical framework in our future work ."}, {"review_id": "S1xCcpNYPr-2", "review_text": "This work tries to build a sub-pile of the test data to save the testing time with minimum effect on the test adequacy and the output distribution. In this paper, the work is done by adding a test-sample search algorithm on top of the HGS algorithm to balance the output distribution. However, the novelty of the proposed work is limited, and there is no evidence to show the proposed algorithms can be applied to other related works. Furthermore, the result does not present a strong success: the error of output distribution is much worse than the compared work. Other comments to the proposed manuscript are: 1. In Definition 1 the authors declare that the goal is to satisfy f(T,M)=f(T\u2019M) and g(T,M)=g(T\u2019M), and then in the following paragraphs they change it to f(T,M)\u2248f(T\u2019M) and g(T,M)\u2248g(T\u2019M) with no justification. More explanation is needed. 2. In Table 2, the authors try to compare the output distribution. To better demonstrate the change between raw testing set and proposed subset, I think that it could be better to present the metrics of distribution or the accuracy of each class instead. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for your valuable comments . The details of our response are as follows : 1 . Regarding the comment on the novelty of the proposed work . Response : We believe that our work is significant and novel . The novelty can be summarized as follows : ( 1 ) Our work is the first work to define multi-objective input reduction problem in DL testing , and the first work to reduce the cost of DL testing by satisfying three objectives ( efficiency , completeness , and effectiveness ) . Related work such as BOT focuses on single-objective ( effectiveness ) and does not ensure testing completeness . ( 2 ) We propose a two-phase reduction algorithm , which is a new approach to input reduction . The two-phase algorithm combines three objectives by using the HGS algorithm and a carefully designed heuristic . Our approach is a meaningful approach in practice , which is also accepted by another reviewer ( # 2 ) . ( 3 ) An extensive evaluation of the proposed approach , including non-regression and regression scenarios . The latter is a more practical scenario in the development process . Our approach can work well in both scenarios . 2.Regarding the applicability of the proposed algorithms to other related works Response : Our algorithm is general and can be easily adapted to other related work ( e.g. , other criteria and DL models ) . In our work , we use neuron coverage and the outputs of the last hidden layer as the two inputs . However , our algorithm is not specific to the neuron coverage criterion , and can support various other coverage criteria . That is , if the coverage of a DL model can be obtained , the first phase ( HGS ) of our algorithm can also be applied to it . Besides , for most of the DL models , the outputs of the last hidden layer ( or the last few hidden layers ) are numerical values , indicating that the latter phase of our algorithm can be applied to other DL models too . To sum up , our algorithm can be easily applied to other work as well . Note that there are some other related works targeting the cost problem in DL ( e.g. , those described in Section 2 ) . Our approach is proposed to solve the input reduction problem in DL testing , and we do not claim that our approach can be applied to the work described in Section 2 , as these work are quite different from our work . However , our approach and these work can complement each other in order to reduce DL costs ."}], "0": {"review_id": "S1xCcpNYPr-0", "review_text": "The paper presents a new approach to create subsets of the testing examples that are representative of the entire test set so that the model can be tested quickly during the training and leaving the check on the full test set only at the end to validate its validity. The key idea is to create the smaller possible subset with the same or similar coverage (in the paper the neurons coverage is considered) and output distribution, maintaining the difference below a (small) threshold. In particular, the output distribution is approximated by dividing the output range of each neuron into K intervals. In this way, an estimate of the output distribution is considered during the extraction of the representative subset of the testing data. The whole process is divided into two phases, the first is to create a first subset using HGS, the second refines this subset in order to achieve the desired precision in the output distribution. The paper also presents a good experimental campaign that shows good performances. The paper is really well written and enjoyable, clear in its description and in the objectives it aims to achieve. To improve the readability a bit further, I would suggest trying to move equation 1 after or close to its reference, or at least to describe before it what KL is. Moreover, a similar problem is present for TI, which is introduced in Algorithm 1 and described later. I would add, for example in the input section of the algorithm, a sentence stating that TI is used in getCandidate and described later. The first sentence on page 5 seems to be incomplete as it is written. I would suggest rephrasing the sentence. In Algorithm 2, about the two \"for\" cycles for i in 1,m and foreach k in 2,K, I suggest unifying them and use the same cycle (only for reasons of readability). Moreover, I think that using braces instead of parentheses would be more correct in these cycles. It is interesting to note that in the first line for VGG19, last column of Table 3, the accuracy is lower with KL < 0.001 than with KL < 0.005. I would expect monotonicity here. Do the authors have any idea of the reasons for this? Finally, it would be interesting to know the runtime to obtain the subsets of the test data of Table 2 required by the considered systems. Typos: On page 6, penultimate paragraph, the word \"the\" is repeated twice in the parentheses. On page 8, at the beginning of the first sentence after Table 3, there is a comma that seems to be useless after the word \"that\". On page 8, \"When the termination crite gets stricter\", crite should be corrected in criterion. There is a typo in the README of the github project linked in the paper: \"coveraeg data\" instead of \"coverage data\".", "rating": "8: Accept", "reply_text": "We thank the reviewer for your helpful and encouraging comments . We have addressed the comments and revised the paper . Please see the detailed replies below : 1 . \u2019 ' ... the accuracy is lower with KL < 0.001 than with KL < 0.005 . I would expect monotonicity here . ' Response : We agree that there is monotonicity among the results with different KL values . The monotonicity can be generally observed in Table 3 . The only exception is on VGG19 when KL < 0.005 and NC=0.25 . This is because our approach is a greedy heuristic , the monotonicity may not be very strict when the absolute difference in accuracy is small . For VGG19 , when KL < 0.005 and NC=0.25 , the absolute difference in accuracy is less than 0.004 , which could be caused by just one or two test data points . 2.About the runtime to obtain the subsets of the test data of Table 2 Response : The runtime to obtain the subsets varies for different models . For example , the runtime for three LeNet models ranges from 8.93 to 16.27 seconds . More details will be given on our website . 3.About the presentation and typos : Response : Thanks for your suggestions ! We have improved the presentation of the paper and fixed the typos ."}, "1": {"review_id": "S1xCcpNYPr-1", "review_text": "The paper develops methods to reduce the test data size while maintaining the coverage and the effectiveness on large test data. The paper proposes a two-phase reduction approach to select representative samples based on heuristics. Extensive experiments have shown the proposed methods can reduce 95% test samples while still obtaining similar measurements. The paper targets a very important problem in practice. Effectively selecting small, representative test sets can save many computational resources and greatly accelerate the research and development. Although the developed technique is quite simple, they are meaningful in practice. Overall, the work can be much improved if a theoretical framework is proposed. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for your helpful and encouraging comments . Please see the detailed replies below : 1 . Regarding the comment on theoretical framework . Response : Thanks for your valuable suggestion ! As you point out , our paper investigates the input reduction problem from a practical perspective . It would be interesting to investigate the theoretical framework in our future work ."}, "2": {"review_id": "S1xCcpNYPr-2", "review_text": "This work tries to build a sub-pile of the test data to save the testing time with minimum effect on the test adequacy and the output distribution. In this paper, the work is done by adding a test-sample search algorithm on top of the HGS algorithm to balance the output distribution. However, the novelty of the proposed work is limited, and there is no evidence to show the proposed algorithms can be applied to other related works. Furthermore, the result does not present a strong success: the error of output distribution is much worse than the compared work. Other comments to the proposed manuscript are: 1. In Definition 1 the authors declare that the goal is to satisfy f(T,M)=f(T\u2019M) and g(T,M)=g(T\u2019M), and then in the following paragraphs they change it to f(T,M)\u2248f(T\u2019M) and g(T,M)\u2248g(T\u2019M) with no justification. More explanation is needed. 2. In Table 2, the authors try to compare the output distribution. To better demonstrate the change between raw testing set and proposed subset, I think that it could be better to present the metrics of distribution or the accuracy of each class instead. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for your valuable comments . The details of our response are as follows : 1 . Regarding the comment on the novelty of the proposed work . Response : We believe that our work is significant and novel . The novelty can be summarized as follows : ( 1 ) Our work is the first work to define multi-objective input reduction problem in DL testing , and the first work to reduce the cost of DL testing by satisfying three objectives ( efficiency , completeness , and effectiveness ) . Related work such as BOT focuses on single-objective ( effectiveness ) and does not ensure testing completeness . ( 2 ) We propose a two-phase reduction algorithm , which is a new approach to input reduction . The two-phase algorithm combines three objectives by using the HGS algorithm and a carefully designed heuristic . Our approach is a meaningful approach in practice , which is also accepted by another reviewer ( # 2 ) . ( 3 ) An extensive evaluation of the proposed approach , including non-regression and regression scenarios . The latter is a more practical scenario in the development process . Our approach can work well in both scenarios . 2.Regarding the applicability of the proposed algorithms to other related works Response : Our algorithm is general and can be easily adapted to other related work ( e.g. , other criteria and DL models ) . In our work , we use neuron coverage and the outputs of the last hidden layer as the two inputs . However , our algorithm is not specific to the neuron coverage criterion , and can support various other coverage criteria . That is , if the coverage of a DL model can be obtained , the first phase ( HGS ) of our algorithm can also be applied to it . Besides , for most of the DL models , the outputs of the last hidden layer ( or the last few hidden layers ) are numerical values , indicating that the latter phase of our algorithm can be applied to other DL models too . To sum up , our algorithm can be easily applied to other work as well . Note that there are some other related works targeting the cost problem in DL ( e.g. , those described in Section 2 ) . Our approach is proposed to solve the input reduction problem in DL testing , and we do not claim that our approach can be applied to the work described in Section 2 , as these work are quite different from our work . However , our approach and these work can complement each other in order to reduce DL costs ."}}