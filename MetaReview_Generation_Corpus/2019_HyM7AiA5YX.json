{"year": "2019", "forum": "HyM7AiA5YX", "title": "Complement Objective Training", "decision": "Accept (Poster)", "meta_review": "This paper proposes adding a second objective to the training of neural network classifiers that aims to make the distribution over incorrect labels as flat as possible for each training sample. The authors describe this as \"maximizing the complement entropy.\" Rather than adding the cross-entropy objective and the (negative) complement entropy term (since the complement entropy should be maximized while the cross-entropy is minimized), this paper proposes an alternating optimization framework in which first a step is taken to reduce the cross-entropy, then a step is taken to maximize the complement entropy. Extensive experiments on image classification (CIFAR-10, CIFAR-100, SVHN, Tiny Imagenet, and Imagenet), neural machine translation (IWSLT 2015 English-Vietnamese task), and small-vocabulary isolated-word recognition (Google Commands), show that the proposed two-objective approach outperforms training only to minimize cross-entropy. Experiments on CIFAR-10 also show that models trained in this framework have somewhat better resistance to single-step adversarial attacks. Concerns about the presentation of the adversarial attack experiments were raised by anonymous commenters and one of the reviewers, but these concerns were addressed in the revision and discussion. The primary remaining concern is a lack of any theoretical guarantees that the alternating optimization converges, but the strong empirical results compensate for this problem.", "reviews": [{"review_id": "HyM7AiA5YX-0", "review_text": "This paper considers augmenting the cross-entropy objective with \"complement\" objective maximization, which aims at neutralizing the predicted probabilities of classes other than the ground truth one. The main idea is to help the ground truth label stands out more easily by smoothing out potential peaks in non-ground-truth labels. The wide application of the cross-entropy objective makes this approach applicable to many different machine/deep learning applications varying from computer vision to NLP. The paper is well-written, with a clear explanation for the motivation of introducing the complement entropy objective and several good visualization of its empirical effects (e.g., Figures 1 and 2). The numerical experiments also incorporate a wide spectrum of applications and network structures as well as dataset sizes, and the performance improvement is quite impressive and consistent. In particular, the adversarial attacks example looks very interesting. One small suggestion is that the authors can also make some comments on the connection between the two-step update algorithm (Algorithm 1) with multi-objective optimization. In particular, I would suggest the authors also try some multi-objective optimization techniques apart from the simple but effective heuristics, and see if some Pareto-optimality can be guaranteed and better practical improvement can be achieved.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "( Q1 ) One small suggestion is that the authors can also make some comments on the connection between the two-step update algorithm ( Algorithm 1 ) with multi-objective optimization . In particular , I would suggest the authors also try some multi-objective optimization techniques apart from the simple but effective heuristics , and see if some Pareto-optimality can be guaranteed and better practical improvement can be achieved . ( A1 ) We sincerely thank the reviewer for the helpful and constructive suggestion about associating COT with multi-objective optimization . This is really a brilliant idea . As a straight-line future work , we will survey multi-objective optimization techniques , and explore the direction of formulating COT into a multi-objective optimization problem ."}, {"review_id": "HyM7AiA5YX-1", "review_text": "In this manuscript, the authors propose a secondary objective for softmax minimization. This complementary objective is based on evaluating the information gathered from the incorrect classes. Considering these two objectives leads to a new training approach. The manuscript ends with a collection of tests on a variety of problems. This is an interesting point of view but the manuscript lacks discussion on several important questions: 1) How is this idea related to regularization? If we increase the regularization parameter, we can attain sparse parameter vectors. 2) Would this method also complement from overfitting? 3) In the numerical experiments, the comparison is carried out against a \"baseline\" method. Do the authors use regularization with these baseline methods? I believe the comparison will be fair if the regularization option is turned on for the baseline methods. 4) Why combining the two objectives in a single optimization problem and then solving the resulting problem is not an option instead of the alternating method given in Algorithm 1? 5) How does alternating between two objectives change the training time? Do the authors use backpropagation?", "rating": "5: Marginally below acceptance threshold", "reply_text": "( Q4 ) Why combining the two objectives in a single optimization problem and then solving the resulting problem is not an option instead of the alternating method given in Algorithm 1 ? ( A4 ) We are very grateful for this novel idea , and we have conducted several preliminary experiments to explore this idea . Below are the comparisons between ( a ) the original COT method , and ( b ) the approach of combining the two objectives into one single objective . The experimental results show that the original COT method works better in almost all cases , and we conjecture that these two methods converge to different local minima . This idea is worth exploring , and we leave it as a straight-line future work . Test error of the state-of-the-art architectures on Cifar10 =========================================================== Combining into one objective COT ResNet-110 7.42 % 6.84 % PreAct ResNet-18 4.92 % 4.86 % ResNeXt-29 ( 2\u00d764d ) 4.79 % 4.55 % WideResNet-28-10 4.00 % 4.30 % DenseNet-BC-121 4.64 % 4.62 % =========================================================== Test error of the state-of-the-art architectures on Cifar100 =========================================================== Combining into one objective COT ResNet-110 28.80 % 27.90 % PreAct ResNet-18 25.30 % 24.73 % ResNeXt-29 ( 2\u00d764d ) 23.20 % 21.90 % WideResNet-28-10 21.96 % 20.99 % DenseNet-BC-121 22.17 % 20.54 % =========================================================== ( Q5 ) How does alternating between two objectives change the training time ? Do the authors use backpropagation ? ( A5 ) Yes , we do use backpropagation . One additional backpropagation is required in each iteration when applying COT , and therefore the overall training time is about 1.6 times longer according to our experiments . [ 1 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , Jian Sun . \u201c Deep Residual Learning for Image Recognition. \u201d In IEEE Conference on Computer Vision and Pattern Recognition , 2016 . [ 2 ] Sergey Zagoruyko , Nikos Komodakis . \u201c Wide Residual Networks . \u201d In British Machine Vision Conference , 2016 . [ 3 ] Gao Huang , Zhuang Liu , Laurens van der Maaten , Kilian Q. Weinberger , David Lopez-Paz . \u201c Densely Connected Convolutional Networks . \u201d In IEEE Conference on Computer Vision and Pattern Recognition , 2017 ."}, {"review_id": "HyM7AiA5YX-2", "review_text": " ======== Summary ======== The paper deals with the training of neural networks for classification or sequence generation tasks, using a cross-entropy loss. Minimizing the cross-entropy means maximizing the predicted probabilities of the ground-truth classes (averaged over the samples). The authors introduce a \"complementary entropy\" loss with the goal of minimizing the predicted probabilities of the complementary (incorrect) classes. To do that, they use the average of sample-wise entropy over the complement classes. By maximizing this entropy, the predicted complementary probabilities are encouraged to be equal and therefore, the authors claim that it neutralizes them as the number of classes grows large. The proposed training procedure, named COT, consists of alternating between the optimization of the two losses. The procedure is tested on image classification tasks with different datasets (CIFAR-10, CIFAR-100, Street View House Numbers, Tiny ImageNet and ImageNet), machine translation (training using IWSLT dataset, validation and test using TED tst2012/2013 datasets), and speech recognition (Gooogle Commands dataset). In the experiments, COT outperforms state-of-the-art models for each task/dataset. Adversarial attacks are also considered for the classification of images of CIFAR-10: using the Fast Gradient Sign and Basic Iterative Fast Gradient Sign methods on different models, adversarial examples specifically designed for each model, are generated. Then results of these models are compared to COT on these examples. The authors admit that the results are biased since the adversarial attacks only target part of the COT objective, hence more accurate comparisons should be done in future work. =========================== Main comments and questions =========================== End of page 1: \"the model behavior for classes other than the ground truth stays unharnessed and not well-defined\". The probabilities should still sum up to 1, so if the ground truth one is maximized, the others are actually implicitly minimized. No? Page 3, sec 2.1: \"optimizing on the complement entropy drives \u0177_ij to 1/(K \u2212 1)\". I believe that it drives each term \u0177_ij /(1 \u2212 \u0177_ig ) to be equal to 1/(K-1). Therefore, it drives \u0177_ij to (1 \u2212 \u0177_ig)/(K-1) for j!=g. This indeed flattens the \u0177_ij for j!=g, but the effect on \u0177_ig is not controlled. In particular this latter can decrease. Then in the next step of the algorithm, \u0177_ig will be maximized, but with no explicit control over the complementary probabilities. There are two objectives that are optimized over the same variable theta. So the question is, are we sure that this procedure will converge? What prevents situations where the probabilities will alternate between two values? For example, with 4 classes, we look at the predicted probabilities of a given sample of class 1: Suppose after step 1 of Algo 1, the predicted probabilities are: 0.5 0.3 0.1 0.1 After step 2: 0.1 0.3 0.3 0.3 Then step 1: 0.5 0.3 0.1 0.1 Then step 2: 0.1 0.3 0.3 0.3 And so on... Can this happen? Or why not? Did the algorithm have trouble converging in any of the experiments? Sec 3.1: \"additional efforts for tuning hyper-parameters might be required for optimizers to achieve the best performance\": Which hyper-parameters are considered here? If it is the learning rate, why not use a different one, tuned for each objective? Sec 3.2: The additional optimization makes each training iteration more costly. How much more? How do the total running times of COT compare to the ones of the baselines? I think this should be mentioned in the paper. Sec 3.4: As the authors mention, the results are biased and so the comparison is not fair here. Therefore I wonder about the relevance of this section. Isn't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT? For example, naively having a two steps perturbation of the input: one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective? =========================== Secondary comments and typos =========================== Page 3, sec 2.1: \"...the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities...\", using maximizes instead of optimizes would be clearer. In the definition of the complement entropy, equation (2), C takes as parameter only y^hat_Cbar but then in the formula, \u0177_ig appears. Shouldn't C take all \\hat_y as an argument in this case? Algorithm 1 page 4: I find it confusing that the (artificial) variable that appears in the argmin (resp. argmax) is theta_{t-1} (resp. theta'_t) which is the previous parameter. Is there a reason for this choice? Sec 3: \"We perform extensive experiments to evaluate COT on the tasks\" --> COT on tasks \"compare it with the baseline algorithms that achieve state-of-the-art in the respective domain.\" --> domainS \"to evaluate the model\u2019s robustness trained by COT when attacked\" needs reformulation. \"we select a state- of-the-art model that has the open-source implementation\" --> an open-source implementation Sec 3.2: Figure 4: why is the median reported and not the mean (as in Figure 3, Tables 2 and 3)? Table 3 and 4: why is it the validation error that is reported and not the test error? Sec 3.3: \"Neural machine translation (NMT) has populated the use of neural sequence models\": populated has not the intended meaning. \"We apply the same pre-processing steps as shown in the model\" --> in the paper? Sec 3.4: \"We believe that the models trained using COT are generalized better\" --> \"..using COT generalize better\" \"using both FGSM and I-FGSM method\" --> methodS \"The baseline models are the same as Section 3.2.\" --> as in Section 3.2. \"the number of iteration is set at 10.\" --> to 10 \"using complement objective may help defend adversarial attacks.\" --> defend against \"Studying on COT and adversarial attacks..\" --> could be better formulated References: there are some inconsistencies (e.g.: initials versus first name) Pros ==== - Paper is clear and well-written - It seems to me that it is a new original idea - Wide applicability - Extensive convincing experimental results Cons ==== - No theoretical guarantee that the procedure should converge - The training time may be twice longer (to clarify) - The adversarial section, as it is, does not seem relevant for me ", "rating": "7: Good paper, accept", "reply_text": "( Q5 ) Sec 3.4 : As the authors mention , the results are biased and so the comparison is not fair here . Therefore I wonder about the relevance of this section . Is n't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT ? For example , naively having a two steps perturbation of the input : one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective ? ( A5 ) Thanks for the comment . We should have made clear that \u201c black box \u201d [ 1 ] ( rather than \u201c white box \u201d ) adversarial attacks are considered in the manuscript . Specifically , we follow the common practice of generating adversarial examples using both FGSM and I-FGSM methods with the gradients from a baseline model ; this way , the model trained by COT is actually a \u201c black box \u201d to these attacks . We have modified the manuscript to clarify this part . Also , thanks for the great suggestion of forming adversarial attacks using \u201c both \u201d gradients ( from both primary & complement objectives ) . We are designing and conducting experiments at the moment and will share results when ready . For the part of secondary comments and typos , we appreciate your thorough reading again and have corrected all these typos according to your suggestions . Meanwhile , in the following , we also provided explanations to your secondary comments . ( Q1 ) Page 3 , sec 2.1 : `` ... the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities ... '' , using maximizes instead of optimizes would be clearer . ( A1 ) Thanks for the suggestion . We have reworded the manuscript to \u201c maximizes. \u201d ( Q2 ) In the definition of the complement entropy , equation ( 2 ) , C takes as parameter only y^hat_Cbar but then in the formula , \u0177_ig appears . Should n't C take all \\hat_y as an argument in this case ? ( A2 ) Since the probabilities sum up to one , \u0177_ig can be inferred from y^hat_Cbar . Also , for us , it seems more direct and clear to show that complement entropy is calculated from y^hat_Cbar when C takes y^hat_Cbar as the only argument . Therefore , we incline to keep the orignal formulation . If the reviewer has strong preference , please kindly let us know and we are happy to make changes accordingly . ( Q3 ) Algorithm 1 page 4 : I find it confusing that the ( artificial ) variable that appears in the argmin ( resp.argmax ) is theta_ { t-1 } ( resp.theta'_t ) which is the previous parameter . Is there a reason for this choice ? ( A3 ) Thanks for the comment . Originally , we want to notify readers that there are two backprops within one iteration . We agree that those symbols are confusing and therefore we have modified the manuscript with those symbols removed . ( Q4 ) Sec 3.2 Figure 4 : why is the median reported and not the mean ( as in Figure 3 , Tables 2 and 3 ) ? ( A4 ) Thanks for pointing this out . This is a typo and we have already corrected it in the manuscript : median - > mean . ( Q5 ) Sec 3.2 , Table 3 and 4 : why is it the validation error that is reported and not the test error ? ( A5 ) Thanks for the detailed comment . For a fair comparison , we report the error in the exact same way as the open-sourced repo from the ResNet authors : https : //github.com/KaimingHe/deep-residual-networks . ( Q6 ) Sec 3.3 : `` Neural machine translation ( NMT ) has populated the use of neural sequence models '' : populated has not the intended meaning . ( A6 ) We thank the reviewer for pointing out this typo . We have already corrected it in our manuscript : populated - > popularized ( Q7 ) `` Studying on COT and adversarial attacks .. '' -- > could be better formulated ( A7 ) Thanks for the comment again . We have modified the manuscript as follows : `` Studying on the relationship between COT and adversarial attacks\u2026 \u201d [ 1 ] Hongyi Zhang , Moustapha Cisse , Yann N. Dauphin , David Lopez-Paz . \u201c Mixup : Beyond Empirical Risk Minimization. \u201d In International Conference on Learning Representation , 2018 ."}], "0": {"review_id": "HyM7AiA5YX-0", "review_text": "This paper considers augmenting the cross-entropy objective with \"complement\" objective maximization, which aims at neutralizing the predicted probabilities of classes other than the ground truth one. The main idea is to help the ground truth label stands out more easily by smoothing out potential peaks in non-ground-truth labels. The wide application of the cross-entropy objective makes this approach applicable to many different machine/deep learning applications varying from computer vision to NLP. The paper is well-written, with a clear explanation for the motivation of introducing the complement entropy objective and several good visualization of its empirical effects (e.g., Figures 1 and 2). The numerical experiments also incorporate a wide spectrum of applications and network structures as well as dataset sizes, and the performance improvement is quite impressive and consistent. In particular, the adversarial attacks example looks very interesting. One small suggestion is that the authors can also make some comments on the connection between the two-step update algorithm (Algorithm 1) with multi-objective optimization. In particular, I would suggest the authors also try some multi-objective optimization techniques apart from the simple but effective heuristics, and see if some Pareto-optimality can be guaranteed and better practical improvement can be achieved.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "( Q1 ) One small suggestion is that the authors can also make some comments on the connection between the two-step update algorithm ( Algorithm 1 ) with multi-objective optimization . In particular , I would suggest the authors also try some multi-objective optimization techniques apart from the simple but effective heuristics , and see if some Pareto-optimality can be guaranteed and better practical improvement can be achieved . ( A1 ) We sincerely thank the reviewer for the helpful and constructive suggestion about associating COT with multi-objective optimization . This is really a brilliant idea . As a straight-line future work , we will survey multi-objective optimization techniques , and explore the direction of formulating COT into a multi-objective optimization problem ."}, "1": {"review_id": "HyM7AiA5YX-1", "review_text": "In this manuscript, the authors propose a secondary objective for softmax minimization. This complementary objective is based on evaluating the information gathered from the incorrect classes. Considering these two objectives leads to a new training approach. The manuscript ends with a collection of tests on a variety of problems. This is an interesting point of view but the manuscript lacks discussion on several important questions: 1) How is this idea related to regularization? If we increase the regularization parameter, we can attain sparse parameter vectors. 2) Would this method also complement from overfitting? 3) In the numerical experiments, the comparison is carried out against a \"baseline\" method. Do the authors use regularization with these baseline methods? I believe the comparison will be fair if the regularization option is turned on for the baseline methods. 4) Why combining the two objectives in a single optimization problem and then solving the resulting problem is not an option instead of the alternating method given in Algorithm 1? 5) How does alternating between two objectives change the training time? Do the authors use backpropagation?", "rating": "5: Marginally below acceptance threshold", "reply_text": "( Q4 ) Why combining the two objectives in a single optimization problem and then solving the resulting problem is not an option instead of the alternating method given in Algorithm 1 ? ( A4 ) We are very grateful for this novel idea , and we have conducted several preliminary experiments to explore this idea . Below are the comparisons between ( a ) the original COT method , and ( b ) the approach of combining the two objectives into one single objective . The experimental results show that the original COT method works better in almost all cases , and we conjecture that these two methods converge to different local minima . This idea is worth exploring , and we leave it as a straight-line future work . Test error of the state-of-the-art architectures on Cifar10 =========================================================== Combining into one objective COT ResNet-110 7.42 % 6.84 % PreAct ResNet-18 4.92 % 4.86 % ResNeXt-29 ( 2\u00d764d ) 4.79 % 4.55 % WideResNet-28-10 4.00 % 4.30 % DenseNet-BC-121 4.64 % 4.62 % =========================================================== Test error of the state-of-the-art architectures on Cifar100 =========================================================== Combining into one objective COT ResNet-110 28.80 % 27.90 % PreAct ResNet-18 25.30 % 24.73 % ResNeXt-29 ( 2\u00d764d ) 23.20 % 21.90 % WideResNet-28-10 21.96 % 20.99 % DenseNet-BC-121 22.17 % 20.54 % =========================================================== ( Q5 ) How does alternating between two objectives change the training time ? Do the authors use backpropagation ? ( A5 ) Yes , we do use backpropagation . One additional backpropagation is required in each iteration when applying COT , and therefore the overall training time is about 1.6 times longer according to our experiments . [ 1 ] Kaiming He , Xiangyu Zhang , Shaoqing Ren , Jian Sun . \u201c Deep Residual Learning for Image Recognition. \u201d In IEEE Conference on Computer Vision and Pattern Recognition , 2016 . [ 2 ] Sergey Zagoruyko , Nikos Komodakis . \u201c Wide Residual Networks . \u201d In British Machine Vision Conference , 2016 . [ 3 ] Gao Huang , Zhuang Liu , Laurens van der Maaten , Kilian Q. Weinberger , David Lopez-Paz . \u201c Densely Connected Convolutional Networks . \u201d In IEEE Conference on Computer Vision and Pattern Recognition , 2017 ."}, "2": {"review_id": "HyM7AiA5YX-2", "review_text": " ======== Summary ======== The paper deals with the training of neural networks for classification or sequence generation tasks, using a cross-entropy loss. Minimizing the cross-entropy means maximizing the predicted probabilities of the ground-truth classes (averaged over the samples). The authors introduce a \"complementary entropy\" loss with the goal of minimizing the predicted probabilities of the complementary (incorrect) classes. To do that, they use the average of sample-wise entropy over the complement classes. By maximizing this entropy, the predicted complementary probabilities are encouraged to be equal and therefore, the authors claim that it neutralizes them as the number of classes grows large. The proposed training procedure, named COT, consists of alternating between the optimization of the two losses. The procedure is tested on image classification tasks with different datasets (CIFAR-10, CIFAR-100, Street View House Numbers, Tiny ImageNet and ImageNet), machine translation (training using IWSLT dataset, validation and test using TED tst2012/2013 datasets), and speech recognition (Gooogle Commands dataset). In the experiments, COT outperforms state-of-the-art models for each task/dataset. Adversarial attacks are also considered for the classification of images of CIFAR-10: using the Fast Gradient Sign and Basic Iterative Fast Gradient Sign methods on different models, adversarial examples specifically designed for each model, are generated. Then results of these models are compared to COT on these examples. The authors admit that the results are biased since the adversarial attacks only target part of the COT objective, hence more accurate comparisons should be done in future work. =========================== Main comments and questions =========================== End of page 1: \"the model behavior for classes other than the ground truth stays unharnessed and not well-defined\". The probabilities should still sum up to 1, so if the ground truth one is maximized, the others are actually implicitly minimized. No? Page 3, sec 2.1: \"optimizing on the complement entropy drives \u0177_ij to 1/(K \u2212 1)\". I believe that it drives each term \u0177_ij /(1 \u2212 \u0177_ig ) to be equal to 1/(K-1). Therefore, it drives \u0177_ij to (1 \u2212 \u0177_ig)/(K-1) for j!=g. This indeed flattens the \u0177_ij for j!=g, but the effect on \u0177_ig is not controlled. In particular this latter can decrease. Then in the next step of the algorithm, \u0177_ig will be maximized, but with no explicit control over the complementary probabilities. There are two objectives that are optimized over the same variable theta. So the question is, are we sure that this procedure will converge? What prevents situations where the probabilities will alternate between two values? For example, with 4 classes, we look at the predicted probabilities of a given sample of class 1: Suppose after step 1 of Algo 1, the predicted probabilities are: 0.5 0.3 0.1 0.1 After step 2: 0.1 0.3 0.3 0.3 Then step 1: 0.5 0.3 0.1 0.1 Then step 2: 0.1 0.3 0.3 0.3 And so on... Can this happen? Or why not? Did the algorithm have trouble converging in any of the experiments? Sec 3.1: \"additional efforts for tuning hyper-parameters might be required for optimizers to achieve the best performance\": Which hyper-parameters are considered here? If it is the learning rate, why not use a different one, tuned for each objective? Sec 3.2: The additional optimization makes each training iteration more costly. How much more? How do the total running times of COT compare to the ones of the baselines? I think this should be mentioned in the paper. Sec 3.4: As the authors mention, the results are biased and so the comparison is not fair here. Therefore I wonder about the relevance of this section. Isn't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT? For example, naively having a two steps perturbation of the input: one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective? =========================== Secondary comments and typos =========================== Page 3, sec 2.1: \"...the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities...\", using maximizes instead of optimizes would be clearer. In the definition of the complement entropy, equation (2), C takes as parameter only y^hat_Cbar but then in the formula, \u0177_ig appears. Shouldn't C take all \\hat_y as an argument in this case? Algorithm 1 page 4: I find it confusing that the (artificial) variable that appears in the argmin (resp. argmax) is theta_{t-1} (resp. theta'_t) which is the previous parameter. Is there a reason for this choice? Sec 3: \"We perform extensive experiments to evaluate COT on the tasks\" --> COT on tasks \"compare it with the baseline algorithms that achieve state-of-the-art in the respective domain.\" --> domainS \"to evaluate the model\u2019s robustness trained by COT when attacked\" needs reformulation. \"we select a state- of-the-art model that has the open-source implementation\" --> an open-source implementation Sec 3.2: Figure 4: why is the median reported and not the mean (as in Figure 3, Tables 2 and 3)? Table 3 and 4: why is it the validation error that is reported and not the test error? Sec 3.3: \"Neural machine translation (NMT) has populated the use of neural sequence models\": populated has not the intended meaning. \"We apply the same pre-processing steps as shown in the model\" --> in the paper? Sec 3.4: \"We believe that the models trained using COT are generalized better\" --> \"..using COT generalize better\" \"using both FGSM and I-FGSM method\" --> methodS \"The baseline models are the same as Section 3.2.\" --> as in Section 3.2. \"the number of iteration is set at 10.\" --> to 10 \"using complement objective may help defend adversarial attacks.\" --> defend against \"Studying on COT and adversarial attacks..\" --> could be better formulated References: there are some inconsistencies (e.g.: initials versus first name) Pros ==== - Paper is clear and well-written - It seems to me that it is a new original idea - Wide applicability - Extensive convincing experimental results Cons ==== - No theoretical guarantee that the procedure should converge - The training time may be twice longer (to clarify) - The adversarial section, as it is, does not seem relevant for me ", "rating": "7: Good paper, accept", "reply_text": "( Q5 ) Sec 3.4 : As the authors mention , the results are biased and so the comparison is not fair here . Therefore I wonder about the relevance of this section . Is n't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT ? For example , naively having a two steps perturbation of the input : one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective ? ( A5 ) Thanks for the comment . We should have made clear that \u201c black box \u201d [ 1 ] ( rather than \u201c white box \u201d ) adversarial attacks are considered in the manuscript . Specifically , we follow the common practice of generating adversarial examples using both FGSM and I-FGSM methods with the gradients from a baseline model ; this way , the model trained by COT is actually a \u201c black box \u201d to these attacks . We have modified the manuscript to clarify this part . Also , thanks for the great suggestion of forming adversarial attacks using \u201c both \u201d gradients ( from both primary & complement objectives ) . We are designing and conducting experiments at the moment and will share results when ready . For the part of secondary comments and typos , we appreciate your thorough reading again and have corrected all these typos according to your suggestions . Meanwhile , in the following , we also provided explanations to your secondary comments . ( Q1 ) Page 3 , sec 2.1 : `` ... the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities ... '' , using maximizes instead of optimizes would be clearer . ( A1 ) Thanks for the suggestion . We have reworded the manuscript to \u201c maximizes. \u201d ( Q2 ) In the definition of the complement entropy , equation ( 2 ) , C takes as parameter only y^hat_Cbar but then in the formula , \u0177_ig appears . Should n't C take all \\hat_y as an argument in this case ? ( A2 ) Since the probabilities sum up to one , \u0177_ig can be inferred from y^hat_Cbar . Also , for us , it seems more direct and clear to show that complement entropy is calculated from y^hat_Cbar when C takes y^hat_Cbar as the only argument . Therefore , we incline to keep the orignal formulation . If the reviewer has strong preference , please kindly let us know and we are happy to make changes accordingly . ( Q3 ) Algorithm 1 page 4 : I find it confusing that the ( artificial ) variable that appears in the argmin ( resp.argmax ) is theta_ { t-1 } ( resp.theta'_t ) which is the previous parameter . Is there a reason for this choice ? ( A3 ) Thanks for the comment . Originally , we want to notify readers that there are two backprops within one iteration . We agree that those symbols are confusing and therefore we have modified the manuscript with those symbols removed . ( Q4 ) Sec 3.2 Figure 4 : why is the median reported and not the mean ( as in Figure 3 , Tables 2 and 3 ) ? ( A4 ) Thanks for pointing this out . This is a typo and we have already corrected it in the manuscript : median - > mean . ( Q5 ) Sec 3.2 , Table 3 and 4 : why is it the validation error that is reported and not the test error ? ( A5 ) Thanks for the detailed comment . For a fair comparison , we report the error in the exact same way as the open-sourced repo from the ResNet authors : https : //github.com/KaimingHe/deep-residual-networks . ( Q6 ) Sec 3.3 : `` Neural machine translation ( NMT ) has populated the use of neural sequence models '' : populated has not the intended meaning . ( A6 ) We thank the reviewer for pointing out this typo . We have already corrected it in our manuscript : populated - > popularized ( Q7 ) `` Studying on COT and adversarial attacks .. '' -- > could be better formulated ( A7 ) Thanks for the comment again . We have modified the manuscript as follows : `` Studying on the relationship between COT and adversarial attacks\u2026 \u201d [ 1 ] Hongyi Zhang , Moustapha Cisse , Yann N. Dauphin , David Lopez-Paz . \u201c Mixup : Beyond Empirical Risk Minimization. \u201d In International Conference on Learning Representation , 2018 ."}}