{"year": "2017", "forum": "ByOvsIqeg", "title": "Regularizing CNNs with Locally Constrained Decorrelations", "decision": "Accept (Poster)", "meta_review": "The paper presents a new regularization approach for deep learning that penalizes positive correlations between features in a network. The experimental evaluation is solid, and suggests the proposed regularized may help in learning better convolutional networks (but the gains are relatively small).", "reviews": [{"review_id": "ByOvsIqeg-0", "review_text": "The author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results. ", "rating": "7: Good paper, accept", "reply_text": "Thank you again !"}, {"review_id": "ByOvsIqeg-1", "review_text": "The paper proposes a new regulariser for CNNs that penalises positive correlations between feature weights, but does not affect negative correlations. An alternative version which penalises all correlations regardless of sign is also considered. The paper refers to these as \"local\" and \"global\" respectively, which I find a bit confusing as these are very general terms that can mean a plethora of things. The experimental validation is quite rigorous. Several experiments are conducted on benchmark datasets (MNIST, CIFAR-10, CIFAR-100, SVHN) and improvements are demonstrated in most cases. While these improvements may seem modest, the baselines are already very competitive as the authors pointed out. In some cases it does raise some questions about statistical significance though. More results with the global regulariser (i.e. not just on MNIST) would have been interesting, as the main novelty in the paper seems to be leaving the negative correlations alone, so it would be interesting to see exactly how much of a difference this makes. One of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights, but refers to both as \"features\". However, the authors have already said they will address this. The paper somewhat ignores interactions with the choice of nonlinearity, which seems like it could be very important; especially because the goal is to obtain feature activations that are uncorrelated, and this is done only by applying a penalty to the weights (i.e. in a data-agnostic way and also ignoring any nonlinearity). I believe the authors already mentioned in their responses to reviewer questions that this would be addressed, but I think this important and it definitely needs to be discussed. In response to the authors' answer to my question about the role of biases: as they point out, it is perfectly possible to combine their proposed technique with the \"multi-bias\" approach, but this was not really my point. Rather, the latter is an example that challenges the idea that features should not be positively correlated / redundant, which seems to be the assumption that this work is built upon. My current intuition is that it's okay to have correlated features, as long as you're not wasting model capacity on them. This is the case for \"multi-bias\", seeing as the weights are shared across sets of correlated features. The dichotomy between regularisation methods that reduce capacity and those that don't which is described in the introduction seems a bit arbitrary to me, especially considering that weight decay is counted among the former and the proposed method is counted among the latter. I think this very much depends on ones definition of model capacity (clearly weight decay does not actually reduce the number of parameters in a model). Overall, the work is perhaps a bit incremental, but it seems to be well-executed. The results are convincing, even if they aren't particularly ground-breaking.", "rating": "7: Good paper, accept", "reply_text": "> > The paper refers to these as `` local '' and `` global '' respectively , which I find a bit confusing as these are very general terms that can mean a plethora of things . We found the words \u201c local \u201d and \u201c global \u201d useful to avoid repeating long sentences like \u201c regularizing positive and negative correlations \u201d , we also explain the origin of this notation on page five : in previous regularizations all feature weights influence each other ( \u201c global \u201d ) , while OrthoReg only regularizes the nearest feature weights in angle ( \u201c local \u201d ) . > > More results with the global regulariser ( i.e.not just on MNIST ) would have been interesting , as the main novelty in the paper seems to be leaving the negative correlations alone , so it would be interesting to see exactly how much of a difference this makes . We have added the suggested comparison in Figure 6 for Wide Resnet on Cifar10 and Cifar100 . As it can be seen , OrthoReg has a lower error bound compared with regularizing the negative correlations ( 0.2 % on Cifar10 , and 0.7 % on Cifar100 ) . > > One of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights , but refers to both as `` features '' . However , the authors have already said they will address this . We have fixed this issue accordingly replacing \u201c features \u201d with \u201c feature weights \u201d or \u201c feature activations . > > The paper somewhat ignores interactions with the choice of nonlinearity , which seems like it could be very important ; especially because the goal is to obtain feature activations that are uncorrelated , and this is done only by applying a penalty to the weights ( i.e.in a data-agnostic way and also ignoring any nonlinearity ) . I believe the authors already mentioned in their responses to reviewer questions that this would be addressed , but I think this important and it definitely needs to be discussed . Although we successfully applied the proposed regularizer on ReLU networks , we agree with the reviewer that a thorough study on the effects of the non-linearities should be done . We have modified the discussion part of the paper so as to reflect this important concern . > > In response to the authors ' answer to my question about the role of biases : as they point out , it is perfectly possible to combine their proposed technique with the `` multi-bias '' approach , but this was not really my point . Rather , the latter is an example that challenges the idea that features should not be positively correlated / redundant , which seems to be the assumption that this work is built upon . My current intuition is that it 's okay to have correlated features , as long as you 're not wasting model capacity on them . This is the case for `` multi-bias '' , seeing as the weights are shared across sets of correlated features . We now better understand the point , but note that there is no guarantee that networks with a high number of filter weights , and thus with a lot of redundant filters , will show a good behaviour in terms of generalization and overfitting . We have added a comment on this fact in section 2 . > > The dichotomy between regularisation methods that reduce capacity and those that do n't which is described in the introduction seems a bit arbitrary to me , especially considering that weight decay is counted among the former and the proposed method is counted among the latter . I think this very much depends on ones definition of model capacity ( clearly weight decay does not actually reduce the number of parameters in a model ) . As we explained in the paper , weight decay is shown to reduce the \u201c effective number of parameters \u201d of neural networks [ 1 ] . Taking this into account we think it is sensible to make the distinction between those regularizations which drop weights , activations\u2026 ( in essence , regularizations which reduce the capacity of the network ) , from those which do use all the capacity as effectively as possible . However , we agree this distinction may not be as clear to a reader as we claimed in the paper , thus , we change the sentence \u201c There are two clearly defined regularization strategies in the literature \u201d for \u201c From the literature , two different regularization strategies can be defined \u201d . [ 1 ] Moody , J. E. ( 1991 , December ) . The effective number of parameters : An analysis of generalization and regularization in nonlinear learning systems . In NIPS ( Vol.4 , pp.847-854 ) . > > Overall , the work is perhaps a bit incremental , but it seems to be well-executed . The results are convincing , even if they are n't particularly ground-breaking . Thanks.We hope the new consistent results on the new version of wide residual networks strengthen our claims and the contribution ."}, {"review_id": "ByOvsIqeg-2", "review_text": "Encouraging orthogonality in weight features has been reported useful for deep networks in many previous works. The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality. Further, they also show why and how negative correlations can and should be avoided for better de-correlation. Orthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting. As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights). Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better. Although the improvement in performances is not significant the direction of research and the observations made are promising.", "rating": "7: Good paper, accept", "reply_text": "> > Although the improvement in performances is not significant the direction of research and the observations made are promising . On the new updated version , we show better performance improvements with the new wide ResNet results . We hope these new results strengthen the significance of our work ."}], "0": {"review_id": "ByOvsIqeg-0", "review_text": "The author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results. ", "rating": "7: Good paper, accept", "reply_text": "Thank you again !"}, "1": {"review_id": "ByOvsIqeg-1", "review_text": "The paper proposes a new regulariser for CNNs that penalises positive correlations between feature weights, but does not affect negative correlations. An alternative version which penalises all correlations regardless of sign is also considered. The paper refers to these as \"local\" and \"global\" respectively, which I find a bit confusing as these are very general terms that can mean a plethora of things. The experimental validation is quite rigorous. Several experiments are conducted on benchmark datasets (MNIST, CIFAR-10, CIFAR-100, SVHN) and improvements are demonstrated in most cases. While these improvements may seem modest, the baselines are already very competitive as the authors pointed out. In some cases it does raise some questions about statistical significance though. More results with the global regulariser (i.e. not just on MNIST) would have been interesting, as the main novelty in the paper seems to be leaving the negative correlations alone, so it would be interesting to see exactly how much of a difference this makes. One of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights, but refers to both as \"features\". However, the authors have already said they will address this. The paper somewhat ignores interactions with the choice of nonlinearity, which seems like it could be very important; especially because the goal is to obtain feature activations that are uncorrelated, and this is done only by applying a penalty to the weights (i.e. in a data-agnostic way and also ignoring any nonlinearity). I believe the authors already mentioned in their responses to reviewer questions that this would be addressed, but I think this important and it definitely needs to be discussed. In response to the authors' answer to my question about the role of biases: as they point out, it is perfectly possible to combine their proposed technique with the \"multi-bias\" approach, but this was not really my point. Rather, the latter is an example that challenges the idea that features should not be positively correlated / redundant, which seems to be the assumption that this work is built upon. My current intuition is that it's okay to have correlated features, as long as you're not wasting model capacity on them. This is the case for \"multi-bias\", seeing as the weights are shared across sets of correlated features. The dichotomy between regularisation methods that reduce capacity and those that don't which is described in the introduction seems a bit arbitrary to me, especially considering that weight decay is counted among the former and the proposed method is counted among the latter. I think this very much depends on ones definition of model capacity (clearly weight decay does not actually reduce the number of parameters in a model). Overall, the work is perhaps a bit incremental, but it seems to be well-executed. The results are convincing, even if they aren't particularly ground-breaking.", "rating": "7: Good paper, accept", "reply_text": "> > The paper refers to these as `` local '' and `` global '' respectively , which I find a bit confusing as these are very general terms that can mean a plethora of things . We found the words \u201c local \u201d and \u201c global \u201d useful to avoid repeating long sentences like \u201c regularizing positive and negative correlations \u201d , we also explain the origin of this notation on page five : in previous regularizations all feature weights influence each other ( \u201c global \u201d ) , while OrthoReg only regularizes the nearest feature weights in angle ( \u201c local \u201d ) . > > More results with the global regulariser ( i.e.not just on MNIST ) would have been interesting , as the main novelty in the paper seems to be leaving the negative correlations alone , so it would be interesting to see exactly how much of a difference this makes . We have added the suggested comparison in Figure 6 for Wide Resnet on Cifar10 and Cifar100 . As it can be seen , OrthoReg has a lower error bound compared with regularizing the negative correlations ( 0.2 % on Cifar10 , and 0.7 % on Cifar100 ) . > > One of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights , but refers to both as `` features '' . However , the authors have already said they will address this . We have fixed this issue accordingly replacing \u201c features \u201d with \u201c feature weights \u201d or \u201c feature activations . > > The paper somewhat ignores interactions with the choice of nonlinearity , which seems like it could be very important ; especially because the goal is to obtain feature activations that are uncorrelated , and this is done only by applying a penalty to the weights ( i.e.in a data-agnostic way and also ignoring any nonlinearity ) . I believe the authors already mentioned in their responses to reviewer questions that this would be addressed , but I think this important and it definitely needs to be discussed . Although we successfully applied the proposed regularizer on ReLU networks , we agree with the reviewer that a thorough study on the effects of the non-linearities should be done . We have modified the discussion part of the paper so as to reflect this important concern . > > In response to the authors ' answer to my question about the role of biases : as they point out , it is perfectly possible to combine their proposed technique with the `` multi-bias '' approach , but this was not really my point . Rather , the latter is an example that challenges the idea that features should not be positively correlated / redundant , which seems to be the assumption that this work is built upon . My current intuition is that it 's okay to have correlated features , as long as you 're not wasting model capacity on them . This is the case for `` multi-bias '' , seeing as the weights are shared across sets of correlated features . We now better understand the point , but note that there is no guarantee that networks with a high number of filter weights , and thus with a lot of redundant filters , will show a good behaviour in terms of generalization and overfitting . We have added a comment on this fact in section 2 . > > The dichotomy between regularisation methods that reduce capacity and those that do n't which is described in the introduction seems a bit arbitrary to me , especially considering that weight decay is counted among the former and the proposed method is counted among the latter . I think this very much depends on ones definition of model capacity ( clearly weight decay does not actually reduce the number of parameters in a model ) . As we explained in the paper , weight decay is shown to reduce the \u201c effective number of parameters \u201d of neural networks [ 1 ] . Taking this into account we think it is sensible to make the distinction between those regularizations which drop weights , activations\u2026 ( in essence , regularizations which reduce the capacity of the network ) , from those which do use all the capacity as effectively as possible . However , we agree this distinction may not be as clear to a reader as we claimed in the paper , thus , we change the sentence \u201c There are two clearly defined regularization strategies in the literature \u201d for \u201c From the literature , two different regularization strategies can be defined \u201d . [ 1 ] Moody , J. E. ( 1991 , December ) . The effective number of parameters : An analysis of generalization and regularization in nonlinear learning systems . In NIPS ( Vol.4 , pp.847-854 ) . > > Overall , the work is perhaps a bit incremental , but it seems to be well-executed . The results are convincing , even if they are n't particularly ground-breaking . Thanks.We hope the new consistent results on the new version of wide residual networks strengthen our claims and the contribution ."}, "2": {"review_id": "ByOvsIqeg-2", "review_text": "Encouraging orthogonality in weight features has been reported useful for deep networks in many previous works. The authors present a explicit regularization cost to achieve de-correlation among weight features in a layer and encourage orthogonality. Further, they also show why and how negative correlations can and should be avoided for better de-correlation. Orthogonal weight features achieve better generalization in case of large number of trainable parameters and less training data, which usually results in over-fitting. As also mentioned by the authors biases help in de-correlation of feature responses even in the presence of correlated features (weights). Regularization techniques like OrthoReg can be more helpful in training deeper and leaner networks, where the representational capacity of each layer is low, and also generalize better. Although the improvement in performances is not significant the direction of research and the observations made are promising.", "rating": "7: Good paper, accept", "reply_text": "> > Although the improvement in performances is not significant the direction of research and the observations made are promising . On the new updated version , we show better performance improvements with the new wide ResNet results . We hope these new results strengthen the significance of our work ."}}