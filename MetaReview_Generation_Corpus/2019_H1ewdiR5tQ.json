{"year": "2019", "forum": "H1ewdiR5tQ", "title": "Graph Wavelet Neural Network", "decision": "Accept (Poster)", "meta_review": "AR1 and AR3 have found this paper interesting in terms of replacing the spectral operations in GCN by wavelet operations. However, AR4 was more critical about the poor complexity of the proposed method compared to approximations in Hammond et al. AR4 was also right to find the proposed work similar to Chebyshev approximations in ChebNet and to highlight that the proposed approach is only marginally better than GCN. On balance, all reviewers find some merit in this work thus AC advocates an accept. The authors are asked to keep the contents of the final draft as agreed with AR4 (and other reviewers) during rebuttal without making any further theoretical changes/brushing over various new claims/ideas unsolicited by the reviewers (otherwise such changes would require passing the draft again through reviewers).", "reviews": [{"review_id": "H1ewdiR5tQ-0", "review_text": "This paper proposes to learn graph wavelet kernels through a neural network. This idea is interesting, even if it is not a real surprise, given the interesting features graph wavelets, and the explosion of proposals for graph neural networks. Yet, the paper is interesting, pretty complete, and the proposed algorithm is shown to be relatively effective. A few more detailed comments: - one for the key motivations the work, namely to avoid eigen-decompositions, is also solved in a different way by methods using Chebyshev approximations (e.g., Khasanova - ICML 2017). The introduction should be clarified accordingly. - retaining the flexibility of convolution kernel (on page 2): what do the authors mean here? - the graph wavelet nn in 2.4 is certainly an interesting combination of known elements - yet, it is not very clear how the network is trained/optimized at this stage. - the section 3 is a bit confusing: are the proposed elements, contributions of the paper, or only ideas with future work? - the idea of detaching feature transformation from convolution is interesting: it would be even better to quantify or discuss the penalty, if any, induced by this design choice. - the results are generally fine and convincing, even if they are not super-impressive. ' GWNN is comfortably ahead of Spectral CNN. ' is probably an over-statement however... - the discussion about interpretability is interesting, and very trendy. However, what the authors discuss is mere localisation and sparsity - this is one way to 'interpret' interpretability of features, but that discussion should be rephrased in a more mild sense then. Generally, the ideas in this paper are interesting, even not surprising. The text should be clarified at places, and the paper looks a bit superficial on many aspects (see above). With good revision, it could lead to an interesting paper, and most likely to interesting discussions at ICLR.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the pertinent comments and accurately summarizing the major contributions of this paper . According to your constructive suggestions , we carefully revised our paper with much improvement . We now offer point-by-point response . Q1 : one for the key motivations the work , namely to avoid eigen-decompositions , is also solved in a different way by methods using Chebyshev approximations ( e.g. , Khasanova - ICML 2017 ) . The introduction should be clarified accordingly . A1 : We revised the statements relevant to this point , and cited the paper ( i.e. , Khasanova , ICML 2017 ) in the introduction part . Q2 : retaining the flexibility of convolution kernel ( on page 2 ) : what do the authors mean here ? A2 : We apologize for the misleading statement . What we mean is that the convolution kernel of ChebyNet is not flexible . Specifically , ChebyNet offers a $ K $ -order polynomial parameterization to graph convolution kernel . A smaller $ K $ causes high approximation bias , while a larger $ K $ results in non-localized convolution kernel . Therefore , ChebyNet has limited flexibility to define graph convolution kernel . In the revised version , we clarified this point in the last paragraph in Section 2.2 . Q3 : the graph wavelet nn in 2.4 is certainly an interesting combination of known elements - yet , it is not very clear how the network is trained/optimized at this stage . A3 : Thank you for pointing out this issue . Prompted by your suggestion , we revised Section 2.4 : ( 1 ) We offered detailed description about the architecture of graph wavelet neural network ; ( 2 ) We added the loss function when training graph wavelet neural networks on semi-supervised node classification task . Q4 : the section 3 is a bit confusing : are the proposed elements , contributions of the paper , or only ideas with future work ? A4 : We apologize for the confusing organization of sections . Indeed , Section 3 is the second contribution of this paper , i.e. , reducing the parameter complexity by detaching feature transformation from graph convolution . It is particularly important for training graph wavelet neural networks . For clarity , we reorganized the sections of this paper : combing the original Section 3 into Section 2 as a subsection , i.e. , Section 2.5 . Q5 : the idea of detaching feature transformation from convolution is interesting : it would be even better to quantify or discuss the penalty , if any , induced by this design choice . A5 : We are delighted to see that this idea is identified and approved . Detaching feature transformation from convolution , we remarkably reduce the number of parameters . This practice is particularly important for scenarios where labeled data is limited , e.g. , graph-based semi-supervised learning task considered in this paper . One potential penalty of this practice is that the modeling capacity is reduced . To quantify the penalty , in Section 4.4 , we compared the influence of detaching feature transformation from convolution to the performance of graph-based semi-supervised node classification . Results demonstrate that detaching feature transformation from convolution achieves comparable ( sometimes better ) performance , with the number of parameters remarkably reduced . Q6 : the results are generally fine and convincing , even if they are not super-impressive . ' GWNN is comfortably ahead of Spectral CNN . ' is probably an over-statement . A6 : For experimental evaluation , we compare the proposed GWNN with existing spectral methods , Spectral CNN , ChebyNet , GCN and some spatial methods like MoNet . The major contribution of this paper is to improve spectral methods , using graph wavelets rather than eigenvectors of Laplacian as bases . Thus , we focus on the comparison with spectral methods . By the sentence ' GWNN is comfortably ahead of Spectral CNN ' , we mean GWNN using graph wavelet transform is ahead of the Spectral CNN using Fourier transform , i.e. , GWNN ( the last row in Table 3 ) and Spectral CNN ( the ninth row in Table 3 ) . Indeed , GWNN is better than Spectral CNN by 10 % improvement on Cora and Citeseer and 5 % improvement on Pubmed . Q7 : the discussion about interpretability is interesting , and very trendy . However , what the authors discuss is mere localisation and sparsity - this is one way to 'interpret ' interpretability of features , but that discussion should be rephrased in a more mild sense then . A7 : Thank you for the inspiring comments . We revised the discussion about interpretability ( Section 4.7 ) , trying to ease the understanding of the interpretability of graph wavelets and graph convolution via graph wavelet transform ."}, {"review_id": "H1ewdiR5tQ-1", "review_text": "This is an empirical paper that proposes to design wavelets on graphs, that can be integrated to neural networks on graphs. It permits to reducing the number of parameters of the \u00ab convolution \u00bb and exploits the sparsity of sparse weighted graphs for computations. I think it\u2019s an interesting work. The perspective I enjoy in using wavelets is that they typically provide a good trade-off in localization in the spectral and graph domains. For instance, large eigenvalues of the laplacian could be potentially captured in a more stable way. This type of work might be a first step. Pros : - good numerical results - nice incorporation of structure via wavelets Cons : - Sometimes the paper is not really clear I have severals comments: 1/ (1) \u00ab Some subsequent works devote to making spectral methods spectrum-free\u2026 avoiding high computational cost \u00bb. I think also those types of representations can be potentially unstable. That\u2019s a second reason. 2/ I\u2019m not sure to understand the point (1) of the fourth paragraph of the introduction. (1.)\u00ab graph wavelet does not depend on the eigen decomposition of Laplacian matrix \u00bb. Does it mean numerically ? This sentence is not clear, because even numerically, it can be done in a dependent way to this matrix. However, if it is implied that the fastest algorithm can be obtained without eigen-decomposition of the Laplacian Matrix, in a cheap way, then I agree and a small rephrasing could be nice. 3/ Please remove all the sentences that are supposed to be an introduction for a section, i.e. the sentences between 2 and 2.1 (\u00ab We use graph\u2026 \u00bb) and 3 and 3.1 (\u00ab In many research\u2026 \u00bb). They are poorly written and do not help the reader. 5/ .(2.2) \u201cHowever, such a polynomial approximation also limits the flexibility to define appropriate convolution on graph\u201d I\u2019m not sure to understand. In the paper you refer to, the set of filters span the polynomial of degree less than n, of a diagonalizable matrix of size nxn. Thus, lagrange polynomial basis could be used to interpolate any desired values? Does it signify learning non-diagonal(in the appropriate basis) matrix? 6/ (2.3) $s$ how is this parameter chosen, crossvalidation? Is it adjusted such that the singular values of $\\psi_s$ have a certain decay? Why is $s$ constant across layers? How is the spectrum of psi_s? Is it well conditionned? A huge difference with (Hammond et al,2011) is that they use a collection of wavelets. Did you consider this kind of approach ? Is there a loss of information in some cases?(like if $\\psi_s$ has a fast decay) 7/ (2.3) \u201cThe matrix $\\psi_s$ and $\\psi_s^{-1}$ are both sparse\u201d. This is a critical affirmation which is not in general true. It is possibly sparse if the weighted laplacian graph is sparse as well, as explained by the remark after theorem 5.5 of page 16 of (Hammond et al, 2011). However, I do agree this typically happens in the application you present. 8/ The (c) of Figure 1 is missing.(in my version at least) 9/ In section 5.3, why not trying to compare the number of parameters with other papers? I think also more results, with maybe a bit higher performances, could be reported, such as: GraphSGAN, GAT. But that\u2019s fine to me. 10/ Appendix A, isn\u2019t it a simple rephrasing of (Hammond et al, 2011)? 11/ How do you compare in term of interpretability with [1]? 12/ Just as a suggestion and/or a comment: it seems similar to approaches such as lifting schemes(which basically builds wavelets on graphs/manifolds), except that there is no learning involved. (e.g. [2]) I think there could be great connexions. [1] Relational inductive biases, deep learning, and graph networks, Battaglia et al 2011? [2] THE LIFTING SCHEME: A CONSTRUCTION OF SECOND GENERATION WAVELETS, Wim Sweldgens", "rating": "7: Good paper, accept", "reply_text": "Q7 : ( 2.3 ) \u201c The matrix $ \\psi_s $ and $ \\psi_s^ { -1 } $ are both sparse \u201d . This is a critical affirmation which is not in general true . It is possibly sparse if the weighted laplacian graph is sparse as well , as explained by the remark after theorem 5.5 of page 16 of ( Hammond et al , 2011 ) . However , I do agree this typically happens in the application you present . A7 : Thank you for drawing our attention to this point . Indeed , the sparsity of the matrix $ \\psi_s $ and $ \\psi_s^ { -1 } $ is related to the sparsity of the Laplacian matrix . Generally , real world networks are sparse with the number of edges much less than the square of the number of nodes . In these cases , the matrix $ \\psi_s $ and $ \\psi_s^ { -1 } $ are both sparse . Prompted by your comments , we revised the statement to offer an accurate claim . Q8 : The ( c ) of Figure 1 is missing . ( in my version at least ) A8 : We corrected the caption of Figure 1 in the revised version . Q9 : In section 5.3 , why not trying to compare the number of parameters with other papers ? I think also more results , with maybe a bit higher performances , could be reported , such as : GraphSGAN , GAT . But that \u2019 s fine to me . A9 : We agree that it is always better to compare with more methods . Yet , the major contribution is about improving spectral methods by using graph wavelets instead of eigenvectors as a set of bases . Therefore , we focus on the comparison with spectral methods . We particularly appreciate your understanding . Q10 : Appendix A , isn \u2019 t it a simple rephrasing of ( Hammond et al , 2011 ) ? A10 : Hammond et al . ( 2011 ) described the locality of graph wavelets . Instead , in Appendix A , we demonstrate that the graph convolution via graph wavelets is localized , i.e. , the nodes used to update target node are its neighboring nodes . We clarified this point in the revised version . Q11 : How do you compare in term of interpretability with [ 1 ] ? A11 : We agree that the meaning of \u201c interpretability \u201d is diverse in the literature . In this paper , we use \u201c interpretability \u201d to offer some intuitive understanding for the wavelet transform , compared with the Fourier transform . In [ 1 ] , the proposed graph neural network , defining a flexible network via graph block , offers the interpretability as the correlation among nodes in spatial domain . Q12 : Just as a suggestion and/or a comment : it seems similar to approaches such as lifting schemes ( which basically builds wavelets on graphs/manifolds ) , except that there is no learning involved . ( e.g . [ 2 ] ) I think there could be great connexions . A12 : The paper you mentioned presents a simple construction of wavelets that could be adapted to graphs without learning process . We cited this paper as related work in the revised version ."}, {"review_id": "H1ewdiR5tQ-2", "review_text": "This paper proposed a new formulation of graph convolution, that is based on graph wavelet transform. The convolution network is expressed in eq.(4) with \\psi_s given by eq.(1). This new formulation is exciting in that it has numerous advantages comparing to spectral graph convolutions such as the sparsity of \\psi_s (see the items listed in section 2.3). The method showed marginal improvement (<1%) on node classification tasks of citation networks. Overall, I am convinced with the technical novelty and that the method can be promising. However, in its current form, there are several major weaknesses, hinting that this work needs further developments before publication. 1. By the vanilla implementation without any approximations, the multiplication with \\phi_s and \\phi_s^{-1} has quadratic complexity, and to compute these two matrices is of cubic complexity. This is not acceptable for large graphs. In section 3.2, the authors mentioned that Hammond et al (2011) have an approximation of \\phi_s and \\phi_s^{-1} with the complexity of O(|E|). However, this important technical detail is disappointingly missing in the main text. I have further concerns about whether the good properties listed in section 2.3 are preserved by this approximation, which is not discussed in the paper. Even with such an approximation, the matrix multiplication still has quadratic complexity. To reduce this complexity needs some non-trivial developments. I suggest the author(s) dive into the expression of \\phi_s and write the convolution in the vertex domain and seek possibilities for a linear approximation. 2. Experimentally, the improvement over GCN is marginal. Taking into account the implementation difficulty and complexity, the proposed method is more like a proof-of-concept instead of being of practical use. The authors are suggested to make the empirical study more comprehensive, by including node classification in an inductive setting, and/or including link prediction experiments. 3. The hyper-parameters (scale of the heat kernel) and t (threshold to zero the \\phi_s matrix) has to be tuned for each data set. This is not the ideal case because these parameters may not be easy to tune for real data sets, making the method difficult to use. The authors should at least give some recipes on how to tune these parameters. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q2 : Experimentally , the improvement over GCN is marginal . Taking into account the implementation difficulty and complexity , the proposed method is more like a proof-of-concept instead of being of practical use . The authors are suggested to make the empirical study more comprehensive , by including node classification in an inductive setting , and/or including link prediction experiments . A2 : In this paper , we focus on spectral methods for graph convolution . The standard spectral method , i.e. , the Spectral CNN , is not localized , limiting its performance . To achieve localization , GCN and ChebyNet are proposed to express the graph convolution defined via graph Fourier transform in vertex domain . Here , we propose a new formulation of graph convolution , i.e. , defining graph convolution via graph wavelet transform , achieving localization in both spectral and spatial domain . Experimental results demonstrate that the proposed GWNN method using graph wavelet transform remarkably ( achieving 10 % improvement on Cora and Citeseer , and 5 % improvement on Pubmed [ Table 3 ] ) outperforms the Spectral CNN method using graph Fourier transform . Meanwhile , GWNN also outperforms GCN and ChebyNet . We fully agree that it is always better to validate a new method on more scenarios and tasks . Here , following the common practice to evaluate spectral methods ( e.g. , GCN and ChebyNet ) for graph CNN , we validate our method on the widely-used playground , i.e. , node classification task on three benchmark datasets . Q3 : The hyper-parameters ( scale of the heat kernel ) and t ( threshold to zero the \\phi_s matrix ) has to be tuned for each data set . This is not the ideal case because these parameters may not be easy to tune for real data sets , making the method difficult to use . The authors should at least give some recipes on how to tune these parameters . A3 : The hyper-parameter $ s $ is used to modulate the range of neighborhood and the smoothness of graph wavelets . The hyper-parameter $ t $ is used only for computational consideration . We use cross-validation to determine the value of the hyper-parameter $ s $ and $ t $ , following the common practice of machine learning community . To offer more intuition , we add the analysis about the impact of hyper-parameters on the accuracy of graph-based semi-supervised learning in Appendix B , and demonstrate our recipes to tune hyper-parameters ."}], "0": {"review_id": "H1ewdiR5tQ-0", "review_text": "This paper proposes to learn graph wavelet kernels through a neural network. This idea is interesting, even if it is not a real surprise, given the interesting features graph wavelets, and the explosion of proposals for graph neural networks. Yet, the paper is interesting, pretty complete, and the proposed algorithm is shown to be relatively effective. A few more detailed comments: - one for the key motivations the work, namely to avoid eigen-decompositions, is also solved in a different way by methods using Chebyshev approximations (e.g., Khasanova - ICML 2017). The introduction should be clarified accordingly. - retaining the flexibility of convolution kernel (on page 2): what do the authors mean here? - the graph wavelet nn in 2.4 is certainly an interesting combination of known elements - yet, it is not very clear how the network is trained/optimized at this stage. - the section 3 is a bit confusing: are the proposed elements, contributions of the paper, or only ideas with future work? - the idea of detaching feature transformation from convolution is interesting: it would be even better to quantify or discuss the penalty, if any, induced by this design choice. - the results are generally fine and convincing, even if they are not super-impressive. ' GWNN is comfortably ahead of Spectral CNN. ' is probably an over-statement however... - the discussion about interpretability is interesting, and very trendy. However, what the authors discuss is mere localisation and sparsity - this is one way to 'interpret' interpretability of features, but that discussion should be rephrased in a more mild sense then. Generally, the ideas in this paper are interesting, even not surprising. The text should be clarified at places, and the paper looks a bit superficial on many aspects (see above). With good revision, it could lead to an interesting paper, and most likely to interesting discussions at ICLR.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the pertinent comments and accurately summarizing the major contributions of this paper . According to your constructive suggestions , we carefully revised our paper with much improvement . We now offer point-by-point response . Q1 : one for the key motivations the work , namely to avoid eigen-decompositions , is also solved in a different way by methods using Chebyshev approximations ( e.g. , Khasanova - ICML 2017 ) . The introduction should be clarified accordingly . A1 : We revised the statements relevant to this point , and cited the paper ( i.e. , Khasanova , ICML 2017 ) in the introduction part . Q2 : retaining the flexibility of convolution kernel ( on page 2 ) : what do the authors mean here ? A2 : We apologize for the misleading statement . What we mean is that the convolution kernel of ChebyNet is not flexible . Specifically , ChebyNet offers a $ K $ -order polynomial parameterization to graph convolution kernel . A smaller $ K $ causes high approximation bias , while a larger $ K $ results in non-localized convolution kernel . Therefore , ChebyNet has limited flexibility to define graph convolution kernel . In the revised version , we clarified this point in the last paragraph in Section 2.2 . Q3 : the graph wavelet nn in 2.4 is certainly an interesting combination of known elements - yet , it is not very clear how the network is trained/optimized at this stage . A3 : Thank you for pointing out this issue . Prompted by your suggestion , we revised Section 2.4 : ( 1 ) We offered detailed description about the architecture of graph wavelet neural network ; ( 2 ) We added the loss function when training graph wavelet neural networks on semi-supervised node classification task . Q4 : the section 3 is a bit confusing : are the proposed elements , contributions of the paper , or only ideas with future work ? A4 : We apologize for the confusing organization of sections . Indeed , Section 3 is the second contribution of this paper , i.e. , reducing the parameter complexity by detaching feature transformation from graph convolution . It is particularly important for training graph wavelet neural networks . For clarity , we reorganized the sections of this paper : combing the original Section 3 into Section 2 as a subsection , i.e. , Section 2.5 . Q5 : the idea of detaching feature transformation from convolution is interesting : it would be even better to quantify or discuss the penalty , if any , induced by this design choice . A5 : We are delighted to see that this idea is identified and approved . Detaching feature transformation from convolution , we remarkably reduce the number of parameters . This practice is particularly important for scenarios where labeled data is limited , e.g. , graph-based semi-supervised learning task considered in this paper . One potential penalty of this practice is that the modeling capacity is reduced . To quantify the penalty , in Section 4.4 , we compared the influence of detaching feature transformation from convolution to the performance of graph-based semi-supervised node classification . Results demonstrate that detaching feature transformation from convolution achieves comparable ( sometimes better ) performance , with the number of parameters remarkably reduced . Q6 : the results are generally fine and convincing , even if they are not super-impressive . ' GWNN is comfortably ahead of Spectral CNN . ' is probably an over-statement . A6 : For experimental evaluation , we compare the proposed GWNN with existing spectral methods , Spectral CNN , ChebyNet , GCN and some spatial methods like MoNet . The major contribution of this paper is to improve spectral methods , using graph wavelets rather than eigenvectors of Laplacian as bases . Thus , we focus on the comparison with spectral methods . By the sentence ' GWNN is comfortably ahead of Spectral CNN ' , we mean GWNN using graph wavelet transform is ahead of the Spectral CNN using Fourier transform , i.e. , GWNN ( the last row in Table 3 ) and Spectral CNN ( the ninth row in Table 3 ) . Indeed , GWNN is better than Spectral CNN by 10 % improvement on Cora and Citeseer and 5 % improvement on Pubmed . Q7 : the discussion about interpretability is interesting , and very trendy . However , what the authors discuss is mere localisation and sparsity - this is one way to 'interpret ' interpretability of features , but that discussion should be rephrased in a more mild sense then . A7 : Thank you for the inspiring comments . We revised the discussion about interpretability ( Section 4.7 ) , trying to ease the understanding of the interpretability of graph wavelets and graph convolution via graph wavelet transform ."}, "1": {"review_id": "H1ewdiR5tQ-1", "review_text": "This is an empirical paper that proposes to design wavelets on graphs, that can be integrated to neural networks on graphs. It permits to reducing the number of parameters of the \u00ab convolution \u00bb and exploits the sparsity of sparse weighted graphs for computations. I think it\u2019s an interesting work. The perspective I enjoy in using wavelets is that they typically provide a good trade-off in localization in the spectral and graph domains. For instance, large eigenvalues of the laplacian could be potentially captured in a more stable way. This type of work might be a first step. Pros : - good numerical results - nice incorporation of structure via wavelets Cons : - Sometimes the paper is not really clear I have severals comments: 1/ (1) \u00ab Some subsequent works devote to making spectral methods spectrum-free\u2026 avoiding high computational cost \u00bb. I think also those types of representations can be potentially unstable. That\u2019s a second reason. 2/ I\u2019m not sure to understand the point (1) of the fourth paragraph of the introduction. (1.)\u00ab graph wavelet does not depend on the eigen decomposition of Laplacian matrix \u00bb. Does it mean numerically ? This sentence is not clear, because even numerically, it can be done in a dependent way to this matrix. However, if it is implied that the fastest algorithm can be obtained without eigen-decomposition of the Laplacian Matrix, in a cheap way, then I agree and a small rephrasing could be nice. 3/ Please remove all the sentences that are supposed to be an introduction for a section, i.e. the sentences between 2 and 2.1 (\u00ab We use graph\u2026 \u00bb) and 3 and 3.1 (\u00ab In many research\u2026 \u00bb). They are poorly written and do not help the reader. 5/ .(2.2) \u201cHowever, such a polynomial approximation also limits the flexibility to define appropriate convolution on graph\u201d I\u2019m not sure to understand. In the paper you refer to, the set of filters span the polynomial of degree less than n, of a diagonalizable matrix of size nxn. Thus, lagrange polynomial basis could be used to interpolate any desired values? Does it signify learning non-diagonal(in the appropriate basis) matrix? 6/ (2.3) $s$ how is this parameter chosen, crossvalidation? Is it adjusted such that the singular values of $\\psi_s$ have a certain decay? Why is $s$ constant across layers? How is the spectrum of psi_s? Is it well conditionned? A huge difference with (Hammond et al,2011) is that they use a collection of wavelets. Did you consider this kind of approach ? Is there a loss of information in some cases?(like if $\\psi_s$ has a fast decay) 7/ (2.3) \u201cThe matrix $\\psi_s$ and $\\psi_s^{-1}$ are both sparse\u201d. This is a critical affirmation which is not in general true. It is possibly sparse if the weighted laplacian graph is sparse as well, as explained by the remark after theorem 5.5 of page 16 of (Hammond et al, 2011). However, I do agree this typically happens in the application you present. 8/ The (c) of Figure 1 is missing.(in my version at least) 9/ In section 5.3, why not trying to compare the number of parameters with other papers? I think also more results, with maybe a bit higher performances, could be reported, such as: GraphSGAN, GAT. But that\u2019s fine to me. 10/ Appendix A, isn\u2019t it a simple rephrasing of (Hammond et al, 2011)? 11/ How do you compare in term of interpretability with [1]? 12/ Just as a suggestion and/or a comment: it seems similar to approaches such as lifting schemes(which basically builds wavelets on graphs/manifolds), except that there is no learning involved. (e.g. [2]) I think there could be great connexions. [1] Relational inductive biases, deep learning, and graph networks, Battaglia et al 2011? [2] THE LIFTING SCHEME: A CONSTRUCTION OF SECOND GENERATION WAVELETS, Wim Sweldgens", "rating": "7: Good paper, accept", "reply_text": "Q7 : ( 2.3 ) \u201c The matrix $ \\psi_s $ and $ \\psi_s^ { -1 } $ are both sparse \u201d . This is a critical affirmation which is not in general true . It is possibly sparse if the weighted laplacian graph is sparse as well , as explained by the remark after theorem 5.5 of page 16 of ( Hammond et al , 2011 ) . However , I do agree this typically happens in the application you present . A7 : Thank you for drawing our attention to this point . Indeed , the sparsity of the matrix $ \\psi_s $ and $ \\psi_s^ { -1 } $ is related to the sparsity of the Laplacian matrix . Generally , real world networks are sparse with the number of edges much less than the square of the number of nodes . In these cases , the matrix $ \\psi_s $ and $ \\psi_s^ { -1 } $ are both sparse . Prompted by your comments , we revised the statement to offer an accurate claim . Q8 : The ( c ) of Figure 1 is missing . ( in my version at least ) A8 : We corrected the caption of Figure 1 in the revised version . Q9 : In section 5.3 , why not trying to compare the number of parameters with other papers ? I think also more results , with maybe a bit higher performances , could be reported , such as : GraphSGAN , GAT . But that \u2019 s fine to me . A9 : We agree that it is always better to compare with more methods . Yet , the major contribution is about improving spectral methods by using graph wavelets instead of eigenvectors as a set of bases . Therefore , we focus on the comparison with spectral methods . We particularly appreciate your understanding . Q10 : Appendix A , isn \u2019 t it a simple rephrasing of ( Hammond et al , 2011 ) ? A10 : Hammond et al . ( 2011 ) described the locality of graph wavelets . Instead , in Appendix A , we demonstrate that the graph convolution via graph wavelets is localized , i.e. , the nodes used to update target node are its neighboring nodes . We clarified this point in the revised version . Q11 : How do you compare in term of interpretability with [ 1 ] ? A11 : We agree that the meaning of \u201c interpretability \u201d is diverse in the literature . In this paper , we use \u201c interpretability \u201d to offer some intuitive understanding for the wavelet transform , compared with the Fourier transform . In [ 1 ] , the proposed graph neural network , defining a flexible network via graph block , offers the interpretability as the correlation among nodes in spatial domain . Q12 : Just as a suggestion and/or a comment : it seems similar to approaches such as lifting schemes ( which basically builds wavelets on graphs/manifolds ) , except that there is no learning involved . ( e.g . [ 2 ] ) I think there could be great connexions . A12 : The paper you mentioned presents a simple construction of wavelets that could be adapted to graphs without learning process . We cited this paper as related work in the revised version ."}, "2": {"review_id": "H1ewdiR5tQ-2", "review_text": "This paper proposed a new formulation of graph convolution, that is based on graph wavelet transform. The convolution network is expressed in eq.(4) with \\psi_s given by eq.(1). This new formulation is exciting in that it has numerous advantages comparing to spectral graph convolutions such as the sparsity of \\psi_s (see the items listed in section 2.3). The method showed marginal improvement (<1%) on node classification tasks of citation networks. Overall, I am convinced with the technical novelty and that the method can be promising. However, in its current form, there are several major weaknesses, hinting that this work needs further developments before publication. 1. By the vanilla implementation without any approximations, the multiplication with \\phi_s and \\phi_s^{-1} has quadratic complexity, and to compute these two matrices is of cubic complexity. This is not acceptable for large graphs. In section 3.2, the authors mentioned that Hammond et al (2011) have an approximation of \\phi_s and \\phi_s^{-1} with the complexity of O(|E|). However, this important technical detail is disappointingly missing in the main text. I have further concerns about whether the good properties listed in section 2.3 are preserved by this approximation, which is not discussed in the paper. Even with such an approximation, the matrix multiplication still has quadratic complexity. To reduce this complexity needs some non-trivial developments. I suggest the author(s) dive into the expression of \\phi_s and write the convolution in the vertex domain and seek possibilities for a linear approximation. 2. Experimentally, the improvement over GCN is marginal. Taking into account the implementation difficulty and complexity, the proposed method is more like a proof-of-concept instead of being of practical use. The authors are suggested to make the empirical study more comprehensive, by including node classification in an inductive setting, and/or including link prediction experiments. 3. The hyper-parameters (scale of the heat kernel) and t (threshold to zero the \\phi_s matrix) has to be tuned for each data set. This is not the ideal case because these parameters may not be easy to tune for real data sets, making the method difficult to use. The authors should at least give some recipes on how to tune these parameters. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q2 : Experimentally , the improvement over GCN is marginal . Taking into account the implementation difficulty and complexity , the proposed method is more like a proof-of-concept instead of being of practical use . The authors are suggested to make the empirical study more comprehensive , by including node classification in an inductive setting , and/or including link prediction experiments . A2 : In this paper , we focus on spectral methods for graph convolution . The standard spectral method , i.e. , the Spectral CNN , is not localized , limiting its performance . To achieve localization , GCN and ChebyNet are proposed to express the graph convolution defined via graph Fourier transform in vertex domain . Here , we propose a new formulation of graph convolution , i.e. , defining graph convolution via graph wavelet transform , achieving localization in both spectral and spatial domain . Experimental results demonstrate that the proposed GWNN method using graph wavelet transform remarkably ( achieving 10 % improvement on Cora and Citeseer , and 5 % improvement on Pubmed [ Table 3 ] ) outperforms the Spectral CNN method using graph Fourier transform . Meanwhile , GWNN also outperforms GCN and ChebyNet . We fully agree that it is always better to validate a new method on more scenarios and tasks . Here , following the common practice to evaluate spectral methods ( e.g. , GCN and ChebyNet ) for graph CNN , we validate our method on the widely-used playground , i.e. , node classification task on three benchmark datasets . Q3 : The hyper-parameters ( scale of the heat kernel ) and t ( threshold to zero the \\phi_s matrix ) has to be tuned for each data set . This is not the ideal case because these parameters may not be easy to tune for real data sets , making the method difficult to use . The authors should at least give some recipes on how to tune these parameters . A3 : The hyper-parameter $ s $ is used to modulate the range of neighborhood and the smoothness of graph wavelets . The hyper-parameter $ t $ is used only for computational consideration . We use cross-validation to determine the value of the hyper-parameter $ s $ and $ t $ , following the common practice of machine learning community . To offer more intuition , we add the analysis about the impact of hyper-parameters on the accuracy of graph-based semi-supervised learning in Appendix B , and demonstrate our recipes to tune hyper-parameters ."}}