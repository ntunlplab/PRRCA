{"year": "2021", "forum": "RHY_9ZVcTa_", "title": "On Linear Identifiability of Learned Representations", "decision": "Reject", "meta_review": "This paper presents novel results on linear identifiability in discriminative models, with three of the four reviewers arguing for acceptance. The paper went through an extensive round of edits, which incorporated detailed responses to issues raised by the reviewers. \n\nWhile this paper would be a nice contribution to the conference, some reviewer concerns remain unresolved, so we encourage the authors to revise and resubmit to a future venue.", "reviews": [{"review_id": "RHY_9ZVcTa_-0", "review_text": "% % % post-rebuttal % % The authors replied to my comments related to the diversity condition in 3.2 and Theorem 1 . Their answers did not fully clarify my concerns or misunderstandings , and it seems the authors did n't make any changes in that regard in the revised version . Except if I missed something in the review system , they did not answer my other comments . % % % % % % % % % % % % % % % The paper discusses identifiability of learnt representations in supervised settings and under `` canonical discriminative '' modelling . It provides conditions under which two learnt representations are equivalent up to a linear transform . The paper discusses an important topic ( related in particular to explicability of DNNs ) and appears to provide valuable results but is difficult to follow . The paper is packed with results and difficult to read as important details are missing . A longer journal version would be more appropriate . Comments : - I got lost from the paragraph `` Diversity condition '' . I do n't understand the meaning of y_A and y_B . What do subscripts A and B refer to ? Then what do y_A^ ( i ) and y_B^ ( i ) refer to ? What are they exactly samples of ? Using illustrative examples would be very useful there . For example , what do they mean in classification where y is a label ? - Because I did n't understand the meaning of y_A and y_B , I failed to understand the impact of Theorem 1 . In particular , I do n't understand why Eq . ( 5 ) holds in the proof . - I am mildly familiar with the `` canonical discriminative '' formulation . Can all DNN tasks but represented as such ? How does it relate to more traditional forms of deep learning , such as based on minimisation of cross entropy or quadratic loss ? I assume that the sum in Eq . ( 1 ) is an integral when y is continuous . - I got the idea of using CCA for comparing two different representations but the exact definition of C_i is unclear . Is it a vector or matrix ? Besides , why using only a subset B of D ? - In the experiments you basically compare two learnt representations and show they are essentially similar up to a linear transform . However it 's not clear how the two representations have been obtained . Is it only a matter of different initialisations ? - I did n't understand how are the labels constructed in Section 5.1 . Do you simply divide the unit circle in K=18 vectors that form the mean of each cluster ? I did n't get the `` model misspecification '' argument and the need to use DNN on such a simple task . - `` This show that increasing model size correlates strongly with increase in linear similarity of learned representations . '' I understand why this can be true when increasing number of samples but I do n't understand why this should be expected with model capacity . What is it in Theorem 1 that reflects this ? Minor : - homogenise use of `` Equation ( x ) '' and `` equation ( x ) '' - missing compiled reference at the beginning of Section 4 - it looks like you are using two different fonts for f_theta in the paper . Or are those different variables ? - many typos : `` share parameters '' , `` but OF the network '' , `` a the '' , ...", "rating": "4: Ok but not good enough - rejection", "reply_text": "> > > Because I did n't understand the meaning of y_A and y_B , I failed to understand the impact of Theorem 1 . In particular , I do n't understand why Eq . ( 5 ) holds in the proof . Equation ( 5 ) holds based on the definitions in Section 2 and from the assumption in the statement to be proved . We hope that we have made this clearer through our explanations above . The impact of Theorem 1 is to tell us what is true for pairs of parameters in the model family defined in Section 2 when the data is sufficiently diverse ( which is the diversity condition , which tells us when the matrices L * and L \u2019 in the proof are invertible ) . When these conditions hold , Theorem 1 establishes that the learned representations are equal up to a linear transformation . ( Comment 2 of 2 )"}, {"review_id": "RHY_9ZVcTa_-1", "review_text": "In this paper , the authors address the model identifiability in a general setting that can be adapted to several recent deep learning models ( DNN supervised learning , CPC , BERT and GPT ) . Since model parameters ( NN weights ) are not identifiable , the authors hypothesize that vector f and g can be identifiably up to a linear transformation . Although the purpose of the work is appealing , there are some issues related to the current structure of the paper , proposed theory and its relationship to the provided experiments . See my detailed comments and questions below : - Figure 1 : This figure is not referenced in the text . It should be referenced as a motivating fact . - Figure 1 : It is mentioned in the Appendix that the subset of words was selected . How these subsets were chosen ? Randomly ? I think this is important to clarify . - Theoretical claims : This figure clearly illustrate a strong relationship between f \u2019 and f * , which is close to a linear mapping , but it is not exactly linear . The main result of the paper is that if diversity condition is met , then models are linearly identifiable , but nothing is said about approximately linearly identifiable models as the ones shown in Figure 1 . One question that should be addressed is how a perturbation in the diversity condition is translated to the identifiability property . It would be good that the authors add some theory about it . - It is suggested that eq . ( 1 ) holds for arbitrary supervised learning problem but , in the Appendix , the equivalence is not shown for this case . Also , the example provided in Fig.2 seems to be a very arbitrary case . Is it possible to write any supervised learning problem as eq . ( 1 ) ? Could you please include its derivation ? - Is the form of eq . ( 1 ) already used as a general approach for different ML problems as the authors claim ? If so , could you please cite relevant references ? - Diversity condition means that such invertible matrices L \u2019 and L * can be constructed from data samples . What can be said if those matrices are invertible but ill-conditioned , i.e.smallest singular value close to zero ? - For supervised learning the condition implies that K > = M+1 . Is it not a very restrictive condition ? For example , it is possible to apply this analysis to a simple 2-classes supervised classification setting . - Agreement between theory and experimental results . With exception of the first experiment ( supervised learning ) the rest of the experiments show that by increasing iterations , dataset size and number of hidden units , then functions f and g tends to be linearly related ( but not closely linear related ) . I don \u2019 t see why the main theoretical result of the paper ( Theorem 1 ) is related to the experimental observations . I disagree with the authors claim that \u201c these experiments validate Theorem 1 \u201d", "rating": "6: Marginally above acceptance threshold", "reply_text": "> Diversity condition means that such invertible matrices L \u2019 and L * can be constructed from data samples . What can be said if those matrices are invertible but ill-conditioned , i.e.smallest singular value close to zero ? Thanks for this interesting question ! If the invertible matrices L \u2019 and L * are ill-conditioned and the diversity condition holds , this means that at least one column could nearly be written as a linear combination of the others , in each matrix respectively . This implies that there exists some tuple ( y_A^ ( i ) , y_B^ ( i ) ) such that the resulting difference vector d_i = g ( y_A^ ( i ) ) - g ( y_B^ ( i ) ) can almost be written as a linear combination of the other columns . Since the ill conditioning in this case is a function of the choice of points that yield the difference vectors ( matrix columns ) , the problem could be handled by resampling different data points until the condition number of the matrices are satisfactory . This amounts to strengthening the diversity condition . We will add the above to the paper . > For supervised learning the condition implies that K > = M+1 . Is it not a very restrictive condition ? For example , it is possible to apply this analysis to a simple 2-classes supervised classification setting . Yes , we agree that this is a rather restrictive condition . Our paper is focused on representation learning using , e.g. , self-supervised learning . In this case , the set S and possible values of the target variable y are very large : the elements in S are generated stochastically for each datapoint x in the batch ( i.e. , random patches of some window size from x , further processed with rotations and other noise-inducing transforms ) , making k > = M+1 easy to satisfy . This is also true for many language tasks , as the output vocabulary is much larger than the features dimension . > Agreement between theory and experimental results . With exception of the first experiment ( supervised learning ) the rest of the experiments show that by increasing iterations , dataset size and number of hidden units , then functions f and g tends to be linearly related ( but not closely linear related ) . I don \u2019 t see why the main theoretical result of the paper ( Theorem 1 ) is related to the experimental observations . We apologize for the lack of clarity about how the experimental results relate to Theorem 1 . We attempted to address this in the first paragraph of the experiments section , we wrote : `` The derivation in Section 3 shows that , for models in the general discriminative family defined in Section 2 , the functions f and g are identifiable up to a linear transformation given unbounded data and assuming model convergence . The question remains as to how close a model trained on finite data and without convergence guarantees will approach this limit\u2013that is , the domain of deep learning practice . Results in this section present evidence for : ( 1 ) close convergence in the small dimensional , large data regime ; and ( 2 ) monotonic increase in linear similarity of learned representations as a function of dataset size and model capacity in the high dimensional regime . '' We appreciate that this was not sufficient for understanding , though , and will add a note to the end of the experimental results section that restates and summarizes the connection . Stated differently from above , we show that the result holds as expected when the dimensionality is relatively small and there is a lot of data , and we can control for other factors . In this situation ( figures 1 and 2 ) , the learned embeddings are closely linearly related . Because the embeddings also depend on how well the model is optimized , this becomes more challenging in higher dimensions , and so we instead show tendency towards the linear relationship that Theorem 1 asserts . With better optimization and increasing amounts of data , we expect that our result will become more and more practical in the future . At present , we can only prove that it is true and provide empirical support for it . > I disagree with the authors claim that \u201c these experiments validate Theorem 1 \u201d We agree that \u201c validate \u201d is a poor word choice here . Theorem 1 is true by the proof we presented in the main body and in the Appendix , regardless of the empirical results . We will change this to \u201c these experiments provide support for the result in the practical regime , \u201d and will add additional discussion in the rebuttal draft ."}, {"review_id": "RHY_9ZVcTa_-2", "review_text": "This paper investigated the identifiability of the learned representations in pre-trained DNN models that fall into a general class of function space , defined by the canonical mathematical form . The identifiability of learned representations , in this paper , is defined as the representations are reproducible on the same data distribution , regardless of the randomness in the training procedure such as the random initialization of parameters and the stochastic optimization procedure . The authors first proved that , in the limit of infinite data , learned representations in this family are asymptotic identifiable in function space up to a linear transformation , with the additional assumption of the diverse condition . They further showed that this property is applicable to several state-of-the-art pre-trained models , including CTC , BERT and GPT-2 and GPT-3 . At last , the authors conducted experiments to empirically investigate the linear identifiability of DNN models in a practical setting : finite data and partial optimization . They adapted Canonical Correlation Analysis ( CCA ) ( and SVCCA for high-dimensional space ) to measure the linear similarity between two learned representations . Results from three sets of experiments , including classification , self-supervised learning for images ( CTC ) and for texts ( GPT-2 ) . Results show that the learned representations , after mapping through the optimal linear transformation from CCA , have a strongly linear relationship . Overall , this paper is well-written and well-motivated . From the theoretical aspect , this paper proved the linear identifiability of a large class of DNN models in an ideal setting . From the empirical aspect , they provided experimental results to demonstrate the theorem in a practical setting .", "rating": "7: Good paper, accept", "reply_text": "We appreciate the time you spent reviewing and your careful attention to and comments on the paper . We are glad you found the result interesting , and hope that you find the theorem useful in framing your own research topics ."}, {"review_id": "RHY_9ZVcTa_-3", "review_text": "This paper claims that , through Canonical Correlation Analysis on the representations learnt by popular deep models , we can show that the representations learnt on the same dataset , with same model using different sets of parameters , the representations are linearly identifiable . This seems like a nice thing to know . But I however am not sure about few things . 1 ) I think the paper does n't really discuss the implications of this enough . That is , I would like to see few example applications where we take advantage of this fact . As is , this contribution in my opinion remains as a cute thing to know . 2 ) It would be interesting to how much would change in initializations would effect the CCA curves . That is , if I initialize my network within a wider range , the identifiability result would still hold in practice ? This would be interesting to add in my opinion . 3 ) Ideally I would like to see if this statement hold for different hyperparameters also . Overall , the conclusion is interesting and I think that it could be published .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the time and efforts given to reviewing our paper . Your questions are important for understanding the present and future impact of our result and we appreciate the opportunity they give us to address them in our rebuttal . We respond below to your questions . We kindly request that you briefly read these responses and let us know if they do not fully satisfy your concerns and curiosity about the result or its implications and potential applications . > I think the paper does n't really discuss the implications of this enough . That is , I would like to see few example applications where we take advantage of this fact . As is , this contribution in my opinion remains as a cute thing to know . We agree that more discussion on implications and practical benefits would be helpful and appreciate the suggestion . We have added the following paragraph to the paper : \u201c Identifiability results are ( as we aim to demonstrate ) helpful in predicting when learned optimal representations are easily reproducible or not . While non-identifiable models might need to be trained many times to reproduce optimal results , identifiable architectures would ( in principle ) only need to be trained once , reducing the amount of computational resources required , thereby reducing energy waste , as well as saving budget and labour time . Moreover , the field of deep learning is characterized by increasingly efficient function approximators trained on growing quantities of data . This trend interfaces with the asymptotic character of Theorem 1 and linear identifiability in a productive way : as network architecture design and optimization continues to improve and more data is collected , we expect the representation functions approximated by deep neural networks to approach a stable set of optima in function space. \u201d As a side note , representation learning is the central topic of this paper , and so learning disentangled representations and causal representations for , e.g. , medical applications are also interesting applications . Due to space constraints we did not explore these in detail , but will add in comments . Our goal in this paper was to introduce the idea so that practical applications can follow on a solid theoretical basis . > It would be interesting to how much would change in initializations would effect the CCA curves . That is , if I initialize my network within a wider range , the identifiability result would still hold in practice ? This would be interesting to add in my opinion . Thanks for this interesting question ! We will add a discussion section to the end of the empirical results section to address this point . Whether the curves look the same depends on how the models are optimized : if the optimization routines ( like Adam , AdaGrad , etc . ) are robust to wider initialization within a certain range , we can expect the curves to look similar but take longer to converge . However , this can not make up for poor initialization or poor optimization , however : just as in any deep neural network , a poor initialization and inadequate optimizer will interfere with learning the model parameters . In the case of a linearly identifiable model , means that the learned representations would not appear to be within a linear transformation of each other , since the models have failed to converge due to poor initialization . We would like to note that all models were initialized using standard techniques in deep learning , and so our results are representative of the typical use case of deep neural networks for representation learning . We will add this remark to the paper as well . > Ideally I would like to see if this statement hold for different hyperparameters also . Thanks also for this interesting question ! We will add a variation of the following answer to the aforementioned discussion section in the rebuttal draft : \u201c As the hyperparameters of a model are changed , this changes what class of functions the network could learn ( i.e. , the size and stride of convolution filters will change what input pixels could be correlated in deeper layers ) . Typically in deep learning models , the hyperparameters are carefully tuned using cross validation based on held-out data . We did so in our experiments also . We expect that such a tuning procedure would yield hyperparameters that are as good as possible for the model to be optimized , allowing sufficient optimization so that the linear identifiability of the learned representations is realized . If the hyperparameters are sufficiently bad and optimization suffers , this will interfere with linear identifiability of the learned representations also. \u201d We hope the above answers your questions !"}], "0": {"review_id": "RHY_9ZVcTa_-0", "review_text": "% % % post-rebuttal % % The authors replied to my comments related to the diversity condition in 3.2 and Theorem 1 . Their answers did not fully clarify my concerns or misunderstandings , and it seems the authors did n't make any changes in that regard in the revised version . Except if I missed something in the review system , they did not answer my other comments . % % % % % % % % % % % % % % % The paper discusses identifiability of learnt representations in supervised settings and under `` canonical discriminative '' modelling . It provides conditions under which two learnt representations are equivalent up to a linear transform . The paper discusses an important topic ( related in particular to explicability of DNNs ) and appears to provide valuable results but is difficult to follow . The paper is packed with results and difficult to read as important details are missing . A longer journal version would be more appropriate . Comments : - I got lost from the paragraph `` Diversity condition '' . I do n't understand the meaning of y_A and y_B . What do subscripts A and B refer to ? Then what do y_A^ ( i ) and y_B^ ( i ) refer to ? What are they exactly samples of ? Using illustrative examples would be very useful there . For example , what do they mean in classification where y is a label ? - Because I did n't understand the meaning of y_A and y_B , I failed to understand the impact of Theorem 1 . In particular , I do n't understand why Eq . ( 5 ) holds in the proof . - I am mildly familiar with the `` canonical discriminative '' formulation . Can all DNN tasks but represented as such ? How does it relate to more traditional forms of deep learning , such as based on minimisation of cross entropy or quadratic loss ? I assume that the sum in Eq . ( 1 ) is an integral when y is continuous . - I got the idea of using CCA for comparing two different representations but the exact definition of C_i is unclear . Is it a vector or matrix ? Besides , why using only a subset B of D ? - In the experiments you basically compare two learnt representations and show they are essentially similar up to a linear transform . However it 's not clear how the two representations have been obtained . Is it only a matter of different initialisations ? - I did n't understand how are the labels constructed in Section 5.1 . Do you simply divide the unit circle in K=18 vectors that form the mean of each cluster ? I did n't get the `` model misspecification '' argument and the need to use DNN on such a simple task . - `` This show that increasing model size correlates strongly with increase in linear similarity of learned representations . '' I understand why this can be true when increasing number of samples but I do n't understand why this should be expected with model capacity . What is it in Theorem 1 that reflects this ? Minor : - homogenise use of `` Equation ( x ) '' and `` equation ( x ) '' - missing compiled reference at the beginning of Section 4 - it looks like you are using two different fonts for f_theta in the paper . Or are those different variables ? - many typos : `` share parameters '' , `` but OF the network '' , `` a the '' , ...", "rating": "4: Ok but not good enough - rejection", "reply_text": "> > > Because I did n't understand the meaning of y_A and y_B , I failed to understand the impact of Theorem 1 . In particular , I do n't understand why Eq . ( 5 ) holds in the proof . Equation ( 5 ) holds based on the definitions in Section 2 and from the assumption in the statement to be proved . We hope that we have made this clearer through our explanations above . The impact of Theorem 1 is to tell us what is true for pairs of parameters in the model family defined in Section 2 when the data is sufficiently diverse ( which is the diversity condition , which tells us when the matrices L * and L \u2019 in the proof are invertible ) . When these conditions hold , Theorem 1 establishes that the learned representations are equal up to a linear transformation . ( Comment 2 of 2 )"}, "1": {"review_id": "RHY_9ZVcTa_-1", "review_text": "In this paper , the authors address the model identifiability in a general setting that can be adapted to several recent deep learning models ( DNN supervised learning , CPC , BERT and GPT ) . Since model parameters ( NN weights ) are not identifiable , the authors hypothesize that vector f and g can be identifiably up to a linear transformation . Although the purpose of the work is appealing , there are some issues related to the current structure of the paper , proposed theory and its relationship to the provided experiments . See my detailed comments and questions below : - Figure 1 : This figure is not referenced in the text . It should be referenced as a motivating fact . - Figure 1 : It is mentioned in the Appendix that the subset of words was selected . How these subsets were chosen ? Randomly ? I think this is important to clarify . - Theoretical claims : This figure clearly illustrate a strong relationship between f \u2019 and f * , which is close to a linear mapping , but it is not exactly linear . The main result of the paper is that if diversity condition is met , then models are linearly identifiable , but nothing is said about approximately linearly identifiable models as the ones shown in Figure 1 . One question that should be addressed is how a perturbation in the diversity condition is translated to the identifiability property . It would be good that the authors add some theory about it . - It is suggested that eq . ( 1 ) holds for arbitrary supervised learning problem but , in the Appendix , the equivalence is not shown for this case . Also , the example provided in Fig.2 seems to be a very arbitrary case . Is it possible to write any supervised learning problem as eq . ( 1 ) ? Could you please include its derivation ? - Is the form of eq . ( 1 ) already used as a general approach for different ML problems as the authors claim ? If so , could you please cite relevant references ? - Diversity condition means that such invertible matrices L \u2019 and L * can be constructed from data samples . What can be said if those matrices are invertible but ill-conditioned , i.e.smallest singular value close to zero ? - For supervised learning the condition implies that K > = M+1 . Is it not a very restrictive condition ? For example , it is possible to apply this analysis to a simple 2-classes supervised classification setting . - Agreement between theory and experimental results . With exception of the first experiment ( supervised learning ) the rest of the experiments show that by increasing iterations , dataset size and number of hidden units , then functions f and g tends to be linearly related ( but not closely linear related ) . I don \u2019 t see why the main theoretical result of the paper ( Theorem 1 ) is related to the experimental observations . I disagree with the authors claim that \u201c these experiments validate Theorem 1 \u201d", "rating": "6: Marginally above acceptance threshold", "reply_text": "> Diversity condition means that such invertible matrices L \u2019 and L * can be constructed from data samples . What can be said if those matrices are invertible but ill-conditioned , i.e.smallest singular value close to zero ? Thanks for this interesting question ! If the invertible matrices L \u2019 and L * are ill-conditioned and the diversity condition holds , this means that at least one column could nearly be written as a linear combination of the others , in each matrix respectively . This implies that there exists some tuple ( y_A^ ( i ) , y_B^ ( i ) ) such that the resulting difference vector d_i = g ( y_A^ ( i ) ) - g ( y_B^ ( i ) ) can almost be written as a linear combination of the other columns . Since the ill conditioning in this case is a function of the choice of points that yield the difference vectors ( matrix columns ) , the problem could be handled by resampling different data points until the condition number of the matrices are satisfactory . This amounts to strengthening the diversity condition . We will add the above to the paper . > For supervised learning the condition implies that K > = M+1 . Is it not a very restrictive condition ? For example , it is possible to apply this analysis to a simple 2-classes supervised classification setting . Yes , we agree that this is a rather restrictive condition . Our paper is focused on representation learning using , e.g. , self-supervised learning . In this case , the set S and possible values of the target variable y are very large : the elements in S are generated stochastically for each datapoint x in the batch ( i.e. , random patches of some window size from x , further processed with rotations and other noise-inducing transforms ) , making k > = M+1 easy to satisfy . This is also true for many language tasks , as the output vocabulary is much larger than the features dimension . > Agreement between theory and experimental results . With exception of the first experiment ( supervised learning ) the rest of the experiments show that by increasing iterations , dataset size and number of hidden units , then functions f and g tends to be linearly related ( but not closely linear related ) . I don \u2019 t see why the main theoretical result of the paper ( Theorem 1 ) is related to the experimental observations . We apologize for the lack of clarity about how the experimental results relate to Theorem 1 . We attempted to address this in the first paragraph of the experiments section , we wrote : `` The derivation in Section 3 shows that , for models in the general discriminative family defined in Section 2 , the functions f and g are identifiable up to a linear transformation given unbounded data and assuming model convergence . The question remains as to how close a model trained on finite data and without convergence guarantees will approach this limit\u2013that is , the domain of deep learning practice . Results in this section present evidence for : ( 1 ) close convergence in the small dimensional , large data regime ; and ( 2 ) monotonic increase in linear similarity of learned representations as a function of dataset size and model capacity in the high dimensional regime . '' We appreciate that this was not sufficient for understanding , though , and will add a note to the end of the experimental results section that restates and summarizes the connection . Stated differently from above , we show that the result holds as expected when the dimensionality is relatively small and there is a lot of data , and we can control for other factors . In this situation ( figures 1 and 2 ) , the learned embeddings are closely linearly related . Because the embeddings also depend on how well the model is optimized , this becomes more challenging in higher dimensions , and so we instead show tendency towards the linear relationship that Theorem 1 asserts . With better optimization and increasing amounts of data , we expect that our result will become more and more practical in the future . At present , we can only prove that it is true and provide empirical support for it . > I disagree with the authors claim that \u201c these experiments validate Theorem 1 \u201d We agree that \u201c validate \u201d is a poor word choice here . Theorem 1 is true by the proof we presented in the main body and in the Appendix , regardless of the empirical results . We will change this to \u201c these experiments provide support for the result in the practical regime , \u201d and will add additional discussion in the rebuttal draft ."}, "2": {"review_id": "RHY_9ZVcTa_-2", "review_text": "This paper investigated the identifiability of the learned representations in pre-trained DNN models that fall into a general class of function space , defined by the canonical mathematical form . The identifiability of learned representations , in this paper , is defined as the representations are reproducible on the same data distribution , regardless of the randomness in the training procedure such as the random initialization of parameters and the stochastic optimization procedure . The authors first proved that , in the limit of infinite data , learned representations in this family are asymptotic identifiable in function space up to a linear transformation , with the additional assumption of the diverse condition . They further showed that this property is applicable to several state-of-the-art pre-trained models , including CTC , BERT and GPT-2 and GPT-3 . At last , the authors conducted experiments to empirically investigate the linear identifiability of DNN models in a practical setting : finite data and partial optimization . They adapted Canonical Correlation Analysis ( CCA ) ( and SVCCA for high-dimensional space ) to measure the linear similarity between two learned representations . Results from three sets of experiments , including classification , self-supervised learning for images ( CTC ) and for texts ( GPT-2 ) . Results show that the learned representations , after mapping through the optimal linear transformation from CCA , have a strongly linear relationship . Overall , this paper is well-written and well-motivated . From the theoretical aspect , this paper proved the linear identifiability of a large class of DNN models in an ideal setting . From the empirical aspect , they provided experimental results to demonstrate the theorem in a practical setting .", "rating": "7: Good paper, accept", "reply_text": "We appreciate the time you spent reviewing and your careful attention to and comments on the paper . We are glad you found the result interesting , and hope that you find the theorem useful in framing your own research topics ."}, "3": {"review_id": "RHY_9ZVcTa_-3", "review_text": "This paper claims that , through Canonical Correlation Analysis on the representations learnt by popular deep models , we can show that the representations learnt on the same dataset , with same model using different sets of parameters , the representations are linearly identifiable . This seems like a nice thing to know . But I however am not sure about few things . 1 ) I think the paper does n't really discuss the implications of this enough . That is , I would like to see few example applications where we take advantage of this fact . As is , this contribution in my opinion remains as a cute thing to know . 2 ) It would be interesting to how much would change in initializations would effect the CCA curves . That is , if I initialize my network within a wider range , the identifiability result would still hold in practice ? This would be interesting to add in my opinion . 3 ) Ideally I would like to see if this statement hold for different hyperparameters also . Overall , the conclusion is interesting and I think that it could be published .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the time and efforts given to reviewing our paper . Your questions are important for understanding the present and future impact of our result and we appreciate the opportunity they give us to address them in our rebuttal . We respond below to your questions . We kindly request that you briefly read these responses and let us know if they do not fully satisfy your concerns and curiosity about the result or its implications and potential applications . > I think the paper does n't really discuss the implications of this enough . That is , I would like to see few example applications where we take advantage of this fact . As is , this contribution in my opinion remains as a cute thing to know . We agree that more discussion on implications and practical benefits would be helpful and appreciate the suggestion . We have added the following paragraph to the paper : \u201c Identifiability results are ( as we aim to demonstrate ) helpful in predicting when learned optimal representations are easily reproducible or not . While non-identifiable models might need to be trained many times to reproduce optimal results , identifiable architectures would ( in principle ) only need to be trained once , reducing the amount of computational resources required , thereby reducing energy waste , as well as saving budget and labour time . Moreover , the field of deep learning is characterized by increasingly efficient function approximators trained on growing quantities of data . This trend interfaces with the asymptotic character of Theorem 1 and linear identifiability in a productive way : as network architecture design and optimization continues to improve and more data is collected , we expect the representation functions approximated by deep neural networks to approach a stable set of optima in function space. \u201d As a side note , representation learning is the central topic of this paper , and so learning disentangled representations and causal representations for , e.g. , medical applications are also interesting applications . Due to space constraints we did not explore these in detail , but will add in comments . Our goal in this paper was to introduce the idea so that practical applications can follow on a solid theoretical basis . > It would be interesting to how much would change in initializations would effect the CCA curves . That is , if I initialize my network within a wider range , the identifiability result would still hold in practice ? This would be interesting to add in my opinion . Thanks for this interesting question ! We will add a discussion section to the end of the empirical results section to address this point . Whether the curves look the same depends on how the models are optimized : if the optimization routines ( like Adam , AdaGrad , etc . ) are robust to wider initialization within a certain range , we can expect the curves to look similar but take longer to converge . However , this can not make up for poor initialization or poor optimization , however : just as in any deep neural network , a poor initialization and inadequate optimizer will interfere with learning the model parameters . In the case of a linearly identifiable model , means that the learned representations would not appear to be within a linear transformation of each other , since the models have failed to converge due to poor initialization . We would like to note that all models were initialized using standard techniques in deep learning , and so our results are representative of the typical use case of deep neural networks for representation learning . We will add this remark to the paper as well . > Ideally I would like to see if this statement hold for different hyperparameters also . Thanks also for this interesting question ! We will add a variation of the following answer to the aforementioned discussion section in the rebuttal draft : \u201c As the hyperparameters of a model are changed , this changes what class of functions the network could learn ( i.e. , the size and stride of convolution filters will change what input pixels could be correlated in deeper layers ) . Typically in deep learning models , the hyperparameters are carefully tuned using cross validation based on held-out data . We did so in our experiments also . We expect that such a tuning procedure would yield hyperparameters that are as good as possible for the model to be optimized , allowing sufficient optimization so that the linear identifiability of the learned representations is realized . If the hyperparameters are sufficiently bad and optimization suffers , this will interfere with linear identifiability of the learned representations also. \u201d We hope the above answers your questions !"}}