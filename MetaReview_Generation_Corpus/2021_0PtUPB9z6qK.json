{"year": "2021", "forum": "0PtUPB9z6qK", "title": "Generalized Energy Based Models", "decision": "Accept (Poster)", "meta_review": "This paper introduces a generative model termed generalized energy-based model (GEBM).\n\nThe goal is modelling complex distributions supported on low-dimensional manifolds, while offering more flexibility in refining the distribution of mass on those manifolds. The key idea is presented as parametrizing the base measure (called a generator in the paper) and the density with respect to this base measure separately. Figure 1 of the paper sketches the idea on a very clear toy example.\n\nThe pros:\n* Flexibility: Decomposing the full problem as learning the support and learning the density on this support \n* Theoretical justification\n* Introducing the KALE objective\n* Comparative empirical results with GANs show the additional benefits. Empirically, the framework outperforms GAN with the same complexity.\n* Clear written paper\n\nThe lack of a comparison with GANs has been raised as a concern.  The authors have satisfactorily answered key questions and\nothers raised during rebuttal and added several new references. They have also improved the narrative and included an additional experiment to contrast GEBM and GANs in response to AnonReviewer2, also provided more detail \non how the energy function (class) is chosen. \n", "reviews": [{"review_id": "0PtUPB9z6qK-0", "review_text": "This paper proposes a framework called GEBM that combine an implicit generator and an EBM to define a probabilistic model on low dimensional manifold . Specifically , the implicit generator defines the base distribution , and the EBM refines the base . Finally , this method is equivalent to define an EBM on the latent space of the implicit generator together with a mapping from the latent space to the data space . The authors propose to use the KALE to train the generator , and provide a theoretical guarantee about the validness of the KALE . I personally enjoy reading this paper . It provides a good method to solve the problem where the EBM can not model the distribution whose support is a low-dimensional manifold in the data space . However , my main concern is about the contribution of this paper , which can be summarized as follows : 1 . The novelty . This paper mainly has two contributions : the generalized ebm framework and the KALE objective function . For the first contribution , the difference between the related work is discussed . But what \u2019 s the advantages ? When the base distribution is an implicit model , GEBM is still a GAN-like model : can not estimate density . When the base distribution is a flow model , GEBM is still an EBM : can not provide tractable partition function . 2.The validness of the proposed method . If the base distribution is an implicit model , which means that the support is only a subset of the data space . However , most implicit models , especially GANs , suffer from the mode collapse problem . In this case , the distribution defined by GEBM can not recover the data distribution . Is there any explanation about the mode collapse problem in GEBM ? Some minor concerns about this paper are provided as follows : 1 . The experimental results are chaotic . First , the neural architectures are not introduced , which is the key factor in the performance of GAN-like models . Second , Figure 1 needs more explanation . What is the architectures of the models ? Which GAN variant is used ? Is EBM trained by MLE using MCMC or score matching ? 2.What \u2019 s the main purpose of using the flow model as the density estimator ? As mentioned in Proposition 2 , GEBM is equivalent an EBM . Besides , in the experiments , the energy model is also an NVP . More explanation should be added to improve the clarity . Further , how NVP is trained with CD ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and for constructive remarks . We are happy to hear that you enjoyed reading the paper and that you found the proposed method as a good alternative to EBMs for data with support in a lower-dimensional manifold . We hope the following discussion answers your questions . We understand that the first two questions you raised are about the advantages/limitations of the proposed model ( GEBM ) ( Density estimation on Manifolds and Mode collapse ) more than they are about the novelty of the model or the validity of the proposed method . Thus we thank you for encouraging us to strengthen the discussion on the advantages/limitations of the model which we will include in the revised version of the paper . We refer to the related work section concerning the novelty and to the theory results for the validity of the method . # # # 1- The advantages of the method : # # # # 1- Density estimation on Manifold data : As you pointed out the GEBM has an advantage over EBM in that it can model data supported on a low-dimensional manifold . Under the manifold assumption , the data do not admit a density on the whole space , as a consequence , it is not possible to estimate this ( undefined ) quantity . What GEBM offers is the possibility to estimate a density relative to the support of the generator . Thus if the model learns the low dimensional support of data using the generator , the GEBM provides a way to model the density on that learned manifold . # # # # 2- Mode collapse : This is an important question , thank you for raising it . Mode collapse can happen if the model doesn \u2019 t have enough capacity to cover the data support . One setting where this can occur is when the data is supported over multiple disjoint sub-manifolds ( e.g.multiple CIFAR classes ) . Multimodality can be a challenge in GANs : the latent noise is usually unimodal , however the data may be highly multimodal . Hence , the generator needs to be very powerful/complicated to transform a unimodal gaussian into a distribution with multiple modes , as shown in Cornish et al 2020 . One ( unsatisfactory ) solution is for the generator to just \u201c collapse \u201d to a small number of easy-to-produce modes , rather than covering all modes . GEBMs have an advantage over GANs in producing highly multimodal outputs . GEBMs allow rich posterior latent distributions that can be multimodal * in latent space * ( by incorporating the energy function into the overall model ) . As a result , the generator doesn \u2019 t need to distort the inputs as much in order to produce multiple modes * in the observed space * . See the answer to reviewer 1 for more details . # # # 2- Motivation of the density estimation experiment . Thank you , we will add the following clarification : We are only interested in this setting as a sanity check for the proposed learning method . This particular setting allows exact computation of the likelihood . Outside of this setting , closed form expressions of the normalizing constant are not available for generic GEBMs . Note that this is not an issue since the proposed method doesn \u2019 t require a closed form expression for the normalizing constant . However , in this experiment only , we want to have access to closed form expressions of the likelihood as a baseline against which to compare other density estimation methods . Details for training the NVP using CD are provided in Appendix G.2 : It is trained as if the normalizing constant wasn \u2019 t known by running Langevin iterations to get samples from the model . This experiment is simply a sanity check and is not central to our setting , we are happy to move it to the appendix if the reviewers think it is more appropriate ( the experiments were in fact added in response to a reviewer request from a submission to an earlier conference ) . We emphasize again that the main strength and intended purpose of the model is when the base measure is lower-dimensional than the ambient space . # # # 3- \u201c Experimental results are chaotic \u201d . We respectfully disagree . We believe the experimental section 6 is structured and separated in two parts : Image generation and density estimation . Each part starts by describing the experimental setting then presents the result . We believe that we provided details about the architectures used for both the Generator and the Energy . We provide them again for the purpose of this discussion in the \u201c Experimental details \u201d response above . We are happy to provide any additional details upon request . We will also release the code for reproducing the experiments # # # 4- Details of Figure 1 in section 2 : We thank the reviewer for requesting the details of this illustrative example , which we think is of independent interest . We will provide those details in the appendix - for now we provide them in the \u201c Experimental details \u201d response above ."}, {"review_id": "0PtUPB9z6qK-1", "review_text": "Summary : In this work , a generalized energy-based model ( GEBM ) is proposed . During the generation , the base distribution and the energy cooperate to combine the strengths of both the energy-based model and the implicit generative model . +ves : 1.This paper has proposed a framework so that the energy function can be used to refine the probability mass on the learned base distribution . The framework is trained by alternating between learning the energy and the base . Empirically , the framework outperforms GAN with the same complexity . 2.KL Approximate Lower-bound Estimate ( KALE ) is used for energy training . There is a lot of detail on this derivation . Concerns : 1 . In this work , a new framework is proposed for training . The whole process seems complicated . Is there is a simple way to refine the probability mass on the learned base distribution ? How about considering exp ( -E ( x ) ) G ( x ) . Here G ( x ) is learned base distribution . A simple way to refine the probability mass is by sampling several generated output near the point and weight the outputs with exp ( -E ( x ) ) G ( x ) ? This seems easier . Could you explain the more possible benefits of your method ? 2.During the generation process , two algorithms are proposed : ULA and KLA . It seems a large variance . It is unclear which one is better . Questions during the rebuttal period : Please address and clarify the cons above : Missed related work . For the training of energy based models , there are several related work [ 2 ] [ 3 ] , etc . In the work [ 1 ] , the residual energy is used to better generation . MCMC is also used for generations . [ 1 ] Residual Energy-Based Models for Text Generation . ICLR 2020 [ 2 ] Structured Prediction Energy Networks , ICML 2016 [ 3 ] Learning Approximate Inference Networks for Structured Prediction . ICLR 2018", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the insightful and constructive remarks and for pointing to the related works which we will include in the revised version . We hope the following discussion answers your questions . # # # 1- What are the benefits of the proposed sampling compared to easier sampling methods ? Thank you for pointing out this essential question . In addition to the current discussion in the introduction and section 4 , we will add a discussion in section 4 on the advantage of MCMC over importance sampling ( IS ) , which is the alternative you suggested ( if we understood correctly ) : Indeed , IS is a simpler way to estimate expectations under the GEBM by sampling multiple points from the generator and then weighting using the energy . However , IS can lead to highly unreliable estimates , a well known problem in the Sequential Monte Carlo ( SMC ) literature which employs IS extensively [ 1,2 ] . Instead we use ULA/KLA which do not have these issues . The other advantages of the proposed method as discussed in section 4 and introduction are : - Latent sampling vs data-space sampling : Avoiding the curse of dimension . As shown in Prop 5 , sampling using methods that exploit the latent structure have a speed of convergence that depends only on the intrinsic dimension of the model ( latent dimension ) and not the ambient space dimension . This is a big advantage in practice for datasets like Images . - Using gradient information during MCMC ( as done with ULA and KLA ) is more efficient as it exploits the local slope of a density to find high density regions quickly . # # # 2- Sampling Algorithms ( ULA vs KLA ) which one is better ? This is an important question with implications even beyond the GEBM framework : ULA is the Unadjusted langevin Algorithm which doesn \u2019 t use a momentum variable . KLA is a kinetic sampler , it uses a momentum variable just like Hamiltonian Monte Carlo . The strength of the momentum depends on a friction parameter $ \\gamma $ . Higher friction leads to an algorithm that behaves like ULA . Lower friction leads to more exploration of the modes . - The experiments of Table 2 , use KLA in a high friction regime thus behaving essentially as ULA . For this experiment , we selected this high friction regime for KLA as we are interested in the sharpness of the generated images . - We also provide empirical evidence in figure 3 , 4 and 5 of appendix D for the qualitative difference in behavior between both : We found that KLA ( in a low friction regime ) is able to explore multiple modes of the distribution in the same chain , unlike ULA which remains in the same mode . As discussed in section 4 , this confirms prior theoretical and empirical results about KLA vs ULA . # # # # In conclusion : if the goal is to get sharper images and thus better FID scores , we recommend using either KLA with high friction or using ULA . If the goal is to explore multiple modes within the same MCMC chain , we recommend using KLA with a smaller friction parameter . # # # # References [ 1 ] : Del Moral , Pierre and Doucet , Arnaud and Jasra , Ajay , Sequential monte carlo samplers . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) [ 2 ] : Doucet , Arnaud and Freitas , Nando de and Gordon , Neil , Sequential Monte Carlo Methods in Practice"}, {"review_id": "0PtUPB9z6qK-2", "review_text": "Originality : The paper presents an interesting generative model termed generalized energy-based model ( GEBM ) , which is essentially an exponential tilting of a plain generator model P_G by an energy based model exp ( -E ) . The appearance of such a model in generative modeling looks to be new . But the structure of the model is not surprisingly new . Significance : The pros : 1 . The purpose of introducing such a model , as stated by the authors , is to make use of the fact that target distribution may concentrate in a low dimensional manifold in the target domain , which can be captured by a low-dimensional generator model P_G . Once tilted by exp ( -E ) , the overall model can generate sharper samples than the plain energy based model . 2.The paper also presents tractable methods to train and sample from the proposed GEBM . 3.The authors show that GEBM performs better than GANs on certain image generation experiments . The cons : While it may be easier to understand why GEBM can be better than EBM on certain generative modeling tasks , the authors did not provide much explanation or justification on why GEBM can outperform GANs . As mentioned by the authors , the energy based models and functional generators are quite different generative models . By combining them , how to guarante that their pros rather than the cons will be strengthened ? In what kind of tasks GEBM can perform better than GANs ? Hopefully the authors can give more theoretical comparisons or intuitions to address these questions . Quality : Pros : The overall quality of the paper is good . Nearly all results regarding the properties of GEBM come with theoretical justifications . The proposed alternative training method and sampling methods look reasonable . Cons : It would be better if there is some analytical comparison between GEBM and GAN . Also , in the experiment section , the authors should reveal more detail on how the energy function ( class ) is chosen . Another concern is that , for Proposition 1 to hold , does it require the generator G to be invertible ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the encouraging and constructive remarks and for the insightful questions . # # # 1- Exponential tilting interpretation : We really like the interpretation of GEBMs as an exponential tilting of a reference measure ( here the generator ) . We will add it to section 2 , further emphasizing how GEBM extends exponential tilting models by allowing the reference measure ( the generator ) to change its support /shape in space . This enables the GEBMs to learn the manifold of low dimensional data in addition to the tilting of mass on that manifold . With this new flexibility comes the question of learning reference measure as well , which is what we propose here . # # # 2- Theoretical advantage of GEBM relatively to GAN : This is an important question which encouraged us to strengthen our theory results . We will add a discussion on this matter in section 2 and add a proposition which provides insights on this question , which we summarize in 3 points : # # # # 1- The proposition : It considers the case when the generator is trained until it matches the support of the data . In this case , the KL between data $ P $ and generator $ G $ is defined and we show that $ KL ( P| Q ) < = KL ( P|G ) $ , where $ Q $ is the GEBM with the generator $ G $ as a base . The best case scenario is when the exponential tilting is equal to the density ratio between P and G , in which case $ KL ( P| Q ) =0 $ . This means that the exponential tilting of the generator $ G $ systematically improves over using the generator alone . # # # # 2- When can we expect most improvement over GAN ? According to the above result , if $ G $ is exactly equal to the data distribution $ P $ , then no further improvement is possible . But as long as there is an error in mass on this common support , the GEBM improves over the generator $ G $ . It may be that , apart from the error in mass on the support , the support itself is not correctly obtained . Even in this case , the addition of an energy function can improve over the generator alone . This is consistent with Table 1 which shows that the improvement is larger for the smaller networks ( SN-GAN ConvNet ) than for the larger one ( SN-GAN ResNet ) . Thus , we expect the highest improvement when the generator has a limited strength . # # # # 3- Multimodality : GEBMs vs GANs In GANs , the latent noise is usually unimodal ( a gaussian for instance ) , however the data is often highly multimodal . Hence , the generator needs to be very powerful/complicated to transform a simple gaussian into a distribution with multiple modes as shown in Cornish et al 2020 . On the other hand , EBMs allow to capture multimodality with rather simpler models . GEBMs build on this intuition and allow to use richer latent distributions ( thanks to proposition 1 ) that can be multimodal in latent space . The energy removes the additional burden from the generator of learning how to put weights on the support , so that it only focuses on getting the support correct . For instance , Table 1 shows that a smaller generator network ( SNGAN ConvNet ) combined with an energy achieves a better FID score on imagenet ( FID = 13.94 ) compared to a GAN using a larger generator SNGAN ResNet ( FID=20.5 ) . This improvement is observed on Imagenet which has many more classes than Cifar10 . As such , we expect it to be much more multimodal . Thus we think that GEBMs are the most useful in high multimodality scenarios . ( Please see also response to AnonReviewer2 . ) # # # # Summary : While these intuitions and empirical observations are favorable to GEBMs , a precise theoretical quantification requires a number of steps to be taken : - Identifying the modeling capacity of GANs and the additional capacity that the energy provides . - Providing a Generalization theory for f-divergences which are harder to analyze than IMPs ( Uppal 2019 ) . - Extending the generalization theory to the case where density ratios are not well defined . We provided analysis when the ratio is well-defined in Theorem 6 of appendix B . All those questions are challenging and interesting future research directions . # # # 3- Network Architectures : Thank you , we will include details for the illustrative example in figure 1 of section 2 ( please see the \u201c Experimental details \u201d response above for those details ) , we believe we provided details for the networks used in section 6 , but we are happy to include any additional details . We will also release the code for reproducing the experiments . # # # 4- Does the generator need to be invertible for proposition 1 to hold ? It might seem surprising at first , but this proposition doesn \u2019 t require invertibility of the generator . Proposition 1 says that exponential tilting of the generator is equivalent to an exponential tilting in latent space before mapping to data space . To prove this result ( in appendix C.1 ) , we rely on a characterization of probabilities through test functions : two probabilities are equal if they have the same expectation for any bounded continuous function ."}], "0": {"review_id": "0PtUPB9z6qK-0", "review_text": "This paper proposes a framework called GEBM that combine an implicit generator and an EBM to define a probabilistic model on low dimensional manifold . Specifically , the implicit generator defines the base distribution , and the EBM refines the base . Finally , this method is equivalent to define an EBM on the latent space of the implicit generator together with a mapping from the latent space to the data space . The authors propose to use the KALE to train the generator , and provide a theoretical guarantee about the validness of the KALE . I personally enjoy reading this paper . It provides a good method to solve the problem where the EBM can not model the distribution whose support is a low-dimensional manifold in the data space . However , my main concern is about the contribution of this paper , which can be summarized as follows : 1 . The novelty . This paper mainly has two contributions : the generalized ebm framework and the KALE objective function . For the first contribution , the difference between the related work is discussed . But what \u2019 s the advantages ? When the base distribution is an implicit model , GEBM is still a GAN-like model : can not estimate density . When the base distribution is a flow model , GEBM is still an EBM : can not provide tractable partition function . 2.The validness of the proposed method . If the base distribution is an implicit model , which means that the support is only a subset of the data space . However , most implicit models , especially GANs , suffer from the mode collapse problem . In this case , the distribution defined by GEBM can not recover the data distribution . Is there any explanation about the mode collapse problem in GEBM ? Some minor concerns about this paper are provided as follows : 1 . The experimental results are chaotic . First , the neural architectures are not introduced , which is the key factor in the performance of GAN-like models . Second , Figure 1 needs more explanation . What is the architectures of the models ? Which GAN variant is used ? Is EBM trained by MLE using MCMC or score matching ? 2.What \u2019 s the main purpose of using the flow model as the density estimator ? As mentioned in Proposition 2 , GEBM is equivalent an EBM . Besides , in the experiments , the energy model is also an NVP . More explanation should be added to improve the clarity . Further , how NVP is trained with CD ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and for constructive remarks . We are happy to hear that you enjoyed reading the paper and that you found the proposed method as a good alternative to EBMs for data with support in a lower-dimensional manifold . We hope the following discussion answers your questions . We understand that the first two questions you raised are about the advantages/limitations of the proposed model ( GEBM ) ( Density estimation on Manifolds and Mode collapse ) more than they are about the novelty of the model or the validity of the proposed method . Thus we thank you for encouraging us to strengthen the discussion on the advantages/limitations of the model which we will include in the revised version of the paper . We refer to the related work section concerning the novelty and to the theory results for the validity of the method . # # # 1- The advantages of the method : # # # # 1- Density estimation on Manifold data : As you pointed out the GEBM has an advantage over EBM in that it can model data supported on a low-dimensional manifold . Under the manifold assumption , the data do not admit a density on the whole space , as a consequence , it is not possible to estimate this ( undefined ) quantity . What GEBM offers is the possibility to estimate a density relative to the support of the generator . Thus if the model learns the low dimensional support of data using the generator , the GEBM provides a way to model the density on that learned manifold . # # # # 2- Mode collapse : This is an important question , thank you for raising it . Mode collapse can happen if the model doesn \u2019 t have enough capacity to cover the data support . One setting where this can occur is when the data is supported over multiple disjoint sub-manifolds ( e.g.multiple CIFAR classes ) . Multimodality can be a challenge in GANs : the latent noise is usually unimodal , however the data may be highly multimodal . Hence , the generator needs to be very powerful/complicated to transform a unimodal gaussian into a distribution with multiple modes , as shown in Cornish et al 2020 . One ( unsatisfactory ) solution is for the generator to just \u201c collapse \u201d to a small number of easy-to-produce modes , rather than covering all modes . GEBMs have an advantage over GANs in producing highly multimodal outputs . GEBMs allow rich posterior latent distributions that can be multimodal * in latent space * ( by incorporating the energy function into the overall model ) . As a result , the generator doesn \u2019 t need to distort the inputs as much in order to produce multiple modes * in the observed space * . See the answer to reviewer 1 for more details . # # # 2- Motivation of the density estimation experiment . Thank you , we will add the following clarification : We are only interested in this setting as a sanity check for the proposed learning method . This particular setting allows exact computation of the likelihood . Outside of this setting , closed form expressions of the normalizing constant are not available for generic GEBMs . Note that this is not an issue since the proposed method doesn \u2019 t require a closed form expression for the normalizing constant . However , in this experiment only , we want to have access to closed form expressions of the likelihood as a baseline against which to compare other density estimation methods . Details for training the NVP using CD are provided in Appendix G.2 : It is trained as if the normalizing constant wasn \u2019 t known by running Langevin iterations to get samples from the model . This experiment is simply a sanity check and is not central to our setting , we are happy to move it to the appendix if the reviewers think it is more appropriate ( the experiments were in fact added in response to a reviewer request from a submission to an earlier conference ) . We emphasize again that the main strength and intended purpose of the model is when the base measure is lower-dimensional than the ambient space . # # # 3- \u201c Experimental results are chaotic \u201d . We respectfully disagree . We believe the experimental section 6 is structured and separated in two parts : Image generation and density estimation . Each part starts by describing the experimental setting then presents the result . We believe that we provided details about the architectures used for both the Generator and the Energy . We provide them again for the purpose of this discussion in the \u201c Experimental details \u201d response above . We are happy to provide any additional details upon request . We will also release the code for reproducing the experiments # # # 4- Details of Figure 1 in section 2 : We thank the reviewer for requesting the details of this illustrative example , which we think is of independent interest . We will provide those details in the appendix - for now we provide them in the \u201c Experimental details \u201d response above ."}, "1": {"review_id": "0PtUPB9z6qK-1", "review_text": "Summary : In this work , a generalized energy-based model ( GEBM ) is proposed . During the generation , the base distribution and the energy cooperate to combine the strengths of both the energy-based model and the implicit generative model . +ves : 1.This paper has proposed a framework so that the energy function can be used to refine the probability mass on the learned base distribution . The framework is trained by alternating between learning the energy and the base . Empirically , the framework outperforms GAN with the same complexity . 2.KL Approximate Lower-bound Estimate ( KALE ) is used for energy training . There is a lot of detail on this derivation . Concerns : 1 . In this work , a new framework is proposed for training . The whole process seems complicated . Is there is a simple way to refine the probability mass on the learned base distribution ? How about considering exp ( -E ( x ) ) G ( x ) . Here G ( x ) is learned base distribution . A simple way to refine the probability mass is by sampling several generated output near the point and weight the outputs with exp ( -E ( x ) ) G ( x ) ? This seems easier . Could you explain the more possible benefits of your method ? 2.During the generation process , two algorithms are proposed : ULA and KLA . It seems a large variance . It is unclear which one is better . Questions during the rebuttal period : Please address and clarify the cons above : Missed related work . For the training of energy based models , there are several related work [ 2 ] [ 3 ] , etc . In the work [ 1 ] , the residual energy is used to better generation . MCMC is also used for generations . [ 1 ] Residual Energy-Based Models for Text Generation . ICLR 2020 [ 2 ] Structured Prediction Energy Networks , ICML 2016 [ 3 ] Learning Approximate Inference Networks for Structured Prediction . ICLR 2018", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the insightful and constructive remarks and for pointing to the related works which we will include in the revised version . We hope the following discussion answers your questions . # # # 1- What are the benefits of the proposed sampling compared to easier sampling methods ? Thank you for pointing out this essential question . In addition to the current discussion in the introduction and section 4 , we will add a discussion in section 4 on the advantage of MCMC over importance sampling ( IS ) , which is the alternative you suggested ( if we understood correctly ) : Indeed , IS is a simpler way to estimate expectations under the GEBM by sampling multiple points from the generator and then weighting using the energy . However , IS can lead to highly unreliable estimates , a well known problem in the Sequential Monte Carlo ( SMC ) literature which employs IS extensively [ 1,2 ] . Instead we use ULA/KLA which do not have these issues . The other advantages of the proposed method as discussed in section 4 and introduction are : - Latent sampling vs data-space sampling : Avoiding the curse of dimension . As shown in Prop 5 , sampling using methods that exploit the latent structure have a speed of convergence that depends only on the intrinsic dimension of the model ( latent dimension ) and not the ambient space dimension . This is a big advantage in practice for datasets like Images . - Using gradient information during MCMC ( as done with ULA and KLA ) is more efficient as it exploits the local slope of a density to find high density regions quickly . # # # 2- Sampling Algorithms ( ULA vs KLA ) which one is better ? This is an important question with implications even beyond the GEBM framework : ULA is the Unadjusted langevin Algorithm which doesn \u2019 t use a momentum variable . KLA is a kinetic sampler , it uses a momentum variable just like Hamiltonian Monte Carlo . The strength of the momentum depends on a friction parameter $ \\gamma $ . Higher friction leads to an algorithm that behaves like ULA . Lower friction leads to more exploration of the modes . - The experiments of Table 2 , use KLA in a high friction regime thus behaving essentially as ULA . For this experiment , we selected this high friction regime for KLA as we are interested in the sharpness of the generated images . - We also provide empirical evidence in figure 3 , 4 and 5 of appendix D for the qualitative difference in behavior between both : We found that KLA ( in a low friction regime ) is able to explore multiple modes of the distribution in the same chain , unlike ULA which remains in the same mode . As discussed in section 4 , this confirms prior theoretical and empirical results about KLA vs ULA . # # # # In conclusion : if the goal is to get sharper images and thus better FID scores , we recommend using either KLA with high friction or using ULA . If the goal is to explore multiple modes within the same MCMC chain , we recommend using KLA with a smaller friction parameter . # # # # References [ 1 ] : Del Moral , Pierre and Doucet , Arnaud and Jasra , Ajay , Sequential monte carlo samplers . Journal of the Royal Statistical Society : Series B ( Statistical Methodology ) [ 2 ] : Doucet , Arnaud and Freitas , Nando de and Gordon , Neil , Sequential Monte Carlo Methods in Practice"}, "2": {"review_id": "0PtUPB9z6qK-2", "review_text": "Originality : The paper presents an interesting generative model termed generalized energy-based model ( GEBM ) , which is essentially an exponential tilting of a plain generator model P_G by an energy based model exp ( -E ) . The appearance of such a model in generative modeling looks to be new . But the structure of the model is not surprisingly new . Significance : The pros : 1 . The purpose of introducing such a model , as stated by the authors , is to make use of the fact that target distribution may concentrate in a low dimensional manifold in the target domain , which can be captured by a low-dimensional generator model P_G . Once tilted by exp ( -E ) , the overall model can generate sharper samples than the plain energy based model . 2.The paper also presents tractable methods to train and sample from the proposed GEBM . 3.The authors show that GEBM performs better than GANs on certain image generation experiments . The cons : While it may be easier to understand why GEBM can be better than EBM on certain generative modeling tasks , the authors did not provide much explanation or justification on why GEBM can outperform GANs . As mentioned by the authors , the energy based models and functional generators are quite different generative models . By combining them , how to guarante that their pros rather than the cons will be strengthened ? In what kind of tasks GEBM can perform better than GANs ? Hopefully the authors can give more theoretical comparisons or intuitions to address these questions . Quality : Pros : The overall quality of the paper is good . Nearly all results regarding the properties of GEBM come with theoretical justifications . The proposed alternative training method and sampling methods look reasonable . Cons : It would be better if there is some analytical comparison between GEBM and GAN . Also , in the experiment section , the authors should reveal more detail on how the energy function ( class ) is chosen . Another concern is that , for Proposition 1 to hold , does it require the generator G to be invertible ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the encouraging and constructive remarks and for the insightful questions . # # # 1- Exponential tilting interpretation : We really like the interpretation of GEBMs as an exponential tilting of a reference measure ( here the generator ) . We will add it to section 2 , further emphasizing how GEBM extends exponential tilting models by allowing the reference measure ( the generator ) to change its support /shape in space . This enables the GEBMs to learn the manifold of low dimensional data in addition to the tilting of mass on that manifold . With this new flexibility comes the question of learning reference measure as well , which is what we propose here . # # # 2- Theoretical advantage of GEBM relatively to GAN : This is an important question which encouraged us to strengthen our theory results . We will add a discussion on this matter in section 2 and add a proposition which provides insights on this question , which we summarize in 3 points : # # # # 1- The proposition : It considers the case when the generator is trained until it matches the support of the data . In this case , the KL between data $ P $ and generator $ G $ is defined and we show that $ KL ( P| Q ) < = KL ( P|G ) $ , where $ Q $ is the GEBM with the generator $ G $ as a base . The best case scenario is when the exponential tilting is equal to the density ratio between P and G , in which case $ KL ( P| Q ) =0 $ . This means that the exponential tilting of the generator $ G $ systematically improves over using the generator alone . # # # # 2- When can we expect most improvement over GAN ? According to the above result , if $ G $ is exactly equal to the data distribution $ P $ , then no further improvement is possible . But as long as there is an error in mass on this common support , the GEBM improves over the generator $ G $ . It may be that , apart from the error in mass on the support , the support itself is not correctly obtained . Even in this case , the addition of an energy function can improve over the generator alone . This is consistent with Table 1 which shows that the improvement is larger for the smaller networks ( SN-GAN ConvNet ) than for the larger one ( SN-GAN ResNet ) . Thus , we expect the highest improvement when the generator has a limited strength . # # # # 3- Multimodality : GEBMs vs GANs In GANs , the latent noise is usually unimodal ( a gaussian for instance ) , however the data is often highly multimodal . Hence , the generator needs to be very powerful/complicated to transform a simple gaussian into a distribution with multiple modes as shown in Cornish et al 2020 . On the other hand , EBMs allow to capture multimodality with rather simpler models . GEBMs build on this intuition and allow to use richer latent distributions ( thanks to proposition 1 ) that can be multimodal in latent space . The energy removes the additional burden from the generator of learning how to put weights on the support , so that it only focuses on getting the support correct . For instance , Table 1 shows that a smaller generator network ( SNGAN ConvNet ) combined with an energy achieves a better FID score on imagenet ( FID = 13.94 ) compared to a GAN using a larger generator SNGAN ResNet ( FID=20.5 ) . This improvement is observed on Imagenet which has many more classes than Cifar10 . As such , we expect it to be much more multimodal . Thus we think that GEBMs are the most useful in high multimodality scenarios . ( Please see also response to AnonReviewer2 . ) # # # # Summary : While these intuitions and empirical observations are favorable to GEBMs , a precise theoretical quantification requires a number of steps to be taken : - Identifying the modeling capacity of GANs and the additional capacity that the energy provides . - Providing a Generalization theory for f-divergences which are harder to analyze than IMPs ( Uppal 2019 ) . - Extending the generalization theory to the case where density ratios are not well defined . We provided analysis when the ratio is well-defined in Theorem 6 of appendix B . All those questions are challenging and interesting future research directions . # # # 3- Network Architectures : Thank you , we will include details for the illustrative example in figure 1 of section 2 ( please see the \u201c Experimental details \u201d response above for those details ) , we believe we provided details for the networks used in section 6 , but we are happy to include any additional details . We will also release the code for reproducing the experiments . # # # 4- Does the generator need to be invertible for proposition 1 to hold ? It might seem surprising at first , but this proposition doesn \u2019 t require invertibility of the generator . Proposition 1 says that exponential tilting of the generator is equivalent to an exponential tilting in latent space before mapping to data space . To prove this result ( in appendix C.1 ) , we rely on a characterization of probabilities through test functions : two probabilities are equal if they have the same expectation for any bounded continuous function ."}}