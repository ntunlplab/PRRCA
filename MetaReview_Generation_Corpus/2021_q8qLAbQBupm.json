{"year": "2021", "forum": "q8qLAbQBupm", "title": "Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics", "decision": "Accept (Poster)", "meta_review": "\nThe paper offers a more systematic treatment of various symmetry-related results in the current literature. Concretely, the invariance properties exhibited by loss functions associated with neural networks give rise to various dynamical invariants of gradient flows. The authors address these dynamical invariants in a unified manner and study them wrt different variants of gradient flows aimed at reflecting different algorithmic aspects of real training processes. \n\nThe simplicity and the generality of dynamical invariants are both the strength and the weakness of the approach. On one hand, they provide a simple way of obtaining non-trivial generalities for the dynamics of learning processes. On the other hand, they abstracts away the very structure of neural networks from which they derive, and hence only allow relatively generic statements. Perhaps the approach should be positioned more as a conceptual method for studying invariant loss functions. \n\nOverall, although the technical contributions in the paper are rather incremental, the conceptual contribution of using dynamical invariants to unify and somewhat simplify existing analyses in a clear and clean symmetry-based approach is appreciated by the reviews and warrant a recommendation for borderline acceptance. \n", "reviews": [{"review_id": "q8qLAbQBupm-0", "review_text": "Pros : - This paper is very well-written and motivated . - The train of thoughts is explained very clearly , such that I ( admittedly not being an expert in this field ) was able to follow . - The idea to unify invariances of the loss function by using symmetries and derive corresponding conservation laws ( for $ \\lambda =0 $ ) in the gradient flow is very elegant . - By adapting a modification of the gradient flow from previous works that accounts for the discrete approximation of SGD , the derived theory was able to predict the behavior of the relevant quantities during training to a remarkable accuracy . Cons : I did not find any major drawbacks of this work . Just two small questions : - Using $ \\ell^2 $ regularization on a problem with scale symmetry does not seem to make sense , because the cost function $ $ \\mathcal { L } ( \\theta ) + \\lambda \\|\\theta\\|^2 $ $ will likely not have a minimizer as soon as $ \\lambda > 0 $ . Reducing the magnitude of any $ \\theta $ that is optimal for $ \\mathcal { L } $ reduces the regularization , but in the limit of $ \\theta=0 $ the loss might jump up . Thus , the costs are not lower semi-continuous . - Additionally , the scale symmetry seems to naturally lead to a discontinuous loss function $ \\mathcal { L } $ . Is there no problem in even defining the gradient flow for such a function ? Which properties of $ \\mathcal { L } $ do you need to derive the continuous gradient flow equations ? Overall , I really enjoyed reading this paper . Since I am not an expert in the field , I can not really judge the novelty/contribution , but aside from this aspect , I clearly recommend the acceptance of this work . -- - There is a typo in Section 6.1 `` graident '' - I stumbled upon the NeurIPS 2020 paper `` Reconciling Modern Deep Learning with Traditional Optimization Analyses : The Intrinsic Learning Rate '' by Li , Lyu , and Arora . Based on the abstract , this seems to be a relevant related work . - After the rebuttal : I 'd like to thank the authors as well as my fellow reviewers for the interesting discussions and corresponding clarifications . Summarizing my impressions from the discussion , the two main points of criticism are that the proposed analysis is not fully predictive ( depends on the norm of the gradients that depend on the empirical data ) , but rather provides the laws that govern the dynamics , and that the analysis is based on a time-continuous differential equation that seems to approximate SGD well instead of being applicable to the SGD iterates directly . The validity of the continuous dynamics is demonstrated in numerical results only . I do agree that a fully predictive framework on SGD directly would be very intersting . Yet , I think the authors are taking important steps towards such a framework , and considering the fact that SGD often behaves surprisingly/unexpectedly ( as also stated by R3 ) , I am still quite impressed how accurately the theory matches the actual SGD behavior . For our understanding of how symmetries/invariances in the weights of network architectures influence the training , I believe this paper does provide interesting insights such that I recommend its acceptance . As for a final score , I could go down to a 7 to account for the concerns raised by my fellow reviews , but I think it would mainly reflect my uncertainty about my intuition that a fully predictive analysis on SGD directly might be infeasible , and this aspect should be reflected by the confidence rather than the rating . Thus , I 'll give the authors the benefit of the doubt and keep my score , since I really enjoyed reading this paper .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We appreciate that you find our paper very well-written , clear , and elegant without finding any major drawbacks . * * Clarifying our contribution in the light of existing literature . * * To help you better understand the novelty and contribution of our work in the light of existing literature , we have added ( i ) a new discussion in the related work section in the main text and ( ii ) a new table in section A in the Supplementary Materials presenting how we unify and generalize existing literature on the geometry of loss landscapes through the lens of symmetry . We would also like to thank you for the reference to the work by Li et al.Indeed this work is very relevant and we discuss it in section A of our Supplementary Material how the geometric properties they use in their work fits into our framework of symmetry . * * Addressing your two small questions . * * You are correct that ( i ) loss functions with scale symmetry are discontinuous at the origin , and ( ii ) that minimizing such a function with L2 regularization under gradient flow does not make sense as you explained . Yet , we know that any neural network using batch normalization has scale symmetry in the weights and it is common practice to train these networks with weight decay . Furthermore , we do not see this problem in empirics ( Figure 5 ) . This discrepancy was the exact issue that motivated us to construct more realistic continuous models of stochastic gradient descent as discussed in section 5 . As we further explained , it is discretization that counteracts the centripetal force of weight decay preventing the weights from collapsing to the origin . See our updated discussion on this explanation in section 6 ."}, {"review_id": "q8qLAbQBupm-1", "review_text": "This paper analyzes the learning dynamics of DNNs from the perspective of symmetry of some parameters . It is interesting to borrow ideas from physics , which I believe is a right way to go . Specifically , the paper derives analytical form of parameters under the cases of translation , scale and inversion invariances , and also modified the underlying gradient flows to accommodate stochastic gradients . The results are very interesting , which I like a lot . However , I have some confusions about some results , which make me not able to give the paper a pass at this time . I will consider revising the score if the authors can clear me in the rebuttal . 1.The papers talks about three invariances , and also given some examples of DNNs that satisfies these invariances . From my understanding , these invariances only apply to a very limited cases of DNN structures , e.g. , the softmax layer . And in Figure 3-5 , the authors verify the convolutional layers for these invariances . It is not clear to me why convolutional layers satisfy these invariances : translation , scale and inversion . Am I missing something ? 2.In 6.1 , a new solution for the translation invariance case is derived . However , it seems that the parameters following the solution still converge to zero ? This is obviously not the case in practice . 3.Third line below eq.16 , it says `` there is a competition between the centripetal effect of weight decay and the centrifugal effect of discretization . '' I don ; 't understand why the descretization has the centrifugal effect . = After rebuttal : The rebuttal resolves most of my concerns . It is more clear to me that this is an interesting paper on describing the dynamics of DNN parameters in the training , which seems novel to me . I also realize that the closed form dynamics are not for individual parameters , but in terms of some statistics of the parameters , e.g. , the sum of the parameters . This makes the results not as existing as what I thought . That is why I decide to raise my score to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your constructive review . We are glad that you like our approach and results , finding them very interesting . We will now respond to your specific questions , hopefully clarifying any confusion we have generated : 1 . * * Nearly every parameter in modern neural network architectures is involved in one of the three invariances discussed in our paper . * * As we discussed in section 3 , at any hidden unit with batch normalization , the scale symmetry applies to the parameters into this neuron . At any hidden unit with a homogeneous activation function ( $ f ( x ) $ obeying $ f ( \\alpha x ) = \\alpha^n f ( x ) $ , for example ReLU obeys when $ n=1 $ ) the rescale symmetry applies to the parameters in and out of this neuron . In the case of consecutive convolutional layers , this means the filters into a channel and the filters out of that channel . And as you mentioned , the parameters immediately preceding a softmax function observe translation symmetry . Thus , most parameters in modern deep neural networks are involved in at least one symmetry . In the case of VGG-16 with batch normalization trained on Tiny-ImageNet ( one of the model/dataset combinations we considered in section 6 ) there are 12,751 distinct symmetries and every single parameter is involved in at least one of these symmetries . See table 5 in Supplementary Material G for a breakdown of the number/type of symmetries for the models we used . 2 . * * Parameters that respect translational symmetry do not converge to zero , their sum does . * * As you can see in equation 18 ( section 6 ) , the projection of parameters observing translation symmetry $ \\theta $ onto the all-ones vectors $ \\langle \\theta , \\mathbb { 1 } _ { \\mathcal { A } } \\rangle $ exponentially converges to zero . Please note that the parameters $ \\theta $ are not going to zero , their sum is . There are many ways for this projection to converge to zero without the parameters themselves converging to zero , unlike in the case of scale symmetry . 3 . * * Discretization of circular motion leads to a growing norm due to the curvature of the trajectory ( see section 6 and Supplementary Material C ) . * * Motivated by your question , we have clarified our explanation in the main text ( section 6 ) and added further intuition in the Supplementary Materials , which we will go over now ( see Figure 7 in Supplementary Material C. ) Consider a particle moving in circular motion as discussed/depicted in Figure 7 . For an infinitesimal step size , then our particle will stay on the circle defined by its initialization . However , for a finite step size then with each discrete update we \u201c fall off \u201d the circle that we were just on to a circle with a larger radius . The larger the step size , the greater the change in radius . Intuitively , we can understand discretization to be leading to a centrifugal effect moving our particle away from the origin . We can formulate these intuitions through modified equation analysis , where we can derive that discretization of these first order dynamics leads to a negative acceleration term , countering the acceleration into the origin needed for circular motion . We have further expanded on the modified equation analysis in the Supplementary Material in the updated manuscript . We sincerely hope that our reply has clarified your confusions , and you could now revise the score in light of the updated manuscript and the detailed discussion above . However , please let us know if you have any further questions ."}, {"review_id": "q8qLAbQBupm-2", "review_text": "This paper studies the dynamics of the parameters while training a neural network via SGD . SGD is not studied directly but three continuous time approximations of SGD are considered : the classical gradient flow , a stochastic differential equation of Langevin type , and a `` modified '' gradient flow whose derivation has roots in the literature . For each dynamics , the authors show that some invariant properties of the loss function ( which are often satisfied in practice ) imply some invariant quantities for the dynamics . Overall , the paper is rather clear . It tries to provide a physical meaning behind the dynamics of learning , which is an interesting question . However , I do not see any significant theoretical contribution . For instance , all derivations are simple differential calculus applications . Moreover , several models for SGD are used , each of them have their own invariant properties ( which look alike ) and then what ? The technical contribution is not clear to me . Finally , the experimental contribution is rather mild because the numerical experiments are not discussed in the main text ( except marginally in the conclusion ) . There is a lot of room for improvement ( see below ) and I do n't recommend the paper for publication in this form . Specific remarks : - The first paragraph of Section 3 is not necessary in my opinion ( already explained in the intro ) . Moreover , the notation for the group action is a bit misleading , e.g.there is confusion between \\psi , \\psi ( \\theta ) and \\psi ( \\theta , \\alpha ) - Eq 2 : the translation invariant suddenly applies only to a subset of the parameters . - Figure 1 , 2 are not commented in the main text . Same for 3,4,5 ( except in the conclusion ) - The authors could recall Noether 's theorem for comparison - The models used for SGD are not theoretically justified , except for the classical gradient flow , for which it is standard ( Kushner & Yin 2003 ) . For Eq ( 11 ) it starts to be sloppy ( CLT + Forward Euler ) . For Eq ( 13 ) , only intuition is provided . - Last paragraph of Page 5 . It seems that the discussion applies for any \\xi ( not necessarily Gaussian ) . Does it help to remove the Gaussian noise assumption ? To improve the paper , I suggest the authors to - clearly locate their work within the existing literature . This would help to understand why the questions answered in this paper are important , and to highlight their contribution . - Turn their paper into an experimental one . To this end , dedicate a whole section to * * commented * * numerical experiments . This does not mean adding more simulations , just explain them in the main text and how they contribute to the main message of the paper . Minor : Page 2 : `` nornalization '' Between Eq 3 and 4 : Notation not defined Page 4 : ReLU not defined ( give the formula ) Section 5.1 , 2nd line . \\nabla is missing Page 6 : graident Page 7 ( two times ) : previosly", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed reviews and constructive suggestions which have reshaped our manuscript significantly . We are glad that you find our question interesting and think our presentation is clear , but with \u201c room for improvement \u201d . We have incorporated all of your specific remarks and minor comments , which you can find in the updated manuscript . We will now explain how we have incorporated your two major suggestions about ( i ) locating our work within broader context and ( ii ) creating a new section in the main text focusing on the empirics . # # # Locating our work in the existing literature To better locate our work within the existing literature , we have ( i ) intensively edited the introduction , ( ii ) added a whole new discussion reviewing existing literature of learning dynamics , ( iii ) created a new table in section A in the Supplementary Materials presenting how we unify and generalize the existing literature on the geometry of loss landscapes through the lens of symmetry . * * Our work opens up a new direction for understanding training dynamics , by not making simplifying assumptions , but rather restricting our predictions to meaningful parameter combinations ( see updated section 1 & 2 ) . * * A foundational question in deep learning theory is : what , if anything , can we quantitatively predict about the training dynamics of large-scale , non-linear neural network models driven by real-world datasets and optimized via stochastic gradient descent with a finite batch size , learning rate , and with or without momentum ? In order to make headway on this extremely difficult question , existing works have made major simplifying assumptions on the network , such as restricting to identity activation functions Saxe et al. , infinite width layers Jacot et al. , or single hidden layers Saad & Solla . Many of these works have also ignored the complexity introduced by stochasticity and discretization by only focusing on the learning dynamics under gradient flow . In the present work , we make the first step in an orthogonal direction . Rather than introducing unrealistic assumptions on the model or learning dynamics , we uncover restricted , but meaningful combinations of parameters with simplified dynamics that can be solved exactly without introducing a single assumption ( see the new Fig.1 in our paper ) . We make this fundamental contribution by using the tools of symmetry and modified equation analysis , which have been underutilized in deep learning . Our work provides a new arsenal of tools to analyze learning dynamics and evidence that exact predictions are possible even for non-linear architectures , real-world datasets , and learning rules at both finite step sizes , batch sizes , and with or without momentum . For example , in the case of VGG-16 with batch normalization trained on Tiny-ImageNet ( one of the model/dataset combinations we considered in section 6 ) there are 12,751 distinct symmetries which means that we can analytically describe the learning dynamics of 12,751 parameter combinations , again at finite step sizes , batch sizes , and with or without momentum , and we confirm our analytic predictions against numerical simulations . To our knowledge , this has never been done before in any real world setting . * * We have added a whole new section describing how our symmetry-based proof on the geometry of the loss landscapes unifies , simplifies and generalizes existing literature ( see Supplementary Material A ) . * * Inspired by your suggestion , we have added a whole new section A in the Supplementary Materials formalizing our derivation of symmetry induced geometric properties and created a new subsection detailing how they relate to the existing literature . First , you are correct that \u201c all derivations are simple differential calculus \u201d . However , we believe that the mathematical simplicity of our proof strategy via symmetry is a strength , not a weakness , just as in Noether \u2019 s theorem . Notably , our symmetry-based proofs automatically , in a unified manner , yield as many as 15 formulas for the geometric properties of loss landscapes , in the form of constraints on gradients and Hessians . Remarkably , to the best of our knowledge , some of the 15 formulas that we derive are new ( see table 1 in the Supplementary Materials ) . Others have been derived in previous literature , but many of their proofs are tedious , algebraic , or sometimes make unnecessary assumptions such as biases can \u2019 t be included or the properties only hold per layer . Conversely , our strategy is as simple as , if you can identify a differentiable invariance for a set of parameters , then the geometric properties associated with that invariance hold for those parameters . In the future , we expect that our proof strategy of harnessing symmetry will become the standard when investigating the geometry of the loss landscape . So we believe our general , unifying proof strategy itself is a significant contribution ."}, {"review_id": "q8qLAbQBupm-3", "review_text": "The current work studies the implications of continuous symmetries of a DNN on its weight dynamics . Specifically , they consider linearly realized symmetries ( such as a shift/translation to the logits ) , and use the fact that the loss/hessian/mini-batches , are insensitive to such shift to decouple their dynamics . They do so both in the continuous/vanishing-learning-rate case and for small learning rates and manage to provide accurate quantitative predictions for the dynamics of these quantities . On the positive side , the current work makes accurate predictions on DNNs trained in a real-world setting ( real datasets , convolutional layers , finite learning rates , mini-batches etc ... ) which is a very complicated problem . They do so with a refreshing toolbox , that of symmetries . On the other hand , what makes their quantities tractable , seems to be the very fact that they have no impact on the DNNs final outputs . For example , this is why they are unaffected by the real-world dataset . While being a clever trick , it can be viewed as an inherent limitation of the approach : one understands quantities that have no bearing on what the DNN learns . I do n't see , for instance , how one expects to perform architecture exploration ( as mentioned in the discussion ) , using quantities that have no implications on the DNNs predictions . If the authors can argue that their approach has a broader horizon , it may increase its potential impact . Two technical comments : 1 . I find the use of subset notation and definition of large theta confusing : It seems that small theta is the set of parameters but it is a subgroup of large theta . 2.Inversion symmetry commonly refers to a discrete Z_2 symmetry , whereas the authors take it to be some representation of GL1 .", "rating": "7: Good paper, accept", "reply_text": "We thank you for clearly recognizing the contribution of our work as \u201c the current work makes accurate predictions on DNNs trained in a real-world setting \u2026 which is a very complicated problem \u201d . To address your technical comments : ( 1 ) In section 3 of the updated manuscript , we have introduced a new notation system to discuss subsets of parameters to avoid any confusion . ( 2 ) Throughout the updated manuscript , we have replaced all the \u201c inversion symmetry \u201d to \u201c rescale symmetry \u201d . This terminology of \u201c rescale invariance/equivariance \u201d is commonly used in the related work Neyshabur et al.Now we will address your larger remarks : * * The parameter combinations do depend on data . * * We apologize for any confusion that we may have created , which lead the reviewer to conclude that parameter combinations that we predict \u201c are unaffected by the real-world datasets. \u201d While the parameter combinations are conserved under gradient flow , they are * not * conserved at finite learning rate , and the gradient norms can drive changes in these quantities ( see equations 19 , 20 ) ; therefore their learning dynamics are indeed directly driven by the data ( except for translation symmetry ) . We have significantly revised our discussion in section 6 to highlight this data dependence . * * Symmetry transformations do impact the learning dynamics as the gradient and regularizer need not to respect symmetry . * * You are correct that , by its very definition , the symmetry transformations on parameters have no effect on the outputs and loss of the network . However , these transformations have a crucial impact on the learning dynamics . This is because , even if the network outputs are unaffected under some transformation of the parameters , the gradient and regularization for these parameters can change . For example , in the case of scale symmetry , if we increase the norm for parameters preceding a batch normalization layer , then the output of the network doesn \u2019 t change , but the gradient and L2 regularization for these parameters decrease and increase , respectively . This property has been used to explain how batch normalization directly controls the effective learning rate for its parameters , impacting their learning dynamics . Therefore , understanding the dynamics of these norms is of crucial importance to understand how the effective learning rate evolves throughout training and thus the network as a whole . Remarkably , our work provides the exact solution for this quantity . Overall , our work provides a theoretically tractable direction for exploring how the training hyperparameters ( weight decay , momentum , batch size , learning rate ) affect the learning dynamics . * * Principles of symmetry have been and will be crucial in designing new network architectures and optimizers . * * Understanding how symmetries in the loss affects learning dynamics through gradient and Hessian geometries has been crucial in designing new network architectures and optimizers . For example , one of the important motivations behind the invention of Batch Normalization by S. Ioffe and C. Szegedy was the realization that the scale invariance of Batch Normalization can stabilize gradient signal propagation while leaving networks \u2019 outputs intact . Similarly , the motivation for the invention of the Path-SGD optimizer by B. Neyshabur et al. , was the fact that gradient descent does not respect rescale equivariance even when the network outputs respect rescale invariance . In this work , we have unified and generalized the geometric properties of the gradient and Hessian induced by symmetry ( see a new section A in the Supplementary Materials ) , thus providing a theoretical foundation for principled design of architectures and optimizers to achieve certain geometric goals . Thank you again for your feedback . Your suggestion to focus on the broader horizon has significantly helped us refine our paper from motivation to interpretation of our theoretical and empirical results . We hope these updates have clarified the potential future impact of our work ."}], "0": {"review_id": "q8qLAbQBupm-0", "review_text": "Pros : - This paper is very well-written and motivated . - The train of thoughts is explained very clearly , such that I ( admittedly not being an expert in this field ) was able to follow . - The idea to unify invariances of the loss function by using symmetries and derive corresponding conservation laws ( for $ \\lambda =0 $ ) in the gradient flow is very elegant . - By adapting a modification of the gradient flow from previous works that accounts for the discrete approximation of SGD , the derived theory was able to predict the behavior of the relevant quantities during training to a remarkable accuracy . Cons : I did not find any major drawbacks of this work . Just two small questions : - Using $ \\ell^2 $ regularization on a problem with scale symmetry does not seem to make sense , because the cost function $ $ \\mathcal { L } ( \\theta ) + \\lambda \\|\\theta\\|^2 $ $ will likely not have a minimizer as soon as $ \\lambda > 0 $ . Reducing the magnitude of any $ \\theta $ that is optimal for $ \\mathcal { L } $ reduces the regularization , but in the limit of $ \\theta=0 $ the loss might jump up . Thus , the costs are not lower semi-continuous . - Additionally , the scale symmetry seems to naturally lead to a discontinuous loss function $ \\mathcal { L } $ . Is there no problem in even defining the gradient flow for such a function ? Which properties of $ \\mathcal { L } $ do you need to derive the continuous gradient flow equations ? Overall , I really enjoyed reading this paper . Since I am not an expert in the field , I can not really judge the novelty/contribution , but aside from this aspect , I clearly recommend the acceptance of this work . -- - There is a typo in Section 6.1 `` graident '' - I stumbled upon the NeurIPS 2020 paper `` Reconciling Modern Deep Learning with Traditional Optimization Analyses : The Intrinsic Learning Rate '' by Li , Lyu , and Arora . Based on the abstract , this seems to be a relevant related work . - After the rebuttal : I 'd like to thank the authors as well as my fellow reviewers for the interesting discussions and corresponding clarifications . Summarizing my impressions from the discussion , the two main points of criticism are that the proposed analysis is not fully predictive ( depends on the norm of the gradients that depend on the empirical data ) , but rather provides the laws that govern the dynamics , and that the analysis is based on a time-continuous differential equation that seems to approximate SGD well instead of being applicable to the SGD iterates directly . The validity of the continuous dynamics is demonstrated in numerical results only . I do agree that a fully predictive framework on SGD directly would be very intersting . Yet , I think the authors are taking important steps towards such a framework , and considering the fact that SGD often behaves surprisingly/unexpectedly ( as also stated by R3 ) , I am still quite impressed how accurately the theory matches the actual SGD behavior . For our understanding of how symmetries/invariances in the weights of network architectures influence the training , I believe this paper does provide interesting insights such that I recommend its acceptance . As for a final score , I could go down to a 7 to account for the concerns raised by my fellow reviews , but I think it would mainly reflect my uncertainty about my intuition that a fully predictive analysis on SGD directly might be infeasible , and this aspect should be reflected by the confidence rather than the rating . Thus , I 'll give the authors the benefit of the doubt and keep my score , since I really enjoyed reading this paper .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We appreciate that you find our paper very well-written , clear , and elegant without finding any major drawbacks . * * Clarifying our contribution in the light of existing literature . * * To help you better understand the novelty and contribution of our work in the light of existing literature , we have added ( i ) a new discussion in the related work section in the main text and ( ii ) a new table in section A in the Supplementary Materials presenting how we unify and generalize existing literature on the geometry of loss landscapes through the lens of symmetry . We would also like to thank you for the reference to the work by Li et al.Indeed this work is very relevant and we discuss it in section A of our Supplementary Material how the geometric properties they use in their work fits into our framework of symmetry . * * Addressing your two small questions . * * You are correct that ( i ) loss functions with scale symmetry are discontinuous at the origin , and ( ii ) that minimizing such a function with L2 regularization under gradient flow does not make sense as you explained . Yet , we know that any neural network using batch normalization has scale symmetry in the weights and it is common practice to train these networks with weight decay . Furthermore , we do not see this problem in empirics ( Figure 5 ) . This discrepancy was the exact issue that motivated us to construct more realistic continuous models of stochastic gradient descent as discussed in section 5 . As we further explained , it is discretization that counteracts the centripetal force of weight decay preventing the weights from collapsing to the origin . See our updated discussion on this explanation in section 6 ."}, "1": {"review_id": "q8qLAbQBupm-1", "review_text": "This paper analyzes the learning dynamics of DNNs from the perspective of symmetry of some parameters . It is interesting to borrow ideas from physics , which I believe is a right way to go . Specifically , the paper derives analytical form of parameters under the cases of translation , scale and inversion invariances , and also modified the underlying gradient flows to accommodate stochastic gradients . The results are very interesting , which I like a lot . However , I have some confusions about some results , which make me not able to give the paper a pass at this time . I will consider revising the score if the authors can clear me in the rebuttal . 1.The papers talks about three invariances , and also given some examples of DNNs that satisfies these invariances . From my understanding , these invariances only apply to a very limited cases of DNN structures , e.g. , the softmax layer . And in Figure 3-5 , the authors verify the convolutional layers for these invariances . It is not clear to me why convolutional layers satisfy these invariances : translation , scale and inversion . Am I missing something ? 2.In 6.1 , a new solution for the translation invariance case is derived . However , it seems that the parameters following the solution still converge to zero ? This is obviously not the case in practice . 3.Third line below eq.16 , it says `` there is a competition between the centripetal effect of weight decay and the centrifugal effect of discretization . '' I don ; 't understand why the descretization has the centrifugal effect . = After rebuttal : The rebuttal resolves most of my concerns . It is more clear to me that this is an interesting paper on describing the dynamics of DNN parameters in the training , which seems novel to me . I also realize that the closed form dynamics are not for individual parameters , but in terms of some statistics of the parameters , e.g. , the sum of the parameters . This makes the results not as existing as what I thought . That is why I decide to raise my score to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your constructive review . We are glad that you like our approach and results , finding them very interesting . We will now respond to your specific questions , hopefully clarifying any confusion we have generated : 1 . * * Nearly every parameter in modern neural network architectures is involved in one of the three invariances discussed in our paper . * * As we discussed in section 3 , at any hidden unit with batch normalization , the scale symmetry applies to the parameters into this neuron . At any hidden unit with a homogeneous activation function ( $ f ( x ) $ obeying $ f ( \\alpha x ) = \\alpha^n f ( x ) $ , for example ReLU obeys when $ n=1 $ ) the rescale symmetry applies to the parameters in and out of this neuron . In the case of consecutive convolutional layers , this means the filters into a channel and the filters out of that channel . And as you mentioned , the parameters immediately preceding a softmax function observe translation symmetry . Thus , most parameters in modern deep neural networks are involved in at least one symmetry . In the case of VGG-16 with batch normalization trained on Tiny-ImageNet ( one of the model/dataset combinations we considered in section 6 ) there are 12,751 distinct symmetries and every single parameter is involved in at least one of these symmetries . See table 5 in Supplementary Material G for a breakdown of the number/type of symmetries for the models we used . 2 . * * Parameters that respect translational symmetry do not converge to zero , their sum does . * * As you can see in equation 18 ( section 6 ) , the projection of parameters observing translation symmetry $ \\theta $ onto the all-ones vectors $ \\langle \\theta , \\mathbb { 1 } _ { \\mathcal { A } } \\rangle $ exponentially converges to zero . Please note that the parameters $ \\theta $ are not going to zero , their sum is . There are many ways for this projection to converge to zero without the parameters themselves converging to zero , unlike in the case of scale symmetry . 3 . * * Discretization of circular motion leads to a growing norm due to the curvature of the trajectory ( see section 6 and Supplementary Material C ) . * * Motivated by your question , we have clarified our explanation in the main text ( section 6 ) and added further intuition in the Supplementary Materials , which we will go over now ( see Figure 7 in Supplementary Material C. ) Consider a particle moving in circular motion as discussed/depicted in Figure 7 . For an infinitesimal step size , then our particle will stay on the circle defined by its initialization . However , for a finite step size then with each discrete update we \u201c fall off \u201d the circle that we were just on to a circle with a larger radius . The larger the step size , the greater the change in radius . Intuitively , we can understand discretization to be leading to a centrifugal effect moving our particle away from the origin . We can formulate these intuitions through modified equation analysis , where we can derive that discretization of these first order dynamics leads to a negative acceleration term , countering the acceleration into the origin needed for circular motion . We have further expanded on the modified equation analysis in the Supplementary Material in the updated manuscript . We sincerely hope that our reply has clarified your confusions , and you could now revise the score in light of the updated manuscript and the detailed discussion above . However , please let us know if you have any further questions ."}, "2": {"review_id": "q8qLAbQBupm-2", "review_text": "This paper studies the dynamics of the parameters while training a neural network via SGD . SGD is not studied directly but three continuous time approximations of SGD are considered : the classical gradient flow , a stochastic differential equation of Langevin type , and a `` modified '' gradient flow whose derivation has roots in the literature . For each dynamics , the authors show that some invariant properties of the loss function ( which are often satisfied in practice ) imply some invariant quantities for the dynamics . Overall , the paper is rather clear . It tries to provide a physical meaning behind the dynamics of learning , which is an interesting question . However , I do not see any significant theoretical contribution . For instance , all derivations are simple differential calculus applications . Moreover , several models for SGD are used , each of them have their own invariant properties ( which look alike ) and then what ? The technical contribution is not clear to me . Finally , the experimental contribution is rather mild because the numerical experiments are not discussed in the main text ( except marginally in the conclusion ) . There is a lot of room for improvement ( see below ) and I do n't recommend the paper for publication in this form . Specific remarks : - The first paragraph of Section 3 is not necessary in my opinion ( already explained in the intro ) . Moreover , the notation for the group action is a bit misleading , e.g.there is confusion between \\psi , \\psi ( \\theta ) and \\psi ( \\theta , \\alpha ) - Eq 2 : the translation invariant suddenly applies only to a subset of the parameters . - Figure 1 , 2 are not commented in the main text . Same for 3,4,5 ( except in the conclusion ) - The authors could recall Noether 's theorem for comparison - The models used for SGD are not theoretically justified , except for the classical gradient flow , for which it is standard ( Kushner & Yin 2003 ) . For Eq ( 11 ) it starts to be sloppy ( CLT + Forward Euler ) . For Eq ( 13 ) , only intuition is provided . - Last paragraph of Page 5 . It seems that the discussion applies for any \\xi ( not necessarily Gaussian ) . Does it help to remove the Gaussian noise assumption ? To improve the paper , I suggest the authors to - clearly locate their work within the existing literature . This would help to understand why the questions answered in this paper are important , and to highlight their contribution . - Turn their paper into an experimental one . To this end , dedicate a whole section to * * commented * * numerical experiments . This does not mean adding more simulations , just explain them in the main text and how they contribute to the main message of the paper . Minor : Page 2 : `` nornalization '' Between Eq 3 and 4 : Notation not defined Page 4 : ReLU not defined ( give the formula ) Section 5.1 , 2nd line . \\nabla is missing Page 6 : graident Page 7 ( two times ) : previosly", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed reviews and constructive suggestions which have reshaped our manuscript significantly . We are glad that you find our question interesting and think our presentation is clear , but with \u201c room for improvement \u201d . We have incorporated all of your specific remarks and minor comments , which you can find in the updated manuscript . We will now explain how we have incorporated your two major suggestions about ( i ) locating our work within broader context and ( ii ) creating a new section in the main text focusing on the empirics . # # # Locating our work in the existing literature To better locate our work within the existing literature , we have ( i ) intensively edited the introduction , ( ii ) added a whole new discussion reviewing existing literature of learning dynamics , ( iii ) created a new table in section A in the Supplementary Materials presenting how we unify and generalize the existing literature on the geometry of loss landscapes through the lens of symmetry . * * Our work opens up a new direction for understanding training dynamics , by not making simplifying assumptions , but rather restricting our predictions to meaningful parameter combinations ( see updated section 1 & 2 ) . * * A foundational question in deep learning theory is : what , if anything , can we quantitatively predict about the training dynamics of large-scale , non-linear neural network models driven by real-world datasets and optimized via stochastic gradient descent with a finite batch size , learning rate , and with or without momentum ? In order to make headway on this extremely difficult question , existing works have made major simplifying assumptions on the network , such as restricting to identity activation functions Saxe et al. , infinite width layers Jacot et al. , or single hidden layers Saad & Solla . Many of these works have also ignored the complexity introduced by stochasticity and discretization by only focusing on the learning dynamics under gradient flow . In the present work , we make the first step in an orthogonal direction . Rather than introducing unrealistic assumptions on the model or learning dynamics , we uncover restricted , but meaningful combinations of parameters with simplified dynamics that can be solved exactly without introducing a single assumption ( see the new Fig.1 in our paper ) . We make this fundamental contribution by using the tools of symmetry and modified equation analysis , which have been underutilized in deep learning . Our work provides a new arsenal of tools to analyze learning dynamics and evidence that exact predictions are possible even for non-linear architectures , real-world datasets , and learning rules at both finite step sizes , batch sizes , and with or without momentum . For example , in the case of VGG-16 with batch normalization trained on Tiny-ImageNet ( one of the model/dataset combinations we considered in section 6 ) there are 12,751 distinct symmetries which means that we can analytically describe the learning dynamics of 12,751 parameter combinations , again at finite step sizes , batch sizes , and with or without momentum , and we confirm our analytic predictions against numerical simulations . To our knowledge , this has never been done before in any real world setting . * * We have added a whole new section describing how our symmetry-based proof on the geometry of the loss landscapes unifies , simplifies and generalizes existing literature ( see Supplementary Material A ) . * * Inspired by your suggestion , we have added a whole new section A in the Supplementary Materials formalizing our derivation of symmetry induced geometric properties and created a new subsection detailing how they relate to the existing literature . First , you are correct that \u201c all derivations are simple differential calculus \u201d . However , we believe that the mathematical simplicity of our proof strategy via symmetry is a strength , not a weakness , just as in Noether \u2019 s theorem . Notably , our symmetry-based proofs automatically , in a unified manner , yield as many as 15 formulas for the geometric properties of loss landscapes , in the form of constraints on gradients and Hessians . Remarkably , to the best of our knowledge , some of the 15 formulas that we derive are new ( see table 1 in the Supplementary Materials ) . Others have been derived in previous literature , but many of their proofs are tedious , algebraic , or sometimes make unnecessary assumptions such as biases can \u2019 t be included or the properties only hold per layer . Conversely , our strategy is as simple as , if you can identify a differentiable invariance for a set of parameters , then the geometric properties associated with that invariance hold for those parameters . In the future , we expect that our proof strategy of harnessing symmetry will become the standard when investigating the geometry of the loss landscape . So we believe our general , unifying proof strategy itself is a significant contribution ."}, "3": {"review_id": "q8qLAbQBupm-3", "review_text": "The current work studies the implications of continuous symmetries of a DNN on its weight dynamics . Specifically , they consider linearly realized symmetries ( such as a shift/translation to the logits ) , and use the fact that the loss/hessian/mini-batches , are insensitive to such shift to decouple their dynamics . They do so both in the continuous/vanishing-learning-rate case and for small learning rates and manage to provide accurate quantitative predictions for the dynamics of these quantities . On the positive side , the current work makes accurate predictions on DNNs trained in a real-world setting ( real datasets , convolutional layers , finite learning rates , mini-batches etc ... ) which is a very complicated problem . They do so with a refreshing toolbox , that of symmetries . On the other hand , what makes their quantities tractable , seems to be the very fact that they have no impact on the DNNs final outputs . For example , this is why they are unaffected by the real-world dataset . While being a clever trick , it can be viewed as an inherent limitation of the approach : one understands quantities that have no bearing on what the DNN learns . I do n't see , for instance , how one expects to perform architecture exploration ( as mentioned in the discussion ) , using quantities that have no implications on the DNNs predictions . If the authors can argue that their approach has a broader horizon , it may increase its potential impact . Two technical comments : 1 . I find the use of subset notation and definition of large theta confusing : It seems that small theta is the set of parameters but it is a subgroup of large theta . 2.Inversion symmetry commonly refers to a discrete Z_2 symmetry , whereas the authors take it to be some representation of GL1 .", "rating": "7: Good paper, accept", "reply_text": "We thank you for clearly recognizing the contribution of our work as \u201c the current work makes accurate predictions on DNNs trained in a real-world setting \u2026 which is a very complicated problem \u201d . To address your technical comments : ( 1 ) In section 3 of the updated manuscript , we have introduced a new notation system to discuss subsets of parameters to avoid any confusion . ( 2 ) Throughout the updated manuscript , we have replaced all the \u201c inversion symmetry \u201d to \u201c rescale symmetry \u201d . This terminology of \u201c rescale invariance/equivariance \u201d is commonly used in the related work Neyshabur et al.Now we will address your larger remarks : * * The parameter combinations do depend on data . * * We apologize for any confusion that we may have created , which lead the reviewer to conclude that parameter combinations that we predict \u201c are unaffected by the real-world datasets. \u201d While the parameter combinations are conserved under gradient flow , they are * not * conserved at finite learning rate , and the gradient norms can drive changes in these quantities ( see equations 19 , 20 ) ; therefore their learning dynamics are indeed directly driven by the data ( except for translation symmetry ) . We have significantly revised our discussion in section 6 to highlight this data dependence . * * Symmetry transformations do impact the learning dynamics as the gradient and regularizer need not to respect symmetry . * * You are correct that , by its very definition , the symmetry transformations on parameters have no effect on the outputs and loss of the network . However , these transformations have a crucial impact on the learning dynamics . This is because , even if the network outputs are unaffected under some transformation of the parameters , the gradient and regularization for these parameters can change . For example , in the case of scale symmetry , if we increase the norm for parameters preceding a batch normalization layer , then the output of the network doesn \u2019 t change , but the gradient and L2 regularization for these parameters decrease and increase , respectively . This property has been used to explain how batch normalization directly controls the effective learning rate for its parameters , impacting their learning dynamics . Therefore , understanding the dynamics of these norms is of crucial importance to understand how the effective learning rate evolves throughout training and thus the network as a whole . Remarkably , our work provides the exact solution for this quantity . Overall , our work provides a theoretically tractable direction for exploring how the training hyperparameters ( weight decay , momentum , batch size , learning rate ) affect the learning dynamics . * * Principles of symmetry have been and will be crucial in designing new network architectures and optimizers . * * Understanding how symmetries in the loss affects learning dynamics through gradient and Hessian geometries has been crucial in designing new network architectures and optimizers . For example , one of the important motivations behind the invention of Batch Normalization by S. Ioffe and C. Szegedy was the realization that the scale invariance of Batch Normalization can stabilize gradient signal propagation while leaving networks \u2019 outputs intact . Similarly , the motivation for the invention of the Path-SGD optimizer by B. Neyshabur et al. , was the fact that gradient descent does not respect rescale equivariance even when the network outputs respect rescale invariance . In this work , we have unified and generalized the geometric properties of the gradient and Hessian induced by symmetry ( see a new section A in the Supplementary Materials ) , thus providing a theoretical foundation for principled design of architectures and optimizers to achieve certain geometric goals . Thank you again for your feedback . Your suggestion to focus on the broader horizon has significantly helped us refine our paper from motivation to interpretation of our theoretical and empirical results . We hope these updates have clarified the potential future impact of our work ."}}