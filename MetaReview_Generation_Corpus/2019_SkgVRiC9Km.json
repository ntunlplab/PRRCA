{"year": "2019", "forum": "SkgVRiC9Km", "title": "Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations", "decision": "Reject", "meta_review": "This paper suggests a method for defending against adversarial examples and out-of-distribution samples via projection onto the data manifold. The paper suggests a new method for detecting when hidden layers are off of the manifold, and uses auto encoders to map them back onto the manifold. \n\nThe paper is well-written and the method is novel and interesting. However, most of the reviewers agree that the original robustness evaluations were not sufficient due to restricting the evaluation to using FGSM baseline and comparison with thermometer encoding (which both are known to not be fully effective baselines). \n\nAfter rebuttal, Reviewer 4 points out that the method offers very little robustness over adversarial training alone, even though it is combined with adversarial training, which suggests that the method itself provides very little robustness. ", "reviews": [{"review_id": "SkgVRiC9Km-0", "review_text": "This paper proposes a new defense to adversarial examples based on the 'fortification' of hidden layers using a denoising autoencoder. While building models that are robust to adversarial examples is an important and relevant research problem, I am not convinced by the evaluation of the defense. Specific comments: - The authors mostly evaluate their defense using FGSM (particularly on CIFAR). To truly establish the merit of a new defense, the authors must benchmark against state-of-the-art defenses such as PGD. It also seems like the epsilon values used for the PGD attacks are fairly small. The authors should report accuracies to a range of epsilon values for the PGD attack, as is standard. - When the authors attack their models using PGD/FGSM, is this only on the classification loss or does this also include the denoising terms? Similar defenses which use denoisers have been broken once you run PGD on the full model [1]. - I do not really understand the motivation behind using an autoencoder here. Firstly, it is not clear that adversarial examples lie off the data manifold - they could form a very small set on the data manifold and thereby not affect standard generalization. Secondly, have the authors tried a simple regularization loss based on the error between hidden layer representations to a natural examples and the corresponding adversarial example? I think the authors must motivate the use of denoising autoencoders here by comparing to such a simple baseline. General comment: The results hard to parse given the arrangement of figures and tables. Also, which approach are the authors denoting as \u2018baseline adv. Train\u2019 in the tables? Overall I feel like building defenses to adversarial examples is a challenging problem and the empirical investigation in this paper is not sufficient to illustrate any real progress on this front. [1] Athalye, A., & Carlini, N. (2018). On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses. arXiv preprint arXiv:1804.03286. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your feedback . We strongly agree that it is absolutely essential to show that the improvements are not a result of gradient obfuscation . \u201c The authors mostly evaluate their defense using FGSM ( particularly on CIFAR ) . To truly establish the merit of a new defense , the authors must benchmark against state-of-the-art defenses such as PGD . \u201d We added new results with PGD on CIFAR-10 with many more PGD-steps for evaluation . We evaluated a convolutional network on CIFAR-10 with 4 convolutional layers followed by a single fully-connected layer . We trained fortified networks , where we added an autoencoder following each hidden layer . We also added a baseline \u201c Extra Layers \u201d where we trained with the layers added to match the capacity of Fortified Networks ( same number of parameters ) . # steps | Baseline | Baseline w/ extra layers | Fortified Networks 7 steps | 33.0 | 34.2 | 45.0 50 steps | 31.6 | 32.5 | 42.1 200 steps | 31.4 | 32.2 | 41.5 Even when running PGD for 200 steps , we found large and consistent advantages for fortified networks , which are not primarily attributable to adding additional layers . \u201c It also seems like the epsilon values used for the PGD attacks are fairly small . The authors should report accuracies to a range of epsilon values for the PGD attack , as is standard. \u201d This is a great point and we performed an additional experiment using the same convolutional neural network discussed above . Using 7 and 100 steps of PGD , we attacked our fortified nets model with varying epsilons : PGD , 100 steps Epsilon | Baseline with extra layers | Fortified Networks 0.03 | 35.3 | 39.2 0.04 | 24.8 | 28.0 0.06 | 14.3 | 15.6 0.08 | 12.0 | 13.0 0.1 | 11.7 | 12.9 0.2 | 10.2 | 11.3 0.3 | 8.4 | 9.6 \u201c When the authors attack their models using PGD/FGSM , is this only on the classification loss or does this also include the denoising terms ? Similar defenses which use denoisers have been broken once you run PGD on the full model [ 1 ] . \u201d We run our attacks on the full model , end-to-end , including the autoencoders . This is a major difference from [ 1 ] , and that change is what broke the paper you referenced . The paper that you referenced did not perform adversarial training on the main part of the network , and only trained the autoencoder , keeping the classifier network itself fixed . We also conducted a new experiment with BPDA ( Athalye 2018 ) , where we consider skipping the autoencoders in the backward pass ( i.e.using the identity function to compute the gradients ) as well as running the forward and backward pass of the network with no noise injected . We also ran some new experiments for this using eps=0.03 and 100 steps of PGD using the same CNN architecture discussed earlier . 33.4 ( baseline , normal attack ) 40.1 ( Fortified Networks , normal attack ) 38.2 ( Fortified Networks , no noise during attack ) 67.1 ( Fortified Networks , skip DAE during attack , BPDA ) This is strong evidence that skipping the autoencoders while generating the attacks significantly weakens them , but turning the noise off slightly strengthens the attack , but it is still much stronger as a defense than the baseline adversarially trained model with the same number of parameters and capacity . \u201c Secondly , have the authors tried a simple regularization loss based on the error between hidden layer representations to a natural examples and the corresponding adversarial example ? I think the authors must motivate the use of denoising autoencoders here by comparing to such a simple baseline \u201d Yes , we conducted new experiments to directly address this issue ( Adversarial Logit Pairing is a special case of the regularizer that you describe ) , but we also note that our method provides improvements even when we don \u2019 t use the L_adv loss comparing the adversarial input \u2019 s hidden states to the clean input \u2019 s h. This is with the same CNN architecture discussed earlier . PGD , 7 iterations : 43.3 ( Fortified Networks ) 38.1 ( Adv.training baseline ) 34.2 ( Penalty between layers ) PGD , 100 iterations : 39.2 ( Fortified Networks ) 35.3 ( Adv.training baseline ) 32.2 ( Penalty between layers ) We found that this penalty between the hidden states , where we attracted the hidden states in the network on adversarial inputs to the hidden states of the network on clean states ( applied at every layer ) , hurts robustness somewhat , but it may be possible that such an approach depends on exactly how it \u2019 s used . We also add that unlike adversarial logit pairing , our improvements hold up after running PGD for a large number of iterations , whereas the benefits from adversarial logit pairing almost entirely disappear . \u201c Also , which approach are the authors denoting as \u2018 baseline adv . Train \u2019 in the tables ? \u201c This refers to the PGD training of Madry 2017 ."}, {"review_id": "SkgVRiC9Km-1", "review_text": "This paper presents an approach of fortifying the neural networks to defend attacks. The major component should be a denoising autoencoder with noise in the hidden layer. However, from the paper, I am still not convinced why this defends the FGSM attack. From my perspective, a more specifically designed algorithm could attack the network described in the paper as the old way, and what is the insight of defending the attacks, whether this objective function is harder to find to adversarial examples, or have to use more adversarial examples? Another problem rise from Ian Goodfellow's comment. I am trying not to be biased. So if the author could address his comments properly, I am willing to change the rating.", "rating": "5: Marginally below acceptance threshold", "reply_text": "\u201c This paper presents an approach of fortifying the neural networks to defend attacks . The major component should be a denoising autoencoder with noise in the hidden layer . However , from the paper , I am still not convinced why this defends the FGSM attack . From my perspective , a more specifically designed algorithm could attack the network described in the paper as the old way \u201d In our experiments ( except where we explicitly test against a special BPDA ) we backpropagate errors through the autoencoders , such that the the autoencoders are not hidden from the attacker . Indeed we found that skipping the autoencoders when running the attacks makes them significantly weaker , but in our main experiments we backpropagate through the autoencoders and allow the attacker to use this information . \u201c and what is the insight of defending the attacks , whether this objective function is harder to find to adversarial examples , or have to use more adversarial examples ? \u201d Our main claim is that using a model which can perform reconstruction can map points off of the data manifold back onto the manifold . For example , we can imagine that unusual noise patterns would not appear in the reconstructions . These off-manifold points are not necessarily adversarial examples and not all adversarial examples are from off of the manifold ( Gilmer 2018 ) . However , our claim is only that * some * of the adversarial examples are off of the manifold , and thus when we use adversarial training , it is more effective and efficient when we have the autoencoders in the network , as it reduces the space that we need to search over . Evidence that some adversarial examples are off of the manifold ( at least for an undefended network ) is in our paper in figure 1 . Some additional qualitative evidence supporting this claim is provided by another submission . Figure 2 and Figure 3 of the \u201c Robustness May be at Odds with Accuracy \u201d paper ( https : //openreview.net/pdf ? id=SyxAb30cY7 ) , show the perturbations for a defended model appear to be somewhat unrealistic ( although much less so then for an undefended model ) . \u201c Another problem rise from Ian Goodfellow 's comment . I am trying not to be biased . So if the author could address his comments properly , I am willing to change the rating. \u201d We strongly believe that it is essential to show that the improvements do not result from gradient obfuscation as well as to demonstrate improvements against strong attacks ( such as PGD ) on CIFAR-10 . We have thus run additional experiments demonstrating effectiveness of the method on CIFAR-10 , on a CNN as well as a ResNet architecture . We ran validation experiments to confirm that our method does not simply operate by obfuscating gradients ."}, {"review_id": "SkgVRiC9Km-2", "review_text": "The method works by substituting a hidden layer with a denoised version. Not only it enable to provide more robust classification results, but also to sense and suggest to the analyst or system when the original example is either adversarial or from a significantly different distribution. Improvements in adversarial robustness on three datasets are significant. Bibliography is good, the text is clear, with interesting and complete experimentations.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "\u201c The method works by substituting a hidden layer with a denoised version . Not only it enable to provide more robust classification results , but also to sense and suggest to the analyst or system when the original example is either adversarial or from a significantly different distribution . Improvements in adversarial robustness on three datasets are significant . Bibliography is good , the text is clear , with interesting and complete experimentations. \u201d Thank you for your feedback . We have obtained several new results to address concerns related to gradient obfuscation raised by other reviewers and the public comment ."}, {"review_id": "SkgVRiC9Km-3", "review_text": "In this paper, the authors proposed a fortified network model, which is an extension to denoising autoencoder. The extension is to perform the denoising module in the hidden layers instead of input layer. The motivation of this extension is that the denoising part is more effective in the hidden layers. Overall, this extension is quite sensible, and empirical results justify the utility of this extension. The major issue, which was left as an open question in the end of Section 3, is that when and where to use fortified layers. The authors discussed this issue, but did not solve this issue. Nevertheless, I do believe solving this issue requires a sequence of papers. Overall the paper reads very well, but there are a number of minor places to be improved. (1) a grammar error at \"provide a reliable signal of the existence of input data that do not lie on the manifold on which it the network trained.\" (2) a grammar error at \"This expectation cannot be computed, therefore a common approach is to to minimize the empirical risk\" (3) The sentence \"For a mini-batch of N clean examples, x(1), ..., x(N), each hidden layer h(1)_k, ..., h(N)_k is fed into a DAE loss\" is a little confusing to me. \"h(1)_k, ..., h(N)_k\" is only for one hidden layer, rather than \"each hidden layer\". Right?", "rating": "6: Marginally above acceptance threshold", "reply_text": "\u201c The major issue , which was left as an open question in the end of Section 3 , is that when and where to use fortified layers . The authors discussed this issue , but did not solve this issue . Nevertheless , I do believe solving this issue requires a sequence of papers . Overall the paper reads very well , but there are a number of minor places to be improved. \u201c We thank the reviewer for the positive and constructive feedback . We also like to point out that we \u2019 ve conducted new experiments to help to demonstrate that our method isn \u2019 t benefiting from obfuscated gradients and additionally we ran PGD attacks with many more iterations ( 200 ) on CIFAR-10 ( see the response to reviewer 4 ) ."}], "0": {"review_id": "SkgVRiC9Km-0", "review_text": "This paper proposes a new defense to adversarial examples based on the 'fortification' of hidden layers using a denoising autoencoder. While building models that are robust to adversarial examples is an important and relevant research problem, I am not convinced by the evaluation of the defense. Specific comments: - The authors mostly evaluate their defense using FGSM (particularly on CIFAR). To truly establish the merit of a new defense, the authors must benchmark against state-of-the-art defenses such as PGD. It also seems like the epsilon values used for the PGD attacks are fairly small. The authors should report accuracies to a range of epsilon values for the PGD attack, as is standard. - When the authors attack their models using PGD/FGSM, is this only on the classification loss or does this also include the denoising terms? Similar defenses which use denoisers have been broken once you run PGD on the full model [1]. - I do not really understand the motivation behind using an autoencoder here. Firstly, it is not clear that adversarial examples lie off the data manifold - they could form a very small set on the data manifold and thereby not affect standard generalization. Secondly, have the authors tried a simple regularization loss based on the error between hidden layer representations to a natural examples and the corresponding adversarial example? I think the authors must motivate the use of denoising autoencoders here by comparing to such a simple baseline. General comment: The results hard to parse given the arrangement of figures and tables. Also, which approach are the authors denoting as \u2018baseline adv. Train\u2019 in the tables? Overall I feel like building defenses to adversarial examples is a challenging problem and the empirical investigation in this paper is not sufficient to illustrate any real progress on this front. [1] Athalye, A., & Carlini, N. (2018). On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses. arXiv preprint arXiv:1804.03286. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your feedback . We strongly agree that it is absolutely essential to show that the improvements are not a result of gradient obfuscation . \u201c The authors mostly evaluate their defense using FGSM ( particularly on CIFAR ) . To truly establish the merit of a new defense , the authors must benchmark against state-of-the-art defenses such as PGD . \u201d We added new results with PGD on CIFAR-10 with many more PGD-steps for evaluation . We evaluated a convolutional network on CIFAR-10 with 4 convolutional layers followed by a single fully-connected layer . We trained fortified networks , where we added an autoencoder following each hidden layer . We also added a baseline \u201c Extra Layers \u201d where we trained with the layers added to match the capacity of Fortified Networks ( same number of parameters ) . # steps | Baseline | Baseline w/ extra layers | Fortified Networks 7 steps | 33.0 | 34.2 | 45.0 50 steps | 31.6 | 32.5 | 42.1 200 steps | 31.4 | 32.2 | 41.5 Even when running PGD for 200 steps , we found large and consistent advantages for fortified networks , which are not primarily attributable to adding additional layers . \u201c It also seems like the epsilon values used for the PGD attacks are fairly small . The authors should report accuracies to a range of epsilon values for the PGD attack , as is standard. \u201d This is a great point and we performed an additional experiment using the same convolutional neural network discussed above . Using 7 and 100 steps of PGD , we attacked our fortified nets model with varying epsilons : PGD , 100 steps Epsilon | Baseline with extra layers | Fortified Networks 0.03 | 35.3 | 39.2 0.04 | 24.8 | 28.0 0.06 | 14.3 | 15.6 0.08 | 12.0 | 13.0 0.1 | 11.7 | 12.9 0.2 | 10.2 | 11.3 0.3 | 8.4 | 9.6 \u201c When the authors attack their models using PGD/FGSM , is this only on the classification loss or does this also include the denoising terms ? Similar defenses which use denoisers have been broken once you run PGD on the full model [ 1 ] . \u201d We run our attacks on the full model , end-to-end , including the autoencoders . This is a major difference from [ 1 ] , and that change is what broke the paper you referenced . The paper that you referenced did not perform adversarial training on the main part of the network , and only trained the autoencoder , keeping the classifier network itself fixed . We also conducted a new experiment with BPDA ( Athalye 2018 ) , where we consider skipping the autoencoders in the backward pass ( i.e.using the identity function to compute the gradients ) as well as running the forward and backward pass of the network with no noise injected . We also ran some new experiments for this using eps=0.03 and 100 steps of PGD using the same CNN architecture discussed earlier . 33.4 ( baseline , normal attack ) 40.1 ( Fortified Networks , normal attack ) 38.2 ( Fortified Networks , no noise during attack ) 67.1 ( Fortified Networks , skip DAE during attack , BPDA ) This is strong evidence that skipping the autoencoders while generating the attacks significantly weakens them , but turning the noise off slightly strengthens the attack , but it is still much stronger as a defense than the baseline adversarially trained model with the same number of parameters and capacity . \u201c Secondly , have the authors tried a simple regularization loss based on the error between hidden layer representations to a natural examples and the corresponding adversarial example ? I think the authors must motivate the use of denoising autoencoders here by comparing to such a simple baseline \u201d Yes , we conducted new experiments to directly address this issue ( Adversarial Logit Pairing is a special case of the regularizer that you describe ) , but we also note that our method provides improvements even when we don \u2019 t use the L_adv loss comparing the adversarial input \u2019 s hidden states to the clean input \u2019 s h. This is with the same CNN architecture discussed earlier . PGD , 7 iterations : 43.3 ( Fortified Networks ) 38.1 ( Adv.training baseline ) 34.2 ( Penalty between layers ) PGD , 100 iterations : 39.2 ( Fortified Networks ) 35.3 ( Adv.training baseline ) 32.2 ( Penalty between layers ) We found that this penalty between the hidden states , where we attracted the hidden states in the network on adversarial inputs to the hidden states of the network on clean states ( applied at every layer ) , hurts robustness somewhat , but it may be possible that such an approach depends on exactly how it \u2019 s used . We also add that unlike adversarial logit pairing , our improvements hold up after running PGD for a large number of iterations , whereas the benefits from adversarial logit pairing almost entirely disappear . \u201c Also , which approach are the authors denoting as \u2018 baseline adv . Train \u2019 in the tables ? \u201c This refers to the PGD training of Madry 2017 ."}, "1": {"review_id": "SkgVRiC9Km-1", "review_text": "This paper presents an approach of fortifying the neural networks to defend attacks. The major component should be a denoising autoencoder with noise in the hidden layer. However, from the paper, I am still not convinced why this defends the FGSM attack. From my perspective, a more specifically designed algorithm could attack the network described in the paper as the old way, and what is the insight of defending the attacks, whether this objective function is harder to find to adversarial examples, or have to use more adversarial examples? Another problem rise from Ian Goodfellow's comment. I am trying not to be biased. So if the author could address his comments properly, I am willing to change the rating.", "rating": "5: Marginally below acceptance threshold", "reply_text": "\u201c This paper presents an approach of fortifying the neural networks to defend attacks . The major component should be a denoising autoencoder with noise in the hidden layer . However , from the paper , I am still not convinced why this defends the FGSM attack . From my perspective , a more specifically designed algorithm could attack the network described in the paper as the old way \u201d In our experiments ( except where we explicitly test against a special BPDA ) we backpropagate errors through the autoencoders , such that the the autoencoders are not hidden from the attacker . Indeed we found that skipping the autoencoders when running the attacks makes them significantly weaker , but in our main experiments we backpropagate through the autoencoders and allow the attacker to use this information . \u201c and what is the insight of defending the attacks , whether this objective function is harder to find to adversarial examples , or have to use more adversarial examples ? \u201d Our main claim is that using a model which can perform reconstruction can map points off of the data manifold back onto the manifold . For example , we can imagine that unusual noise patterns would not appear in the reconstructions . These off-manifold points are not necessarily adversarial examples and not all adversarial examples are from off of the manifold ( Gilmer 2018 ) . However , our claim is only that * some * of the adversarial examples are off of the manifold , and thus when we use adversarial training , it is more effective and efficient when we have the autoencoders in the network , as it reduces the space that we need to search over . Evidence that some adversarial examples are off of the manifold ( at least for an undefended network ) is in our paper in figure 1 . Some additional qualitative evidence supporting this claim is provided by another submission . Figure 2 and Figure 3 of the \u201c Robustness May be at Odds with Accuracy \u201d paper ( https : //openreview.net/pdf ? id=SyxAb30cY7 ) , show the perturbations for a defended model appear to be somewhat unrealistic ( although much less so then for an undefended model ) . \u201c Another problem rise from Ian Goodfellow 's comment . I am trying not to be biased . So if the author could address his comments properly , I am willing to change the rating. \u201d We strongly believe that it is essential to show that the improvements do not result from gradient obfuscation as well as to demonstrate improvements against strong attacks ( such as PGD ) on CIFAR-10 . We have thus run additional experiments demonstrating effectiveness of the method on CIFAR-10 , on a CNN as well as a ResNet architecture . We ran validation experiments to confirm that our method does not simply operate by obfuscating gradients ."}, "2": {"review_id": "SkgVRiC9Km-2", "review_text": "The method works by substituting a hidden layer with a denoised version. Not only it enable to provide more robust classification results, but also to sense and suggest to the analyst or system when the original example is either adversarial or from a significantly different distribution. Improvements in adversarial robustness on three datasets are significant. Bibliography is good, the text is clear, with interesting and complete experimentations.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "\u201c The method works by substituting a hidden layer with a denoised version . Not only it enable to provide more robust classification results , but also to sense and suggest to the analyst or system when the original example is either adversarial or from a significantly different distribution . Improvements in adversarial robustness on three datasets are significant . Bibliography is good , the text is clear , with interesting and complete experimentations. \u201d Thank you for your feedback . We have obtained several new results to address concerns related to gradient obfuscation raised by other reviewers and the public comment ."}, "3": {"review_id": "SkgVRiC9Km-3", "review_text": "In this paper, the authors proposed a fortified network model, which is an extension to denoising autoencoder. The extension is to perform the denoising module in the hidden layers instead of input layer. The motivation of this extension is that the denoising part is more effective in the hidden layers. Overall, this extension is quite sensible, and empirical results justify the utility of this extension. The major issue, which was left as an open question in the end of Section 3, is that when and where to use fortified layers. The authors discussed this issue, but did not solve this issue. Nevertheless, I do believe solving this issue requires a sequence of papers. Overall the paper reads very well, but there are a number of minor places to be improved. (1) a grammar error at \"provide a reliable signal of the existence of input data that do not lie on the manifold on which it the network trained.\" (2) a grammar error at \"This expectation cannot be computed, therefore a common approach is to to minimize the empirical risk\" (3) The sentence \"For a mini-batch of N clean examples, x(1), ..., x(N), each hidden layer h(1)_k, ..., h(N)_k is fed into a DAE loss\" is a little confusing to me. \"h(1)_k, ..., h(N)_k\" is only for one hidden layer, rather than \"each hidden layer\". Right?", "rating": "6: Marginally above acceptance threshold", "reply_text": "\u201c The major issue , which was left as an open question in the end of Section 3 , is that when and where to use fortified layers . The authors discussed this issue , but did not solve this issue . Nevertheless , I do believe solving this issue requires a sequence of papers . Overall the paper reads very well , but there are a number of minor places to be improved. \u201c We thank the reviewer for the positive and constructive feedback . We also like to point out that we \u2019 ve conducted new experiments to help to demonstrate that our method isn \u2019 t benefiting from obfuscated gradients and additionally we ran PGD attacks with many more iterations ( 200 ) on CIFAR-10 ( see the response to reviewer 4 ) ."}}