{"year": "2021", "forum": "JHcqXGaqiGn", "title": "Accurate Learning of Graph Representations with Graph Multiset Pooling", "decision": "Accept (Poster)", "meta_review": "The paper addresses a very important issue in GNN, the definition of a well-defined pooling function for node aggregation. The proposed Graph Multiset Transformer, although not entirely new, seems to be useful in practice. Issues related to experimental results, as well as problems with presentation, have been solved by the authors's rebuttal, that presented solid experimental results and analysis.\nConcerns about the real expressivity of the proposed approach when compared to Weisfeiler-Lehman graph isomorphism test do not affect the contribution delivered by the paper, that seems, at this point, significant. ", "reviews": [{"review_id": "JHcqXGaqiGn-0", "review_text": "This paper proposes a Transformer-like model : Graph Multiset Transformer to perform the graph pool/aggregation . Overall , the technique part is concrete and clear and the experimental evaluations are comprehensive . The authors also prove the expressive power regarding WL-test . However , several points need to be clarified or addressed . 1.The idea to utilize the Transformer-like architecture to model the graph neural network ( GNN ) is not new . Some existing works [ 1,2 ] have employed the transformer to enhance the expressive power of GNN . It \u2019 s better to add more discussions between GTM and the existing works to highlight its contribution . Meanwhile , several studies [ 3,4,5,6,7 ] about graph pooling / self-attention are missing . It \u2019 s better to make discussions in the related works . Moreover , if possible , I suggest making the comparison with these methods , especially the recent studies to make the whole experimental results more convincing , e.g.HaarPool , EigenPool , etc . 1.About the experimental settings . 1 ) In Section 4.1 , the authors state that the 4 molecule datasets are obtained from OGB dataset . However , in OGB dataset , it only contains HIV , while Tox21 , Toxcast , BBBP is not included . Maybe there is a mistake . 2 ) For the molecular dataset , the data splitting is very curial for the final results . Meanwhile , the atom/bond feature extraction process for the molecular datasets is unclear . The authors need to clarify the data splitting ( random/scaffold ) and feature extraction process to ensure the reproducibility of experiments . Minor : In Equation 6 , what is the QW_i^Q ? In Equation 8 , why we need MH ( H , H , H ) ? How about directly applying H into SelfAtt block , i.e. , Z=H ? In the experiments , this paper only evaluates the memory efficiency of GMT . I would like to see the evaluation about the time efficiency of GMT with other baselines . Overall , this paper is well written and the experiments look solid . Considering the novelty issue , I think this is a borderline paper . I recommend \u201c Marginally below acceptance threshold \u201d and would like to see the author 's response . [ 1 ] Rong , Yu , et al . `` GROVER : Self-supervised Message Passing Transformer on Large-scale Molecular Data . '' arXiv preprint arXiv:2007.02835 ( 2020 ) . [ 2 ] Chithrananda , Seyone , Gabe Grand , and Bharath Ramsundar . `` ChemBERTa : Large-Scale Self-Supervised Pretraining for Molecular Property Prediction . '' arXiv preprint arXiv:2010.09885 ( 2020 ) . [ 3 ] Wang , Yu Guang , et al . `` Haar graph pooling . '' arXiv ( 2019 ) : arXiv-1909 . [ 4 ] Ma , Yao , et al . `` Graph convolutional networks with eigenpooling . '' Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2019 . [ 5 ] Bianchi , Filippo Maria , Daniele Grattarola , and Cesare Alippi . `` Spectral clustering with graph neural networks for graph pooling . '' ( 2020 ) . [ 6 ] Ranjan , Ekagra , Soumya Sanyal , and Partha P. Talukdar . `` ASAP : Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations . '' AAAI.2020 . [ 7 ] Li , Jia , et al . `` Semi-supervised graph classification : A hierarchical graph perspective . '' The World Wide Web Conference . 2019 .", "rating": "7: Good paper, accept", "reply_text": "* * Question 3 : * * In Equation 6 , what is the $ QW_i^Q $ ? * * Answer : * * $ QW_i^Q $ consists of the input query $ Q $ and the learnable parameterized weight matrix $ W_i^Q $ . Therefore , it depends on the input query $ Q $ , which is $ S $ in equation 7 and $ H $ in equation 8 . * * Question 4 : * * In Equation 8 , why we need MH ( $ H $ , $ H $ , $ H $ ) ? How about directly applying $ H $ into SelfAtt block , i.e. , $ Z=H $ ? * * Answer : * * The term MH ( $ H $ , $ H $ , $ H $ ) is not an operation newly introduced with our method , but is the * * standard multi-headed attention operation for a self-attention * * layer . The conventional attention operation MH ( $ Q $ , $ K $ , $ V $ ) computes the relevance score of $ Q $ to $ K $ , and then performs the weighted sum of $ V $ with the calculated relevance scores . For self-attentions , $ Q=K=V $ , and thus MH ( $ H $ , $ H $ , $ H $ ) first computes the relevance scores of every pair of elements in the input vector $ H $ , and then performs the weighted sum of elements in $ H $ with calculated relevance scores . If we set $ Z = H $ , then the SelfAtt function becomes a simple linear layer with skip connection , and thus can not consider the inter-node relationships . * * Question 5 : * * In the experiments , this paper only evaluates the memory efficiency of GMT . I would like to see the evaluation about the time efficiency of GMT with other baselines . * * Answer : * * Thanks for your suggestion . We have additionally * * provided the results about the time efficiency * * of GMT with other baselines in Figure 4 of the revision . [ A ] Hu et al. \u201c Open Graph Benchmark : Datasets for Machine Learning on Graphs. \u201d arXiv 2020 . [ B ] https : //ogb.stanford.edu/docs/graphprop/ [ C ] Niepert et al. \u201c Learning convolutional neural networks for graphs. \u201d ICML 2016 . [ D ] Zhang et al. \u201c An End-to-End Deep Learning Architecture for Graph Classification. \u201d AAAI 2018 . [ E ] Xu et al. \u201c How Powerful are Graph Neural Networks ? . \u201d ICLR 2019 . [ F ] Chang and Lin . \u201c Libsvm : A library for support vector machines. \u201d TIST 2011 . [ G ] Errica et al. \u201c A Fair Comparison of Graph Neural Networks for Graph Classification \u201d ICLR 2020 ."}, {"review_id": "JHcqXGaqiGn-1", "review_text": "This work studies the graph pooling operation for graph neural networks . It proposes the Graph Multiset Pooling which treats the graph pooling as a multiset encoding problem and can capture the graph structural information . It first employs multi-head attention to learn node features , where the query Q is a learnable matrix contains k vectors . Then a GMPool operation is performed and finally , the self-attention is used for learning inter-node relationships . Experimental results show the effectiveness of the proposed method . Strengths : + This work studies an important problem , graph pooling . Graph pooling can learn high-level graph representations but is still less explored . + The proposed method is interesting . By using a learnable query matrix , the method can reduce the n-node input to k-node output . The self-attention used after GMPool can learn the relationships between high-level embeddings . + The experimental results are promising . The proposed method outperforms other compared methods . Weaknesses : - Even though the method is called Multiset Pooling , its method is not related to Multiset . The proposed method is mainly based on attention and self-attention mechanism . Then claiming the proposed method as Multiset Pooling is not convincing . - The experimental settings are not fair enough . The pooling operation is defined as reducing n-node input to k-node output . For all other methods , the pooling layer is connected with the global sum/average . However , in the proposed GMT , the GMPool is connected with a self-attention layer . It is not clear whether the proposed GMPool or the self-attention layer leads to the performance gain ? A careful ablation study is needed . - I think the proposed method can be regarded as using SAGPool in the clustering-based pooling . The main difference is the multi-head attention and the learnable matrix S. Please comment if I missed something . - The use of graph structures is not very convincing . The GNN ( H , A ) is the simple message passing of GNNs . Then the graph structural information A is already incorporated in H since H is obtained by GNNs . - Several baselines are missing , such as Structpool and Edge Pooling . They should be discussed and compared . Questions : 1 . If the query Q is obtained from the learnable matrix S , which is k * d dimensions . Then the output of Att ( Q ; K ; V ) should also have k * d \u2019 dimension , which means the output is already reduced from n vectors to k vectors . Why do we still need the GMPool operation ? The GMPool does not \u201c compress the n nodes into the k typical nodes \u201d . ==Update after rebuttal== I have read the authors ' rebuttal . Most of my concerns are addressed properly , and hence I am willing to increase my score from 4 to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Question 3 : * * I think the proposed method can be regarded as using SAGPool in the clustering-based pooling . The main difference is the multi-head attention and the learnable matrix $ S $ . * * Answer : * * This is a critical misunderstanding since our method and SAGPool have little in common , even when not considering the fact that our method captures both a multiset and a global graph structure among the nodes . SAGPool refers to the * * message-passing operation as the self-attention * * in equation 1 , and they are not conventional self-attention mechanisms . The main idea of SAGPool is to drop unimportant nodes for pooling , whereas the proposed GMT uses the attention to weight the node representations with their relevance to the query . Further note that GMT encodes a multiset and considers the graph structure when pooling ( Figure 1 ( Right ) ) . * * Question 4 : * * The use of graph structures in GMPool is not very convincing since the graph structural information $ A $ is already incorporated in $ H $ . * * Answer : * * While the graph structural information $ A $ might be preserved in $ H $ , the naive attention function generates the keys and values that are linearly transformed from $ H $ , which is suboptimal in capturing the graph structure . To tackle these limitations , we use two separate GNNs with graph structural information $ A $ to generate the key and value matrices , which can more explicitly preserve the structural information on $ A $ than the simple linear transformation . We empirically show that the proposed Graph Multi-head Attention achieves significantly better results than naive multi-head attention ( See Table 2 and Figure 6 ) . We have clarified this point in Section 3.2 , Graph Multi-head Attention paragraph in the revision . * * Question 5 : * * More baselines , such as Structpool and Edge Pooling , should be discussed and compared . * * Answer : * * Thank you for the suggestion . Please note that we compare against MinCutPool ( ICML 2020 ) , which is * * more recent work than StructPool * * and were not aware of Edge Pooling at the time of submission since it is an Arxiv paper . We have discussed Structpool and Edge Pooling in the related work section of the revision , and further provide initial results that we obtained with them thus far ( Note that Edge Pooling is very slow ) in the common comment , and have also included them in Table 1 , Figure 3 , and Figure 4 of the revision . We will include the full experimental comparison against the two baselines in the final revision . * * Question 6 : * * If the query $ Q $ is obtained from the learnable matrix $ S $ , which is $ k \\times d $ dimensions . Then the output of Att ( $ Q $ ; $ K $ ; $ V $ ) should also have $ k \\times d $ dimension , which means the output is already reduced from $ n $ vectors to $ k $ vectors . Why do we still need the GMPool operation ? * * Answer : * * We require the GMPool operation , since nodes are reduced from $ n $ nodes to $ k $ nodes through GMPool , not by the conventional attention function which learns an $ n $ -to- $ n $ mapping . The reason why we construct several attention functions such as Att ( Attention ) , MH ( Multi-head Attention in equation 5 ) , and GMH ( Graph Multi-head Attention in equation 6 ) in the Graph Multi-head Attention paragraph is that these operations are used for both GMPool and SelfAtt . In other words , these attention components are not only used for compressing nodes with GMPool but are also used for capturing global node relationships with SelfAtt . Thus , the Att function with $ k \\times d $ dimension is a specific design choice for compressing $ n $ nodes into $ k $ typical nodes , and GMPool makes this possible with the learnable matrix $ S $ ."}, {"review_id": "JHcqXGaqiGn-2", "review_text": "The work extends the set transformer to obtain a method for multi-head attention pooling on multisets with connectivity ( graphs ) . The authors show that the approach is as expressive as the WL isomorphism test and has better space complexity than existing node clustering networks . The method achieves state-of-the-art results on graph classification and strong results on graph reconstruction and generation . Strengths : - The paper is very well written and polished . - The figures complement the text well . - The work is technically and mathematically sound . - The method shows good results and is scalable , making it a valuable addition to the set of existing GNN operators . - The authors make an effort to substantiate their statements about expressivity and scalability with proofs . - The experiments are well chosen and show where improvements come from . Weaknesses : - The proven expressiveness is not a very strong statement , since most pooling approaches adhere to this property . It is nice to have the theoretical analysis though . - The method itself is an incremental variation of set transformers ( Lee et al . ) , although adapted for a different type of input data . - Using the identity matrix as adjacency ( as described in Appendix B , to work around the scalability issue of node clustering methods ) seems to make the approach identical to the set transformer in all layers except the first , which dampens the contribution . There is some potential for improvement in clarity of presentation : - In abstract : `` may yield representations that do not pass the graph isomorphism test '' , in introduction , page 2 : `` accurate representations of given graphs that can pass the WL-test '' . Those sentences are confusing as it is not about representations passing the WL test , is it ? It is about two graphs which are distinguished by WL get different representations ( as correctly stated elsewhere in the work ) . - Regarding page 4 , paragraph `` Graph Multi-head Attention '' and the following ones : - On the one hand , they are extremely close to Vaswani et al.and Lee et al . ( sometimes even nearly the same sentences ) . - On the other hand , some things are left out , which are crucial for understanding , such as definitions of symbols for dimensionalities ( n_q , d_k , d_v , d_model ) and the origin of some matrices , see next point . - Where do the seeds S come from ( for the non self attention operator ) ? It probably is a parameter matrix that is directly optimized but it is not completely clear ( `` learnable '' is ambiguous , can also be the output of a network ) - Suggestion : maybe the description of the pooling method becomes more clear when described in a top-down manner : Eq 7 - > Eq 6 - > Eq 5 - > Att Experiments : - Variances of graph classification results over the cross validation would be greatly appreciated ( since there seems to be a space issue , they can go into the appendix ) - The reconstruction architecture does not reconstruct adjacency . It might be interesting to see how well the method can do that ( for the synthetic graphs for example ) Related work : - The work [ 1 ] should be mentioned and discussed in related work and compared against in experiments . It is also a pooling method with attention but seems to follow a different approach . Typos : - Figure 2 caption : `` to compress the all nodes '' -all - Proof of Theorem 4 : `` but it is highly efficient than node clustering methods '' - Proof of Proposition 5 : `` then the first them inherently generates '' All in all , I think this paper has a valuable contribution , even if the method is incremental . Therefore , I tend to vote for accepting the paper but encourage the authors to improve on the mentioned issues in experiments , related work and presentation . [ 1 ] Ranjan et al . : ASAP : Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations , AAAI 2020", "rating": "7: Good paper, accept", "reply_text": "We sincerely appreciate your constructive and helpful comments . We initially address all your comments below : * * Question 1 : * * In abstract : `` may yield representations that do not pass the graph isomorphism test '' , in introduction , page 2 : `` accurate representations of given graphs that can pass the WL-test '' . Those sentences are confusing as it is not about representations passing the WL test . It is about two graphs which are distinguished by WL get different representations ( as correctly stated elsewhere in the work ) . * * Answer : * * We apologize for the confusion , and thank you for pointing out the inaccurate wording . The two sentences in the abstract and introduction mean that the representation power for the proposed method is as powerful as the WL test , such that the proposed pooling method represents the two distinct graphs differently , while the baseline may yield the same representation for two different graphs distinguished by the WL test . We have corrected them in the revision . * * Question : 2 * * Some things are left out in the Graph Multi-head Attention paragraph , such as definitions of symbols for dimensionalities ( n_q , d_k , d_v , d_model ) . Also , where do the seeds S come from ? * * Answer : * * We apologize for the confusion , and thank you for pointing them out . We carefully revised the description of the Graph Multi-head Attention for improved clarity . The seed matrix S is a parameter matrix that is directly optimized , and not an output of a network . * * Question 3 : * * Variances of graph classification results over the cross validation would be greatly appreciated . * * Answer : * * We have reported the standard deviation on graph classification results with a p-value for the significance test in Table 1 ( graph classification ) , and also visualized the variance in Figure 6 ( graph reconstruction ) of the revision , as suggested . * * Question 4 : * * The reconstruction architecture does not reconstruct adjacency . It might be interesting to see how well the method can do that . * * Answer : * * Thanks for your suggestion , and we have * * included and discussed the results on reconstructing the adjacency matrix * * from the compressed representation in Appendix D , Adjacency Reconstruction for Graph Reconstruction paragraph of the revision . Please note that reconstruction for the adjacency matrix is more difficult than reconstruction for nodes and thus are not covered in any of the existing works to our knowledge , since , while the reconstructed node feature can be represented as continuous values , the reconstructed adjacency matrix should be further transformed from continuous values to discrete values ( 0 or 1 for the undirected simple graph ) . We thus leave the reconstruction of the adjacency matrices as future work . * * Question 5 : * * The work [ 1 ] should be mentioned and discussed in related work and compared against in experiments . It is also a pooling method with attention but seems to follow a different approach . * * Answer : * * ASAP [ 1 ] is clearly different from the proposed GMPool . ASAP first computes the local cluster assignment using the message-passing operation , and then drops the unimportant local clusters with their scores . However , the proposed GMPool globally compresses the $ n $ nodes into $ k $ nodes with their relevance score for the seed matrix S , and does not drop any nodes , and thus does not suffer from any information loss . We have compared against ASAP , and you can see its result in the response to the common comments and Table 1 . * * Question 6 : * * Typos . * * Answer : * * Thanks for pointing them out . We have corrected them in the revision . [ 1 ] Ranjan et al . : ASAP : Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations , AAAI 2020"}, {"review_id": "JHcqXGaqiGn-3", "review_text": "This work proposes a Graph Multiset Transformer ( GMT ) that uses a multi-head attention-based approach to capture potential interactions between nodes when pooling nodes to produce a graph representation . Multi-head attention mechanism is used to group nodes into clusters , each of which produces a representation . Self-attention is then used to pool representations of clusters into the representation of a graph . Pros : The proposed pooling is more reasonable than a simple sum or average pooling as the multi-head attention mechanism can potentially capture dependencies between nodes . Cons : Several parts of the manuscript need more explanations . Additional experimental results ( see below for details ) are needed . ( 1 ) It is not clear what multi-head attention achieve semantically ( as claimed : better capture structure information ) . ( 2 ) Figures are neither self-explained nor well explained in the main text . ( 3 ) The std of cross validation in each experiment should be reported . The mean performance alone is not enough to say a method performs better or not . It will be better to provide a p-value ( e.g. , t-test ) to show if a method is statistically significantly better . ( 4 ) Four datasets from the Open Graph Benchmark were used . The authors should refer to the leaderboard for the performance of some baseline methods . For example , the leaderboard of HIV dataset reports that GIN has 0.7654 rather than 0.7595 listed in this manuscript . ( 5 ) The abstract points out that without considering task relevance is a weakness of the previous pooling method . However , it is not clear how the proposed approach make improvement ( s ) on this . ( 6 ) The proposed method does not necessary pass graph isomorphism as the nodes in the manuscript have attributes but the proof does not consider node attributes . ( 7 ) In appendix , examples in Figure 10 are confusing . More explanations are needed .", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * Question 4 : * * Four datasets from the Open Graph Benchmark were used . The authors should refer to the leaderboard for the performance of some baseline methods . * * Answer : * * We reproduced all baselines for a * * fair comparison of all pooling models * * and not the performance of message-passing layers . We ran all experiments for all baselines and our models in the * * same setting * * as described in appendix C.2 . Specifically , we average the results over 10 different runs with the same hidden dimension ( 128 , and leaderboard uses 300 ) , and the same number of message-passing layers ( 3 , and leaderboard uses 5 ) with 10 different seeds for all models , which is clearly described in the appendix C.2 implementation details . Therefore , the results can be slightly different from the leaderboard results ( Please see the table below ) , since the leaderboard uses different hyperparameters with different random seeds compared to the experimented pooling models . Note that our reproduction sometimes outperforms the leaderboard results ( See the GCN results on HIV ) . However , as suggested , we have included the leaderboard results in the revision ( See Appendix D ) . | | HIV | Tox21 | | | | | | GCN | 76.81 | 75.04 | | GCN ( Leaderboard ) | 76.06 | 75.29 | | GIN | 75.95 | 73.27 | | GIN ( Leaderboard ) | 75.58 | 74.91 | | GMT ( Ours ) | * * 77.56 * * | * * 77.30 * * | * * Question 5-1 : * * The abstract points out that without considering task relevance is a weakness of the previous pooling method . * * Answer : * * This is a critical misunderstanding . We clearly state in the abstract that inconsideration of task relevance is a weakness of the simple sum/average function , not the previous learnable parametric pooling methods that are optimized in an end-to-end fashion . We stated this point in the introduction section as follows : * As a simplest approach for graph pooling , we can average or sum all node features in the given graph . However , since such simple aggregation schemes treat all nodes equally without considering their relative importance on the given tasks , they can not generate a meaningful graph representation in a task-specific manner . Furthermore , we stated this point in the related work section as follows : * While averaging all node features is directly used as simplest pooling methods , simplest pooling methods result in a loss of information since they consider all node information equally without considering key features for graphs . Therefore , we did not claim that previous pooling methods , except the flat pooling method such as average/sum , could not consider the task relevance . * * Question 5-2 : * * It is not clear how the proposed approach makes improvement ( s ) on this sum/average pooling . * * Answer : * * Please note that all existing parameterized pooling operations that are learnable in an end-to-end fashion , including ours , can treat nodes differently according to their task relevance , rather than considering all nodes equally as in simple sum/average pooling . This is done by the trained networks allocating more weights on the important nodes for the specific task . Figure 7 shows one specific example that different nodes have different cluster weights for the graph reconstruction task . * * Question 6 : * * The proposed method does not necessarily pass graph isomorphism as the nodes in the manuscript have attributes but the proof does not consider node attributes . * * Answer : * * We want to clarify that the proposed method can be at most * * as powerful as the WL test * * for the graph isomorphism test , rather than directly passing the graph isomorphism test . To clarify this point , we have * * revised the text to tone down * * on the claim , as suggested . Note that the proposed Graph Multiset Pooling can uniquely map each multiset $ H $ with a bounded size , such that whether a node has attributes or not does not matter for establishing the proposed pooling function that is as powerful as the WL test . Specifically , there exists a mapping function $ f $ that maps nodes to prime numbers , and the summation over nodes in a multiset constitutes a unique mapping with a function $ f $ , as shown in our proof of Lemma 2 . * * Question 7 : * * In Appendix , examples in Figure 10 ( Figure 11 in the revision ) are confusing . More explanations are needed . * * Answer : * * Thanks for the suggestion . We provide more explanations about the reconstruction examples in Figure 10 ( Figure 11 in the revision ) , including the detailed descriptions of components embedded , and the generated cluster assignments for each node or a set of nodes ."}], "0": {"review_id": "JHcqXGaqiGn-0", "review_text": "This paper proposes a Transformer-like model : Graph Multiset Transformer to perform the graph pool/aggregation . Overall , the technique part is concrete and clear and the experimental evaluations are comprehensive . The authors also prove the expressive power regarding WL-test . However , several points need to be clarified or addressed . 1.The idea to utilize the Transformer-like architecture to model the graph neural network ( GNN ) is not new . Some existing works [ 1,2 ] have employed the transformer to enhance the expressive power of GNN . It \u2019 s better to add more discussions between GTM and the existing works to highlight its contribution . Meanwhile , several studies [ 3,4,5,6,7 ] about graph pooling / self-attention are missing . It \u2019 s better to make discussions in the related works . Moreover , if possible , I suggest making the comparison with these methods , especially the recent studies to make the whole experimental results more convincing , e.g.HaarPool , EigenPool , etc . 1.About the experimental settings . 1 ) In Section 4.1 , the authors state that the 4 molecule datasets are obtained from OGB dataset . However , in OGB dataset , it only contains HIV , while Tox21 , Toxcast , BBBP is not included . Maybe there is a mistake . 2 ) For the molecular dataset , the data splitting is very curial for the final results . Meanwhile , the atom/bond feature extraction process for the molecular datasets is unclear . The authors need to clarify the data splitting ( random/scaffold ) and feature extraction process to ensure the reproducibility of experiments . Minor : In Equation 6 , what is the QW_i^Q ? In Equation 8 , why we need MH ( H , H , H ) ? How about directly applying H into SelfAtt block , i.e. , Z=H ? In the experiments , this paper only evaluates the memory efficiency of GMT . I would like to see the evaluation about the time efficiency of GMT with other baselines . Overall , this paper is well written and the experiments look solid . Considering the novelty issue , I think this is a borderline paper . I recommend \u201c Marginally below acceptance threshold \u201d and would like to see the author 's response . [ 1 ] Rong , Yu , et al . `` GROVER : Self-supervised Message Passing Transformer on Large-scale Molecular Data . '' arXiv preprint arXiv:2007.02835 ( 2020 ) . [ 2 ] Chithrananda , Seyone , Gabe Grand , and Bharath Ramsundar . `` ChemBERTa : Large-Scale Self-Supervised Pretraining for Molecular Property Prediction . '' arXiv preprint arXiv:2010.09885 ( 2020 ) . [ 3 ] Wang , Yu Guang , et al . `` Haar graph pooling . '' arXiv ( 2019 ) : arXiv-1909 . [ 4 ] Ma , Yao , et al . `` Graph convolutional networks with eigenpooling . '' Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2019 . [ 5 ] Bianchi , Filippo Maria , Daniele Grattarola , and Cesare Alippi . `` Spectral clustering with graph neural networks for graph pooling . '' ( 2020 ) . [ 6 ] Ranjan , Ekagra , Soumya Sanyal , and Partha P. Talukdar . `` ASAP : Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations . '' AAAI.2020 . [ 7 ] Li , Jia , et al . `` Semi-supervised graph classification : A hierarchical graph perspective . '' The World Wide Web Conference . 2019 .", "rating": "7: Good paper, accept", "reply_text": "* * Question 3 : * * In Equation 6 , what is the $ QW_i^Q $ ? * * Answer : * * $ QW_i^Q $ consists of the input query $ Q $ and the learnable parameterized weight matrix $ W_i^Q $ . Therefore , it depends on the input query $ Q $ , which is $ S $ in equation 7 and $ H $ in equation 8 . * * Question 4 : * * In Equation 8 , why we need MH ( $ H $ , $ H $ , $ H $ ) ? How about directly applying $ H $ into SelfAtt block , i.e. , $ Z=H $ ? * * Answer : * * The term MH ( $ H $ , $ H $ , $ H $ ) is not an operation newly introduced with our method , but is the * * standard multi-headed attention operation for a self-attention * * layer . The conventional attention operation MH ( $ Q $ , $ K $ , $ V $ ) computes the relevance score of $ Q $ to $ K $ , and then performs the weighted sum of $ V $ with the calculated relevance scores . For self-attentions , $ Q=K=V $ , and thus MH ( $ H $ , $ H $ , $ H $ ) first computes the relevance scores of every pair of elements in the input vector $ H $ , and then performs the weighted sum of elements in $ H $ with calculated relevance scores . If we set $ Z = H $ , then the SelfAtt function becomes a simple linear layer with skip connection , and thus can not consider the inter-node relationships . * * Question 5 : * * In the experiments , this paper only evaluates the memory efficiency of GMT . I would like to see the evaluation about the time efficiency of GMT with other baselines . * * Answer : * * Thanks for your suggestion . We have additionally * * provided the results about the time efficiency * * of GMT with other baselines in Figure 4 of the revision . [ A ] Hu et al. \u201c Open Graph Benchmark : Datasets for Machine Learning on Graphs. \u201d arXiv 2020 . [ B ] https : //ogb.stanford.edu/docs/graphprop/ [ C ] Niepert et al. \u201c Learning convolutional neural networks for graphs. \u201d ICML 2016 . [ D ] Zhang et al. \u201c An End-to-End Deep Learning Architecture for Graph Classification. \u201d AAAI 2018 . [ E ] Xu et al. \u201c How Powerful are Graph Neural Networks ? . \u201d ICLR 2019 . [ F ] Chang and Lin . \u201c Libsvm : A library for support vector machines. \u201d TIST 2011 . [ G ] Errica et al. \u201c A Fair Comparison of Graph Neural Networks for Graph Classification \u201d ICLR 2020 ."}, "1": {"review_id": "JHcqXGaqiGn-1", "review_text": "This work studies the graph pooling operation for graph neural networks . It proposes the Graph Multiset Pooling which treats the graph pooling as a multiset encoding problem and can capture the graph structural information . It first employs multi-head attention to learn node features , where the query Q is a learnable matrix contains k vectors . Then a GMPool operation is performed and finally , the self-attention is used for learning inter-node relationships . Experimental results show the effectiveness of the proposed method . Strengths : + This work studies an important problem , graph pooling . Graph pooling can learn high-level graph representations but is still less explored . + The proposed method is interesting . By using a learnable query matrix , the method can reduce the n-node input to k-node output . The self-attention used after GMPool can learn the relationships between high-level embeddings . + The experimental results are promising . The proposed method outperforms other compared methods . Weaknesses : - Even though the method is called Multiset Pooling , its method is not related to Multiset . The proposed method is mainly based on attention and self-attention mechanism . Then claiming the proposed method as Multiset Pooling is not convincing . - The experimental settings are not fair enough . The pooling operation is defined as reducing n-node input to k-node output . For all other methods , the pooling layer is connected with the global sum/average . However , in the proposed GMT , the GMPool is connected with a self-attention layer . It is not clear whether the proposed GMPool or the self-attention layer leads to the performance gain ? A careful ablation study is needed . - I think the proposed method can be regarded as using SAGPool in the clustering-based pooling . The main difference is the multi-head attention and the learnable matrix S. Please comment if I missed something . - The use of graph structures is not very convincing . The GNN ( H , A ) is the simple message passing of GNNs . Then the graph structural information A is already incorporated in H since H is obtained by GNNs . - Several baselines are missing , such as Structpool and Edge Pooling . They should be discussed and compared . Questions : 1 . If the query Q is obtained from the learnable matrix S , which is k * d dimensions . Then the output of Att ( Q ; K ; V ) should also have k * d \u2019 dimension , which means the output is already reduced from n vectors to k vectors . Why do we still need the GMPool operation ? The GMPool does not \u201c compress the n nodes into the k typical nodes \u201d . ==Update after rebuttal== I have read the authors ' rebuttal . Most of my concerns are addressed properly , and hence I am willing to increase my score from 4 to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Question 3 : * * I think the proposed method can be regarded as using SAGPool in the clustering-based pooling . The main difference is the multi-head attention and the learnable matrix $ S $ . * * Answer : * * This is a critical misunderstanding since our method and SAGPool have little in common , even when not considering the fact that our method captures both a multiset and a global graph structure among the nodes . SAGPool refers to the * * message-passing operation as the self-attention * * in equation 1 , and they are not conventional self-attention mechanisms . The main idea of SAGPool is to drop unimportant nodes for pooling , whereas the proposed GMT uses the attention to weight the node representations with their relevance to the query . Further note that GMT encodes a multiset and considers the graph structure when pooling ( Figure 1 ( Right ) ) . * * Question 4 : * * The use of graph structures in GMPool is not very convincing since the graph structural information $ A $ is already incorporated in $ H $ . * * Answer : * * While the graph structural information $ A $ might be preserved in $ H $ , the naive attention function generates the keys and values that are linearly transformed from $ H $ , which is suboptimal in capturing the graph structure . To tackle these limitations , we use two separate GNNs with graph structural information $ A $ to generate the key and value matrices , which can more explicitly preserve the structural information on $ A $ than the simple linear transformation . We empirically show that the proposed Graph Multi-head Attention achieves significantly better results than naive multi-head attention ( See Table 2 and Figure 6 ) . We have clarified this point in Section 3.2 , Graph Multi-head Attention paragraph in the revision . * * Question 5 : * * More baselines , such as Structpool and Edge Pooling , should be discussed and compared . * * Answer : * * Thank you for the suggestion . Please note that we compare against MinCutPool ( ICML 2020 ) , which is * * more recent work than StructPool * * and were not aware of Edge Pooling at the time of submission since it is an Arxiv paper . We have discussed Structpool and Edge Pooling in the related work section of the revision , and further provide initial results that we obtained with them thus far ( Note that Edge Pooling is very slow ) in the common comment , and have also included them in Table 1 , Figure 3 , and Figure 4 of the revision . We will include the full experimental comparison against the two baselines in the final revision . * * Question 6 : * * If the query $ Q $ is obtained from the learnable matrix $ S $ , which is $ k \\times d $ dimensions . Then the output of Att ( $ Q $ ; $ K $ ; $ V $ ) should also have $ k \\times d $ dimension , which means the output is already reduced from $ n $ vectors to $ k $ vectors . Why do we still need the GMPool operation ? * * Answer : * * We require the GMPool operation , since nodes are reduced from $ n $ nodes to $ k $ nodes through GMPool , not by the conventional attention function which learns an $ n $ -to- $ n $ mapping . The reason why we construct several attention functions such as Att ( Attention ) , MH ( Multi-head Attention in equation 5 ) , and GMH ( Graph Multi-head Attention in equation 6 ) in the Graph Multi-head Attention paragraph is that these operations are used for both GMPool and SelfAtt . In other words , these attention components are not only used for compressing nodes with GMPool but are also used for capturing global node relationships with SelfAtt . Thus , the Att function with $ k \\times d $ dimension is a specific design choice for compressing $ n $ nodes into $ k $ typical nodes , and GMPool makes this possible with the learnable matrix $ S $ ."}, "2": {"review_id": "JHcqXGaqiGn-2", "review_text": "The work extends the set transformer to obtain a method for multi-head attention pooling on multisets with connectivity ( graphs ) . The authors show that the approach is as expressive as the WL isomorphism test and has better space complexity than existing node clustering networks . The method achieves state-of-the-art results on graph classification and strong results on graph reconstruction and generation . Strengths : - The paper is very well written and polished . - The figures complement the text well . - The work is technically and mathematically sound . - The method shows good results and is scalable , making it a valuable addition to the set of existing GNN operators . - The authors make an effort to substantiate their statements about expressivity and scalability with proofs . - The experiments are well chosen and show where improvements come from . Weaknesses : - The proven expressiveness is not a very strong statement , since most pooling approaches adhere to this property . It is nice to have the theoretical analysis though . - The method itself is an incremental variation of set transformers ( Lee et al . ) , although adapted for a different type of input data . - Using the identity matrix as adjacency ( as described in Appendix B , to work around the scalability issue of node clustering methods ) seems to make the approach identical to the set transformer in all layers except the first , which dampens the contribution . There is some potential for improvement in clarity of presentation : - In abstract : `` may yield representations that do not pass the graph isomorphism test '' , in introduction , page 2 : `` accurate representations of given graphs that can pass the WL-test '' . Those sentences are confusing as it is not about representations passing the WL test , is it ? It is about two graphs which are distinguished by WL get different representations ( as correctly stated elsewhere in the work ) . - Regarding page 4 , paragraph `` Graph Multi-head Attention '' and the following ones : - On the one hand , they are extremely close to Vaswani et al.and Lee et al . ( sometimes even nearly the same sentences ) . - On the other hand , some things are left out , which are crucial for understanding , such as definitions of symbols for dimensionalities ( n_q , d_k , d_v , d_model ) and the origin of some matrices , see next point . - Where do the seeds S come from ( for the non self attention operator ) ? It probably is a parameter matrix that is directly optimized but it is not completely clear ( `` learnable '' is ambiguous , can also be the output of a network ) - Suggestion : maybe the description of the pooling method becomes more clear when described in a top-down manner : Eq 7 - > Eq 6 - > Eq 5 - > Att Experiments : - Variances of graph classification results over the cross validation would be greatly appreciated ( since there seems to be a space issue , they can go into the appendix ) - The reconstruction architecture does not reconstruct adjacency . It might be interesting to see how well the method can do that ( for the synthetic graphs for example ) Related work : - The work [ 1 ] should be mentioned and discussed in related work and compared against in experiments . It is also a pooling method with attention but seems to follow a different approach . Typos : - Figure 2 caption : `` to compress the all nodes '' -all - Proof of Theorem 4 : `` but it is highly efficient than node clustering methods '' - Proof of Proposition 5 : `` then the first them inherently generates '' All in all , I think this paper has a valuable contribution , even if the method is incremental . Therefore , I tend to vote for accepting the paper but encourage the authors to improve on the mentioned issues in experiments , related work and presentation . [ 1 ] Ranjan et al . : ASAP : Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations , AAAI 2020", "rating": "7: Good paper, accept", "reply_text": "We sincerely appreciate your constructive and helpful comments . We initially address all your comments below : * * Question 1 : * * In abstract : `` may yield representations that do not pass the graph isomorphism test '' , in introduction , page 2 : `` accurate representations of given graphs that can pass the WL-test '' . Those sentences are confusing as it is not about representations passing the WL test . It is about two graphs which are distinguished by WL get different representations ( as correctly stated elsewhere in the work ) . * * Answer : * * We apologize for the confusion , and thank you for pointing out the inaccurate wording . The two sentences in the abstract and introduction mean that the representation power for the proposed method is as powerful as the WL test , such that the proposed pooling method represents the two distinct graphs differently , while the baseline may yield the same representation for two different graphs distinguished by the WL test . We have corrected them in the revision . * * Question : 2 * * Some things are left out in the Graph Multi-head Attention paragraph , such as definitions of symbols for dimensionalities ( n_q , d_k , d_v , d_model ) . Also , where do the seeds S come from ? * * Answer : * * We apologize for the confusion , and thank you for pointing them out . We carefully revised the description of the Graph Multi-head Attention for improved clarity . The seed matrix S is a parameter matrix that is directly optimized , and not an output of a network . * * Question 3 : * * Variances of graph classification results over the cross validation would be greatly appreciated . * * Answer : * * We have reported the standard deviation on graph classification results with a p-value for the significance test in Table 1 ( graph classification ) , and also visualized the variance in Figure 6 ( graph reconstruction ) of the revision , as suggested . * * Question 4 : * * The reconstruction architecture does not reconstruct adjacency . It might be interesting to see how well the method can do that . * * Answer : * * Thanks for your suggestion , and we have * * included and discussed the results on reconstructing the adjacency matrix * * from the compressed representation in Appendix D , Adjacency Reconstruction for Graph Reconstruction paragraph of the revision . Please note that reconstruction for the adjacency matrix is more difficult than reconstruction for nodes and thus are not covered in any of the existing works to our knowledge , since , while the reconstructed node feature can be represented as continuous values , the reconstructed adjacency matrix should be further transformed from continuous values to discrete values ( 0 or 1 for the undirected simple graph ) . We thus leave the reconstruction of the adjacency matrices as future work . * * Question 5 : * * The work [ 1 ] should be mentioned and discussed in related work and compared against in experiments . It is also a pooling method with attention but seems to follow a different approach . * * Answer : * * ASAP [ 1 ] is clearly different from the proposed GMPool . ASAP first computes the local cluster assignment using the message-passing operation , and then drops the unimportant local clusters with their scores . However , the proposed GMPool globally compresses the $ n $ nodes into $ k $ nodes with their relevance score for the seed matrix S , and does not drop any nodes , and thus does not suffer from any information loss . We have compared against ASAP , and you can see its result in the response to the common comments and Table 1 . * * Question 6 : * * Typos . * * Answer : * * Thanks for pointing them out . We have corrected them in the revision . [ 1 ] Ranjan et al . : ASAP : Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations , AAAI 2020"}, "3": {"review_id": "JHcqXGaqiGn-3", "review_text": "This work proposes a Graph Multiset Transformer ( GMT ) that uses a multi-head attention-based approach to capture potential interactions between nodes when pooling nodes to produce a graph representation . Multi-head attention mechanism is used to group nodes into clusters , each of which produces a representation . Self-attention is then used to pool representations of clusters into the representation of a graph . Pros : The proposed pooling is more reasonable than a simple sum or average pooling as the multi-head attention mechanism can potentially capture dependencies between nodes . Cons : Several parts of the manuscript need more explanations . Additional experimental results ( see below for details ) are needed . ( 1 ) It is not clear what multi-head attention achieve semantically ( as claimed : better capture structure information ) . ( 2 ) Figures are neither self-explained nor well explained in the main text . ( 3 ) The std of cross validation in each experiment should be reported . The mean performance alone is not enough to say a method performs better or not . It will be better to provide a p-value ( e.g. , t-test ) to show if a method is statistically significantly better . ( 4 ) Four datasets from the Open Graph Benchmark were used . The authors should refer to the leaderboard for the performance of some baseline methods . For example , the leaderboard of HIV dataset reports that GIN has 0.7654 rather than 0.7595 listed in this manuscript . ( 5 ) The abstract points out that without considering task relevance is a weakness of the previous pooling method . However , it is not clear how the proposed approach make improvement ( s ) on this . ( 6 ) The proposed method does not necessary pass graph isomorphism as the nodes in the manuscript have attributes but the proof does not consider node attributes . ( 7 ) In appendix , examples in Figure 10 are confusing . More explanations are needed .", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * Question 4 : * * Four datasets from the Open Graph Benchmark were used . The authors should refer to the leaderboard for the performance of some baseline methods . * * Answer : * * We reproduced all baselines for a * * fair comparison of all pooling models * * and not the performance of message-passing layers . We ran all experiments for all baselines and our models in the * * same setting * * as described in appendix C.2 . Specifically , we average the results over 10 different runs with the same hidden dimension ( 128 , and leaderboard uses 300 ) , and the same number of message-passing layers ( 3 , and leaderboard uses 5 ) with 10 different seeds for all models , which is clearly described in the appendix C.2 implementation details . Therefore , the results can be slightly different from the leaderboard results ( Please see the table below ) , since the leaderboard uses different hyperparameters with different random seeds compared to the experimented pooling models . Note that our reproduction sometimes outperforms the leaderboard results ( See the GCN results on HIV ) . However , as suggested , we have included the leaderboard results in the revision ( See Appendix D ) . | | HIV | Tox21 | | | | | | GCN | 76.81 | 75.04 | | GCN ( Leaderboard ) | 76.06 | 75.29 | | GIN | 75.95 | 73.27 | | GIN ( Leaderboard ) | 75.58 | 74.91 | | GMT ( Ours ) | * * 77.56 * * | * * 77.30 * * | * * Question 5-1 : * * The abstract points out that without considering task relevance is a weakness of the previous pooling method . * * Answer : * * This is a critical misunderstanding . We clearly state in the abstract that inconsideration of task relevance is a weakness of the simple sum/average function , not the previous learnable parametric pooling methods that are optimized in an end-to-end fashion . We stated this point in the introduction section as follows : * As a simplest approach for graph pooling , we can average or sum all node features in the given graph . However , since such simple aggregation schemes treat all nodes equally without considering their relative importance on the given tasks , they can not generate a meaningful graph representation in a task-specific manner . Furthermore , we stated this point in the related work section as follows : * While averaging all node features is directly used as simplest pooling methods , simplest pooling methods result in a loss of information since they consider all node information equally without considering key features for graphs . Therefore , we did not claim that previous pooling methods , except the flat pooling method such as average/sum , could not consider the task relevance . * * Question 5-2 : * * It is not clear how the proposed approach makes improvement ( s ) on this sum/average pooling . * * Answer : * * Please note that all existing parameterized pooling operations that are learnable in an end-to-end fashion , including ours , can treat nodes differently according to their task relevance , rather than considering all nodes equally as in simple sum/average pooling . This is done by the trained networks allocating more weights on the important nodes for the specific task . Figure 7 shows one specific example that different nodes have different cluster weights for the graph reconstruction task . * * Question 6 : * * The proposed method does not necessarily pass graph isomorphism as the nodes in the manuscript have attributes but the proof does not consider node attributes . * * Answer : * * We want to clarify that the proposed method can be at most * * as powerful as the WL test * * for the graph isomorphism test , rather than directly passing the graph isomorphism test . To clarify this point , we have * * revised the text to tone down * * on the claim , as suggested . Note that the proposed Graph Multiset Pooling can uniquely map each multiset $ H $ with a bounded size , such that whether a node has attributes or not does not matter for establishing the proposed pooling function that is as powerful as the WL test . Specifically , there exists a mapping function $ f $ that maps nodes to prime numbers , and the summation over nodes in a multiset constitutes a unique mapping with a function $ f $ , as shown in our proof of Lemma 2 . * * Question 7 : * * In Appendix , examples in Figure 10 ( Figure 11 in the revision ) are confusing . More explanations are needed . * * Answer : * * Thanks for the suggestion . We provide more explanations about the reconstruction examples in Figure 10 ( Figure 11 in the revision ) , including the detailed descriptions of components embedded , and the generated cluster assignments for each node or a set of nodes ."}}