{"year": "2021", "forum": "tbwjUvUzQRU", "title": "A Communication Efficient Federated Kernel $k$-Means", "decision": "Reject", "meta_review": "This paper presents a approach to the distributed kernel k-means problem using a combination of random features to efficiently approximate the kernel matrix, a distributed stochastic proximal gradient algorithm which calls a distributed lanczos algorithm as a primitive to find a low-rank approximation to the kernel matrix, and additional compression to reduce the cost of the communications. \n\nThe algorithm is a novel combination of prior ideas, and empirically works well. However, the claimed theoretical convergence rate is not convincing: e.g., the convergence rate depends on the Frobenius norm of the error in approximating the kernel with random feature maps, which is O(n^2) for a problem with n data samples. This implies that O(n^2) iterations must be used in the algorithm, which is already slower than a naive approach to kernel k-means.\n\nThis paper takes a promising approach to the problem, but as the potential contribution lies in combining prior ideas in order to obtain a provably guaranteed approximate solution to the distributed kernel k-means problem, and the proposed algorithm was not shown to satisfy this promise, the recommendation is to reject.", "reviews": [{"review_id": "tbwjUvUzQRU-0", "review_text": "Summary : This paper proposes a federated kernel k-means algorithm ( FK k-means ) . The algorithm consists of two parts : a distributed stochastic proximal gradient descent ( DSPGD ) update rule , and a communication efficient mechanism ( CEM ) to reduce the communication cost . Instead of solving the original integer programming problem , the authors consider the relaxed convex stochastic composite optimization ( SCO ) problem and extends it to a distributed setting . The main contribution is that the authors show , both theoretically and experimentally , that their proposed algorithm converges to the true solution of the SCO problem at O ( 1/T ) rate . It is also proved that the communication cost does not grow with the number of samples N. In addition the authors characterize the error ratio between their algorithm and the original k-means . At the end the authors prove that the central server can not recover the feature matrices . The authors compare their algorithm with other k-means algorithms on several datasets . Pros : - The federated setting of k-means with privacy preservation property is interesting and relatively new . - The presentation of the theorems is very clear . The authors put interpretation after each statement and it is easy to follow most of the time . In particular it is shown that the proposed algorithm approaches the baseline scalable kernel k-means algorithm as T increases , both theoretically and experimentally . - I appreciate the discussion of the motivation and related works in the first two section of the paper . Cons : - The whole Section 4 of the paper is filled with technical details and hard to follow . Considering the page limit , why not put the algorithms here , and put the details in the appendix ? In addition the authors can consider putting important observations as lemmas . - It seems the proof techniques are somehow standard . The whole proof of Theorem 1 feels like an extension to [ Zhang et al. , 2016 ] in a distributed setting . I did not check the other proofs though . - My biggest concern is about novelty . Currently the proposed algorithm is heavily influenced by [ Zhang et al. , 2016 ] and [ Wang et al. , 2019 ] . If that is not the case , the authors can consider including a table to illustrate the difference between the algorithms . Post-rebuttal : I appreciate the authors ' feedbacks . However the authors ' response to the proof of Theorem 1 is not the most convincing , which is a big part of the claimed contribution .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the positive comments and constructive suggestions on our paper . 1.The whole Section 4 of the paper is filled with technical details and hard to follow . Considering the page limit , why not put the algorithms here , and put the details in the appendix ? We have moved the pseudo code of the algorithm back to Section 4.2 , and moved some technical details to the appendix . 2.It seems the proof techniques are somehow standard . The whole proof of Theorem 1 feels like an extension to [ Zhang et al. , 2016 ] in a distributed setting . The difference between the proof in [ Zhang et al. , 2016 ] and our proof is as follows . In [ Zhang et al. , 2016 ] , the $ ( t+1 ) $ -th solution $ \\mathrm { Z } _ { t+1 } ^ * $ determined by the stochastic proximal gradient descent ( SPGD ) is the exact solution to the following problem $ $ \\min_ { \\mathrm { Z } \\in \\mathbb { R } ^ { n \\times n } } { \\frac { 1 } { 2 } ||\\mathrm { Z } - \\mathrm { Z } _ { t } ||_F^2 + \\eta_t \\langle \\mathrm { Z } } -\\mathrm { Z } _t , \\mathrm { Z } _t - \\mathrm { \\xi } _t \\rangle + \\eta_t \\lambda ||\\mathrm { Z } || * $ $ However , in FK $ k $ -means , to reduce the communication cost , the $ ( t+1 ) $ -th solution $ \\mathrm { Z } _ { t+1 } $ determined by the distributed stochastic proximal gradient descent ( DSPGD ) is an approximate solution . Since it is unknown how these approximate solutions affect the convergence of DSPGD , we need to prove that the approximate solutions do not affect the convergence of DSPGD . For the details of the proof , please refer to the proof of Theorem~1 ( from the equation ( 6 ) to equation ( 15 ) ) . 3.My biggest concern is about novelty . Currently the proposed algorithm is heavily influenced by [ Zhang et al. , 2016 ] and [ Wang et al. , 2019 ] . If that is not the case , the authors can consider including a table to illustrate the difference between the algorithms . The novelties of FK $ k $ -means are as follows . First , we design DSPGD to approach the top- $ s $ eigenpairs of the kernel matrix $ \\mathrm { K } $ under federated settings where raw data are maintained by users and the cloud can not access the raw data . Second , a communication efficient mechanism ( CEM ) is designed to highly reduce the communication cost of DSPGD . Among these two novelties , DSPGD is a counterpart of the stochastic proximal gradient descent ( SPGD ) algorithm in stochastic PCA [ Zhang et al. , 2016 ] . However , DSPGD is distinct from SPGD in the following three features . DSPGD is conducted under federated settings . However , SPGD is conducted in a centralized manner where users ' raw data are collected at the cloud server . The communication cost is considered in the design of DSPGD , which results in CEM , while the communication cost is not considered in SPGD . Although both DSPGD and SPGD aim at approaching the top eigenpairs of $ \\mathrm { K } $ , in the t-th iteration , DSPGD only needs to obtain an approximate solution $ \\mathrm { Z } \\_ { t+1 } $ instead of the exact solution $ \\mathrm { Z } _ { t+1 } ^ * $ to the same problem like SPGD , which leads to less communication cost under federated settings . oreover , FK $ k $ -means is distinct from the scalable kernel $ k $ -means [ Wang et al. , 2019 ] ( denoted as SK $ k $ -means ) in the following two features . FK $ k $ -means approaches the top eigenvectors without collecting users ' raw data or users ' random features at the cloud server . However , SK $ k $ -means requires users to upload some of their raw data or random features to the cloud server to approximate the kernel matrix $ \\mathrm { K } $ . FK $ k $ -means is an iterative algorithm , and it can approach the top eigenvectors of $ \\mathrm { K } $ more accurately by employing more iterations . In contrast to FK $ k $ -means , SK $ k $ -means is a one-shot algorithm , i.e. , it only determines an approximate kernel matrix $ \\tilde { \\mathrm { K } } $ once and then applies SVD to $ \\tilde { \\mathrm { K } } $ to approach the top eigenvectors of $ \\mathrm { K } $ . Thus , the accuracy of these top eigenvectors is limited by the number of data samples or random features uploaded by users . Reference [ Zhang et al. , 2016 ] Lijun Zhang , Tianbao Yang , Jinfeng Yi , Rong Jin , and Zhi-Hua Zhou . Stochastic optimization for kernel PCA . In Proceedings of the 30th AAAI Conference on Artificial Intelligence , pp . 2316\u2013 2322 , 2016 . [ Wang et al. , 2019 ] Shusen Wang , Alex Gittens , and Michael W Mahoney . Scalable kernel k-means clustering with Nystr { \\ '' o } m approximation : relative-error bounds . The Journal of Machine Learning Research , 20 ( 1 ) :431\u2013479 , 2019"}, {"review_id": "tbwjUvUzQRU-1", "review_text": "The paper proposed a new distributed kernel k-means algorithm that has lower communication overhead compared to the available methods while does not require transmitting the data samples from agents to the master node and therefore claims to preserve the privacy of agents . The paper is rather difficult to follow and the novelty of the paper is not very clear to me . It seems to me that the major contribution of the paper is to come up with efficient tricks during the implementation of the distributed Lanczos algorithm ( DLA ) to find the eigenvalues of the kernel approximated by using the well-known random Fourier features ( Rahimi , nips 2008 ) . So the end result is not something utterly novel but rather an efficient way of utilizing available tools to design a new distributed algorithm . There are several points that I think need to be clarified : 1 . The literature review does not clearly illustrate the contribution of the paper . For instance , in Section 2.1 , the author says `` However , these algorithms are designed with an assumption that they are executed at the cloud server where the constraint on transmissions of the raw data samples does not exist . '' Does this mean that your work is the first to constrain the transmission of data samples ? If so you should clearly say so . Besides , why transmitting the raw data sample is important here ? Is it due to privacy issues ? Because the way I see it , one does not need to transmit the raw data samples but rather its local kernel matrix K , and doing so does not necessarily endanger the privacy of an agent , since one can not easily recover data samples from K. 2 . In Section 6.2 , the author says `` According to the results in the four subfigures , it is shown that CEM can reduce communication cost of DSPGD by more than 95 % . '' It would be interesting to explain what values of r_t , Q_0 , and Q_1 leads to this improvement and why . 3.The 60 % communication cost reduction mentioned throughout the paper seems a bit exaggerated as it does not consistently happen . It only happens for the MNIST dataset in Figure 3 ( b ) . Do you know why ? My guess is that due to the high sparsity level of MNIST samples , the kernel matrix might contain a lot of zeros leading to such behavior . 4.What about the drawbacks of the proposed method ? What I see is a very complicated algorithm that requires heavy computational resources at each agent , which makes it unsuitable for the toy example explained in the introduction about smartphones . After rebuttal : I thank the author for their response but I have to lower my rating by one step after reading the comments of Reviewer2 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the constructive suggestions on our paper . Summary : The paper is rather difficult to follow and the novelty of the paper is not very clear to me . It seems to me that the major contribution of the paper is to come up with efficient tricks during the implementation of the distributed Lanczos algorithm ( DLA ) to find the eigenvalues of the kernel approximated by using the well-known random Fourier features ( Rahimi , nips 2008 ) . In Section 4.1 , we first point out that the key problem for designing FK $ k $ -means is to obtain the top eigenpair of the kernel matrix $ \\mathrm { K } $ in a distributed manner . To solve this problem , we design a distributed stochastic proximal gradient descent ( DSPGD ) algorithm . We then present the challenges on solving the key problem and how we resolve these challenges . Since $ \\mathrm { K } $ is not available under federated settings . To this end , an estimate of $ \\mathrm { K } $ , denoted as $ \\mathrm { \\xi } $ , is constructed jointly at users ' devices based on random features [ 1 ] of local data samples . Since $ \\mathrm { \\xi } $ is distributed among different devices , it is processed by the distributed Lanczos algorithm ( DLA ) [ 3 ] to obtain an estimate of $ \\mathrm { K } $ , i.e. , $ \\mathrm { Z } $ , at the cloud server . Afterwards , an approximate version of the top eigenpairs of $ \\mathrm { K } $ can be obtained from $ \\mathrm { Z } $ through singular value decomposition ( SVD ) . To improve the accuracy of approximation , $ \\mathrm { Z } $ is iteratively updated by DSPGD . More specifically , in the $ t $ -th iteration , an estimate $ \\mathrm { \\xi } \\_ { t } $ is constructed at users ' devices , and then the estimate $ \\mathrm { Z } \\_ { t } $ at the cloud server is updated to $ \\mathrm { Z } \\_ { t+1 } $ by applying DLA to a weighted sum of $ \\mathrm { Z } \\_ { t } $ and $ \\mathrm { \\xi } \\_ { t } $ . In Section 4.2 , we first point out that the process of obtaining an updated $ \\mathrm { Z } _ { t } $ results in high communication cost , because DLA is applied to matrices with the number of rows/columns equals the number of data samples . To this end , we design a communication efficient mechanism ( CEM ) so that DLA is applied to a new type of matrices . We then present the design of the new type of matrices and the procedure of obtaining the top eigenpairs of $ \\mathrm { K } $ in CEM . The major novelty of our work is that we design a new distributed stochastic proximal gradient descent ( DSPGD ) algorithm to approximate the top- $ s $ eigenpairs of the kernel matrix $ \\mathrm { K } $ under the federated settings where raw data are maintained by users and the cloud can not access the raw data . Besides , we design a communication efficient mechanism ( CEM ) that can highly reduce the communication cost of DSPGD . In addition , we derive the convergence rate of DSPGD , analyze the communication cost of DSPGD before and after employing CEM , and analyze the clustering quality of FK $ k $ -means . 1.The literature review does not clearly illustrate the contribution of the paper . For instance , in Section 2.1 , the author says `` However , these algorithms are designed with an assumption that they are executed at the cloud server where the constraint on transmissions of the raw data samples does not exist . '' Does this mean that your work is the first to constrain the transmission of data samples ? If so you should clearly say so . We have revised the literature review and clearly pointed out the differences between our work and the existing schemes . In Section 2.1 , the clause `` where the constraint on transmissions of the raw data samples does not exist '' is indeed confusing . We actually want to express that the distributed kernel $ k $ -means schemes mentioned in Section 2 are designed for the cloud server where users ' raw data are collected . In contrast to these distributed kernel $ k $ -means schemes , FK $ k $ -means is the first scheme to conduct the kernel $ k $ -means clustering without uploading users ' raw data to the cloud server . Besides , FK $ k $ -means also considers the communication efficiency during the clustering task ."}, {"review_id": "tbwjUvUzQRU-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper proposes a new method and two algorithms for solving kernel k-means . The contribution is that the algorithms converge to the optimal solution . The downsides are 1 ) the method is not better than a simple baseline ( i.e. , random features + distributed power method ) and 2 ) the main theorem ( Theorem 3 ) is wrong . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : I vote for rejection for two reasons . First , the proposed method appears not useful . The same problem can be solved in a much simpler and faster way . Second , the main theorem , Theorem 3 , is wrong . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : + This paper develops a new method of distributed kernel k-means . The method is new , although I do not find it very useful . + This paper proves that two algorithms can correctly solve the trace-norm regularized problem in Section 4.1 . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . First and foremost , I do not see a good reason for using the proposed algorithm . The goal of the algorithm is to find the top singular vectors of the random features , $ A $ . - The solution to the trace norm regularized problem , $ Z^ * $ , has the same singular vectors as K. By finding $ Z^ * $ , you can find the eigenvectors of $ K $ . - However , the same goal could be achieved in an easier and less expensive way , i.e. , random features + distributed power method . Random features are naturally distributed among the clients . Their truncated SVD could be found by the distributed power method or Krylov subspace methods . Truncated SVD is easier than solved the proposed trace norm regularized problem because the latter uses SVS which repeatedly performs SVD . 2.I am very surprised that Theorem 3 does not reply on $ D $ ( the number of random features ) . So I checked some of the proofs . I found Theorem 3 , which is the main theorem , is wrong . - $ Z^ * $ has the same top eigenvectors as $ \\xi $ . But $ Z^ * $ may not have the same as $ K $ . - The proof of Theorem 3 relies on that $ Z^ * $ has the same eigenvectors as $ K $ . This is wrong . 3.The description of the algorithm is difficult to follow . I \u2019 d suggest splitting algorithm description into 3 paragraphs : 1 ) Client-side computation , 2 ) server-side computation , and 3 ) communications . Typos : 17th page : `` The following two lemmas will be used in the proof of Theorem 4. \u201d Do you mean Theorem 3 ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Updates after discussing with the authors 1 . The paper is not very clearly written , and I had misunderstandings . Some of my comments above are not right . 2.However , I will not change my rating . I found the convergence rates stated in the paper are misleading . The paper claims $ O ( 1/T ) $ convergence rate . In fact , this is WRONG . The authors assume the Frobenius and trace norms of $ n\\times n $ matrices are CONSTANTS . This is not possible . The norms are $ O ( n ) $ . Simple arguments can show $ || \\xi ||_F = G $ is $ O ( n ) $ . 3.Based on the right assumption that $ || \\xi ||_F = G = O ( n ) $ , the required number of iterations is $ T = O ( n^2 ) $ . The algorithm is not communication-efficient . It is more expensive than communicating the $ n\\times n $ kernel matrices . 4.After reading my comments , the authors changed their notation from $ G $ to $ \\gamma $ , $ C $ , $ G $ , and $ H $ . They are also Frobenius and trace norms of $ n\\times n $ matrices . The authors assume $ \\gamma $ , $ C $ , $ G $ and $ H $ are constants . This is WRONG . They are $ O ( n ) $ . - For example , if they use the bound of Rahimi and Recht , then $ || \\xi - K ||_F^2 = G^2 $ is $ O ( n^2 ) $ . A bound as good as $ || \\xi - K ||_F^2 = O ( n ) $ would surprise me ; if the authors know such a bound , please let me know . - Let me strengthen my point again : IT IS WRONG TO ASSUME MATRIX NORMS ARE CONSTANTS ! If the authors can prove they are constants , they need to show me the proofs . If they can not , they should assume Frobenius norm and trace norm are $ O ( n ) $ .", "rating": "1: Trivial or wrong", "reply_text": "We thank the reviewer for the constructive suggestions on our paper . Summary : This paper proposes a new method and two algorithms for solving kernel k-means . The contribution is that the algorithms converge to the optimal solution . The downsides are 1 ) the method is not better than a simple baseline ( i.e. , random features + distributed power method ) and 2 ) the main theorem ( Theorem 3 ) is wrong . The contribution of our work is as follows . We design a new distributed stochastic proximal gradient descent ( DSPGD ) algorithm to approximate the top- $ s $ eigenpairs of the kernel matrix $ \\mathrm { K } $ under federated settings where raw data are maintained by users and the cloud can not access the raw data . Besides , we design a communication efficient mechanism ( CEM ) that can highly reduce the communication cost of DSPGD . In addition , we theoretically analyze the performance of FK $ k $ -means and its components in three aspects : the convergence rate of DSPGD ; the communication cost of DSPGD before and after employing CEM ; the clustering quality of FK $ k $ -means . We would like to point out that our method ( i.e. , distributed stochastic proximal gradient descent ( DSPGD ) ) is actually better than the baseline ( i.e. , random features + distributed power method ) . The baseline is actually a special case of the DSPGD . If DSPGD adopts the same value of $ D $ as that in the baseline and is only executed for one iteration , then DSPGD is reduced to the baseline . For the comparison between DSPGD and the baseline , please refer to the response to comment 2 . Besides , we have checked the proof of Theorem 3 and confirmed that Theorem 3 is correct . For the detailed clarification , please refer to the responses to comment 3 and comment 4 . 1.The goal of the algorithm is to find the top singular vectors of the random features $ \\mathrm { A } $ . However , the same goal could be achieved in an easier and less expensive way , i.e. , random features + distributed power method . Random features are naturally distributed among the clients . Their truncated SVD could be found by the distributed power method or Krylov subspace methods . Truncated SVD is easier than solved the proposed trace norm regularized problem because the latter uses SVS which repeatedly performs SVD . The goal of the distributed stochastic proximal gradient descent ( DSPGD ) is actually to approach the top eigenpairs of the kernel matrix $ \\mathrm { K } $ , not the the top singular vectors and the corresponding singular values of the random features $ \\mathrm { A } $ . In FK $ k $ -means , DSPGD is designed to solve a stochastic composite optimization ( SCO ) problem in a distributed manner : $ $ \\min_ { \\mathrm { Z } \\in \\mathbb { R } ^ { n \\times n } } { \\frac { 1 } { 2 } \\mathbb { E } [ ||\\mathrm { Z } - \\mathrm { \\xi } ||_F^2 ] +\\lambda ||\\mathrm { Z } || * } . $ $ The optimal solution $ \\mathrm { Z } ^ * $ to SCO problem is $ $ \\mathrm { Z } ^ * = \\sum_ { i : \\lambda_i > \\lambda } { ( \\lambda_i - \\lambda ) \\mathrm { u } _i \\mathrm { u } _i^\\top } , $ $ where $ { \\mathrm { u } } _i $ and $ \\lambda_i $ are the $ i $ -th eigenvector and the $ i $ -th eigenvalue of $ \\mathrm { K } $ , respectively . Assume that the rank of $ \\mathrm { Z } ^ * $ equals $ s $ . By applying SVD to $ \\mathrm { Z } ^ * $ , its eigenvectors $ \\ { { \\mathrm { u } } _i , i=1 , ... , s\\ } $ of $ \\mathrm { K } $ and eigenvalues $ \\ { \\lambda_i - \\lambda , i=1 , ... , s\\ } $ are determined . The eigenvectors of $ \\mathrm { Z } ^ * $ are exactly the top- $ s $ eigenvectors of $ \\mathrm { K } $ . Moreover , by adding $ \\lambda $ to the eigenvalues of $ \\mathrm { Z } ^ * $ , the top- $ s $ eigenvalues of $ \\mathrm { K } $ are obtained . Thus , the top eigenpairs of $ \\mathrm { K } $ are approached by DSPGD . The baseline can indeed be used to approximate the top eigenvectors of $ \\mathrm { K } $ . However , the number of random features $ D $ should be set to a very large value in order to obtain an accurate estimate of $ \\mathrm { K } $ according to [ 1 ] . This method is feasible only if users ' devices have enough memory to maintain these random feature vectors . Compared with the baseline , DSPGD is a generic method that provides a tradeoff between memory and computational time : the larger is $ D $ , the fewer iterations are required ( an extreme case is that only one iteration is required ) to approach the top eigenvectors of $ \\mathrm { K } $ . Thus , DSPGD can be adapted to devices with different memory space ."}, {"review_id": "tbwjUvUzQRU-3", "review_text": "Summary : This paper proposes a distributed version of kernel k-means clustering where some federated structure is used to do distributed processing on the data . Privacy and communication issues are also studied . Numerical results are provided . Reasons for the score : This paper seems provide a new algorithm for distributed clustering . However , the way the algorithm is presented look like a patch of a number of things coming together one after the other with no general structure . This might be caused by the fact that the algorithm is only presented in the appendix . The paper is written in a convoluted manner . This is the main limitation , at some point , we are talking about k means , SVS , DLA , DSPGD , EVD , SPGD , a bunch of other methods that are coupled together towards the main approach . Problem 1 seems to be an integer programming problem , thus with very high computation complexity . It is not clear how this is solved . In the abstract please let me know what are those two levels of privacy you are talking about . In the abstract , what does it mean that the clustering loss of the distributed method approaches the centralized one , please elaborate . Why developing a federated learning algorithm is a promising approach ? Please elaborate . The second part of the intro turns into a detail technical analysis of the algorithm components , and so far we haven \u2019 t seen the algorithm so it all remains a technical abstract discussion that takes away the main messages . The algorithm is in the appendix , so the description and analysis is made on an item that has not been presented in the main text . The way the result is presented makes it look like the proposed method is a concatenation of other results , rather than the solution of a technical challenge in the problem . Numerical results are well presented ,", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the constructive suggestions on our paper . 1.This paper seems provide a new algorithm for distributed clustering . However , the way the algorithm is presented look like a patch of a number of things coming together one after the other with no general structure . This might be caused by the fact that the algorithm is only presented in the appendix . To improve the presentation of the algorithm , we have revised the paper as follows . We have added an overview of federated kernel $ k $ -means ( denoted as FK $ k $ -means ) at the beginning of Section 4 . In Section 4.1 , we first point out that the key problem for designing FK $ k $ -means is to obtain the top eigenpair of the kernel matrix $ \\mathrm { K } $ in a distributed manner . To solve this problem , we design a distributed stochastic proximal gradient descent ( DSPGD ) algorithm . We then present two challenges on solving the key problem and how we resolve these challenges . The first challenge is that $ \\mathrm { K } $ is not available under federated settings . To this end , an estimate of $ \\mathrm { K } $ , denoted as $ \\mathrm { \\xi } $ , is jointly constructed at users ' devices based on random features [ 1 ] of local data samples . Based on $ \\mathrm { \\xi } $ , an estimate of $ \\mathrm { K } $ , denoted as $ \\mathrm { Z } $ can be obtained at the cloud server . Afterwards , an approximate version of the top eigenpairs of $ \\mathrm { K } $ can be obtained from $ \\mathrm { Z } $ through singular value decomposition ( SVD ) . The second challenge is how to improve the accuracy of approximation . To this end , $ \\mathrm { Z } $ is iteratively updated by DSPGD . More specifically , in the $ t $ -th iteration , an estimate $ \\mathrm { \\xi } \\_ { t } $ is constructed at users ' devices , and then the estimate $ \\mathrm { Z } \\_ { t } $ at the cloud server is updated to $ \\mathrm { Z } \\_ { t+1 } $ by applying DLA to a weighted sum of $ \\mathrm { Z } \\_ { t } $ and $ \\mathrm { \\xi } \\_ { t } $ . In Section 4.2 , we first point out that the process of obtaining an updated $ \\mathrm { Z } _ { t } $ results in high communication cost , because DLA is applied to matrices with the number of rows/columns equal to the number of data samples . To this end , we design a communication efficient mechanism ( CEM ) so that DLA is applied to a new type of matrices with reduced dimensions . We then present the design of the new type of matrices and the procedure of obtaining the top eigenpairs of $ \\mathrm { K } $ in CEM . In addition , to improve the clarity of the algorithm , we have moved the pseudo code of the algorithm back to the end of Section 4.2 . 2.The paper is written in a convoluted manner . This is the main limitation , at some point , we are talking about $ k $ -means , SVS , DLA , DSPGD , EVD , SPGD , a bunch of other methods that are coupled together towards the main approach . We have revised the introduction and also Section 4.1 to clarify the relationship between our method with the bunch of other methods . For the detailed revision of the introduction , please refer to the response to comment 7. we first point out that the key problem for designing FK $ k $ -means is to obtain the top eigenpair of the kernel matrix $ \\mathrm { K } $ in a distributed manner . To solve this problem , we design a distributed stochastic proximal gradient descent ( DSPGD ) algorithm . We then present two challenges on solving the key problem and how we resolve these challenges . The first challenge is that $ \\mathrm { K } $ is not available under federated settings . To this end , an estimate of $ \\mathrm { K } $ , denoted as $ \\mathrm { \\xi } $ , is jointly constructed at users ' devices based on random features [ 1 ] of local data samples . Based on $ \\mathrm { \\xi } $ , an estimate of $ \\mathrm { K } $ , denoted as $ \\mathrm { Z } $ can be obtained at the cloud server . Afterwards , an approximate version of the top eigenpairs of $ \\mathrm { K } $ can be obtained from $ \\mathrm { Z } $ through singular value decomposition ( SVD ) . The second challenge is how to improve the accuracy of approximation . To this end , $ \\mathrm { Z } $ is iteratively updated by DSPGD . More specifically , in the $ t $ -th iteration , an estimate $ \\mathrm { \\xi } \\_ { t } $ is constructed at users ' devices , and then the estimate $ \\mathrm { Z } \\_ { t } $ at the cloud server is updated to $ \\mathrm { Z } \\_ { t+1 } $ by applying DLA to a weighted sum of $ \\mathrm { Z } \\_ { t } $ and $ \\mathrm { \\xi } \\_ { t } $ . After the revision , the relationship between FK $ k $ -means and other methods is clear , and it is easier to see the novelty and contribution of our work from the revised algorithm description ."}], "0": {"review_id": "tbwjUvUzQRU-0", "review_text": "Summary : This paper proposes a federated kernel k-means algorithm ( FK k-means ) . The algorithm consists of two parts : a distributed stochastic proximal gradient descent ( DSPGD ) update rule , and a communication efficient mechanism ( CEM ) to reduce the communication cost . Instead of solving the original integer programming problem , the authors consider the relaxed convex stochastic composite optimization ( SCO ) problem and extends it to a distributed setting . The main contribution is that the authors show , both theoretically and experimentally , that their proposed algorithm converges to the true solution of the SCO problem at O ( 1/T ) rate . It is also proved that the communication cost does not grow with the number of samples N. In addition the authors characterize the error ratio between their algorithm and the original k-means . At the end the authors prove that the central server can not recover the feature matrices . The authors compare their algorithm with other k-means algorithms on several datasets . Pros : - The federated setting of k-means with privacy preservation property is interesting and relatively new . - The presentation of the theorems is very clear . The authors put interpretation after each statement and it is easy to follow most of the time . In particular it is shown that the proposed algorithm approaches the baseline scalable kernel k-means algorithm as T increases , both theoretically and experimentally . - I appreciate the discussion of the motivation and related works in the first two section of the paper . Cons : - The whole Section 4 of the paper is filled with technical details and hard to follow . Considering the page limit , why not put the algorithms here , and put the details in the appendix ? In addition the authors can consider putting important observations as lemmas . - It seems the proof techniques are somehow standard . The whole proof of Theorem 1 feels like an extension to [ Zhang et al. , 2016 ] in a distributed setting . I did not check the other proofs though . - My biggest concern is about novelty . Currently the proposed algorithm is heavily influenced by [ Zhang et al. , 2016 ] and [ Wang et al. , 2019 ] . If that is not the case , the authors can consider including a table to illustrate the difference between the algorithms . Post-rebuttal : I appreciate the authors ' feedbacks . However the authors ' response to the proof of Theorem 1 is not the most convincing , which is a big part of the claimed contribution .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the positive comments and constructive suggestions on our paper . 1.The whole Section 4 of the paper is filled with technical details and hard to follow . Considering the page limit , why not put the algorithms here , and put the details in the appendix ? We have moved the pseudo code of the algorithm back to Section 4.2 , and moved some technical details to the appendix . 2.It seems the proof techniques are somehow standard . The whole proof of Theorem 1 feels like an extension to [ Zhang et al. , 2016 ] in a distributed setting . The difference between the proof in [ Zhang et al. , 2016 ] and our proof is as follows . In [ Zhang et al. , 2016 ] , the $ ( t+1 ) $ -th solution $ \\mathrm { Z } _ { t+1 } ^ * $ determined by the stochastic proximal gradient descent ( SPGD ) is the exact solution to the following problem $ $ \\min_ { \\mathrm { Z } \\in \\mathbb { R } ^ { n \\times n } } { \\frac { 1 } { 2 } ||\\mathrm { Z } - \\mathrm { Z } _ { t } ||_F^2 + \\eta_t \\langle \\mathrm { Z } } -\\mathrm { Z } _t , \\mathrm { Z } _t - \\mathrm { \\xi } _t \\rangle + \\eta_t \\lambda ||\\mathrm { Z } || * $ $ However , in FK $ k $ -means , to reduce the communication cost , the $ ( t+1 ) $ -th solution $ \\mathrm { Z } _ { t+1 } $ determined by the distributed stochastic proximal gradient descent ( DSPGD ) is an approximate solution . Since it is unknown how these approximate solutions affect the convergence of DSPGD , we need to prove that the approximate solutions do not affect the convergence of DSPGD . For the details of the proof , please refer to the proof of Theorem~1 ( from the equation ( 6 ) to equation ( 15 ) ) . 3.My biggest concern is about novelty . Currently the proposed algorithm is heavily influenced by [ Zhang et al. , 2016 ] and [ Wang et al. , 2019 ] . If that is not the case , the authors can consider including a table to illustrate the difference between the algorithms . The novelties of FK $ k $ -means are as follows . First , we design DSPGD to approach the top- $ s $ eigenpairs of the kernel matrix $ \\mathrm { K } $ under federated settings where raw data are maintained by users and the cloud can not access the raw data . Second , a communication efficient mechanism ( CEM ) is designed to highly reduce the communication cost of DSPGD . Among these two novelties , DSPGD is a counterpart of the stochastic proximal gradient descent ( SPGD ) algorithm in stochastic PCA [ Zhang et al. , 2016 ] . However , DSPGD is distinct from SPGD in the following three features . DSPGD is conducted under federated settings . However , SPGD is conducted in a centralized manner where users ' raw data are collected at the cloud server . The communication cost is considered in the design of DSPGD , which results in CEM , while the communication cost is not considered in SPGD . Although both DSPGD and SPGD aim at approaching the top eigenpairs of $ \\mathrm { K } $ , in the t-th iteration , DSPGD only needs to obtain an approximate solution $ \\mathrm { Z } \\_ { t+1 } $ instead of the exact solution $ \\mathrm { Z } _ { t+1 } ^ * $ to the same problem like SPGD , which leads to less communication cost under federated settings . oreover , FK $ k $ -means is distinct from the scalable kernel $ k $ -means [ Wang et al. , 2019 ] ( denoted as SK $ k $ -means ) in the following two features . FK $ k $ -means approaches the top eigenvectors without collecting users ' raw data or users ' random features at the cloud server . However , SK $ k $ -means requires users to upload some of their raw data or random features to the cloud server to approximate the kernel matrix $ \\mathrm { K } $ . FK $ k $ -means is an iterative algorithm , and it can approach the top eigenvectors of $ \\mathrm { K } $ more accurately by employing more iterations . In contrast to FK $ k $ -means , SK $ k $ -means is a one-shot algorithm , i.e. , it only determines an approximate kernel matrix $ \\tilde { \\mathrm { K } } $ once and then applies SVD to $ \\tilde { \\mathrm { K } } $ to approach the top eigenvectors of $ \\mathrm { K } $ . Thus , the accuracy of these top eigenvectors is limited by the number of data samples or random features uploaded by users . Reference [ Zhang et al. , 2016 ] Lijun Zhang , Tianbao Yang , Jinfeng Yi , Rong Jin , and Zhi-Hua Zhou . Stochastic optimization for kernel PCA . In Proceedings of the 30th AAAI Conference on Artificial Intelligence , pp . 2316\u2013 2322 , 2016 . [ Wang et al. , 2019 ] Shusen Wang , Alex Gittens , and Michael W Mahoney . Scalable kernel k-means clustering with Nystr { \\ '' o } m approximation : relative-error bounds . The Journal of Machine Learning Research , 20 ( 1 ) :431\u2013479 , 2019"}, "1": {"review_id": "tbwjUvUzQRU-1", "review_text": "The paper proposed a new distributed kernel k-means algorithm that has lower communication overhead compared to the available methods while does not require transmitting the data samples from agents to the master node and therefore claims to preserve the privacy of agents . The paper is rather difficult to follow and the novelty of the paper is not very clear to me . It seems to me that the major contribution of the paper is to come up with efficient tricks during the implementation of the distributed Lanczos algorithm ( DLA ) to find the eigenvalues of the kernel approximated by using the well-known random Fourier features ( Rahimi , nips 2008 ) . So the end result is not something utterly novel but rather an efficient way of utilizing available tools to design a new distributed algorithm . There are several points that I think need to be clarified : 1 . The literature review does not clearly illustrate the contribution of the paper . For instance , in Section 2.1 , the author says `` However , these algorithms are designed with an assumption that they are executed at the cloud server where the constraint on transmissions of the raw data samples does not exist . '' Does this mean that your work is the first to constrain the transmission of data samples ? If so you should clearly say so . Besides , why transmitting the raw data sample is important here ? Is it due to privacy issues ? Because the way I see it , one does not need to transmit the raw data samples but rather its local kernel matrix K , and doing so does not necessarily endanger the privacy of an agent , since one can not easily recover data samples from K. 2 . In Section 6.2 , the author says `` According to the results in the four subfigures , it is shown that CEM can reduce communication cost of DSPGD by more than 95 % . '' It would be interesting to explain what values of r_t , Q_0 , and Q_1 leads to this improvement and why . 3.The 60 % communication cost reduction mentioned throughout the paper seems a bit exaggerated as it does not consistently happen . It only happens for the MNIST dataset in Figure 3 ( b ) . Do you know why ? My guess is that due to the high sparsity level of MNIST samples , the kernel matrix might contain a lot of zeros leading to such behavior . 4.What about the drawbacks of the proposed method ? What I see is a very complicated algorithm that requires heavy computational resources at each agent , which makes it unsuitable for the toy example explained in the introduction about smartphones . After rebuttal : I thank the author for their response but I have to lower my rating by one step after reading the comments of Reviewer2 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the constructive suggestions on our paper . Summary : The paper is rather difficult to follow and the novelty of the paper is not very clear to me . It seems to me that the major contribution of the paper is to come up with efficient tricks during the implementation of the distributed Lanczos algorithm ( DLA ) to find the eigenvalues of the kernel approximated by using the well-known random Fourier features ( Rahimi , nips 2008 ) . In Section 4.1 , we first point out that the key problem for designing FK $ k $ -means is to obtain the top eigenpair of the kernel matrix $ \\mathrm { K } $ in a distributed manner . To solve this problem , we design a distributed stochastic proximal gradient descent ( DSPGD ) algorithm . We then present the challenges on solving the key problem and how we resolve these challenges . Since $ \\mathrm { K } $ is not available under federated settings . To this end , an estimate of $ \\mathrm { K } $ , denoted as $ \\mathrm { \\xi } $ , is constructed jointly at users ' devices based on random features [ 1 ] of local data samples . Since $ \\mathrm { \\xi } $ is distributed among different devices , it is processed by the distributed Lanczos algorithm ( DLA ) [ 3 ] to obtain an estimate of $ \\mathrm { K } $ , i.e. , $ \\mathrm { Z } $ , at the cloud server . Afterwards , an approximate version of the top eigenpairs of $ \\mathrm { K } $ can be obtained from $ \\mathrm { Z } $ through singular value decomposition ( SVD ) . To improve the accuracy of approximation , $ \\mathrm { Z } $ is iteratively updated by DSPGD . More specifically , in the $ t $ -th iteration , an estimate $ \\mathrm { \\xi } \\_ { t } $ is constructed at users ' devices , and then the estimate $ \\mathrm { Z } \\_ { t } $ at the cloud server is updated to $ \\mathrm { Z } \\_ { t+1 } $ by applying DLA to a weighted sum of $ \\mathrm { Z } \\_ { t } $ and $ \\mathrm { \\xi } \\_ { t } $ . In Section 4.2 , we first point out that the process of obtaining an updated $ \\mathrm { Z } _ { t } $ results in high communication cost , because DLA is applied to matrices with the number of rows/columns equals the number of data samples . To this end , we design a communication efficient mechanism ( CEM ) so that DLA is applied to a new type of matrices . We then present the design of the new type of matrices and the procedure of obtaining the top eigenpairs of $ \\mathrm { K } $ in CEM . The major novelty of our work is that we design a new distributed stochastic proximal gradient descent ( DSPGD ) algorithm to approximate the top- $ s $ eigenpairs of the kernel matrix $ \\mathrm { K } $ under the federated settings where raw data are maintained by users and the cloud can not access the raw data . Besides , we design a communication efficient mechanism ( CEM ) that can highly reduce the communication cost of DSPGD . In addition , we derive the convergence rate of DSPGD , analyze the communication cost of DSPGD before and after employing CEM , and analyze the clustering quality of FK $ k $ -means . 1.The literature review does not clearly illustrate the contribution of the paper . For instance , in Section 2.1 , the author says `` However , these algorithms are designed with an assumption that they are executed at the cloud server where the constraint on transmissions of the raw data samples does not exist . '' Does this mean that your work is the first to constrain the transmission of data samples ? If so you should clearly say so . We have revised the literature review and clearly pointed out the differences between our work and the existing schemes . In Section 2.1 , the clause `` where the constraint on transmissions of the raw data samples does not exist '' is indeed confusing . We actually want to express that the distributed kernel $ k $ -means schemes mentioned in Section 2 are designed for the cloud server where users ' raw data are collected . In contrast to these distributed kernel $ k $ -means schemes , FK $ k $ -means is the first scheme to conduct the kernel $ k $ -means clustering without uploading users ' raw data to the cloud server . Besides , FK $ k $ -means also considers the communication efficiency during the clustering task ."}, "2": {"review_id": "tbwjUvUzQRU-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper proposes a new method and two algorithms for solving kernel k-means . The contribution is that the algorithms converge to the optimal solution . The downsides are 1 ) the method is not better than a simple baseline ( i.e. , random features + distributed power method ) and 2 ) the main theorem ( Theorem 3 ) is wrong . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : I vote for rejection for two reasons . First , the proposed method appears not useful . The same problem can be solved in a much simpler and faster way . Second , the main theorem , Theorem 3 , is wrong . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : + This paper develops a new method of distributed kernel k-means . The method is new , although I do not find it very useful . + This paper proves that two algorithms can correctly solve the trace-norm regularized problem in Section 4.1 . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . First and foremost , I do not see a good reason for using the proposed algorithm . The goal of the algorithm is to find the top singular vectors of the random features , $ A $ . - The solution to the trace norm regularized problem , $ Z^ * $ , has the same singular vectors as K. By finding $ Z^ * $ , you can find the eigenvectors of $ K $ . - However , the same goal could be achieved in an easier and less expensive way , i.e. , random features + distributed power method . Random features are naturally distributed among the clients . Their truncated SVD could be found by the distributed power method or Krylov subspace methods . Truncated SVD is easier than solved the proposed trace norm regularized problem because the latter uses SVS which repeatedly performs SVD . 2.I am very surprised that Theorem 3 does not reply on $ D $ ( the number of random features ) . So I checked some of the proofs . I found Theorem 3 , which is the main theorem , is wrong . - $ Z^ * $ has the same top eigenvectors as $ \\xi $ . But $ Z^ * $ may not have the same as $ K $ . - The proof of Theorem 3 relies on that $ Z^ * $ has the same eigenvectors as $ K $ . This is wrong . 3.The description of the algorithm is difficult to follow . I \u2019 d suggest splitting algorithm description into 3 paragraphs : 1 ) Client-side computation , 2 ) server-side computation , and 3 ) communications . Typos : 17th page : `` The following two lemmas will be used in the proof of Theorem 4. \u201d Do you mean Theorem 3 ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Updates after discussing with the authors 1 . The paper is not very clearly written , and I had misunderstandings . Some of my comments above are not right . 2.However , I will not change my rating . I found the convergence rates stated in the paper are misleading . The paper claims $ O ( 1/T ) $ convergence rate . In fact , this is WRONG . The authors assume the Frobenius and trace norms of $ n\\times n $ matrices are CONSTANTS . This is not possible . The norms are $ O ( n ) $ . Simple arguments can show $ || \\xi ||_F = G $ is $ O ( n ) $ . 3.Based on the right assumption that $ || \\xi ||_F = G = O ( n ) $ , the required number of iterations is $ T = O ( n^2 ) $ . The algorithm is not communication-efficient . It is more expensive than communicating the $ n\\times n $ kernel matrices . 4.After reading my comments , the authors changed their notation from $ G $ to $ \\gamma $ , $ C $ , $ G $ , and $ H $ . They are also Frobenius and trace norms of $ n\\times n $ matrices . The authors assume $ \\gamma $ , $ C $ , $ G $ and $ H $ are constants . This is WRONG . They are $ O ( n ) $ . - For example , if they use the bound of Rahimi and Recht , then $ || \\xi - K ||_F^2 = G^2 $ is $ O ( n^2 ) $ . A bound as good as $ || \\xi - K ||_F^2 = O ( n ) $ would surprise me ; if the authors know such a bound , please let me know . - Let me strengthen my point again : IT IS WRONG TO ASSUME MATRIX NORMS ARE CONSTANTS ! If the authors can prove they are constants , they need to show me the proofs . If they can not , they should assume Frobenius norm and trace norm are $ O ( n ) $ .", "rating": "1: Trivial or wrong", "reply_text": "We thank the reviewer for the constructive suggestions on our paper . Summary : This paper proposes a new method and two algorithms for solving kernel k-means . The contribution is that the algorithms converge to the optimal solution . The downsides are 1 ) the method is not better than a simple baseline ( i.e. , random features + distributed power method ) and 2 ) the main theorem ( Theorem 3 ) is wrong . The contribution of our work is as follows . We design a new distributed stochastic proximal gradient descent ( DSPGD ) algorithm to approximate the top- $ s $ eigenpairs of the kernel matrix $ \\mathrm { K } $ under federated settings where raw data are maintained by users and the cloud can not access the raw data . Besides , we design a communication efficient mechanism ( CEM ) that can highly reduce the communication cost of DSPGD . In addition , we theoretically analyze the performance of FK $ k $ -means and its components in three aspects : the convergence rate of DSPGD ; the communication cost of DSPGD before and after employing CEM ; the clustering quality of FK $ k $ -means . We would like to point out that our method ( i.e. , distributed stochastic proximal gradient descent ( DSPGD ) ) is actually better than the baseline ( i.e. , random features + distributed power method ) . The baseline is actually a special case of the DSPGD . If DSPGD adopts the same value of $ D $ as that in the baseline and is only executed for one iteration , then DSPGD is reduced to the baseline . For the comparison between DSPGD and the baseline , please refer to the response to comment 2 . Besides , we have checked the proof of Theorem 3 and confirmed that Theorem 3 is correct . For the detailed clarification , please refer to the responses to comment 3 and comment 4 . 1.The goal of the algorithm is to find the top singular vectors of the random features $ \\mathrm { A } $ . However , the same goal could be achieved in an easier and less expensive way , i.e. , random features + distributed power method . Random features are naturally distributed among the clients . Their truncated SVD could be found by the distributed power method or Krylov subspace methods . Truncated SVD is easier than solved the proposed trace norm regularized problem because the latter uses SVS which repeatedly performs SVD . The goal of the distributed stochastic proximal gradient descent ( DSPGD ) is actually to approach the top eigenpairs of the kernel matrix $ \\mathrm { K } $ , not the the top singular vectors and the corresponding singular values of the random features $ \\mathrm { A } $ . In FK $ k $ -means , DSPGD is designed to solve a stochastic composite optimization ( SCO ) problem in a distributed manner : $ $ \\min_ { \\mathrm { Z } \\in \\mathbb { R } ^ { n \\times n } } { \\frac { 1 } { 2 } \\mathbb { E } [ ||\\mathrm { Z } - \\mathrm { \\xi } ||_F^2 ] +\\lambda ||\\mathrm { Z } || * } . $ $ The optimal solution $ \\mathrm { Z } ^ * $ to SCO problem is $ $ \\mathrm { Z } ^ * = \\sum_ { i : \\lambda_i > \\lambda } { ( \\lambda_i - \\lambda ) \\mathrm { u } _i \\mathrm { u } _i^\\top } , $ $ where $ { \\mathrm { u } } _i $ and $ \\lambda_i $ are the $ i $ -th eigenvector and the $ i $ -th eigenvalue of $ \\mathrm { K } $ , respectively . Assume that the rank of $ \\mathrm { Z } ^ * $ equals $ s $ . By applying SVD to $ \\mathrm { Z } ^ * $ , its eigenvectors $ \\ { { \\mathrm { u } } _i , i=1 , ... , s\\ } $ of $ \\mathrm { K } $ and eigenvalues $ \\ { \\lambda_i - \\lambda , i=1 , ... , s\\ } $ are determined . The eigenvectors of $ \\mathrm { Z } ^ * $ are exactly the top- $ s $ eigenvectors of $ \\mathrm { K } $ . Moreover , by adding $ \\lambda $ to the eigenvalues of $ \\mathrm { Z } ^ * $ , the top- $ s $ eigenvalues of $ \\mathrm { K } $ are obtained . Thus , the top eigenpairs of $ \\mathrm { K } $ are approached by DSPGD . The baseline can indeed be used to approximate the top eigenvectors of $ \\mathrm { K } $ . However , the number of random features $ D $ should be set to a very large value in order to obtain an accurate estimate of $ \\mathrm { K } $ according to [ 1 ] . This method is feasible only if users ' devices have enough memory to maintain these random feature vectors . Compared with the baseline , DSPGD is a generic method that provides a tradeoff between memory and computational time : the larger is $ D $ , the fewer iterations are required ( an extreme case is that only one iteration is required ) to approach the top eigenvectors of $ \\mathrm { K } $ . Thus , DSPGD can be adapted to devices with different memory space ."}, "3": {"review_id": "tbwjUvUzQRU-3", "review_text": "Summary : This paper proposes a distributed version of kernel k-means clustering where some federated structure is used to do distributed processing on the data . Privacy and communication issues are also studied . Numerical results are provided . Reasons for the score : This paper seems provide a new algorithm for distributed clustering . However , the way the algorithm is presented look like a patch of a number of things coming together one after the other with no general structure . This might be caused by the fact that the algorithm is only presented in the appendix . The paper is written in a convoluted manner . This is the main limitation , at some point , we are talking about k means , SVS , DLA , DSPGD , EVD , SPGD , a bunch of other methods that are coupled together towards the main approach . Problem 1 seems to be an integer programming problem , thus with very high computation complexity . It is not clear how this is solved . In the abstract please let me know what are those two levels of privacy you are talking about . In the abstract , what does it mean that the clustering loss of the distributed method approaches the centralized one , please elaborate . Why developing a federated learning algorithm is a promising approach ? Please elaborate . The second part of the intro turns into a detail technical analysis of the algorithm components , and so far we haven \u2019 t seen the algorithm so it all remains a technical abstract discussion that takes away the main messages . The algorithm is in the appendix , so the description and analysis is made on an item that has not been presented in the main text . The way the result is presented makes it look like the proposed method is a concatenation of other results , rather than the solution of a technical challenge in the problem . Numerical results are well presented ,", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the constructive suggestions on our paper . 1.This paper seems provide a new algorithm for distributed clustering . However , the way the algorithm is presented look like a patch of a number of things coming together one after the other with no general structure . This might be caused by the fact that the algorithm is only presented in the appendix . To improve the presentation of the algorithm , we have revised the paper as follows . We have added an overview of federated kernel $ k $ -means ( denoted as FK $ k $ -means ) at the beginning of Section 4 . In Section 4.1 , we first point out that the key problem for designing FK $ k $ -means is to obtain the top eigenpair of the kernel matrix $ \\mathrm { K } $ in a distributed manner . To solve this problem , we design a distributed stochastic proximal gradient descent ( DSPGD ) algorithm . We then present two challenges on solving the key problem and how we resolve these challenges . The first challenge is that $ \\mathrm { K } $ is not available under federated settings . To this end , an estimate of $ \\mathrm { K } $ , denoted as $ \\mathrm { \\xi } $ , is jointly constructed at users ' devices based on random features [ 1 ] of local data samples . Based on $ \\mathrm { \\xi } $ , an estimate of $ \\mathrm { K } $ , denoted as $ \\mathrm { Z } $ can be obtained at the cloud server . Afterwards , an approximate version of the top eigenpairs of $ \\mathrm { K } $ can be obtained from $ \\mathrm { Z } $ through singular value decomposition ( SVD ) . The second challenge is how to improve the accuracy of approximation . To this end , $ \\mathrm { Z } $ is iteratively updated by DSPGD . More specifically , in the $ t $ -th iteration , an estimate $ \\mathrm { \\xi } \\_ { t } $ is constructed at users ' devices , and then the estimate $ \\mathrm { Z } \\_ { t } $ at the cloud server is updated to $ \\mathrm { Z } \\_ { t+1 } $ by applying DLA to a weighted sum of $ \\mathrm { Z } \\_ { t } $ and $ \\mathrm { \\xi } \\_ { t } $ . In Section 4.2 , we first point out that the process of obtaining an updated $ \\mathrm { Z } _ { t } $ results in high communication cost , because DLA is applied to matrices with the number of rows/columns equal to the number of data samples . To this end , we design a communication efficient mechanism ( CEM ) so that DLA is applied to a new type of matrices with reduced dimensions . We then present the design of the new type of matrices and the procedure of obtaining the top eigenpairs of $ \\mathrm { K } $ in CEM . In addition , to improve the clarity of the algorithm , we have moved the pseudo code of the algorithm back to the end of Section 4.2 . 2.The paper is written in a convoluted manner . This is the main limitation , at some point , we are talking about $ k $ -means , SVS , DLA , DSPGD , EVD , SPGD , a bunch of other methods that are coupled together towards the main approach . We have revised the introduction and also Section 4.1 to clarify the relationship between our method with the bunch of other methods . For the detailed revision of the introduction , please refer to the response to comment 7. we first point out that the key problem for designing FK $ k $ -means is to obtain the top eigenpair of the kernel matrix $ \\mathrm { K } $ in a distributed manner . To solve this problem , we design a distributed stochastic proximal gradient descent ( DSPGD ) algorithm . We then present two challenges on solving the key problem and how we resolve these challenges . The first challenge is that $ \\mathrm { K } $ is not available under federated settings . To this end , an estimate of $ \\mathrm { K } $ , denoted as $ \\mathrm { \\xi } $ , is jointly constructed at users ' devices based on random features [ 1 ] of local data samples . Based on $ \\mathrm { \\xi } $ , an estimate of $ \\mathrm { K } $ , denoted as $ \\mathrm { Z } $ can be obtained at the cloud server . Afterwards , an approximate version of the top eigenpairs of $ \\mathrm { K } $ can be obtained from $ \\mathrm { Z } $ through singular value decomposition ( SVD ) . The second challenge is how to improve the accuracy of approximation . To this end , $ \\mathrm { Z } $ is iteratively updated by DSPGD . More specifically , in the $ t $ -th iteration , an estimate $ \\mathrm { \\xi } \\_ { t } $ is constructed at users ' devices , and then the estimate $ \\mathrm { Z } \\_ { t } $ at the cloud server is updated to $ \\mathrm { Z } \\_ { t+1 } $ by applying DLA to a weighted sum of $ \\mathrm { Z } \\_ { t } $ and $ \\mathrm { \\xi } \\_ { t } $ . After the revision , the relationship between FK $ k $ -means and other methods is clear , and it is easier to see the novelty and contribution of our work from the revised algorithm description ."}}