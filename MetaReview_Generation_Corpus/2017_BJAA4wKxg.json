{"year": "2017", "forum": "BJAA4wKxg", "title": "A Convolutional Encoder Model for Neural Machine Translation", "decision": "Reject", "meta_review": "This work demonstrates architectural choices to make conv nets work for NMT. In general the reviewers liked the work and were convinced by the results but found the main contributions to be \"incremental\". \n \n Pros:\n - Clarity: The work was clearly presented, and besides for minor comments (diagrams) the reviewers understood the work\n - Quality: The experimental results were thorough, \"very extensive and leaves no doubt that the proposed approach works well\".\n \n Mixed:\n - Novelty: There is appreciation that the work is novel. However as the work is somewhat \"application-specific\" the reviewers felt the technical contribution was not an overwhelming contribution.\n - Impact: While some speed ups were shown, not all reviewers were convinced that the benefit was sufficient, or \"main speed-up factor(s)\" were. \n \n This work is clearly worthwhile, but the reviews place it slightly below the top papers in this area.", "reviews": [{"review_id": "BJAA4wKxg-0", "review_text": "The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable. Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN's for translation has been attempted previously (as described by the authors), but presumably it is the authors' combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN's, whereas earlier attempts were not. They describe system's sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN's). The experimental results are well reported in detail. One or two figures would definitely be required to help clarify the architecture. This paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in order to get the good results that they do on the reported NMT tasks. In this respect, while I am fairly confident that the paper represents good work in machine learning, I am not quite as confident about its fit for this particular conference.", "rating": "7: Good paper, accept", "reply_text": "We will add figures illustrating the architecture . Please see our response regarding conference fit in the replies to the other reviewers below ."}, {"review_id": "BJAA4wKxg-1", "review_text": "This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference. One minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as well (it does take some effort to understand the last paragraph in Section 2, especially the part on having a linear layer to compute z).", "rating": "6: Marginally above acceptance threshold", "reply_text": "We will add figures illustrating our model architecture better ; our baseline biLSTM architecture closely follows [ Zhou et al , TACL 2016 ] https : //arxiv.org/pdf/1606.04199v3.pdf who have detailed figures illustrating the model . Position embeddings are important . Without them our encoder has no notion of position . We investigated position embeddings numbered from the left to right and right to left but found that one direction was enough . Regarding conference fit : we show that RNNs are not necessary to perform encoding in a complex sequence to sequence task and that CNNs are comparable or better . Of course , this is of interest to the NLP community but also to the ML community . In term of representation learning , the two stack CNN highlight that attention weights and the input representation might benefit from different depth , which can impact other tasks , and other models . Our work also highlights that the success of RNNs in MT can not be attributed to their unique ability to model long term dependencies . We believe that those findings are of great interest to the ML community ."}, {"review_id": "BJAA4wKxg-2", "review_text": "The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. Apart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations. The empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open. The experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor. It\u2019s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered. My main concern is whether or not the paper is appropriate for ICLR, because the contribution is quite incremental and rather application-specific. ACL, EMNLP and other NLP conferences would be a better venue, I think. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Paper is application specific : we chose to focus on machine translation which is a challenging task with strong competition systems , dedicated product teams interested in improvements , and MT has been used in the past to introduce sequence to sequence models ( e.g.Sutskever et al , Bahdanau et al ) . It is very likely that tasks relying on architectures similar to attention-based translation will benefit by our architecture as well , e.g.summarization , constituency parsing , or dialog modeling . In future work , we plan to evaluate on other sequence to sequence problems as well . Two stacks : our experimentation with the two stacks suggests that the attention weights ( CNN-a ) need to integrate information from a wide context which can be done with a deep stack ; at the same time , the vectors which are averaged ( CNN-c ) seem to benefit from a shallower , more local representation closer to the input words . Two stacks is an easy way to achieve these contradicting requirements . We will add this discussion to the text to make the point clearer . Inspired by LSTMs , we are currently exploring non-linearities with gating that could potentially derive a single representation satisfying both requirements . Faster evaluation : we will clarify this in the next version of the paper . Please refer to our previous answer to this question in the thread below ."}], "0": {"review_id": "BJAA4wKxg-0", "review_text": "The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable. Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN's for translation has been attempted previously (as described by the authors), but presumably it is the authors' combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN's, whereas earlier attempts were not. They describe system's sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN's). The experimental results are well reported in detail. One or two figures would definitely be required to help clarify the architecture. This paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in order to get the good results that they do on the reported NMT tasks. In this respect, while I am fairly confident that the paper represents good work in machine learning, I am not quite as confident about its fit for this particular conference.", "rating": "7: Good paper, accept", "reply_text": "We will add figures illustrating the architecture . Please see our response regarding conference fit in the replies to the other reviewers below ."}, "1": {"review_id": "BJAA4wKxg-1", "review_text": "This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference. One minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as well (it does take some effort to understand the last paragraph in Section 2, especially the part on having a linear layer to compute z).", "rating": "6: Marginally above acceptance threshold", "reply_text": "We will add figures illustrating our model architecture better ; our baseline biLSTM architecture closely follows [ Zhou et al , TACL 2016 ] https : //arxiv.org/pdf/1606.04199v3.pdf who have detailed figures illustrating the model . Position embeddings are important . Without them our encoder has no notion of position . We investigated position embeddings numbered from the left to right and right to left but found that one direction was enough . Regarding conference fit : we show that RNNs are not necessary to perform encoding in a complex sequence to sequence task and that CNNs are comparable or better . Of course , this is of interest to the NLP community but also to the ML community . In term of representation learning , the two stack CNN highlight that attention weights and the input representation might benefit from different depth , which can impact other tasks , and other models . Our work also highlights that the success of RNNs in MT can not be attributed to their unique ability to model long term dependencies . We believe that those findings are of great interest to the ML community ."}, "2": {"review_id": "BJAA4wKxg-2", "review_text": "The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. Apart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations. The empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open. The experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor. It\u2019s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered. My main concern is whether or not the paper is appropriate for ICLR, because the contribution is quite incremental and rather application-specific. ACL, EMNLP and other NLP conferences would be a better venue, I think. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Paper is application specific : we chose to focus on machine translation which is a challenging task with strong competition systems , dedicated product teams interested in improvements , and MT has been used in the past to introduce sequence to sequence models ( e.g.Sutskever et al , Bahdanau et al ) . It is very likely that tasks relying on architectures similar to attention-based translation will benefit by our architecture as well , e.g.summarization , constituency parsing , or dialog modeling . In future work , we plan to evaluate on other sequence to sequence problems as well . Two stacks : our experimentation with the two stacks suggests that the attention weights ( CNN-a ) need to integrate information from a wide context which can be done with a deep stack ; at the same time , the vectors which are averaged ( CNN-c ) seem to benefit from a shallower , more local representation closer to the input words . Two stacks is an easy way to achieve these contradicting requirements . We will add this discussion to the text to make the point clearer . Inspired by LSTMs , we are currently exploring non-linearities with gating that could potentially derive a single representation satisfying both requirements . Faster evaluation : we will clarify this in the next version of the paper . Please refer to our previous answer to this question in the thread below ."}}