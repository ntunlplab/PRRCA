{"year": "2021", "forum": "r-gPPHEjpmw", "title": "Hierarchical Reinforcement Learning by Discovering Intrinsic Options", "decision": "Accept (Poster)", "meta_review": "This paper presents an approach to hierarchical RL which automatically learns intrinsic task-agnostic options. The approach involves a two-level hierarchy, with policies learned by lower-layer Workers and selected by a higher-layer Scheduler. The approach is evaluated on four complex tasks and is shown to outperform existing methods.\nThere were initial concerns with this paper around clarity of a number of points. These included the contributions of this work and questions around the experimental results, such as  discussing the learned options themselves. The authors provided extensive responses to these concerns, and updated the paper accordingly, including addition results and analysis. I believe the paper is now much clearer with interesting contributions.", "reviews": [{"review_id": "r-gPPHEjpmw-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : In this paper the authors present a new method for hierarchical reinforcement learning , demonstrated with a 2 layer architecture , in which the higher layer Scheduler policy choose lower level Worker policies at fixed intervals . The lower level Worker policies ( options ) are trained through an intrinsic entropy minimization . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : This paper seems to be based strongly on the work in Sharma et al. , 2019b and Eysenbach et al. , 2019 , and while it makes a number of interesting modifications to those methods there is in sufficient decoupling / ablation of the issues . I think the paper would be notably stronger with additional experimental evidence # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . The paper makes some reasonably sensible extension to well-know methods . 2.The authors treatment of hyper parameters and implementation details looks to be thorough # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . I found parts of the paper difficult to parse . For example it was not immediately clear to me what exactly an option $ \\bf { u } \\in [ -1,1 ] ^D $ is ... What values can / does D take ? Another free parameter ? Is $ \\bf { u } $ a latent variable that yields an option through the worker policy ? ( this terminology would seem to better align with that in Eysenbach et al. , 2019 and the broader literature around options ) . I 'm also not sure $ \\bar { \\bf { s } } _t $ ( single subscript ) is defined ? 2.As the authors note the paper introduces a number of new elements : the reset of the worker policy every k-step , the utilization of full sub-trajectories , and the various instantiations of the discriminator ... but these are never unwound . It would seem to be the case that there would be a fair bit of interplay between these elements . Additionally it appears that there is some need to modify the implementation to support these choices - most notably the discounting for the `` shortsighted worker '' ... is this still necessary if full sub-trajectories are not used ? 3.The experimentation did not address some fairly obvious enquires ( what is the effect of different choices of K in your experiments ? How would a practitioner approach choosing this value ? ) 4.There is remarkably little in this paper that talks about the actual options uncovered via this method . Does the method uncover different options for different values K ? What do they look like ? etc. # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : Q1 : a fair amount of the focus of the paper appears to be on the instantiation of the discriminator - do you consider that to be a central contribution ? Q2 : you talk about the fact that a key divergence from priori work is that there is `` no constraint on the value on what $ \\bf { u } $ could represent . '' Can you talk about why this is a good thing and have you explored when it might not be such a good thing ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review and detailed feedback . > This paper seems to be based strongly on the work in Sharma et al. , 2019b and Eysenbach et al. , 2019 Please see our response to your Q1 below for a summary of major differences with their methods , and a clarification of our central contributions . > and while it makes a number of interesting modifications to those methods there is insufficient decoupling / ablation of the issues . The reviewer might think that the proposed elements ( e.g. , use of sub-trajectories , shortsighted worker , discriminator instantiation ) are exclusive or completely independent of each other , however , this is not true . Please see our detailed response to your question \u201c As the authors note the paper introduces a number of new elements\u2026 \u201d below . In a word , these new elements are essential components of realizing the single worker objective in Eq ( 5 ) . They are definitely not unrelated improvements proposed from different angles . Besides , we believe that we did provide enough ablation studies for our approach : 1 . In section 4.1 , we explored 6 different instantiations of the discriminator and found that roughly 3 instantiations ( Action , StateAction , and StateDiff ) can perform reasonably well ( Figure 3 ) . This disentangles the instantiation choice with the other components of the framework . 2.We also compare the SOFT and HARD versions of the worker policy in section 4.1 . Note that their implementation difference is only a hyperparameter ( discounting factor ) . So they are not two different approaches but ablations of our approach . 3.In section 4.3 , we explore another ablation baseline that pretrains the worker policy , to explore if it \u2019 s necessary for a joint training of the worker and scheduler . > \u201c I think the paper would be notably stronger with additional experimental evidence \u201d ~~We are running a small experiment and obtaining some extra results for different values of $ K $ ( the option length ) ~~ We have added results for ablations with different option lengths ( see edits in the second reply to this review ) . Given the existing ablation studies we have performed ( summarized above ) , it would be better if the reviewer can further suggest what additional experiments are needed . > \u201c 1.I found parts of the paper difficult to parse . For example it was not immediately clear to me what exactly an option $ u \\in [ -1 , 1 ] ^D $ is ... What values can / does D take ? Another free parameter ? Is $ u $ a latent variable that yields an option through the worker policy ? ( this terminology would seem to better align with that in Eysenbach et al. , 2019 and the broader literature around options ) . I 'm also not sure $ \\bar { s } $ ( single subscript ) is defined ? \u201d An option $ u $ can be any valid point in the defined space $ [ -1 , 1 ] ^D $ , and yes it can be treated as a latent variable to modulate the worker policy . The value of $ D $ is a hyperparameter ( see appendix D.1.3 and D.2.3 for the values we chose in the experiments ) . We \u2019 ve clarified this in the revision ( section 3 , paragraph 2 ) . For the single subscript t , we intended to use it to refer to a general time step $ 0 \\le t < T $ , and any $ t = h * K + k $ for a pair of $ ( h , k ) $ ( see the line right before Eq ( 2 ) ) . But to avoid the confusion , we \u2019 ve removed the use of single subscripts $ t $ completely in the revision ."}, {"review_id": "r-gPPHEjpmw-1", "review_text": "The paper describes an interesting approach to self-supervised option learning for hierarchical reinforcement learning . In particular , the authors propose to learn options through an intrinsic entropy minimization objective conditioned on option trajectories . Empirical studies are performed to demonstrate the success rate and sample efficiency of this method over two state-of-the-art methods . The key concern that I have with this paper is its organization and presentation . I am concerned that the paper takes too much space describing what other methods do not do ( throughout the contributions sections ) , rather than concretely describing the approach . Furthermore , many sections seem to be an aggregation of contributions , related work , and results , which further makes the paper hard to follow . I believe that this does the paper a disservice by decreasing readability . Therefore , I am currently voting for rejecting the paper . Questions and Comments : 1 ) In the Introduction , the authors claim that their approach requires `` little '' manual design ? Can this claim be better grounded ? What design is required ? 2 ) There is extensive work on option discovery [ 1,2 ] . as well as learning decompositions for reinforcement learning [ 3-6 ] . I would have liked to see a more extensive literature review comparing and contrasting with these works . 3 ) On page 2 , the authors say , `` for the purpose of this paper , we consider a hierarchy of two levels '' . However , it is not clear to me how this approach would extend to a hierarchy with more than two levels . Can the authors please clarify this ? 4 ) What is k ? h ? ( page 2 ) 5 ) On page 3 , the authors claim that , unlike prior work , their formulation has no constraints on what the options can be . Can the authors comment on this claim in more detail ? 6 ) In section 3.2 , the authors mention the use of goals . Are the authors considering goal-conditioned MDPs [ 7 ] ? If so , this should be made more explicit . 8 ) The MDP definition in section 3.2 is unclear . Could the authors please clarify this definition ? 9 ) What is meant by `` resolution '' ( section 3.2 ) ? 10 ) Why does the objective defined by the authors enable skills that have more diverse semantics than previous approaches ( claimed on page 4 , section 3.2 ) ? 11 ) Page 5 , section 3.4 : is the importance correction not used at all ( for either the scheduler or discriminator ) ? 12 ) What assumptions are needed for this method to work well ? 13 ) In the experiment section , it says that in the pusher environment , success means that the goal is achieved in the final step of the episode . Does this mean that pushing the object to the position does not terminate the episode and , instead , the object must remain there until the episode terminates in a predetermined number of steps ? 14 ) Can the authors please clarify the purpose of the additional suite of environments ? What do these environments enable that existing ones do not ? [ 1 ] Brunskill , et al. , PAC-inspired option discovery in lifelong reinforcement learning . ICML , 2014 . [ 2 ] Topin , et al. , Portable option discovery for automated learning transfer in object-oriented Markov decision processes . IJCAI , 2015 . [ 3 ] Mehta et al. , Automatic discovery and transfer of MAXQ hierarchies . ICML , 2008 . [ 4 ] Winder et al. , Planning with abstract learned models while learning transferable subtasks . AAAI , 2020 . [ 5 ] Li , et al. , An efficient approach to model-based hierarchical reinforcement learning . AAAI , 2017 . [ 6 ] Rafati , et al. , Learning representations in model-free hierarchical reinforcement learning . AAAI , 2019 . [ 7 ] Nasiriany , et al. , Planning with goal-conditioned policies . NeurIPS , 2019 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "> \u201c 9.Why does the objective defined by the authors enable skills that have more diverse semantics than previous approaches ( claimed on page 4 , section 3.2 ) ? \u201d As already explained in the same paragraph , the previous approaches [ Gregor et al. , 2016 ] and [ Eysenbach et al. , 2019 ] always define an option ( skill ) with respect to the initial state $ s_0 $ sampled from a fixed initial distribution . The semantics of an option spans over the entire episode starting from $ s_0 $ in their case . Our options are defined with respect to $ s_0 , s_K , s_ { 2K } , \\cdots $ because we are using sub-trajectories of length $ K $ to identify options . So in our case , the same option vector ( e.g. , [ 1.0 , 1.0 , 1.0 ] ) could mean different things given different values of $ s_K $ , depending on which state the agent is in currently in the middle of an episode . > \u201c 10.Page 5 , section 3.4 : is the importance correction not used at all ( for either the scheduler or discriminator ) ? \u201d Correct . We tried including the importance correction but it yields high variance in importance ratios and actually hinders the training . Similar observations were made by [ Nachum et al.2018 ; Fedus et al.2020 ] .A baseline without importance correction was tested in [ Nachum et al.2018 ] and had competitive performance compared to their full method . We also found that our method is able to perform well empirically even without importance correction . We include that paragraph for discussions and inform readers what we \u2019 ve tried but didn \u2019 t work in practice . > \u201c 11.What assumptions are needed for this method to work well ? \u201d One important assumption is that by optimizing the worker objective Eq ( 5 ) , the discovered options can actually be assembled by the scheduler to accomplish the environment task . This assumption can be made true if 1 ) the discovered options are diverse ( differentiable from each other ) and 2 ) the population of the discovered options covers the trajectory space well ( made possible by maximizing the entropy of the worker policy as in SAC [ Haarnoja et al. , 2018 ] ) . > \u201c 12.In the experiment section , it says that in the pusher environment , success means that the goal is achieved in the final step of the episode . Does this mean that pushing the object to the position does not terminate the episode and , instead , the object must remain there until the episode terminates in a predetermined number of steps ? \u201d Correct . The pusher environment doesn \u2019 t have an early termination . Each episode will run for a fixed number of steps . We \u2019 ve clarified this in the revision . This setup is more challenging as it can test if the agent actually knows how to achieve the task or simply accomplishes it accidentally when exploring . > \u201c 13.Can the authors please clarify the purpose of the additional suite of environments ? What do these environments enable that existing ones do not ? \u201d Many environments used in other hierarchical works are navigation and goal-reaching tasks that have * dense * rewards ( such as some of the Ant tasks in HIRO [ Nachum et al.2018 ] ) or are different variants of the same navigation task ( such as the Snake/Ant Gather tasks in HIRO and HiPPO [ Li et al.2019 ] ) .Meanwhile , all of our environments are sparse-reward , continuous action-space tasks that serve different purposes and are based upon real robots . While our reaching task can be thought of as a sparse-reward navigation task , our pushing task is a robotic object manipulation task , and both simulate a one-armed PR2 robot ( https : //blog.robotiq.com/bid/65419/Collaborative-Robot-Series-PR2-from-Willow-Garage ) . Furthermore , GoalTask is a more complex navigation task that involves randomly placed distractor objects that the agent must learn to avoid and not get stuck on , and Kickball is difficult in that the agent must learn to perform the skills required to solve GoalTask and then also learn to kick a ball into a goal , all under sparse reward supervision . These two tasks emulate the Pioneer wheeled robot ( https : //www.generationrobots.com/media/Pioneer3DX-P3DX-RevA.pdf ) ."}, {"review_id": "r-gPPHEjpmw-2", "review_text": "This paper present HIDIO , a hierarchical RL method that leverages self-supervised losses to discover intrinsic options while learning a scheduler to leverage the learned options to optimize the accumulated reward . HIDIO differentiates from prior work through enabling the low-level network/worker to discover task-agnostic options that can be generalized to future tasks , thus requiring no pre-training of skills , and at the same time makes minimal assumption about the task structure . This paper is in general well-written with clarity . The proposed method is technically sound and empirically outperforms existing methods . The experiments in four different tasks first demonstrate an ablation of different feature extractors used for the self-supervised loss and analyzed the each of their advantages/disadvantages ; followed by experiments comparing HIDIO with existing hRL methods and show that HIDIO outperforms in all tasks . Although the experiments are conducted in two different domains and four different tasks , all four tasks seem to be similar in nature ( i.e.all pushing and reaching tasks ) . It would be great to see how HIDIO compare with other methods in more complex task domains such as in the work of [ 1 ] and [ 2 ] , where the skill discovery network need to work with high dimensional raw input data and computing state-similarity can be tricky . [ 1 ] Lynch , C. , Khansari , M. , Xiao , T. , Kumar , V. , Tompson , J. , Levine , S. , & Sermanet , P. ( 2020 , May ) . Learning latent plans from play . In Conference on Robot Learning ( pp.1113-1132 ) . [ 2 ] Chuck , C. , Chockchowwat , S. , & Niekum , S. ( 2020 ) . Hypothesis-Driven Skill Discovery for Hierarchical Deep Reinforcement Learning . International Conference on Intelligent Robots and Systems , 2020 .", "rating": "7: Good paper, accept", "reply_text": "Thanks for the positive review and paper feedback . * * * > Although the experiments are conducted in two different domains and four different tasks , all four tasks seem to be similar in nature ( i.e.all pushing and reaching tasks ) . Regarding the similarity of the four different tasks , we believe that while they are somewhat similar in nature , they are difficult in different ways and test the ability of agents to learn options not present in the commonly used navigation tasks in other hierarchical RL works . For a detailed analysis of what each task is testing and how they \u2019 re different , please see our response to AnonReviewer4 \u2019 s Question 13 ( \u201c Many environments used in other hierarchical works are navigation\u2026 \u201d ) . * * * > It would be great to see how HIDIO compare with other methods in more complex task domains such as in the work of [ 1 ] and [ 2 ] , where the skill discovery network need to work with high dimensional raw input data and computing state-similarity can be tricky . The complex tasks in the works you referenced are very interesting , although we are currently focusing on state-based rather than image-based environments . In theory , we believe that HIDIO can be readily applied to image-based environments with CNNs instead of MLPs as the early layers of the policy networks and discriminator , without changing the overall framework and objectives . However , one might need to resolve some practical issues induced during this process ( mostly hyperparameter and network selection ) , and we leave this extension to future work . We \u2019 ve added the mentioned works to our related works section ( last part of section 5 ) . * * *"}, {"review_id": "r-gPPHEjpmw-3", "review_text": "The paper develops a hierarchical reinforcement learning algorithm and analyzes its behaviour in four robotic manipulation and navigation tasks . The approach is based on a two-level hierarchy , * scheduler * at the top and * worker * at the bottom . This is similar to other approaches in the literature and the algorithm uses many ideas and elements from existing algorithms . However , these ideas and elements are combined in a novel and well-justified manner . The result is an algorithm that yields good results in a range of problems . The experiments are well done . The paper is generally organised well and written clearly . Relevant literature is reviewed well . The paper can be improved by a more comprehensive and detailed analysis of the behaviour of the algorithm , in particular , the options that are found . Section 4.4 is useful but very short . It describes only two options . Perhaps such an analysis can be added to the appendix . In the proposed algorithm , the scheduler outputs an option every K steps in the environment . It would be reasonable to question whether more flexibility would be useful here , one that allows for varying option durations . The authors state in Section 2 that they 'will use the terms \u201c goal \u201d , \u201c option \u201d , and \u201c skill \u201d interchangeably . ' In the literature , these terms refer to related but different concepts . Using them interchangeably is not good scientific practice . I did not find Figure 1 particularly useful . The simple and intuitive structure of the algorithm does not come through in the figure .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your positive review and feedback . * * * > The paper can be improved by a more comprehensive and detailed analysis of the behaviour of the algorithm , in particular , the options that are found . Section 4.4 is useful but very short . It describes only two options . Perhaps such an analysis can be added to the appendix . Per your recommendation , we have now added Appendix Section C , which contains analysis of 8 more options in two environments , and Figure 9 , which visualizes those 8 options . ~~Furthermore , we are running an extra experiment by ablating $ K $ across all four environments , and we will add extra figures and analyses regarding option behaviors at different $ K $ values once this finishes . ~~ Furthermore , we have added Figures 7 and 8 and analysis in appendix Section C.1 which compares performance across all tasks with different option lengths and visualizes their distribution of trajectories in GoalTask and KickBall . * * * > In the proposed algorithm , the scheduler outputs an option every K steps in the environment . It would be reasonable to question whether more flexibility would be useful here , one that allows for varying option durations . Using a fixed value of $ K $ is a simplification in our method since our focus in this paper is Eq ( 5 ) and the various instantiations of the discriminator . Many existing methods like HIRO [ Nachum et al. , NeurIPS 2018 ] and HiPPO [ Li et al. , 2020 ] also assume a fixed $ K $ . It \u2019 s indeed interesting to question if varying option durations would benefit the method or not . However , that involves a non-trivial addition to the method ( e.g. , learning another network head in the scheduler policy to decide the option duration ) . Such a question can be considered for future work . * * * > The authors state in Section 2 that they 'will use the terms \u201c goal \u201d , \u201c option \u201d , and \u201c skill \u201d interchangeably . ' In the literature , these terms refer to related but different concepts . Using them interchangeably is not good scientific practice . Thanks for pointing out this issue . Originally we used them interchangeably for an easier comparison with other similar methods from different backgrounds . We realize that it \u2019 s indeed confusing in the current way and have changed to use the word \u201c option \u201d throughout the paper . * * * > I did not find Figure 1 particularly useful . The simple and intuitive structure of the algorithm does not come through in the figure . Figure 1 has been replaced by a more straightforward illustration that focuses on the hierarchical interplay between the scheduler and worker policy . * * *"}], "0": {"review_id": "r-gPPHEjpmw-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : In this paper the authors present a new method for hierarchical reinforcement learning , demonstrated with a 2 layer architecture , in which the higher layer Scheduler policy choose lower level Worker policies at fixed intervals . The lower level Worker policies ( options ) are trained through an intrinsic entropy minimization . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : This paper seems to be based strongly on the work in Sharma et al. , 2019b and Eysenbach et al. , 2019 , and while it makes a number of interesting modifications to those methods there is in sufficient decoupling / ablation of the issues . I think the paper would be notably stronger with additional experimental evidence # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . The paper makes some reasonably sensible extension to well-know methods . 2.The authors treatment of hyper parameters and implementation details looks to be thorough # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . I found parts of the paper difficult to parse . For example it was not immediately clear to me what exactly an option $ \\bf { u } \\in [ -1,1 ] ^D $ is ... What values can / does D take ? Another free parameter ? Is $ \\bf { u } $ a latent variable that yields an option through the worker policy ? ( this terminology would seem to better align with that in Eysenbach et al. , 2019 and the broader literature around options ) . I 'm also not sure $ \\bar { \\bf { s } } _t $ ( single subscript ) is defined ? 2.As the authors note the paper introduces a number of new elements : the reset of the worker policy every k-step , the utilization of full sub-trajectories , and the various instantiations of the discriminator ... but these are never unwound . It would seem to be the case that there would be a fair bit of interplay between these elements . Additionally it appears that there is some need to modify the implementation to support these choices - most notably the discounting for the `` shortsighted worker '' ... is this still necessary if full sub-trajectories are not used ? 3.The experimentation did not address some fairly obvious enquires ( what is the effect of different choices of K in your experiments ? How would a practitioner approach choosing this value ? ) 4.There is remarkably little in this paper that talks about the actual options uncovered via this method . Does the method uncover different options for different values K ? What do they look like ? etc. # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : Q1 : a fair amount of the focus of the paper appears to be on the instantiation of the discriminator - do you consider that to be a central contribution ? Q2 : you talk about the fact that a key divergence from priori work is that there is `` no constraint on the value on what $ \\bf { u } $ could represent . '' Can you talk about why this is a good thing and have you explored when it might not be such a good thing ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review and detailed feedback . > This paper seems to be based strongly on the work in Sharma et al. , 2019b and Eysenbach et al. , 2019 Please see our response to your Q1 below for a summary of major differences with their methods , and a clarification of our central contributions . > and while it makes a number of interesting modifications to those methods there is insufficient decoupling / ablation of the issues . The reviewer might think that the proposed elements ( e.g. , use of sub-trajectories , shortsighted worker , discriminator instantiation ) are exclusive or completely independent of each other , however , this is not true . Please see our detailed response to your question \u201c As the authors note the paper introduces a number of new elements\u2026 \u201d below . In a word , these new elements are essential components of realizing the single worker objective in Eq ( 5 ) . They are definitely not unrelated improvements proposed from different angles . Besides , we believe that we did provide enough ablation studies for our approach : 1 . In section 4.1 , we explored 6 different instantiations of the discriminator and found that roughly 3 instantiations ( Action , StateAction , and StateDiff ) can perform reasonably well ( Figure 3 ) . This disentangles the instantiation choice with the other components of the framework . 2.We also compare the SOFT and HARD versions of the worker policy in section 4.1 . Note that their implementation difference is only a hyperparameter ( discounting factor ) . So they are not two different approaches but ablations of our approach . 3.In section 4.3 , we explore another ablation baseline that pretrains the worker policy , to explore if it \u2019 s necessary for a joint training of the worker and scheduler . > \u201c I think the paper would be notably stronger with additional experimental evidence \u201d ~~We are running a small experiment and obtaining some extra results for different values of $ K $ ( the option length ) ~~ We have added results for ablations with different option lengths ( see edits in the second reply to this review ) . Given the existing ablation studies we have performed ( summarized above ) , it would be better if the reviewer can further suggest what additional experiments are needed . > \u201c 1.I found parts of the paper difficult to parse . For example it was not immediately clear to me what exactly an option $ u \\in [ -1 , 1 ] ^D $ is ... What values can / does D take ? Another free parameter ? Is $ u $ a latent variable that yields an option through the worker policy ? ( this terminology would seem to better align with that in Eysenbach et al. , 2019 and the broader literature around options ) . I 'm also not sure $ \\bar { s } $ ( single subscript ) is defined ? \u201d An option $ u $ can be any valid point in the defined space $ [ -1 , 1 ] ^D $ , and yes it can be treated as a latent variable to modulate the worker policy . The value of $ D $ is a hyperparameter ( see appendix D.1.3 and D.2.3 for the values we chose in the experiments ) . We \u2019 ve clarified this in the revision ( section 3 , paragraph 2 ) . For the single subscript t , we intended to use it to refer to a general time step $ 0 \\le t < T $ , and any $ t = h * K + k $ for a pair of $ ( h , k ) $ ( see the line right before Eq ( 2 ) ) . But to avoid the confusion , we \u2019 ve removed the use of single subscripts $ t $ completely in the revision ."}, "1": {"review_id": "r-gPPHEjpmw-1", "review_text": "The paper describes an interesting approach to self-supervised option learning for hierarchical reinforcement learning . In particular , the authors propose to learn options through an intrinsic entropy minimization objective conditioned on option trajectories . Empirical studies are performed to demonstrate the success rate and sample efficiency of this method over two state-of-the-art methods . The key concern that I have with this paper is its organization and presentation . I am concerned that the paper takes too much space describing what other methods do not do ( throughout the contributions sections ) , rather than concretely describing the approach . Furthermore , many sections seem to be an aggregation of contributions , related work , and results , which further makes the paper hard to follow . I believe that this does the paper a disservice by decreasing readability . Therefore , I am currently voting for rejecting the paper . Questions and Comments : 1 ) In the Introduction , the authors claim that their approach requires `` little '' manual design ? Can this claim be better grounded ? What design is required ? 2 ) There is extensive work on option discovery [ 1,2 ] . as well as learning decompositions for reinforcement learning [ 3-6 ] . I would have liked to see a more extensive literature review comparing and contrasting with these works . 3 ) On page 2 , the authors say , `` for the purpose of this paper , we consider a hierarchy of two levels '' . However , it is not clear to me how this approach would extend to a hierarchy with more than two levels . Can the authors please clarify this ? 4 ) What is k ? h ? ( page 2 ) 5 ) On page 3 , the authors claim that , unlike prior work , their formulation has no constraints on what the options can be . Can the authors comment on this claim in more detail ? 6 ) In section 3.2 , the authors mention the use of goals . Are the authors considering goal-conditioned MDPs [ 7 ] ? If so , this should be made more explicit . 8 ) The MDP definition in section 3.2 is unclear . Could the authors please clarify this definition ? 9 ) What is meant by `` resolution '' ( section 3.2 ) ? 10 ) Why does the objective defined by the authors enable skills that have more diverse semantics than previous approaches ( claimed on page 4 , section 3.2 ) ? 11 ) Page 5 , section 3.4 : is the importance correction not used at all ( for either the scheduler or discriminator ) ? 12 ) What assumptions are needed for this method to work well ? 13 ) In the experiment section , it says that in the pusher environment , success means that the goal is achieved in the final step of the episode . Does this mean that pushing the object to the position does not terminate the episode and , instead , the object must remain there until the episode terminates in a predetermined number of steps ? 14 ) Can the authors please clarify the purpose of the additional suite of environments ? What do these environments enable that existing ones do not ? [ 1 ] Brunskill , et al. , PAC-inspired option discovery in lifelong reinforcement learning . ICML , 2014 . [ 2 ] Topin , et al. , Portable option discovery for automated learning transfer in object-oriented Markov decision processes . IJCAI , 2015 . [ 3 ] Mehta et al. , Automatic discovery and transfer of MAXQ hierarchies . ICML , 2008 . [ 4 ] Winder et al. , Planning with abstract learned models while learning transferable subtasks . AAAI , 2020 . [ 5 ] Li , et al. , An efficient approach to model-based hierarchical reinforcement learning . AAAI , 2017 . [ 6 ] Rafati , et al. , Learning representations in model-free hierarchical reinforcement learning . AAAI , 2019 . [ 7 ] Nasiriany , et al. , Planning with goal-conditioned policies . NeurIPS , 2019 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "> \u201c 9.Why does the objective defined by the authors enable skills that have more diverse semantics than previous approaches ( claimed on page 4 , section 3.2 ) ? \u201d As already explained in the same paragraph , the previous approaches [ Gregor et al. , 2016 ] and [ Eysenbach et al. , 2019 ] always define an option ( skill ) with respect to the initial state $ s_0 $ sampled from a fixed initial distribution . The semantics of an option spans over the entire episode starting from $ s_0 $ in their case . Our options are defined with respect to $ s_0 , s_K , s_ { 2K } , \\cdots $ because we are using sub-trajectories of length $ K $ to identify options . So in our case , the same option vector ( e.g. , [ 1.0 , 1.0 , 1.0 ] ) could mean different things given different values of $ s_K $ , depending on which state the agent is in currently in the middle of an episode . > \u201c 10.Page 5 , section 3.4 : is the importance correction not used at all ( for either the scheduler or discriminator ) ? \u201d Correct . We tried including the importance correction but it yields high variance in importance ratios and actually hinders the training . Similar observations were made by [ Nachum et al.2018 ; Fedus et al.2020 ] .A baseline without importance correction was tested in [ Nachum et al.2018 ] and had competitive performance compared to their full method . We also found that our method is able to perform well empirically even without importance correction . We include that paragraph for discussions and inform readers what we \u2019 ve tried but didn \u2019 t work in practice . > \u201c 11.What assumptions are needed for this method to work well ? \u201d One important assumption is that by optimizing the worker objective Eq ( 5 ) , the discovered options can actually be assembled by the scheduler to accomplish the environment task . This assumption can be made true if 1 ) the discovered options are diverse ( differentiable from each other ) and 2 ) the population of the discovered options covers the trajectory space well ( made possible by maximizing the entropy of the worker policy as in SAC [ Haarnoja et al. , 2018 ] ) . > \u201c 12.In the experiment section , it says that in the pusher environment , success means that the goal is achieved in the final step of the episode . Does this mean that pushing the object to the position does not terminate the episode and , instead , the object must remain there until the episode terminates in a predetermined number of steps ? \u201d Correct . The pusher environment doesn \u2019 t have an early termination . Each episode will run for a fixed number of steps . We \u2019 ve clarified this in the revision . This setup is more challenging as it can test if the agent actually knows how to achieve the task or simply accomplishes it accidentally when exploring . > \u201c 13.Can the authors please clarify the purpose of the additional suite of environments ? What do these environments enable that existing ones do not ? \u201d Many environments used in other hierarchical works are navigation and goal-reaching tasks that have * dense * rewards ( such as some of the Ant tasks in HIRO [ Nachum et al.2018 ] ) or are different variants of the same navigation task ( such as the Snake/Ant Gather tasks in HIRO and HiPPO [ Li et al.2019 ] ) .Meanwhile , all of our environments are sparse-reward , continuous action-space tasks that serve different purposes and are based upon real robots . While our reaching task can be thought of as a sparse-reward navigation task , our pushing task is a robotic object manipulation task , and both simulate a one-armed PR2 robot ( https : //blog.robotiq.com/bid/65419/Collaborative-Robot-Series-PR2-from-Willow-Garage ) . Furthermore , GoalTask is a more complex navigation task that involves randomly placed distractor objects that the agent must learn to avoid and not get stuck on , and Kickball is difficult in that the agent must learn to perform the skills required to solve GoalTask and then also learn to kick a ball into a goal , all under sparse reward supervision . These two tasks emulate the Pioneer wheeled robot ( https : //www.generationrobots.com/media/Pioneer3DX-P3DX-RevA.pdf ) ."}, "2": {"review_id": "r-gPPHEjpmw-2", "review_text": "This paper present HIDIO , a hierarchical RL method that leverages self-supervised losses to discover intrinsic options while learning a scheduler to leverage the learned options to optimize the accumulated reward . HIDIO differentiates from prior work through enabling the low-level network/worker to discover task-agnostic options that can be generalized to future tasks , thus requiring no pre-training of skills , and at the same time makes minimal assumption about the task structure . This paper is in general well-written with clarity . The proposed method is technically sound and empirically outperforms existing methods . The experiments in four different tasks first demonstrate an ablation of different feature extractors used for the self-supervised loss and analyzed the each of their advantages/disadvantages ; followed by experiments comparing HIDIO with existing hRL methods and show that HIDIO outperforms in all tasks . Although the experiments are conducted in two different domains and four different tasks , all four tasks seem to be similar in nature ( i.e.all pushing and reaching tasks ) . It would be great to see how HIDIO compare with other methods in more complex task domains such as in the work of [ 1 ] and [ 2 ] , where the skill discovery network need to work with high dimensional raw input data and computing state-similarity can be tricky . [ 1 ] Lynch , C. , Khansari , M. , Xiao , T. , Kumar , V. , Tompson , J. , Levine , S. , & Sermanet , P. ( 2020 , May ) . Learning latent plans from play . In Conference on Robot Learning ( pp.1113-1132 ) . [ 2 ] Chuck , C. , Chockchowwat , S. , & Niekum , S. ( 2020 ) . Hypothesis-Driven Skill Discovery for Hierarchical Deep Reinforcement Learning . International Conference on Intelligent Robots and Systems , 2020 .", "rating": "7: Good paper, accept", "reply_text": "Thanks for the positive review and paper feedback . * * * > Although the experiments are conducted in two different domains and four different tasks , all four tasks seem to be similar in nature ( i.e.all pushing and reaching tasks ) . Regarding the similarity of the four different tasks , we believe that while they are somewhat similar in nature , they are difficult in different ways and test the ability of agents to learn options not present in the commonly used navigation tasks in other hierarchical RL works . For a detailed analysis of what each task is testing and how they \u2019 re different , please see our response to AnonReviewer4 \u2019 s Question 13 ( \u201c Many environments used in other hierarchical works are navigation\u2026 \u201d ) . * * * > It would be great to see how HIDIO compare with other methods in more complex task domains such as in the work of [ 1 ] and [ 2 ] , where the skill discovery network need to work with high dimensional raw input data and computing state-similarity can be tricky . The complex tasks in the works you referenced are very interesting , although we are currently focusing on state-based rather than image-based environments . In theory , we believe that HIDIO can be readily applied to image-based environments with CNNs instead of MLPs as the early layers of the policy networks and discriminator , without changing the overall framework and objectives . However , one might need to resolve some practical issues induced during this process ( mostly hyperparameter and network selection ) , and we leave this extension to future work . We \u2019 ve added the mentioned works to our related works section ( last part of section 5 ) . * * *"}, "3": {"review_id": "r-gPPHEjpmw-3", "review_text": "The paper develops a hierarchical reinforcement learning algorithm and analyzes its behaviour in four robotic manipulation and navigation tasks . The approach is based on a two-level hierarchy , * scheduler * at the top and * worker * at the bottom . This is similar to other approaches in the literature and the algorithm uses many ideas and elements from existing algorithms . However , these ideas and elements are combined in a novel and well-justified manner . The result is an algorithm that yields good results in a range of problems . The experiments are well done . The paper is generally organised well and written clearly . Relevant literature is reviewed well . The paper can be improved by a more comprehensive and detailed analysis of the behaviour of the algorithm , in particular , the options that are found . Section 4.4 is useful but very short . It describes only two options . Perhaps such an analysis can be added to the appendix . In the proposed algorithm , the scheduler outputs an option every K steps in the environment . It would be reasonable to question whether more flexibility would be useful here , one that allows for varying option durations . The authors state in Section 2 that they 'will use the terms \u201c goal \u201d , \u201c option \u201d , and \u201c skill \u201d interchangeably . ' In the literature , these terms refer to related but different concepts . Using them interchangeably is not good scientific practice . I did not find Figure 1 particularly useful . The simple and intuitive structure of the algorithm does not come through in the figure .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your positive review and feedback . * * * > The paper can be improved by a more comprehensive and detailed analysis of the behaviour of the algorithm , in particular , the options that are found . Section 4.4 is useful but very short . It describes only two options . Perhaps such an analysis can be added to the appendix . Per your recommendation , we have now added Appendix Section C , which contains analysis of 8 more options in two environments , and Figure 9 , which visualizes those 8 options . ~~Furthermore , we are running an extra experiment by ablating $ K $ across all four environments , and we will add extra figures and analyses regarding option behaviors at different $ K $ values once this finishes . ~~ Furthermore , we have added Figures 7 and 8 and analysis in appendix Section C.1 which compares performance across all tasks with different option lengths and visualizes their distribution of trajectories in GoalTask and KickBall . * * * > In the proposed algorithm , the scheduler outputs an option every K steps in the environment . It would be reasonable to question whether more flexibility would be useful here , one that allows for varying option durations . Using a fixed value of $ K $ is a simplification in our method since our focus in this paper is Eq ( 5 ) and the various instantiations of the discriminator . Many existing methods like HIRO [ Nachum et al. , NeurIPS 2018 ] and HiPPO [ Li et al. , 2020 ] also assume a fixed $ K $ . It \u2019 s indeed interesting to question if varying option durations would benefit the method or not . However , that involves a non-trivial addition to the method ( e.g. , learning another network head in the scheduler policy to decide the option duration ) . Such a question can be considered for future work . * * * > The authors state in Section 2 that they 'will use the terms \u201c goal \u201d , \u201c option \u201d , and \u201c skill \u201d interchangeably . ' In the literature , these terms refer to related but different concepts . Using them interchangeably is not good scientific practice . Thanks for pointing out this issue . Originally we used them interchangeably for an easier comparison with other similar methods from different backgrounds . We realize that it \u2019 s indeed confusing in the current way and have changed to use the word \u201c option \u201d throughout the paper . * * * > I did not find Figure 1 particularly useful . The simple and intuitive structure of the algorithm does not come through in the figure . Figure 1 has been replaced by a more straightforward illustration that focuses on the hierarchical interplay between the scheduler and worker policy . * * *"}}