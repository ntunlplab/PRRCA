{"year": "2021", "forum": "px0-N3_KjA", "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning", "decision": "Reject", "meta_review": "This paper proposes benchmark tasks for offline reinforcement learning.  The paper has major strength and weakness, and it has resulted in very active discussion among reviewers, authors, and other participants.\n\nThe major strength includes the following:\n- The proposed benchmark is already heavily used in the community\n- Offline reinforcement learning is very important to solve reinforcement learning tasks in the real world\n- The paper covers a range of tasks and provides through evaluation of existing methods to be used as baselines\n\nThe major weakness is that it is not sufficiently convincing that the methods that perform well in the proposed benchmark tasks will perform well in the offline reinforcement learning tasks in the real world.  \n\nThis is partly due to the nature of the benchmark tasks of offline reinforcement learning, which require simulators to evaluate the policies learned with offline reinforcement learning.  This means that one cannot simply collect datasets from real world tasks and provide them as benchmark datasets.  \n\nAlthough one cannot do much about simulators, benchmark tasks for offline reinforcement learning still have many design choices.  In particular, how should the datasets in the benchmark be collected (i.e., behavior policies)?\n\nWhile the datasets in the proposed benchmark are collected with various behavior policies including humans, it is not necessarily convincing that the resulting benchmark tasks are good for the purpose of evaluating offline reinforcement learning to be used in the real world.\n\nIn addition to the suggestions given by the reviewers, a possible direction to improve the paper is to focus on the choice of behavior policies used to generate the datasets in the proposed benchmark.  One might then be able to provide some convincing arguments as to why performing well in the benchmark might imply good performance in the real world by relating it to the choice of behavior policies.", "reviews": [{"review_id": "px0-N3_KjA-0", "review_text": "Summary : In this paper a test suite of data sets and corresponding benchmarks for offline reinforcement learning is introduced . Several existing RL benchmarks are used , the results of several algorithms are presented . The authors claim that the benchmarks were specifically designed for the offline setting and are guided by the key properties of datasets in real-world applications of offline RL . Strong points : The present paper has already been cited and the benchmarks suite has already been used by other publications . Obviously there is a need for offline RL test suites . Weak points : The authors ' claim that such a benchmark for offline RL should `` be composed of tasks that reflect challenges in real-world applications of data-driven RL '' is only partially met by the paper in its present form . The area of robotics , with deterministic dynamics , is comparatively well represented , but there is no real , industrial application . In particular it seems that so far no benchmark has been included that has the ambition to have the characteristics and complexity of a real application . Recommendation : On the one hand , the really realistic benchmarks are missing , so that a publication seems premature . On the other hand , the current status is already used by the research community , since there seems to be no test suite for offline RL apart from `` RL unplugged : Benchmarks for offline reinforcement learning '' . I therefore recommend to accept the paper . Questions : To what extent are the current benchmarks stochastic ? Are there bi- or multimodal transition probabilities ? Additional feedback with the aim to improve the paper : In Table 2 and Table 3 average results are reported over only 3 random seeds . This seems to me to be clearly too little , especially since the policy performance of Q-function based algorithms often fluctuates strongly . Since no uncertainties , e.g.in the form of standard error , are given , the reliability of the results can not be assessed . The way policies are selected before they are tested should be described more clearly . My impression was that in each case the policy is used that results for a considered algorithm and random seed after 500K training iterations or gradient steps . Since different algorithms require different computational efforts this approach does not seem to be in the sense of a real-world application . In most cases there should be the willingness to use much more computational effort for especially good policies . According to the motto : computing power is cheap , data is expensive . I like the formulation \u201c Effective offline RL algorithms must handle [ \u2026 ] data collected via processes that may not be representable by the chosen policy class. \u201c This expresses the , in my opinion , correct view of the real situation well , while the assumption that there is a `` behavior policy '' that generated the data is not true in general . It may have been different people at different times who performed the actions while the data set was recorded . Please check the bibliography for accidental lower case , like \u201e markov \u201c , \u201e adobeindoornav \u201c - ( Dec 3 ) Taking into account the other reviews , the authors ' responses and the changes made by the authors , as well as the extensive and controversial discussion , I rate the paper still with a score of 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback . We believe your main concern is in the real-world ramifications of the tasks used in the benchmark , to which we respond to below , along with several other clarifications . Please let us know if you have any additional questions , and if this addresses your concerns . > \u201c The authors ' claim that such a benchmark for offline RL should `` be composed of tasks that reflect challenges in real-world applications of data-driven RL '' is only partially met by the paper in its present form . The area of robotics , with deterministic dynamics , is comparatively well represented , but there is no real , industrial application\u2026 on the one hand , the really realistic benchmarks are missing \u201d We agree that incorporating tasks with real-life implications is important for a benchmark , and therefore we leveraged some of the most realistic simulated domains which are widely available for research use . For example , the Adroit and FrankaKitchen domains are models of Shadow Hand and Franka robots , respectively , and leverage real human demonstrations collected via motion capture . Likewise , CARLA is a photorealistic simulator used in the autonomous driving community ( e.g . [ 1 ] ) and Flow is a traffic simulator used in the operations research/transportation community ( e.g . [ 2 ] ) .Robotic manipulation , autonomous driving , and traffic control are all domains with real , challenging applications and significant implications for everyday life . We considered other domains with real datasets , such as healthcare ( MIMIC-III ) or recommender systems ( various , across industry ) , but for the purposes of constructing a benchmark , we do not have a reliable method for evaluating the performance of algorithms on these tasks aside from costly real-world evaluation . [ 1 ] \u201c End-to-end driving via conditional imitation learning. \u201d Codevilla et . al.ICRA 2018 [ 2 ] \u201c Stabilizing traffic with autonomous vehicles \u201d Wu et . al.ICRA 2018 > \u201c To what extent are the current benchmarks stochastic ? Are there bi- or multimodal transition probabilities ? \u201d The current benchmarks are stochastic in the initial state ( i.e.in the navigation environments the starting location is randomized ) , and through partial observability in the case of CARLA . Stochasticity is however , a property that is not explored in-depth compared to the other properties we outlined in Section 4 . We added additional discussion of this point in Section 7 . > \u201c \u2026 average results are reported over only 3 random seeds . This seems to me to be clearly too little , especially since the policy performance of Q-function based algorithms often fluctuates strongly . \u2026 In most cases there should be the willingness to use much more computational effort for especially good policies . According to the motto : computing power is cheap , data is expensive. \u201d The cost for evaluating this benchmark was already quite expensive due to its scope , with 11 algorithms evaluated and 42 tasks ( for a total of 462 evaluations ) , including several which require GPU access . We will evaluate additional seeds , but this may not be complete during the rebuttal period ."}, {"review_id": "px0-N3_KjA-1", "review_text": "The paper proposes a standardized benchmark for offline RL research . The data collection is well motivated from real world scenarios covering many important design factors . I really appreciate that human demonstration and hand-crafted controllers are also included . The evaluation protocol and the API looks clear and easy to use . The benchmark of existing methods is thorough and provides many useful insights . I believe this work will have a high impact on the offline RL community . I can expect this benchmark will be used by many papers in the future and will function as the starting point for many offline RL research . However , I notice that this dataset may not be accessible for underrepresented groups . I therefore vote to reject . As the authors note in the paper , each task consists of a dataset for training and a simulator for evaluation . In my understanding , half of the six tasks ( Maze2D , AntMaze , Gym-mujoco ) depend heavily on the MuJoCo simulator , which is a commercial software and is not free even for academic use . A personal MuJoCo license costs 500 USD per year . I am concerned that MuJoCo is not accessible for most underrepresented researchers . It is not clear when MuJoCo becomes a dominating benchmark for online RL research , though there are indeed free , open-sourced alternatives , e.g.PyBullet ( https : //github.com/bulletphysics/bullet3 ) . In online RL , we need the simulator for training . One reason MuJoCo becomes popular may be because it 's more stable and faster than PyBullet . However , in offline RL , a simulator is used only for evaluation not for training . So the high reliability of MuJoCo may no longer be so necessary . I therefore view offline RL as a good opportunity for the community to get rid of commercial simulation softwares , making RL research more accessible for underrepresented groups . If accepted , this paper will indeed greatly promote the use of MuJoCo given its potential high impact , making RL more privileged . Overall , I really enjoy reading the paper and am glad to see a standardized benchmark for offline RL . I am happy to raise my score if the accessibility issue is addressed , e.g. , by using PyBullet as the physical engine . == ( Nov 24 ) I appreciate the effort put into the Bullet reimplementation and therefore increase my score from 3 to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments , and raising awareness on this important issue . We are glad to hear that you think this benchmark has potential to have a high impact on the RL community and will be used by many papers to come . We agree that using free , open-source simulators would benefit the community and the introduction of a new benchmark is a great place to make that shift in the community . We have begun implementing a PyBullet version of our tasks , and will add an update here on the progress before the rebuttal deadline . Nevertheless , MuJoCo is already used by the community including many RL benchmark papers such as RLLAB [ 1 ] , RLUnplugged [ 2 ] , Metaworld [ 3 ] , and Gym [ 4 ] . However , adding PyBullet versions to the current tasks seems like the best path forward . We also emphasize that several of our domains , such as CARLA and Flow , use simulators that do not require a paid license . We note that the authors are not affiliated with the company that sells MuJoCo . MuJoCo does offer free licenses on its website for personal use , for projects not \u201c part of employment \u201d and not already receiving financial support . [ 1 ] \u201c Benchmarking Deep Reinforcement Learning for Continuous Control \u201d Duan et . al.ICML 2016 [ 2 ] \u201c RL Unplugged : Benchmarks for Offline Reinforcement Learning \u201d Gulcehre et . al.NeurIPS 2020 [ 3 ] \u201c Meta-World : A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning \u201c Yu et . al.CoRL 2019 [ 4 ] \u201c OpenAI Gym \u201d Brockman et . al.2016"}, {"review_id": "px0-N3_KjA-2", "review_text": "This paper offers a new set of challenges for batch reinforcement learning coupled with a set of benchmarks , including autonomous robotics and driving domains . All domains also come with simulation environments . While the dataset provided by the authors may be a useful resource for researchers in offline-RL , this paper does not introduce any new or novel ideas . Overall the main contribution of this work is the gathering of offline data in one place , reducing the time needed for other researchers to do so . It does not seem as if the authors did any non-trivial work other than annotating the data . Furthermore , there have been many previous work which have already collected offline datasets as part of their work . I do not underestimate the importance the authors ' hard work , nor the importance of the provided datasets to the RL community . Nevertheless , I do not believe this work should be published in a high-end conference without presenting any ideas that are not trivial or known to other researches . The authors propose to use the following design factors in their offline datasets : narrow distributions , multi-task data , sparse rewards , suboptimal data , and partially observable policies . While these factors may indeed be good for testing offline-RL algorithms , they do not provide a complete picture of real world datasets . Real datasets present many more real-world problems , including : 1 . Non-stationary data . Many real datasets are non-stationary . The non-stationary behavior could be mimicked or simulated from real behavior . 2.Real policies . Real datasets do n't involve policies that were trained by RL agents . The authors could create datasets that are constructed by real human beings ( for example , humans playing atari games , with mixed or different expertise ) . In a controlled setup , the datasets could be constructed so that policies are categorized ( e.g. , `` level of non Markovianess ` `` ) . 3.Causal structures . Real policies may act according to some causal structure in the background that is not necessarily known . 4.Reduction from real datasets . One could collect or use large amounts of high quality datasets from the real world and add certain corruptions to the data as to lower its quality ( e.g. , removing certain trajectories ) . If the initial data is of high quality , the corrupted data could be controlled well . 5.Changing and/or very large action sets . Real world datasets have changing and large datasets . As an example , consider ad placement , or text based tasks . 6.Non-robotic environments , including games but also real world problems . If the authors bring together datasets that generate original ideas that have never been previously explored , then I believe it 's more likely that this paper could be accepted in future venues .", "rating": "2: Strong rejection", "reply_text": "Thank you for your comments and feedback . We respond to individual comments below . Please let us know if this addresses your concerns . > \u201c If the authors bring together datasets that generate original ideas that have never been previously explored , then I believe it 's more likely that this paper could be accepted in future venues. \u201d We believe that benchmark and evaluation papers should be measured on their potential impact and insights provided . Constructing a systematic evaluation of existing methods , providing insights into their shortcomings to guide future research ( Section 6 ) , and implementing an easy-to-use research platform ( see our website and code : https : //sites.google.com/view/d4rl-anonymous/ ) are all contributions that further advance this field . D4RL has already facilitated standardized evaluation and benchmarking , and moving beyond reliance on MuJoCo locomotion tasks and datasets obtained from replay buffers of online RL algorithms . This work has been used as a starting point for novel ideas as evidenced by the rapid adoption of D4RL , and the number of papers submitted to this conference that use it as their primary evaluation task such as : - \u201c Offline Policy Optimization with Variance Regularization \u201d - \u201c Uncertainty Weighted Offline Reinforcement Learning \u201d - \u201c Risk-Averse Offline Reinforcement Learning \u201d - \u201c Addressing Distribution Shift in Online Reinforcement Learning with Offline Datasets \u201d - \u201c BRAC+ : Going Deeper with Behavior Regularized Offline Reinforcement Learning \u201d - \u201c Fine-Tuning Offline Reinforcement Learning with Model-Based Policy Optimization \u201d - \u201c EMaQ : Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL \u201d - \u201c OPAL : Offline Primitive Discovery for Accelerating Offline Reinforcement Learning \u201d - \u201c Model-Based Offline Planning \u201d And in NeurIPS 2020 papers : - \u201c Conservative Q-Learning for Offline Reinforcement Learning \u201d . Kumar et.al.- \u201c Model-Based Offline Policy Optimization \u201d . Yu et al.- \u201c Continual Learning of Control Primitives : Skill Discovery via Reset-Games \u201d . Xu et.al. > \u201c Real datasets present many more real-world problems , including : Non-stationary data\u2026 Real policies\u2026 Causal structures\u2026 Reduction from real datasets\u2026 Changing and/or very large action sets\u2026 Non-robotic environments \u201d While we agree that these are important properties of real-world datasets , no single benchmark can cover every real world problem . Our benchmark tasks provide coverage of several important real-world properties that we suspected have a large impact on offline RL and have not previously been evaluated in offline RL . For example , the Adroit/Franka domains contain real policies obtained by teleoperation . We have added a discussion of these properties and the limitations of our benchmark to emphasize their importance and future opportunity in Section 7 . > \u201c While the dataset provided by the authors may be a useful resource ... this paper does not introduce any new or novel ideas . Overall the main contribution of this work is the gathering of offline data in one place \u201d We agree that our primary contribution is not novel methods or analyses , however , you understate our contributions . In the Gym/Franka/Adroit domains , we do use existing datasets , however , for the remaining domains , we either construct datasets for tasks that have not previously been studied in offline RL or in the case of Maze2D/AntMaze , construct novel domains to study specific properties . Appendix C details which datasets we borrowed as part of the construction of this benchmark . Prior to our work , the predominant method for evaluating offline RL algorithms was to generate data from online RL agents on Atari or Gym tasks . As discussed in [ 2 ] and shown in our empirical evaluation , this does not exercise important dimensions of the problem . Our data generation procedures bring attention to important and neglected properties in offline RL ( e.g. , narrow data distributions , multitask data , and non-representable policies ) that have a significant impact on performance and have already guided followup research . For example , recent work [ 1 , 2 ] has provided contradicting evidence on the importance of the data generating distribution in offline RL . D4RL contains multiple domains where this problem can be explored in depth , across a wide variety of evaluated algorithms . Our empirical evaluations in Table 1 reveal clear performance differences under different data generation settings . Finally , we provide a systematic evaluation of 11 RL methods , including state-of-the-art algorithms . Previously , no such extensive evaluation on a common dataset had been done in offline RL . The standardization and thorough evaluation of a benchmark with appealing properties ( including several that you listed as important real-world properties ) is a valuable contribution . [ 1 ] \u201c An Optimistic Perspective on Offline Reinforcement Learning \u201d Agrawal et . al.2020 [ 2 ] \u201c Behavior Regularized Offline Reinforcement Learning \u201d Wu et . al.2019"}, {"review_id": "px0-N3_KjA-3", "review_text": "D4RL : Datasets for Deep Data-Driven Reinforcement Learning review : summarization : In this paper , the authors consider the problems of offline reinforcement learning problems , and has a focus of dataset and shareable code base . While no novel algorithms are proposed in this project , a systematic evaluation of existing algorithms ( offline RL algorithms ) is proposed . Pros : 1.Offline RL is a hot topic this year , with a lot of papers exploring efficient and robust ways to utilize offline collected data . This paper , while using simulated data , provides a general platform to benchmark these algorithms . 2.The project provides a comprehensive evaluation and discussion on existing considerations in offline RL . It provides several interesting directions in offline RL . 3.The paper is well-written . It is very clear what the purpose of the project is , and it is very clear why the authors make the dataset in the way they did . Cons : 1.The dataset is mostly simulated . While I understand the difficulty of collection real-data , it does raise some concerns that a simulated dataset can be generated by researchers themselves . Summary : While the dataset is simulated , I do think there \u2019 s value in the dataset and the shared code-base to facilitate recent progress in offline RL research .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> \u201c The dataset is mostly simulated . While I understand the difficulty of collection real-data , it does raise some concerns that a simulated dataset can be generated by researchers themselves. \u201d Thank you for your comments . We made a very conscious choice in the design of the benchmark to keep everything in simulation , and select domains for which realistic , accurate simulations exist and have been vetted by the research community . We would be open to suggestions which allow both accurate evaluation and a convincing degree of realism in order to improve the benchmark . We would also like some additional clarification on what concerns you have with using a dataset generated by a simulator , and we can add this to the discussion in the main text ."}], "0": {"review_id": "px0-N3_KjA-0", "review_text": "Summary : In this paper a test suite of data sets and corresponding benchmarks for offline reinforcement learning is introduced . Several existing RL benchmarks are used , the results of several algorithms are presented . The authors claim that the benchmarks were specifically designed for the offline setting and are guided by the key properties of datasets in real-world applications of offline RL . Strong points : The present paper has already been cited and the benchmarks suite has already been used by other publications . Obviously there is a need for offline RL test suites . Weak points : The authors ' claim that such a benchmark for offline RL should `` be composed of tasks that reflect challenges in real-world applications of data-driven RL '' is only partially met by the paper in its present form . The area of robotics , with deterministic dynamics , is comparatively well represented , but there is no real , industrial application . In particular it seems that so far no benchmark has been included that has the ambition to have the characteristics and complexity of a real application . Recommendation : On the one hand , the really realistic benchmarks are missing , so that a publication seems premature . On the other hand , the current status is already used by the research community , since there seems to be no test suite for offline RL apart from `` RL unplugged : Benchmarks for offline reinforcement learning '' . I therefore recommend to accept the paper . Questions : To what extent are the current benchmarks stochastic ? Are there bi- or multimodal transition probabilities ? Additional feedback with the aim to improve the paper : In Table 2 and Table 3 average results are reported over only 3 random seeds . This seems to me to be clearly too little , especially since the policy performance of Q-function based algorithms often fluctuates strongly . Since no uncertainties , e.g.in the form of standard error , are given , the reliability of the results can not be assessed . The way policies are selected before they are tested should be described more clearly . My impression was that in each case the policy is used that results for a considered algorithm and random seed after 500K training iterations or gradient steps . Since different algorithms require different computational efforts this approach does not seem to be in the sense of a real-world application . In most cases there should be the willingness to use much more computational effort for especially good policies . According to the motto : computing power is cheap , data is expensive . I like the formulation \u201c Effective offline RL algorithms must handle [ \u2026 ] data collected via processes that may not be representable by the chosen policy class. \u201c This expresses the , in my opinion , correct view of the real situation well , while the assumption that there is a `` behavior policy '' that generated the data is not true in general . It may have been different people at different times who performed the actions while the data set was recorded . Please check the bibliography for accidental lower case , like \u201e markov \u201c , \u201e adobeindoornav \u201c - ( Dec 3 ) Taking into account the other reviews , the authors ' responses and the changes made by the authors , as well as the extensive and controversial discussion , I rate the paper still with a score of 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback . We believe your main concern is in the real-world ramifications of the tasks used in the benchmark , to which we respond to below , along with several other clarifications . Please let us know if you have any additional questions , and if this addresses your concerns . > \u201c The authors ' claim that such a benchmark for offline RL should `` be composed of tasks that reflect challenges in real-world applications of data-driven RL '' is only partially met by the paper in its present form . The area of robotics , with deterministic dynamics , is comparatively well represented , but there is no real , industrial application\u2026 on the one hand , the really realistic benchmarks are missing \u201d We agree that incorporating tasks with real-life implications is important for a benchmark , and therefore we leveraged some of the most realistic simulated domains which are widely available for research use . For example , the Adroit and FrankaKitchen domains are models of Shadow Hand and Franka robots , respectively , and leverage real human demonstrations collected via motion capture . Likewise , CARLA is a photorealistic simulator used in the autonomous driving community ( e.g . [ 1 ] ) and Flow is a traffic simulator used in the operations research/transportation community ( e.g . [ 2 ] ) .Robotic manipulation , autonomous driving , and traffic control are all domains with real , challenging applications and significant implications for everyday life . We considered other domains with real datasets , such as healthcare ( MIMIC-III ) or recommender systems ( various , across industry ) , but for the purposes of constructing a benchmark , we do not have a reliable method for evaluating the performance of algorithms on these tasks aside from costly real-world evaluation . [ 1 ] \u201c End-to-end driving via conditional imitation learning. \u201d Codevilla et . al.ICRA 2018 [ 2 ] \u201c Stabilizing traffic with autonomous vehicles \u201d Wu et . al.ICRA 2018 > \u201c To what extent are the current benchmarks stochastic ? Are there bi- or multimodal transition probabilities ? \u201d The current benchmarks are stochastic in the initial state ( i.e.in the navigation environments the starting location is randomized ) , and through partial observability in the case of CARLA . Stochasticity is however , a property that is not explored in-depth compared to the other properties we outlined in Section 4 . We added additional discussion of this point in Section 7 . > \u201c \u2026 average results are reported over only 3 random seeds . This seems to me to be clearly too little , especially since the policy performance of Q-function based algorithms often fluctuates strongly . \u2026 In most cases there should be the willingness to use much more computational effort for especially good policies . According to the motto : computing power is cheap , data is expensive. \u201d The cost for evaluating this benchmark was already quite expensive due to its scope , with 11 algorithms evaluated and 42 tasks ( for a total of 462 evaluations ) , including several which require GPU access . We will evaluate additional seeds , but this may not be complete during the rebuttal period ."}, "1": {"review_id": "px0-N3_KjA-1", "review_text": "The paper proposes a standardized benchmark for offline RL research . The data collection is well motivated from real world scenarios covering many important design factors . I really appreciate that human demonstration and hand-crafted controllers are also included . The evaluation protocol and the API looks clear and easy to use . The benchmark of existing methods is thorough and provides many useful insights . I believe this work will have a high impact on the offline RL community . I can expect this benchmark will be used by many papers in the future and will function as the starting point for many offline RL research . However , I notice that this dataset may not be accessible for underrepresented groups . I therefore vote to reject . As the authors note in the paper , each task consists of a dataset for training and a simulator for evaluation . In my understanding , half of the six tasks ( Maze2D , AntMaze , Gym-mujoco ) depend heavily on the MuJoCo simulator , which is a commercial software and is not free even for academic use . A personal MuJoCo license costs 500 USD per year . I am concerned that MuJoCo is not accessible for most underrepresented researchers . It is not clear when MuJoCo becomes a dominating benchmark for online RL research , though there are indeed free , open-sourced alternatives , e.g.PyBullet ( https : //github.com/bulletphysics/bullet3 ) . In online RL , we need the simulator for training . One reason MuJoCo becomes popular may be because it 's more stable and faster than PyBullet . However , in offline RL , a simulator is used only for evaluation not for training . So the high reliability of MuJoCo may no longer be so necessary . I therefore view offline RL as a good opportunity for the community to get rid of commercial simulation softwares , making RL research more accessible for underrepresented groups . If accepted , this paper will indeed greatly promote the use of MuJoCo given its potential high impact , making RL more privileged . Overall , I really enjoy reading the paper and am glad to see a standardized benchmark for offline RL . I am happy to raise my score if the accessibility issue is addressed , e.g. , by using PyBullet as the physical engine . == ( Nov 24 ) I appreciate the effort put into the Bullet reimplementation and therefore increase my score from 3 to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments , and raising awareness on this important issue . We are glad to hear that you think this benchmark has potential to have a high impact on the RL community and will be used by many papers to come . We agree that using free , open-source simulators would benefit the community and the introduction of a new benchmark is a great place to make that shift in the community . We have begun implementing a PyBullet version of our tasks , and will add an update here on the progress before the rebuttal deadline . Nevertheless , MuJoCo is already used by the community including many RL benchmark papers such as RLLAB [ 1 ] , RLUnplugged [ 2 ] , Metaworld [ 3 ] , and Gym [ 4 ] . However , adding PyBullet versions to the current tasks seems like the best path forward . We also emphasize that several of our domains , such as CARLA and Flow , use simulators that do not require a paid license . We note that the authors are not affiliated with the company that sells MuJoCo . MuJoCo does offer free licenses on its website for personal use , for projects not \u201c part of employment \u201d and not already receiving financial support . [ 1 ] \u201c Benchmarking Deep Reinforcement Learning for Continuous Control \u201d Duan et . al.ICML 2016 [ 2 ] \u201c RL Unplugged : Benchmarks for Offline Reinforcement Learning \u201d Gulcehre et . al.NeurIPS 2020 [ 3 ] \u201c Meta-World : A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning \u201c Yu et . al.CoRL 2019 [ 4 ] \u201c OpenAI Gym \u201d Brockman et . al.2016"}, "2": {"review_id": "px0-N3_KjA-2", "review_text": "This paper offers a new set of challenges for batch reinforcement learning coupled with a set of benchmarks , including autonomous robotics and driving domains . All domains also come with simulation environments . While the dataset provided by the authors may be a useful resource for researchers in offline-RL , this paper does not introduce any new or novel ideas . Overall the main contribution of this work is the gathering of offline data in one place , reducing the time needed for other researchers to do so . It does not seem as if the authors did any non-trivial work other than annotating the data . Furthermore , there have been many previous work which have already collected offline datasets as part of their work . I do not underestimate the importance the authors ' hard work , nor the importance of the provided datasets to the RL community . Nevertheless , I do not believe this work should be published in a high-end conference without presenting any ideas that are not trivial or known to other researches . The authors propose to use the following design factors in their offline datasets : narrow distributions , multi-task data , sparse rewards , suboptimal data , and partially observable policies . While these factors may indeed be good for testing offline-RL algorithms , they do not provide a complete picture of real world datasets . Real datasets present many more real-world problems , including : 1 . Non-stationary data . Many real datasets are non-stationary . The non-stationary behavior could be mimicked or simulated from real behavior . 2.Real policies . Real datasets do n't involve policies that were trained by RL agents . The authors could create datasets that are constructed by real human beings ( for example , humans playing atari games , with mixed or different expertise ) . In a controlled setup , the datasets could be constructed so that policies are categorized ( e.g. , `` level of non Markovianess ` `` ) . 3.Causal structures . Real policies may act according to some causal structure in the background that is not necessarily known . 4.Reduction from real datasets . One could collect or use large amounts of high quality datasets from the real world and add certain corruptions to the data as to lower its quality ( e.g. , removing certain trajectories ) . If the initial data is of high quality , the corrupted data could be controlled well . 5.Changing and/or very large action sets . Real world datasets have changing and large datasets . As an example , consider ad placement , or text based tasks . 6.Non-robotic environments , including games but also real world problems . If the authors bring together datasets that generate original ideas that have never been previously explored , then I believe it 's more likely that this paper could be accepted in future venues .", "rating": "2: Strong rejection", "reply_text": "Thank you for your comments and feedback . We respond to individual comments below . Please let us know if this addresses your concerns . > \u201c If the authors bring together datasets that generate original ideas that have never been previously explored , then I believe it 's more likely that this paper could be accepted in future venues. \u201d We believe that benchmark and evaluation papers should be measured on their potential impact and insights provided . Constructing a systematic evaluation of existing methods , providing insights into their shortcomings to guide future research ( Section 6 ) , and implementing an easy-to-use research platform ( see our website and code : https : //sites.google.com/view/d4rl-anonymous/ ) are all contributions that further advance this field . D4RL has already facilitated standardized evaluation and benchmarking , and moving beyond reliance on MuJoCo locomotion tasks and datasets obtained from replay buffers of online RL algorithms . This work has been used as a starting point for novel ideas as evidenced by the rapid adoption of D4RL , and the number of papers submitted to this conference that use it as their primary evaluation task such as : - \u201c Offline Policy Optimization with Variance Regularization \u201d - \u201c Uncertainty Weighted Offline Reinforcement Learning \u201d - \u201c Risk-Averse Offline Reinforcement Learning \u201d - \u201c Addressing Distribution Shift in Online Reinforcement Learning with Offline Datasets \u201d - \u201c BRAC+ : Going Deeper with Behavior Regularized Offline Reinforcement Learning \u201d - \u201c Fine-Tuning Offline Reinforcement Learning with Model-Based Policy Optimization \u201d - \u201c EMaQ : Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL \u201d - \u201c OPAL : Offline Primitive Discovery for Accelerating Offline Reinforcement Learning \u201d - \u201c Model-Based Offline Planning \u201d And in NeurIPS 2020 papers : - \u201c Conservative Q-Learning for Offline Reinforcement Learning \u201d . Kumar et.al.- \u201c Model-Based Offline Policy Optimization \u201d . Yu et al.- \u201c Continual Learning of Control Primitives : Skill Discovery via Reset-Games \u201d . Xu et.al. > \u201c Real datasets present many more real-world problems , including : Non-stationary data\u2026 Real policies\u2026 Causal structures\u2026 Reduction from real datasets\u2026 Changing and/or very large action sets\u2026 Non-robotic environments \u201d While we agree that these are important properties of real-world datasets , no single benchmark can cover every real world problem . Our benchmark tasks provide coverage of several important real-world properties that we suspected have a large impact on offline RL and have not previously been evaluated in offline RL . For example , the Adroit/Franka domains contain real policies obtained by teleoperation . We have added a discussion of these properties and the limitations of our benchmark to emphasize their importance and future opportunity in Section 7 . > \u201c While the dataset provided by the authors may be a useful resource ... this paper does not introduce any new or novel ideas . Overall the main contribution of this work is the gathering of offline data in one place \u201d We agree that our primary contribution is not novel methods or analyses , however , you understate our contributions . In the Gym/Franka/Adroit domains , we do use existing datasets , however , for the remaining domains , we either construct datasets for tasks that have not previously been studied in offline RL or in the case of Maze2D/AntMaze , construct novel domains to study specific properties . Appendix C details which datasets we borrowed as part of the construction of this benchmark . Prior to our work , the predominant method for evaluating offline RL algorithms was to generate data from online RL agents on Atari or Gym tasks . As discussed in [ 2 ] and shown in our empirical evaluation , this does not exercise important dimensions of the problem . Our data generation procedures bring attention to important and neglected properties in offline RL ( e.g. , narrow data distributions , multitask data , and non-representable policies ) that have a significant impact on performance and have already guided followup research . For example , recent work [ 1 , 2 ] has provided contradicting evidence on the importance of the data generating distribution in offline RL . D4RL contains multiple domains where this problem can be explored in depth , across a wide variety of evaluated algorithms . Our empirical evaluations in Table 1 reveal clear performance differences under different data generation settings . Finally , we provide a systematic evaluation of 11 RL methods , including state-of-the-art algorithms . Previously , no such extensive evaluation on a common dataset had been done in offline RL . The standardization and thorough evaluation of a benchmark with appealing properties ( including several that you listed as important real-world properties ) is a valuable contribution . [ 1 ] \u201c An Optimistic Perspective on Offline Reinforcement Learning \u201d Agrawal et . al.2020 [ 2 ] \u201c Behavior Regularized Offline Reinforcement Learning \u201d Wu et . al.2019"}, "3": {"review_id": "px0-N3_KjA-3", "review_text": "D4RL : Datasets for Deep Data-Driven Reinforcement Learning review : summarization : In this paper , the authors consider the problems of offline reinforcement learning problems , and has a focus of dataset and shareable code base . While no novel algorithms are proposed in this project , a systematic evaluation of existing algorithms ( offline RL algorithms ) is proposed . Pros : 1.Offline RL is a hot topic this year , with a lot of papers exploring efficient and robust ways to utilize offline collected data . This paper , while using simulated data , provides a general platform to benchmark these algorithms . 2.The project provides a comprehensive evaluation and discussion on existing considerations in offline RL . It provides several interesting directions in offline RL . 3.The paper is well-written . It is very clear what the purpose of the project is , and it is very clear why the authors make the dataset in the way they did . Cons : 1.The dataset is mostly simulated . While I understand the difficulty of collection real-data , it does raise some concerns that a simulated dataset can be generated by researchers themselves . Summary : While the dataset is simulated , I do think there \u2019 s value in the dataset and the shared code-base to facilitate recent progress in offline RL research .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> \u201c The dataset is mostly simulated . While I understand the difficulty of collection real-data , it does raise some concerns that a simulated dataset can be generated by researchers themselves. \u201d Thank you for your comments . We made a very conscious choice in the design of the benchmark to keep everything in simulation , and select domains for which realistic , accurate simulations exist and have been vetted by the research community . We would be open to suggestions which allow both accurate evaluation and a convincing degree of realism in order to improve the benchmark . We would also like some additional clarification on what concerns you have with using a dataset generated by a simulator , and we can add this to the discussion in the main text ."}}