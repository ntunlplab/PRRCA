{"year": "2021", "forum": "D_KeYoqCYC", "title": "Sparse encoding for more-interpretable feature-selecting representations in probabilistic matrix factorization", "decision": "Accept (Poster)", "meta_review": "The authors present a hierarchical factorization of the Poisson matrix and explain why sparcity in the encoder is important for interpretability. The reviewers appreciated the contribution of the paper and highlighted the advantage of such an approach for users. The authors have improved their initial version by adding more detail on inferences and experiments.  The decision is to accept the paper.", "reviews": [{"review_id": "D_KeYoqCYC-0", "review_text": "Summary : The paper proposes a PMF variant that replaces the free-form representation for latent factors from explicit mapping from inputs , to improve interpretability . A reasonable probabilistic formulation with carefully selected priors is provided , but inference is carried out using generic tools , and the method is illustrated on artificial data and a simple comorbidity application . Reasons for score : The proposed model is interesting and well motivated , and the technical details regarding the choice of priors for encouraging sparsity are good and match current recommendations . The model itself is a bit counter-intuitive , explaining a generally desirable property ( latent variables following a reasonably chosen prior distribution , free from additional computational constraints ) as a limitation and proceeds to replace it with a simplified mapping from inputs , but as the mapping itself is well justified in terms of sparsity the overall construct still makes sense . The presentation angle is somewhat narrow and some connections are missed ( e.g.the authors do not explain this as amortizing the inference for the latent variables , but explain the model in terms of encoders/decoders ) , but this is not a major issue . My biggest issue with the paper concerns inference . The description is limited , only referring to a specific algorithm ( ADVI ) without specifying all details ( mean-field vs full-rank approximation ) , and there is no discussion or analysis on how well it works . The authors do say that the algorithm converged fast , but do not show this in any experiment . More importantly , the convergence does not yet guarantee the approximation is good and there are well-known cases for which ADVI does not really work that well . Expanding both the discussion and empirical demonstration of this would be critical . Now you say `` one may use ... '' and `` one can access ... '' with references to specific techniques for evaluating the quality , which gives the impression you have not actually done that . As I presume you implemented the model in Stan ( no point in using ADVI if not ; there are better stochastic VI methods around that are also easier to implement ) , would you be able to compare the inference results against HMC at least in some small-scale problem ? Pros : 1.The idea is insightful and matches well the application needs . 2.The priors match current literature on suggestions for sparsity-inducing priors . Cons : 1.Very limited coverage of inference , which is an important aspect even if carried out by an external software . Both theoretical and empirical evidence is missing , even though methods for evaluating the approximation quality are referred to . 2.Figures 3 and 4 are pretty , but the spherical representation does not seem to add anything here and only results in the plots taking too much space while being slightly more difficult to read . More generally , the comorbidity example is a bit superficial and could have been developed a bit further . Questions : 1 . How was ADVI applied ? 2.How did you check the approximation is good ? Did you apply WAIC/PSIS-LOO or just say that it could be done ? Modifications after discussion : Increased score by one since the revised paper clarifies the missing details on inference and also improves the motivation .", "rating": "7: Good paper, accept", "reply_text": "- * * The model itself is a bit counter-intuitive , explaining a generally desirable property ( latent variables following a reasonably chosen prior distribution , free from additional computational constraints ) as a limitation and proceeds to replace it with a simplified mapping from inputs , but as the mapping itself is well justified in terms of sparsity the overall construct still makes sense . * * We see constraints in statistical problems as generally desirable as they remove ambiguity and add regularization . However , the intent of our method is to only have minimal impact on the generating process implied by the decoder portion of the model ( which corresponds to the standard matrix factorization method ) . Other than the usage of a updated sparsity model , our generating process is the same that is used in standard HPF . In all pure matrix factorization methods , some mapping $ Y \\to \\pi ( \\theta|Y , ... ) $ from data to representation exists , however , is not explicitly specified or necessarily well-posed . In other words , for a given decoding , an encoding may or may not exist that is sparse in an useful way -- regardless of decoder sparsity . The constraint removes this ambiguity completely . Furthermore , in the original HPF , a prior is still placed on the representation which is itself encouraged to be sparse . Hence , in terms of a priori restriction on the representation space , we believe that our method is at least equivalent to that of HPF . - * * The presentation angle is somewhat narrow and some connections are missed ( e.g.the authors do not explain this as amortizing the inference for the latent variables , but explain the model in terms of encoders/decoders ) , but this is not a major issue . * * Thank you for the suggestion on how we should highlight the fact that our method amortizes inference for the latent variables . We are making this point more prominent ( previously it was mentioned in passing under the caption of Fig 1 ) . We have added the following text to our revision in preparation : The distributions of the parameters of the encoder are learned self-consistently with other model parameters . In the process , one is training not only the generative model , but also the subsequent Bayesian inference of mapping data to representation by learning the statistics of the posterior distribution , \\begin { equation } \\theta_u \\vert \\mathbf { y } _u \\sim \\iint \\pi ( \\theta_u \\vert \\mathbf { B } , \\varphi , \\mathbf { y } _u ) d\\mathbf { B } d\\varphi , \\end { equation } where the generative process has been marginalized . In short , the model of Eq.2 uses the marginal posterior distribution of the encoding matrix $ \\mathbf { A } $ to reparameterize this Bayesian inference . Doing so makes it easier to apply the model to new data in order to compute representations . It also allows us to impose desirable constraints on the representations themselves . From our perspective , it is the mathematical connection between matrix factorization methods and autoencoders that is key to thinking of our overall method . The generative ( decoder ) process of a linear autoencoder is exactly a matrix factorization . Hence , by adding an encoding machinery to factorization ( representation inference ) , a matrix factorization model becomes an autoencoder . The first advantage of thinking in terms of encoder/decoder structures is that principled extensions to the method , while retaining the full intrinsic interpretability , become evident . However , these extensions are not the main message behind our manuscript , and we see how they may be confusing the message , so we have reduced prominence of the GAM aspect in the revision . In a forthcoming work we will expand on theoretical aspects of extension of the the method to non-linearity . As the reviewer noted , `` The paper proposes a PMF variant that replaces the free-form representation for latent factors from explicit mapping from inputs , to improve interpretability . '' We are devoting more attention to this main message in our revision ."}, {"review_id": "D_KeYoqCYC-1", "review_text": "* * Description This paper provides a new approach for finding a sparse encoding of count data matrices and hence automatically achieve feature selection . * * Pros The proposed technique is clearly efficient and practical . It identifies a failing in traditional hierarchical Poisson matrix factorisation ( HPF ) and proposes a solution . This approach is tested on real world datasets where its usefulness is demonstrated . * * Cons For me the weakness of the approach is that the solution proposed appears very ad hoc with no probabilistic basis . Particularly in matrix factorisation problems where there is often no easy way to establish ground truth : this often leads to setting hyperparameters arbitrarily . To be fair the authors point to ways of assessing predictive power which might help in this domain . A less ad hoc model might allow a more principled approach to choosing hyperparameters .", "rating": "6: Marginally above acceptance threshold", "reply_text": "- * * For me the weakness of the approach is that the solution proposed appears very ad hoc with no probabilistic basis . Particularly in matrix factorisation problems where there is often no easy way to establish ground truth : this often leads to setting hyperparameters arbitrarily . To be fair the authors point to ways of assessing predictive power which might help in this domain . A less ad hoc model might allow a more principled approach to choosing hyperparameters . * * Thank you for your critique of the method . We think we can help address your concern by relating our method to other matrix factorization approaches like PCA and SVD . It is true that the unsupervised problem lacks a ground truth . In our view , the objective of methods like PCA and SVD is to find a useful parameterization of the data in fewer dimensions , that retains most of the data 's variability . PCA and SVD do so under a Gaussian noise model . HPF and our variant of HPF do so under a Poisson model . The question then is whether such a parameterization is useful - the answer to this question is problem-dependent and the general solution outside of the scope of our manuscript . However , in our revision we are expanding our exposition of the comorbidity application to better-describe how the output of our method can be used downstream in analysis . In our case , the method is useful because it first reduces the dimensionality of the data and second makes coarse-graining of the data , by grouping like datapoints together , more tractable . All this is done while maintaining interpretability of the overall model in terms of the original data features , at all times . As to hyperparameters , the parameters within the model can influence the result . However , we have carefully tuned the parameters using standard Bayesian considerations . For instance , the scaling on the background process is set to a somewhat large multiple of the average value for each feature in the dataset . This type of prior is known as a weakly-informative prior in the Bayesian literature . We set the scaling of the regularization so that the variance of the priors is invariant with data size . This setting yielded the results demonstrated in Fig 2 -- in particular we are able to replicate the same results on matrices of different sizes , using the scalings provided in our manuscript . In that respect , there are few hyperparameters that need to be tuned ."}, {"review_id": "D_KeYoqCYC-2", "review_text": "This manuscript revisits the hierarchical Poisson matrix factorization ( HPF ) promoting sparsity in both the representation and the decoding function with a Horseshoe+ prior . Sparsity on both sides is put forward for interpretation purposes , and to create a column-clustering property . In addition , the proposed approach caters for non-linear decoding using a GAM model . The main practical benefit compared to sparsifying priors ( as in Gopalan et al . ( 2014 ) ) is that the contributed method can be solved with automatic differentiation variational inference and hence stochastic solvers , which makes it in theory easier to scale , though this improvement is not demonstrated empirically in the manuscript . The manuscript is well written : precise and well articulated . It develops well the theoretical point that sparsity in encoding and decoding is important . However , the practical value of the contribution is not strongly demonstrated . In the real-life application , on comorbidity data , the sparsity is demonstrated as expected , but the benefit compared to other approaches is difficult to gauge , whether it is to a non-sparse approach , or an approach based on sparsifying priors . The benefit of the GAM decoding is not clear . While there are good theoretical arguments for the model , they only partly convince in practical terms : a full analytics pipeline has made aspects to it , and the arguments might not be as important as they seem in practice . It could help to perform more empirical comparison , and to study more the contribution in the context of full analyses . The empirical demonstrations show that the model exhibit the properties that it was designed for : sparsity in the decoding . What are practical consequences of these properties in real-life applications ? In the bigger picture , it is unclear to see how this contribution positions itself in terms of practical benefits in the vast literature on latent factors with distangling approaches ( distangling autoencoders , various matrix factorizations including NMF with different losses ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank you for the interesting perspectives on our paper that we had not fully appreciated . Our focus is on interpretability - in particular addressing a gap between how similar factorization methods are interpreted versus what the models actually say . This gap appears superficially subtle but we believe it has large consequence . For this reason , we would like to refrain from straying too much from this central message . That said , we will address your comments in our revision that we will post this weekend . In the meantime , please see below for our responses . - * * The main practical benefit compared to sparsifying priors ( as in Gopalan et al . ( 2014 ) ) is that the contributed method can be solved with automatic differentiation variational inference and hence stochastic solvers , which makes it in theory easier to scale , though this improvement is not demonstrated empirically in the manuscript . * * Interestingly , it was issues with the scaling of HPF that lead us to first create our method . For HPF , the posterior distribution of an UxK representation matrix is learned at training . When we initially implemented HPF in custom Tensorflow code , we found that we could not scale it to suit our problem ( to fit into GPU memory ) where U was on the order of tens of millions and K is sometimes on the order of tens to hundreds . We also did not see a clean way of performing batch inference on this problem . In standard HPF , the representation matrix is itself a model parameter . In minibatch optimization , one avoids storing the entire dataset in GPU memory . However , one still needs to store the parameters for the model ( including the representation for HPF ) because they are what is updated in each iteration of inference . In our revision we are expanding a bit on this theoretical discussion . We should note that the package hpfrec , that we use in our comparisons , has implemented a nonstandard minibatch algorithm for standard HPF . Regardless , it soon became clear to us that interpretability of HPF was most lacking . We would like the focus of our manuscript to be on interpretability . Even if HPF is faster , through various tunings and hacks for implementing minibatch training , it does n't provide what our method provides in terms of interpretability . That said , we will expand more on why having a constrained encoder transform leads to more memory-efficient training . It is not however an apples to apples comparison when contrasting the computational cost of our method against that of standard HPF . This is mainly because we are using an updated sparsity model whereas HPF uses gamma distributions . For this reason , HPF has the advantage of possessing exact variational updates whereas we use stochastic gradient descent . Additionally , hpfrec , which we use for comparisons , is implemented in CPU code and our implementation is GPU . For these reasons , we hope that the reviewer will be sympathetic to our perspective that a direct benchmark comparison of the two algorithms is of limited use ."}], "0": {"review_id": "D_KeYoqCYC-0", "review_text": "Summary : The paper proposes a PMF variant that replaces the free-form representation for latent factors from explicit mapping from inputs , to improve interpretability . A reasonable probabilistic formulation with carefully selected priors is provided , but inference is carried out using generic tools , and the method is illustrated on artificial data and a simple comorbidity application . Reasons for score : The proposed model is interesting and well motivated , and the technical details regarding the choice of priors for encouraging sparsity are good and match current recommendations . The model itself is a bit counter-intuitive , explaining a generally desirable property ( latent variables following a reasonably chosen prior distribution , free from additional computational constraints ) as a limitation and proceeds to replace it with a simplified mapping from inputs , but as the mapping itself is well justified in terms of sparsity the overall construct still makes sense . The presentation angle is somewhat narrow and some connections are missed ( e.g.the authors do not explain this as amortizing the inference for the latent variables , but explain the model in terms of encoders/decoders ) , but this is not a major issue . My biggest issue with the paper concerns inference . The description is limited , only referring to a specific algorithm ( ADVI ) without specifying all details ( mean-field vs full-rank approximation ) , and there is no discussion or analysis on how well it works . The authors do say that the algorithm converged fast , but do not show this in any experiment . More importantly , the convergence does not yet guarantee the approximation is good and there are well-known cases for which ADVI does not really work that well . Expanding both the discussion and empirical demonstration of this would be critical . Now you say `` one may use ... '' and `` one can access ... '' with references to specific techniques for evaluating the quality , which gives the impression you have not actually done that . As I presume you implemented the model in Stan ( no point in using ADVI if not ; there are better stochastic VI methods around that are also easier to implement ) , would you be able to compare the inference results against HMC at least in some small-scale problem ? Pros : 1.The idea is insightful and matches well the application needs . 2.The priors match current literature on suggestions for sparsity-inducing priors . Cons : 1.Very limited coverage of inference , which is an important aspect even if carried out by an external software . Both theoretical and empirical evidence is missing , even though methods for evaluating the approximation quality are referred to . 2.Figures 3 and 4 are pretty , but the spherical representation does not seem to add anything here and only results in the plots taking too much space while being slightly more difficult to read . More generally , the comorbidity example is a bit superficial and could have been developed a bit further . Questions : 1 . How was ADVI applied ? 2.How did you check the approximation is good ? Did you apply WAIC/PSIS-LOO or just say that it could be done ? Modifications after discussion : Increased score by one since the revised paper clarifies the missing details on inference and also improves the motivation .", "rating": "7: Good paper, accept", "reply_text": "- * * The model itself is a bit counter-intuitive , explaining a generally desirable property ( latent variables following a reasonably chosen prior distribution , free from additional computational constraints ) as a limitation and proceeds to replace it with a simplified mapping from inputs , but as the mapping itself is well justified in terms of sparsity the overall construct still makes sense . * * We see constraints in statistical problems as generally desirable as they remove ambiguity and add regularization . However , the intent of our method is to only have minimal impact on the generating process implied by the decoder portion of the model ( which corresponds to the standard matrix factorization method ) . Other than the usage of a updated sparsity model , our generating process is the same that is used in standard HPF . In all pure matrix factorization methods , some mapping $ Y \\to \\pi ( \\theta|Y , ... ) $ from data to representation exists , however , is not explicitly specified or necessarily well-posed . In other words , for a given decoding , an encoding may or may not exist that is sparse in an useful way -- regardless of decoder sparsity . The constraint removes this ambiguity completely . Furthermore , in the original HPF , a prior is still placed on the representation which is itself encouraged to be sparse . Hence , in terms of a priori restriction on the representation space , we believe that our method is at least equivalent to that of HPF . - * * The presentation angle is somewhat narrow and some connections are missed ( e.g.the authors do not explain this as amortizing the inference for the latent variables , but explain the model in terms of encoders/decoders ) , but this is not a major issue . * * Thank you for the suggestion on how we should highlight the fact that our method amortizes inference for the latent variables . We are making this point more prominent ( previously it was mentioned in passing under the caption of Fig 1 ) . We have added the following text to our revision in preparation : The distributions of the parameters of the encoder are learned self-consistently with other model parameters . In the process , one is training not only the generative model , but also the subsequent Bayesian inference of mapping data to representation by learning the statistics of the posterior distribution , \\begin { equation } \\theta_u \\vert \\mathbf { y } _u \\sim \\iint \\pi ( \\theta_u \\vert \\mathbf { B } , \\varphi , \\mathbf { y } _u ) d\\mathbf { B } d\\varphi , \\end { equation } where the generative process has been marginalized . In short , the model of Eq.2 uses the marginal posterior distribution of the encoding matrix $ \\mathbf { A } $ to reparameterize this Bayesian inference . Doing so makes it easier to apply the model to new data in order to compute representations . It also allows us to impose desirable constraints on the representations themselves . From our perspective , it is the mathematical connection between matrix factorization methods and autoencoders that is key to thinking of our overall method . The generative ( decoder ) process of a linear autoencoder is exactly a matrix factorization . Hence , by adding an encoding machinery to factorization ( representation inference ) , a matrix factorization model becomes an autoencoder . The first advantage of thinking in terms of encoder/decoder structures is that principled extensions to the method , while retaining the full intrinsic interpretability , become evident . However , these extensions are not the main message behind our manuscript , and we see how they may be confusing the message , so we have reduced prominence of the GAM aspect in the revision . In a forthcoming work we will expand on theoretical aspects of extension of the the method to non-linearity . As the reviewer noted , `` The paper proposes a PMF variant that replaces the free-form representation for latent factors from explicit mapping from inputs , to improve interpretability . '' We are devoting more attention to this main message in our revision ."}, "1": {"review_id": "D_KeYoqCYC-1", "review_text": "* * Description This paper provides a new approach for finding a sparse encoding of count data matrices and hence automatically achieve feature selection . * * Pros The proposed technique is clearly efficient and practical . It identifies a failing in traditional hierarchical Poisson matrix factorisation ( HPF ) and proposes a solution . This approach is tested on real world datasets where its usefulness is demonstrated . * * Cons For me the weakness of the approach is that the solution proposed appears very ad hoc with no probabilistic basis . Particularly in matrix factorisation problems where there is often no easy way to establish ground truth : this often leads to setting hyperparameters arbitrarily . To be fair the authors point to ways of assessing predictive power which might help in this domain . A less ad hoc model might allow a more principled approach to choosing hyperparameters .", "rating": "6: Marginally above acceptance threshold", "reply_text": "- * * For me the weakness of the approach is that the solution proposed appears very ad hoc with no probabilistic basis . Particularly in matrix factorisation problems where there is often no easy way to establish ground truth : this often leads to setting hyperparameters arbitrarily . To be fair the authors point to ways of assessing predictive power which might help in this domain . A less ad hoc model might allow a more principled approach to choosing hyperparameters . * * Thank you for your critique of the method . We think we can help address your concern by relating our method to other matrix factorization approaches like PCA and SVD . It is true that the unsupervised problem lacks a ground truth . In our view , the objective of methods like PCA and SVD is to find a useful parameterization of the data in fewer dimensions , that retains most of the data 's variability . PCA and SVD do so under a Gaussian noise model . HPF and our variant of HPF do so under a Poisson model . The question then is whether such a parameterization is useful - the answer to this question is problem-dependent and the general solution outside of the scope of our manuscript . However , in our revision we are expanding our exposition of the comorbidity application to better-describe how the output of our method can be used downstream in analysis . In our case , the method is useful because it first reduces the dimensionality of the data and second makes coarse-graining of the data , by grouping like datapoints together , more tractable . All this is done while maintaining interpretability of the overall model in terms of the original data features , at all times . As to hyperparameters , the parameters within the model can influence the result . However , we have carefully tuned the parameters using standard Bayesian considerations . For instance , the scaling on the background process is set to a somewhat large multiple of the average value for each feature in the dataset . This type of prior is known as a weakly-informative prior in the Bayesian literature . We set the scaling of the regularization so that the variance of the priors is invariant with data size . This setting yielded the results demonstrated in Fig 2 -- in particular we are able to replicate the same results on matrices of different sizes , using the scalings provided in our manuscript . In that respect , there are few hyperparameters that need to be tuned ."}, "2": {"review_id": "D_KeYoqCYC-2", "review_text": "This manuscript revisits the hierarchical Poisson matrix factorization ( HPF ) promoting sparsity in both the representation and the decoding function with a Horseshoe+ prior . Sparsity on both sides is put forward for interpretation purposes , and to create a column-clustering property . In addition , the proposed approach caters for non-linear decoding using a GAM model . The main practical benefit compared to sparsifying priors ( as in Gopalan et al . ( 2014 ) ) is that the contributed method can be solved with automatic differentiation variational inference and hence stochastic solvers , which makes it in theory easier to scale , though this improvement is not demonstrated empirically in the manuscript . The manuscript is well written : precise and well articulated . It develops well the theoretical point that sparsity in encoding and decoding is important . However , the practical value of the contribution is not strongly demonstrated . In the real-life application , on comorbidity data , the sparsity is demonstrated as expected , but the benefit compared to other approaches is difficult to gauge , whether it is to a non-sparse approach , or an approach based on sparsifying priors . The benefit of the GAM decoding is not clear . While there are good theoretical arguments for the model , they only partly convince in practical terms : a full analytics pipeline has made aspects to it , and the arguments might not be as important as they seem in practice . It could help to perform more empirical comparison , and to study more the contribution in the context of full analyses . The empirical demonstrations show that the model exhibit the properties that it was designed for : sparsity in the decoding . What are practical consequences of these properties in real-life applications ? In the bigger picture , it is unclear to see how this contribution positions itself in terms of practical benefits in the vast literature on latent factors with distangling approaches ( distangling autoencoders , various matrix factorizations including NMF with different losses ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank you for the interesting perspectives on our paper that we had not fully appreciated . Our focus is on interpretability - in particular addressing a gap between how similar factorization methods are interpreted versus what the models actually say . This gap appears superficially subtle but we believe it has large consequence . For this reason , we would like to refrain from straying too much from this central message . That said , we will address your comments in our revision that we will post this weekend . In the meantime , please see below for our responses . - * * The main practical benefit compared to sparsifying priors ( as in Gopalan et al . ( 2014 ) ) is that the contributed method can be solved with automatic differentiation variational inference and hence stochastic solvers , which makes it in theory easier to scale , though this improvement is not demonstrated empirically in the manuscript . * * Interestingly , it was issues with the scaling of HPF that lead us to first create our method . For HPF , the posterior distribution of an UxK representation matrix is learned at training . When we initially implemented HPF in custom Tensorflow code , we found that we could not scale it to suit our problem ( to fit into GPU memory ) where U was on the order of tens of millions and K is sometimes on the order of tens to hundreds . We also did not see a clean way of performing batch inference on this problem . In standard HPF , the representation matrix is itself a model parameter . In minibatch optimization , one avoids storing the entire dataset in GPU memory . However , one still needs to store the parameters for the model ( including the representation for HPF ) because they are what is updated in each iteration of inference . In our revision we are expanding a bit on this theoretical discussion . We should note that the package hpfrec , that we use in our comparisons , has implemented a nonstandard minibatch algorithm for standard HPF . Regardless , it soon became clear to us that interpretability of HPF was most lacking . We would like the focus of our manuscript to be on interpretability . Even if HPF is faster , through various tunings and hacks for implementing minibatch training , it does n't provide what our method provides in terms of interpretability . That said , we will expand more on why having a constrained encoder transform leads to more memory-efficient training . It is not however an apples to apples comparison when contrasting the computational cost of our method against that of standard HPF . This is mainly because we are using an updated sparsity model whereas HPF uses gamma distributions . For this reason , HPF has the advantage of possessing exact variational updates whereas we use stochastic gradient descent . Additionally , hpfrec , which we use for comparisons , is implemented in CPU code and our implementation is GPU . For these reasons , we hope that the reviewer will be sympathetic to our perspective that a direct benchmark comparison of the two algorithms is of limited use ."}}