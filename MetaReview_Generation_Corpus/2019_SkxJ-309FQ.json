{"year": "2019", "forum": "SkxJ-309FQ", "title": "Hallucinations in Neural Machine Translation", "decision": "Reject", "meta_review": "Strengths\n\n-  Hallucinations are a problem for seq2seq models, esp trained on small datasets\n\nWeankesses\n\n- Hallucinations are known to exists, the analyses / observations are not very novel \n\n- The considered space of hallucinations source (i.e. added noise) is fairly limited, it is not clear that these are the most natural sources of hallucination and not clear if the methods defined to combat these types would generalize to other types. E.g., I'd rather see hallucinations appearing when running NMT on some natural (albeit noisy) corpus, rather than defining the noise model manually.\n\n-  The proposed approach is not particularly interesting, and may not be general. Alternative techniques (e.g., modeling coverage) have been proposed in the past. \n\n-  A wider variety of language pairs, amounts of data, etc needed to validate the methods. This is an empirical paper, I would expect higher quality of evaluation.\n\nTwo reviewers argued that the baseline system is somewhat weak and the method is not very exciting. \n\n\n", "reviews": [{"review_id": "SkxJ-309FQ-0", "review_text": "I think this paper conducts several interesting analysis about MT hallucinations and also proposes several different ways of reducing this effect. My questions are as follows: * I am very curious about how do you decide the chosen noisy words. I am also wondering what is the difference if you do choose different noisy words. Another thing, if the noisy words are unseen in the training set, will it be treated as \"UNK\"? * Can you highlight what is changed in the upper right side of fig.4? It would be great if you include gloss in the figure as well.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you ! At your request , we have updated figure 4 to make it more clear what each part represents . We 've also added to the caption to explain what the differences between the two attention matrices are . Below , we 've expanded more on the questions you 've raised . 1.We chose subword tokens ( segmented with byte pair encoding ) from our source language ( German ) vocabulary so we never have a noisy word that \u2019 s unseen in the training set . We \u2019 ve described how we develop our source , subword vocabulary in section 3 at the bottom of page 3 . We chose these tokens as representative of the distribution of tokens : The specific tokens we \u2019 ve chosen are based on one of four types of subword tokens : common , rare , mid-frequency , and punctuation tokens . We first sorted our vocabulary of subword tokens by frequency , then formed the following groups : a . Common tokens : the 100 most common tokens b . Rare tokens : the 100 least common tokens c. Mid-frequency tokens : After removing common and rare tokens from our sorted vocabulary of subword tokens , we sample 100 random tokens . d. Punctuation tokens : All punctuation marks that exist in the vocabulary . ( This selection process is described in the first paragraph of section 4 . ) Since we use BPE encoding , which segments words into sub-word units depending on their frequencies ( character level co-occurrences to be precise ) , unseen words are never treated as UNK tokens . If a word does not appear in the training set , the BPE algorithm will segment it into the sub-words or characters that appear in our final vocabulary instead of using the UNK token . 2.Here is a further explanation of the difference in the upper right of figure 4 compared to the upper left . The attention matrix shows the attention weight applied to each input token in the source sentence ( x-axis ) as the model decodes and outputs the translated sentence ( y-axis ) . On the upper left , we show the attention matrix of an unperturbed translation . We see weight is applied to most of the input source tokens . On the upper right , we show the attention matrix of the same source sentence , but with a perturbation at the beginning ( \u2018 und \u2019 ) that causes the translation to hallucinate . We observe that weight is applied to very few input source tokens throughout translation , which is highly atypical and indicative of a broken translation ."}, {"review_id": "SkxJ-309FQ-1", "review_text": " My major concern about the work is that the studied model is quite weak. \"All models we present are well trained with a BLEU score of at least 20.0 on the test set, a reasonable score for 2-layer models with 256 hidden units.\" \"We then used the WMT De!En 2016 test set (2,999 examples) to compute the hallucination percentage for each model.\" I checked the WMT official website http://matrix.statmt.org/matrix. It shows that the best result was a BLEU score of 40.2, which was obtained at 2016. The models used in this work are about 20.0, which are much less than the WMT results reported two years ago. Note that neural machine translation has made remarkable progress in recent two years, not to mention that production systems like Google translator perform much better than research systems. Therefore, the discoveries reported in this work are questionable. I strongly suggest the authors to conduct the studies base on the latest NMT architecture, i.e., Transformer. Furthermore, I checked the examples given in introduction in Google translator and found no hallucination. So I'm not sure whether such hallucinations are really critical to today's NMT systems. I'd like to see that the study on some production translation systems, e.g., applying Algo 1 to Google translator and check its outputs, which can better motivate this work. For the analysis in Section 6.1, if attention is the root cause of hallucinations, some existing methods should have already address this issue. Can you check whether the model trained by the following work still suffers from hallucinations? Modeling Coverage for Neural Machine Translation, ACL 16.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your feedback . As per your suggestions , we added the following additional models and experiments , resulting in the following changes . * The BLEU scores we reported in the paper are with greedy decoding . Since the NMT community frequently reports BLEU with beam search , we have updated our paper to reflect this . Our canonical model achieves a competitive BLEU score of 25.6 on newstest16 ( https : //github.com/tensorflow/nmt # wmt-german-english ) . We now report this in the paper . * We added a Transformer model to our results . * We perturbed the Transformer model to hallucinate and found that it hallucinates on average ( over ten random seeds ) 16.6 % of the time ( there exists a token such that 16.6 % of source sentences can be made to a hallucinate ) . We expand on and give a discussion of the Transformer model we used in the paper . * You are right that coverage would be an interesting model variant to look at . We ran a coverage study and found that coverage with beam search hallucinates on average 49.4 % of the time , whereas beam search hallucinates 48.2 % of the time and greedy decoding hallucinates 73.3 % of the time . We expand more on why we chose this version of coverage below . * Finally , we correlated BLEU score to perturbation percentages and did not see a decrease in perturbation percentage as BLEU score increased . We have added an additional figure to show this . The data shows that there is a correlation coefficient of 0.33 between BLEU score and hallucination percentage . This correlation should be interpreted cautiously because we haven \u2019 t exhaustively explored the full space of models . Our paper is an analysis paper . Our goals are to quantify the phenomenon of hallucinations and explore what this tells us about training and using NMT models . To make these goals technically feasible , we extract the core NMT neural network model from the layers of techniques and fail-safes in production systems and SoTA-level competition entries . To make these goals technically tractable , we scale down the bare model which allows us to study as many hyperparameters , architectural variants , and random seeds over the thousand+ models we studied . Results we find on small models are not irrelevant . Our canonical models train to an average BLEU score of 25.6 , competitive for its size , and we show that an increase in BLEU score does not correlate to a decrease in the percentage of hallucinations . In the next comment , we 'll expand further on our decisions ."}, {"review_id": "SkxJ-309FQ-2", "review_text": "The authors introduce hallucinations in NMT and propose some algorithms to avoid them. The paper is clear (except section 6.2, which could have been more clearly described) and the work is original. The paper points out hallucination problems in NMT which looks like adversarial examples in the paper \"Explaining and Harnessing Adversarial Examples\". So, the authors might want to compare the perturbed sources to the adversarial examples. If analysis is provided for each hallucination patten, that would be better. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your feedback ! We 're glad you find this exploration interesting . We 've given some thought to how hallucinations compare to adversarial examples . Like adversarial examples , hallucinations illustrate a form of instability in models which can be useful to understand why the model behaves a particular way and help propose ideas for improving stability and generalizability of models . One difference is that we are n't looking for worst case perturbations ( or to perturb an input to a particular result ) , nor do we use gradient based methods . We show it is simple to find a perturbation that causes such a divergent hallucination . So we have similar motivations , but go about it in different ways ."}], "0": {"review_id": "SkxJ-309FQ-0", "review_text": "I think this paper conducts several interesting analysis about MT hallucinations and also proposes several different ways of reducing this effect. My questions are as follows: * I am very curious about how do you decide the chosen noisy words. I am also wondering what is the difference if you do choose different noisy words. Another thing, if the noisy words are unseen in the training set, will it be treated as \"UNK\"? * Can you highlight what is changed in the upper right side of fig.4? It would be great if you include gloss in the figure as well.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you ! At your request , we have updated figure 4 to make it more clear what each part represents . We 've also added to the caption to explain what the differences between the two attention matrices are . Below , we 've expanded more on the questions you 've raised . 1.We chose subword tokens ( segmented with byte pair encoding ) from our source language ( German ) vocabulary so we never have a noisy word that \u2019 s unseen in the training set . We \u2019 ve described how we develop our source , subword vocabulary in section 3 at the bottom of page 3 . We chose these tokens as representative of the distribution of tokens : The specific tokens we \u2019 ve chosen are based on one of four types of subword tokens : common , rare , mid-frequency , and punctuation tokens . We first sorted our vocabulary of subword tokens by frequency , then formed the following groups : a . Common tokens : the 100 most common tokens b . Rare tokens : the 100 least common tokens c. Mid-frequency tokens : After removing common and rare tokens from our sorted vocabulary of subword tokens , we sample 100 random tokens . d. Punctuation tokens : All punctuation marks that exist in the vocabulary . ( This selection process is described in the first paragraph of section 4 . ) Since we use BPE encoding , which segments words into sub-word units depending on their frequencies ( character level co-occurrences to be precise ) , unseen words are never treated as UNK tokens . If a word does not appear in the training set , the BPE algorithm will segment it into the sub-words or characters that appear in our final vocabulary instead of using the UNK token . 2.Here is a further explanation of the difference in the upper right of figure 4 compared to the upper left . The attention matrix shows the attention weight applied to each input token in the source sentence ( x-axis ) as the model decodes and outputs the translated sentence ( y-axis ) . On the upper left , we show the attention matrix of an unperturbed translation . We see weight is applied to most of the input source tokens . On the upper right , we show the attention matrix of the same source sentence , but with a perturbation at the beginning ( \u2018 und \u2019 ) that causes the translation to hallucinate . We observe that weight is applied to very few input source tokens throughout translation , which is highly atypical and indicative of a broken translation ."}, "1": {"review_id": "SkxJ-309FQ-1", "review_text": " My major concern about the work is that the studied model is quite weak. \"All models we present are well trained with a BLEU score of at least 20.0 on the test set, a reasonable score for 2-layer models with 256 hidden units.\" \"We then used the WMT De!En 2016 test set (2,999 examples) to compute the hallucination percentage for each model.\" I checked the WMT official website http://matrix.statmt.org/matrix. It shows that the best result was a BLEU score of 40.2, which was obtained at 2016. The models used in this work are about 20.0, which are much less than the WMT results reported two years ago. Note that neural machine translation has made remarkable progress in recent two years, not to mention that production systems like Google translator perform much better than research systems. Therefore, the discoveries reported in this work are questionable. I strongly suggest the authors to conduct the studies base on the latest NMT architecture, i.e., Transformer. Furthermore, I checked the examples given in introduction in Google translator and found no hallucination. So I'm not sure whether such hallucinations are really critical to today's NMT systems. I'd like to see that the study on some production translation systems, e.g., applying Algo 1 to Google translator and check its outputs, which can better motivate this work. For the analysis in Section 6.1, if attention is the root cause of hallucinations, some existing methods should have already address this issue. Can you check whether the model trained by the following work still suffers from hallucinations? Modeling Coverage for Neural Machine Translation, ACL 16.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your feedback . As per your suggestions , we added the following additional models and experiments , resulting in the following changes . * The BLEU scores we reported in the paper are with greedy decoding . Since the NMT community frequently reports BLEU with beam search , we have updated our paper to reflect this . Our canonical model achieves a competitive BLEU score of 25.6 on newstest16 ( https : //github.com/tensorflow/nmt # wmt-german-english ) . We now report this in the paper . * We added a Transformer model to our results . * We perturbed the Transformer model to hallucinate and found that it hallucinates on average ( over ten random seeds ) 16.6 % of the time ( there exists a token such that 16.6 % of source sentences can be made to a hallucinate ) . We expand on and give a discussion of the Transformer model we used in the paper . * You are right that coverage would be an interesting model variant to look at . We ran a coverage study and found that coverage with beam search hallucinates on average 49.4 % of the time , whereas beam search hallucinates 48.2 % of the time and greedy decoding hallucinates 73.3 % of the time . We expand more on why we chose this version of coverage below . * Finally , we correlated BLEU score to perturbation percentages and did not see a decrease in perturbation percentage as BLEU score increased . We have added an additional figure to show this . The data shows that there is a correlation coefficient of 0.33 between BLEU score and hallucination percentage . This correlation should be interpreted cautiously because we haven \u2019 t exhaustively explored the full space of models . Our paper is an analysis paper . Our goals are to quantify the phenomenon of hallucinations and explore what this tells us about training and using NMT models . To make these goals technically feasible , we extract the core NMT neural network model from the layers of techniques and fail-safes in production systems and SoTA-level competition entries . To make these goals technically tractable , we scale down the bare model which allows us to study as many hyperparameters , architectural variants , and random seeds over the thousand+ models we studied . Results we find on small models are not irrelevant . Our canonical models train to an average BLEU score of 25.6 , competitive for its size , and we show that an increase in BLEU score does not correlate to a decrease in the percentage of hallucinations . In the next comment , we 'll expand further on our decisions ."}, "2": {"review_id": "SkxJ-309FQ-2", "review_text": "The authors introduce hallucinations in NMT and propose some algorithms to avoid them. The paper is clear (except section 6.2, which could have been more clearly described) and the work is original. The paper points out hallucination problems in NMT which looks like adversarial examples in the paper \"Explaining and Harnessing Adversarial Examples\". So, the authors might want to compare the perturbed sources to the adversarial examples. If analysis is provided for each hallucination patten, that would be better. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your feedback ! We 're glad you find this exploration interesting . We 've given some thought to how hallucinations compare to adversarial examples . Like adversarial examples , hallucinations illustrate a form of instability in models which can be useful to understand why the model behaves a particular way and help propose ideas for improving stability and generalizability of models . One difference is that we are n't looking for worst case perturbations ( or to perturb an input to a particular result ) , nor do we use gradient based methods . We show it is simple to find a perturbation that causes such a divergent hallucination . So we have similar motivations , but go about it in different ways ."}}