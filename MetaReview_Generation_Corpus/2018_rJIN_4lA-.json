{"year": "2018", "forum": "rJIN_4lA-", "title": "Maintaining cooperation in complex social dilemmas using deep reinforcement learning", "decision": "Reject", "meta_review": "The reviewers found numerous issues in the paper, including unclear problem definitions, lack of motivation, no support for desiderata, clarity issues, points in discussion appearing to be technically incorrect, restrictive setting, sloppy definitions, and uninteresting experiments.  Unfortunately, little note of positive aspects was mentioned.  The authors wrote substantial rebuttals, including an extended exchange with Reviewer2, but this had no effect in terms of score changes. Given the current state of the paper, the committee feels the paper falls short of acceptance in its current form.", "reviews": [{"review_id": "rJIN_4lA--0", "review_text": "This paper addresses multiagent learning problems in which there is a social dilemma: settings where there are no 'cooperative polices' that form an equilibrium. The paper proposes a way of dealing with these problems via amTFT, a variation of the well-known tit-for-that strategy, and presents some empirical results. My main problem with this paper is clarity and I am afraid that not everything might be technically correct. Let me just list my main concerns in the below. The definition of social dilemma, is unclear: \"A social dilemma is a game where there are no cooperative policies which form equilibria. In other words, if one player commits to play a cooperative policy at every state, there is a way for their partner to exploit them and earn higher rewards at their expense.\" does this mean to say \"there are no cooperative *Markov* policies\" ? It seems to me that the paper precisely intents to show that by resorting to history-dependent policies (such as both using amTFT), there is a cooperative equilibrium. I don't understand: \"Note that in a social dilemma there may be policies which achieve the payoffs of cooperative policies because they cooperate on the trajectory of play and prevent exploitation by threatening non-cooperation on states which are never reached by the trajectory. If such policies exist, we call the social dilemma solvable.\" is this now talking about non-Markov policies? If not, there seems to be a contradiction? The work focuses on TFT-like policies, motivated by \"if one can commit to them, create incentives for a partner to behave cooperatively\" however it seems that, as made clear below definition 4, we can only create such incentives for sufficiently powerful agents, that remember and learn from their failures to cooperate in the past? Why is the method called \"approximate Markov\"? As soon as one introduces history dependence, the Markov property stops to hold? On page 4, I have problems following the text due to inconsistent use of notation: subscripts and superscripts seem random, it is not clear which symbols denote strategy profiles (rather than individual strategies), there seems mix-ups between 'i' and '1' / '2', there is sudden use of \\hat{}, and other undefined symbols (Q_CC?). For all practical purposes, it seems that the made assumptions imply uniqueness of the cooperative joint strategy. I fully appreciate that the coordination question is difficult and important, so if the proposed method is not compatible with dealing with that important question, that strikes me as a large drawback. I have problems understanding how it is possible to guarantee \"If they start in a D phase, they eventually return to a C phase.\" without making more assumptions on the domain. The clear example being the typical 'heaven or hell' type of problems: what if after one defect, we are trapped in the 'hell' state where no cooperation is even possible? \"If policies converge with this training then \u03c0\u02c6 is a Markov equilibrium (up to function approximation).\" There are two problems here: 1) A problem is that very typically things will not converge... E.g., Wunder, Michael, Michael L. Littman, and Monica Babes. \"Classes of multiagent q-learning dynamics with epsilon-greedy exploration.\" Proceedings of the 27th International Conference on Machine Learning (ICML-10). 2010. 2) \"Up to function approximation\" could be arbitrary large? Another significant problem seems to be with this statement: \"while in the cooperative reward schedule the standard RL convergence guarantees apply. The latter is because cooperative training is equivalent to one super-agent controlling both players and trying to optimize for a single scalar reward.\" The training of individual learners is quite different from \"joint action learners\" [Claus & Boutilier 98], and this in turn is different from a 'super-agent' which would also control the exploration. In absence of the super-agent, I believe that the only guarantee is that one will, in the limit, converge to a Nash equilibrum, which might be arbitrary far from the optimal joint policy. And this only holds for the tabular case. See the discussion in A concise introduction to multiagent systems and distributed artificial intelligence. N Vlassis. Synthesis Lectures on Artificial Intelligence and Machine Learning 1 (1), 1-71 Also, the approach used in the experiments \"Cooperative (self play with both agents receiving sum of rewards) training for both games\", would be insufficient for many settings where a cooperative joint policy would be asymmetric. The entire approach hinges on using rollouts (the commented lines in Algo. 1). However, it is completely not clear to me how this works. The one paragraph is insufficient to get across these crucial parts of the proposed approach. It is not clear why the tables in Figure 1 are not symmetric; this strikes me as extremely problematic. It is not clear what the colors encode either. It also seems that \"grim\" is better against all, except against amTFT, why should we not use that? In general, the explanation of this closely related paper by De Cote & Littman (which was published at UAI'08), is insufficient. It is not quite clear to me what the proposed approach offers over the previous method. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for pointing out several important issues . We believe these are mostly issues of clarity in exposition/notation . We have edited the text to address these issues . > > The definition of social dilemma , is unclear : `` A social dilemma is a game where there are no cooperative policies which form equilibria\u2026 . does this mean to say `` there are no cooperative * Markov * policies '' ? * * The referee is correct , this should mean that there are no Markov policies . Note that when we refer to cooperative policies we specifically refer to ones which cooperate at ALL states . Thus we define a social dilemma to be one where cooperation after EVERY STATE is impossible in equilibrium \u2013 that is , if there is cooperation along the path of play it must be because OFF THE PATH of play cooperation stops . This is identical to the logic in the standard repeated Prisoner \u2019 s Dilemma where policies which always cooperate are not equilibria , rather , in order to maintain cooperation along the path of play there must be defection off the path of play ( eg.Grim Trigger ) . We have edited the text to make this point clearer . * * > > Why is the method called `` approximate Markov '' ? As soon as one introduces history dependence , the Markov property stops to hold ? * * We call the method approximate Markov because we use function approximation ( approximate ) and because amTFT only uses Markov policies from the original game ( only using the augmented memory to switch between them ) . We have made this clearer in the paper . * * > > On page 4 , I have problems following the text due to inconsistent use of notation : subscripts and superscripts seem random , it is not clear which symbols denote strategy profiles ( rather than individual strategies ) , there seems mix-ups between ' i ' and ' 1 ' / ' 2 ' , there is sudden use of \\hat { } , and other undefined symbols ( Q_CC ? ) . * * We apologize if the mixup between sub/superscripts caused any confusion , we have fixed these typos . In addition , we now clarify the hat/no hat notation - as in statistics we use the no hat symbol to refer to a `` real '' policy whereas \\hat { } objects refer to approximations ( eg.the output of the deep RL training ) . We note that the Q function is introduced in Definition 2 but the notation Q_CC is introduced in section 4 ( \u201c we call the converged policies under the selfish reward schedule \u03c0\u02c6iD and the associated Q function approximations Q\u02c6iDD. \u201d ) . We apologize for this confusion and will edit Defintion 2 notation to match the Section 4 notation . * * * * * * > > For all practical purposes , it seems that the made assumptions imply uniqueness of the cooperative joint strategy . I fully appreciate that the coordination question is difficult and important , so if the proposed method is not compatible with dealing with that important question , that strikes me as a large drawback . * * * * * * The main assumption used is the exchangeability assumption that all strategies form an equivalence class in that any two pairs of cooperative strategies ( C1 , C1 ) , ( C2 , C2 ) are compatible with each other in the sense that ( C1 , C2 ) generates the same stream of payoffs . This is much weaker than a uniqueness assumption . Indeed , working in value space is one of the innovations of am TFT . As an example of this , consider the Pong Player \u2019 s Dilemma . The exchangeability assumption it allows both players to do whatever they want as long as they \u201c softly \u201d hit the ball over to the other player in some way . For example , our partner can move the paddle around however they like while the ball is in flight , and , importantly , it allows for a partner ( eg.a human ) who hits the ball slightly too fast sometimes ( but not so fast that our agent ca n't get to it ) . In both of these situations a strategy like the Grim Trigger ( a direct application of De Cote & Littman 2008 ) will assume the partner is not cooperating and defect . We agree that there are situations where this assumption fails ( for example if we need to make simultaneous decisions that may or may not be compatible with one another as in , eg.a coordination games ) , but there is no good zero-shot solution to those issues in the literature as well ."}, {"review_id": "rJIN_4lA--1", "review_text": "This paper studies learning to play two-player general-sum games with state (Markov games). The idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains. Generally, in repeated prisoner's dilemma, one can punish one's opponent for noncooperation. In this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner's dilemma game. This is fun but I did not find it particularly surprising from a game-theoretic or from a deep learning point of view. From a game-theoretic point of view, the paper begins with somewhat sloppy definitions followed by a theorem that is not very surprising. It is basically a straightforward generalization of the idea of punishing, which is common in \"folk theorems\" from game theory, to give a particular equilibrium for cooperating in Markov games. Many Markov games do not have a cooperative equilibrium, so this paper restricts attention to those that do. Even in games where there is a cooperative solution that maximizes the total welfare, it is not clear why players would choose to do so. When the game is symmetric, this might be \"the natural\" solution but in general it is far from clear why all players would want to maximize the total payoff. The paper follows with some fun experiments implementing these new game theory notions. Unfortunately, since the game theory was not particularly well-motivated, I did not find the overall story compelling. It is perhaps interesting that one can make deep learning learn to cooperate, but one could have illustrated the game theory equally well with other techniques. In contrast, the paper \"Coco-Q: Learning in Stochastic Games with Side Payments\" by Sodomka et. al. is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning. I would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting. It should also be noted that I was asked to review another ICLR submission entitled \"CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION \" which amazingly introduced the same \"Pong Player\u2019s Dilemma\" game as in this paper. Notice the following suspiciously similar paragraphs from the two papers: From \"MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING\": We also look at an environment where strategies must be learned from raw pixels. We use the method of Tampuu et al. (2017) to alter the reward structure of Atari Pong so that whenever an agent scores a point they receive a reward of 1 and the other player receives \u22122. We refer to this game as the Pong Player\u2019s Dilemma (PPD). In the PPD the only (jointly) winning move is not to play. However, a fully cooperative agent can be exploited by a defector. From \"CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION\": To demonstrate this we follow the method of Tampuu et al. (2017) to construct a version of Atari Pong which makes the game into a social dilemma. In what we call the Pong Player\u2019s Dilemma (PPD) when an agent scores they gain a reward of 1 but the partner receives a reward of \u22122. Thus, in the PPD the only (jointly) winning move is not to play, but selfish agents are again tempted to defect and try to score points even though this decreases total social reward. We see that CCC is a successful, robust, and simple strategy in this game.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank R1 for their thorough comments . We have made several changes to the presentation of the paper to address them . * * > > > \u201c The paper follows with some fun experiments implementing these new game theory notions . Unfortunately , since the game theory was not particularly well-motivated , I did not find the overall story compelling\u2026 \u201d * * Response : This may stem from our lack of clarity in our problem definition ( see reply to R3 above ) . The point here is not to \u201c get RL to cooperate. \u201d Rather , we are interested in expanding the ideas proposed by Axelrod ( 1984 ) to Markov games . Please see our reply to R3 above discussing why our work is related to but also quite different from what is done in other work on cooperative games . > > Similar paragraphs in 2 papers We are also the authors of the other paper . Could the reviewer please clarify the issue here ? Is it that the game is re-used without attribution or is it that we use similar text to describe it ? We are happy to make it clear that this ( the amTFT paper ) is the first one to use the PPD as an environment and the CCC paper uses it for robustness checks . The important differences here are as follows : amTFT ( this paper ) is a strategy that can only be implemented in Markov perfectly observed games . The CCC paper looks at IMPERFECTLY observed games where amTFT can not be used . Note that there are other major differences in the guarantees between strategies ( eg.CCC only has infinite time limit guarantees ) . Since any MDP can be trivially written into a POMDP it follows that the CCC strategy introduced in the other paper can also be used whenever amTFT can be used . Does this mean that amTFT is completely dominated by CCC ? The answer is no , in the CCC paper the PPD is used as an example to show that the CCC algorithm works well in some places ( standard PPD ) and not others ( risky PPD ) . * * * * Other Comments * * * * > > > Even in games where there is a cooperative solution that maximizes the total welfare , it is not clear why players would choose to do so . When the game is symmetric , this might be `` the natural '' solution but in general it is far from clear why all players would want to maximize the total payoff . We agree with the reviewer on this point . One can view this as a discussion about whether a particular equilibrium is a focal point or not . It is well known that in symmetric games ( including bargaining and coordination games ) that people view the symmetric sum of payoffs to be a natural focal point while in asymmetric versions of the problem they do not ( see eg.the chapter on bargaining in Kagel & Roth Handbook of Experimental Economics or more recent work on inequality in public goods games eg . Hauser , Kraft-Todd , Rand , Nowak & Norton 2016 ) . Figuring out which kinds of payoff distributions are \u201c reasonable \u201d focal points , especially for playing with humans , is an important direction for future research but beyond the scope of this paper ( the question is not even settled in behavioral science as there are many debates on whether people are averse to inequality itself , unequal treatment or perhaps something else ) . Note that amTFT can be adapted to any focal point that can be expressed in terms of payoffs ( for example , pure inequity aversion can be expressed as U_1 ( payoff1 , payoff2 ) = payoff1 - A * |payoff1-payoff2| , see eg . Charness & Rabin ( 2002 ) for a generic utility function that can express many social goals ) . The way one can adapt amTFT to these focal points is to train the D policies as we do in the paper , but now train the C policies using this modified reward at each time step ( and use the amTFT switching rule at test time ) . A full discussion of when particular focal points can be implemented in particular games is beyond the scope of the paper . We have made this point clear in the both the introduction and conclusion of the paper ."}, {"review_id": "rJIN_4lA--2", "review_text": "About the first point, it does not present a clear problem definition. The paper continues stating what it should do (e.g. \"our agents only live once at at test time and must maintain cooperation by behaving intelligently within the confines of a single game rather than threats across games.\") without any support for these desiderata. It then continues explaining how to achieve these desiderata, but at this point it is impossible to follow a coherent argument without understanding why are the authors making these strong assumptions about the problem they are trying to solve, and why. Without this problem description and a good motivation, it is impossible to assess why such desiderata (which look awkward to me) are important. The paper continues defining some joint behavior (e.g. cooperative policies), but then construct arguments for individual policy deviations, including elements like \\pi_A and \\Pi_2^{A_k} that, as you see, A is used sometimes as subindex and sometimes as supperindex. Could not follow this part, as such elements lack definition. D_k is also not defined. Experiments are uninteresting and show same results as many other RL algorithms that have been proposed in the past. No comparison with such other approaches is presented, nor even recognized. The paper should include a related work section that explain such similar approaches and their difference with this approach. The paper should continue the experimental section making explicit comparisons with such related work. **Detailed suggestions** - On page 2 you say \"This methodology cannot be directly applied to our problem\" without first defining what the problem is. - When authors talk about the agent, it is unclear what agent they refer to - \\delta undefined - You say selfish reward schedule each agent i treats the other agent just as a part of their environment. However, you need to make some assumption about its behavior (e.g. adversarial, cooperative, etc.) and this disregarded. ", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for their comments . We believe that many of the reviewer 's issues are actually addressed in the paper already , though we were unclear in our presentation . We have made major revisions to the motivating text and the presentation of the main results in the newly uploaded version . > > > The reviewer argues there is a lack of clear problem definition We apologize if the problem definition is unclear , we have edited the text to be clearer . In addition , we have reformulated some of the results presentations to more clearly align with our problem defintion . The goal of the paper is to begin with a question discussed by * The Evolution of Cooperation * ( Axelrod ( 1984 ) ) : suppose that we are going to enter into a repeated Prisoner 's Dilemma with an unknown partner , how should we behave ? Axelrod ( and much follow up work ) comes up with strategies which seek to work well against mixed populations where some individuals are cooperators , some are pure defectors but most are conditional cooperators ( often this is justified by the idea that this is a good approximation of the distribution of people ) . This literature seeks to construct strategies ( eg.Tit-for-Tat , or Win-Stay-Lose-Shift/Pavlov ) which 1 ) cooperate with cooperators 2 ) are n't exploited by defectors 3 ) incentivize conditional cooperators to cooperate 4 ) are simple to explain These are fine desiderata for what it means to `` solve '' a social dilemma . However , a weakness of this literature is that it mostly works with simple 2 player repeated Prisoner 's Dilemma games . Our goal is to expand the Axelrod ideas from the repeated PD case ( where there are 2 actions that are clearly labeled ) to some perfect information Markov game G which is not repeated ( we only play G once ) , has a social dilemma structure , and is too complex to be solved in a tabular format ( so requires deep RL ) . Our question is related to , but actually quite different from , the literatures on : * The folk theorem in game theory ( Fudenberg & Maskin 1986 , Fudenberg , Levine & Maskin 1996 ) \u2013 this literature asks \u201c given a repeated game G , does an efficient equilibrium exist ? \u201d * The work on \u201c computational folk theorem \u201d ( De Cote & Littman 2008 , Littman & Stone 2005 ) \u2013 this literature asks : \u201c can I compute the efficient equilibrium strategies in a repeated game or Markov game ? \u201d * Alternative solution concepts ( eg.Sodomka et al.2013 ) \u2013 this literature asks : \u201c can we define solution concepts beyond Nash and under what conditions will learning converge to them ? \u201d * The learning in ( Markov ) games literature ( Fudenberg & Levine 1998 , Sandholm & Crites 1996 , Leibo et.al.2017 ) \u2013 this literature asks : \u201c which equilibrium will learners converge to as a function of game parameters/learning rules ? \u201d * The shaping in learning in games literature ( Babes et al 2008 ) \u2013 this literature asks : \u201c if I can change the reward functions of agents , can I guide them to a good equilibrium ? \u201d * Friend-or-Foe learning ( Littman 2001 ) \u2013 this paper asks \u201c what kind of learning rule should I use in positive sum games ? \u201d This is quite related to our work though again requires multiple plays of G with the same partner rather than self play training and then a SINGLE play of G. * How do humans behave in these kinds of situations ? ( eg.Rand et al.2012 , Kleinman-Weiner 2016 ) Again , our situation is that we have access to the game and we can do whatever we want at training time , but at test time we play G once and we want to achieve good performance in the Axelrod sense : sometimes we face pure cooperators , sometimes pure defectors , but mostly we face conditional cooperators . As we can see , this question is related to but not the same as the literatures above ( though they all provide valuable tools and context ) . Note also that we are not looking for equilibria in the game , in the PD tit-for-tat is not an equilibrium strategy ( the best response is to always cooperate ) , however it is a very good commitment strategy if we seek to design an agent . We can see from the reviews that the relationship between our work and prior work was unclear from the text , we have edited the introduction and main text significantly to address these comments ."}], "0": {"review_id": "rJIN_4lA--0", "review_text": "This paper addresses multiagent learning problems in which there is a social dilemma: settings where there are no 'cooperative polices' that form an equilibrium. The paper proposes a way of dealing with these problems via amTFT, a variation of the well-known tit-for-that strategy, and presents some empirical results. My main problem with this paper is clarity and I am afraid that not everything might be technically correct. Let me just list my main concerns in the below. The definition of social dilemma, is unclear: \"A social dilemma is a game where there are no cooperative policies which form equilibria. In other words, if one player commits to play a cooperative policy at every state, there is a way for their partner to exploit them and earn higher rewards at their expense.\" does this mean to say \"there are no cooperative *Markov* policies\" ? It seems to me that the paper precisely intents to show that by resorting to history-dependent policies (such as both using amTFT), there is a cooperative equilibrium. I don't understand: \"Note that in a social dilemma there may be policies which achieve the payoffs of cooperative policies because they cooperate on the trajectory of play and prevent exploitation by threatening non-cooperation on states which are never reached by the trajectory. If such policies exist, we call the social dilemma solvable.\" is this now talking about non-Markov policies? If not, there seems to be a contradiction? The work focuses on TFT-like policies, motivated by \"if one can commit to them, create incentives for a partner to behave cooperatively\" however it seems that, as made clear below definition 4, we can only create such incentives for sufficiently powerful agents, that remember and learn from their failures to cooperate in the past? Why is the method called \"approximate Markov\"? As soon as one introduces history dependence, the Markov property stops to hold? On page 4, I have problems following the text due to inconsistent use of notation: subscripts and superscripts seem random, it is not clear which symbols denote strategy profiles (rather than individual strategies), there seems mix-ups between 'i' and '1' / '2', there is sudden use of \\hat{}, and other undefined symbols (Q_CC?). For all practical purposes, it seems that the made assumptions imply uniqueness of the cooperative joint strategy. I fully appreciate that the coordination question is difficult and important, so if the proposed method is not compatible with dealing with that important question, that strikes me as a large drawback. I have problems understanding how it is possible to guarantee \"If they start in a D phase, they eventually return to a C phase.\" without making more assumptions on the domain. The clear example being the typical 'heaven or hell' type of problems: what if after one defect, we are trapped in the 'hell' state where no cooperation is even possible? \"If policies converge with this training then \u03c0\u02c6 is a Markov equilibrium (up to function approximation).\" There are two problems here: 1) A problem is that very typically things will not converge... E.g., Wunder, Michael, Michael L. Littman, and Monica Babes. \"Classes of multiagent q-learning dynamics with epsilon-greedy exploration.\" Proceedings of the 27th International Conference on Machine Learning (ICML-10). 2010. 2) \"Up to function approximation\" could be arbitrary large? Another significant problem seems to be with this statement: \"while in the cooperative reward schedule the standard RL convergence guarantees apply. The latter is because cooperative training is equivalent to one super-agent controlling both players and trying to optimize for a single scalar reward.\" The training of individual learners is quite different from \"joint action learners\" [Claus & Boutilier 98], and this in turn is different from a 'super-agent' which would also control the exploration. In absence of the super-agent, I believe that the only guarantee is that one will, in the limit, converge to a Nash equilibrum, which might be arbitrary far from the optimal joint policy. And this only holds for the tabular case. See the discussion in A concise introduction to multiagent systems and distributed artificial intelligence. N Vlassis. Synthesis Lectures on Artificial Intelligence and Machine Learning 1 (1), 1-71 Also, the approach used in the experiments \"Cooperative (self play with both agents receiving sum of rewards) training for both games\", would be insufficient for many settings where a cooperative joint policy would be asymmetric. The entire approach hinges on using rollouts (the commented lines in Algo. 1). However, it is completely not clear to me how this works. The one paragraph is insufficient to get across these crucial parts of the proposed approach. It is not clear why the tables in Figure 1 are not symmetric; this strikes me as extremely problematic. It is not clear what the colors encode either. It also seems that \"grim\" is better against all, except against amTFT, why should we not use that? In general, the explanation of this closely related paper by De Cote & Littman (which was published at UAI'08), is insufficient. It is not quite clear to me what the proposed approach offers over the previous method. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for pointing out several important issues . We believe these are mostly issues of clarity in exposition/notation . We have edited the text to address these issues . > > The definition of social dilemma , is unclear : `` A social dilemma is a game where there are no cooperative policies which form equilibria\u2026 . does this mean to say `` there are no cooperative * Markov * policies '' ? * * The referee is correct , this should mean that there are no Markov policies . Note that when we refer to cooperative policies we specifically refer to ones which cooperate at ALL states . Thus we define a social dilemma to be one where cooperation after EVERY STATE is impossible in equilibrium \u2013 that is , if there is cooperation along the path of play it must be because OFF THE PATH of play cooperation stops . This is identical to the logic in the standard repeated Prisoner \u2019 s Dilemma where policies which always cooperate are not equilibria , rather , in order to maintain cooperation along the path of play there must be defection off the path of play ( eg.Grim Trigger ) . We have edited the text to make this point clearer . * * > > Why is the method called `` approximate Markov '' ? As soon as one introduces history dependence , the Markov property stops to hold ? * * We call the method approximate Markov because we use function approximation ( approximate ) and because amTFT only uses Markov policies from the original game ( only using the augmented memory to switch between them ) . We have made this clearer in the paper . * * > > On page 4 , I have problems following the text due to inconsistent use of notation : subscripts and superscripts seem random , it is not clear which symbols denote strategy profiles ( rather than individual strategies ) , there seems mix-ups between ' i ' and ' 1 ' / ' 2 ' , there is sudden use of \\hat { } , and other undefined symbols ( Q_CC ? ) . * * We apologize if the mixup between sub/superscripts caused any confusion , we have fixed these typos . In addition , we now clarify the hat/no hat notation - as in statistics we use the no hat symbol to refer to a `` real '' policy whereas \\hat { } objects refer to approximations ( eg.the output of the deep RL training ) . We note that the Q function is introduced in Definition 2 but the notation Q_CC is introduced in section 4 ( \u201c we call the converged policies under the selfish reward schedule \u03c0\u02c6iD and the associated Q function approximations Q\u02c6iDD. \u201d ) . We apologize for this confusion and will edit Defintion 2 notation to match the Section 4 notation . * * * * * * > > For all practical purposes , it seems that the made assumptions imply uniqueness of the cooperative joint strategy . I fully appreciate that the coordination question is difficult and important , so if the proposed method is not compatible with dealing with that important question , that strikes me as a large drawback . * * * * * * The main assumption used is the exchangeability assumption that all strategies form an equivalence class in that any two pairs of cooperative strategies ( C1 , C1 ) , ( C2 , C2 ) are compatible with each other in the sense that ( C1 , C2 ) generates the same stream of payoffs . This is much weaker than a uniqueness assumption . Indeed , working in value space is one of the innovations of am TFT . As an example of this , consider the Pong Player \u2019 s Dilemma . The exchangeability assumption it allows both players to do whatever they want as long as they \u201c softly \u201d hit the ball over to the other player in some way . For example , our partner can move the paddle around however they like while the ball is in flight , and , importantly , it allows for a partner ( eg.a human ) who hits the ball slightly too fast sometimes ( but not so fast that our agent ca n't get to it ) . In both of these situations a strategy like the Grim Trigger ( a direct application of De Cote & Littman 2008 ) will assume the partner is not cooperating and defect . We agree that there are situations where this assumption fails ( for example if we need to make simultaneous decisions that may or may not be compatible with one another as in , eg.a coordination games ) , but there is no good zero-shot solution to those issues in the literature as well ."}, "1": {"review_id": "rJIN_4lA--1", "review_text": "This paper studies learning to play two-player general-sum games with state (Markov games). The idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains. Generally, in repeated prisoner's dilemma, one can punish one's opponent for noncooperation. In this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner's dilemma game. This is fun but I did not find it particularly surprising from a game-theoretic or from a deep learning point of view. From a game-theoretic point of view, the paper begins with somewhat sloppy definitions followed by a theorem that is not very surprising. It is basically a straightforward generalization of the idea of punishing, which is common in \"folk theorems\" from game theory, to give a particular equilibrium for cooperating in Markov games. Many Markov games do not have a cooperative equilibrium, so this paper restricts attention to those that do. Even in games where there is a cooperative solution that maximizes the total welfare, it is not clear why players would choose to do so. When the game is symmetric, this might be \"the natural\" solution but in general it is far from clear why all players would want to maximize the total payoff. The paper follows with some fun experiments implementing these new game theory notions. Unfortunately, since the game theory was not particularly well-motivated, I did not find the overall story compelling. It is perhaps interesting that one can make deep learning learn to cooperate, but one could have illustrated the game theory equally well with other techniques. In contrast, the paper \"Coco-Q: Learning in Stochastic Games with Side Payments\" by Sodomka et. al. is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning. I would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting. It should also be noted that I was asked to review another ICLR submission entitled \"CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION \" which amazingly introduced the same \"Pong Player\u2019s Dilemma\" game as in this paper. Notice the following suspiciously similar paragraphs from the two papers: From \"MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING\": We also look at an environment where strategies must be learned from raw pixels. We use the method of Tampuu et al. (2017) to alter the reward structure of Atari Pong so that whenever an agent scores a point they receive a reward of 1 and the other player receives \u22122. We refer to this game as the Pong Player\u2019s Dilemma (PPD). In the PPD the only (jointly) winning move is not to play. However, a fully cooperative agent can be exploited by a defector. From \"CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION\": To demonstrate this we follow the method of Tampuu et al. (2017) to construct a version of Atari Pong which makes the game into a social dilemma. In what we call the Pong Player\u2019s Dilemma (PPD) when an agent scores they gain a reward of 1 but the partner receives a reward of \u22122. Thus, in the PPD the only (jointly) winning move is not to play, but selfish agents are again tempted to defect and try to score points even though this decreases total social reward. We see that CCC is a successful, robust, and simple strategy in this game.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank R1 for their thorough comments . We have made several changes to the presentation of the paper to address them . * * > > > \u201c The paper follows with some fun experiments implementing these new game theory notions . Unfortunately , since the game theory was not particularly well-motivated , I did not find the overall story compelling\u2026 \u201d * * Response : This may stem from our lack of clarity in our problem definition ( see reply to R3 above ) . The point here is not to \u201c get RL to cooperate. \u201d Rather , we are interested in expanding the ideas proposed by Axelrod ( 1984 ) to Markov games . Please see our reply to R3 above discussing why our work is related to but also quite different from what is done in other work on cooperative games . > > Similar paragraphs in 2 papers We are also the authors of the other paper . Could the reviewer please clarify the issue here ? Is it that the game is re-used without attribution or is it that we use similar text to describe it ? We are happy to make it clear that this ( the amTFT paper ) is the first one to use the PPD as an environment and the CCC paper uses it for robustness checks . The important differences here are as follows : amTFT ( this paper ) is a strategy that can only be implemented in Markov perfectly observed games . The CCC paper looks at IMPERFECTLY observed games where amTFT can not be used . Note that there are other major differences in the guarantees between strategies ( eg.CCC only has infinite time limit guarantees ) . Since any MDP can be trivially written into a POMDP it follows that the CCC strategy introduced in the other paper can also be used whenever amTFT can be used . Does this mean that amTFT is completely dominated by CCC ? The answer is no , in the CCC paper the PPD is used as an example to show that the CCC algorithm works well in some places ( standard PPD ) and not others ( risky PPD ) . * * * * Other Comments * * * * > > > Even in games where there is a cooperative solution that maximizes the total welfare , it is not clear why players would choose to do so . When the game is symmetric , this might be `` the natural '' solution but in general it is far from clear why all players would want to maximize the total payoff . We agree with the reviewer on this point . One can view this as a discussion about whether a particular equilibrium is a focal point or not . It is well known that in symmetric games ( including bargaining and coordination games ) that people view the symmetric sum of payoffs to be a natural focal point while in asymmetric versions of the problem they do not ( see eg.the chapter on bargaining in Kagel & Roth Handbook of Experimental Economics or more recent work on inequality in public goods games eg . Hauser , Kraft-Todd , Rand , Nowak & Norton 2016 ) . Figuring out which kinds of payoff distributions are \u201c reasonable \u201d focal points , especially for playing with humans , is an important direction for future research but beyond the scope of this paper ( the question is not even settled in behavioral science as there are many debates on whether people are averse to inequality itself , unequal treatment or perhaps something else ) . Note that amTFT can be adapted to any focal point that can be expressed in terms of payoffs ( for example , pure inequity aversion can be expressed as U_1 ( payoff1 , payoff2 ) = payoff1 - A * |payoff1-payoff2| , see eg . Charness & Rabin ( 2002 ) for a generic utility function that can express many social goals ) . The way one can adapt amTFT to these focal points is to train the D policies as we do in the paper , but now train the C policies using this modified reward at each time step ( and use the amTFT switching rule at test time ) . A full discussion of when particular focal points can be implemented in particular games is beyond the scope of the paper . We have made this point clear in the both the introduction and conclusion of the paper ."}, "2": {"review_id": "rJIN_4lA--2", "review_text": "About the first point, it does not present a clear problem definition. The paper continues stating what it should do (e.g. \"our agents only live once at at test time and must maintain cooperation by behaving intelligently within the confines of a single game rather than threats across games.\") without any support for these desiderata. It then continues explaining how to achieve these desiderata, but at this point it is impossible to follow a coherent argument without understanding why are the authors making these strong assumptions about the problem they are trying to solve, and why. Without this problem description and a good motivation, it is impossible to assess why such desiderata (which look awkward to me) are important. The paper continues defining some joint behavior (e.g. cooperative policies), but then construct arguments for individual policy deviations, including elements like \\pi_A and \\Pi_2^{A_k} that, as you see, A is used sometimes as subindex and sometimes as supperindex. Could not follow this part, as such elements lack definition. D_k is also not defined. Experiments are uninteresting and show same results as many other RL algorithms that have been proposed in the past. No comparison with such other approaches is presented, nor even recognized. The paper should include a related work section that explain such similar approaches and their difference with this approach. The paper should continue the experimental section making explicit comparisons with such related work. **Detailed suggestions** - On page 2 you say \"This methodology cannot be directly applied to our problem\" without first defining what the problem is. - When authors talk about the agent, it is unclear what agent they refer to - \\delta undefined - You say selfish reward schedule each agent i treats the other agent just as a part of their environment. However, you need to make some assumption about its behavior (e.g. adversarial, cooperative, etc.) and this disregarded. ", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for their comments . We believe that many of the reviewer 's issues are actually addressed in the paper already , though we were unclear in our presentation . We have made major revisions to the motivating text and the presentation of the main results in the newly uploaded version . > > > The reviewer argues there is a lack of clear problem definition We apologize if the problem definition is unclear , we have edited the text to be clearer . In addition , we have reformulated some of the results presentations to more clearly align with our problem defintion . The goal of the paper is to begin with a question discussed by * The Evolution of Cooperation * ( Axelrod ( 1984 ) ) : suppose that we are going to enter into a repeated Prisoner 's Dilemma with an unknown partner , how should we behave ? Axelrod ( and much follow up work ) comes up with strategies which seek to work well against mixed populations where some individuals are cooperators , some are pure defectors but most are conditional cooperators ( often this is justified by the idea that this is a good approximation of the distribution of people ) . This literature seeks to construct strategies ( eg.Tit-for-Tat , or Win-Stay-Lose-Shift/Pavlov ) which 1 ) cooperate with cooperators 2 ) are n't exploited by defectors 3 ) incentivize conditional cooperators to cooperate 4 ) are simple to explain These are fine desiderata for what it means to `` solve '' a social dilemma . However , a weakness of this literature is that it mostly works with simple 2 player repeated Prisoner 's Dilemma games . Our goal is to expand the Axelrod ideas from the repeated PD case ( where there are 2 actions that are clearly labeled ) to some perfect information Markov game G which is not repeated ( we only play G once ) , has a social dilemma structure , and is too complex to be solved in a tabular format ( so requires deep RL ) . Our question is related to , but actually quite different from , the literatures on : * The folk theorem in game theory ( Fudenberg & Maskin 1986 , Fudenberg , Levine & Maskin 1996 ) \u2013 this literature asks \u201c given a repeated game G , does an efficient equilibrium exist ? \u201d * The work on \u201c computational folk theorem \u201d ( De Cote & Littman 2008 , Littman & Stone 2005 ) \u2013 this literature asks : \u201c can I compute the efficient equilibrium strategies in a repeated game or Markov game ? \u201d * Alternative solution concepts ( eg.Sodomka et al.2013 ) \u2013 this literature asks : \u201c can we define solution concepts beyond Nash and under what conditions will learning converge to them ? \u201d * The learning in ( Markov ) games literature ( Fudenberg & Levine 1998 , Sandholm & Crites 1996 , Leibo et.al.2017 ) \u2013 this literature asks : \u201c which equilibrium will learners converge to as a function of game parameters/learning rules ? \u201d * The shaping in learning in games literature ( Babes et al 2008 ) \u2013 this literature asks : \u201c if I can change the reward functions of agents , can I guide them to a good equilibrium ? \u201d * Friend-or-Foe learning ( Littman 2001 ) \u2013 this paper asks \u201c what kind of learning rule should I use in positive sum games ? \u201d This is quite related to our work though again requires multiple plays of G with the same partner rather than self play training and then a SINGLE play of G. * How do humans behave in these kinds of situations ? ( eg.Rand et al.2012 , Kleinman-Weiner 2016 ) Again , our situation is that we have access to the game and we can do whatever we want at training time , but at test time we play G once and we want to achieve good performance in the Axelrod sense : sometimes we face pure cooperators , sometimes pure defectors , but mostly we face conditional cooperators . As we can see , this question is related to but not the same as the literatures above ( though they all provide valuable tools and context ) . Note also that we are not looking for equilibria in the game , in the PD tit-for-tat is not an equilibrium strategy ( the best response is to always cooperate ) , however it is a very good commitment strategy if we seek to design an agent . We can see from the reviews that the relationship between our work and prior work was unclear from the text , we have edited the introduction and main text significantly to address these comments ."}}