{"year": "2019", "forum": "H1eH4n09KX", "title": "Adversarial Audio Super-Resolution with Unsupervised Feature Losses", "decision": "Reject", "meta_review": "The paper presents an algorithm for audio super-resolution using adversarial models along with additional losses, e.g. using auto-encoders and reconstruction losses, to improve the generation process. \n\nStrengths\n- Proposes audio super resolution based on GANs, extending some of the techniques proposed for vision / image to audio.\n- The authors improved the paper during the review process by including results from a user study and ablation analysis.\n\nWeaknesses\n- Although the paper presents an interesting application of GANs for the audio task, overall novelty is limited since the setup closely follows what has been done for vision and related tasks, and the baseline system. This is also not the first application of GANs for audio tasks. \n- Performance improvement over previously proposed (U-Net) models is small. It would have been useful to also include UNet4 in user-study, as one of the reviewers\u2019 pointed out, since it sounds better in a few cases.\n- It is not entirely clear if the method would be an improvement of state-of-the-art audio generative models like Wavenet.\n\nReviewers agree that the general direction of this work is interesting, but the results are not compelling enough at the moment for the paper to be accepted to ICLR. Given these review comments, the recommendation is to reject the paper.", "reviews": [{"review_id": "H1eH4n09KX-0", "review_text": " The paper presents a model to perform audio super resolution. The proposed model trains a neural network to produce a high-resolution audio sample given a low resolution input. It uses three losses: sample reconstructon, adversarialy loss and feature matching on a representation learned on an unsupervised way. From a technical perspective, I do not find the proposed approach very novel. It uses architectures following closely what has been done for Image supre-resolution. I am not aware of an effective use of GANs in the audio processing domain. This would be a good point for the paper. However, the evidence presented does not seem very convincing in my view. While this is an audio processing paper, it lacks domain insights (even the terminology feels borrowed from the image domain). Again, most of the modeling decisions seem to follow what has been done for images. The empirical results seem good, but the generated audio does not match the quality of the state-of-the-art. The presentation of the paper is correct. It would be good to list or summarize the contributions of this work. Recent works have shown the amazing power of auto-regressive generative models (WaveNet) in producing audio signals. This is, as far as I know, the state-of-the-art in audio generation. The authors should motivate why the proposed model is better or worth studying in light of those approaches. In particular, a recent work [A] has shown very high quality results in the problem of speech conversion (which seems harder than bandwidth extension). It would seem to me that applying such models to the bandwith extension task should also lead to very high quality results as well. What is the advantage of the proposed approach? Would a WaveNet decoder also be improved by including these auxiliary losses? While the audio samples seem to be good, they are also a bit noisy even compared with the baseline. This is not the case in the samples generated by [A] (which is of course a different problem). The qualitative results are evaluated using PESQ. While this is a good proxy it is much better to perform blind tests with listeners. That would certainly improve the paper. Feature spaces are used in super resolution to provide a space in which the an L2 loss is perceptually more relevant. There are many such representations for audio signals. Specifically the magnitude of time-frequency representations (like spectrograms) or more sophisticated features such as scattering coefficients. In my view, the paper would be much stronger if these features would be evaluated as alternative to the features provided by the proposed autoencoder. One of the motivations for defining the loss in the feature space is the lack (or difficulty to train) auxiliary classifiers on large amounts of data. However, speech recognition models using neural networks are quite common. It would be good to also test features obtained from an off-the-shelf speech recognition system. How would this compare to the proposed model? The L2 \"pixel\" loss seems a bit strange in my view. Particularly in audio processing, the recovered high frequency components can be synthesized with an arbitrary phase. This means that imposing an exact match seems like a constraint as the phase cannot be predicted from the low resolution signal (which is what a GAN loss could achieve). The paper should present ablations on the use of the different losses. In particular, one of the main contributions is the inclusion of the loss measured in the learned feature space. The authors mention that not including it leads to audible artifacts. I think that more studies should be presented (including quantitative evaluations and audio samples). How where the hyper parameters chosen? is there a lot of sensitivity to their values? [A] van den Oord, Aaron, and Oriol Vinyals. \"Neural discrete representation learning.\" Advances in Neural Information Processing Systems. 2017. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the thoughtful and detailed response . Please see our enumerated responses below . We have also posted a comment above that summarized the changes in the latest revision . * Q1 * : From a technical perspective , I do not find the proposed approach very novel . It uses architectures following closely what has been done for Image super-resolution . I am not aware of an effective use of GANs in the audio processing domain . This would be a good point for the paper . However , the evidence presented does not seem very convincing in my view . While this is an audio processing paper , it lacks domain insights ( even the terminology feels borrowed from the image domain ) . Again , most of the modeling decisions seem to follow what has been done for images . The empirical results seem good , but the generated audio does not match the quality of the state-of-the-art . * A1 * : Thank you for this feedback . We agree that some of our high-level design choices are inspired by architectures from image processing literature . However , the main focus of our work is the exploration of such techniques and their adaptations to audio processing , which has been little-explored previously . Most importantly , we develop several new techniques and present analysis that is found in neither image nor audio processing literature . Indeed , autoregressive methods produce audio of excellent quality ; we have added a discussion of this to the paper , and also elaborate more in this topic in answer A3 below . * Q2 * : The presentation of the paper is correct . It would be good to list or summarize the contributions of this work . * A2 * : We agree - the introduction has been revised and now includes an explicit list of contributions . * Q3 * : Recent works have shown the amazing power of auto-regressive generative models ( WaveNet ) in producing audio signals . This is , as far as I know , the state-of-the-art in audio generation . The authors should motivate why the proposed model is better or worth studying in light of those approaches . In particular , a recent work [ A ] has shown very high quality results in the problem of speech conversion ( which seems harder than bandwidth extension ) . It would seem to me that applying such models to the bandwith extension task should also lead to very high quality results as well . What is the advantage of the proposed approach ? Would a WaveNet decoder also be improved by including these auxiliary losses ? * A3 * : We agree and note in the paper that auto-regressive models are indeed a promising avenue for audio generation . The primary difference from our work is that auto-regressive methods require an inference pass to generate a single output sample , with an input sequence that grows with each inference inference pass . This process is computationally intensive - for instance , at 16 KHz , an optimized Wavenet requires ~1.5 minutes to generate one second of audio [ C ] . We were recently made aware of a Wavenet variant that alleviates the issue of slow sample generation with a model distillation/student-teacher method [ B ] . As you correctly point out , Wavenet models can be improved with these auxiliary losses , and the Wavenet variant in [ B ] actually integrates a feature loss based on a speech phoneme-classifier network . This supports our view that our work is not in conflict with Wavenet and other auto-regressive methods , but rather augments them and can be used successfully in conjunction . * Q4 * : While the audio samples seem to be good , they are also a bit noisy even compared with the baseline . This is not the case in the samples generated by [ A ] ( which is of course a different problem ) . * A4 * : We did notice that in some samples , especially at higher upsampling rates , there are instances of noise on utterances with significant high-frequency content ( e.g. , fricatives and aspiration ) . We are not entirely certain on the cause of this noise , but we suspect that it is related to inherent ambiguity in the phase and magnitude of high frequency signals . Furthermore , we found that this noise is present even if we replace the unsupervised feature loss with other conventional feature losses . We made sure to include samples at high up-sampling ratios that included this noise in the user study , and the study indicated that users preferred audio produced by our method in spite of spurious noise . Nevertheless we have added a note in the paper regarding this problem . * Q5 * : The qualitative results are evaluated using PESQ . While this is a good proxy it is much better to perform blind tests with listeners . That would certainly improve the paper . * A5 * : Thanks for this point . We have acted on this recommendation and now have results from a user study in the Experiments section . [ B ] van Den Oord et al. \u201c Parallel WaveNet : Fast High-Fidelity Speech Synthesis. \u201d ICML 2018 ."}, {"review_id": "H1eH4n09KX-1", "review_text": "This paper presents a GAN-based method to perform audio super-resolution. In contrast to previous work, this work uses auto-encoder to obtain feature losses derived from unlabeled data. Comments: (1) Redundant comma: \u201cfilters with very large receptive fields are required to create high quality, raw audio\u201d. (2) There are some state-of-the-art non-autoregressive generative models for audio waveform e.g., parallel wavenet, clarinet. One may properly discuss them in related work section. Although GAN performs very well for images, it hasn't obtained any compelling results for raw audios. Still, it\u2019s very interesting to explore that. Any nontrivial insight would be highly appreciated. (3) In multiscale convolutional layers, it seems only larger filter plays a significant role. What if we omit small filter, e.g., 3X1? (4) It seems the proposed MU-GAN introduces noticeable noise in the upsampled audios. Pros: - Interesting idea and fascinating problem. Cons: - The results are fair. I didn\u2019t see big improvement over previous work (Kuleshov et al., 2017). I'd like to reconsider my rating after the rebuttal. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the feedback . Please see our enumerated responses below . We have also posted a comment above that summarized the changes in the latest revision . * Q1 * : Redundant comma : \u201c filters with very large receptive fields are required to create high quality , raw audio \u201d . * A1 * : We have corrected this , thank you . * Q2 * : There are some state-of-the-art non-autoregressive generative models for audio waveform e.g. , parallel wavenet , clarinet . One may properly discuss them in related work section . Although GAN performs very well for images , it has n't obtained any compelling results for raw audios . Still , it \u2019 s very interesting to explore that . Any nontrivial insight would be highly appreciated . * A2 * : Thanks for pointing this out . We have added a discussion of these works , and a comparison to other GAN-based methods in the Related Works section . * Q3 * : In multiscale convolutional layers , it seems only larger filter plays a significant role . What if we omit small filter , e.g. , 3X1 ? * A3 * : This is a good point - we found that small filters do play a marginal but noticeable role in audio quality and speed of convergence . We suspect that while smaller filters are comparatively less powerful , they are easier to optimize and may have an important role in fitting many of the \u201c easy \u201d components in audio signals . Smaller filters also have lower overhead in terms of computational requirements , making our networks ( which have hundreds of feature maps in some layers ) feasible to train in a few days or less with a single GPU . * Q4 * : It seems the proposed MU-GAN introduces noticeable noise in the upsampled audios . * A4 * : Thanks for this comment - we noticed this as well . We have copied our answer A4 to reviewer # 1 below : We did notice that in some samples , especially at higher upsampling rates , there are instances of noise on utterances with significant high-frequency content ( e.g. , fricatives and aspiration ) . We are not entirely certain on the cause of this noise , but we suspect that it is related to inherent ambiguity in the phase and magnitude of high frequency signals . Furthermore , we found that this noise is present even if we replace the unsupervised feature loss with other conventional feature losses . Nevertheless , we made sure to include samples at high up-sampling ratios that included this noise in the user study , which indicated that users preferred audio produced by our method in spite of spurious noise . We have added a note in the paper regarding this problem . * Q5 * : The results are fair . I didn \u2019 t see big improvement over previous work ( Kuleshov et al. , 2017 ) . * A5 * : We appreciate the criticism . We wanted to highlight that while our baselines are based on the work from Kuleshov et al. , 2017 , our primary baseline for evaluation ( U-net8 ) is a much deeper and more powerful model compared to the model evaluated in Kuleshov et al. , 2017 . For a more direct comparison with Kuleshov et al. , 2017 , see the results for U-net4 in the Experiments section , which are taken from the authors \u2019 paper ."}, {"review_id": "H1eH4n09KX-2", "review_text": "PRO\u2019s: +well-written +nice overall system: GAN framework for super-sampling audio incorporating features from an autoencoder +some good-sounding examples CON\u2019s: -some confusing/weakly-presented parts (admittedly covering lots of material in short space) -I am confused about the evaluation; would like additional qualitative/observational understanding of what works, including more on how the results differ from baseline SUMMARY: The task addressed in this work is: given a low-resolution audio signal, generate corresponding high-quality audio. The approach is a generative neural network that operates on raw audio and train within a GAN framework. Working in raw sample-space (e.g. pixels) is known to be challenging, so a stabilizing solution is to incorporate a feature loss. Feature loss, however, usually requires a network trained on a related task, and if such a net one does not already exist, then building one can have its own (possibly significant) challenges. In this work, the authors avoid this auxiliary challenge by using unsupervised feature losses, taking advantage of the fact that any audio signal can be downsampled, and therefore one has the corresponding upsampled signal as well. The training framework is basically that of a GAN, but where, rather than providing the generator with a low-dimensional noise signal input, they provide the generator with the subsampled audio signal. The architecture includes a generator ( G(lo-fidelity)=high-fidelity ), a discriminator ( D(high-fidelity) = real or by super-sampled ? ), and an autoencoder ( \\phi( signal x) = features of signal x at AE\u2019s bottleneck). COMMENTS: The generator network appears to be nearly identical to that of Kuleshov et al (2017)-- which becomes the baseline-- and so the primary contribution differentiating this work is the insertion of that network into a GAN framework along with the additional feature-based loss term. This is overall a nice problem and a nice approach! In that light, I believe that there is a new focus in this work on the perceptual quality of the outputs, as compared to (Kuleshov et al 2017). I would therefore ideally like to see (a) some attempts at perceptually evaluating the resulting output (beyond PESQ, e.g. with human subjects and with the understanding that, e.g. not all AMT workers have the same aural discriminative abilities themselves), and/or (b) more detailed associated qualitative descriptions/visualization of the super-sampled signal, perhaps with a few more samples if that would help. That said, I understand that there are page/space limitations. (more on this next) Given the similarity of the U-net architectures to (Kuleshov et al 2017), why not move some of those descriptions to the appendix? For example, I found the description and figure illustrating the \u201csuperpixel layers\u201d to be fairly uninformative: I see that the figure shows interleaving and de-interleaving, resulting in trading-off dimensionalities/ranks/etc, and we are told that this helps with well-known checkerboard artifacts, but I was confused about what the white elements represent, and the caption just reiterated that resolution was being increased and decreased. Overall, I didn\u2019t really understand exactly the role that this plays in the system; I wondered if it either needed a lot more clarification (in an appendix?), or just less space spent on it, but keeping the pointers to the relevant references. It seems that the subpixel layer was already implemented in Kuleshov 2017, with some explanation, yet in the present work a large table (Table 1(b)) is presented showing that there is no difference in quality metrics, and the text also mentions that there is no significant perceptual difference in audio. If the subpixel layer were explained in detail, and with justification, then I would potentially be OK with the negative results, but in this case it\u2019s not clear why spend this time on it here. It\u2019s possible that there is something simple about it that I am not understanding. I\u2019m open to being convinced. Otherwise, why not just write: \u201cFollowing (Kuleshov et al 2017), we use subpixel layers (Shi et al) [instead of ...] to speed up training, although we found that they make no significant perceptual effects.\u201d or something along those lines, and leave it at that? I did appreciate the descriptions of models\u2019 sensitivity to size/structure of the conv filters, importance of the res connections, etc. My biggest confusion was with the evaluation & results: Since the most directly related work was (Kuleshov 2017), I compared the super resolution (U-net) samples on that website (https://kuleshov.github.io/audio-super-res/ ) to the samples provided for the present work ( https://sites.google.com/view/unsupervised-audiosr/home ) and I was a bit confused, because the quality of the U-net samples in (Kuleshov 2017) seemed to be perceptually significantly better than the quality of the Deep CNN (U-net) baseline in the present work. Perhaps I am in error about this, but as far as I can tell, the superresolution in (Kuleshov et al 2017) is significantly better than the Deep CNN examples here. Is this a result of careful selection of examples? I do believe what I hear, e.g. that the MU-GAN8 is clearly better on some examples than the U-net8. But then for non-identical samples, how come U-net4 actually generally sounds better than U-net8? That doesn\u2019t make immediate sense either (assuming no overfitting etc). Is the benefit in moving from U-net4 to U-net8 within a GAN context but then stabilizing it with the feature-based loss? If so, then how does MU-GAN8 compare to U-net4? Would there be any info for the reader by doing an ablation removing the feature loss from the GAN framework? etc. I guess I would like to get a better understanding of what is actually going on, even if qualitative. Is there any qualitative or anecdotal observation about which \u201ctypes\u201d of samples one system works better on than another? For example, in the provided examples for the present paper, it seemed to be the case that perhaps the MU-GAN8 was more helpful for supersampling female voices, which might have more high-frequency components that seem to get lost when downsampling, but maybe I\u2019m overgeneralizing from the few examples I heard. Some spectrograms might be helpful, since they do after all convey some useful information despite not telling much of the perceptual story. For example, are there visible but inaudible artifacts? Are such artifacts systematic? Were individual audio samples represented as a one-hot encoding, or as floats? (I assume floats since there was no mention of sampling from a distribution to select the value). A couple of typos: descriminator \u2192 discriminator pg 6 \u201cImpact of superpixel layers\u201d -- last sentence of 2nd par is actually not a sentence. \u201cthe reduction in convolutional kernels prior to the superpixel operation.\u201d Overall, interesting work, and I enjoyed reading it. If some of my questions around evaluation could be addressed-- either in a revision, or in a rebuttal (e.g. if I completely misunderstood something)-- I would gladly consider revising my rating (which is currently somewhere between 6 and 7). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the thoughtful and detailed review . Please see our enumerated responses below . We have also posted a comment above that summarized the changes in the latest revisio * Q1 * : The generator network appears to be nearly identical to that of Kuleshov et al ( 2017 ) -- which becomes the baseline -- and so the primary contribution differentiating this work is the insertion of that network into a GAN framework along with the additional feature-based loss term . This is overall a nice problem and a nice approach ! In that light , I believe that there is a new focus in this work on the perceptual quality of the outputs , as compared to ( Kuleshov et al 2017 ) . I would therefore ideally like to see ( a ) some attempts at perceptually evaluating the resulting output ( beyond PESQ , e.g.with human subjects and with the understanding that , e.g.not all AMT workers have the same aural discriminative abilities themselves ) , and/or ( b ) more detailed associated qualitative descriptions/visualization of the super-sampled signal , perhaps with a few more samples if that would help . That said , I understand that there are page/space limitations . ( more on this next ) * A1 * : Thank you for the detailed feedback . While time was short , we were able to add a subjective user study to the paper - see the updated Experiments section . We also added more qualitative discussion of super-sampled audio , with associated spectrograms . We are currently updating the web page with more audio samples as well and will update it shortly . * Q2 * : Given the similarity of the U-net architectures to ( Kuleshov et al 2017 ) , why not move some of those descriptions to the appendix ? * A2 * : Thanks for this feedback - indeed , we found that our original architectural descriptions were overly detailed . We have removed much of this unnecessary detail , and plan to make a minor revision later today with additional architectural parameters in the appendix . * Q3 * : Overall , I didn \u2019 t really understand exactly the role that [ superpixel ] plays in the system ; I wondered if it either needed a lot more clarification ( in an appendix ? ) , or just less space spent on it , but keeping the pointers to the relevant references . It seems that the subpixel layer was already implemented in Kuleshov 2017 , with some explanation , yet in the present work a large table ( Table 1 ( b ) ) is presented showing that there is no difference in quality metrics , and the text also mentions that there is no significant perceptual difference in audio . If the subpixel layer were explained in detail , and with justification , then I would potentially be OK with the negative results , but in this case it \u2019 s not clear why spend this time on it here . It \u2019 s possible that there is something simple about it that I am not understanding . I \u2019 m open to being convinced . Otherwise , why not just write : \u201c Following ( Kuleshov et al 2017 ) , we use subpixel layers ( Shi et al ) [ instead of ... ] to speed up training , although we found that they make no significant perceptual effects. \u201d or something along those lines , and leave it at that ? * A3 * : We concede that the presentation of superpixel and subpixel layers was somewhat confusing ; we have revised the description in the Methods and Experiments sections to hopefully clarify this . One important detail we want to clarify is that while subpixel layers have been previously evaluated , no previous works have attempted to use its simple inverse to * decrease * spatial resolution ( previous works use strided convolution or pooling ) . Our paper evaluates this somewhat obvious inverse operator , referred to as a superpixel layer , and finds that it actually reduces training time without loss of performance . * Q4 * : Some spectrograms might be helpful , since they do after all convey some useful information despite not telling much of the perceptual story . For example , are there visible but inaudible artifacts ? Are such artifacts systematic ? * A4 * : Indeed , while we were not able to include spectrograms in the initial draft , the revision includes spectrograms of super-resolved audio , as well as an example produced by a GAN that exhibits the artifacts we mentioned in the paper . To explicitly answer your question , we find that artifacts introduced by GANs are generally systematic ( e.g. , high-frequency whines ) , and both visible and audible . * Q5 * : Were individual audio samples represented as a one-hot encoding , or as floats ? ( I assume floats since there was no mention of sampling from a distribution to select the value ) . * A5 * : You are correct - the individual audio samples are 32-bit ( single-precision ) floats . We have added a short footnote in the Method section to clarify this . Thanks ! * Q6 * : A couple of typos : \u2026 * A6 * : Thank you , these have all been corrected ."}], "0": {"review_id": "H1eH4n09KX-0", "review_text": " The paper presents a model to perform audio super resolution. The proposed model trains a neural network to produce a high-resolution audio sample given a low resolution input. It uses three losses: sample reconstructon, adversarialy loss and feature matching on a representation learned on an unsupervised way. From a technical perspective, I do not find the proposed approach very novel. It uses architectures following closely what has been done for Image supre-resolution. I am not aware of an effective use of GANs in the audio processing domain. This would be a good point for the paper. However, the evidence presented does not seem very convincing in my view. While this is an audio processing paper, it lacks domain insights (even the terminology feels borrowed from the image domain). Again, most of the modeling decisions seem to follow what has been done for images. The empirical results seem good, but the generated audio does not match the quality of the state-of-the-art. The presentation of the paper is correct. It would be good to list or summarize the contributions of this work. Recent works have shown the amazing power of auto-regressive generative models (WaveNet) in producing audio signals. This is, as far as I know, the state-of-the-art in audio generation. The authors should motivate why the proposed model is better or worth studying in light of those approaches. In particular, a recent work [A] has shown very high quality results in the problem of speech conversion (which seems harder than bandwidth extension). It would seem to me that applying such models to the bandwith extension task should also lead to very high quality results as well. What is the advantage of the proposed approach? Would a WaveNet decoder also be improved by including these auxiliary losses? While the audio samples seem to be good, they are also a bit noisy even compared with the baseline. This is not the case in the samples generated by [A] (which is of course a different problem). The qualitative results are evaluated using PESQ. While this is a good proxy it is much better to perform blind tests with listeners. That would certainly improve the paper. Feature spaces are used in super resolution to provide a space in which the an L2 loss is perceptually more relevant. There are many such representations for audio signals. Specifically the magnitude of time-frequency representations (like spectrograms) or more sophisticated features such as scattering coefficients. In my view, the paper would be much stronger if these features would be evaluated as alternative to the features provided by the proposed autoencoder. One of the motivations for defining the loss in the feature space is the lack (or difficulty to train) auxiliary classifiers on large amounts of data. However, speech recognition models using neural networks are quite common. It would be good to also test features obtained from an off-the-shelf speech recognition system. How would this compare to the proposed model? The L2 \"pixel\" loss seems a bit strange in my view. Particularly in audio processing, the recovered high frequency components can be synthesized with an arbitrary phase. This means that imposing an exact match seems like a constraint as the phase cannot be predicted from the low resolution signal (which is what a GAN loss could achieve). The paper should present ablations on the use of the different losses. In particular, one of the main contributions is the inclusion of the loss measured in the learned feature space. The authors mention that not including it leads to audible artifacts. I think that more studies should be presented (including quantitative evaluations and audio samples). How where the hyper parameters chosen? is there a lot of sensitivity to their values? [A] van den Oord, Aaron, and Oriol Vinyals. \"Neural discrete representation learning.\" Advances in Neural Information Processing Systems. 2017. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the thoughtful and detailed response . Please see our enumerated responses below . We have also posted a comment above that summarized the changes in the latest revision . * Q1 * : From a technical perspective , I do not find the proposed approach very novel . It uses architectures following closely what has been done for Image super-resolution . I am not aware of an effective use of GANs in the audio processing domain . This would be a good point for the paper . However , the evidence presented does not seem very convincing in my view . While this is an audio processing paper , it lacks domain insights ( even the terminology feels borrowed from the image domain ) . Again , most of the modeling decisions seem to follow what has been done for images . The empirical results seem good , but the generated audio does not match the quality of the state-of-the-art . * A1 * : Thank you for this feedback . We agree that some of our high-level design choices are inspired by architectures from image processing literature . However , the main focus of our work is the exploration of such techniques and their adaptations to audio processing , which has been little-explored previously . Most importantly , we develop several new techniques and present analysis that is found in neither image nor audio processing literature . Indeed , autoregressive methods produce audio of excellent quality ; we have added a discussion of this to the paper , and also elaborate more in this topic in answer A3 below . * Q2 * : The presentation of the paper is correct . It would be good to list or summarize the contributions of this work . * A2 * : We agree - the introduction has been revised and now includes an explicit list of contributions . * Q3 * : Recent works have shown the amazing power of auto-regressive generative models ( WaveNet ) in producing audio signals . This is , as far as I know , the state-of-the-art in audio generation . The authors should motivate why the proposed model is better or worth studying in light of those approaches . In particular , a recent work [ A ] has shown very high quality results in the problem of speech conversion ( which seems harder than bandwidth extension ) . It would seem to me that applying such models to the bandwith extension task should also lead to very high quality results as well . What is the advantage of the proposed approach ? Would a WaveNet decoder also be improved by including these auxiliary losses ? * A3 * : We agree and note in the paper that auto-regressive models are indeed a promising avenue for audio generation . The primary difference from our work is that auto-regressive methods require an inference pass to generate a single output sample , with an input sequence that grows with each inference inference pass . This process is computationally intensive - for instance , at 16 KHz , an optimized Wavenet requires ~1.5 minutes to generate one second of audio [ C ] . We were recently made aware of a Wavenet variant that alleviates the issue of slow sample generation with a model distillation/student-teacher method [ B ] . As you correctly point out , Wavenet models can be improved with these auxiliary losses , and the Wavenet variant in [ B ] actually integrates a feature loss based on a speech phoneme-classifier network . This supports our view that our work is not in conflict with Wavenet and other auto-regressive methods , but rather augments them and can be used successfully in conjunction . * Q4 * : While the audio samples seem to be good , they are also a bit noisy even compared with the baseline . This is not the case in the samples generated by [ A ] ( which is of course a different problem ) . * A4 * : We did notice that in some samples , especially at higher upsampling rates , there are instances of noise on utterances with significant high-frequency content ( e.g. , fricatives and aspiration ) . We are not entirely certain on the cause of this noise , but we suspect that it is related to inherent ambiguity in the phase and magnitude of high frequency signals . Furthermore , we found that this noise is present even if we replace the unsupervised feature loss with other conventional feature losses . We made sure to include samples at high up-sampling ratios that included this noise in the user study , and the study indicated that users preferred audio produced by our method in spite of spurious noise . Nevertheless we have added a note in the paper regarding this problem . * Q5 * : The qualitative results are evaluated using PESQ . While this is a good proxy it is much better to perform blind tests with listeners . That would certainly improve the paper . * A5 * : Thanks for this point . We have acted on this recommendation and now have results from a user study in the Experiments section . [ B ] van Den Oord et al. \u201c Parallel WaveNet : Fast High-Fidelity Speech Synthesis. \u201d ICML 2018 ."}, "1": {"review_id": "H1eH4n09KX-1", "review_text": "This paper presents a GAN-based method to perform audio super-resolution. In contrast to previous work, this work uses auto-encoder to obtain feature losses derived from unlabeled data. Comments: (1) Redundant comma: \u201cfilters with very large receptive fields are required to create high quality, raw audio\u201d. (2) There are some state-of-the-art non-autoregressive generative models for audio waveform e.g., parallel wavenet, clarinet. One may properly discuss them in related work section. Although GAN performs very well for images, it hasn't obtained any compelling results for raw audios. Still, it\u2019s very interesting to explore that. Any nontrivial insight would be highly appreciated. (3) In multiscale convolutional layers, it seems only larger filter plays a significant role. What if we omit small filter, e.g., 3X1? (4) It seems the proposed MU-GAN introduces noticeable noise in the upsampled audios. Pros: - Interesting idea and fascinating problem. Cons: - The results are fair. I didn\u2019t see big improvement over previous work (Kuleshov et al., 2017). I'd like to reconsider my rating after the rebuttal. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the feedback . Please see our enumerated responses below . We have also posted a comment above that summarized the changes in the latest revision . * Q1 * : Redundant comma : \u201c filters with very large receptive fields are required to create high quality , raw audio \u201d . * A1 * : We have corrected this , thank you . * Q2 * : There are some state-of-the-art non-autoregressive generative models for audio waveform e.g. , parallel wavenet , clarinet . One may properly discuss them in related work section . Although GAN performs very well for images , it has n't obtained any compelling results for raw audios . Still , it \u2019 s very interesting to explore that . Any nontrivial insight would be highly appreciated . * A2 * : Thanks for pointing this out . We have added a discussion of these works , and a comparison to other GAN-based methods in the Related Works section . * Q3 * : In multiscale convolutional layers , it seems only larger filter plays a significant role . What if we omit small filter , e.g. , 3X1 ? * A3 * : This is a good point - we found that small filters do play a marginal but noticeable role in audio quality and speed of convergence . We suspect that while smaller filters are comparatively less powerful , they are easier to optimize and may have an important role in fitting many of the \u201c easy \u201d components in audio signals . Smaller filters also have lower overhead in terms of computational requirements , making our networks ( which have hundreds of feature maps in some layers ) feasible to train in a few days or less with a single GPU . * Q4 * : It seems the proposed MU-GAN introduces noticeable noise in the upsampled audios . * A4 * : Thanks for this comment - we noticed this as well . We have copied our answer A4 to reviewer # 1 below : We did notice that in some samples , especially at higher upsampling rates , there are instances of noise on utterances with significant high-frequency content ( e.g. , fricatives and aspiration ) . We are not entirely certain on the cause of this noise , but we suspect that it is related to inherent ambiguity in the phase and magnitude of high frequency signals . Furthermore , we found that this noise is present even if we replace the unsupervised feature loss with other conventional feature losses . Nevertheless , we made sure to include samples at high up-sampling ratios that included this noise in the user study , which indicated that users preferred audio produced by our method in spite of spurious noise . We have added a note in the paper regarding this problem . * Q5 * : The results are fair . I didn \u2019 t see big improvement over previous work ( Kuleshov et al. , 2017 ) . * A5 * : We appreciate the criticism . We wanted to highlight that while our baselines are based on the work from Kuleshov et al. , 2017 , our primary baseline for evaluation ( U-net8 ) is a much deeper and more powerful model compared to the model evaluated in Kuleshov et al. , 2017 . For a more direct comparison with Kuleshov et al. , 2017 , see the results for U-net4 in the Experiments section , which are taken from the authors \u2019 paper ."}, "2": {"review_id": "H1eH4n09KX-2", "review_text": "PRO\u2019s: +well-written +nice overall system: GAN framework for super-sampling audio incorporating features from an autoencoder +some good-sounding examples CON\u2019s: -some confusing/weakly-presented parts (admittedly covering lots of material in short space) -I am confused about the evaluation; would like additional qualitative/observational understanding of what works, including more on how the results differ from baseline SUMMARY: The task addressed in this work is: given a low-resolution audio signal, generate corresponding high-quality audio. The approach is a generative neural network that operates on raw audio and train within a GAN framework. Working in raw sample-space (e.g. pixels) is known to be challenging, so a stabilizing solution is to incorporate a feature loss. Feature loss, however, usually requires a network trained on a related task, and if such a net one does not already exist, then building one can have its own (possibly significant) challenges. In this work, the authors avoid this auxiliary challenge by using unsupervised feature losses, taking advantage of the fact that any audio signal can be downsampled, and therefore one has the corresponding upsampled signal as well. The training framework is basically that of a GAN, but where, rather than providing the generator with a low-dimensional noise signal input, they provide the generator with the subsampled audio signal. The architecture includes a generator ( G(lo-fidelity)=high-fidelity ), a discriminator ( D(high-fidelity) = real or by super-sampled ? ), and an autoencoder ( \\phi( signal x) = features of signal x at AE\u2019s bottleneck). COMMENTS: The generator network appears to be nearly identical to that of Kuleshov et al (2017)-- which becomes the baseline-- and so the primary contribution differentiating this work is the insertion of that network into a GAN framework along with the additional feature-based loss term. This is overall a nice problem and a nice approach! In that light, I believe that there is a new focus in this work on the perceptual quality of the outputs, as compared to (Kuleshov et al 2017). I would therefore ideally like to see (a) some attempts at perceptually evaluating the resulting output (beyond PESQ, e.g. with human subjects and with the understanding that, e.g. not all AMT workers have the same aural discriminative abilities themselves), and/or (b) more detailed associated qualitative descriptions/visualization of the super-sampled signal, perhaps with a few more samples if that would help. That said, I understand that there are page/space limitations. (more on this next) Given the similarity of the U-net architectures to (Kuleshov et al 2017), why not move some of those descriptions to the appendix? For example, I found the description and figure illustrating the \u201csuperpixel layers\u201d to be fairly uninformative: I see that the figure shows interleaving and de-interleaving, resulting in trading-off dimensionalities/ranks/etc, and we are told that this helps with well-known checkerboard artifacts, but I was confused about what the white elements represent, and the caption just reiterated that resolution was being increased and decreased. Overall, I didn\u2019t really understand exactly the role that this plays in the system; I wondered if it either needed a lot more clarification (in an appendix?), or just less space spent on it, but keeping the pointers to the relevant references. It seems that the subpixel layer was already implemented in Kuleshov 2017, with some explanation, yet in the present work a large table (Table 1(b)) is presented showing that there is no difference in quality metrics, and the text also mentions that there is no significant perceptual difference in audio. If the subpixel layer were explained in detail, and with justification, then I would potentially be OK with the negative results, but in this case it\u2019s not clear why spend this time on it here. It\u2019s possible that there is something simple about it that I am not understanding. I\u2019m open to being convinced. Otherwise, why not just write: \u201cFollowing (Kuleshov et al 2017), we use subpixel layers (Shi et al) [instead of ...] to speed up training, although we found that they make no significant perceptual effects.\u201d or something along those lines, and leave it at that? I did appreciate the descriptions of models\u2019 sensitivity to size/structure of the conv filters, importance of the res connections, etc. My biggest confusion was with the evaluation & results: Since the most directly related work was (Kuleshov 2017), I compared the super resolution (U-net) samples on that website (https://kuleshov.github.io/audio-super-res/ ) to the samples provided for the present work ( https://sites.google.com/view/unsupervised-audiosr/home ) and I was a bit confused, because the quality of the U-net samples in (Kuleshov 2017) seemed to be perceptually significantly better than the quality of the Deep CNN (U-net) baseline in the present work. Perhaps I am in error about this, but as far as I can tell, the superresolution in (Kuleshov et al 2017) is significantly better than the Deep CNN examples here. Is this a result of careful selection of examples? I do believe what I hear, e.g. that the MU-GAN8 is clearly better on some examples than the U-net8. But then for non-identical samples, how come U-net4 actually generally sounds better than U-net8? That doesn\u2019t make immediate sense either (assuming no overfitting etc). Is the benefit in moving from U-net4 to U-net8 within a GAN context but then stabilizing it with the feature-based loss? If so, then how does MU-GAN8 compare to U-net4? Would there be any info for the reader by doing an ablation removing the feature loss from the GAN framework? etc. I guess I would like to get a better understanding of what is actually going on, even if qualitative. Is there any qualitative or anecdotal observation about which \u201ctypes\u201d of samples one system works better on than another? For example, in the provided examples for the present paper, it seemed to be the case that perhaps the MU-GAN8 was more helpful for supersampling female voices, which might have more high-frequency components that seem to get lost when downsampling, but maybe I\u2019m overgeneralizing from the few examples I heard. Some spectrograms might be helpful, since they do after all convey some useful information despite not telling much of the perceptual story. For example, are there visible but inaudible artifacts? Are such artifacts systematic? Were individual audio samples represented as a one-hot encoding, or as floats? (I assume floats since there was no mention of sampling from a distribution to select the value). A couple of typos: descriminator \u2192 discriminator pg 6 \u201cImpact of superpixel layers\u201d -- last sentence of 2nd par is actually not a sentence. \u201cthe reduction in convolutional kernels prior to the superpixel operation.\u201d Overall, interesting work, and I enjoyed reading it. If some of my questions around evaluation could be addressed-- either in a revision, or in a rebuttal (e.g. if I completely misunderstood something)-- I would gladly consider revising my rating (which is currently somewhere between 6 and 7). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the thoughtful and detailed review . Please see our enumerated responses below . We have also posted a comment above that summarized the changes in the latest revisio * Q1 * : The generator network appears to be nearly identical to that of Kuleshov et al ( 2017 ) -- which becomes the baseline -- and so the primary contribution differentiating this work is the insertion of that network into a GAN framework along with the additional feature-based loss term . This is overall a nice problem and a nice approach ! In that light , I believe that there is a new focus in this work on the perceptual quality of the outputs , as compared to ( Kuleshov et al 2017 ) . I would therefore ideally like to see ( a ) some attempts at perceptually evaluating the resulting output ( beyond PESQ , e.g.with human subjects and with the understanding that , e.g.not all AMT workers have the same aural discriminative abilities themselves ) , and/or ( b ) more detailed associated qualitative descriptions/visualization of the super-sampled signal , perhaps with a few more samples if that would help . That said , I understand that there are page/space limitations . ( more on this next ) * A1 * : Thank you for the detailed feedback . While time was short , we were able to add a subjective user study to the paper - see the updated Experiments section . We also added more qualitative discussion of super-sampled audio , with associated spectrograms . We are currently updating the web page with more audio samples as well and will update it shortly . * Q2 * : Given the similarity of the U-net architectures to ( Kuleshov et al 2017 ) , why not move some of those descriptions to the appendix ? * A2 * : Thanks for this feedback - indeed , we found that our original architectural descriptions were overly detailed . We have removed much of this unnecessary detail , and plan to make a minor revision later today with additional architectural parameters in the appendix . * Q3 * : Overall , I didn \u2019 t really understand exactly the role that [ superpixel ] plays in the system ; I wondered if it either needed a lot more clarification ( in an appendix ? ) , or just less space spent on it , but keeping the pointers to the relevant references . It seems that the subpixel layer was already implemented in Kuleshov 2017 , with some explanation , yet in the present work a large table ( Table 1 ( b ) ) is presented showing that there is no difference in quality metrics , and the text also mentions that there is no significant perceptual difference in audio . If the subpixel layer were explained in detail , and with justification , then I would potentially be OK with the negative results , but in this case it \u2019 s not clear why spend this time on it here . It \u2019 s possible that there is something simple about it that I am not understanding . I \u2019 m open to being convinced . Otherwise , why not just write : \u201c Following ( Kuleshov et al 2017 ) , we use subpixel layers ( Shi et al ) [ instead of ... ] to speed up training , although we found that they make no significant perceptual effects. \u201d or something along those lines , and leave it at that ? * A3 * : We concede that the presentation of superpixel and subpixel layers was somewhat confusing ; we have revised the description in the Methods and Experiments sections to hopefully clarify this . One important detail we want to clarify is that while subpixel layers have been previously evaluated , no previous works have attempted to use its simple inverse to * decrease * spatial resolution ( previous works use strided convolution or pooling ) . Our paper evaluates this somewhat obvious inverse operator , referred to as a superpixel layer , and finds that it actually reduces training time without loss of performance . * Q4 * : Some spectrograms might be helpful , since they do after all convey some useful information despite not telling much of the perceptual story . For example , are there visible but inaudible artifacts ? Are such artifacts systematic ? * A4 * : Indeed , while we were not able to include spectrograms in the initial draft , the revision includes spectrograms of super-resolved audio , as well as an example produced by a GAN that exhibits the artifacts we mentioned in the paper . To explicitly answer your question , we find that artifacts introduced by GANs are generally systematic ( e.g. , high-frequency whines ) , and both visible and audible . * Q5 * : Were individual audio samples represented as a one-hot encoding , or as floats ? ( I assume floats since there was no mention of sampling from a distribution to select the value ) . * A5 * : You are correct - the individual audio samples are 32-bit ( single-precision ) floats . We have added a short footnote in the Method section to clarify this . Thanks ! * Q6 * : A couple of typos : \u2026 * A6 * : Thank you , these have all been corrected ."}}