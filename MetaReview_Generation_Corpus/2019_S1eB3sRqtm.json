{"year": "2019", "forum": "S1eB3sRqtm", "title": "Exploring Curvature Noise in Large-Batch Stochastic Optimization", "decision": "Reject", "meta_review": "Dear authors,\n\nYour proposition of adding a noise scaling with the diagonal of the gradient covariance to the updates as a middle-ground between the identity and the full covariance is interesting and tackles the timely question of the links between optimization and generalization.\n\nHowever, the reviewers had concerns about the experiments that did not reveal to which extent each trick had an influence.\nI would like to add that, even though the term Fisher is used for both the true Fisher and tne empirical one, these two matrices encore very different kind of information. In particular, the latter is only defined when there is a dataset. Hence, your case study  (section 3.2) which uses the true Fisher does not apply to the empirical Fisher.\n\nI encourage the authors to pursue in this direction but to update the experimental section in order to highlight the impact of each technique used.", "reviews": [{"review_id": "S1eB3sRqtm-0", "review_text": "In this paper, the authors propose a method to close the generalization gap that arises in training DNNs with large batch. The author reasons about the effectiveness in SGD small batch training by looking at the curvature structure of the noise. Instead of using the na\u00efve empirical fisher matrix, the authors propose to use diagonal fisher noise for large batch SGD training for DNNs. The proposed method is shown empirically to achieve both comparable generalization and the training speedup compared to small batch training. A convergence analysis is provided for the proposed method under convex quadratic setting. The idea of exploring the curvature information in the noise in SGD has been studied in (Hoffer et al. 2017). The difference between this approach and the proposed method in the paper is the use of diagonal fisher instead of the empirical fisher. Although there is convergence analysis provided under convex quadratic setting, I feel that the motivation behind using diagonal fisher for faster convergence is not clear to me, although in the experiment part, the comparison of some of the statistics of diagonal fisher appear similar to the small batch SGD. The intuition of using diagonal fisher for faster convergence in generalization performance is still missing from my perspective. In the convergence analysis, as there is a difference between the full fisher and diagonal fisher in the Tr(C\u2019AC) term. It would be interesting to see the effect of how this term play on convergence rate, and also how this term scale with batch size. But this is more of a minor issue as we are mostly caring about its generalization performance which is different from optimization error convergence. In the experiments section, the authors claim that noise structure is only important for the first 50 epochs. But it would be better if the authors could show experimental results of using the same training method all the way during the experiment. The experiments are conducted on MNIST and CIFAR10 and 100, which I feel is a bit insufficient for a paper that deals with generalization gap in large batch. As in large batch training, we care more about bigger dataset such as ImageNet, and hence I would expect results reported on various models on ImageNet. Another interesting thing to show would be the generalization error over epochs for different methods, which could give a more detailed characterization of the behavior of different methods. Overall, I feel the motivation and intuition behind the proposed method is not clear enough and experimental studies are not sufficient for understanding the behavior of the proposed method as an empirical paper. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We like to thank the reviewer for the comments and suggestions to improve this paper . We address the reviewer \u2019 s concerns : - \u201c The idea of exploring the curvature information in the noise in SGD has been studied in ( Hoffer et al.2017 ) .The difference between this approach and the proposed method in the paper is the use of diagonal fisher instead of the empirical fisher. \u201d Hoffer et al , 2017 discussed intrinsic curvature noise in SGD ( as do many other recent papers such as Smith et al , 2017 , Chaudhari et al , 2017 , etc . ) . The proposed solutions in all these papers did not explicitly implement any form of empirical Fisher gradient noise . The main approach implemented in Hoffer et al , 2017 is the use of Ghost-Batch Normalization ( GBN ) . GBN , similar to usual BN , should be thought as an architectural modification rather than incorporating curvature noise information . In addition , the training procedure is elongated for LB . However , extending the training regime for LB is against the very goal of using LB in the first place . In contrast , we show that using diagonal Fisher noise preserves the desirable convergence performance of LB training per parameter update and significantly improves generalization performance of LB without training longer . - \u201c Although there is convergence analysis provided under convex quadratic setting , I feel that the motivation behind using diagonal fisher for faster convergence is not clear to me , although in the experiment part , the comparison of some of the statistics of diagonal fisher appear similar to the small batch SGD . The intuition of using diagonal fisher for faster convergence in generalization performance is still missing from my perspective. \u201d The intuition can be understood in the following way . In Fig 1 , we notice that the empirical full Fisher update is orthogonal to the loss curvature . Thus , adding full Fisher noise to the gradients gives large perturbation in the high curvature direction , which leads to a higher expected training loss . In comparison , only taking the diagonal results in a smaller perturbation in the high curvature direction , which leads to a smaller expected training loss . This is quantified in Theorem 3.1 for the convex quadratic setting . The overall convergence rate of the bound is O ( 1/k ) but the constant is Tr ( C^TAC ) . The difference between using full Fisher ( C=\\sqrt { A } ) and diagonal Fisher is ( C=\\sqrt { diag A } ) is exactly the difference between their Frobenius norms . We show that this carries over to the deep learning setting : in Figure 3a ) , we show that the Frobenius norm of full Fisher is much larger than that of diagonal Fisher . Finally , in Fig 3c ) , we showed that FB ( full-batch ) + diagonal Fisher attains much faster training than FB + full Fisher , which verifies the above-mentioned statement . - \u201c In the convergence analysis , as there is a difference between the full fisher and diagonal fisher in the Tr ( C \u2019 AC ) term . It would be interesting to see the effect of how this term play on convergence rate , and also how this term scale with batch size . But this is more of a minor issue as we are mostly caring about its generalization performance which is different from optimization error convergence. \u201d The difference between the Frobenius norm of the Fisher matrix and diagonal Fisher matrix is independent of the batch-sizes involved . That being said , the batch-sizes are used in the coefficient \\sqrt { N-M } { NM } before the diagonal Fisher term in Algorithm 1 . - \u201c The experiments are conducted on MNIST and CIFAR-10/100 , which I feel is a bit insufficient for a paper that deals with generalization gap in large batch . As in large batch training , we care more about bigger dataset such as ImageNet , and hence I would expect results reported on various models on ImageNet. \u201c The reason that all of our experiments were conducted on smaller models and datasets such as MNIST , CIFAR-10/100 are due to constraints in computing resources . We did not have the computing power or budget to run experiments on ImageNet . However , we feel that the empirical analysis given in our paper addresses key research questions concerning both convergence and generalization of LB training . - \u201c Another interesting thing to show would be the generalization error over epochs for different methods , which could give a more detailed characterization of the behavior of different methods. \u201d Please see Appendix E in the latest version of our paper . In light of the changes that we made suggested by the reviewer as well as the contributions in our meta-review , we would appreciate if the reviewer can reconsider their score ."}, {"review_id": "S1eB3sRqtm-1", "review_text": "Summary: This paper proposes the method which improves the generalization performance of large-batch SGD by adding the diagonal Fisher matrix noise. In the theoretical analysis, it is shown that gradient descent with the diagonal noise is faster than it with the full-matrix noise on positive-quadratic problems. Moreover, the effectiveness of the method is verified in several experiments. Comments: The idea of the proposed method is based on the following observations and assumptions: - Stochastic gradient methods with small-batch can be regarded as a gradient method with Fisher matrix noise. - The generalization ability is comparable between diagonal Fisher and full Fisher matrix. - Gradient method with diagonal Fisher is faster than that with full Fisher matrix. This conjecture is theoretically validated for the case of quadratic problems. In short, the algorithm derivation seems to be reasonable and the derived algorithm is executable. Moreover, experiments are well conducted and the results are also good. Minor comment: - There is a typo in the next line of Eq. (2): \\nabla_{M_L} (\\theta_k)} -> \\nabla_{M_L} L(\\theta_k)} In addition, the notation \"l_i\" is not defined at this time. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the positive comments regarding our work . \u201c There is a typo in the next line of Eq . ( 2 ) : \\nabla_ { M_L } ( \\theta_k ) } - > \\nabla_ { M_L } L ( \\theta_k ) } \u201d We have corrected this typo in the latest version of our paper Since the reviewer \u2019 s positive assessment of our paper and in consideration of the novel contributions of this paper given in our meta-review , we would very much appreciate if the reviewer can increase their score ."}, {"review_id": "S1eB3sRqtm-2", "review_text": "It has previously been observed that training deep networks using large batch-sizes leads to a larger generalization gap compared to the gap when training with a relatively small batch-size. This paper proposes to add noise sampled from diagonal \"empirical\" Fisher matrix to the large batch gradient as a method for closing the generalization gap. The authors motivate the use of empirical Fisher for sampling noise by arguing that the covariance of gradients from small batch-sizes can be seen as approximately equal to a scaled version of the Fisher matrix. It is then pointed out that using the Fisher matrix directly to sample noise could in principle close the generalization gap but would lead to slow converegence similar to SGD with a small batch-size. The authors then claim that the convergence speed is better when noise is sampled from the diagonal Fisher matrix instead of the full Fisher matrix. This claim is proven in theory for a convex quadratic loss surface and experiments are conducted to empirically verify this claim both in the quadratic setting are for realistic deep networks. Finally an efficient method for sampling noise from the diagonal empirical Fisher matrix is proposed. Comments: I think the paper is very well written and the results are presented clearly. In terms of novelty, I found the argument about convergence using diagonal Fisher being faster compared with full Fisher quite interesting, and its application for large batch training to be insightful. As a minor comment, for motivating theorem 3.1, it is pointed out by the authors that the diagonal Fisher acts as an approximation of the full Fisher and hence their regularization effects should be similar while convergence should be faster for diagonal Fisher. As a caveat, I think the authors should also point out that the convergence rate would be best when C is set to 0 in the result of the theorem. This implies no noise is used during SGD updates. However, this would imply the regularization effect from the noise will also vanish which would lead to poor generalization. However, there is a crucial detail that makes the main argument of the paper weak. In the main experiments in section 4.3, for the proposed large batch training method, the authors mention that they use a small batch-size of 128 for the first 50 epochs similar to Smith et al (2017) and then switch to the large batch-size of 4096, at which point, the learning rate is linearly scaled proportional to the large batch-size with a warmup scheme similar to Goyal et al (2017) and ghost batch normalization is used similar to Hoffer et al (2017). The former two tricks have individually been shown on their own to close the generalization gap for large batch-size training on large datasets like ImageNet. This paper combines these tricks and adds noise sampled from the diagonal Fisher matrix on top when switching to large batch-size after epoch 50 and reports experiments on smaller datasets-- MNIST, Fashion MNIST and the CIFAR datasets. Finally, the accuracy numbers for the proposed method is only marginally better than the baseline where isotropic noise is added to the large batch-size gradient. For these reasons, I do not consider the proposed method a significant improvement over existing techniques for closing the generalization gap for large batch training. There is also a statement in the paper that is problematic but can be fixed by re-writing. In the paper, empirical Fisher matrix, as termed by the authors in the paper, refers to the Fisher matrix where the target values in the dataset is used as the output of the model rather than sampling it from the model itself as done for computing the true Fisher matrix. This empirical (diagonal) Fisher matrix is used to sample noise which is added to the large batch gradient in the proposed method. It is mentioned that the covariance of the noise in small batch SGD is exactly same as the empirical Fisher matrix. This claim is premised on the argument that the expected gradient (over dataset) is unconditionally roughly 0, i.e., throughout the training. This is absolutely false. If this was the case, gradient descent (using full dataset) should not be able to find minima and this is far from the truth. Even if we compare the scale of expected gradient to the mini-batch gradient (for small batch-size), the scale of these two gradients at any point during training (using say small batch-size SGD) is of the same order. I am saying the latter statement from my personal experience. The authors can verify this as well. Overall, while I found the theoretical argument of the paper to be mostly interesting, I was dissapointed by the experimental details as they make the gains from the proposed method questionable when considered in isolation from the existing methods that close the generalization gap for large batch training.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We apologize for the confusion in writing in Section 4.3 and the issues that stemmed from it . We clarify our experimental setup here . All of the results concerning LB=4096 , the last three columns presented in Table 1 are strictly LB during the entire training process . We did not use SB=128 for the first 50 epochs of training . We did perform an initial experiment where we used SB for first 50 epochs and LB for 150 epochs to suggest that the noise is relevant in the early stages of training . The results of this particular experiment are reported as \u201c BatchChange \u201d in Appendix F. We iterate that this is not a preferred solution ; it is not strictly LB training during the entire process and hence it sacrifices the benefits of LB training in the early stages of training . In upcoming versions of this paper , we will modify Section 4.3 along with other sections of the paper to improve clarity and presentation of the paper . We will address your other comments and concerns in later rebuttals ."}], "0": {"review_id": "S1eB3sRqtm-0", "review_text": "In this paper, the authors propose a method to close the generalization gap that arises in training DNNs with large batch. The author reasons about the effectiveness in SGD small batch training by looking at the curvature structure of the noise. Instead of using the na\u00efve empirical fisher matrix, the authors propose to use diagonal fisher noise for large batch SGD training for DNNs. The proposed method is shown empirically to achieve both comparable generalization and the training speedup compared to small batch training. A convergence analysis is provided for the proposed method under convex quadratic setting. The idea of exploring the curvature information in the noise in SGD has been studied in (Hoffer et al. 2017). The difference between this approach and the proposed method in the paper is the use of diagonal fisher instead of the empirical fisher. Although there is convergence analysis provided under convex quadratic setting, I feel that the motivation behind using diagonal fisher for faster convergence is not clear to me, although in the experiment part, the comparison of some of the statistics of diagonal fisher appear similar to the small batch SGD. The intuition of using diagonal fisher for faster convergence in generalization performance is still missing from my perspective. In the convergence analysis, as there is a difference between the full fisher and diagonal fisher in the Tr(C\u2019AC) term. It would be interesting to see the effect of how this term play on convergence rate, and also how this term scale with batch size. But this is more of a minor issue as we are mostly caring about its generalization performance which is different from optimization error convergence. In the experiments section, the authors claim that noise structure is only important for the first 50 epochs. But it would be better if the authors could show experimental results of using the same training method all the way during the experiment. The experiments are conducted on MNIST and CIFAR10 and 100, which I feel is a bit insufficient for a paper that deals with generalization gap in large batch. As in large batch training, we care more about bigger dataset such as ImageNet, and hence I would expect results reported on various models on ImageNet. Another interesting thing to show would be the generalization error over epochs for different methods, which could give a more detailed characterization of the behavior of different methods. Overall, I feel the motivation and intuition behind the proposed method is not clear enough and experimental studies are not sufficient for understanding the behavior of the proposed method as an empirical paper. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We like to thank the reviewer for the comments and suggestions to improve this paper . We address the reviewer \u2019 s concerns : - \u201c The idea of exploring the curvature information in the noise in SGD has been studied in ( Hoffer et al.2017 ) .The difference between this approach and the proposed method in the paper is the use of diagonal fisher instead of the empirical fisher. \u201d Hoffer et al , 2017 discussed intrinsic curvature noise in SGD ( as do many other recent papers such as Smith et al , 2017 , Chaudhari et al , 2017 , etc . ) . The proposed solutions in all these papers did not explicitly implement any form of empirical Fisher gradient noise . The main approach implemented in Hoffer et al , 2017 is the use of Ghost-Batch Normalization ( GBN ) . GBN , similar to usual BN , should be thought as an architectural modification rather than incorporating curvature noise information . In addition , the training procedure is elongated for LB . However , extending the training regime for LB is against the very goal of using LB in the first place . In contrast , we show that using diagonal Fisher noise preserves the desirable convergence performance of LB training per parameter update and significantly improves generalization performance of LB without training longer . - \u201c Although there is convergence analysis provided under convex quadratic setting , I feel that the motivation behind using diagonal fisher for faster convergence is not clear to me , although in the experiment part , the comparison of some of the statistics of diagonal fisher appear similar to the small batch SGD . The intuition of using diagonal fisher for faster convergence in generalization performance is still missing from my perspective. \u201d The intuition can be understood in the following way . In Fig 1 , we notice that the empirical full Fisher update is orthogonal to the loss curvature . Thus , adding full Fisher noise to the gradients gives large perturbation in the high curvature direction , which leads to a higher expected training loss . In comparison , only taking the diagonal results in a smaller perturbation in the high curvature direction , which leads to a smaller expected training loss . This is quantified in Theorem 3.1 for the convex quadratic setting . The overall convergence rate of the bound is O ( 1/k ) but the constant is Tr ( C^TAC ) . The difference between using full Fisher ( C=\\sqrt { A } ) and diagonal Fisher is ( C=\\sqrt { diag A } ) is exactly the difference between their Frobenius norms . We show that this carries over to the deep learning setting : in Figure 3a ) , we show that the Frobenius norm of full Fisher is much larger than that of diagonal Fisher . Finally , in Fig 3c ) , we showed that FB ( full-batch ) + diagonal Fisher attains much faster training than FB + full Fisher , which verifies the above-mentioned statement . - \u201c In the convergence analysis , as there is a difference between the full fisher and diagonal fisher in the Tr ( C \u2019 AC ) term . It would be interesting to see the effect of how this term play on convergence rate , and also how this term scale with batch size . But this is more of a minor issue as we are mostly caring about its generalization performance which is different from optimization error convergence. \u201d The difference between the Frobenius norm of the Fisher matrix and diagonal Fisher matrix is independent of the batch-sizes involved . That being said , the batch-sizes are used in the coefficient \\sqrt { N-M } { NM } before the diagonal Fisher term in Algorithm 1 . - \u201c The experiments are conducted on MNIST and CIFAR-10/100 , which I feel is a bit insufficient for a paper that deals with generalization gap in large batch . As in large batch training , we care more about bigger dataset such as ImageNet , and hence I would expect results reported on various models on ImageNet. \u201c The reason that all of our experiments were conducted on smaller models and datasets such as MNIST , CIFAR-10/100 are due to constraints in computing resources . We did not have the computing power or budget to run experiments on ImageNet . However , we feel that the empirical analysis given in our paper addresses key research questions concerning both convergence and generalization of LB training . - \u201c Another interesting thing to show would be the generalization error over epochs for different methods , which could give a more detailed characterization of the behavior of different methods. \u201d Please see Appendix E in the latest version of our paper . In light of the changes that we made suggested by the reviewer as well as the contributions in our meta-review , we would appreciate if the reviewer can reconsider their score ."}, "1": {"review_id": "S1eB3sRqtm-1", "review_text": "Summary: This paper proposes the method which improves the generalization performance of large-batch SGD by adding the diagonal Fisher matrix noise. In the theoretical analysis, it is shown that gradient descent with the diagonal noise is faster than it with the full-matrix noise on positive-quadratic problems. Moreover, the effectiveness of the method is verified in several experiments. Comments: The idea of the proposed method is based on the following observations and assumptions: - Stochastic gradient methods with small-batch can be regarded as a gradient method with Fisher matrix noise. - The generalization ability is comparable between diagonal Fisher and full Fisher matrix. - Gradient method with diagonal Fisher is faster than that with full Fisher matrix. This conjecture is theoretically validated for the case of quadratic problems. In short, the algorithm derivation seems to be reasonable and the derived algorithm is executable. Moreover, experiments are well conducted and the results are also good. Minor comment: - There is a typo in the next line of Eq. (2): \\nabla_{M_L} (\\theta_k)} -> \\nabla_{M_L} L(\\theta_k)} In addition, the notation \"l_i\" is not defined at this time. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the positive comments regarding our work . \u201c There is a typo in the next line of Eq . ( 2 ) : \\nabla_ { M_L } ( \\theta_k ) } - > \\nabla_ { M_L } L ( \\theta_k ) } \u201d We have corrected this typo in the latest version of our paper Since the reviewer \u2019 s positive assessment of our paper and in consideration of the novel contributions of this paper given in our meta-review , we would very much appreciate if the reviewer can increase their score ."}, "2": {"review_id": "S1eB3sRqtm-2", "review_text": "It has previously been observed that training deep networks using large batch-sizes leads to a larger generalization gap compared to the gap when training with a relatively small batch-size. This paper proposes to add noise sampled from diagonal \"empirical\" Fisher matrix to the large batch gradient as a method for closing the generalization gap. The authors motivate the use of empirical Fisher for sampling noise by arguing that the covariance of gradients from small batch-sizes can be seen as approximately equal to a scaled version of the Fisher matrix. It is then pointed out that using the Fisher matrix directly to sample noise could in principle close the generalization gap but would lead to slow converegence similar to SGD with a small batch-size. The authors then claim that the convergence speed is better when noise is sampled from the diagonal Fisher matrix instead of the full Fisher matrix. This claim is proven in theory for a convex quadratic loss surface and experiments are conducted to empirically verify this claim both in the quadratic setting are for realistic deep networks. Finally an efficient method for sampling noise from the diagonal empirical Fisher matrix is proposed. Comments: I think the paper is very well written and the results are presented clearly. In terms of novelty, I found the argument about convergence using diagonal Fisher being faster compared with full Fisher quite interesting, and its application for large batch training to be insightful. As a minor comment, for motivating theorem 3.1, it is pointed out by the authors that the diagonal Fisher acts as an approximation of the full Fisher and hence their regularization effects should be similar while convergence should be faster for diagonal Fisher. As a caveat, I think the authors should also point out that the convergence rate would be best when C is set to 0 in the result of the theorem. This implies no noise is used during SGD updates. However, this would imply the regularization effect from the noise will also vanish which would lead to poor generalization. However, there is a crucial detail that makes the main argument of the paper weak. In the main experiments in section 4.3, for the proposed large batch training method, the authors mention that they use a small batch-size of 128 for the first 50 epochs similar to Smith et al (2017) and then switch to the large batch-size of 4096, at which point, the learning rate is linearly scaled proportional to the large batch-size with a warmup scheme similar to Goyal et al (2017) and ghost batch normalization is used similar to Hoffer et al (2017). The former two tricks have individually been shown on their own to close the generalization gap for large batch-size training on large datasets like ImageNet. This paper combines these tricks and adds noise sampled from the diagonal Fisher matrix on top when switching to large batch-size after epoch 50 and reports experiments on smaller datasets-- MNIST, Fashion MNIST and the CIFAR datasets. Finally, the accuracy numbers for the proposed method is only marginally better than the baseline where isotropic noise is added to the large batch-size gradient. For these reasons, I do not consider the proposed method a significant improvement over existing techniques for closing the generalization gap for large batch training. There is also a statement in the paper that is problematic but can be fixed by re-writing. In the paper, empirical Fisher matrix, as termed by the authors in the paper, refers to the Fisher matrix where the target values in the dataset is used as the output of the model rather than sampling it from the model itself as done for computing the true Fisher matrix. This empirical (diagonal) Fisher matrix is used to sample noise which is added to the large batch gradient in the proposed method. It is mentioned that the covariance of the noise in small batch SGD is exactly same as the empirical Fisher matrix. This claim is premised on the argument that the expected gradient (over dataset) is unconditionally roughly 0, i.e., throughout the training. This is absolutely false. If this was the case, gradient descent (using full dataset) should not be able to find minima and this is far from the truth. Even if we compare the scale of expected gradient to the mini-batch gradient (for small batch-size), the scale of these two gradients at any point during training (using say small batch-size SGD) is of the same order. I am saying the latter statement from my personal experience. The authors can verify this as well. Overall, while I found the theoretical argument of the paper to be mostly interesting, I was dissapointed by the experimental details as they make the gains from the proposed method questionable when considered in isolation from the existing methods that close the generalization gap for large batch training.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We apologize for the confusion in writing in Section 4.3 and the issues that stemmed from it . We clarify our experimental setup here . All of the results concerning LB=4096 , the last three columns presented in Table 1 are strictly LB during the entire training process . We did not use SB=128 for the first 50 epochs of training . We did perform an initial experiment where we used SB for first 50 epochs and LB for 150 epochs to suggest that the noise is relevant in the early stages of training . The results of this particular experiment are reported as \u201c BatchChange \u201d in Appendix F. We iterate that this is not a preferred solution ; it is not strictly LB training during the entire process and hence it sacrifices the benefits of LB training in the early stages of training . In upcoming versions of this paper , we will modify Section 4.3 along with other sections of the paper to improve clarity and presentation of the paper . We will address your other comments and concerns in later rebuttals ."}}