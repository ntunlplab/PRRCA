{"year": "2020", "forum": "rJx0Q6EFPB", "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "decision": "Reject", "meta_review": "This paper proposes a new distillation-based method for using large pretrained models like BERT to produce much *smaller* fine-tuned target-task models. \n\nThis paper is low-borderline: It has merit and meets our basic standards, but owing to capacity limitations we had to give preference to papers we see as having a higher potential impact. Reviewers had some concerns about experimental design, but those seem to have been fully resolved after discussion. Reviewers were not convinced, even after some discussion, that the method and results were sufficiently novel and effective to have a substantial impact on the state of practice in this area.", "reviews": [{"review_id": "rJx0Q6EFPB-0", "review_text": "This paper proposes a new knowledge distillation method for BERT models. A number of modifications to the vanilla knowledge distillation method of Hinton et al (2015) are proposed. First, authors suggest adding L2 loss functions between alignment matrices, embedding layer values and prediction layer values. Second, authors propose run knowledge-distillation twice, once with the original pre-trained BERT model as teacher, and then again with task specific fine-tuned BERT as a new teacher. Third, authors emphasize the use of data augmentation for successful knowledge distillation. In Table 2, authors claim a significant lift across GLUE benchmarks with respect to other baseline methods with comparable model size. While the main contribution of this paper is the proposal of empirically useful techniques than theoretical development, the empirical results reported in this paper are somewhat puzzling. First of all, GLUE benchmark scores reported in Table 2 don't seem to be consistent with Table 1 of Sun et al (2019) for BERT-PKD ( https://arxiv.org/pdf/1908.09355.pdf ) or DistilBERT ( https://medium.com/huggingface/distilbert-8cf3380435b5 ). Indeed, BERT-PKD in Sun et al seems to significantly outperform TinyBERT on QNLI (89.0 vs 87.7) and RTE (65.5 vs 62.9), and the gap between BERT-PKD and TinyBERT on other tasks are much smaller if we take numbers reported in the original paper. In Table 6, ablation studies with different distillation objectives are reported. Quite surprisingly, without Transformer-layer distillation (No Trm) the performance drops quite significantly. This is unexpected, because baselines such as Sun et al and DistilBERT do not use the Transformer-layer distillation but much more competitive to full TinyBERT than TinyBERT without Transformer-layer distillation. Would there be a reason why TinyBERT is so critically dependent on Transformer-layer distillation? Similarly, the removal of data augmentation (Table 5, No DA) is so detrimental to the performance of the model that it makes me to suspect whether the most of gain is from successful data augmentation. Indeed, 'No DA' row of Table 5 is very close to the performance BERT-PKD in Table 4, although the number of layers is different (4 vs 6). In order for the research community to understand the contribution of proposed techniques more thoroughly, I suggest authors to conduct ablation studies with the simplest baseline. That is, rather than starting with the full TinyBERT model, start with a simple but competitive baseline like BERT-PKD, and only add one technique (DA, GD, Transformer-layer distillation) at a time so that readers shall understand what technique is the most important to be added to the baseline, and also whether some of the proposed techniques should always be used in combination. --- After Author Rebuttal: authors have addressed all of my concerns quite clearly. Additional experiments which targeted a specific design choice at a time made me much more convinced that the techniques proposed in this paper are useful not only for this particular context but also more broadly applicable.", "rating": "8: Accept", "reply_text": "Thank you for the helpful comments ! Q1 : GLUE scores reported in Table 2 don \u2019 t seem to be consistent . A1 : In Table2 , all the results of TinyBERT , BERT-PKD and DistilBERT are based on 4-layer architectures and evaluated on the TEST set of official GLUE benchmark . The results of Table 1 of Sun et al ( 2019 ) for BERT-PKD and the results of DistilBERT [ 1 ] are based on 6-layer architectures , and DistilBERT only reported their results on the DEV set of GLUE . As described in Appendix B ( \u201c Baseline setup \u201d ) , to ensure the correct implementations of BERT-PKD and DistilBERT , we firstly re-reproduced the reported results of baselines with 6-layer architecture , then we trained the baselines with 4-layer architecture by following the confirmed correct implementations , and evaluated them on the TEST set of official GLUE benchmarks . For a direct comparisons with BERT-PKD and DistilBERT , we here also present the results of 6-layer TinyBERT with the same architecture as the original BERT-PKD ( Sun et al. , 2019 ) and original DistilBERT [ 1 ] , and directly use the reported results of BERT-PKD and DistilBERT . As BERT-PKD and DistilBERT are evaluated on the test and dev set of GLUE , respectively . Thus , we present the results in the following two tables separately , and the results have been added to the Appendix E of our paper . Table : the comparisons between TinyBERT and BERT-PKD , and the results are evaluated on the test set of official GLUE tasks . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE ( 67k ) ( 3.7k ) ( 364k ) ( 393k ) ( 393k ) ( 105k ) ( 2.5k ) acc f1/acc f1/acc acc acc acc acc -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BERT_6-PKD ( Sun et al. , 2019 ) 92.0 85.0/79.9 70.7/88.9 81.5 81.0 89.0 65.5 ( M=6 ; d \u2019 =768 ; d \u2019 _i=3072 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Mcc refers to Matthews correlation and pear/spea refer to pearson/spearman . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - CoLA MNLI MNLI-mm MRPC QNLI QQP RTE SST-2 STS-B mcc acc acc f1/acc acc f1/acc acc acc pear/spea -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- DistilBERT [ 1 ] 42.5 81.6 81.1 88.3/82.4 85.5 87.7/90.6 60.0 92.7 84.5/85.0 ( M=6 ; d \u2019 =768 ; d \u2019 _i=3072 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"}, {"review_id": "rJx0Q6EFPB-1", "review_text": "The authors propose TinyBERT, a smaller version of BERT that is trained with knowledge distillation. The authors evaluate on the GLUE benchmark. Overall, I find the direction of this work exciting and making these large models smaller for practical use is an important research area. The authors provide various ablation experiments that provide insight into their method. The main contribution is experiments comparing various existing distillation methods to different parts of the model (embeddings, layers, prediction layer), so is not particularly novel in contributing new techniques for distillation. That being said, there is importance in contributing these results as they are very useful for others working in the area and on making smaller models. But I would expect the authors to be much more detailed in their experimental description and make it clear in the paper that the comparative baselines are fair and well tuned. Comments: 1. Can the authors please add details for how the model has been trained, such as the datasets used, the number of update steps, the batch size, etc. as well as the finetuning parameters that were cross validated for GLUE? It is difficult to tell in the current setting if the models are comparable to the baselines. The current paper doesn't seem like it could be reproduced. It is particularly important to detail how the finetuning was done, as this is very important for the smaller datasets in GLUE. 2. Is the learning of the distilled model only done on the training dataset, or there is data augmentation beyond the training set? What is the effect without data augmentation? 3. Unfortunately, the performance drop on the GLUE benchmark as shown in Table 2 is fairly large. The authors compare to BERT Small and DistilBERT and I like the baselines, but the claim that the model achieves comparable performance to BERT Base is not true. 4. Was the BERT Small model tuned, or the same learning parameters from BERT Base were used? 5. Can the authors clarify the inference time of BERT Small? The speed improvement of TinyBERT should be the same as BERT Small based on parameter size. 6. The authors experiment with distilling the embedding layer to reduce the number of parameters, why not reduce the parameter size by reducing the vocabulary size? Existing approaches to BERT training use BPE with ~30k vocabulary size or RoBERTa with ~50k vocabulary size, but large gains could be applied here by reducing the size or using softmax reduction techniques that were popular on full vocabulary language modeling datasets like wikitext-103 or billion word. 7. Can the authors please clarify the construction of Table 2? Are those results on the test set (e.g. evaluated on the official GLUE benchmark), or on the dev set? Where are the DistilBERT numbers on the test set coming from, as it is not reported in their paper? ", "rating": "3: Weak Reject", "reply_text": "Thank you for the helpful comments ! Reproducibility * * * We will release the source code , all the models ( including the general TinyBERT variants and task-specific TinyBERT models for each task in GLUE and SQuAD , so other researchers can easily reproduce the results in the paper ) , and all the training details for reproducibility , as soon as possible . * * * Q1 : Details for how the model has been trained . A1 : We have presented all the training details of TinyBERT and baselines in the Appendix B ( TinyBERT setup and Baselines setup ) , which includes the datasets used , the number of update steps , the batch size as well as the settings for fine-tuning . Our TinyBERT and baselines use the same hyper-parameters and datasets at both the pre-training and fine-tuning stages . We here list the main setting details as follows and other details can be referred in Appendix B . Table : the hyper-parameters of DistilBERT , TinyBERT and BERT_small ; BERT_PKD does not include the pre-training stage , we use the BERT_base released by google as the teacher . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - At Pre-training Stage -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Dataset English Wikipedia ( 2,500 M words ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Training steps ~350k ( 3epoch ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Batch size 256 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Max_seq_length refers to the maximum sequence length . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Learning rate batch size Epoch Max_seq_length -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - MNLI ( 392k ) 3e-5 32 3 128 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- QQP ( 363k ) 3e-5 32 3 128 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- The effect of TinyBERT without DA ( data augmentation ) is presented in the Table 5 ( the \u201c No DA \u201d row ) and section 4.4 . Q3 : Claim that model achieves comparable performance to BERT_base . A3 : Thanks for the suggestion , we have changed the related claims . Q4 : Was the BERT_small model tuned , or the same learning paramters from BERT_Base were used ? A4 : As described in the subsection \u201c Baselines setup \u201d of Appendix B , BERT_small and BERT_Base use the same hyper-parameters for learning . Q5 : Can the authors clarify the inference time of BERT Small ? A5 : Yes , BERT_small and TinyBERT have the same architecture , thus they have the same inference time . We have added the inference time of BERT_small in Table 3 ."}, {"review_id": "rJx0Q6EFPB-2", "review_text": "What is the task? Knowledge distillation of BERT What has been done before? Unlike prior works such as Distilled BiLSTMSOFT (Tang et al., 2019), BERT-PKD (Sun et al., 2019) and DistilBERT, this work i) Do knowledge distillation at pre training stage also in addition to fine tuning stage. ii) Student learns from all - embedding layers, attention matrices, hidden states, and final prediction layers. In BERT-PKD, student learns from the [CLS] hidden states of the teacher. What are the main contributions of the paper? Novel Transformer distillation method that is specially designed for knowledge distillation of the Transformer-based models. Novel two-stage learning framework which performs Transformer distillation at both the pre-training and task-specific learning stages Resulting TinyBERT being 7.5x smaller and 9.4x faster on inference and significantly outperforms other state-of-the-art baselines on BERT distillation. What are the key techniques used to tackle this task? Novel Transformer distillation method that is specially designed for knowledge distillation of the Transformer-based models. Novel two-stage learning framework which performs Transformer distillation at both the pre-training and task-specific learning stages What are the main results? Are they significant? Resulting TinyBERT being 7.5x smaller and 9.4x faster on inference and significantly outperforms other state-of-the-art baselines on BERT distillation with only \u223c28% parameters and \u223c31% inference time of them. Results show that three key procedures: TD (Task-specific Distillation), GD (General Distillation) and DA (Data Augmentation) are crucial for the proposed KD method. Proposed distillation objectives - Transformer-layer distillation (attention matrices and hidden states), embedding-layer distillation and prediction layer distillation are crucial for the proposed KD method. Weaknesses experimental results were not easily comparable to prior work so it is hard to say if claims are well-supported experimental results Questions Did authors try other values of lambda ", "rating": "6: Weak Accept", "reply_text": "Thank you for the helpful comments ! Q1 : Experimental results are not easily comparable to prior work . A1 : * * * Comparison results as shown in Table 2 , Table 3 and Table4 * * * The comparison results as shown in the Table 2 are all evaluated on the test set of the official GLUE tasks . As shown in the Table 3 , our TinyBERT , baselines BERT-PKD and DistilBERT , all have the same number of layers ( M=4 ) , and our TinyBERT has a relatively challenging setting with smaller hidden size ( d \u2019 =312 vs d \u2019 =768 ) and feedforward/filter size ( d \u2019 _i=1200 vs d \u2019 _i=3072 ) . If we increase the hidden size and feedforward/filter size of TinyBERT , it can obtain better performances , which is validated in our experiments in the Table 4 ( wider TinyBERT variants achieve better results ) . In the Table 4 , we also directly compared the performances of TinyBERT , BERT-PKD and DistilBERT with the same architecture settings ( M=6 ; d \u2019 =768 ; d \u2019 i=3072 ) , and TinyBERT has significantly better performances . * * * More complete comparisons with the same student architecture * * * For complete and direct comparisons with prior works , we here also present the results of TinyBERT ( M=6 ; d \u2019 =768 ; d \u2019 _i=3072 ) with the same architectures as used in the original BERT-PKD ( Sun et al. , 2019 ) and DistilBERT [ 1 ] papers . Since in the original papers , the BERT-PKD is evaluated on the TEST set , and the DistilBERT is evaluated on the DEV set . Thus , for a clear illustration , we present the results in the following two tables , separately , and the results have been added to the Appendix E of our paper . Table : the comparisons between TinyBERT and BERT-PKD , and the results are evaluated on the test set of official GLUE tasks . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE ( 67k ) ( 3.7k ) ( 364k ) ( 393k ) ( 393k ) ( 105k ) ( 2.5k ) acc f1/acc f1/acc acc acc acc acc -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BERT_6-PKD ( Sun et al. , 2019 ) 92.0 85.0/79.9 70.7/88.9 81.5 81.0 89.0 65.5 ( M=6 ; d \u2019 =768 ; d \u2019 _i=3072 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Mcc refers to Matthews correlation and pear/spea refer to pearson/spearman . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - CoLA MNLI MNLI-mm MRPC QNLI QQP RTE SST-2 STS-B mcc acc acc f1/acc acc f1/acc acc acc pear/spea -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- DistilBERT [ 1 ] 42.5 81.6 81.1 88.3/82.4 85.5 87.7/90.6 60.0 92.7 84.5/85.0 ( M=6 ; d \u2019 =768 ; d \u2019 _i=3072 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"}], "0": {"review_id": "rJx0Q6EFPB-0", "review_text": "This paper proposes a new knowledge distillation method for BERT models. A number of modifications to the vanilla knowledge distillation method of Hinton et al (2015) are proposed. First, authors suggest adding L2 loss functions between alignment matrices, embedding layer values and prediction layer values. Second, authors propose run knowledge-distillation twice, once with the original pre-trained BERT model as teacher, and then again with task specific fine-tuned BERT as a new teacher. Third, authors emphasize the use of data augmentation for successful knowledge distillation. In Table 2, authors claim a significant lift across GLUE benchmarks with respect to other baseline methods with comparable model size. While the main contribution of this paper is the proposal of empirically useful techniques than theoretical development, the empirical results reported in this paper are somewhat puzzling. First of all, GLUE benchmark scores reported in Table 2 don't seem to be consistent with Table 1 of Sun et al (2019) for BERT-PKD ( https://arxiv.org/pdf/1908.09355.pdf ) or DistilBERT ( https://medium.com/huggingface/distilbert-8cf3380435b5 ). Indeed, BERT-PKD in Sun et al seems to significantly outperform TinyBERT on QNLI (89.0 vs 87.7) and RTE (65.5 vs 62.9), and the gap between BERT-PKD and TinyBERT on other tasks are much smaller if we take numbers reported in the original paper. In Table 6, ablation studies with different distillation objectives are reported. Quite surprisingly, without Transformer-layer distillation (No Trm) the performance drops quite significantly. This is unexpected, because baselines such as Sun et al and DistilBERT do not use the Transformer-layer distillation but much more competitive to full TinyBERT than TinyBERT without Transformer-layer distillation. Would there be a reason why TinyBERT is so critically dependent on Transformer-layer distillation? Similarly, the removal of data augmentation (Table 5, No DA) is so detrimental to the performance of the model that it makes me to suspect whether the most of gain is from successful data augmentation. Indeed, 'No DA' row of Table 5 is very close to the performance BERT-PKD in Table 4, although the number of layers is different (4 vs 6). In order for the research community to understand the contribution of proposed techniques more thoroughly, I suggest authors to conduct ablation studies with the simplest baseline. That is, rather than starting with the full TinyBERT model, start with a simple but competitive baseline like BERT-PKD, and only add one technique (DA, GD, Transformer-layer distillation) at a time so that readers shall understand what technique is the most important to be added to the baseline, and also whether some of the proposed techniques should always be used in combination. --- After Author Rebuttal: authors have addressed all of my concerns quite clearly. Additional experiments which targeted a specific design choice at a time made me much more convinced that the techniques proposed in this paper are useful not only for this particular context but also more broadly applicable.", "rating": "8: Accept", "reply_text": "Thank you for the helpful comments ! Q1 : GLUE scores reported in Table 2 don \u2019 t seem to be consistent . A1 : In Table2 , all the results of TinyBERT , BERT-PKD and DistilBERT are based on 4-layer architectures and evaluated on the TEST set of official GLUE benchmark . The results of Table 1 of Sun et al ( 2019 ) for BERT-PKD and the results of DistilBERT [ 1 ] are based on 6-layer architectures , and DistilBERT only reported their results on the DEV set of GLUE . As described in Appendix B ( \u201c Baseline setup \u201d ) , to ensure the correct implementations of BERT-PKD and DistilBERT , we firstly re-reproduced the reported results of baselines with 6-layer architecture , then we trained the baselines with 4-layer architecture by following the confirmed correct implementations , and evaluated them on the TEST set of official GLUE benchmarks . For a direct comparisons with BERT-PKD and DistilBERT , we here also present the results of 6-layer TinyBERT with the same architecture as the original BERT-PKD ( Sun et al. , 2019 ) and original DistilBERT [ 1 ] , and directly use the reported results of BERT-PKD and DistilBERT . As BERT-PKD and DistilBERT are evaluated on the test and dev set of GLUE , respectively . Thus , we present the results in the following two tables separately , and the results have been added to the Appendix E of our paper . Table : the comparisons between TinyBERT and BERT-PKD , and the results are evaluated on the test set of official GLUE tasks . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE ( 67k ) ( 3.7k ) ( 364k ) ( 393k ) ( 393k ) ( 105k ) ( 2.5k ) acc f1/acc f1/acc acc acc acc acc -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BERT_6-PKD ( Sun et al. , 2019 ) 92.0 85.0/79.9 70.7/88.9 81.5 81.0 89.0 65.5 ( M=6 ; d \u2019 =768 ; d \u2019 _i=3072 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Mcc refers to Matthews correlation and pear/spea refer to pearson/spearman . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - CoLA MNLI MNLI-mm MRPC QNLI QQP RTE SST-2 STS-B mcc acc acc f1/acc acc f1/acc acc acc pear/spea -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- DistilBERT [ 1 ] 42.5 81.6 81.1 88.3/82.4 85.5 87.7/90.6 60.0 92.7 84.5/85.0 ( M=6 ; d \u2019 =768 ; d \u2019 _i=3072 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"}, "1": {"review_id": "rJx0Q6EFPB-1", "review_text": "The authors propose TinyBERT, a smaller version of BERT that is trained with knowledge distillation. The authors evaluate on the GLUE benchmark. Overall, I find the direction of this work exciting and making these large models smaller for practical use is an important research area. The authors provide various ablation experiments that provide insight into their method. The main contribution is experiments comparing various existing distillation methods to different parts of the model (embeddings, layers, prediction layer), so is not particularly novel in contributing new techniques for distillation. That being said, there is importance in contributing these results as they are very useful for others working in the area and on making smaller models. But I would expect the authors to be much more detailed in their experimental description and make it clear in the paper that the comparative baselines are fair and well tuned. Comments: 1. Can the authors please add details for how the model has been trained, such as the datasets used, the number of update steps, the batch size, etc. as well as the finetuning parameters that were cross validated for GLUE? It is difficult to tell in the current setting if the models are comparable to the baselines. The current paper doesn't seem like it could be reproduced. It is particularly important to detail how the finetuning was done, as this is very important for the smaller datasets in GLUE. 2. Is the learning of the distilled model only done on the training dataset, or there is data augmentation beyond the training set? What is the effect without data augmentation? 3. Unfortunately, the performance drop on the GLUE benchmark as shown in Table 2 is fairly large. The authors compare to BERT Small and DistilBERT and I like the baselines, but the claim that the model achieves comparable performance to BERT Base is not true. 4. Was the BERT Small model tuned, or the same learning parameters from BERT Base were used? 5. Can the authors clarify the inference time of BERT Small? The speed improvement of TinyBERT should be the same as BERT Small based on parameter size. 6. The authors experiment with distilling the embedding layer to reduce the number of parameters, why not reduce the parameter size by reducing the vocabulary size? Existing approaches to BERT training use BPE with ~30k vocabulary size or RoBERTa with ~50k vocabulary size, but large gains could be applied here by reducing the size or using softmax reduction techniques that were popular on full vocabulary language modeling datasets like wikitext-103 or billion word. 7. Can the authors please clarify the construction of Table 2? Are those results on the test set (e.g. evaluated on the official GLUE benchmark), or on the dev set? Where are the DistilBERT numbers on the test set coming from, as it is not reported in their paper? ", "rating": "3: Weak Reject", "reply_text": "Thank you for the helpful comments ! Reproducibility * * * We will release the source code , all the models ( including the general TinyBERT variants and task-specific TinyBERT models for each task in GLUE and SQuAD , so other researchers can easily reproduce the results in the paper ) , and all the training details for reproducibility , as soon as possible . * * * Q1 : Details for how the model has been trained . A1 : We have presented all the training details of TinyBERT and baselines in the Appendix B ( TinyBERT setup and Baselines setup ) , which includes the datasets used , the number of update steps , the batch size as well as the settings for fine-tuning . Our TinyBERT and baselines use the same hyper-parameters and datasets at both the pre-training and fine-tuning stages . We here list the main setting details as follows and other details can be referred in Appendix B . Table : the hyper-parameters of DistilBERT , TinyBERT and BERT_small ; BERT_PKD does not include the pre-training stage , we use the BERT_base released by google as the teacher . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - At Pre-training Stage -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Dataset English Wikipedia ( 2,500 M words ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Training steps ~350k ( 3epoch ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Batch size 256 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Max_seq_length refers to the maximum sequence length . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Learning rate batch size Epoch Max_seq_length -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - MNLI ( 392k ) 3e-5 32 3 128 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- QQP ( 363k ) 3e-5 32 3 128 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- The effect of TinyBERT without DA ( data augmentation ) is presented in the Table 5 ( the \u201c No DA \u201d row ) and section 4.4 . Q3 : Claim that model achieves comparable performance to BERT_base . A3 : Thanks for the suggestion , we have changed the related claims . Q4 : Was the BERT_small model tuned , or the same learning paramters from BERT_Base were used ? A4 : As described in the subsection \u201c Baselines setup \u201d of Appendix B , BERT_small and BERT_Base use the same hyper-parameters for learning . Q5 : Can the authors clarify the inference time of BERT Small ? A5 : Yes , BERT_small and TinyBERT have the same architecture , thus they have the same inference time . We have added the inference time of BERT_small in Table 3 ."}, "2": {"review_id": "rJx0Q6EFPB-2", "review_text": "What is the task? Knowledge distillation of BERT What has been done before? Unlike prior works such as Distilled BiLSTMSOFT (Tang et al., 2019), BERT-PKD (Sun et al., 2019) and DistilBERT, this work i) Do knowledge distillation at pre training stage also in addition to fine tuning stage. ii) Student learns from all - embedding layers, attention matrices, hidden states, and final prediction layers. In BERT-PKD, student learns from the [CLS] hidden states of the teacher. What are the main contributions of the paper? Novel Transformer distillation method that is specially designed for knowledge distillation of the Transformer-based models. Novel two-stage learning framework which performs Transformer distillation at both the pre-training and task-specific learning stages Resulting TinyBERT being 7.5x smaller and 9.4x faster on inference and significantly outperforms other state-of-the-art baselines on BERT distillation. What are the key techniques used to tackle this task? Novel Transformer distillation method that is specially designed for knowledge distillation of the Transformer-based models. Novel two-stage learning framework which performs Transformer distillation at both the pre-training and task-specific learning stages What are the main results? Are they significant? Resulting TinyBERT being 7.5x smaller and 9.4x faster on inference and significantly outperforms other state-of-the-art baselines on BERT distillation with only \u223c28% parameters and \u223c31% inference time of them. Results show that three key procedures: TD (Task-specific Distillation), GD (General Distillation) and DA (Data Augmentation) are crucial for the proposed KD method. Proposed distillation objectives - Transformer-layer distillation (attention matrices and hidden states), embedding-layer distillation and prediction layer distillation are crucial for the proposed KD method. Weaknesses experimental results were not easily comparable to prior work so it is hard to say if claims are well-supported experimental results Questions Did authors try other values of lambda ", "rating": "6: Weak Accept", "reply_text": "Thank you for the helpful comments ! Q1 : Experimental results are not easily comparable to prior work . A1 : * * * Comparison results as shown in Table 2 , Table 3 and Table4 * * * The comparison results as shown in the Table 2 are all evaluated on the test set of the official GLUE tasks . As shown in the Table 3 , our TinyBERT , baselines BERT-PKD and DistilBERT , all have the same number of layers ( M=4 ) , and our TinyBERT has a relatively challenging setting with smaller hidden size ( d \u2019 =312 vs d \u2019 =768 ) and feedforward/filter size ( d \u2019 _i=1200 vs d \u2019 _i=3072 ) . If we increase the hidden size and feedforward/filter size of TinyBERT , it can obtain better performances , which is validated in our experiments in the Table 4 ( wider TinyBERT variants achieve better results ) . In the Table 4 , we also directly compared the performances of TinyBERT , BERT-PKD and DistilBERT with the same architecture settings ( M=6 ; d \u2019 =768 ; d \u2019 i=3072 ) , and TinyBERT has significantly better performances . * * * More complete comparisons with the same student architecture * * * For complete and direct comparisons with prior works , we here also present the results of TinyBERT ( M=6 ; d \u2019 =768 ; d \u2019 _i=3072 ) with the same architectures as used in the original BERT-PKD ( Sun et al. , 2019 ) and DistilBERT [ 1 ] papers . Since in the original papers , the BERT-PKD is evaluated on the TEST set , and the DistilBERT is evaluated on the DEV set . Thus , for a clear illustration , we present the results in the following two tables , separately , and the results have been added to the Appendix E of our paper . Table : the comparisons between TinyBERT and BERT-PKD , and the results are evaluated on the test set of official GLUE tasks . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE ( 67k ) ( 3.7k ) ( 364k ) ( 393k ) ( 393k ) ( 105k ) ( 2.5k ) acc f1/acc f1/acc acc acc acc acc -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BERT_6-PKD ( Sun et al. , 2019 ) 92.0 85.0/79.9 70.7/88.9 81.5 81.0 89.0 65.5 ( M=6 ; d \u2019 =768 ; d \u2019 _i=3072 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Mcc refers to Matthews correlation and pear/spea refer to pearson/spearman . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - CoLA MNLI MNLI-mm MRPC QNLI QQP RTE SST-2 STS-B mcc acc acc f1/acc acc f1/acc acc acc pear/spea -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- DistilBERT [ 1 ] 42.5 81.6 81.1 88.3/82.4 85.5 87.7/90.6 60.0 92.7 84.5/85.0 ( M=6 ; d \u2019 =768 ; d \u2019 _i=3072 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"}}