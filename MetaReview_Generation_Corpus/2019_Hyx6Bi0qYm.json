{"year": "2019", "forum": "Hyx6Bi0qYm", "title": "Adversarial Domain Adaptation for Stable Brain-Machine Interfaces", "decision": "Accept (Poster)", "meta_review": "BMIs need per-patient and per-session calibration, and this paper seeks to amend that.  Using VAEs and RNNs, it relates sEEG to sEMG, in principle a ten-year old approach, but do so using a novel adversarial approach that seems to work.\n\nThe reviewers agree the approach is nice, the statements in the paper are too strong, but publication is recommended.  Clinical evaluation is an important next step.", "reviews": [{"review_id": "Hyx6Bi0qYm-0", "review_text": "This contribution describes a novel approach for implanted brain-machine interface in order to address calibration problem and covariate shift. A latent representation is extracted from SEEG signals and is the input of a LTSM trained to predict muscle activity. To mitigate the variation of neural activities across days, the authors compare a CCA approach, a Kullback-Leibler divergence minimization and a novel adversarial approach called ADAN. The authors evaluate their approach on 16-days recording of neurons from the motor cortex of rhesus monkey, along with EMG recording of corresponding the arm and hand. The results show that the domain adaptation from the first recording is best handled with the proposed adversarial scheme. Compared to CCA-based and KL-based approaches, the ADAN scheme is able to significantly improve the EMG prediction, requiring a relatively small calibration dataset. The individual variability in day-to-day brain signal is difficult to harness and this work offers an interesting approach to address this problem. The contributions are well described, the limitation of CCA and KL are convincing and are supported by the experimental results. The important work on the figure help to provide a good understanding of the benefit of this approach. Some parts could be improved. The results of Fig. 2B to investigate the role of latent variables extracted from the trained autoencoder are not clear, the simultaneous training could be better explained. As the authors claimed that their method allows to make an unsupervised alignment neural recording, independently of the task, an experiment on another dataset could enforce this claim.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank the reviewer for a careful reading of our paper and the positive comments about our work . Q : \u201c Some parts could be improved . The results of Fig.2B to investigate the role of latent variables extracted from the trained autoencoder are not clear , the simultaneous training could be better explained . As the authors claimed that their method allows to make an unsupervised alignment neural recording , independently of the task , an experiment on another dataset could enforce this claim. \u201d A : In the revised version of our paper we have clarified the procedure for training the AE . It is based on a loss function that includes not only the unsupervised neural reconstruction loss but also a supervised regression loss that quantifies the quality of EMG prediction ( see Eq 1 ) . This combined training resulted in low-dimensional latent variables that were then used as inputs to a muscle predictor ; this predictor performed as well as a muscle predictor based directly on the high-dimensional neural activity . We do agree that additional experiments , both open loop ( offline ) and closed loop ( online ) , with additional animals , and involving additional tasks , are required to fully validate our results ; we are currently in the process of developing and running these experiments . A vetting of the computational ideas within the machine learning community is crucial before embarking into extremely time-consuming experiments ."}, {"review_id": "Hyx6Bi0qYm-1", "review_text": "Here the authors define a BMI that uses an autoencoder -> LSTM -> EMG. The authors then address the problem of data drift in BMI and describe a number of domain adaptation algorithms from simple (CCA to more complex ADAN) to help ameliorate it. There are a lot of extremely interesting ideas in this paper, but the paper is not particularly well written, and the overall effect to me was confusion. What problem is being solved here? Are we describing using latent variables (AE approach) for BMI? Are we discussing domain adaptation, i.e. handling the nonstationarity that so plagues BMI and array data? Clearly the issue of stability is being addressed but how? A number of different approaches are described from creating a pre-execution calibration routine whereby trials on the given day are used to calibrate to an already trained BMI (e.g. required for CCA) to putting data into an adversarial network trained on data from earlier days. Are we instead attempting to show that a single BMI can be used across multiple days? This paper is extremely interesting but suffers from lack of focus, rigor, and clarity. Focus : AE to RNN to EMG is that the idea to compare vs. Domain adaptation via CCA/KLDM/ADAM. Of course a paper can explore multiple ideas, but in this case the comparisons and controls for both are not adequate. Rigor: What are meaningful comparisons for all for the AE and DA portions? The AE part is strongly related to either to Kao 2017 or Pandarinath 2018 but nothing like that is compared. The domain adaptation part evokes data augmentation strategies of Sussillo 2016 but that is not compared. If I were reviewing this manuscript for a biological journal a rigorous standard would be online BMI results in two animals. Is there a reason why this isn\u2019t the standard for ICLR? Is the idea that non-biological journals / conferences are adequate to vet new ideas before really putting them to the test in a biological journal? The manuscript is concerned with the vexing problem of BMI stability of time, which seems to be a problem where online testing in two animals would be critical. (I appreciate this is a broader topic relevant to the BMI field beyond just this paper, but it would be helpful to get some thinking on this in the rebuttal). Clarity : This paper needs to be pretty seriously clarified. The mathematical notation is not adequate to the job, nor is the motivation for the varied methodology. I cannot tell if the subscript is for time or for day. Also, what is the difference between z_0 vs. Z_0? I do not know what exactly is going into the AE or the ADAN. The neural networks are not described to a point where one could reproduce this work. The notation for handling time is inadequate. E.g. despite repeated readings I cannot tell how time is handled in the auto-encoder, e.g. nxt is vectorized vs feeding n-sized vector one time step at a time? Questions What is the point of the latent representation in the AE if it is just fed to an LSTM? Is it to compare to not using it? Page 3, how precisely is time handled in the AE? If time is just vectorized, how can one get real-time readouts? In general there is not enough detail to understand what is implemented in the AE. If only one time slice is entered into AE, then it seems clear AE won\u2019t be very good because one desires latent representation of the dynamics, not single time slices. How big is the LSTM used to generate the EMG? It seems like a the most relevant baseline is to compare to the data perturbation strategies in Sussillo 2016. If you have an LSTM already up and running to predict EMG, this seems very doable. Page 4, \u201cWe then use an ADAN to align either the distribution of latent variables or the distributions of the residuals of the reconstructed neural data, the latter a proxy for the alignment of the neural latent variables.\u201d This sentence is not adequate to explain the concepts of the various distributions, the residuals of reconstructed neural data (where do the residuals come from?), and why is one a proxy for the other. Please expand this sentence into a few sentences, if necessary to define these concepts for the naive reader. Page 5, What parameters are minimized in equation (2)? Please expand the top sentence of page 5. Page 6, top - \u201cIn contrast, when the EMG predictor is trained simultaneously with the AE\u2026\u201d Do you mean there is again a loss function defined by both EMG prediction and AE and summed, and then backprop is used to train both in an end-to-end fashion? Please clarify. Page 8, How do the AE results and architecture fit into the EMG reconstruction \u201cBMI\u201d results? Is that all decoding results are first put through the AE -> LSTM -> EMG pipeline? I.e. your BMI is neural data -> AE -> LSTM -> EMG? If so, then how does the ADAN / CCA and KLDM fit in? You first run those three DA algorithms and then pipe it through the BMI? Page 8, How can you say that the BMI improvement of 6% is meaningful to the BMI user if you did not test the BMI online? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q : \u201c Page 5 , What parameters are minimized in equation ( 2 ) ? Please expand the top sentence of page 5. \u201d A : The KLD of Eq 2 is always positive , and reaches its minimum at zero when the mean and covariance matrix for day-k match those for day-0 . The KLDM method thus aligns the latent statistics of day-k to those of day-0 by implementing a transformation that equalizes the first and second moments of these complex PDFs . To minimize the KLD , we used a map from neural activity to latent activity implemented by a network with the same architecture as the encoder section of the BMI \u2019 s AE . This network was initialized with the weights obtained after training the BMI \u2019 s AE on the day-0 data . Training proceeded on inputs provided by day-k recordings of neural activity . The loss function on the latent variables was as shown in Eq 2 . We have added these clarifications to the revised version of our paper . Q : \u201c Page 6 , top - \u201c In contrast , when the EMG predictor is trained simultaneously with the AE\u2026 \u201d Do you mean there is again a loss function defined by both EMG prediction and AE and summed , and then backprop is used to train both in an end-to-end fashion ? Please clarify. \u201d A : When the EMG predictor is trained simultaneously with the AE , the AE is trained using the joint loss function of Eq 1 . The alternative , is to independently train the AE in a purely unsupervised manner , not including the second term in Eq 1 . We have clarified this point in the revised version of our paper . Q : \u201c Page 8 , How do the AE results and architecture fit into the EMG reconstruction \u201c BMI \u201d results ? Is that all decoding results are first put through the AE - > LSTM - > EMG pipeline ? I.e.your BMI is neural data - > AE - > LSTM - > EMG ? If so , then how does the ADAN / CCA and KLDM fit in ? You first run those three DA algorithms and then pipe it through the BMI ? \u201d A : The BMI consists of two computational modules : the neural AE and the EMG predictor . These were trained using only the data of day-0 and remained fixed afterward . Once the BMI is trained , the fixed encoder part of the AE maps neural activity into latent activity . Both CCA and KLDM were designed to match latent variables across days . Therefore , when using these methods , we first obtained the latent variables Zk of subsequent days using the encoder part of the fixed AE of the BMI , then applied CCA and KLDM to align these latent variables to those of day-0 , and finally used the fixed EMG predictor to predict EMGs from the aligned latent variables . In contrast , ADAN was designed to match high-dimensional neural recordings across days . Therefore , when using ADAN , first we aligned the neural recordings Xk of a subsequent day to those of a day-0 and then used the aligned vectors of neural activity as inputs to the fixed BMI . We have clarified this aspect of domain adaptation in the revised version of our paper . Q : \u201c Page 8 , How can you say that the BMI improvement of 6 % is meaningful to the BMI user if you did not test the BMI online ? \u201d A : We agree . We have removed this sentence from the revised version of our paper . We thank the reviewer again for the feedback and comments , which have improved the manuscript ."}, {"review_id": "Hyx6Bi0qYm-2", "review_text": "The paper considers invasive BMIs and studies various ways to avoid daily recalibration due to changes in the brain signals. While I like the paper and studied methods -- using adverserial domain adaptation is interesting to use in this context --, I think that the authors oversell a bit. The problem of nonstationarity rsp. stability is an old one in non-invasive BCIs (shenoy et al JNE 2006 was among the first) and a large number of prior methods have been defined to robustify feature spaces, to project to stable subspaces etc. Clearly no Gans at that time. The least the authors could do is to make reference to this literature, some methods may even apply also for the invasive data of the paper. While the authors did not clearly say that they present an offline analysis; one method, the GAN, gets 6% better results then the competitors. I am not sure whether this is practically relevant in an online setting. But this needs to be clearly discussed in the paper and put into perspective to avoid wrong impression. Only an online study would be convincing. Overall, I think the paper could be accepted, the experiments are nice, the data is interesting, if it is appropriately toned down (avoiding statements about having done something for the first time) and properly references to prior work are given. It is an interesting application domain. I additionally recommend releasing the data upon acceptance. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the feedback and comments . Q : \u201c The paper considers invasive BMIs and studies various ways to avoid daily recalibration due to changes in the brain signals . While I like the paper and studied methods -- using adversarial domain adaptation is interesting to use in this context -- , I think that the authors oversell a bit . The problem of nonstationarity rsp . stability is an old one in non-invasive BCIs ( shenoy et al JNE 2006 was among the first ) and a large number of prior methods have been defined to robustify feature spaces , to project to stable subspaces etc . Clearly no Gans at that time . The least the authors could do is to make reference to this literature , some methods may even apply also for the invasive data of the paper. \u201d A : We thank the reviewer for the positive comment about our work . We do not claim to be the first to address the issue of stability in the presence of non-stationary recorded signals . We have added references to Zhang & Chase 2013 , Nuyujukian et al 2014 , Dyer et al 2017 , and Downey et al 2018 to the papers we had already listed in the section on Related Work : Orsborn et al 2012 , Dangi et al 2013 , Bishop et al 2014 , Jarosiewicz et al 2015 , Susillo et al 2016 , Kao et al 2017 , and Pandarinath et al 2017 . We could not find the Shenoy et al JNE 2006 article mentioned by the reviewer . We would appreciate more details regarding this paper \u2013 does this refer to the Nature 2006 or the JNE 2007 paper from the Shenoy group ? In the revised version of our paper , we have expanded the description of the methods previously put forward by these authors . Our claim to novelty is in formulating the problem as one of domain adaptation for neural signals , a problem that can be addressed through the use of adversarial training . Q : \u201c While the authors did not clearly say that they present an offline analysis ; one method , the GAN , gets 6 % better results then the competitors . I am not sure whether this is practically relevant in an online setting . But this needs to be clearly discussed in the paper and put into perspective to avoid wrong impression . Only an online study would be convincing. \u201d A : The reviewer is correct in pointing out that our evaluation of BMI performance is offline , in an open-loop scenario and that we can not claim improved ease of use since the aligned BMI has not been tested in an online , closed-loop scenario . We have removed the statement to that effect in the revised version of the paper . However , the 6 % improvement over the competitors in open-loop was statistically significant . A further advantage of ADAN in comparison to CCA and KLDM is that it is an unsupervised method that involves no assumption on the statistics of the latent activity . The question of online vs offline comparison is an important one . Online BMI performance is not perfectly correlated with offline accuracy ; this is actually the reason that offline comparison is important in this case . In an online evaluation of BMI performance , the user \u2019 s ability to adapt at an unknown rate and to an unknown extent to an imperfect BMI obscures the performance improvements obtained with domain adaptation . Although experiments , both open and closed loop , with additional animals and involving additional tasks , are in process as required to validate our results , the open loop performance improvement demonstrated here is a more stringent metric than improvements achieved in a closed loop setting . Q : \u201c Overall , I think the paper could be accepted , the experiments are nice , the data is interesting , if it is appropriately toned down ( avoiding statements about having done something for the first time ) and properly references to prior work are given . It is an interesting application domain . I additionally recommend releasing the data upon acceptance. \u201d A : We once more thank the reviewer for the positive comments . We have followed the advice and are more careful and specific in claiming novelty in the revised version of the paper . We plan to make data and code available on GitHub upon acceptance ."}], "0": {"review_id": "Hyx6Bi0qYm-0", "review_text": "This contribution describes a novel approach for implanted brain-machine interface in order to address calibration problem and covariate shift. A latent representation is extracted from SEEG signals and is the input of a LTSM trained to predict muscle activity. To mitigate the variation of neural activities across days, the authors compare a CCA approach, a Kullback-Leibler divergence minimization and a novel adversarial approach called ADAN. The authors evaluate their approach on 16-days recording of neurons from the motor cortex of rhesus monkey, along with EMG recording of corresponding the arm and hand. The results show that the domain adaptation from the first recording is best handled with the proposed adversarial scheme. Compared to CCA-based and KL-based approaches, the ADAN scheme is able to significantly improve the EMG prediction, requiring a relatively small calibration dataset. The individual variability in day-to-day brain signal is difficult to harness and this work offers an interesting approach to address this problem. The contributions are well described, the limitation of CCA and KL are convincing and are supported by the experimental results. The important work on the figure help to provide a good understanding of the benefit of this approach. Some parts could be improved. The results of Fig. 2B to investigate the role of latent variables extracted from the trained autoencoder are not clear, the simultaneous training could be better explained. As the authors claimed that their method allows to make an unsupervised alignment neural recording, independently of the task, an experiment on another dataset could enforce this claim.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank the reviewer for a careful reading of our paper and the positive comments about our work . Q : \u201c Some parts could be improved . The results of Fig.2B to investigate the role of latent variables extracted from the trained autoencoder are not clear , the simultaneous training could be better explained . As the authors claimed that their method allows to make an unsupervised alignment neural recording , independently of the task , an experiment on another dataset could enforce this claim. \u201d A : In the revised version of our paper we have clarified the procedure for training the AE . It is based on a loss function that includes not only the unsupervised neural reconstruction loss but also a supervised regression loss that quantifies the quality of EMG prediction ( see Eq 1 ) . This combined training resulted in low-dimensional latent variables that were then used as inputs to a muscle predictor ; this predictor performed as well as a muscle predictor based directly on the high-dimensional neural activity . We do agree that additional experiments , both open loop ( offline ) and closed loop ( online ) , with additional animals , and involving additional tasks , are required to fully validate our results ; we are currently in the process of developing and running these experiments . A vetting of the computational ideas within the machine learning community is crucial before embarking into extremely time-consuming experiments ."}, "1": {"review_id": "Hyx6Bi0qYm-1", "review_text": "Here the authors define a BMI that uses an autoencoder -> LSTM -> EMG. The authors then address the problem of data drift in BMI and describe a number of domain adaptation algorithms from simple (CCA to more complex ADAN) to help ameliorate it. There are a lot of extremely interesting ideas in this paper, but the paper is not particularly well written, and the overall effect to me was confusion. What problem is being solved here? Are we describing using latent variables (AE approach) for BMI? Are we discussing domain adaptation, i.e. handling the nonstationarity that so plagues BMI and array data? Clearly the issue of stability is being addressed but how? A number of different approaches are described from creating a pre-execution calibration routine whereby trials on the given day are used to calibrate to an already trained BMI (e.g. required for CCA) to putting data into an adversarial network trained on data from earlier days. Are we instead attempting to show that a single BMI can be used across multiple days? This paper is extremely interesting but suffers from lack of focus, rigor, and clarity. Focus : AE to RNN to EMG is that the idea to compare vs. Domain adaptation via CCA/KLDM/ADAM. Of course a paper can explore multiple ideas, but in this case the comparisons and controls for both are not adequate. Rigor: What are meaningful comparisons for all for the AE and DA portions? The AE part is strongly related to either to Kao 2017 or Pandarinath 2018 but nothing like that is compared. The domain adaptation part evokes data augmentation strategies of Sussillo 2016 but that is not compared. If I were reviewing this manuscript for a biological journal a rigorous standard would be online BMI results in two animals. Is there a reason why this isn\u2019t the standard for ICLR? Is the idea that non-biological journals / conferences are adequate to vet new ideas before really putting them to the test in a biological journal? The manuscript is concerned with the vexing problem of BMI stability of time, which seems to be a problem where online testing in two animals would be critical. (I appreciate this is a broader topic relevant to the BMI field beyond just this paper, but it would be helpful to get some thinking on this in the rebuttal). Clarity : This paper needs to be pretty seriously clarified. The mathematical notation is not adequate to the job, nor is the motivation for the varied methodology. I cannot tell if the subscript is for time or for day. Also, what is the difference between z_0 vs. Z_0? I do not know what exactly is going into the AE or the ADAN. The neural networks are not described to a point where one could reproduce this work. The notation for handling time is inadequate. E.g. despite repeated readings I cannot tell how time is handled in the auto-encoder, e.g. nxt is vectorized vs feeding n-sized vector one time step at a time? Questions What is the point of the latent representation in the AE if it is just fed to an LSTM? Is it to compare to not using it? Page 3, how precisely is time handled in the AE? If time is just vectorized, how can one get real-time readouts? In general there is not enough detail to understand what is implemented in the AE. If only one time slice is entered into AE, then it seems clear AE won\u2019t be very good because one desires latent representation of the dynamics, not single time slices. How big is the LSTM used to generate the EMG? It seems like a the most relevant baseline is to compare to the data perturbation strategies in Sussillo 2016. If you have an LSTM already up and running to predict EMG, this seems very doable. Page 4, \u201cWe then use an ADAN to align either the distribution of latent variables or the distributions of the residuals of the reconstructed neural data, the latter a proxy for the alignment of the neural latent variables.\u201d This sentence is not adequate to explain the concepts of the various distributions, the residuals of reconstructed neural data (where do the residuals come from?), and why is one a proxy for the other. Please expand this sentence into a few sentences, if necessary to define these concepts for the naive reader. Page 5, What parameters are minimized in equation (2)? Please expand the top sentence of page 5. Page 6, top - \u201cIn contrast, when the EMG predictor is trained simultaneously with the AE\u2026\u201d Do you mean there is again a loss function defined by both EMG prediction and AE and summed, and then backprop is used to train both in an end-to-end fashion? Please clarify. Page 8, How do the AE results and architecture fit into the EMG reconstruction \u201cBMI\u201d results? Is that all decoding results are first put through the AE -> LSTM -> EMG pipeline? I.e. your BMI is neural data -> AE -> LSTM -> EMG? If so, then how does the ADAN / CCA and KLDM fit in? You first run those three DA algorithms and then pipe it through the BMI? Page 8, How can you say that the BMI improvement of 6% is meaningful to the BMI user if you did not test the BMI online? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q : \u201c Page 5 , What parameters are minimized in equation ( 2 ) ? Please expand the top sentence of page 5. \u201d A : The KLD of Eq 2 is always positive , and reaches its minimum at zero when the mean and covariance matrix for day-k match those for day-0 . The KLDM method thus aligns the latent statistics of day-k to those of day-0 by implementing a transformation that equalizes the first and second moments of these complex PDFs . To minimize the KLD , we used a map from neural activity to latent activity implemented by a network with the same architecture as the encoder section of the BMI \u2019 s AE . This network was initialized with the weights obtained after training the BMI \u2019 s AE on the day-0 data . Training proceeded on inputs provided by day-k recordings of neural activity . The loss function on the latent variables was as shown in Eq 2 . We have added these clarifications to the revised version of our paper . Q : \u201c Page 6 , top - \u201c In contrast , when the EMG predictor is trained simultaneously with the AE\u2026 \u201d Do you mean there is again a loss function defined by both EMG prediction and AE and summed , and then backprop is used to train both in an end-to-end fashion ? Please clarify. \u201d A : When the EMG predictor is trained simultaneously with the AE , the AE is trained using the joint loss function of Eq 1 . The alternative , is to independently train the AE in a purely unsupervised manner , not including the second term in Eq 1 . We have clarified this point in the revised version of our paper . Q : \u201c Page 8 , How do the AE results and architecture fit into the EMG reconstruction \u201c BMI \u201d results ? Is that all decoding results are first put through the AE - > LSTM - > EMG pipeline ? I.e.your BMI is neural data - > AE - > LSTM - > EMG ? If so , then how does the ADAN / CCA and KLDM fit in ? You first run those three DA algorithms and then pipe it through the BMI ? \u201d A : The BMI consists of two computational modules : the neural AE and the EMG predictor . These were trained using only the data of day-0 and remained fixed afterward . Once the BMI is trained , the fixed encoder part of the AE maps neural activity into latent activity . Both CCA and KLDM were designed to match latent variables across days . Therefore , when using these methods , we first obtained the latent variables Zk of subsequent days using the encoder part of the fixed AE of the BMI , then applied CCA and KLDM to align these latent variables to those of day-0 , and finally used the fixed EMG predictor to predict EMGs from the aligned latent variables . In contrast , ADAN was designed to match high-dimensional neural recordings across days . Therefore , when using ADAN , first we aligned the neural recordings Xk of a subsequent day to those of a day-0 and then used the aligned vectors of neural activity as inputs to the fixed BMI . We have clarified this aspect of domain adaptation in the revised version of our paper . Q : \u201c Page 8 , How can you say that the BMI improvement of 6 % is meaningful to the BMI user if you did not test the BMI online ? \u201d A : We agree . We have removed this sentence from the revised version of our paper . We thank the reviewer again for the feedback and comments , which have improved the manuscript ."}, "2": {"review_id": "Hyx6Bi0qYm-2", "review_text": "The paper considers invasive BMIs and studies various ways to avoid daily recalibration due to changes in the brain signals. While I like the paper and studied methods -- using adverserial domain adaptation is interesting to use in this context --, I think that the authors oversell a bit. The problem of nonstationarity rsp. stability is an old one in non-invasive BCIs (shenoy et al JNE 2006 was among the first) and a large number of prior methods have been defined to robustify feature spaces, to project to stable subspaces etc. Clearly no Gans at that time. The least the authors could do is to make reference to this literature, some methods may even apply also for the invasive data of the paper. While the authors did not clearly say that they present an offline analysis; one method, the GAN, gets 6% better results then the competitors. I am not sure whether this is practically relevant in an online setting. But this needs to be clearly discussed in the paper and put into perspective to avoid wrong impression. Only an online study would be convincing. Overall, I think the paper could be accepted, the experiments are nice, the data is interesting, if it is appropriately toned down (avoiding statements about having done something for the first time) and properly references to prior work are given. It is an interesting application domain. I additionally recommend releasing the data upon acceptance. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the feedback and comments . Q : \u201c The paper considers invasive BMIs and studies various ways to avoid daily recalibration due to changes in the brain signals . While I like the paper and studied methods -- using adversarial domain adaptation is interesting to use in this context -- , I think that the authors oversell a bit . The problem of nonstationarity rsp . stability is an old one in non-invasive BCIs ( shenoy et al JNE 2006 was among the first ) and a large number of prior methods have been defined to robustify feature spaces , to project to stable subspaces etc . Clearly no Gans at that time . The least the authors could do is to make reference to this literature , some methods may even apply also for the invasive data of the paper. \u201d A : We thank the reviewer for the positive comment about our work . We do not claim to be the first to address the issue of stability in the presence of non-stationary recorded signals . We have added references to Zhang & Chase 2013 , Nuyujukian et al 2014 , Dyer et al 2017 , and Downey et al 2018 to the papers we had already listed in the section on Related Work : Orsborn et al 2012 , Dangi et al 2013 , Bishop et al 2014 , Jarosiewicz et al 2015 , Susillo et al 2016 , Kao et al 2017 , and Pandarinath et al 2017 . We could not find the Shenoy et al JNE 2006 article mentioned by the reviewer . We would appreciate more details regarding this paper \u2013 does this refer to the Nature 2006 or the JNE 2007 paper from the Shenoy group ? In the revised version of our paper , we have expanded the description of the methods previously put forward by these authors . Our claim to novelty is in formulating the problem as one of domain adaptation for neural signals , a problem that can be addressed through the use of adversarial training . Q : \u201c While the authors did not clearly say that they present an offline analysis ; one method , the GAN , gets 6 % better results then the competitors . I am not sure whether this is practically relevant in an online setting . But this needs to be clearly discussed in the paper and put into perspective to avoid wrong impression . Only an online study would be convincing. \u201d A : The reviewer is correct in pointing out that our evaluation of BMI performance is offline , in an open-loop scenario and that we can not claim improved ease of use since the aligned BMI has not been tested in an online , closed-loop scenario . We have removed the statement to that effect in the revised version of the paper . However , the 6 % improvement over the competitors in open-loop was statistically significant . A further advantage of ADAN in comparison to CCA and KLDM is that it is an unsupervised method that involves no assumption on the statistics of the latent activity . The question of online vs offline comparison is an important one . Online BMI performance is not perfectly correlated with offline accuracy ; this is actually the reason that offline comparison is important in this case . In an online evaluation of BMI performance , the user \u2019 s ability to adapt at an unknown rate and to an unknown extent to an imperfect BMI obscures the performance improvements obtained with domain adaptation . Although experiments , both open and closed loop , with additional animals and involving additional tasks , are in process as required to validate our results , the open loop performance improvement demonstrated here is a more stringent metric than improvements achieved in a closed loop setting . Q : \u201c Overall , I think the paper could be accepted , the experiments are nice , the data is interesting , if it is appropriately toned down ( avoiding statements about having done something for the first time ) and properly references to prior work are given . It is an interesting application domain . I additionally recommend releasing the data upon acceptance. \u201d A : We once more thank the reviewer for the positive comments . We have followed the advice and are more careful and specific in claiming novelty in the revised version of the paper . We plan to make data and code available on GitHub upon acceptance ."}}