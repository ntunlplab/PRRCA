{"year": "2017", "forum": "rkE8pVcle", "title": "Learning through Dialogue Interactions by Asking Questions", "decision": "Accept (Poster)", "meta_review": " This paper is a clear accept. Reviewers were both positive and confident about their assessments. Paper introduces a simulator and synthetic question answering task where interactions with the teacher are used for learning. Reviewers felt paper was well written with clear descriptions of tasks, models and experiments. Reviewer did comment on limitations due to the simple factoid QA framework explored for which hand crafted rules seems sufficient to solve the problem.", "reviews": [{"review_id": "rkE8pVcle-0", "review_text": "The goal of this paper is to analyze the behaviour of dialogue agents when they must answer factoid questions, but must query an oracle for additional information. This can be interpreted as a form of interaction between the dialogue agent and a \u2018teacher\u2019. The problem under investigation is indeed very important. The authors create a synthetic environment in which to test their agent. The main strength of the paper is that the paper tests many different combinations of environments, where either some knowledge is missing (and the agent has to query for it), or there is some misspelling in the teacher\u2019s question, and different ways the agent can ask for extra information. I am a bit concerned that many of the tasks are too easy (e.g. the AQ question paraphrase), and I am also concerned that the environment presented is very limited, and quite far (in terms of richness of linguistic structure) from how real humans would interact with chatbots. I think the paper would be better positioned as testing the basic reasoning capabilities of agents/ their ability to do question answering, rather than dialogue. However, I think the \u2018ground-up\u2019 approach that starts with simple environments is indeed worthy of analysis, and this paper makes an interesting contribution in that direction. Of course, the paper would be much more convincing with human experiments. Additional notes: I think the simulation, in the synthetic environment, for the first mistake a learner can make during dialogue: \u201cthe learner has problems understanding the surface form of the text of the dialogue partner, e.g., the phrasing of a question\u201d, is particularly limited since only word misspellings are considered (and the models used don\u2019t work at the character level), which is of course only a tiny fraction of ways an agent can misunderstand the context. I would be particularly interested to see some discussion of how the authors plan to scale this up to more realistic settings. EDIT: I have updated my score to reflect the addition of the Mechanical Turk experiments", "rating": "7: Good paper, accept", "reply_text": "Yes , please also see our response to AnonReviewer1 concerning the nature of the dataset . The advantage of simulated datasets is that one can break down the analysis , e.g.we have identified types of questions that can be asked , and now we can test which models are capable of benefitting from them . We can also test a reinforcement learning setting easily without incurring impractical data collection costs during model development . Now that we have some ( limited ) success future work can try other models ( many of our results can still clearly be improved e.g.they are in the 10 - 20 % error region when they could be < 5 % at least ) and develop more challenging/real datasets . For the first set of tasks ( Question Clarification ) we only reported experiments in the \u201c typo \u201d domain for simplicity . As R1 comments , we already have lot of experiments , so we did n't add more . However , we actually did implement other 'misunderstanding the surface form ' variants but left them out for simplicity and space reasons . In particular , for each question type , we have a set number of ways of asking it ( different phrase templates ) and we tried leaving out 1 or 2 ( can be a hyperparameter ) such phrases that are only seen in test , and that the bot can ask a question about to receive a rephrasing that it does know about . This is a setting that also closely mimics a real world situation ."}, {"review_id": "rkE8pVcle-1", "review_text": "This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better. The motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications. However, the paper falls short on the execution. All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues. The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules. The framework also assumes that the user's feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further. Because of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback. To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary). This would provide much stronger evidence that a dialogue agent can learn from such feedback. Other comments: - The abstract uses the phrase \"interactive dialogue agents\". What is meant by \"interactive\" dialogue agents? All dialogue agents interact with the user, so isn't it redundant to call them interactive? - A major limitation of the experiments is that the questions the agent can ask are specified a priori. If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent. While in the RL setting, the paper states \"For each dialogue, the bot takes two sequential actions $(a_1 , a_2)$: to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)\". This means the agent learns *when* to ask questions but not *what* questions to ask. - Related to the previous comment, in the sub-section \"ONLINE REINFORCEMENT LEARNING (RL)\" the paper states \"We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask.\". Please clarify this by removing the part \"what to ask\". - The paper presents an overwhelming amount of results. I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results. For example, what was the reason for including the \"TrainAQ(+FP)\" and \"TrainMix\" training settings? How do these results help validate the original hypothesis? If they don't, they should be taken out or moved to the appendix. - Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix. --- UPDATE --- Following the discussion below and the additional experiments provided by the authors, I have increased my score to 8.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "As far as we know , there is very little in the literature ( see related work ) on what the reviewer agrees is an important direction : dialogue agents which learn directly from unstructured human feedback ( as opposed to reward signals alone ) . To start in a new direction like this you have to have to start with a first approach . One has to see in a relatively simple setup if it works at all , and understand where it does and does not work . Now that we have showed some modest success on the tasks we have developed we hope : ( i ) both we and other researchers can develop better techniques and test them in this setup ; and ( ii ) we can continue on to more difficult , real-world challenges . So far , we are happy to show the positive result that a bot asking questions can significantly improve results in the setup we tested on . Reply to other comments : - \u201c interactive dialogue agents \u201d meaning bots that can both answer questions and ask questions , where learning is exhibited from the latter . We have now reworded and clarify this . - R1 states \u201c A major limitation of the experiments is that the questions the agent can ask are specified a priori \u201d \u2014 however , that is not correct . In the TestModelAQ setting the model has to get the form of the question correct as well . E.g.in the Question Verification and Knowledge Verification tasks there are many possible ways of forming the question and some of them are correct \u2014 the model has to choose the right question to ask . E.g.it should ask \u201c Does it have something to do with the fact that Larry Crowne directed by Tom Hanks ? \u201d rather than \u201c Does it have something to do with the fact that Forrest Gump directed by Robert Zemeckis ? \u201d when the latter is irrelevant ( the candidate list of questions is generated from the known knowledge base entries with respect to that question ) . We have now clarified this further in the text . - However , in the RL experiment we did not use TestAQ so R1 is correct in that case . We have now removed the `` what to ask '' phrasing there . - FP ( forward prediction ) is very important to measure because our goal is to find a model that can learn during dialogue . This means asking questions , and learning from the user 's responses , which is what this method does ( the reward-based method does n't really use the user 's responses ) . The FP method outperforms the reward-based method on some of the tasks which is a very encouraging sign ."}, {"review_id": "rkE8pVcle-2", "review_text": " The paper introduces a simulator and a set of synthetic question answering tasks where interaction with the \"teacher\" via asking questions is desired. The motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users. The paper studies this problem in an offline supervised and an online reinforcement learning settings. The results show that the models improve by asking questions. -- The idea is novel, and is relatively unexplored in the research community. The paper serves as a good first step in that direction. -- The paper studies three different types of tasks where the agent can benefit from user feedback. -- The paper is well written and provides a clear and detailed description of the tasks, models and experimental settings. Other comments/questions: -- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N? Is using both resulting in any conclusions which are adding to the paper's contributions? -- In the Question Clarification setting, what is the distribution of misspelled words over question entity, answer entity, relation entity or none of these? If most of the misspelled words come from relation entities, it might be a much easier problem than it seems. -- The first point on Page 10 \"The performance of TestModelAQ is worse than TestAQ but better than TestQA.\" is not true for Task 2 from the numbers in Tables 2 and 4. -- What happens if the conversational history is smaller or none? -- Figure 5, Task 6, why does the accuracy for good student drop when it stops asking questions? It already knows the relevant facts, so asking questions is not providing any additional information to the good student. -- Figure 5, Task 2, the poor student is able to achieve almost 70% of the questions correct even without asking questions. I would expect this number to be quite low. Any explanation behind this? -- Figure 1, Task 2 AQ, last sentence should have a negative response \"(-)\" instead of positive as currently shown. Preliminary Evaluation: A good first step in the research direction of learning dialogue agents from unstructured user interaction. ", "rating": "7: Good paper, accept", "reply_text": "> -- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N ? Is using both resulting in any conclusions which are adding to the paper 's contributions ? vanilla-MemN2N is a standard baseline so we should compare to it . Cont-MemN2N is a simple improvement introduced in this paper which can make a big difference . > -- In the Question Clarification setting , what is the distribution of misspelled words over question entity , answer entity , relation entity or none of these ? If most of the misspelled words come from relation entities , it might be a much easier problem than it seems . Missing words in this task are always in the non-entity words . Missing entities are Tasks 5-9 . Still the unknown misspelled words can make the question ambiguous ( e.g.do you want to know the director or writer of a movie ? ) . > -- The first point on Page 10 `` The performance of TestModelAQ is worse than TestAQ but better than TestQA . '' is not true for Task 2 from the numbers in Tables 2 and 4 . Has been fixed and re-uploaded . Thanks. > -- What happens if the conversational history is smaller or none ? The results are likely somewhat better . The code is now released so many variations can be tried . > -- Figure 5 , Task 6 , why does the accuracy for good student drop when it stops asking questions ? It already knows the relevant facts , so asking questions is not providing any additional information to the good student . While it 's true the good student does have the necessary information already , when asking the question and getting the right answer the correct response is more immediately evident later , rather than trying to fish it out of the known facts . You can think of this as stronger features given to a classifier , if you like , which helps . Hence , the good student still benefits from question asking in these tasks . > -- Figure 5 , Task 2 , the poor student is able to achieve almost 70 % of the questions correct even without asking questions . I would expect this number to be quite low . Any explanation behind this ? The reason why the poor student can still do fairly well is that we introduce typos to some of the non-entity words in the questions but not all of them . The student can still to some extent understand ( or guess correctly ) the meaning of some of the questions . This is in line with Table 3 , we are using the Cont-MemN2N in the online experiments as well as it worked better in the offline experiments , and indeed works relatively well in this setting -- but still works better when asking questions . > -- Figure 1 , Task 2 AQ , last sentence should have a negative response `` ( - ) '' instead of positive as currently shown . Has been fixed and re-uploaded . Thanks ."}], "0": {"review_id": "rkE8pVcle-0", "review_text": "The goal of this paper is to analyze the behaviour of dialogue agents when they must answer factoid questions, but must query an oracle for additional information. This can be interpreted as a form of interaction between the dialogue agent and a \u2018teacher\u2019. The problem under investigation is indeed very important. The authors create a synthetic environment in which to test their agent. The main strength of the paper is that the paper tests many different combinations of environments, where either some knowledge is missing (and the agent has to query for it), or there is some misspelling in the teacher\u2019s question, and different ways the agent can ask for extra information. I am a bit concerned that many of the tasks are too easy (e.g. the AQ question paraphrase), and I am also concerned that the environment presented is very limited, and quite far (in terms of richness of linguistic structure) from how real humans would interact with chatbots. I think the paper would be better positioned as testing the basic reasoning capabilities of agents/ their ability to do question answering, rather than dialogue. However, I think the \u2018ground-up\u2019 approach that starts with simple environments is indeed worthy of analysis, and this paper makes an interesting contribution in that direction. Of course, the paper would be much more convincing with human experiments. Additional notes: I think the simulation, in the synthetic environment, for the first mistake a learner can make during dialogue: \u201cthe learner has problems understanding the surface form of the text of the dialogue partner, e.g., the phrasing of a question\u201d, is particularly limited since only word misspellings are considered (and the models used don\u2019t work at the character level), which is of course only a tiny fraction of ways an agent can misunderstand the context. I would be particularly interested to see some discussion of how the authors plan to scale this up to more realistic settings. EDIT: I have updated my score to reflect the addition of the Mechanical Turk experiments", "rating": "7: Good paper, accept", "reply_text": "Yes , please also see our response to AnonReviewer1 concerning the nature of the dataset . The advantage of simulated datasets is that one can break down the analysis , e.g.we have identified types of questions that can be asked , and now we can test which models are capable of benefitting from them . We can also test a reinforcement learning setting easily without incurring impractical data collection costs during model development . Now that we have some ( limited ) success future work can try other models ( many of our results can still clearly be improved e.g.they are in the 10 - 20 % error region when they could be < 5 % at least ) and develop more challenging/real datasets . For the first set of tasks ( Question Clarification ) we only reported experiments in the \u201c typo \u201d domain for simplicity . As R1 comments , we already have lot of experiments , so we did n't add more . However , we actually did implement other 'misunderstanding the surface form ' variants but left them out for simplicity and space reasons . In particular , for each question type , we have a set number of ways of asking it ( different phrase templates ) and we tried leaving out 1 or 2 ( can be a hyperparameter ) such phrases that are only seen in test , and that the bot can ask a question about to receive a rephrasing that it does know about . This is a setting that also closely mimics a real world situation ."}, "1": {"review_id": "rkE8pVcle-1", "review_text": "This paper introduces a simulator and a set of synthetic tasks for evaluating a dialogue agent's ability to learn from user feedback. For solving these tasks, the paper uses memory networks (Sukhbaatar et al., 2015) learned through previously proposed supervised learning and reinforcement learning methods. In this setup, it is demonstrated that the agent learning from feedback (e.g. through question asking or question clarification) performs better. The motivation for the paper is excellent; dialogue agents which learn directly from unstructured human feedback (as opposed to reward signals alone) could be very useful in real-world applications. However, the paper falls short on the execution. All the numerous experiments presented are based on the synthetic dialogue simulator, which is highly artificial and different from real-world dialogues. The simulator is based on a simple factoid question-answering framework, which normally is not considered dialogue and which appears to be solvable with a few hand-crafted rules. The framework also assumes that the user's feedback is always correct and is given in one of a handful of forms (e.g. paraphrase of original question without typos) and that the agent can learn from examples of another agent asking questions or making clarifications, which simplifies the task even further. Because of the artificial setting and limited scope of the experiments, it seems difficult to draw conclusions about how to learn from unstructured user feedback. To test the hypothesis that it is possible to learn from such user feedback, I would strongly recommend the authors to continue working on this project by carrying out experiments with real human users (even in the factoid question answering domain, if necessary). This would provide much stronger evidence that a dialogue agent can learn from such feedback. Other comments: - The abstract uses the phrase \"interactive dialogue agents\". What is meant by \"interactive\" dialogue agents? All dialogue agents interact with the user, so isn't it redundant to call them interactive? - A major limitation of the experiments is that the questions the agent can ask are specified a priori. If I understand correctly, in the supervised learning setting the agent is trained to imitate the questions of another rule-based agent. While in the RL setting, the paper states \"For each dialogue, the bot takes two sequential actions $(a_1 , a_2)$: to ask or not to ask a question (denoted as a_1 ); and guessing the final answer (denoted as a_2)\". This means the agent learns *when* to ask questions but not *what* questions to ask. - Related to the previous comment, in the sub-section \"ONLINE REINFORCEMENT LEARNING (RL)\" the paper states \"We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask.\". Please clarify this by removing the part \"what to ask\". - The paper presents an overwhelming amount of results. I understand the benefit of synthetic tasks is precisely the ability to measure many aspects of model performance, but in this case it confuses the reader to present so many results. For example, what was the reason for including the \"TrainAQ(+FP)\" and \"TrainMix\" training settings? How do these results help validate the original hypothesis? If they don't, they should be taken out or moved to the appendix. - Since the contribution of the paper lies in the tasks and evaluation, it might be better to move either the vanilla-MemN2N (Table 2) to the appendix or to move the Cont-MemN2N results (Table 3) to the appendix. --- UPDATE --- Following the discussion below and the additional experiments provided by the authors, I have increased my score to 8.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "As far as we know , there is very little in the literature ( see related work ) on what the reviewer agrees is an important direction : dialogue agents which learn directly from unstructured human feedback ( as opposed to reward signals alone ) . To start in a new direction like this you have to have to start with a first approach . One has to see in a relatively simple setup if it works at all , and understand where it does and does not work . Now that we have showed some modest success on the tasks we have developed we hope : ( i ) both we and other researchers can develop better techniques and test them in this setup ; and ( ii ) we can continue on to more difficult , real-world challenges . So far , we are happy to show the positive result that a bot asking questions can significantly improve results in the setup we tested on . Reply to other comments : - \u201c interactive dialogue agents \u201d meaning bots that can both answer questions and ask questions , where learning is exhibited from the latter . We have now reworded and clarify this . - R1 states \u201c A major limitation of the experiments is that the questions the agent can ask are specified a priori \u201d \u2014 however , that is not correct . In the TestModelAQ setting the model has to get the form of the question correct as well . E.g.in the Question Verification and Knowledge Verification tasks there are many possible ways of forming the question and some of them are correct \u2014 the model has to choose the right question to ask . E.g.it should ask \u201c Does it have something to do with the fact that Larry Crowne directed by Tom Hanks ? \u201d rather than \u201c Does it have something to do with the fact that Forrest Gump directed by Robert Zemeckis ? \u201d when the latter is irrelevant ( the candidate list of questions is generated from the known knowledge base entries with respect to that question ) . We have now clarified this further in the text . - However , in the RL experiment we did not use TestAQ so R1 is correct in that case . We have now removed the `` what to ask '' phrasing there . - FP ( forward prediction ) is very important to measure because our goal is to find a model that can learn during dialogue . This means asking questions , and learning from the user 's responses , which is what this method does ( the reward-based method does n't really use the user 's responses ) . The FP method outperforms the reward-based method on some of the tasks which is a very encouraging sign ."}, "2": {"review_id": "rkE8pVcle-2", "review_text": " The paper introduces a simulator and a set of synthetic question answering tasks where interaction with the \"teacher\" via asking questions is desired. The motivation is that an intelligent agent can improve its performance by asking questions and getting corresponding feedback from users. The paper studies this problem in an offline supervised and an online reinforcement learning settings. The results show that the models improve by asking questions. -- The idea is novel, and is relatively unexplored in the research community. The paper serves as a good first step in that direction. -- The paper studies three different types of tasks where the agent can benefit from user feedback. -- The paper is well written and provides a clear and detailed description of the tasks, models and experimental settings. Other comments/questions: -- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N? Is using both resulting in any conclusions which are adding to the paper's contributions? -- In the Question Clarification setting, what is the distribution of misspelled words over question entity, answer entity, relation entity or none of these? If most of the misspelled words come from relation entities, it might be a much easier problem than it seems. -- The first point on Page 10 \"The performance of TestModelAQ is worse than TestAQ but better than TestQA.\" is not true for Task 2 from the numbers in Tables 2 and 4. -- What happens if the conversational history is smaller or none? -- Figure 5, Task 6, why does the accuracy for good student drop when it stops asking questions? It already knows the relevant facts, so asking questions is not providing any additional information to the good student. -- Figure 5, Task 2, the poor student is able to achieve almost 70% of the questions correct even without asking questions. I would expect this number to be quite low. Any explanation behind this? -- Figure 1, Task 2 AQ, last sentence should have a negative response \"(-)\" instead of positive as currently shown. Preliminary Evaluation: A good first step in the research direction of learning dialogue agents from unstructured user interaction. ", "rating": "7: Good paper, accept", "reply_text": "> -- What is the motivation behind using both vanilla-MemN2N AND Cont-MemN2N ? Is using both resulting in any conclusions which are adding to the paper 's contributions ? vanilla-MemN2N is a standard baseline so we should compare to it . Cont-MemN2N is a simple improvement introduced in this paper which can make a big difference . > -- In the Question Clarification setting , what is the distribution of misspelled words over question entity , answer entity , relation entity or none of these ? If most of the misspelled words come from relation entities , it might be a much easier problem than it seems . Missing words in this task are always in the non-entity words . Missing entities are Tasks 5-9 . Still the unknown misspelled words can make the question ambiguous ( e.g.do you want to know the director or writer of a movie ? ) . > -- The first point on Page 10 `` The performance of TestModelAQ is worse than TestAQ but better than TestQA . '' is not true for Task 2 from the numbers in Tables 2 and 4 . Has been fixed and re-uploaded . Thanks. > -- What happens if the conversational history is smaller or none ? The results are likely somewhat better . The code is now released so many variations can be tried . > -- Figure 5 , Task 6 , why does the accuracy for good student drop when it stops asking questions ? It already knows the relevant facts , so asking questions is not providing any additional information to the good student . While it 's true the good student does have the necessary information already , when asking the question and getting the right answer the correct response is more immediately evident later , rather than trying to fish it out of the known facts . You can think of this as stronger features given to a classifier , if you like , which helps . Hence , the good student still benefits from question asking in these tasks . > -- Figure 5 , Task 2 , the poor student is able to achieve almost 70 % of the questions correct even without asking questions . I would expect this number to be quite low . Any explanation behind this ? The reason why the poor student can still do fairly well is that we introduce typos to some of the non-entity words in the questions but not all of them . The student can still to some extent understand ( or guess correctly ) the meaning of some of the questions . This is in line with Table 3 , we are using the Cont-MemN2N in the online experiments as well as it worked better in the offline experiments , and indeed works relatively well in this setting -- but still works better when asking questions . > -- Figure 1 , Task 2 AQ , last sentence should have a negative response `` ( - ) '' instead of positive as currently shown . Has been fixed and re-uploaded . Thanks ."}}