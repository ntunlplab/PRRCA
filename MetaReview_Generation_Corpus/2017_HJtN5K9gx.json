{"year": "2017", "forum": "HJtN5K9gx", "title": "Learning Disentangled Representations in Deep Generative Models", "decision": "Reject", "meta_review": "The paper is a clearly presented application of deep generative models in the semi-supervised setting. After reviewing the discussion and responses, the reviewers felt that the paper while interesting, is limited in scope, and unfortunately not yet ready for inclusion in this year's proceeding.", "reviews": [{"review_id": "HJtN5K9gx-0", "review_text": "This paper investigates deep generative models with multiple stochastic nodes and gives them meaning by semi-supervision. From a methodological point of view, there is nothing fundamentally novel (it is very similar to the semi-supervised work of Kingma et al; although this work has sometimes more than two latent nodes, it is not a complex extension). There is a fairly classical auxiliary variable trick used to make sure the inference network for y is trained over all data points (by supposing y is in fact is a latent variable with an observation \\tilde y; the observation is y if y is observed, or uninformative for unobserved y). Alternatively, one can separate the inference used to learn the generative model (which throws out inference over y if it is observed), from an inference used to 'exercise' the model (approximate the complex p(y|x) in the model by a simpler q(y|x) - effectively inferring the target p(y|x) for the data where only x is collected). Results are strong, although on simple datasets. Overall this is a well written, interesting paper, but lacking in terms of methodological advances. Minor: - I feel the title is a bit too general for the content of the paper. I personally don't agree with the strong contrast made between deep generative models and graphical models (deep generative models are graphical models, but they are more typically learned and un-interpretable than classical graphical models; and having multiple stochastic variables is not exclusive to graphical models, see DRAW, Deep Kalman Filter, Recurrent VAE, etc.). The word 'structure' is a bit problematic; here, the paper seems more concerned with disentangling and semanticizing the latent representation of a generative model by supervision. It is debatable whether the models themselves have structure.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments and suggestions . > [ ... ] lacking in terms of methodological advances As set out in our top-level response , and clarified in the updated manuscript , our formulation actually allows us to deal with more general models involving continuous random variables and automatically scales the classifier/regressor term within the objective -- neither of which featured in their work . > [ ... ] do n't agree with the strong contrast made between deep generative > models and graphical models We do not intend to particularly contrast graphical models and deep generative models ; indeed as the reviewer points out , the latter are a particular variant of the former . However , we do intend to contrast the kinds of models assumed for the recognition networks ( typically simple mean-field ) against more general , and complex , graphical models . Our particular interests herein are twofold : a . Incorporating a particular graphical model into the recognition network naturally lends interpretability to the latents in terms of the variables ( and their semantics ) of the graphical model . b.Adding domain knowledge in the form of the given graphical model , particularly in perceptual domains such as vision , helps better model the complex variation introduced by the implicit 'rendering ' process in the generative model . > [ ... ] having multiple stochastic variables is not exclusive to graphical > models Our claim in this work is not simply that we can employ multiple stochastic variables , but that we can encode arbitrary dependencies in the * recognition model * . This has the effect of disentangling the latent representation through the provided dependency structure and supervision . The Multi-MNIST experiment ( Section 4.3 ) employs elements of both DRAW and rVAE , with the recognition network structured as an RNN with disentangled state , and with the generative model attending to each digit sequentially , in the spirit of DRAW . This is indicated in the models specified in the Appendix . > It is debatable whether the models themselves have structure . As we note in our top-level response , we agree that the word \u201c structure \u201d can be interpreted in multiple ways . It is of course true that the prior on latent variables has no ( or minimal , in the case of multi-MNIST ) structure . It is however not clear whether such structure in the prior is necessary , since the neural network that approximates the generative model can capture correlations and dependencies between variables . Our results show that dependency structures in the recognition networks ( Figures 3 , 6 , and 8 ) help capture the complex dependencies which occur in the true model posterior . As noted above , we have edited the manuscript as well as changed the title to help clarify this perspective ."}, {"review_id": "HJtN5K9gx-1", "review_text": "This paper proposed a variant of the semi-supervised VAE model which leads to a unified objective for supervised and unsupervised VAE. This variant gives software implementation of these VAE models more flexibility in specifying which variables are supervised and which are not. This development introduces a few extra terms compared to the original semi-supervised VAE formulation proposed by Kingma et al., 2014. From the experiment results it seems that these terms do not do much as the new formulation and the performance difference between the proposed method and Kingma et al. 2014 are not very significant (Figure 5). Therefore the benefit of the new formulation is likely to be just software engineering flexibility and convenience. This flexibility and convenience is nice to have, but it is better to demonstrate a few situations where the proposed method can be applied while for other previous methods it is non-trivial to do. The paper's title and the way it is written make me expect a lot more than what is currently in the paper. I was expecting to see, for example, structured hidden variable model for the posterior (page 4, top), or really \"structured interpretation\" of the generative model (title), but I didn't see any of these. The main contribution of this paper (a variant of the semi-supervised VAE model) is quite far from these. Aside from these, the plug-in estimation for discrete variables only works when the function h(x,y) is a continuous function of y. If however, h(x, y) is not continuous in y, for example h takes one form when y=1 and another form when y=2, then the approach of using Expectation[y] to replace y will not work. Therefore the \"plug-in\" estimation has its limitations. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "> From the experiment results it seems that these terms do not do much [ ... ] > [ ... ] benefit of the new formulation is likely to be just software > engineering flexibility and convenience . As set out in our top-level response , and clarified in the updated manuscript , our formulation actually allows us to deal with more general models involving continuous random variables and automatically scales the classifier/regressor term within the objective -- neither of which were possible before . > performance difference [ ... ] not very significant ( Figure 5 ) . Our primary contribution in this work is not the development of better classifiers or regressors , but to extend the state-of-the-art in semi-supervised learning in deep generative models to incorporate a wider variety of models and settings , in a unified framework . The purpose of the experiments in Section 4.1 were to indicate that our general re-formulation of the problem matches the specific single-discrete-variable latent model used in Kingma et.al 2014 . It serves as a sanity check in the simpler case . > [ ... ] better to demonstrate a few situations where the proposed method can be > applied while for other previous methods it is non-trivial to do . The experiments in Sections 4.2 and 4.3 are intended to do precisely this . - Intrinsic Faces Here , we evaluate the ability to incorporate partial supervision on * continuous * random variables , where the lighting is taken to be sampled from a 3-dimensional Gaussian distribution . - Multi-MNIST Here we evaluate the ability to incorporate recurrent structures , allowing for * variable-dimension * latent spaces , where the number of digits that are represented in the latent space are varied between 1 and 3 . Neither of these experiments would be feasible under the standard formulation in Kingma et.al 2014 , especially with partial supervision . > [ ... ] plug-in estimation [ ... ] limitations . Yes indeed ! As we note in the manuscript , the plug-in estimator holds only in some particular cases . The assumption we make though is that h ( x , y ) is a function that parameterises the variational approximation q ( z|x , y ) . In the case of the VAE , this typically means that h ( x , y ) is a neural network and is thus continuous in y . We do not always have to rely on the plug-in estimator though ; as for the more general cases , we can use any of the REINFORCE method , marginalisation ( where appropriate ) , or even adopt the ( very ) recently developed Concrete/Gumbel-Softmax distribution instead ."}, {"review_id": "HJtN5K9gx-2", "review_text": "This paper introduces a variant of the semi-supervised variational auto-encoder (VAE) framework. The authors present a way of introducing structure (observed variables) inside the recognition network. I find that the presentation of the inference with auxiliary variables could be avoided, as it actually makes the presentation unnecessarily complicated. Specifically, the expressions with auxiliary variables are helpful for devising a unified implementation, but modeling-wise one can get the same model without these auxiliary variables and recover a minimal extension of VAE where part of the generating space is actually observed. The observed variables mean that the posterior needs to also condition on those, so as to incorporate the information they convey. The way this is done in this paper is actually not very different from Kingma et al. 2014, and I am surprised that the experiments show a large deviation in these two methods' results. Given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to Kingma et al. 2014. By the way, I was wondering if the experimental setup is the same as in Kingma et al. 2014 for the results of Fig. 5 (bottom) - the authors mention that they use CNNs for feature extraction but from the paper it's not clear if Kingma et al. do the same. On a related note, I was wondering the same for the comparison with Jampani et al. 2015. In particular, is that model also using the same rate of supervision for a fair comparison? The experiment in section 4.3 is interesting and demonstrates a useful property of the approach. The discussion of the supervision rate (and the pre-review answer) is helpful in giving some insight about what is a successful training protocol to use in semi-supervised learning. Overall, the paper is interesting but the title and introduction made me expect something more from it. From the title I expected a method for interpreting general deep generative models, instead the described approach was about a semi-supervised variant of VAE - naturally including labelled examples disentangles the latent space, but this is a general property of any semi-supervised probabilistic model and not unique to the approach described here. Moreover, from the intro I expected to see a more general approximation scheme for the variational posterior (similar to Ranganath et al. 2015 which trully allows very flexible distributions), however this is not the case here. Given the above, the contributions of this paper are in defining a slight variant of the semi-supervised VAE, and (perhaps more importantly) formulating it in a way that is amendable to easier automation in terms of software. But methodologically there is not much contribution to the current literature. The authors mention that they plan to extend the framework in the probabilistic programming setting. It seems indeed that this would be a very promising and useful extension. Minor note: three of Kingma's papers are all cited in the main text as Kingma et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "> [ ... ] auxiliary variables could be avoided [ ... ] modelling-wise one can get > the same model without these auxiliary variables and recover a minimal > extension of VAE where part of the generating space is actually observed While this is true in principle , in practise such an approach can often be much harder to handle . In particular , our formulation allows us to handle different parts of the model being observed for different data points . For example , in the experiment with intrinsic faces ( Section 4.2 ) , some labelled data points could each have only lighting as their observed variables , and some others only identity . Having to reformulate each such case to recover the minimal version is a tedious and potentially unnecessary process , when instead one can have this happen automatically . > [ ... ] experiments show a large deviation in these two methods ' results > [ ... ] give a possible explanation on the superiority of their method The MNIST results are marginally lower that the comparison largely due to our use of the plug-in estimator . The comparison marginalises out the discrete label variable when unsupervised . The SVHN results are better than the comparison due to our use of CNNs for the recognition ( convolutional ) and generative ( deconvolutional ) networks in contrast to using ( multi-stage M1+M2 ) MLPs . > if the experimental setup is the same [ ... ] for the results [ ... ] not clear > if Kingma et al.do the same The experiment setup is the same as the M2 model from Kingma et.al 2014 for the MNIST dataset . That is , we go directly from data to latents and back to data . For the SVHN dataset , we compare against the M1+M2 model ( since that is the only model reported for that dataset ) , but we do n't do the preprocessing steps of PCA whitening and learning the M1 model that they do with MLPs . Instead , we directly employ a CNN as a feature learner , and simultaneously learn a classifier with it . > [ ... ] comparison with Jampani et al.2015 [ ... ] is that model also using the > same rate of supervision for a fair comparison ? The comparison with Jampani et al.2015 is not equal as they employ * fully-supervised * learning . We in contrast , only use a fraction of available labels to perform semi-supervised learning . > Moreover , from the intro I expected to see a more general approximation > scheme for the variational posterior ( similar to Ranganath et al.2015 which > truly allows very flexible distributions ) , however this is not the case > here . Unless we misunderstand the reviewer \u2019 s point , our formulation is indeed general enough to represent a wide variety of models , including control-flow structures like if-then-else , folds , and loops . We can express the model indicated in Ranganath et al.2015 ( Figure 1 , right ) using the constructs in our domain specific language . Maybe the reviewer could clarify in what sense the model described by Ranganath et al.could be seen as more flexible ? The exposition in the manuscript talks about a particular simple factorisation of the approximation q ( z , y|x ) as q ( y|x ) and q ( z|y , x ) , in order to separate the observed and unobserved variables . However , nothing in the formulation requires the factorisation to be exactly this . In fact , z , y , and x can all be viewed as multivariate , with dependencies between the dimensions . Indeed the experiments incorporate much richer factorisations . > Minor note : three of Kingma 's papers are all cited in the main text as Kingma > et al.2014 , causing confusion . I suggest using Kingma et al.2014a etc . Thank you for catching this error ! We have fixed it in the revised manuscript . The three variants are now : - Kingma & Welling -- Auto-Encoding Variational Bayes - Kingma et.al . -- Semi-supervised learning with deep generative models - Kingma & Ba -- Adam : A Method for Stochastic Optimization"}], "0": {"review_id": "HJtN5K9gx-0", "review_text": "This paper investigates deep generative models with multiple stochastic nodes and gives them meaning by semi-supervision. From a methodological point of view, there is nothing fundamentally novel (it is very similar to the semi-supervised work of Kingma et al; although this work has sometimes more than two latent nodes, it is not a complex extension). There is a fairly classical auxiliary variable trick used to make sure the inference network for y is trained over all data points (by supposing y is in fact is a latent variable with an observation \\tilde y; the observation is y if y is observed, or uninformative for unobserved y). Alternatively, one can separate the inference used to learn the generative model (which throws out inference over y if it is observed), from an inference used to 'exercise' the model (approximate the complex p(y|x) in the model by a simpler q(y|x) - effectively inferring the target p(y|x) for the data where only x is collected). Results are strong, although on simple datasets. Overall this is a well written, interesting paper, but lacking in terms of methodological advances. Minor: - I feel the title is a bit too general for the content of the paper. I personally don't agree with the strong contrast made between deep generative models and graphical models (deep generative models are graphical models, but they are more typically learned and un-interpretable than classical graphical models; and having multiple stochastic variables is not exclusive to graphical models, see DRAW, Deep Kalman Filter, Recurrent VAE, etc.). The word 'structure' is a bit problematic; here, the paper seems more concerned with disentangling and semanticizing the latent representation of a generative model by supervision. It is debatable whether the models themselves have structure.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments and suggestions . > [ ... ] lacking in terms of methodological advances As set out in our top-level response , and clarified in the updated manuscript , our formulation actually allows us to deal with more general models involving continuous random variables and automatically scales the classifier/regressor term within the objective -- neither of which featured in their work . > [ ... ] do n't agree with the strong contrast made between deep generative > models and graphical models We do not intend to particularly contrast graphical models and deep generative models ; indeed as the reviewer points out , the latter are a particular variant of the former . However , we do intend to contrast the kinds of models assumed for the recognition networks ( typically simple mean-field ) against more general , and complex , graphical models . Our particular interests herein are twofold : a . Incorporating a particular graphical model into the recognition network naturally lends interpretability to the latents in terms of the variables ( and their semantics ) of the graphical model . b.Adding domain knowledge in the form of the given graphical model , particularly in perceptual domains such as vision , helps better model the complex variation introduced by the implicit 'rendering ' process in the generative model . > [ ... ] having multiple stochastic variables is not exclusive to graphical > models Our claim in this work is not simply that we can employ multiple stochastic variables , but that we can encode arbitrary dependencies in the * recognition model * . This has the effect of disentangling the latent representation through the provided dependency structure and supervision . The Multi-MNIST experiment ( Section 4.3 ) employs elements of both DRAW and rVAE , with the recognition network structured as an RNN with disentangled state , and with the generative model attending to each digit sequentially , in the spirit of DRAW . This is indicated in the models specified in the Appendix . > It is debatable whether the models themselves have structure . As we note in our top-level response , we agree that the word \u201c structure \u201d can be interpreted in multiple ways . It is of course true that the prior on latent variables has no ( or minimal , in the case of multi-MNIST ) structure . It is however not clear whether such structure in the prior is necessary , since the neural network that approximates the generative model can capture correlations and dependencies between variables . Our results show that dependency structures in the recognition networks ( Figures 3 , 6 , and 8 ) help capture the complex dependencies which occur in the true model posterior . As noted above , we have edited the manuscript as well as changed the title to help clarify this perspective ."}, "1": {"review_id": "HJtN5K9gx-1", "review_text": "This paper proposed a variant of the semi-supervised VAE model which leads to a unified objective for supervised and unsupervised VAE. This variant gives software implementation of these VAE models more flexibility in specifying which variables are supervised and which are not. This development introduces a few extra terms compared to the original semi-supervised VAE formulation proposed by Kingma et al., 2014. From the experiment results it seems that these terms do not do much as the new formulation and the performance difference between the proposed method and Kingma et al. 2014 are not very significant (Figure 5). Therefore the benefit of the new formulation is likely to be just software engineering flexibility and convenience. This flexibility and convenience is nice to have, but it is better to demonstrate a few situations where the proposed method can be applied while for other previous methods it is non-trivial to do. The paper's title and the way it is written make me expect a lot more than what is currently in the paper. I was expecting to see, for example, structured hidden variable model for the posterior (page 4, top), or really \"structured interpretation\" of the generative model (title), but I didn't see any of these. The main contribution of this paper (a variant of the semi-supervised VAE model) is quite far from these. Aside from these, the plug-in estimation for discrete variables only works when the function h(x,y) is a continuous function of y. If however, h(x, y) is not continuous in y, for example h takes one form when y=1 and another form when y=2, then the approach of using Expectation[y] to replace y will not work. Therefore the \"plug-in\" estimation has its limitations. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "> From the experiment results it seems that these terms do not do much [ ... ] > [ ... ] benefit of the new formulation is likely to be just software > engineering flexibility and convenience . As set out in our top-level response , and clarified in the updated manuscript , our formulation actually allows us to deal with more general models involving continuous random variables and automatically scales the classifier/regressor term within the objective -- neither of which were possible before . > performance difference [ ... ] not very significant ( Figure 5 ) . Our primary contribution in this work is not the development of better classifiers or regressors , but to extend the state-of-the-art in semi-supervised learning in deep generative models to incorporate a wider variety of models and settings , in a unified framework . The purpose of the experiments in Section 4.1 were to indicate that our general re-formulation of the problem matches the specific single-discrete-variable latent model used in Kingma et.al 2014 . It serves as a sanity check in the simpler case . > [ ... ] better to demonstrate a few situations where the proposed method can be > applied while for other previous methods it is non-trivial to do . The experiments in Sections 4.2 and 4.3 are intended to do precisely this . - Intrinsic Faces Here , we evaluate the ability to incorporate partial supervision on * continuous * random variables , where the lighting is taken to be sampled from a 3-dimensional Gaussian distribution . - Multi-MNIST Here we evaluate the ability to incorporate recurrent structures , allowing for * variable-dimension * latent spaces , where the number of digits that are represented in the latent space are varied between 1 and 3 . Neither of these experiments would be feasible under the standard formulation in Kingma et.al 2014 , especially with partial supervision . > [ ... ] plug-in estimation [ ... ] limitations . Yes indeed ! As we note in the manuscript , the plug-in estimator holds only in some particular cases . The assumption we make though is that h ( x , y ) is a function that parameterises the variational approximation q ( z|x , y ) . In the case of the VAE , this typically means that h ( x , y ) is a neural network and is thus continuous in y . We do not always have to rely on the plug-in estimator though ; as for the more general cases , we can use any of the REINFORCE method , marginalisation ( where appropriate ) , or even adopt the ( very ) recently developed Concrete/Gumbel-Softmax distribution instead ."}, "2": {"review_id": "HJtN5K9gx-2", "review_text": "This paper introduces a variant of the semi-supervised variational auto-encoder (VAE) framework. The authors present a way of introducing structure (observed variables) inside the recognition network. I find that the presentation of the inference with auxiliary variables could be avoided, as it actually makes the presentation unnecessarily complicated. Specifically, the expressions with auxiliary variables are helpful for devising a unified implementation, but modeling-wise one can get the same model without these auxiliary variables and recover a minimal extension of VAE where part of the generating space is actually observed. The observed variables mean that the posterior needs to also condition on those, so as to incorporate the information they convey. The way this is done in this paper is actually not very different from Kingma et al. 2014, and I am surprised that the experiments show a large deviation in these two methods' results. Given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to Kingma et al. 2014. By the way, I was wondering if the experimental setup is the same as in Kingma et al. 2014 for the results of Fig. 5 (bottom) - the authors mention that they use CNNs for feature extraction but from the paper it's not clear if Kingma et al. do the same. On a related note, I was wondering the same for the comparison with Jampani et al. 2015. In particular, is that model also using the same rate of supervision for a fair comparison? The experiment in section 4.3 is interesting and demonstrates a useful property of the approach. The discussion of the supervision rate (and the pre-review answer) is helpful in giving some insight about what is a successful training protocol to use in semi-supervised learning. Overall, the paper is interesting but the title and introduction made me expect something more from it. From the title I expected a method for interpreting general deep generative models, instead the described approach was about a semi-supervised variant of VAE - naturally including labelled examples disentangles the latent space, but this is a general property of any semi-supervised probabilistic model and not unique to the approach described here. Moreover, from the intro I expected to see a more general approximation scheme for the variational posterior (similar to Ranganath et al. 2015 which trully allows very flexible distributions), however this is not the case here. Given the above, the contributions of this paper are in defining a slight variant of the semi-supervised VAE, and (perhaps more importantly) formulating it in a way that is amendable to easier automation in terms of software. But methodologically there is not much contribution to the current literature. The authors mention that they plan to extend the framework in the probabilistic programming setting. It seems indeed that this would be a very promising and useful extension. Minor note: three of Kingma's papers are all cited in the main text as Kingma et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "> [ ... ] auxiliary variables could be avoided [ ... ] modelling-wise one can get > the same model without these auxiliary variables and recover a minimal > extension of VAE where part of the generating space is actually observed While this is true in principle , in practise such an approach can often be much harder to handle . In particular , our formulation allows us to handle different parts of the model being observed for different data points . For example , in the experiment with intrinsic faces ( Section 4.2 ) , some labelled data points could each have only lighting as their observed variables , and some others only identity . Having to reformulate each such case to recover the minimal version is a tedious and potentially unnecessary process , when instead one can have this happen automatically . > [ ... ] experiments show a large deviation in these two methods ' results > [ ... ] give a possible explanation on the superiority of their method The MNIST results are marginally lower that the comparison largely due to our use of the plug-in estimator . The comparison marginalises out the discrete label variable when unsupervised . The SVHN results are better than the comparison due to our use of CNNs for the recognition ( convolutional ) and generative ( deconvolutional ) networks in contrast to using ( multi-stage M1+M2 ) MLPs . > if the experimental setup is the same [ ... ] for the results [ ... ] not clear > if Kingma et al.do the same The experiment setup is the same as the M2 model from Kingma et.al 2014 for the MNIST dataset . That is , we go directly from data to latents and back to data . For the SVHN dataset , we compare against the M1+M2 model ( since that is the only model reported for that dataset ) , but we do n't do the preprocessing steps of PCA whitening and learning the M1 model that they do with MLPs . Instead , we directly employ a CNN as a feature learner , and simultaneously learn a classifier with it . > [ ... ] comparison with Jampani et al.2015 [ ... ] is that model also using the > same rate of supervision for a fair comparison ? The comparison with Jampani et al.2015 is not equal as they employ * fully-supervised * learning . We in contrast , only use a fraction of available labels to perform semi-supervised learning . > Moreover , from the intro I expected to see a more general approximation > scheme for the variational posterior ( similar to Ranganath et al.2015 which > truly allows very flexible distributions ) , however this is not the case > here . Unless we misunderstand the reviewer \u2019 s point , our formulation is indeed general enough to represent a wide variety of models , including control-flow structures like if-then-else , folds , and loops . We can express the model indicated in Ranganath et al.2015 ( Figure 1 , right ) using the constructs in our domain specific language . Maybe the reviewer could clarify in what sense the model described by Ranganath et al.could be seen as more flexible ? The exposition in the manuscript talks about a particular simple factorisation of the approximation q ( z , y|x ) as q ( y|x ) and q ( z|y , x ) , in order to separate the observed and unobserved variables . However , nothing in the formulation requires the factorisation to be exactly this . In fact , z , y , and x can all be viewed as multivariate , with dependencies between the dimensions . Indeed the experiments incorporate much richer factorisations . > Minor note : three of Kingma 's papers are all cited in the main text as Kingma > et al.2014 , causing confusion . I suggest using Kingma et al.2014a etc . Thank you for catching this error ! We have fixed it in the revised manuscript . The three variants are now : - Kingma & Welling -- Auto-Encoding Variational Bayes - Kingma et.al . -- Semi-supervised learning with deep generative models - Kingma & Ba -- Adam : A Method for Stochastic Optimization"}}