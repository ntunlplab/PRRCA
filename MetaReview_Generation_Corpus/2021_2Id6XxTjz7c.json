{"year": "2021", "forum": "2Id6XxTjz7c", "title": "Post-Training Weighted Quantization of Neural Networks for Language Models", "decision": "Reject", "meta_review": "The paper builds upon a recent paper BiQGEMM, providing a binary coding based post training quantization technique. The authors show how to combine magnitude-based importance metrics to these techniques and achieve superior performance. The use of importance metrics for quantization and pruning is not new, and magnitude-based metrics are among the more common metrics. With that in mind, the novelty of the paper is in the integration of importance metrics to the techniques of BiQGEMM. The provided methods lead to several hyper-parameters and the task of tuning these can be non-trivial and time consuming. Due to this the authors devote a detailed section showing how to properly tune these hyper-parameters. This is appreciated and indeed alleviates the problem coming with new hyper-parameters.\n\nThe paper received mixed opinions by the reviewers related to its overall novelty, but the resulting conclusion is that although the combination of binary coding based quantization with importance scores is not trivial, the challenges faced relate more to correct implementation as opposed to scientific novelty. Combined with other issues raised by the reviewers such as a need for further comparison with existing work, this lead me to recommend rejection for this paper.\n", "reviews": [{"review_id": "2Id6XxTjz7c-0", "review_text": "Based on a previous classic binary coding scheme , this paper proposed to introduce a modification $ m_i $ on the binarization scaling factor $ \\alpha $ , by considering the weight magnitude . It further use 3 hyperparamters to refine $ m_i $ by constraining its upper/lower bound and exponent . Besides , this work spent lengthy content to describe how to determine the hyperparamters . This paper contains the following drawbacks : 1 . The proposed post-training quantization method has no connection to language model . All we can say is that : Post-training weighted quantization of neural network * in * ( or applied in ) language models . As author also mentioned that this method also works well in image tasks . 2.There is a confusion on experimental setting : The quantization should be applied to the * fine-tuned * model . But it seems that author did n't pay attention to the disambiguation of pre-trained and fine-tuned , as I am quite lost in Sec.4.3 where author mention that `` perform post-training ... using * pre-trained * models of BERT-base ... on MNLI and MRPC . '' And did n't emphasize that the quantization is conducted on the fine-tuned models by MNLI/MRPC . 3.Author spent much effort on how to determine the hyperparameters , which is also one defect of the method : it requires exquisite tuning on hyperparameters , which are sensitive as shown in Table 2 . Question : 1 . How does the work deal with parameters in batch normalization layers ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "First of all , we appreciate your effort and time to review the manuscript . We address your concerns in detail below . R1 : The proposed post-training quantization method has no connection to language model . A1 : We acknowledge that the proposed post-training quantization can also be applied to image tasks ONLY IF practical implementation issues in inference system design are ignored . We selected a specific binary-coding-based quantization to be performed for language models because some practical libraries ( e.g. , BiQGEMM ( Jeon et al. , 2020 ) ) exist . Up to date , most ( if not all ) language models rely on LSTM layers or Transformer-like structures that involve matrix multiplications as the main computation engine . On the other hand , image tasks still depend on various types of convolution layers with a huge amount of features that may be even larger than the size of weights . Then , uniform quantization ( rather than non-uniform quantization that we consider in the manuscript ) associated with fixed-point representations would be useful for deploying image task models . We have not discussed different characteristics of numerous quantization methods a lot in our manuscript because such discussions have been done in other literature . We believe that Section 2 explains our motivation to consider binary-coding-based quantization and language models specifically for our experiments and detailed analysis . R2 : There is a confusion on experimental setting : The quantization should be applied to the fine-tuned model . A2 : We appreciate your careful review . We have conducted all quantization experiments using fine-tuned models only throughout the entire manuscript . In the original manuscript , we intended to explain that all procedures ( including fine-tuning and quantization ) start from a pre-trained model . To avoid any confusion , we revised section 4.3 to clearly indicate that quantization is performed on the fine-tuned models . R3 : It requires exquisite tuning on hyperparameters , which are sensitive as shown in Table 2 . A3 : Table 2 shows the motivation for exploring various hyper-parameter sets . In section 5 , we show that such an exploration is not challenging because 1 ) for a grid search , we utilize a pre-arranged fixed 16 combinations of hyper-parameters only , and then 2 ) BO performs hyper-parameter optimization process automatically . Table 3 confirms that various language models can be quantized by such a simple hyper-parameter search strategy . We revised section 5 to explain the hyper-parameter search process in more detail . In summary , our proposed hyper-parameter tuning can be performed in a structured and efficient manner . R4 : Parameters in batch normalization layers ? A4 : Our quantization scheme does not quantize activations since quantizing weights only can enhance performance significantly for language models ( as discussed in BiQGEMM ( Jeon et al. , 2020 ) ) . Thus , considerations on all kinds of normalization layers are basically optional in our scheme . Moreover , batch normalization is not utilized in the list of language models we have chosen . In Appendix , for image tasks , we did not perform quantization on batch normalization layers that have relatively a lot smaller number of parameters . Please let us know if you have any other questions or comments . Thanks a lot ."}, {"review_id": "2Id6XxTjz7c-1", "review_text": "The paper employs the binary-coding-based post-training quantization ( without retraining ) for language modeling . The key contribution is that weight importance is considered while determining binary code ( a , B ) . Two methods , Greedy and Alternating , are also modified to use the importance . The algorithm uses a novel normalized importance , which directly uses weight magnitude and some hyper-parameters . Because the performance is sensitive to hyperparameters , Bayesian optimization is used to find task- and model-specific settings . Adopting weight importance to previous algorithms is quite novel and makes sense . Binary-coding-based quantization is recently introduced but seems to be quite a promising direction , so this paper has some significance . The idea to replace pruned parameters with the smallest ones is also smart . The choice of E , C , P as controllable parameters seems reasonable . However , it looks like these parameters are sensitive and differs much task-by-task and model-by-model , that the robustness to the hyper-parameter is not ensured . Major questions : 1 ) I am not sure that in Section 4.1 , Equation ( 7 ) and ( 8 ) properly support the state \u201c weight magnitude can be a dominating factor for the loss function perturbation \u201d . \u2018 Delta-L \u2019 is a function of both \u2018 h \u2019 and \u2018 delta-w \u2019 , not \u2018 w \u2019 itself directly . ( I might be misunderstood\u2026 ) 2 ) Can you provide some explanation ( or guess ) ? ( a ) why sometimes greedy method is better than alter method ( b ) when does the pruning helps . Minor comments : 1 ) It would be great if a detailed scaling factor sharing scheme inside the parameter is given . For example , row-wise/column-wise/block tiling for a weight matrix ? 2 ) When the sample dataset ( all or some ) is not given ( that \u2019 s why post-training quantization is valuable ) , how can we select proper hyper-parameters ? BO maybe not applicable . 3 ) The term \u201c Original \u201d in tables may be misleading that the values are from the floating-point baseline . For example , for Table 1 , I recommend clarifying three cases : float baseline , greedy without importance ( previously \u201c Original \u201d ) , greedy with importance ( E=1.0 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate your positive feedback and careful review . R1 : I am not sure Eq . ( 7 ) and ( 8 ) properly support the state `` weight magnitude can be a dominating factor for the loss function perturbation '' . A1 : We admit that Section 4.1 was quite confusing . To clarify our major contributions and state our assumptions clearly , we revised Section 4.1 to show that a study magnitude-based importance estimation ( for post-training quantization ) is necessary to replace the Hessian that has been wide-utilized for various sensitivity studies ( because the Hessian would not be available for post-training quantization ) . We acknowledge that it is challenging to connect the magnitude of weights to the loss perturbation in theory . Hence , our contribution should be to 'empirically ' verify that magnitude can also be a useful importance estimation when partial derivatives are not available . Accordingly , we conducted various importance-aware quantization methods with numerous language models in order to support our assumptions . It would be an exciting research topic to investigate theories of how such a simple function using magnitude can provide importance metrics for post-training quantization . R2 : Can you provide some explanation ( a ) why sometimes greedy method is better than alter . method ( b ) when does the pruning helps . A2 ( a ) : Since quantization is basically a non-convex optimization process , depending on the loss landscape , the amount of quantization error may not lead to a minimized training accuracy drop . Hence , even though Greedy method shows a larger weight quantization error compared to Alternating method , training accuracy drop by Greedy method can be less than that of Alternating method . A2 ( b ) : The impact of pruning may not be significant if importance metrics ( determined by E ) for small weights are already small . We expect the impact of pruning would be noticeable when 1 ) importance metrics for small weights are relatively high by setting small E and 2 ) pruning rate is high such that perturbation of small weights does not affect training loss noticeably . R3 : A detailed scaling factor sharing scheme inside the parameters ? A3 : For all experiments in the manuscript , we selected row-wise quantization that BiQGEMM library ( Jeon et al. , 2020 ) also follows . Even though in Section 4.3 we described that \\alpha and \\mB are computed for each row , we added a sentence to clearly state that row-wise quantization is considered . R4 : When the sample dataset ( all or some ) is not given , how can we select proper hyper-parameters ? A4 : Data-free or data-agnostic post-training quantization is also being introduced as a popular post-training quantization method . Data-free quantization , however , does not consider input domains or particular distributions . Even though BO requires sample dataset , as Figure 1 shows , we expect that increasing scaling factors generally improves model accuracy regardless of input domains . It would be an interesting research topic to extend observations of Figure 1 to propose a magnitude-based data-agnostic post-training quantization . R5 : The term `` Original '' in tables may be misleading . A5 : We totally agree with you that `` Original '' can be misleading . Thank you for this constructive suggestion . Throughout the whole revised manuscript , `` Original '' is replaced with `` No Importance '' . We would be happy to answer if you have any other questions or comments . Thank you so much for reading our response ."}, {"review_id": "2Id6XxTjz7c-2", "review_text": "Summary : - This paper proposes to consider the importance of each parameter in the post-training quantization of weights . The authors propose to use weight magnitude as the importance indicator and to minimize the weighted distance between the full-precision weights and quantized weights . Experiments are performed on various NLP models and tasks . Strengths : - The paper is structured clearly and the proposed method is simple . Weaknesses : 1 . It is not clear to me why the change in the loss in ( 7 ) and ( 8 ) is necessarily related to the magnitude of the weights . From ( 8 ) , the loss change \\delta L is only related to the hessian , and each weight 's perturbation , but not the weight 's magnitude . 2.The second concern comes from the novelty of the proposed method . Indeed , loss-aware ( or importance-aware as in this paper ) quantization which considers the quantization effect of each parameter to the loss has already been proposed several years ago in [ 1,2 ] . In [ 1,2 ] , they also approximate the loss using the second-order Taylor expansion like in equation ( 7 ) in this submission . Moreover , the proposed importance-aware quantization solution in equation ( 5 ) is exactly the same as equation ( 8 ) in [ 1 ] , except that their importance is derived from the diagonal hessian instead of weight magnitude ( which is not quite reasonable , refer to the first point ) . 3.One other post-training quantization method GOBO in [ 3 ] , which also uses codes and codebooks to quantize language models , is also not compared . From their reported results , under the same number of bits , GOBO has higher accuracy than the proposed method . E.g. , GOBO has 83.76 % accuracy for the 3-bit quantization on MNLI while the proposed method only has 82.9 % . 4.From Table 2 , there is no winning configuration of ( E , C , P ) that works well on all studied models and tasks . How to determine these hyperparameters for new tasks empirically ? Tuning these parameters separately for each task can be inefficient . Reference : [ 1 ] Hou et al . `` Loss-aware binarization of deep networks . '' International Conference on Learning Representations . 2017 . [ 2 ] Hou et al . `` Loss-aware weight quantization of deep networks . '' International Conference on Learning Representations . 2017 . [ 3 ] Zadeh , Ali Hadi , and Andreas Moshovos . `` GOBO : Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference . '' arXiv:2005.03842 , 2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We highly appreciate your time and efforts to review our manuscript . R1 : It is not clear why the change in the loss in ( 7 ) and ( 8 ) is necessarily related to the magnitude of the weights . A1 : We admit that Section 4.1 was quite confusing . To clarify our major contributions and state our assumptions clearly , we revised Section 4.1 to show that a study magnitude-based importance estimation ( for post-training quantization ) is necessary to replace the Hessian that has been wide-utilized for various sensitivity studies ( because the Hessian would not be available for post-training quantization ) . We acknowledge that it is challenging to connect the magnitude of weights to the loss perturbation in theory . Hence , our contribution should be to 'empirically ' verify that magnitude can also be a useful importance estimation when partial derivatives are not available . Accordingly , we conducted various importance-aware quantization methods with numerous language models in order to support our assumptions . It would be an exciting research topic to investigate theories of how such a simple function using magnitude can provide importance metrics for post-training quantization . R2 : Loss-aware ( or importance-aware as in this paper ) quantization which considers the quantization effect of each parameter to the loss has already been proposed several years ago in [ 1,2 ] . A2 : We acknowledge that loss-aware quantization has been widely utilized for the compression-aware training method . On the other hand , post-training quantization has serious challenges to produce sensitivity metrics because partial derivatives would not be available once training is completed ( inference codes do not include backward propagation descriptions or optimization steps using gradient descent ) . We believe that is why there are few studies to find importance metrics for post-training quantization in the past because most importance or sensitivity estimations have been deeply related to the 1st or 2nd partial derivatives ( e.g , the gradients or the Hessian ) . Thus , in the manuscript , we put a bold assumption that a function using magnitude only can be also used as an importance estimation in the case of post-training quantization . Of course , such an assumption needs to be empirically supported as we demonstrated throughout various experiments in the manuscript . We revised Section 4.1 accordingly . Eq . ( 5 ) and ( 6 ) in the manuscript are fundamentally different from the solutions introduced in [ 1 ] and [ 2 ] . For example , reference [ 1 ] considers binary neural networks where an analytical solution for quantization exists while we propose various importance-aware approximation methods in Eq . ( 5 ) and ( 6 ) for multi-bit quantization . Reference [ 2 ] assumes ternary or symmetric uniform quantization where the number of scaling factors would be the same as that of binary neural networks . Overall , reference [ 1 ] and [ 2 ] are vastly different from ours because 1 ) [ 1 ] and [ 2 ] are not post-training methods , 2 ) [ 1 ] and [ 2 ] study compression-aware training methods where partial derivatives ( e.g. , the Hessian ) can be derived , 3 ) quantization formats are different from ours such that implementation principles are also different from ours ."}, {"review_id": "2Id6XxTjz7c-3", "review_text": "This paper proposes a weighted quantization framework that could be applied to general neural networks for language models . One core idea is that different parameters might show different sensitivity towards a loss function change . Thus it would make sense to take use of importance metrics to do weighted quantization across parameters . Clarity : The paper is clearly written . With a good introduction of related work and a pretty self-inclusive references to experiment setup . I also like how the paper is organized to motivate the inclusion of weight importance into quantization . Originality : Originality in this paper is mostly from introducing the concept of parameter ( weight ) importance and how it is defined and applied to the quantization problem . This paper is based on an earlier paper ( also new ) , e.g. , BiQGEMM ( Jeon et al. , 2020 ) , which paved the foundation to support binary-coding-based quantization techniques to accelerate quantized neural networks . A magnitude-based importance metric is proposed and approved effective in the form of the binary codes . To fine-tune model accuracy , three hyper-parameters are explored and empirically investigated to discover best performance ( regarding model quality ) . Significance : Based on the experiment result , I agree that this paper 's contribution is significant . If we combine the effort of quantization with distillation , as shown with the metrics in DistillBert , we could achieve great ability of inference speed , a very smaller model size but still maintain reasonable model quality .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate your positive feedback . We are happy to see that our contributions are well recognized in your review . We totally agree that combining distillation and quantization ( and potentially also structured/unstructured pruning and/or low-rank approximation ) would be an important and exciting research direction since the size of language models is substantially increasing ."}], "0": {"review_id": "2Id6XxTjz7c-0", "review_text": "Based on a previous classic binary coding scheme , this paper proposed to introduce a modification $ m_i $ on the binarization scaling factor $ \\alpha $ , by considering the weight magnitude . It further use 3 hyperparamters to refine $ m_i $ by constraining its upper/lower bound and exponent . Besides , this work spent lengthy content to describe how to determine the hyperparamters . This paper contains the following drawbacks : 1 . The proposed post-training quantization method has no connection to language model . All we can say is that : Post-training weighted quantization of neural network * in * ( or applied in ) language models . As author also mentioned that this method also works well in image tasks . 2.There is a confusion on experimental setting : The quantization should be applied to the * fine-tuned * model . But it seems that author did n't pay attention to the disambiguation of pre-trained and fine-tuned , as I am quite lost in Sec.4.3 where author mention that `` perform post-training ... using * pre-trained * models of BERT-base ... on MNLI and MRPC . '' And did n't emphasize that the quantization is conducted on the fine-tuned models by MNLI/MRPC . 3.Author spent much effort on how to determine the hyperparameters , which is also one defect of the method : it requires exquisite tuning on hyperparameters , which are sensitive as shown in Table 2 . Question : 1 . How does the work deal with parameters in batch normalization layers ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "First of all , we appreciate your effort and time to review the manuscript . We address your concerns in detail below . R1 : The proposed post-training quantization method has no connection to language model . A1 : We acknowledge that the proposed post-training quantization can also be applied to image tasks ONLY IF practical implementation issues in inference system design are ignored . We selected a specific binary-coding-based quantization to be performed for language models because some practical libraries ( e.g. , BiQGEMM ( Jeon et al. , 2020 ) ) exist . Up to date , most ( if not all ) language models rely on LSTM layers or Transformer-like structures that involve matrix multiplications as the main computation engine . On the other hand , image tasks still depend on various types of convolution layers with a huge amount of features that may be even larger than the size of weights . Then , uniform quantization ( rather than non-uniform quantization that we consider in the manuscript ) associated with fixed-point representations would be useful for deploying image task models . We have not discussed different characteristics of numerous quantization methods a lot in our manuscript because such discussions have been done in other literature . We believe that Section 2 explains our motivation to consider binary-coding-based quantization and language models specifically for our experiments and detailed analysis . R2 : There is a confusion on experimental setting : The quantization should be applied to the fine-tuned model . A2 : We appreciate your careful review . We have conducted all quantization experiments using fine-tuned models only throughout the entire manuscript . In the original manuscript , we intended to explain that all procedures ( including fine-tuning and quantization ) start from a pre-trained model . To avoid any confusion , we revised section 4.3 to clearly indicate that quantization is performed on the fine-tuned models . R3 : It requires exquisite tuning on hyperparameters , which are sensitive as shown in Table 2 . A3 : Table 2 shows the motivation for exploring various hyper-parameter sets . In section 5 , we show that such an exploration is not challenging because 1 ) for a grid search , we utilize a pre-arranged fixed 16 combinations of hyper-parameters only , and then 2 ) BO performs hyper-parameter optimization process automatically . Table 3 confirms that various language models can be quantized by such a simple hyper-parameter search strategy . We revised section 5 to explain the hyper-parameter search process in more detail . In summary , our proposed hyper-parameter tuning can be performed in a structured and efficient manner . R4 : Parameters in batch normalization layers ? A4 : Our quantization scheme does not quantize activations since quantizing weights only can enhance performance significantly for language models ( as discussed in BiQGEMM ( Jeon et al. , 2020 ) ) . Thus , considerations on all kinds of normalization layers are basically optional in our scheme . Moreover , batch normalization is not utilized in the list of language models we have chosen . In Appendix , for image tasks , we did not perform quantization on batch normalization layers that have relatively a lot smaller number of parameters . Please let us know if you have any other questions or comments . Thanks a lot ."}, "1": {"review_id": "2Id6XxTjz7c-1", "review_text": "The paper employs the binary-coding-based post-training quantization ( without retraining ) for language modeling . The key contribution is that weight importance is considered while determining binary code ( a , B ) . Two methods , Greedy and Alternating , are also modified to use the importance . The algorithm uses a novel normalized importance , which directly uses weight magnitude and some hyper-parameters . Because the performance is sensitive to hyperparameters , Bayesian optimization is used to find task- and model-specific settings . Adopting weight importance to previous algorithms is quite novel and makes sense . Binary-coding-based quantization is recently introduced but seems to be quite a promising direction , so this paper has some significance . The idea to replace pruned parameters with the smallest ones is also smart . The choice of E , C , P as controllable parameters seems reasonable . However , it looks like these parameters are sensitive and differs much task-by-task and model-by-model , that the robustness to the hyper-parameter is not ensured . Major questions : 1 ) I am not sure that in Section 4.1 , Equation ( 7 ) and ( 8 ) properly support the state \u201c weight magnitude can be a dominating factor for the loss function perturbation \u201d . \u2018 Delta-L \u2019 is a function of both \u2018 h \u2019 and \u2018 delta-w \u2019 , not \u2018 w \u2019 itself directly . ( I might be misunderstood\u2026 ) 2 ) Can you provide some explanation ( or guess ) ? ( a ) why sometimes greedy method is better than alter method ( b ) when does the pruning helps . Minor comments : 1 ) It would be great if a detailed scaling factor sharing scheme inside the parameter is given . For example , row-wise/column-wise/block tiling for a weight matrix ? 2 ) When the sample dataset ( all or some ) is not given ( that \u2019 s why post-training quantization is valuable ) , how can we select proper hyper-parameters ? BO maybe not applicable . 3 ) The term \u201c Original \u201d in tables may be misleading that the values are from the floating-point baseline . For example , for Table 1 , I recommend clarifying three cases : float baseline , greedy without importance ( previously \u201c Original \u201d ) , greedy with importance ( E=1.0 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate your positive feedback and careful review . R1 : I am not sure Eq . ( 7 ) and ( 8 ) properly support the state `` weight magnitude can be a dominating factor for the loss function perturbation '' . A1 : We admit that Section 4.1 was quite confusing . To clarify our major contributions and state our assumptions clearly , we revised Section 4.1 to show that a study magnitude-based importance estimation ( for post-training quantization ) is necessary to replace the Hessian that has been wide-utilized for various sensitivity studies ( because the Hessian would not be available for post-training quantization ) . We acknowledge that it is challenging to connect the magnitude of weights to the loss perturbation in theory . Hence , our contribution should be to 'empirically ' verify that magnitude can also be a useful importance estimation when partial derivatives are not available . Accordingly , we conducted various importance-aware quantization methods with numerous language models in order to support our assumptions . It would be an exciting research topic to investigate theories of how such a simple function using magnitude can provide importance metrics for post-training quantization . R2 : Can you provide some explanation ( a ) why sometimes greedy method is better than alter . method ( b ) when does the pruning helps . A2 ( a ) : Since quantization is basically a non-convex optimization process , depending on the loss landscape , the amount of quantization error may not lead to a minimized training accuracy drop . Hence , even though Greedy method shows a larger weight quantization error compared to Alternating method , training accuracy drop by Greedy method can be less than that of Alternating method . A2 ( b ) : The impact of pruning may not be significant if importance metrics ( determined by E ) for small weights are already small . We expect the impact of pruning would be noticeable when 1 ) importance metrics for small weights are relatively high by setting small E and 2 ) pruning rate is high such that perturbation of small weights does not affect training loss noticeably . R3 : A detailed scaling factor sharing scheme inside the parameters ? A3 : For all experiments in the manuscript , we selected row-wise quantization that BiQGEMM library ( Jeon et al. , 2020 ) also follows . Even though in Section 4.3 we described that \\alpha and \\mB are computed for each row , we added a sentence to clearly state that row-wise quantization is considered . R4 : When the sample dataset ( all or some ) is not given , how can we select proper hyper-parameters ? A4 : Data-free or data-agnostic post-training quantization is also being introduced as a popular post-training quantization method . Data-free quantization , however , does not consider input domains or particular distributions . Even though BO requires sample dataset , as Figure 1 shows , we expect that increasing scaling factors generally improves model accuracy regardless of input domains . It would be an interesting research topic to extend observations of Figure 1 to propose a magnitude-based data-agnostic post-training quantization . R5 : The term `` Original '' in tables may be misleading . A5 : We totally agree with you that `` Original '' can be misleading . Thank you for this constructive suggestion . Throughout the whole revised manuscript , `` Original '' is replaced with `` No Importance '' . We would be happy to answer if you have any other questions or comments . Thank you so much for reading our response ."}, "2": {"review_id": "2Id6XxTjz7c-2", "review_text": "Summary : - This paper proposes to consider the importance of each parameter in the post-training quantization of weights . The authors propose to use weight magnitude as the importance indicator and to minimize the weighted distance between the full-precision weights and quantized weights . Experiments are performed on various NLP models and tasks . Strengths : - The paper is structured clearly and the proposed method is simple . Weaknesses : 1 . It is not clear to me why the change in the loss in ( 7 ) and ( 8 ) is necessarily related to the magnitude of the weights . From ( 8 ) , the loss change \\delta L is only related to the hessian , and each weight 's perturbation , but not the weight 's magnitude . 2.The second concern comes from the novelty of the proposed method . Indeed , loss-aware ( or importance-aware as in this paper ) quantization which considers the quantization effect of each parameter to the loss has already been proposed several years ago in [ 1,2 ] . In [ 1,2 ] , they also approximate the loss using the second-order Taylor expansion like in equation ( 7 ) in this submission . Moreover , the proposed importance-aware quantization solution in equation ( 5 ) is exactly the same as equation ( 8 ) in [ 1 ] , except that their importance is derived from the diagonal hessian instead of weight magnitude ( which is not quite reasonable , refer to the first point ) . 3.One other post-training quantization method GOBO in [ 3 ] , which also uses codes and codebooks to quantize language models , is also not compared . From their reported results , under the same number of bits , GOBO has higher accuracy than the proposed method . E.g. , GOBO has 83.76 % accuracy for the 3-bit quantization on MNLI while the proposed method only has 82.9 % . 4.From Table 2 , there is no winning configuration of ( E , C , P ) that works well on all studied models and tasks . How to determine these hyperparameters for new tasks empirically ? Tuning these parameters separately for each task can be inefficient . Reference : [ 1 ] Hou et al . `` Loss-aware binarization of deep networks . '' International Conference on Learning Representations . 2017 . [ 2 ] Hou et al . `` Loss-aware weight quantization of deep networks . '' International Conference on Learning Representations . 2017 . [ 3 ] Zadeh , Ali Hadi , and Andreas Moshovos . `` GOBO : Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference . '' arXiv:2005.03842 , 2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We highly appreciate your time and efforts to review our manuscript . R1 : It is not clear why the change in the loss in ( 7 ) and ( 8 ) is necessarily related to the magnitude of the weights . A1 : We admit that Section 4.1 was quite confusing . To clarify our major contributions and state our assumptions clearly , we revised Section 4.1 to show that a study magnitude-based importance estimation ( for post-training quantization ) is necessary to replace the Hessian that has been wide-utilized for various sensitivity studies ( because the Hessian would not be available for post-training quantization ) . We acknowledge that it is challenging to connect the magnitude of weights to the loss perturbation in theory . Hence , our contribution should be to 'empirically ' verify that magnitude can also be a useful importance estimation when partial derivatives are not available . Accordingly , we conducted various importance-aware quantization methods with numerous language models in order to support our assumptions . It would be an exciting research topic to investigate theories of how such a simple function using magnitude can provide importance metrics for post-training quantization . R2 : Loss-aware ( or importance-aware as in this paper ) quantization which considers the quantization effect of each parameter to the loss has already been proposed several years ago in [ 1,2 ] . A2 : We acknowledge that loss-aware quantization has been widely utilized for the compression-aware training method . On the other hand , post-training quantization has serious challenges to produce sensitivity metrics because partial derivatives would not be available once training is completed ( inference codes do not include backward propagation descriptions or optimization steps using gradient descent ) . We believe that is why there are few studies to find importance metrics for post-training quantization in the past because most importance or sensitivity estimations have been deeply related to the 1st or 2nd partial derivatives ( e.g , the gradients or the Hessian ) . Thus , in the manuscript , we put a bold assumption that a function using magnitude only can be also used as an importance estimation in the case of post-training quantization . Of course , such an assumption needs to be empirically supported as we demonstrated throughout various experiments in the manuscript . We revised Section 4.1 accordingly . Eq . ( 5 ) and ( 6 ) in the manuscript are fundamentally different from the solutions introduced in [ 1 ] and [ 2 ] . For example , reference [ 1 ] considers binary neural networks where an analytical solution for quantization exists while we propose various importance-aware approximation methods in Eq . ( 5 ) and ( 6 ) for multi-bit quantization . Reference [ 2 ] assumes ternary or symmetric uniform quantization where the number of scaling factors would be the same as that of binary neural networks . Overall , reference [ 1 ] and [ 2 ] are vastly different from ours because 1 ) [ 1 ] and [ 2 ] are not post-training methods , 2 ) [ 1 ] and [ 2 ] study compression-aware training methods where partial derivatives ( e.g. , the Hessian ) can be derived , 3 ) quantization formats are different from ours such that implementation principles are also different from ours ."}, "3": {"review_id": "2Id6XxTjz7c-3", "review_text": "This paper proposes a weighted quantization framework that could be applied to general neural networks for language models . One core idea is that different parameters might show different sensitivity towards a loss function change . Thus it would make sense to take use of importance metrics to do weighted quantization across parameters . Clarity : The paper is clearly written . With a good introduction of related work and a pretty self-inclusive references to experiment setup . I also like how the paper is organized to motivate the inclusion of weight importance into quantization . Originality : Originality in this paper is mostly from introducing the concept of parameter ( weight ) importance and how it is defined and applied to the quantization problem . This paper is based on an earlier paper ( also new ) , e.g. , BiQGEMM ( Jeon et al. , 2020 ) , which paved the foundation to support binary-coding-based quantization techniques to accelerate quantized neural networks . A magnitude-based importance metric is proposed and approved effective in the form of the binary codes . To fine-tune model accuracy , three hyper-parameters are explored and empirically investigated to discover best performance ( regarding model quality ) . Significance : Based on the experiment result , I agree that this paper 's contribution is significant . If we combine the effort of quantization with distillation , as shown with the metrics in DistillBert , we could achieve great ability of inference speed , a very smaller model size but still maintain reasonable model quality .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate your positive feedback . We are happy to see that our contributions are well recognized in your review . We totally agree that combining distillation and quantization ( and potentially also structured/unstructured pruning and/or low-rank approximation ) would be an important and exciting research direction since the size of language models is substantially increasing ."}}