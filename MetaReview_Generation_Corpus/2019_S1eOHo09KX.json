{"year": "2019", "forum": "S1eOHo09KX", "title": "Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data Streams", "decision": "Accept (Poster)", "meta_review": "This paper presents a reinforcement learning approach for online cost-aware feature acquisition. The utility of each feature is measured in terms of expected variations of the model uncertainty (using MC dropout sampling as an estimate of certainty) which is subsequently used as a reward function in the reinforcement learning formulation. The empirical evaluations show improvements over prior approaches in terms of accuracy-cost trade-off on three datasets. AC can confirm that all three reviewers have read the author responses and have significantly contributed to the revision of the manuscript.\n \nInitially, R1 and R2 raised important concerns regarding low technical novelty. R1 requested an ablation study to understand which of the following components gives the most improvement: 1) using proper certainty estimation; 2) using immediate reward; 3) new policy architecture. Pleased to report that the authors addressed the ablation study in their rebuttal and confirmed that MC-dropout certainty plays a crucial rule in the performance of the proposed method. R1 subsequently increased the assigned score to 6. R2 raised concerns about related prior work Contardo et al 2016, which similarly evaluates the most informative features given budget constraints with a recurrent neural network approach. After a long discussion and a detailed rebuttal, R2 upgraded the rating from below the threshold to 7, albeit acknowledging an incremental technical contribution. R3 raised important concerns regarding presentation clarity that were subsequently addressed by the authors. In conclusion, all three reviewers were convinced by the authors rebuttal and have upgraded their initial rating, and AC recommends acceptance of this paper \u2013 congratulations to the authors!\n", "reviews": [{"review_id": "S1eOHo09KX-0", "review_text": "This paper presents a novel method for budgeted cost sensitive learning from Data Streams. This paper seems very similar to the work of Contrado\u2019s RADIN algorithm which similarly evaluates sequential datapoints with a recurrent neural network by adaptively \u201cpurchasing\u201d the most valuable features for the current datapoint under evaluation according to a budget. In this process, a sample (S_i) with up to \u201cd\u201d features arrives for evaluation. A partially revealed feature vector x_i arrives at time \u201ct\u201d for consideration. There seems to exist a set of \u201cknown features\u201d that that are revealed \u201cfor free\u201d before the budget is considered (Algorithm 1). Then while either the budget is not exhausted or some other stopping condition is met features are sequentially revealed either randomly (an explore option with a decaying rate of probability) or according to their cost sensitive utility. When the stopping condition is reached, a prediction is made. After a prediction is made, a random mini-batch of the partially revealed features is pushed into replay memory along with the correct class label and the P. Q, and target Q networks are updated. The ideas of using a sequentially revealed vector of features and sequentially training a network are in Contrado\u2019s RADIN paper. The main novelty of the paper seems to be the use of MC dropout as an estimate of certainty in place of the softmax output layer and the methods of updating the P and Q networks. The value of this paper is in the idea that we can learn online and in a cost sensitive way. The most compelling example of this is the idea that a patient shows up at time \u201ct\u201d and we would like to make a prediction of disease in a cost sensitive way. To this end I would have liked to have seen a chart on how well this algorithm performs across time/history. How well does the algorithm perform on the first 100 patients vs the last 91,962-91,062 patients at what point would it make sense to start to use the algorithm (how much history is needed). Am I correct in assuming there are some base features that are revealed \u201cfor free\u201d for all samples? If so how are these chosen? If so how does the number of these impact the results? In Contrado\u2019s RADIN paper the authors explore both the MNIST dataset and others, including a medical dataset \u201ccardio.\u201d Why did you only use RADIN as a comparison for the MNIST dataset and not the LTRC or diabetes dataset? Did you actually re-implement RADIN or just take the numbers from their paper? In which case, are you certain which MNIST set was used in this paper? (it was not as well specified as in your paper). With respect to the real world validity of the paper, given that the primary value of the paper has to do with cost sensitive online learning, it would have been better to talk more about the various cost structure and how those impact the value of your algorithm. For the first example, MNIST, the assumed uniform cost structure is a toy example that equates feature acquisition with cost. The second example uses computational cost vs relevance gain. This would just me a measure of computational efficiency, in which case all of the computational cost of running the updates to your networks should also be considered as cost. With respect to the third proprietary diabetes dataset, the costs are real and relevant, however there discussion of these are given except to say that you had a single person familiar with medical billing create them for you (also the web address you cite is a general address and does not go to the dataset you are using). In reality, these costs would be bundled. You say you estimate the cost in terms of overall financial burden, patient privacy and patient inconvenience. Usually if you ask the patient to fill out a survey it has multiple questions, so for the same cost you get all the answers. Similarly if you do a blood draw and test for multiple factors the cost to the patient and the hospital are paid for the most part upfront. It is not realistic to say that the cost of asking a patient a questions is 1/20th of the cost of the survey. The first survey question asked would be more likely 90-95% of the cost with each additional question some incremental percentage. To show the value of your work, a better discussion of the cost savings would be appreciated. ", "rating": "7: Good paper, accept", "reply_text": "* Comment : \u201c In Contrado \u2019 s RADIN paper the authors explore both the MNIST dataset and others , including a medical dataset \u201c cardio. \u201d Why did you only use RADIN as a comparison for the MNIST dataset and not the LTRC or diabetes dataset ? Did you actually re-implement RADIN or just take the numbers from their paper ? In which case , are you certain which MNIST set was used in this paper ? ( it was not as well specified as in your paper ) . \u201d We have compared our results with Contrado \u2019 s results as reported on the RADIN paper . The reason behind this was the fact that RADIN is consisting of many components and parameters which makes reproducing their results for our comparisons with RADIN very difficult . We would be glad to include comparisons with RADIN on other datasets , if the reviewer could point us to an open source implementation of RADIN . Regarding the reviewer \u2019 s comment on which samples of the MNIST was used for training/validation/test : we use the standard MNIST separation using the provided train set for our train and validation , and the MNIST test set is used for testing the suggested algorithm . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c With respect to the real world validity of the paper , given that the primary value of the paper has to do with cost sensitive online learning , it would have been better to talk more about the various cost structure and how those impact the value of your algorithm ... \u201d We agree with the reviewer that it is very important to consider cost structures in real-world scenarios . However , a deep study of any specific cost structure ( e.g. , in specific healthcare problems ) is itself an area of research and any problem would require an in-depth study . In this paper , we introduced a general formulation for the problem for cost-sensitive feature acquisition from stream data that is evaluated on different applications . However , a deeper study of any specific cost structure would require integrating domain expertise and is out of the scope of this study . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c the web address you cite is a general address and does not go to the dataset you are using \u201d The web address provided contains links to the dataset download page ( the \u201c Questionnaires , Datasets , and Related Documentation \u201d option on the left sidebar ) . Additionally , we plan to publish the dataset preprocessing source code to help other future work to reproduce and compare our results . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c In reality , these costs would be bundled ... To show the value of your work , a better discussion of the cost savings would be appreciated. \u201d The current formulation presented in this paper allows for having bundled feature sets . In this case , each action would be acquiring a bundle and the reward function is evaluated for the acquisition of this bundle by measuring the variations of uncertainty before and after acquiring the bundle . As suggested by the reviewer , we have added a discussion on this to the revised paper : \u201c In our experiments , for the simplicity of presentation , we assume that all features are independently acquirable at a certain cost , while in many scenarios , features are bundled and acquired together ( e.g. , certain clinical measurements ) . However , it should be noted that the current formulation presented in this paper allows for having bundled feature sets . In this case , each action would be acquiring each bundle and the reward function is evaluated for the acquisition of the bundle by measuring the variations of uncertainty before and after acquiring the bundle . \u201d"}, {"review_id": "S1eOHo09KX-1", "review_text": "The paper presents a RL approach for sequential feature acquisition in a budgeted learning setting, where each feature comes at some cost and the goal is to find a good trade-off between accuracy and cost. Starting with zero feature, the model sequentially acquires new features to update its prediction and stops when the budget is exhausted. The feature selection policy is learned by deep Q-learning. The authors have shown improvements over several prior approaches in terms of accuracy-cost trade-off on three datasets, including a real-world health dataset with real feature costs. While the results are nice, the novelty of this paper is limited. As mentioned in the paper, the RL framework for sequential feature acquisition has been explored multiple times. Compared to prior work, the main novelty in this paper is a reward function based on better calibrated classifier confidence. However, ablations study on the reward function is needed to understand to what extent is this helpful. I find the model description confusing. 1. What is the loss function? In particular, how is the P-Network learned? It seems that the model is based on actor-critic algorithms, but this is not clear from the text. 2. What is the reward function? Only immediate reward is given. 3. What is the state representation? How do you represent features not acquired yet? It is great that the authors have done extensive comparison with prior approaches; however, I find more ablation study needed to understand what made the model works better. There are at least 3 improvements: 1) using proper certainty estimation; 2) using immediate reward; 3) new policy architecture. Right now not clear which one gives the most improvement. Overall, this paper has done some nice improvement over prior work along similar lines, but novelty is limited and more analysis of the model is needed.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for reviewing the manuscript and helpful comments . Please find a point-to-point response to your comments in the following . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * Comment : \u201c 1 . What is the loss function ? In particular , how is the P-Network learned ? It seems that the model is based on actor-critic algorithms , but this is not clear from the text. \u201d The loss function used for the P-Network is a cross-entropy loss as a typical loss used for classification tasks . For training the Q-Network we use mean squared error ( MSE ) between the estimated reward and the observed reward values . Please note that the replay buffer is used to sample batches of feature vectors , labels , actions , and reward values required to measure P- and Q-Network losses . We added the following clarification to the revised paper ( see Sec.3.2 ) : \u201c Cross-entropy and mean squared error ( MSE ) loss functions were used as the objective functions for the P and Q networks , respectively. \u201d -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * Comment : \u201c 2 . What is the reward function ? Only immediate reward is given. \u201d The reward function which is suggested by this paper is presented in Eq . ( 7 ) .Here , we are using epsilon-greedy explorations and Bellman equations to fit the action-value function . It allows the general formulation of non-immediate and accumulated rewards through a discount factor . Intuitively , the suggested reward function in Eq . ( 7 ) measures the expected change of model hypothesis corresponding to each feature acquisition action . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * Comment : \u201c 3 . What is the state representation ? How do you represent features not acquired yet ? \u201d In this paper , each state is the current feature vector containing values for features that are acquired at that state . From the first paragraph of Section 3.1 : \u201c At each point , the current state is defined as the current realization of the feature vector ( i.e. , $ \\bm { x } _i^t $ ) for a given instance. \u201d We use NaN ( not a number ) values to internally represent the features that are not available . However , the implementation we use replaces the NaN values with zeros during the forward/backward computation . We believe it is an efficient approach compared to using separate mask vectors to represent missing features as it reduces the memory and I/O overheads . We included a brief explanation in the revised version ( See Sec.3.2 ) : \u201c Note that , in our implementation , for efficiency reasons , we use NaN ( not a number ) values to represent features that are not available and impute them with zeros during the forward/backward computation. \u201d -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * Comment : \u201c It is great that the authors have done extensive comparison with prior approaches ; however , I find more ablation study needed to understand what made the model works better. \u201d Thank you for suggesting this . In the revised version , we added a new subsection ( Sec.4.3.1 ) to the results section entitled \u201c ablation study \u201d . In summary , it presents an ablation study and comparisons of : - Using the MC-Dropout certainty versus the uncalibrated softmax estimates . We compared the accuracy of the estimated certainty values achieved as well as the overall impact on the feature acquisition performance ( see Fig.5a and Fig.5b of the revised version ) . As it can be seen from these figures , the idea of using MC-Dropout certainty plays a crucial rule in the performance of the proposed method . - Demonstrating the effectiveness of the suggested representation sharing between the P and Q networks ( see Fig.6 ) demonstrating that the representation sharing would result in a faster convergence . - We added an analysis of the suggested method under different enforced budget constraints ( see Fig.7 ) .According to results , the suggested method is able to efficiently operate at different enforced budget constraints . - Regarding other ablation analysis suggested by the reviewer , we have comparisons of the suggested approach ( OL ) and a basic reinforcement learning based method ( RL-based ) in the comparison results presented in Section 4.2 . Due to space considerations , in the revised version , we discussed this case in the ablation study section without reiterating the plots and by referring to Fig.2 and Fig.4b ( see Sec 4.3.1 ) : \u201c A comparison between the suggested feature-value function ( OL ) in this paper with a traditional feature-value function ( RL-Based ) was presented in Figure 2 and Figure 4b . Here , RL-Based method is using a similar architecture and learning algorithm as the OL , while the reward function is simply the negative of feature costs for acquiring each feature and a positive value for making correct predictions . As it can be seen from the comparison of these approaches , the reward function suggested in this paper results in a more efficient feature acquisition. \u201c"}, {"review_id": "S1eOHo09KX-2", "review_text": "I like the approach, however: I consider the paper to be poorly written. The presentation needs to be improved for me to find it acceptable. It presents a stream-oriented (aka online) version of the algorithm, but experiments treat the algorithm as an offline training algorithm. This is particularly critical in this area because feature acquisition costs during the \"warm-up\" phase are actual costs, and given the inherent sample complexity challenges of reinforcement learning, I would expect them to be significant in practice. This would be fine if the setup is \"we have a fixed offline set of examples where all features have been acquired (full cost paid) from which we will learn a selector+predictor for test time\". The algorithm 1 float greatly helped intelligibility, but I'm left confused. * Is this underlying predictor trained simultaneously to the selector? * Exposition suggests yes (\"At the same time, learning should take place by updating the model while maintaining the budgets.\"), but algorithm block doesn't make it obvious. * Maybe line 21 reference to \"train data\" refers to the underlying predictor. * Line 16 pushes a value estimate into the replay buffer based upon the current underlying predictor, but: * this value will be stale when we dequeue from the replay buffer if the underlying predictor has changed, and * we have enough information stored in the replay buffer to recompute the value estimate using the new predictor, but * this is not discussed at all. Also, I'm wondering about the annealing schedule for the exploration parameter (this is related to my concern that the algorithm is not really an online algorithm). The experiments are all silent on the \"exploration\" feature acquisition cost. Furthermore I'm wondering: when you do the test evaluations, do you set exploration to 0? I also found the following disturbing: \"It is also worth noting that, as the proposed method is incremental, we continued feature acquisition until all features were acquired and reported the average accuracy corresponding to each feature acquisition budget.\" Does this mean the underlying predictor was trained on data that it would not have if the budget constraint were strictly enforced? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "* Comment : Also , I 'm wondering about the annealing schedule for the exploration parameter ( this is related to my concern that the algorithm is not really an online algorithm ) . The experiments are all silent on the `` exploration '' feature acquisition cost . Furthermore I 'm wondering : when you do the test evaluations , do you set exploration to 0 ? In an online learning setup , data becomes available sequentially and the goal for an online learner is to update its hypothesis as more data is being observed . There are two main considerations for an online method . First , data is not provided or can be stored as a batch . Second , the hypothesis should be refined incrementally as more observations take place . Regarding the reviewer \u2019 s concern about annealing , annealing is a standard approach widely used in the literature helping early steps of optimization . We believe that the suggested algorithm is online because , initially , there is no viable alternative strategy to follow due to the limited number of samples . However , as we observe more samples , we anneal the random decisions and try to use the captured knowledge instead . In this respect , the suggested algorithm is online according to the definition above . Regarding the exploration probability used in our experiments : during the training and validation phase , we use the random exploration mechanism . However , for the comparison of the results with other work in the literature , as they are all offline methods , we decided to not to do the exploration . In order to address the reviewer \u2019 s comment , we added the following explanation to the revised paper : \u201c During the training and validation phase , we use the random exploration mechanism . However , for the comparison of the results with other work in the literature , as they are all offline methods , the random exploration is not used during the feature acquisition. \u201d -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : I also found the following disturbing : `` It is also worth noting that , as the proposed method is incremental , we continued feature acquisition until all features were acquired and reported the average accuracy corresponding to each feature acquisition budget . '' Does this mean the underlying predictor was trained on data that it would not have if the budget constraint were strictly enforced ? In order to address the reviewer \u2019 s concern , we conducted experiments using different enforced budgets ( see Fig.7 ) .In summary , according to our experiments , the suggested method is able to efficiently operate at different enforced budget constraints . We have also included the following discussion to the paper : \u201c Figure 7 shows the performance of the OL method having various limited budgets during the operation . Here , we report the accuracy-cost curves for 25 % , 50 % , 75 % , and 100 % of the budget required to acquire all features . As it can be inferred from this figure , the suggested method is able to efficiently operate at different enforced budget constraints . \u201d"}], "0": {"review_id": "S1eOHo09KX-0", "review_text": "This paper presents a novel method for budgeted cost sensitive learning from Data Streams. This paper seems very similar to the work of Contrado\u2019s RADIN algorithm which similarly evaluates sequential datapoints with a recurrent neural network by adaptively \u201cpurchasing\u201d the most valuable features for the current datapoint under evaluation according to a budget. In this process, a sample (S_i) with up to \u201cd\u201d features arrives for evaluation. A partially revealed feature vector x_i arrives at time \u201ct\u201d for consideration. There seems to exist a set of \u201cknown features\u201d that that are revealed \u201cfor free\u201d before the budget is considered (Algorithm 1). Then while either the budget is not exhausted or some other stopping condition is met features are sequentially revealed either randomly (an explore option with a decaying rate of probability) or according to their cost sensitive utility. When the stopping condition is reached, a prediction is made. After a prediction is made, a random mini-batch of the partially revealed features is pushed into replay memory along with the correct class label and the P. Q, and target Q networks are updated. The ideas of using a sequentially revealed vector of features and sequentially training a network are in Contrado\u2019s RADIN paper. The main novelty of the paper seems to be the use of MC dropout as an estimate of certainty in place of the softmax output layer and the methods of updating the P and Q networks. The value of this paper is in the idea that we can learn online and in a cost sensitive way. The most compelling example of this is the idea that a patient shows up at time \u201ct\u201d and we would like to make a prediction of disease in a cost sensitive way. To this end I would have liked to have seen a chart on how well this algorithm performs across time/history. How well does the algorithm perform on the first 100 patients vs the last 91,962-91,062 patients at what point would it make sense to start to use the algorithm (how much history is needed). Am I correct in assuming there are some base features that are revealed \u201cfor free\u201d for all samples? If so how are these chosen? If so how does the number of these impact the results? In Contrado\u2019s RADIN paper the authors explore both the MNIST dataset and others, including a medical dataset \u201ccardio.\u201d Why did you only use RADIN as a comparison for the MNIST dataset and not the LTRC or diabetes dataset? Did you actually re-implement RADIN or just take the numbers from their paper? In which case, are you certain which MNIST set was used in this paper? (it was not as well specified as in your paper). With respect to the real world validity of the paper, given that the primary value of the paper has to do with cost sensitive online learning, it would have been better to talk more about the various cost structure and how those impact the value of your algorithm. For the first example, MNIST, the assumed uniform cost structure is a toy example that equates feature acquisition with cost. The second example uses computational cost vs relevance gain. This would just me a measure of computational efficiency, in which case all of the computational cost of running the updates to your networks should also be considered as cost. With respect to the third proprietary diabetes dataset, the costs are real and relevant, however there discussion of these are given except to say that you had a single person familiar with medical billing create them for you (also the web address you cite is a general address and does not go to the dataset you are using). In reality, these costs would be bundled. You say you estimate the cost in terms of overall financial burden, patient privacy and patient inconvenience. Usually if you ask the patient to fill out a survey it has multiple questions, so for the same cost you get all the answers. Similarly if you do a blood draw and test for multiple factors the cost to the patient and the hospital are paid for the most part upfront. It is not realistic to say that the cost of asking a patient a questions is 1/20th of the cost of the survey. The first survey question asked would be more likely 90-95% of the cost with each additional question some incremental percentage. To show the value of your work, a better discussion of the cost savings would be appreciated. ", "rating": "7: Good paper, accept", "reply_text": "* Comment : \u201c In Contrado \u2019 s RADIN paper the authors explore both the MNIST dataset and others , including a medical dataset \u201c cardio. \u201d Why did you only use RADIN as a comparison for the MNIST dataset and not the LTRC or diabetes dataset ? Did you actually re-implement RADIN or just take the numbers from their paper ? In which case , are you certain which MNIST set was used in this paper ? ( it was not as well specified as in your paper ) . \u201d We have compared our results with Contrado \u2019 s results as reported on the RADIN paper . The reason behind this was the fact that RADIN is consisting of many components and parameters which makes reproducing their results for our comparisons with RADIN very difficult . We would be glad to include comparisons with RADIN on other datasets , if the reviewer could point us to an open source implementation of RADIN . Regarding the reviewer \u2019 s comment on which samples of the MNIST was used for training/validation/test : we use the standard MNIST separation using the provided train set for our train and validation , and the MNIST test set is used for testing the suggested algorithm . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c With respect to the real world validity of the paper , given that the primary value of the paper has to do with cost sensitive online learning , it would have been better to talk more about the various cost structure and how those impact the value of your algorithm ... \u201d We agree with the reviewer that it is very important to consider cost structures in real-world scenarios . However , a deep study of any specific cost structure ( e.g. , in specific healthcare problems ) is itself an area of research and any problem would require an in-depth study . In this paper , we introduced a general formulation for the problem for cost-sensitive feature acquisition from stream data that is evaluated on different applications . However , a deeper study of any specific cost structure would require integrating domain expertise and is out of the scope of this study . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c the web address you cite is a general address and does not go to the dataset you are using \u201d The web address provided contains links to the dataset download page ( the \u201c Questionnaires , Datasets , and Related Documentation \u201d option on the left sidebar ) . Additionally , we plan to publish the dataset preprocessing source code to help other future work to reproduce and compare our results . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c In reality , these costs would be bundled ... To show the value of your work , a better discussion of the cost savings would be appreciated. \u201d The current formulation presented in this paper allows for having bundled feature sets . In this case , each action would be acquiring a bundle and the reward function is evaluated for the acquisition of this bundle by measuring the variations of uncertainty before and after acquiring the bundle . As suggested by the reviewer , we have added a discussion on this to the revised paper : \u201c In our experiments , for the simplicity of presentation , we assume that all features are independently acquirable at a certain cost , while in many scenarios , features are bundled and acquired together ( e.g. , certain clinical measurements ) . However , it should be noted that the current formulation presented in this paper allows for having bundled feature sets . In this case , each action would be acquiring each bundle and the reward function is evaluated for the acquisition of the bundle by measuring the variations of uncertainty before and after acquiring the bundle . \u201d"}, "1": {"review_id": "S1eOHo09KX-1", "review_text": "The paper presents a RL approach for sequential feature acquisition in a budgeted learning setting, where each feature comes at some cost and the goal is to find a good trade-off between accuracy and cost. Starting with zero feature, the model sequentially acquires new features to update its prediction and stops when the budget is exhausted. The feature selection policy is learned by deep Q-learning. The authors have shown improvements over several prior approaches in terms of accuracy-cost trade-off on three datasets, including a real-world health dataset with real feature costs. While the results are nice, the novelty of this paper is limited. As mentioned in the paper, the RL framework for sequential feature acquisition has been explored multiple times. Compared to prior work, the main novelty in this paper is a reward function based on better calibrated classifier confidence. However, ablations study on the reward function is needed to understand to what extent is this helpful. I find the model description confusing. 1. What is the loss function? In particular, how is the P-Network learned? It seems that the model is based on actor-critic algorithms, but this is not clear from the text. 2. What is the reward function? Only immediate reward is given. 3. What is the state representation? How do you represent features not acquired yet? It is great that the authors have done extensive comparison with prior approaches; however, I find more ablation study needed to understand what made the model works better. There are at least 3 improvements: 1) using proper certainty estimation; 2) using immediate reward; 3) new policy architecture. Right now not clear which one gives the most improvement. Overall, this paper has done some nice improvement over prior work along similar lines, but novelty is limited and more analysis of the model is needed.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for reviewing the manuscript and helpful comments . Please find a point-to-point response to your comments in the following . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * Comment : \u201c 1 . What is the loss function ? In particular , how is the P-Network learned ? It seems that the model is based on actor-critic algorithms , but this is not clear from the text. \u201d The loss function used for the P-Network is a cross-entropy loss as a typical loss used for classification tasks . For training the Q-Network we use mean squared error ( MSE ) between the estimated reward and the observed reward values . Please note that the replay buffer is used to sample batches of feature vectors , labels , actions , and reward values required to measure P- and Q-Network losses . We added the following clarification to the revised paper ( see Sec.3.2 ) : \u201c Cross-entropy and mean squared error ( MSE ) loss functions were used as the objective functions for the P and Q networks , respectively. \u201d -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * Comment : \u201c 2 . What is the reward function ? Only immediate reward is given. \u201d The reward function which is suggested by this paper is presented in Eq . ( 7 ) .Here , we are using epsilon-greedy explorations and Bellman equations to fit the action-value function . It allows the general formulation of non-immediate and accumulated rewards through a discount factor . Intuitively , the suggested reward function in Eq . ( 7 ) measures the expected change of model hypothesis corresponding to each feature acquisition action . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * Comment : \u201c 3 . What is the state representation ? How do you represent features not acquired yet ? \u201d In this paper , each state is the current feature vector containing values for features that are acquired at that state . From the first paragraph of Section 3.1 : \u201c At each point , the current state is defined as the current realization of the feature vector ( i.e. , $ \\bm { x } _i^t $ ) for a given instance. \u201d We use NaN ( not a number ) values to internally represent the features that are not available . However , the implementation we use replaces the NaN values with zeros during the forward/backward computation . We believe it is an efficient approach compared to using separate mask vectors to represent missing features as it reduces the memory and I/O overheads . We included a brief explanation in the revised version ( See Sec.3.2 ) : \u201c Note that , in our implementation , for efficiency reasons , we use NaN ( not a number ) values to represent features that are not available and impute them with zeros during the forward/backward computation. \u201d -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * Comment : \u201c It is great that the authors have done extensive comparison with prior approaches ; however , I find more ablation study needed to understand what made the model works better. \u201d Thank you for suggesting this . In the revised version , we added a new subsection ( Sec.4.3.1 ) to the results section entitled \u201c ablation study \u201d . In summary , it presents an ablation study and comparisons of : - Using the MC-Dropout certainty versus the uncalibrated softmax estimates . We compared the accuracy of the estimated certainty values achieved as well as the overall impact on the feature acquisition performance ( see Fig.5a and Fig.5b of the revised version ) . As it can be seen from these figures , the idea of using MC-Dropout certainty plays a crucial rule in the performance of the proposed method . - Demonstrating the effectiveness of the suggested representation sharing between the P and Q networks ( see Fig.6 ) demonstrating that the representation sharing would result in a faster convergence . - We added an analysis of the suggested method under different enforced budget constraints ( see Fig.7 ) .According to results , the suggested method is able to efficiently operate at different enforced budget constraints . - Regarding other ablation analysis suggested by the reviewer , we have comparisons of the suggested approach ( OL ) and a basic reinforcement learning based method ( RL-based ) in the comparison results presented in Section 4.2 . Due to space considerations , in the revised version , we discussed this case in the ablation study section without reiterating the plots and by referring to Fig.2 and Fig.4b ( see Sec 4.3.1 ) : \u201c A comparison between the suggested feature-value function ( OL ) in this paper with a traditional feature-value function ( RL-Based ) was presented in Figure 2 and Figure 4b . Here , RL-Based method is using a similar architecture and learning algorithm as the OL , while the reward function is simply the negative of feature costs for acquiring each feature and a positive value for making correct predictions . As it can be seen from the comparison of these approaches , the reward function suggested in this paper results in a more efficient feature acquisition. \u201c"}, "2": {"review_id": "S1eOHo09KX-2", "review_text": "I like the approach, however: I consider the paper to be poorly written. The presentation needs to be improved for me to find it acceptable. It presents a stream-oriented (aka online) version of the algorithm, but experiments treat the algorithm as an offline training algorithm. This is particularly critical in this area because feature acquisition costs during the \"warm-up\" phase are actual costs, and given the inherent sample complexity challenges of reinforcement learning, I would expect them to be significant in practice. This would be fine if the setup is \"we have a fixed offline set of examples where all features have been acquired (full cost paid) from which we will learn a selector+predictor for test time\". The algorithm 1 float greatly helped intelligibility, but I'm left confused. * Is this underlying predictor trained simultaneously to the selector? * Exposition suggests yes (\"At the same time, learning should take place by updating the model while maintaining the budgets.\"), but algorithm block doesn't make it obvious. * Maybe line 21 reference to \"train data\" refers to the underlying predictor. * Line 16 pushes a value estimate into the replay buffer based upon the current underlying predictor, but: * this value will be stale when we dequeue from the replay buffer if the underlying predictor has changed, and * we have enough information stored in the replay buffer to recompute the value estimate using the new predictor, but * this is not discussed at all. Also, I'm wondering about the annealing schedule for the exploration parameter (this is related to my concern that the algorithm is not really an online algorithm). The experiments are all silent on the \"exploration\" feature acquisition cost. Furthermore I'm wondering: when you do the test evaluations, do you set exploration to 0? I also found the following disturbing: \"It is also worth noting that, as the proposed method is incremental, we continued feature acquisition until all features were acquired and reported the average accuracy corresponding to each feature acquisition budget.\" Does this mean the underlying predictor was trained on data that it would not have if the budget constraint were strictly enforced? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "* Comment : Also , I 'm wondering about the annealing schedule for the exploration parameter ( this is related to my concern that the algorithm is not really an online algorithm ) . The experiments are all silent on the `` exploration '' feature acquisition cost . Furthermore I 'm wondering : when you do the test evaluations , do you set exploration to 0 ? In an online learning setup , data becomes available sequentially and the goal for an online learner is to update its hypothesis as more data is being observed . There are two main considerations for an online method . First , data is not provided or can be stored as a batch . Second , the hypothesis should be refined incrementally as more observations take place . Regarding the reviewer \u2019 s concern about annealing , annealing is a standard approach widely used in the literature helping early steps of optimization . We believe that the suggested algorithm is online because , initially , there is no viable alternative strategy to follow due to the limited number of samples . However , as we observe more samples , we anneal the random decisions and try to use the captured knowledge instead . In this respect , the suggested algorithm is online according to the definition above . Regarding the exploration probability used in our experiments : during the training and validation phase , we use the random exploration mechanism . However , for the comparison of the results with other work in the literature , as they are all offline methods , we decided to not to do the exploration . In order to address the reviewer \u2019 s comment , we added the following explanation to the revised paper : \u201c During the training and validation phase , we use the random exploration mechanism . However , for the comparison of the results with other work in the literature , as they are all offline methods , the random exploration is not used during the feature acquisition. \u201d -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : I also found the following disturbing : `` It is also worth noting that , as the proposed method is incremental , we continued feature acquisition until all features were acquired and reported the average accuracy corresponding to each feature acquisition budget . '' Does this mean the underlying predictor was trained on data that it would not have if the budget constraint were strictly enforced ? In order to address the reviewer \u2019 s concern , we conducted experiments using different enforced budgets ( see Fig.7 ) .In summary , according to our experiments , the suggested method is able to efficiently operate at different enforced budget constraints . We have also included the following discussion to the paper : \u201c Figure 7 shows the performance of the OL method having various limited budgets during the operation . Here , we report the accuracy-cost curves for 25 % , 50 % , 75 % , and 100 % of the budget required to acquire all features . As it can be inferred from this figure , the suggested method is able to efficiently operate at different enforced budget constraints . \u201d"}}