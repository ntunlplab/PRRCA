{"year": "2018", "forum": "HJNGGmZ0Z", "title": "What is image captioning made of?", "decision": "Reject", "meta_review": "Paper reviewed by three experts who have provided detailed feedback. All three recommend rejection, and this AC sees no reason to overrule their recommendation. ", "reviews": [{"review_id": "HJNGGmZ0Z-0", "review_text": "This paper analyzes the effect of image features on image captioning. The authors propose to use a model similar to that of Vinyals et al., 2015 and change the image features it is conditioned on. The MSCOCO captioning and Flickr30K datasets are used for evaluation. Introduction - The introduction to the paper could be made clearer - the authors talk about the language of captioning datasets being repetitive, but that fact is neither used or discussed later. - The introduction also states that the authors will propose ways to improve image captioning. This is never discussed. Captioning Model and Table 1 - The authors use greedy (argmax) decoding which is known to result in repetitive captions. In fact, Vinyals et al. note this very point in their paper. I understand this design choice was made to focus more on the image side, rather than the decoding (language) side, but I find it to be very limiting. In this regime of greedy decoding it is hard to see any difference between the different ConvNet features used for captioning - Table 1 shows meteor scores within 0.19 - 0.22 for all methods. - Another effect (possibly due to greedy decoding + choice of model), is that the numbers in Table 1 are rather low compared to the COCO leaderboard. The top 50 entries have METEOR scores >= 0.25, while the maximum METEOR score reported by the authors is 0.22. Similar trend holds for other metrics like BLEU-4. - The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding. Experimental Setup and Training Details - How was the model optimized? No training details are provided. Did you use dropout? Were hyperparamters fixed for training across different feature sizes of VGG19 and ResNet-152? What is the variance in the numbers for Table 1? Main claim of the paper Devlin et al., 2015 show a simple nearest neighbor baseline which in my opinion shows this more convincingly. Two more papers from the same group which use also make similar observations - tweaking the image representation makes image captioning better: (1) Fang et al., 2015: Multiple-instance Learning using bag-of-objects helps captioning (2) Misra et al. 2016 (not cited): label noise can be modeled which helps captioning. This claim has been both made and empirically demonstrated earlier. Metrics for evaluation - Anderson et al., 2016 (not cited) proposed the SPICE metric and also showed how current metrics including CiDER may not be suitable for evaluating image captions. The COCO leaderboard also uses this metric as one of its evaluation metrics. If the authors are evaluating on the test set and reporting numbers, then it is odd that they `skipped' reporting SPICE numbers. Choice of Datasets - If we are thoroughly evaluating the effect of image features, doing so on other datasets is very important. Visual Genome (Krishnan et al., not cited) and SIND (Huang et al., not cited) are two datasets which are both larger than Flickr30k and have different image distributions from MSCOCO. These datasets should show whether using more general features (YOLO-9k) helps. The authors should evaluate on these datasets to make their findings stronger and more valuable. Minor comments - Figure 1 is hard to read on paper. Please improve it. - Figure 2 is hard to read even on screen. It is really interesting, so improving the quality of this figure will really help.", "rating": "4: Ok but not good enough - rejection", "reply_text": "> - The introduction to the paper could be made clearer We have updated the introduction to make it clearer . > the authors talk about the language of captioning datasets being repetitive , but that fact is neither used or discussed later . In our analysis we observed that in all cases , i.e. , using any type of representation , there is only a small subset ( 20-30 % ) of the captions that are unique . This was mentioned in section 4.5 of our original submission of the paper . We have further clarified this section in the updated version . > The introduction also states that the authors will propose ways to improve image captioning . This is never discussed . We do not promise to do that , but rather state that findings could help improve image captioning systems . > Captioning Model and Table 1 - The authors use greedy ( argmax ) decoding which is known to result in repetitive captions . In fact , Vinyals et al.note this very point in their paper . I understand this design choice was made to focus more on the image side , rather than the decoding ( language ) side , but I find it to be very limiting . > In this regime of greedy decoding it is hard to see any difference between the different ConvNet features used for captioning This was purposefully done for determinism . We wanted to understand the best 'choice of words ' by the model given a particular representation . > The top 50 entries have METEOR scores > = 0.25 , while the maximum METEOR score reported by the authors is 0.22 . Similar trend holds for other metrics like BLEU-4 . Our model should be compared with the Neuraltalk model as it has the same settings . Other similar models ( like Vinyals et al 2015 ) use ensembles and other engineering tricks that we are not interested in . > - The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding . Experimental Setup and Training Details - How was the model optimized ? No training details are provided . Did you use dropout ? Were hyperparameters fixed for training across different feature sizes of VGG19 and ResNet-152 ? What is the variance in the numbers for Table 1 ? Our settings are : LSTM with 128 dimensional word embeddings and 256 dimensional hidden representations Dropout over LSTM of 0.8 Adam for optimization . Learning rate = 4e-4 We \u2019 ll add the variance figures to an improved version of the paper ."}, {"review_id": "HJNGGmZ0Z-1", "review_text": "The paper claims that image captioning systems work so well, while most recent state of the art papers show that they produce 50% errors, so far from perfect. The paper lacks novelty, just reports some results without proper analysis or insights. Main weakness of the paper: - Missing many IC systems citations and comparisons (see https://competitions.codalab.org/competitions/3221#results) - According to \"SPICE: Semantic Propositional Image Caption Evaluation\" current metrics used in image captioning don't correlate with human judgement. - Most Image Caption papers which use a pre-trained CNN model, do fine-tune the image feature extractor to improve the results (see Vinyals et al. 2016). Therefore correlation of the image features with the captions is weaker that it could be. - The experiments reported in Table1 are way below state-of-the-art results, there a tons of previous work with much better results, see https://competitions.codalab.org/competitions/3221#results - To provide a fair comparison authors, should compare their results with other paper results. - Tables 2 and 3 are missing the original baselines. The evaluation used in the paper don't correlate well with human ratings see (SPICE paper), therefore trying to improve them marginally doesn't make a difference. - Getting better performance by switching from VGG19 to ResNet152 is expected, however they obtain worse results than Vinyals et al. 2016 with inception_v3. - The claim \"The bag of objects model clusters these group the best\" is not supported by any evidence or metric. One interesting experiment but missing in section 4.4 would be how the image features change after fine-tuning for the captioning task. Typos: - synsest-level -> synsets-level", "rating": "4: Ok but not good enough - rejection", "reply_text": "> The paper lacks novelty , just reports some results without proper analysis or insights . > Main weakness of the paper : > - Missing many IC systems citations and comparisons ( see https : //competitions.codalab.org/competitions/3221 # results ) We stress that our evaluations are with respect to the model proposed by Karpathy et al 2015 . Our goal is not to 'beat or break ' systems but to understand the 'whys ' and 'hows ' . > - According to `` SPICE : Semantic Propositional Image Caption Evaluation '' current metrics used in image captioning do n't correlate with human judgement . We are not claiming explicitly that any of the metrics has good correlation with human judgements . As we mentioned before our focus on CIDEr is because a ) the official evaluation script from MSCOCO contains only CIDEr , Meteor , BLEU and ROUGE , b ) CIDEr is a metric that was officially developed for the task of image captioning , c ) CIDEr is the official metric for MSCOCO , d ) papers by Liu et al 2017 , Kilickaya et al.2017 and Vedantam et al , 2015 ( with the human correlation experiments over Flickr8k dataset ) still state the importance of CIDEr as a metric for image captioning . We further note that we observe a similar trend as we found in CIDEr , so all our observations are still valid . > - Most Image Caption papers which use a pre-trained CNN model , do fine-tune the image feature extractor to improve the results ( see Vinyals et al.2016 ) .Therefore correlation of the image features with the captions is weaker that it could be . While it is true that fine-tuning could have been helpful to bump performance , our paper deals with an exploration of representational properties . Vinyals et al.2016 has shown that fine-tuning gives only a minor 1-point improvement for BLEU . This is also using an ensemble of models . We again state that our experiments are about understanding image captioning models . > - To provide a fair comparison , authors should compare their results with other paper results . - Tables 2 and 3 are missing the original baselines . We will add the results from the comparable papers , even though our focus is not comparisons or to show performance improvements over other models . However , we do not understand what the reviewer means by \u201c original baselines \u201d . Could you please clarify ? > The evaluation used in the paper do n't correlate well with human ratings see ( SPICE paper ) , therefore trying to improve them marginally does n't make a difference . Please see answer above regarding metrics . In addition , our focus is not to improve the performance of the system , but to interpret the 'how ' and 'why ' of the system . To this end , we have made significant progress . > - Getting better performance by switching from VGG19 to ResNet152 is expected , however they obtain worse results than Vinyals et al.2016 with inception_v3 . We have not chosen Vinyals et al.2016 since it uses ensembles and other clever engineering tricks . This would make it hard to answer the questions we ask in this paper -- namely , the contribution of image representation . Our results are comparable to those in Karpathy et al , 2015 . We will add this into the table . > - The claim `` The bag of objects model clusters these group the best '' is not supported by any evidence or metric . We believe that the reviewer has misunderstood the sentence . This sentence explains the observations in Figure 1 ( more specifically Figure 1a ) . The figure shows that the bag of objects representation forms better clusters . It shows the cosine distances between each group for the bag of objects representation . We see from the figure that the bag of objects representations clusters these groups best . For example , the average image representation of \u201c dog \u201d correlates with images containing \u201c dog \u201d as a pair like \u201c dog+person \u201d and \u201c dog+toilet \u201d . We are aware that this is true for our given example , however we expect this to extrapolate over other examples in the dataset . > One interesting experiment but missing in section 4.4 would be how the image features change after fine-tuning for the captioning task . We will do it as a future work , even though this does not allow us to answer our questions posed in this paper ."}, {"review_id": "HJNGGmZ0Z-2", "review_text": "This paper is an experimental paper. It investigates what sort of image representations are good for image captioning systems. Overall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings. The main issue of the paper is the lack of novelty. Even for an experimental paper, I would argue that novelty in the experimental methodology is an important fact. Unfortunately, I do not see any novel concept in the experimental setup. I recomend this paper for a workshop presentation. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "> Overall , the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings . > The main issue of the paper is the lack of novelty . Even for an experimental paper , I would argue that novelty in the experimental methodology is an important fact . Our claim is in the novel 'insights ' into end-to-end model of image captioning models . Our empirical evaluations with multiple representations , visualizations and out of domain experiments reveal new and important insights that should be of interest to the community . We kindly ask clarification from the reviewer regarding what is meant by 'novelty in experimental methodology ' ."}], "0": {"review_id": "HJNGGmZ0Z-0", "review_text": "This paper analyzes the effect of image features on image captioning. The authors propose to use a model similar to that of Vinyals et al., 2015 and change the image features it is conditioned on. The MSCOCO captioning and Flickr30K datasets are used for evaluation. Introduction - The introduction to the paper could be made clearer - the authors talk about the language of captioning datasets being repetitive, but that fact is neither used or discussed later. - The introduction also states that the authors will propose ways to improve image captioning. This is never discussed. Captioning Model and Table 1 - The authors use greedy (argmax) decoding which is known to result in repetitive captions. In fact, Vinyals et al. note this very point in their paper. I understand this design choice was made to focus more on the image side, rather than the decoding (language) side, but I find it to be very limiting. In this regime of greedy decoding it is hard to see any difference between the different ConvNet features used for captioning - Table 1 shows meteor scores within 0.19 - 0.22 for all methods. - Another effect (possibly due to greedy decoding + choice of model), is that the numbers in Table 1 are rather low compared to the COCO leaderboard. The top 50 entries have METEOR scores >= 0.25, while the maximum METEOR score reported by the authors is 0.22. Similar trend holds for other metrics like BLEU-4. - The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding. Experimental Setup and Training Details - How was the model optimized? No training details are provided. Did you use dropout? Were hyperparamters fixed for training across different feature sizes of VGG19 and ResNet-152? What is the variance in the numbers for Table 1? Main claim of the paper Devlin et al., 2015 show a simple nearest neighbor baseline which in my opinion shows this more convincingly. Two more papers from the same group which use also make similar observations - tweaking the image representation makes image captioning better: (1) Fang et al., 2015: Multiple-instance Learning using bag-of-objects helps captioning (2) Misra et al. 2016 (not cited): label noise can be modeled which helps captioning. This claim has been both made and empirically demonstrated earlier. Metrics for evaluation - Anderson et al., 2016 (not cited) proposed the SPICE metric and also showed how current metrics including CiDER may not be suitable for evaluating image captions. The COCO leaderboard also uses this metric as one of its evaluation metrics. If the authors are evaluating on the test set and reporting numbers, then it is odd that they `skipped' reporting SPICE numbers. Choice of Datasets - If we are thoroughly evaluating the effect of image features, doing so on other datasets is very important. Visual Genome (Krishnan et al., not cited) and SIND (Huang et al., not cited) are two datasets which are both larger than Flickr30k and have different image distributions from MSCOCO. These datasets should show whether using more general features (YOLO-9k) helps. The authors should evaluate on these datasets to make their findings stronger and more valuable. Minor comments - Figure 1 is hard to read on paper. Please improve it. - Figure 2 is hard to read even on screen. It is really interesting, so improving the quality of this figure will really help.", "rating": "4: Ok but not good enough - rejection", "reply_text": "> - The introduction to the paper could be made clearer We have updated the introduction to make it clearer . > the authors talk about the language of captioning datasets being repetitive , but that fact is neither used or discussed later . In our analysis we observed that in all cases , i.e. , using any type of representation , there is only a small subset ( 20-30 % ) of the captions that are unique . This was mentioned in section 4.5 of our original submission of the paper . We have further clarified this section in the updated version . > The introduction also states that the authors will propose ways to improve image captioning . This is never discussed . We do not promise to do that , but rather state that findings could help improve image captioning systems . > Captioning Model and Table 1 - The authors use greedy ( argmax ) decoding which is known to result in repetitive captions . In fact , Vinyals et al.note this very point in their paper . I understand this design choice was made to focus more on the image side , rather than the decoding ( language ) side , but I find it to be very limiting . > In this regime of greedy decoding it is hard to see any difference between the different ConvNet features used for captioning This was purposefully done for determinism . We wanted to understand the best 'choice of words ' by the model given a particular representation . > The top 50 entries have METEOR scores > = 0.25 , while the maximum METEOR score reported by the authors is 0.22 . Similar trend holds for other metrics like BLEU-4 . Our model should be compared with the Neuraltalk model as it has the same settings . Other similar models ( like Vinyals et al 2015 ) use ensembles and other engineering tricks that we are not interested in . > - The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding . Experimental Setup and Training Details - How was the model optimized ? No training details are provided . Did you use dropout ? Were hyperparameters fixed for training across different feature sizes of VGG19 and ResNet-152 ? What is the variance in the numbers for Table 1 ? Our settings are : LSTM with 128 dimensional word embeddings and 256 dimensional hidden representations Dropout over LSTM of 0.8 Adam for optimization . Learning rate = 4e-4 We \u2019 ll add the variance figures to an improved version of the paper ."}, "1": {"review_id": "HJNGGmZ0Z-1", "review_text": "The paper claims that image captioning systems work so well, while most recent state of the art papers show that they produce 50% errors, so far from perfect. The paper lacks novelty, just reports some results without proper analysis or insights. Main weakness of the paper: - Missing many IC systems citations and comparisons (see https://competitions.codalab.org/competitions/3221#results) - According to \"SPICE: Semantic Propositional Image Caption Evaluation\" current metrics used in image captioning don't correlate with human judgement. - Most Image Caption papers which use a pre-trained CNN model, do fine-tune the image feature extractor to improve the results (see Vinyals et al. 2016). Therefore correlation of the image features with the captions is weaker that it could be. - The experiments reported in Table1 are way below state-of-the-art results, there a tons of previous work with much better results, see https://competitions.codalab.org/competitions/3221#results - To provide a fair comparison authors, should compare their results with other paper results. - Tables 2 and 3 are missing the original baselines. The evaluation used in the paper don't correlate well with human ratings see (SPICE paper), therefore trying to improve them marginally doesn't make a difference. - Getting better performance by switching from VGG19 to ResNet152 is expected, however they obtain worse results than Vinyals et al. 2016 with inception_v3. - The claim \"The bag of objects model clusters these group the best\" is not supported by any evidence or metric. One interesting experiment but missing in section 4.4 would be how the image features change after fine-tuning for the captioning task. Typos: - synsest-level -> synsets-level", "rating": "4: Ok but not good enough - rejection", "reply_text": "> The paper lacks novelty , just reports some results without proper analysis or insights . > Main weakness of the paper : > - Missing many IC systems citations and comparisons ( see https : //competitions.codalab.org/competitions/3221 # results ) We stress that our evaluations are with respect to the model proposed by Karpathy et al 2015 . Our goal is not to 'beat or break ' systems but to understand the 'whys ' and 'hows ' . > - According to `` SPICE : Semantic Propositional Image Caption Evaluation '' current metrics used in image captioning do n't correlate with human judgement . We are not claiming explicitly that any of the metrics has good correlation with human judgements . As we mentioned before our focus on CIDEr is because a ) the official evaluation script from MSCOCO contains only CIDEr , Meteor , BLEU and ROUGE , b ) CIDEr is a metric that was officially developed for the task of image captioning , c ) CIDEr is the official metric for MSCOCO , d ) papers by Liu et al 2017 , Kilickaya et al.2017 and Vedantam et al , 2015 ( with the human correlation experiments over Flickr8k dataset ) still state the importance of CIDEr as a metric for image captioning . We further note that we observe a similar trend as we found in CIDEr , so all our observations are still valid . > - Most Image Caption papers which use a pre-trained CNN model , do fine-tune the image feature extractor to improve the results ( see Vinyals et al.2016 ) .Therefore correlation of the image features with the captions is weaker that it could be . While it is true that fine-tuning could have been helpful to bump performance , our paper deals with an exploration of representational properties . Vinyals et al.2016 has shown that fine-tuning gives only a minor 1-point improvement for BLEU . This is also using an ensemble of models . We again state that our experiments are about understanding image captioning models . > - To provide a fair comparison , authors should compare their results with other paper results . - Tables 2 and 3 are missing the original baselines . We will add the results from the comparable papers , even though our focus is not comparisons or to show performance improvements over other models . However , we do not understand what the reviewer means by \u201c original baselines \u201d . Could you please clarify ? > The evaluation used in the paper do n't correlate well with human ratings see ( SPICE paper ) , therefore trying to improve them marginally does n't make a difference . Please see answer above regarding metrics . In addition , our focus is not to improve the performance of the system , but to interpret the 'how ' and 'why ' of the system . To this end , we have made significant progress . > - Getting better performance by switching from VGG19 to ResNet152 is expected , however they obtain worse results than Vinyals et al.2016 with inception_v3 . We have not chosen Vinyals et al.2016 since it uses ensembles and other clever engineering tricks . This would make it hard to answer the questions we ask in this paper -- namely , the contribution of image representation . Our results are comparable to those in Karpathy et al , 2015 . We will add this into the table . > - The claim `` The bag of objects model clusters these group the best '' is not supported by any evidence or metric . We believe that the reviewer has misunderstood the sentence . This sentence explains the observations in Figure 1 ( more specifically Figure 1a ) . The figure shows that the bag of objects representation forms better clusters . It shows the cosine distances between each group for the bag of objects representation . We see from the figure that the bag of objects representations clusters these groups best . For example , the average image representation of \u201c dog \u201d correlates with images containing \u201c dog \u201d as a pair like \u201c dog+person \u201d and \u201c dog+toilet \u201d . We are aware that this is true for our given example , however we expect this to extrapolate over other examples in the dataset . > One interesting experiment but missing in section 4.4 would be how the image features change after fine-tuning for the captioning task . We will do it as a future work , even though this does not allow us to answer our questions posed in this paper ."}, "2": {"review_id": "HJNGGmZ0Z-2", "review_text": "This paper is an experimental paper. It investigates what sort of image representations are good for image captioning systems. Overall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings. The main issue of the paper is the lack of novelty. Even for an experimental paper, I would argue that novelty in the experimental methodology is an important fact. Unfortunately, I do not see any novel concept in the experimental setup. I recomend this paper for a workshop presentation. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "> Overall , the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings . > The main issue of the paper is the lack of novelty . Even for an experimental paper , I would argue that novelty in the experimental methodology is an important fact . Our claim is in the novel 'insights ' into end-to-end model of image captioning models . Our empirical evaluations with multiple representations , visualizations and out of domain experiments reveal new and important insights that should be of interest to the community . We kindly ask clarification from the reviewer regarding what is meant by 'novelty in experimental methodology ' ."}}