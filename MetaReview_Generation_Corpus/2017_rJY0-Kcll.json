{"year": "2017", "forum": "rJY0-Kcll", "title": "Optimization as a Model for Few-Shot Learning", "decision": "Accept (Oral)", "meta_review": "The authors propose a meta-learner to address the problem of few-shot learning. The algorithm is interesting, and results are convincing. It's a very timely paper that will receive attention in the community. All three reviewers recommend an accept, with two being particularly enthusiastic. The authors also addressed some issues raised by the more negative reviewer. The AC also agrees that the writing needs a little more work to improve clarity. Overall, this is a clear accept.", "reviews": [{"review_id": "rJY0-Kcll-0", "review_text": "In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8. ----- This manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each \"example\" includes a sequence of batches of \"training\" pairs, followed by a final \"test\" batch. The inputs at each \"step\" include the outputs of a \"base learner\" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final \"test\" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning. Strengths: - It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning. - The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments. - The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work. - As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless. - The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless. Weaknesses: - The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience. - Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question). - The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them. This is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your feedback ! 1. \u201c The writing is at times quite opaque . While it describes very interesting work , I would not call the paper an enjoyable read . It took me multiple passes ( as well as consulting related work ) to understand the general learning problem . The task description in Section 2 ( Page 2 ) is very abstract and uses notation and language that is not common outside of this sub-area . The paper could benefit from a brief concrete example ( based on MNIST is fine ) , perhaps paired with a diagram illustrating a sequence of few-shot learning tasks . This would definitely make it accessible to a wider audience. \u201d Those are valid points . We added a concrete example to Section 2 so that it is clear what task we will be solving in the experimental section . Thanks for this great suggestion ! We also slightly reworded some of the text , in the hope to clarify it . Please do not hesitate to request other changes to the text that you think would improve its clarity . Meta-learning isn \u2019 t an easy topic to discuss with perfect clarity , so any additional suggestion will be appreciated ! Or if you wish to point out specific paragraphs or sentences that you find particularly hard to digest , we \u2019 ll be happy to clarify them ASAP in revisions of the draft . 2 . `` Following up on that note , the precise nature of the N-class , few-shot learning problem here is unclear to me . Specifically , the Mini-ImageNet data set has 100 labels , of which 64/16/20 are used during meta-training/validation/testing . Does this mean that only 64/100 classes are observed through meta-training ? Or does it mean that only 64/100 are observed in each batch , but on average all 100 are observed during meta-training ? If it 's the former , how many outputs does the softmax layer of the ConvNet base learner have during meta-training ? 64 ( only those observed in training ) or 100 ( of which 36 are never observed ) ? Many other details like these are unclear ( see question ) . \u201d Sorry for the confusion . We mean that a specific 64/100 classes are assigned to meta-training so that for all train/test sets in meta-training we can only randomly pick from those classes . This is true for the 16 and 20 classes picked for meta-validation and meta-testing , respectively . We consider 1-shot , 5-class and 5-shot , 5-class classification , where for 5-shot , 5-class classification ( for example ) , to create a training set for each dataset , we pick 5 random classes from the classes assigned to the meta-set and then we pick 5 random examples for each of these classes . These 5 classes are randomly assigned labels 1-5 for this episode . Thus , the softmax layer will have 5 outputs , indicating predictions for each of the 5 classes . 3 . `` The plots in Figure 2 are pretty uninformative in and of themselves , and the discussion section offers very little insight around them. \u201d The plots were mainly to show two points : ( 1 ) Different update rules were used for different layers , which benefits training the learner . ( 2 ) Different update rules were used across episodes , meaning that the meta-learner was adjusting for the data in each episode . Unfortunately , it is hard to derive a concrete learning strategy used by the meta-learner . That said , for completeness and transparency \u2019 s sake , we felt it was important to show something like Figure 2 ."}, {"review_id": "rJY0-Kcll-1", "review_text": "This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN). The paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. Several tricks re-used from (Andrychowicz et al. 2016) such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well motivated. The experiments are convincing. This is a strong paper. My only concerns/questions are the following: 1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough. 2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this? 3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs: - Samy Bengio PhD thesis (1989) is all about this ;-) - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994) - I am convince Schmidhuber has done something, make sure you find it and update related work section. Overall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We appreciate your feedback ! 1 . `` Can it be redundant to use the loss , gradient and parameters as input to the meta-learner ? Did you do ablative studies to make sure simpler combinations are not enough. \u201d We have not yet performed studies to study which inputs are most useful . Though there could be redundancy , by removing some inputs , we expect to gain only in efficiency as the performance with less inputs would be similar or worse . 2 . `` It would be great if other architectural components of the network can be learned in a similar fashion ( number of neurons , type of units , etc . ) . Do you have an opinion about this ? \u201d This would be the ideal as the meta-learner could control the structure of the learner more carefully for the task at hand . Allowing the meta-learner to also control the architecture of the learner would give the meta-learner another way to control overfitting on a few-shot task . Because optimizing those parameters means we would be operating in a discrete space , learning would be a bit complicated and would likely require reinforcement learning or approximations using continuous relaxations . This would definitely be interesting to pursue in future work . 3 . `` The related work section ( mainly focused on meta learning ) is a bit shallow . Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs : \u201d We apologize for missing some previous work . We have added the references you mentioned and added some discussion about older work in meta-learning in the updated version of the submission ."}, {"review_id": "rJY0-Kcll-2", "review_text": "This paper describes a new approach to meta learning by interpreting the SGD update rule as gated recurrent model with trainable parameters. The idea is original and important for research related to transfer learning. The paper has a clear structure, but clarity could be improved at some points. Pros: - An interesting and feasible approach to meta-learning - Competitive results and proper comparison to state-of-the-art - Good recommendations for practical systems Cons: - The analogy would be closer to GRUs than LSTMs - The description of the data separation in meta sets is hard to follow and could be visualized - The experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest - Fig 2 doesn't have much value Remarks: - Small typo in 3.2: \"This means each coordinate has it\" -> its > We plan on releasing the code used in our evaluation experiments. This would certainly be a major plus.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your thoughts ! 1 . `` The analogy would be closer to GRUs than LSTMs '' Yes , there is a similarity to the GRU in that meta-learning LSTM uses only the cell state and does not have an additional hidden state . We have added a note about this in the new draft of the paper . 2 . `` The description of the data separation in meta sets is hard to follow and could be visualized \u201d We have added a figure that gives an example with concrete data to help understand the notation . Any additional feedback is welcome . 3 . `` The experimental evaluation is only partly satisfying , especially the effect of the parameters of i_t and f_t would be of interest \u201d `` Fig 2 does n't have much value \u201d The plots were mainly to show two points : ( 1 ) Different update rules ( with regard to i_t and f_t ) were used for different layers , which benefits training the learner . ( 2 ) Different update rules were used across episodes , meaning that the meta-learner was adjusting for the data in each episode . Unfortunately , it is hard to derive a concrete learning strategy used by the meta-learner . That said , for completeness and transparency \u2019 s sake , we felt it was important to show something like Figure 2 ."}], "0": {"review_id": "rJY0-Kcll-0", "review_text": "In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8. ----- This manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each \"example\" includes a sequence of batches of \"training\" pairs, followed by a final \"test\" batch. The inputs at each \"step\" include the outputs of a \"base learner\" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final \"test\" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning. Strengths: - It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning. - The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments. - The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work. - As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless. - The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless. Weaknesses: - The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience. - Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question). - The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them. This is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your feedback ! 1. \u201c The writing is at times quite opaque . While it describes very interesting work , I would not call the paper an enjoyable read . It took me multiple passes ( as well as consulting related work ) to understand the general learning problem . The task description in Section 2 ( Page 2 ) is very abstract and uses notation and language that is not common outside of this sub-area . The paper could benefit from a brief concrete example ( based on MNIST is fine ) , perhaps paired with a diagram illustrating a sequence of few-shot learning tasks . This would definitely make it accessible to a wider audience. \u201d Those are valid points . We added a concrete example to Section 2 so that it is clear what task we will be solving in the experimental section . Thanks for this great suggestion ! We also slightly reworded some of the text , in the hope to clarify it . Please do not hesitate to request other changes to the text that you think would improve its clarity . Meta-learning isn \u2019 t an easy topic to discuss with perfect clarity , so any additional suggestion will be appreciated ! Or if you wish to point out specific paragraphs or sentences that you find particularly hard to digest , we \u2019 ll be happy to clarify them ASAP in revisions of the draft . 2 . `` Following up on that note , the precise nature of the N-class , few-shot learning problem here is unclear to me . Specifically , the Mini-ImageNet data set has 100 labels , of which 64/16/20 are used during meta-training/validation/testing . Does this mean that only 64/100 classes are observed through meta-training ? Or does it mean that only 64/100 are observed in each batch , but on average all 100 are observed during meta-training ? If it 's the former , how many outputs does the softmax layer of the ConvNet base learner have during meta-training ? 64 ( only those observed in training ) or 100 ( of which 36 are never observed ) ? Many other details like these are unclear ( see question ) . \u201d Sorry for the confusion . We mean that a specific 64/100 classes are assigned to meta-training so that for all train/test sets in meta-training we can only randomly pick from those classes . This is true for the 16 and 20 classes picked for meta-validation and meta-testing , respectively . We consider 1-shot , 5-class and 5-shot , 5-class classification , where for 5-shot , 5-class classification ( for example ) , to create a training set for each dataset , we pick 5 random classes from the classes assigned to the meta-set and then we pick 5 random examples for each of these classes . These 5 classes are randomly assigned labels 1-5 for this episode . Thus , the softmax layer will have 5 outputs , indicating predictions for each of the 5 classes . 3 . `` The plots in Figure 2 are pretty uninformative in and of themselves , and the discussion section offers very little insight around them. \u201d The plots were mainly to show two points : ( 1 ) Different update rules were used for different layers , which benefits training the learner . ( 2 ) Different update rules were used across episodes , meaning that the meta-learner was adjusting for the data in each episode . Unfortunately , it is hard to derive a concrete learning strategy used by the meta-learner . That said , for completeness and transparency \u2019 s sake , we felt it was important to show something like Figure 2 ."}, "1": {"review_id": "rJY0-Kcll-1", "review_text": "This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN). The paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. Several tricks re-used from (Andrychowicz et al. 2016) such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well motivated. The experiments are convincing. This is a strong paper. My only concerns/questions are the following: 1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough. 2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this? 3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs: - Samy Bengio PhD thesis (1989) is all about this ;-) - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994) - I am convince Schmidhuber has done something, make sure you find it and update related work section. Overall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We appreciate your feedback ! 1 . `` Can it be redundant to use the loss , gradient and parameters as input to the meta-learner ? Did you do ablative studies to make sure simpler combinations are not enough. \u201d We have not yet performed studies to study which inputs are most useful . Though there could be redundancy , by removing some inputs , we expect to gain only in efficiency as the performance with less inputs would be similar or worse . 2 . `` It would be great if other architectural components of the network can be learned in a similar fashion ( number of neurons , type of units , etc . ) . Do you have an opinion about this ? \u201d This would be the ideal as the meta-learner could control the structure of the learner more carefully for the task at hand . Allowing the meta-learner to also control the architecture of the learner would give the meta-learner another way to control overfitting on a few-shot task . Because optimizing those parameters means we would be operating in a discrete space , learning would be a bit complicated and would likely require reinforcement learning or approximations using continuous relaxations . This would definitely be interesting to pursue in future work . 3 . `` The related work section ( mainly focused on meta learning ) is a bit shallow . Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs : \u201d We apologize for missing some previous work . We have added the references you mentioned and added some discussion about older work in meta-learning in the updated version of the submission ."}, "2": {"review_id": "rJY0-Kcll-2", "review_text": "This paper describes a new approach to meta learning by interpreting the SGD update rule as gated recurrent model with trainable parameters. The idea is original and important for research related to transfer learning. The paper has a clear structure, but clarity could be improved at some points. Pros: - An interesting and feasible approach to meta-learning - Competitive results and proper comparison to state-of-the-art - Good recommendations for practical systems Cons: - The analogy would be closer to GRUs than LSTMs - The description of the data separation in meta sets is hard to follow and could be visualized - The experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest - Fig 2 doesn't have much value Remarks: - Small typo in 3.2: \"This means each coordinate has it\" -> its > We plan on releasing the code used in our evaluation experiments. This would certainly be a major plus.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your thoughts ! 1 . `` The analogy would be closer to GRUs than LSTMs '' Yes , there is a similarity to the GRU in that meta-learning LSTM uses only the cell state and does not have an additional hidden state . We have added a note about this in the new draft of the paper . 2 . `` The description of the data separation in meta sets is hard to follow and could be visualized \u201d We have added a figure that gives an example with concrete data to help understand the notation . Any additional feedback is welcome . 3 . `` The experimental evaluation is only partly satisfying , especially the effect of the parameters of i_t and f_t would be of interest \u201d `` Fig 2 does n't have much value \u201d The plots were mainly to show two points : ( 1 ) Different update rules ( with regard to i_t and f_t ) were used for different layers , which benefits training the learner . ( 2 ) Different update rules were used across episodes , meaning that the meta-learner was adjusting for the data in each episode . Unfortunately , it is hard to derive a concrete learning strategy used by the meta-learner . That said , for completeness and transparency \u2019 s sake , we felt it was important to show something like Figure 2 ."}}