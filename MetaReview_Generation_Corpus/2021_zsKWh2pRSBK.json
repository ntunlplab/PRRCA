{"year": "2021", "forum": "zsKWh2pRSBK", "title": "Poisoned classifiers are not only backdoored, they are fundamentally broken", "decision": "Reject", "meta_review": "The paper argues that a successful backdoor attack on classifiers is connected with further fundamental security issues. In particular they demonstrate and not only an original backdoor trigger but also other triggers can be inserted by anyone with access to the classifiers. Furthermore, the alternative triggers may appear very different from the original triggers, which confirms the claim in the paper's title that such classifiers are \"fundamentally broken\".\n\nThe paper offers an interesting insight into the features of poisoned classifiers. However, such insight is diminished by the fact that the proposed attack requires a substantial manual interaction. The user must manually analyze the adversarial examples generated for robustified classifiers in order to determine the key parameters of alternative triggers. While manual intervention as such does not undermine the main observation of the paper, this makes an automatic exploitation of this idea hardly feasible and hence decreases the significance of the paper's main result. ", "reviews": [{"review_id": "zsKWh2pRSBK-0", "review_text": "# # Summary # # Suppose that we wish to have a classifier that works well under normal circumstances , but fails when we want it to . One way to do so is via data poisoning attacks : introduce a special datapoint to the training data , such that if the classifier observes it , all of its labels become completely broken . The basic premise of the paper is that poisoned classifiers are broken in a fundamental way - not only are they vulnerable to attacks based on the original trigger image , they are also vulnerable to attacks by adversaries who do not know the original trigger . This is interesting for both the learning and the privacy communities , as it shows that backdoor attacks introduce a host of vulnerabilities far beyond that which was assumed . To test this hypothesis , the authors show that one can design novel triggers on poisoned classifiers , that , in some cases , outperform the attacks based upon the original trigger . Similar avenues do not work as effectively on \u201c clean \u201d classifiers , where no triggers have been introduced . The authors show that their methodology is effective on a variety of datasets ; in fact , the authors show that users are better able to distinguish poisoned classifiers even without the backdoor present . Overall I liked the authors \u2019 approach , and the paper was a pleasure to read . The authors make clear , concise and refutable claims , and proceed to analyze them in a rigorous manner . # # Pros # # 1 . The paper tackles an interesting and well-motivated problem , and shows that a single line of attack is much more worrisome than previously thought . 2.The paper is well written , all claims are easy to follow . 3.I like the fact that the authors incorporated user studies into the evaluation . # # Cons # # 1 . It is not fully clear whether the approach extends beyond theoretical interest and curiosity . Is there a clear and immediate implication for how we train or protect our ML models ? 2.The paper provides a heuristic approach that may or may not be generalized beyond the datasets that were tested . There is no analysis showing why the approach makes sense . # # Questions to the Authors # # 1 . How does this line of work relate to formal privacy notions ? The authors mention a few similar modes of attack , but am I correct in assuming that differentially private training methods are immune to such attacks ? 2.If we use robust training methods , does this issue go away ? # # After Rebuttal # # I thank the authors for their response to my question . I think that the comment regarding differential privacy being ineffective is particularly interesting . It would be nice to actually demonstrate this empirically - construct a DP classifier with a poisoned backdoor , and show that the authors ' method is still effective . My support for the paper remains unchanged .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your detailed review . We are happy to address your concerns . * * Extension and Implication * * A direct implication of our results is that we need to rethink backdoored classifiers . On one hand , for an adversary who aims to poison a classifier , he/she should not expect that the embedded backdoor trigger which he/she holds is secret . Instead , the adversary should be aware of the possibility that it is easy for other people to attack the backdoored classifiers in the same way he/she does . On the other hand , for machine learning practitioners who train and deploy their models , our results suggest that they need to be more careful on sanitizing their data . Otherwise , backdoored classifiers could pose much greater danger than previously thought . * * Generalization beyond datasets tested * * We evaluated our attack on two large scale datasets : ImageNet and TrojAI . Both datasets consist of high resolution images ( 224x224 ) . ImageNet is the de facto dataset for evaluating common vision tasks . TrojAI dataset is specially created to investigate backdoor poisoned classifiers , with a huge amount of pretrained poisoned classifiers . TrojAI dataset contains a wide variety of model architectures ( ResNet , Inception , DenseNet , etc ) and backdoor patterns . Therefore , we believe these two datasets are enough to show the generality of our attack . * * Why our approach makes sense * * In terms of why backdoor patterns exist in adversarial examples , we believe we have discussed it in section 3.1 in detail . The reason is that we are able to construct a robustified poisoned classifier ( with Denoised Smoothing ) . Robust classifiers have perceptual-aligned gradients [ 1 ] , which basically says that adversarial examples of robust classifiers resemble instances from the misclassified class . In the setting of backdoor poisoned classifiers , the backdoored instances B ( x ) can be seen as instances of the target class , then if we generate adversarial examples of backdoored classifiers towards the target class , we expect the adversarial examples to resemble the backdoored instances . From our analysis in section 3.2 ( Figure 2 and Figure 4 ) , the adversarial examples indeed contain patterns that are similar to the original backdoor ( in terms of color ) , which validates our expectation . Given the backdoor patterns observed in the adversarial examples , it is then natural to try to extract these backdoor patterns as new triggers and test their attack success rate , which is basically what we do in the paper . As for why the color patch and cropped patch we constructed have similar or higher success rates than the original trigger , we believe that this is an intrinsic property of backdoor poisoned classifiers . In other words , by introducing a secret backdoor trigger with a backdoor attack method , it also introduces potentially many alternative backdoors . What our approach does in this paper is to show the existence of these alternative backdoors . There could be other approaches to obtaining these alternative backdoors , but we believe our method is both conceptually simple and also effective . * * Relation to privacy * * Our attack can be seen as uncovering a privacy issue in backdoor poisoned classifiers . We show how to extract alternative triggers despite the original backdoor being private . As for differentially private training algorithms , we believe it is addressing a different problem from ours . Differentially private training aims to make the final model leak as few information as possible on the training data . However , in our paper , we are not using the training data or trying to recover the training data . Naively applying differentially private training does not affect our attack as long as the trained classifiers are poisoned with a backdoor . We hypothesize that a variant version of differentially private training that specially aims to conceal the backdoor may be able to avoid our attack . We think this is an interesting open question . * * Robust Training * * A recent work [ 2 ] investigated this setting . [ 2 ] show that adversarially robust models are still vulnerable to backdoor poisoning . Therefore , we believe that the phenomenon we observed in this paper for standard classifiers still exists for adversarial robust ones . In our updated version , we have added and discussed this work in the related work part . Thank you for your review again ! If you have any further questions , we are happy to answer . [ 1 ] Robustness may be at odds with accuracy . Tsipras et al.ICLR 2019 . [ 2 ] Exposing Backdoors in Robust Machine Learning Models . Soremekun et al.ArXiv 2020 ."}, {"review_id": "zsKWh2pRSBK-1", "review_text": "This submission just describes some phenomenons in a strange setting but not proposes any valuable questions . The authors claim that `` anyone with access to the classifier , even without access to any original training data or trigger , can construct several alternative triggers that are as effective or more so at eliciting the target class at test time . '' However , this paper creates the so-called trigger from the poisoned calssifier . Such a choice is unsual in the adversarial machine larening problem . In most cases , this model is not occupied by the adversary . The authors did not provide some convicing reasons to verify the rationality of this setting . Moreover , the proposed method constructs the alternative triggers by first generating adversarial examples for a smoothed version of the poisoned classifier and then extracting colors or cropped portions of adversarial images . But the motivations of building the adversarial robust version for the poisoned model and generating the adversarial examples are not well presented in the paper .", "rating": "2: Strong rejection", "reply_text": "Thank you for your review ! We have several clarifications regarding our work : \u201c A experimental report but not a research paper \u201d We do not agree that our paper is not a research paper . First , we propose to use a new method to analyze poisoned classifiers , which we believe is novel . Second , our analysis demonstrates the existence of multiple alternative triggers for a poisoned classifier , which has not been discovered before . We believe our results are valuable for understanding poisoned classifiers . Overall , we believe our work is novel in this research area on backdoor attacks . We would appreciate it if the reviewer can give specifics about why our paper does not count as a research paper . \u201c This submission just describes some phenomenons in a strange setting but not proposes any valuable questions. \u201d We do not believe that our results are in a strange setting . In this paper , we study backdoor attacks , which is a well studied research area [ 1,2,3,4,5,6 ] . The settings we investigated in this paper are the common settings for backdoored classifiers . We would appreciate it if the reviewer can point out why our observations are in a strange setting . \u201c However , this paper creates the so-called trigger from the poisoned classifier . Such a choice is unusual in the adversarial machine learning problem . In most cases , this model is not occupied by the adversary . The authors did not provide some convincing reasons to verify the rationality of this setting. \u201d We believe our setting is reasonable . In our method , we assume that we can compute the gradients of poisoned classifiers . We think this is similar to the setting of white-box attacks in related research on adversarial attacks and defense , where white-box attacks also assume one can compute the gradients of victim classifiers directly . \u201c But the motivations of building the adversarial robust version for the poisoned model and generating the adversarial examples are not well presented in the paper. \u201d We think the motivations of our approach are well-presented in the paper . In section 3.1 , we have discussed in detail the motivations for building an adversarial robust version of the poisoned classifier . Basically , we want to use the special property of robust classifiers , that they have perceptually-aligned gradients [ 7 ] . This property allows us to inspect the adversarial examples of poisoned classifiers in a meaningful way . [ 1 ] BadNets : Identifying Vulnerabilities in the Machine Learning Model Supply Chain . Gu et al.ArXiv , 2017 . [ 2 ] Clean-Label Backdoor Attacks . Turner et al.https : //openreview.net/forum ? id=HJg6e2CcK7 , 2019 . [ 3 ] Hidden-Trigger Backdoor Attacks . Saha et al.AAAI 2020 . [ 4 ] Clean-Label Backdoor Attacks on Video Recognition Models . Zhao et al.CVPR 2020 . [ 5 ] Trojaning Attack on Neural Networks . Liu et al.NDSS 2018 . [ 6 ] Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning . Chen et al.ArXiv 2017 . [ 7 ] Robustness may be at odds with accuracy . Tsipras et al.ICLR 2019 ."}, {"review_id": "zsKWh2pRSBK-2", "review_text": "Summary : This paper demonstrates that backdoor-poisoned machine learning models can also be vulnerable to alternative triggers . Specifically , adversarial samples that are generated against models robustified with Denoised Smoothing often show backdoor patterns . Therefore , these adversarial samples can be used to create new triggers either by ( 1 ) choosing a representative colour from a backdoor pattern , or ( 2 ) cropping an image that contains a backdoor pattern . Experimental results suggest that alternative triggers can be equally or even more effective than the original trigger . Pros : 1.This paper studies an important question of the vulnerability of backdoor-poisoned machine learning models . 2.Two methods are proposed to generate alternative triggers that can cause the poisoned machine learning model to misbehave . Cons : While it is a novel finding that poisoned machine learning models are not only vulnerable to the initial triggers , I have the following questions on the proposed method : 1 . The biggest concern is that as pointed out in Section 3.3 , both the two proposed strategies for generating alternative triggers require human inspection . Can these triggers be generated automatically ? How easy is it for an algorithm to identify the backdoor pattern , and then to crop a patch image that contains the pattern ? 2.What could be the potential reason why the proposed approaches are effective for generating alternative triggers ? What could be the relation between Denoised Smoothing and the backdoor pattern ? Could there be other ways to create triggers ? There is a lack of discussion on this issue . 3.In terms of parameters , the perturbation size \\epsilon is set to 20 and 60 ( in l2 norm ) in the experiments . What is the impact of \\epsilon on the attack success rate ? Specifically , with a smaller value of \\epsilon , can the attack still achieve such high success rate ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Perturbation size * * From the observations we made from Figure 7 , Figure 11 and Figure 12 , in general , epsilon 60 leads to better attack success rate than epsilon 20 . For perturbation size smaller than 20 , we do not expect it to have better attack success rates because if the perturbation size is small , the backdoor patterns do not occur or they are not obvious as compared to larger perturbation size . This is also the reason why we choose 20 as the minimum perturbation size to experiment with . Thank you for your review again ! Any further questions or suggestions are welcome . [ 1 ] Robustness may be at odds with accuracy . Tsipras et al.ICLR 2019 . [ 2 ] Image Synthesis with a Single ( Robust ) Classifier . Santurkar et al.Neurips 2019 . [ 3 ] Are Perceptually-Aligned Gradients a General Property of Robust Classifiers ? Kaur et al.ArXiv 2019 . [ 4 ] Defense against adversarial attacks using high-level representation guided denoiser . Liao et al.CVPR 2018 . [ 5 ] Mitigating Adversarial Effects Through Randomization . Xie et al.ICLR 2018 . [ 6 ] PixelDefend : Leveraging Generative Models to Understand and Defend against Adversarial Examples . Song et al.ICLR 2018 . [ 7 ] Obfuscated Gradients Give a False Sense of Security : Circumventing Defenses to Adversarial Examples . Athalye et al.ICML 2018 ."}, {"review_id": "zsKWh2pRSBK-3", "review_text": "Summary : This is an interesting study on the analysis of poisoned classifiers and backdoor attacks . The authors showed that with some post-processing analysis on a poisoned classifier , it is possible to construct effective alternative triggers against a backdoor classifier . In particular , after creating several poisoned classifiers , and smoothing them using a Denoised smoothing technique , one can generate adversarial examples . Using these examples , it is possible to extract color or cropped patch as new triggers to break the poisoned classifier . Reason for score : The process of generating alternative triggers and finding effective triggers for each backdoor attacker is mainly manual and needs human intervention . This makes the proposed solution very challenging . Also , the experimental results do not clearly show the generalizability of the model , especially on more difficult backdoor attacks . I think more experiments need to be done to evaluate the consistency of the results and to study the generalizability of this work across various datasets . More detailed comments : - How the adversarial examples of robustified poisoned classifiers look like when we have invisible backdoor attacks ? This is important since the trigger generation is directly related to the backdoor pattern of the generated adversarial examples . Did the authors investigate this matter ? - Does changing the location or appearance of the original trigger affect the backdoor patterns in adversarial examples ? - In the experiment section , it would be better if the authors showed both color patch and cropped patch results on fixed images and shows which technique has a higher success rate . - In the experiment section , the authors only showed two samples to prove that clean classifiers are not easily broken . Please show the overall success rate for clean classifiers on both datasets as well . - It would be better if the authors also showed the results on larger datasets with more number of classes . - I would recommend to open source the code . Minor comments : - In section 3 , please briefly explain how the poisoned classifier is trained using the two triggers ( it was not clear before reading the experiment section ) - For table 1 , please mention the number of samples used to calculate the success rate . The authors mentioned the highest success rate is picked for different triggers . It would be great to see how each trigger performs separately regarding the success rate . - Some references are missing , please add more recent papers . For example : Rethinking the Trigger of Backdoor Attack Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization etc .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Location and appearance of trigger * * We have trained BadNet poisoned classifiers with different trigger locations ( center , upper left , upper right , lower left , lower right ) . We plotted the adversarial examples of robustified poisoned classifiers . We have added the results in Figure 20 in Appendix E in the new version . We find that the trigger location during training does not affect the backdoor patterns in adversarial examples . The appearance of trigger does affect the backdoor patterns . In Figure 3 , we use two backdoors ( Trigger A and Trigger B ) and we can see that the backdoor patterns are different . For example , adversarial examples of robustified poisoned classifier 2 ( Trigger B ) only have special pink regions , which is different from Trigger A . We also have an additional analysis in the paper on the effect of trigger color on backdoor patterns . Results are in Figure 4 , where we find that different triggers lead to different backdoor patterns in adversarial examples . * * Color and Cropped patch on fixed images * * We have updated the experiment section to address this issue . Specifically , for each adversarial example , we show both color patch and cropped patch constructed from this example . We have updated Figure 7 , 11 and 12 accordingly . In general , we find that whether color patch or cropped patch perform better depends on the example considered . Last , we would like to mention that for five of six poisoned classifiers we experimented with , the highest attack success rates in Table 1 are achieved by cropped patches . This may suggest that cropped patches may be more effective overall . We have included these observations and discussions in the paper . * * Results for clean classifiers and more number of classes * * Thanks a lot for the suggestions . We will add more results on clean classifiers and ImageNet classifiers with more number of classes in the paper soon . * * Source code * * We have uploaded our anonymized code in the supplementary material in the new version . * * Minor comments * * We have updated our paper to address the issues the reviewer mentioned . Our modifications are as follows : 1 . In section 3 , we added a sentence describing how we obtained the backdoor poisoned classifiers . 2.In section 4 , we included the number of test samples we used to compute the attack success rate . Specifically , on ImageNet , we used 50 images per source class , which is 50 images for the binary classifier and 200 images for the multi-class classifier . On TrojAI dataset , we used the 500 sample test images provided along with each classifier . In the updated version , we added this information in the first paragraph of section 4 . In terms of how each trigger performs separately regarding the success rate , we have already included all of it in the initial version . In Figure 7 , we showed the four triggers for BadNet poisoned multi-class classifier , which corresponds to one entry in Table 1 . In Figure 11 and Figure 12 in Appendix B , we presented the results for other five entries in Table 1 . We have updated the descriptions in the paper to resolve the confusion and make this point more clear . 3.We have added the references of two papers in the related work part . Thank you for your review again ! Any further questions or suggestions are welcome . [ 1 ] BadNets : Identifying Vulnerabilities in the Machine Learning Model Supply Chain . Gu et al.ArXiv , 2017 . [ 2 ] Clean-Label Backdoor Attacks . Turner et al.https : //openreview.net/forum ? id=HJg6e2CcK7 , 2019 . [ 3 ] Hidden-Trigger Backdoor Attacks . Saha et al.AAAI 2020 . [ 4 ] Clean-Label Backdoor Attacks on Video Recognition Models . Zhao et al.CVPR 2020 . [ 5 ] Trojaning Attack on Neural Networks . Liu et al.NDSS 2018 . [ 6 ] Neural Cleanse : Identifying and Mitigating Backdoor Attacks in Neural Networks . Wang et al.S & P 2019 . [ 7 ] Exposing backdoors in robust machine learning models . Soremekun et al.ArXiv , 2020 . [ 8 ] Practical detection of trojan neural networks : Data-limited and data-free cases . Wang et al.ECCV 2020 . [ 9 ] Tabor : A highly accurate approach to inspecting and restoring trojan backdoors in ai systems . Guo et al.ICDM , 2020 . [ 10 ] Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering . Chen et al.ArXiv , 2018 ."}], "0": {"review_id": "zsKWh2pRSBK-0", "review_text": "# # Summary # # Suppose that we wish to have a classifier that works well under normal circumstances , but fails when we want it to . One way to do so is via data poisoning attacks : introduce a special datapoint to the training data , such that if the classifier observes it , all of its labels become completely broken . The basic premise of the paper is that poisoned classifiers are broken in a fundamental way - not only are they vulnerable to attacks based on the original trigger image , they are also vulnerable to attacks by adversaries who do not know the original trigger . This is interesting for both the learning and the privacy communities , as it shows that backdoor attacks introduce a host of vulnerabilities far beyond that which was assumed . To test this hypothesis , the authors show that one can design novel triggers on poisoned classifiers , that , in some cases , outperform the attacks based upon the original trigger . Similar avenues do not work as effectively on \u201c clean \u201d classifiers , where no triggers have been introduced . The authors show that their methodology is effective on a variety of datasets ; in fact , the authors show that users are better able to distinguish poisoned classifiers even without the backdoor present . Overall I liked the authors \u2019 approach , and the paper was a pleasure to read . The authors make clear , concise and refutable claims , and proceed to analyze them in a rigorous manner . # # Pros # # 1 . The paper tackles an interesting and well-motivated problem , and shows that a single line of attack is much more worrisome than previously thought . 2.The paper is well written , all claims are easy to follow . 3.I like the fact that the authors incorporated user studies into the evaluation . # # Cons # # 1 . It is not fully clear whether the approach extends beyond theoretical interest and curiosity . Is there a clear and immediate implication for how we train or protect our ML models ? 2.The paper provides a heuristic approach that may or may not be generalized beyond the datasets that were tested . There is no analysis showing why the approach makes sense . # # Questions to the Authors # # 1 . How does this line of work relate to formal privacy notions ? The authors mention a few similar modes of attack , but am I correct in assuming that differentially private training methods are immune to such attacks ? 2.If we use robust training methods , does this issue go away ? # # After Rebuttal # # I thank the authors for their response to my question . I think that the comment regarding differential privacy being ineffective is particularly interesting . It would be nice to actually demonstrate this empirically - construct a DP classifier with a poisoned backdoor , and show that the authors ' method is still effective . My support for the paper remains unchanged .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your detailed review . We are happy to address your concerns . * * Extension and Implication * * A direct implication of our results is that we need to rethink backdoored classifiers . On one hand , for an adversary who aims to poison a classifier , he/she should not expect that the embedded backdoor trigger which he/she holds is secret . Instead , the adversary should be aware of the possibility that it is easy for other people to attack the backdoored classifiers in the same way he/she does . On the other hand , for machine learning practitioners who train and deploy their models , our results suggest that they need to be more careful on sanitizing their data . Otherwise , backdoored classifiers could pose much greater danger than previously thought . * * Generalization beyond datasets tested * * We evaluated our attack on two large scale datasets : ImageNet and TrojAI . Both datasets consist of high resolution images ( 224x224 ) . ImageNet is the de facto dataset for evaluating common vision tasks . TrojAI dataset is specially created to investigate backdoor poisoned classifiers , with a huge amount of pretrained poisoned classifiers . TrojAI dataset contains a wide variety of model architectures ( ResNet , Inception , DenseNet , etc ) and backdoor patterns . Therefore , we believe these two datasets are enough to show the generality of our attack . * * Why our approach makes sense * * In terms of why backdoor patterns exist in adversarial examples , we believe we have discussed it in section 3.1 in detail . The reason is that we are able to construct a robustified poisoned classifier ( with Denoised Smoothing ) . Robust classifiers have perceptual-aligned gradients [ 1 ] , which basically says that adversarial examples of robust classifiers resemble instances from the misclassified class . In the setting of backdoor poisoned classifiers , the backdoored instances B ( x ) can be seen as instances of the target class , then if we generate adversarial examples of backdoored classifiers towards the target class , we expect the adversarial examples to resemble the backdoored instances . From our analysis in section 3.2 ( Figure 2 and Figure 4 ) , the adversarial examples indeed contain patterns that are similar to the original backdoor ( in terms of color ) , which validates our expectation . Given the backdoor patterns observed in the adversarial examples , it is then natural to try to extract these backdoor patterns as new triggers and test their attack success rate , which is basically what we do in the paper . As for why the color patch and cropped patch we constructed have similar or higher success rates than the original trigger , we believe that this is an intrinsic property of backdoor poisoned classifiers . In other words , by introducing a secret backdoor trigger with a backdoor attack method , it also introduces potentially many alternative backdoors . What our approach does in this paper is to show the existence of these alternative backdoors . There could be other approaches to obtaining these alternative backdoors , but we believe our method is both conceptually simple and also effective . * * Relation to privacy * * Our attack can be seen as uncovering a privacy issue in backdoor poisoned classifiers . We show how to extract alternative triggers despite the original backdoor being private . As for differentially private training algorithms , we believe it is addressing a different problem from ours . Differentially private training aims to make the final model leak as few information as possible on the training data . However , in our paper , we are not using the training data or trying to recover the training data . Naively applying differentially private training does not affect our attack as long as the trained classifiers are poisoned with a backdoor . We hypothesize that a variant version of differentially private training that specially aims to conceal the backdoor may be able to avoid our attack . We think this is an interesting open question . * * Robust Training * * A recent work [ 2 ] investigated this setting . [ 2 ] show that adversarially robust models are still vulnerable to backdoor poisoning . Therefore , we believe that the phenomenon we observed in this paper for standard classifiers still exists for adversarial robust ones . In our updated version , we have added and discussed this work in the related work part . Thank you for your review again ! If you have any further questions , we are happy to answer . [ 1 ] Robustness may be at odds with accuracy . Tsipras et al.ICLR 2019 . [ 2 ] Exposing Backdoors in Robust Machine Learning Models . Soremekun et al.ArXiv 2020 ."}, "1": {"review_id": "zsKWh2pRSBK-1", "review_text": "This submission just describes some phenomenons in a strange setting but not proposes any valuable questions . The authors claim that `` anyone with access to the classifier , even without access to any original training data or trigger , can construct several alternative triggers that are as effective or more so at eliciting the target class at test time . '' However , this paper creates the so-called trigger from the poisoned calssifier . Such a choice is unsual in the adversarial machine larening problem . In most cases , this model is not occupied by the adversary . The authors did not provide some convicing reasons to verify the rationality of this setting . Moreover , the proposed method constructs the alternative triggers by first generating adversarial examples for a smoothed version of the poisoned classifier and then extracting colors or cropped portions of adversarial images . But the motivations of building the adversarial robust version for the poisoned model and generating the adversarial examples are not well presented in the paper .", "rating": "2: Strong rejection", "reply_text": "Thank you for your review ! We have several clarifications regarding our work : \u201c A experimental report but not a research paper \u201d We do not agree that our paper is not a research paper . First , we propose to use a new method to analyze poisoned classifiers , which we believe is novel . Second , our analysis demonstrates the existence of multiple alternative triggers for a poisoned classifier , which has not been discovered before . We believe our results are valuable for understanding poisoned classifiers . Overall , we believe our work is novel in this research area on backdoor attacks . We would appreciate it if the reviewer can give specifics about why our paper does not count as a research paper . \u201c This submission just describes some phenomenons in a strange setting but not proposes any valuable questions. \u201d We do not believe that our results are in a strange setting . In this paper , we study backdoor attacks , which is a well studied research area [ 1,2,3,4,5,6 ] . The settings we investigated in this paper are the common settings for backdoored classifiers . We would appreciate it if the reviewer can point out why our observations are in a strange setting . \u201c However , this paper creates the so-called trigger from the poisoned classifier . Such a choice is unusual in the adversarial machine learning problem . In most cases , this model is not occupied by the adversary . The authors did not provide some convincing reasons to verify the rationality of this setting. \u201d We believe our setting is reasonable . In our method , we assume that we can compute the gradients of poisoned classifiers . We think this is similar to the setting of white-box attacks in related research on adversarial attacks and defense , where white-box attacks also assume one can compute the gradients of victim classifiers directly . \u201c But the motivations of building the adversarial robust version for the poisoned model and generating the adversarial examples are not well presented in the paper. \u201d We think the motivations of our approach are well-presented in the paper . In section 3.1 , we have discussed in detail the motivations for building an adversarial robust version of the poisoned classifier . Basically , we want to use the special property of robust classifiers , that they have perceptually-aligned gradients [ 7 ] . This property allows us to inspect the adversarial examples of poisoned classifiers in a meaningful way . [ 1 ] BadNets : Identifying Vulnerabilities in the Machine Learning Model Supply Chain . Gu et al.ArXiv , 2017 . [ 2 ] Clean-Label Backdoor Attacks . Turner et al.https : //openreview.net/forum ? id=HJg6e2CcK7 , 2019 . [ 3 ] Hidden-Trigger Backdoor Attacks . Saha et al.AAAI 2020 . [ 4 ] Clean-Label Backdoor Attacks on Video Recognition Models . Zhao et al.CVPR 2020 . [ 5 ] Trojaning Attack on Neural Networks . Liu et al.NDSS 2018 . [ 6 ] Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning . Chen et al.ArXiv 2017 . [ 7 ] Robustness may be at odds with accuracy . Tsipras et al.ICLR 2019 ."}, "2": {"review_id": "zsKWh2pRSBK-2", "review_text": "Summary : This paper demonstrates that backdoor-poisoned machine learning models can also be vulnerable to alternative triggers . Specifically , adversarial samples that are generated against models robustified with Denoised Smoothing often show backdoor patterns . Therefore , these adversarial samples can be used to create new triggers either by ( 1 ) choosing a representative colour from a backdoor pattern , or ( 2 ) cropping an image that contains a backdoor pattern . Experimental results suggest that alternative triggers can be equally or even more effective than the original trigger . Pros : 1.This paper studies an important question of the vulnerability of backdoor-poisoned machine learning models . 2.Two methods are proposed to generate alternative triggers that can cause the poisoned machine learning model to misbehave . Cons : While it is a novel finding that poisoned machine learning models are not only vulnerable to the initial triggers , I have the following questions on the proposed method : 1 . The biggest concern is that as pointed out in Section 3.3 , both the two proposed strategies for generating alternative triggers require human inspection . Can these triggers be generated automatically ? How easy is it for an algorithm to identify the backdoor pattern , and then to crop a patch image that contains the pattern ? 2.What could be the potential reason why the proposed approaches are effective for generating alternative triggers ? What could be the relation between Denoised Smoothing and the backdoor pattern ? Could there be other ways to create triggers ? There is a lack of discussion on this issue . 3.In terms of parameters , the perturbation size \\epsilon is set to 20 and 60 ( in l2 norm ) in the experiments . What is the impact of \\epsilon on the attack success rate ? Specifically , with a smaller value of \\epsilon , can the attack still achieve such high success rate ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Perturbation size * * From the observations we made from Figure 7 , Figure 11 and Figure 12 , in general , epsilon 60 leads to better attack success rate than epsilon 20 . For perturbation size smaller than 20 , we do not expect it to have better attack success rates because if the perturbation size is small , the backdoor patterns do not occur or they are not obvious as compared to larger perturbation size . This is also the reason why we choose 20 as the minimum perturbation size to experiment with . Thank you for your review again ! Any further questions or suggestions are welcome . [ 1 ] Robustness may be at odds with accuracy . Tsipras et al.ICLR 2019 . [ 2 ] Image Synthesis with a Single ( Robust ) Classifier . Santurkar et al.Neurips 2019 . [ 3 ] Are Perceptually-Aligned Gradients a General Property of Robust Classifiers ? Kaur et al.ArXiv 2019 . [ 4 ] Defense against adversarial attacks using high-level representation guided denoiser . Liao et al.CVPR 2018 . [ 5 ] Mitigating Adversarial Effects Through Randomization . Xie et al.ICLR 2018 . [ 6 ] PixelDefend : Leveraging Generative Models to Understand and Defend against Adversarial Examples . Song et al.ICLR 2018 . [ 7 ] Obfuscated Gradients Give a False Sense of Security : Circumventing Defenses to Adversarial Examples . Athalye et al.ICML 2018 ."}, "3": {"review_id": "zsKWh2pRSBK-3", "review_text": "Summary : This is an interesting study on the analysis of poisoned classifiers and backdoor attacks . The authors showed that with some post-processing analysis on a poisoned classifier , it is possible to construct effective alternative triggers against a backdoor classifier . In particular , after creating several poisoned classifiers , and smoothing them using a Denoised smoothing technique , one can generate adversarial examples . Using these examples , it is possible to extract color or cropped patch as new triggers to break the poisoned classifier . Reason for score : The process of generating alternative triggers and finding effective triggers for each backdoor attacker is mainly manual and needs human intervention . This makes the proposed solution very challenging . Also , the experimental results do not clearly show the generalizability of the model , especially on more difficult backdoor attacks . I think more experiments need to be done to evaluate the consistency of the results and to study the generalizability of this work across various datasets . More detailed comments : - How the adversarial examples of robustified poisoned classifiers look like when we have invisible backdoor attacks ? This is important since the trigger generation is directly related to the backdoor pattern of the generated adversarial examples . Did the authors investigate this matter ? - Does changing the location or appearance of the original trigger affect the backdoor patterns in adversarial examples ? - In the experiment section , it would be better if the authors showed both color patch and cropped patch results on fixed images and shows which technique has a higher success rate . - In the experiment section , the authors only showed two samples to prove that clean classifiers are not easily broken . Please show the overall success rate for clean classifiers on both datasets as well . - It would be better if the authors also showed the results on larger datasets with more number of classes . - I would recommend to open source the code . Minor comments : - In section 3 , please briefly explain how the poisoned classifier is trained using the two triggers ( it was not clear before reading the experiment section ) - For table 1 , please mention the number of samples used to calculate the success rate . The authors mentioned the highest success rate is picked for different triggers . It would be great to see how each trigger performs separately regarding the success rate . - Some references are missing , please add more recent papers . For example : Rethinking the Trigger of Backdoor Attack Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization etc .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Location and appearance of trigger * * We have trained BadNet poisoned classifiers with different trigger locations ( center , upper left , upper right , lower left , lower right ) . We plotted the adversarial examples of robustified poisoned classifiers . We have added the results in Figure 20 in Appendix E in the new version . We find that the trigger location during training does not affect the backdoor patterns in adversarial examples . The appearance of trigger does affect the backdoor patterns . In Figure 3 , we use two backdoors ( Trigger A and Trigger B ) and we can see that the backdoor patterns are different . For example , adversarial examples of robustified poisoned classifier 2 ( Trigger B ) only have special pink regions , which is different from Trigger A . We also have an additional analysis in the paper on the effect of trigger color on backdoor patterns . Results are in Figure 4 , where we find that different triggers lead to different backdoor patterns in adversarial examples . * * Color and Cropped patch on fixed images * * We have updated the experiment section to address this issue . Specifically , for each adversarial example , we show both color patch and cropped patch constructed from this example . We have updated Figure 7 , 11 and 12 accordingly . In general , we find that whether color patch or cropped patch perform better depends on the example considered . Last , we would like to mention that for five of six poisoned classifiers we experimented with , the highest attack success rates in Table 1 are achieved by cropped patches . This may suggest that cropped patches may be more effective overall . We have included these observations and discussions in the paper . * * Results for clean classifiers and more number of classes * * Thanks a lot for the suggestions . We will add more results on clean classifiers and ImageNet classifiers with more number of classes in the paper soon . * * Source code * * We have uploaded our anonymized code in the supplementary material in the new version . * * Minor comments * * We have updated our paper to address the issues the reviewer mentioned . Our modifications are as follows : 1 . In section 3 , we added a sentence describing how we obtained the backdoor poisoned classifiers . 2.In section 4 , we included the number of test samples we used to compute the attack success rate . Specifically , on ImageNet , we used 50 images per source class , which is 50 images for the binary classifier and 200 images for the multi-class classifier . On TrojAI dataset , we used the 500 sample test images provided along with each classifier . In the updated version , we added this information in the first paragraph of section 4 . In terms of how each trigger performs separately regarding the success rate , we have already included all of it in the initial version . In Figure 7 , we showed the four triggers for BadNet poisoned multi-class classifier , which corresponds to one entry in Table 1 . In Figure 11 and Figure 12 in Appendix B , we presented the results for other five entries in Table 1 . We have updated the descriptions in the paper to resolve the confusion and make this point more clear . 3.We have added the references of two papers in the related work part . Thank you for your review again ! Any further questions or suggestions are welcome . [ 1 ] BadNets : Identifying Vulnerabilities in the Machine Learning Model Supply Chain . Gu et al.ArXiv , 2017 . [ 2 ] Clean-Label Backdoor Attacks . Turner et al.https : //openreview.net/forum ? id=HJg6e2CcK7 , 2019 . [ 3 ] Hidden-Trigger Backdoor Attacks . Saha et al.AAAI 2020 . [ 4 ] Clean-Label Backdoor Attacks on Video Recognition Models . Zhao et al.CVPR 2020 . [ 5 ] Trojaning Attack on Neural Networks . Liu et al.NDSS 2018 . [ 6 ] Neural Cleanse : Identifying and Mitigating Backdoor Attacks in Neural Networks . Wang et al.S & P 2019 . [ 7 ] Exposing backdoors in robust machine learning models . Soremekun et al.ArXiv , 2020 . [ 8 ] Practical detection of trojan neural networks : Data-limited and data-free cases . Wang et al.ECCV 2020 . [ 9 ] Tabor : A highly accurate approach to inspecting and restoring trojan backdoors in ai systems . Guo et al.ICDM , 2020 . [ 10 ] Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering . Chen et al.ArXiv , 2018 ."}}