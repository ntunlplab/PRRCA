{"year": "2018", "forum": "HJaDJZ-0W", "title": "Block-Sparse Recurrent Neural Networks", "decision": "Reject", "meta_review": "Pros\n-- Interesting approach to induce sparsity, trains faster than alternative approaches\nCons\n-- Fairly complex set of heuristics for pruning weights\n-- Han et al. works well, although the authors claim it takes more time to train, which may not not hold for all training sets and doesn\u2019t seem like a strong enough reason to choose an alternative appraoch\n\nGiven these comments, the AC recommends that the paper be rejected.\n", "reviews": [{"review_id": "HJaDJZ-0W-0", "review_text": "The authors propose a block sparsity pruning approach to compress RNNs. There are several ways. One is using group LASSO to promote sparsity. The other is to prune, but with a very specialized schedule as to the pruning and pruning weight, motivated by the work of Narang et al 2017 for non-group sparsity. The block sizes used in experiments are about 4x4, 8x8, up to 32 x 32. The relative performance degradation ranges between 10% to 96%, depending on the method, severity of compression, and task. The speedup for a matrix multiply is between 1.5x to 4x, and varies according to batch size. This is certainly a well-motivated problem, and the procedure is simple but makes sense. Also, the paper contains a good overview of related work in compression, and is not hiding anything. One paper that is missing is Scardapane, S., Comminiello, D., Hussain, A., & Uncini, A. (2017). Group sparse regularization for deep neural networks. Neurocomputing, 241, 81-89. A major complaint is the lack of comparison of results against other compression techniques. Since it is a block sparsity approach, and the caching / fetching overhead is reduced, one does not need to have competitively superior results to basic pruning approaches, but one should come close on the same types of problems. This is not well presented. Additionally, the speedup should be superior to the sparse methods, which is also not shown (against previously published results, not personally run experiments.) Another issue I find is the general writing, especially for the results section, is not entirely clear. For example, when showing a relative performance degradation of 96%, why is that happening? Is that significant? What should an implementer be aware of in order to avoid that? Finally, a meta issue to address is, if the block size is small (realistically, less than 64 x 64) usually I doubt there will be significant speedup. (4x is not considered terribly significant.) What we need to see is what happens when, say, block size is 256 x 256? What is the performance degradation? If you can give 10x speedup in the feedforward part (testing only) then if you have a 10% degradation in performance that might be acceptable in certain applications. Overall, I believe this is a very promising and well-motivated work, but needs to be \"marinated\" further to be publishable. Actually, with the inclusion of 2-3 tables against known, previously published results, and clearly stated benefits, I would change my review to accept. Minor complaints: The axis labels/numbers in figure 2 are too small. Also, please reread for some grammar / writing issues (last paragraph of 1st page, caption of figure 2, for example) I think also figure 2 should be rerun with more trials. The noise in the curves are not showing a highly interpretable trend. (Though, actually, batch size = 8 being super low might have a significance; can this be explained?) ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the helpful feedback and comments . Thank you for pointing out the missing reference . We have added it to the paper and will update it along with rest of the changes . We agree that we should compare our results to more prior approaches . In the paper , we have compared the block pruning approaches to prior pruning approaches like Narang et . al.In our paper , Group Lasso only experiments are also another baseline . However , this is n't clear in the paper . We are working on adding comparisons to more prior work including Han et al.However , all prior work induces random unstructured sparsity in the network and therefore the speedup is limited . We will add these baselines in our paper and clearly state the benefits of our approach . Group Lasso alone does n't work for our models . Without regularization , the baseline dense model does n't overfit the dataset . Therefore , adding regularization to generate sparse blocks hurts the accuracy of the model . We suspect that group lasso regularization is resulting in underfitting . Therefore , we do n't advocate using Group Lasso alone to introduce block sparsity when training on large datasets . Instead , we recommend using block pruning or group lasso combined with block pruning which produces good accuracy results . It is true that for certain processors ( like a Tensor Processing Unit ) larger block sizes like 256x256 would realize higher speedup . However , for GPUs and processors with SIMD units , smaller block sizes can result in higher speedup with efficient kernels . For example , ARM CPUs support 16x1 vectors and the NVIDIA Volta TensorCores support 16x16 blocks . For 4x4 and 16x16 blocks , we show that the speedup ranges from 4.5x to 1.5x depending on the matrix size and minibatch size using libraries that were not tuned explicitly for block sparse RNNs . Recently , new block sparse kernel libraries have been released ( https : //blog.openai.com/block-sparse-gpu-kernels/ ) which demonstrate that it is possible to achieve speedup close to the theoretical maximum of 1 ( 1-s/100 ) where s is the sparsity of the network . The work shows that smaller blocks of 32x32 even with 80 % sparsity can achieve about 4x speedup . While benchmarking speedups , we run each GEMM 10,000 times . We will run the benchmark with more iterations to ensure that the speedup holds true . Additionally , we will run with more minibatch sizes to confirm the trend . Also , at batch size of 8 , the dense kernels are very fast and therefore speedup observed with sparse kernels is small . For batch sizes larger than 8 , the dense time increases significantly resulting in improved speedup . We will update the paper fixing axis labels and reread for grammar issues . Thanks again for the review and feedback ."}, {"review_id": "HJaDJZ-0W-1", "review_text": "Compressing/pruning of neural networks is required to enable running on devices with limited compute resources. While previous works have proposed to 0 out weights, especially for the case of RNNs, in an unstructured way, the current paper proposes to 0 out weights blocks at a time via thresholding. The process is further aided by utilizing group lasso regularization. The resulting networks are sparse, memory efficient and can be run more efficiently while resulting in minimal loss in accuracy when compared to networks learned with full density. The proposed techniques are evaluated on RNNs for speech recognition and benefits clearly spelled out. Further experiments thresh out how much benefit is provided by thresholding (block sparsity) and regularizing via group lasso. The paper quality seems high, presentation clarity sufficient, the ideas presented (especially the use of group lasso) well thought out and original, and the work seems significant. If I were to nitpick then I would suggest trying out these techniques on RNNs meant for something other than speech recognition (machine translation perhaps?).", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and feedback . We are working on extending this approach to Language Modelling and will hopefully have results on small dataset soon . We will update the paper with the results if they are available before the rebuttal deadline ."}, {"review_id": "HJaDJZ-0W-2", "review_text": "Thanks to the authors for their response. Though the paper presents an interesting approach, but it relies heavily on heuristics (such as those mentioned in the initial review) without a thorough investigation of scenarios in which this might not work. Also, it might be helpful to investigate if there ways to better group the variables for group lasso regularization. The paper therefore needs further improvements towards following a more principled approach. ===================================== This paper presents methods for inducing sparsity in terms of blocks of weights in neural networks which aims to combine benefits of sparsity and faster access based on computing architectures. This is achieved by pruning blocks of weights and group lasso regularization. It is demonstrated empirically that model size can be reduced by upto 10 times with some loss in prediction accuracy. Though the paper presents some interesting evaluations on the impact of block based sparsity in RNNs, some of the shortcomings of the paper seem to be : - The approach taken consists of several heuristics rather than following a more principled approach such as taking the maximum of the weights in a block to represent that block and stop pruning till 40% training has been achieved. Also, the algorithm for computing the pruning threshold is based on a new set of hyper-parameters. It is not clear under what conditions the above settings will (not) work. - For the group lasso method, since there are many ways to group the variable, it is not clear how the variables are grouped. Is there a reasoning behind a particular grouping of the variables. Individually, group lasso does not seem to work, and gives much worse results. The reasons for worse performance could be investigated. It is possible that important weights are in different groups, and group sparsity is forcing some of them to be zero, and hence leading to worse results. It would be insightful to explain the kind of solver used for group lasso regularization, and if that works for large-scale problems. - The results for various kinds of sparsity are unclear in the sense that it is not clear how to set the block size a-priori for having minimum reduction in accuracy and still significant sparsity without having to repeat the process for various choices. Overall, the paper does not seem to present novel ideas, and is mainly focused on evaluating the impact of block-based sparsity instead of weight pruning by Han etal. As mentioned in Section 2, regularization has been used earlier to achieve sparsity in deep networks. In this view the significance over existing work is relatively narrow, and no explicit comparison with existing methods is provided. It is possible that an existing method leads to pruning method such as by Han etal. leads to 8x decrease in model size while retaining the accuracy, while the proposed method leads to 10x decrease while also decreasing the accuracy by 10%. Scenarios like these need to be evaluated to understand the impact of the method proposed in this paper.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their comments and helpful feedback . We present several heuristics related to hyperparameters , and we regard these heuristics as an aid for practitioners , to narrow the range of their hyperparameter search . We agree with the reviewer that it 's not clear under what conditions these heuristics might break down . The requirement for hyperparameter tuning is a drawback of our approach , but other pruning approaches within the field share this drawback . For our group lasso experiments , we pick groups to exactly match the blocks in our block pruning experiments . This is a regular tiling of 2D blocks across an individual 2D weight matrix . Unlike some prior work using group lasso , our reasoning for this grouping is not based on any expectation about grouping or correlation in the input features . Instead , we choose this grouping purely to induce a block sparsity format in the weight matrix that is efficient for hardware implementation . We 'll update the paper to clarify these points . Group Lasso alone does n't work for our models due to underfitting . Without regularization , the baseline dense model does n't overfit the dataset . Therefore , adding regularization to generate sparse blocks hurts the accuracy of the model . Group Lasso could be more effective in inducing block sparsity in a data-limited problem . In section 4.3 , we demonstrate that our block pruning approach works for block sizes up to 32x32 . The section demonstrates a tradeoff between block size and sparsity . The exact choice of block sizes will depend on the underlying hardware . E.g.The best block size for the Nvidia Tesla V100 is 16x16 due to the array data paths used by the TensorCores . We will add some notes to the paper to aid a practitioner in making this choice . Finally , we are working on adding comparisons to previous work including Han et . al.and will update the paper with these results including the pros and cons of each approach . Our work is novel because this is first approach to introduce block sparsity for Recurrent Neural Networks with vanilla RNN and GRU cells . To the best of our knowledge , no prior work has applied Group Lasso Regularization to large RNN models to induce block sparsity . Additionally , our block pruning algorithm does not increase training time unlike some prior work ."}], "0": {"review_id": "HJaDJZ-0W-0", "review_text": "The authors propose a block sparsity pruning approach to compress RNNs. There are several ways. One is using group LASSO to promote sparsity. The other is to prune, but with a very specialized schedule as to the pruning and pruning weight, motivated by the work of Narang et al 2017 for non-group sparsity. The block sizes used in experiments are about 4x4, 8x8, up to 32 x 32. The relative performance degradation ranges between 10% to 96%, depending on the method, severity of compression, and task. The speedup for a matrix multiply is between 1.5x to 4x, and varies according to batch size. This is certainly a well-motivated problem, and the procedure is simple but makes sense. Also, the paper contains a good overview of related work in compression, and is not hiding anything. One paper that is missing is Scardapane, S., Comminiello, D., Hussain, A., & Uncini, A. (2017). Group sparse regularization for deep neural networks. Neurocomputing, 241, 81-89. A major complaint is the lack of comparison of results against other compression techniques. Since it is a block sparsity approach, and the caching / fetching overhead is reduced, one does not need to have competitively superior results to basic pruning approaches, but one should come close on the same types of problems. This is not well presented. Additionally, the speedup should be superior to the sparse methods, which is also not shown (against previously published results, not personally run experiments.) Another issue I find is the general writing, especially for the results section, is not entirely clear. For example, when showing a relative performance degradation of 96%, why is that happening? Is that significant? What should an implementer be aware of in order to avoid that? Finally, a meta issue to address is, if the block size is small (realistically, less than 64 x 64) usually I doubt there will be significant speedup. (4x is not considered terribly significant.) What we need to see is what happens when, say, block size is 256 x 256? What is the performance degradation? If you can give 10x speedup in the feedforward part (testing only) then if you have a 10% degradation in performance that might be acceptable in certain applications. Overall, I believe this is a very promising and well-motivated work, but needs to be \"marinated\" further to be publishable. Actually, with the inclusion of 2-3 tables against known, previously published results, and clearly stated benefits, I would change my review to accept. Minor complaints: The axis labels/numbers in figure 2 are too small. Also, please reread for some grammar / writing issues (last paragraph of 1st page, caption of figure 2, for example) I think also figure 2 should be rerun with more trials. The noise in the curves are not showing a highly interpretable trend. (Though, actually, batch size = 8 being super low might have a significance; can this be explained?) ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the helpful feedback and comments . Thank you for pointing out the missing reference . We have added it to the paper and will update it along with rest of the changes . We agree that we should compare our results to more prior approaches . In the paper , we have compared the block pruning approaches to prior pruning approaches like Narang et . al.In our paper , Group Lasso only experiments are also another baseline . However , this is n't clear in the paper . We are working on adding comparisons to more prior work including Han et al.However , all prior work induces random unstructured sparsity in the network and therefore the speedup is limited . We will add these baselines in our paper and clearly state the benefits of our approach . Group Lasso alone does n't work for our models . Without regularization , the baseline dense model does n't overfit the dataset . Therefore , adding regularization to generate sparse blocks hurts the accuracy of the model . We suspect that group lasso regularization is resulting in underfitting . Therefore , we do n't advocate using Group Lasso alone to introduce block sparsity when training on large datasets . Instead , we recommend using block pruning or group lasso combined with block pruning which produces good accuracy results . It is true that for certain processors ( like a Tensor Processing Unit ) larger block sizes like 256x256 would realize higher speedup . However , for GPUs and processors with SIMD units , smaller block sizes can result in higher speedup with efficient kernels . For example , ARM CPUs support 16x1 vectors and the NVIDIA Volta TensorCores support 16x16 blocks . For 4x4 and 16x16 blocks , we show that the speedup ranges from 4.5x to 1.5x depending on the matrix size and minibatch size using libraries that were not tuned explicitly for block sparse RNNs . Recently , new block sparse kernel libraries have been released ( https : //blog.openai.com/block-sparse-gpu-kernels/ ) which demonstrate that it is possible to achieve speedup close to the theoretical maximum of 1 ( 1-s/100 ) where s is the sparsity of the network . The work shows that smaller blocks of 32x32 even with 80 % sparsity can achieve about 4x speedup . While benchmarking speedups , we run each GEMM 10,000 times . We will run the benchmark with more iterations to ensure that the speedup holds true . Additionally , we will run with more minibatch sizes to confirm the trend . Also , at batch size of 8 , the dense kernels are very fast and therefore speedup observed with sparse kernels is small . For batch sizes larger than 8 , the dense time increases significantly resulting in improved speedup . We will update the paper fixing axis labels and reread for grammar issues . Thanks again for the review and feedback ."}, "1": {"review_id": "HJaDJZ-0W-1", "review_text": "Compressing/pruning of neural networks is required to enable running on devices with limited compute resources. While previous works have proposed to 0 out weights, especially for the case of RNNs, in an unstructured way, the current paper proposes to 0 out weights blocks at a time via thresholding. The process is further aided by utilizing group lasso regularization. The resulting networks are sparse, memory efficient and can be run more efficiently while resulting in minimal loss in accuracy when compared to networks learned with full density. The proposed techniques are evaluated on RNNs for speech recognition and benefits clearly spelled out. Further experiments thresh out how much benefit is provided by thresholding (block sparsity) and regularizing via group lasso. The paper quality seems high, presentation clarity sufficient, the ideas presented (especially the use of group lasso) well thought out and original, and the work seems significant. If I were to nitpick then I would suggest trying out these techniques on RNNs meant for something other than speech recognition (machine translation perhaps?).", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and feedback . We are working on extending this approach to Language Modelling and will hopefully have results on small dataset soon . We will update the paper with the results if they are available before the rebuttal deadline ."}, "2": {"review_id": "HJaDJZ-0W-2", "review_text": "Thanks to the authors for their response. Though the paper presents an interesting approach, but it relies heavily on heuristics (such as those mentioned in the initial review) without a thorough investigation of scenarios in which this might not work. Also, it might be helpful to investigate if there ways to better group the variables for group lasso regularization. The paper therefore needs further improvements towards following a more principled approach. ===================================== This paper presents methods for inducing sparsity in terms of blocks of weights in neural networks which aims to combine benefits of sparsity and faster access based on computing architectures. This is achieved by pruning blocks of weights and group lasso regularization. It is demonstrated empirically that model size can be reduced by upto 10 times with some loss in prediction accuracy. Though the paper presents some interesting evaluations on the impact of block based sparsity in RNNs, some of the shortcomings of the paper seem to be : - The approach taken consists of several heuristics rather than following a more principled approach such as taking the maximum of the weights in a block to represent that block and stop pruning till 40% training has been achieved. Also, the algorithm for computing the pruning threshold is based on a new set of hyper-parameters. It is not clear under what conditions the above settings will (not) work. - For the group lasso method, since there are many ways to group the variable, it is not clear how the variables are grouped. Is there a reasoning behind a particular grouping of the variables. Individually, group lasso does not seem to work, and gives much worse results. The reasons for worse performance could be investigated. It is possible that important weights are in different groups, and group sparsity is forcing some of them to be zero, and hence leading to worse results. It would be insightful to explain the kind of solver used for group lasso regularization, and if that works for large-scale problems. - The results for various kinds of sparsity are unclear in the sense that it is not clear how to set the block size a-priori for having minimum reduction in accuracy and still significant sparsity without having to repeat the process for various choices. Overall, the paper does not seem to present novel ideas, and is mainly focused on evaluating the impact of block-based sparsity instead of weight pruning by Han etal. As mentioned in Section 2, regularization has been used earlier to achieve sparsity in deep networks. In this view the significance over existing work is relatively narrow, and no explicit comparison with existing methods is provided. It is possible that an existing method leads to pruning method such as by Han etal. leads to 8x decrease in model size while retaining the accuracy, while the proposed method leads to 10x decrease while also decreasing the accuracy by 10%. Scenarios like these need to be evaluated to understand the impact of the method proposed in this paper.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their comments and helpful feedback . We present several heuristics related to hyperparameters , and we regard these heuristics as an aid for practitioners , to narrow the range of their hyperparameter search . We agree with the reviewer that it 's not clear under what conditions these heuristics might break down . The requirement for hyperparameter tuning is a drawback of our approach , but other pruning approaches within the field share this drawback . For our group lasso experiments , we pick groups to exactly match the blocks in our block pruning experiments . This is a regular tiling of 2D blocks across an individual 2D weight matrix . Unlike some prior work using group lasso , our reasoning for this grouping is not based on any expectation about grouping or correlation in the input features . Instead , we choose this grouping purely to induce a block sparsity format in the weight matrix that is efficient for hardware implementation . We 'll update the paper to clarify these points . Group Lasso alone does n't work for our models due to underfitting . Without regularization , the baseline dense model does n't overfit the dataset . Therefore , adding regularization to generate sparse blocks hurts the accuracy of the model . Group Lasso could be more effective in inducing block sparsity in a data-limited problem . In section 4.3 , we demonstrate that our block pruning approach works for block sizes up to 32x32 . The section demonstrates a tradeoff between block size and sparsity . The exact choice of block sizes will depend on the underlying hardware . E.g.The best block size for the Nvidia Tesla V100 is 16x16 due to the array data paths used by the TensorCores . We will add some notes to the paper to aid a practitioner in making this choice . Finally , we are working on adding comparisons to previous work including Han et . al.and will update the paper with these results including the pros and cons of each approach . Our work is novel because this is first approach to introduce block sparsity for Recurrent Neural Networks with vanilla RNN and GRU cells . To the best of our knowledge , no prior work has applied Group Lasso Regularization to large RNN models to induce block sparsity . Additionally , our block pruning algorithm does not increase training time unlike some prior work ."}}