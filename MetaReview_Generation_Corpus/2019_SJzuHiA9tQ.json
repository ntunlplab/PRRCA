{"year": "2019", "forum": "SJzuHiA9tQ", "title": "Generative Adversarial Network Training is a Continual Learning Problem", "decision": "Reject", "meta_review": "This paper studies training of the generative adversarial networks (GANs), specifically the discriminator, as a continual learning problem, where the discriminator does not forget previously generated samples. This model can be potentially used for improving GANs training and for generating synthetic datasets for evaluating continual learning methods. All the reviewers and AC agree that showing how continual learning techniques applied to the discriminator can alleviate mode collapse in GANs training is an important direction to study.\n\nThere is reviewer disagreement on this paper. AC can confirm that all three reviewers have read the author responses and have contributed to the final discussion.\n\nWhile acknowledging that continual learning setting is potentially useful, the reviewers have raised several important concerns: (1) low technical novelty in light of EWC++ and online EWC methods (R1 and R3) -- methodological and empirical comparison to these baselines is required to assess the difference and benefits of the proposed approach; the authors response to these concerns (and also R2\u2019s comments in the discussion) were insufficient to assess the scope of the contribution. (2) More diverse/convincing empirical findings would strengthen the evaluation (e.g. assessing whether or not generator could help to overcome forgetting; showing that memory replay strategy by storing sufficient fake examples from previously generated samples cannot prevent mode collapse in GANs training \u2013 see the R3\u2019s comment; showing the benefits of the generated samples for evaluating continual learning methods). (3) R1 left unconvinced that GAN training can be improved via continual learning training, as the relation between the proposed view and the minimax optimization difficulties in GANs is not addressed \u2013 see R1\u2019s comment about this. The authors briefly discussed in their response to the review that the proposed approach is orthogonal to these works. However, a better (possibly theoretical) analysis of GANs training and continual learning would indeed help to evaluate the scope of the contribution of this work.\n\nRegarding the available datasets that exhibit a coherent time evolution -- see the Continuous Manifold Based Adaptation for Evolving Visual Domains by Hoffman et al, CVPR 2014. \n\nAmong (1)-(3):  (2) and (3) did not have a substantial impact on the decision, but would be helpful to address in a subsequent revision. However, (1) makes it very difficult to assess the benefits of the proposed approach, and was viewed by AC as a critical issue. \n\nAC suggests that in this current state the paper can be considered for a workshop and recommend to prepare a major revision before resubmitting it for the second round of reviews. \n\n\n", "reviews": [{"review_id": "SJzuHiA9tQ-0", "review_text": "The authors argue that catastrophic forgetting may cause mode collapse and oscillation, and propose a novel plug-and-play regularizer that can be applied to a variety of GANs' training process to counter catastrophic forgetting of the discriminator. The regularizer is a clever adaption of EWC and IS into the context of GAN training. With the authors' formulation, this regularizer will account for the discriminator's parameter from all previous \"tasks\" (snapshots taken at certain iterations) with extra memory budget of only one set of parameters, while assigning higher regularization strengths to parameters learned from recent tasks. Experiments demonstrate such regularizer improves GAN models including DCGAN, SN-DCGAN, WGAN-GP on image generation tasks and textGAN on text generation tasks. Pros: The paper is well-written. The formulation of online memory and controlled forgetting are clever, giving rise to the adaption of EWC and IS as a practical regularizer to overcome the problem of catastrophic forgetting in GANs. The experiments also demonstrate the regularizer is superior than historical averaging and SN on the synthetic dataset, and it is able to improve multiple GAN models in both image and text generation tasks. Cons/Suggestions: 1. Although I can see the method is working, the empirical evidence to support \"mode oscillation\" is not strong enough for me. I think in order for continual learning to make perfect sense, mode oscillation should be an obvious issue for GANs; otherwise, we probably don't need remembering the history, as the generator is probably evolving towards the right direction even in the vanilla approach. Still, since there have been several papers showing history is important, it should be helpful in some sense. In Figure 1, I cannot tell whether in (d), the generator returned to the previous space (probably refers to (a)). Even the centers of mass of (a) and (d) look different for me. Figure 2 (left) only shows the distribution of generated data is changing as the training proceeds in vanilla GANs, since few of them (some shallow blue lines) have low peaks in previous datasets. If the mode oscillates and the generator returns to previous state, there should at least be another peak along the line, which is missing in curves on later datasets (darker blue ones). (I guess I have understood this figure correctly, but Figure 2 seems horizontally flipped to me. Since you are testing on previous fake datasets, and the accuracy should drop on previous datasets; however, the accuracy drops on later datasets in the figure.) 2. I doubt the authors may not have tried enough sets of hyper parameters for baseline models. In table 1, the variance of GAN, GAN + l2 weight and GAN + SN are significantly higher than the others. I don't think with l2 weight regularizer, the model will be much more unstable than the authors' approach. 3. The authors didn't give the results of their regularizer with LeakGAN on text generation. Currently their model has lower test BLEU than LeakGAN, which indicates lower fluency, but its self BLEU is lower than LeakGAN, which indicates higher diversity. It would be much better if the proposed method can surpass LeakGAN on both metrics. 4. Using inception score on mixture of eight Gaussians may not make much sense, if they are using the ImageNet pre-trained model, since such a model is not trained to fit this distribution. Still, the author has reported symmetric KL. 5. The authors did not specify their inception score on real Celeb-A and CIFAR10 images. Overall, I tend to accept this paper for its contribution on methods. It would be even better if my concerns could be addressed. Edit: after seeing the review of Reviewer 3, I find the proposed method seems to be the same as Online EWC and I have downgraded the rating. The authors should address these concerns.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review . Responses to your comments : 1 . Apologies if the figures were unclear . With regard to Figure 1 , the oscillations in an 8 Gaussians GAN are especially obvious if each time step ( each corresponding to a plot like Figure 1 a , b , c , d ) are compiled into frames of a video . In such videos , the generator of a vanilla GAN can be seen oscillating through the space in 2D indefinitely , returning to various previous locations repeatedly . For Figure 1 , we hoped to demonstrate this compactly with several proximal frames , but given that the space is 2D , the generator at d ) doesn \u2019 t necessarily return to exactly the same location as a ) while oscillating , and it may also visit other modes before returning . Regardless of exact positioning , we can see that the generator is once again producing synthesized samples in d ) that it had been disincentivized from producing after a ) . In Figure 2 , the x-axis represents the dataset being trained on , while each line represents performance on a particular dataset as the model is fine-tuned on additional datasets . For example , the darkest blue line represents the model \u2019 s test accuracy on D_1 , which starts off high when the model is first trained specifically on D_1 \u2019 s training set , but it drops precipitously once the model is finetuned on D_2 . 2.We report the best performances we found from hyperparameter search on our baseline models . The variances for many of the baseline models is higher due to their propensity to fail to converge more often than our methods . This results in at least a few significantly lower ICP/higher Sym-KL values than a converged model , resulting in corresponding higher variances . 3.We agree it would be better if we surpassed LeakGAN on both metrics . However , we would like to emphasize that the proposed method did improve its baseline model textGAN on both metrics , which demonstrates the main point of the utility of continual learning augmentation . 4.Reporting ICP with ImageNet pre-trained features indeed makes little sense . Our ICP is more accurately Inception Score-inspired : we pre-trained a classifier on 8 Gaussians data sample and used that to calculate ICP , much in the way an ImageNet classifier is used for the standard ICP . 5. \u201c Specify inception score on real Celeb-A and CIFAR10 images. \u201d Inception score on real CIFAR10 images is 11.23 . Note that Inception score is computed on datasets with labels , which the Celeb-A dataset doesn \u2019 t have . Further , we reported FID score on both datasets , which past works have found to be a better metric than Inception score [ * ] . [ * ] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium , NIPS 2017 Please see our response to Reviewer 3 for concerns about Online EWC ."}, {"review_id": "SJzuHiA9tQ-1", "review_text": "Summary: This paper proposes the use of GANs as a realistic benchmark for continual learning, and shows how continual learning techniques applied to the discriminator can alleviate mode collapse. Existing continual learning approaches for discrete task structure (EWC and IS) are adapted to the continually shifting domain of GANs, and evaluated on a toy mixture of Gaussians, CelebA and CIFAR-10 image generation as well as textGANs. This is a clearly written paper that nicely addresses some of the challenges of bridging the toy problems of continual learning with a real world problem of GAN training. The experiments and ablations are thorough, but the empirical gains in terms of improving GAN metrics are relatively minor. It's also not obvious that GAN training is really a continual learning problem, as every time the generator distribution shifts, the discriminator has to shift as well. Thus progress on stabilizing and improving GANs might not transfer back to domains that truly represent continual learning where the goal is to build a single network that perform well at all points in time. In terms of more realistic benchmarks for continual learning, I believe a controlled synthetic dataset would be more practical than the sequence of GAN checkpoints proposed here. Strengths: + Clearly written, with good background discussion of continual learning approaches and challenges. + Interesting adaptation of EWC and IS to the continual setting with task rates, online memory (sum of quadratics is quadratic), and controlled forgetting with a time decay. + Thorough experiments and ablations on toy tasks, CelebA and CIFAR-10, and textGANs. Nicely includes error bars and compares computation time for each approach. Weaknesses: - The paper could benefit from more discussion on the goals of continual learning, and what is wrong with existing toy benchmarks. Why not come up with a tractable toy problem that addresses these difficulties directly? - I remain unconvinced that GANs as a good benchmark for continual learning. For example, it has been argued that many of the problems with GANs arise from dynamics of minimax optimization difficulties, and there are many recent approaches that were not compared to that focus on this optimization aspect of GAN training (Metz et al., Roth et al., Mescheder et al.). How would you relate these theoretical ideas to continual learning? - Most the experimental improvements are incremental. How did you choose or tune hyperparameters of your approach? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review . Responses to your comments : - GAN as a continual learning problem : Our argument comes from the rationale that the ideal discriminator should be able to identify any fake sample , produced from any past generator ( G_ { \\theta_1 } , \u2026 , G_ { \\theta_t } ) from any point in time , not just the current instantiation of the generator ( G_ { \\theta_t } ) . If this isn \u2019 t the case , because of catastrophic forgetting and the way training samples are presented ( only from G_ { \\theta_t } ) , the discriminator will forget past generator samples in favor of current ones . As a result , the generator has the option of simply shifting to one of the previous instantiations ( g_ { \\theta_ < t } ) , as can be seen Figure 1 . This can result in oscillations in the training process , particularly in mode-collapsed generators . Please see https : //youtu.be/91a2gPWngo8 as an example ( we 've posted this video with an anonymized account , to respect ICLR 's double submission policy ) . - Continual learning benchmarks : The primary concern with current continual learning benchmarks is that many choose to focus on learning a sequence of classification tasks , often with each task bearing little resemblance to the others . [ 1 ] points out that this is actually an \u201c unrealistic best case scenario , \u201d because the forgetting effect is actually minimized , due to the completely unrelated tasks . Our opinion is that in real world applications , continual learning is more likely to be of utility when used for a model encountering a slowly evolving environment , as is likely to happen in the real world . This kind of setting isn \u2019 t commonly considered in the continual learning literature , partly because most of the common machine learning datasets are static , collected from a distribution at a single snapshot in time . We are unaware of any common datasets that exhibit a coherent time evolution , which if they existed could be used to test continual learning in an evolving setting . Because of GAN \u2019 s evolving nature and impressive generative capabilities , we \u2019 re proposing GANs as a way to synthetically create such datasets . - Relation to GAN optimization literature : We are aware of the many works on GAN optimization , but we consider our perspective to be orthogonal to these works . Even with these various techniques to stabilize the minimax , the discriminator is still presented with the task of learning a changing distribution , and as such , GAN training remains a continual learning problem . - Hyperparameters : We used the 8 Gaussians toy setting to get a feel for how the various hyperparameters interacted with each other . For the real datasets , we used \\alpha=100 , \\gamma=0.99 , and \\lambda=1e-3 , keeping the rest of the hyperparameters the same as the baselines ( pre-augmentation ) that we compared against . [ 1 ] Sebastian Farquhar and Yarin Gal . Towards Robust Evaluations of Continual Learning . arXiv preprint , 2018 ."}, {"review_id": "SJzuHiA9tQ-2", "review_text": "This paper connects continual learning and GAN training together, and propose to use standard continual learning schemes (EWC etc) to improve GAN training. Continual learning for GANs is certainly an important problem to look at. Even though I like the problem, I'm not convinced with the solution provided by the paper. The paper in it's current form, in terms of technical contributions and experimental analyses presented to support the hypotheses it started with, is not good enough to be accepted in ICLR. My comments: 1) Catastrophic forgetting of discriminator: Interesting view about mode collapse. I have following concerns: (a) I like the view in which the training is shown as a sequential process. I wonder if we could solve the issue of catastrophic forgetting of discriminator by storing sufficient fake examples from previously generated samples from the generator. Storing previous generations has already been explored, however, just to prove the point that forgetting is an issue, it would be interesting to store enough samples for the mixture of Gaussian setting and analyse. (b) Why not thinking of catastrophic forgetting of generator? What if we constrain the generator to not-to-forget about previously generated samples by Fisher or something similar? In this case, every new task in the training process will have sufficient fake samples from all the modes. 2) Lack of technical contributions: The approach, in which EWC or IS is being used to regulate the discriminator's parameters, seems to be straightforward. The issue of multiple parameters is being resolved by storing one Fisher/Score vector using moving average type scheme. This, to me, is almost same as Online EWC or EWC++. Both these papers have already addressed this issue and discussed them in great detail. How is this approach different? 3) Not sure what exactly Fig 2 is conveying as D_1^{gen}, \\cdots, D_T^{gen} should almost be the same, so training on one and testing forgetting on other doesn't actually say much. I think we can't conclude anything from this figure, at least using MNIST experiments. Minor: 4) I'm assuming that you call your method EWC/IS GAN (I think it wasn't explicitly mentioned in the paper). Why don't you call Seff et al. 2018 (they used EWC with GAN for the first time) work EWC-GAN? I personally feel that it's important to give acronyms so that it doesn't undermine previous works. Just to clarify, I'm not advocating the work by Seff et al.. 5) Please provide citations for mode collapse Online EWC: Progress & compress: A scalable framework for continual learnin, ICML 2018. EWC++: Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence, ECCV 2018. ", "rating": "3: Clear rejection", "reply_text": "Thank you for the review . Responses to your comments : 1a . Storing previously generated samples would indeed be an option to combat forgetting , and as we discuss in our Related Works , this has indeed been considered before [ 1 ] . However , such an approach necessitates access to previous generations . One could at each time step t either save i ) a very large ( to prevent memorization ) number of samples , or ii ) the generator \u2019 s weights . Option i ) could become very space prohibitive quickly , and ii ) even more so . Additionally , injecting old samples limits the number of current samples that can be considered . In our method , we avoid both of these by maintaining memory in our parameters , without requiring a buffer . 1b.We indeed considered applying continual learning to the generator , but ultimately decided not to . While it is clear that a discriminator capable of distinguishing any fake from any point in time is desirable , a generator capable of generating any previous fake is not as clearly of use . Additionally , if the discriminator indeed truly retains the ability to recognize any fake from any point in time , then there is no reason for the generator to remember how to fool previous discriminators . 2.We approached the problem from the starting point of the characteristics of GAN training . As we outlined in our submission , unlike typical continual learning problems , where there are explicit disjoint tasks ( e.g.permutations of MNIST , different Atari games ) , the definition of a \u201c task \u201d in GAN is less clear cut ; this results in a series of challenges that we outline in the bullets of Section 3.2 , which we subsequently solve by deriving an in-place update rule . While our ultimate solution may resemble Online EWC and EWC++ , we arrived at it independently , to fit the needs of the problem at hand . More specific differences with prior works include our definition of a task rate \\alpha , as well as \u201c completing the square \u201d of quadratics to derive our update rule , as opposed to re-centering the posterior on the latest solution . 3.Apologies if Figure 2 is not immediately clear . D_1^ { gen } , \\cdots , D_T^ { gen } may appear similar to the human eye , but each D_ { t } ^ { gen } is in fact the result of the generator evolving to make the classifier with weights from time t-1 as poor as possible . Figure 2 illustrates that when the discriminator learns a new generator distribution , catastrophic forgetting results in the severe degradation of the discriminator \u2019 s ability to recognize previous generator distributions . This process occurs repeatedly during the training process of a GAN . See our reply to reviewer 1 for additional comments . 4.We started referring to our model as EWC-GAN and IS-GAN early during development ( before becoming aware of Seff et.al.2018 ) .The name stuck during our discussions , and since our work isn \u2019 t directly comparable with Seff et . al 2018 , we didn \u2019 t notice the name collision . Would GAN+EWC/GAN+IS be sufficiently different , or would you still consider that too close ? 5.Noted.They \u2019 ll be in our next draft . Among many papers discussing \u201c Mode collapse \u201d , we would like mention a few : Mode regularized generative adversarial networks . ICLR 2017 . Improved techniques for training GANs . NIPS 2016 VEEGAN : Reducing Mode Collapse in GANs using Implicit Variational Learning , NIPS 2017 [ 1 ] Learning from Simulated and Unsupervised Images through Adversarial Training Ashish Shrivastava , Tomas Pfister , Oncel Tuzel , Josh Susskind , Wenda Wang , Russ Webb"}], "0": {"review_id": "SJzuHiA9tQ-0", "review_text": "The authors argue that catastrophic forgetting may cause mode collapse and oscillation, and propose a novel plug-and-play regularizer that can be applied to a variety of GANs' training process to counter catastrophic forgetting of the discriminator. The regularizer is a clever adaption of EWC and IS into the context of GAN training. With the authors' formulation, this regularizer will account for the discriminator's parameter from all previous \"tasks\" (snapshots taken at certain iterations) with extra memory budget of only one set of parameters, while assigning higher regularization strengths to parameters learned from recent tasks. Experiments demonstrate such regularizer improves GAN models including DCGAN, SN-DCGAN, WGAN-GP on image generation tasks and textGAN on text generation tasks. Pros: The paper is well-written. The formulation of online memory and controlled forgetting are clever, giving rise to the adaption of EWC and IS as a practical regularizer to overcome the problem of catastrophic forgetting in GANs. The experiments also demonstrate the regularizer is superior than historical averaging and SN on the synthetic dataset, and it is able to improve multiple GAN models in both image and text generation tasks. Cons/Suggestions: 1. Although I can see the method is working, the empirical evidence to support \"mode oscillation\" is not strong enough for me. I think in order for continual learning to make perfect sense, mode oscillation should be an obvious issue for GANs; otherwise, we probably don't need remembering the history, as the generator is probably evolving towards the right direction even in the vanilla approach. Still, since there have been several papers showing history is important, it should be helpful in some sense. In Figure 1, I cannot tell whether in (d), the generator returned to the previous space (probably refers to (a)). Even the centers of mass of (a) and (d) look different for me. Figure 2 (left) only shows the distribution of generated data is changing as the training proceeds in vanilla GANs, since few of them (some shallow blue lines) have low peaks in previous datasets. If the mode oscillates and the generator returns to previous state, there should at least be another peak along the line, which is missing in curves on later datasets (darker blue ones). (I guess I have understood this figure correctly, but Figure 2 seems horizontally flipped to me. Since you are testing on previous fake datasets, and the accuracy should drop on previous datasets; however, the accuracy drops on later datasets in the figure.) 2. I doubt the authors may not have tried enough sets of hyper parameters for baseline models. In table 1, the variance of GAN, GAN + l2 weight and GAN + SN are significantly higher than the others. I don't think with l2 weight regularizer, the model will be much more unstable than the authors' approach. 3. The authors didn't give the results of their regularizer with LeakGAN on text generation. Currently their model has lower test BLEU than LeakGAN, which indicates lower fluency, but its self BLEU is lower than LeakGAN, which indicates higher diversity. It would be much better if the proposed method can surpass LeakGAN on both metrics. 4. Using inception score on mixture of eight Gaussians may not make much sense, if they are using the ImageNet pre-trained model, since such a model is not trained to fit this distribution. Still, the author has reported symmetric KL. 5. The authors did not specify their inception score on real Celeb-A and CIFAR10 images. Overall, I tend to accept this paper for its contribution on methods. It would be even better if my concerns could be addressed. Edit: after seeing the review of Reviewer 3, I find the proposed method seems to be the same as Online EWC and I have downgraded the rating. The authors should address these concerns.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review . Responses to your comments : 1 . Apologies if the figures were unclear . With regard to Figure 1 , the oscillations in an 8 Gaussians GAN are especially obvious if each time step ( each corresponding to a plot like Figure 1 a , b , c , d ) are compiled into frames of a video . In such videos , the generator of a vanilla GAN can be seen oscillating through the space in 2D indefinitely , returning to various previous locations repeatedly . For Figure 1 , we hoped to demonstrate this compactly with several proximal frames , but given that the space is 2D , the generator at d ) doesn \u2019 t necessarily return to exactly the same location as a ) while oscillating , and it may also visit other modes before returning . Regardless of exact positioning , we can see that the generator is once again producing synthesized samples in d ) that it had been disincentivized from producing after a ) . In Figure 2 , the x-axis represents the dataset being trained on , while each line represents performance on a particular dataset as the model is fine-tuned on additional datasets . For example , the darkest blue line represents the model \u2019 s test accuracy on D_1 , which starts off high when the model is first trained specifically on D_1 \u2019 s training set , but it drops precipitously once the model is finetuned on D_2 . 2.We report the best performances we found from hyperparameter search on our baseline models . The variances for many of the baseline models is higher due to their propensity to fail to converge more often than our methods . This results in at least a few significantly lower ICP/higher Sym-KL values than a converged model , resulting in corresponding higher variances . 3.We agree it would be better if we surpassed LeakGAN on both metrics . However , we would like to emphasize that the proposed method did improve its baseline model textGAN on both metrics , which demonstrates the main point of the utility of continual learning augmentation . 4.Reporting ICP with ImageNet pre-trained features indeed makes little sense . Our ICP is more accurately Inception Score-inspired : we pre-trained a classifier on 8 Gaussians data sample and used that to calculate ICP , much in the way an ImageNet classifier is used for the standard ICP . 5. \u201c Specify inception score on real Celeb-A and CIFAR10 images. \u201d Inception score on real CIFAR10 images is 11.23 . Note that Inception score is computed on datasets with labels , which the Celeb-A dataset doesn \u2019 t have . Further , we reported FID score on both datasets , which past works have found to be a better metric than Inception score [ * ] . [ * ] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium , NIPS 2017 Please see our response to Reviewer 3 for concerns about Online EWC ."}, "1": {"review_id": "SJzuHiA9tQ-1", "review_text": "Summary: This paper proposes the use of GANs as a realistic benchmark for continual learning, and shows how continual learning techniques applied to the discriminator can alleviate mode collapse. Existing continual learning approaches for discrete task structure (EWC and IS) are adapted to the continually shifting domain of GANs, and evaluated on a toy mixture of Gaussians, CelebA and CIFAR-10 image generation as well as textGANs. This is a clearly written paper that nicely addresses some of the challenges of bridging the toy problems of continual learning with a real world problem of GAN training. The experiments and ablations are thorough, but the empirical gains in terms of improving GAN metrics are relatively minor. It's also not obvious that GAN training is really a continual learning problem, as every time the generator distribution shifts, the discriminator has to shift as well. Thus progress on stabilizing and improving GANs might not transfer back to domains that truly represent continual learning where the goal is to build a single network that perform well at all points in time. In terms of more realistic benchmarks for continual learning, I believe a controlled synthetic dataset would be more practical than the sequence of GAN checkpoints proposed here. Strengths: + Clearly written, with good background discussion of continual learning approaches and challenges. + Interesting adaptation of EWC and IS to the continual setting with task rates, online memory (sum of quadratics is quadratic), and controlled forgetting with a time decay. + Thorough experiments and ablations on toy tasks, CelebA and CIFAR-10, and textGANs. Nicely includes error bars and compares computation time for each approach. Weaknesses: - The paper could benefit from more discussion on the goals of continual learning, and what is wrong with existing toy benchmarks. Why not come up with a tractable toy problem that addresses these difficulties directly? - I remain unconvinced that GANs as a good benchmark for continual learning. For example, it has been argued that many of the problems with GANs arise from dynamics of minimax optimization difficulties, and there are many recent approaches that were not compared to that focus on this optimization aspect of GAN training (Metz et al., Roth et al., Mescheder et al.). How would you relate these theoretical ideas to continual learning? - Most the experimental improvements are incremental. How did you choose or tune hyperparameters of your approach? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review . Responses to your comments : - GAN as a continual learning problem : Our argument comes from the rationale that the ideal discriminator should be able to identify any fake sample , produced from any past generator ( G_ { \\theta_1 } , \u2026 , G_ { \\theta_t } ) from any point in time , not just the current instantiation of the generator ( G_ { \\theta_t } ) . If this isn \u2019 t the case , because of catastrophic forgetting and the way training samples are presented ( only from G_ { \\theta_t } ) , the discriminator will forget past generator samples in favor of current ones . As a result , the generator has the option of simply shifting to one of the previous instantiations ( g_ { \\theta_ < t } ) , as can be seen Figure 1 . This can result in oscillations in the training process , particularly in mode-collapsed generators . Please see https : //youtu.be/91a2gPWngo8 as an example ( we 've posted this video with an anonymized account , to respect ICLR 's double submission policy ) . - Continual learning benchmarks : The primary concern with current continual learning benchmarks is that many choose to focus on learning a sequence of classification tasks , often with each task bearing little resemblance to the others . [ 1 ] points out that this is actually an \u201c unrealistic best case scenario , \u201d because the forgetting effect is actually minimized , due to the completely unrelated tasks . Our opinion is that in real world applications , continual learning is more likely to be of utility when used for a model encountering a slowly evolving environment , as is likely to happen in the real world . This kind of setting isn \u2019 t commonly considered in the continual learning literature , partly because most of the common machine learning datasets are static , collected from a distribution at a single snapshot in time . We are unaware of any common datasets that exhibit a coherent time evolution , which if they existed could be used to test continual learning in an evolving setting . Because of GAN \u2019 s evolving nature and impressive generative capabilities , we \u2019 re proposing GANs as a way to synthetically create such datasets . - Relation to GAN optimization literature : We are aware of the many works on GAN optimization , but we consider our perspective to be orthogonal to these works . Even with these various techniques to stabilize the minimax , the discriminator is still presented with the task of learning a changing distribution , and as such , GAN training remains a continual learning problem . - Hyperparameters : We used the 8 Gaussians toy setting to get a feel for how the various hyperparameters interacted with each other . For the real datasets , we used \\alpha=100 , \\gamma=0.99 , and \\lambda=1e-3 , keeping the rest of the hyperparameters the same as the baselines ( pre-augmentation ) that we compared against . [ 1 ] Sebastian Farquhar and Yarin Gal . Towards Robust Evaluations of Continual Learning . arXiv preprint , 2018 ."}, "2": {"review_id": "SJzuHiA9tQ-2", "review_text": "This paper connects continual learning and GAN training together, and propose to use standard continual learning schemes (EWC etc) to improve GAN training. Continual learning for GANs is certainly an important problem to look at. Even though I like the problem, I'm not convinced with the solution provided by the paper. The paper in it's current form, in terms of technical contributions and experimental analyses presented to support the hypotheses it started with, is not good enough to be accepted in ICLR. My comments: 1) Catastrophic forgetting of discriminator: Interesting view about mode collapse. I have following concerns: (a) I like the view in which the training is shown as a sequential process. I wonder if we could solve the issue of catastrophic forgetting of discriminator by storing sufficient fake examples from previously generated samples from the generator. Storing previous generations has already been explored, however, just to prove the point that forgetting is an issue, it would be interesting to store enough samples for the mixture of Gaussian setting and analyse. (b) Why not thinking of catastrophic forgetting of generator? What if we constrain the generator to not-to-forget about previously generated samples by Fisher or something similar? In this case, every new task in the training process will have sufficient fake samples from all the modes. 2) Lack of technical contributions: The approach, in which EWC or IS is being used to regulate the discriminator's parameters, seems to be straightforward. The issue of multiple parameters is being resolved by storing one Fisher/Score vector using moving average type scheme. This, to me, is almost same as Online EWC or EWC++. Both these papers have already addressed this issue and discussed them in great detail. How is this approach different? 3) Not sure what exactly Fig 2 is conveying as D_1^{gen}, \\cdots, D_T^{gen} should almost be the same, so training on one and testing forgetting on other doesn't actually say much. I think we can't conclude anything from this figure, at least using MNIST experiments. Minor: 4) I'm assuming that you call your method EWC/IS GAN (I think it wasn't explicitly mentioned in the paper). Why don't you call Seff et al. 2018 (they used EWC with GAN for the first time) work EWC-GAN? I personally feel that it's important to give acronyms so that it doesn't undermine previous works. Just to clarify, I'm not advocating the work by Seff et al.. 5) Please provide citations for mode collapse Online EWC: Progress & compress: A scalable framework for continual learnin, ICML 2018. EWC++: Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence, ECCV 2018. ", "rating": "3: Clear rejection", "reply_text": "Thank you for the review . Responses to your comments : 1a . Storing previously generated samples would indeed be an option to combat forgetting , and as we discuss in our Related Works , this has indeed been considered before [ 1 ] . However , such an approach necessitates access to previous generations . One could at each time step t either save i ) a very large ( to prevent memorization ) number of samples , or ii ) the generator \u2019 s weights . Option i ) could become very space prohibitive quickly , and ii ) even more so . Additionally , injecting old samples limits the number of current samples that can be considered . In our method , we avoid both of these by maintaining memory in our parameters , without requiring a buffer . 1b.We indeed considered applying continual learning to the generator , but ultimately decided not to . While it is clear that a discriminator capable of distinguishing any fake from any point in time is desirable , a generator capable of generating any previous fake is not as clearly of use . Additionally , if the discriminator indeed truly retains the ability to recognize any fake from any point in time , then there is no reason for the generator to remember how to fool previous discriminators . 2.We approached the problem from the starting point of the characteristics of GAN training . As we outlined in our submission , unlike typical continual learning problems , where there are explicit disjoint tasks ( e.g.permutations of MNIST , different Atari games ) , the definition of a \u201c task \u201d in GAN is less clear cut ; this results in a series of challenges that we outline in the bullets of Section 3.2 , which we subsequently solve by deriving an in-place update rule . While our ultimate solution may resemble Online EWC and EWC++ , we arrived at it independently , to fit the needs of the problem at hand . More specific differences with prior works include our definition of a task rate \\alpha , as well as \u201c completing the square \u201d of quadratics to derive our update rule , as opposed to re-centering the posterior on the latest solution . 3.Apologies if Figure 2 is not immediately clear . D_1^ { gen } , \\cdots , D_T^ { gen } may appear similar to the human eye , but each D_ { t } ^ { gen } is in fact the result of the generator evolving to make the classifier with weights from time t-1 as poor as possible . Figure 2 illustrates that when the discriminator learns a new generator distribution , catastrophic forgetting results in the severe degradation of the discriminator \u2019 s ability to recognize previous generator distributions . This process occurs repeatedly during the training process of a GAN . See our reply to reviewer 1 for additional comments . 4.We started referring to our model as EWC-GAN and IS-GAN early during development ( before becoming aware of Seff et.al.2018 ) .The name stuck during our discussions , and since our work isn \u2019 t directly comparable with Seff et . al 2018 , we didn \u2019 t notice the name collision . Would GAN+EWC/GAN+IS be sufficiently different , or would you still consider that too close ? 5.Noted.They \u2019 ll be in our next draft . Among many papers discussing \u201c Mode collapse \u201d , we would like mention a few : Mode regularized generative adversarial networks . ICLR 2017 . Improved techniques for training GANs . NIPS 2016 VEEGAN : Reducing Mode Collapse in GANs using Implicit Variational Learning , NIPS 2017 [ 1 ] Learning from Simulated and Unsupervised Images through Adversarial Training Ashish Shrivastava , Tomas Pfister , Oncel Tuzel , Josh Susskind , Wenda Wang , Russ Webb"}}