{"year": "2020", "forum": "H1x-3xSKDr", "title": "Batch Normalization is a Cause of Adversarial Vulnerability", "decision": "Reject", "meta_review": "This article studies the effects of BN on robustness. The article presents a series of experiments on various datasets with noise, PGD adversarial attacks, and various corruption benchmarks, that show a drop in robustness when using BN. It is suggested that a main cause of vulnerability is the tiling angle of the decision boundary, which is illustrated in a toy example. \nThe reviewers found the contribution interesting and that the effect will impact many DNNs. However, they the did not find the arguments for the tiling explanation convincing enough, and suggested more theory and experimental illustration of this explanation would be important. In the rebuttal the authors maintain that the main contribution is to link BN and adversarial vulnerability and consider their explanation reasonable. In the initial discussion the reviewers also mentioned that the experiments were not convincing enough and that the phenomenon could be an effect of gradient masking, and that more experiments with other attack strategies would be important to clarify this. In response, the revision included various experiments, including some with various initial learning schedules. The revision clarified some of these issues. However, the reviewers still found that the reason behind the effect requires more explanations. In summary, this article makes an important observation that is already generating a vivid discussion and will likely have an impact, but the reviewers were not convinced by the explanations provided for these observations. \n", "reviews": [{"review_id": "H1x-3xSKDr-0", "review_text": "This paper identifies an important weakness of batch normalization: it increases adversarial vulnerability. It is very well written and the claims are theoretically sound. In the experiments, the authors demonstrated a significant difference in robustness between networks with or without batch normalization layers, in varies settings against both random input noise and adversarial noise. This weakness of batch norm was explained due to the \"decision boundary tilting\" effect caused by the normalization. Overall, this paper has done solid work to reveal an interesting phenomenon. If it is true, this finding will impact almost all DNN models. My concern is that this phenomenon is just another effect of \"gradient masking \" (as pointed out by Athalye, et al.). Batch norm is a well-known technique to avoid overfitting, without batch norm the network can be easily trained to be saturated with almost zero gradients, demonstrating a false signal of \"robustness\" to noise. The random noise and real-world corruption experiments are definitely helpful to clear this doubt, but only partially. My concern remains because of two obvious signs of gradient masking: 1. The accuracy on PGD-li (epsilon=0.031) attacks are suspiciously too high (20% - 40% Table 3/4). For this level of attack, the acc should be nearly zero. This is likely caused by the gradient masking effect, considering the cifar-10 networks were trained for longer time with larger learning rate (150 epochs, fixed lr 0.01). Training on MNIST is much easier to get zero gradients. 2. The weight decay discussion is not helpful at all, on the contrary, it confirms my concern on the gradient masking effect. In Table 8, the robustness was increased ~40% by just using large weight decay. This is not the \"real robustness\", and can be easily evaded by adaptive attack (see Athalye's paper). With the above two concerns in mind, I doubt the phenomenon revealed in this paper is just \"one can easily train a saturated model without batch norm\" or equivalently \"it's hard to train a saturated model with batch norm\". It is hard to say if this is a bad thing for batch norm. I am quite surprised that the authors ignore this completely. Here are a few things that can be done to rule out the possibility of gradient masking. The masked gradient can be identified by: 1) One-step attacks perform better than iterative attacks; 2) Unbounded attacks do not reach 100% success., etc (see Section 3.1 of Athalye's paper). 1. Including FGSM in the experiments and show the same trends as PGD-li. 2. Show two networks have similar gradient norms. 3. Apply cw-l2 attack, and show batch norm has forced large perturbation. Two other suggestions: 1. Summarize the different angles/steps taken to verify the phenomenon, somewhere before the experiments. 2. Cannot see why the input dimension discussion contribute to explanations of the batch norm weakness. ============ My rating stays the same after rebuttal. My original concerns are like the other reviewers: why BN, not other techniques such as structure of DNNs MLP vs CNN vs ResNet, activation functions, weight decay, learning rates, softmax etc. My initial suspect was that it is caused by gradient masking likely caused by the l2 weight regularization, so asked the authors to look at the gradient norms and run some testes to rule this out. Yes, the weight norm is directly related to the Lipschitz continuity of the function represented by the network, but it often becomes more complicated on complex nonlinear neural networks. According to the new experiment results, the vulnerability is indeed not an effect of gradient masking, thanks for the clarification. However, the new results also indicate that the finding is susceptible to both weight decay and learning rate: in Figure 16 (a): \"Un PGD\" < \"BN PGD\" before learning rate decay, andFigure 17 (a) vs (b), doubling the weight decay penalty to 1e-3 also increases the vulnerability of BN. Overall, I believe the phenomenon exists, but the reasons behind requires more explanations, at least not just the batch norm.", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for recognising the positive aspects of this work , and for stating that they believe the work to be theoretically sound . We agree that it 's imperative to ensure results are not tainted by gradient masking ; we have taken care to ensure this did not play a role here and wish to alleviate this natural concern . -- Re : 1. \u201c The accuracy on PGD-li ( epsilon=0.031 ) are suspiciously too high ( 20 % - 40 % in Tables 3 , 4 ) , the accuracy should be nearly zero \u201d . For the experiments of Section 4 , the input was normalized to zero mean and unit variance using per-channel statistics computed per dataset . Thus , epsilon=0.03 represents about 4/255 rather than 8/255 on the [ 0 , 1 ] scale . In Figure 5 , epsilon is increased until zero accuracy is reached for a vanilla ResNet , and accuracy is indeed reduced to zero by epsilon=6 for all models under standard training . The difference in accuracy at epsilon 4/255 compared to Tables 3/4 is explained by the difference in architecture , WideResNet and VGG . In particular , VGG is more robust than the residual networks we tested under standard training . -- Re : 2. \u201c The weight decay discussion is not helpful at all , on the contrary , it confirms my concern on the gradient masking effect . In Table 8 , the robustness was increased ~40 % by just using large weight decay . This is not the `` real robustness '' , and can be easily evaded by adaptive attack ( see Athalye 's paper ) . 1.The claim you mention ( 40 % increase in robustness ) was evaluated using an adaptive method . 2.We interpret this comment as an affirmation that our findings are non-obvious , rather than as a limitation . 3.We would really appreciate if you can clarify why you believe the weight decay discussion is not helpful , and the robustness \u201c not real \u201d . There are strong theoretical connections between penalizing the parameter norms , robust optimization , and decision boundary tilting , see e.g. , Xu & Manor , ( JMLR 2009 ) , Tanay & Griffin , ( 2016 ) . We also showed that this is essential for mitigating an increase in vulnerability as the input dimension increases . L2 weight decay minimizes an upper bound on the Frobenius norm of the linear operators in the network , thus bounding the Lipschitz constant of the network . The Lipschitz constant taken together with the mean prediction margin is well known to govern adversarial perturbation robustness ( Tsuzuku et al. , in NeurIPS 2018 ) . 4.Can you recommend a specific attack you would like us to evaluate against that would increase your confidence in our results ? We are familiar with the work of Athalye et al and do not believe that the attack evaluation is in any way related to the `` defense '' , all attacks are unseen to all models as of training time . Furthermore , a concurrent submission and Reviewer 2 appear to have successfully reproduced our main result . -- Re : `` One-step attacks perform better than iterative attacks '' . Are you referring to an instance of this rule being violated in our work ( if you believe you saw this , it would help us if you could point to a specific instance ) , or are you suggesting that we show results for one step attacks alongside those of iterative attacks ? It 's generally agreed that one step attacks are n't as meaningful for deep/nonlinear models , which is why we only use FGSM for linear models and PGD for deep models . Nonetheless , we have added this result to a new section of the Appendix titled \u201c On the Initial Learning Rate \u201d where we plot the test accuracy under various perturbations vs training epochs . The FGSM curves lie above the 40-step PGD curve for a given model in all cases . Also , Appendix C deals with unbounded attacks which reach 100 % success in all cases . -- Re : \u201c Apply cw-l2 attack , and show batch norm has forced large perturbation. \u201d Thank you for this suggestion . We have added adversarial examples crafted by the CWL2 method in Figure 13 of an Appendix I titled \u201c Adversarial Examples \u201d . The L2 distortion required to achieve a fixed misclassification confidence threshold is 0.95 in the case of batch norm , and 2.89 for the baseline , which represents a three fold improvement on the relevant performance metric for this attack . The white-box procedures would not work without access to a clean gradient signal , yet all reach 100 % success , or confidence , in all cases . Please let us know if these clarifications address your concerns . We agree that our work would be far less impactful if our result could be reduced to gradient masking . We hope that the additional experiments and analysis we have performed lays this concern to rest ."}, {"review_id": "H1x-3xSKDr-1", "review_text": "Summary: In this empirical study, the authors identify that batch normalization -- a common technique for accelerating training -- leads to brittle representations that exhibit a lack of robustness and are more susceptible to adversarial attacks. The authors demonstrate their results on SVHN, CIFAR-10, CIFAR-100 CIFAR-10.1 using a variety of network architectures including VGG, BagNet, WideResNet, AlexNet, etc. Major Concerns: 1. As presented, the experiments are not convincing. I do not know how much of the changes in adversarial vulnerability are due to batch normalization as opposed to other facets of the training procedure that may have changed in their BN vs no-BN experiments. For instance, batch normalization usually accommodates higher learning rates and it is not clear if the authors adjusted the initial learning rate, learning rate schedule or training schedule accordingly. If so, it would be important to run a set of experiments with these parameters fixed as per the baseline no-BN models. That said, even if the authors did run these experiments, it is still not clear if the cause of adversarial vulnerability is due to BN. Consider that what is truly important in model training is not the learning rate (i.e. step size), but rather the magnitude of the changes in each weight (or the ratio of weight change to the weight). By swapping in batch normalization, the authors may just be altering the norm of the weight change in the (re-parameterized) weights. In this scenario, the gains of removing batch normalization could just as well be explained by the effective change in the learning rate, and not about batch normalization itself, c.f. Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks Yuanzhi Li, Colin Wei, Tengyu Ma https://arxiv.org/abs/1907.04595 If the differences in the adversarial vulnerability could be ascribed to effective changes in gradient updates, then this would change the interpretation of these results notably. 2. The underlying hypothesis is specious. I have several reservations about the underlying hypothesis that requires stronger evidence to overcome. In particular, I have reservations in believing that BN itself is a cause of adversarial vulnerability because BN is just a factorization of a network's weights. That is, there is nothing \"special\" nor unique about BN-networks; instead, the BN factorization merely permits accelerated training efficiency. Consider the fact that a BN model may be re-expressed by merely folding in the parameters (i.e. applying the matrix multiplications) into the MLP weights or CNN filters. Thus, the numerical function approximated by the BN and the \"folded\" non-BN model is identical. What would it mean to say that the BN is \"causing\" adversarial vulnerability in the BN model given that both the BN and non-BN model perform the identical function? Another way to say this is to pretend we train a non-BN MLP or CNN model. After training the model, we could apply a BN factorization of the weights. Thus, the non-BN model may be factorized into a BN model. If the resulting BN model were adversarial vulnerable (which I suspect is the case), it would seem very hard to believe that BN was the cause of the vulnerability given it was a post-hoc factorization of the weights. That said, I could definitely imagine that the training procedure itself could lead to adversarial vulnerability (e.g. citation above) and by employing a BN factorization, one may be encouraged to use a training procedure which leads to increased vulnerability. I would encourage the authors to consider this line of attack and thus, re-orient their analysis and discussion accordingly. 3. The title is poorly worded. Not withstanding the point above, adversarial vulnerability predates BN. Likewise, non-BN models exhibit adversarial vulnerability. Thus, this title is not a great reflection of the findings of the paper. I would strongly suggest replacing \"is a cause\" with \"increases\" or \"exacerbates\".", "rating": "1: Reject", "reply_text": "We thank the reviewer for their frank comments and pointing us to the work on explaining the effect of using a large initial learning rate . Responding to this review has helped us improve the work , as well as our own understanding . We aim to satisfy the reviewer \u2019 s concerns with a new section titled \u201c On The Initial Learning Rate \u201d in which we evaluate robustness on CIFAR-10 during training under various initial learning rates and schedules . In-line responses/clarifications to the review follow : -- `` The authors demonstrate their results on SVHN , CIFAR-10 , CIFAR-100 CIFAR-10.1 '' To clarify , we evaluated on CIFAR- { 10 , 10.1 , 10-C } , but not on CIFAR-100 . We also evaluated pre-trained models on ImageNet , as well as the Adversarial Spheres dataset ( Gilmer et al. , ICLR Workshop 2018 ) ( albeit in an Appendix ) . -- `` I do not know how much of the changes in adversarial vulnerability are due to batch normalization as opposed to other facets of the training procedure that may have changed in their BN vs no-BN experiments . '' No aspect of the training procedure was changed for BN vs no-BN , unless explicitly stated otherwise , e.g. , to allow BN a larger initial learning rate , see next bullet . -- \u201c batch normalization usually accommodates higher learning rates and it is not clear if the authors adjusted the initial learning rate \u201d Quoting from the paper at the time of submission : ( In ref Table 4 ) : \u201c It has been suggested that one of the benefits of BN is that it facilitates training with a larger learning rate ( Ioffe & Szegedy , 2015 ; Bjorck et al. , 2018 ) . We test this from a robustness perspective in an experiment summarized in Table 4 , where the initial learning rate is increased to 0.1 when BN is used. \u201d To prevent this from being missed and improve the clarity of our work , we can summarize the scenarios considered at the beginning of Section 4 , e.g. , \u201c Case 1 ) both models get small LR \u201d , \u201c Case 2 ) BN gets high initial LR \u201d . If the reviewer previously read this and still believes it to be unclear , please let us know . Note that we also provide a counterexample to the conventional wisdom that BN allows a higher learning rate in Appendix G on the Adversarial Spheres dataset . -- `` By swapping in batch normalization , the authors may just be altering the norm of the weight change in the ( re-parameterized ) weights . In this scenario , the gains of removing batch normalization could just as well be explained by the effective change in the learning rate , and not about batch normalization itself , c.f . ( Li et al. , 2019 ) . '' As indicated by the two cases ( on the synthetic and MNIST dataset ) where we compute the boundary tilting angle , BN affects not only the norm ( invariance ) of the weights , but also the angle . Consider the first thing that happens when we feed forward a batch of inputs after random initialization : the weights are rescaled by the inverse of the standard deviation ( and numerical stability const ) along each dimension . Thus , the weights corresponding to low variance features increase in value , while those corresponding to high variance features shrink , and the resulting batch-normalized weight vectors can be nearly orthogonal to those without BN . Thus , the angle is a critical difference between batch-normalized and unnormalized weights . Although we motivated and explicitly characterized this for linear models , Section 6 of Labatie , ( ICML 2019 ) shows that deep batch-normalized networks accordingly suffer from increased sensitivity w.r.t.the input as a result , and similarly remark : `` [ under BN ] directions of high signal variance are dampened , while directions of low signal variance are amplified . This preferential exploration of low signal directions naturally deteriorates the signal-to-noise ratio and amplifies $ \\chi^l $ '' . Where $ \\chi^l $ is defined as the normalized sensitivity from layer 0 to $ l $ , such that $ \\chi^l > 1 $ degrades the signal-to-noise ratio . We found Li et al. , 2019 and the learning order concept interesting . We agree that in the context of this work , it is imperative to consider the case where a higher initial learning rate is used with BN given that it does usually facilitate this . Please see Table 4 , and the new section \u201c On The Initial Learning Rate \u201d regarding this point which shows that over the course of 150 epochs of training using several learning rate schedules , BN obtains at most 40 % accuracy to 40-step PGD while the baseline exceeds 50 % accuracy on the same ."}, {"review_id": "H1x-3xSKDr-2", "review_text": "Overview: This is an interesting work. The paper is dedicated to studying the effect of BN to network robustness. The author shows that BN can reduce network robustness to small adversarial input perturbations and common corruptions by double-digit percentages. Then, they use a linear \"toy model\" to explain the mechanism that the actual cause is the tilting of the decision boundary. Moreover, the author conducts extensive experiments on popular datasets to show the robustness margin with or without the BN module. Finally, the author finds that substituting weight decay for BN is good enough to nullify a relationship between adversarial vulnerability and the input resolution. Strength Bullets: 1. I like the linear toy example. For that binary classification example, the author explicitly explains the boundary tilting, which increases the adversarial vulnerability of the model. It is clear. 2. The paper conducts extensive experiment on SVHN, MNIST, CIFAR10 (C) datasets. And they show performance margin with or without the BN module. And for the attacker setting, they do use the popular setting (i.e. Mardy's PGD setting) in this field which makes the results more convincing. Weakness Bullets: 1. Why do not visualize the decision boundary of networks (used in this work) to valid the boundary tilting \"theory\". The toy example is clear but not convincing enough. There exist several techniques may be helpful to the visualization. (i.e. Robustness via curvature regularization, and vice versa). I think it is one of the important parts of this work. The observation of BN causes adversarial vulnerability is interesting but the main focus should be offering more convincing explanations. 2. I do run experiments for VGG11,13,16,19 on cifar10 with PGD 3 attack (Mardy's setting). There exist ~20 ATA performance gaps between networks with BN and BN networks without BN. But for adversarial trained models, the gaps don't exist anymore, at least for VGG11,13,16,19 on cifar10 with PGD 3 attack. (The performance gap is less than 0.5). In other words, without the BN layers, the robustness of adversarially trained models will not increase in my experiments. I see you report some results in Appendix C. But it is not enough to convince me. Could you provide more implementation details about the adversarial training and attacker? And more experiment results about this point are needed. If adversarial training can fix the vulnerability by BN and BN can give a TA boost, there is no reason we need to remove BN in our adversarial training setting. I see there are similar concerns in the OpenReview. 3. [Minior] The experiments need to be organized better. Especially for section 3, it will be better to divide different experiments or observations into the different subsections. Recommendations: For the above weakness bullets, this is a week reject. Suggestions: 1. To solve the weakness bullets; 2. minor suggestion: add the reference mention in the OpenReview, they are related to this work. Questions: 1. You mention that you run PGD for 20-40 iterations in the experiment at the bottom of page three. But at each table, you only report one number. So my question is for that accuracy number, you run how many iterations for PGD?", "rating": "3: Weak Reject", "reply_text": "Thank you for your feedback , and for taking the time to reproduce the main result on several architectures , this effort goes above and beyond , we appreciate it . We respond to each concern below . -- `` Why do not visualize the decision boundary of networks ( used in this work ) to valid the boundary tilting `` theory '' . The toy example is clear but not convincing enough . There exist several techniques may be helpful to the visualization . ( i.e.Robustness via curvature regularization , and vice versa ) . I think it is one of the important parts of this work . '' Aside from space limitations , we wanted to restrict visualizations to scenarios where they are faithful to the underlying model . Although such visualizations would be nice to have for further intuition , we believe the experiments provide sufficient evidence to support the main hypothesis . Also , beyond the toy model , we computed the boundary tilting angle of linear models w.r.t.the nearest centroid classifier on MNIST . Any dimensionality reduction technique used to visualize a high dimensional decision boundary introduces trade-offs that we didn \u2019 t feel were necessary to support the main message . We may opt to add such visualizations in a subsequent blog article as you suggest . -- `` The observation of BN causes adversarial vulnerability is interesting , but the main focus should be offering more convincing explanations . '' Thank you for stating that you believe the main finding of this work is interesting . We believe we have provided a reasonably accessible explanation , but wish to emphasize that our main contribution is that , to the best of our knowledge , this is the first work to explicitly link batch norm and adversarial vulnerability . This has major implications for state-of-the-art networks , and we believe this connection was previously unknown in the adversarial examples / robustness to distribution shift literature . Many laws of physics that we now take for granted were initially discovered via systematic experiment , and subsequently formalized by others . Other works , e.g. , Yang et al. , ICLR 2019 , Labatie , ICML 2019 examine batch norm at length from a theoretical perspective which supports our conclusions , and a concurrent submission provides further insight as to why the vulnerability we discovered occurs . The official reviews of Yang et al. , ( https : //openreview.net/forum ? id=SyMDXnCcF7 ) expressed concern that their approach was not particularly intuitive or reflecting of popular practice , hence we made an effort to cast the limitations of BN in a practical light for practitioners and researchers of broad backgrounds to understand . -- `` I do run experiments \u2026 There exist ~20 ATA performance gaps between networks with BN and networks without BN . But for adversarial trained models , the gaps do n't exist anymore , at least for VGG11,13,16,19 on cifar10 with PGD 3 attack . ( The performance gap is less than 0.5 ) . In other words , without the BN layers , the robustness of adversarially trained models will not increase in my experiments . I see you report some results in Appendix C. But it is not enough to convince me . If adversarial training can fix the vulnerability by BN and BN can give a TA boost , there is no reason we need to remove BN in our adversarial training setting . '' We agree that the adversarial training results originally presented in Appendix C were perhaps a bit too informal . We have tightened up this section , including a full breakdown showing test accuracy and variance for each corruption , instead of simply stating the mean test accuracy over all corruptions . As we motivated in the main text , it is imperative to consider robustness to unseen adversaries . Thus , it is unfair to benchmark the robustness of natural and adversarially trained networks using the same procedure , when one approach directly optimizes performance w.r.t.one of the evaluations . As you found , in some circumstances the performance degradation of BN seems small if we train on PGD and evaluate on the same , but this no longer holds if we consider other more realistic threat models and common corruptions . To be more clear , we have renamed Appendix C to \u201c PGD Training Yields Unfortunate Robustness Trade-offs \u201d , as PGD can fail to yield a model with semantically meaningful gradients or convincingly broad robustness even for MNIST , due to the thresholding operation that is learned to favor $ \\ell_ { \\infty } $ robustness ( see the discussion of Appendix E \u201c MNIST Inspection \u201d of Madry et al. , ( ICLR 2018 ) ) . These limitations have been widely discussed , e.g. , Sharma & Chen. , ( ICLR Workshop 2018 ) , Schott et al. , ( ICLR 2019 ) , Jacobsen et al. , ( ICLR 2019 , ICLR Workshop 2019 ) , Mu & Gilmer. , ( ICML Workshop 2019 ) and are not too surprising given Goodhart \u2019 s law : \u201c When a measure becomes a target , it ceases to be a good measure \u201d . The \u201c measure \u201d in this case being the $ \\ell_ { \\infty } $ norm of the perturbations , which has become , somewhat arbitrarily , a focal point in the adversarial examples literature ."}], "0": {"review_id": "H1x-3xSKDr-0", "review_text": "This paper identifies an important weakness of batch normalization: it increases adversarial vulnerability. It is very well written and the claims are theoretically sound. In the experiments, the authors demonstrated a significant difference in robustness between networks with or without batch normalization layers, in varies settings against both random input noise and adversarial noise. This weakness of batch norm was explained due to the \"decision boundary tilting\" effect caused by the normalization. Overall, this paper has done solid work to reveal an interesting phenomenon. If it is true, this finding will impact almost all DNN models. My concern is that this phenomenon is just another effect of \"gradient masking \" (as pointed out by Athalye, et al.). Batch norm is a well-known technique to avoid overfitting, without batch norm the network can be easily trained to be saturated with almost zero gradients, demonstrating a false signal of \"robustness\" to noise. The random noise and real-world corruption experiments are definitely helpful to clear this doubt, but only partially. My concern remains because of two obvious signs of gradient masking: 1. The accuracy on PGD-li (epsilon=0.031) attacks are suspiciously too high (20% - 40% Table 3/4). For this level of attack, the acc should be nearly zero. This is likely caused by the gradient masking effect, considering the cifar-10 networks were trained for longer time with larger learning rate (150 epochs, fixed lr 0.01). Training on MNIST is much easier to get zero gradients. 2. The weight decay discussion is not helpful at all, on the contrary, it confirms my concern on the gradient masking effect. In Table 8, the robustness was increased ~40% by just using large weight decay. This is not the \"real robustness\", and can be easily evaded by adaptive attack (see Athalye's paper). With the above two concerns in mind, I doubt the phenomenon revealed in this paper is just \"one can easily train a saturated model without batch norm\" or equivalently \"it's hard to train a saturated model with batch norm\". It is hard to say if this is a bad thing for batch norm. I am quite surprised that the authors ignore this completely. Here are a few things that can be done to rule out the possibility of gradient masking. The masked gradient can be identified by: 1) One-step attacks perform better than iterative attacks; 2) Unbounded attacks do not reach 100% success., etc (see Section 3.1 of Athalye's paper). 1. Including FGSM in the experiments and show the same trends as PGD-li. 2. Show two networks have similar gradient norms. 3. Apply cw-l2 attack, and show batch norm has forced large perturbation. Two other suggestions: 1. Summarize the different angles/steps taken to verify the phenomenon, somewhere before the experiments. 2. Cannot see why the input dimension discussion contribute to explanations of the batch norm weakness. ============ My rating stays the same after rebuttal. My original concerns are like the other reviewers: why BN, not other techniques such as structure of DNNs MLP vs CNN vs ResNet, activation functions, weight decay, learning rates, softmax etc. My initial suspect was that it is caused by gradient masking likely caused by the l2 weight regularization, so asked the authors to look at the gradient norms and run some testes to rule this out. Yes, the weight norm is directly related to the Lipschitz continuity of the function represented by the network, but it often becomes more complicated on complex nonlinear neural networks. According to the new experiment results, the vulnerability is indeed not an effect of gradient masking, thanks for the clarification. However, the new results also indicate that the finding is susceptible to both weight decay and learning rate: in Figure 16 (a): \"Un PGD\" < \"BN PGD\" before learning rate decay, andFigure 17 (a) vs (b), doubling the weight decay penalty to 1e-3 also increases the vulnerability of BN. Overall, I believe the phenomenon exists, but the reasons behind requires more explanations, at least not just the batch norm.", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for recognising the positive aspects of this work , and for stating that they believe the work to be theoretically sound . We agree that it 's imperative to ensure results are not tainted by gradient masking ; we have taken care to ensure this did not play a role here and wish to alleviate this natural concern . -- Re : 1. \u201c The accuracy on PGD-li ( epsilon=0.031 ) are suspiciously too high ( 20 % - 40 % in Tables 3 , 4 ) , the accuracy should be nearly zero \u201d . For the experiments of Section 4 , the input was normalized to zero mean and unit variance using per-channel statistics computed per dataset . Thus , epsilon=0.03 represents about 4/255 rather than 8/255 on the [ 0 , 1 ] scale . In Figure 5 , epsilon is increased until zero accuracy is reached for a vanilla ResNet , and accuracy is indeed reduced to zero by epsilon=6 for all models under standard training . The difference in accuracy at epsilon 4/255 compared to Tables 3/4 is explained by the difference in architecture , WideResNet and VGG . In particular , VGG is more robust than the residual networks we tested under standard training . -- Re : 2. \u201c The weight decay discussion is not helpful at all , on the contrary , it confirms my concern on the gradient masking effect . In Table 8 , the robustness was increased ~40 % by just using large weight decay . This is not the `` real robustness '' , and can be easily evaded by adaptive attack ( see Athalye 's paper ) . 1.The claim you mention ( 40 % increase in robustness ) was evaluated using an adaptive method . 2.We interpret this comment as an affirmation that our findings are non-obvious , rather than as a limitation . 3.We would really appreciate if you can clarify why you believe the weight decay discussion is not helpful , and the robustness \u201c not real \u201d . There are strong theoretical connections between penalizing the parameter norms , robust optimization , and decision boundary tilting , see e.g. , Xu & Manor , ( JMLR 2009 ) , Tanay & Griffin , ( 2016 ) . We also showed that this is essential for mitigating an increase in vulnerability as the input dimension increases . L2 weight decay minimizes an upper bound on the Frobenius norm of the linear operators in the network , thus bounding the Lipschitz constant of the network . The Lipschitz constant taken together with the mean prediction margin is well known to govern adversarial perturbation robustness ( Tsuzuku et al. , in NeurIPS 2018 ) . 4.Can you recommend a specific attack you would like us to evaluate against that would increase your confidence in our results ? We are familiar with the work of Athalye et al and do not believe that the attack evaluation is in any way related to the `` defense '' , all attacks are unseen to all models as of training time . Furthermore , a concurrent submission and Reviewer 2 appear to have successfully reproduced our main result . -- Re : `` One-step attacks perform better than iterative attacks '' . Are you referring to an instance of this rule being violated in our work ( if you believe you saw this , it would help us if you could point to a specific instance ) , or are you suggesting that we show results for one step attacks alongside those of iterative attacks ? It 's generally agreed that one step attacks are n't as meaningful for deep/nonlinear models , which is why we only use FGSM for linear models and PGD for deep models . Nonetheless , we have added this result to a new section of the Appendix titled \u201c On the Initial Learning Rate \u201d where we plot the test accuracy under various perturbations vs training epochs . The FGSM curves lie above the 40-step PGD curve for a given model in all cases . Also , Appendix C deals with unbounded attacks which reach 100 % success in all cases . -- Re : \u201c Apply cw-l2 attack , and show batch norm has forced large perturbation. \u201d Thank you for this suggestion . We have added adversarial examples crafted by the CWL2 method in Figure 13 of an Appendix I titled \u201c Adversarial Examples \u201d . The L2 distortion required to achieve a fixed misclassification confidence threshold is 0.95 in the case of batch norm , and 2.89 for the baseline , which represents a three fold improvement on the relevant performance metric for this attack . The white-box procedures would not work without access to a clean gradient signal , yet all reach 100 % success , or confidence , in all cases . Please let us know if these clarifications address your concerns . We agree that our work would be far less impactful if our result could be reduced to gradient masking . We hope that the additional experiments and analysis we have performed lays this concern to rest ."}, "1": {"review_id": "H1x-3xSKDr-1", "review_text": "Summary: In this empirical study, the authors identify that batch normalization -- a common technique for accelerating training -- leads to brittle representations that exhibit a lack of robustness and are more susceptible to adversarial attacks. The authors demonstrate their results on SVHN, CIFAR-10, CIFAR-100 CIFAR-10.1 using a variety of network architectures including VGG, BagNet, WideResNet, AlexNet, etc. Major Concerns: 1. As presented, the experiments are not convincing. I do not know how much of the changes in adversarial vulnerability are due to batch normalization as opposed to other facets of the training procedure that may have changed in their BN vs no-BN experiments. For instance, batch normalization usually accommodates higher learning rates and it is not clear if the authors adjusted the initial learning rate, learning rate schedule or training schedule accordingly. If so, it would be important to run a set of experiments with these parameters fixed as per the baseline no-BN models. That said, even if the authors did run these experiments, it is still not clear if the cause of adversarial vulnerability is due to BN. Consider that what is truly important in model training is not the learning rate (i.e. step size), but rather the magnitude of the changes in each weight (or the ratio of weight change to the weight). By swapping in batch normalization, the authors may just be altering the norm of the weight change in the (re-parameterized) weights. In this scenario, the gains of removing batch normalization could just as well be explained by the effective change in the learning rate, and not about batch normalization itself, c.f. Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks Yuanzhi Li, Colin Wei, Tengyu Ma https://arxiv.org/abs/1907.04595 If the differences in the adversarial vulnerability could be ascribed to effective changes in gradient updates, then this would change the interpretation of these results notably. 2. The underlying hypothesis is specious. I have several reservations about the underlying hypothesis that requires stronger evidence to overcome. In particular, I have reservations in believing that BN itself is a cause of adversarial vulnerability because BN is just a factorization of a network's weights. That is, there is nothing \"special\" nor unique about BN-networks; instead, the BN factorization merely permits accelerated training efficiency. Consider the fact that a BN model may be re-expressed by merely folding in the parameters (i.e. applying the matrix multiplications) into the MLP weights or CNN filters. Thus, the numerical function approximated by the BN and the \"folded\" non-BN model is identical. What would it mean to say that the BN is \"causing\" adversarial vulnerability in the BN model given that both the BN and non-BN model perform the identical function? Another way to say this is to pretend we train a non-BN MLP or CNN model. After training the model, we could apply a BN factorization of the weights. Thus, the non-BN model may be factorized into a BN model. If the resulting BN model were adversarial vulnerable (which I suspect is the case), it would seem very hard to believe that BN was the cause of the vulnerability given it was a post-hoc factorization of the weights. That said, I could definitely imagine that the training procedure itself could lead to adversarial vulnerability (e.g. citation above) and by employing a BN factorization, one may be encouraged to use a training procedure which leads to increased vulnerability. I would encourage the authors to consider this line of attack and thus, re-orient their analysis and discussion accordingly. 3. The title is poorly worded. Not withstanding the point above, adversarial vulnerability predates BN. Likewise, non-BN models exhibit adversarial vulnerability. Thus, this title is not a great reflection of the findings of the paper. I would strongly suggest replacing \"is a cause\" with \"increases\" or \"exacerbates\".", "rating": "1: Reject", "reply_text": "We thank the reviewer for their frank comments and pointing us to the work on explaining the effect of using a large initial learning rate . Responding to this review has helped us improve the work , as well as our own understanding . We aim to satisfy the reviewer \u2019 s concerns with a new section titled \u201c On The Initial Learning Rate \u201d in which we evaluate robustness on CIFAR-10 during training under various initial learning rates and schedules . In-line responses/clarifications to the review follow : -- `` The authors demonstrate their results on SVHN , CIFAR-10 , CIFAR-100 CIFAR-10.1 '' To clarify , we evaluated on CIFAR- { 10 , 10.1 , 10-C } , but not on CIFAR-100 . We also evaluated pre-trained models on ImageNet , as well as the Adversarial Spheres dataset ( Gilmer et al. , ICLR Workshop 2018 ) ( albeit in an Appendix ) . -- `` I do not know how much of the changes in adversarial vulnerability are due to batch normalization as opposed to other facets of the training procedure that may have changed in their BN vs no-BN experiments . '' No aspect of the training procedure was changed for BN vs no-BN , unless explicitly stated otherwise , e.g. , to allow BN a larger initial learning rate , see next bullet . -- \u201c batch normalization usually accommodates higher learning rates and it is not clear if the authors adjusted the initial learning rate \u201d Quoting from the paper at the time of submission : ( In ref Table 4 ) : \u201c It has been suggested that one of the benefits of BN is that it facilitates training with a larger learning rate ( Ioffe & Szegedy , 2015 ; Bjorck et al. , 2018 ) . We test this from a robustness perspective in an experiment summarized in Table 4 , where the initial learning rate is increased to 0.1 when BN is used. \u201d To prevent this from being missed and improve the clarity of our work , we can summarize the scenarios considered at the beginning of Section 4 , e.g. , \u201c Case 1 ) both models get small LR \u201d , \u201c Case 2 ) BN gets high initial LR \u201d . If the reviewer previously read this and still believes it to be unclear , please let us know . Note that we also provide a counterexample to the conventional wisdom that BN allows a higher learning rate in Appendix G on the Adversarial Spheres dataset . -- `` By swapping in batch normalization , the authors may just be altering the norm of the weight change in the ( re-parameterized ) weights . In this scenario , the gains of removing batch normalization could just as well be explained by the effective change in the learning rate , and not about batch normalization itself , c.f . ( Li et al. , 2019 ) . '' As indicated by the two cases ( on the synthetic and MNIST dataset ) where we compute the boundary tilting angle , BN affects not only the norm ( invariance ) of the weights , but also the angle . Consider the first thing that happens when we feed forward a batch of inputs after random initialization : the weights are rescaled by the inverse of the standard deviation ( and numerical stability const ) along each dimension . Thus , the weights corresponding to low variance features increase in value , while those corresponding to high variance features shrink , and the resulting batch-normalized weight vectors can be nearly orthogonal to those without BN . Thus , the angle is a critical difference between batch-normalized and unnormalized weights . Although we motivated and explicitly characterized this for linear models , Section 6 of Labatie , ( ICML 2019 ) shows that deep batch-normalized networks accordingly suffer from increased sensitivity w.r.t.the input as a result , and similarly remark : `` [ under BN ] directions of high signal variance are dampened , while directions of low signal variance are amplified . This preferential exploration of low signal directions naturally deteriorates the signal-to-noise ratio and amplifies $ \\chi^l $ '' . Where $ \\chi^l $ is defined as the normalized sensitivity from layer 0 to $ l $ , such that $ \\chi^l > 1 $ degrades the signal-to-noise ratio . We found Li et al. , 2019 and the learning order concept interesting . We agree that in the context of this work , it is imperative to consider the case where a higher initial learning rate is used with BN given that it does usually facilitate this . Please see Table 4 , and the new section \u201c On The Initial Learning Rate \u201d regarding this point which shows that over the course of 150 epochs of training using several learning rate schedules , BN obtains at most 40 % accuracy to 40-step PGD while the baseline exceeds 50 % accuracy on the same ."}, "2": {"review_id": "H1x-3xSKDr-2", "review_text": "Overview: This is an interesting work. The paper is dedicated to studying the effect of BN to network robustness. The author shows that BN can reduce network robustness to small adversarial input perturbations and common corruptions by double-digit percentages. Then, they use a linear \"toy model\" to explain the mechanism that the actual cause is the tilting of the decision boundary. Moreover, the author conducts extensive experiments on popular datasets to show the robustness margin with or without the BN module. Finally, the author finds that substituting weight decay for BN is good enough to nullify a relationship between adversarial vulnerability and the input resolution. Strength Bullets: 1. I like the linear toy example. For that binary classification example, the author explicitly explains the boundary tilting, which increases the adversarial vulnerability of the model. It is clear. 2. The paper conducts extensive experiment on SVHN, MNIST, CIFAR10 (C) datasets. And they show performance margin with or without the BN module. And for the attacker setting, they do use the popular setting (i.e. Mardy's PGD setting) in this field which makes the results more convincing. Weakness Bullets: 1. Why do not visualize the decision boundary of networks (used in this work) to valid the boundary tilting \"theory\". The toy example is clear but not convincing enough. There exist several techniques may be helpful to the visualization. (i.e. Robustness via curvature regularization, and vice versa). I think it is one of the important parts of this work. The observation of BN causes adversarial vulnerability is interesting but the main focus should be offering more convincing explanations. 2. I do run experiments for VGG11,13,16,19 on cifar10 with PGD 3 attack (Mardy's setting). There exist ~20 ATA performance gaps between networks with BN and BN networks without BN. But for adversarial trained models, the gaps don't exist anymore, at least for VGG11,13,16,19 on cifar10 with PGD 3 attack. (The performance gap is less than 0.5). In other words, without the BN layers, the robustness of adversarially trained models will not increase in my experiments. I see you report some results in Appendix C. But it is not enough to convince me. Could you provide more implementation details about the adversarial training and attacker? And more experiment results about this point are needed. If adversarial training can fix the vulnerability by BN and BN can give a TA boost, there is no reason we need to remove BN in our adversarial training setting. I see there are similar concerns in the OpenReview. 3. [Minior] The experiments need to be organized better. Especially for section 3, it will be better to divide different experiments or observations into the different subsections. Recommendations: For the above weakness bullets, this is a week reject. Suggestions: 1. To solve the weakness bullets; 2. minor suggestion: add the reference mention in the OpenReview, they are related to this work. Questions: 1. You mention that you run PGD for 20-40 iterations in the experiment at the bottom of page three. But at each table, you only report one number. So my question is for that accuracy number, you run how many iterations for PGD?", "rating": "3: Weak Reject", "reply_text": "Thank you for your feedback , and for taking the time to reproduce the main result on several architectures , this effort goes above and beyond , we appreciate it . We respond to each concern below . -- `` Why do not visualize the decision boundary of networks ( used in this work ) to valid the boundary tilting `` theory '' . The toy example is clear but not convincing enough . There exist several techniques may be helpful to the visualization . ( i.e.Robustness via curvature regularization , and vice versa ) . I think it is one of the important parts of this work . '' Aside from space limitations , we wanted to restrict visualizations to scenarios where they are faithful to the underlying model . Although such visualizations would be nice to have for further intuition , we believe the experiments provide sufficient evidence to support the main hypothesis . Also , beyond the toy model , we computed the boundary tilting angle of linear models w.r.t.the nearest centroid classifier on MNIST . Any dimensionality reduction technique used to visualize a high dimensional decision boundary introduces trade-offs that we didn \u2019 t feel were necessary to support the main message . We may opt to add such visualizations in a subsequent blog article as you suggest . -- `` The observation of BN causes adversarial vulnerability is interesting , but the main focus should be offering more convincing explanations . '' Thank you for stating that you believe the main finding of this work is interesting . We believe we have provided a reasonably accessible explanation , but wish to emphasize that our main contribution is that , to the best of our knowledge , this is the first work to explicitly link batch norm and adversarial vulnerability . This has major implications for state-of-the-art networks , and we believe this connection was previously unknown in the adversarial examples / robustness to distribution shift literature . Many laws of physics that we now take for granted were initially discovered via systematic experiment , and subsequently formalized by others . Other works , e.g. , Yang et al. , ICLR 2019 , Labatie , ICML 2019 examine batch norm at length from a theoretical perspective which supports our conclusions , and a concurrent submission provides further insight as to why the vulnerability we discovered occurs . The official reviews of Yang et al. , ( https : //openreview.net/forum ? id=SyMDXnCcF7 ) expressed concern that their approach was not particularly intuitive or reflecting of popular practice , hence we made an effort to cast the limitations of BN in a practical light for practitioners and researchers of broad backgrounds to understand . -- `` I do run experiments \u2026 There exist ~20 ATA performance gaps between networks with BN and networks without BN . But for adversarial trained models , the gaps do n't exist anymore , at least for VGG11,13,16,19 on cifar10 with PGD 3 attack . ( The performance gap is less than 0.5 ) . In other words , without the BN layers , the robustness of adversarially trained models will not increase in my experiments . I see you report some results in Appendix C. But it is not enough to convince me . If adversarial training can fix the vulnerability by BN and BN can give a TA boost , there is no reason we need to remove BN in our adversarial training setting . '' We agree that the adversarial training results originally presented in Appendix C were perhaps a bit too informal . We have tightened up this section , including a full breakdown showing test accuracy and variance for each corruption , instead of simply stating the mean test accuracy over all corruptions . As we motivated in the main text , it is imperative to consider robustness to unseen adversaries . Thus , it is unfair to benchmark the robustness of natural and adversarially trained networks using the same procedure , when one approach directly optimizes performance w.r.t.one of the evaluations . As you found , in some circumstances the performance degradation of BN seems small if we train on PGD and evaluate on the same , but this no longer holds if we consider other more realistic threat models and common corruptions . To be more clear , we have renamed Appendix C to \u201c PGD Training Yields Unfortunate Robustness Trade-offs \u201d , as PGD can fail to yield a model with semantically meaningful gradients or convincingly broad robustness even for MNIST , due to the thresholding operation that is learned to favor $ \\ell_ { \\infty } $ robustness ( see the discussion of Appendix E \u201c MNIST Inspection \u201d of Madry et al. , ( ICLR 2018 ) ) . These limitations have been widely discussed , e.g. , Sharma & Chen. , ( ICLR Workshop 2018 ) , Schott et al. , ( ICLR 2019 ) , Jacobsen et al. , ( ICLR 2019 , ICLR Workshop 2019 ) , Mu & Gilmer. , ( ICML Workshop 2019 ) and are not too surprising given Goodhart \u2019 s law : \u201c When a measure becomes a target , it ceases to be a good measure \u201d . The \u201c measure \u201d in this case being the $ \\ell_ { \\infty } $ norm of the perturbations , which has become , somewhat arbitrarily , a focal point in the adversarial examples literature ."}}