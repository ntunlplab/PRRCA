{"year": "2017", "forum": "BJC8LF9ex", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "decision": "Reject", "meta_review": "This paper presents a modification of GRU-RNNs to handle missing data explicitly, allowing them to exploit data not missing at random. The method is presented clearly enough, but the reviewers felt that the claims were overreaching. It's also unsatisfying that the method depends on specific modifications of RNN architectures for a particular domain, instead of being a more general approach.", "reviews": [{"review_id": "BJC8LF9ex-0", "review_text": "This paper presents a modified gated RNN caled GRU-D that deals with time series which display a lot of missing values in their input. They work on two fronts. The first deals with the missing inputs directly by using a learned convex combination of the previous available value (forward imputation) and the mean value (mean imputation). The second includes dampening the recurrent layer not unlike a second reset gate, but parametrized according to the time elapsed since the last available value of each attributes. Positives ------------ - Clear definition of the task (handling missing values for classification of time series) - Many interesting baselines to test the new model against. - The model presented deals with the missing values in a novel, ML-type way (learn new dampening parameters). - The extensive tests done on the datasets is probably the greatest asset of this paper. Negatives ------------- - The paper could use some double checking for typos. - The Section A.2.3 really belongs in the main article as it deals with important related works. Swap it with the imprecise diagrams of the model if you need space. - No mention of any methods from the statistics litterature. Here are the two main points of this review that informs my decision: 1. The results, while promising, are below expectations. The paper hasn\u2019t been able to convince me that GRU-simple (without intervals) isn\u2019t just as well-suited for the task of handling missing inputs as GRU-D. In the main paper, GRU-simple is presented as the main baseline. Yet, it includes a lot of extraneous parameters (the intervals) that, according to Table 5, probably hurts the model more than it helps it. Having a third of it\u2019s parameters being of dubious value, it brings the question of the fairness of the comparison done in the main paper, especially since in the one table where GRU-simple (without intervals) is present, GRU-D doesn\u2019t significantly outperforms it. 2. My second concern, and biggest, is with some claims that are peppered through the paper. The first is about the relationship with the presence rate of data in the dataset and the diagnostics. I might be wrong, but that only indicates that the doctor in charge of that patient requested the relevant analyses be done according to the patient\u2019s condition. That would mean that an expert system based on this data would always seem to be one step behind. The second claim is the last sentence of the introduction, which sets huge expectations that were not met by the paper. Another is that \u201csimply concatenating masking and time interval vectors fails to exploit the temporal structure of missing values\u201d is unsubstantiated and actually disproven later in the paper. Yet another is the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added. This fails to consider that non-GRU models actually started with much better results than most GRU ones. Lastly is their claim to capture informative missingness by incorporating masking and time intervals directly inside the GRU architecture. While the authors did make these changes, the fact that they also concatenate the mask to the input, just like GRU-simple (without intervals), leads me to question the actual improvement made by GRU-D. Given that, while I find that the work that has been put into the paper is above average, I wouldn\u2019t accept that paper without a reframing of the findings and a better focus on the real contribution of this paper, which I believe is the novel way to parametrize the choice of imputation method.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your time and thoughtful comments ! Before addressing detailed comments , we would like to clarify our contribution in this work : we develop a reasonable and novel RNN framework for time series classification , which 1 ) captures missing patterns , 2 ) automatically imputes or accounts for missing values , and 3 ) consistently obtains the state-of-the-art results on the challenging healthcare domain classification problems . To the best of our knowledge , our work is one of the first of this kind . None of the previous works explicitly modify the RNN structure to capture and utilize missingness information for applications such as healthcare . Experiments on synthetic and real datasets have demonstrated our proposed GRU-D model is the best among existing state-of-the-art models . We agree that GRU-simple ( with and without intervals ) models are strong baselines , but the proposed GRU-D model is definitely the best among all GRU baselines for handling and utilizing missingness . GRU-simple models often but not always get marginal improvement against other GRU baselines ( i.e. , GRU-mean/forward ) . Moreover , the masking and/or time interval are fed into the model same as other input values without considering their roles as ` missingness indicators ` . Empirically we found GRU-simple baselines ( with and without intervals ) have quite close performance and are not as good as GRU-D . Thus we take GRU-simple as the representative in the main paper and show its variants in the appendix . It \u2019 s worth noting that while time interval doesn \u2019 t help much in GRU-simple , it not only improves the performance of GRU-D but also provides useful interpretations for the prediction tasks ( as shown in Fig.4 ) .We believe the superiority of GRU-D model over GRU baselines for the following reasons : 1 . It achieves consistent performance improvement on the difficult prediction tasks : ~0.02 AUC score in PhysioNet mortality prediction and ~0.01 AUC score in MIMIC-III mortality prediction over the best baselines . 2.GRU-D model for healthcare applications is an example of how domain knowledge can be simply incorporated into the model architecture design to better handle and utilize missing values . 3.GRU-D provides useful insights for raw input features from decay plots/distributions . These results help us understand the underlying variable impact on prediction tasks and also help domain experts understand and apply our model and discover new domain knowledge . Other detailed comments : Q : the relationship with the presence rate of data in the dataset and the diagnostics . A : Figure 1 shows the observations on missingness and prediction tasks and , thus demonstrates the usefulness of missingness . Missingness can be due to several factors , such as medical events , saving costs , anomalies , inconvenience and so on . Healthcare providers ' judgments and actions may also lead to some missing values in the data . However , missing data is widespread in our dataset and it 's hard to figure out the reason for the missing values of each variable . Q : the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added . This fails to consider that non-GRU models actually started with much better results than most GRU ones . A : As shown in Figure 7 , GRU-D has comparable performance on 2k samples and beats non-GRU models with increasing margin on 10k and 19k samples . Other GRU baselines also improve with more training samples . Given these results , we expect this trend will continue if we have even larger healthcare datasets . Especially , the performance gap between the proposed GRU-D and non-RNN baselines is expected to increase . Q : The Section A.2.3 really belongs in the main article as it deals with important related works . Swap it with the imprecise diagrams of the model if you need space . A : We have revised and reorganized the contents on related baseline models as you suggested to make it more clear . Q : No mention of any methods from the statistics literature . A : The goal of this work is to make predictions on whether a patient has a disease ( or what is his/her mortality status ) based on the time series observations with missing values from ICU . In short , it is a time series classification task with utilization of missing patterns but not a data imputation task . Also , as discussed in the paper ( Wells et al. , 2013 ) , missing not at random imputation techniques lead to suboptimal performance ."}, {"review_id": "BJC8LF9ex-1", "review_text": "This paper proposed a way to deal with supervised multivariate time series tasks involving missing values. The high level idea is still using the recurrent neural network (specifically, GRU in this paper) to do sequence supervised learning, e.g., classification, but modifications have been made to the input and hidden layers of RNNs to tackle the missing value problem. pros: 1) the insight of utilizing missing value is critical. the observation of decaying effect in the healthcare application is also interesting; 2) the experiment seems to be solid; the baseline algorithms and analysis of results are also done properly. cons: 1) the novelty of this work is not enough. Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture. 2) the datasets used in this paper are small. 3) the decaying effect might not be able to generalize to other domains. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your time and thoughtful comments ! We would like to address your the questions and concerns below . Q1 : the novelty of this work is not enough . Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture . A : We would like to clarify our contributions : we develop a reasonable and novel RNN framework for time series classification , which 1 ) capture missing patterns , 2 ) automatically imputes or accounts for missing values , and 3 ) consistently improves classification results . To the best of our knowledge , our work is one of the first of this kind . None of the previous works explicitly modified the RNN structure to capture and utilize missingness information for applications such as healthcare . Experiments on both synthetic and real datasets have demonstrated our proposed GRU-D model is the best among existing state-of-the-art models . Q2 : the datasets used in this paper are small . A : ( This question is also asked by AnonReviewer2 and we thus copied the same answer here . ) As far as we know , the MIMIC-III dataset we used is the largest publicly available one of its kind of tens of thousands of admission records . We agreed that the datasets in healthcare is not as many as in other mature domains such as computer vision or natural language processing , where massive public datasets are available and deep learning models are demonstrated to be powerful . Data collection and sharing in healthcare has it own myriad of challenges and is beyond the scope of this paper . Our methodologies work irrespective of the dataset size . Also , it is quite important to make deep learning more applicable in domains with moderate amount of available data such as healthcare . Motivated by this , we believe our work is a reasonable approach to improve the RNN performance in such condition by resorting to the missing patterns in the data . Q3 : the decaying effect might not be able to generalize to other domains . A : The proposed model works in a broad set of domains where we encounter time series data missing at random or not at random . In this paper we evaluated our model for healthcare applications . Given that we 're taking ICU data as example , our decay idea is very well matched to homeostatic systems ( e.g. , human body ) that actively control themselves to revert to mean . We demonstrated it is indeed helpful in practice . Furthermore , our framework is flexible enough to incorporate decay functions in a variety of forms which can handle different missing mechanisms ."}, {"review_id": "BJC8LF9ex-2", "review_text": "The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written. IMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I\u2019m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments and your pre-review suggestions for us to improve this work . We 'd like to clarify that our model is widely applicable and competitive as well on big datasets . As shown in the experiments ( Section 3.3 and Appendix A.3.5 in the updated draft ) , the proposed GRU-D model achieves consistent improvement in both one-layer and multi-layer RNN settings on all the datasets of varied size . As far as we know , the MIMIC-III dataset we used is the largest publicly available one of its kind of tens of thousands of admission records . We agreed that the datasets in healthcare is not as many as in other mature domains such as computer vision or natural language processing , where massive public datasets are available and deep learning models are demonstrated to be powerful . Data collection and sharing in healthcare has it own myriad of challenges and is beyond the scope of this paper . Our methodologies work irrespective of the dataset size . Also , it is quite important to make deep learning more applicable in domains with moderate amount of available data such as healthcare . Motivated by this , our work is a reasonable approach to improve the RNN performance in such condition by resorting to the missing patterns in the data ."}], "0": {"review_id": "BJC8LF9ex-0", "review_text": "This paper presents a modified gated RNN caled GRU-D that deals with time series which display a lot of missing values in their input. They work on two fronts. The first deals with the missing inputs directly by using a learned convex combination of the previous available value (forward imputation) and the mean value (mean imputation). The second includes dampening the recurrent layer not unlike a second reset gate, but parametrized according to the time elapsed since the last available value of each attributes. Positives ------------ - Clear definition of the task (handling missing values for classification of time series) - Many interesting baselines to test the new model against. - The model presented deals with the missing values in a novel, ML-type way (learn new dampening parameters). - The extensive tests done on the datasets is probably the greatest asset of this paper. Negatives ------------- - The paper could use some double checking for typos. - The Section A.2.3 really belongs in the main article as it deals with important related works. Swap it with the imprecise diagrams of the model if you need space. - No mention of any methods from the statistics litterature. Here are the two main points of this review that informs my decision: 1. The results, while promising, are below expectations. The paper hasn\u2019t been able to convince me that GRU-simple (without intervals) isn\u2019t just as well-suited for the task of handling missing inputs as GRU-D. In the main paper, GRU-simple is presented as the main baseline. Yet, it includes a lot of extraneous parameters (the intervals) that, according to Table 5, probably hurts the model more than it helps it. Having a third of it\u2019s parameters being of dubious value, it brings the question of the fairness of the comparison done in the main paper, especially since in the one table where GRU-simple (without intervals) is present, GRU-D doesn\u2019t significantly outperforms it. 2. My second concern, and biggest, is with some claims that are peppered through the paper. The first is about the relationship with the presence rate of data in the dataset and the diagnostics. I might be wrong, but that only indicates that the doctor in charge of that patient requested the relevant analyses be done according to the patient\u2019s condition. That would mean that an expert system based on this data would always seem to be one step behind. The second claim is the last sentence of the introduction, which sets huge expectations that were not met by the paper. Another is that \u201csimply concatenating masking and time interval vectors fails to exploit the temporal structure of missing values\u201d is unsubstantiated and actually disproven later in the paper. Yet another is the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added. This fails to consider that non-GRU models actually started with much better results than most GRU ones. Lastly is their claim to capture informative missingness by incorporating masking and time intervals directly inside the GRU architecture. While the authors did make these changes, the fact that they also concatenate the mask to the input, just like GRU-simple (without intervals), leads me to question the actual improvement made by GRU-D. Given that, while I find that the work that has been put into the paper is above average, I wouldn\u2019t accept that paper without a reframing of the findings and a better focus on the real contribution of this paper, which I believe is the novel way to parametrize the choice of imputation method.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your time and thoughtful comments ! Before addressing detailed comments , we would like to clarify our contribution in this work : we develop a reasonable and novel RNN framework for time series classification , which 1 ) captures missing patterns , 2 ) automatically imputes or accounts for missing values , and 3 ) consistently obtains the state-of-the-art results on the challenging healthcare domain classification problems . To the best of our knowledge , our work is one of the first of this kind . None of the previous works explicitly modify the RNN structure to capture and utilize missingness information for applications such as healthcare . Experiments on synthetic and real datasets have demonstrated our proposed GRU-D model is the best among existing state-of-the-art models . We agree that GRU-simple ( with and without intervals ) models are strong baselines , but the proposed GRU-D model is definitely the best among all GRU baselines for handling and utilizing missingness . GRU-simple models often but not always get marginal improvement against other GRU baselines ( i.e. , GRU-mean/forward ) . Moreover , the masking and/or time interval are fed into the model same as other input values without considering their roles as ` missingness indicators ` . Empirically we found GRU-simple baselines ( with and without intervals ) have quite close performance and are not as good as GRU-D . Thus we take GRU-simple as the representative in the main paper and show its variants in the appendix . It \u2019 s worth noting that while time interval doesn \u2019 t help much in GRU-simple , it not only improves the performance of GRU-D but also provides useful interpretations for the prediction tasks ( as shown in Fig.4 ) .We believe the superiority of GRU-D model over GRU baselines for the following reasons : 1 . It achieves consistent performance improvement on the difficult prediction tasks : ~0.02 AUC score in PhysioNet mortality prediction and ~0.01 AUC score in MIMIC-III mortality prediction over the best baselines . 2.GRU-D model for healthcare applications is an example of how domain knowledge can be simply incorporated into the model architecture design to better handle and utilize missing values . 3.GRU-D provides useful insights for raw input features from decay plots/distributions . These results help us understand the underlying variable impact on prediction tasks and also help domain experts understand and apply our model and discover new domain knowledge . Other detailed comments : Q : the relationship with the presence rate of data in the dataset and the diagnostics . A : Figure 1 shows the observations on missingness and prediction tasks and , thus demonstrates the usefulness of missingness . Missingness can be due to several factors , such as medical events , saving costs , anomalies , inconvenience and so on . Healthcare providers ' judgments and actions may also lead to some missing values in the data . However , missing data is widespread in our dataset and it 's hard to figure out the reason for the missing values of each variable . Q : the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added . This fails to consider that non-GRU models actually started with much better results than most GRU ones . A : As shown in Figure 7 , GRU-D has comparable performance on 2k samples and beats non-GRU models with increasing margin on 10k and 19k samples . Other GRU baselines also improve with more training samples . Given these results , we expect this trend will continue if we have even larger healthcare datasets . Especially , the performance gap between the proposed GRU-D and non-RNN baselines is expected to increase . Q : The Section A.2.3 really belongs in the main article as it deals with important related works . Swap it with the imprecise diagrams of the model if you need space . A : We have revised and reorganized the contents on related baseline models as you suggested to make it more clear . Q : No mention of any methods from the statistics literature . A : The goal of this work is to make predictions on whether a patient has a disease ( or what is his/her mortality status ) based on the time series observations with missing values from ICU . In short , it is a time series classification task with utilization of missing patterns but not a data imputation task . Also , as discussed in the paper ( Wells et al. , 2013 ) , missing not at random imputation techniques lead to suboptimal performance ."}, "1": {"review_id": "BJC8LF9ex-1", "review_text": "This paper proposed a way to deal with supervised multivariate time series tasks involving missing values. The high level idea is still using the recurrent neural network (specifically, GRU in this paper) to do sequence supervised learning, e.g., classification, but modifications have been made to the input and hidden layers of RNNs to tackle the missing value problem. pros: 1) the insight of utilizing missing value is critical. the observation of decaying effect in the healthcare application is also interesting; 2) the experiment seems to be solid; the baseline algorithms and analysis of results are also done properly. cons: 1) the novelty of this work is not enough. Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture. 2) the datasets used in this paper are small. 3) the decaying effect might not be able to generalize to other domains. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your time and thoughtful comments ! We would like to address your the questions and concerns below . Q1 : the novelty of this work is not enough . Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture . A : We would like to clarify our contributions : we develop a reasonable and novel RNN framework for time series classification , which 1 ) capture missing patterns , 2 ) automatically imputes or accounts for missing values , and 3 ) consistently improves classification results . To the best of our knowledge , our work is one of the first of this kind . None of the previous works explicitly modified the RNN structure to capture and utilize missingness information for applications such as healthcare . Experiments on both synthetic and real datasets have demonstrated our proposed GRU-D model is the best among existing state-of-the-art models . Q2 : the datasets used in this paper are small . A : ( This question is also asked by AnonReviewer2 and we thus copied the same answer here . ) As far as we know , the MIMIC-III dataset we used is the largest publicly available one of its kind of tens of thousands of admission records . We agreed that the datasets in healthcare is not as many as in other mature domains such as computer vision or natural language processing , where massive public datasets are available and deep learning models are demonstrated to be powerful . Data collection and sharing in healthcare has it own myriad of challenges and is beyond the scope of this paper . Our methodologies work irrespective of the dataset size . Also , it is quite important to make deep learning more applicable in domains with moderate amount of available data such as healthcare . Motivated by this , we believe our work is a reasonable approach to improve the RNN performance in such condition by resorting to the missing patterns in the data . Q3 : the decaying effect might not be able to generalize to other domains . A : The proposed model works in a broad set of domains where we encounter time series data missing at random or not at random . In this paper we evaluated our model for healthcare applications . Given that we 're taking ICU data as example , our decay idea is very well matched to homeostatic systems ( e.g. , human body ) that actively control themselves to revert to mean . We demonstrated it is indeed helpful in practice . Furthermore , our framework is flexible enough to incorporate decay functions in a variety of forms which can handle different missing mechanisms ."}, "2": {"review_id": "BJC8LF9ex-2", "review_text": "The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written. IMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I\u2019m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments and your pre-review suggestions for us to improve this work . We 'd like to clarify that our model is widely applicable and competitive as well on big datasets . As shown in the experiments ( Section 3.3 and Appendix A.3.5 in the updated draft ) , the proposed GRU-D model achieves consistent improvement in both one-layer and multi-layer RNN settings on all the datasets of varied size . As far as we know , the MIMIC-III dataset we used is the largest publicly available one of its kind of tens of thousands of admission records . We agreed that the datasets in healthcare is not as many as in other mature domains such as computer vision or natural language processing , where massive public datasets are available and deep learning models are demonstrated to be powerful . Data collection and sharing in healthcare has it own myriad of challenges and is beyond the scope of this paper . Our methodologies work irrespective of the dataset size . Also , it is quite important to make deep learning more applicable in domains with moderate amount of available data such as healthcare . Motivated by this , our work is a reasonable approach to improve the RNN performance in such condition by resorting to the missing patterns in the data ."}}