{"year": "2020", "forum": "H1l3s6NtvH", "title": "A Bayes-Optimal View on Adversarial Examples", "decision": "Reject", "meta_review": "The paper studies how adversarial robustness and Bayes optimality relate in a simple gaussian mixture setting. The paper received two recommendations for rejection and one weak accept. One of the central complaints was whether the study had any bearing on \"real world\" adversarial examples. I think this is a fair concern, given how limited the model appears on the surface, although perhaps the model is a good model of any local \"piece\" of a decision boundary in a real problem. That said, I do not agree with the strong rejection (1) in most places. The weak reject asked for some experiments. The revision produced these experiments, but I'm not sure how convincing these are since only one robust training method was used, and it's not clear that it's the best one could do among SOTA methods. For whatever reason, the reviewers did not update their scores. I am not certain that they reviewed the revision, despite my prodding.", "reviews": [{"review_id": "H1l3s6NtvH-0", "review_text": "The paper analyzed the adversarial examples from the Bayes-optimal view. Specifically, the authors analyzed the relationship between the symmetry of covariance of data distribution and the amount of data which are close to the decision boundary. The authors proved that when the covariance of data distribution is asymmetric, a large amount of data will be close to the decision boundary (easy to be attacked). The authors also provided the new datasets which is easy to compute for the bayes-optimal classifier so as to verify the effect of symmetry of covariance on vulnerability of classifier. Moreover, the paper indicated that the vulnerability of CNNs is due to asymmetric distributions or non-optimal learning. It is interesting that the paper investigated the adversarial examples from the Bayes-optimal view. However, there are some drawbacks: 1. The motivation of this paper is not clear to me. In other words, what is the benefit of analyzing the adversarial examples from the Bayes-optimal viewpoint, since Bayes model mentioned in this paper is easy to attack. I am not fully convinced by the presentation of the paper. 2. The theorem or the observation in the paper appears too straightforward. And the \u2018observation 1\u2019is not general. The authors may need to consider more general cases that when the standard deviation of eigen value of covariance matrix is large, the Bayes model will be easily attacked. (not just the case that one of eigen value is zero). 3. One minor point, it appears somewhat strange that \u201cobservations\u201d were proved. It is better to change observations to theorems or lemmas. 4. The authors tried to explain directly the vulnerability of CNN in a same way. However, CNN is a totally different model compared with the Bayes model (one is a discriminative model and the other is a generative model). For generative models, the classification boundary is closely related to all training samples. Therefore, the variance of data distribution is important for attack. For discriminative models, the decision boundary is related to local information. It may not be proper to analyze CNN in the way same as the Bayes model. This should be further clarified and discussed. ", "rating": "1: Reject", "reply_text": "Thank you for reading our paper . We hope we can clarify ( below ) some misunderstandings and hope that you will read it again in the light of our comments . \u201c What is the benefit of analyzing the adversarial examples from the Bayes-Optimal viewpoint , since Bayes model mentioned in this paper is easy to attack. \u201d Actually , as we show in the paper , the Bayes-Optimal classifier may be easy or hard to attack depending on the data distribution . More generally , adversarial examples present an intriguing and complex phenomena , and we believe that analyzing them from the Bayes-Optimal perspective allows us to disentangle the different possible causes . We didn \u2019 t fully understand the comment that \u201c observation 1 is not general \u201d . Specifically , we do not require zero eigenvalue for the vulnerability condition to occur ( but asymmetry in the variance ) . \u201c CNN is a totally different model compared with the Bayes model ( one is a discriminative model and the other is a generative model ) \u201d . Actually the Bayes-Optimal model ( as its name suggests ) gives the best possible accuracy in the discrimination task . Thus if the CNN wants to optimize the accuracy , then it should agree with the Bayes-Optimal model ."}, {"review_id": "H1l3s6NtvH-1", "review_text": "This paper proposes studying adversarial examples from the perspective of Bayes-optimal classifiers. They construct a pair of synthetic but somewhat realistic datasets\u2014in one case, the Bayes-optimal classifier is *not* robust, demonstrating that the Bayes-optimal classifier may not be robust for real-world datasets. In the other case, the Bayes-optimal classifier is robust, but neural networks fail to learn the robust decision boundary. This demonstrates that even when the Bayes-optimal classifier is robust, we may need to explicitly regularize/incentivize neural networks to learn the correct decision boundary. The contribution of the two datasets (the symmetric and asymetric CelebA) is, in my opinion, an extremely important contribution in studying adversarial robustness and on their own these datasets warrant further study. Previously, all studies of this sort had to be done with small-scale classifiers and simplistic datasets such as Gaussians. The paper also definitively proves that there are realistic datasets where the Bayes-optimal classifier is non-robust, which goes against quite a bit of conventional wisdom in the field and opens up many new paths for research. However, there are a few (in my opinion) critical concerns that currently bar me from strongly recommending acceptance of the paper. I outline these below. 1. Prior work: the paper seems to ignore a plethora of prior work around studying adversarial robustness and understanding its roots. For example, a few very closely related works are as follows: - Adversarial examples are not Bugs, they are Features (https://arxiv.org/abs/1905.02175): Ilyas et al (2019) demonstrate that adversarial perturbations are not in meaningless directions with respect to the data distribution, and in fact a classifier can be recovered from a labeled dataset of adversarial examples. While not in conflict with this work, it does closely relate and discuss many of the same issues discussed in this work, so relating them would be fruitful. - A Discussion of Adversarial Examples are not Bugs they are Features (https://distill.pub/2019/advex-bugs-discussion/): Nakkiran (2019) actually constructs a dataset (called adversarial squares) where the Bayes-optimal classifier is robust but neural networks learn a non-robust classifier due to label noise and overfitting. Interestingly, they also construct a dataset where they Bayes-optimal classifier is robust and neural networks *do* learn a robust classifier (adversarial squares sans label noise). While I think the datasets presented in this work are much more interesting and certainly more realistic, this work should be put in context. - Excessive Invariance causes Adversarial Vulnerability (https://arxiv.org/abs/1811.00401v3): Jacobsen et al offers an explanation for adversarial examples based on the fact that NNs are not sensitive to many task-relevant changes in inputs, which seems to tie in nicely to the discussion in this paper, as under the presented setup the Bayes-optimal classifier will certainly exploit (and be somewhat sensitive) to such changes. - Adversarially robust generalization requires more data (https://arxiv.org/abs/1804.11285): Schmidt et al show a setup where many more samples are required for adversarial robustness than for standard classification error. And it seems to have very relevant connections to your work. - In general this list is not comprehensive either: there are many relevant connections to the robustness-accuracy tradeoff (https://arxiv.org/abs/1901.08573, https://arxiv.org/abs/1805.12152), and other works. 2. Discussion/interpretation of the results: - Sufficient vs necessary: While the experimental design and results are both of very high quality, I am slightly confused about the interpretation of the results. First, if my understanding of the paper is correct, the experiments show that (a) the Bayes-optimal classifier can be non-robust in real-world settings, and (b) even when the Bayes-optimal classifier is robust, NNs can learn a non-robust decision boundary. In particular, (b) indicates that it may be *necessary* to design regularization methods that steer NNs towards the correct decision boundary\u2014it says nothing about whether these regularization methods will be *sufficient*, which the paper seems to suggest, e.g. in the abstract \"our results suggest that adversarial vulnerability is not an unavoidable consequence of machine learning in high dimensions, and may often be a result of suboptimal training methods used in current practice.\" In fact, if real-world datasets end up being like the asymmetric dataset, then the results of this paper would actually indicate the *opposite* of the above statement. It is unclear on what basis one can say that real-world datasets are more like the symmetric case or the asymmetric case. I believe a more measured conclusion (perhaps that we *need* more regularization methods, but even then we may not be able to get perfect robustness and accuracy) would better fit the strong results presented in the paper. - CNN vs Linear SVM: I am confused about why we would expect a CNN to be able to learn the Bayes-optimal decision boundary but not the Linear SVM. The paper justifies the adversarial vulnerability of the Linear SVM by arguing that the Bayes-optimal classifier is not in the Linear SVM hypothesis class, which makes sense. The RBF SVM, for small enough bandwidth can express any function and is convex, so no argument needs to be made about its ability to find the Bayes-optimal classifier. For CNNs, however, it is unclear if the Bayes-optimal classifier lies in the hypothesis class (there are \"universal approximation\" arguments but these usually require arbitrarily wide networks and are non-constructive)\u2014couldn't it be that the CNNs used here is in the same boat as the Linear SVM (i.e. the Bayes-optimal decision boundary is not expressible by the CNN?) 3. Experimental setup: - One somewhat concerning (but perhaps unavoidable) thing about the experimental setup is that all the considered datasets are not perfectly linearly separable, i.e. the Bayes-optimal classifier has non-zero test error in expectation, and moreover the data variance is full-rank in the embedded space. This is in stark contrast to real datasets, where there seem to be many different ways to perfectly separate say, dogs from cats, and the variance of the data seems to be very heavily concentrated in a small subset of directions. I am concerned that these properties are what drive the Bayes-optimal classifier for the symmetric dataset to be robust (concretely, if 0.01 * Identity was not added to the covariance matrix of the symmetric model and the covariance was left to be low-rank, then any classifier which was Bayes-optimal along the positive-variance directions would be Bayes-optimal, and could behave arbitrarily poorly along the zero-variance directions, still being vulnerable). This concern does not make the contribution of the symmetric dataset less valuable, but a discussion of such caveats would help further elucidate the similarities and differences of this setup from real datasets. - It is unclear if what is lacking from the NN is explicit regularization, or just more data. In particular, with such low-variance directions, at standard dataset sizes the distributions generated here are most likely statistically indistinguishable from their robust/non-robust counterparts (you can see hints of this in the fact that the CNN gets . While completely alleviating this concern may once again be quite difficult/impossible, it could be significantly alleviated by generating training samples dynamically (at every iteration) instead of generating a dataset in one shot and training on it. It would be very interesting to see whether these results differ at all from the one-shot approach here. 4. A suggestion rather than a concern and not impacting my current score: but it would be very interesting to see what happens for robustly trained classifiers on the symmetric and asymmetric datasets. Overall, this paper is a very promising step in studying adversarial robustness, but concerns about discussion of prior work, discussion of experimental setup, and conclusions drawn, currently bar me from recommending acceptance. I would be more than happy to significantly improve my score if these concerns can be addressed in the revision and corresponding rebuttal.", "rating": "6: Weak Accept", "reply_text": "Thank you for your review . It is a pleasure to read such a detailed and constructive review . We have updated the paper to reflect your suggested improvements . Specifically : Prior Work . We agree that all these related works are relevant and not in conflict with our results , and have updated the text to include them . Sufficient vs . Necessary . We agree with the comment and have updated the text to more accurately reflect our view that from the Bayes-Optimal perspective there can be two different causes for adversarial vulnerability : asymmetries in the dataset or suboptimal learning . CNN vs Linear SVM . To check whether the failure of CNNs to learn a robust classifier is due to a lack of expressive power , we systematically increased the number of channels in the model and measured mean L2 as a function of the expressiveness . The results ( shown in figure 8 ) do not show any increase in robustness as expressive power grows . We believe that these experiments , together with the well-known results on the \u201c universal approximation \u201d property of CNNs , strongly suggest that the problem with CNNs is not that they can not approximate a robust classifier for this task . In particular , note that the RBF robust classifier is a smooth function of the input and the rate of approximation in the universal approximation literature depends on the smoothness . CNNs have been shown to be capable of expressing random functions of more than one million images ( Zhang et al.2017 ) , so expressing a smooth function , such as the RBF classifier or the Bayes-Optimal classifier , seems perfectly reasonable . Similarities between synthetic datasets and real datasets . We do believe that the synthetic datasets capture much of the variability that appears in the real datasets . An additional experiment we performed ( now in the appendix ) is to train a CNN on the synthetic data and measure its performance on the real data . The results show that training on synthetic data , generalizes reasonably well to the real data ( with approximately 5 % drop in accuracy ) . While we agree that there are many ways to perfectly discriminate a finite dataset of dogs and cats , this is also true for a finite sample from the synthetic dataset . Increasing the amount of training examples . Following your suggestion , we conducted additional experiments where the data samples were generated dynamically from the MFA model , and measured robustness as a function of the number of examples ( figure 8 ) . We did not see any significant improvement in robustness , even when we went as far as 1 Million training examples for a binary classification task . Note also that the RBF SVM did learn a robust classifier with as little as 10,000 examples for the same binary tasks . Robust training . We included an additional experiment where we used robust training for the CNN ( We used the TRADES method , from Zhang et al.2019 , which was the winner in a recent adversarial training challenge ) . As shown in the appendix , this gave only a modest improvement in robustness . Still far from the robustness of the Bayes-Optimal classifier ( or the RBF SVM ) . There are of course many different variants of robust training , but this result shows that the problem of effectively optimizing for both accuracy and robustness is much more difficult in CNNs compared to shallow architectures ."}, {"review_id": "H1l3s6NtvH-2", "review_text": "The paper studies the adversarial robustness of the Bayes-optimal classifier (i.e., optimal for the standard \"benign\" risk). To do so, the authors construct various synthetic distributions and show that in some cases the Bayes-optimal classifier is also adversarially robust, while in other cases it is not. In the main experiment, the authors construct two high-dimensional synthetic distributions of human faces via a generative model. In one of the distributions, even the Bayes-optimal classifier is vulnerable to adversarial examples. In the other distribution (where the Bayes-optimal classifier is robust), CNNs do not achieve high robustness while other approaches such as an RBF SVM are more robust. Overall I find the experiment in the paper interesting, but it is unclear how representative the experiments are for adversarial robustness on real data. There are natural follow-up experiments that would shed some light on this question and could substantially strengthen the paper. Hence I unfortunately recommend to reject the paper at this point and encourage the authors to deepen their experimental investigation. For instance, the following points would be relevant: - Does adversarial training / robust optimization result in a robust neural network on the synthetic data distribution where the Bayes-optimal classifier is robust (and the RBF SVM is more robust than a CNN)? - Do RBF SVMs also exhibit higher adversarial robustness than CNNs on comparable real datasets? This would indicate to what extent the synthetic distributions are representative of real data w.r.t. adversarial robustness. Additional comments: - Briefly defining the MFA model in the main text would provide helpful context. - A few more details about the experiments could be informative in the main text, e.g., the CNN architecture and the accuracies the various methods achieve. - An end-of-proof symbol at the end of proofs would be helpful to the reader. - Is there an index i missing in \\pi in Equation (8)? - Is the probability given in (9) exact? Gaussians are supported on all of R^d, so even a Gaussian component far away will contribute to the probability of a point under p_1, at least a (very) small amount. - The proof of Observation 2 is more a sketch. It would be good to include a more formal proof in the appendix.", "rating": "3: Weak Reject", "reply_text": "Thank you for your review . We have uploaded a new version with additional experiments based on your suggestions . We hope you take a second look at the paper and see if it changes your recommendation . Regarding your major comments : Adversarial training . We included an additional experiment where we used robust training for the CNN ( We used the TRADES method , from Zhang et al.2019 , which was the winner in a recent adversarial training challenge ) . As shown in the appendix , this gave only a modest increase in robustness . Still far from the robustness of the Bayes-Optimal classifier ( or the RBF SVM ) . There are of course many different variants of robust training , but this result shows that the problem of effectively optimizing for both accuracy and robustness is much more difficult in CNNs compared to shallow architectures . Similarities between synthetic datasets and real datasets . We do believe that the synthetic datasets capture much of the variability that appears in the real datasets . An additional experiment we performed ( now in the appendix ) is to train a CNN on the synthetic data and measure its performance on the real data . The results show that training on synthetic data , generalizes reasonably well to the real data ( with approximately 5 % drop in accuracy ) . We focused on experiments with synthetic datasets because they allow us to systematically explore the different possible causes of adversarial examples ( for example , having a robust optimal classifier indicates that the cause of the vulnerability of a trained model does not lie in the data ) . While we agree that the robustness of RBF SVMs on real data is interesting to pursue , this is not the main focus of our work , which aims to disentangle the different causes of adversarial vulnerability . We have also changed the manuscript in light of your \u201c additional comments \u201d ( e.g.we added the missing index i in equation 8 ) . Regarding equation 9 , if $ g_ { ik } ( x ) $ is a delta function ( as is assumed above equation 9 ) then equation 9 is exact ( see e.g.Neal and Hinton 98 ) ."}], "0": {"review_id": "H1l3s6NtvH-0", "review_text": "The paper analyzed the adversarial examples from the Bayes-optimal view. Specifically, the authors analyzed the relationship between the symmetry of covariance of data distribution and the amount of data which are close to the decision boundary. The authors proved that when the covariance of data distribution is asymmetric, a large amount of data will be close to the decision boundary (easy to be attacked). The authors also provided the new datasets which is easy to compute for the bayes-optimal classifier so as to verify the effect of symmetry of covariance on vulnerability of classifier. Moreover, the paper indicated that the vulnerability of CNNs is due to asymmetric distributions or non-optimal learning. It is interesting that the paper investigated the adversarial examples from the Bayes-optimal view. However, there are some drawbacks: 1. The motivation of this paper is not clear to me. In other words, what is the benefit of analyzing the adversarial examples from the Bayes-optimal viewpoint, since Bayes model mentioned in this paper is easy to attack. I am not fully convinced by the presentation of the paper. 2. The theorem or the observation in the paper appears too straightforward. And the \u2018observation 1\u2019is not general. The authors may need to consider more general cases that when the standard deviation of eigen value of covariance matrix is large, the Bayes model will be easily attacked. (not just the case that one of eigen value is zero). 3. One minor point, it appears somewhat strange that \u201cobservations\u201d were proved. It is better to change observations to theorems or lemmas. 4. The authors tried to explain directly the vulnerability of CNN in a same way. However, CNN is a totally different model compared with the Bayes model (one is a discriminative model and the other is a generative model). For generative models, the classification boundary is closely related to all training samples. Therefore, the variance of data distribution is important for attack. For discriminative models, the decision boundary is related to local information. It may not be proper to analyze CNN in the way same as the Bayes model. This should be further clarified and discussed. ", "rating": "1: Reject", "reply_text": "Thank you for reading our paper . We hope we can clarify ( below ) some misunderstandings and hope that you will read it again in the light of our comments . \u201c What is the benefit of analyzing the adversarial examples from the Bayes-Optimal viewpoint , since Bayes model mentioned in this paper is easy to attack. \u201d Actually , as we show in the paper , the Bayes-Optimal classifier may be easy or hard to attack depending on the data distribution . More generally , adversarial examples present an intriguing and complex phenomena , and we believe that analyzing them from the Bayes-Optimal perspective allows us to disentangle the different possible causes . We didn \u2019 t fully understand the comment that \u201c observation 1 is not general \u201d . Specifically , we do not require zero eigenvalue for the vulnerability condition to occur ( but asymmetry in the variance ) . \u201c CNN is a totally different model compared with the Bayes model ( one is a discriminative model and the other is a generative model ) \u201d . Actually the Bayes-Optimal model ( as its name suggests ) gives the best possible accuracy in the discrimination task . Thus if the CNN wants to optimize the accuracy , then it should agree with the Bayes-Optimal model ."}, "1": {"review_id": "H1l3s6NtvH-1", "review_text": "This paper proposes studying adversarial examples from the perspective of Bayes-optimal classifiers. They construct a pair of synthetic but somewhat realistic datasets\u2014in one case, the Bayes-optimal classifier is *not* robust, demonstrating that the Bayes-optimal classifier may not be robust for real-world datasets. In the other case, the Bayes-optimal classifier is robust, but neural networks fail to learn the robust decision boundary. This demonstrates that even when the Bayes-optimal classifier is robust, we may need to explicitly regularize/incentivize neural networks to learn the correct decision boundary. The contribution of the two datasets (the symmetric and asymetric CelebA) is, in my opinion, an extremely important contribution in studying adversarial robustness and on their own these datasets warrant further study. Previously, all studies of this sort had to be done with small-scale classifiers and simplistic datasets such as Gaussians. The paper also definitively proves that there are realistic datasets where the Bayes-optimal classifier is non-robust, which goes against quite a bit of conventional wisdom in the field and opens up many new paths for research. However, there are a few (in my opinion) critical concerns that currently bar me from strongly recommending acceptance of the paper. I outline these below. 1. Prior work: the paper seems to ignore a plethora of prior work around studying adversarial robustness and understanding its roots. For example, a few very closely related works are as follows: - Adversarial examples are not Bugs, they are Features (https://arxiv.org/abs/1905.02175): Ilyas et al (2019) demonstrate that adversarial perturbations are not in meaningless directions with respect to the data distribution, and in fact a classifier can be recovered from a labeled dataset of adversarial examples. While not in conflict with this work, it does closely relate and discuss many of the same issues discussed in this work, so relating them would be fruitful. - A Discussion of Adversarial Examples are not Bugs they are Features (https://distill.pub/2019/advex-bugs-discussion/): Nakkiran (2019) actually constructs a dataset (called adversarial squares) where the Bayes-optimal classifier is robust but neural networks learn a non-robust classifier due to label noise and overfitting. Interestingly, they also construct a dataset where they Bayes-optimal classifier is robust and neural networks *do* learn a robust classifier (adversarial squares sans label noise). While I think the datasets presented in this work are much more interesting and certainly more realistic, this work should be put in context. - Excessive Invariance causes Adversarial Vulnerability (https://arxiv.org/abs/1811.00401v3): Jacobsen et al offers an explanation for adversarial examples based on the fact that NNs are not sensitive to many task-relevant changes in inputs, which seems to tie in nicely to the discussion in this paper, as under the presented setup the Bayes-optimal classifier will certainly exploit (and be somewhat sensitive) to such changes. - Adversarially robust generalization requires more data (https://arxiv.org/abs/1804.11285): Schmidt et al show a setup where many more samples are required for adversarial robustness than for standard classification error. And it seems to have very relevant connections to your work. - In general this list is not comprehensive either: there are many relevant connections to the robustness-accuracy tradeoff (https://arxiv.org/abs/1901.08573, https://arxiv.org/abs/1805.12152), and other works. 2. Discussion/interpretation of the results: - Sufficient vs necessary: While the experimental design and results are both of very high quality, I am slightly confused about the interpretation of the results. First, if my understanding of the paper is correct, the experiments show that (a) the Bayes-optimal classifier can be non-robust in real-world settings, and (b) even when the Bayes-optimal classifier is robust, NNs can learn a non-robust decision boundary. In particular, (b) indicates that it may be *necessary* to design regularization methods that steer NNs towards the correct decision boundary\u2014it says nothing about whether these regularization methods will be *sufficient*, which the paper seems to suggest, e.g. in the abstract \"our results suggest that adversarial vulnerability is not an unavoidable consequence of machine learning in high dimensions, and may often be a result of suboptimal training methods used in current practice.\" In fact, if real-world datasets end up being like the asymmetric dataset, then the results of this paper would actually indicate the *opposite* of the above statement. It is unclear on what basis one can say that real-world datasets are more like the symmetric case or the asymmetric case. I believe a more measured conclusion (perhaps that we *need* more regularization methods, but even then we may not be able to get perfect robustness and accuracy) would better fit the strong results presented in the paper. - CNN vs Linear SVM: I am confused about why we would expect a CNN to be able to learn the Bayes-optimal decision boundary but not the Linear SVM. The paper justifies the adversarial vulnerability of the Linear SVM by arguing that the Bayes-optimal classifier is not in the Linear SVM hypothesis class, which makes sense. The RBF SVM, for small enough bandwidth can express any function and is convex, so no argument needs to be made about its ability to find the Bayes-optimal classifier. For CNNs, however, it is unclear if the Bayes-optimal classifier lies in the hypothesis class (there are \"universal approximation\" arguments but these usually require arbitrarily wide networks and are non-constructive)\u2014couldn't it be that the CNNs used here is in the same boat as the Linear SVM (i.e. the Bayes-optimal decision boundary is not expressible by the CNN?) 3. Experimental setup: - One somewhat concerning (but perhaps unavoidable) thing about the experimental setup is that all the considered datasets are not perfectly linearly separable, i.e. the Bayes-optimal classifier has non-zero test error in expectation, and moreover the data variance is full-rank in the embedded space. This is in stark contrast to real datasets, where there seem to be many different ways to perfectly separate say, dogs from cats, and the variance of the data seems to be very heavily concentrated in a small subset of directions. I am concerned that these properties are what drive the Bayes-optimal classifier for the symmetric dataset to be robust (concretely, if 0.01 * Identity was not added to the covariance matrix of the symmetric model and the covariance was left to be low-rank, then any classifier which was Bayes-optimal along the positive-variance directions would be Bayes-optimal, and could behave arbitrarily poorly along the zero-variance directions, still being vulnerable). This concern does not make the contribution of the symmetric dataset less valuable, but a discussion of such caveats would help further elucidate the similarities and differences of this setup from real datasets. - It is unclear if what is lacking from the NN is explicit regularization, or just more data. In particular, with such low-variance directions, at standard dataset sizes the distributions generated here are most likely statistically indistinguishable from their robust/non-robust counterparts (you can see hints of this in the fact that the CNN gets . While completely alleviating this concern may once again be quite difficult/impossible, it could be significantly alleviated by generating training samples dynamically (at every iteration) instead of generating a dataset in one shot and training on it. It would be very interesting to see whether these results differ at all from the one-shot approach here. 4. A suggestion rather than a concern and not impacting my current score: but it would be very interesting to see what happens for robustly trained classifiers on the symmetric and asymmetric datasets. Overall, this paper is a very promising step in studying adversarial robustness, but concerns about discussion of prior work, discussion of experimental setup, and conclusions drawn, currently bar me from recommending acceptance. I would be more than happy to significantly improve my score if these concerns can be addressed in the revision and corresponding rebuttal.", "rating": "6: Weak Accept", "reply_text": "Thank you for your review . It is a pleasure to read such a detailed and constructive review . We have updated the paper to reflect your suggested improvements . Specifically : Prior Work . We agree that all these related works are relevant and not in conflict with our results , and have updated the text to include them . Sufficient vs . Necessary . We agree with the comment and have updated the text to more accurately reflect our view that from the Bayes-Optimal perspective there can be two different causes for adversarial vulnerability : asymmetries in the dataset or suboptimal learning . CNN vs Linear SVM . To check whether the failure of CNNs to learn a robust classifier is due to a lack of expressive power , we systematically increased the number of channels in the model and measured mean L2 as a function of the expressiveness . The results ( shown in figure 8 ) do not show any increase in robustness as expressive power grows . We believe that these experiments , together with the well-known results on the \u201c universal approximation \u201d property of CNNs , strongly suggest that the problem with CNNs is not that they can not approximate a robust classifier for this task . In particular , note that the RBF robust classifier is a smooth function of the input and the rate of approximation in the universal approximation literature depends on the smoothness . CNNs have been shown to be capable of expressing random functions of more than one million images ( Zhang et al.2017 ) , so expressing a smooth function , such as the RBF classifier or the Bayes-Optimal classifier , seems perfectly reasonable . Similarities between synthetic datasets and real datasets . We do believe that the synthetic datasets capture much of the variability that appears in the real datasets . An additional experiment we performed ( now in the appendix ) is to train a CNN on the synthetic data and measure its performance on the real data . The results show that training on synthetic data , generalizes reasonably well to the real data ( with approximately 5 % drop in accuracy ) . While we agree that there are many ways to perfectly discriminate a finite dataset of dogs and cats , this is also true for a finite sample from the synthetic dataset . Increasing the amount of training examples . Following your suggestion , we conducted additional experiments where the data samples were generated dynamically from the MFA model , and measured robustness as a function of the number of examples ( figure 8 ) . We did not see any significant improvement in robustness , even when we went as far as 1 Million training examples for a binary classification task . Note also that the RBF SVM did learn a robust classifier with as little as 10,000 examples for the same binary tasks . Robust training . We included an additional experiment where we used robust training for the CNN ( We used the TRADES method , from Zhang et al.2019 , which was the winner in a recent adversarial training challenge ) . As shown in the appendix , this gave only a modest improvement in robustness . Still far from the robustness of the Bayes-Optimal classifier ( or the RBF SVM ) . There are of course many different variants of robust training , but this result shows that the problem of effectively optimizing for both accuracy and robustness is much more difficult in CNNs compared to shallow architectures ."}, "2": {"review_id": "H1l3s6NtvH-2", "review_text": "The paper studies the adversarial robustness of the Bayes-optimal classifier (i.e., optimal for the standard \"benign\" risk). To do so, the authors construct various synthetic distributions and show that in some cases the Bayes-optimal classifier is also adversarially robust, while in other cases it is not. In the main experiment, the authors construct two high-dimensional synthetic distributions of human faces via a generative model. In one of the distributions, even the Bayes-optimal classifier is vulnerable to adversarial examples. In the other distribution (where the Bayes-optimal classifier is robust), CNNs do not achieve high robustness while other approaches such as an RBF SVM are more robust. Overall I find the experiment in the paper interesting, but it is unclear how representative the experiments are for adversarial robustness on real data. There are natural follow-up experiments that would shed some light on this question and could substantially strengthen the paper. Hence I unfortunately recommend to reject the paper at this point and encourage the authors to deepen their experimental investigation. For instance, the following points would be relevant: - Does adversarial training / robust optimization result in a robust neural network on the synthetic data distribution where the Bayes-optimal classifier is robust (and the RBF SVM is more robust than a CNN)? - Do RBF SVMs also exhibit higher adversarial robustness than CNNs on comparable real datasets? This would indicate to what extent the synthetic distributions are representative of real data w.r.t. adversarial robustness. Additional comments: - Briefly defining the MFA model in the main text would provide helpful context. - A few more details about the experiments could be informative in the main text, e.g., the CNN architecture and the accuracies the various methods achieve. - An end-of-proof symbol at the end of proofs would be helpful to the reader. - Is there an index i missing in \\pi in Equation (8)? - Is the probability given in (9) exact? Gaussians are supported on all of R^d, so even a Gaussian component far away will contribute to the probability of a point under p_1, at least a (very) small amount. - The proof of Observation 2 is more a sketch. It would be good to include a more formal proof in the appendix.", "rating": "3: Weak Reject", "reply_text": "Thank you for your review . We have uploaded a new version with additional experiments based on your suggestions . We hope you take a second look at the paper and see if it changes your recommendation . Regarding your major comments : Adversarial training . We included an additional experiment where we used robust training for the CNN ( We used the TRADES method , from Zhang et al.2019 , which was the winner in a recent adversarial training challenge ) . As shown in the appendix , this gave only a modest increase in robustness . Still far from the robustness of the Bayes-Optimal classifier ( or the RBF SVM ) . There are of course many different variants of robust training , but this result shows that the problem of effectively optimizing for both accuracy and robustness is much more difficult in CNNs compared to shallow architectures . Similarities between synthetic datasets and real datasets . We do believe that the synthetic datasets capture much of the variability that appears in the real datasets . An additional experiment we performed ( now in the appendix ) is to train a CNN on the synthetic data and measure its performance on the real data . The results show that training on synthetic data , generalizes reasonably well to the real data ( with approximately 5 % drop in accuracy ) . We focused on experiments with synthetic datasets because they allow us to systematically explore the different possible causes of adversarial examples ( for example , having a robust optimal classifier indicates that the cause of the vulnerability of a trained model does not lie in the data ) . While we agree that the robustness of RBF SVMs on real data is interesting to pursue , this is not the main focus of our work , which aims to disentangle the different causes of adversarial vulnerability . We have also changed the manuscript in light of your \u201c additional comments \u201d ( e.g.we added the missing index i in equation 8 ) . Regarding equation 9 , if $ g_ { ik } ( x ) $ is a delta function ( as is assumed above equation 9 ) then equation 9 is exact ( see e.g.Neal and Hinton 98 ) ."}}