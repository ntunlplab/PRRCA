{"year": "2017", "forum": "HyecJGP5ge", "title": "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD", "decision": "Reject", "meta_review": "This paper combines simple heuristics to adapt the size of a dictionary during learning. The heuristics are intuitive: augmenting the dictionary size when correlation between reconstruction and inputs falls below a certain pre-determined threshold, reducing the dictionary size by adding a group-sparsity L1,2 penalty on the overall dictionary (the L1,2 penalty for pruning models had appeared elsewhere before, as the authors acknowledge).\n The topic of online adjustment of model complexity is an important one, and work like this is an interesting and welcome direction. The simplicity of the heuristics presented here would be appealing if there was more empirical support to demonstrate the usefulness of the proposed adaptation. However, the empirical validation falls short here:\n - the claim that this work offers more than existing off-line model adaptation methods because of its online setting is the crux of the value of the paper, but the experimental validation does not offer much in terms of how to adjust hyperparameters to a truly nonstationary setting. Here, the hyperparameters are set rather arbitrarily, and the experimental setting is a single switch between two datasets (where off-line methods would do well), so this obscures the challenges that would be presented by a less artificial level of non-stationarity\n - an extensive set of experiments is presented, but all these experiments are confined in a strange experimental setting that is divorced from any practical metrics: the \"state-of-the-art\" method of reference is Mairal et al 2009 which was focused on speeding learning of dictionaries. Here, the metrics offered are correlation / reconstruction error, and classification, over full images instead of patches as is usually done for large images, without justification. This unnecessarily puts the work in a vacuum in terms of evaluation and comparison to existing art. In fact, the reconstructions in the appendix Fig. 17 and 18 look pretty bad, and the classification performance 1) does not demonstrate superiority of the method over standard dictionary learning (both lines are within the error bars and virtually indistinguishable if using more than 60 elements, which is not much), 2) performance overall is pretty bad for a 2-class discrimination task, again because of the unusual setting of using full images.\n In summary, this paper could be a very nice paper if the ideas were validated in a way that shows true usefulness and true robustness to the challenges of realistic nonstationarity, but this is unfortunately not the case here.\n Ps: two recent papers that could be worth mentioning in terms of model adaptation are diversity networks for pruning neurons (Diversity Networks, Mariet and Sra ICLR 2016, https://arxiv.org/abs/1511.05077), and augmenting neural networks with extra parameters while retaining previous learning (Progressive Neural Networks, Rusu et al 2016, https://arxiv.org/pdf/1606.04671v3.pdf).", "reviews": [{"review_id": "HyecJGP5ge-0", "review_text": "The authors propose a simple modification of online dictionary learning: inspired by neurogenesis, they propose to add steps of atom addition, or atom deletion, in order to extent the online dictionary learning algorithm algorithm of Mairal et al. Such extensions helps to adapt the dictionary to changing properties of the data. The online adaptation is very interesting, even if it is quite simple. The overall algorithm is quite reasonable, but not always described in sufficient details: for example, the thresholds or conditions for neuronal birth or death are not supported by a strong analysis, even if the resulting algorithm seems to perform well on quite extensive experiments. The overall idea is nevertheless interesting (even if not completely new), and the paper generally well written and pretty easy to follow. The analysis is however quite minimal: it could have been interesting to study the evolving properties of the dictionary, to analyse its accuracy for following the changes in the data, etc. Still: this is a nice work! ", "rating": "7: Good paper, accept", "reply_text": "We would like to thank the reviewer for useful comments and constructive suggestions . In response to the comment on insufficient details of the algorithm , we added a new section 3.1 fully devoted to discussing such details . However , as suggested by the reviewer , a further analysis of the algorithm , involving the evolving properties of the dictionary and performance changes due to changes in the data , remain the directions for future work ."}, {"review_id": "HyecJGP5ge-1", "review_text": "The paper is interesting, it relates findings from neurscience and biology to a method for sparse coding that is adaptive and able to automatically generate (or even delete) codes as new data is coming, from a nonstationary distribution. I have a few points to make: 1. the algorithm could be discussed more, to give a more solid view of the contribution. The technique is not novel in spirit. Codes are added when they are needed, and removed when they dont do much. 2. Is there a way to relate the organization of the data to the behavior of this method? In this paper, buildings are shown first, and natural images (which are less structured, more difficult) later. Is this just a way to perform curriculum learning? What happens when data simply changes in structure, with no apparent movement from simple to more complex (e.g. from flowers, to birds, to fish, to leaves, to trees etc) In a way, it makes sense to see an improvement when the training data has such a structure, by going from something artificial and simpler to a more complex, less structured domain. The paper is interesting, the idea useful with some interesting insights. I am not sure it is ready for publication yet. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the insightful comments . Please consider our response to the review as below . 1 . `` the algorithm could be discussed more , to give a more solid view of the contribution '' A new section ( 3.1 ) was added to provide a detailed discussion . 2 . `` The technique is not novel in spirit '' Please see the item 4 in the summary above . We are convinced that our approach is actually quite novel , in the context of online dictionary learning or more general autoencoders . Clearly , at a very general level , model selection is not a novel idea , and at the same level of generality , the ideas of adding or deleting variables during model selection may have appeared earlier in very different contexts ( e.g. , in learning Bayesian network structure , or in LASSO solved by active set methods such as LARS , just to name a few ) . We are not claiming the novelty at such extremely general level , but the dynamic addition/deletion ONLINE model selection in dictionary learning setting is a completely novel approach . As discussed in item 4 above , not only is our approach quite different from the corresponding related work , our key contribution is actually empirical and theoretical evaluation of situations/conditions which make such online model selection beneficial for dictionary learning \u2013 the topic never explored in the sparse coding literature , to the best of our knowledge . We also added a bullet-point list of our contributions at the end of the introduction paragraph in the paper . If the novelty of the proposed approach still does not seem convincing , we would really appreciate specific references to prior work which ( 1 ) implements birth/death of hidden units in an autoencoder model , in an online setting , outperforming state-of-art in nonstationary settings and ( 2 ) provides both empirical and theoretical evaluation of conditions under which adaptive approach is beneficial vs non-adaptive . 3 . `` Is there a way to relate the organization of the data to the behavior of this method ? In this paper , buildings are shown first , and natural images ( which are less structured , more difficult ) later . Is this just a way to perform curriculum learning ? What happens when data simply changes in structure , with no apparent movement from simple to more complex ( e.g.from flowers , to birds , to fish , to leaves , to trees etc . ) In a way , it makes sense to see an improvement when the training data has such a structure , by going from something artificial and simpler to a more complex , less structured domain . '' The objective here was to explore ANY nonstationary conditions , not necessarily the `` curriculum learning '' setting which indeed would imply going from simpler to more complex concepts . We are not convinced that the natural images are necessarily `` more difficult '' that buildings ( e.g. , a complicated city scene with multiple buildings can be seen , vice versa , more 'complex ' than a flower , as in some of those images ) . In fact , we performed additional experiments , added to the Appendix , section B.8 , Figure 20 , where we tried all possible permutations of the input datasets ( buildings , flowers , animals ) . The results were very similar to the ones presented in the main paper when we flipped the order of input data ( e.g. , with flowers followed by building , or flowers followed by animals , our methods showed similar magnitude of improvements over the baseline method as it did for buildings followed by flowers or by animals ) . Overall , our general observation is that there is no processing order on datasets where baseline ( ODL ) could perform better than the proposed approach . We see a significant advantage of NODL over ODL if we have Oxford or Flowers data sets as the first domain data . However , this advantage is smaller when using the Animals dataset first , followed by flowers or buildings , so , empirically , it turns out that perhaps animal images were more 'complex ' , and then flowers and buildings looked 'easier ' to learn afterwards , but this is just a speculation at this point . In the future , it would be interesting to investigate scenarios where the complexity of the input would be measured precisely and controlled for , e.g. , using the compressed size of an image as a proxy for its complexity , or some similar 'exact ' measure . 4 . `` The paper is interesting , the idea useful with some interesting insights . I am not sure it is ready for publication yet. `` Can you please make any specific suggestions regarding the steps that would make the paper ready for publication ? Thank you !"}, {"review_id": "HyecJGP5ge-2", "review_text": " I'd like to thank the authors for their detailed response and clarifications. This work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. The paper has two main innovations over the baseline approach (Mairal et al): (i) \u201cneuronal birth\u201d which represents an adaptive way of increasing the number of atoms in the dictionary (ii) \"neuronal death\", which corresponds to removing \u201cuseless\u201d dictionary atoms. Neural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size. I believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature. The paper is very well written and easy to follow. On the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting. The authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance, Ramirez, Ignacio, and Guillermo Sapiro. \"An MDL framework for sparse coding and dictionary learning.\" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the insightful comments . Please consider our response to the review as below . 1 . `` On the other hand , the overall technique is not very novel . Although not exactly equivalent , similar ideas have been explored '' Please see the discussion above in response to the Reviewer1 , item 4 in the summary above , as well as a paragraph at the end of the introduction which hopefully emphasizes more clearly the novelty of the proposed approach . As discussed above , at some level of generality , the ideas of addition or deletion of variables have been explored , but never in online dictionary learning context . We thank the reviewer for the reference on MDL approach to model selection in sparse coding , in offline setting ; we cited and discussed this reference in the introduction , however , our approach is quite different as it involves an online setting and assumes nonstationary inputs . If our arguments concerning novelty of our approach are still not sufficiently convincing , we would really appreciate any specific references on prior art proposing an online adaptive model-selection within any autoencoder setting ( sparse dictionary or beyond ) , based on hidden units birth/death dynamics , with a thorough evaluation , empirical and theoretical , of situations where such adaptive approach is beneficial ( or not ) as compared vs non-adaptive baseline . We were unable to find any such prior work . 2 . `` the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data . Which depending on the `` level '' of non-stationarity in the incoming data ( or presence of outliers ) could be difficult to set . '' This is a good point ; as usual , we end up having a tunable parameter controlling the birth rate ( here , a threshold on the representation error measured as 1 - correlation between actual and reconstructed input ) , and must resort to cross-validation to set it up , or potentially provide another layer of automation in order to tune this parameter online ( e.g. , an RL approach to parameter tuning in our method ) . However , we view this as a direction for future work , since the automated parameter tuning is a sufficiently involved procedure on its own . Regarding the earlier question about reinitialization of dead elements in the Mairal 's original algorithm : we added new experiments in the Appendix , section B.7 , Figure 19 . First of all , we noticed that the fraction of \u201c dead \u201d elements was typically very small in our experiments when using the original ODL method of Mairal et al . ( 2009 ) .We next followed the idea of reinitialization of dead elements in ODL , referring to this modification as \u201c ODL * \u201d in Figure 19 . Specifically , we reinitialize the \u201c dead \u201d elements with random initializations , and then relearn the reinitialized elements , along with other elements , on the current batch of data . Fig.19 extends Fig.2 with additional plots for the baseline extension \u201c ODL * \u201d of ODL ; keeping the experiment settings same . We see that there is negligible difference in the performance of ODL and its extension \u201c ODL * \u201d , and our method NODL maintains its superior performance w.r.t.ODL as well as \u201c ODL * \u201d ."}], "0": {"review_id": "HyecJGP5ge-0", "review_text": "The authors propose a simple modification of online dictionary learning: inspired by neurogenesis, they propose to add steps of atom addition, or atom deletion, in order to extent the online dictionary learning algorithm algorithm of Mairal et al. Such extensions helps to adapt the dictionary to changing properties of the data. The online adaptation is very interesting, even if it is quite simple. The overall algorithm is quite reasonable, but not always described in sufficient details: for example, the thresholds or conditions for neuronal birth or death are not supported by a strong analysis, even if the resulting algorithm seems to perform well on quite extensive experiments. The overall idea is nevertheless interesting (even if not completely new), and the paper generally well written and pretty easy to follow. The analysis is however quite minimal: it could have been interesting to study the evolving properties of the dictionary, to analyse its accuracy for following the changes in the data, etc. Still: this is a nice work! ", "rating": "7: Good paper, accept", "reply_text": "We would like to thank the reviewer for useful comments and constructive suggestions . In response to the comment on insufficient details of the algorithm , we added a new section 3.1 fully devoted to discussing such details . However , as suggested by the reviewer , a further analysis of the algorithm , involving the evolving properties of the dictionary and performance changes due to changes in the data , remain the directions for future work ."}, "1": {"review_id": "HyecJGP5ge-1", "review_text": "The paper is interesting, it relates findings from neurscience and biology to a method for sparse coding that is adaptive and able to automatically generate (or even delete) codes as new data is coming, from a nonstationary distribution. I have a few points to make: 1. the algorithm could be discussed more, to give a more solid view of the contribution. The technique is not novel in spirit. Codes are added when they are needed, and removed when they dont do much. 2. Is there a way to relate the organization of the data to the behavior of this method? In this paper, buildings are shown first, and natural images (which are less structured, more difficult) later. Is this just a way to perform curriculum learning? What happens when data simply changes in structure, with no apparent movement from simple to more complex (e.g. from flowers, to birds, to fish, to leaves, to trees etc) In a way, it makes sense to see an improvement when the training data has such a structure, by going from something artificial and simpler to a more complex, less structured domain. The paper is interesting, the idea useful with some interesting insights. I am not sure it is ready for publication yet. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the insightful comments . Please consider our response to the review as below . 1 . `` the algorithm could be discussed more , to give a more solid view of the contribution '' A new section ( 3.1 ) was added to provide a detailed discussion . 2 . `` The technique is not novel in spirit '' Please see the item 4 in the summary above . We are convinced that our approach is actually quite novel , in the context of online dictionary learning or more general autoencoders . Clearly , at a very general level , model selection is not a novel idea , and at the same level of generality , the ideas of adding or deleting variables during model selection may have appeared earlier in very different contexts ( e.g. , in learning Bayesian network structure , or in LASSO solved by active set methods such as LARS , just to name a few ) . We are not claiming the novelty at such extremely general level , but the dynamic addition/deletion ONLINE model selection in dictionary learning setting is a completely novel approach . As discussed in item 4 above , not only is our approach quite different from the corresponding related work , our key contribution is actually empirical and theoretical evaluation of situations/conditions which make such online model selection beneficial for dictionary learning \u2013 the topic never explored in the sparse coding literature , to the best of our knowledge . We also added a bullet-point list of our contributions at the end of the introduction paragraph in the paper . If the novelty of the proposed approach still does not seem convincing , we would really appreciate specific references to prior work which ( 1 ) implements birth/death of hidden units in an autoencoder model , in an online setting , outperforming state-of-art in nonstationary settings and ( 2 ) provides both empirical and theoretical evaluation of conditions under which adaptive approach is beneficial vs non-adaptive . 3 . `` Is there a way to relate the organization of the data to the behavior of this method ? In this paper , buildings are shown first , and natural images ( which are less structured , more difficult ) later . Is this just a way to perform curriculum learning ? What happens when data simply changes in structure , with no apparent movement from simple to more complex ( e.g.from flowers , to birds , to fish , to leaves , to trees etc . ) In a way , it makes sense to see an improvement when the training data has such a structure , by going from something artificial and simpler to a more complex , less structured domain . '' The objective here was to explore ANY nonstationary conditions , not necessarily the `` curriculum learning '' setting which indeed would imply going from simpler to more complex concepts . We are not convinced that the natural images are necessarily `` more difficult '' that buildings ( e.g. , a complicated city scene with multiple buildings can be seen , vice versa , more 'complex ' than a flower , as in some of those images ) . In fact , we performed additional experiments , added to the Appendix , section B.8 , Figure 20 , where we tried all possible permutations of the input datasets ( buildings , flowers , animals ) . The results were very similar to the ones presented in the main paper when we flipped the order of input data ( e.g. , with flowers followed by building , or flowers followed by animals , our methods showed similar magnitude of improvements over the baseline method as it did for buildings followed by flowers or by animals ) . Overall , our general observation is that there is no processing order on datasets where baseline ( ODL ) could perform better than the proposed approach . We see a significant advantage of NODL over ODL if we have Oxford or Flowers data sets as the first domain data . However , this advantage is smaller when using the Animals dataset first , followed by flowers or buildings , so , empirically , it turns out that perhaps animal images were more 'complex ' , and then flowers and buildings looked 'easier ' to learn afterwards , but this is just a speculation at this point . In the future , it would be interesting to investigate scenarios where the complexity of the input would be measured precisely and controlled for , e.g. , using the compressed size of an image as a proxy for its complexity , or some similar 'exact ' measure . 4 . `` The paper is interesting , the idea useful with some interesting insights . I am not sure it is ready for publication yet. `` Can you please make any specific suggestions regarding the steps that would make the paper ready for publication ? Thank you !"}, "2": {"review_id": "HyecJGP5ge-2", "review_text": " I'd like to thank the authors for their detailed response and clarifications. This work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. The paper has two main innovations over the baseline approach (Mairal et al): (i) \u201cneuronal birth\u201d which represents an adaptive way of increasing the number of atoms in the dictionary (ii) \"neuronal death\", which corresponds to removing \u201cuseless\u201d dictionary atoms. Neural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size. I believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature. The paper is very well written and easy to follow. On the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting. The authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance, Ramirez, Ignacio, and Guillermo Sapiro. \"An MDL framework for sparse coding and dictionary learning.\" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the insightful comments . Please consider our response to the review as below . 1 . `` On the other hand , the overall technique is not very novel . Although not exactly equivalent , similar ideas have been explored '' Please see the discussion above in response to the Reviewer1 , item 4 in the summary above , as well as a paragraph at the end of the introduction which hopefully emphasizes more clearly the novelty of the proposed approach . As discussed above , at some level of generality , the ideas of addition or deletion of variables have been explored , but never in online dictionary learning context . We thank the reviewer for the reference on MDL approach to model selection in sparse coding , in offline setting ; we cited and discussed this reference in the introduction , however , our approach is quite different as it involves an online setting and assumes nonstationary inputs . If our arguments concerning novelty of our approach are still not sufficiently convincing , we would really appreciate any specific references on prior art proposing an online adaptive model-selection within any autoencoder setting ( sparse dictionary or beyond ) , based on hidden units birth/death dynamics , with a thorough evaluation , empirical and theoretical , of situations where such adaptive approach is beneficial ( or not ) as compared vs non-adaptive baseline . We were unable to find any such prior work . 2 . `` the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data . Which depending on the `` level '' of non-stationarity in the incoming data ( or presence of outliers ) could be difficult to set . '' This is a good point ; as usual , we end up having a tunable parameter controlling the birth rate ( here , a threshold on the representation error measured as 1 - correlation between actual and reconstructed input ) , and must resort to cross-validation to set it up , or potentially provide another layer of automation in order to tune this parameter online ( e.g. , an RL approach to parameter tuning in our method ) . However , we view this as a direction for future work , since the automated parameter tuning is a sufficiently involved procedure on its own . Regarding the earlier question about reinitialization of dead elements in the Mairal 's original algorithm : we added new experiments in the Appendix , section B.7 , Figure 19 . First of all , we noticed that the fraction of \u201c dead \u201d elements was typically very small in our experiments when using the original ODL method of Mairal et al . ( 2009 ) .We next followed the idea of reinitialization of dead elements in ODL , referring to this modification as \u201c ODL * \u201d in Figure 19 . Specifically , we reinitialize the \u201c dead \u201d elements with random initializations , and then relearn the reinitialized elements , along with other elements , on the current batch of data . Fig.19 extends Fig.2 with additional plots for the baseline extension \u201c ODL * \u201d of ODL ; keeping the experiment settings same . We see that there is negligible difference in the performance of ODL and its extension \u201c ODL * \u201d , and our method NODL maintains its superior performance w.r.t.ODL as well as \u201c ODL * \u201d ."}}