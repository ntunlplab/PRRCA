{"year": "2017", "forum": "SJGCiw5gl", "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference", "decision": "Accept (Poster)", "meta_review": "The paper presents a method for pruning filters from convolutional neural networks based on the first order Taylor expansion of the loss change. The method is novel and well justified with extensive empirical evaluation.", "reviews": [{"review_id": "SJGCiw5gl-0", "review_text": "Authors propose a strategy for pruning weights with the eventual goal of reducing GFLOP computations. The pruning strategy is well motivated using the taylor expansion of the neural network function with respect to the feature activations. The obtained strategy removes feature maps that have both a small activation and a small gradient (eqn 7). (A) Ideally the gradient of the output with respect to the activation functions should be 0 at the optimal, but as a result of stochastic gradient evaluations this would practically never be zero. Small variance in the gradient across mini-batches indicates that irrespective of input data the specific network parameter is unlikely to change - intuitively these are parameters that are closer to convergence. Parameters/weights that are close to convergence and also result in a small activation are intuitively good candidates for pruning. This is essentially what eqn 7 conveys and is likely to be reason why just removing weights that result in small activations is not as good of a pruning strategy (as shown by results in the paper). There are two kind of differences in weights that are removed by activation v/s taylor expansion: 1. Weights with high-activations but very low gradients will be removed by taylor expansion, but not by activation alone. 2. Weights with low-activation but high gradients will be removed by activation criterion, but not by taylor expansion. It will be interesting to analyze which of (1) or (2) contribute more to the differences in weights that are removed by the taylor expansion v/s activation criterion. Intuitively it seems that weight that satisfy (1) are important because they are converged and contribute significantly to network's activation. It is possible that a modified criterion - eqn (7) + \\lambda feature activation, (where \\lambda needs to be found by cross-validation) may lead to even better results at the cost of more parameter tuning. (B) Another interesting comparison is with the with the optimal damage framework - where the first order gradients are assumed to be zero and pruning is performed using the second-order information (also discussed by authors in the appendix). Critically, only the diagonal of the Hessian is computed. There is no comparison with optimal damage as authors claim it is memory and computation inefficient. Back of envelope calculations suggest that this would result only in 50% increase in memory and computation during pruning, but no loss in efficiency during testing. Therefore from a standpoint of deployment, I don't think this missing comparison is justified. (C) The eventual goal of the authors is to reduce GFLOPs. Some recent papers have proposed using lower precision computation for this. A comparison in GFLOPs with lower precision v/s pruning would be a great. While both these approaches are complementary and it is expected that combining both of them can lead to superior performance than either of the two - it is unclear when we are operating in the low-precision regime how much pruning can be performed. Any analysis on this tradeoff would be great (but not necessary). (D) On finetuning, authors report results of AlexNet and VGG on two different datasets - Flowers and Birds respectively. Why is this the case? It would be great to see the results of both the networks on both the datasets. (E) Authors report there is only a small drop in performance after pruning. Suppose the network was originally trained with N iterations, and then M finetuning iterations were performed during pruning. This means that pruned networks were trained for N + M iterations. The correct comparison in accuracies would be if we the original network was also trained for N + M iterations. In figure 4, does the performance at 100% parameters reports accuracy after N+M iterations or after N iterations? Overall I think the paper is technically and empirically sound, it proposes a new strategy for pruning: (1) Based on taylor expansion (2) Feature normalization to reduce parameter tuning efforts. (3) Iterative finetuning. However, I would like to see some comparisons mentioned in my comments above. If those comparisons are made I would change my ratings to an accept. ", "rating": "7: Good paper, accept", "reply_text": "We would like to thank reviewer for comments and interesting suggestions . Please see answers on comments listed below : ( A ) It is a very interesting observation of how gradient and activation can contribute to the Taylor criteria . Reviewer \u2019 s observation that gradient of the cost function with respect to the activation functions tends to zero for a converged network is on point . As a result of stochasticity in the training procedure , there is a small variance of the gradient which after multiplication with activation value produces Taylor criterion . Reviewer 's suggestion to verify assumptions ( 1 ) and ( 2 ) sounds interesting and we are on the last stage of verifying obtained results . ( Jan 16 ) This analysis was added as appendix A.7 . Our conclusions is that low activations seem to correlate more strongly with the Taylor criteria for less important neurons . However , both activation and gradients contribute to the set of neurons with higher Taylor scores . We evaluated the idea of combining Taylor and activation criteria in appendix A.5 and found that it does not provide any significant improvement . It seems that Taylor criteria already has all information contained in activations . ( B ) We agree that optimal brain damage ( OBD ) is an important baseline for any pruning algorithm . Also , it is interesting to see how much second order information can provide for pruning . Reviewer \u2019 s comment on that OBD takes 50 % more memory and computations is confirmed in the \u201c Efficient backprop \u201d paper by LeCun et al.1998.However , we found that efficiently implementing backpropagation of the diagonal of the Hessian for a large network is a rather challenging task that requires significant changes to the framework . We implemented computation of the diagonal of the Hessian in a different way explained in the section 2.2 , in the paragraph related to OBD and in appendix A.6 . Comparison of OBD and Taylor criteria demonstrated slightly better results of the latest in terms of correlation coefficient with oracle and actual iterative pruning . OBD is 50 % ( theoretically ) to 300 % ( our implementation ) slower than Taylor criteria that relies on first order gradient only . These observations are added to section 3.3 . ( C ) We agree that using math with lower precision speed ups inference time , and that it can be applied complementary to pruning . Combining both techniques leads to even better speed up as shown in [ 1 ] . Pruning helps with 100 % of the hardware installed today , while lowering precision is likely to become more important in the future when hardware with faster low-precision math will be widely available . [ 1 ] S.Han , H. Mao , W. Dally , \u201c Deep Compression : Compressing Deep Neural Networks with Pruning , Trained Quantization and Huffman Coding \u201d , ICLR 2016 ( D ) Originally we did not want to clutter paper to much with results that are similar in nature , therefore decided to show detailed analysis of 2 networks on 2 different datasets . To make our contribution stronger we added analysis of Spearman \u2019 s rank coefficient correlation for missing combinations ( VGG16/Flowers and AlexNet/Birds ) into Table 1 . There is a surprising result of OBD on VGG16/Flowers which is quite low . We observed significant overfitting of VGG16 ( Flowers training set is 3x smaller than Birds ) and the sparsest oracle out of all tests what might be the reason . ( E ) Observation is correct , we refer to the result of unpruned network after fine-tuning for N iterations only . Resulting classification accuracies can be improved with further fine-tuning of pruned or unpruned networks . We added the last paragraph in section 3.2 addressing this comment . Unpruned networks tested on ImageNet datasets do not show improvement after further fine-tuning because of long original training till convergence ."}, {"review_id": "SJGCiw5gl-1", "review_text": "Authors propose a neural pruning technique starting from trained models using an approximation of change in the cost function and outperform other criteria. Authors obtain solid speedups while maintaining reasonable accuracy thanks to finetuning after pruning. Comparisons to existing methods is weak as GFLOPS graphs only show a couple simple baselines and no prior work baselines. I would be more convinced of the superiority of the approach with such comparison.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate comment of the reviewer to consider more baselines . We implemented 3 extra methods : 1 ) Optimal Brain Damage , 2 ) APoZ , 3 ) pruning by regularization . Our conclusions on comparing with them : 1 ) OBD shows good results that are slightly worse compared to Taylor criteria . OBD requires computation of diagonal of Hessian which takes extra 50 % of computations as pointed in the original papers , or extra 3x to 30x computations as in our implementation . We putted details of our implementation into appendix A.6 . 2 ) Proposed Taylor criteria shows significantly better performance than criterion based on Average percentage of zeros ( APoZ ) . 3 ) Taylor criteria shows better accuracy when compared with second norm regularization based pruning as shown in appendix A.4 ."}, {"review_id": "SJGCiw5gl-2", "review_text": "This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification. The proposed methods is derived from the first order Taylor expansion of the loss change while pruning a particular unit. This leads to simple weighting of the unit activation with its gradient w.r.t. loss function and performs better than simply using the activation magnitude as the heuristic for pruning. This intuitively makes sense, as we would like to remove not only the filters with low activation, but also filters where the incorrect activation value would not have small influence on the target loss. Authors thoroughly investigate multiple baselines, including an oracle which sets an upper bound on the target performance even though it is computationally expensive. The devised method seems to be quite elegant and authors show that it generalizes well on multiple tasks and is computationally more than feasible as it is easy to combine with traditional fine tuning procedure. Also, the work clearly shows the trade-offs of increased speed and decreased performance, which is useful for practical applications. It would be also useful to compare against different baselines, e.g. [1]. However this method seems to be more useful as it does not involve training of a new network (and thus is probably much faster). Suggestion - maybe it can be extended in the future towards also removing only parts of the filters(e.g. for the 3D convolution)? This may be more complicated as it would need to change the implementation of convolution operator, but can lead to further speedup. [1] https://arxiv.org/abs/1503.02531", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We would like to thank reviewer for comments and suggestions . We added 3 more comparisons as baseline methods : Optimal Brain Damage , APoZ and pruning by regularization . Taylor criterion performs slightly better than OBD and requires no change to back-propagation algorithm , OBD on the other hand requires back-propagation of diagonal of the Hessian which adds extra computations and is at least 3 times slower in our implementation . Proposed criterion demonstrates better performance than APoZ and pruning by regularization on network transfer tasks . `` Dark Knowledge '' or network distillation is a very interesting approach to reduce computations by training a smaller network . However it requires search of network architecture , which is done automatically for the proposed approach . Proposed criterion and method can be used for pruning network parameters at different levels : individual connections , part of filters or entire filter . Modern GPU hardware exploits regularities in computation for high throughput and only pruning the entire filter provides immediate speed up . Considering other levels of pruning is our goal for future work . Analysing pre-review questions we added section 3.6 Speed up measurements where we measured wall-clock time of inference . Also we added FLOPs numbers per layer used in FLOPs regularization for pruning ."}], "0": {"review_id": "SJGCiw5gl-0", "review_text": "Authors propose a strategy for pruning weights with the eventual goal of reducing GFLOP computations. The pruning strategy is well motivated using the taylor expansion of the neural network function with respect to the feature activations. The obtained strategy removes feature maps that have both a small activation and a small gradient (eqn 7). (A) Ideally the gradient of the output with respect to the activation functions should be 0 at the optimal, but as a result of stochastic gradient evaluations this would practically never be zero. Small variance in the gradient across mini-batches indicates that irrespective of input data the specific network parameter is unlikely to change - intuitively these are parameters that are closer to convergence. Parameters/weights that are close to convergence and also result in a small activation are intuitively good candidates for pruning. This is essentially what eqn 7 conveys and is likely to be reason why just removing weights that result in small activations is not as good of a pruning strategy (as shown by results in the paper). There are two kind of differences in weights that are removed by activation v/s taylor expansion: 1. Weights with high-activations but very low gradients will be removed by taylor expansion, but not by activation alone. 2. Weights with low-activation but high gradients will be removed by activation criterion, but not by taylor expansion. It will be interesting to analyze which of (1) or (2) contribute more to the differences in weights that are removed by the taylor expansion v/s activation criterion. Intuitively it seems that weight that satisfy (1) are important because they are converged and contribute significantly to network's activation. It is possible that a modified criterion - eqn (7) + \\lambda feature activation, (where \\lambda needs to be found by cross-validation) may lead to even better results at the cost of more parameter tuning. (B) Another interesting comparison is with the with the optimal damage framework - where the first order gradients are assumed to be zero and pruning is performed using the second-order information (also discussed by authors in the appendix). Critically, only the diagonal of the Hessian is computed. There is no comparison with optimal damage as authors claim it is memory and computation inefficient. Back of envelope calculations suggest that this would result only in 50% increase in memory and computation during pruning, but no loss in efficiency during testing. Therefore from a standpoint of deployment, I don't think this missing comparison is justified. (C) The eventual goal of the authors is to reduce GFLOPs. Some recent papers have proposed using lower precision computation for this. A comparison in GFLOPs with lower precision v/s pruning would be a great. While both these approaches are complementary and it is expected that combining both of them can lead to superior performance than either of the two - it is unclear when we are operating in the low-precision regime how much pruning can be performed. Any analysis on this tradeoff would be great (but not necessary). (D) On finetuning, authors report results of AlexNet and VGG on two different datasets - Flowers and Birds respectively. Why is this the case? It would be great to see the results of both the networks on both the datasets. (E) Authors report there is only a small drop in performance after pruning. Suppose the network was originally trained with N iterations, and then M finetuning iterations were performed during pruning. This means that pruned networks were trained for N + M iterations. The correct comparison in accuracies would be if we the original network was also trained for N + M iterations. In figure 4, does the performance at 100% parameters reports accuracy after N+M iterations or after N iterations? Overall I think the paper is technically and empirically sound, it proposes a new strategy for pruning: (1) Based on taylor expansion (2) Feature normalization to reduce parameter tuning efforts. (3) Iterative finetuning. However, I would like to see some comparisons mentioned in my comments above. If those comparisons are made I would change my ratings to an accept. ", "rating": "7: Good paper, accept", "reply_text": "We would like to thank reviewer for comments and interesting suggestions . Please see answers on comments listed below : ( A ) It is a very interesting observation of how gradient and activation can contribute to the Taylor criteria . Reviewer \u2019 s observation that gradient of the cost function with respect to the activation functions tends to zero for a converged network is on point . As a result of stochasticity in the training procedure , there is a small variance of the gradient which after multiplication with activation value produces Taylor criterion . Reviewer 's suggestion to verify assumptions ( 1 ) and ( 2 ) sounds interesting and we are on the last stage of verifying obtained results . ( Jan 16 ) This analysis was added as appendix A.7 . Our conclusions is that low activations seem to correlate more strongly with the Taylor criteria for less important neurons . However , both activation and gradients contribute to the set of neurons with higher Taylor scores . We evaluated the idea of combining Taylor and activation criteria in appendix A.5 and found that it does not provide any significant improvement . It seems that Taylor criteria already has all information contained in activations . ( B ) We agree that optimal brain damage ( OBD ) is an important baseline for any pruning algorithm . Also , it is interesting to see how much second order information can provide for pruning . Reviewer \u2019 s comment on that OBD takes 50 % more memory and computations is confirmed in the \u201c Efficient backprop \u201d paper by LeCun et al.1998.However , we found that efficiently implementing backpropagation of the diagonal of the Hessian for a large network is a rather challenging task that requires significant changes to the framework . We implemented computation of the diagonal of the Hessian in a different way explained in the section 2.2 , in the paragraph related to OBD and in appendix A.6 . Comparison of OBD and Taylor criteria demonstrated slightly better results of the latest in terms of correlation coefficient with oracle and actual iterative pruning . OBD is 50 % ( theoretically ) to 300 % ( our implementation ) slower than Taylor criteria that relies on first order gradient only . These observations are added to section 3.3 . ( C ) We agree that using math with lower precision speed ups inference time , and that it can be applied complementary to pruning . Combining both techniques leads to even better speed up as shown in [ 1 ] . Pruning helps with 100 % of the hardware installed today , while lowering precision is likely to become more important in the future when hardware with faster low-precision math will be widely available . [ 1 ] S.Han , H. Mao , W. Dally , \u201c Deep Compression : Compressing Deep Neural Networks with Pruning , Trained Quantization and Huffman Coding \u201d , ICLR 2016 ( D ) Originally we did not want to clutter paper to much with results that are similar in nature , therefore decided to show detailed analysis of 2 networks on 2 different datasets . To make our contribution stronger we added analysis of Spearman \u2019 s rank coefficient correlation for missing combinations ( VGG16/Flowers and AlexNet/Birds ) into Table 1 . There is a surprising result of OBD on VGG16/Flowers which is quite low . We observed significant overfitting of VGG16 ( Flowers training set is 3x smaller than Birds ) and the sparsest oracle out of all tests what might be the reason . ( E ) Observation is correct , we refer to the result of unpruned network after fine-tuning for N iterations only . Resulting classification accuracies can be improved with further fine-tuning of pruned or unpruned networks . We added the last paragraph in section 3.2 addressing this comment . Unpruned networks tested on ImageNet datasets do not show improvement after further fine-tuning because of long original training till convergence ."}, "1": {"review_id": "SJGCiw5gl-1", "review_text": "Authors propose a neural pruning technique starting from trained models using an approximation of change in the cost function and outperform other criteria. Authors obtain solid speedups while maintaining reasonable accuracy thanks to finetuning after pruning. Comparisons to existing methods is weak as GFLOPS graphs only show a couple simple baselines and no prior work baselines. I would be more convinced of the superiority of the approach with such comparison.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate comment of the reviewer to consider more baselines . We implemented 3 extra methods : 1 ) Optimal Brain Damage , 2 ) APoZ , 3 ) pruning by regularization . Our conclusions on comparing with them : 1 ) OBD shows good results that are slightly worse compared to Taylor criteria . OBD requires computation of diagonal of Hessian which takes extra 50 % of computations as pointed in the original papers , or extra 3x to 30x computations as in our implementation . We putted details of our implementation into appendix A.6 . 2 ) Proposed Taylor criteria shows significantly better performance than criterion based on Average percentage of zeros ( APoZ ) . 3 ) Taylor criteria shows better accuracy when compared with second norm regularization based pruning as shown in appendix A.4 ."}, "2": {"review_id": "SJGCiw5gl-2", "review_text": "This paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification. The proposed methods is derived from the first order Taylor expansion of the loss change while pruning a particular unit. This leads to simple weighting of the unit activation with its gradient w.r.t. loss function and performs better than simply using the activation magnitude as the heuristic for pruning. This intuitively makes sense, as we would like to remove not only the filters with low activation, but also filters where the incorrect activation value would not have small influence on the target loss. Authors thoroughly investigate multiple baselines, including an oracle which sets an upper bound on the target performance even though it is computationally expensive. The devised method seems to be quite elegant and authors show that it generalizes well on multiple tasks and is computationally more than feasible as it is easy to combine with traditional fine tuning procedure. Also, the work clearly shows the trade-offs of increased speed and decreased performance, which is useful for practical applications. It would be also useful to compare against different baselines, e.g. [1]. However this method seems to be more useful as it does not involve training of a new network (and thus is probably much faster). Suggestion - maybe it can be extended in the future towards also removing only parts of the filters(e.g. for the 3D convolution)? This may be more complicated as it would need to change the implementation of convolution operator, but can lead to further speedup. [1] https://arxiv.org/abs/1503.02531", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We would like to thank reviewer for comments and suggestions . We added 3 more comparisons as baseline methods : Optimal Brain Damage , APoZ and pruning by regularization . Taylor criterion performs slightly better than OBD and requires no change to back-propagation algorithm , OBD on the other hand requires back-propagation of diagonal of the Hessian which adds extra computations and is at least 3 times slower in our implementation . Proposed criterion demonstrates better performance than APoZ and pruning by regularization on network transfer tasks . `` Dark Knowledge '' or network distillation is a very interesting approach to reduce computations by training a smaller network . However it requires search of network architecture , which is done automatically for the proposed approach . Proposed criterion and method can be used for pruning network parameters at different levels : individual connections , part of filters or entire filter . Modern GPU hardware exploits regularities in computation for high throughput and only pruning the entire filter provides immediate speed up . Considering other levels of pruning is our goal for future work . Analysing pre-review questions we added section 3.6 Speed up measurements where we measured wall-clock time of inference . Also we added FLOPs numbers per layer used in FLOPs regularization for pruning ."}}