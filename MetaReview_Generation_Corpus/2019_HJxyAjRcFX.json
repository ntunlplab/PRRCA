{"year": "2019", "forum": "HJxyAjRcFX", "title": "Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation", "decision": "Accept (Poster)", "meta_review": "The paper presents new loss functions (which replace the reconstruction part) for the training of conditional GANs. Theoretical considerations and an empirical analysis show that the proposed loss can better handle multimodality of the target distribution than reconstruction based losses while being competitive in terms of image quality.", "reviews": [{"review_id": "HJxyAjRcFX-0", "review_text": "The paper proposes a modification to the traditional conditional GAN objective (which minimizes GAN loss as well as either L1 or L2 pixel-wise reconstruction losses) in order to promote diverse, multimodal generation of images. The modification involves replacing the L1/L2 reconstruction loss -- which predicts the first moment of a pixel-wise gaussian/laplace (respectively) likelihood model assuming a constant spherical covariance matrix -- with a new objective that matches the first and second moments of a pixel-wise gaussian/laplace likelihood model with diagonal covariance matrix. Two models are proposed for matching the first and second moments - the first one involves using a separate network to predict the moments from data which are then used to match the generator\u2019s empirical estimates of the moments (using K samples of generated images). The second involves directly matching the empirical moment estimates using monte carlo. The paper makes use of a well-established idea - modeling pixel-wise image likelihood with a diagonal covariance matrix i.e. heteroscedastic variance (which, as explained in [1], is a way to learn data-dependent aleatoric uncertainty). Following [1], the usage of first and second moment prediction is also prevalent in recent deep generative models (for example, [2]) i.e. image likelihood models predict the per-pixel mean and variance in the L2 likelihood case, for optimizing Equation 4 from the paper. Recent work has also attempted to go beyond the assumption of a diagonal covariance matrix (for example, in [3] a band-diagonal covariance matrix is estimated). Hence, the only novel idea in the paper seems to be the method for matching the empirical estimates of the first and second moments over K samples. The motivation for doing this makes intuitive sense since diversity in generation is desired, which is also demonstrated in the results. Section specific comments: - The loss of modality of reconstruction loss (section 3.2) seems like something which doesn\u2019t require the extent of mathematical and empirical detail presented in the paper. Several of the cited works already mention the pitfalls of using reconstruction loss. - The analyses in section 4.4 are sound in derivation but not so much in the conclusions drawn. It is not clear that the lack of existence of a generator that is an optimal solution to the GAN and L2 loss (individually) implies that any learnt generator using GAN + L2 loss is suboptimal. More explanation on this part would help. The paper is well written, presents a simple idea, complete with experiments for comparing diversity with competing methods. Some theoretical analyses do no directly support the proposition - e.g. sections 3.2 and 4.4 in my specific comments above. Hence, the claim that the proposed method prevents mode collapse (training stability) and gives diverse multi-modal predictions is supported by experiments and intuition for the method, but not so much theoretically. However, the major weakness of the paper is the lack of novelty of the core idea. === Update after rebuttal: Having read through the other reviews and the author's rebuttal, I am unsatisfied with the rebuttal and I do not recommend accepting the paper. My rating has decreased accordingly. The reasons for my recommendation, after discussion with other reviews, are -- (1) lack of novelty and (2) weak theoretical results (some justification of which was stated in my initial review above). Elaborating more on the second point, I would like to mention some points which came up during the discussion with other reviewers: The theoretical result which states that not using reconstruction loss given that multi-modal outputs are desired is a weaker result than proving that the proposed method is actually effective in what it is designed to do. There are empirical results to back that claim, but I strongly believe that the theoretical results fall short and feel out of place in the overall justification for the proposed method. This, along with my earlier point of lack of novelty are the basis for my decision. References: [1] Kendall, Alex, and Yarin Gal. \"What uncertainties do we need in bayesian deep learning for computer vision?.\" Advances in neural information processing systems. 2017. [2] Bloesch, M., Czarnowski, J., Clark, R., Leutenegger, S., & Davison, A. J. (2018). CodeSLAM-Learning a Compact, Optimisable Representation for Dense Visual SLAM. CVPR 2018. [3] Dorta, G., Vicente, S., Agapito, L., Campbell, N. D., & Simpson, I. (2018, February). Structured Uncertainty Prediction Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We are sincerely grateful for Reviewer 3 \u2019 s thoughtful review . Please see blue fonts in the newly uploaded draft to check how our paper is updated . 1.Novelty =================================== We believe that our work has significant novelties as follows : ( 1 ) To the best of our knowledge , our work is the first to formally criticize the use of reconstruction loss in conditional GANs . We also connect this problem to mode collapse ( lose of multimodality ) . Of the prior works in conditional generation tasks , several papers empirically mention the loss of stochasticity in conditional GANs . However , they fail to analyze why this happens or propose what solutions can solve this problem . On the other hand , we reveal that the GAN loss and the reconstruction loss can not coexist in harmony , and propose a solution to overcome this problem . ( 2 ) We propose alternatives to the reconstruction loss to greatly improve the multimodality of conditional GANs . As Reviewer 3 pointed out , the components of our methods , MLE and moment matching , are well-established ideas . However , it is novel to combine them as a solution to the loss of multimodality in conditional generation . Furthermore , we think the simplicity of our methods is not a weakness but a strength , which makes our methods easily applicable to a wide range of conditional generation tasks . 2.Specific comments on organization and drawn conclusions =================================== We reorganize section 3.2 and 4.4 to reflect Reviewer 3 \u2019 s suggestion . Specifically , we simplify section 3.2 and move some content about reconstruction loss from 4.4 to 3.2 . We agree with Reviewer 3 that the conclusion of section 4.4 may be rather over-stated . Our proof says that any generator can not be optimal to both GAN and L2 loss simultaneously . It does not prove the generator is underperforming or suboptimal . Therefore , we remove the term \u2018 suboptimal \u2019 and tone down the overall argument . We also cite the papers that Reviewer 3 suggested ."}, {"review_id": "HJxyAjRcFX-1", "review_text": "The paper describes an alternative to L1/L2 errors (wrt output and one ground-truth example) that are used to augment adversarial losses when training conditional GANs. While these augmented losses are often needed to stabilize and guide GAN training, the authors argue that they also bias the optimization of the generator towards mode collapse. To address this, the method proposes two kinds of alternate losses--both of which essentially generate multiple sample outputs from the same input, fit these with a Gaussian distribution by computing the generating sample mean and variance, and try to maximize the likelihood of the true training output under this distribution. The paper provides theoretical and empirical analysis to show that the proposed approach leads to generators that produce samples that are both diverse and high-quality. I think this is a good paper and solves an important problem---where one usually had to sacrifice diversity to obtain stable training by adding a reconstruction loss. I recommend acceptance. An interesting ablation experiment might be to see what happens when one no longer includes the GAN loss and trains only with the MLMM or MCMLE losses, and compare this to training with only the L1/L2 losses. The other thing I'd like the authors to comment on are the potential shortcomings of using a simple un-correlated Gaussian to model the sample distributions. It seems that such a distribution may not capture the fact that multiple dimensions of the output (i.e., multiple pixel intensities) are not independent conditioned on the input. Perhaps, it may be worth exploring whether Gaussians with general co-variance matrices, or independent in some de-correlated space (learned from say simply the set of outputs) may increase the efficacy of these losses. ====Post-rebuttal I've read the other reviews and retain my positive impression of the paper. I also appreciate that the authors have conducted additional experiments based on my (non-binding) suggestions---and the results are indeed interesting. I am upgrading my score accordingly.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank Reviewer 1 for your encouraging and constructive comments . Please see blue fonts in the newly uploaded draft to check how our paper is updated . 1.Ablation experiments =================================== We carry out the ablation experiments and present the results in appendix G ( page 22 ) . The results are indeed interesting . When trained with MLMM_1 or MCMLE_1 only , the outputs are indistinguishable from those with the reconstruction loss only , since there is no variation-inducing term to generate diverse output . In the case of MLMM_ { 1/2 } and MCMLE_ { 1/2 } , the model shows high variation in the output . However , the patterns of the variations differ greatly . Specifically , MLMM_ { 1/2 } shows variations in low-frequency while MCMLE_ { 1/2 } shows those in high-frequency . We also add experiments of using GAN loss , MLMM_ { 1/2 } loss , and reconstruction loss altogether . Whiling fixing the coefficient of GAN loss and MLMM loss to 1 and 10 respectively , we gradually increase the coefficient of reconstruction loss from 0 to 100 . We find that the output variation decreases as the reconstruction loss increases . Interestingly , the sample quality is high when the reconstruction loss is absolutely zero or dominated by the MLMM loss . In contrast , the samples show poor quality when the reconstruction coefficient is 1 or 10 . It seems that either method can assist the GAN loss to find visually appealing local optima but the joint use of them leads to a troublesome behavior . 2.Shortcomings of un-correlated Gaussian =================================== This is a very interesting and profound question that may need to be further investigated in the future work . In summary , we believe that incorporating more statistics is not guaranteed to improve the performance , and un-correlated Gaussian may not be a bad choice . An ideal GAN loss can match with any kind of statistics since it minimizes the JS divergence between sample distribution and real distribution . In this sense , additional loss term should be regarded as a \u2018 guidance \u2019 signal , while the key player is still the GAN loss . However , it is unclear whether a tighter guidance necessarily yields better outputs . Regarding the tightness of guidance , the loss terms can be ordered as follows : MLMM_1 = MCMLE_1 < MLMM_ { 1/2 } = MCMLE_ { 1/2 } < general covariance Gaussian . Interestingly , our qualitative evaluations show that MLMM_1 and MCMLE_1 generate comparable or even better outputs compared to MLMM_ { 1/2 } and MCMLE_ { 1/2 } . That is , matching means could be enough to guide GAN training in many cases . Adding more statistics may be helpful in some cases , but generally may not improve the performance . Moreover , we should consider the errors arising from the statistics prediction because a wrong estimation of statistics can even misguide the GAN training . Please see blue fonts in section 5.2 of the newly uploaded draft to check how our paper is updated ."}, {"review_id": "HJxyAjRcFX-2", "review_text": "This paper analyzes the model collapse problems on training conditional GANs and attribute it to the mismatch between GAN loss and reconstruction loss. This paper also proposes new types of reconstruction loss by measuring higher statistics for better multimodal conditional generation. Pros: 1. The analysis in Sec 4.4 is insightful, which partially explains the success of MLMM and MCMLE over previous method in generating diverse conditional outputs. 2. The paper is well written and easy to follow. Cons: Analysis on the experiments is a little insufficient, as shown below. I have some questions (and suggestions) about experiments. 1. How does the training process affected by changing the reconstruction loss (e.g., how the training curve changes?)? Do MLMM and MCMLE converge slower or faster than the original ones? What about training stability? 2. Why only MLMM_1 is not compared with other methods on SRGAN-celebA and GLCIC-A? From pix2pix cases it seems that Gaussian MLMM_1 performs much better than MLMM_{1/2}. ", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 2 for positive and constructive reviews . Please see blue fonts in the newly uploaded draft to check how our paper is updated . 1.Convergence speed =================================== We observe that our methods need more training steps ( about 1.5x ) to generate high-quality images compared to that with the reconstruction loss . It might be obvious because our methods train the model to generate a much wider range of outputs . We add some comments to Appendix B.1 regarding the convergence speed . 2.Training stability =================================== MLMM is similar to the reconstruction loss in terms of training stability . Encouragingly , our methods stably work with a large range of hyperparameter \\lambda . For example , the loss coefficient of MLMM is settable across several orders of magnitude ( from tens to thousands ) with similar results . However , as noted in the paper , MCMLE is unstable compared to MLMM . 3.Why only MLMM_1 is not compared =================================== Due to many combinations between our methods and tasks , we had to choose only a few of our methods for human evaluation . Although MLMM_1 and MLMM_ { 1/2 } attained similar performance for all three tasks , we chose MLMM_ { 1/2 } as the \u2018 default \u2019 method because it better implements our idea - matching more statistics ( i.e.not only means but also variances ) ."}], "0": {"review_id": "HJxyAjRcFX-0", "review_text": "The paper proposes a modification to the traditional conditional GAN objective (which minimizes GAN loss as well as either L1 or L2 pixel-wise reconstruction losses) in order to promote diverse, multimodal generation of images. The modification involves replacing the L1/L2 reconstruction loss -- which predicts the first moment of a pixel-wise gaussian/laplace (respectively) likelihood model assuming a constant spherical covariance matrix -- with a new objective that matches the first and second moments of a pixel-wise gaussian/laplace likelihood model with diagonal covariance matrix. Two models are proposed for matching the first and second moments - the first one involves using a separate network to predict the moments from data which are then used to match the generator\u2019s empirical estimates of the moments (using K samples of generated images). The second involves directly matching the empirical moment estimates using monte carlo. The paper makes use of a well-established idea - modeling pixel-wise image likelihood with a diagonal covariance matrix i.e. heteroscedastic variance (which, as explained in [1], is a way to learn data-dependent aleatoric uncertainty). Following [1], the usage of first and second moment prediction is also prevalent in recent deep generative models (for example, [2]) i.e. image likelihood models predict the per-pixel mean and variance in the L2 likelihood case, for optimizing Equation 4 from the paper. Recent work has also attempted to go beyond the assumption of a diagonal covariance matrix (for example, in [3] a band-diagonal covariance matrix is estimated). Hence, the only novel idea in the paper seems to be the method for matching the empirical estimates of the first and second moments over K samples. The motivation for doing this makes intuitive sense since diversity in generation is desired, which is also demonstrated in the results. Section specific comments: - The loss of modality of reconstruction loss (section 3.2) seems like something which doesn\u2019t require the extent of mathematical and empirical detail presented in the paper. Several of the cited works already mention the pitfalls of using reconstruction loss. - The analyses in section 4.4 are sound in derivation but not so much in the conclusions drawn. It is not clear that the lack of existence of a generator that is an optimal solution to the GAN and L2 loss (individually) implies that any learnt generator using GAN + L2 loss is suboptimal. More explanation on this part would help. The paper is well written, presents a simple idea, complete with experiments for comparing diversity with competing methods. Some theoretical analyses do no directly support the proposition - e.g. sections 3.2 and 4.4 in my specific comments above. Hence, the claim that the proposed method prevents mode collapse (training stability) and gives diverse multi-modal predictions is supported by experiments and intuition for the method, but not so much theoretically. However, the major weakness of the paper is the lack of novelty of the core idea. === Update after rebuttal: Having read through the other reviews and the author's rebuttal, I am unsatisfied with the rebuttal and I do not recommend accepting the paper. My rating has decreased accordingly. The reasons for my recommendation, after discussion with other reviews, are -- (1) lack of novelty and (2) weak theoretical results (some justification of which was stated in my initial review above). Elaborating more on the second point, I would like to mention some points which came up during the discussion with other reviewers: The theoretical result which states that not using reconstruction loss given that multi-modal outputs are desired is a weaker result than proving that the proposed method is actually effective in what it is designed to do. There are empirical results to back that claim, but I strongly believe that the theoretical results fall short and feel out of place in the overall justification for the proposed method. This, along with my earlier point of lack of novelty are the basis for my decision. References: [1] Kendall, Alex, and Yarin Gal. \"What uncertainties do we need in bayesian deep learning for computer vision?.\" Advances in neural information processing systems. 2017. [2] Bloesch, M., Czarnowski, J., Clark, R., Leutenegger, S., & Davison, A. J. (2018). CodeSLAM-Learning a Compact, Optimisable Representation for Dense Visual SLAM. CVPR 2018. [3] Dorta, G., Vicente, S., Agapito, L., Campbell, N. D., & Simpson, I. (2018, February). Structured Uncertainty Prediction Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We are sincerely grateful for Reviewer 3 \u2019 s thoughtful review . Please see blue fonts in the newly uploaded draft to check how our paper is updated . 1.Novelty =================================== We believe that our work has significant novelties as follows : ( 1 ) To the best of our knowledge , our work is the first to formally criticize the use of reconstruction loss in conditional GANs . We also connect this problem to mode collapse ( lose of multimodality ) . Of the prior works in conditional generation tasks , several papers empirically mention the loss of stochasticity in conditional GANs . However , they fail to analyze why this happens or propose what solutions can solve this problem . On the other hand , we reveal that the GAN loss and the reconstruction loss can not coexist in harmony , and propose a solution to overcome this problem . ( 2 ) We propose alternatives to the reconstruction loss to greatly improve the multimodality of conditional GANs . As Reviewer 3 pointed out , the components of our methods , MLE and moment matching , are well-established ideas . However , it is novel to combine them as a solution to the loss of multimodality in conditional generation . Furthermore , we think the simplicity of our methods is not a weakness but a strength , which makes our methods easily applicable to a wide range of conditional generation tasks . 2.Specific comments on organization and drawn conclusions =================================== We reorganize section 3.2 and 4.4 to reflect Reviewer 3 \u2019 s suggestion . Specifically , we simplify section 3.2 and move some content about reconstruction loss from 4.4 to 3.2 . We agree with Reviewer 3 that the conclusion of section 4.4 may be rather over-stated . Our proof says that any generator can not be optimal to both GAN and L2 loss simultaneously . It does not prove the generator is underperforming or suboptimal . Therefore , we remove the term \u2018 suboptimal \u2019 and tone down the overall argument . We also cite the papers that Reviewer 3 suggested ."}, "1": {"review_id": "HJxyAjRcFX-1", "review_text": "The paper describes an alternative to L1/L2 errors (wrt output and one ground-truth example) that are used to augment adversarial losses when training conditional GANs. While these augmented losses are often needed to stabilize and guide GAN training, the authors argue that they also bias the optimization of the generator towards mode collapse. To address this, the method proposes two kinds of alternate losses--both of which essentially generate multiple sample outputs from the same input, fit these with a Gaussian distribution by computing the generating sample mean and variance, and try to maximize the likelihood of the true training output under this distribution. The paper provides theoretical and empirical analysis to show that the proposed approach leads to generators that produce samples that are both diverse and high-quality. I think this is a good paper and solves an important problem---where one usually had to sacrifice diversity to obtain stable training by adding a reconstruction loss. I recommend acceptance. An interesting ablation experiment might be to see what happens when one no longer includes the GAN loss and trains only with the MLMM or MCMLE losses, and compare this to training with only the L1/L2 losses. The other thing I'd like the authors to comment on are the potential shortcomings of using a simple un-correlated Gaussian to model the sample distributions. It seems that such a distribution may not capture the fact that multiple dimensions of the output (i.e., multiple pixel intensities) are not independent conditioned on the input. Perhaps, it may be worth exploring whether Gaussians with general co-variance matrices, or independent in some de-correlated space (learned from say simply the set of outputs) may increase the efficacy of these losses. ====Post-rebuttal I've read the other reviews and retain my positive impression of the paper. I also appreciate that the authors have conducted additional experiments based on my (non-binding) suggestions---and the results are indeed interesting. I am upgrading my score accordingly.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank Reviewer 1 for your encouraging and constructive comments . Please see blue fonts in the newly uploaded draft to check how our paper is updated . 1.Ablation experiments =================================== We carry out the ablation experiments and present the results in appendix G ( page 22 ) . The results are indeed interesting . When trained with MLMM_1 or MCMLE_1 only , the outputs are indistinguishable from those with the reconstruction loss only , since there is no variation-inducing term to generate diverse output . In the case of MLMM_ { 1/2 } and MCMLE_ { 1/2 } , the model shows high variation in the output . However , the patterns of the variations differ greatly . Specifically , MLMM_ { 1/2 } shows variations in low-frequency while MCMLE_ { 1/2 } shows those in high-frequency . We also add experiments of using GAN loss , MLMM_ { 1/2 } loss , and reconstruction loss altogether . Whiling fixing the coefficient of GAN loss and MLMM loss to 1 and 10 respectively , we gradually increase the coefficient of reconstruction loss from 0 to 100 . We find that the output variation decreases as the reconstruction loss increases . Interestingly , the sample quality is high when the reconstruction loss is absolutely zero or dominated by the MLMM loss . In contrast , the samples show poor quality when the reconstruction coefficient is 1 or 10 . It seems that either method can assist the GAN loss to find visually appealing local optima but the joint use of them leads to a troublesome behavior . 2.Shortcomings of un-correlated Gaussian =================================== This is a very interesting and profound question that may need to be further investigated in the future work . In summary , we believe that incorporating more statistics is not guaranteed to improve the performance , and un-correlated Gaussian may not be a bad choice . An ideal GAN loss can match with any kind of statistics since it minimizes the JS divergence between sample distribution and real distribution . In this sense , additional loss term should be regarded as a \u2018 guidance \u2019 signal , while the key player is still the GAN loss . However , it is unclear whether a tighter guidance necessarily yields better outputs . Regarding the tightness of guidance , the loss terms can be ordered as follows : MLMM_1 = MCMLE_1 < MLMM_ { 1/2 } = MCMLE_ { 1/2 } < general covariance Gaussian . Interestingly , our qualitative evaluations show that MLMM_1 and MCMLE_1 generate comparable or even better outputs compared to MLMM_ { 1/2 } and MCMLE_ { 1/2 } . That is , matching means could be enough to guide GAN training in many cases . Adding more statistics may be helpful in some cases , but generally may not improve the performance . Moreover , we should consider the errors arising from the statistics prediction because a wrong estimation of statistics can even misguide the GAN training . Please see blue fonts in section 5.2 of the newly uploaded draft to check how our paper is updated ."}, "2": {"review_id": "HJxyAjRcFX-2", "review_text": "This paper analyzes the model collapse problems on training conditional GANs and attribute it to the mismatch between GAN loss and reconstruction loss. This paper also proposes new types of reconstruction loss by measuring higher statistics for better multimodal conditional generation. Pros: 1. The analysis in Sec 4.4 is insightful, which partially explains the success of MLMM and MCMLE over previous method in generating diverse conditional outputs. 2. The paper is well written and easy to follow. Cons: Analysis on the experiments is a little insufficient, as shown below. I have some questions (and suggestions) about experiments. 1. How does the training process affected by changing the reconstruction loss (e.g., how the training curve changes?)? Do MLMM and MCMLE converge slower or faster than the original ones? What about training stability? 2. Why only MLMM_1 is not compared with other methods on SRGAN-celebA and GLCIC-A? From pix2pix cases it seems that Gaussian MLMM_1 performs much better than MLMM_{1/2}. ", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 2 for positive and constructive reviews . Please see blue fonts in the newly uploaded draft to check how our paper is updated . 1.Convergence speed =================================== We observe that our methods need more training steps ( about 1.5x ) to generate high-quality images compared to that with the reconstruction loss . It might be obvious because our methods train the model to generate a much wider range of outputs . We add some comments to Appendix B.1 regarding the convergence speed . 2.Training stability =================================== MLMM is similar to the reconstruction loss in terms of training stability . Encouragingly , our methods stably work with a large range of hyperparameter \\lambda . For example , the loss coefficient of MLMM is settable across several orders of magnitude ( from tens to thousands ) with similar results . However , as noted in the paper , MCMLE is unstable compared to MLMM . 3.Why only MLMM_1 is not compared =================================== Due to many combinations between our methods and tasks , we had to choose only a few of our methods for human evaluation . Although MLMM_1 and MLMM_ { 1/2 } attained similar performance for all three tasks , we chose MLMM_ { 1/2 } as the \u2018 default \u2019 method because it better implements our idea - matching more statistics ( i.e.not only means but also variances ) ."}}