{"year": "2017", "forum": "HkYhZDqxg", "title": "Tree-structured decoding with doubly-recurrent neural networks", "decision": "Accept (Poster)", "meta_review": "The paper introduces a new model for generating trees decorated with node embeddings. Interestingly the authors do not assume that even leaf nodes in the tree are known a-priori. There has been very little work on this setting, and, the problem is quite important and general. Though the experiments are somewhat limited, reviewers generally believe that they are sufficient to show that the approach holds a promise.\n \n + an important and under-explored setting\n + novel model\n + well written\n \n - experimentation could be stronger (but seems sufficient -- both on real and artificial data)", "reviews": [{"review_id": "HkYhZDqxg-0", "review_text": "Authors' response well answered my questions. Thanks. Evaluation not changed. ### This paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset. There are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence. Moreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding. On the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides? The paper is well written, except for minor typo as mentioned in my pre-review questions. In general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments and observations . We will add additional statistics on the results in the synthetic dataset and of the IFTTT task and revise the manuscript soon with this information . Regarding your question , precision drops as tree size grows likely because of both of the factors you mention : more information has to be encoded in a fixed size vector , and in a larger tree there is more opportunity for information degradation while decoding . We \u2019 ve observed that output tree depth has the strongest effect in performance degradation . A possible way to further investigate which of these two components ( encoder or decoder ) has a greater impact in performance degradation would be to compare against a sequence to sequence encoder-decoder , and measure the effect of input length in that case . This , however , still has the issue that an encoder might be good at generating representations to be used as starting states for sequence decoding , but not as good when generating hidden states to be used to generate a tree . Thus , really isolating the effect of encoder and decoder in this phenomenon is quite challenging . Regarding using attention and beam search , we agree these are interesting directions and we will leave them for future work ."}, {"review_id": "HkYhZDqxg-1", "review_text": "This paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures (including the topology) in an encoder-decoder setting. The architecture is well motivated and I can see several applications (in addition to what's presented in the paper) that need to generate tree structures given an unstructured data. One weakness of the paper is the limitation of experiments. IFTTT dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures. Still, I consider the experiments sufficient as a first step to showcase a novel architecture. A strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture, about when / how to decide to terminate, as opposed to making a single arbitrary choice. I see future applications of this architecture and it seems to have interesting directions for future work so I suggest its acceptance as a conference contribution.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . We chose the IFTTT task because of two main reasons . First , we wanted a task where the input was sequential , so that we could use a vanilla RNN as an encoder , thus minimizing the variability due to design choices on the encoder side . Second , we wanted a task where the actual tree was a crucial part of the output ( as opposed to just being a latent structure to generate the nodes ) , so that a regular sequence-to-sequence model could not be directly used . Though we think the IFTTT task was reasonable for this experimental setting , we agree that there are other interesting applications of the proposed model . To expand the experimental framework , we are working on an additional NLP task that will more clearly highlight the advantage of a DRNNs over sequence decoders , and which we \u2019 re aiming to have ready before the end of the review discussion period ."}, {"review_id": "HkYhZDqxg-2", "review_text": "The paper propose DRNN as a neural decoder for tree structures. I like the model architecture since it has two clear improvements over traditional approaches \u2014 (1) the information flows in two directions, both from the parent and from siblings, which is desirable in tree structures (2) the model use a probability distribution to model the tree boundary (i.e. the last sibling or the leaf). This avoids the use of special ending symbols which is larger in size and putting more things to learn for the parameters (shared with other symbols). The authors test the DRNN using the tasks of recovering the synthetic trees and recovering functional programs. The model did better than traditional methods like seq2seq models. I think the recovering synthetic tree task is not very satisfying for two reasons \u2014 (1) the surface form itself already containing some of the topological information which makes the task easier than it should be (2) as we can see from figure 3, when the number of nodes grows (even to a number not very large), the performance of the model drops dramatically, I am not sure if a simple baseline only captures the topological information in the surface string would be much worse than this. And DRNN in this case, seems can\u2019t show its full potentials since the length of the information flow in the model won\u2019t be very long. I think the experiments are interesting. But I think there are some other tasks which are more difficult and the tree structure information are more important in such tasks. For example, we have the seq2seq parsing model (Vinyals et al, 2014), is it possible to use the DRNN proposed here on the decoder side? I think tasks like this can show more potentials of the DRNN and can be very convincing that model architectures like this are better than traditional alternatives.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . Regarding the difficulty of the synthetic task , the surface form indeed contains some topological information , otherwise the task would be unrealistic . But this does not necessarily mean the task is easy . Given that two topological decisions must be made for every node , there are is a combinatorial number of possible output trees for a fixed size sentence , even if always processing sentences in a breadth-first left-to-right manner . For example , using lambda-logical notation , the surface form \u201c A B C D \u201d can lead to the trees : \u201c A B C D \u201d , \u201c A ( B C D ) \u201d , \u201c A ( B ) C ( D ) \u201d , \u201c A ( B ( C ( D ) ) ) \u201d , etc . There are roughly O ( n^2 ) such possibilities if we knew the node labels . But in our setting the decoder is not given the labels , it generates them , so there \u2019 s an exponential ( in n ) number of possible outputs . So , even if there is some topological information in the surface form , recovering the correct tree directly from it without learning some general properties about feasible output trees would be very hard . The task of the DRNN decoder is thus to learn these properties during training , similar to how a seq-to-seq model must implicitly learn a language model on the decoder side , to be able to generate sentences not only utilizing the information provided by the input , but also generating feasible outputs as per previously seen data . We agree that in its current form , it \u2019 s not easy to appreciate the difficulty of the task . To address this and your point ( 2 ) , we are implementing a simple baseline to give the reader more information to understand the complexity of the task and the performance of our method . We will add these results as they become available . Regarding additional tasks , as we mention to another reviewer , we are setting up an additional ( significantly harder ) task that will evaluate the performance of our method when dealing with larger , more diverse trees . We hope to have the results ready before the review discussion deadline ."}], "0": {"review_id": "HkYhZDqxg-0", "review_text": "Authors' response well answered my questions. Thanks. Evaluation not changed. ### This paper proposes a neural model for generating tree structure output from scratch. The model does 1) separate the recurrence between depths and siblings; 2) separate the topology and label generation, and outperforms previous methods on a benchmark IFTTT dataset. Compared to previous tree-decoding methods, the model avoids manually annotating subtrees with special tokens, and thus is a very good alternative to such problems. The paper does solid experiments on one synthetic dataset, and outperforms alternative methods on one real-world IFTTT dataset. There are couple of interesting results in the paper that I believe is worth further investigation. Firstly, on the synthetic dataset, the precision drops rapidly with the number of nodes. Is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences, such that the tree decoder can not do a good job? Or is it because that such tree decoder is not tolerant to the long sequence input, i.e., large tree structure? I believe that it is important to understand this before a better model can be developed. For example, if it is the fault of encoder, maybe an attention layer can be added, as in a seq-to-seq model, to preserve more information of the input sequence. Moreover, besides only showing how the precision changes with the number of nodes in the tree, it might be interesting to investigate how it goes with 1) number of depths; 2) number of widths; 3) symmetricity; etc. Moreover, as greedy search is used in decoding, it might be interesting to see how it helps, if it does, to use beam-search in tree decoding. On the IFTTT dataset, listing more statistics about this dataset might be helpful for better understanding the difficulty of this task. How deep are the trees? How large are the vocabularies on both language and program sides? The paper is well written, except for minor typo as mentioned in my pre-review questions. In general, I believe this is a solid paper, and more can be explored in this direction. So I tend to accept it. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments and observations . We will add additional statistics on the results in the synthetic dataset and of the IFTTT task and revise the manuscript soon with this information . Regarding your question , precision drops as tree size grows likely because of both of the factors you mention : more information has to be encoded in a fixed size vector , and in a larger tree there is more opportunity for information degradation while decoding . We \u2019 ve observed that output tree depth has the strongest effect in performance degradation . A possible way to further investigate which of these two components ( encoder or decoder ) has a greater impact in performance degradation would be to compare against a sequence to sequence encoder-decoder , and measure the effect of input length in that case . This , however , still has the issue that an encoder might be good at generating representations to be used as starting states for sequence decoding , but not as good when generating hidden states to be used to generate a tree . Thus , really isolating the effect of encoder and decoder in this phenomenon is quite challenging . Regarding using attention and beam search , we agree these are interesting directions and we will leave them for future work ."}, "1": {"review_id": "HkYhZDqxg-1", "review_text": "This paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures (including the topology) in an encoder-decoder setting. The architecture is well motivated and I can see several applications (in addition to what's presented in the paper) that need to generate tree structures given an unstructured data. One weakness of the paper is the limitation of experiments. IFTTT dataset seems to be an interesting appropriate application, and there is also a synthetic dataset, however it would be more interesting to see more natural language applications with syntactic tree structures. Still, I consider the experiments sufficient as a first step to showcase a novel architecture. A strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture, about when / how to decide to terminate, as opposed to making a single arbitrary choice. I see future applications of this architecture and it seems to have interesting directions for future work so I suggest its acceptance as a conference contribution.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . We chose the IFTTT task because of two main reasons . First , we wanted a task where the input was sequential , so that we could use a vanilla RNN as an encoder , thus minimizing the variability due to design choices on the encoder side . Second , we wanted a task where the actual tree was a crucial part of the output ( as opposed to just being a latent structure to generate the nodes ) , so that a regular sequence-to-sequence model could not be directly used . Though we think the IFTTT task was reasonable for this experimental setting , we agree that there are other interesting applications of the proposed model . To expand the experimental framework , we are working on an additional NLP task that will more clearly highlight the advantage of a DRNNs over sequence decoders , and which we \u2019 re aiming to have ready before the end of the review discussion period ."}, "2": {"review_id": "HkYhZDqxg-2", "review_text": "The paper propose DRNN as a neural decoder for tree structures. I like the model architecture since it has two clear improvements over traditional approaches \u2014 (1) the information flows in two directions, both from the parent and from siblings, which is desirable in tree structures (2) the model use a probability distribution to model the tree boundary (i.e. the last sibling or the leaf). This avoids the use of special ending symbols which is larger in size and putting more things to learn for the parameters (shared with other symbols). The authors test the DRNN using the tasks of recovering the synthetic trees and recovering functional programs. The model did better than traditional methods like seq2seq models. I think the recovering synthetic tree task is not very satisfying for two reasons \u2014 (1) the surface form itself already containing some of the topological information which makes the task easier than it should be (2) as we can see from figure 3, when the number of nodes grows (even to a number not very large), the performance of the model drops dramatically, I am not sure if a simple baseline only captures the topological information in the surface string would be much worse than this. And DRNN in this case, seems can\u2019t show its full potentials since the length of the information flow in the model won\u2019t be very long. I think the experiments are interesting. But I think there are some other tasks which are more difficult and the tree structure information are more important in such tasks. For example, we have the seq2seq parsing model (Vinyals et al, 2014), is it possible to use the DRNN proposed here on the decoder side? I think tasks like this can show more potentials of the DRNN and can be very convincing that model architectures like this are better than traditional alternatives.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . Regarding the difficulty of the synthetic task , the surface form indeed contains some topological information , otherwise the task would be unrealistic . But this does not necessarily mean the task is easy . Given that two topological decisions must be made for every node , there are is a combinatorial number of possible output trees for a fixed size sentence , even if always processing sentences in a breadth-first left-to-right manner . For example , using lambda-logical notation , the surface form \u201c A B C D \u201d can lead to the trees : \u201c A B C D \u201d , \u201c A ( B C D ) \u201d , \u201c A ( B ) C ( D ) \u201d , \u201c A ( B ( C ( D ) ) ) \u201d , etc . There are roughly O ( n^2 ) such possibilities if we knew the node labels . But in our setting the decoder is not given the labels , it generates them , so there \u2019 s an exponential ( in n ) number of possible outputs . So , even if there is some topological information in the surface form , recovering the correct tree directly from it without learning some general properties about feasible output trees would be very hard . The task of the DRNN decoder is thus to learn these properties during training , similar to how a seq-to-seq model must implicitly learn a language model on the decoder side , to be able to generate sentences not only utilizing the information provided by the input , but also generating feasible outputs as per previously seen data . We agree that in its current form , it \u2019 s not easy to appreciate the difficulty of the task . To address this and your point ( 2 ) , we are implementing a simple baseline to give the reader more information to understand the complexity of the task and the performance of our method . We will add these results as they become available . Regarding additional tasks , as we mention to another reviewer , we are setting up an additional ( significantly harder ) task that will evaluate the performance of our method when dealing with larger , more diverse trees . We hope to have the results ready before the review discussion deadline ."}}