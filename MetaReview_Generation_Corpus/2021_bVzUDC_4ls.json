{"year": "2021", "forum": "bVzUDC_4ls", "title": "Exploiting Verified Neural Networks via Floating Point Numerical Error", "decision": "Reject", "meta_review": "There are many recent methods for the formal verification of neural networks. However, most of these methods do not soundly model the floating-point representation of real numbers. This paper shows that this unsoundness can be exploited to construct adversarial examples for supposedly verified networks. The takeaway is that future approaches to neural network verification should take into account floating-point semantics.\n\nThis was a borderline paper. On the other hand, to anyone well-versed in formal methods, it is not surprising that unsound verification leaves the door open for exploits. Also, there is prior work (Singh et al., NeurIPS 2018) on verification of neural networks that explicitly aims for soundness w.r.t. floating-point arithmetic. On the other hand, it is true that many adversarial learning researchers do not appreciate the value of this kind of soundness. In the end, the decision came down to the significance of the result. Here I have to side with Reviewer 1: the impact of this problem is limited in the first place, and also, the issue of floating-point soundness has come up in prior work on neural network verification. For these reasons, the paper cannot be accepted this time around.", "reviews": [{"review_id": "bVzUDC_4ls-0", "review_text": "The paper presents a method to find adversarial inputs for neural networks in regions where the networks can be `` proven '' not to admit any such adversarial examples , practically demonstrating the unsoundness of a `` complete verifier '' as well as an `` incomplete verifier '' . While it was already obvious to me that `` verifiers '' that assume floating-point arithmetic is the same as real arithmetic are unsound , the paper is a service to the community in that it also makes this very obvious to informed outsiders who may not have already questioned the validity of robustness verification research that does not model round-off and even ignores it in its own implementation . The related work section does a good job of surveying the state of the art as it relates to floating-point soundness . The authors also took some space to discuss how their findings relate to current and future research on robustness verification , which I think is important in this case . Perhaps there could be a short discussion of challenges that different approaches face to become sound with respect to floating-point semantics . ( For example , it seems particularly challenging for approaches based on duality , as the correctness of certificates depends non-trivially on closed-form solutions to optimization problems as well as associativity of addition . ) The technical sections are mostly well-written , though I was not able to figure out some details . For example , it is not so clear how precisely binary search is used to find \u03b1 and \u03b4 simultaneously . Section 4.2 is a bit dense and its presentation could probably be improved . `` inevitable presence of numerical error in [ ... ] the verifier '' . It is not inevitable that the verifier is subject to `` error '' . We could encode the precise floating-point semantics of the neural network as a SAT formula ( and then watch the SAT solver time out , but this does give a sound and complete method ) .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks so much for those helpful comments ! > There could be a short discussion of challenges that different approaches face to become sound with respect to floating-point semantics . Thanks for providing this suggestion . We will include related discussions in our next revision . > For example , it is not so clear how precisely binary search is used to find $ \\alpha $ and $ \\delta $ simultaneously . Section 4.2 is a bit dense and its presentation could probably be improved . We maintain the lower bound $ \\alpha_0 $ and the upper bound $ \\alpha_1 $ in binary search , where $ \\delta = \\alpha_1 - \\alpha_0 $ . Ideally we 'd like to repeat the binary search until $ \\delta $ decreases to a predefined threshold , but it does not always work due to solver timing out . Therefore , we only conduct the binary search to minimize $ \\delta $ as much as possible , and switch to grid search if the solver times out . Thanks for pointing out this confusing part . We will improve the algorithm presentation in the next revision . > It is not inevitable that the verifier is subject to `` error '' . That 's right ! We have also discussed the possibility of using exact floating point or real arithmetic in SMT solvers . In the context of this paragraph , we are referring to `` practical '' solvers that scale nontrivially , which can not afford the complexity of exact FP modeling . Thanks for pointing out this inaccurate sentence . We 'll improve it in the next revision ."}, {"review_id": "bVzUDC_4ls-1", "review_text": "Summary : The authors develop a method to generate pairs of sample that are separated by a small adversarial perturbation , that have different class , but with the specificity that the a complete verifier would returns a result indicating that this sample admits no adversarial perturbation ( despite the fact that it does , as evidenced by the second element of the pair ) . These samples are obtained by considering a brightness perturbation of the image and finding the parameter ( alpha ) at which the verifier switch from returning `` safe '' to `` unsafe '' . The resulting perturbed image is going to have adversarial examples very close to the boundary of the region considered , so small floating point errors might result in returning incorrect results . Main thoughts : The problem that the author discuss is very well highlighted and explained . It is clear what vulnerability they identified , as well as the mechanism that they use to highlight it . On the other hand , in terms of importance , I would rank it more as an interesting observation that an actual critical problem . If we assume , that what I 'm caring is robustness of my image classification system for perturbation of size epsilon=0.1 , then it seems that the worst that can happen is that some samples that I verified to be robust for epsilon=0.1 , are in practice only robust for epsilon=0.09999 ? This does n't seem overtly critical and would result in essentially the same result in any application . Questions : - The choice of what solver to use as a backend for a MIP formulation of the Neural Network verification problem is an implementation detail . MIPVerify could well be implemented with a different solver ? ( MIP solvers returning incorrect result due to floating point errors is not a new problem and there seems to be some literature in how to adress these problems if they are considered of importance `` Safe bounds in linear and mixed-integer linear programming , Neumaier & Shcherbina '' ) In addition , could this problem be solved by simply adjusting the tolerance parameters of the solver ? I did not see any discussion of this by the authors , but I imagine that the default parameters used by the verifier might be geared more towards speed than towards perfect accuracy . - The authors mention verifiers that incorporate proper handling of floating point errors ( ERAN ) but then reject it by saying that it rely on specific implementation details of the inference algorithm . This seems strange because that 's exactly the recommendation that the authors make . Page 2 : `` any sound verifier for this class of networks must reason about the specific floating point error characteristics of the neural network implementation at hand . '' Minor questions : In Figure 1.a , it seems like for the first 4 graphs , the dotted lines which I assume implies what the difference should be are lines with slope 1 . Why would the change in the logit vector vary at the same rate as the perturbations ? Should n't there be a slope dependent on the corresponding gradient coefficient ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks so much for the constructive criticism ! # # Regarding the technical concerns > MIPVerify could well be implemented with a different solver ? It might be possible to compute the pre-relu bounds conservatively in MIPVerify and also use a safe solver such as the one discussed in [ 1 ] . However , there are potentially nontrivial barriers regarding adopting such methods : 1 . Directed rounding to maintain FP soundness poses implementation and engineering challenges . In fact , [ 1 ] presents only a theoretical analysis and does not conduct experiments to evaluate their methods . The loss of scalability needs to be investigated . 2.The directed rounding only works with a straightforward NN implementation that computes the dot product sequentially . Further research is needed to address how to adapt the method to verify other implementations . A possible option is to completely formulate the implementation computation in the MILP formulation , but that might be very slow . 3.The conservative formulation renders the verification incomplete . The exact loss of completeness needs to be evaluated . We are happy to include a brief discussion in the next version . [ 1 ] Neumaier , Arnold , and Oleg Shcherbina . `` Safe bounds in linear and mixed-integer linear programming . '' Mathematical Programming 99.2 ( 2004 ) : 283-296 . > Could this problem be solved by simply adjusting the tolerance parameters of the solver ? It 's challenging to obtain a sound tolerance . The error due to FP unsoundness can be scaled by malicious networks ( for example , multiplying $ y $ by $ 1e7 $ in Section 5 ) . > ERAN is rejected by saying that it rely on specific implementation details of the inference algorithm ERAN is sound with respect to the kind of straightforward NN implementation considered in their formulation . It can be unsound for other accelerated implementations ( such as Winograd ) that are widely adopted . How to adapt it to other implementations has not been discussed by the original authors and calls for further research . Here we are not rejecting ERAN , but emphasizing that a verifier should explicitly specify its targeted implementation ( s ) , and being versatile for many implementations is an appealing feature . We will clarify in the next version . > Why would the change in the logit vector vary at the same rate as the perturbations ? Should n't there be a slope dependent on the corresponding gradient coefficient ? The dotted line is a reference for the `` identity change '' and can be helpful in emphasizing that output change is random with respect to input change and showing that they are of the same magnitude in most cases . It is not intended to indicate what the difference should be . The change is random due to FP error and does not depend on the gradient . We will clarify in the next version . # # Regarding the impact > It seems that the worst that can happen is that some samples that I verified to be robust for epsilon=0.1 , are in practice only robust for epsilon=0.09999 ? It can not be proved that the gap of 0.00001 is sound for the target implementation and the verifier . The most important takeaway from our paper is that verifiers that ignore FP error provide * * no correctness guarantees * * . While we work on naturally trained networks for which the gap between the verifiable perturbation bound and the true robust perturbation bound might be small , in practice it is totally possible for an adversary to manipulate network weights and architecture to make this gap significantly larger . We hope our work raises the awareness regarding FP soundness among researchers , and future NN verifiers can incorporate FP soundness into their design ."}, {"review_id": "bVzUDC_4ls-2", "review_text": "This paper focuses on the floating point unsoundness of neural network verification procedures , and shows that the results from such verifiers can not be trusted . To drive home the message of the paper , the authors take MIPVerify ( which does n't ensure FP soundness ) and shows that they are able to construct adversarial examples for the cases that are returned as verified by MIPVerify . It is an interesting nice paper but the contribution is weakened by two facts : One , it is already known that MIPVerify does n't ensure FP soundness ; which is also acknowledged by its authors . Second , when FP soundness is not ensured , and given the fact that adversarial examples are widely present , it is no surprise that one could find adversarial examples . So I am split on the paper . From a formal methods perspective , the discovery of the paper is not surprising as Floating point computations are known to be important ( this is one of the reasons that SMT solvers put a lot of emphasis on FP ) . But perhaps from ML practitioner , it may be interesting .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments . > One , it is already known that MIPVerify does n't ensure FP soundness ; which is also acknowledged by its authors . The FP soundness problem has not been systematically addressed in previous work . Previous work has not gone further than reporting the accidentally found unsound results , while we discuss the fundamental tradeoff between soundness and scalability , and show that FP unsoundness can be systematically exploited to invalidate verification claims . > Second , when FP soundness is not ensured , and given the fact that adversarial examples are widely present , it is no surprise that one could find adversarial examples . We are the first to actually present a practical method that can systematically construct adversarial examples for networks claimed to be robust by a complete verifier via exploiting FP error , which we believe is an important discovery that further highlights the unsoundness of the verifiers for FP neural networks . As a not-so-proper analogy of this situation , it is unsurprising to acknowledge that the factorization of big integers exists , but presenting a practical factorization method invalidates the security guarantees of many cryptography algorithms ."}, {"review_id": "bVzUDC_4ls-3", "review_text": "In the recent literature there has been a rise in the number of papers which attempt to verify neural networks . The specification of the verification problems often gets adapted according to the application in mind . More specifically , for image classification networks , the problem is to prove that the output of the neural network does not flip for small perturbations to the pixel values . For a robotic setting , the problem is often safety and convergence to some goal state . Where the neural network operates in closed loop with the system dynamics . The authors in this paper present an adversarial attack model on neural networks , which is deemed correct by some verifier . More specifically , given a neural network which can be shown to be robust to adversarial perturbations around some input , the authors exploit numerical errors in the computations to attack the network . Demonstrating the presence of loop holes in the proving engines itself . This is due to the approximation errors introduced by using floating point numbers . In my opinion , the notion of input sets in the space of images , is not a very useful one . Mainly because the interval valued sets representing perturbations of the input image , is far removed from the intended specification . It 's a step in the right direction , if the verification of computer vision task was a well defined problem . Since it 's not clear what to verify in the first place , the use case of this paper is not a very convincing one in my opinion . The problem of verifying neural networks in a robotic setting has a more meaningful specification . Hence , i do n't think that this paper in itself will be interesting to the general theme of the conference .", "rating": "3: Clear rejection", "reply_text": "The only criticism appears to be related to the choice of the correctness specification ( specifically , the robustness against $ \\ell_\\inf $ -bounded perturbations ) . The reviewer appears to reject this specification as not useful . There is no other criticism of the research in this review . This robustness specification and similar ones are now widely accepted and used within the field , and related research using these specifications has produced interesting and useful results . For example , many existing NN verifiers , including both complete and incomplete ones , such as MIPVerify , ERAN , and CROWN , have targeted the robustness of computer vision NNs against $ \\ell_\\inf $ -bounded perturbations as an application . As another example , Madry et al . ( 2018 ) demonstrate their PGD robust training method using this $ \\ell_\\inf $ robustness formulation . Their following work ( * Adversarial Examples Are Not Bugs , They Are Features * ) that analyzes robustness with a similar specification ( based on $ \\ell_2 $ distance ) proposes robust features in datasets , which are better aligned with human perception . While we agree that such specifications may not fully capture every desirable robustness criteria , they provide a baseline that , if violated , highlights the vulnerability of even verified networks to adversarial attacks . Moreover , our work is not specifically tailored for this specification . Our most important message , that verifiers should soundly model FP computations , also applies to the verification of other specifications in other settings . Many papers with similar settings have been published in ICLR , such as [ this one ] ( https : //openreview.net/forum ? id=rJzIBfZAb ) , [ that one ] ( https : //openreview.net/forum ? id=HyGIdiRqtm ) and [ another one ] ( https : //openreview.net/forum ? id=BJfIVjAcKm ) . This fact suggests that this setting , which is the robustness of deep neural networks for computer vision tasks under $ \\ell_\\inf $ -bounded perturbations , is relevant to the general theme of the conference . ."}], "0": {"review_id": "bVzUDC_4ls-0", "review_text": "The paper presents a method to find adversarial inputs for neural networks in regions where the networks can be `` proven '' not to admit any such adversarial examples , practically demonstrating the unsoundness of a `` complete verifier '' as well as an `` incomplete verifier '' . While it was already obvious to me that `` verifiers '' that assume floating-point arithmetic is the same as real arithmetic are unsound , the paper is a service to the community in that it also makes this very obvious to informed outsiders who may not have already questioned the validity of robustness verification research that does not model round-off and even ignores it in its own implementation . The related work section does a good job of surveying the state of the art as it relates to floating-point soundness . The authors also took some space to discuss how their findings relate to current and future research on robustness verification , which I think is important in this case . Perhaps there could be a short discussion of challenges that different approaches face to become sound with respect to floating-point semantics . ( For example , it seems particularly challenging for approaches based on duality , as the correctness of certificates depends non-trivially on closed-form solutions to optimization problems as well as associativity of addition . ) The technical sections are mostly well-written , though I was not able to figure out some details . For example , it is not so clear how precisely binary search is used to find \u03b1 and \u03b4 simultaneously . Section 4.2 is a bit dense and its presentation could probably be improved . `` inevitable presence of numerical error in [ ... ] the verifier '' . It is not inevitable that the verifier is subject to `` error '' . We could encode the precise floating-point semantics of the neural network as a SAT formula ( and then watch the SAT solver time out , but this does give a sound and complete method ) .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks so much for those helpful comments ! > There could be a short discussion of challenges that different approaches face to become sound with respect to floating-point semantics . Thanks for providing this suggestion . We will include related discussions in our next revision . > For example , it is not so clear how precisely binary search is used to find $ \\alpha $ and $ \\delta $ simultaneously . Section 4.2 is a bit dense and its presentation could probably be improved . We maintain the lower bound $ \\alpha_0 $ and the upper bound $ \\alpha_1 $ in binary search , where $ \\delta = \\alpha_1 - \\alpha_0 $ . Ideally we 'd like to repeat the binary search until $ \\delta $ decreases to a predefined threshold , but it does not always work due to solver timing out . Therefore , we only conduct the binary search to minimize $ \\delta $ as much as possible , and switch to grid search if the solver times out . Thanks for pointing out this confusing part . We will improve the algorithm presentation in the next revision . > It is not inevitable that the verifier is subject to `` error '' . That 's right ! We have also discussed the possibility of using exact floating point or real arithmetic in SMT solvers . In the context of this paragraph , we are referring to `` practical '' solvers that scale nontrivially , which can not afford the complexity of exact FP modeling . Thanks for pointing out this inaccurate sentence . We 'll improve it in the next revision ."}, "1": {"review_id": "bVzUDC_4ls-1", "review_text": "Summary : The authors develop a method to generate pairs of sample that are separated by a small adversarial perturbation , that have different class , but with the specificity that the a complete verifier would returns a result indicating that this sample admits no adversarial perturbation ( despite the fact that it does , as evidenced by the second element of the pair ) . These samples are obtained by considering a brightness perturbation of the image and finding the parameter ( alpha ) at which the verifier switch from returning `` safe '' to `` unsafe '' . The resulting perturbed image is going to have adversarial examples very close to the boundary of the region considered , so small floating point errors might result in returning incorrect results . Main thoughts : The problem that the author discuss is very well highlighted and explained . It is clear what vulnerability they identified , as well as the mechanism that they use to highlight it . On the other hand , in terms of importance , I would rank it more as an interesting observation that an actual critical problem . If we assume , that what I 'm caring is robustness of my image classification system for perturbation of size epsilon=0.1 , then it seems that the worst that can happen is that some samples that I verified to be robust for epsilon=0.1 , are in practice only robust for epsilon=0.09999 ? This does n't seem overtly critical and would result in essentially the same result in any application . Questions : - The choice of what solver to use as a backend for a MIP formulation of the Neural Network verification problem is an implementation detail . MIPVerify could well be implemented with a different solver ? ( MIP solvers returning incorrect result due to floating point errors is not a new problem and there seems to be some literature in how to adress these problems if they are considered of importance `` Safe bounds in linear and mixed-integer linear programming , Neumaier & Shcherbina '' ) In addition , could this problem be solved by simply adjusting the tolerance parameters of the solver ? I did not see any discussion of this by the authors , but I imagine that the default parameters used by the verifier might be geared more towards speed than towards perfect accuracy . - The authors mention verifiers that incorporate proper handling of floating point errors ( ERAN ) but then reject it by saying that it rely on specific implementation details of the inference algorithm . This seems strange because that 's exactly the recommendation that the authors make . Page 2 : `` any sound verifier for this class of networks must reason about the specific floating point error characteristics of the neural network implementation at hand . '' Minor questions : In Figure 1.a , it seems like for the first 4 graphs , the dotted lines which I assume implies what the difference should be are lines with slope 1 . Why would the change in the logit vector vary at the same rate as the perturbations ? Should n't there be a slope dependent on the corresponding gradient coefficient ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks so much for the constructive criticism ! # # Regarding the technical concerns > MIPVerify could well be implemented with a different solver ? It might be possible to compute the pre-relu bounds conservatively in MIPVerify and also use a safe solver such as the one discussed in [ 1 ] . However , there are potentially nontrivial barriers regarding adopting such methods : 1 . Directed rounding to maintain FP soundness poses implementation and engineering challenges . In fact , [ 1 ] presents only a theoretical analysis and does not conduct experiments to evaluate their methods . The loss of scalability needs to be investigated . 2.The directed rounding only works with a straightforward NN implementation that computes the dot product sequentially . Further research is needed to address how to adapt the method to verify other implementations . A possible option is to completely formulate the implementation computation in the MILP formulation , but that might be very slow . 3.The conservative formulation renders the verification incomplete . The exact loss of completeness needs to be evaluated . We are happy to include a brief discussion in the next version . [ 1 ] Neumaier , Arnold , and Oleg Shcherbina . `` Safe bounds in linear and mixed-integer linear programming . '' Mathematical Programming 99.2 ( 2004 ) : 283-296 . > Could this problem be solved by simply adjusting the tolerance parameters of the solver ? It 's challenging to obtain a sound tolerance . The error due to FP unsoundness can be scaled by malicious networks ( for example , multiplying $ y $ by $ 1e7 $ in Section 5 ) . > ERAN is rejected by saying that it rely on specific implementation details of the inference algorithm ERAN is sound with respect to the kind of straightforward NN implementation considered in their formulation . It can be unsound for other accelerated implementations ( such as Winograd ) that are widely adopted . How to adapt it to other implementations has not been discussed by the original authors and calls for further research . Here we are not rejecting ERAN , but emphasizing that a verifier should explicitly specify its targeted implementation ( s ) , and being versatile for many implementations is an appealing feature . We will clarify in the next version . > Why would the change in the logit vector vary at the same rate as the perturbations ? Should n't there be a slope dependent on the corresponding gradient coefficient ? The dotted line is a reference for the `` identity change '' and can be helpful in emphasizing that output change is random with respect to input change and showing that they are of the same magnitude in most cases . It is not intended to indicate what the difference should be . The change is random due to FP error and does not depend on the gradient . We will clarify in the next version . # # Regarding the impact > It seems that the worst that can happen is that some samples that I verified to be robust for epsilon=0.1 , are in practice only robust for epsilon=0.09999 ? It can not be proved that the gap of 0.00001 is sound for the target implementation and the verifier . The most important takeaway from our paper is that verifiers that ignore FP error provide * * no correctness guarantees * * . While we work on naturally trained networks for which the gap between the verifiable perturbation bound and the true robust perturbation bound might be small , in practice it is totally possible for an adversary to manipulate network weights and architecture to make this gap significantly larger . We hope our work raises the awareness regarding FP soundness among researchers , and future NN verifiers can incorporate FP soundness into their design ."}, "2": {"review_id": "bVzUDC_4ls-2", "review_text": "This paper focuses on the floating point unsoundness of neural network verification procedures , and shows that the results from such verifiers can not be trusted . To drive home the message of the paper , the authors take MIPVerify ( which does n't ensure FP soundness ) and shows that they are able to construct adversarial examples for the cases that are returned as verified by MIPVerify . It is an interesting nice paper but the contribution is weakened by two facts : One , it is already known that MIPVerify does n't ensure FP soundness ; which is also acknowledged by its authors . Second , when FP soundness is not ensured , and given the fact that adversarial examples are widely present , it is no surprise that one could find adversarial examples . So I am split on the paper . From a formal methods perspective , the discovery of the paper is not surprising as Floating point computations are known to be important ( this is one of the reasons that SMT solvers put a lot of emphasis on FP ) . But perhaps from ML practitioner , it may be interesting .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments . > One , it is already known that MIPVerify does n't ensure FP soundness ; which is also acknowledged by its authors . The FP soundness problem has not been systematically addressed in previous work . Previous work has not gone further than reporting the accidentally found unsound results , while we discuss the fundamental tradeoff between soundness and scalability , and show that FP unsoundness can be systematically exploited to invalidate verification claims . > Second , when FP soundness is not ensured , and given the fact that adversarial examples are widely present , it is no surprise that one could find adversarial examples . We are the first to actually present a practical method that can systematically construct adversarial examples for networks claimed to be robust by a complete verifier via exploiting FP error , which we believe is an important discovery that further highlights the unsoundness of the verifiers for FP neural networks . As a not-so-proper analogy of this situation , it is unsurprising to acknowledge that the factorization of big integers exists , but presenting a practical factorization method invalidates the security guarantees of many cryptography algorithms ."}, "3": {"review_id": "bVzUDC_4ls-3", "review_text": "In the recent literature there has been a rise in the number of papers which attempt to verify neural networks . The specification of the verification problems often gets adapted according to the application in mind . More specifically , for image classification networks , the problem is to prove that the output of the neural network does not flip for small perturbations to the pixel values . For a robotic setting , the problem is often safety and convergence to some goal state . Where the neural network operates in closed loop with the system dynamics . The authors in this paper present an adversarial attack model on neural networks , which is deemed correct by some verifier . More specifically , given a neural network which can be shown to be robust to adversarial perturbations around some input , the authors exploit numerical errors in the computations to attack the network . Demonstrating the presence of loop holes in the proving engines itself . This is due to the approximation errors introduced by using floating point numbers . In my opinion , the notion of input sets in the space of images , is not a very useful one . Mainly because the interval valued sets representing perturbations of the input image , is far removed from the intended specification . It 's a step in the right direction , if the verification of computer vision task was a well defined problem . Since it 's not clear what to verify in the first place , the use case of this paper is not a very convincing one in my opinion . The problem of verifying neural networks in a robotic setting has a more meaningful specification . Hence , i do n't think that this paper in itself will be interesting to the general theme of the conference .", "rating": "3: Clear rejection", "reply_text": "The only criticism appears to be related to the choice of the correctness specification ( specifically , the robustness against $ \\ell_\\inf $ -bounded perturbations ) . The reviewer appears to reject this specification as not useful . There is no other criticism of the research in this review . This robustness specification and similar ones are now widely accepted and used within the field , and related research using these specifications has produced interesting and useful results . For example , many existing NN verifiers , including both complete and incomplete ones , such as MIPVerify , ERAN , and CROWN , have targeted the robustness of computer vision NNs against $ \\ell_\\inf $ -bounded perturbations as an application . As another example , Madry et al . ( 2018 ) demonstrate their PGD robust training method using this $ \\ell_\\inf $ robustness formulation . Their following work ( * Adversarial Examples Are Not Bugs , They Are Features * ) that analyzes robustness with a similar specification ( based on $ \\ell_2 $ distance ) proposes robust features in datasets , which are better aligned with human perception . While we agree that such specifications may not fully capture every desirable robustness criteria , they provide a baseline that , if violated , highlights the vulnerability of even verified networks to adversarial attacks . Moreover , our work is not specifically tailored for this specification . Our most important message , that verifiers should soundly model FP computations , also applies to the verification of other specifications in other settings . Many papers with similar settings have been published in ICLR , such as [ this one ] ( https : //openreview.net/forum ? id=rJzIBfZAb ) , [ that one ] ( https : //openreview.net/forum ? id=HyGIdiRqtm ) and [ another one ] ( https : //openreview.net/forum ? id=BJfIVjAcKm ) . This fact suggests that this setting , which is the robustness of deep neural networks for computer vision tasks under $ \\ell_\\inf $ -bounded perturbations , is relevant to the general theme of the conference . ."}}