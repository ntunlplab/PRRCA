{"year": "2020", "forum": "ryxC6kSYPr", "title": "Infinite-Horizon Differentiable Model Predictive Control", "decision": "Accept (Poster)", "meta_review": "This paper develops a linear quadratic model predictive control approach for safe imitation learning.  The main contribution is an analytic solution for the derivative of the discrete-time algebraic Riccati equation (DARE).  This allows an infinite horizon optimality objective to be used with differentiation-based learning methods.  An additional contribution is the problem reformulation with a pre-stabilizing controller and the support of state constraints throughout the learning process.  The method is tested on a damped-spring system and a vehicle platooning problem.\n\nThe reviewers and the author response covered several topics. The reviewers appreciated the research direction and theoretical contributions of this work.  The reviewers main concern was the experimental evaluation, which was originally limited to a damped spring system.  The authors added another experiment for a substantially more complex continuous control domain.  In response to the reviewers, the authors also described how this work relates to non-linear control problems.  The authors also clarified the ability of the proposed method to handle state-based constraints that are not handled by earlier methods.  The reviewers were largely satisfied with these changes.\n\nThis paper should be accepted as the reviewers are satisfied that the paper has useful contributions.", "reviews": [{"review_id": "ryxC6kSYPr-0", "review_text": "The paper shows how to use the Discrete-time Algebraic Riccati Equation (DARE) to provide infinite horizon stability & optimality to differentiable MPC learning. The paper also shows how to use DARE to derive a pre-stabilizing (linear state-feedback) controller. The paper provides a theoretical characterization of the problem setting, which shows that prior work on differentiable MPC learning may lead to unstable controllers without the proposed augmentations using DARE. I'm not sure I understand the implications of imitating \"from an expert of the same class\". Can the authors elaborate? Can the authors compare & contrast with this paper? https://arxiv.org/abs/1709.07174 (I have my own views, but I'd like hear the authors' thoughts first) My biggest complaint is with regards to the experiments. Unless I'm mistaken, it seems there isn't a thorough empirical study of the theoretical claims, especially as it relates to previous work. E.g., can one construct scenarios where the baseline approach (Amos et al., 2018) fails, and compare with the proposed approach? The idea of pre-stabilization is interesting, and seems related to this paper: https://arxiv.org/abs/1905.05380 **** After Author Response **** Thanks for the response, I am raising my score to weak accept.", "rating": "6: Weak Accept", "reply_text": "We would like to thank the reviewers for taking the time to provide comprehensive and constructive reviews . An issue that has been highlighted by all three reviewers is the low complexity of the numerical experiments , so the response to all three reviewers starts with the same text that addresses this concern : We concur with Reviewer 1 \u2019 s observation that the investigation of simple systems is necessary as a sanity check , and this was the rationale of the numerical experiments presented here , but we also agree that it is a fair observation to highlight the lack of complexity as a limitation of the paper . In direct response to Reviewer 4 \u2019 s suggestion to investigate a nonlinear system that has been successively linearised , the DARE can not be used for non-linear systems , unless in cases where the non-linearities are quite limited . This is also discussed in Appendix E of our paper . Extensions to some interesting non-linear cases are going to be the subject of a follow-up study . In direct response to Reviewer 3 \u2019 s comment \u2018 can one construct scenarios where the baseline approach ( Amos et al. , 2018 ) fails ? \u2019 \u2013 one of the significant limitations of ( Amos et al. , 2018 ) is that state constraints are not included in their differentiable MPC formulation ( see equation ( 10 ) in their paper ) . As a consequence , their approach very quickly fails in general for even the simple LTI case presented here when state constraints are included because the MPC optimization has become infeasible in the forwards pass , and so one of the major contributions of this paper is ensuring feasibility in the presence of state constraints . We therefore believe that the numerical demonstration with an LTI system is necessary to provide credibility for when the approach is extended to non-linear systems , but agree that a 2DOF system is insufficient . We propose to investigate a second LTI system with a larger amount of states and inputs , inspired by a real-world application . Would the reviewers be satisfied with this additional experiment ? Comments specific to Reviewer 3 : \u2022 By \u2018 the same class \u2019 we mean that the expert and learner are both 2DOF LTI systems controlled with Quadratic box-constrained MPC controllers . This comment has been removed in the updated paper to improve clarity . \u2022 We thank the reviewer for the suggested references , which are both are of interest \u2022 In the first suggested paper ( Y. Pan et al . ) the \u2018 expert \u2019 controller takes the form of a model predictive controller where the model is a gaussian process and the solution is provided using differential dynamic programming . The \u2018 learner \u2019 however , is simply a deep neural network , for which it is generally impossible to determine whether the learned controller will satisfy hard constraints on the system state a-priori , or whether it will be stabilizing . The entire purpose of the work presented in this paper is to provide interpretable structure to a neural network when used for imitation learning , for which hard constraint satisfaction and stability can be guaranteed a-priori , provided that the prediction error of the MPC model is limited . Ultimately , our study , as well as the previous ones on differentiable MPC , aims towards future integration of more complex sensory ( e.g.visual ) information as done in Y. Pan et al.However , in this work we decided to focus on improving one part of the stack , i.e.on establishing conditions for having a more stable and optimal method to imitate controllers from given state-action measurements . The integration of visual data and of convolutional networks is of great interest and will be addressed in future work . \u2022 The second paper ( R. Cheng et al . ) deals with reinforcement learning , where a prior policy is introduced to reduce variance and , in the case of H-infinity robust control , to provide stability with respect to the \u201c uncertainty \u201d resulting in the use of an RL policy . The two policies are weighted and it is shown that this results in a regularization of the original loss . This is clearly inspired by robust control , and in the general case adding stability or robustness will lead to an inevitable level of conservatism or sub optimality with respect to the original reward or loss . In this paper the re-parameterisation does not lead to sub-optimality and does not alter the MPC problem ."}, {"review_id": "ryxC6kSYPr-1", "review_text": "This paper shows how to make infinite-horizon MPC differentiable by differentiating through the terminal cost function and controller. Recent work in non-convex finite-horizon continuous control [1,2,3] face a huge issue in selecting the controller's horizon length and better-understanding differentiable infinite horizon control has potentially strong applications in these domains. As a step in this non-convex direction, this paper provides a nice investigation in the convex LTI case. The imitation learning experiments on a small spring dynamical system are a necessary sanity check for further work, but many other more complex systems could be empirically studied and would have made this paper stronger. One point that would be useful to clarify: the DARE solution in (7,8) is derived to optimally control a LTI system *without* control/state bounds but is then used to control the LTI system *with* control/state bounds in (4). Does this lead to suboptimal solutions to the true infinite-horizon problem? [1] Chua, K., Calandra, R., McAllister, R., & Levine, S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. NeurIPS 2018. [2] Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., & Davidson, J. Learning latent dynamics for planning from pixels. ICML 2019. [3] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba. Benchmarking Model-Based Reinforcement Learning. arXiv 2019.", "rating": "6: Weak Accept", "reply_text": "We would like to thank the reviewers for taking the time to provide comprehensive and constructive reviews . An issue that has been highlighted by all three reviewers is the low complexity of the numerical experiments , so the response to all three reviewers starts with the same text that addresses this concern : We concur with Reviewer 1 \u2019 s observation that the investigation of simple systems is necessary as a sanity check , and this was the rationale of the numerical experiments presented here , but we also agree that it is a fair observation to highlight the lack of complexity as a limitation of the paper . In direct response to Reviewer 4 \u2019 s suggestion to investigate a nonlinear system that has been successively linearised , the DARE can not be used for non-linear systems , unless in cases where the non-linearities are quite limited . This is also discussed in Appendix E of our paper . Extensions to some interesting non-linear cases are going to be the subject of a follow-up study . In direct response to Reviewer 3 \u2019 s comment \u2018 can one construct scenarios where the baseline approach ( Amos et al. , 2018 ) fails ? \u2019 \u2013 one of the significant limitations of ( Amos et al. , 2018 ) is that state constraints are not included in their differentiable MPC formulation ( see equation ( 10 ) in their paper ) . As a consequence , their approach very quickly fails in general for even the simple LTI case presented here when state constraints are included because the MPC optimization has become infeasible in the forwards pass , and so one of the major contributions of this paper is ensuring feasibility in the presence of state constraints . We therefore believe that the numerical demonstration with an LTI system is necessary to provide credibility for when the approach is extended to non-linear systems , but agree that a 2DOF system is insufficient . We propose to investigate a second LTI system with a larger amount of states and inputs , inspired by a real-world application . Would the reviewers be satisfied with this additional experiment ? Comments specific to Reviewer 1 : \u201c the DARE solution in ( 7,8 ) is derived to optimally control a LTI system * without * control/state bounds but is then used to control the LTI system * with * control/state bounds in ( 4 ) \u2026 \u201d The DARE solution simply stabilizes the system in the absence of constraints and thus improves the numerical conditioning of the problem of optimizing the perturbation ( delta u ) . The optimization of this perturbation signal ensures that the predicted control sequence is optimal ( in an open-loop sense ) for the constrained problem . A feedback control law is obtained by repeating the optimization at each timestep using current information on the system state ."}, {"review_id": "ryxC6kSYPr-2", "review_text": "This paper continues the recent direction (e.g. Amos & Kolter 2017) of differentiating through optimal control solutions, allowing for the combination of optimal control methods and learning systems. The paper has some nice contributions and I find this research direction to be very exciting, which is why I think it merits acceptance, however I find the experiments (Section 4) could be greatly improved. The main contribution of the paper are the analytical derivative of the solution to the DARE. The pre-stabilising controller reformulation is a neat trick. The main issue I have with this paper is that the experiments are performed only on a toy 2D problem. Even an LTI system can be interesting! Of course it is important to start with a toy problem, but once positive results have been shown, it would be much more convincing if the paper showed some more complicated system, possibly an iteratively linearised non-linear system. My feeling (and possibly many others') is that these type on differentiable controllers can be extremely powerful, however this power is sadly not demonstrated here. errata: before eq (3): dt is not a pertubation to the feedback control eq (4) argmin over \\delta u rather than \\delta, presumably ", "rating": "6: Weak Accept", "reply_text": "We would like to thank the reviewers for taking the time to provide comprehensive and constructive reviews . An issue that has been highlighted by all three reviewers is the low complexity of the numerical experiments , so the response to all three reviewers starts with the same text that addresses this concern : We concur with Reviewer 1 \u2019 s observation that the investigation of simple systems is necessary as a sanity check , and this was the rationale of the numerical experiments presented here , but we also agree that it is a fair observation to highlight the lack of complexity as a limitation of the paper . In direct response to Reviewer 4 \u2019 s suggestion to investigate a nonlinear system that has been successively linearised , the DARE can not be used for non-linear systems , unless in cases where the non-linearities are quite limited . This is also discussed in Appendix E of our paper . Extensions to some interesting non-linear cases are going to be the subject of a follow-up study . In direct response to Reviewer 3 \u2019 s comment \u2018 can one construct scenarios where the baseline approach ( Amos et al. , 2018 ) fails ? \u2019 \u2013 one of the significant limitations of ( Amos et al. , 2018 ) is that state constraints are not included in their differentiable MPC formulation ( see equation ( 10 ) in their paper ) . As a consequence , their approach very quickly fails in general for even the simple LTI case presented here when state constraints are included because the MPC optimization has become infeasible in the forwards pass , and so one of the major contributions of this paper is ensuring feasibility in the presence of state constraints . We therefore believe that the numerical demonstration with an LTI system is necessary to provide credibility for when the approach is extended to non-linear systems , but agree that a 2DOF system is insufficient . We propose to investigate a second LTI system with a larger amount of states and inputs , inspired by a real-world application . Would the reviewers be satisfied with this additional experiment ? Comments specific to Reviewer 4 : We thank the reviewer for the thorough reading of the paper and will include the corrections in an updated paper ."}], "0": {"review_id": "ryxC6kSYPr-0", "review_text": "The paper shows how to use the Discrete-time Algebraic Riccati Equation (DARE) to provide infinite horizon stability & optimality to differentiable MPC learning. The paper also shows how to use DARE to derive a pre-stabilizing (linear state-feedback) controller. The paper provides a theoretical characterization of the problem setting, which shows that prior work on differentiable MPC learning may lead to unstable controllers without the proposed augmentations using DARE. I'm not sure I understand the implications of imitating \"from an expert of the same class\". Can the authors elaborate? Can the authors compare & contrast with this paper? https://arxiv.org/abs/1709.07174 (I have my own views, but I'd like hear the authors' thoughts first) My biggest complaint is with regards to the experiments. Unless I'm mistaken, it seems there isn't a thorough empirical study of the theoretical claims, especially as it relates to previous work. E.g., can one construct scenarios where the baseline approach (Amos et al., 2018) fails, and compare with the proposed approach? The idea of pre-stabilization is interesting, and seems related to this paper: https://arxiv.org/abs/1905.05380 **** After Author Response **** Thanks for the response, I am raising my score to weak accept.", "rating": "6: Weak Accept", "reply_text": "We would like to thank the reviewers for taking the time to provide comprehensive and constructive reviews . An issue that has been highlighted by all three reviewers is the low complexity of the numerical experiments , so the response to all three reviewers starts with the same text that addresses this concern : We concur with Reviewer 1 \u2019 s observation that the investigation of simple systems is necessary as a sanity check , and this was the rationale of the numerical experiments presented here , but we also agree that it is a fair observation to highlight the lack of complexity as a limitation of the paper . In direct response to Reviewer 4 \u2019 s suggestion to investigate a nonlinear system that has been successively linearised , the DARE can not be used for non-linear systems , unless in cases where the non-linearities are quite limited . This is also discussed in Appendix E of our paper . Extensions to some interesting non-linear cases are going to be the subject of a follow-up study . In direct response to Reviewer 3 \u2019 s comment \u2018 can one construct scenarios where the baseline approach ( Amos et al. , 2018 ) fails ? \u2019 \u2013 one of the significant limitations of ( Amos et al. , 2018 ) is that state constraints are not included in their differentiable MPC formulation ( see equation ( 10 ) in their paper ) . As a consequence , their approach very quickly fails in general for even the simple LTI case presented here when state constraints are included because the MPC optimization has become infeasible in the forwards pass , and so one of the major contributions of this paper is ensuring feasibility in the presence of state constraints . We therefore believe that the numerical demonstration with an LTI system is necessary to provide credibility for when the approach is extended to non-linear systems , but agree that a 2DOF system is insufficient . We propose to investigate a second LTI system with a larger amount of states and inputs , inspired by a real-world application . Would the reviewers be satisfied with this additional experiment ? Comments specific to Reviewer 3 : \u2022 By \u2018 the same class \u2019 we mean that the expert and learner are both 2DOF LTI systems controlled with Quadratic box-constrained MPC controllers . This comment has been removed in the updated paper to improve clarity . \u2022 We thank the reviewer for the suggested references , which are both are of interest \u2022 In the first suggested paper ( Y. Pan et al . ) the \u2018 expert \u2019 controller takes the form of a model predictive controller where the model is a gaussian process and the solution is provided using differential dynamic programming . The \u2018 learner \u2019 however , is simply a deep neural network , for which it is generally impossible to determine whether the learned controller will satisfy hard constraints on the system state a-priori , or whether it will be stabilizing . The entire purpose of the work presented in this paper is to provide interpretable structure to a neural network when used for imitation learning , for which hard constraint satisfaction and stability can be guaranteed a-priori , provided that the prediction error of the MPC model is limited . Ultimately , our study , as well as the previous ones on differentiable MPC , aims towards future integration of more complex sensory ( e.g.visual ) information as done in Y. Pan et al.However , in this work we decided to focus on improving one part of the stack , i.e.on establishing conditions for having a more stable and optimal method to imitate controllers from given state-action measurements . The integration of visual data and of convolutional networks is of great interest and will be addressed in future work . \u2022 The second paper ( R. Cheng et al . ) deals with reinforcement learning , where a prior policy is introduced to reduce variance and , in the case of H-infinity robust control , to provide stability with respect to the \u201c uncertainty \u201d resulting in the use of an RL policy . The two policies are weighted and it is shown that this results in a regularization of the original loss . This is clearly inspired by robust control , and in the general case adding stability or robustness will lead to an inevitable level of conservatism or sub optimality with respect to the original reward or loss . In this paper the re-parameterisation does not lead to sub-optimality and does not alter the MPC problem ."}, "1": {"review_id": "ryxC6kSYPr-1", "review_text": "This paper shows how to make infinite-horizon MPC differentiable by differentiating through the terminal cost function and controller. Recent work in non-convex finite-horizon continuous control [1,2,3] face a huge issue in selecting the controller's horizon length and better-understanding differentiable infinite horizon control has potentially strong applications in these domains. As a step in this non-convex direction, this paper provides a nice investigation in the convex LTI case. The imitation learning experiments on a small spring dynamical system are a necessary sanity check for further work, but many other more complex systems could be empirically studied and would have made this paper stronger. One point that would be useful to clarify: the DARE solution in (7,8) is derived to optimally control a LTI system *without* control/state bounds but is then used to control the LTI system *with* control/state bounds in (4). Does this lead to suboptimal solutions to the true infinite-horizon problem? [1] Chua, K., Calandra, R., McAllister, R., & Levine, S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. NeurIPS 2018. [2] Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., & Davidson, J. Learning latent dynamics for planning from pixels. ICML 2019. [3] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba. Benchmarking Model-Based Reinforcement Learning. arXiv 2019.", "rating": "6: Weak Accept", "reply_text": "We would like to thank the reviewers for taking the time to provide comprehensive and constructive reviews . An issue that has been highlighted by all three reviewers is the low complexity of the numerical experiments , so the response to all three reviewers starts with the same text that addresses this concern : We concur with Reviewer 1 \u2019 s observation that the investigation of simple systems is necessary as a sanity check , and this was the rationale of the numerical experiments presented here , but we also agree that it is a fair observation to highlight the lack of complexity as a limitation of the paper . In direct response to Reviewer 4 \u2019 s suggestion to investigate a nonlinear system that has been successively linearised , the DARE can not be used for non-linear systems , unless in cases where the non-linearities are quite limited . This is also discussed in Appendix E of our paper . Extensions to some interesting non-linear cases are going to be the subject of a follow-up study . In direct response to Reviewer 3 \u2019 s comment \u2018 can one construct scenarios where the baseline approach ( Amos et al. , 2018 ) fails ? \u2019 \u2013 one of the significant limitations of ( Amos et al. , 2018 ) is that state constraints are not included in their differentiable MPC formulation ( see equation ( 10 ) in their paper ) . As a consequence , their approach very quickly fails in general for even the simple LTI case presented here when state constraints are included because the MPC optimization has become infeasible in the forwards pass , and so one of the major contributions of this paper is ensuring feasibility in the presence of state constraints . We therefore believe that the numerical demonstration with an LTI system is necessary to provide credibility for when the approach is extended to non-linear systems , but agree that a 2DOF system is insufficient . We propose to investigate a second LTI system with a larger amount of states and inputs , inspired by a real-world application . Would the reviewers be satisfied with this additional experiment ? Comments specific to Reviewer 1 : \u201c the DARE solution in ( 7,8 ) is derived to optimally control a LTI system * without * control/state bounds but is then used to control the LTI system * with * control/state bounds in ( 4 ) \u2026 \u201d The DARE solution simply stabilizes the system in the absence of constraints and thus improves the numerical conditioning of the problem of optimizing the perturbation ( delta u ) . The optimization of this perturbation signal ensures that the predicted control sequence is optimal ( in an open-loop sense ) for the constrained problem . A feedback control law is obtained by repeating the optimization at each timestep using current information on the system state ."}, "2": {"review_id": "ryxC6kSYPr-2", "review_text": "This paper continues the recent direction (e.g. Amos & Kolter 2017) of differentiating through optimal control solutions, allowing for the combination of optimal control methods and learning systems. The paper has some nice contributions and I find this research direction to be very exciting, which is why I think it merits acceptance, however I find the experiments (Section 4) could be greatly improved. The main contribution of the paper are the analytical derivative of the solution to the DARE. The pre-stabilising controller reformulation is a neat trick. The main issue I have with this paper is that the experiments are performed only on a toy 2D problem. Even an LTI system can be interesting! Of course it is important to start with a toy problem, but once positive results have been shown, it would be much more convincing if the paper showed some more complicated system, possibly an iteratively linearised non-linear system. My feeling (and possibly many others') is that these type on differentiable controllers can be extremely powerful, however this power is sadly not demonstrated here. errata: before eq (3): dt is not a pertubation to the feedback control eq (4) argmin over \\delta u rather than \\delta, presumably ", "rating": "6: Weak Accept", "reply_text": "We would like to thank the reviewers for taking the time to provide comprehensive and constructive reviews . An issue that has been highlighted by all three reviewers is the low complexity of the numerical experiments , so the response to all three reviewers starts with the same text that addresses this concern : We concur with Reviewer 1 \u2019 s observation that the investigation of simple systems is necessary as a sanity check , and this was the rationale of the numerical experiments presented here , but we also agree that it is a fair observation to highlight the lack of complexity as a limitation of the paper . In direct response to Reviewer 4 \u2019 s suggestion to investigate a nonlinear system that has been successively linearised , the DARE can not be used for non-linear systems , unless in cases where the non-linearities are quite limited . This is also discussed in Appendix E of our paper . Extensions to some interesting non-linear cases are going to be the subject of a follow-up study . In direct response to Reviewer 3 \u2019 s comment \u2018 can one construct scenarios where the baseline approach ( Amos et al. , 2018 ) fails ? \u2019 \u2013 one of the significant limitations of ( Amos et al. , 2018 ) is that state constraints are not included in their differentiable MPC formulation ( see equation ( 10 ) in their paper ) . As a consequence , their approach very quickly fails in general for even the simple LTI case presented here when state constraints are included because the MPC optimization has become infeasible in the forwards pass , and so one of the major contributions of this paper is ensuring feasibility in the presence of state constraints . We therefore believe that the numerical demonstration with an LTI system is necessary to provide credibility for when the approach is extended to non-linear systems , but agree that a 2DOF system is insufficient . We propose to investigate a second LTI system with a larger amount of states and inputs , inspired by a real-world application . Would the reviewers be satisfied with this additional experiment ? Comments specific to Reviewer 4 : We thank the reviewer for the thorough reading of the paper and will include the corrections in an updated paper ."}}