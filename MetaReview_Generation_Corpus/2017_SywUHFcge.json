{"year": "2017", "forum": "SywUHFcge", "title": " A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Samples", "decision": "Invite to Workshop Track", "meta_review": "The authors propose a framework to analyze \"robustness\" to adversarial perturbations using topological concepts. The authors conduct an empirical study using a siamese networks. \n \n The paper generated extensive discussions. The authors implemented many changes following the reviewers' suggestions. The resulting version of the paper is rather dense. It is unclear whether a conference is appropriate for reviewing such material in limited time. \n \n We invite the authors to present their main results in the workshop track. Revising the material of the paper will generate a stronger submission to a future venue such as a journal.", "reviews": [{"review_id": "SywUHFcge-0", "review_text": "Under my point of view, the robustness of a classifier against adversarial noise it is interesting if we find any relationship between that robustness and generalization to new unseen test samples. I guess that this relationship is direct in most of the problems but perhaps classifier C1 could be more robust than C2 against adv. noise but not better for new unseen samples from the task in consideration. Best results on new unseen samples are normally related to robustness against the common distortions of the data, e.g. invariance to scale, rotation\u2026 than robustness to adv. noise. I can not see any direct conclusion from table 5 results. Essentially i am not convinced about the necessity to measure the robustness against adversarial noise. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q1 : \u2026.the robustness of a classifier against adversarial noise is interesting if we find any relationship between that robustness and generalization to new unseen test samples . I guess that this relationship is direct in most of the problems \u2026 Answer : If we understand this question correctly , we think this question is about the relationship between adversarial robustness and test accuracy . Our theoretical analysis in Table 3 provides four cases to understand the relationship between robustness and accuracy . Two typical examples can be derived from Table 3 : - If $ f_1 $ misses discovering some related features and does not extract unrelated features , $ f_1 $ is strong-robust ( even tough its accuracy may be not satisfactory ) . This is illustrated by a simple example in Figure 6 showing 1 = n_1 < n_2 = 2 , X_1 \\subset X_2 . In terms of classification , $ f_1 $ ( green boundary line ) is not accurate according to $ f_2 $ ( red boundary line ) . However , $ f_1 $ is strong-robust to the adversarial samples . For Test-sample Case ( c ) see the discussions of boundary points in Section 3.5.4 . - If $ f_1 $ uses some extra and unrelated features , it will not be strong-robust to adversarial attacks ( though it may be an accurate predictor ) . This is illustrated by a simple example in Figure 2 . In the adversarial setting , we should aim to get a classifier that is both strong-robust and accurate . A better feature learning is exactly the solution that may improve both goals . This question partially relates to the discussion of `` how g function and c function influence the adverial robustness '' . For concrete discussion see Section 3.5.3 -- `` g_1 MATTERS FOR THE STRONG - ROBUSTNESS AND c_1 DOES NOT '' ."}, {"review_id": "SywUHFcge-1", "review_text": "This paper aims at making three contributions: - Charecterizing robustness to adversarials in a topological manner. - Connecting the topological characterization to more quantitative measurements and evaluating deep networks. - Using Siamese network training to create models robust to adversarial in a practical manner and evaluate their properties. In my opinion the paper would improve greatly if the first, topological analysis attempt would be removed from the paper altogether. A central notion of the paper is the abstract characterization of robustness. The main weakness is the notion of strong robustness itself, which is an extremely rigid notion. It requires the partitioning of the predictor function by class to match the exact partitioning of the oracle. This robustness is almost never the case in real life: it requires that the predictor is almost perfect. The main flaw however is that the output space is assumed to have discrete topology and continuity is assumed for the classifier. Continuity of the classifier wrt. a discrete output is also never really satisfied. However, if the output space is assumed to be continues values with an interesting topology (like probabilities), then the notion of strong robustness becomes so constrained and strict, that it has even less practical sense and relevance. Based on those definition, several uninteresting, trivial consequences follow. They seem to be true, with inelegant proofs, but that matters little as they seem irrelevant for any practical purposes. The second part is a well executed experiment by training a Siamese architecture with an explicit additional robustness constraint. The approach seems to be working very well, but is compared only to a baseline (stability training) which performs worse than the original model without any trainings for adversarials. This is strange as adversarial examples have been studied extensively in the past year and several methods claimed improvements over the original model not trained for robustness. The experimental section and approach would look interesting, if it were compared with a stronger baseline, however the empty theoretical definitions and analysis attempts make the paper in its current form unappealing. ", "rating": "3: Clear rejection", "reply_text": "Q1 : The main weakness is the notion of strong robustness itself , which is an extremely rigid notion . It requires the partitioning of the predictor function by class to match the exact partitioning of the oracle . This robustness is almost never the case in real life : it requires that the predictor is almost perfect . Answer : We want to point out that this comment indicates some incorrect understanding of our paper : 1 . Strong-robustness \u201c does NOT require the partitioning of the predictor function by class to match the exact partitioning of the oracle \u201d . 2.A robust classifier does not need to be perfect . A trivial example for strong-robust models is $ f_1 ( x ) = 1 , \\forall x \\in X $ . However , it is not a perfect predictor at all . 3.Our theoretical analysis provides a much better understanding of the relationship between robustness and accuracy . Two typical examples can be derived from Table-3 : - If f_1 misses discovering some related features and does not extract unrelated features , f_1 is strong-robust ( even tough its accuracy may be not satisfactory ) . - If f_1 uses some extra and unrelated features , it will not be strong-robust to adversarial attacks ( though it may be a very accurate predictor ) . We are updating the paper to explain the theoretical conclusions using a more accessible language . 4.In the adversarial setting , we should aim to get a classifier that is both strong-robust and accurate . A better feature learning is exactly the solution that may improve both goals !"}, {"review_id": "SywUHFcge-2", "review_text": "This paper theoretically analyzes the adversarial phenomenon by modeling the topological relationship between the feature space of the trained and the oracle discriminate function. In particular, the (complicated) discriminant function (f) is decomposed into a feature extractor (g) and a classifier (c), where the feature extractor (g) defines the feature space. The main contribution of this paper is to propose abstract understanding and analysis for adversarial phenomenon, which is interesting and important. However, this paper also has the following problems. 1) It is not clear how the classifier c can affect the overall robustness to adversarial noises. The classifier c seems absent from the analysis, which somehow indicates that the classifier does not matter. (Please correct me if it is not true) This is counter-intuitive. For example, if we always take the input space as the feature space and the entire f as the classifier c, the strong robustness can always hold. I am also wondering if the metric d has anything to do with the classifier c. 2) A very relevant problem is how to decompose f into g and c. For examples, one can take any intermediate layer or the input space as the feature space for a neural network. Will this affect the analysis of the adversarial robustness? 3) The oracle is a good concept. However, it is hard to explicitly define it. In this paper, the feature space of the oracle is just the input image space, and the inf-norm is used as the metric. This implementation makes the algorithm in Section 4 quite similar to existing methods (though there are some detailed differences as mentioned in the discussion). Due to the above problems, I feel that some aspects of the paper are not ready. If the problems are resolved or better clarified, I believe a higher rating can be assigned to this paper. In addition, the main text of this paper is somehow too long, the arguments can be more focused if the main paper become more concise. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for raising this important question . We added two figures in the appendix to illustrate classification c function does not directly influence robustness . 1.In Figure 5 , we show one case of $ n_1 = n_2 = 2 , X_1 = X_2 = \\RR^2 $ . In terms of classification , $ f_1 $ ( green boundary line ) is not accurate according to $ f_2 $ ( red boundary line ) . However , $ f_1 $ is strong-robust to the adversarial samples . 2.In Figure 6 , we show one case of $ 1 = n_1 < n_2 = 2 , X_1 \\subset X_2 $ . Similar to Figure 5 , in terms of classification , $ f_1 $ ( green boundary line ) is not accurate according to $ f_2 $ ( red boundary line ) . However , $ f_1 $ is strong-robust to the adversarial samples . 3.For both figures Test-sample Case ( c ) , we want to point out for the case of $ X $ being an infinite space , $ \\P ( ( x , x ' ) | d_2 ( x , x ' ) < \\epsilon \\ & \\text { $ ( x , x ' ) $ are across the boundary of $ f_1 $ } ) = 0 $ . The adversarial sample problem this paper focuses on is different from `` boundary based adversarial attacks '' . `` Boundary based adversarial attacks '' can only attack the points whose distance to the boundary of $ f_1 $ is smaller than $ \\epsilon $ . In fact , based on probability theory , the probability of this set is $ 0 $ if $ X $ is infinite ."}], "0": {"review_id": "SywUHFcge-0", "review_text": "Under my point of view, the robustness of a classifier against adversarial noise it is interesting if we find any relationship between that robustness and generalization to new unseen test samples. I guess that this relationship is direct in most of the problems but perhaps classifier C1 could be more robust than C2 against adv. noise but not better for new unseen samples from the task in consideration. Best results on new unseen samples are normally related to robustness against the common distortions of the data, e.g. invariance to scale, rotation\u2026 than robustness to adv. noise. I can not see any direct conclusion from table 5 results. Essentially i am not convinced about the necessity to measure the robustness against adversarial noise. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q1 : \u2026.the robustness of a classifier against adversarial noise is interesting if we find any relationship between that robustness and generalization to new unseen test samples . I guess that this relationship is direct in most of the problems \u2026 Answer : If we understand this question correctly , we think this question is about the relationship between adversarial robustness and test accuracy . Our theoretical analysis in Table 3 provides four cases to understand the relationship between robustness and accuracy . Two typical examples can be derived from Table 3 : - If $ f_1 $ misses discovering some related features and does not extract unrelated features , $ f_1 $ is strong-robust ( even tough its accuracy may be not satisfactory ) . This is illustrated by a simple example in Figure 6 showing 1 = n_1 < n_2 = 2 , X_1 \\subset X_2 . In terms of classification , $ f_1 $ ( green boundary line ) is not accurate according to $ f_2 $ ( red boundary line ) . However , $ f_1 $ is strong-robust to the adversarial samples . For Test-sample Case ( c ) see the discussions of boundary points in Section 3.5.4 . - If $ f_1 $ uses some extra and unrelated features , it will not be strong-robust to adversarial attacks ( though it may be an accurate predictor ) . This is illustrated by a simple example in Figure 2 . In the adversarial setting , we should aim to get a classifier that is both strong-robust and accurate . A better feature learning is exactly the solution that may improve both goals . This question partially relates to the discussion of `` how g function and c function influence the adverial robustness '' . For concrete discussion see Section 3.5.3 -- `` g_1 MATTERS FOR THE STRONG - ROBUSTNESS AND c_1 DOES NOT '' ."}, "1": {"review_id": "SywUHFcge-1", "review_text": "This paper aims at making three contributions: - Charecterizing robustness to adversarials in a topological manner. - Connecting the topological characterization to more quantitative measurements and evaluating deep networks. - Using Siamese network training to create models robust to adversarial in a practical manner and evaluate their properties. In my opinion the paper would improve greatly if the first, topological analysis attempt would be removed from the paper altogether. A central notion of the paper is the abstract characterization of robustness. The main weakness is the notion of strong robustness itself, which is an extremely rigid notion. It requires the partitioning of the predictor function by class to match the exact partitioning of the oracle. This robustness is almost never the case in real life: it requires that the predictor is almost perfect. The main flaw however is that the output space is assumed to have discrete topology and continuity is assumed for the classifier. Continuity of the classifier wrt. a discrete output is also never really satisfied. However, if the output space is assumed to be continues values with an interesting topology (like probabilities), then the notion of strong robustness becomes so constrained and strict, that it has even less practical sense and relevance. Based on those definition, several uninteresting, trivial consequences follow. They seem to be true, with inelegant proofs, but that matters little as they seem irrelevant for any practical purposes. The second part is a well executed experiment by training a Siamese architecture with an explicit additional robustness constraint. The approach seems to be working very well, but is compared only to a baseline (stability training) which performs worse than the original model without any trainings for adversarials. This is strange as adversarial examples have been studied extensively in the past year and several methods claimed improvements over the original model not trained for robustness. The experimental section and approach would look interesting, if it were compared with a stronger baseline, however the empty theoretical definitions and analysis attempts make the paper in its current form unappealing. ", "rating": "3: Clear rejection", "reply_text": "Q1 : The main weakness is the notion of strong robustness itself , which is an extremely rigid notion . It requires the partitioning of the predictor function by class to match the exact partitioning of the oracle . This robustness is almost never the case in real life : it requires that the predictor is almost perfect . Answer : We want to point out that this comment indicates some incorrect understanding of our paper : 1 . Strong-robustness \u201c does NOT require the partitioning of the predictor function by class to match the exact partitioning of the oracle \u201d . 2.A robust classifier does not need to be perfect . A trivial example for strong-robust models is $ f_1 ( x ) = 1 , \\forall x \\in X $ . However , it is not a perfect predictor at all . 3.Our theoretical analysis provides a much better understanding of the relationship between robustness and accuracy . Two typical examples can be derived from Table-3 : - If f_1 misses discovering some related features and does not extract unrelated features , f_1 is strong-robust ( even tough its accuracy may be not satisfactory ) . - If f_1 uses some extra and unrelated features , it will not be strong-robust to adversarial attacks ( though it may be a very accurate predictor ) . We are updating the paper to explain the theoretical conclusions using a more accessible language . 4.In the adversarial setting , we should aim to get a classifier that is both strong-robust and accurate . A better feature learning is exactly the solution that may improve both goals !"}, "2": {"review_id": "SywUHFcge-2", "review_text": "This paper theoretically analyzes the adversarial phenomenon by modeling the topological relationship between the feature space of the trained and the oracle discriminate function. In particular, the (complicated) discriminant function (f) is decomposed into a feature extractor (g) and a classifier (c), where the feature extractor (g) defines the feature space. The main contribution of this paper is to propose abstract understanding and analysis for adversarial phenomenon, which is interesting and important. However, this paper also has the following problems. 1) It is not clear how the classifier c can affect the overall robustness to adversarial noises. The classifier c seems absent from the analysis, which somehow indicates that the classifier does not matter. (Please correct me if it is not true) This is counter-intuitive. For example, if we always take the input space as the feature space and the entire f as the classifier c, the strong robustness can always hold. I am also wondering if the metric d has anything to do with the classifier c. 2) A very relevant problem is how to decompose f into g and c. For examples, one can take any intermediate layer or the input space as the feature space for a neural network. Will this affect the analysis of the adversarial robustness? 3) The oracle is a good concept. However, it is hard to explicitly define it. In this paper, the feature space of the oracle is just the input image space, and the inf-norm is used as the metric. This implementation makes the algorithm in Section 4 quite similar to existing methods (though there are some detailed differences as mentioned in the discussion). Due to the above problems, I feel that some aspects of the paper are not ready. If the problems are resolved or better clarified, I believe a higher rating can be assigned to this paper. In addition, the main text of this paper is somehow too long, the arguments can be more focused if the main paper become more concise. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for raising this important question . We added two figures in the appendix to illustrate classification c function does not directly influence robustness . 1.In Figure 5 , we show one case of $ n_1 = n_2 = 2 , X_1 = X_2 = \\RR^2 $ . In terms of classification , $ f_1 $ ( green boundary line ) is not accurate according to $ f_2 $ ( red boundary line ) . However , $ f_1 $ is strong-robust to the adversarial samples . 2.In Figure 6 , we show one case of $ 1 = n_1 < n_2 = 2 , X_1 \\subset X_2 $ . Similar to Figure 5 , in terms of classification , $ f_1 $ ( green boundary line ) is not accurate according to $ f_2 $ ( red boundary line ) . However , $ f_1 $ is strong-robust to the adversarial samples . 3.For both figures Test-sample Case ( c ) , we want to point out for the case of $ X $ being an infinite space , $ \\P ( ( x , x ' ) | d_2 ( x , x ' ) < \\epsilon \\ & \\text { $ ( x , x ' ) $ are across the boundary of $ f_1 $ } ) = 0 $ . The adversarial sample problem this paper focuses on is different from `` boundary based adversarial attacks '' . `` Boundary based adversarial attacks '' can only attack the points whose distance to the boundary of $ f_1 $ is smaller than $ \\epsilon $ . In fact , based on probability theory , the probability of this set is $ 0 $ if $ X $ is infinite ."}}