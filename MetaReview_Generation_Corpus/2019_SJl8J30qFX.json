{"year": "2019", "forum": "SJl8J30qFX", "title": "Learning Global Additive Explanations for Neural Nets Using Model Distillation", "decision": "Reject", "meta_review": "This paper introduces a distillation approach for black-box classifiers that trains generalized additive models (GAM), an additive model over feature shapes, thus providing global explanations for the model. Given the importance of interpretability, the reviewers appreciated the focus of this work. The reviewers also found the experiments, both on real and synthetic datasets, extremely thorough and were impressed by the results. Finally, they also mentioned that the paper was clearly well-written.\n\nThe reviewers and AC note the following potential weaknesses: \n(1) The primary concern, raised by all of the reviewers, is the lack of novelty;the proposed approach is a straightforward application of GAMs to model distillation, where black box output is the training data of the GAM, (2) The reviewers are also concern that the proposed approach is limited in scope to tabular datasets, and would not work for more interesting, complex domains like text or images, and (3) The reviewers are concerned that the interpretability of GAMS is assumed, without describing the limitations, for example, if there are correlated features, the shapes would affect each other in uninterpretable ways. Amongst other concerns, the reviewers were concerned about the formatting of the plots and tables in the paper, which made it difficult to read them, and the lack of a user study to verify the interpretability claims.\n\nIn response to these criticisms, the authors provided comments and a substantial revision to the papers, heavily restructuring the paper to fit extra experiments (comparison to other global explanation techniques, including a user study) and make the figures and tables readable. While the paper was much improved by these changes, and two of the reviewers increased their scores accordingly, concerns about the limited novelty and scope still remained.\n\nUltimately, the reviewers did not reach a conclusion, but the concerns of novelty and scope overwhelmed the clear benefits of the approach and the strong results. This paper was very close to getting accepted, and we strongly urge the authors to submit it to other premier ML conferences.", "reviews": [{"review_id": "SJl8J30qFX-0", "review_text": "Summary: This paper incorporates Generalized Additive Models (GAMs) with model distillation to provide global explanations of neural nets (fully-connected nets as black-box in the paper). It is well written with detailed experiments of synthetic and real tabular data, and makes some contribution towards the interpretability of black-box models. However, it lacks novelty and is limited to tabular data as presented. Pros: - The paper is well written. - The experiments are detailed and thorough with both synthetic and real data. Cons: - The novelty is limited. The core consists of GAMs well studied in the literature, e.g. Caruana et al 2015. Admittedly, this work also tries to incorporate model distillation to explain black-box models globally. The concept of student models approximating teacher models is not new either. The originality seems incremental in both directions. - The scope is limited. The paper only presents applications in tabular data. Also, it would be better to experiment with black-box models beside simple fully-connected nets. - The interpretability is not convincing. It is not sufficient to demonstrate the interpretability of the proposed method, or the expressive advantage of feature shapes. It is encouraged to include studies with human subjective to compare against other existing interpretable approaches. Specifics: - With Figure 3, it is not convincing that the student model actually explains the teacher model, so the paper tries to elaborate more with Table 1. I think Table 1 also needs more details to help, such as the significance of error difference and '-' elements. - Many figures are hard to read mostly because of font, color, and overlap. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments and suggestions . Since your concerns are similar to Reviewer 2 's , we addressed them jointly . Please see our comments Parts 1 and 2 in the main thread , and the updated paper ."}, {"review_id": "SJl8J30qFX-1", "review_text": "Summary: This paper makes an interesting contribution of providing global explanations of black box models (such as neural nets) using a special class of models called generalized additive models. While the paper is well written and experiments are quite detailed, I have some problems with some of the basic premises of this work. 1. The concept of using simpler models to approximate other complex models (model distillation) is not new and has been explored quite a bit already in ML literature. The only new proposition of this work is to use generalized additive models to approximate other complex models. This seems rather incremental. 2. The premise behind using generalized additive models (GAMs) to explain other complex models is that GAMs are interpretable. I am not convinced about this premise. While I can intuitively see that GAMs might be able to better approximate complex models compared to rules and trees, I highly doubt if they are even interpretable. Pros: 1. The paper is well written 2. Experiments are very detailed and thorough Cons: 1. The proposed approach lacks novelty 2. Experimentation lacks a user study which helps understand if and when GAMs are at least as interpretable as rule-based approaches. Detailed Comments: I actually like the way this paper is written and executed. The writing is very clear and experiments are quite thorough. But, as discussed earlier, I have some issues with the basic premises of this paper i.e., novelty of the proposed approach and justification for the claim that GAMs are interpretable. I would encourage the authors to discuss these two aspects in their rebuttal. I would strongly encourage the authors to carry out at least a simple user study which compares the interpretability aspect of GAMs with rule-based or prototype based approaches. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments and suggestions . Since your concerns are similar to Reviewer 1 \u2019 s , we addressed them jointly . Please see our comments Parts 1 and 2 in the main thread , and the updated paper ."}, {"review_id": "SJl8J30qFX-2", "review_text": "This paper is of high quality and clarity. I think it's originality is at least decent. Whether it is significant or not depends on how significant one thinks fully connected neural networks are as these are the models for which this explanation model makes sense. Good things: - It is a very elegant method. It is also very simple (in a good way). - The paper is really well written. - The experiments are carefully conducted and are indeed showing what the authors describe. - I think the method is potentially of practical use. Problems: - I think qualifying this paper as a paper on representation learning is a small stretch. It would be perhaps more suitable to submit it to ICML or NIPS. I think it is close enough though. - The font is too small in many figures. It is impossible to read it. - I am not sure whether model compression is actually necessary here. How good is the additive model if it is trained as a standalone model straight from the training data in comparison to the neural networks and to the additive model when trained with model compression? If the neural network and the additive model were similar in performance when trained from scratch, I would not see the point in explaining the neural network. - Only makes sense to apply this to fully connected networks.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We 'd like to thank for the reviewer for the overall positive and encouraging comments . We will update the figure fonts to make sure they are readable . Regarding the other concerns : - `` Only makes sense to apply this to fully connected networks '' . Although our evaluation is on fully-connected neural networks ( FNNs ) , our approach is not technically limited to FNNs . We have applied our approach to other teachers such as gradient boosted trees and random forests , linear models ( as a sanity check , because there we expect the student to have linear feature shapes , and we do find that ) , and even other non-linear additive models . We note that it is not the model class but the data input that determines how informative and practical our feature shapes are . For example , one could apply this approach to a convolutional neural net ( CNN ) or a FNN trained on MNIST or CIFAR , and the result would be a feature shape for each pixel position , that tells us how much the value at that pixel position contributes to the prediction . Despite being correct , this feature shape would not be as informative as the feature shapes of \u201c tabular \u201d data ( common in healthcare and social science domains ) that we use in this paper . - \u201c I think qualifying this paper as a paper on representation learning is a small stretch . [ ... ] .I think it is close enough though '' : We agree that our approach differs somewhat from traditional representation learning tasks . But , along with the referee , we still see this as a form of finding representations . In particular , we are finding the best additive representation of the FNN \u2019 s prediction function . - `` If the neural network and the additive model were similar in performance when trained from scratch , I would not see the point in explaining the neural network '' . We agree that if the neural net \u2019 s training data ( features and labels ) are available and the additive model can be trained to be highly accurate and deployed , then we don \u2019 t need to explain a neural net using an additive model . However , this is not always the case . E.g . : i ) Availability of original training data . Data regulations -- e.g.General Data Protection Regulation ( GDPR ) or California 's Consumer Privacy Act ( CPA ) -- may limit how the data can be used or retained . In that case , the data and labels used to train the original model may no longer be available to train the additive model on directly . However , training the additive model to inspect the original model does not require the original data and labels , only a second set of unlabeled data ( to be passed through the original model to get soft labels ) , which is easier to obtain . ii ) Deployment constraints . In some scenarios , the model class may be constrained due to external factors or preferences . For example , a model has to be deployed using specific hardware or have particular requirements in terms of memory bandwidth , CPU , etc . Another example would be a model requiring a costly verification and approval process before it can be deployed . Then , the original model could not be simply replaced by the additive model , even if the additive model could be trained to the same level of accuracy or higher . However , it would still be valuable to have techniques to inspect the model . For these reasons , we argue that post-hoc explanations of black-box models are still useful , and our work contributes one such approach . If the reviewer has any additional experiments in mind , we would be happy to perform them ."}], "0": {"review_id": "SJl8J30qFX-0", "review_text": "Summary: This paper incorporates Generalized Additive Models (GAMs) with model distillation to provide global explanations of neural nets (fully-connected nets as black-box in the paper). It is well written with detailed experiments of synthetic and real tabular data, and makes some contribution towards the interpretability of black-box models. However, it lacks novelty and is limited to tabular data as presented. Pros: - The paper is well written. - The experiments are detailed and thorough with both synthetic and real data. Cons: - The novelty is limited. The core consists of GAMs well studied in the literature, e.g. Caruana et al 2015. Admittedly, this work also tries to incorporate model distillation to explain black-box models globally. The concept of student models approximating teacher models is not new either. The originality seems incremental in both directions. - The scope is limited. The paper only presents applications in tabular data. Also, it would be better to experiment with black-box models beside simple fully-connected nets. - The interpretability is not convincing. It is not sufficient to demonstrate the interpretability of the proposed method, or the expressive advantage of feature shapes. It is encouraged to include studies with human subjective to compare against other existing interpretable approaches. Specifics: - With Figure 3, it is not convincing that the student model actually explains the teacher model, so the paper tries to elaborate more with Table 1. I think Table 1 also needs more details to help, such as the significance of error difference and '-' elements. - Many figures are hard to read mostly because of font, color, and overlap. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments and suggestions . Since your concerns are similar to Reviewer 2 's , we addressed them jointly . Please see our comments Parts 1 and 2 in the main thread , and the updated paper ."}, "1": {"review_id": "SJl8J30qFX-1", "review_text": "Summary: This paper makes an interesting contribution of providing global explanations of black box models (such as neural nets) using a special class of models called generalized additive models. While the paper is well written and experiments are quite detailed, I have some problems with some of the basic premises of this work. 1. The concept of using simpler models to approximate other complex models (model distillation) is not new and has been explored quite a bit already in ML literature. The only new proposition of this work is to use generalized additive models to approximate other complex models. This seems rather incremental. 2. The premise behind using generalized additive models (GAMs) to explain other complex models is that GAMs are interpretable. I am not convinced about this premise. While I can intuitively see that GAMs might be able to better approximate complex models compared to rules and trees, I highly doubt if they are even interpretable. Pros: 1. The paper is well written 2. Experiments are very detailed and thorough Cons: 1. The proposed approach lacks novelty 2. Experimentation lacks a user study which helps understand if and when GAMs are at least as interpretable as rule-based approaches. Detailed Comments: I actually like the way this paper is written and executed. The writing is very clear and experiments are quite thorough. But, as discussed earlier, I have some issues with the basic premises of this paper i.e., novelty of the proposed approach and justification for the claim that GAMs are interpretable. I would encourage the authors to discuss these two aspects in their rebuttal. I would strongly encourage the authors to carry out at least a simple user study which compares the interpretability aspect of GAMs with rule-based or prototype based approaches. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments and suggestions . Since your concerns are similar to Reviewer 1 \u2019 s , we addressed them jointly . Please see our comments Parts 1 and 2 in the main thread , and the updated paper ."}, "2": {"review_id": "SJl8J30qFX-2", "review_text": "This paper is of high quality and clarity. I think it's originality is at least decent. Whether it is significant or not depends on how significant one thinks fully connected neural networks are as these are the models for which this explanation model makes sense. Good things: - It is a very elegant method. It is also very simple (in a good way). - The paper is really well written. - The experiments are carefully conducted and are indeed showing what the authors describe. - I think the method is potentially of practical use. Problems: - I think qualifying this paper as a paper on representation learning is a small stretch. It would be perhaps more suitable to submit it to ICML or NIPS. I think it is close enough though. - The font is too small in many figures. It is impossible to read it. - I am not sure whether model compression is actually necessary here. How good is the additive model if it is trained as a standalone model straight from the training data in comparison to the neural networks and to the additive model when trained with model compression? If the neural network and the additive model were similar in performance when trained from scratch, I would not see the point in explaining the neural network. - Only makes sense to apply this to fully connected networks.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We 'd like to thank for the reviewer for the overall positive and encouraging comments . We will update the figure fonts to make sure they are readable . Regarding the other concerns : - `` Only makes sense to apply this to fully connected networks '' . Although our evaluation is on fully-connected neural networks ( FNNs ) , our approach is not technically limited to FNNs . We have applied our approach to other teachers such as gradient boosted trees and random forests , linear models ( as a sanity check , because there we expect the student to have linear feature shapes , and we do find that ) , and even other non-linear additive models . We note that it is not the model class but the data input that determines how informative and practical our feature shapes are . For example , one could apply this approach to a convolutional neural net ( CNN ) or a FNN trained on MNIST or CIFAR , and the result would be a feature shape for each pixel position , that tells us how much the value at that pixel position contributes to the prediction . Despite being correct , this feature shape would not be as informative as the feature shapes of \u201c tabular \u201d data ( common in healthcare and social science domains ) that we use in this paper . - \u201c I think qualifying this paper as a paper on representation learning is a small stretch . [ ... ] .I think it is close enough though '' : We agree that our approach differs somewhat from traditional representation learning tasks . But , along with the referee , we still see this as a form of finding representations . In particular , we are finding the best additive representation of the FNN \u2019 s prediction function . - `` If the neural network and the additive model were similar in performance when trained from scratch , I would not see the point in explaining the neural network '' . We agree that if the neural net \u2019 s training data ( features and labels ) are available and the additive model can be trained to be highly accurate and deployed , then we don \u2019 t need to explain a neural net using an additive model . However , this is not always the case . E.g . : i ) Availability of original training data . Data regulations -- e.g.General Data Protection Regulation ( GDPR ) or California 's Consumer Privacy Act ( CPA ) -- may limit how the data can be used or retained . In that case , the data and labels used to train the original model may no longer be available to train the additive model on directly . However , training the additive model to inspect the original model does not require the original data and labels , only a second set of unlabeled data ( to be passed through the original model to get soft labels ) , which is easier to obtain . ii ) Deployment constraints . In some scenarios , the model class may be constrained due to external factors or preferences . For example , a model has to be deployed using specific hardware or have particular requirements in terms of memory bandwidth , CPU , etc . Another example would be a model requiring a costly verification and approval process before it can be deployed . Then , the original model could not be simply replaced by the additive model , even if the additive model could be trained to the same level of accuracy or higher . However , it would still be valuable to have techniques to inspect the model . For these reasons , we argue that post-hoc explanations of black-box models are still useful , and our work contributes one such approach . If the reviewer has any additional experiments in mind , we would be happy to perform them ."}}