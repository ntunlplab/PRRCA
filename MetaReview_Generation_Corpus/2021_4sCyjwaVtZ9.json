{"year": "2021", "forum": "4sCyjwaVtZ9", "title": "Whitening and second order optimization both destroy information about the dataset, and can make generalization impossible", "decision": "Reject", "meta_review": "The paper suggests that whitening the data harms generalization and optimization performance when learning models of the form h(W x) i.e. those that are based on a linear projection of the inputs (which includes DNNs for instance). The main concern of the reviewers is that their theoretical developments were not that convincing; it seemed more along the lines of providing some specific anecdotes. But more broadly, the caveat is that their development seems very simple: their results in high-dimensional settings (where d = dim(x) > number of samples n) is not that relevant since vanilla whitening is anyway fraught in such high-dimensions, since the sample covariance matrix is not a good estimator in high dimensions anyway. And when n > d, their result focuses on linear models, where they say that whitening reduces information about the singular vector directions where the input data might mostly lie on. But if the data lies on a lower dimensional linear manifold, then whitening is again fraught: the covariance matrix is singular. The linear manifold assumption also seems very specific given the general title of the paper. Overall, the paper needs to narrow their focus on specific settings where whitening is harmful, but the specific settings above in and of themselves do not necessarily say anything other than to estimate the covariance matrix carefully before doing whitening.\n", "reviews": [{"review_id": "4sCyjwaVtZ9-0", "review_text": "Topic : whitening destroys generalization main contribution This work offers a mutual information perspective to explain the relations between whitening/second-order optimization and generalization . Strength - The perspective is interesting and a bit intriguing . But many discrepancies may need clarifications . Weakness - The title and the sentiment of the paper may be misleading . It is scary at first glance , but the conclusion and experiments do not really support the claim \u201c destroy \u201d . It may be important to keep the title accurate other than eye-catching . The authors may argue \u201c can \u201d is the key in the title , but the reviewer feels this may be a bit too subtle . - Conditional independence and generalization . The theorems established a certain conditional independence between training features , the model , and the test data . The conditional independence is derived using gradient descent of the whitened data . This is understandable , but the implication on generalization is not crystal clear . - The claims may need more explanation and be better represented ; some simple examples may suggest different conclusion . For example , the reviewer considered a simple least squares problem min_theta ||y-X * theta||^2 with whitened X the solution is theta_opt = X^T * y , which is clearly dependent with X , even if y is revealed . If the training set changes , the learned theta changes . The reviewer wonders if this can be explained by the theorems in the paper \u2013 or did the reviewer miss something ( response is welcome here ) ? - The above leads to another question : The derivation of the work relies on ( 4 ) and ( 5 ) , but either of which fully expressed the gradient . The impression from there is the updates are dominated by the covariance matrices and if the covariance matrices are identity , then there is no information about the training data passed through the training process . This may not be true , if , again , consider the least squares problem , where the gradient is grad = X \u2019 * X theta \u2013 X \u2019 * y where the second term is still data-dependent . In particular , the claim \u201c To establish this result , we note that the first layer activation at initialization , Z0 train , is a random variable due to random weight initialization , and only depends on Xtrain through Ktrain : \u201d The last sentence is a bit questionable to the reviewer . - There may be some clarity issues . For example , ( 8 ) is not easy to understand . In the test stage , why is there still a gradient step performed ? The test stage should not involve any optimization . This may be clarified . - Basic generalization theorem suggests that generalization is only related to function class \u2019 s complexity , but not the correlation among the coordinates of the data samples . As long as the training and test data are sampled from the same distribution , it is hard to see , from classic generalization analysis viewpoint , why using whitening or second-order methods can destroy generalization . The claim from this work is contradicting to what we learn from textbooks , e.g. , Shalev-Shwartz , Shai , and Shai Ben-David . Understanding machine learning : From theory to algorithms . Cambridge university press , 2014 . It may be good that the authors compare their results with classic results , e.g. , those based on uniform convergence , under some function hypothesis complexity measures ( e.g. , finite class , VC dimension or even Radamacher complexity ) . The classic proofs of generalization are insensitive to data whitening or algorithm design , and thus the reviewer feel that there may be a big gap to fill if the claims of this work holds . - The main results claim that whitening is harmful , the simulation may have suggested otherwise . From most of the figures , the generalization error becomes better and better when the sample size increases . Note that when the sample size increases , the sample correlation matrix converges in probability to the ensemble mean , and the whitening could be performed in a more accurate way . This set of results may suggest that , if whitening is done accurately , then it is fine . When the sample size is small , whitening can not be done very accurately since sample correlation is not accurately estimated . The reviewer \u2019 s understanding is that the high test error with small sample size is unlikely an effect of whitening , but * inaccurate * whitening , since more noise was brought into the training process . But when the whitening step is performed without too much noise , generalization only depends on function class used and the number of samples , per classic generalization theories . This interpretation seems more consistent with classic generalization theory . The authors may hope to comment on this . - after the discussion period I would like to thank the authors for the reply , clarification , and additional experiments . Although I do like the perspectives revealed in this work , many points are still quite unclear to me . For example , the theory seems not be able to explain why whitening does not hurt testing when sample size is large , as demonstrated in the paper . This work also does not draw connection between sample size and generalization error , which may make the claims a bit incredible . I would encourage the authors to work towards this direction and solidify the contribution .", "rating": "4: Ok but not good enough - rejection", "reply_text": "\\ > ___It may be good that the authors compare their results with classic results \u2026 classic proofs of generalization are insensitive to data whitening or algorithm design.___ A major revelation of modern learning theory has been that classical generalization results , such as those referenced by the reviewer are incapable of explaining the generalization properties observed in practice . This is sometimes referred to as the generalization puzzle [ Zhang et . al.ICLR 2017 ( arXiv:1611.03530 ) , Neyshabur et . al.NeurIPS 2017 ( arXiv : 1706.08947 ) ] . It has been realized that a major shortcoming of classic work is that generalization dynamics of neural networks and even linear models [ Belkin et . al.PMLR 2018 ( arXiv : 1802.01396 ) ] depends crucially on the optimization procedure ( including early stopping ) [ Neyshabur er . al . ( arXiv : 1705.03071 ) ] , and structure of the data [ Gerace et al.2020 ( arxiv : 2002.09339 ) ] , not just the worst case behavior of the function class . \\ > ___Note that when the sample size increases , the sample correlation matrix converges in probability to the ensemble mean , and the whitening could be performed in a more accurate way . This set of results may suggest that , if whitening is done accurately , then it is fine . When the sample size is small , whitening can not be done very accurately since sample correlation is not accurately estimated.___ We emphasize that whitening is * * defined * * in terms of a finite dataset , rather than the distribution that generated that dataset ( eg see https : //en.wikipedia.org/wiki/Whitening_transformation ) . As such , the whitening we consider in the paper is done exactly and accurately . The question of what happens if the whitening matrix is allowed to go to its ensemble limit is still interesting , though ! To answer this , we repeated our linear model experiment , but using a whitening matrix computed on the entire dataset even when training on a subset of the data . See the new Appendix Figure App 2 . We find that using the whole-dataset whitening transform , which we call \u201c distribution whitening \u201d , provides a slight performance improvement , but still performs most similarly to when the whitening transform is computed on only a subset of the data ."}, {"review_id": "4sCyjwaVtZ9-1", "review_text": "# # Overview In supervised learning tasks , it is common in practice to apply a * whitening * transformation to remove correlations between input features . This can improve the conditioning of the underlying data manifold , enabling faster convergence . This paper shows that for a large class of models models $ f $ consisting of a fully-connected layer followed by an arbitrary parameterized function , $ f ( X ) = g_\\theta ( WX ) $ data whitening removes all information that is relevant for generalization . Furthermore , this has implications for the generalization ability of second-order methods such as Newton 's method due to a well-known equivalence between Newton 's method and steepest-descent applied to whitened data . The effects suggested by the presented theory are verified empirically , and the are additionally observed in convolutional neural networks , suggesting that the phenomenon could apply more broadly to more complicated connectionist models as well . * * Overall , I recommend that the paper be accepted * * . The theoretical results are interesting and potentially high-impact , and the writing clarity was excellent throughout . My main reservation about the paper is that it 's not clear to me which insights are being proposed as novel , and among those , which actually are novel . In particular , - * * Section 2.5 * * : The relationship between Newton 's method and data whitening is well-known . Does section 2.5 include any novel insights , or is this included for completeness ? If it 's the latter it seems like it would be better-placed in an appendix ; the fact that the main results relate also to second-order methods could then be noted in the discussion - * * Generally * * speaking , how does what 's already known from e.g.dimensionality reduction and PCA play into this ? As mentioned in the paper , whitening is essentially putting the signal and noise in the data on equal footing . From a principal components perspective , it 's as if you 're forcing the model to consider all components to be equally predictive , which would clearly harm generalization when the data has strong feature correlations . Is there truly no prior result of this kind in that literature ? It seems quite fundamental , and my concern is that this phenomenon might be already well-known under different terminology in another literature . * ( The following question is out of scope but I think could potentially increase the impact of the paper a great deal ) * Do you suspect these issues hold also for the online second-order methods , such as Online Newton Step and AdaGrad ? If so , adding some discussion about this could potentially increase the impact of this work since AdaGrad ( and it 's heuristic descendants Adam , RMSProp etc . ) are by-far the most common algorithms used for optimizing neural networks in practice # # Clarifications - * * Page 4 * * : W is isotropic , and whitening makes X isotropic , so that Z is isotropic , so intuitively the combination of whitening and isotropic initialization results in trying to make predictions from isotropic noise . Is it specifically the * combination * of whitening and isotropic initialization that 's the problem ? The paper seems to be really centered around the whitening , when it seems to be the specific combination of the two rather than whitening alone . Or is there a nuance I 'm missing here ? - * * Page 8 * * : * '' We therefore believe that regularized Gauss-Newton should be viewed as discarding information in the large-eigenvector subspace '' * . I 'm not sure I understand what is being said here . Would n't this suggest that discarding principle components is beneficial ? this seems to run contrary to a lot of the dimensionality reduction literature . # # Minor Comments ( which did not influence my score but could improve clarity ) - * * Page 2 * * : * '' Our result is not restricted to neural networks , and applies to any model in which the input is transformed by a dense matrix multiply with isotropic weight initialization '' * should read * dense matrix with isotropic weight initialization * ? - * * Page 2 * * : It is not really necessary to list 34 ( ! ! ) papers on second-order optimization in a single sentence ; there are plenty of surveys on the topic that could be linked instead - * * Page 7 , Figure 4 * * : * '' Linear models trained on whitened data optimize faster , but their best test accuracy is always worse '' * . This could use some minor rephrasing to say that the best test accuracy * was * always worse , as this conclusion is about this specific problem rather than for all problems generally - * * Page 8 * * : * '' Regularized Gauss-Newton optimization acts similarly to unregularized Gauss-Newton in the subspace spanned by eigenvectors with eigenvalues larger than \u03bb/ ( 1 \u2212 \u03bb ) , and similarly to steepest descent in the subspace spanned by eigenvectors with eigenvalues smaller than \u03bb/ ( 1 \u2212 \u03bb ) '' * This fact is not immediately obvious , so a citation would be helpful here ( or potentially a link to an appendix if it 's not a well-known result ) - * * Page 14 , appendix A * * : The integral notation here is unclear to me ; what kind of integral is this supposed to be ? what is delta ?", "rating": "7: Good paper, accept", "reply_text": "\\ > ___Re : minor comments___ We thank the reviewer for their suggestions to improve the clarity of our exposition , and have implemented most of these . In Appendix A the measure of integration is the uniform measure over the components of W^ { 0 } . This is defined in the text . It can also be written as the product \\prod_i dW^ { 0 } _ { i } . The delta is a Dirac delta function . We have expanded the discussion on page 8 . The primary concern you expressed about our paper was ( boldly paraphrasing ! ) that the results seemed basic and important enough that a subset of them must surely already be known and published . We hope you are more convinced of the work \u2019 s novelty after our rebuttal , and after seeing that no other reviewer raised concerns about missing related work . If you are now more convinced of the paper \u2019 s novelty , we respectfully ask you to consider more strongly supporting acceptance , and raising your score accordingly . Thank you again !"}, {"review_id": "4sCyjwaVtZ9-2", "review_text": "Summary The authors analyse the training dynamics of a machine learning model consisting in a linear unit , followed by any parametrized function . The authors in particular focus on the impact of whitening the data beforehand or using second order methods . They show that the learned parameters of the model only depend on the training data through its Gram matrix . Since whitening trivializes the Gram matrix , the authors argue that whitening destroys important information . Major comments - The article is well written , the arguments are well presented and easily understood . - The main message of the paper , contained in Fig.3 , is easily reproduced with a few lines of code . - The paper sheds an interesting light on the generalization properties of second order methods . Thm.2.2.1 and 2.2.2 treat the initialization of the weights as random variables . I believe that it makes them rather useless for practical applications , because in practice the machine learning model will only be initialized once . Of course , the theorems also apply if we assume that the initial weights are zero ( or close to zero ) , which is still a very interesting case . In the experiments description of fig.3 . it would be worthwhile to better describe the initialization strategy for the linear weights . For instance , I used as X the load_digits dataset in scikit-learn , as Y a one hot encoding of the target , and a simple linear model with MSE loss $ f ( W ) = \\|WX - Y\\|^2 $ , and managed to get the same fig.3.a as the authors , but only if I initialize the weights very close to 0 , or at 0 . - I think that Sec.2.4 is too shallow . First , it should be clearly stated that eq.14 only works for MSE loss with a purely linear model . Second , I think that another important concept which is not mentioned in the article is that early stopping is similar to regularization of the parameters , which is why the reported \u2018 test error \u2019 in the experiments is lower than the test error obtained by perfectly minimizing the train error . See e.g.Yao , Yuan , Lorenzo Rosasco , and Andrea Caponnetto . `` On early stopping in gradient descent learning . '' Constructive Approximation 26.2 ( 2007 ) : 289-315 . - In Sec.2.5 , the authors consider Newton \u2019 s method with MSE loss on a linear problem , which converges in just one step with $ \\eta=1 $ . This should be acknowledged , and maybe it would be easier to defend the point by letting $ \\eta \\to 0 $ instead . Minor comments - The meaning of \u2018 test error \u2019 in the figure legend is a bit vague , and I think it should rather be replaced by \u2018 best test error during training \u2019 or something similar . - In fig.4.b , does training epochs mean \u2018 number of epochs to reach the best test error \u2019 ? Misc - What does WRNs mean ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for volunteering your time to review our paper , and for your careful review ! \\ > ___The main message of the paper , contained in Fig.3 , is easily reproduced with a few lines of code.___ It \u2019 s very neat that you were able to reproduce our results so easily ! \\ > ___In the experiments description of Fig.3 . it would be worthwhile to better describe the initialization strategy for the linear weights.___ We used a Gaussian with mean zero and variance 1e-4 to initialize weights for all experiments except the linear model , which we initialized with zero weights . We thank the reviewer for catching our inadvertent omission of this information from the Methods ( Appendix E ) , and have now corrected it ! \\ > ___Thm.2.2.1 and 2.2.2 treat the initialization of the weights as random variables . I believe that it makes them rather useless for practical applications , because in practice the machine learning model will only be initialized once.___ We believe these results are relevant for practical applications . For high dimensional input , the outputs of a model trained on full-whitened data will contain no information about its training inputs . It is possible for a single random initialization of this model to do well , in the same way it would be possible to do well on a test set by randomly flipping a coin to predict each test example . However , * a typical random initialization will not perform any better than would a typical sequence of coin flips ! * As further evidence for the relevance of the results to individual models , we can see in Figure 3 that the performance of randomly initialized models concentrates around their average case performance . \\ > ___It should be clearly stated that eq.14 only works for MSE loss with a purely linear model\u2026 early stopping is similar to regularization of the parameters.___ Thank you for catching this . We have updated Section 2.4 to clarify that we analyze the mean squared loss . We emphasize though that our results in Sections 2.1 through 2.3 hold for any loss function , not just MSE loss , and that many of our experiments use cross entropy loss . We have also added a brief discussion of the effect of early stopping , and its relationship to weight regularization , to Section 2.4 . \\ > ___In Sec.2.5 , the authors consider Newton \u2019 s method with MSE loss on a linear problem , which converges in just one step with \u03b7=1 . This should be acknowledged.___ We have updated Section 2.5 with a sentence stating that a single Newton step with $ \\eta = 1 $ solves the optimization problem over the training dataset . \\ > ___Re : minor comments and miscellaneous___ Figure 4b . In all experiments with MLPs , the models were trained to a fixed training accuracy cutoff , at which point test error was measured . The label of the y-axis indicates the number of epochs it took to reach this training cutoff . This is explained in the figure caption and in Appendix E. WRN is the acronym for \u201c wide residual network \u201d ."}, {"review_id": "4sCyjwaVtZ9-3", "review_text": "This paper shows theoretical and empirical evidence that under some conditions , whitening the input data and second-order methods may hurt the generalization performance . The paper is well written with good intuitions but overclaims its results . The theoretical results are limited to specific cases , and the experimental results would benefit from systematic ablation studies . All in all , I do not think that the paper provides sufficient evidence that justifies its title and main message of the paper . Detailed review below . - In Section 2.1 , `` the trained model depends on the training data only through K '' , this is technically incorrect , since there is a dependence on y_train and the Z_train is independent of X_train , only when conditioned on both K_train and y_train . Since whitening only affects K_train , there still might be some information in y_train , especially for the case of data realizable by the model . Please clarify this . - In Section 2.3 , the statement `` whitening hurts generalization '' is true only for high-dimensional datasets , where the input dimension > number of samples . I agree that such datasets ( arising for example in genetics ) are important , this is not the typical case where neural networks are used . For example , for all vision tasks , d < n. Please explicitly say this . - In Section 2.4 , please also consider d > n case . This is more important because there are multiple solutions , even in the linear case , and whitening may hurt generalization . For the d < n case analyzed in the paper , for a linear model , for most losses , the optimization problem is strictly convex and a unique solution . This is hardly a claim justifying that `` whitening hurts generalization '' , it is an optimization problem and is orthogonal to the paper . For convex problems with a unique solution , there is no reason to early stop the optimization . The convergence rate is linear implying that the solution is reached quickly and this result is not interesting , either from an optimization/generalization perspective . Moreover , such a claim is only valid for the squared loss , and not , for example , the logistic loss . Please clearly explain this . - In Section 2.5 , the paper claims that `` unregularized second-order methods have poor generalization '' , but as the authors admit themselves , second-order methods are rarely used without regularization , again making this result not practically important . Moreover , there have been second-order methods ( KFAC for example ) that employ regularization , approximations to the Hessian and generalize well . The statement `` second-order information destroys generalization '' is therefore an overclaim . - Moreover , this section focuses on the squared loss with a linear model . If n > d , then Newton 's method converges to the min-norm solution , same as GD . For d > n , the solution of Newton 's method lies in the span of the training data ( if we use the pseudo-inverse for the Hessian or add a small regularizer ) and also converges to the min-norm solution , meaning in both cases , its generalization is as good as GD . Again , the behaviour for other losses is not clear . - In Section 3 , for the experimental results , there are numerous confounding factors that need to be controlled for . For example , is the optimization deterministic or stochastic ? We know that stochastic methods typically generalize better . How is the learning rate selected ? Is there a warmup phase ? These factors play an important role in the generalization . Similarly , both the optimization method and the loss function also influence the generalization . Is there regularization , either L2 or L1 in the case when d > n ( this is typically done for high dimensional datasets ) . Please explain how did you control for these factors . - In Figure 3 ( a ) , what is the effect of using a regularized Newton method ? What if you run a more standard second-order method like KFAC ? - In Figure 5 , the paper shows `` Regularized second-order methods can train faster than gradient descent , with minimal or even positive impact on generalization '' . This contradicts the paper 's title . As the authors claim , when done without regularization , second-order methods can harm generalization , but when used with proper regularization , second order methods help . Like I said before , there are a number of confounding factors and the story is not as simple .", "rating": "4: Ok but not good enough - rejection", "reply_text": "\\ > ___In Section 2.4 \u2026 such a claim is only valid for the squared loss , and not , for example , the logistic loss . Please clearly explain this . \u2026 In Section 2.5 \u2026 this section focuses on the squared loss with a linear model \u2026 the behaviour for other losses is not clear.___ Thank you for catching this ! We have clarified that the discussion in Section 2.4 is valid for the squared loss ( we had accidentally moved the key sentences into Appendix B ) . In Section 2.5 we have also expanded our discussion around our assumption of a squared loss , to : further emphasize that the theory results only hold for squared loss ; but also to foreshadow that we also observe the effect experimentally for cross entropy loss . For casual readers ( we recognize the reviewer already understands this ) -- we would also like to emphasize that our theory results in Sections 2.1 , 2.2 , and 2.3 hold for any loss . \\ > ___Newton 's method converges to the min-norm solution \u2026 meaning in both cases , its generalization is as good as GD.___ This is incorrect . Similar to our above discussion , GD on unwhitened data can outperform Newton \u2019 s method , or GD on whitened data , precisely because the early stopped performance of GD is better . This is not the case for Newton \u2019 s method . The fact that GD on whitened and unwhitened data and Newton \u2019 s method find the same optimum if run to convergence is true , but not key to why linear models trained on unwhitened data by GD , if early stopped , perform better . \\ > ___In Section 3 , for the experimental results , there are numerous confounding factors that need to be controlled for\u2026 Please explain how did you control for these factors.___ All the details of our experiments are given in Appendix E. The information in this appendix addresses your specific concerns such as the method by which the learning rate was chosen , whether optimization was stochastic or deterministic for each experiment , etc . If you have further questions please follow up . \\ > ___In Figure 5 , the paper shows `` Regularized second-order methods can train faster than gradient descent , with minimal or even positive impact on generalization '' . This contradicts the paper 's title.___ There is no contradiction . Second order methods destroy information that could otherwise be used for generalization . Whether this destruction of information that could be used for generalization is harmful in practice depends on the specifics of the model , task , and optimizer . In addition to Figure 5 , we also discuss this in the abstract and in Section 3 . Thank you again for your time and detailed review ."}], "0": {"review_id": "4sCyjwaVtZ9-0", "review_text": "Topic : whitening destroys generalization main contribution This work offers a mutual information perspective to explain the relations between whitening/second-order optimization and generalization . Strength - The perspective is interesting and a bit intriguing . But many discrepancies may need clarifications . Weakness - The title and the sentiment of the paper may be misleading . It is scary at first glance , but the conclusion and experiments do not really support the claim \u201c destroy \u201d . It may be important to keep the title accurate other than eye-catching . The authors may argue \u201c can \u201d is the key in the title , but the reviewer feels this may be a bit too subtle . - Conditional independence and generalization . The theorems established a certain conditional independence between training features , the model , and the test data . The conditional independence is derived using gradient descent of the whitened data . This is understandable , but the implication on generalization is not crystal clear . - The claims may need more explanation and be better represented ; some simple examples may suggest different conclusion . For example , the reviewer considered a simple least squares problem min_theta ||y-X * theta||^2 with whitened X the solution is theta_opt = X^T * y , which is clearly dependent with X , even if y is revealed . If the training set changes , the learned theta changes . The reviewer wonders if this can be explained by the theorems in the paper \u2013 or did the reviewer miss something ( response is welcome here ) ? - The above leads to another question : The derivation of the work relies on ( 4 ) and ( 5 ) , but either of which fully expressed the gradient . The impression from there is the updates are dominated by the covariance matrices and if the covariance matrices are identity , then there is no information about the training data passed through the training process . This may not be true , if , again , consider the least squares problem , where the gradient is grad = X \u2019 * X theta \u2013 X \u2019 * y where the second term is still data-dependent . In particular , the claim \u201c To establish this result , we note that the first layer activation at initialization , Z0 train , is a random variable due to random weight initialization , and only depends on Xtrain through Ktrain : \u201d The last sentence is a bit questionable to the reviewer . - There may be some clarity issues . For example , ( 8 ) is not easy to understand . In the test stage , why is there still a gradient step performed ? The test stage should not involve any optimization . This may be clarified . - Basic generalization theorem suggests that generalization is only related to function class \u2019 s complexity , but not the correlation among the coordinates of the data samples . As long as the training and test data are sampled from the same distribution , it is hard to see , from classic generalization analysis viewpoint , why using whitening or second-order methods can destroy generalization . The claim from this work is contradicting to what we learn from textbooks , e.g. , Shalev-Shwartz , Shai , and Shai Ben-David . Understanding machine learning : From theory to algorithms . Cambridge university press , 2014 . It may be good that the authors compare their results with classic results , e.g. , those based on uniform convergence , under some function hypothesis complexity measures ( e.g. , finite class , VC dimension or even Radamacher complexity ) . The classic proofs of generalization are insensitive to data whitening or algorithm design , and thus the reviewer feel that there may be a big gap to fill if the claims of this work holds . - The main results claim that whitening is harmful , the simulation may have suggested otherwise . From most of the figures , the generalization error becomes better and better when the sample size increases . Note that when the sample size increases , the sample correlation matrix converges in probability to the ensemble mean , and the whitening could be performed in a more accurate way . This set of results may suggest that , if whitening is done accurately , then it is fine . When the sample size is small , whitening can not be done very accurately since sample correlation is not accurately estimated . The reviewer \u2019 s understanding is that the high test error with small sample size is unlikely an effect of whitening , but * inaccurate * whitening , since more noise was brought into the training process . But when the whitening step is performed without too much noise , generalization only depends on function class used and the number of samples , per classic generalization theories . This interpretation seems more consistent with classic generalization theory . The authors may hope to comment on this . - after the discussion period I would like to thank the authors for the reply , clarification , and additional experiments . Although I do like the perspectives revealed in this work , many points are still quite unclear to me . For example , the theory seems not be able to explain why whitening does not hurt testing when sample size is large , as demonstrated in the paper . This work also does not draw connection between sample size and generalization error , which may make the claims a bit incredible . I would encourage the authors to work towards this direction and solidify the contribution .", "rating": "4: Ok but not good enough - rejection", "reply_text": "\\ > ___It may be good that the authors compare their results with classic results \u2026 classic proofs of generalization are insensitive to data whitening or algorithm design.___ A major revelation of modern learning theory has been that classical generalization results , such as those referenced by the reviewer are incapable of explaining the generalization properties observed in practice . This is sometimes referred to as the generalization puzzle [ Zhang et . al.ICLR 2017 ( arXiv:1611.03530 ) , Neyshabur et . al.NeurIPS 2017 ( arXiv : 1706.08947 ) ] . It has been realized that a major shortcoming of classic work is that generalization dynamics of neural networks and even linear models [ Belkin et . al.PMLR 2018 ( arXiv : 1802.01396 ) ] depends crucially on the optimization procedure ( including early stopping ) [ Neyshabur er . al . ( arXiv : 1705.03071 ) ] , and structure of the data [ Gerace et al.2020 ( arxiv : 2002.09339 ) ] , not just the worst case behavior of the function class . \\ > ___Note that when the sample size increases , the sample correlation matrix converges in probability to the ensemble mean , and the whitening could be performed in a more accurate way . This set of results may suggest that , if whitening is done accurately , then it is fine . When the sample size is small , whitening can not be done very accurately since sample correlation is not accurately estimated.___ We emphasize that whitening is * * defined * * in terms of a finite dataset , rather than the distribution that generated that dataset ( eg see https : //en.wikipedia.org/wiki/Whitening_transformation ) . As such , the whitening we consider in the paper is done exactly and accurately . The question of what happens if the whitening matrix is allowed to go to its ensemble limit is still interesting , though ! To answer this , we repeated our linear model experiment , but using a whitening matrix computed on the entire dataset even when training on a subset of the data . See the new Appendix Figure App 2 . We find that using the whole-dataset whitening transform , which we call \u201c distribution whitening \u201d , provides a slight performance improvement , but still performs most similarly to when the whitening transform is computed on only a subset of the data ."}, "1": {"review_id": "4sCyjwaVtZ9-1", "review_text": "# # Overview In supervised learning tasks , it is common in practice to apply a * whitening * transformation to remove correlations between input features . This can improve the conditioning of the underlying data manifold , enabling faster convergence . This paper shows that for a large class of models models $ f $ consisting of a fully-connected layer followed by an arbitrary parameterized function , $ f ( X ) = g_\\theta ( WX ) $ data whitening removes all information that is relevant for generalization . Furthermore , this has implications for the generalization ability of second-order methods such as Newton 's method due to a well-known equivalence between Newton 's method and steepest-descent applied to whitened data . The effects suggested by the presented theory are verified empirically , and the are additionally observed in convolutional neural networks , suggesting that the phenomenon could apply more broadly to more complicated connectionist models as well . * * Overall , I recommend that the paper be accepted * * . The theoretical results are interesting and potentially high-impact , and the writing clarity was excellent throughout . My main reservation about the paper is that it 's not clear to me which insights are being proposed as novel , and among those , which actually are novel . In particular , - * * Section 2.5 * * : The relationship between Newton 's method and data whitening is well-known . Does section 2.5 include any novel insights , or is this included for completeness ? If it 's the latter it seems like it would be better-placed in an appendix ; the fact that the main results relate also to second-order methods could then be noted in the discussion - * * Generally * * speaking , how does what 's already known from e.g.dimensionality reduction and PCA play into this ? As mentioned in the paper , whitening is essentially putting the signal and noise in the data on equal footing . From a principal components perspective , it 's as if you 're forcing the model to consider all components to be equally predictive , which would clearly harm generalization when the data has strong feature correlations . Is there truly no prior result of this kind in that literature ? It seems quite fundamental , and my concern is that this phenomenon might be already well-known under different terminology in another literature . * ( The following question is out of scope but I think could potentially increase the impact of the paper a great deal ) * Do you suspect these issues hold also for the online second-order methods , such as Online Newton Step and AdaGrad ? If so , adding some discussion about this could potentially increase the impact of this work since AdaGrad ( and it 's heuristic descendants Adam , RMSProp etc . ) are by-far the most common algorithms used for optimizing neural networks in practice # # Clarifications - * * Page 4 * * : W is isotropic , and whitening makes X isotropic , so that Z is isotropic , so intuitively the combination of whitening and isotropic initialization results in trying to make predictions from isotropic noise . Is it specifically the * combination * of whitening and isotropic initialization that 's the problem ? The paper seems to be really centered around the whitening , when it seems to be the specific combination of the two rather than whitening alone . Or is there a nuance I 'm missing here ? - * * Page 8 * * : * '' We therefore believe that regularized Gauss-Newton should be viewed as discarding information in the large-eigenvector subspace '' * . I 'm not sure I understand what is being said here . Would n't this suggest that discarding principle components is beneficial ? this seems to run contrary to a lot of the dimensionality reduction literature . # # Minor Comments ( which did not influence my score but could improve clarity ) - * * Page 2 * * : * '' Our result is not restricted to neural networks , and applies to any model in which the input is transformed by a dense matrix multiply with isotropic weight initialization '' * should read * dense matrix with isotropic weight initialization * ? - * * Page 2 * * : It is not really necessary to list 34 ( ! ! ) papers on second-order optimization in a single sentence ; there are plenty of surveys on the topic that could be linked instead - * * Page 7 , Figure 4 * * : * '' Linear models trained on whitened data optimize faster , but their best test accuracy is always worse '' * . This could use some minor rephrasing to say that the best test accuracy * was * always worse , as this conclusion is about this specific problem rather than for all problems generally - * * Page 8 * * : * '' Regularized Gauss-Newton optimization acts similarly to unregularized Gauss-Newton in the subspace spanned by eigenvectors with eigenvalues larger than \u03bb/ ( 1 \u2212 \u03bb ) , and similarly to steepest descent in the subspace spanned by eigenvectors with eigenvalues smaller than \u03bb/ ( 1 \u2212 \u03bb ) '' * This fact is not immediately obvious , so a citation would be helpful here ( or potentially a link to an appendix if it 's not a well-known result ) - * * Page 14 , appendix A * * : The integral notation here is unclear to me ; what kind of integral is this supposed to be ? what is delta ?", "rating": "7: Good paper, accept", "reply_text": "\\ > ___Re : minor comments___ We thank the reviewer for their suggestions to improve the clarity of our exposition , and have implemented most of these . In Appendix A the measure of integration is the uniform measure over the components of W^ { 0 } . This is defined in the text . It can also be written as the product \\prod_i dW^ { 0 } _ { i } . The delta is a Dirac delta function . We have expanded the discussion on page 8 . The primary concern you expressed about our paper was ( boldly paraphrasing ! ) that the results seemed basic and important enough that a subset of them must surely already be known and published . We hope you are more convinced of the work \u2019 s novelty after our rebuttal , and after seeing that no other reviewer raised concerns about missing related work . If you are now more convinced of the paper \u2019 s novelty , we respectfully ask you to consider more strongly supporting acceptance , and raising your score accordingly . Thank you again !"}, "2": {"review_id": "4sCyjwaVtZ9-2", "review_text": "Summary The authors analyse the training dynamics of a machine learning model consisting in a linear unit , followed by any parametrized function . The authors in particular focus on the impact of whitening the data beforehand or using second order methods . They show that the learned parameters of the model only depend on the training data through its Gram matrix . Since whitening trivializes the Gram matrix , the authors argue that whitening destroys important information . Major comments - The article is well written , the arguments are well presented and easily understood . - The main message of the paper , contained in Fig.3 , is easily reproduced with a few lines of code . - The paper sheds an interesting light on the generalization properties of second order methods . Thm.2.2.1 and 2.2.2 treat the initialization of the weights as random variables . I believe that it makes them rather useless for practical applications , because in practice the machine learning model will only be initialized once . Of course , the theorems also apply if we assume that the initial weights are zero ( or close to zero ) , which is still a very interesting case . In the experiments description of fig.3 . it would be worthwhile to better describe the initialization strategy for the linear weights . For instance , I used as X the load_digits dataset in scikit-learn , as Y a one hot encoding of the target , and a simple linear model with MSE loss $ f ( W ) = \\|WX - Y\\|^2 $ , and managed to get the same fig.3.a as the authors , but only if I initialize the weights very close to 0 , or at 0 . - I think that Sec.2.4 is too shallow . First , it should be clearly stated that eq.14 only works for MSE loss with a purely linear model . Second , I think that another important concept which is not mentioned in the article is that early stopping is similar to regularization of the parameters , which is why the reported \u2018 test error \u2019 in the experiments is lower than the test error obtained by perfectly minimizing the train error . See e.g.Yao , Yuan , Lorenzo Rosasco , and Andrea Caponnetto . `` On early stopping in gradient descent learning . '' Constructive Approximation 26.2 ( 2007 ) : 289-315 . - In Sec.2.5 , the authors consider Newton \u2019 s method with MSE loss on a linear problem , which converges in just one step with $ \\eta=1 $ . This should be acknowledged , and maybe it would be easier to defend the point by letting $ \\eta \\to 0 $ instead . Minor comments - The meaning of \u2018 test error \u2019 in the figure legend is a bit vague , and I think it should rather be replaced by \u2018 best test error during training \u2019 or something similar . - In fig.4.b , does training epochs mean \u2018 number of epochs to reach the best test error \u2019 ? Misc - What does WRNs mean ?", "rating": "7: Good paper, accept", "reply_text": "Thank you for volunteering your time to review our paper , and for your careful review ! \\ > ___The main message of the paper , contained in Fig.3 , is easily reproduced with a few lines of code.___ It \u2019 s very neat that you were able to reproduce our results so easily ! \\ > ___In the experiments description of Fig.3 . it would be worthwhile to better describe the initialization strategy for the linear weights.___ We used a Gaussian with mean zero and variance 1e-4 to initialize weights for all experiments except the linear model , which we initialized with zero weights . We thank the reviewer for catching our inadvertent omission of this information from the Methods ( Appendix E ) , and have now corrected it ! \\ > ___Thm.2.2.1 and 2.2.2 treat the initialization of the weights as random variables . I believe that it makes them rather useless for practical applications , because in practice the machine learning model will only be initialized once.___ We believe these results are relevant for practical applications . For high dimensional input , the outputs of a model trained on full-whitened data will contain no information about its training inputs . It is possible for a single random initialization of this model to do well , in the same way it would be possible to do well on a test set by randomly flipping a coin to predict each test example . However , * a typical random initialization will not perform any better than would a typical sequence of coin flips ! * As further evidence for the relevance of the results to individual models , we can see in Figure 3 that the performance of randomly initialized models concentrates around their average case performance . \\ > ___It should be clearly stated that eq.14 only works for MSE loss with a purely linear model\u2026 early stopping is similar to regularization of the parameters.___ Thank you for catching this . We have updated Section 2.4 to clarify that we analyze the mean squared loss . We emphasize though that our results in Sections 2.1 through 2.3 hold for any loss function , not just MSE loss , and that many of our experiments use cross entropy loss . We have also added a brief discussion of the effect of early stopping , and its relationship to weight regularization , to Section 2.4 . \\ > ___In Sec.2.5 , the authors consider Newton \u2019 s method with MSE loss on a linear problem , which converges in just one step with \u03b7=1 . This should be acknowledged.___ We have updated Section 2.5 with a sentence stating that a single Newton step with $ \\eta = 1 $ solves the optimization problem over the training dataset . \\ > ___Re : minor comments and miscellaneous___ Figure 4b . In all experiments with MLPs , the models were trained to a fixed training accuracy cutoff , at which point test error was measured . The label of the y-axis indicates the number of epochs it took to reach this training cutoff . This is explained in the figure caption and in Appendix E. WRN is the acronym for \u201c wide residual network \u201d ."}, "3": {"review_id": "4sCyjwaVtZ9-3", "review_text": "This paper shows theoretical and empirical evidence that under some conditions , whitening the input data and second-order methods may hurt the generalization performance . The paper is well written with good intuitions but overclaims its results . The theoretical results are limited to specific cases , and the experimental results would benefit from systematic ablation studies . All in all , I do not think that the paper provides sufficient evidence that justifies its title and main message of the paper . Detailed review below . - In Section 2.1 , `` the trained model depends on the training data only through K '' , this is technically incorrect , since there is a dependence on y_train and the Z_train is independent of X_train , only when conditioned on both K_train and y_train . Since whitening only affects K_train , there still might be some information in y_train , especially for the case of data realizable by the model . Please clarify this . - In Section 2.3 , the statement `` whitening hurts generalization '' is true only for high-dimensional datasets , where the input dimension > number of samples . I agree that such datasets ( arising for example in genetics ) are important , this is not the typical case where neural networks are used . For example , for all vision tasks , d < n. Please explicitly say this . - In Section 2.4 , please also consider d > n case . This is more important because there are multiple solutions , even in the linear case , and whitening may hurt generalization . For the d < n case analyzed in the paper , for a linear model , for most losses , the optimization problem is strictly convex and a unique solution . This is hardly a claim justifying that `` whitening hurts generalization '' , it is an optimization problem and is orthogonal to the paper . For convex problems with a unique solution , there is no reason to early stop the optimization . The convergence rate is linear implying that the solution is reached quickly and this result is not interesting , either from an optimization/generalization perspective . Moreover , such a claim is only valid for the squared loss , and not , for example , the logistic loss . Please clearly explain this . - In Section 2.5 , the paper claims that `` unregularized second-order methods have poor generalization '' , but as the authors admit themselves , second-order methods are rarely used without regularization , again making this result not practically important . Moreover , there have been second-order methods ( KFAC for example ) that employ regularization , approximations to the Hessian and generalize well . The statement `` second-order information destroys generalization '' is therefore an overclaim . - Moreover , this section focuses on the squared loss with a linear model . If n > d , then Newton 's method converges to the min-norm solution , same as GD . For d > n , the solution of Newton 's method lies in the span of the training data ( if we use the pseudo-inverse for the Hessian or add a small regularizer ) and also converges to the min-norm solution , meaning in both cases , its generalization is as good as GD . Again , the behaviour for other losses is not clear . - In Section 3 , for the experimental results , there are numerous confounding factors that need to be controlled for . For example , is the optimization deterministic or stochastic ? We know that stochastic methods typically generalize better . How is the learning rate selected ? Is there a warmup phase ? These factors play an important role in the generalization . Similarly , both the optimization method and the loss function also influence the generalization . Is there regularization , either L2 or L1 in the case when d > n ( this is typically done for high dimensional datasets ) . Please explain how did you control for these factors . - In Figure 3 ( a ) , what is the effect of using a regularized Newton method ? What if you run a more standard second-order method like KFAC ? - In Figure 5 , the paper shows `` Regularized second-order methods can train faster than gradient descent , with minimal or even positive impact on generalization '' . This contradicts the paper 's title . As the authors claim , when done without regularization , second-order methods can harm generalization , but when used with proper regularization , second order methods help . Like I said before , there are a number of confounding factors and the story is not as simple .", "rating": "4: Ok but not good enough - rejection", "reply_text": "\\ > ___In Section 2.4 \u2026 such a claim is only valid for the squared loss , and not , for example , the logistic loss . Please clearly explain this . \u2026 In Section 2.5 \u2026 this section focuses on the squared loss with a linear model \u2026 the behaviour for other losses is not clear.___ Thank you for catching this ! We have clarified that the discussion in Section 2.4 is valid for the squared loss ( we had accidentally moved the key sentences into Appendix B ) . In Section 2.5 we have also expanded our discussion around our assumption of a squared loss , to : further emphasize that the theory results only hold for squared loss ; but also to foreshadow that we also observe the effect experimentally for cross entropy loss . For casual readers ( we recognize the reviewer already understands this ) -- we would also like to emphasize that our theory results in Sections 2.1 , 2.2 , and 2.3 hold for any loss . \\ > ___Newton 's method converges to the min-norm solution \u2026 meaning in both cases , its generalization is as good as GD.___ This is incorrect . Similar to our above discussion , GD on unwhitened data can outperform Newton \u2019 s method , or GD on whitened data , precisely because the early stopped performance of GD is better . This is not the case for Newton \u2019 s method . The fact that GD on whitened and unwhitened data and Newton \u2019 s method find the same optimum if run to convergence is true , but not key to why linear models trained on unwhitened data by GD , if early stopped , perform better . \\ > ___In Section 3 , for the experimental results , there are numerous confounding factors that need to be controlled for\u2026 Please explain how did you control for these factors.___ All the details of our experiments are given in Appendix E. The information in this appendix addresses your specific concerns such as the method by which the learning rate was chosen , whether optimization was stochastic or deterministic for each experiment , etc . If you have further questions please follow up . \\ > ___In Figure 5 , the paper shows `` Regularized second-order methods can train faster than gradient descent , with minimal or even positive impact on generalization '' . This contradicts the paper 's title.___ There is no contradiction . Second order methods destroy information that could otherwise be used for generalization . Whether this destruction of information that could be used for generalization is harmful in practice depends on the specifics of the model , task , and optimizer . In addition to Figure 5 , we also discuss this in the abstract and in Section 3 . Thank you again for your time and detailed review ."}}