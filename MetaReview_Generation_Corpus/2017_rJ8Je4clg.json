{"year": "2017", "forum": "rJ8Je4clg", "title": "Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening", "decision": "Accept (Poster)", "meta_review": "The authors have proposed an improvement to q-learning which imposes upper and lower constraint bounds on the q-function, which are implemented using quadratic penalties. This speeds up learning, at least for a limited set of Atari games. The reviewers are strongly divided on the merits of this paper. The authors have worked to be responsive to the reviewers' concerns, however, and the paper has been improved, so I side with the positives.", "reviews": [{"review_id": "rJ8Je4clg-0", "review_text": "In this paper, the authors proposed a extension to the DQN algorithm by introducing both an upper and lower bound to the optimal Q function. The authors show experimentally that this approach improves the data efficiency quite dramatically such that they can achieve or even supersede the performance of DQN that is trained in 8 days. The idea is novel to the best of my knowledge and the improvement over DQN seems very significant. Recently, Remi et al have introduced the Retrace algorithm which can make use of multi-step returns to estimate Q values. As I suspect, some of the improvements that comes from the bounds is due to the fact that multi-step returns is used effectively. Therefore, I was wondering whether the authors have tried any approach like Retrace or Tree backup by Precup et al. and if so how do these methods stack up against the proposed method. The author have very impressive results and the paper proposes a very promising direction for future research and as a result I would like to make a few suggestions: First, it would be great if the authors could include a discussion about deterministic vs stochastic MDPs. Second, it would be great if the authors could include some kind of theoretically analysis about the approach. Finally, I would like to apologize for the late review.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank the reviewer for the encouraging feedback and suggestions . About deterministic vs stochastic MDPs : We updated the submission and added a discussion about stochastic MDPs to the appendix . About theoretical analysis : A theoretical analysis should follow the arguments of Q-learning . Usage of non-convex Q-functions likely makes any proofs hard though . We can add a discussion if really desired . About Retrace/Tree backup : At this point we haven \u2019 t compared to Retrace or Tree backup by Precup et al.Since there is no public implementation of Retrace/Tree backup , as suggested by other reviewers , we will compare our method to several other baselines , such as n-step Q-learning . It is also worth noting that in the Retrace/Tree backup paper , the Q * ( \\lambda ) algorithm seems to not work that well on Atari games . We will add those results as soon as they become available . Please keep in mind that we have some resource constraints ."}, {"review_id": "rJ8Je4clg-1", "review_text": "In this paper, a Q-Learning variant is proposed that aims at \"propagating\" rewards faster by adding extra costs corresponding to bounds on the Q function, that are based on both past and future rewards. This leads to faster convergence, as shown on the Atari Learning Environment benchmark. The paper is well written and easy to follow. The core idea of using relaxed inequality bounds in the optimization problem is original to the best of my knowledge, and results seem promising. This submission however has a number of important shortcomings that prevent me from recommending it for publication at ICLR: 1. The theoretical justification and analysis is very limited. As far as I can tell the bounds as defined require a deterministic reward to hold, which is rarely the case in practice. There is also the fact that the bounds are computed using the so-called \"target network\" with different parameters theta-, which is another source of discrepancy. And even before that, the bounds hold for Q* but are applied on Q for which they may not be valid until Q gets close enough to Q*. It also looks weird to take the max over k in (1, ..., K) when the definition of L_j,k makes it look like the max has to be L_j,1 (or even L_j,0, but I am not sure why that one is not considered), since L*_j,0 >= L*_j,1 >= ... >= L*_j,K. Neither of these issues are discussed in the paper, and there is no theoretical analysis of the convergence properties of the proposed method. [Update: some of these concerns were addressed in OpenReview comments] 2. The empirical evaluation does not compensate, in my opinion, for the lack of theory. First, since there are two bounds introduced, I would have expected \"ablative\" experiments showing the improvement brought by each one independently. It is also unfortunate that the authors did not have time to let their algorithm run longer, since as shown in Fig. 1 there remain a significant amount of games where it performs worse compared to DQN. In addition, comparisons are limited to vanilla DQN and DDQN: I believe it would have been important to compare to other ways of incorporating longer-term rewards, like n-step Q-Learning or actor-critic. Finally, there is no experiment demonstrating that the proposed algorithm can indeed improve other existing DQN variants: I agree with the author when they say \"We believe that our method can be readily combined with other techniques developed for DQN\", however providing actual results showing this would have made the paper much stronger. In conclusion, I do believe this line of research is worth pursuing, but also that additional work is required to really prove and understand its benefits. Minor comments: - Instead of citing the arxiv Wang et al (2015), it would be best to cite the 2016 ICML paper - The description of Q-Learning in section 3 says \"The estimated future reward is computed based on the current state s or a series of past states s_t if available.\" I am not sure what you mean by \"a series of past states\", since Q is defined as Q(s, a) and thus can only take the current state s as input, when defined this way. - The introduction of R_j in Alg. 1 is confusing since its use is only explained later in the text (in section 5 \"In addition, we also incorporate the discounted return R_j in the lower bound calculation to further stabilize the training\") - In Fig. S1 the legend should not say \"10M\" since the plot is from 1M to 10M", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the feedback and suggestions . About convergence : Since we are using non-convex functions , a convergence result would not be applicable . Note that this is generally true for all DQN approaches , and ours is no different . About bounds : Note that the max over the different lower bounds is not necessarily attained for close by steps ( k=1 ) . Particularly at the beginning of training the tightest bounds are actually obtained for large values of k. Intuitively this can be understood when considering high rewards in a maze environment that have been propagated to MDP states a few locations away . Hence we disagree with R1s claim that the maximum lower bound arises from neighboring states , i.e. , the max is L_j,1 . Note that L_j,0 is the term used in the standard DQN cost function , i.e. , y_j = L_j,0 , and we aim at regressing to that value . In fact we could include it as an additional constraint , e.g. , as both a lower bound and an upper bound . This would only change the relative weight between the terms however , hence no need to further pursue this direction . Usage of a target network to compute y_j = L_j,0 is common practice . Hence it seems intuitive to follow the same idea for the bounds . About empirical evaluation : We decided to focus on a comparison with vanilla DQN because the proposed technique can be combined with almost all of the later improvements , e.g. , n-step Q-learning , actor-critic methods , double-DQN etc . In addition , we felt it is important to assess the differences to the core technique on a large number of tasks . Therefore we conduct experiments on a variety of different benchmarks as opposed to cherry-picking a few examples . We think using few benchmarks and many algorithms is just as reasonable as using many benchmarks and few algorithms . Both is unfortunately not possible given our current resource constraints . We are in the process of performing additional experiments which we will add as they become available ."}, {"review_id": "rJ8Je4clg-2", "review_text": "This paper proposes an improvement to the q-learning/DQN algorithm using constraint bounds on the q-function, which are implemented using quadratic penalties in practice. The proposed change is simple to implement and remarkably effective, enabling both significantly faster learning and better performance on the suite of Atari games. I have a few suggestions for improving the paper: The paper could be improved by including qualitative observations of the learning process with and without the proposed penalties, to better understand the scenarios in which this method is most useful, and to develop a better understanding of its empirical performance. It would also be nice to include zoomed-out versions of the learned curves in Figure 3, as the DQN has yet to converge. Error bars would also be helpful to judge stability over different random seeds. As mentioned in the paper, this method could be combined with D-DQN. It would be interesting to see this combination, to see if the two are complementary. Do you plan to do this in the final version? Also, a couple questions: - Do you think the performance of this method would continue to improve after 10M frames? - Could the ideas in this paper be extended to methods for continuous control like DDPG or NAF?", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for all your and other reviewers ' insightful comments . We hope to address them and improve our paper . We have been working on the combination of our method and other useful techniques to check whether they are complementary . We have been also implementing other baselines , such as n-step Q learning . Due to the high computational demand of these experiments , we will try to include as many results as possible in the upcoming revision . In addition , we will also include discussion and justification of our approach for stochastic MDPs ."}], "0": {"review_id": "rJ8Je4clg-0", "review_text": "In this paper, the authors proposed a extension to the DQN algorithm by introducing both an upper and lower bound to the optimal Q function. The authors show experimentally that this approach improves the data efficiency quite dramatically such that they can achieve or even supersede the performance of DQN that is trained in 8 days. The idea is novel to the best of my knowledge and the improvement over DQN seems very significant. Recently, Remi et al have introduced the Retrace algorithm which can make use of multi-step returns to estimate Q values. As I suspect, some of the improvements that comes from the bounds is due to the fact that multi-step returns is used effectively. Therefore, I was wondering whether the authors have tried any approach like Retrace or Tree backup by Precup et al. and if so how do these methods stack up against the proposed method. The author have very impressive results and the paper proposes a very promising direction for future research and as a result I would like to make a few suggestions: First, it would be great if the authors could include a discussion about deterministic vs stochastic MDPs. Second, it would be great if the authors could include some kind of theoretically analysis about the approach. Finally, I would like to apologize for the late review.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank the reviewer for the encouraging feedback and suggestions . About deterministic vs stochastic MDPs : We updated the submission and added a discussion about stochastic MDPs to the appendix . About theoretical analysis : A theoretical analysis should follow the arguments of Q-learning . Usage of non-convex Q-functions likely makes any proofs hard though . We can add a discussion if really desired . About Retrace/Tree backup : At this point we haven \u2019 t compared to Retrace or Tree backup by Precup et al.Since there is no public implementation of Retrace/Tree backup , as suggested by other reviewers , we will compare our method to several other baselines , such as n-step Q-learning . It is also worth noting that in the Retrace/Tree backup paper , the Q * ( \\lambda ) algorithm seems to not work that well on Atari games . We will add those results as soon as they become available . Please keep in mind that we have some resource constraints ."}, "1": {"review_id": "rJ8Je4clg-1", "review_text": "In this paper, a Q-Learning variant is proposed that aims at \"propagating\" rewards faster by adding extra costs corresponding to bounds on the Q function, that are based on both past and future rewards. This leads to faster convergence, as shown on the Atari Learning Environment benchmark. The paper is well written and easy to follow. The core idea of using relaxed inequality bounds in the optimization problem is original to the best of my knowledge, and results seem promising. This submission however has a number of important shortcomings that prevent me from recommending it for publication at ICLR: 1. The theoretical justification and analysis is very limited. As far as I can tell the bounds as defined require a deterministic reward to hold, which is rarely the case in practice. There is also the fact that the bounds are computed using the so-called \"target network\" with different parameters theta-, which is another source of discrepancy. And even before that, the bounds hold for Q* but are applied on Q for which they may not be valid until Q gets close enough to Q*. It also looks weird to take the max over k in (1, ..., K) when the definition of L_j,k makes it look like the max has to be L_j,1 (or even L_j,0, but I am not sure why that one is not considered), since L*_j,0 >= L*_j,1 >= ... >= L*_j,K. Neither of these issues are discussed in the paper, and there is no theoretical analysis of the convergence properties of the proposed method. [Update: some of these concerns were addressed in OpenReview comments] 2. The empirical evaluation does not compensate, in my opinion, for the lack of theory. First, since there are two bounds introduced, I would have expected \"ablative\" experiments showing the improvement brought by each one independently. It is also unfortunate that the authors did not have time to let their algorithm run longer, since as shown in Fig. 1 there remain a significant amount of games where it performs worse compared to DQN. In addition, comparisons are limited to vanilla DQN and DDQN: I believe it would have been important to compare to other ways of incorporating longer-term rewards, like n-step Q-Learning or actor-critic. Finally, there is no experiment demonstrating that the proposed algorithm can indeed improve other existing DQN variants: I agree with the author when they say \"We believe that our method can be readily combined with other techniques developed for DQN\", however providing actual results showing this would have made the paper much stronger. In conclusion, I do believe this line of research is worth pursuing, but also that additional work is required to really prove and understand its benefits. Minor comments: - Instead of citing the arxiv Wang et al (2015), it would be best to cite the 2016 ICML paper - The description of Q-Learning in section 3 says \"The estimated future reward is computed based on the current state s or a series of past states s_t if available.\" I am not sure what you mean by \"a series of past states\", since Q is defined as Q(s, a) and thus can only take the current state s as input, when defined this way. - The introduction of R_j in Alg. 1 is confusing since its use is only explained later in the text (in section 5 \"In addition, we also incorporate the discounted return R_j in the lower bound calculation to further stabilize the training\") - In Fig. S1 the legend should not say \"10M\" since the plot is from 1M to 10M", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the feedback and suggestions . About convergence : Since we are using non-convex functions , a convergence result would not be applicable . Note that this is generally true for all DQN approaches , and ours is no different . About bounds : Note that the max over the different lower bounds is not necessarily attained for close by steps ( k=1 ) . Particularly at the beginning of training the tightest bounds are actually obtained for large values of k. Intuitively this can be understood when considering high rewards in a maze environment that have been propagated to MDP states a few locations away . Hence we disagree with R1s claim that the maximum lower bound arises from neighboring states , i.e. , the max is L_j,1 . Note that L_j,0 is the term used in the standard DQN cost function , i.e. , y_j = L_j,0 , and we aim at regressing to that value . In fact we could include it as an additional constraint , e.g. , as both a lower bound and an upper bound . This would only change the relative weight between the terms however , hence no need to further pursue this direction . Usage of a target network to compute y_j = L_j,0 is common practice . Hence it seems intuitive to follow the same idea for the bounds . About empirical evaluation : We decided to focus on a comparison with vanilla DQN because the proposed technique can be combined with almost all of the later improvements , e.g. , n-step Q-learning , actor-critic methods , double-DQN etc . In addition , we felt it is important to assess the differences to the core technique on a large number of tasks . Therefore we conduct experiments on a variety of different benchmarks as opposed to cherry-picking a few examples . We think using few benchmarks and many algorithms is just as reasonable as using many benchmarks and few algorithms . Both is unfortunately not possible given our current resource constraints . We are in the process of performing additional experiments which we will add as they become available ."}, "2": {"review_id": "rJ8Je4clg-2", "review_text": "This paper proposes an improvement to the q-learning/DQN algorithm using constraint bounds on the q-function, which are implemented using quadratic penalties in practice. The proposed change is simple to implement and remarkably effective, enabling both significantly faster learning and better performance on the suite of Atari games. I have a few suggestions for improving the paper: The paper could be improved by including qualitative observations of the learning process with and without the proposed penalties, to better understand the scenarios in which this method is most useful, and to develop a better understanding of its empirical performance. It would also be nice to include zoomed-out versions of the learned curves in Figure 3, as the DQN has yet to converge. Error bars would also be helpful to judge stability over different random seeds. As mentioned in the paper, this method could be combined with D-DQN. It would be interesting to see this combination, to see if the two are complementary. Do you plan to do this in the final version? Also, a couple questions: - Do you think the performance of this method would continue to improve after 10M frames? - Could the ideas in this paper be extended to methods for continuous control like DDPG or NAF?", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for all your and other reviewers ' insightful comments . We hope to address them and improve our paper . We have been working on the combination of our method and other useful techniques to check whether they are complementary . We have been also implementing other baselines , such as n-step Q learning . Due to the high computational demand of these experiments , we will try to include as many results as possible in the upcoming revision . In addition , we will also include discussion and justification of our approach for stochastic MDPs ."}}