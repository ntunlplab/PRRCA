{"year": "2020", "forum": "HJe4Cp4KwH", "title": "GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation", "decision": "Reject", "meta_review": "While considerable effort went into improving the paper during the author response period, the concerns outlined by reviewer 2 remain and the aggregate score across reviewers reflects this issue. The AC recommends rejection with strong encouragement to resubmit this work to another high quality venue upon further revision to the work.", "reviews": [{"review_id": "HJe4Cp4KwH-0", "review_text": "This paper introduces a new type of Graph Neural Network (GNN) that incorporates Feature-wise Linear Modulation (FiLM) layers. Current GNNs update the target representations by aggregating information from neighbouring nodes without taking into the account the target node representation. As graph networks might benefit from such target-source interactions, the current work proposes to use FiLM layers to let the target node modulate the source node representations. The authors thoroughly evaluate this new architecture \u2014 called GNN-FiLM\u2014-on several graph benchmarks, including Citeseer, PPI, QM9, and VarMisuse. The proposed network outperforms the other methods on QM9 and is on par on the other benchmarks. Strengths - The literature review of existing work on GNN was a pleasure to read and provided a good motivation for the proposed GNN-FiLM architecture. - The authors put significant effort into reproducing other GNN baselines. Perhaps the most surprising result of this work is that all GNNs perform remarkably similar (contrary to what previous work has reported) Weaknesses - There seems to be a much tighter relationship between GNN-FiLM and Gated GNNs than currently discussed. If you actually write down the equations of the recurrent cell $r$ in Eq 1), you\u2019ll notice that there are feature-wise interactions between the target node representations and the (sum of) source node representations. The paper should discuss in more depth what the exact differences are. Some visualizations would also help here. - Related to the previous point, I\u2019d like to see a bit more discussion on *why* tight interactions between target and source nodes are helpful. For example, one could perhaps provide a toy example for which that\u2019s obviously the case. All in all, I believe the ideas and results of this paper are promising but insufficient for publication in its current form. The paper tries to communicate two messages: 1) a model paper arguing for GNN-FiLM, 2) an unbiased evaluation of existing GNN models, showing that their performance is surprisingly similar on a number of benchmarks (with equal hyperparameter search). Both points are interesting but are not worked out sufficiently to pass the bar. For 1), I\u2019d like to see an in-depth discussion on the benefits of target-source interactions, providing more insights into why this might be beneficial. Your current experiments report very minimal gains for your proposed network, questioning why such interactions might be necessary in the first place. For 2), I\u2019d suggest to rewrite the paper from a slightly different angle and add more graph benchmarks if available (disclaimer: I\u2019m not in the graph network community, so I can\u2019t fully evaluate how significant these results are) Typos \u2014\u2014- Last paragraph of intro: \u201ctwo two\u201d - > two *EDIT After reading the rebuttal and revised paper, I've updated my score to \"Weak Accept\". The toy example and connection to GGNN are important additions which makes the paper more complete, though I believe there's room for further improvement here (see comments below). All in all, I weakly recommend to accept the paper. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your kind ( emergency ? ) review and engaging with this submission . > - There seems to be a much tighter relationship between GNN-FiLM and > Gated GNNs than currently discussed . If you actually write down the > equations of the recurrent cell in Eq 1 ) , you \u2019 ll notice that there > are feature-wise interactions between the target node representations > and the ( sum of ) source node representations . This is indeed true ( for GRU/LSTM cells ) and should maybe discussed in the paper in more depth , though it is a bit unclear where to best do this to not interrupt the flow of presentation . The differences arise from the application of the gated cell after summation of incoming messages . Concretely , the `` forgetting '' of memories in a GRU/LSTM is similar to modulation of messages from the self-loop edges , and the gating of the cell input is similar to the modulation of incoming messages . However , as GNN-FiLM uses _different_ $ \\gamma_\\ell $ / $ \\beta_\\ell $ values for different edge types $ \\ell $ , the modulation is additionally dependent on the kind of relationship between nodes . In the case of a single edge type ( + a fresh self-loop edge type ) , GGNNs are indeed mathematically very similar to GNN-FiLM . > - Related to the previous point , I \u2019 d like to see a bit more discussion > on * why * tight interactions between target and source nodes are > helpful . For example , one could perhaps provide a toy example for > which that \u2019 s obviously the case . A simple toy example may be the following : Assume nodes are of type VA and type VB , and there are two edge types E1 and E2 . Our task is to count the number of E1-neighbours of VA nodes , and the number of E2-neighbours of VB nodes . In FiLM , we can `` filter '' edges by using $ \\gamma_ { E1 } = 1 , \\gamma_ { E2 } = 0 $ for VA nodes and $ \\gamma_ { E1 } = 0 , \\gamma_ { E2 } =1 $ for VB nodes . Hence , GNN-FiLM can solve this task using a single layer . Other models obviously can solve this task as well with more layers and feature dimensions by counting VA-E1 , VA-E2 , VB-E1 , VB-E1 neighbours separately in a first message passing step , and then projecting to the desired dimension in a second layer . More generally , as the modulation of messages depends on the edge target representation and the edge type , GNN-FiLM can learn to ( softly ) select a subset of the graph edges for message passing , conditioned on the current representation of nodes , _per feature dimension_ ( this is a substantial difference to the newly-introduced R-GAT setting ; the original GAT model has no notion of edge types ) . For example , in the Program Graph setting of the VarMisuse task , it may choose to emphasise `` NextVariableUse '' edges for some dimensions to gain information about how a variable is used later . You 're right that this intuition is not provided clearly in the paper at the moment , and will be added in the next revision ( but maybe only after feedback if these explanations make sense to a reader ) . > Your current experiments report very minimal gains for your proposed > network , questioning why such interactions might be necessary in the > first place . Note that the experimental gains over _standard_ baselines are quite substantial : * For Tab . 1 , the reported state of the art is a Micro-F1 of 0.973 +/- 0.002 * For Tab . 2 , there is no clear state of the art as many papers use slightly different experiment design for this task ( e.g. , provide more features , ... ) ; but the standard models are GGNN/R-GCN . * For Tab . 3 , the reported state of the art is 84.0 % ( on SeenProjTest ) and 74.1 % ( on UnseenProjTest ) accuracy . While more graph tasks are studied , these 3 provide a good coverage of graph tasks ( from small to large graph , classification and regression , node-level and graph-level ) . The trends in these results are very likely to hold for the majority of tasks . Early feedback ( and curiosity ) led to more and more baselines in the experiments , and existing baselines were improved in a number of ways ( e.g. , the generalisations to the multi-relational setting , which are crucial for the performance in the reported experiments ) . Your review fits exactly to fear around submitting this paper : By doing a very thorough job on the experiments , the importance of the new method looks smaller ; doing a worse job on the experiments ( not including GNN-MLP * ; not extending GAT/GIN to R-GAT/R-GIN ) would have made it look better . These nuances can not be obvious to someone who 's not active in the GNN research community ( and so this rant is not meant as a `` you did a bad job as a reviewer '' ) , but this explanation may help to understand the context . However , the result that the differences between well-tuned , equally-well implemented models are small ( and that the GNN-MLP baselines outperform other models ) seems to be scientifically quite relevant , and of particular importance to the GNN research community . Disseminating this result more widely should be a reason for acceptance for publication ."}, {"review_id": "HJe4Cp4KwH-1", "review_text": "The paper proposes a new Graph Neural Network (GNN) architecture that uses Feature-wise Linear Modulation (FiLM) to condition the source-to-target node message-passing based on the target node representation. In this way, GNN-FiLM aims to allow a GNN's message propagation to \"focus on feature that are especially relevant for the update of the target node.\" The authors clearly describe prior GNN architectures, showing that the do not incorporate such forms of message propagation. The authors then describe several intuitive ways of adding such a form of message propagation, before describing why those approaches do not work in practice. Finally, the authors introduce GNN-FiLM, which is computationally reasonable and works well in practice, as evaluated according to several GNN benchmarks. The GNN-FiLM model is also quite simple and elegant, which makes me think it is likely to work on more tasks than the authors experiment on. The paper is clear and easy to follow. The authors' description of other GNNs architectures is clear, and their own approach seems well motivated and clearly described in relation to previous GNNs. The authors have released the code for method, where they have also reimplemented several popular GNN methods. The open-source codebase also seems to be valuable contribution for future research and reproducibility in work on GNNs. The empirical evaluation seems thorough. GNN-FiLM works well on 3 graph tasks (PPI, QM9, VarMisuse) with different properties. To compare models, the authors conduct a search of hyperparameter ranges for each model. The authors even improve several of the baseline methods, generalizing some approaches to include different edge types and to add self-loops, as well as using better networks (adding dropout and increasing hidden dimensions). The paper even finds that one existing/obvious GNN architecture (GNN-MLP) is underrated. The paper reads like an honest analysis of existing methods, even though it also introduces its own, new method that works better. Questions: * Do you have any intuition about why the Eqn. 5 model is less stable than GNN-FiLM? * On GNN-FiLM's training stability and regularization: I am interested in the negative result described in the following sentence: \"Preliminary experiments on the citation network data showed results that were at best comparable to the baseline methods, but changes of a random seed led to substantial fluctuations (mirroring the problems with evaluation on these tasks reported by Shchur et al. (2018))\" Do the authors have any intuition about why the results are highly dependent on the random seed? The results on other tasks seems to not have much variance. It could be interesting to try various techniques to stabilize learning on that task. A simple approach like gradient clipping might work, or there are perhaps other techniques. For example, in \"TADAM: Task dependent adaptive metric for improved few-shot learning\", the authors find it important to regularize FiLM parameters \\gamma and \\beta towards 1 and 0, respectively, which may make learning more stable here. In general, previous work seems to find it important to regularize the parameters that predict FiLM parameters, which may also fix GNN-FiLM's overfitting on VarMisuse. Exploring such approaches could make it easier for future work to use FiLM with GNNs. * On potential related work: GNN-FiLM is a \"self-conditioned\" model which learns to apply feature-wise transformations based on the activations at the current layer. If I am not mistaken, the self-conditioning aspect of GNN-FiLM makes it related to the self-conditioned models described in \"Feature-wise Transformations\" ([Feature-wise transformations](https://distill.pub/2018/feature-wise-transformations/)) - for example, see the \"Image Recognition\" section (Squeeze-and-Excitation Networks and Highway Networks) and the \"Natural Language Processing\" section (LSTMs, gated linear units, and gated-attention reader). Do the authors see a connection with such models? If so, it would be interesting to hear these works discussed (in the rebuttal and paper) and the exact connection described.", "rating": "8: Accept", "reply_text": "Thank you for your kind review and your interesting questions ! > * Do you have any intuition about why the Eqn . 5 model is less stable > than GNN-FiLM ? While there are no clean experiments to back this up , the instability seems to have two sources : ( 1 ) The computation of $ \\mathbf { W } _ { \\ell , t , v , c } $ takes a state $ h_v $ of size $ d $ and blows it up into a $ d \\times d $ matrix . Obviously , the values ( or worse , outliers ) of a single dimension of $ h_v $ then affect a number of values in $ \\mathbf { W } _ { \\ell , t , v , c } $ , which are then applied to some $ h_u $ . This blow-up-then-collapse pattern exacerbates the effect of outlier data . Using squashing activation functions such as sigmoids to limit the range of values helped with this , but seems to have not entirely solved the problem . ( 2 ) Computing $ \\mathbf { W } _ { \\ell , t , v , c } $ by a linear layer $ W_f $ from $ h_v $ requires careful initialisation of $ W_f $ to ensure that $ \\mathbf { W } _ { \\ell , t , v , c } $ is appropriate ( e.g. , that the sum of each row is near 1 , to avoid strong growth / decay in values ) . This somewhat worked , but some unlucky initializations would still diverge . Doing a deeper analysis and then apply , for example , appropriate rescaling of weights after init ( maybe in the style of Fixup Initialization https : //arxiv.org/pdf/1901.09321.pdf ) could work , but was not explored as the FiLM variant worked well , and the memory and time cost of computing $ \\mathbf { W } _ { \\ell , t , v , c } $ is substantial . > * On GNN-FiLM 's training stability and regularization : I am interested > in the negative result described in the following sentence : > `` Preliminary experiments on the citation network data showed results > that were at best comparable to the baseline methods , but changes of a > random seed led to substantial fluctuations ( mirroring the problems > with evaluation on these tasks reported by Shchur et al . ( 2018 ) ) '' Do > the authors have any intuition about why the results are highly > dependent on the random seed ? The results on other tasks seems to not > have much variance . It could be interesting to try various techniques > to stabilize learning on that task . This seems to be a property of the datasets , see the error bars of the plots in appendix D of Shchur et al . ( 2018 ) ( https : //arxiv.org/pdf/1811.05868.pdf ) . > A simple approach like gradient clipping might work , or there are perhaps > other techniques . Gradient clipping is implemented for all evaluated models ( with a max grad norm of 1.0 , though different clipping values had little influence on results in cursory experiments ) . > For example , in `` TADAM : Task dependent adaptive metric for improved > few-shot learning '' , the authors find it important to regularize FiLM > parameters \\gamma and \\beta towards 1 and 0 , respectively , which may > make learning more stable here . In general , previous work seems to > find it important to regularize the parameters that predict FiLM > parameters , which may also fix GNN-FiLM 's overfitting on VarMisuse . > Exploring such approaches could make it easier for future work to use > FiLM with GNNs . Using a regularizer to explicitly push $ \\gamma $ to 1 and $ \\beta $ to 0 sounds like a reasonable idea , which has not been used in the experiments yet . If time allows , experiments with such a penalty term will be run during the rebuttal period . > * On potential related work : GNN-FiLM is a `` self-conditioned '' model > which learns to apply feature-wise transformations based on the > activations at the current layer . If I am not mistaken , the > self-conditioning aspect of GNN-FiLM makes it related to the > self-conditioned models described in `` Feature-wise Transformations '' > ( [ Feature-wise > transformations ] ( https : //distill.pub/2018/feature-wise-transformations/ ) ) > - for example , see the `` Image Recognition '' section > ( Squeeze-and-Excitation Networks and Highway Networks ) and the > `` Natural Language Processing '' section ( LSTMs , gated linear units , and > gated-attention reader ) . Do the authors see a connection with such > models ? If so , it would be interesting to hear these works discussed > ( in the rebuttal and paper ) and the exact connection described . Yes , GNN-FiLM clearly fits into this general ( fairly large ) class of self-conditioned works , though it is unclear what direct conclusions to draw from this . Self-conditioning is clearly a substantial trend at the moment ( primarily in the form of attention mechanism ) , often used as step towards multi-step reasoning ( in which results on layer $ \\ell $ are used to determine computation at layer $ \\ell+1 $ ) . While a general connection to feature-wise examples of this exists , it does not seem clear how to exploit this for deeper understanding or better results at this time ."}, {"review_id": "HJe4Cp4KwH-2", "review_text": "This paper performs a concise experimental survey of graph neural network method, and introduce GNN-FiLM as a new approach. They reproduce all the baseline and show that GNN-FiLM is in line with SOTA models. The paper is pretty easy to follow, and the intuition is clearly explained. The idea is pretty simple, and it is a natural extension of FiLM to the graph setting. The results are tested on 4 different datasets, and the experimental protocol is clearly explained, and the results are convincing.) However, I am missing a discussion of the proposed methods. For instance, what are the FiLM clusters? What are the key errors with GNN-FiLM, GNN-MLP0? Are the models complementary (e.g., by using a mixture of experts)? Can we combine GNN-MLP0 and GNN-FiLM? Remarks: - The bibliography work is incomplete. For instance, the authors do not cite the Hypernetwork paper [0], when stating that several GNN variant exists, we expect to have some references, idem when the authors mention that [FiLM] has been very effective in several domain (GAN [1], Meta-learning [2] Sound [3], language-vision task [4]). As a hint, it is pretty rare to have empirical papers with only 16 citations. I would say that 20-25 is the bare minimum. - Adding a visual sketch of the different models may provide additional intuition - I would encourage the author to define the dimension of W_l, h_u, etc. clearly. - On a personal note, I like informal writing and paper honesty, but I would not always recommend it for submission. - As the authors reproduce the experiments, It would have been useful to add the original results in the table whenever it is possible - The paper gives a feeling that it lacks some rigor (missing ref, detected bug, mathematical formalism is light); it makes me a bit skeptic regarding some experimental conclusion. It would have been helpful to look at the code (at least to see if it can be easily parsed and analyzed.) I think the paper is valuable for the community as it provides a simple survey of the current methods, the code is available, and the authors provide a lot of intuition. However, the overall level is slightly below the ICLR level for the following reason: incomplete bibliography, lack of complementary experiments, lack of discussion. The overall writing style is quite unusual, but It is not a negative point from my perspective. In its current state, it is a strong workshop submission but a not-good-enough paper for ICLR. However, I am open to discussion, especially if the authors have complementary discussions and results. PS: the reviewer is very familiar with the modulation and multi-modal literature, but he has only basic knowledge in graph networks. [0] Ha, David, Andrew Dai, and Quoc V. Le. ICLR 2017. [1] Brock, Andrew, Jeff Donahue, and Karen Simonyan. \"Large scale gan training for high fidelity natural image synthesis.\" ICLR 2019 [3] Jiang, X., Havaei, M., Varno, F., Chartrand, G., Chapados, N., & Matwin, S.. Learning to learn with conditional class dependencies. ICLR 2019 [3] Abdelnour, Jerome, Giampiero Salvi, and Jean Rouat. \"From Visual to Acoustic Question Answering.\" arXiv preprint arXiv:1902.11280 (2019). [4] Strub, F., Seurin, M., Perez, E., De Vries, H., Mary, J., Preux, P., & CourvilleOlivier Pietquin, A.. Visual reasoning with multi-hop feature modulation. ECCV 2018", "rating": "3: Weak Reject", "reply_text": "First , thank you for your careful review of this submission . Note that this reply is split into two parts for character limit reasons . > This paper performs a concise experimental survey of graph neural > network method , and introduce GNN-FiLM as a new approach . They > reproduce all the baseline and show that GNN-FiLM is in line with SOTA > models . Please note that the generalisations to the relational setting for the GAT ( and now GIN ) baselines are also novel contributions of this paper . > The paper is pretty easy to follow , and the intuition is clearly > explained . The idea is pretty simple , and it is a natural extension of > FiLM to the graph setting . The results are tested on 4 different > datasets , and the experimental protocol is clearly explained , and the > results are convincing . ) Note : The models are only properly evaluated on three datasets ( the Citation Network datasets are not properly considered in the paper ) . > However , I am missing a discussion of the proposed methods . For > instance , what are the FiLM clusters ? What are the key errors with > GNN-FiLM , GNN-MLP0 ? Are the models complementary ( e.g. , by using a > mixture of experts ) ? Can we combine GNN-MLP0 and GNN-FiLM ? The requested error analysis of the different models is desirable , but it is not clear how to achieve it . The datasets themselves are not annotated with additional meaningful labels that could be used for deeper analysis ( e.g. , `` VarMisuse problems of type A show worse performances '' ) , and hence only a qualitative analysis of the graphs would be possible . However , these graphs have thousands of nodes , so its unclear how to present such an analysis in a paper . What _can_ be done is to quantitively study if the models are complimentary in a mixture-of-experts setting , and we will try to provide numbers for this ( for VarMisuse , where the semantics of a mixture of experts model are most clearly defined and room for improvements exists ) . Finally , it would be nice if you could clarify what you mean by `` FiLM clusters . '' Some papers reporting on the use of FiLM do display PCA visualisations of embeddings of either the actual \\alpha/\\beta values or the embeddings before/after application of the FiLM layer . If you are asking for such visualisations here , it is unclear what they would look like , as the FiLM-conditioning is based on the embeddings of other graph nodes . Again , as graphs ( and nodes ) are not provided with a small set of interpretable labels , it 's unclear what information cluster visualisations of the embeddings would provide ."}], "0": {"review_id": "HJe4Cp4KwH-0", "review_text": "This paper introduces a new type of Graph Neural Network (GNN) that incorporates Feature-wise Linear Modulation (FiLM) layers. Current GNNs update the target representations by aggregating information from neighbouring nodes without taking into the account the target node representation. As graph networks might benefit from such target-source interactions, the current work proposes to use FiLM layers to let the target node modulate the source node representations. The authors thoroughly evaluate this new architecture \u2014 called GNN-FiLM\u2014-on several graph benchmarks, including Citeseer, PPI, QM9, and VarMisuse. The proposed network outperforms the other methods on QM9 and is on par on the other benchmarks. Strengths - The literature review of existing work on GNN was a pleasure to read and provided a good motivation for the proposed GNN-FiLM architecture. - The authors put significant effort into reproducing other GNN baselines. Perhaps the most surprising result of this work is that all GNNs perform remarkably similar (contrary to what previous work has reported) Weaknesses - There seems to be a much tighter relationship between GNN-FiLM and Gated GNNs than currently discussed. If you actually write down the equations of the recurrent cell $r$ in Eq 1), you\u2019ll notice that there are feature-wise interactions between the target node representations and the (sum of) source node representations. The paper should discuss in more depth what the exact differences are. Some visualizations would also help here. - Related to the previous point, I\u2019d like to see a bit more discussion on *why* tight interactions between target and source nodes are helpful. For example, one could perhaps provide a toy example for which that\u2019s obviously the case. All in all, I believe the ideas and results of this paper are promising but insufficient for publication in its current form. The paper tries to communicate two messages: 1) a model paper arguing for GNN-FiLM, 2) an unbiased evaluation of existing GNN models, showing that their performance is surprisingly similar on a number of benchmarks (with equal hyperparameter search). Both points are interesting but are not worked out sufficiently to pass the bar. For 1), I\u2019d like to see an in-depth discussion on the benefits of target-source interactions, providing more insights into why this might be beneficial. Your current experiments report very minimal gains for your proposed network, questioning why such interactions might be necessary in the first place. For 2), I\u2019d suggest to rewrite the paper from a slightly different angle and add more graph benchmarks if available (disclaimer: I\u2019m not in the graph network community, so I can\u2019t fully evaluate how significant these results are) Typos \u2014\u2014- Last paragraph of intro: \u201ctwo two\u201d - > two *EDIT After reading the rebuttal and revised paper, I've updated my score to \"Weak Accept\". The toy example and connection to GGNN are important additions which makes the paper more complete, though I believe there's room for further improvement here (see comments below). All in all, I weakly recommend to accept the paper. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your kind ( emergency ? ) review and engaging with this submission . > - There seems to be a much tighter relationship between GNN-FiLM and > Gated GNNs than currently discussed . If you actually write down the > equations of the recurrent cell in Eq 1 ) , you \u2019 ll notice that there > are feature-wise interactions between the target node representations > and the ( sum of ) source node representations . This is indeed true ( for GRU/LSTM cells ) and should maybe discussed in the paper in more depth , though it is a bit unclear where to best do this to not interrupt the flow of presentation . The differences arise from the application of the gated cell after summation of incoming messages . Concretely , the `` forgetting '' of memories in a GRU/LSTM is similar to modulation of messages from the self-loop edges , and the gating of the cell input is similar to the modulation of incoming messages . However , as GNN-FiLM uses _different_ $ \\gamma_\\ell $ / $ \\beta_\\ell $ values for different edge types $ \\ell $ , the modulation is additionally dependent on the kind of relationship between nodes . In the case of a single edge type ( + a fresh self-loop edge type ) , GGNNs are indeed mathematically very similar to GNN-FiLM . > - Related to the previous point , I \u2019 d like to see a bit more discussion > on * why * tight interactions between target and source nodes are > helpful . For example , one could perhaps provide a toy example for > which that \u2019 s obviously the case . A simple toy example may be the following : Assume nodes are of type VA and type VB , and there are two edge types E1 and E2 . Our task is to count the number of E1-neighbours of VA nodes , and the number of E2-neighbours of VB nodes . In FiLM , we can `` filter '' edges by using $ \\gamma_ { E1 } = 1 , \\gamma_ { E2 } = 0 $ for VA nodes and $ \\gamma_ { E1 } = 0 , \\gamma_ { E2 } =1 $ for VB nodes . Hence , GNN-FiLM can solve this task using a single layer . Other models obviously can solve this task as well with more layers and feature dimensions by counting VA-E1 , VA-E2 , VB-E1 , VB-E1 neighbours separately in a first message passing step , and then projecting to the desired dimension in a second layer . More generally , as the modulation of messages depends on the edge target representation and the edge type , GNN-FiLM can learn to ( softly ) select a subset of the graph edges for message passing , conditioned on the current representation of nodes , _per feature dimension_ ( this is a substantial difference to the newly-introduced R-GAT setting ; the original GAT model has no notion of edge types ) . For example , in the Program Graph setting of the VarMisuse task , it may choose to emphasise `` NextVariableUse '' edges for some dimensions to gain information about how a variable is used later . You 're right that this intuition is not provided clearly in the paper at the moment , and will be added in the next revision ( but maybe only after feedback if these explanations make sense to a reader ) . > Your current experiments report very minimal gains for your proposed > network , questioning why such interactions might be necessary in the > first place . Note that the experimental gains over _standard_ baselines are quite substantial : * For Tab . 1 , the reported state of the art is a Micro-F1 of 0.973 +/- 0.002 * For Tab . 2 , there is no clear state of the art as many papers use slightly different experiment design for this task ( e.g. , provide more features , ... ) ; but the standard models are GGNN/R-GCN . * For Tab . 3 , the reported state of the art is 84.0 % ( on SeenProjTest ) and 74.1 % ( on UnseenProjTest ) accuracy . While more graph tasks are studied , these 3 provide a good coverage of graph tasks ( from small to large graph , classification and regression , node-level and graph-level ) . The trends in these results are very likely to hold for the majority of tasks . Early feedback ( and curiosity ) led to more and more baselines in the experiments , and existing baselines were improved in a number of ways ( e.g. , the generalisations to the multi-relational setting , which are crucial for the performance in the reported experiments ) . Your review fits exactly to fear around submitting this paper : By doing a very thorough job on the experiments , the importance of the new method looks smaller ; doing a worse job on the experiments ( not including GNN-MLP * ; not extending GAT/GIN to R-GAT/R-GIN ) would have made it look better . These nuances can not be obvious to someone who 's not active in the GNN research community ( and so this rant is not meant as a `` you did a bad job as a reviewer '' ) , but this explanation may help to understand the context . However , the result that the differences between well-tuned , equally-well implemented models are small ( and that the GNN-MLP baselines outperform other models ) seems to be scientifically quite relevant , and of particular importance to the GNN research community . Disseminating this result more widely should be a reason for acceptance for publication ."}, "1": {"review_id": "HJe4Cp4KwH-1", "review_text": "The paper proposes a new Graph Neural Network (GNN) architecture that uses Feature-wise Linear Modulation (FiLM) to condition the source-to-target node message-passing based on the target node representation. In this way, GNN-FiLM aims to allow a GNN's message propagation to \"focus on feature that are especially relevant for the update of the target node.\" The authors clearly describe prior GNN architectures, showing that the do not incorporate such forms of message propagation. The authors then describe several intuitive ways of adding such a form of message propagation, before describing why those approaches do not work in practice. Finally, the authors introduce GNN-FiLM, which is computationally reasonable and works well in practice, as evaluated according to several GNN benchmarks. The GNN-FiLM model is also quite simple and elegant, which makes me think it is likely to work on more tasks than the authors experiment on. The paper is clear and easy to follow. The authors' description of other GNNs architectures is clear, and their own approach seems well motivated and clearly described in relation to previous GNNs. The authors have released the code for method, where they have also reimplemented several popular GNN methods. The open-source codebase also seems to be valuable contribution for future research and reproducibility in work on GNNs. The empirical evaluation seems thorough. GNN-FiLM works well on 3 graph tasks (PPI, QM9, VarMisuse) with different properties. To compare models, the authors conduct a search of hyperparameter ranges for each model. The authors even improve several of the baseline methods, generalizing some approaches to include different edge types and to add self-loops, as well as using better networks (adding dropout and increasing hidden dimensions). The paper even finds that one existing/obvious GNN architecture (GNN-MLP) is underrated. The paper reads like an honest analysis of existing methods, even though it also introduces its own, new method that works better. Questions: * Do you have any intuition about why the Eqn. 5 model is less stable than GNN-FiLM? * On GNN-FiLM's training stability and regularization: I am interested in the negative result described in the following sentence: \"Preliminary experiments on the citation network data showed results that were at best comparable to the baseline methods, but changes of a random seed led to substantial fluctuations (mirroring the problems with evaluation on these tasks reported by Shchur et al. (2018))\" Do the authors have any intuition about why the results are highly dependent on the random seed? The results on other tasks seems to not have much variance. It could be interesting to try various techniques to stabilize learning on that task. A simple approach like gradient clipping might work, or there are perhaps other techniques. For example, in \"TADAM: Task dependent adaptive metric for improved few-shot learning\", the authors find it important to regularize FiLM parameters \\gamma and \\beta towards 1 and 0, respectively, which may make learning more stable here. In general, previous work seems to find it important to regularize the parameters that predict FiLM parameters, which may also fix GNN-FiLM's overfitting on VarMisuse. Exploring such approaches could make it easier for future work to use FiLM with GNNs. * On potential related work: GNN-FiLM is a \"self-conditioned\" model which learns to apply feature-wise transformations based on the activations at the current layer. If I am not mistaken, the self-conditioning aspect of GNN-FiLM makes it related to the self-conditioned models described in \"Feature-wise Transformations\" ([Feature-wise transformations](https://distill.pub/2018/feature-wise-transformations/)) - for example, see the \"Image Recognition\" section (Squeeze-and-Excitation Networks and Highway Networks) and the \"Natural Language Processing\" section (LSTMs, gated linear units, and gated-attention reader). Do the authors see a connection with such models? If so, it would be interesting to hear these works discussed (in the rebuttal and paper) and the exact connection described.", "rating": "8: Accept", "reply_text": "Thank you for your kind review and your interesting questions ! > * Do you have any intuition about why the Eqn . 5 model is less stable > than GNN-FiLM ? While there are no clean experiments to back this up , the instability seems to have two sources : ( 1 ) The computation of $ \\mathbf { W } _ { \\ell , t , v , c } $ takes a state $ h_v $ of size $ d $ and blows it up into a $ d \\times d $ matrix . Obviously , the values ( or worse , outliers ) of a single dimension of $ h_v $ then affect a number of values in $ \\mathbf { W } _ { \\ell , t , v , c } $ , which are then applied to some $ h_u $ . This blow-up-then-collapse pattern exacerbates the effect of outlier data . Using squashing activation functions such as sigmoids to limit the range of values helped with this , but seems to have not entirely solved the problem . ( 2 ) Computing $ \\mathbf { W } _ { \\ell , t , v , c } $ by a linear layer $ W_f $ from $ h_v $ requires careful initialisation of $ W_f $ to ensure that $ \\mathbf { W } _ { \\ell , t , v , c } $ is appropriate ( e.g. , that the sum of each row is near 1 , to avoid strong growth / decay in values ) . This somewhat worked , but some unlucky initializations would still diverge . Doing a deeper analysis and then apply , for example , appropriate rescaling of weights after init ( maybe in the style of Fixup Initialization https : //arxiv.org/pdf/1901.09321.pdf ) could work , but was not explored as the FiLM variant worked well , and the memory and time cost of computing $ \\mathbf { W } _ { \\ell , t , v , c } $ is substantial . > * On GNN-FiLM 's training stability and regularization : I am interested > in the negative result described in the following sentence : > `` Preliminary experiments on the citation network data showed results > that were at best comparable to the baseline methods , but changes of a > random seed led to substantial fluctuations ( mirroring the problems > with evaluation on these tasks reported by Shchur et al . ( 2018 ) ) '' Do > the authors have any intuition about why the results are highly > dependent on the random seed ? The results on other tasks seems to not > have much variance . It could be interesting to try various techniques > to stabilize learning on that task . This seems to be a property of the datasets , see the error bars of the plots in appendix D of Shchur et al . ( 2018 ) ( https : //arxiv.org/pdf/1811.05868.pdf ) . > A simple approach like gradient clipping might work , or there are perhaps > other techniques . Gradient clipping is implemented for all evaluated models ( with a max grad norm of 1.0 , though different clipping values had little influence on results in cursory experiments ) . > For example , in `` TADAM : Task dependent adaptive metric for improved > few-shot learning '' , the authors find it important to regularize FiLM > parameters \\gamma and \\beta towards 1 and 0 , respectively , which may > make learning more stable here . In general , previous work seems to > find it important to regularize the parameters that predict FiLM > parameters , which may also fix GNN-FiLM 's overfitting on VarMisuse . > Exploring such approaches could make it easier for future work to use > FiLM with GNNs . Using a regularizer to explicitly push $ \\gamma $ to 1 and $ \\beta $ to 0 sounds like a reasonable idea , which has not been used in the experiments yet . If time allows , experiments with such a penalty term will be run during the rebuttal period . > * On potential related work : GNN-FiLM is a `` self-conditioned '' model > which learns to apply feature-wise transformations based on the > activations at the current layer . If I am not mistaken , the > self-conditioning aspect of GNN-FiLM makes it related to the > self-conditioned models described in `` Feature-wise Transformations '' > ( [ Feature-wise > transformations ] ( https : //distill.pub/2018/feature-wise-transformations/ ) ) > - for example , see the `` Image Recognition '' section > ( Squeeze-and-Excitation Networks and Highway Networks ) and the > `` Natural Language Processing '' section ( LSTMs , gated linear units , and > gated-attention reader ) . Do the authors see a connection with such > models ? If so , it would be interesting to hear these works discussed > ( in the rebuttal and paper ) and the exact connection described . Yes , GNN-FiLM clearly fits into this general ( fairly large ) class of self-conditioned works , though it is unclear what direct conclusions to draw from this . Self-conditioning is clearly a substantial trend at the moment ( primarily in the form of attention mechanism ) , often used as step towards multi-step reasoning ( in which results on layer $ \\ell $ are used to determine computation at layer $ \\ell+1 $ ) . While a general connection to feature-wise examples of this exists , it does not seem clear how to exploit this for deeper understanding or better results at this time ."}, "2": {"review_id": "HJe4Cp4KwH-2", "review_text": "This paper performs a concise experimental survey of graph neural network method, and introduce GNN-FiLM as a new approach. They reproduce all the baseline and show that GNN-FiLM is in line with SOTA models. The paper is pretty easy to follow, and the intuition is clearly explained. The idea is pretty simple, and it is a natural extension of FiLM to the graph setting. The results are tested on 4 different datasets, and the experimental protocol is clearly explained, and the results are convincing.) However, I am missing a discussion of the proposed methods. For instance, what are the FiLM clusters? What are the key errors with GNN-FiLM, GNN-MLP0? Are the models complementary (e.g., by using a mixture of experts)? Can we combine GNN-MLP0 and GNN-FiLM? Remarks: - The bibliography work is incomplete. For instance, the authors do not cite the Hypernetwork paper [0], when stating that several GNN variant exists, we expect to have some references, idem when the authors mention that [FiLM] has been very effective in several domain (GAN [1], Meta-learning [2] Sound [3], language-vision task [4]). As a hint, it is pretty rare to have empirical papers with only 16 citations. I would say that 20-25 is the bare minimum. - Adding a visual sketch of the different models may provide additional intuition - I would encourage the author to define the dimension of W_l, h_u, etc. clearly. - On a personal note, I like informal writing and paper honesty, but I would not always recommend it for submission. - As the authors reproduce the experiments, It would have been useful to add the original results in the table whenever it is possible - The paper gives a feeling that it lacks some rigor (missing ref, detected bug, mathematical formalism is light); it makes me a bit skeptic regarding some experimental conclusion. It would have been helpful to look at the code (at least to see if it can be easily parsed and analyzed.) I think the paper is valuable for the community as it provides a simple survey of the current methods, the code is available, and the authors provide a lot of intuition. However, the overall level is slightly below the ICLR level for the following reason: incomplete bibliography, lack of complementary experiments, lack of discussion. The overall writing style is quite unusual, but It is not a negative point from my perspective. In its current state, it is a strong workshop submission but a not-good-enough paper for ICLR. However, I am open to discussion, especially if the authors have complementary discussions and results. PS: the reviewer is very familiar with the modulation and multi-modal literature, but he has only basic knowledge in graph networks. [0] Ha, David, Andrew Dai, and Quoc V. Le. ICLR 2017. [1] Brock, Andrew, Jeff Donahue, and Karen Simonyan. \"Large scale gan training for high fidelity natural image synthesis.\" ICLR 2019 [3] Jiang, X., Havaei, M., Varno, F., Chartrand, G., Chapados, N., & Matwin, S.. Learning to learn with conditional class dependencies. ICLR 2019 [3] Abdelnour, Jerome, Giampiero Salvi, and Jean Rouat. \"From Visual to Acoustic Question Answering.\" arXiv preprint arXiv:1902.11280 (2019). [4] Strub, F., Seurin, M., Perez, E., De Vries, H., Mary, J., Preux, P., & CourvilleOlivier Pietquin, A.. Visual reasoning with multi-hop feature modulation. ECCV 2018", "rating": "3: Weak Reject", "reply_text": "First , thank you for your careful review of this submission . Note that this reply is split into two parts for character limit reasons . > This paper performs a concise experimental survey of graph neural > network method , and introduce GNN-FiLM as a new approach . They > reproduce all the baseline and show that GNN-FiLM is in line with SOTA > models . Please note that the generalisations to the relational setting for the GAT ( and now GIN ) baselines are also novel contributions of this paper . > The paper is pretty easy to follow , and the intuition is clearly > explained . The idea is pretty simple , and it is a natural extension of > FiLM to the graph setting . The results are tested on 4 different > datasets , and the experimental protocol is clearly explained , and the > results are convincing . ) Note : The models are only properly evaluated on three datasets ( the Citation Network datasets are not properly considered in the paper ) . > However , I am missing a discussion of the proposed methods . For > instance , what are the FiLM clusters ? What are the key errors with > GNN-FiLM , GNN-MLP0 ? Are the models complementary ( e.g. , by using a > mixture of experts ) ? Can we combine GNN-MLP0 and GNN-FiLM ? The requested error analysis of the different models is desirable , but it is not clear how to achieve it . The datasets themselves are not annotated with additional meaningful labels that could be used for deeper analysis ( e.g. , `` VarMisuse problems of type A show worse performances '' ) , and hence only a qualitative analysis of the graphs would be possible . However , these graphs have thousands of nodes , so its unclear how to present such an analysis in a paper . What _can_ be done is to quantitively study if the models are complimentary in a mixture-of-experts setting , and we will try to provide numbers for this ( for VarMisuse , where the semantics of a mixture of experts model are most clearly defined and room for improvements exists ) . Finally , it would be nice if you could clarify what you mean by `` FiLM clusters . '' Some papers reporting on the use of FiLM do display PCA visualisations of embeddings of either the actual \\alpha/\\beta values or the embeddings before/after application of the FiLM layer . If you are asking for such visualisations here , it is unclear what they would look like , as the FiLM-conditioning is based on the embeddings of other graph nodes . Again , as graphs ( and nodes ) are not provided with a small set of interpretable labels , it 's unclear what information cluster visualisations of the embeddings would provide ."}}