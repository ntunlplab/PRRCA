{"year": "2020", "forum": "B1x6BTEKwr", "title": "Piecewise linear activations substantially shape the loss surfaces of neural networks", "decision": "Accept (Poster)", "meta_review": "Quoting R3: \"This paper studies the theoretical property of neural network's loss surface. The main contribution is to prove that the loss surface of every neural network (with arbitrary depth) with piecewise linear activations has infinite spurious local minima.\"\n\nThere were split reviews, with two reviewers recommending acceptance and one recommending rejection.  During a robust rebuttal and discussion phase, both R2 and R3's appreciation for the work was strengthened.  The authors also provided a robust response to R1, whose main concerns included (i) that the paper's analysis is limited to piecewise linear activation functions, (ii) technical questions about the difficulty of proving theorem 2, which appear to have been answered in the discussion, and (iii) concerns about the strength of the language employed.\n\nOn the balance, the reviewers were positively impressed with the relevance of the theoretical study and its contributions.  Genuine shortcomings and misunderstandings were systematically resolved during the rebuttal process.", "reviews": [{"review_id": "B1x6BTEKwr-0", "review_text": "This paper focus on how activation functions\u2019 nonlinearities shape the loss surface of neural networks. The authors first show why the loss surface of every neural network has infinite spurious local minima. Secondly, the authors prove one theorem to show four properties of the loss surfaces of nonlinear neural networks. Although this paper is generally easy to follow, and the motivation about nonlinearities and the loss surface is clear, the insight of this paper is somehow shortcoming. Though this work can prove such properties within different preconditions, whether other works\u2019 conditions are inconvenient or not may remain further discussions. This work is established based on several preconditions, while it is hard to assert that most kinds of neural networks can satisfy them perfectly. For instance, this work mentions \u201cDeep learning without poor local minima (NeurIPS2016)\u201d, which requires full-rank and conditional independence of each node. It could be feasible when training a stacked network with particular limitations. This work requires all hidden layers are wider than the output layer, which may not be suitable for image segmentation, generative tasks or super-resolution, etc. Besides, it is laudable to prove fundamental rules in neural networks, while showing or inspiring researchers about how to implement or approximate such results to improve neural networks might be more helpful. Some questions: 1. The authors assert that \u201cthe loss surface of *every* neural network has infinite spurious local minima\u201d in the abstract, while in chapter 3 line 2, authors mention, \u201cWe find that *almost all* practical neural networks have infinitely many spurious local minima.\u201d Which one is correct? Based on the following description in this paper, I guess this result is conditionally tenable. 2. In lemma 3, authors construct the local minima by adding very negative biases and show they are spurious. However, it is less likely to learn such negative biases in the real case. Besides, some networks require biases equal to zero to achieve some specific target. My question is: if biases are conditioned on real-world data distribution, will lemma 3 and 4 still work in this case? 3. This paper mentions \u201cinfinite\u201d many times. Based on the reference, I believe that the \u201cneural network\u201d in this work refers to the \u201cartificial neural network,\u201d which is majorly stored within float tensors. So the number of combinations of parameters is finite. So why use \u201cinfinite\u201d instead of \u201cmany\u201d? Finite means I can train a small scale of networks with fewer precisions and check the global minima with a fixed dataset. All in all, I believe this paper can be significantly improved if more details and experiments are provided.", "rating": "6: Weak Accept", "reply_text": "We appreciate your thorough review and constructive comments . All your concerns have been duly addressed below . We have also updated the final version accordingly . We sincerely hope that you can take into account the response we have made and reevaluate the merit of this paper . Q1 : The authors assert that \u201c the loss surface of * every * neural network has infinite spurious local minima \u201d in the abstract , while in chapter 3 line 2 , authors mention , \u201c We find that * almost all * practical neural networks have infinitely many spurious local minima. \u201d Which one is correct ? Based on the following description in this paper , I guess this result is conditionally tenable . A1 : Thanks and revised accordingly . In the final version , we have added the following description . We first prove that the loss surfaces of many neural networks have infinite spurious local minima , which are defined as the local minima with higher empirical risks than the global minima . Our result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions ( excluding linear functions ) under many popular loss functions in practice with some mild assumptions . Q2 : In lemma 3 , authors construct the local minima by adding very negative biases and show they are spurious . However , it is less likely to learn such negative biases in the real case . Besides , some networks require biases equal to zero to achieve some specific target . My question is : if biases are conditioned on real-world data distribution , will lemma 3 and 4 still work in this case ? A2 : We respectfully argue that the construction of negative bias does not undermine the generality of the obtained results . Under a strong restriction that all activations are linear functions , Kawaguchi ( 2016 ) , Zhou & Liang ( 2018 ) , and Lu & Kawaguchi ( 2017 ) showed that all local minima are global minima , which accounts for the success of deep learning . However , it has been well observed and acknowledged that SGD can converge to points with large training errors , which are apparently not globally optimal . This phenomenon motivates us to study the existence of spurious local minima by relaxing this strong restriction . Theorem 1 of this paper ( based on Lemmas 3 and 4 ) exactly constructs spurious local minima on the loss surface of a nonlinear neural network ( with an arbitrary depth , a differentiable loss and an arbitrary-dimensional output ) . This counterexample proves that the existing theoretical results can not be applied to nonlinear networks . Constructing counterexamples is a widely used approach to prove a proposition is wrong . Therefore , our construction does not undermine the generality . Q3 : This paper mentions \u201c infinite \u201d many times . Based on the reference , I believe that the \u201c neural network \u201d in this work refers to the \u201c artificial neural network , \u201d which is majorly stored within float tensors . So the number of combinations of parameters is finite . So why use \u201c infinite \u201d instead of \u201c many \u201d ? Finite means I can train a small scale of networks with fewer precisions and check the global minima with a fixed dataset . A3 : We respectfully argue that it is common yet mild to treat the parameters of neural networks as continuous numbers for theoretical studies , which has been widely used in related studies . Moreover , the constructed local minima are connected with each other by a continuous path , on which every point has the same empirical risk . Therefore , it is impractical to check all the constructed local minima even when they are represented by float tensors , because the number of float tensors on a continuous path is extremely large . For example , there are $ 2^ { 52 } = 4.5 \\times 10^ { 15 } $ $ 64 $ -bit float values between $ 1 $ and $ 2 $ when using the double precision . Reference Kenji Kawaguchi . Deep learning without poor local minima . In Advances in Neural Information Processing Systems , 2016 . Haihao Lu and Kenji Kawaguchi . Depth creates no bad local minima . arXiv preprint arXiv:1702.08580 , 2017 . Yi Zhou and Yingbin Liang . Critical points of neural networks : Analytical forms and landscape properties . In International Conference on Learning Representations , 2018 ."}, {"review_id": "B1x6BTEKwr-1", "review_text": "This paper studies the theoretical property of neural network's loss surface. The main contribution is to prove that the loss surface of every neural network (with arbitrary depth) with piecewise linear activations has infinite spurious local minima. Moreover, the paper further characterizes the partition of the local minima. More precisely, the loss surface is partitioned into multiple smooth and multilinear open cells and within each cell, the local minima are equally good. This result can also explain the linear neural network case where there is only one cell, implying that all local minima are global. On one hand, I find the paper very clear and the result very clean, which unites a lot of existing results. On the other hand, with a reasonable initialization in practice, we will not attain the local minima constructed in the paper since it requires all the activations to be positive. This limits the plausible implication from this theoretical study. Overall, I am very positive of the paper, the following are some detailed comments. a. Please be more precise in the abstract that the activation function need to be piecewise linear. The current sentence \"the loss surface of every neural network has infinite spurious local minima\" does not include this specification. Moreover, if the activation is differentiable, is the claim still hold? It seems to me from the middle of page 3 that Li et al 2018 shows a non-local minima result in this case. b. How different is the analysis comparing to existing result? I have only go through the skeleton of the proof and have not read into the details. It seems to me the construction of the local minima is very similar to [1], since the main idea is to consider the linear region by activating all the neurons. Could you summarize the main difficulty to extend their results to multi-layer cases? (Maybe it would be good to illustrate with a simple case like 3 layers few neurons per layer) Moreover, when considering the local convexity, is it sufficient to say that locally in each cell it is a linear network and then the results on linear network transfers to it locally? [1] Small nonlinearities in activation functions create bad local minima in neural networks, Yun et al, 2019", "rating": "6: Weak Accept", "reply_text": "We appreciate your thorough review and constructive comments . Thank you very much for your kind support . All your concerns have been duly addressed below . We have also updated the final version accordingly . Q1 : Please be more precise in the abstract that the activation function needs to be piecewise linear . Moreover , if the activation is differentiable , is the claim still hold ? A1 : Thanks and revised accordingly . We have stated in the abstract that we proved the cases of piecewise linear activation functions . In addition , the results have not been extended to differentiable activations . Q2 : How different is the analysis comparing to existing result with [ 1 ] ? Could you summarize the main difficulty to extend their results to multi-layer cases ? A2 : Thanks and revised accordingly . A detailed comparison of the analysis has been added to the final version . We summarise it as follows . We first acknowledge that [ 1 ] and our paper both employ the following strategy : ( a ) construct a series of local minima based on a linear classifier ; and ( b ) construct a new point with smaller empirical risk and by this way we prove that the constructed local minima are spurious . However , due to the differences in the loss function and the output dimensions , the exact constructions of local minima are substantially different . Meanwhile , our Stages ( 2 ) and ( 3 ) employ the transformation operation to force the data flow to go through the same series of the linear parts of the activations . The operations are carefully designed and the whole construction is novel and non-trivial . Besides , we also made extensions on the loss function and the output dimension . The difficulties are justified below : 1 . From squared loss to arbitrary differentiable loss : Yun et al . ( 2019b ) calculate the analytic formations of derivatives of the loss to construct the local minima and then prove they are spurious . This technique can not be transplanted to the case of arbitrary differentiable loss functions , because we can not assume the analytic formation . To prove that the loss surface under an arbitrary differentiable loss has an infinite number of spurious local minima , we employ a new proof technique based on Taylor series and a new separation lemma ( see Appendix A.5 , Lemma 6 , p. 31 ) to avoid the use of the analytic formulations ( see a detailed proof in Appendix A.2 , Step ( b ) , pp.14-15 ) . 2.From one-dimensional output to arbitrary-dimensional output : To prove the loss surface of a neural network with an arbitrary-dimensional output has an infinite number of spurious local minima , we need to deal with the calculus of functions whose domain and codomain are a matrix space and a vector space , respectively . By contrast , when the output dimension is one , the codomain is only the space of real numbers . Therefore , the extension of output dimension significantly mounts the difficulty of the whole proof ."}, {"review_id": "B1x6BTEKwr-2", "review_text": "Summary: This paper studies the landscape of deep neural networks with piecewise-linear activation functions. The paper showed that under very mild assumptions, the loss surface admits infinite spurious local minima. Further, it is shown that the loss surface is partitioned into many multilinear cells. If the network is two-layer with two-piece linear activations, it is proved that within each cell every local minimum is global. Pros: --Constructed spurious local minima for piecewise linear activations, for a broader setting than previous papers. --The paper is well written, with detailed explanation of proof skeleton. Cons: The significance of the results are not clear. Details are given below. 1. This paper only considers piecewise linear activation, which is a special type of non-linear activation. The major examples are ReLU and leaky ReLU. However, related results for ReLU have been studied for a few previous works mentioned in the last two paragraphs of Sec. 3.2. In particular, Yun et al. 2019b already proves a similar result for 1-hidden-layer neural-net with ReLU activation. I think extending the construction to broader settings (any depth, any piecewise linear and more losses) is mathematically nice, but the motivation of this extension is somewhat unclear to me. One motivation is that this is helpful for the purpose of understanding a \u201cbig picture\u201d of the landscape, which I will discuss next. 2. The second major result is Theorem 2, on the \u201cbig picture\u201d with ReLU-like activations. However, Theorem 2 is somewhat trivial to prove, and the link to Theorem 1 is rather weak. (a) The main message of Thm 2 is the partition of the surface into multiple pieces, and each piece has good property. This partition is somewhat straightforward, and has been studied before, in, e.g., [R1]. For a global \u201cbig picture\u201d, partitioning itself is not very interesting. Theorem 2 mainly describes the property of each region separately for 2-layer network, which is weaker than [R1]. (b) Theorem 2 seems easy to prove. The 1st, 3rd and 4th property are all straightforward. The 2nd property \u201clocal analogous convexity\u201d was given a 2-page proof in the paper. However, I don\u2019t understand why not use the following simple argument: for each region, the network behaves like a deep linear network, thus directly applying existing result shall imply \u201cevery local minimum in the region is the global minimum of the region\u201d, right? If not, what is the difficulty? (c) The 3rd property says \u201csome local minima are concentrated as a valley in some cell\u201d. What are the formal definitions of \u201cconcentrated\u201d and \u201cvalley\u201d in this sentence? (d) The link to Theorem 1 is weak: the link is the 3rd property of Theorem 2 that \u201csome local minima are in a valley\u201d. It is just about some special local minima and weakly related to the other properties on the \u201cglobal view\u201d. In addition, the fact that \u201csome of them are in a valley\u201d may be due to the very special construction, thus it is not surprising and does not reveal anything interesting about the \u201cbig picture\u201d. 3. Other issues: a) While ReLU-type activations are popular, there are still commonly-used activation functions are not piece-wise linear, e.g., tanh, swish. It is not proper to claim that \"this paper presents how nonlinearities in activations substantially shape the loss surface\" and \"almost every practical neural network ....\". I suggest replacing \"nonlinearity\" with \"piecewise linearity\" in both the title and the abstract, and modifying the over-statements. b) In Property 1 of Theorem 2, \u201csmooth and multilinear partition\u201d might be a bit misleading. The loss surface should be fractional in general, where multilinear cells are separated by non-smooth boundaries. \u201cSmooth partition\u201d seems to imply that the boundaries are smooth or the partition method is smooth in some sense. c) The name \u201canalogous convexity\u201d is not appropriate. Analogous convexity is not formally defined in the paper. According to Sec. 4.3 third paragraph, \u201cthe property of analogous convexity that the local minima wherein are equally good\u201d. It seems that \u201canalogous convexity\u201d is just \u201call local minima are good\u201d, which is very different from convexity. It is a weaker property than quasi-convexity, star-convexity, etc, and thus it is better not to call it \u201canalogous convexity\u201d. d) Property 3 of Theorem 2 is very far from \u201cmode connectivity\u201d. The proof of Property 3 relies on a special construction of Theorem 1, and the latter is for two arbitrary global minima. [R1] Soudry and Hoffer. \"Exponentially vanishing sub-optimal local minima in multilayer neural networks.\" arXiv preprint arXiv:1702.05777 (2017). Conclusion: I think this paper is studying an important and interesting question, and the efforts of constructing local minima and understanding big picture are both interesting to me. However, I\u2019m afraid the current form of the paper does not meet the standard of the conference. That being said, it would be a nice paper if the big picture can be explored deeper, and the link to the spurious local minima can be built stronger. ", "rating": "3: Weak Reject", "reply_text": "We appreciate your thorough review and constructive comments . All your concerns have been duly addressed below . We have also updated the final version accordingly . We sincerely hope that you can take into account the response we have made and reevaluate the merit of this paper . Q1 : Related results have been studied in a few previous works . In particular , Yun et al . ( 2019b ) prove a similar result for 1-hidden-layer neural-net with ReLU activation . The extension is mathematically nice , but the motivation of this extension is somewhat unclear . A1 : Our paper studies the theoretical foundations of deep learning . We acknowledge the significant contributions made by Yun et al . ( 2019b ) , which are however under some restrictions , including one hidden layer , squared loss , and one-dimensional output . Thus , it is not sufficient to comprehensively build the theoretical foundation for deep learning . To date , the theoretical study of deep learning is still in its infancy . Significant efforts are really demanded . The motivation and significance of our studies are justified below : 1 . From one hidden layer to arbitrary depth : Empirical results have overwhelmingly suggested that the increase of the depth of neural networks may substantially improve the performance . Additionally , training neural networks is increasingly difficult when the networks turn deeper . Therefore , the depth would play a significant role in shaping the loss surface of a neural network . To prove that networks with an arbitrary depth have infinite spurious local minima , we develop a novel strategy that employs transformation operations to force data flow through the same linear parts of the activations , in order to construct the spurious local minima ( see a summary in Section 3.3 , Stages 2 and 3 , pp.5-6 ; and a detailed proof in Appendix A.3 and A.4 , pp . 20-31 ) .2.From squared loss to arbitrary differentiable loss : Many other loss functions , such as cross-entropy loss , are widely utilized in deep learning . Only considering squared loss is insufficient . Yun et al . ( 2019b ) calculate the analytic formations of derivatives of the loss to construct the local minima and then prove they are spurious . This technique can not be transplanted to the case of arbitrary differentiable loss functions , because we can not assume the analytic formation . To prove that the loss surface under an arbitrary differentiable loss has an infinite number of spurious local minima , we employ a new proof technique based on Taylor series and a new separation lemma ( see Appendix A.5 , Lemma 6 , p. 31 ) to avoid the use of the analytic formulations ( see a detailed proof in Appendix A.2 , Step ( b ) , pp.14-15 ) . 3.From one-dimensional output to arbitrary-dimensional output : Most datasets have high-dimensional labels . For example , the label dimension is 10 in CIFAR-10 , 100 in CIFAR-100 , and 1,000 in ImageNet . To prove the loss surface of a neural network with an arbitrary-dimensional output has an infinite number of spurious local minima , we need to deal with the calculus of functions whose domain and codomain are a matrix space and a vector space , respectively . By contrast , when the output dimension is one , the codomain is only the space of real numbers . Therefore , the extension of output dimension significantly mounts the difficulty of the whole proof . Q2 ( a ) : The main message of Thm 2 is the partition of the surface into multiple pieces , and each piece has good property . This partition has been studied before , in , e.g. , [ R1 ] . Theorem 2 mainly describes the property of each region separately for 2-layer network , which is weaker than [ R1 ] . A2 ( a ) : Thank you for bringing the paper [ R1 ] to our attention . We agree that we overlooked this paper in the original submission . We have duly acknowledged this paper in the final version . We noted that Lemma 2 in [ R1 ] is similar to the 2nd property of our Thm 2 . However , our proof is completely different from that in [ R1 ] and our result is stronger and more general . Specifically , in Property 2 of Thm 2 , we proved that within every cell , all local minima are glocal minimal in the cell . However , the Lemma 2 in [ R1 ] only proves that the local minima in a cell are the same ; there would be some point near the boundary has smaller empirical risk and is not locally minimal . Unfortunately , the proof in [ R1 ] can not exclude this possibility . Furthermore , our proof holds for any convex loss , including squared loss and cross-entropy loss , but [ R1 ] only stands for squared loss ."}], "0": {"review_id": "B1x6BTEKwr-0", "review_text": "This paper focus on how activation functions\u2019 nonlinearities shape the loss surface of neural networks. The authors first show why the loss surface of every neural network has infinite spurious local minima. Secondly, the authors prove one theorem to show four properties of the loss surfaces of nonlinear neural networks. Although this paper is generally easy to follow, and the motivation about nonlinearities and the loss surface is clear, the insight of this paper is somehow shortcoming. Though this work can prove such properties within different preconditions, whether other works\u2019 conditions are inconvenient or not may remain further discussions. This work is established based on several preconditions, while it is hard to assert that most kinds of neural networks can satisfy them perfectly. For instance, this work mentions \u201cDeep learning without poor local minima (NeurIPS2016)\u201d, which requires full-rank and conditional independence of each node. It could be feasible when training a stacked network with particular limitations. This work requires all hidden layers are wider than the output layer, which may not be suitable for image segmentation, generative tasks or super-resolution, etc. Besides, it is laudable to prove fundamental rules in neural networks, while showing or inspiring researchers about how to implement or approximate such results to improve neural networks might be more helpful. Some questions: 1. The authors assert that \u201cthe loss surface of *every* neural network has infinite spurious local minima\u201d in the abstract, while in chapter 3 line 2, authors mention, \u201cWe find that *almost all* practical neural networks have infinitely many spurious local minima.\u201d Which one is correct? Based on the following description in this paper, I guess this result is conditionally tenable. 2. In lemma 3, authors construct the local minima by adding very negative biases and show they are spurious. However, it is less likely to learn such negative biases in the real case. Besides, some networks require biases equal to zero to achieve some specific target. My question is: if biases are conditioned on real-world data distribution, will lemma 3 and 4 still work in this case? 3. This paper mentions \u201cinfinite\u201d many times. Based on the reference, I believe that the \u201cneural network\u201d in this work refers to the \u201cartificial neural network,\u201d which is majorly stored within float tensors. So the number of combinations of parameters is finite. So why use \u201cinfinite\u201d instead of \u201cmany\u201d? Finite means I can train a small scale of networks with fewer precisions and check the global minima with a fixed dataset. All in all, I believe this paper can be significantly improved if more details and experiments are provided.", "rating": "6: Weak Accept", "reply_text": "We appreciate your thorough review and constructive comments . All your concerns have been duly addressed below . We have also updated the final version accordingly . We sincerely hope that you can take into account the response we have made and reevaluate the merit of this paper . Q1 : The authors assert that \u201c the loss surface of * every * neural network has infinite spurious local minima \u201d in the abstract , while in chapter 3 line 2 , authors mention , \u201c We find that * almost all * practical neural networks have infinitely many spurious local minima. \u201d Which one is correct ? Based on the following description in this paper , I guess this result is conditionally tenable . A1 : Thanks and revised accordingly . In the final version , we have added the following description . We first prove that the loss surfaces of many neural networks have infinite spurious local minima , which are defined as the local minima with higher empirical risks than the global minima . Our result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions ( excluding linear functions ) under many popular loss functions in practice with some mild assumptions . Q2 : In lemma 3 , authors construct the local minima by adding very negative biases and show they are spurious . However , it is less likely to learn such negative biases in the real case . Besides , some networks require biases equal to zero to achieve some specific target . My question is : if biases are conditioned on real-world data distribution , will lemma 3 and 4 still work in this case ? A2 : We respectfully argue that the construction of negative bias does not undermine the generality of the obtained results . Under a strong restriction that all activations are linear functions , Kawaguchi ( 2016 ) , Zhou & Liang ( 2018 ) , and Lu & Kawaguchi ( 2017 ) showed that all local minima are global minima , which accounts for the success of deep learning . However , it has been well observed and acknowledged that SGD can converge to points with large training errors , which are apparently not globally optimal . This phenomenon motivates us to study the existence of spurious local minima by relaxing this strong restriction . Theorem 1 of this paper ( based on Lemmas 3 and 4 ) exactly constructs spurious local minima on the loss surface of a nonlinear neural network ( with an arbitrary depth , a differentiable loss and an arbitrary-dimensional output ) . This counterexample proves that the existing theoretical results can not be applied to nonlinear networks . Constructing counterexamples is a widely used approach to prove a proposition is wrong . Therefore , our construction does not undermine the generality . Q3 : This paper mentions \u201c infinite \u201d many times . Based on the reference , I believe that the \u201c neural network \u201d in this work refers to the \u201c artificial neural network , \u201d which is majorly stored within float tensors . So the number of combinations of parameters is finite . So why use \u201c infinite \u201d instead of \u201c many \u201d ? Finite means I can train a small scale of networks with fewer precisions and check the global minima with a fixed dataset . A3 : We respectfully argue that it is common yet mild to treat the parameters of neural networks as continuous numbers for theoretical studies , which has been widely used in related studies . Moreover , the constructed local minima are connected with each other by a continuous path , on which every point has the same empirical risk . Therefore , it is impractical to check all the constructed local minima even when they are represented by float tensors , because the number of float tensors on a continuous path is extremely large . For example , there are $ 2^ { 52 } = 4.5 \\times 10^ { 15 } $ $ 64 $ -bit float values between $ 1 $ and $ 2 $ when using the double precision . Reference Kenji Kawaguchi . Deep learning without poor local minima . In Advances in Neural Information Processing Systems , 2016 . Haihao Lu and Kenji Kawaguchi . Depth creates no bad local minima . arXiv preprint arXiv:1702.08580 , 2017 . Yi Zhou and Yingbin Liang . Critical points of neural networks : Analytical forms and landscape properties . In International Conference on Learning Representations , 2018 ."}, "1": {"review_id": "B1x6BTEKwr-1", "review_text": "This paper studies the theoretical property of neural network's loss surface. The main contribution is to prove that the loss surface of every neural network (with arbitrary depth) with piecewise linear activations has infinite spurious local minima. Moreover, the paper further characterizes the partition of the local minima. More precisely, the loss surface is partitioned into multiple smooth and multilinear open cells and within each cell, the local minima are equally good. This result can also explain the linear neural network case where there is only one cell, implying that all local minima are global. On one hand, I find the paper very clear and the result very clean, which unites a lot of existing results. On the other hand, with a reasonable initialization in practice, we will not attain the local minima constructed in the paper since it requires all the activations to be positive. This limits the plausible implication from this theoretical study. Overall, I am very positive of the paper, the following are some detailed comments. a. Please be more precise in the abstract that the activation function need to be piecewise linear. The current sentence \"the loss surface of every neural network has infinite spurious local minima\" does not include this specification. Moreover, if the activation is differentiable, is the claim still hold? It seems to me from the middle of page 3 that Li et al 2018 shows a non-local minima result in this case. b. How different is the analysis comparing to existing result? I have only go through the skeleton of the proof and have not read into the details. It seems to me the construction of the local minima is very similar to [1], since the main idea is to consider the linear region by activating all the neurons. Could you summarize the main difficulty to extend their results to multi-layer cases? (Maybe it would be good to illustrate with a simple case like 3 layers few neurons per layer) Moreover, when considering the local convexity, is it sufficient to say that locally in each cell it is a linear network and then the results on linear network transfers to it locally? [1] Small nonlinearities in activation functions create bad local minima in neural networks, Yun et al, 2019", "rating": "6: Weak Accept", "reply_text": "We appreciate your thorough review and constructive comments . Thank you very much for your kind support . All your concerns have been duly addressed below . We have also updated the final version accordingly . Q1 : Please be more precise in the abstract that the activation function needs to be piecewise linear . Moreover , if the activation is differentiable , is the claim still hold ? A1 : Thanks and revised accordingly . We have stated in the abstract that we proved the cases of piecewise linear activation functions . In addition , the results have not been extended to differentiable activations . Q2 : How different is the analysis comparing to existing result with [ 1 ] ? Could you summarize the main difficulty to extend their results to multi-layer cases ? A2 : Thanks and revised accordingly . A detailed comparison of the analysis has been added to the final version . We summarise it as follows . We first acknowledge that [ 1 ] and our paper both employ the following strategy : ( a ) construct a series of local minima based on a linear classifier ; and ( b ) construct a new point with smaller empirical risk and by this way we prove that the constructed local minima are spurious . However , due to the differences in the loss function and the output dimensions , the exact constructions of local minima are substantially different . Meanwhile , our Stages ( 2 ) and ( 3 ) employ the transformation operation to force the data flow to go through the same series of the linear parts of the activations . The operations are carefully designed and the whole construction is novel and non-trivial . Besides , we also made extensions on the loss function and the output dimension . The difficulties are justified below : 1 . From squared loss to arbitrary differentiable loss : Yun et al . ( 2019b ) calculate the analytic formations of derivatives of the loss to construct the local minima and then prove they are spurious . This technique can not be transplanted to the case of arbitrary differentiable loss functions , because we can not assume the analytic formation . To prove that the loss surface under an arbitrary differentiable loss has an infinite number of spurious local minima , we employ a new proof technique based on Taylor series and a new separation lemma ( see Appendix A.5 , Lemma 6 , p. 31 ) to avoid the use of the analytic formulations ( see a detailed proof in Appendix A.2 , Step ( b ) , pp.14-15 ) . 2.From one-dimensional output to arbitrary-dimensional output : To prove the loss surface of a neural network with an arbitrary-dimensional output has an infinite number of spurious local minima , we need to deal with the calculus of functions whose domain and codomain are a matrix space and a vector space , respectively . By contrast , when the output dimension is one , the codomain is only the space of real numbers . Therefore , the extension of output dimension significantly mounts the difficulty of the whole proof ."}, "2": {"review_id": "B1x6BTEKwr-2", "review_text": "Summary: This paper studies the landscape of deep neural networks with piecewise-linear activation functions. The paper showed that under very mild assumptions, the loss surface admits infinite spurious local minima. Further, it is shown that the loss surface is partitioned into many multilinear cells. If the network is two-layer with two-piece linear activations, it is proved that within each cell every local minimum is global. Pros: --Constructed spurious local minima for piecewise linear activations, for a broader setting than previous papers. --The paper is well written, with detailed explanation of proof skeleton. Cons: The significance of the results are not clear. Details are given below. 1. This paper only considers piecewise linear activation, which is a special type of non-linear activation. The major examples are ReLU and leaky ReLU. However, related results for ReLU have been studied for a few previous works mentioned in the last two paragraphs of Sec. 3.2. In particular, Yun et al. 2019b already proves a similar result for 1-hidden-layer neural-net with ReLU activation. I think extending the construction to broader settings (any depth, any piecewise linear and more losses) is mathematically nice, but the motivation of this extension is somewhat unclear to me. One motivation is that this is helpful for the purpose of understanding a \u201cbig picture\u201d of the landscape, which I will discuss next. 2. The second major result is Theorem 2, on the \u201cbig picture\u201d with ReLU-like activations. However, Theorem 2 is somewhat trivial to prove, and the link to Theorem 1 is rather weak. (a) The main message of Thm 2 is the partition of the surface into multiple pieces, and each piece has good property. This partition is somewhat straightforward, and has been studied before, in, e.g., [R1]. For a global \u201cbig picture\u201d, partitioning itself is not very interesting. Theorem 2 mainly describes the property of each region separately for 2-layer network, which is weaker than [R1]. (b) Theorem 2 seems easy to prove. The 1st, 3rd and 4th property are all straightforward. The 2nd property \u201clocal analogous convexity\u201d was given a 2-page proof in the paper. However, I don\u2019t understand why not use the following simple argument: for each region, the network behaves like a deep linear network, thus directly applying existing result shall imply \u201cevery local minimum in the region is the global minimum of the region\u201d, right? If not, what is the difficulty? (c) The 3rd property says \u201csome local minima are concentrated as a valley in some cell\u201d. What are the formal definitions of \u201cconcentrated\u201d and \u201cvalley\u201d in this sentence? (d) The link to Theorem 1 is weak: the link is the 3rd property of Theorem 2 that \u201csome local minima are in a valley\u201d. It is just about some special local minima and weakly related to the other properties on the \u201cglobal view\u201d. In addition, the fact that \u201csome of them are in a valley\u201d may be due to the very special construction, thus it is not surprising and does not reveal anything interesting about the \u201cbig picture\u201d. 3. Other issues: a) While ReLU-type activations are popular, there are still commonly-used activation functions are not piece-wise linear, e.g., tanh, swish. It is not proper to claim that \"this paper presents how nonlinearities in activations substantially shape the loss surface\" and \"almost every practical neural network ....\". I suggest replacing \"nonlinearity\" with \"piecewise linearity\" in both the title and the abstract, and modifying the over-statements. b) In Property 1 of Theorem 2, \u201csmooth and multilinear partition\u201d might be a bit misleading. The loss surface should be fractional in general, where multilinear cells are separated by non-smooth boundaries. \u201cSmooth partition\u201d seems to imply that the boundaries are smooth or the partition method is smooth in some sense. c) The name \u201canalogous convexity\u201d is not appropriate. Analogous convexity is not formally defined in the paper. According to Sec. 4.3 third paragraph, \u201cthe property of analogous convexity that the local minima wherein are equally good\u201d. It seems that \u201canalogous convexity\u201d is just \u201call local minima are good\u201d, which is very different from convexity. It is a weaker property than quasi-convexity, star-convexity, etc, and thus it is better not to call it \u201canalogous convexity\u201d. d) Property 3 of Theorem 2 is very far from \u201cmode connectivity\u201d. The proof of Property 3 relies on a special construction of Theorem 1, and the latter is for two arbitrary global minima. [R1] Soudry and Hoffer. \"Exponentially vanishing sub-optimal local minima in multilayer neural networks.\" arXiv preprint arXiv:1702.05777 (2017). Conclusion: I think this paper is studying an important and interesting question, and the efforts of constructing local minima and understanding big picture are both interesting to me. However, I\u2019m afraid the current form of the paper does not meet the standard of the conference. That being said, it would be a nice paper if the big picture can be explored deeper, and the link to the spurious local minima can be built stronger. ", "rating": "3: Weak Reject", "reply_text": "We appreciate your thorough review and constructive comments . All your concerns have been duly addressed below . We have also updated the final version accordingly . We sincerely hope that you can take into account the response we have made and reevaluate the merit of this paper . Q1 : Related results have been studied in a few previous works . In particular , Yun et al . ( 2019b ) prove a similar result for 1-hidden-layer neural-net with ReLU activation . The extension is mathematically nice , but the motivation of this extension is somewhat unclear . A1 : Our paper studies the theoretical foundations of deep learning . We acknowledge the significant contributions made by Yun et al . ( 2019b ) , which are however under some restrictions , including one hidden layer , squared loss , and one-dimensional output . Thus , it is not sufficient to comprehensively build the theoretical foundation for deep learning . To date , the theoretical study of deep learning is still in its infancy . Significant efforts are really demanded . The motivation and significance of our studies are justified below : 1 . From one hidden layer to arbitrary depth : Empirical results have overwhelmingly suggested that the increase of the depth of neural networks may substantially improve the performance . Additionally , training neural networks is increasingly difficult when the networks turn deeper . Therefore , the depth would play a significant role in shaping the loss surface of a neural network . To prove that networks with an arbitrary depth have infinite spurious local minima , we develop a novel strategy that employs transformation operations to force data flow through the same linear parts of the activations , in order to construct the spurious local minima ( see a summary in Section 3.3 , Stages 2 and 3 , pp.5-6 ; and a detailed proof in Appendix A.3 and A.4 , pp . 20-31 ) .2.From squared loss to arbitrary differentiable loss : Many other loss functions , such as cross-entropy loss , are widely utilized in deep learning . Only considering squared loss is insufficient . Yun et al . ( 2019b ) calculate the analytic formations of derivatives of the loss to construct the local minima and then prove they are spurious . This technique can not be transplanted to the case of arbitrary differentiable loss functions , because we can not assume the analytic formation . To prove that the loss surface under an arbitrary differentiable loss has an infinite number of spurious local minima , we employ a new proof technique based on Taylor series and a new separation lemma ( see Appendix A.5 , Lemma 6 , p. 31 ) to avoid the use of the analytic formulations ( see a detailed proof in Appendix A.2 , Step ( b ) , pp.14-15 ) . 3.From one-dimensional output to arbitrary-dimensional output : Most datasets have high-dimensional labels . For example , the label dimension is 10 in CIFAR-10 , 100 in CIFAR-100 , and 1,000 in ImageNet . To prove the loss surface of a neural network with an arbitrary-dimensional output has an infinite number of spurious local minima , we need to deal with the calculus of functions whose domain and codomain are a matrix space and a vector space , respectively . By contrast , when the output dimension is one , the codomain is only the space of real numbers . Therefore , the extension of output dimension significantly mounts the difficulty of the whole proof . Q2 ( a ) : The main message of Thm 2 is the partition of the surface into multiple pieces , and each piece has good property . This partition has been studied before , in , e.g. , [ R1 ] . Theorem 2 mainly describes the property of each region separately for 2-layer network , which is weaker than [ R1 ] . A2 ( a ) : Thank you for bringing the paper [ R1 ] to our attention . We agree that we overlooked this paper in the original submission . We have duly acknowledged this paper in the final version . We noted that Lemma 2 in [ R1 ] is similar to the 2nd property of our Thm 2 . However , our proof is completely different from that in [ R1 ] and our result is stronger and more general . Specifically , in Property 2 of Thm 2 , we proved that within every cell , all local minima are glocal minimal in the cell . However , the Lemma 2 in [ R1 ] only proves that the local minima in a cell are the same ; there would be some point near the boundary has smaller empirical risk and is not locally minimal . Unfortunately , the proof in [ R1 ] can not exclude this possibility . Furthermore , our proof holds for any convex loss , including squared loss and cross-entropy loss , but [ R1 ] only stands for squared loss ."}}