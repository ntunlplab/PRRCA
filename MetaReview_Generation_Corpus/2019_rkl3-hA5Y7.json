{"year": "2019", "forum": "rkl3-hA5Y7", "title": "Towards Decomposed Linguistic Representation with Holographic Reduced Representation", "decision": "Reject", "meta_review": "This paper proposes the use of holographic reduced representations in language modeling, which allows for a cleaner decomposition of various linguistic traits in the representation. Results show improvements over baseline language models, and analysis shows that the representations are indeed decomposing as expected.\n\nThe main reviewer concern was the lack of strength of the baseline, although the authors stress that they were using the default baseline from TensorFlow, which seems like it will be reasonable to me. Another concern is that there is other work on using HRR to disentangle syntax and semantics in representations for language (e.g. \"Distributed Tree Kernels\" ICML 2012, but also others), that has not been considered. \n\nBased on this, this seems like a very borderline case. Given that no reviewer is pushing strongly for the paper I'm leaning towards not recommending acceptance, but I could very easily see the paper being accepted as well.", "reviews": [{"review_id": "rkl3-hA5Y7-0", "review_text": "The paper proposes a new approach for neural language models based on holographic reduced representations (HRRs). The goal of the approach is to learn disentangled representations that separate different aspects of a term, such as its semantic and its syntax. For this purpose the paper proposes models both on the word and chunk level. These models aim disentangle the latent space by structuring the latent space into different aspects via role-filler bindings. Learning disentangled representations is a promising research direction that fits well into ICLR. The paper proposes interesting ideas to achieve this goal in neural language models via HRRs. Compositional models like HRRs make a lot of sense for disentangling structure in the embedding space. Some of the experimental results seem to indicate that the proposed approach is indeed capable to discover rough linguistic roles. However, I am currently concerned about different aspects of the paper: - From a modeling perspective, the paper seems to conflate two points: a) language modeling vie role-filler/variable-binding models and b) holographic models as specific instance of variable bindings. The benefits of HRRs (compared e.g., to tensor-product based models) are likely in terms of parameter efficiency. However, the benefits from a variable-binding approach for disentanglement should remain across the different binding operators. It would be good to separate these aspects and also evaluate other binding operators like tensors products in the experiments. - It is also not clear to me in what way we can interpret the different filler embeddings. The paper seems to argue that the two spaces correspond to semantics and syntax. However, this seems in no way guaranteed or enforced in the current model. For instance, on a different dataset, it could entirely be possible that the embedding spaces capture different aspects of polysemy. However, this is a central point of the paper and would require a more thorough analysis, either by a theoretical motivation or a more comprehensive evaluation across multiple datasets. - In its current form, I found the experimental evaluation not convincing. The qualitative analysis of filler embeddings is indeed interesting and promising. However, the comparisons to baseline models is currently lacking. For instance, perplexity results are far from state of the art and more importantly below serious baselines. For instance, the RNN+LDA baseline from Mikolov (2012) achieves already a perplexity of 92.0 on PTB (best model in the paper is 92.4). State-of-the-art models acheive perplexities around 50 on PTB. Without an evaluation against proper baselines I find it difficult to accurately assess the benefits of these models. While language modeling in terms of perplexity is not necessarily a focus of this paper, my concern translates also to the remaining experiments as they use the same weak baseline. - Related to my point above, the experimental section would benefit significantly if the paper also included evaluations on downstream tasks and/or evaluated against existing methods to incorporate structure in language models. Overall, I found that the paper pursues interesting and promising ideas, but is currently not fully satisfying in terms of evaluation and discussion.", "rating": "5: Marginally below acceptance threshold", "reply_text": "First , we would like to thank you for the kind words regarding our general idea . We fully acknowledge the validity of the many concerned raised here . We provide responses to them below : ( 1 ) \u201c From a modeling perspective , the paper seems to conflate two points \u201d : First of all , we agree that there are two points to be made here as you have pointed out : ( a ) the potential benefit of a role-filler approach ( b ) the architectural or computational advantage of any specific instance of such an approach . In writing this paper , we have the following considerations . First , we use language modeling as our testbed to investigate ( a ) . As we explained in the intro , \u201c the versatility of language modeling [ as a complementary or pretraining task ] demonstrates that some linguistic regularities much be present \u201d . The recent success of BERT [ 1 ] and ELMO [ 2 ] across many tasks ( including some very linguistics-oriented benchmarks ) reflects this point as well . This being said , we acknowledge there are many other tasks that could be used to investigate ( a ) -- for instance , [ 3 ] used QA as the main task , and we personally thought about summarization on the ground that a summary has a clear designation of sentential roles ( e.g. , event name , location , etc ) . However , the simplicity of LM , coupled with its minimal necessity for supervision , convinced us to focus on LM instead . Second , while there are many other instances of variable-binding framework ( TPR being one of the most prominent examples ) , we decided to investigate into HRR on computational grounds . This was explained in our background section ( \u201c makes HRR a more practical choice \u201d ) . We will elaborate on this point more in the updated version . Our claim in the paper is based on the two considerations above . We believe that both ( a ) and ( b ) should be investigated fully , but given that this is our initial attempt , we think it \u2019 s reasonable to make some simplifying assumptions . We hope this address your concern . ( 2 ) \u201c It is also not clear to me in what way we can interpret the different filler embedding \u201d We agree that the separation of semantics and syntax is not guaranteed . However , nor did we claim it to be . We stated in the intro that our model can \u201c effectively separate certain aspects of word or chunk representation , which roughly corresponds to a division between syntax and semantics \u201d . The vagueness of our statement is precisely due to the fact that our model doesn \u2019 t have a \u201c syntax training loss \u201d or \u201c semantics training loss \u201d . In light of this , we argue that it is interesting and somewhat surprising that HRR-enabled models learned to separate these two aspects without a dedicated loss term . This goes to show that an inductive bias can be beneficial . We also agree that we need to make more comprehensive evaluation . We are currently expanding our experiments to more datasets ( wiki , one-billion-word , possibly some domain-specific texts , or some subset of them ) , and we will provide an updated version as soon as possible . On the other hand , we would like to point out that for all our experiments on PTB , we observed a consistent pattern that the first role ( the one without downweighting the dot product at the start of training ) always corresponds more to syntax than semantics , regardless of hyperparameter setting or random seed . We think it \u2019 s because syntactic cues/signals ( e.g. , POS tags ) are relatively easier to identify than semantics ones ( e.g. , topic relatedness ) , and therefore the first set of embeddings tend fo consistently capture the more syntactic aspect . Of course , this pattern will carry more weight if our new round of experiments also confirm it . ( 3 ) \u201c In its current form , I found the experimental evaluation not convincing \u201d We are fully aware that our baseline seems to underperform , as pointed out by reviewer 3 as well . First we would like to point out that contrary to common practice in LM literature , \u201c we do not assume that the contiguous sentences in the raw data are fed sequentially as input \u201d , and as a result \u201c we do not initialize the hidden state of LSTM with the last state from the last batch \u201d ( section 4.1 ) . We took this approach to ensure that chunk-level representations capture only intra-sentential roles -- we do not consider discourse-level features . The downside of this is that we can no longer reply on information from the last sentence to help predict the current one . Meanwhile , we are also running another word-level baseline that follows the common practice in LM literature . We will update the results shortly ."}, {"review_id": "rkl3-hA5Y7-1", "review_text": "This paper is very interesting as it seems to bring the clock back to Holographic Reduced Representations (HRRs) and their role in Deep Learning. It is an important paper as it is always important to learn from the past. HRRs have been introduced as a form of representation that is invertible. There are two important aspects of this compositional representation: base vectors are generally drawn from a multivariate gaussian distribution and the vector composition operation is the circular convolution. In this paper, it is not clear why random vectors have not been used. It seems that everything is based on the fact that orthonormality is impose with a regularization function. But, how can this regularization function can preserve the properties of the vectors such that when these vectors are composed the properties are preserved. Moreover, the sentence \"this is computationally infeasible due to the vast number of unique chunks\" is not completely true as HRR have been used to represent trees in \"Distributed Tree Kernels\" by modifying the composition operation in a shuffled circular convolution. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your interest in our work and your kind words regarding the direction our paper takes . We summarize all the concerns raised and provide a point-by-point response below . ( 1 ) \u201c In this paper , it is not clear why random vectors have not been used \u201d We have two points to make regarding this comment . First , we did experiment on using fixed random basis embeddings , be it basis role embeddings or basis filler embeddings . This is denoted by models with names Fixed- * in Table 1 . This is also mentioned in Page 4 , right below Figure 1 , \u201c we also consider using fixed random vectors for basis embeddings \u201d . Second , if you are referring to using random vectors for not just bases , but also other trainable word-embedding related parameters ( such as s^w_i ) , we think it is better to treat them as learnable parameters since random vectors do not cluster together in a meaningful way that corresponds to natural language . ( 2 ) \u201c But , how can this regularization function preserve the properties of the vectors such that when these vectors are composed the properties are preserved \u201d We agree with this characterization . However , we want to remake the point we make in response to reviewer 3 ( point 1 , reproduced below ) \u201c ... in our case , the decomposed scoring function actually acts as an ( soft ) enforcer that makes sure decoding works properly . The loss would only go down when the predicted filler embedding ( after decoding ) is close to the original filler embedding ( before encoding ) . This is largely mediated by dot product -- the more accurate the decoding is , the bigger the value of dot product is. \u201d Although it is out our intention to design a theoretically complete model that preserves the properties all the way through , we do mean to take advantage of HRR properties , combined with black-box modeling from neural networks . We believe this is a reasonable approach to take in order to make our model viable in the world of deep learning . ( 3 ) \u201c Moreover , the sentence \u2018 this is computationally infeasible due to the vast number of unique chunks \u2019 is not completely true \u201d We meant to say that directly extending our word-level model to chunk-level is not plausible , because for word-level model , we designate a learnable vectorial parameter to each word type . By analogy , we would have to use a learnable vectorial parameter for each unique chunk type , which renders it intractable in our case . It is in this sense that we meant by saying \u201c this is computationally infeasible \u201d . Hopefully these responses address your concerns ."}, {"review_id": "rkl3-hA5Y7-2", "review_text": " Summary: ======== Theis paper proposes a method for learning decomposable representations in the context of a language modeling task. Using holographic reduced representations (HRR), a word embedding is composed of a role and a filler. The embedding is then fed to an LSTM language model. There is also an extension to chunk-level representations. Experimentally, the model achieves perplexity comparable to a (weak) baseline LSTM model. The analysis of the learned representations shows a separation into syntactic and semantic roles. The paper targets an important problem, that of learning decomposable representations. As far as I know, it introduces a novel perspective using HRR and does so in the context of language modeling, which is a core NLP task. The analysis of the learned representations is quite interesting. I do have some concerns with regards to the quality of the language model, the clarity of some of the model description, and the validity of using HRR in this scenario. Please see detailed comments below. Comments: ========= 1. Section 2 refers to Plate (1995) for the conditions when the approximate decoding via correlation holds. I think it's important to mention these conditions and discuss whether they apply to the language modeling case. In particular, Plate mentions that the elements of each vector need to be iid with mean zero and variance 1/n (where n is the length of the vector). Is this true for the present case? Typically, word embeddings and LSTM states are do not exhibit this distribution. Are there other conditions that are (not) met? 2. Learning separate bases for different role-filler bindings is said to encourage the model to learn a decomposition of word representation. On the other hand, if I understand correctly, this means that word embeddings are not shared between roles, because s^w_i is also a role-specific vector (not just a word-specific vector). Is that a cause of concern? 3. It's not clear to me where in the overall model the next word is predicted. Figure 1b has an LSTM that predicts filler embeddings. Does this replace predicting the next word in a vanilla LSTM? Equation 5 still computes a word score. Is this used to compute the probability of the next word as in equation 2? 4. Comparison to other methods for composing words. Since much of the paper is concerned with composing words, it seem natural to compare the methods (and maybe some of the results) to methods for composing words. Some examples include [2] and the line of work on recursive neural networks by Socher et al., but there are many others. 5. Perplexity results: - The baseline results (100.5 ppl on PTB) are very weak for an LSTM. There are multiple papers showing that a simple LSTM can do much better. The heavily tuned LSTM of [1] gets 59.6 but even less tuned LSTMs go under 80 or 80 ppl. See some results in [1]. This raises a concern that the improvements from the HRR model may not be significant. Would they hold in a more competitive model? - Can you speculate or analyze in more detail why the chunk-level model doesn't perform well, and why adding more fillers doesn't help in this case? 6. Motivation: - The introduction claims that the dominant encoder-decoder paradigm learns \"transformations from many smaller comprising units to one complex emedding, and vice versa\". This claim should be qualified by the use of attention, where there is not a single complex embedding, rather a distribution over multiple embeddings. - Introduction, first paragraph, claims that \"such crude way of representing the structure is unsatisfactory, due to a lack of transparency, interpretability or transferability\" - what do you mean by these concepts and how exactly is the current approach limited with respect to them? Giving a bit more details about this point here or elsewhere in the paper would help motivate the work. 7. Section 3.3 was not so clear to me: - In step 1, what are these r_i^{chunk}? Should we assume that all chunks have the same role embeddings, despite them potentially being syntactically different? How do you determine where to split output vectors from the RNN to two parts? What is the motivation for doing this? - In prediction, how do you predict the next chunk embedding? Is there a different loss function for this? - Please provide more details on decoding, such as the mentioned annealing and regularization. - Finally, the reliance on a chunker is quite limiting. These may not be always available or of high quality. 8. The analysis in section 4.3 is very interesting and compelling. Figure 2 makes a good point. I would have liked to see more analysis along these lines. For example, more discussion of the word analogy results, including categories where HRR does not do better than the baseline. Also consider other analogy datasets that capture different aspects. 9. While I agree that automatic evaluation at chunk-level is challenging, I think more can be done. For instance, annotations in PTB can be used to automatically assign roles such as those in table 4, or others (there are plenty of annotations on PTB), and then to evaluate clustering along different annotations at a larger scale. 10. The introduction mentions a subset of the one billion word LM dataset (why a subset?), but then the rest of the papers evaluates only on PTB. Is this additional dataset used or not? 11. Introduction, first paragraph, last sentence: \"much previous work\" - please cite such relevant work on inducing disentangled representations. 12. Please improve the visibility of Figure 1. Some symbols are hard to see when printed. 13. More details on the regularization on basis embeddings (page 4) would be useful. 14. Section 3.3 says that each unique word token is assigned a vectorial parameter. Should this be word type? 15. Why not initialize the hidden state with the last state from the last batch? I understand that this is done to assure that the chunk-level models only consider intra-sentential information, but why is this desired? 16. Have you considered using more than two roles? I wonder how figure 2 would look in this case. Writing, grammar, etc.: ====================== - End of section 1: Our papers -> Our paper - Section 2: such approach -> such an approach; HRR use -> HRR uses; three operations -> three operations*:* - Section 3.1: \"the next token w_t\" - should this be w_{t+1)? - Section 3.2, decoding: remain -> remains - Section 3.3: work token -> word token - Section 4.1: word analogy task -> a word analogy task; number basis -> numbers of basis - Section 4.2: that the increasing -> that increasing - Section 4.3: no space before comma (first paragraph); on word analogy task -> on a word analogy task; belong -> belongs - Section 4.4: performed similar -> performed a similar; luster -> cluster - Section 5: these work -> these works/papers/studies; share common goal -> share a common goal; we makes -> we make; has been -> have been References ========== [1] Melis et al., On the State of the Art of Evaluation in Neural Language Models [2] Mitchell and Lapata, Vector-based Models of Semantic Composition ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your kind words and very detailed comments . Before delving into the detail , we would appreciate it if you could elaborate a bit more on the concern about \u201c the validity of using HRR in this scenario \u201d mentioned in the second paragraph ? ( a ) Did you mean using HRR in language modeling ? ( b ) If this is case , does it concern you because the choice of this task , or because of the inadequate baseline performance ? Thank you very much ! ( 1 ) \u201c the conditions when the approximate decoding via correlation holds \u201d Thanks for pointing this out . We will explain a bit more in our updated version . For our experiments though , only the models run with fixed basis embeddings are with mean zero and variance 1/n because they are randomly sampled and fixed throughout training . I agree that word embeddings and LSTM states do not typically exhibit such a distribution ( especially iid condition ) . However , we would also like to make two points . First , past work [ 1 ] that successfully uses HRR as associative memory , where these conditions are also not explicitly met ( or at least they didn \u2019 t show it ) . Second , in our case , the decomposed scoring function actually acts as an ( soft ) enforcer that makes sure decoding works properly . The loss would only go down when the predicted filler embedding ( after decoding ) is close to the original filler embedding ( before encoding ) . This is largely mediated by dot product -- the more accurate the decoding is , the bigger the value of dot product is . As for other conditions , it is also required that the dimensionality of the vector be sufficiently bigger than the number of stored items . This obviously holds in our case since we are only using a couple of variable bindings , and we will make it more clear in the updated version . ( 2 ) \u201c Learning separate bases for different role-filler bindings is said to encourage the model to learn a decomposition of word representation \u201d If we understand the question correctly , for each word , there are two ( equal to the number of roles ) filler embeddings , which have separate bases . These filler embeddings are then bound with their associated role embeddings . In this sense , base filler embeddings and role embeddings are shared across all words , but not between roles . Our earlier experiments showed that without separating these bases , decomposition of representations did not occur . We think it makes sense intuitively -- a decomposition of representation usually necessitates a separation of feature space . Does this address your concern ? ( 3 ) \u201c It 's not clear to me where in the overall model the next word is predicted \u201d We apologize for this confusion . The decoding module in 1 ( b ) corresponds to equation 5 . Instead of using one dot product as in a vanilla LSTM , we use the sum of two dot products , each of which is responsible for one role-filler binding . Indeed the score in equation 5 is used similarly as in equation 2 . We will make this more clear in the updated version . ( 4 ) \u201c Comparison to other methods for composing words \u201d If we understand it correctly , you are referring to the word-level model since this is where we spent most time entailing and analyzing . As we argued in the response to reviewer 2 ( point 4 , reproduced below ) , we do not find any directly comparable method to the best our knowledge . \u201c ... There are certainly many existing methods that try to incorporate structures , but mostly to enhance their representation , not decompose their representation . Moreover , the unsupervised nature of our approach makes direct comparison even harder. \u201d The cited work you provided [ 2 ] , and also Socher \u2019 s recursive network network deal with composing phrases from individual words , which does not concern the decomposition of word representation . Moreover , recursive neural networks need additional input such as parsed trees , which is definitely outside the scope of our paper . We would like to emphasize that the main contribution is about the decomposition/separation of representations . This decomposition , in HRR \u2019 s framework , is accompanied by the initial operation of encoding/composing . Due to space limit , we do not fully investigate the potential advantage/disadvantage of using HRR as an encoder ( compared to ( say ) Socher \u2019 s work ) , but rather spend most of the time using HRR to set up a model that can induce decomposition . Of course , we can be totally ignorant of other directly comparable methods . If you have any specific method in mind , we would really appreciate it if you can provide us some pointers ."}], "0": {"review_id": "rkl3-hA5Y7-0", "review_text": "The paper proposes a new approach for neural language models based on holographic reduced representations (HRRs). The goal of the approach is to learn disentangled representations that separate different aspects of a term, such as its semantic and its syntax. For this purpose the paper proposes models both on the word and chunk level. These models aim disentangle the latent space by structuring the latent space into different aspects via role-filler bindings. Learning disentangled representations is a promising research direction that fits well into ICLR. The paper proposes interesting ideas to achieve this goal in neural language models via HRRs. Compositional models like HRRs make a lot of sense for disentangling structure in the embedding space. Some of the experimental results seem to indicate that the proposed approach is indeed capable to discover rough linguistic roles. However, I am currently concerned about different aspects of the paper: - From a modeling perspective, the paper seems to conflate two points: a) language modeling vie role-filler/variable-binding models and b) holographic models as specific instance of variable bindings. The benefits of HRRs (compared e.g., to tensor-product based models) are likely in terms of parameter efficiency. However, the benefits from a variable-binding approach for disentanglement should remain across the different binding operators. It would be good to separate these aspects and also evaluate other binding operators like tensors products in the experiments. - It is also not clear to me in what way we can interpret the different filler embeddings. The paper seems to argue that the two spaces correspond to semantics and syntax. However, this seems in no way guaranteed or enforced in the current model. For instance, on a different dataset, it could entirely be possible that the embedding spaces capture different aspects of polysemy. However, this is a central point of the paper and would require a more thorough analysis, either by a theoretical motivation or a more comprehensive evaluation across multiple datasets. - In its current form, I found the experimental evaluation not convincing. The qualitative analysis of filler embeddings is indeed interesting and promising. However, the comparisons to baseline models is currently lacking. For instance, perplexity results are far from state of the art and more importantly below serious baselines. For instance, the RNN+LDA baseline from Mikolov (2012) achieves already a perplexity of 92.0 on PTB (best model in the paper is 92.4). State-of-the-art models acheive perplexities around 50 on PTB. Without an evaluation against proper baselines I find it difficult to accurately assess the benefits of these models. While language modeling in terms of perplexity is not necessarily a focus of this paper, my concern translates also to the remaining experiments as they use the same weak baseline. - Related to my point above, the experimental section would benefit significantly if the paper also included evaluations on downstream tasks and/or evaluated against existing methods to incorporate structure in language models. Overall, I found that the paper pursues interesting and promising ideas, but is currently not fully satisfying in terms of evaluation and discussion.", "rating": "5: Marginally below acceptance threshold", "reply_text": "First , we would like to thank you for the kind words regarding our general idea . We fully acknowledge the validity of the many concerned raised here . We provide responses to them below : ( 1 ) \u201c From a modeling perspective , the paper seems to conflate two points \u201d : First of all , we agree that there are two points to be made here as you have pointed out : ( a ) the potential benefit of a role-filler approach ( b ) the architectural or computational advantage of any specific instance of such an approach . In writing this paper , we have the following considerations . First , we use language modeling as our testbed to investigate ( a ) . As we explained in the intro , \u201c the versatility of language modeling [ as a complementary or pretraining task ] demonstrates that some linguistic regularities much be present \u201d . The recent success of BERT [ 1 ] and ELMO [ 2 ] across many tasks ( including some very linguistics-oriented benchmarks ) reflects this point as well . This being said , we acknowledge there are many other tasks that could be used to investigate ( a ) -- for instance , [ 3 ] used QA as the main task , and we personally thought about summarization on the ground that a summary has a clear designation of sentential roles ( e.g. , event name , location , etc ) . However , the simplicity of LM , coupled with its minimal necessity for supervision , convinced us to focus on LM instead . Second , while there are many other instances of variable-binding framework ( TPR being one of the most prominent examples ) , we decided to investigate into HRR on computational grounds . This was explained in our background section ( \u201c makes HRR a more practical choice \u201d ) . We will elaborate on this point more in the updated version . Our claim in the paper is based on the two considerations above . We believe that both ( a ) and ( b ) should be investigated fully , but given that this is our initial attempt , we think it \u2019 s reasonable to make some simplifying assumptions . We hope this address your concern . ( 2 ) \u201c It is also not clear to me in what way we can interpret the different filler embedding \u201d We agree that the separation of semantics and syntax is not guaranteed . However , nor did we claim it to be . We stated in the intro that our model can \u201c effectively separate certain aspects of word or chunk representation , which roughly corresponds to a division between syntax and semantics \u201d . The vagueness of our statement is precisely due to the fact that our model doesn \u2019 t have a \u201c syntax training loss \u201d or \u201c semantics training loss \u201d . In light of this , we argue that it is interesting and somewhat surprising that HRR-enabled models learned to separate these two aspects without a dedicated loss term . This goes to show that an inductive bias can be beneficial . We also agree that we need to make more comprehensive evaluation . We are currently expanding our experiments to more datasets ( wiki , one-billion-word , possibly some domain-specific texts , or some subset of them ) , and we will provide an updated version as soon as possible . On the other hand , we would like to point out that for all our experiments on PTB , we observed a consistent pattern that the first role ( the one without downweighting the dot product at the start of training ) always corresponds more to syntax than semantics , regardless of hyperparameter setting or random seed . We think it \u2019 s because syntactic cues/signals ( e.g. , POS tags ) are relatively easier to identify than semantics ones ( e.g. , topic relatedness ) , and therefore the first set of embeddings tend fo consistently capture the more syntactic aspect . Of course , this pattern will carry more weight if our new round of experiments also confirm it . ( 3 ) \u201c In its current form , I found the experimental evaluation not convincing \u201d We are fully aware that our baseline seems to underperform , as pointed out by reviewer 3 as well . First we would like to point out that contrary to common practice in LM literature , \u201c we do not assume that the contiguous sentences in the raw data are fed sequentially as input \u201d , and as a result \u201c we do not initialize the hidden state of LSTM with the last state from the last batch \u201d ( section 4.1 ) . We took this approach to ensure that chunk-level representations capture only intra-sentential roles -- we do not consider discourse-level features . The downside of this is that we can no longer reply on information from the last sentence to help predict the current one . Meanwhile , we are also running another word-level baseline that follows the common practice in LM literature . We will update the results shortly ."}, "1": {"review_id": "rkl3-hA5Y7-1", "review_text": "This paper is very interesting as it seems to bring the clock back to Holographic Reduced Representations (HRRs) and their role in Deep Learning. It is an important paper as it is always important to learn from the past. HRRs have been introduced as a form of representation that is invertible. There are two important aspects of this compositional representation: base vectors are generally drawn from a multivariate gaussian distribution and the vector composition operation is the circular convolution. In this paper, it is not clear why random vectors have not been used. It seems that everything is based on the fact that orthonormality is impose with a regularization function. But, how can this regularization function can preserve the properties of the vectors such that when these vectors are composed the properties are preserved. Moreover, the sentence \"this is computationally infeasible due to the vast number of unique chunks\" is not completely true as HRR have been used to represent trees in \"Distributed Tree Kernels\" by modifying the composition operation in a shuffled circular convolution. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your interest in our work and your kind words regarding the direction our paper takes . We summarize all the concerns raised and provide a point-by-point response below . ( 1 ) \u201c In this paper , it is not clear why random vectors have not been used \u201d We have two points to make regarding this comment . First , we did experiment on using fixed random basis embeddings , be it basis role embeddings or basis filler embeddings . This is denoted by models with names Fixed- * in Table 1 . This is also mentioned in Page 4 , right below Figure 1 , \u201c we also consider using fixed random vectors for basis embeddings \u201d . Second , if you are referring to using random vectors for not just bases , but also other trainable word-embedding related parameters ( such as s^w_i ) , we think it is better to treat them as learnable parameters since random vectors do not cluster together in a meaningful way that corresponds to natural language . ( 2 ) \u201c But , how can this regularization function preserve the properties of the vectors such that when these vectors are composed the properties are preserved \u201d We agree with this characterization . However , we want to remake the point we make in response to reviewer 3 ( point 1 , reproduced below ) \u201c ... in our case , the decomposed scoring function actually acts as an ( soft ) enforcer that makes sure decoding works properly . The loss would only go down when the predicted filler embedding ( after decoding ) is close to the original filler embedding ( before encoding ) . This is largely mediated by dot product -- the more accurate the decoding is , the bigger the value of dot product is. \u201d Although it is out our intention to design a theoretically complete model that preserves the properties all the way through , we do mean to take advantage of HRR properties , combined with black-box modeling from neural networks . We believe this is a reasonable approach to take in order to make our model viable in the world of deep learning . ( 3 ) \u201c Moreover , the sentence \u2018 this is computationally infeasible due to the vast number of unique chunks \u2019 is not completely true \u201d We meant to say that directly extending our word-level model to chunk-level is not plausible , because for word-level model , we designate a learnable vectorial parameter to each word type . By analogy , we would have to use a learnable vectorial parameter for each unique chunk type , which renders it intractable in our case . It is in this sense that we meant by saying \u201c this is computationally infeasible \u201d . Hopefully these responses address your concerns ."}, "2": {"review_id": "rkl3-hA5Y7-2", "review_text": " Summary: ======== Theis paper proposes a method for learning decomposable representations in the context of a language modeling task. Using holographic reduced representations (HRR), a word embedding is composed of a role and a filler. The embedding is then fed to an LSTM language model. There is also an extension to chunk-level representations. Experimentally, the model achieves perplexity comparable to a (weak) baseline LSTM model. The analysis of the learned representations shows a separation into syntactic and semantic roles. The paper targets an important problem, that of learning decomposable representations. As far as I know, it introduces a novel perspective using HRR and does so in the context of language modeling, which is a core NLP task. The analysis of the learned representations is quite interesting. I do have some concerns with regards to the quality of the language model, the clarity of some of the model description, and the validity of using HRR in this scenario. Please see detailed comments below. Comments: ========= 1. Section 2 refers to Plate (1995) for the conditions when the approximate decoding via correlation holds. I think it's important to mention these conditions and discuss whether they apply to the language modeling case. In particular, Plate mentions that the elements of each vector need to be iid with mean zero and variance 1/n (where n is the length of the vector). Is this true for the present case? Typically, word embeddings and LSTM states are do not exhibit this distribution. Are there other conditions that are (not) met? 2. Learning separate bases for different role-filler bindings is said to encourage the model to learn a decomposition of word representation. On the other hand, if I understand correctly, this means that word embeddings are not shared between roles, because s^w_i is also a role-specific vector (not just a word-specific vector). Is that a cause of concern? 3. It's not clear to me where in the overall model the next word is predicted. Figure 1b has an LSTM that predicts filler embeddings. Does this replace predicting the next word in a vanilla LSTM? Equation 5 still computes a word score. Is this used to compute the probability of the next word as in equation 2? 4. Comparison to other methods for composing words. Since much of the paper is concerned with composing words, it seem natural to compare the methods (and maybe some of the results) to methods for composing words. Some examples include [2] and the line of work on recursive neural networks by Socher et al., but there are many others. 5. Perplexity results: - The baseline results (100.5 ppl on PTB) are very weak for an LSTM. There are multiple papers showing that a simple LSTM can do much better. The heavily tuned LSTM of [1] gets 59.6 but even less tuned LSTMs go under 80 or 80 ppl. See some results in [1]. This raises a concern that the improvements from the HRR model may not be significant. Would they hold in a more competitive model? - Can you speculate or analyze in more detail why the chunk-level model doesn't perform well, and why adding more fillers doesn't help in this case? 6. Motivation: - The introduction claims that the dominant encoder-decoder paradigm learns \"transformations from many smaller comprising units to one complex emedding, and vice versa\". This claim should be qualified by the use of attention, where there is not a single complex embedding, rather a distribution over multiple embeddings. - Introduction, first paragraph, claims that \"such crude way of representing the structure is unsatisfactory, due to a lack of transparency, interpretability or transferability\" - what do you mean by these concepts and how exactly is the current approach limited with respect to them? Giving a bit more details about this point here or elsewhere in the paper would help motivate the work. 7. Section 3.3 was not so clear to me: - In step 1, what are these r_i^{chunk}? Should we assume that all chunks have the same role embeddings, despite them potentially being syntactically different? How do you determine where to split output vectors from the RNN to two parts? What is the motivation for doing this? - In prediction, how do you predict the next chunk embedding? Is there a different loss function for this? - Please provide more details on decoding, such as the mentioned annealing and regularization. - Finally, the reliance on a chunker is quite limiting. These may not be always available or of high quality. 8. The analysis in section 4.3 is very interesting and compelling. Figure 2 makes a good point. I would have liked to see more analysis along these lines. For example, more discussion of the word analogy results, including categories where HRR does not do better than the baseline. Also consider other analogy datasets that capture different aspects. 9. While I agree that automatic evaluation at chunk-level is challenging, I think more can be done. For instance, annotations in PTB can be used to automatically assign roles such as those in table 4, or others (there are plenty of annotations on PTB), and then to evaluate clustering along different annotations at a larger scale. 10. The introduction mentions a subset of the one billion word LM dataset (why a subset?), but then the rest of the papers evaluates only on PTB. Is this additional dataset used or not? 11. Introduction, first paragraph, last sentence: \"much previous work\" - please cite such relevant work on inducing disentangled representations. 12. Please improve the visibility of Figure 1. Some symbols are hard to see when printed. 13. More details on the regularization on basis embeddings (page 4) would be useful. 14. Section 3.3 says that each unique word token is assigned a vectorial parameter. Should this be word type? 15. Why not initialize the hidden state with the last state from the last batch? I understand that this is done to assure that the chunk-level models only consider intra-sentential information, but why is this desired? 16. Have you considered using more than two roles? I wonder how figure 2 would look in this case. Writing, grammar, etc.: ====================== - End of section 1: Our papers -> Our paper - Section 2: such approach -> such an approach; HRR use -> HRR uses; three operations -> three operations*:* - Section 3.1: \"the next token w_t\" - should this be w_{t+1)? - Section 3.2, decoding: remain -> remains - Section 3.3: work token -> word token - Section 4.1: word analogy task -> a word analogy task; number basis -> numbers of basis - Section 4.2: that the increasing -> that increasing - Section 4.3: no space before comma (first paragraph); on word analogy task -> on a word analogy task; belong -> belongs - Section 4.4: performed similar -> performed a similar; luster -> cluster - Section 5: these work -> these works/papers/studies; share common goal -> share a common goal; we makes -> we make; has been -> have been References ========== [1] Melis et al., On the State of the Art of Evaluation in Neural Language Models [2] Mitchell and Lapata, Vector-based Models of Semantic Composition ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your kind words and very detailed comments . Before delving into the detail , we would appreciate it if you could elaborate a bit more on the concern about \u201c the validity of using HRR in this scenario \u201d mentioned in the second paragraph ? ( a ) Did you mean using HRR in language modeling ? ( b ) If this is case , does it concern you because the choice of this task , or because of the inadequate baseline performance ? Thank you very much ! ( 1 ) \u201c the conditions when the approximate decoding via correlation holds \u201d Thanks for pointing this out . We will explain a bit more in our updated version . For our experiments though , only the models run with fixed basis embeddings are with mean zero and variance 1/n because they are randomly sampled and fixed throughout training . I agree that word embeddings and LSTM states do not typically exhibit such a distribution ( especially iid condition ) . However , we would also like to make two points . First , past work [ 1 ] that successfully uses HRR as associative memory , where these conditions are also not explicitly met ( or at least they didn \u2019 t show it ) . Second , in our case , the decomposed scoring function actually acts as an ( soft ) enforcer that makes sure decoding works properly . The loss would only go down when the predicted filler embedding ( after decoding ) is close to the original filler embedding ( before encoding ) . This is largely mediated by dot product -- the more accurate the decoding is , the bigger the value of dot product is . As for other conditions , it is also required that the dimensionality of the vector be sufficiently bigger than the number of stored items . This obviously holds in our case since we are only using a couple of variable bindings , and we will make it more clear in the updated version . ( 2 ) \u201c Learning separate bases for different role-filler bindings is said to encourage the model to learn a decomposition of word representation \u201d If we understand the question correctly , for each word , there are two ( equal to the number of roles ) filler embeddings , which have separate bases . These filler embeddings are then bound with their associated role embeddings . In this sense , base filler embeddings and role embeddings are shared across all words , but not between roles . Our earlier experiments showed that without separating these bases , decomposition of representations did not occur . We think it makes sense intuitively -- a decomposition of representation usually necessitates a separation of feature space . Does this address your concern ? ( 3 ) \u201c It 's not clear to me where in the overall model the next word is predicted \u201d We apologize for this confusion . The decoding module in 1 ( b ) corresponds to equation 5 . Instead of using one dot product as in a vanilla LSTM , we use the sum of two dot products , each of which is responsible for one role-filler binding . Indeed the score in equation 5 is used similarly as in equation 2 . We will make this more clear in the updated version . ( 4 ) \u201c Comparison to other methods for composing words \u201d If we understand it correctly , you are referring to the word-level model since this is where we spent most time entailing and analyzing . As we argued in the response to reviewer 2 ( point 4 , reproduced below ) , we do not find any directly comparable method to the best our knowledge . \u201c ... There are certainly many existing methods that try to incorporate structures , but mostly to enhance their representation , not decompose their representation . Moreover , the unsupervised nature of our approach makes direct comparison even harder. \u201d The cited work you provided [ 2 ] , and also Socher \u2019 s recursive network network deal with composing phrases from individual words , which does not concern the decomposition of word representation . Moreover , recursive neural networks need additional input such as parsed trees , which is definitely outside the scope of our paper . We would like to emphasize that the main contribution is about the decomposition/separation of representations . This decomposition , in HRR \u2019 s framework , is accompanied by the initial operation of encoding/composing . Due to space limit , we do not fully investigate the potential advantage/disadvantage of using HRR as an encoder ( compared to ( say ) Socher \u2019 s work ) , but rather spend most of the time using HRR to set up a model that can induce decomposition . Of course , we can be totally ignorant of other directly comparable methods . If you have any specific method in mind , we would really appreciate it if you can provide us some pointers ."}}