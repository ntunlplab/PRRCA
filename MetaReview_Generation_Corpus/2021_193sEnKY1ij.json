{"year": "2021", "forum": "193sEnKY1ij", "title": "No Cost Likelihood Manipulation at Test Time for Making Better Mistakes in Deep Networks", "decision": "Accept (Poster)", "meta_review": "The approach explore the use of Conditional Risk Minimization (CRM) as a post-hoc operation to amend a classifier decision by averaging a prior class hierarchy. The authors show that it is beneficial for ranking predictions without sacrifying top-1 accuracy.\n\nThe rebuttal period clarified some reviewers' concern on paper presentation and experiments, and all reviewers recommend acceptance after the discussion period.\n\nAlthough the approach is simple and directly revisits the use of CRM for deep models, the AC considers that the contribution is meaningful, and that the proposed method provides predictions with good ranking and calibration properties. The paper also sheds light into interesting issues in state-of-the-art methods integrating class hierarchies during training.\nThe AC therefore recommends paper acceptance.\n", "reviews": [{"review_id": "193sEnKY1ij-0", "review_text": "Summary : The authors propose a model to improve the output distribution of neural nets in image classification problems . Their model is a post hoc procedure and is based on the tree structure of WordNet . The model revises the classifier output based on the distance of the labels in the tree . Intuitively , their solution is to pick the candidate label that is located in the region of the tree with a higher accumulated probability mass value . They also experimentally show that the previous evaluation metrics are inconclusive . Pros : - The authors provide a different perspective on the evaluation procedure of the previous studies and experimentally show that it was incomplete . This is an important finding . - Their experiments are thorough . Cons : - The article lacks enough novelty : The problem has been investigated before . The solution is not novel . The WordNet tree structure has been extensively used in the information retrieval community before . - The article is not written well : There are informal vocabulary in the paper ( e.g. , \u201c something similar \u201d or \u201c grossly miscalibrated \u201d ) . There are also typos ( e.g. , see the paragraph before Theorem 1 ) . In Section 3 it is not formally stated that the tree structure is derived from WordNet ( the authors mention this in Abstract section ) . In Section 4 the baselines are not cited !", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their thoughtful feedback . > The article lacks enough novelty : The problem has been investigated before . The solution is not novel . We would like to highlight the following here : - As opposed to all recent literature in hierarchy-aware classification , our approach * does not require retraining * - Previous works often introduce algorithmic novelty by designing ad-hoc loss functions and retraining models on ImageNet with sensitive additional hyperparameters , which we demonstrate do not improve over the cross-entropy baseline in making better mistakes . - Our simple method beats all existing sophisticated methods with no retraining in ranking classes on hierarchical distance @ k by large margins while preserving far better calibration necessary for real-world use . - The novelty we introduce is not in the algorithm itself , but primarily in uncovering important shortcomings in literature , demonstrating a simple , classical approach which works effectively when combined with deep models and use its simplicity to provide insights into the problem ( Theorem 1 ) . - We believe our method can have a major real-world impact due to its simplicity and deployability with no additional training . We request the reviewer to reconsider and not dismiss the paper solely on this ground . > In Section 3 it is not formally stated that the tree structure is derived from WordNet The taxonomy need not be from WordNet -- approaches should work with any given hierarchy . We also demonstrate this in experiments with the large-scale dataset iNaturalist-19 which uses the biological taxonomy . We have added this in Section 3 for better clarity . > informal vocabulary , typos and missing citations . Thanks for pointing this out ! We have corrected informal vocabulary , added detail to make crisper statements , fixed all typos and missing citations issues in the updated draft . Hope it resolves this issue ."}, {"review_id": "193sEnKY1ij-1", "review_text": "This paper addresses the problem of hierarchy-aware classification , which utilizes a hierarchy to specify certain mistakes as being worse than others . They make two main contributions . First , they claim that the metric used in prior work , average mistake severity , is flawed because it rewards methods that make many `` easy '' mistakes , as opposed to fewer , `` harder '' mistakes . Based on this analysis , they claim that no prior methods actually improve over the simplest baseline of cross-entropy ( i.e.doing nothing ) . Second , they introduce their own method , an adaptation of the classical CRM framework . They argue that CRM : 1 ) Is the only method to improve over cross-entropy under average mistake severity ( the metric from prior work they argue is flawed ) 2 ) Beats all other metrics under their new , improved metric 3 ) Improves on the calibration of the predictions Strengths : The method is very simple and easy to understand , builds upon existing work , and I could probably implement it from scratch in 30 minutes on top of my existing models . While ML reviewers very frequently about a lack of complexity , I think this is a great strength . The methods section is very clearly written , and it was quite easy to quickly understand what the method is doing . Weaknesses : I am very concerned about the experiment sections . 1 ) To my understanding , Figure 2/Section 4.1 are factually incorrect . In particular , it appears that the soft-labels technique does essentially the same , or better than , CRM , across all fronts . In detail , a ) In Figure 2 ( a ) , the leftmost softlabel point is equal to or better than CRM ( and cross-entropy ) b ) Figure 2 ( b ) really concerns me , as it appears that the hyperparameters have been chosen to make a fairly narrow point - that it is possible to have low average hierarchical distance , and high top-1 error . I agree that that indicates a problem with the metric . However , the authors make the fair broader claim that CRM is the only method which beats cross-entropy , which I do not think is justified . Looking at Figure 4 in A.1 , it is readily apparent that choosing different hyperparameters for existing methods would yield similar error distributions to CRM . Having these plots in the appendix , combined with claims that the authors chose the best hyperparameters , feels a bit misleading . c ) As in 1 ) , soft labels is essentially on top of CRM and Cross entropy ( for iNaturalist19 , it looks like a higher beta value would be directly on top , it 's unclear why the authors did not extend the curve further ) 2 ) These results , at first blush , seem fairly impressive . For the leftmost plots , I am concerned that the authors are using subpar hyperparameters , similarly to 1 ) ( b ) above . Strangely , in this instance the results for other hyperparameters are not included in the appendix . The remaining experiments are fairly convincing , though . Reccomendation I do not think this paper can be accepted in its current form . While I suspect that CRM is a good method that I would like to use , some of the core arguments ( Figure 2/Section 4.1 ) in the paper appear to be fatally flawed . Smaller notes : - The paper could use an additional proofread , as there are often odd phrasings . I found the experiment section particularly hard to follow - The acronym HXE is never defined , or linked to a citation", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their feedback . We also thank the reviewer for finding simplicity as one of the major strengths of our approach ( CRM ) as it does not require retraining , hyperparameter cross-validation , and can be used with almost no computational overhead . The major questions/concerns raised by the reviewer primarily are related to Figure 2 , evaluation metric , and hyperparameters . Below we discuss them all in detail and hope to resolve all the concerns . We hope the reviewer makes an additional pass over the experiment section after resolving the confusion for better clarity . Figure 2 Performance : Note that the purpose of Figure 2 is to show how current metrics to evaluate mistake severity do not reflect the true nature of the models . We do this by highlighting that the current evaluation of mistake severity , which is evaluated _only_ over the incorrectly classified samples ( hence _different_ test sets for different models as the mistakes will be different ) , will favour models making additional _easy mistakes_ ( as it involves division by the number of mistakes ) which is undesirable . We propose an easy fix to correct this bias which involves evaluation of mistake severity on a _fixed_ test set ( not only on the misclassified ones ) , and , as expected , this simple fix changes the evaluation significantly and provides a reliable evaluation . Now , looking at Fig 2 ( a ) ( the old way of evaluating only on mistakes ) , it seems like softlabels with $ \\beta = 4 $ and HXE with $ \\alpha=0.6 $ are the best models with the lowest hierarchical distance @ 1 over mistakes ( average mistake severity ) as seen on the bottom right . However , as we correct the above said bias , those very models show the highest hierarchical distance @ 1 ( Fig 2 ( c ) top right ) . And to answer why this is the case , the mistake distribution in Fig 2 ( b ) clearly shows that this model is just making more easy mistakes while * not * improving at all the hierarchical mistakes over the entire test set . Hyperparameters : We * did not * cherry-pick hyperparameters $ \\alpha $ and $ \\beta $ in Fig 2 ( b ) . They are the best-performing ones ( refer [ 2 ] ) . In fact , we provided a proper analysis to show the impact of hyperparameter on the mistake distribution ( Figure 4 in A.1 ) . In the case of softlabels , as $ \\beta $ decreases , the model is supposed to become better aligned towards the hierarchy with a tradeoff with top1 accuracy , but in reality , performance simply gets worse . Note , with increasing values of $ \\beta $ , the softlabel method becomes similar to cross-entropy . Practically , beyond $ \\beta=50 $ its performance converges , remaining almost the same . To make it more evident , we added four additional points corresponding to $ \\beta $ = [ 50,75,100,200 ] in Figure 2 for iNaturalist19 . As expected , they all converge to a point ( refer Table2 in A.4 for exact values ) . Therefore , at high values of $ \\beta $ , softlabel is very similar to cross-entropy ; otherwise , it merely adds easy mistakes . Please note , this observation is for hierarchical distance @ 1 . As we have shown in Figure 3 , softlabel performs better than cross-entropy for ranking classes measured by distance @ k , but still worse than CRM . > For the leftmost plots ( in Fig 3 ) , I am concerned that the authors are using subpar hyperparameters . Regarding Figure 3 , for a fair comparison , we picked the hyperparameters for other models where their best results occurred . We have added results with all other hyperparameters in the appendix ( Fig 5 & 6 ) to justify this claim : We can observe that the best results mostly occur at $ \\beta=4 $ and $ \\alpha=0.6 $ . We hope this addresses the reviewer \u2019 s concern and shows that our method outperforms existing methods at their optimal parameters ( y-axis is in log-scale w.r.t classes ) both in terms of ranking classes in Fig 3 ( left ) and tradeoffs between hierarchical distance @ k and accuracy in Fig 3 ( centre and right ) . > The authors make the fair broader claim that CRM is the only method which beats cross-entropy , which I do not think is justified : We have rephrased the suggested line in Section 4.1 to better align with our intended claim ( as stated in abstract & introduction ) -- the existing methods do not practically improve over the cross-entropy baseline when considering the top-1 hierarchical distance . > Odd phrasings , missing citations . HXE acronym Thanks for pointing this out ! We have fixed these in the latest draft for ensuring better clarity for readers . [ 2 ] Bertinetto et al. , Making Better Mistakes : Leveraging Class Hierarchies with Deep Networks , CVPR20"}, {"review_id": "193sEnKY1ij-2", "review_text": "This paper proposes to use conditional risk minimization ( CRM ) for hierarchy aware classification . The proposed method simply amends mistakes using a cost matrix with the lowest common ancestor information . The method outperforms SOTA deep hierarchy-aware classifiers by large margins at ranking classes with little loss in classification accuracy . As the authors mentioned , CRM was already proposed several decades ago , so the novelty of this paper is limited . However , the paper does demonstrate the power of this old technique when equipped with modern deep learning tools . Also , the simplicity and intuition are much appreciated . I myself am not an expert in image classification . However , for text classification , recent studies ( e.g. , [ 1 ] ) shows that using binary cross-entropy ( i.e. , viewing a multi-label classification problem as L binary classification task , where L is the number of classes ) can achieve higher performance than multi-label cross-entropy . In this case , it is possible that more than one label can have p ( y|x ) > 0.5 . I wonder whether your approach is still effective for models using binary cross-entropy . It would also be better if the authors can show some experimental results on hierarchical text classification . [ 1 ] Liu et al.Deep Learning for Extreme Multi-label Text Classification . SIGIR'17 .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their positive feedback and are glad that they found our demonstration of CRM combined with modern deep learning tools impactful in its simplicity and intuition . > I wonder whether your approach is still effective for ( multi-label ) models using binary cross-entropy We believe it would not be trivial to directly apply CRM , or any recent label-hierarchy approach directly with binary cross-entropy since there is no straightforward way to combine L sigmoids into one so that different probabilities can be compared . We did not think about this particular problem as the community at large is mostly focused on single-label classification ; however , it definitely is an interesting question , and we would like to explore it properly in the future . Thank you for this proposal . > It would also be better if the authors can show some experimental results on hierarchical text classification . Could the reviewer point to a suitable hierarchical multi-class text classification datasets with an elaborate hierarchy ( > 3 depth ) ? We will experiment using cross-entropy and CRM corrections on them and do our best to post our findings if available before the closure of the discussion phase otherwise will add them in the draft ."}, {"review_id": "193sEnKY1ij-3", "review_text": "The paper addresses hierarchical classification , where the classes live in a hierarchy , and the cost of a mistake is the tree distance between the nodes . The paper tests the latest cool algorithms for hierarchical-aware loss functions , versus a very old idea : CRM . In CRM , you make your best estimate of the posterior probability of a class y given input x P ( y|x ) , and then you make a final decision based on minimizing the expected loss . There seems to be a belief that modifying the loss function to be hierarchy-aware is clearly better than doing boring old CRM . But there is not much evidence in favor of that hypothesis . This paper offers negative evidence for that hypothesis , with two experiments : 1 . By comparing hierarchical loss to top-1 loss with modified loss functions , there is a tradeoff , and there does not seem to be an advantage in using the modified loss function . 2.For the top-k case , using CRM clearly dominates the proposals for modifying the loss function . These support the use of CRM . I find this paper to be really nice -- I 'd far rather have a paper with good experiments with known algorithms , where I can learn something useful ; than a paper with a new algorithm with somewhat useless experiments . So I would argue for acceptance . One thing for the authors to think about : When they test the calibration of the modified loss functions , they find them to be poorly calibrated . This is not surprising , since the modified loss functions are not proper scoring rules . They attempt to calibrate by using a softmax with variable T. Would n't it make more sense to train exp ( alpha_i x + beta_i ) / \\sum_ { i=1 } ^N exp ( alpha_i x + beta_i ) ? that is , a gain and offset for all classes after the first one ?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for their positive feedback . > Would n't it make more sense to train exp ( alpha_i x + beta_i ) / \\sum_ { i=1 } ^N exp ( alpha_i x + beta_i ) instead of T ? We agree that there are many more sophisticated approaches to improve calibration ; however since our focus was to show the effectiveness of CRM . We used temperature scaling as it has only 1 degree of freedom and does not affect the ranking of the classes or accuracy . The suggested approach , which is similar to vector scaling [ 1 ] , and similarly more sophisticated ways to enforce calibration have an important issue : they are expected to significantly alter the weighting between different classes which might have an unintended effect on the hierarchical corrections made and impact their decisions -- hence we avoid them . We are nevertheless performing experiments as requested based on this suggestion and most likely post our findings within the next two days . [ 1 ] Guo et al.On Calibration of Modern Neural Networks , ICML17"}], "0": {"review_id": "193sEnKY1ij-0", "review_text": "Summary : The authors propose a model to improve the output distribution of neural nets in image classification problems . Their model is a post hoc procedure and is based on the tree structure of WordNet . The model revises the classifier output based on the distance of the labels in the tree . Intuitively , their solution is to pick the candidate label that is located in the region of the tree with a higher accumulated probability mass value . They also experimentally show that the previous evaluation metrics are inconclusive . Pros : - The authors provide a different perspective on the evaluation procedure of the previous studies and experimentally show that it was incomplete . This is an important finding . - Their experiments are thorough . Cons : - The article lacks enough novelty : The problem has been investigated before . The solution is not novel . The WordNet tree structure has been extensively used in the information retrieval community before . - The article is not written well : There are informal vocabulary in the paper ( e.g. , \u201c something similar \u201d or \u201c grossly miscalibrated \u201d ) . There are also typos ( e.g. , see the paragraph before Theorem 1 ) . In Section 3 it is not formally stated that the tree structure is derived from WordNet ( the authors mention this in Abstract section ) . In Section 4 the baselines are not cited !", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their thoughtful feedback . > The article lacks enough novelty : The problem has been investigated before . The solution is not novel . We would like to highlight the following here : - As opposed to all recent literature in hierarchy-aware classification , our approach * does not require retraining * - Previous works often introduce algorithmic novelty by designing ad-hoc loss functions and retraining models on ImageNet with sensitive additional hyperparameters , which we demonstrate do not improve over the cross-entropy baseline in making better mistakes . - Our simple method beats all existing sophisticated methods with no retraining in ranking classes on hierarchical distance @ k by large margins while preserving far better calibration necessary for real-world use . - The novelty we introduce is not in the algorithm itself , but primarily in uncovering important shortcomings in literature , demonstrating a simple , classical approach which works effectively when combined with deep models and use its simplicity to provide insights into the problem ( Theorem 1 ) . - We believe our method can have a major real-world impact due to its simplicity and deployability with no additional training . We request the reviewer to reconsider and not dismiss the paper solely on this ground . > In Section 3 it is not formally stated that the tree structure is derived from WordNet The taxonomy need not be from WordNet -- approaches should work with any given hierarchy . We also demonstrate this in experiments with the large-scale dataset iNaturalist-19 which uses the biological taxonomy . We have added this in Section 3 for better clarity . > informal vocabulary , typos and missing citations . Thanks for pointing this out ! We have corrected informal vocabulary , added detail to make crisper statements , fixed all typos and missing citations issues in the updated draft . Hope it resolves this issue ."}, "1": {"review_id": "193sEnKY1ij-1", "review_text": "This paper addresses the problem of hierarchy-aware classification , which utilizes a hierarchy to specify certain mistakes as being worse than others . They make two main contributions . First , they claim that the metric used in prior work , average mistake severity , is flawed because it rewards methods that make many `` easy '' mistakes , as opposed to fewer , `` harder '' mistakes . Based on this analysis , they claim that no prior methods actually improve over the simplest baseline of cross-entropy ( i.e.doing nothing ) . Second , they introduce their own method , an adaptation of the classical CRM framework . They argue that CRM : 1 ) Is the only method to improve over cross-entropy under average mistake severity ( the metric from prior work they argue is flawed ) 2 ) Beats all other metrics under their new , improved metric 3 ) Improves on the calibration of the predictions Strengths : The method is very simple and easy to understand , builds upon existing work , and I could probably implement it from scratch in 30 minutes on top of my existing models . While ML reviewers very frequently about a lack of complexity , I think this is a great strength . The methods section is very clearly written , and it was quite easy to quickly understand what the method is doing . Weaknesses : I am very concerned about the experiment sections . 1 ) To my understanding , Figure 2/Section 4.1 are factually incorrect . In particular , it appears that the soft-labels technique does essentially the same , or better than , CRM , across all fronts . In detail , a ) In Figure 2 ( a ) , the leftmost softlabel point is equal to or better than CRM ( and cross-entropy ) b ) Figure 2 ( b ) really concerns me , as it appears that the hyperparameters have been chosen to make a fairly narrow point - that it is possible to have low average hierarchical distance , and high top-1 error . I agree that that indicates a problem with the metric . However , the authors make the fair broader claim that CRM is the only method which beats cross-entropy , which I do not think is justified . Looking at Figure 4 in A.1 , it is readily apparent that choosing different hyperparameters for existing methods would yield similar error distributions to CRM . Having these plots in the appendix , combined with claims that the authors chose the best hyperparameters , feels a bit misleading . c ) As in 1 ) , soft labels is essentially on top of CRM and Cross entropy ( for iNaturalist19 , it looks like a higher beta value would be directly on top , it 's unclear why the authors did not extend the curve further ) 2 ) These results , at first blush , seem fairly impressive . For the leftmost plots , I am concerned that the authors are using subpar hyperparameters , similarly to 1 ) ( b ) above . Strangely , in this instance the results for other hyperparameters are not included in the appendix . The remaining experiments are fairly convincing , though . Reccomendation I do not think this paper can be accepted in its current form . While I suspect that CRM is a good method that I would like to use , some of the core arguments ( Figure 2/Section 4.1 ) in the paper appear to be fatally flawed . Smaller notes : - The paper could use an additional proofread , as there are often odd phrasings . I found the experiment section particularly hard to follow - The acronym HXE is never defined , or linked to a citation", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their feedback . We also thank the reviewer for finding simplicity as one of the major strengths of our approach ( CRM ) as it does not require retraining , hyperparameter cross-validation , and can be used with almost no computational overhead . The major questions/concerns raised by the reviewer primarily are related to Figure 2 , evaluation metric , and hyperparameters . Below we discuss them all in detail and hope to resolve all the concerns . We hope the reviewer makes an additional pass over the experiment section after resolving the confusion for better clarity . Figure 2 Performance : Note that the purpose of Figure 2 is to show how current metrics to evaluate mistake severity do not reflect the true nature of the models . We do this by highlighting that the current evaluation of mistake severity , which is evaluated _only_ over the incorrectly classified samples ( hence _different_ test sets for different models as the mistakes will be different ) , will favour models making additional _easy mistakes_ ( as it involves division by the number of mistakes ) which is undesirable . We propose an easy fix to correct this bias which involves evaluation of mistake severity on a _fixed_ test set ( not only on the misclassified ones ) , and , as expected , this simple fix changes the evaluation significantly and provides a reliable evaluation . Now , looking at Fig 2 ( a ) ( the old way of evaluating only on mistakes ) , it seems like softlabels with $ \\beta = 4 $ and HXE with $ \\alpha=0.6 $ are the best models with the lowest hierarchical distance @ 1 over mistakes ( average mistake severity ) as seen on the bottom right . However , as we correct the above said bias , those very models show the highest hierarchical distance @ 1 ( Fig 2 ( c ) top right ) . And to answer why this is the case , the mistake distribution in Fig 2 ( b ) clearly shows that this model is just making more easy mistakes while * not * improving at all the hierarchical mistakes over the entire test set . Hyperparameters : We * did not * cherry-pick hyperparameters $ \\alpha $ and $ \\beta $ in Fig 2 ( b ) . They are the best-performing ones ( refer [ 2 ] ) . In fact , we provided a proper analysis to show the impact of hyperparameter on the mistake distribution ( Figure 4 in A.1 ) . In the case of softlabels , as $ \\beta $ decreases , the model is supposed to become better aligned towards the hierarchy with a tradeoff with top1 accuracy , but in reality , performance simply gets worse . Note , with increasing values of $ \\beta $ , the softlabel method becomes similar to cross-entropy . Practically , beyond $ \\beta=50 $ its performance converges , remaining almost the same . To make it more evident , we added four additional points corresponding to $ \\beta $ = [ 50,75,100,200 ] in Figure 2 for iNaturalist19 . As expected , they all converge to a point ( refer Table2 in A.4 for exact values ) . Therefore , at high values of $ \\beta $ , softlabel is very similar to cross-entropy ; otherwise , it merely adds easy mistakes . Please note , this observation is for hierarchical distance @ 1 . As we have shown in Figure 3 , softlabel performs better than cross-entropy for ranking classes measured by distance @ k , but still worse than CRM . > For the leftmost plots ( in Fig 3 ) , I am concerned that the authors are using subpar hyperparameters . Regarding Figure 3 , for a fair comparison , we picked the hyperparameters for other models where their best results occurred . We have added results with all other hyperparameters in the appendix ( Fig 5 & 6 ) to justify this claim : We can observe that the best results mostly occur at $ \\beta=4 $ and $ \\alpha=0.6 $ . We hope this addresses the reviewer \u2019 s concern and shows that our method outperforms existing methods at their optimal parameters ( y-axis is in log-scale w.r.t classes ) both in terms of ranking classes in Fig 3 ( left ) and tradeoffs between hierarchical distance @ k and accuracy in Fig 3 ( centre and right ) . > The authors make the fair broader claim that CRM is the only method which beats cross-entropy , which I do not think is justified : We have rephrased the suggested line in Section 4.1 to better align with our intended claim ( as stated in abstract & introduction ) -- the existing methods do not practically improve over the cross-entropy baseline when considering the top-1 hierarchical distance . > Odd phrasings , missing citations . HXE acronym Thanks for pointing this out ! We have fixed these in the latest draft for ensuring better clarity for readers . [ 2 ] Bertinetto et al. , Making Better Mistakes : Leveraging Class Hierarchies with Deep Networks , CVPR20"}, "2": {"review_id": "193sEnKY1ij-2", "review_text": "This paper proposes to use conditional risk minimization ( CRM ) for hierarchy aware classification . The proposed method simply amends mistakes using a cost matrix with the lowest common ancestor information . The method outperforms SOTA deep hierarchy-aware classifiers by large margins at ranking classes with little loss in classification accuracy . As the authors mentioned , CRM was already proposed several decades ago , so the novelty of this paper is limited . However , the paper does demonstrate the power of this old technique when equipped with modern deep learning tools . Also , the simplicity and intuition are much appreciated . I myself am not an expert in image classification . However , for text classification , recent studies ( e.g. , [ 1 ] ) shows that using binary cross-entropy ( i.e. , viewing a multi-label classification problem as L binary classification task , where L is the number of classes ) can achieve higher performance than multi-label cross-entropy . In this case , it is possible that more than one label can have p ( y|x ) > 0.5 . I wonder whether your approach is still effective for models using binary cross-entropy . It would also be better if the authors can show some experimental results on hierarchical text classification . [ 1 ] Liu et al.Deep Learning for Extreme Multi-label Text Classification . SIGIR'17 .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their positive feedback and are glad that they found our demonstration of CRM combined with modern deep learning tools impactful in its simplicity and intuition . > I wonder whether your approach is still effective for ( multi-label ) models using binary cross-entropy We believe it would not be trivial to directly apply CRM , or any recent label-hierarchy approach directly with binary cross-entropy since there is no straightforward way to combine L sigmoids into one so that different probabilities can be compared . We did not think about this particular problem as the community at large is mostly focused on single-label classification ; however , it definitely is an interesting question , and we would like to explore it properly in the future . Thank you for this proposal . > It would also be better if the authors can show some experimental results on hierarchical text classification . Could the reviewer point to a suitable hierarchical multi-class text classification datasets with an elaborate hierarchy ( > 3 depth ) ? We will experiment using cross-entropy and CRM corrections on them and do our best to post our findings if available before the closure of the discussion phase otherwise will add them in the draft ."}, "3": {"review_id": "193sEnKY1ij-3", "review_text": "The paper addresses hierarchical classification , where the classes live in a hierarchy , and the cost of a mistake is the tree distance between the nodes . The paper tests the latest cool algorithms for hierarchical-aware loss functions , versus a very old idea : CRM . In CRM , you make your best estimate of the posterior probability of a class y given input x P ( y|x ) , and then you make a final decision based on minimizing the expected loss . There seems to be a belief that modifying the loss function to be hierarchy-aware is clearly better than doing boring old CRM . But there is not much evidence in favor of that hypothesis . This paper offers negative evidence for that hypothesis , with two experiments : 1 . By comparing hierarchical loss to top-1 loss with modified loss functions , there is a tradeoff , and there does not seem to be an advantage in using the modified loss function . 2.For the top-k case , using CRM clearly dominates the proposals for modifying the loss function . These support the use of CRM . I find this paper to be really nice -- I 'd far rather have a paper with good experiments with known algorithms , where I can learn something useful ; than a paper with a new algorithm with somewhat useless experiments . So I would argue for acceptance . One thing for the authors to think about : When they test the calibration of the modified loss functions , they find them to be poorly calibrated . This is not surprising , since the modified loss functions are not proper scoring rules . They attempt to calibrate by using a softmax with variable T. Would n't it make more sense to train exp ( alpha_i x + beta_i ) / \\sum_ { i=1 } ^N exp ( alpha_i x + beta_i ) ? that is , a gain and offset for all classes after the first one ?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for their positive feedback . > Would n't it make more sense to train exp ( alpha_i x + beta_i ) / \\sum_ { i=1 } ^N exp ( alpha_i x + beta_i ) instead of T ? We agree that there are many more sophisticated approaches to improve calibration ; however since our focus was to show the effectiveness of CRM . We used temperature scaling as it has only 1 degree of freedom and does not affect the ranking of the classes or accuracy . The suggested approach , which is similar to vector scaling [ 1 ] , and similarly more sophisticated ways to enforce calibration have an important issue : they are expected to significantly alter the weighting between different classes which might have an unintended effect on the hierarchical corrections made and impact their decisions -- hence we avoid them . We are nevertheless performing experiments as requested based on this suggestion and most likely post our findings within the next two days . [ 1 ] Guo et al.On Calibration of Modern Neural Networks , ICML17"}}