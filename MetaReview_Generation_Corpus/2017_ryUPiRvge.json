{"year": "2017", "forum": "ryUPiRvge", "title": "Extrapolation and learning equations", "decision": "Invite to Workshop Track", "meta_review": "This paper proposes using functions such as sin and cos as basis functions, then training a neural network with L1 regularization to obtain a simple estimate of functions that can extrapolate under some circumstances.\n \n Pros:\n - the paper has a wide-ranging discussion connecting extrapolation in regression problems to adjacent fields of system identification and causal learning.\n - the method is sensible enough, and should probably be a baseline in the time-series literature. It also seems like an advance on the hard-to-optimize Eureqa method.\n \n Cons:\n I agree with the authors that Reviewer 5's comments aren't very helpful, but this paper really does ignore or dismiss a lot of recent progress and related methods. Specifically:\n - The authors claim that cross-validation can't be used to choose the model, since it wouldn't encourage extrapolation - but why not partition the data in contiguous chunks, as is done in time-series methods?\n - The authors introduce an annealing trick to help with the L1 objective, but there is a rich literature on gradient-based optimization methods with L1 regularization that address exactly this problem.\n - The authors mostly consider toy data, limiting the potential impact of their method.\n - The authors don't compare against closely related methods developed to address the exact same setting. Namely, Schmit + Lipson's Eureqa method, and the Gaussian process methods of Duvenaud, Lloyd, Grosse, Tenenbaum and Ghahramani.\n - The authors invent their own ad-hoc model-selection procedure, again ignoring a massive literature.\n \n Given the many \"cons\", it is recommended that this paper not be presented at the conference track, but be featured at the workshop track.", "reviews": [{"review_id": "ryUPiRvge-0", "review_text": "Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc. Pros - The approach is rather simple and hence can be applied to existing methods. The major difference is incorporating functions with 2 or more inputs which was done successfully in the paper. - It seems that MLP, even though it is good for interpolation, it fails to extrapolate data to model the correct function. It was a great idea to use basis functions like sine, cosine to make the approach more explicit. Cons - Page 8, the claim that x2 cos(ax1 + b) ~ 1.21(cos(-ax1 + \u03c0 + b + 0.41x2) + sin(ax1 + b + 0.41x2)) for y in [-2,2] is not entirely correct. There should be some restrictions on 'a' and 'b' as well as the approximate equality doesn't hold for all real values of 'a' and 'b'. Although, for a=2*pi and b=pi/4, the claim is correct so the model is predicting a correct solution within certain limits. - Most of the experiments involve up to 4 variables. It would be interesting to see how the neural approach models hundreds of variables. - Another way of looking at the model is that the non-linearities like sine, cosine, multiplication act as basis functions. If the data is a linear combination of such functions, the model will be able to learn the weights. As division is not one of the non-linearities, predicting expressions in Equation 13 seems unlikely. Hence, I was wondering, is it possible to make sure that this architecture is a universal approximator. Suggested Edits - Page 8, It seems that there is a typographical error in the expression 1.21(cos(ax1 + \u03c0 + b + 0.41x2) + sin(ax1 + b + 0.41x2)). When compared with the predicted formula in Figure 4(b), it should be 1.21(cos(-ax1 + \u03c0 + b + 0.41x2) + sin(ax1 + b + 0.41x2)). ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for his comments . Page 8 : thanks for pointing this out . The approximation only holds for b=Pi/4 . We change the manuscript accordingly . Typo : yes indeed the sign was missing . Only 4 variables : True , we only considered cases with up to 5 variables ( robotic arm ) . We did not try it on systems with many more variables and we expect that lots of training data would be required to be able to uncover the true functional form in case all variables interact . So interpolation will scale but extrapolation might not , at least not to hundreds . Base functions , and universal approximator : It is correct that sine , cosine , and multiplication are the new base functions , however , the model is not just considering a linear combination of those , but also their functional composition ( sine of cosine times \u2026 ) . Since division is not a base function Eq.13 can not be represented exactly . In this case , EQL will just act as a normal approximator and performs well in the interpolation regime but fails on extrapolation like all other methods . EQL is surely a universal approximator because MLPs are a subclass of EQL ."}, {"review_id": "ryUPiRvge-1", "review_text": "The authors attempt to extract analytical equations governing physical systems from observations - an important task. Being able to capture succinct and interpretable rules which a physical system follows is of great importance. However, the authors do this with simple and naive tools which will not scale to complex tasks, offering no new insights or advances to the field. The contribution of the paper (and the first four pages of the submission!) can be summarised in one sentence: \"Learn the weights of a small network with cosine, sinusoid, and input elements products activation functions s.t. the weights are sparse (L1)\". The learnt network weights with its fixed structure are then presented as the learnt equation. This research uses tools from literature from the '90s (I haven't seen the abbreviation ANN (page 3) for a long time) and does not build on modern techniques which have advanced a lot since then. I would encourage the authors to review modern literature and continue working on this important task.", "rating": "3: Clear rejection", "reply_text": "We find of the criticism in this review mostly unjustified . A scientific review should be based on scientific contents . Citing Smith \u2019 s \u201c Task of the Referee \u201d , we would ask to be judged on questions such as \u201c Is the goal of the paper significant ? \u201d , \u201c Is the method of approach valid ? \u201d , \u201c Is the execution of research correct ? \u201d , \u201c Are the correct conclusion being drawn ? \u201d , etc . We believe the answer to all of these is yes , but we can accept if reviewers disagree . We do not find a statement \u201c This research uses tools from literature from the '90s [ ... ] and does not build on modern techniques which have advanced a lot since then \u201d appropriate without indicating which \u2018 modern \u2019 techniques are meant and how or why they would be more appropriate for the task of learning physical equations . Despite the above , we would like to justify our choice of methods . We use the tools that we find are suitable for the problem , regardless when they were invented . In particular , EQL relies on a differentiable artificial neural network , so why call it something else ? We do not use ReLUs or lots of layers , not because we would n't be aware of them , but because these would not represent the function class we are interested in . Note that , in contrast to e.g.image classification or generation , we are not trying to learn a black box but an analytic expression . Similar to many of the current Deep Learning works , who are based on methods from the 1990s with today \u2019 s computation power and data resources , we do apply recent techniques where appropriate , e.g.the Adam optimizer , which is from 2014 . Also , our model selection mechanism is novel and essential for the extrapolation task . In summary : please judge the manuscript based on a ) if it tackles an important task , b ) if it achieves in solving the task , and c ) if it enables other researchers to do the same . We believe the answer to all three questions is yes ."}, {"review_id": "ryUPiRvge-2", "review_text": "Thank you for an interesting read. To my knowledge, very few papers have looked at transfer learning with **no** target domain data (the authors called this task as \"extrapolation\"). This paper clearly shows that the knowledge of the underlying system dynamics is crucial in this case. The experiments clearly showed the promising potential of the proposed EQL model. I think EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains. Quesions and comments: 1. Multiplication units. By the universal approximation theorem, multiplication can also be represented by a neural network in the usual sense. I agree with the authors' explanation of interpolation and extrapolation, but I still don't quite understand why multiplication unit is crucial here. I guess is it because this representation generalises better when training data is not that representative for the future? 2. Fitting an EQL vs. fitting a polynomial. It seems to me that the number of layers in EQL has some connections to the degree of the polynomial. Assume we know the underlying dynamics we want to learn can be represented by a polynomial. Then what's the difference between fitting a polynomial (with model selection techniques to determine the degree) and fitting an EQL (with model selection techniques to determine the number of layers)? Also your experiments showed that the selection of basis functions (specific to the underlying dynamics you want to learn) is crucial for the performance. This means you need to have some prior knowledge on the form of the equation anyway! 3. Ben-David et al. 2010 has presented some error bounds for the hypothesis that is trained on source data but tested on the target data. I wonder if your EQL model can achieve better error bounds? 4. Can you comment on the comparison of your method to those who modelled the extrapolation data with **uncertainty**?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for his comments . To 1 : The reason for relying on multiplication units is twofold : first , it is needed for model selection according to Occam \u2019 s Razor . We select for models with a small number of active units . For \u201c simulating \u201d multiplication with additive units a large amount of units are required with would favor instances with a different representation . The second reason is that the learned multiplication would only work in the training domain and would to extrapolate . To 2 : We need prior knowledge on the function class of the underlying dynamics . Again a generic polynomial is a universal approximator and can also be used for interpolation . True , EQL can also learn polynomials up to order ( 2 * number of layers ) , BUT it also contains the trigonometric functions and their powers and products . The exhaustive search through all functional forms is quickly intractable . In order to extrapolate the underlying functional expression has to be identified . Simply fitting a polynomial will not do ( and is likely to extrapolate badly ) . To 3 : Thank you for the interesting view of interpreting extrapolation as domain adaptation with no data of the target task . Unfortunately , we do not think that the situation will lead to provable bounds , at least not comparable ones to Ben-David 's , except if we make assumptions about the underlying data distribution . This we would like to avoid , especially in the context of modeling a physical system , as we do , where future data ( conditions ) will not even be i.i.d .. Note that even if we would have some ( unlabeled ) target data , Ben-David 's bound would not be informative : by definition the support of the training data is disjoint from the data distribution during extrapolation , so the discrepancy terms in the bound would almost surely be maximal , and the bound vacuous . To 4 : We assume you mean methods such as Gaussian Processes . For the GP : Constructing a kernel that can express our hypothesis class of functions would be very difficult , since it 's not just linear combinations of base elements , but also their concatenations . Just products of inputs are not hard ( polynomial kernel ) , but products of intermediate terms would , require some hierarchical construct , with kernel acting on the output of other kernels . If we are not using our function class but general kernels then both mean extrapolation performance and \u201c uncertainty \u201d will grow very quickly when departing from the training domain ."}], "0": {"review_id": "ryUPiRvge-0", "review_text": "Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc. Pros - The approach is rather simple and hence can be applied to existing methods. The major difference is incorporating functions with 2 or more inputs which was done successfully in the paper. - It seems that MLP, even though it is good for interpolation, it fails to extrapolate data to model the correct function. It was a great idea to use basis functions like sine, cosine to make the approach more explicit. Cons - Page 8, the claim that x2 cos(ax1 + b) ~ 1.21(cos(-ax1 + \u03c0 + b + 0.41x2) + sin(ax1 + b + 0.41x2)) for y in [-2,2] is not entirely correct. There should be some restrictions on 'a' and 'b' as well as the approximate equality doesn't hold for all real values of 'a' and 'b'. Although, for a=2*pi and b=pi/4, the claim is correct so the model is predicting a correct solution within certain limits. - Most of the experiments involve up to 4 variables. It would be interesting to see how the neural approach models hundreds of variables. - Another way of looking at the model is that the non-linearities like sine, cosine, multiplication act as basis functions. If the data is a linear combination of such functions, the model will be able to learn the weights. As division is not one of the non-linearities, predicting expressions in Equation 13 seems unlikely. Hence, I was wondering, is it possible to make sure that this architecture is a universal approximator. Suggested Edits - Page 8, It seems that there is a typographical error in the expression 1.21(cos(ax1 + \u03c0 + b + 0.41x2) + sin(ax1 + b + 0.41x2)). When compared with the predicted formula in Figure 4(b), it should be 1.21(cos(-ax1 + \u03c0 + b + 0.41x2) + sin(ax1 + b + 0.41x2)). ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for his comments . Page 8 : thanks for pointing this out . The approximation only holds for b=Pi/4 . We change the manuscript accordingly . Typo : yes indeed the sign was missing . Only 4 variables : True , we only considered cases with up to 5 variables ( robotic arm ) . We did not try it on systems with many more variables and we expect that lots of training data would be required to be able to uncover the true functional form in case all variables interact . So interpolation will scale but extrapolation might not , at least not to hundreds . Base functions , and universal approximator : It is correct that sine , cosine , and multiplication are the new base functions , however , the model is not just considering a linear combination of those , but also their functional composition ( sine of cosine times \u2026 ) . Since division is not a base function Eq.13 can not be represented exactly . In this case , EQL will just act as a normal approximator and performs well in the interpolation regime but fails on extrapolation like all other methods . EQL is surely a universal approximator because MLPs are a subclass of EQL ."}, "1": {"review_id": "ryUPiRvge-1", "review_text": "The authors attempt to extract analytical equations governing physical systems from observations - an important task. Being able to capture succinct and interpretable rules which a physical system follows is of great importance. However, the authors do this with simple and naive tools which will not scale to complex tasks, offering no new insights or advances to the field. The contribution of the paper (and the first four pages of the submission!) can be summarised in one sentence: \"Learn the weights of a small network with cosine, sinusoid, and input elements products activation functions s.t. the weights are sparse (L1)\". The learnt network weights with its fixed structure are then presented as the learnt equation. This research uses tools from literature from the '90s (I haven't seen the abbreviation ANN (page 3) for a long time) and does not build on modern techniques which have advanced a lot since then. I would encourage the authors to review modern literature and continue working on this important task.", "rating": "3: Clear rejection", "reply_text": "We find of the criticism in this review mostly unjustified . A scientific review should be based on scientific contents . Citing Smith \u2019 s \u201c Task of the Referee \u201d , we would ask to be judged on questions such as \u201c Is the goal of the paper significant ? \u201d , \u201c Is the method of approach valid ? \u201d , \u201c Is the execution of research correct ? \u201d , \u201c Are the correct conclusion being drawn ? \u201d , etc . We believe the answer to all of these is yes , but we can accept if reviewers disagree . We do not find a statement \u201c This research uses tools from literature from the '90s [ ... ] and does not build on modern techniques which have advanced a lot since then \u201d appropriate without indicating which \u2018 modern \u2019 techniques are meant and how or why they would be more appropriate for the task of learning physical equations . Despite the above , we would like to justify our choice of methods . We use the tools that we find are suitable for the problem , regardless when they were invented . In particular , EQL relies on a differentiable artificial neural network , so why call it something else ? We do not use ReLUs or lots of layers , not because we would n't be aware of them , but because these would not represent the function class we are interested in . Note that , in contrast to e.g.image classification or generation , we are not trying to learn a black box but an analytic expression . Similar to many of the current Deep Learning works , who are based on methods from the 1990s with today \u2019 s computation power and data resources , we do apply recent techniques where appropriate , e.g.the Adam optimizer , which is from 2014 . Also , our model selection mechanism is novel and essential for the extrapolation task . In summary : please judge the manuscript based on a ) if it tackles an important task , b ) if it achieves in solving the task , and c ) if it enables other researchers to do the same . We believe the answer to all three questions is yes ."}, "2": {"review_id": "ryUPiRvge-2", "review_text": "Thank you for an interesting read. To my knowledge, very few papers have looked at transfer learning with **no** target domain data (the authors called this task as \"extrapolation\"). This paper clearly shows that the knowledge of the underlying system dynamics is crucial in this case. The experiments clearly showed the promising potential of the proposed EQL model. I think EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains. Quesions and comments: 1. Multiplication units. By the universal approximation theorem, multiplication can also be represented by a neural network in the usual sense. I agree with the authors' explanation of interpolation and extrapolation, but I still don't quite understand why multiplication unit is crucial here. I guess is it because this representation generalises better when training data is not that representative for the future? 2. Fitting an EQL vs. fitting a polynomial. It seems to me that the number of layers in EQL has some connections to the degree of the polynomial. Assume we know the underlying dynamics we want to learn can be represented by a polynomial. Then what's the difference between fitting a polynomial (with model selection techniques to determine the degree) and fitting an EQL (with model selection techniques to determine the number of layers)? Also your experiments showed that the selection of basis functions (specific to the underlying dynamics you want to learn) is crucial for the performance. This means you need to have some prior knowledge on the form of the equation anyway! 3. Ben-David et al. 2010 has presented some error bounds for the hypothesis that is trained on source data but tested on the target data. I wonder if your EQL model can achieve better error bounds? 4. Can you comment on the comparison of your method to those who modelled the extrapolation data with **uncertainty**?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for his comments . To 1 : The reason for relying on multiplication units is twofold : first , it is needed for model selection according to Occam \u2019 s Razor . We select for models with a small number of active units . For \u201c simulating \u201d multiplication with additive units a large amount of units are required with would favor instances with a different representation . The second reason is that the learned multiplication would only work in the training domain and would to extrapolate . To 2 : We need prior knowledge on the function class of the underlying dynamics . Again a generic polynomial is a universal approximator and can also be used for interpolation . True , EQL can also learn polynomials up to order ( 2 * number of layers ) , BUT it also contains the trigonometric functions and their powers and products . The exhaustive search through all functional forms is quickly intractable . In order to extrapolate the underlying functional expression has to be identified . Simply fitting a polynomial will not do ( and is likely to extrapolate badly ) . To 3 : Thank you for the interesting view of interpreting extrapolation as domain adaptation with no data of the target task . Unfortunately , we do not think that the situation will lead to provable bounds , at least not comparable ones to Ben-David 's , except if we make assumptions about the underlying data distribution . This we would like to avoid , especially in the context of modeling a physical system , as we do , where future data ( conditions ) will not even be i.i.d .. Note that even if we would have some ( unlabeled ) target data , Ben-David 's bound would not be informative : by definition the support of the training data is disjoint from the data distribution during extrapolation , so the discrepancy terms in the bound would almost surely be maximal , and the bound vacuous . To 4 : We assume you mean methods such as Gaussian Processes . For the GP : Constructing a kernel that can express our hypothesis class of functions would be very difficult , since it 's not just linear combinations of base elements , but also their concatenations . Just products of inputs are not hard ( polynomial kernel ) , but products of intermediate terms would , require some hierarchical construct , with kernel acting on the output of other kernels . If we are not using our function class but general kernels then both mean extrapolation performance and \u201c uncertainty \u201d will grow very quickly when departing from the training domain ."}}