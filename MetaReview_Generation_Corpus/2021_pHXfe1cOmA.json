{"year": "2021", "forum": "pHXfe1cOmA", "title": "HyperDynamics: Meta-Learning Object and Agent Dynamics with Hypernetworks", "decision": "Accept (Poster)", "meta_review": "This paper proposes \"HyperDynamics\" a framework that takes into account the history of an agents recent interactions with the environment to predict physical parameters such as mass and friction. These parameters are fed into a forward dynamics model, represented as a neural network, that is used for control.\n\nPros:\n- addresses an important problem (adapting dynamics models to \"new\" environments) and provides strong baselines\n- well written and authors have improved clarity even further based on reviewers comments\n\nCons:\n- I agree with the reviewer that it is currently unclear how well this will transfer to the real world\n- The idea of predicting physical parameters from a history of environment interactions is not not novel in itself (although the proposed framework is, as far as I know). The authors should include related work along the lines of (1) (this is just one paper that comes to mind, others exist)\n\n(1) Preparing for the Unknown: Learning a Universal Policy with Online System Identification", "reviews": [{"review_id": "pHXfe1cOmA-0", "review_text": "# # Summary The authors present `` HyperDynamics '' , a novel method for system-identification and learning of flexible forward models that can be used in planning tasks . The presented methods is generic and is shown on both locomotion and pushing tasks with different simulated robots . I enjoyed reading this work a lot and I hope it gets accepted . It 's a clever idea and most flaws that I 'm about to point out are easily addressable by the authors . # # Strengths & Weaknesses # # # # Strengths 1 ) The method is generic ( shown to work across tasks and environments ) . 2 ) The baselines are strong . When I started reading your paper , I thought that DensePhysNet and some form of MAML would be good candidates for this to compare again and it turns out these were indeed included . 3 ) Figure 1 + caption as well as the introduction to section 3 do a great job at introducing the architecture in a way that would allow the reader to create a basic implementation . 4 ) Code was included . I did n't run it but it 's clean and seems functional from what I can tell . # # # # Weaknesses 1 ) You really , really need to be more clear in the main paper on the implementation details . You ca n't move the amount of training data to the appendix and it 's not good practice to only include the network architecture by name in the main paper . And what are all your losses ? You make the method looks super simple but then you train on shapenet , some 2D reconstruction , something about cropping , and there 's a GRU in there too ( full backprop vs truncated backprop ? ) . The appendix shines a _little_ light on this but you need to be way more specific in the main paper . That has to stand on its own . 2 ) You do n't motivate all the nitty-gritty implementation choices . Why did you add the decoder ? What 's the performance if you remove it ? What about the cropping ? What if you do n't do an object-centric feature map , but instead a few CNN layers ? What are the individual contributions of all these details ? 3 ) DensePhysNet , Visual Foresight , and many other works in this domain use ( simple ) real-robot experiments to demonstrate that their method can handle realistic robot noise . Obviously there 's a global pandemic happening at the moment , so I wo n't require you adding this for the rebuttal , but I think in order to really establish this method ( maybe before putting it on Arxiv ) , you 'd have to add some real-robot experiments . This can be as simple as a 180USD RealSense and a 500USD robot arm plus a few objects and a playfield . It 's become a standard for system-identification-style works and it 's justified in my opinion since your method is n't inherently useful in simulation where the user has access to all the information and can arbitrarily reset/reposition the model . And since you do n't have ShapeNet data for many real-world objects ( which you seem to need for pretraining ) , could you at least add a sentence or two detailing how this would transfer to real-world problems ? * * TL ; DR my main requests : * * ( 2 ) Motivate implementation details ( add ablations if you have any ) and ( 1 ) be more explicit about them in the main part of the paper . # # Impact & Recommendation Despite that there seem to be a lot of hacks that make this method in the specific settings , I think the general idea behind it is sound . And I think the authors show that it performs better than the sota , at least in simulation . Therefore I 'd recommend acceptance given that the authors add the requested information . In its current shape , it 's a 6 for me but if my main concerns are addressed , I 'm happy to up this to a 7 or if major improvements are made and my questions below are answered , to an 8 . # # Questions , Nitpicks , Comments - Kudos for not making another acronym method - There are a lot of typos and orthographic errors , would recommend a spell-checked or getting this proofed . Examples : section 2 `` poinclouds '' , section 2 `` properties in hand '' - > `` properties at hand '' - Maybe start the introduction with an example , e.g.how children are able to chew on a block of wood to assess its hardness and then build towers with it . - * * important * * Introduction : when you go over ( i-iv ) , that feels a bit too long and lit-reviewy and misplaced in the introduction . I would recommend the following changes : ( a ) trim this severely , only mention that there are model-based methods that usually do only one environment and there 's meta-learning and how your method is more adaptable than either , ( b ) move this into the literature section , where you have to come back to it anyway , ( c ) move the Hypernetworks section from the literature into a separate `` Background '' section and develop it a bit further , since it 's less `` competing method '' and more `` you should know about this to understand out method '' . - Also in the introduction , you present ( i-iv ) , and you mention how your method is better/different than ( i-iii ) but you never address ( iv ) . - It 's become a standard to summarize the contributions again at the end of the introduction , ideally as bullet points . Please add these . - In equation ( 1 ) , why is the ordering O-T-N for the sums ? I feel like ONT would be more natural , no ? - When reading the method , my main question was if the method would work on `` dense '' trajectories or on before-and-after photos like DensePhysNet . This is only answered a few pages later but I think this belongs in 3-Overview or 3.1 . Just to be clear , you 're gathering trajectories of length 4s , i.e.5 frames of 800ms where you do NOT retract the robot arm when pushing , right ? ( Compared to DensePhysNet , where the arm is never visible because they take photos before and after complete standstill ) . If that 's the case , how do you deal with occlusion from the arm ? - Do you encourage object-object interactions in any way or do they just occur randomly ? Or do you only ever experiment with single objects ? - The object orientation vs state section is n't super clear ? You 're subtracting an object 's absolute starting position+orientation from it 's future trajectory points ? - In 3.2 : Why a GRU , why not LSTM ? Why k=16 ( and similarly why k=5 ) ... This ties into the main criticism from above . Please motivate your choices . - In 4.1 : I think this is a typo , but it says you added beds to your experiment table . I think they 'd be a bit too large , no ? : D - 4.1 : specify the random mass+friction range , please ! - 4.1 : same with the total amount of training data/frames - And since you wo n't have ShapeNet - 4.2 : I think it 's a half-cheetah , not a cheetah . - 4.2 : I do n't understand why it 's unrealistic to assume arbitrary resetting in simulation . That 's one of the benefits of running simulations and common practice . - 5 : What do you mean `` predicting both the structure and parameters of the target dynamics model '' ? Parameters is clear ( mass , friction , etc . ) but what 's the structure here ?", "rating": "7: Good paper, accept", "reply_text": "We are really encouraged that you liked our idea and agreed with our baseline choices , and we really appreciate your comments and suggestions with so much details ! We hope to address all of your concerns below . ( 1 ) Lack of implementation details in main paper . Thanks for raising this issue . We added more detailed description into the \u2018 baseline \u2019 paragraphs , and also added a paragraph ( Implementation Details ) after the baseline paragraphs in both Section 4.1 and 4.2 . We have moved the architecture details of our model from the appendix to there . We also added more details of the baselines ( architecture , training , etc . ) and the amount of training data in these sections . To briefly summarize here , MB-MAML takes the exact same input as our model does , except that it uses interaction data for online model adaptation . The Direct and Recurrent baselines use the same input and use the same visual and interaction encoders as our model does in their corresponding tasks . All the dynamics model used in Direct , Recurrent , XYZ , MB-MAML and Expert-Ens use the same architecture . VF and DensePhysNet are implemented using the architectures described in their corresponding papers . The GRU is trained with full backpropagation and we have added this to the \u2018 Implementation Details \u2019 section too . Regarding the training of the visual encoder ( object detection , view prediction , losses , etc . ) , we have added a detailed description in Appendix A.1 . Since it \u2019 s a bit lengthy and it \u2019 s built mostly based on GRNNs , which is not the main contribution of this work , we think it \u2019 s reasonable to put it in Appendix , and referred to it in relevant places in the main paper . ( 2 ) Details of implementation choices Thanks for bringing this up ! The decoder is crucial since it helps regularize the training of the visual encoder and makes sure the produced z_vis captures sufficient shape information . The cropping step is also important to produce the object-centric feature map , and drives the model \u2019 s attention to the object under pushing . In order to help readers better understand these components , we have added an ablation study in Appendix B.1 . We hope the results there could explain our implementation choices ."}, {"review_id": "pHXfe1cOmA-1", "review_text": "== Update == Thank you for your detailed response . The newly added clarifications and sanity checks have greatly improved the quality of the paper , and I am therefore increasing my score from 4 to 6 . I believe the model capacity comparison ( Table 6 ) is especially important for demonstrating the value of the new architecture , and would recommend mentioning that result in the main paper . == Original Review == The paper proposes a model for predicting the dynamics of a physical system based on hypernetworks : given some observed interactions and some visual input , the hypernetwork outputs the parameters of a dynamics model , which then predicts the evolution of the system 's state over time . Experiments are conducted on an object pushing and a locomotion task . Strengths : 1 . The paper addresses an important question , namely , how a dynamics model may adapt to environments that do n't fully match its training distribution . 2.The proposed use of a hypernetwork is plausible and novel to my knowledge . 3.The related work section appears comprehensive , and , to my knowledge , does not miss any major prior work . Weaknesses : 1 . The main claim of the paper is that hyperdynamics network offers better prediction accuracy and generalization than a standard dynamics model . I feel like the evaluation of this question is confounded by the choice of tasks and baselines . On the pushing benchmark , the XYZ , VF , and DensePhysNet operate on different modalities than HyperDynamics ( either no state information or no visual information ) , and are therefore difficult to compare . For the MB-MAML baseline , this is not specified . The Expert-Ens model can not be expected to generalize , since it is designed to overfit on individual objects . As a result , only the 'Direct ' baseline clearly operates in the same experimental regime as HyperDynamics . However , nothing is reported on the model architecture or the training method for that baseline , raising the question if its model capacity was competitive . My impression is that this experimental design blurs the effects of ( a ) using side-information to infer system properties , and ( b ) utilizing such information through a hypernetwork as opposed to a standard dynamics predictor . If the goal is to evaluate the new architecture , these should be disentangled . 2.On the locomotion benchmark , the Recurrent baseline is similarly unclear . Sanchez-Gonzalez et al.is cited , but that paper focusses on comparing recurrent models based on graph networks to those based on MLPs , and it is unclear which model was used . 3.No results are reported for the prediction accuracy on the locomotion task , which would have helped evaluate the performance of the dynamics models more directly than the task scores . 4.Many of these issues could have been avoided by testing on established benchmarks from the literature , for which results are available . If there is a simulator available , generalization ability could still have been tested by varying the physical constants of the dataset . 5.The paper contains a decent amount of typos and grammatical errors . Overall , while the paper presents an interesting idea , the experimental evaluation is not convincing in its current state : Baseline architectures are not fully specified , many of them did not receive the same input , and no benchmark task with previously reported results has been used . As a result , I recommend rejection at this time . Questions : 1 . In eq.1 , should omega be a parameter of H ( . ) instead of F ( . ) ? 2.Section 3.1 introduces the `` 1-dimensional code $ z_ { int } \\in \\mathbb { R } ^2 $ '' . So is it one or two-dimensional ? 3.Overall , the dimensionality of the latent codes and hidden layers seems incredibly small , e.g. , only 1/2 numbers to encode prior interactions , and 8 to encode shape . Is there really no benefit to using higher capacity models ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the detailed comments and criticisms ! We feel very encouraged that you found the question we study important and our approach novel . We hope to address all your concerns below . ( 1 ) Design and details of the baselines . We apologize for missing important details of the baselines and we really appreciate you for raising this issue . Indeed , your description of the purpose of each baseline matches closely to our intention , and we would like to explain them in further details here : - The XYZ baseline is more like an ablation where we want to evaluate how a standard dynamics model performs without having access to the visual features and physics properties . - You are absolutely right that VF and DensePhysNet operate on different modalities than our method . We included them in the paper because we believe it would be good to compare our method with some image-based methods in the literature . We think this makes the comparison more complete . - MB-MAML uses the exact same architecture as the generated dynamics model of HyperDynamics , it also uses the exact same input data . The only difference is that it updates the model parameters online using the interaction data . - Expert-Ens is included because we want to evaluate if our model generate good expert online that can match the performance of these separately trained ones . Each expert in Expert-Ens uses the exact same architecture as the generated model of HyperDynamics . - Direct : you are right that it operates in the same experimental regime as ours . It uses the exact same input as ours ( both side information and system state ) . It uses the same visual encoder and interaction encoder as ours , and feed the concatenated code z into a dynamics model , which follows the exact same architecture as the generated dynamics model of HyperDynamics . We believe this is a clear comparison since it clearly shows adding a hypernetwork to process such system properties could help improve performance . Again , we are sorry for not explaining these details clearly in the main paper . We have added more details into the baseline description section in the paper . We also added an \u2018 Implementation Details \u2019 paragraph after the baseline paragraph . It contains the architecture details that were in the appendix , and we also added some more details there to describe the architecture of the baselines . We also appreciate that you brought up the issue regarding model capacity . Indeed , although the dynamics model in Direct uses the same architecture as the the generated one in HyperDynamics , it needs to process more information than our generated model , since it also handles the latent code z , while in HyperDynamics , z is fed into the hypernetwork . We believe a more fair comparison on model capacity should be between the dynamics model in Direct and the hypernetwork in HyperDynamics ( the last layer of the hypernetwork outputs the weights of the generated dynamics model ) . Considering that the hypernetwork is a fully-connected network with a 16-unit hidden layer , effectively it has around 16x more parameters than the Direct baseline . However , in our experiments we found that further increasing the model capacity of Direct does not help further improve its performance , and would result in either overfitting or underfitting . We apologize for not including such comparison in the original paper . We have now added an analysis on model capacity in Appendix B.2 . We hope this shows that our architecture is a more effective and structured way to handle system properties and low-dimensional system states . ( 2 ) Recurrent baseline in locomotion task Yes , we agree that the focus of [ 1 ] is the proposed graph networks . We chose the recurrent model in their work because it \u2019 s a straightforward architecture and is easy to implement . The purpose of our work is not to investigate which recurrent architecture is the best , and HyperDynamics and the Recurrent baseline uses the same recurrent model to encode interaction trajectories . We believe such comparison is reasonable and does illustrate the advantage of bringing hypernetworks into dynamics learning . We apologize for not explaining this well in the original paper . We have now added more details of the baselines in Section 4.2 . We hope it \u2019 s more informative now ."}, {"review_id": "pHXfe1cOmA-2", "review_text": "# # # # Summary : This paper proposes an adaptive dynamics model based on the idea of hypernetworks . It is demonstrated that this approach compares favorably to other ways of adapting dynamics models such as conditioning on a separate feature input and meta learning by gradient-based model updates . The proposed approach is evaluated on Pushing and Locomotion tasks . # # # # Pros : - The proposed approach for conditioning dynamics models on rollouts to model system-specific properties using the hypernetworks idea seems novel and is interesting . - Paper is clearly written , the provided figures help understanding - Outperforms state-of-the-art adaptive dynamics modeling approaches [ Nagabandi et al. , 2019 ] , [ Sanchez-Gonzalez et al. , 2018b ] - Reasonable baselines are used for comparison , such as fixed model ( XYZ ) , input feature conditioning ( Direct ) , expert ensemble , and state-of-the-art adaptive dynamics models # # # # Cons : - The paper does not explain training details for the architecture sufficiently well . How are the network components trained , especially the visual recognition part for object pushing ? What kind of supervision with ground truth is required to train the components , for instance for object detection and shape representation ? Are components pretrained and how ? Which losses/data are used for training ? - Its unclear why moving from a canonical to an oriented shape representation in Sec.3.1 should improve results . Shouldnt this limit generalization and require more training data ? - Giving standard deviations in addition to the average values in table 1-3 would complete the numerical results - Sec.1 ) Why is PlaNet [ Hafner et al. , 2019 ] listed as `` no adaptation '' , although it contains a recurrent state representation ? - It appears magical that the approach performs better on novel than on seen objects during training for Cheetah-Slope or Ant-Slope in Table 3 . Please discuss . # # # # Recommendation : The paper reads well and proposes an interesting novel approach which could deserve acceptance . The paper should address the points raised in paper weaknesses . # # # # Questions for rebuttal : Please address points raised above in `` weaknesses '' . # # # # Typos : - p2 : `` They are are '' - p4 : `` E_int then maps z_int to a 1-dimensional code z_int \u2208 R2 '' - should n't this be `` E_int maps interactions to 2-dimensional code z_int \u2208 R2 '' ? - p4 : `` which is typically comprised of an agent and its external environment '' - > `` which typically comprises / which is typically composed of '' - Table 1 : Motion rediction error - > Motion prediction error - Advice : In Figure 1 , the concatenation symbol is slightly misleading , as it could be interpreted as elementwise multiplication . Maybe replace it by $ [ \\cdot , \\cdot ] $ . # # # # Post-rebuttal comments : The authors ' comments addressed my concerns on method and experimental details mostly well . I keep with my rating `` 6 : Marginally above acceptance threshold '' .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks a lot for the detailed feedback ! We are very grateful that you found our idea novel and interesting . Below we address your concerns . ( 1 ) Training details Thanks for bringing up this issue ! We apologize for not including sufficient details on training . We have updated Appendix A.1 with a new paragraph talking about how the components are trained in detail . We have also added some descriptions of the detection module in Section 3.1 . To briefly summarize here , HyperDynamics for pushing is trained to optimize the sum of three losses : 1 ) the main dynamics prediction loss discussed in Equation ( 1 ) . 2 ) object detection loss which follows the exact same form described in Mask R-CNN , as GRNNs follows Mask R-CNN for object detection , except that it operates in 3D now . It \u2019 s supervised using ground-truth object locations provided by the simulator . 3 ) a top-down view prediction loss using standard cross-entropy pixel matching loss , supervised with ground-truth top-down view of the object . All the components are trained jointly ; none of them are pretrained . For loss 3 , the training data alternates between the actual data collected during pushing , and data rendered by loading random objects from ShapeNet in the simulation . We do this because such rendered data can be easily and fast collected while actual pushing data is relatively expensive . Doing this allows us to train dynamics prediction only on a few objects ( 24 in our case ) , while letting our model to generalize to novel objects unseen during actual pushing . We have also move architecture and training details that were originally in appendix to the main paper now . ( See implementation details in Section 4.1 and 4.2 ) ( 2 ) Moving from canonical to oriented shape representation . Thanks for pointing out this ! We apologize for causing the confusion about our orientation representation . To make it more clear , consider an object oriented at orientations A and B ; we now have 2 design choices : - ( 1 ) treating them as the same object but with different orientation state , then we would feed its canonical shape into E_vis , and feed different orientations into the generated dynamics model ( since the dynamics model takes as input the object state ) , or - ( 2 ) simply treating them as 2 different objects as if they have different shapes ! ( Imagine you consider a rotated mug as a new mug . ) Then we can simply remove the orientation info from its state , only feed its position info ( in addition to other system states ) into the generated dynamics model , and encode their shape ( which already includes their orientation ) into the shape code z_vis . If we choose ( 1 ) , then the problem is that when trained on only 24 objects , the model would only see 24 distinct z_vis , which could result in very bad overfitting for both E_vis and Hypernetwork H. So in our framework we chose ( 2 ) , which enables both E_vis and H to see a much more smooth distribution of input data , and this helps our method to generalize . In practice , we did try ( 1 ) at first but the model failed to generalize . We hope this clarifies your doubt . We have also updated the last paragraph in Section 3.1 to make this design choice more clear . ( 3 ) Adding standard deviations in tables . Thank you for the suggestion ! We have updated both Table 1 and 3 with standard deviations added . For table 2 , it \u2019 s reporting success rate , so we believe its current form should be fine . ( 4 ) PlaNet listed as \u201c no adaptation \u201d Thanks for pointing out this ! Indeed , PlaNet uses a recurrent state space model . The problem that we are considering in the scope of this paper is potentially changing dynamics and physical properties of a dynamical system with a fixed set of properties . In PlaNet , the recurrent state representation is not designed with the explicit purpose to implicitly encode such property changes . However , it does present an experiment where only one agent is trained and it needs to infer which task it is facing by encoding past observations , while the dynamical system completely changes ( even its morphology ) from task to task . Therefore , we agree that it is a general framework that is able to handle changes in system dynamics . We have corrected this and moved PlaNet to group ( ii ) ( Visual dynamics and recurrent state representations ) . ( Note that we have re-structured the introduction and the related work section following suggestions given by R1 , and the groupings of the contemporary methods are now in Section 2.1 . )"}, {"review_id": "pHXfe1cOmA-3", "review_text": "Summary This paper proposes a framework , HyperDynamics , that takes in observations of how the environment changes when applying rounds of interactions , and then , generates parameters to help a learning-based dynamics model quickly adapt to new environments . The framework consists of three modules : - an encoding module that maps the observation of a few agent-environment interactions into a latent feature vector , - a hypernetwork that conditions on the latent vector and generates all parameters of a dynamics model dedicated to the observed system , and - a target dynamics model constructed using the generated parameters that predicts the future state by taking the current system state and the input action as input . The authors evaluate the framework in a series of object pushing and locomotion tasks . They have shown that a single HyerDynamics model allows few-shot adaptation to new environments , outperforming several baselines while maintaining a performance that is on par with a set of models trained separately for each environment . Strengths This paper targets an important question of building a more generalizable dynamics model that can perform online adaptation to environments with different physical properties and scenarios that are not seen during training . The authors have evaluated the method in several object pushing and robot locomotion tasks and shown superior performance over baselines that uses recurrent state representations or gradient-based meta-optimization . Many practical treatments used in the pipeline can be good references for the community to learn from , e.g. , how to encode object information in 3d , specific representation of the object orientation , and the use of Geometry-Aware Recurrent Networks ( GRNNs ) to learn 3D feature grids , etc . Weaknesses Although I like the idea of this paper , I believe the authors should provide more clarification and illustration of the experimental results to solidify the claims in the paper : ( 1 ) What are the objects used in the pushing task ? The authors claim that their `` dataset consists of only 31 different object meshes with distinct shapes . '' It is important to include images of the objects to give the readers a better understanding of how diverse the dataset is and how different the geometry of the `` seen '' and `` novel '' objects are . This can help the readers better appreciate the generalization ability of the proposed method . ( 2 ) It would be great if the authors can include some qualitative examples , e.g. , video , to show the performance of the method . Purely from the numbers in the tables , it is hard for the readers to imagine how well the proposed approach solves the tasks . ( 3 ) It would make the paper more illustrative if the authors can include some analysis and visualization of the learned representations in the middle of the network . For example : - How are the latent embeddings different for different objects ? - Are there any correlations between the embeddings and the actual physical properties ? - How do the interactions affect the embedding ? Will different interaction sequences result in the same embedding ? - How do the learned representations from Geometry-Aware Recurrent Networks ( GRNNs ) look like ? The authors claim that it can `` complete missing or occluded shape information . '' Can the authors provide some concrete evidence supporting this claim in the specific scenarios used in this paper ? How do different numbers of interactions affect the quality of the representation ? ( 4 ) How does E_vis detect the objects in the scene ? Are these detections in 2d or 3d ? How accurate is the detection algorithm ? ( 5 ) The beginning of Section 3.1 describes that an object 's orientation is represented as a quaternion . However , at the end of Section 3.1 , the authors suggest that they `` discard the orientation information from states fed into the generated dynamics model . '' This seems to me makes the `` state '' an incomplete representation of the environment , where the authors only predict the position of the object , which makes me wonder : How does the model encode the geometry of the object ? Will the missing of the orientation information introduce any ambiguities or uncertainties ? What if the object is re-oriented ? It may be better to include comparisons of different state representations . Also , in Section 3.3 , the authors suggest that they update the orientation using quaternion composition , which seems to be inconsistent with what has been described before . Have n't the model already discarded the orientation information ? Other comments This paper only shows experiments in the simulation . I 'm curious , are there any gaps before applying the method to the real world , and what are these gaps ? For example , how long does the model take to optimize the action trajectories when performing MPC ? Can it support real-time feedback control in real physical scenarios , especially when the environment is dynamic ? Model-predictive control relies on the environment 's feedback to correct the action sequences , which can achieve a good control performance while tolerating a larger long-term prediction error . In your experiments , how important is the accuracy of the dynamics model ? In other words , even if some baselines have a poorer forward prediction performance , will MPC be able to bridge some of the performance gaps ? In table 3 , why are there multiple red numbers in the Ant-Slope columns ? Typo ? Page 4 , Section 3.1 : `` E_int then maps z_int to a 1-dimensional code z_int \\in R^2 . '' This sentence seems weird . How does E_int map z_int to its own ? Why does a `` 1-dimensional code '' lies in a 2d space ? Post rebuttal The authors ' response and the revisions to the manuscript have greatly improved the quality and clarity of the paper . Most of my major concerns regarding the implementation and evaluation details have been sufficiently addressed ; hence , I decide to increase the score from 5 ( Marginally below acceptance threshold ) to 6 ( Marginally above acceptance threshold ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your detailed comments and suggestions ! We are very encouraged that you found the problem we study important and our idea interesting . Below we address your concerns . ( 1 ) Details of objects used . Yes , we agree that including images of the objects would help readers to better understand how the selected objects are like . We have added a figure into Appendix A.1 showing all the objects used and the train-test split . We also added a sentence in the experimental section for pushing ( section 4.1 ) in the main paper to refer readers to Appendix A . We hope it \u2019 s more clear now . ( 2 ) Qualitative example . E.g.video.Thanks for the suggestion ! We agree that providing more qualitative examples such as a video would greatly help readers understand the capability of our methods . We are currently in progress of constructing a project website and also making a detailed video . We will make them publicly available once they are done . ( 3 ) Analysis and visualization of the learned latent representation . We now have included additional visualizations and analysis in Appendix B.4 . For the interaction encoder and the latent code z_int , we visualize the learned latent space in Figure 4 with varying number of interactions . We believe such additional results help answer your questions : indeed , there \u2019 s a correlation between the embeddings and the actual properties , and different interaction sequences would result in close embedding as long as their corresponding physical properties are close . The figure also suggests that the values of k we used are sufficient for the latent space to resemble the original space of physical properties well . As for the visual encoder E_vis , it is directly built upon Geometry-Aware Recurrent Networks ( GRNNs ) [ 1 ] , and we would like to argue that the details of GRNNs themselves are not the main focus of this work , and we don \u2019 t consider deploying GRNNs in object pushing as one of our major contributions . We hope it \u2019 s reasonable to refer readers to the original paper [ 1 ] and its follow-up work [ 4 , 5 ] for the latent representation visualization and details for shape completion . However , we do agree that providing some visualizations would help appreciate how the visual model works in our specific scenarios . We have added Figure 5 in Appendix B.4 with some qualitative results to show how our model learns to complete missing shape information and predicts the top-down view under potential occlusions . Regarding your last question on \u201c how do different numbers of interactions affect the quality of the representation ? \u201d : The number of interactions only affects the produced interaction code z_int , as shown in Figure 4 , while z_vis ( produced by the visual encoder ) is a function of the image input only and is not affected by the interactions . ( 4 ) How does E_vis detect objects ? Our perception module E_vis directly uses GRNNs [ 1 ] , which allow us to do both object detection and top-down view prediction . GRNNs detect objects using a 3D object detector that operates over its latent 3D feature map of the scene M. The object detector maps the scene feature map M to a variable number of axis-aligned 3D bounding boxes of the objects ( and their segmentation masks if needed ) . Its architecture is similar to Mask R-CNN but instead uses RGB-D inputs and produces 3D bounding boxes . During our application , we crop the scene feature map using the object bounding boxes produced by GRNNs to obtain object-centric feature maps for shape encoding . We refer readers to [ 1 ] for the details of the detection pipeline . We do agree that adding some descriptions of it would make our paper more clear , so we have added some descriptions of the detection module in Section 3.1 . The accuracy of this detection module is also studied and reported in [ 1 ] . We believe explicitly evaluating the accuracy of such 3D detector is out of the scope of this paper since neither 3D detection nor using GRNNs for object pushing is within the contributions of our work . In our experiments , since the baselines used for pushing ( XYZ , Direct , MB-MAML and Expert-Ens ) all use the same detector to obtain object positions ( note that Expert-Ens only assumes access to ground-truth orientation , mass , and friction ) , we believe such comparison is fair and reasonable . However , since detection is a key stage in our dynamics prediction pipeline , we do agree that it \u2019 s good to present clear ablations to give readers an idea of how well such detector works . We have added a comparison between using ground-truth object positions and using detected object positions in Section B.1 . The results show that the dynamics prediction performance of our model using detected object positions is close to the one using gt object positions , suggesting that the 3D detector in our pipeline works reasonably well ."}], "0": {"review_id": "pHXfe1cOmA-0", "review_text": "# # Summary The authors present `` HyperDynamics '' , a novel method for system-identification and learning of flexible forward models that can be used in planning tasks . The presented methods is generic and is shown on both locomotion and pushing tasks with different simulated robots . I enjoyed reading this work a lot and I hope it gets accepted . It 's a clever idea and most flaws that I 'm about to point out are easily addressable by the authors . # # Strengths & Weaknesses # # # # Strengths 1 ) The method is generic ( shown to work across tasks and environments ) . 2 ) The baselines are strong . When I started reading your paper , I thought that DensePhysNet and some form of MAML would be good candidates for this to compare again and it turns out these were indeed included . 3 ) Figure 1 + caption as well as the introduction to section 3 do a great job at introducing the architecture in a way that would allow the reader to create a basic implementation . 4 ) Code was included . I did n't run it but it 's clean and seems functional from what I can tell . # # # # Weaknesses 1 ) You really , really need to be more clear in the main paper on the implementation details . You ca n't move the amount of training data to the appendix and it 's not good practice to only include the network architecture by name in the main paper . And what are all your losses ? You make the method looks super simple but then you train on shapenet , some 2D reconstruction , something about cropping , and there 's a GRU in there too ( full backprop vs truncated backprop ? ) . The appendix shines a _little_ light on this but you need to be way more specific in the main paper . That has to stand on its own . 2 ) You do n't motivate all the nitty-gritty implementation choices . Why did you add the decoder ? What 's the performance if you remove it ? What about the cropping ? What if you do n't do an object-centric feature map , but instead a few CNN layers ? What are the individual contributions of all these details ? 3 ) DensePhysNet , Visual Foresight , and many other works in this domain use ( simple ) real-robot experiments to demonstrate that their method can handle realistic robot noise . Obviously there 's a global pandemic happening at the moment , so I wo n't require you adding this for the rebuttal , but I think in order to really establish this method ( maybe before putting it on Arxiv ) , you 'd have to add some real-robot experiments . This can be as simple as a 180USD RealSense and a 500USD robot arm plus a few objects and a playfield . It 's become a standard for system-identification-style works and it 's justified in my opinion since your method is n't inherently useful in simulation where the user has access to all the information and can arbitrarily reset/reposition the model . And since you do n't have ShapeNet data for many real-world objects ( which you seem to need for pretraining ) , could you at least add a sentence or two detailing how this would transfer to real-world problems ? * * TL ; DR my main requests : * * ( 2 ) Motivate implementation details ( add ablations if you have any ) and ( 1 ) be more explicit about them in the main part of the paper . # # Impact & Recommendation Despite that there seem to be a lot of hacks that make this method in the specific settings , I think the general idea behind it is sound . And I think the authors show that it performs better than the sota , at least in simulation . Therefore I 'd recommend acceptance given that the authors add the requested information . In its current shape , it 's a 6 for me but if my main concerns are addressed , I 'm happy to up this to a 7 or if major improvements are made and my questions below are answered , to an 8 . # # Questions , Nitpicks , Comments - Kudos for not making another acronym method - There are a lot of typos and orthographic errors , would recommend a spell-checked or getting this proofed . Examples : section 2 `` poinclouds '' , section 2 `` properties in hand '' - > `` properties at hand '' - Maybe start the introduction with an example , e.g.how children are able to chew on a block of wood to assess its hardness and then build towers with it . - * * important * * Introduction : when you go over ( i-iv ) , that feels a bit too long and lit-reviewy and misplaced in the introduction . I would recommend the following changes : ( a ) trim this severely , only mention that there are model-based methods that usually do only one environment and there 's meta-learning and how your method is more adaptable than either , ( b ) move this into the literature section , where you have to come back to it anyway , ( c ) move the Hypernetworks section from the literature into a separate `` Background '' section and develop it a bit further , since it 's less `` competing method '' and more `` you should know about this to understand out method '' . - Also in the introduction , you present ( i-iv ) , and you mention how your method is better/different than ( i-iii ) but you never address ( iv ) . - It 's become a standard to summarize the contributions again at the end of the introduction , ideally as bullet points . Please add these . - In equation ( 1 ) , why is the ordering O-T-N for the sums ? I feel like ONT would be more natural , no ? - When reading the method , my main question was if the method would work on `` dense '' trajectories or on before-and-after photos like DensePhysNet . This is only answered a few pages later but I think this belongs in 3-Overview or 3.1 . Just to be clear , you 're gathering trajectories of length 4s , i.e.5 frames of 800ms where you do NOT retract the robot arm when pushing , right ? ( Compared to DensePhysNet , where the arm is never visible because they take photos before and after complete standstill ) . If that 's the case , how do you deal with occlusion from the arm ? - Do you encourage object-object interactions in any way or do they just occur randomly ? Or do you only ever experiment with single objects ? - The object orientation vs state section is n't super clear ? You 're subtracting an object 's absolute starting position+orientation from it 's future trajectory points ? - In 3.2 : Why a GRU , why not LSTM ? Why k=16 ( and similarly why k=5 ) ... This ties into the main criticism from above . Please motivate your choices . - In 4.1 : I think this is a typo , but it says you added beds to your experiment table . I think they 'd be a bit too large , no ? : D - 4.1 : specify the random mass+friction range , please ! - 4.1 : same with the total amount of training data/frames - And since you wo n't have ShapeNet - 4.2 : I think it 's a half-cheetah , not a cheetah . - 4.2 : I do n't understand why it 's unrealistic to assume arbitrary resetting in simulation . That 's one of the benefits of running simulations and common practice . - 5 : What do you mean `` predicting both the structure and parameters of the target dynamics model '' ? Parameters is clear ( mass , friction , etc . ) but what 's the structure here ?", "rating": "7: Good paper, accept", "reply_text": "We are really encouraged that you liked our idea and agreed with our baseline choices , and we really appreciate your comments and suggestions with so much details ! We hope to address all of your concerns below . ( 1 ) Lack of implementation details in main paper . Thanks for raising this issue . We added more detailed description into the \u2018 baseline \u2019 paragraphs , and also added a paragraph ( Implementation Details ) after the baseline paragraphs in both Section 4.1 and 4.2 . We have moved the architecture details of our model from the appendix to there . We also added more details of the baselines ( architecture , training , etc . ) and the amount of training data in these sections . To briefly summarize here , MB-MAML takes the exact same input as our model does , except that it uses interaction data for online model adaptation . The Direct and Recurrent baselines use the same input and use the same visual and interaction encoders as our model does in their corresponding tasks . All the dynamics model used in Direct , Recurrent , XYZ , MB-MAML and Expert-Ens use the same architecture . VF and DensePhysNet are implemented using the architectures described in their corresponding papers . The GRU is trained with full backpropagation and we have added this to the \u2018 Implementation Details \u2019 section too . Regarding the training of the visual encoder ( object detection , view prediction , losses , etc . ) , we have added a detailed description in Appendix A.1 . Since it \u2019 s a bit lengthy and it \u2019 s built mostly based on GRNNs , which is not the main contribution of this work , we think it \u2019 s reasonable to put it in Appendix , and referred to it in relevant places in the main paper . ( 2 ) Details of implementation choices Thanks for bringing this up ! The decoder is crucial since it helps regularize the training of the visual encoder and makes sure the produced z_vis captures sufficient shape information . The cropping step is also important to produce the object-centric feature map , and drives the model \u2019 s attention to the object under pushing . In order to help readers better understand these components , we have added an ablation study in Appendix B.1 . We hope the results there could explain our implementation choices ."}, "1": {"review_id": "pHXfe1cOmA-1", "review_text": "== Update == Thank you for your detailed response . The newly added clarifications and sanity checks have greatly improved the quality of the paper , and I am therefore increasing my score from 4 to 6 . I believe the model capacity comparison ( Table 6 ) is especially important for demonstrating the value of the new architecture , and would recommend mentioning that result in the main paper . == Original Review == The paper proposes a model for predicting the dynamics of a physical system based on hypernetworks : given some observed interactions and some visual input , the hypernetwork outputs the parameters of a dynamics model , which then predicts the evolution of the system 's state over time . Experiments are conducted on an object pushing and a locomotion task . Strengths : 1 . The paper addresses an important question , namely , how a dynamics model may adapt to environments that do n't fully match its training distribution . 2.The proposed use of a hypernetwork is plausible and novel to my knowledge . 3.The related work section appears comprehensive , and , to my knowledge , does not miss any major prior work . Weaknesses : 1 . The main claim of the paper is that hyperdynamics network offers better prediction accuracy and generalization than a standard dynamics model . I feel like the evaluation of this question is confounded by the choice of tasks and baselines . On the pushing benchmark , the XYZ , VF , and DensePhysNet operate on different modalities than HyperDynamics ( either no state information or no visual information ) , and are therefore difficult to compare . For the MB-MAML baseline , this is not specified . The Expert-Ens model can not be expected to generalize , since it is designed to overfit on individual objects . As a result , only the 'Direct ' baseline clearly operates in the same experimental regime as HyperDynamics . However , nothing is reported on the model architecture or the training method for that baseline , raising the question if its model capacity was competitive . My impression is that this experimental design blurs the effects of ( a ) using side-information to infer system properties , and ( b ) utilizing such information through a hypernetwork as opposed to a standard dynamics predictor . If the goal is to evaluate the new architecture , these should be disentangled . 2.On the locomotion benchmark , the Recurrent baseline is similarly unclear . Sanchez-Gonzalez et al.is cited , but that paper focusses on comparing recurrent models based on graph networks to those based on MLPs , and it is unclear which model was used . 3.No results are reported for the prediction accuracy on the locomotion task , which would have helped evaluate the performance of the dynamics models more directly than the task scores . 4.Many of these issues could have been avoided by testing on established benchmarks from the literature , for which results are available . If there is a simulator available , generalization ability could still have been tested by varying the physical constants of the dataset . 5.The paper contains a decent amount of typos and grammatical errors . Overall , while the paper presents an interesting idea , the experimental evaluation is not convincing in its current state : Baseline architectures are not fully specified , many of them did not receive the same input , and no benchmark task with previously reported results has been used . As a result , I recommend rejection at this time . Questions : 1 . In eq.1 , should omega be a parameter of H ( . ) instead of F ( . ) ? 2.Section 3.1 introduces the `` 1-dimensional code $ z_ { int } \\in \\mathbb { R } ^2 $ '' . So is it one or two-dimensional ? 3.Overall , the dimensionality of the latent codes and hidden layers seems incredibly small , e.g. , only 1/2 numbers to encode prior interactions , and 8 to encode shape . Is there really no benefit to using higher capacity models ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the detailed comments and criticisms ! We feel very encouraged that you found the question we study important and our approach novel . We hope to address all your concerns below . ( 1 ) Design and details of the baselines . We apologize for missing important details of the baselines and we really appreciate you for raising this issue . Indeed , your description of the purpose of each baseline matches closely to our intention , and we would like to explain them in further details here : - The XYZ baseline is more like an ablation where we want to evaluate how a standard dynamics model performs without having access to the visual features and physics properties . - You are absolutely right that VF and DensePhysNet operate on different modalities than our method . We included them in the paper because we believe it would be good to compare our method with some image-based methods in the literature . We think this makes the comparison more complete . - MB-MAML uses the exact same architecture as the generated dynamics model of HyperDynamics , it also uses the exact same input data . The only difference is that it updates the model parameters online using the interaction data . - Expert-Ens is included because we want to evaluate if our model generate good expert online that can match the performance of these separately trained ones . Each expert in Expert-Ens uses the exact same architecture as the generated model of HyperDynamics . - Direct : you are right that it operates in the same experimental regime as ours . It uses the exact same input as ours ( both side information and system state ) . It uses the same visual encoder and interaction encoder as ours , and feed the concatenated code z into a dynamics model , which follows the exact same architecture as the generated dynamics model of HyperDynamics . We believe this is a clear comparison since it clearly shows adding a hypernetwork to process such system properties could help improve performance . Again , we are sorry for not explaining these details clearly in the main paper . We have added more details into the baseline description section in the paper . We also added an \u2018 Implementation Details \u2019 paragraph after the baseline paragraph . It contains the architecture details that were in the appendix , and we also added some more details there to describe the architecture of the baselines . We also appreciate that you brought up the issue regarding model capacity . Indeed , although the dynamics model in Direct uses the same architecture as the the generated one in HyperDynamics , it needs to process more information than our generated model , since it also handles the latent code z , while in HyperDynamics , z is fed into the hypernetwork . We believe a more fair comparison on model capacity should be between the dynamics model in Direct and the hypernetwork in HyperDynamics ( the last layer of the hypernetwork outputs the weights of the generated dynamics model ) . Considering that the hypernetwork is a fully-connected network with a 16-unit hidden layer , effectively it has around 16x more parameters than the Direct baseline . However , in our experiments we found that further increasing the model capacity of Direct does not help further improve its performance , and would result in either overfitting or underfitting . We apologize for not including such comparison in the original paper . We have now added an analysis on model capacity in Appendix B.2 . We hope this shows that our architecture is a more effective and structured way to handle system properties and low-dimensional system states . ( 2 ) Recurrent baseline in locomotion task Yes , we agree that the focus of [ 1 ] is the proposed graph networks . We chose the recurrent model in their work because it \u2019 s a straightforward architecture and is easy to implement . The purpose of our work is not to investigate which recurrent architecture is the best , and HyperDynamics and the Recurrent baseline uses the same recurrent model to encode interaction trajectories . We believe such comparison is reasonable and does illustrate the advantage of bringing hypernetworks into dynamics learning . We apologize for not explaining this well in the original paper . We have now added more details of the baselines in Section 4.2 . We hope it \u2019 s more informative now ."}, "2": {"review_id": "pHXfe1cOmA-2", "review_text": "# # # # Summary : This paper proposes an adaptive dynamics model based on the idea of hypernetworks . It is demonstrated that this approach compares favorably to other ways of adapting dynamics models such as conditioning on a separate feature input and meta learning by gradient-based model updates . The proposed approach is evaluated on Pushing and Locomotion tasks . # # # # Pros : - The proposed approach for conditioning dynamics models on rollouts to model system-specific properties using the hypernetworks idea seems novel and is interesting . - Paper is clearly written , the provided figures help understanding - Outperforms state-of-the-art adaptive dynamics modeling approaches [ Nagabandi et al. , 2019 ] , [ Sanchez-Gonzalez et al. , 2018b ] - Reasonable baselines are used for comparison , such as fixed model ( XYZ ) , input feature conditioning ( Direct ) , expert ensemble , and state-of-the-art adaptive dynamics models # # # # Cons : - The paper does not explain training details for the architecture sufficiently well . How are the network components trained , especially the visual recognition part for object pushing ? What kind of supervision with ground truth is required to train the components , for instance for object detection and shape representation ? Are components pretrained and how ? Which losses/data are used for training ? - Its unclear why moving from a canonical to an oriented shape representation in Sec.3.1 should improve results . Shouldnt this limit generalization and require more training data ? - Giving standard deviations in addition to the average values in table 1-3 would complete the numerical results - Sec.1 ) Why is PlaNet [ Hafner et al. , 2019 ] listed as `` no adaptation '' , although it contains a recurrent state representation ? - It appears magical that the approach performs better on novel than on seen objects during training for Cheetah-Slope or Ant-Slope in Table 3 . Please discuss . # # # # Recommendation : The paper reads well and proposes an interesting novel approach which could deserve acceptance . The paper should address the points raised in paper weaknesses . # # # # Questions for rebuttal : Please address points raised above in `` weaknesses '' . # # # # Typos : - p2 : `` They are are '' - p4 : `` E_int then maps z_int to a 1-dimensional code z_int \u2208 R2 '' - should n't this be `` E_int maps interactions to 2-dimensional code z_int \u2208 R2 '' ? - p4 : `` which is typically comprised of an agent and its external environment '' - > `` which typically comprises / which is typically composed of '' - Table 1 : Motion rediction error - > Motion prediction error - Advice : In Figure 1 , the concatenation symbol is slightly misleading , as it could be interpreted as elementwise multiplication . Maybe replace it by $ [ \\cdot , \\cdot ] $ . # # # # Post-rebuttal comments : The authors ' comments addressed my concerns on method and experimental details mostly well . I keep with my rating `` 6 : Marginally above acceptance threshold '' .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks a lot for the detailed feedback ! We are very grateful that you found our idea novel and interesting . Below we address your concerns . ( 1 ) Training details Thanks for bringing up this issue ! We apologize for not including sufficient details on training . We have updated Appendix A.1 with a new paragraph talking about how the components are trained in detail . We have also added some descriptions of the detection module in Section 3.1 . To briefly summarize here , HyperDynamics for pushing is trained to optimize the sum of three losses : 1 ) the main dynamics prediction loss discussed in Equation ( 1 ) . 2 ) object detection loss which follows the exact same form described in Mask R-CNN , as GRNNs follows Mask R-CNN for object detection , except that it operates in 3D now . It \u2019 s supervised using ground-truth object locations provided by the simulator . 3 ) a top-down view prediction loss using standard cross-entropy pixel matching loss , supervised with ground-truth top-down view of the object . All the components are trained jointly ; none of them are pretrained . For loss 3 , the training data alternates between the actual data collected during pushing , and data rendered by loading random objects from ShapeNet in the simulation . We do this because such rendered data can be easily and fast collected while actual pushing data is relatively expensive . Doing this allows us to train dynamics prediction only on a few objects ( 24 in our case ) , while letting our model to generalize to novel objects unseen during actual pushing . We have also move architecture and training details that were originally in appendix to the main paper now . ( See implementation details in Section 4.1 and 4.2 ) ( 2 ) Moving from canonical to oriented shape representation . Thanks for pointing out this ! We apologize for causing the confusion about our orientation representation . To make it more clear , consider an object oriented at orientations A and B ; we now have 2 design choices : - ( 1 ) treating them as the same object but with different orientation state , then we would feed its canonical shape into E_vis , and feed different orientations into the generated dynamics model ( since the dynamics model takes as input the object state ) , or - ( 2 ) simply treating them as 2 different objects as if they have different shapes ! ( Imagine you consider a rotated mug as a new mug . ) Then we can simply remove the orientation info from its state , only feed its position info ( in addition to other system states ) into the generated dynamics model , and encode their shape ( which already includes their orientation ) into the shape code z_vis . If we choose ( 1 ) , then the problem is that when trained on only 24 objects , the model would only see 24 distinct z_vis , which could result in very bad overfitting for both E_vis and Hypernetwork H. So in our framework we chose ( 2 ) , which enables both E_vis and H to see a much more smooth distribution of input data , and this helps our method to generalize . In practice , we did try ( 1 ) at first but the model failed to generalize . We hope this clarifies your doubt . We have also updated the last paragraph in Section 3.1 to make this design choice more clear . ( 3 ) Adding standard deviations in tables . Thank you for the suggestion ! We have updated both Table 1 and 3 with standard deviations added . For table 2 , it \u2019 s reporting success rate , so we believe its current form should be fine . ( 4 ) PlaNet listed as \u201c no adaptation \u201d Thanks for pointing out this ! Indeed , PlaNet uses a recurrent state space model . The problem that we are considering in the scope of this paper is potentially changing dynamics and physical properties of a dynamical system with a fixed set of properties . In PlaNet , the recurrent state representation is not designed with the explicit purpose to implicitly encode such property changes . However , it does present an experiment where only one agent is trained and it needs to infer which task it is facing by encoding past observations , while the dynamical system completely changes ( even its morphology ) from task to task . Therefore , we agree that it is a general framework that is able to handle changes in system dynamics . We have corrected this and moved PlaNet to group ( ii ) ( Visual dynamics and recurrent state representations ) . ( Note that we have re-structured the introduction and the related work section following suggestions given by R1 , and the groupings of the contemporary methods are now in Section 2.1 . )"}, "3": {"review_id": "pHXfe1cOmA-3", "review_text": "Summary This paper proposes a framework , HyperDynamics , that takes in observations of how the environment changes when applying rounds of interactions , and then , generates parameters to help a learning-based dynamics model quickly adapt to new environments . The framework consists of three modules : - an encoding module that maps the observation of a few agent-environment interactions into a latent feature vector , - a hypernetwork that conditions on the latent vector and generates all parameters of a dynamics model dedicated to the observed system , and - a target dynamics model constructed using the generated parameters that predicts the future state by taking the current system state and the input action as input . The authors evaluate the framework in a series of object pushing and locomotion tasks . They have shown that a single HyerDynamics model allows few-shot adaptation to new environments , outperforming several baselines while maintaining a performance that is on par with a set of models trained separately for each environment . Strengths This paper targets an important question of building a more generalizable dynamics model that can perform online adaptation to environments with different physical properties and scenarios that are not seen during training . The authors have evaluated the method in several object pushing and robot locomotion tasks and shown superior performance over baselines that uses recurrent state representations or gradient-based meta-optimization . Many practical treatments used in the pipeline can be good references for the community to learn from , e.g. , how to encode object information in 3d , specific representation of the object orientation , and the use of Geometry-Aware Recurrent Networks ( GRNNs ) to learn 3D feature grids , etc . Weaknesses Although I like the idea of this paper , I believe the authors should provide more clarification and illustration of the experimental results to solidify the claims in the paper : ( 1 ) What are the objects used in the pushing task ? The authors claim that their `` dataset consists of only 31 different object meshes with distinct shapes . '' It is important to include images of the objects to give the readers a better understanding of how diverse the dataset is and how different the geometry of the `` seen '' and `` novel '' objects are . This can help the readers better appreciate the generalization ability of the proposed method . ( 2 ) It would be great if the authors can include some qualitative examples , e.g. , video , to show the performance of the method . Purely from the numbers in the tables , it is hard for the readers to imagine how well the proposed approach solves the tasks . ( 3 ) It would make the paper more illustrative if the authors can include some analysis and visualization of the learned representations in the middle of the network . For example : - How are the latent embeddings different for different objects ? - Are there any correlations between the embeddings and the actual physical properties ? - How do the interactions affect the embedding ? Will different interaction sequences result in the same embedding ? - How do the learned representations from Geometry-Aware Recurrent Networks ( GRNNs ) look like ? The authors claim that it can `` complete missing or occluded shape information . '' Can the authors provide some concrete evidence supporting this claim in the specific scenarios used in this paper ? How do different numbers of interactions affect the quality of the representation ? ( 4 ) How does E_vis detect the objects in the scene ? Are these detections in 2d or 3d ? How accurate is the detection algorithm ? ( 5 ) The beginning of Section 3.1 describes that an object 's orientation is represented as a quaternion . However , at the end of Section 3.1 , the authors suggest that they `` discard the orientation information from states fed into the generated dynamics model . '' This seems to me makes the `` state '' an incomplete representation of the environment , where the authors only predict the position of the object , which makes me wonder : How does the model encode the geometry of the object ? Will the missing of the orientation information introduce any ambiguities or uncertainties ? What if the object is re-oriented ? It may be better to include comparisons of different state representations . Also , in Section 3.3 , the authors suggest that they update the orientation using quaternion composition , which seems to be inconsistent with what has been described before . Have n't the model already discarded the orientation information ? Other comments This paper only shows experiments in the simulation . I 'm curious , are there any gaps before applying the method to the real world , and what are these gaps ? For example , how long does the model take to optimize the action trajectories when performing MPC ? Can it support real-time feedback control in real physical scenarios , especially when the environment is dynamic ? Model-predictive control relies on the environment 's feedback to correct the action sequences , which can achieve a good control performance while tolerating a larger long-term prediction error . In your experiments , how important is the accuracy of the dynamics model ? In other words , even if some baselines have a poorer forward prediction performance , will MPC be able to bridge some of the performance gaps ? In table 3 , why are there multiple red numbers in the Ant-Slope columns ? Typo ? Page 4 , Section 3.1 : `` E_int then maps z_int to a 1-dimensional code z_int \\in R^2 . '' This sentence seems weird . How does E_int map z_int to its own ? Why does a `` 1-dimensional code '' lies in a 2d space ? Post rebuttal The authors ' response and the revisions to the manuscript have greatly improved the quality and clarity of the paper . Most of my major concerns regarding the implementation and evaluation details have been sufficiently addressed ; hence , I decide to increase the score from 5 ( Marginally below acceptance threshold ) to 6 ( Marginally above acceptance threshold ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your detailed comments and suggestions ! We are very encouraged that you found the problem we study important and our idea interesting . Below we address your concerns . ( 1 ) Details of objects used . Yes , we agree that including images of the objects would help readers to better understand how the selected objects are like . We have added a figure into Appendix A.1 showing all the objects used and the train-test split . We also added a sentence in the experimental section for pushing ( section 4.1 ) in the main paper to refer readers to Appendix A . We hope it \u2019 s more clear now . ( 2 ) Qualitative example . E.g.video.Thanks for the suggestion ! We agree that providing more qualitative examples such as a video would greatly help readers understand the capability of our methods . We are currently in progress of constructing a project website and also making a detailed video . We will make them publicly available once they are done . ( 3 ) Analysis and visualization of the learned latent representation . We now have included additional visualizations and analysis in Appendix B.4 . For the interaction encoder and the latent code z_int , we visualize the learned latent space in Figure 4 with varying number of interactions . We believe such additional results help answer your questions : indeed , there \u2019 s a correlation between the embeddings and the actual properties , and different interaction sequences would result in close embedding as long as their corresponding physical properties are close . The figure also suggests that the values of k we used are sufficient for the latent space to resemble the original space of physical properties well . As for the visual encoder E_vis , it is directly built upon Geometry-Aware Recurrent Networks ( GRNNs ) [ 1 ] , and we would like to argue that the details of GRNNs themselves are not the main focus of this work , and we don \u2019 t consider deploying GRNNs in object pushing as one of our major contributions . We hope it \u2019 s reasonable to refer readers to the original paper [ 1 ] and its follow-up work [ 4 , 5 ] for the latent representation visualization and details for shape completion . However , we do agree that providing some visualizations would help appreciate how the visual model works in our specific scenarios . We have added Figure 5 in Appendix B.4 with some qualitative results to show how our model learns to complete missing shape information and predicts the top-down view under potential occlusions . Regarding your last question on \u201c how do different numbers of interactions affect the quality of the representation ? \u201d : The number of interactions only affects the produced interaction code z_int , as shown in Figure 4 , while z_vis ( produced by the visual encoder ) is a function of the image input only and is not affected by the interactions . ( 4 ) How does E_vis detect objects ? Our perception module E_vis directly uses GRNNs [ 1 ] , which allow us to do both object detection and top-down view prediction . GRNNs detect objects using a 3D object detector that operates over its latent 3D feature map of the scene M. The object detector maps the scene feature map M to a variable number of axis-aligned 3D bounding boxes of the objects ( and their segmentation masks if needed ) . Its architecture is similar to Mask R-CNN but instead uses RGB-D inputs and produces 3D bounding boxes . During our application , we crop the scene feature map using the object bounding boxes produced by GRNNs to obtain object-centric feature maps for shape encoding . We refer readers to [ 1 ] for the details of the detection pipeline . We do agree that adding some descriptions of it would make our paper more clear , so we have added some descriptions of the detection module in Section 3.1 . The accuracy of this detection module is also studied and reported in [ 1 ] . We believe explicitly evaluating the accuracy of such 3D detector is out of the scope of this paper since neither 3D detection nor using GRNNs for object pushing is within the contributions of our work . In our experiments , since the baselines used for pushing ( XYZ , Direct , MB-MAML and Expert-Ens ) all use the same detector to obtain object positions ( note that Expert-Ens only assumes access to ground-truth orientation , mass , and friction ) , we believe such comparison is fair and reasonable . However , since detection is a key stage in our dynamics prediction pipeline , we do agree that it \u2019 s good to present clear ablations to give readers an idea of how well such detector works . We have added a comparison between using ground-truth object positions and using detected object positions in Section B.1 . The results show that the dynamics prediction performance of our model using detected object positions is close to the one using gt object positions , suggesting that the 3D detector in our pipeline works reasonably well ."}}