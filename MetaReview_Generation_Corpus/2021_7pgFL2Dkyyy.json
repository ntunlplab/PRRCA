{"year": "2021", "forum": "7pgFL2Dkyyy", "title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "decision": "Accept (Poster)", "meta_review": "This paper got mixed reviews. One for reject and three for acceptance. The reviewers and authors have extensive discussion. Authors also provided additional experiments for further clarifying some questions from the reviewers. The paper has some clarify issue in the theoretical justification part as pointed out by AR1. Authors should extensively improve this part or revise the statement. However, the method proposed in this paper is simple and the results are indeed good. This paper is valuable and should be shared within the community to advance research on ZSL. Therefore, AC recommends acceptance. ", "reviews": [{"review_id": "7pgFL2Dkyyy-0", "review_text": "Summary : This paper presents a theoretical justification for normalization in model training on how it affects model performance and training time . It proposes two normalization tricks : normalize + scale trick and attributes normalization trick and apply in the zero-shot image classification task . This paper also shows that two normalization tricks are not enough to variance control in a deep architecture . To address this problem , a new initialization scheme is introduced . Apart from theoretical analysis and a new initialization scheme for normalization , it extends the zero-short learning approach in a continual learning framework . This new framework is called continual zero-shot learning ( CZSL ) and provides corresponding evaluation metrics . The experiments for CZSL are performed in two datasets , CUB and SUN . This paper experimentally shows the effectiveness of the initialization , normalization , and scaling trick . Strong Points : 1- Paper is well organized and easy to follow . 2- This paper took an interesting problem and developed a compelling investigation for how normalization affects performance . The theoretical justification for the normalization tricks sounds interesting and makes sense . 3- It introduced two new techniques for normalization and shows by the theoretical justification that only normalization techniques are not sufficient for proper model training . For good model training apart from normalization tricks , it introduced an initialization scheme . 4- Using normalization tricks and a new initialization scheme reduces a significant model training time compared to previous approaches . It presents training speed results for several baseline approaches . 5- Innovative attempts in introducing a new ZSL problem , and several evaluation metrics are proposed for continual ZSL . Weaknesses : 1- This paper presents a robust analysis of normalization , initialization , and scaling trick for ZSL . It also extends ZSL in continual learning . I appreciate the author 's effort for this solid analysis . I expect a new proposed model from authors to make the paper more strong . 2- Missing comparison : I recommend including paper [ a ] in the comparison table for CZSL . Approach [ a ] is the first proposed baseline for continual zero-shot learning . Therefore it must be included in the comparison table . 3- Some recent state-of-the approaches are missing in the comparison table for ZSL . Please compare it with [ b ] , [ c ] models . 4- Why have aPY dataset is not included in the experiments ? Does this model not perform well in aPY dataset ? 5- Is it possible for this normalization and scaling tricks for other applications such as object detection , action recognition , and image retrieval ? 6- I wonder by the training time you reported . I understand you have used quite a small neural network . Still , to have a clear view or fair comparison , you should have compared the timings with other initialization and default normalization and scaling tricks as well . [ a ] - Lifelong Zero-Shot Learning , by Kun Wei et al.IJCAI 2020 . [ b ] - Episode-Based Prototype Generating Network for Zero-Shot Learning , by Yu et al.CVPR2020 . [ c ] - Meta-Learning for Generalized Zero-Shot Learning , by Verma et al.AAAI 2020 . Rating Reason : This paper has included a well-detailed analysis and mathematical formulations for normalization , initialization about model training . But it does not propose a novel model , which limits the novelty of the model . CZSL formulation is also already explored in [ a ] .", "rating": "7: Good paper, accept", "reply_text": "> Is it possible for this normalization and scaling tricks for other applications such as object detection , action recognition , and image retrieval ? We think class normalization can be applied to other domains with multiple modalities and where $ y $ is a continuous variable as in zero-shot learning represented by class attributes or image-sentence retrieval . We chose to focus in this paper on the zero-shot learning and continual ZSL tasks , due to both their importance and clarity of evaluation . > You should have compared the timings with other initialization and default normalization and scaling tricks as well . Other initialization and normalization techniques would have precisely the same timings , but very low accuracy ( see table 5 ) . Our proposed model works that well because it follows a rigorously derived normalization scheme which adds negligible computation overhead . While modern ZSL methods also attain good performance \u2014 they achieve at much higher computational cost by using some sophisticated training , like GAN/VAE training with intricate loss terms or episode-based/meta-learning training schemes . This contrast is at the heart of our work : you do not need anything sophisticated to obtain SotA results , all you need is a good signal normalization ."}, {"review_id": "7pgFL2Dkyyy-1", "review_text": "The paper shows that normalization is critical for zero-shot learning ( ZSL ) . In the ZSL randomization is coming from the two sources , attribute and feature . Normalization of the two source helps to reduce the variance . The paper uses an embedding based model where normalize visual feature are projected to the attribute space and in the attribute space , cosine similarity is measured to predict the class label . Paper also extend ZSL framework to the continual learning ZSL ( CZSL ) setup where whole data are not present at a time ; instead , data comes in the form of a task , sequentially . The author shows that normalization helps to improve the CZSL result . Positive : 1 : The normalization is important in the NN/CNN model , in the ZSL , there are two sources of information , and proper normalization is important on both source for the better result . The paper gives a theoretical justification of why normalization is important and how we can do the same for the performance gain . 2 : The proposed normalization shows the significant performance gain on the standard dataset for the GZSL setup ( provided evaluation and code is correct ) . 3 : Recently generative model shows the SOTA result for the GZSL setting since they can synthesize the unseen class samples and easily can handle the data biasness . It is nice to see that the non-generative model shows a significant improvement using the simple method , and it is much faster to train . Comment : 1 : The main concern is the result , I am unable to understand from where the exact gain is coming . Many previous works use Normalize+scale or normalization in the supervised learning or meta-learning scenario ; it helps the generalization ability and smooth training and resulting in a performance gain . The performance gain using eq : [ 9 ] and [ 10 ] is expected , but [ 9 ] + [ 10 ] ( CN ) shows the much better result , I am unable to understand why this happens ? I request the author ; please explain the same . 2 : The proposed approach is an embedding based model , it does not generate the samples from the unseen classes , then how model overcome the data biasness towards the seen class ? Generally , it observes that seen-class shows the high accuracy and unseen-class show the low accuracy ; hence H-mean is very poor . The generative model can handle this scenario since they can generate data from the unseen class . The normalization technique does not help to overcome the data biasness towards the seen-class then how model handle the data biases ? 3 : I appreciate the theoretical justification and identifying the problem in the deep model and providing the solution for that ( section 3.4 ) 4 : The comparison with the few recent meta-learning based approach [ a ] [ b ] [ c ] for the ZSL are missing , can you show the result compared with these approaches ? Also , I request the author please provide the result if the same normalization is applied with the approach [ c ] ( it is also embedding based model in the meta-learning framework ) . [ a ] Episode-based Prototype Generating Network for Zero-Shot Learning , CVPR-20 [ b ] Meta-Learning for Generalized Zero-Shot Learning [ c ] Learning to Compare : Relation Network for Few-Shot Learning , CVPR=18 Overall I like the idea and contribution , but I suspect the provided result for the GZSL result in table-2 , I request the author please provide the code for the AWA2 and AWA1 dataset . I will further increase the score on the successful verification of the result . 4 : If you do n't use cosine similarity , then what is the dependency relation between the variance and weight W. I mean in the statement-1 if we do n't use normalization then how variance depends on weight W ? Also Var [ \\hat ( y ) _c ] is independent of W , but learning is not independent of the initialization of W , in this case , variance does not matter , proper learning and generalization are more important . 5 : I agree with the author that the provided evaluation metric for the CZSL is more generic and realistic . Here the model and CL procedure is not clear . What is Multi-task ? What is sequential ? How you ensure to overcome the catastrophic forgetting over the previous task while training the current task , The proper description is not provided , I request the author please provide the same . 6 : In continual learning scenario with the increase of task , the model performance is degraded . It is shown in the figure-9 , and 10 ( supplementary ) with the increase of the task model 's performance is increasing , it means that model does not forget anything and also you have backward transfer how this is possible ? Maybe I misunderstood something please explain .", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer2 for the valuable feedback . We here address the comments and will incorporate all the feedback . > The main concern is the result , I am unable to understand from where the exact gain is coming . Many previous works use Normalize+scale or normalization in the supervised learning or meta-learning scenario ; it helps the generalization ability and smooth training and resulting in a performance gain . The performance gain using eq : [ 9 ] and [ 10 ] is expected , but [ 9 ] + [ 10 ] ( CN ) shows the much better result , I am unable to understand why this happens ? I request the author ; please explain the same . As you correctly noted , incorporating CN in its \u201c full \u201d form ( i.e.using both eq 9 and eq 10 ) gives the best performance due to the better signal normalization and improved smoothness . The theoretical problem of using only eq 9 or eq 10 is that there is just no guarantee that this will lead to anything good . And from the practical perspective , we think that it should be beneficial to use eq 9 because standardization procedure smoothes the loss surface ( see Appendix F and [ 1 ] ) . For example , Table 2 shows that just using eq 9 gives close performance as using eq 9 + eq 10 . Eq 10 only does not perform well because without eq 9 it may make your signal vanish since what it does under the hood is just reducing the scale for the output matrix to make things align with the theory ( which in turn leads to the better performance ) . - [ 1 ] https : //arxiv.org/abs/1805.11604 > The proposed approach is an embedding based model , it does not generate the samples from the unseen classes , then how does the model overcome the data biasness towards the seen class ? Thanks for asking this interesting question . The \u201c trick \u201d is the following : previously ( ~before [ 1 ] ) , people trained a model which projects data onto the labels , i.e.a mapping X - > A from data space X to attribute space A . And this results in the very exact behaviour you foresee : the model becomes biased towards the seen representations which result in very low performance on the unseen . In our case , we follow the idea of [ 1 , 2 ] and learn the mapping A- > X . We describe the setting in Section 3 and depict the model on Figure 3 . Table 2 of [ 2 ] shows how much difference this makes compared to using X- > A projection . Since the model does not a direct access to seen data anymore , and its access to attributes happens only at the final discrimination phase , this avoids it getting biased towards the seen . Thank you for bringing this up , we will include this exposition in the updated version of the paper . - [ 1 ] Learning a Deep Embedding Model for Zero-Shot Learning Li Zhang , Tao Xiang , Shaogang Gong , https : //arxiv.org/abs/1611.05088 - [ 2 ] Rethinking Zero-Shot Learning : A Conditional Visual Classification Perspective , https : //arxiv.org/abs/1909.05995 > The comparison with the few recent meta-learning based approach [ a ] [ b ] [ c ] for the ZSL are missing , can you show the result compared with these approaches ? We already compare to [ a ] ( see Table 2 ) and we will shortly include the missing comparisons in the nearest update . We follow up on the remaining concerns ."}, {"review_id": "7pgFL2Dkyyy-2", "review_text": "= Summary : This paper provides a thorough analysis in the perspective of data variances on the widely used normalization tricks in the zero-shot learning research : normalize+scale and attribute normalization . It also demonstrates these tricks are not enough w.r.t.normalizing the variance in a non-linear model and propose a normalization trick to alleviate the issue . Both theoretical and empirical analysis are provided and results look convincing . Finally the authors propose a continual zero-shot learning problem scheme and illustrate some pioneering experimental results . = Reason for my score : There is rare work on the normalization trick in the context of zero-shot learning , although techniques like attribute normalization are widely used in practice . This paper investigates the normalization effect extensively for zero-shot learning , and provides many insightful thoughts for utilizing these tricks . The authors also evaluate the proposed class normalization with a simple implementation on benchmark datasets and show convincing results . Such work makes good contributions to the related community and hence I give my score . = Pros : 1 . The paper provides both theoretical and empirical analysis on the effect of commonly used normalization tricks for zero-shot learning , in the perspective of data variances . 2.The paper proposes a class normalization trick to alleviate the variance inflation/diminish in the non-linear model , and demonstrates its effectiveness on benchmark datasets . 3.The empirical analysis in the paper are extensive and convincing . 4.The paper also proposes a new framework of continual zero-shot learning . = Cons : 1 . The paper did n't evaluate on another widely used benchmark dataset aPY , can author explain the reason ? 2.On CUB dataset , the proposed method has a considerably large margin to the state-of-the-art methods , in contrast to other datasets . Is there any explanation on why it is the case ? Have you tried to explain this failure especially from the perspective of the proposed normalization trick ?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank Reviewer4 for the valuable input and for appreciating our contribution . We address below the raised concerns . > The paper did n't evaluate on another widely used benchmark dataset aPY , can author explain the reason ? The reason is that the very recent works tend not to benchmark on aPY . Examples from Table includes LsrGAN ( 2020 ) . DVBE ( 2020 ) , EPGN ( 2020 ) , TF-VAEGAN ( 2020 ) , F-VAEGAN-D2 ( 2019 ) . There is only a single method ( DVBE ) which computes the scores on aPY . After taking a deeper look , we think reasons include that small number of classes , and the very sparse attribute representation across classes questioning if it needs to be improved and enriched . .aPY Experiment . We launched our method on aPY without any hyperparameters tuning whatsoever and obtained the score of 38.9 . We believe that after tuning the hyperparameters , the scores could be improved . But even at the current state , this puts us on the third place after DVBE which has a score of 41.8 just right after CVC-ZSL with a score of 39.0 ( very similar to our score ) . We prepared a [ Google Colab notebook for aPY ] ( https : //gist.github.com/iclr2021-classnorm/cebb4dd7ca9ccc9a91d18b84aee056f7 ) that you can use to reproduce the results . We emphasize how simple our method is : all the code including data downloading/preprocessing/training/etc takes just ~150 lines of code . And for aPY , the training took just 8 seconds ! > On CUB dataset , the proposed method has a considerably large margin to the state-of-the-art methods , in contrast to other datasets . Is there any explanation on why it is the case ? Have you tried to explain this failure especially from the perspective of the proposed normalization trick ? We hypothesize that our method improves the scores by inherently allowing a model to better capture a signal from those attribute dimensions that are usually suppressed due to their low magnitude and by reducing the excessive influence of attributes with too high magnitudes . And for CUB , attributes are really well-balanced in terms of magnitudes compared to other datasets . We attach [ histograms of averaged attributes magnitudes values ] ( https : //www.dropbox.com/s/xxo2mhdhl76q7rh/attributes-avg-magnitudes-hists.pdf ? dl=0 ) for SUN , CUB , AwA and aPY . As you can see from these histograms , the attributes magnitude distribution for CUB has a much smaller tail , i.e.it is less \u201c skewed \u201d to the right . This makes the signal from large-magnitude attributes not suppress small-magnitude attributes as much as for other datasets . We achieve the best performance on AwA1/AwA2 and aPY datasets where the distribution is the most long-tailed ( as we said , for aPY we achieved a GZSL-H score of 38.9 with the very first try without any bells and whistles ) ."}, {"review_id": "7pgFL2Dkyyy-3", "review_text": "# Summary This paper claims to have 3 main contributions . C1 : Understanding/Theory . It explains why the two tricks work in zero-shot learning ( ZSL ) : ( i ) normalization + scaling in the compatibility function of the class features and the attributes , and ( ii ) attribute unit normalization . C2 : Method . It proposes a \u201c class normalization \u201d scheme ( Eq.9 and 10 ) and Fig.3.C2.1 From a \u201c theoretical explanation \u201d of C1 ( ii ) , this fixes ( ii ) in a \u201c deep \u201d ZSL model . C2.2 It improves \u201c smoothness \u201d of a \u201c irregular \u201d loss landscape in ZSL . C3 : Experiments . It demonstrates strong accuracy and training speed of the proposed approach in standard generalized ZSL . It also considers continual ZSL ( Sect.4 ) , in which the proposed method is evaluated via mean accuracy ( over timesteps ) accuracy metrics and a forgetting metric . # # # # Strengths S1 . Simple method . This is a simple feature-attribute scoring function via scaled cosine similarity ( with normalization ) . S2.Strong empirical results ( on both accuracy and training speed ) . See Table 2 . # Weaknesses W1 . Clarity The organization of the paper is such that the reader has to refer to the appendix a lot . My biggest concern on clarity is on the \u201c theoretical \u201d results which are not rigorous and at times unsupported . Further , some statements/claims are not precise or clear enough for me to be convinced that the method is well-motivated and is doing what it is claimed to be doing . W2.Soundness I have a lot of concerns and questions here as I read through Sect . 3.At a high-level , I don \u2019 t see a clear connection between \u201c improved variance control of prediction y^ or the smoothness of loss landscape \u201d and \u201c zero-shot learning effectiveness. \u201d Details below . This is in part due to poor clarity . W3.Experiments IMO , if the main claim is really about the effectiveness of the two tricks and the proposed class normalization , then the experiments should go beyond one zero-shot learning starting point 3-layer MLP ( Table 2 ) . - If baseline methods already adopt some of these tricks , it should be made clear and see if removing these tricks lead to inferior performance . - If baseline methods do not adopt some of these tricks , these tricks , especially class normalization , could be applied to show improved performance . If it is difficult to apply these tricks , further explanation should be given ( generally , also mention applicability of these tricks . ) This is done to some degree in the continual setting . W4.Related work As I mentioned in W3 , it is unclear which methods are linear/deep , and which methods have already benefited from existing/proposed tricks . # # # # Detailed comments ( mainly to clarify my points about weaknesses ) # # Statement 1 The main claim for this part is that this statement provides \u201c a theoretical understanding of the trick \u201d and \u201c allows to speed up the search [ of the optimal value fo \\gamma ] . \u201d However , I feel that we need further justifications on the correlation between Statement 1 ( variance of y^_c , \u201c better stability \u201d and \u201c the training would not stale \u201d ) and the zero-shot learning accuracy for this to be the \u201c why normalization + scaling works. \u201d My understanding is that the Appendix simply validates that Eq . ( 4 ) seems to hold in practice . Moreover , is the usual search region [ 5,10 ] actually effective ? Do we have stronger supporting empirical evidence than the three groups of practitioners ( Li et al 2019 , Zhang et al.2019 , Guo et al.2020 ) , who may have influenced each other , used it ? Finally , can the authors comment on the validity of multiple assumptions in Appendix A ? To which degrees does each of them hold in practice ? # # Statement 2 and 3 Why wouldn \u2019 t the following statement in Sect . 3.3 invalidate Statement 1 ? \u201c This may create an impression that it does not matter how we initialize the weights \u2014 normalization would undo any fluctuations . However it is not true , because it is still important how the signal flows , i.e.for an unnormalized and unscaled logit value \u201d It is unclear ( at least not from the beginning ) why understanding attribute normalization has to do with initialization of the weights . Similar to my comments to Statement 1 , why should we believe that the explanation in Sect . 3.3 and Sect . 3.4 is the reason for zero-shot learning effectiveness ? In particular , the authors again claim that the main bottleneck in improving zero-shot learning is \u201c variance control \u201d ( the end of Sect.3.3 ) .I also have a hard time understanding some statements in Appendix H , which is needed to motivate the following statement in Sect . 3.3 : \u201c And these assumptions are safe to assume only for z but not for a_c , because they do not hold for the standard datasets ( see Appendix H ) . \u201d H1 : Would this statement still be true after we transform a_c with an MLP ? H2 : Why is it not \u201c a sensible thing to do \u201d if we just want zero mean and unit variance ? H3 : Why is \u201c such an approach far from being scalable \u201d ? H4 : What if these are things like word embeddings ? H5 : Fig.12 and Fig.13 are not explained . H6 : Histograms in Fig.13 look quite normal . How useful is Statement 2 ? Why is the connection with Xavier initialization important ? Why is \u201c preserving the variance between z and y~ \u201d in Statement 3 important for zero-shot learning ? # # Improved smoothness The claim \u201c improved smoothness \u201d at the end of Sect . 3 and Appendix F is really hard to understand . F1 : How do the authors define \u201c irregular loss surface \u201d ? F2 : \u201c Santurkar et al . ( 2018 ) showed that batch-wise standardization procedure decreases the Lipschitz constant of a model , which suggests that our class-wise standardization will provide the same impact. \u201d This is not very precise and seems unsupported . Please make it clear how . If this is a hypothesis , please make it clear . Similarly to my comments to Statement 1-3 , how is improved smoothness related to zero-shot learning effectiveness ? # # Other more minor comments 1 . Abstract : Are the authors the one to \u201c generalize ZSL to a broader problem \u201d ? Please tone down the claim if not . 2.After Eq . ( 2 ) : Why does attribute normalization look \u201c inconsiderable \u201d ( possibly this is not the right word ? ) or why is it \u201c surprising \u201d that this is preferred in practice ? Don \u2019 t most zero-shot learning methods use this ( see for example Table 4 in [ A ] ) ? 3.Suggestions for references for attribute normalization . This can be improved ; I can trace this back to much earlier work such as [ A ] and [ B ] ( though I think this fact is stated more explicitly in [ A ] ) . 4.Under Table 1 \u201c These two tricks work well and normalize the variance to a unit value when the underlying ZSL model is linear ( see Figure 1 ) , but they fail when we use a multi-layer architecture. \u201d : Could the authors provide a reference to evidence to support this ? I think it is also important to provide a clear statement of what separates a \u201c linear \u201d or \u201c multi-layer \u201d model . 5.The first paragraph of Sect . 3 : Could you provide references for motivations for different activation functions ? Further , It is unclear that all of them perform normalization . 6.The second paragraph of Sect . 3 : What exactly limits \u201c the tools \u201d for zero-shot learning vs. supervised learning ? Further , it would also be nice to separate traditional supervised learning where classes are balanced and imbalanced ; see , e.g. , [ C ] . 7.What is the closest existing zero-shot model to the one the authors describe in Sect . 3.1 ? Why is the described model considered/selected ? [ A ] Synthesized Classifiers for Zero-Shot Learning [ B ] Zero-Shot Learning by Convex Combination of Semantic Embeddings [ C ] Class-Balanced Loss Based on Effective Number of Samples", "rating": "3: Clear rejection", "reply_text": "We appreciate Reviewer1 \u2019 s detailed and valuable feedback . R1 's review does not give credit to several efforts and key contributions in the paper , based on which we think there could be a misunderstanding . We hope it is alright that we highlight two key points that may help clarify our key message in the paper . We also acted with what is in our hands to improve the paper from an R1 perspective that we appreciated . # # # Misunderstandings : * * 1 ) Our method effectiveness and Experimental Validation . * * Apart from the theoretical analysis that we think is correct and shows desirable learning characteristics of our study on normalization in ZSL , we have very solid empirical results and we believe that you didn \u2019 t give us enough credit for them : - Strong empirical results for ZSL on four datasets : our method is very simple and effective ( which we believe to be an important result ) , trains very fast ( about 1 minute ) , and beats modern SotA which usually follows a very complicated design and takes > 1 hour to train . - We proposed a more rigorous formulation of CZSL as a direct generalization of ZSL + 5 novel metrics for it . We tested several continual learning benchmarks and showed that our method improved several baselines by a lot ( ~40 % on average ) . We showed the value of CN on five CL methods ( A-GEM , EWC , A-GEM , Sequential , and Multitask ) on CUB and SUN datasets . * * 2 ) Theoretical Analysis . * * We believe that the judgment used to evaluate our theoretical claims seems less related to our focus . We understood it as \u201c statement X shows that property Y holds , but this does not show that it improves ZSL performance . Thus , statement X is not sound/not rigorous. \u201d While it is true that we do not prove the increased performance for ZSL directly , we think this does not make our theoretical analysis not sound as we detail below : - For each statement , we provided very solid empirical evidence and showed that it correlates with the good ZSL performance in practice . For example , our proposed model has more steady and closer-to-unit variance ( shown by statement 3 and figures 1,5,6,7,8 ) while other methods do not \u2014 and it also outperforms them in terms of performance . Or it has a smoother loss landscape , shown from the exposition in Appendix F. - The current state of deep learning theory is at such a level that proving that some property would directly increase the performance is almost always out of reach unless you have a very specific simplified model ( like infinite-width networks ) trained for a very well-studied task like binary classification where you have a lot of existing theoretical tricks to employ . Let 's consider for example the task we are solving : investigating the analysis of variance inside a network . One of the most famous early works on it is Xavier init [ 1 ] . In their paper , authors derive a proper initialization scale for neural network weights that would preserve the variance during a forward or backward pass \u2014 an analysis which is similar in spirit to what we did for ZSL models . Then they provide strong empirical evidence of why it is a good thing ( which we also do ! ) by showing that the theoretical claims hold in practice and correlates with the improved performance . But they do not have any proof that the variance preservation directly improves a model \u2019 s performance \u2014 would you discard their contributions as well ? And we still do not have a rigorous theoretical connection between this \u201c variance preservation \u201d and final model performance . Why ? Because it is too hard . There is a series of works on dynamical isometry [ 2,3,4 ] that took the authors years to develop , and we still do not have an exact connection between the signal propagation and the model scores ! In their works , they show the \u201c trainability \u201d of a model depending on the initialization but this \u201c trainability \u201d is not directly connected to the performance , but rather as a property of the Jacobian matrix \u2014 which , we emphasize it again \u2014 is not directly connected to the score . With all this being said , we disagree with how you evaluate theoretical analysis . And we believe that it is a decent way to develop the understanding of deep learning the way it is done in the paper : first , formulate and prove a statement that some property holds ; and second , demonstrate rigorous evidence that supports both the statement and how it correlates with the performance . - [ 1 ] http : //proceedings.mlr.press/v9/glorot10a/glorot10a.pdf - [ 2 ] https : //arxiv.org/abs/1711.04735 - [ 3 ] https : //arxiv.org/abs/1806.05393 - [ 4 ] https : //arxiv.org/abs/1901.08987"}], "0": {"review_id": "7pgFL2Dkyyy-0", "review_text": "Summary : This paper presents a theoretical justification for normalization in model training on how it affects model performance and training time . It proposes two normalization tricks : normalize + scale trick and attributes normalization trick and apply in the zero-shot image classification task . This paper also shows that two normalization tricks are not enough to variance control in a deep architecture . To address this problem , a new initialization scheme is introduced . Apart from theoretical analysis and a new initialization scheme for normalization , it extends the zero-short learning approach in a continual learning framework . This new framework is called continual zero-shot learning ( CZSL ) and provides corresponding evaluation metrics . The experiments for CZSL are performed in two datasets , CUB and SUN . This paper experimentally shows the effectiveness of the initialization , normalization , and scaling trick . Strong Points : 1- Paper is well organized and easy to follow . 2- This paper took an interesting problem and developed a compelling investigation for how normalization affects performance . The theoretical justification for the normalization tricks sounds interesting and makes sense . 3- It introduced two new techniques for normalization and shows by the theoretical justification that only normalization techniques are not sufficient for proper model training . For good model training apart from normalization tricks , it introduced an initialization scheme . 4- Using normalization tricks and a new initialization scheme reduces a significant model training time compared to previous approaches . It presents training speed results for several baseline approaches . 5- Innovative attempts in introducing a new ZSL problem , and several evaluation metrics are proposed for continual ZSL . Weaknesses : 1- This paper presents a robust analysis of normalization , initialization , and scaling trick for ZSL . It also extends ZSL in continual learning . I appreciate the author 's effort for this solid analysis . I expect a new proposed model from authors to make the paper more strong . 2- Missing comparison : I recommend including paper [ a ] in the comparison table for CZSL . Approach [ a ] is the first proposed baseline for continual zero-shot learning . Therefore it must be included in the comparison table . 3- Some recent state-of-the approaches are missing in the comparison table for ZSL . Please compare it with [ b ] , [ c ] models . 4- Why have aPY dataset is not included in the experiments ? Does this model not perform well in aPY dataset ? 5- Is it possible for this normalization and scaling tricks for other applications such as object detection , action recognition , and image retrieval ? 6- I wonder by the training time you reported . I understand you have used quite a small neural network . Still , to have a clear view or fair comparison , you should have compared the timings with other initialization and default normalization and scaling tricks as well . [ a ] - Lifelong Zero-Shot Learning , by Kun Wei et al.IJCAI 2020 . [ b ] - Episode-Based Prototype Generating Network for Zero-Shot Learning , by Yu et al.CVPR2020 . [ c ] - Meta-Learning for Generalized Zero-Shot Learning , by Verma et al.AAAI 2020 . Rating Reason : This paper has included a well-detailed analysis and mathematical formulations for normalization , initialization about model training . But it does not propose a novel model , which limits the novelty of the model . CZSL formulation is also already explored in [ a ] .", "rating": "7: Good paper, accept", "reply_text": "> Is it possible for this normalization and scaling tricks for other applications such as object detection , action recognition , and image retrieval ? We think class normalization can be applied to other domains with multiple modalities and where $ y $ is a continuous variable as in zero-shot learning represented by class attributes or image-sentence retrieval . We chose to focus in this paper on the zero-shot learning and continual ZSL tasks , due to both their importance and clarity of evaluation . > You should have compared the timings with other initialization and default normalization and scaling tricks as well . Other initialization and normalization techniques would have precisely the same timings , but very low accuracy ( see table 5 ) . Our proposed model works that well because it follows a rigorously derived normalization scheme which adds negligible computation overhead . While modern ZSL methods also attain good performance \u2014 they achieve at much higher computational cost by using some sophisticated training , like GAN/VAE training with intricate loss terms or episode-based/meta-learning training schemes . This contrast is at the heart of our work : you do not need anything sophisticated to obtain SotA results , all you need is a good signal normalization ."}, "1": {"review_id": "7pgFL2Dkyyy-1", "review_text": "The paper shows that normalization is critical for zero-shot learning ( ZSL ) . In the ZSL randomization is coming from the two sources , attribute and feature . Normalization of the two source helps to reduce the variance . The paper uses an embedding based model where normalize visual feature are projected to the attribute space and in the attribute space , cosine similarity is measured to predict the class label . Paper also extend ZSL framework to the continual learning ZSL ( CZSL ) setup where whole data are not present at a time ; instead , data comes in the form of a task , sequentially . The author shows that normalization helps to improve the CZSL result . Positive : 1 : The normalization is important in the NN/CNN model , in the ZSL , there are two sources of information , and proper normalization is important on both source for the better result . The paper gives a theoretical justification of why normalization is important and how we can do the same for the performance gain . 2 : The proposed normalization shows the significant performance gain on the standard dataset for the GZSL setup ( provided evaluation and code is correct ) . 3 : Recently generative model shows the SOTA result for the GZSL setting since they can synthesize the unseen class samples and easily can handle the data biasness . It is nice to see that the non-generative model shows a significant improvement using the simple method , and it is much faster to train . Comment : 1 : The main concern is the result , I am unable to understand from where the exact gain is coming . Many previous works use Normalize+scale or normalization in the supervised learning or meta-learning scenario ; it helps the generalization ability and smooth training and resulting in a performance gain . The performance gain using eq : [ 9 ] and [ 10 ] is expected , but [ 9 ] + [ 10 ] ( CN ) shows the much better result , I am unable to understand why this happens ? I request the author ; please explain the same . 2 : The proposed approach is an embedding based model , it does not generate the samples from the unseen classes , then how model overcome the data biasness towards the seen class ? Generally , it observes that seen-class shows the high accuracy and unseen-class show the low accuracy ; hence H-mean is very poor . The generative model can handle this scenario since they can generate data from the unseen class . The normalization technique does not help to overcome the data biasness towards the seen-class then how model handle the data biases ? 3 : I appreciate the theoretical justification and identifying the problem in the deep model and providing the solution for that ( section 3.4 ) 4 : The comparison with the few recent meta-learning based approach [ a ] [ b ] [ c ] for the ZSL are missing , can you show the result compared with these approaches ? Also , I request the author please provide the result if the same normalization is applied with the approach [ c ] ( it is also embedding based model in the meta-learning framework ) . [ a ] Episode-based Prototype Generating Network for Zero-Shot Learning , CVPR-20 [ b ] Meta-Learning for Generalized Zero-Shot Learning [ c ] Learning to Compare : Relation Network for Few-Shot Learning , CVPR=18 Overall I like the idea and contribution , but I suspect the provided result for the GZSL result in table-2 , I request the author please provide the code for the AWA2 and AWA1 dataset . I will further increase the score on the successful verification of the result . 4 : If you do n't use cosine similarity , then what is the dependency relation between the variance and weight W. I mean in the statement-1 if we do n't use normalization then how variance depends on weight W ? Also Var [ \\hat ( y ) _c ] is independent of W , but learning is not independent of the initialization of W , in this case , variance does not matter , proper learning and generalization are more important . 5 : I agree with the author that the provided evaluation metric for the CZSL is more generic and realistic . Here the model and CL procedure is not clear . What is Multi-task ? What is sequential ? How you ensure to overcome the catastrophic forgetting over the previous task while training the current task , The proper description is not provided , I request the author please provide the same . 6 : In continual learning scenario with the increase of task , the model performance is degraded . It is shown in the figure-9 , and 10 ( supplementary ) with the increase of the task model 's performance is increasing , it means that model does not forget anything and also you have backward transfer how this is possible ? Maybe I misunderstood something please explain .", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer2 for the valuable feedback . We here address the comments and will incorporate all the feedback . > The main concern is the result , I am unable to understand from where the exact gain is coming . Many previous works use Normalize+scale or normalization in the supervised learning or meta-learning scenario ; it helps the generalization ability and smooth training and resulting in a performance gain . The performance gain using eq : [ 9 ] and [ 10 ] is expected , but [ 9 ] + [ 10 ] ( CN ) shows the much better result , I am unable to understand why this happens ? I request the author ; please explain the same . As you correctly noted , incorporating CN in its \u201c full \u201d form ( i.e.using both eq 9 and eq 10 ) gives the best performance due to the better signal normalization and improved smoothness . The theoretical problem of using only eq 9 or eq 10 is that there is just no guarantee that this will lead to anything good . And from the practical perspective , we think that it should be beneficial to use eq 9 because standardization procedure smoothes the loss surface ( see Appendix F and [ 1 ] ) . For example , Table 2 shows that just using eq 9 gives close performance as using eq 9 + eq 10 . Eq 10 only does not perform well because without eq 9 it may make your signal vanish since what it does under the hood is just reducing the scale for the output matrix to make things align with the theory ( which in turn leads to the better performance ) . - [ 1 ] https : //arxiv.org/abs/1805.11604 > The proposed approach is an embedding based model , it does not generate the samples from the unseen classes , then how does the model overcome the data biasness towards the seen class ? Thanks for asking this interesting question . The \u201c trick \u201d is the following : previously ( ~before [ 1 ] ) , people trained a model which projects data onto the labels , i.e.a mapping X - > A from data space X to attribute space A . And this results in the very exact behaviour you foresee : the model becomes biased towards the seen representations which result in very low performance on the unseen . In our case , we follow the idea of [ 1 , 2 ] and learn the mapping A- > X . We describe the setting in Section 3 and depict the model on Figure 3 . Table 2 of [ 2 ] shows how much difference this makes compared to using X- > A projection . Since the model does not a direct access to seen data anymore , and its access to attributes happens only at the final discrimination phase , this avoids it getting biased towards the seen . Thank you for bringing this up , we will include this exposition in the updated version of the paper . - [ 1 ] Learning a Deep Embedding Model for Zero-Shot Learning Li Zhang , Tao Xiang , Shaogang Gong , https : //arxiv.org/abs/1611.05088 - [ 2 ] Rethinking Zero-Shot Learning : A Conditional Visual Classification Perspective , https : //arxiv.org/abs/1909.05995 > The comparison with the few recent meta-learning based approach [ a ] [ b ] [ c ] for the ZSL are missing , can you show the result compared with these approaches ? We already compare to [ a ] ( see Table 2 ) and we will shortly include the missing comparisons in the nearest update . We follow up on the remaining concerns ."}, "2": {"review_id": "7pgFL2Dkyyy-2", "review_text": "= Summary : This paper provides a thorough analysis in the perspective of data variances on the widely used normalization tricks in the zero-shot learning research : normalize+scale and attribute normalization . It also demonstrates these tricks are not enough w.r.t.normalizing the variance in a non-linear model and propose a normalization trick to alleviate the issue . Both theoretical and empirical analysis are provided and results look convincing . Finally the authors propose a continual zero-shot learning problem scheme and illustrate some pioneering experimental results . = Reason for my score : There is rare work on the normalization trick in the context of zero-shot learning , although techniques like attribute normalization are widely used in practice . This paper investigates the normalization effect extensively for zero-shot learning , and provides many insightful thoughts for utilizing these tricks . The authors also evaluate the proposed class normalization with a simple implementation on benchmark datasets and show convincing results . Such work makes good contributions to the related community and hence I give my score . = Pros : 1 . The paper provides both theoretical and empirical analysis on the effect of commonly used normalization tricks for zero-shot learning , in the perspective of data variances . 2.The paper proposes a class normalization trick to alleviate the variance inflation/diminish in the non-linear model , and demonstrates its effectiveness on benchmark datasets . 3.The empirical analysis in the paper are extensive and convincing . 4.The paper also proposes a new framework of continual zero-shot learning . = Cons : 1 . The paper did n't evaluate on another widely used benchmark dataset aPY , can author explain the reason ? 2.On CUB dataset , the proposed method has a considerably large margin to the state-of-the-art methods , in contrast to other datasets . Is there any explanation on why it is the case ? Have you tried to explain this failure especially from the perspective of the proposed normalization trick ?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank Reviewer4 for the valuable input and for appreciating our contribution . We address below the raised concerns . > The paper did n't evaluate on another widely used benchmark dataset aPY , can author explain the reason ? The reason is that the very recent works tend not to benchmark on aPY . Examples from Table includes LsrGAN ( 2020 ) . DVBE ( 2020 ) , EPGN ( 2020 ) , TF-VAEGAN ( 2020 ) , F-VAEGAN-D2 ( 2019 ) . There is only a single method ( DVBE ) which computes the scores on aPY . After taking a deeper look , we think reasons include that small number of classes , and the very sparse attribute representation across classes questioning if it needs to be improved and enriched . .aPY Experiment . We launched our method on aPY without any hyperparameters tuning whatsoever and obtained the score of 38.9 . We believe that after tuning the hyperparameters , the scores could be improved . But even at the current state , this puts us on the third place after DVBE which has a score of 41.8 just right after CVC-ZSL with a score of 39.0 ( very similar to our score ) . We prepared a [ Google Colab notebook for aPY ] ( https : //gist.github.com/iclr2021-classnorm/cebb4dd7ca9ccc9a91d18b84aee056f7 ) that you can use to reproduce the results . We emphasize how simple our method is : all the code including data downloading/preprocessing/training/etc takes just ~150 lines of code . And for aPY , the training took just 8 seconds ! > On CUB dataset , the proposed method has a considerably large margin to the state-of-the-art methods , in contrast to other datasets . Is there any explanation on why it is the case ? Have you tried to explain this failure especially from the perspective of the proposed normalization trick ? We hypothesize that our method improves the scores by inherently allowing a model to better capture a signal from those attribute dimensions that are usually suppressed due to their low magnitude and by reducing the excessive influence of attributes with too high magnitudes . And for CUB , attributes are really well-balanced in terms of magnitudes compared to other datasets . We attach [ histograms of averaged attributes magnitudes values ] ( https : //www.dropbox.com/s/xxo2mhdhl76q7rh/attributes-avg-magnitudes-hists.pdf ? dl=0 ) for SUN , CUB , AwA and aPY . As you can see from these histograms , the attributes magnitude distribution for CUB has a much smaller tail , i.e.it is less \u201c skewed \u201d to the right . This makes the signal from large-magnitude attributes not suppress small-magnitude attributes as much as for other datasets . We achieve the best performance on AwA1/AwA2 and aPY datasets where the distribution is the most long-tailed ( as we said , for aPY we achieved a GZSL-H score of 38.9 with the very first try without any bells and whistles ) ."}, "3": {"review_id": "7pgFL2Dkyyy-3", "review_text": "# Summary This paper claims to have 3 main contributions . C1 : Understanding/Theory . It explains why the two tricks work in zero-shot learning ( ZSL ) : ( i ) normalization + scaling in the compatibility function of the class features and the attributes , and ( ii ) attribute unit normalization . C2 : Method . It proposes a \u201c class normalization \u201d scheme ( Eq.9 and 10 ) and Fig.3.C2.1 From a \u201c theoretical explanation \u201d of C1 ( ii ) , this fixes ( ii ) in a \u201c deep \u201d ZSL model . C2.2 It improves \u201c smoothness \u201d of a \u201c irregular \u201d loss landscape in ZSL . C3 : Experiments . It demonstrates strong accuracy and training speed of the proposed approach in standard generalized ZSL . It also considers continual ZSL ( Sect.4 ) , in which the proposed method is evaluated via mean accuracy ( over timesteps ) accuracy metrics and a forgetting metric . # # # # Strengths S1 . Simple method . This is a simple feature-attribute scoring function via scaled cosine similarity ( with normalization ) . S2.Strong empirical results ( on both accuracy and training speed ) . See Table 2 . # Weaknesses W1 . Clarity The organization of the paper is such that the reader has to refer to the appendix a lot . My biggest concern on clarity is on the \u201c theoretical \u201d results which are not rigorous and at times unsupported . Further , some statements/claims are not precise or clear enough for me to be convinced that the method is well-motivated and is doing what it is claimed to be doing . W2.Soundness I have a lot of concerns and questions here as I read through Sect . 3.At a high-level , I don \u2019 t see a clear connection between \u201c improved variance control of prediction y^ or the smoothness of loss landscape \u201d and \u201c zero-shot learning effectiveness. \u201d Details below . This is in part due to poor clarity . W3.Experiments IMO , if the main claim is really about the effectiveness of the two tricks and the proposed class normalization , then the experiments should go beyond one zero-shot learning starting point 3-layer MLP ( Table 2 ) . - If baseline methods already adopt some of these tricks , it should be made clear and see if removing these tricks lead to inferior performance . - If baseline methods do not adopt some of these tricks , these tricks , especially class normalization , could be applied to show improved performance . If it is difficult to apply these tricks , further explanation should be given ( generally , also mention applicability of these tricks . ) This is done to some degree in the continual setting . W4.Related work As I mentioned in W3 , it is unclear which methods are linear/deep , and which methods have already benefited from existing/proposed tricks . # # # # Detailed comments ( mainly to clarify my points about weaknesses ) # # Statement 1 The main claim for this part is that this statement provides \u201c a theoretical understanding of the trick \u201d and \u201c allows to speed up the search [ of the optimal value fo \\gamma ] . \u201d However , I feel that we need further justifications on the correlation between Statement 1 ( variance of y^_c , \u201c better stability \u201d and \u201c the training would not stale \u201d ) and the zero-shot learning accuracy for this to be the \u201c why normalization + scaling works. \u201d My understanding is that the Appendix simply validates that Eq . ( 4 ) seems to hold in practice . Moreover , is the usual search region [ 5,10 ] actually effective ? Do we have stronger supporting empirical evidence than the three groups of practitioners ( Li et al 2019 , Zhang et al.2019 , Guo et al.2020 ) , who may have influenced each other , used it ? Finally , can the authors comment on the validity of multiple assumptions in Appendix A ? To which degrees does each of them hold in practice ? # # Statement 2 and 3 Why wouldn \u2019 t the following statement in Sect . 3.3 invalidate Statement 1 ? \u201c This may create an impression that it does not matter how we initialize the weights \u2014 normalization would undo any fluctuations . However it is not true , because it is still important how the signal flows , i.e.for an unnormalized and unscaled logit value \u201d It is unclear ( at least not from the beginning ) why understanding attribute normalization has to do with initialization of the weights . Similar to my comments to Statement 1 , why should we believe that the explanation in Sect . 3.3 and Sect . 3.4 is the reason for zero-shot learning effectiveness ? In particular , the authors again claim that the main bottleneck in improving zero-shot learning is \u201c variance control \u201d ( the end of Sect.3.3 ) .I also have a hard time understanding some statements in Appendix H , which is needed to motivate the following statement in Sect . 3.3 : \u201c And these assumptions are safe to assume only for z but not for a_c , because they do not hold for the standard datasets ( see Appendix H ) . \u201d H1 : Would this statement still be true after we transform a_c with an MLP ? H2 : Why is it not \u201c a sensible thing to do \u201d if we just want zero mean and unit variance ? H3 : Why is \u201c such an approach far from being scalable \u201d ? H4 : What if these are things like word embeddings ? H5 : Fig.12 and Fig.13 are not explained . H6 : Histograms in Fig.13 look quite normal . How useful is Statement 2 ? Why is the connection with Xavier initialization important ? Why is \u201c preserving the variance between z and y~ \u201d in Statement 3 important for zero-shot learning ? # # Improved smoothness The claim \u201c improved smoothness \u201d at the end of Sect . 3 and Appendix F is really hard to understand . F1 : How do the authors define \u201c irregular loss surface \u201d ? F2 : \u201c Santurkar et al . ( 2018 ) showed that batch-wise standardization procedure decreases the Lipschitz constant of a model , which suggests that our class-wise standardization will provide the same impact. \u201d This is not very precise and seems unsupported . Please make it clear how . If this is a hypothesis , please make it clear . Similarly to my comments to Statement 1-3 , how is improved smoothness related to zero-shot learning effectiveness ? # # Other more minor comments 1 . Abstract : Are the authors the one to \u201c generalize ZSL to a broader problem \u201d ? Please tone down the claim if not . 2.After Eq . ( 2 ) : Why does attribute normalization look \u201c inconsiderable \u201d ( possibly this is not the right word ? ) or why is it \u201c surprising \u201d that this is preferred in practice ? Don \u2019 t most zero-shot learning methods use this ( see for example Table 4 in [ A ] ) ? 3.Suggestions for references for attribute normalization . This can be improved ; I can trace this back to much earlier work such as [ A ] and [ B ] ( though I think this fact is stated more explicitly in [ A ] ) . 4.Under Table 1 \u201c These two tricks work well and normalize the variance to a unit value when the underlying ZSL model is linear ( see Figure 1 ) , but they fail when we use a multi-layer architecture. \u201d : Could the authors provide a reference to evidence to support this ? I think it is also important to provide a clear statement of what separates a \u201c linear \u201d or \u201c multi-layer \u201d model . 5.The first paragraph of Sect . 3 : Could you provide references for motivations for different activation functions ? Further , It is unclear that all of them perform normalization . 6.The second paragraph of Sect . 3 : What exactly limits \u201c the tools \u201d for zero-shot learning vs. supervised learning ? Further , it would also be nice to separate traditional supervised learning where classes are balanced and imbalanced ; see , e.g. , [ C ] . 7.What is the closest existing zero-shot model to the one the authors describe in Sect . 3.1 ? Why is the described model considered/selected ? [ A ] Synthesized Classifiers for Zero-Shot Learning [ B ] Zero-Shot Learning by Convex Combination of Semantic Embeddings [ C ] Class-Balanced Loss Based on Effective Number of Samples", "rating": "3: Clear rejection", "reply_text": "We appreciate Reviewer1 \u2019 s detailed and valuable feedback . R1 's review does not give credit to several efforts and key contributions in the paper , based on which we think there could be a misunderstanding . We hope it is alright that we highlight two key points that may help clarify our key message in the paper . We also acted with what is in our hands to improve the paper from an R1 perspective that we appreciated . # # # Misunderstandings : * * 1 ) Our method effectiveness and Experimental Validation . * * Apart from the theoretical analysis that we think is correct and shows desirable learning characteristics of our study on normalization in ZSL , we have very solid empirical results and we believe that you didn \u2019 t give us enough credit for them : - Strong empirical results for ZSL on four datasets : our method is very simple and effective ( which we believe to be an important result ) , trains very fast ( about 1 minute ) , and beats modern SotA which usually follows a very complicated design and takes > 1 hour to train . - We proposed a more rigorous formulation of CZSL as a direct generalization of ZSL + 5 novel metrics for it . We tested several continual learning benchmarks and showed that our method improved several baselines by a lot ( ~40 % on average ) . We showed the value of CN on five CL methods ( A-GEM , EWC , A-GEM , Sequential , and Multitask ) on CUB and SUN datasets . * * 2 ) Theoretical Analysis . * * We believe that the judgment used to evaluate our theoretical claims seems less related to our focus . We understood it as \u201c statement X shows that property Y holds , but this does not show that it improves ZSL performance . Thus , statement X is not sound/not rigorous. \u201d While it is true that we do not prove the increased performance for ZSL directly , we think this does not make our theoretical analysis not sound as we detail below : - For each statement , we provided very solid empirical evidence and showed that it correlates with the good ZSL performance in practice . For example , our proposed model has more steady and closer-to-unit variance ( shown by statement 3 and figures 1,5,6,7,8 ) while other methods do not \u2014 and it also outperforms them in terms of performance . Or it has a smoother loss landscape , shown from the exposition in Appendix F. - The current state of deep learning theory is at such a level that proving that some property would directly increase the performance is almost always out of reach unless you have a very specific simplified model ( like infinite-width networks ) trained for a very well-studied task like binary classification where you have a lot of existing theoretical tricks to employ . Let 's consider for example the task we are solving : investigating the analysis of variance inside a network . One of the most famous early works on it is Xavier init [ 1 ] . In their paper , authors derive a proper initialization scale for neural network weights that would preserve the variance during a forward or backward pass \u2014 an analysis which is similar in spirit to what we did for ZSL models . Then they provide strong empirical evidence of why it is a good thing ( which we also do ! ) by showing that the theoretical claims hold in practice and correlates with the improved performance . But they do not have any proof that the variance preservation directly improves a model \u2019 s performance \u2014 would you discard their contributions as well ? And we still do not have a rigorous theoretical connection between this \u201c variance preservation \u201d and final model performance . Why ? Because it is too hard . There is a series of works on dynamical isometry [ 2,3,4 ] that took the authors years to develop , and we still do not have an exact connection between the signal propagation and the model scores ! In their works , they show the \u201c trainability \u201d of a model depending on the initialization but this \u201c trainability \u201d is not directly connected to the performance , but rather as a property of the Jacobian matrix \u2014 which , we emphasize it again \u2014 is not directly connected to the score . With all this being said , we disagree with how you evaluate theoretical analysis . And we believe that it is a decent way to develop the understanding of deep learning the way it is done in the paper : first , formulate and prove a statement that some property holds ; and second , demonstrate rigorous evidence that supports both the statement and how it correlates with the performance . - [ 1 ] http : //proceedings.mlr.press/v9/glorot10a/glorot10a.pdf - [ 2 ] https : //arxiv.org/abs/1711.04735 - [ 3 ] https : //arxiv.org/abs/1806.05393 - [ 4 ] https : //arxiv.org/abs/1901.08987"}}