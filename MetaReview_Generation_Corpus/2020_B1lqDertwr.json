{"year": "2020", "forum": "B1lqDertwr", "title": "Regularization Matters in Policy Optimization", "decision": "Reject", "meta_review": "This paper proposes an analysis of regularization for policy optimization. While the multiple effects of regularization are well known in the statistics and optimization community, it is less the case in the RL community. This makes the novelty of the paper difficult to judge as it depends on the familiarity of RL researchers with the two aforementioned communities.\n\nBesides the novelty aspect, which is debatable, reviewers had doubts on the significance of the results, and in particular on the metrics chosen (based on the rank). While defining a \"best\" algorithm is notoriously difficult, and could be considered outside of the scope of this paper, the fact is that the conclusions reached are still sensitive to that difficulty.\n\nI thus regret to reject this paper as I feel not much more work is necessary to provide a compelling story. I encourage the authors to extend their choice of metrics to be more convincing in their conclusions.", "reviews": [{"review_id": "B1lqDertwr-0", "review_text": "An interesting paper on the role of regularization in policy optimization In this paper, the authors study a set of existing direct policy optimization methods in the field of reinforcement learning. The authors provide a detailed investigation of the effect of regulations on the performance and behavior of agents following these methods. The authors present that regularization methods mostly help to improve the agents' performance in terms of final scores. Specifically, they show that direct regularizations on model parameters, such as the standard case of L2 or L1 regularization, generally improve the agent performance. They also show that these regularizations, in their study, is more proper than entropy regularization. The authors also show that, in the presence of such regularizations, the learning algorithms become less sensitive to the hyperparameters. Few comments: 1) The paper is well written and easy to follow. I appreciate it. I found the writing of the paper has a bit of repetition. The authors might find it slightly more proper to remove some of the repetitions (e.g. section 4.2) 2) While I appreciate the clear writing and reasoning in this paper, I might suggest a slight change in the second paraphrase of the intro. I agree with the authors' reason on the first three lines, but I think it would be useful to also emphasize the role of the questions the researchers investigate to answer. I might also add one the main reason that the researchers in the field of DRL have spent less time on regulation or architecture search was their focus on more high-level algorithm design which is in the more immediate step of relevance and specialty to the field of reinforcement learning. 3) I would suggest rephrasing the last two sentences of the second paragraph in related work: \"Also, these techniques consider ...\". Regularizing the output also regularizes the parameters, I think the authors' point was \"directly regularize\" the parameters. 4) In the \"Entropy Regularization\" part of section 3, I guess the Hs has not been defined. 5) Repeated \"the\" in the last paragraph of section 4.1 (despite it already incorporates the the maximization of) 6) The authors used the term \"not converge\" multiple times. While it is hard from the plots to see whether the series converges or not, I have a strong feeling that by this term the authors mean the algorithm does not converge to a resealable solution rather than being divergent up to a bandwidth. Maybe clarifying would be helpful. 7) In section 5, the authors study the sensitivity to the hyperparameters. In this section, I had a hard time to understand the role of term 3 \"BN and dropout hurts on-policy algorithms but can bring improvement only for the off-policy SAC algorithm.\" Does it mean that deploying BN, results in a more sensitive algorithm? or it means that the performance degrades (which is a different topic than section 5 is supposed to serve)? 8) In section 7, the authors put out a hypothesis \" However, there is still generalization between samples: the agents are only trained on the limited\" but the provided empirical study might not fully be considered to be designed to test this hypothesis. In order to test this hypothesis, the author might be interested in training the models with bigger sample sizes, more training iteration, different function classes, and more fitting in order to test this hypothesis. 9) Section 7 on \"Why do BN and dropout work only with off-policy algorithms?\" while I agree with the authors on their first reason which is quite commonly known, I might hesitate to make the second statement (2) Generally, I found this paper an interesting paper and appreciate the authors for their careful empirical study. But I found the contribution of this work to be not significant enough. Most of the statements and claims in this paper are well know in the community, especially among deep learning practitioners. While I acknowledge the scientific value of this study, its concreteness, and appreciate the contribution of this paper, due to the low acceptance rate of this conference, I might be reluctant in accepting this paper. ", "rating": "3: Weak Reject", "reply_text": "References : [ 1 ] Peter Henderson , Riashat Islam , Philip Bachman , Joelle Pineau , Doina Precup , and David Meger.Deep reinforcement learning that matters . In Thirty-Second AAAI Conference on Artificial Intelli-gence , 2018 . [ 2 ] Islam , R. , Henderson , P. , Gomrokchi , M. , and Precup , D. ( 2017 ) . Reproducibility of benchmarked deep reinforcement learning tasks for continuous control . In ICML 2017 Reproducibility in Machine Learning Workshop . [ 3 ] https : //github.com/jwyang/faster-rcnn.pytorch/blob/master/lib/model/faster_rcnn/resnet.py # L288 [ 4 ] https : //github.com/torch/nn/issues/873 [ 5 ] Zafarali Ahmed , Nicolas Le Roux , Mohammad Norouzi , and Dale Schuurmans . Understanding the impact of entropy in policy learning . arXiv:1811.11214 , 2018 . [ 6 ] Volodymyr Mnih , Adria Puigdomenech Badia , Mehdi Mirza , Alex Graves , Timothy Lillicrap , TimHarley , David Silver , and Koray Kavukcuoglu . Asynchronous methods for deep reinforcement learning . In International conference on machine learning , pp . 1928\u20131937 , 2016 . [ 7 ] Chenyang Zhao , Olivier Sigaud , Freek Stulp , and Timothy M. Hospedales . Investigating generalisation in continuous deep reinforcement learning . arXiv:1902.07015 , 2019 . [ 8 ] Chiyuan Zhang , Oriol Vinyals , Remi Munos , and Samy Bengio . A study on overfitting in deep reinforcement learning . arXiv:1804.06893 , 2018 . [ 9 ] Jesse Farebrother , Marlos C Machado , and Michael Bowling . Generalization and regularization indqn.arXiv preprint arXiv:1810.00123 , 2018 . [ 10 ] Karl Cobbe , Oleg Klimov , Chris Hesse , Taehoon Kim , and John Schulman . Quantifying generalization in reinforcement learning . arXiv:1812.02341 , 2018 . [ 11 ] https : //github.com/openai/baselines [ 12 ] https : //github.com/hill-a/stable-baselines [ 13 ] https : //github.com/ray-project/ray"}, {"review_id": "B1lqDertwr-1", "review_text": "The paper provides an empirical study of regularization in policy optimization methods in multiple continuous control tasks. The paper focuses on the effect of conventional regularization on performance in training environments, not generalization ability to different (but similar) testing environments. Their findings suggest that L2 and entropy regularization can improve the performance, be robust to hyperparameters on the tasks studied in the paper. Overall, the paper is well written. However, I am leaning to reject this paper because (1) the experimental finding is not well justified (2) the experiments are missing some details and do not provide convincing evidence. First, the paper does not well justify why regularization methods improve performance in training environments. One potential reason is discussed in Section 7: regularization can improve generalization to unseen samples. However, the improvement can simply due to better hyperparemer optimization. When we introduce more hyperparemers and computation compared to baselines, it\u2019s not surprising to see a better performance, especially in deep RL where using a different seed or using a different implementation can have significant difference in performance [1]. Moreover, it is unclear that inability to generalize to unseen samples is a problem in the continuous control tasks evaluated in the paper. I think the paper should demonstrate that this is indeed a problem. If it is not a problem, why would you expect regularization to help? There are some missing details which makes it difficult to draw conclusion: 1. How was \\sigma_{env,r} computed? Is it the standard error of the mean return, or the standard deviation of the return? 2. What does the average rank mean (in Table 2 and 3)? the average ranking over 5 seeds and all environments? If so, does it make sense to compare these numbers? e.g. Algorithm A with rank 1, 1, 7, 7 and Algorithm B with rank 4, 4, 4, 4 have the same average rank, but totally different performance. 3. The experiment in Figure 3 seems very interesting, however, what\u2019s the conclusion here? 4. Why do you use difference hyperparamer ranges (lambda for L2, L1 and entropy regularization) for different algorithms in appendix A? Minor comment which does not impact the score: 1. It would have been better if there\u2019s a brief description of each algorithm (before section 4 or in appendix). [1] Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control ", "rating": "3: Weak Reject", "reply_text": "Q3.Missing Details ( 1 ) . How was $ \\sigma_ { env , r } $ calculated . $ \\sigma_ { env , r } $ is the standard deviation of the 5 returns obtained by 5 runs of random seeds . It is calculated as $ \\sqrt { \\frac { \\sum_ { i=1 } ^ { n } { ( r_i - \\mu_ { env , r } ) ^2 } } { n } } $ , where $ n=5 $ and $ r_i $ is the return with $ i $ th seed . In the revision , we have made it clear in the second sentence of the paragraph , that we use standard deviation ( not standard error of mean return ) . ( 2 ) . \u201c What does the average rank mean ( in Table 2 and 3 ) ? the average ranking over 5 seeds and all environments ? If so , does it make sense to compare these numbers ? e.g.Algorithm A with rank 1 , 1 , 7 , 7 and Algorithm B with rank 4 , 4 , 4 , 4 have the same average rank , but totally different performance. \u201d The ranks of mean return ( $ \\mu_ { env , r } $ ) , are collected for each environment . Then the average is calculated . In other words , the average ranks are over environments , but not over different random seeds , since we only rank $ \\mu_ { env , r } $ s which is already averaged over random seeds . We have made how we calculated average rank more clear in the paragraph \u201c Ranking all regularizations \u201d in section 4 of the revision . We agree that average rank alone is not the best metric to reflect detailed algorithm behaviors , especially the stability/variance of the algorithm . To better measure the variation , we added three tables ( Table 3 , 5 , and 15 ) in the paper presenting the standard deviation of ranks . If we look closely , we find L2 , L1 and weight clipping actually have relatively smaller stds in most times , whereas baseline , entropy , dropout and BN have larger stds . This means the methods that rank higher also rank ( slightly ) more stably . The average rank and rank standard deviations serve as summary statistics of each method . In addition to that , we provided the average percentage of \u201c improving \u201d and \u201c hurting \u201d using our definition . We hope those summarized information could serve as a fair comparison among different regularizers , as fully analyzing the detailed results for each ( algorithm , regularizer , environment ) tuple would be overwhelming and use too much space . For the detailed behaviors/training curves of each ( algorithm , regularizor , environment ) tuple , we refer our readers to Figure 1 , Appendix C , K and L. ( 3 ) . Conclusion of Figure 3 . We had a brief analysis of Figure 3 in the paragraph below it , and we have added more analysis in the revision . There are several observations we can draw : 1 ) The baseline performance can be either increasing , decreasing or staying roughly the same when the network depth/width increases . 2 ) Certain regularizations can help with various widths or depths , demonstrating their robustness against these hyperparameter s and ability to ease hyperparameter tuning . 3 ) Regularizations do not necessarily help more when the network sizes are bigger , contrary to what we might expect : larger networks may suffer more from overfitting and thus regularization can help more . As an example , L2 sometimes helps more with thinner network ( TRPO Ant ) , and sometimes more with wider network ( PPO HumanoidStandup ) . ( 4 ) . `` Why do you use difference hyperparameter ranges ( lambda for L2 , L1 and entropy regularization ) for different algorithms in appendix A ? '' For the three on-policy algorithms ( A2C , TRPO , PPO ) we use the same tuning range , and the only exception is the off-policy SAC . The reason why SAC \u2019 s tuning range is different is that SAC uses a hyperparameter that controls the scaling of the reward signal , while A2C , TRPO , and PPO don \u2019 t . In the original implementation of SAC , the reward signals are pre-tuned to be scaled up by a factor ranging from 5 to 100 , according to specific environments . Also , unlike A2C , TRPO , and PPO , SAC uses unnormalized reward because if the reward magnitude is small , then , according to the paper , the policy becomes almost uniform . Due to the above reasons , the reward magnitude of SAC is much higher than the magnitude of rewards used by A2C , TRPO , and PPO . Thus , the policy network loss and the value network loss have larger magnitude than those of A2C , TRPO , and PPO , so the appropriate regularization strengths become higher . Considering the SAC \u2019 s much larger reward magnitude , in our preliminary experiments , we selected a different range of hyperparameters for SAC before we run the whole experiments . Q4.Minor comment A4 . In the revision , we have added brief descriptions of each algorithm in Appendix A . Thank you again for your review ! If you have any further questions we are happy to answer . [ 1 ] Peter Henderson , Riashat Islam , Philip Bachman , Joelle Pineau , Doina Precup , and David Meger.Deep reinforcement learning that matters . In Thirty-Second AAAI Conference on Artificial Intelli-gence , 2018 . [ 2 ] Islam , R. , Henderson , P. , Gomrokchi , M. , and Precup , D. ( 2017 ) . Reproducibility of benchmarked deep reinforcement learning tasks for continuous control . In ICML 2017 Reproducibility in Machine Learning Workshop ."}, {"review_id": "B1lqDertwr-2", "review_text": "This paper investigates the use of conventional regularizers for neural networks in the reinforcement learning setting. Contrary to the standard practice of foregoing regularizers in deep RL, the paper finds that their addition can improve the performance of policy gradient algorithms on a standard suite of continuous control tasks. Various regularizers are tried, including l2/l1 regularization, entropy regularization and dropout in a combination with a few standard deep RL algorithms such as TRPO, PPO and SAC. Other experiments also verify the impact of these regularizers on the sensitivity of other hyperparameters and whether regularization should be applied to the value or policy networks. Overall, I find this paper to be a solid empirical study of regularization in deep reinforcement learning. The experiments are thorough, with various aspects being examined in more detail. I find several of the findings interesting, such as the importance of regularizing solely the policy network and that batch norm/dropout are effective for off-policy methods but not on-policy ones. There were certain points which warranted some further clarification. I would be willing to increase my score based on the authors' response to the following points: 1) In section 6, the last two sentences (\"For A2C, TRPO, and PPO ... so further regularization is unnecessary.\") are unclear to me. - \"rewards are already normalized using running mean filters.\" I thought that rewards are also normalized for SAC, so I'm not sure how this could explain the difference between the on-policy algorithms and the off-policy ones. - \"mitigates the overestimation bias...further regularization is unncessary.\" Could you clarify the connection between regularization and overestimation bias? Related to this point, in section 2 of the paper, it is written that \"L2 regularization is applied to the critic Q network because it tends to have overestimation bias (Fujimoto et al., 2018)\" but I was not able to find such an explanation in the cited paper though I may have missed it. 2) In section 7, in the paragraph on BN/Dropout, could you clarify the point starting from \"1) For both BN and dropout,...\"? In particular, which discrepancy between the sampling policy and the optimization policy is being referred to here? 3) Did you consider trying weight decay (\"Fixing Weight Decay Regularization in Adam\", Loschilov et al. 2018) as a regularizer? Given the success of L2 regularization, it could be possible that weight decay is even more effective. 4) For the hyperparameter sensitivity plots, where one hyperparameter is varied at a time, why are the step sizes for the policy and value networks not included in these experiments? They are usually a critical hyperparameter. Minor comments and typos: - On p.5, when defining \"hurting\", perhaps it could be better to choose a looser definition such as \"\\mu_r < \\mu_b\" or \"\\mu_r - \\sigma_r < \\mu_b - \\sigma_b\". This way, there could be a larger distinction between the most effective methods. Currently, both l2 and entropy regularization achieve 0.0% and the next best two regularizers are also under 10%. - In abstract: \"regularizing the policy network is typically enough.\" Rephrase perhaps? The experiments seem to show that applying a regularizer to only the policy network is better than on both. - In abstract: \"large improvement\" -> \"large improvements\" - p.2, par. 2: \"those regularizations\" -> \"those regularizers\" - p.3, Weight Clipping: \"This greatly stablizes\" -> \"This greatly stabilizes\". This sentence could be rephrased since \"This\" seems to refer to only weight clipping, but is not the only change in WGANs. - p.3, Dropout: \"regularization technique\" -> \"regularization techniques\" - p.4, par. 1: \"due to more stochasticity\" -> \"due to increased stochasticity\" - p.4, 2nd to last par.: \"during policy update\" -> \"during policy updates\" - p.5, 2nd to last par.: \"sometimes help\" -> \"sometimes helps\", \"easier ones baseline is\" -> \"easier ones the baseline is\" - p.8, 2nd to last par.: \"it naturally accepts\" -> \"they naturally accept\", \"been shown effective\" -> \"been shown to be effective\" - p.8, last sentence: \"policy network without the value network.\" -> \"policy network but not the value network.\" ", "rating": "6: Weak Accept", "reply_text": "Q3.Weight Decay ( Loschilov et al.2018 ) and L2 regularization . A3.Following your suggestion , we implemented \u201c fixed weight decay \u201d ( AdamW in the paper ) following Loschilov et al.2018 and compared it with baseline and L2 regularization . We evaluated them with PPO on Humanoid and HumanoidStandup . Similar to L2 , we briefly tune the strength of weight decay in AdamW and the optimal one is used . The results are shown in Appendix J ( Figure 9 ) . Interestingly , we found that while both L2 regularization and AdamW can significantly improve the performance over baseline , the performance of AdamW tends to be slightly lower than the performance of L2 regularization . Q4.Step size changes in hyperparameters sensitivity plots . A4.We have added experiments on step size ( learning rate ) variation in Figure 2 , Section 5 . We find that L2 , L1 and weight clipping can consistently improve baseline and make the algorithm less sensitive to learning rate changes . We would like to mention that learning rate is an important hyperparameter we vary in our original experiments in section 5 ( Table 4 and 5 ) . ( More hyperparameter sampling details in Table 11 , Appendix E ) Q4 . Minor Comments and typos Thanks for your detailed comments ! We have revised the draft in the revision . ( 1 ) .Looser definitions of \u201c hurting \u201d . We have added the resulting percentages with definition of hurting being $ \\mu_r < \\mu_b $ in the same paragraph . The results are 11.1 % for L2 , 16.7 % for L1 , 22.2 % for weight clipping , 55.6 % for dropout , 72.2 % for BN , and 16.7 % for entropy . For reference , if we define hurting $ \\mu_r - \\sigma_r < \\mu_b - \\sigma_b $ , the results are 5.6 % for L2 , 16.7 % for L1 , 19.4 % for weight clipping , 55.6 % for dropout , 69.4 % for BN , and 13.9 % for entropy . We observe similar trends among different methods with different definitions , and we still observe that regularization rarely hurts , except for BN and dropout ( for off-policy algorithms ) . ( 2 ) .Rephrase of \u201c only regularizing policy network \u201d . We have replaced \u201c is typically enough \u201d to \u201c is typically the best option \u201d to reflect that it is better than regularizing both policy and value network . ( 3 ) - ( 4 ) .We have corrected the typos accordingly . ( 5 ) .Weight clipping . We have corrected the typo , and changed the sentence to \u201c This plays an important role in stabilizing the training of GANs \u201d to indicate it is not the sole factor . ( 6 ) - ( 11 ) .We have corrected the typos accordingly . Thanks again for your review ! We hope our response addresses your concerns . Any further questions or suggestions are welcome . [ 1 ] https : //github.com/haarnoja/sac"}], "0": {"review_id": "B1lqDertwr-0", "review_text": "An interesting paper on the role of regularization in policy optimization In this paper, the authors study a set of existing direct policy optimization methods in the field of reinforcement learning. The authors provide a detailed investigation of the effect of regulations on the performance and behavior of agents following these methods. The authors present that regularization methods mostly help to improve the agents' performance in terms of final scores. Specifically, they show that direct regularizations on model parameters, such as the standard case of L2 or L1 regularization, generally improve the agent performance. They also show that these regularizations, in their study, is more proper than entropy regularization. The authors also show that, in the presence of such regularizations, the learning algorithms become less sensitive to the hyperparameters. Few comments: 1) The paper is well written and easy to follow. I appreciate it. I found the writing of the paper has a bit of repetition. The authors might find it slightly more proper to remove some of the repetitions (e.g. section 4.2) 2) While I appreciate the clear writing and reasoning in this paper, I might suggest a slight change in the second paraphrase of the intro. I agree with the authors' reason on the first three lines, but I think it would be useful to also emphasize the role of the questions the researchers investigate to answer. I might also add one the main reason that the researchers in the field of DRL have spent less time on regulation or architecture search was their focus on more high-level algorithm design which is in the more immediate step of relevance and specialty to the field of reinforcement learning. 3) I would suggest rephrasing the last two sentences of the second paragraph in related work: \"Also, these techniques consider ...\". Regularizing the output also regularizes the parameters, I think the authors' point was \"directly regularize\" the parameters. 4) In the \"Entropy Regularization\" part of section 3, I guess the Hs has not been defined. 5) Repeated \"the\" in the last paragraph of section 4.1 (despite it already incorporates the the maximization of) 6) The authors used the term \"not converge\" multiple times. While it is hard from the plots to see whether the series converges or not, I have a strong feeling that by this term the authors mean the algorithm does not converge to a resealable solution rather than being divergent up to a bandwidth. Maybe clarifying would be helpful. 7) In section 5, the authors study the sensitivity to the hyperparameters. In this section, I had a hard time to understand the role of term 3 \"BN and dropout hurts on-policy algorithms but can bring improvement only for the off-policy SAC algorithm.\" Does it mean that deploying BN, results in a more sensitive algorithm? or it means that the performance degrades (which is a different topic than section 5 is supposed to serve)? 8) In section 7, the authors put out a hypothesis \" However, there is still generalization between samples: the agents are only trained on the limited\" but the provided empirical study might not fully be considered to be designed to test this hypothesis. In order to test this hypothesis, the author might be interested in training the models with bigger sample sizes, more training iteration, different function classes, and more fitting in order to test this hypothesis. 9) Section 7 on \"Why do BN and dropout work only with off-policy algorithms?\" while I agree with the authors on their first reason which is quite commonly known, I might hesitate to make the second statement (2) Generally, I found this paper an interesting paper and appreciate the authors for their careful empirical study. But I found the contribution of this work to be not significant enough. Most of the statements and claims in this paper are well know in the community, especially among deep learning practitioners. While I acknowledge the scientific value of this study, its concreteness, and appreciate the contribution of this paper, due to the low acceptance rate of this conference, I might be reluctant in accepting this paper. ", "rating": "3: Weak Reject", "reply_text": "References : [ 1 ] Peter Henderson , Riashat Islam , Philip Bachman , Joelle Pineau , Doina Precup , and David Meger.Deep reinforcement learning that matters . In Thirty-Second AAAI Conference on Artificial Intelli-gence , 2018 . [ 2 ] Islam , R. , Henderson , P. , Gomrokchi , M. , and Precup , D. ( 2017 ) . Reproducibility of benchmarked deep reinforcement learning tasks for continuous control . In ICML 2017 Reproducibility in Machine Learning Workshop . [ 3 ] https : //github.com/jwyang/faster-rcnn.pytorch/blob/master/lib/model/faster_rcnn/resnet.py # L288 [ 4 ] https : //github.com/torch/nn/issues/873 [ 5 ] Zafarali Ahmed , Nicolas Le Roux , Mohammad Norouzi , and Dale Schuurmans . Understanding the impact of entropy in policy learning . arXiv:1811.11214 , 2018 . [ 6 ] Volodymyr Mnih , Adria Puigdomenech Badia , Mehdi Mirza , Alex Graves , Timothy Lillicrap , TimHarley , David Silver , and Koray Kavukcuoglu . Asynchronous methods for deep reinforcement learning . In International conference on machine learning , pp . 1928\u20131937 , 2016 . [ 7 ] Chenyang Zhao , Olivier Sigaud , Freek Stulp , and Timothy M. Hospedales . Investigating generalisation in continuous deep reinforcement learning . arXiv:1902.07015 , 2019 . [ 8 ] Chiyuan Zhang , Oriol Vinyals , Remi Munos , and Samy Bengio . A study on overfitting in deep reinforcement learning . arXiv:1804.06893 , 2018 . [ 9 ] Jesse Farebrother , Marlos C Machado , and Michael Bowling . Generalization and regularization indqn.arXiv preprint arXiv:1810.00123 , 2018 . [ 10 ] Karl Cobbe , Oleg Klimov , Chris Hesse , Taehoon Kim , and John Schulman . Quantifying generalization in reinforcement learning . arXiv:1812.02341 , 2018 . [ 11 ] https : //github.com/openai/baselines [ 12 ] https : //github.com/hill-a/stable-baselines [ 13 ] https : //github.com/ray-project/ray"}, "1": {"review_id": "B1lqDertwr-1", "review_text": "The paper provides an empirical study of regularization in policy optimization methods in multiple continuous control tasks. The paper focuses on the effect of conventional regularization on performance in training environments, not generalization ability to different (but similar) testing environments. Their findings suggest that L2 and entropy regularization can improve the performance, be robust to hyperparameters on the tasks studied in the paper. Overall, the paper is well written. However, I am leaning to reject this paper because (1) the experimental finding is not well justified (2) the experiments are missing some details and do not provide convincing evidence. First, the paper does not well justify why regularization methods improve performance in training environments. One potential reason is discussed in Section 7: regularization can improve generalization to unseen samples. However, the improvement can simply due to better hyperparemer optimization. When we introduce more hyperparemers and computation compared to baselines, it\u2019s not surprising to see a better performance, especially in deep RL where using a different seed or using a different implementation can have significant difference in performance [1]. Moreover, it is unclear that inability to generalize to unseen samples is a problem in the continuous control tasks evaluated in the paper. I think the paper should demonstrate that this is indeed a problem. If it is not a problem, why would you expect regularization to help? There are some missing details which makes it difficult to draw conclusion: 1. How was \\sigma_{env,r} computed? Is it the standard error of the mean return, or the standard deviation of the return? 2. What does the average rank mean (in Table 2 and 3)? the average ranking over 5 seeds and all environments? If so, does it make sense to compare these numbers? e.g. Algorithm A with rank 1, 1, 7, 7 and Algorithm B with rank 4, 4, 4, 4 have the same average rank, but totally different performance. 3. The experiment in Figure 3 seems very interesting, however, what\u2019s the conclusion here? 4. Why do you use difference hyperparamer ranges (lambda for L2, L1 and entropy regularization) for different algorithms in appendix A? Minor comment which does not impact the score: 1. It would have been better if there\u2019s a brief description of each algorithm (before section 4 or in appendix). [1] Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control ", "rating": "3: Weak Reject", "reply_text": "Q3.Missing Details ( 1 ) . How was $ \\sigma_ { env , r } $ calculated . $ \\sigma_ { env , r } $ is the standard deviation of the 5 returns obtained by 5 runs of random seeds . It is calculated as $ \\sqrt { \\frac { \\sum_ { i=1 } ^ { n } { ( r_i - \\mu_ { env , r } ) ^2 } } { n } } $ , where $ n=5 $ and $ r_i $ is the return with $ i $ th seed . In the revision , we have made it clear in the second sentence of the paragraph , that we use standard deviation ( not standard error of mean return ) . ( 2 ) . \u201c What does the average rank mean ( in Table 2 and 3 ) ? the average ranking over 5 seeds and all environments ? If so , does it make sense to compare these numbers ? e.g.Algorithm A with rank 1 , 1 , 7 , 7 and Algorithm B with rank 4 , 4 , 4 , 4 have the same average rank , but totally different performance. \u201d The ranks of mean return ( $ \\mu_ { env , r } $ ) , are collected for each environment . Then the average is calculated . In other words , the average ranks are over environments , but not over different random seeds , since we only rank $ \\mu_ { env , r } $ s which is already averaged over random seeds . We have made how we calculated average rank more clear in the paragraph \u201c Ranking all regularizations \u201d in section 4 of the revision . We agree that average rank alone is not the best metric to reflect detailed algorithm behaviors , especially the stability/variance of the algorithm . To better measure the variation , we added three tables ( Table 3 , 5 , and 15 ) in the paper presenting the standard deviation of ranks . If we look closely , we find L2 , L1 and weight clipping actually have relatively smaller stds in most times , whereas baseline , entropy , dropout and BN have larger stds . This means the methods that rank higher also rank ( slightly ) more stably . The average rank and rank standard deviations serve as summary statistics of each method . In addition to that , we provided the average percentage of \u201c improving \u201d and \u201c hurting \u201d using our definition . We hope those summarized information could serve as a fair comparison among different regularizers , as fully analyzing the detailed results for each ( algorithm , regularizer , environment ) tuple would be overwhelming and use too much space . For the detailed behaviors/training curves of each ( algorithm , regularizor , environment ) tuple , we refer our readers to Figure 1 , Appendix C , K and L. ( 3 ) . Conclusion of Figure 3 . We had a brief analysis of Figure 3 in the paragraph below it , and we have added more analysis in the revision . There are several observations we can draw : 1 ) The baseline performance can be either increasing , decreasing or staying roughly the same when the network depth/width increases . 2 ) Certain regularizations can help with various widths or depths , demonstrating their robustness against these hyperparameter s and ability to ease hyperparameter tuning . 3 ) Regularizations do not necessarily help more when the network sizes are bigger , contrary to what we might expect : larger networks may suffer more from overfitting and thus regularization can help more . As an example , L2 sometimes helps more with thinner network ( TRPO Ant ) , and sometimes more with wider network ( PPO HumanoidStandup ) . ( 4 ) . `` Why do you use difference hyperparameter ranges ( lambda for L2 , L1 and entropy regularization ) for different algorithms in appendix A ? '' For the three on-policy algorithms ( A2C , TRPO , PPO ) we use the same tuning range , and the only exception is the off-policy SAC . The reason why SAC \u2019 s tuning range is different is that SAC uses a hyperparameter that controls the scaling of the reward signal , while A2C , TRPO , and PPO don \u2019 t . In the original implementation of SAC , the reward signals are pre-tuned to be scaled up by a factor ranging from 5 to 100 , according to specific environments . Also , unlike A2C , TRPO , and PPO , SAC uses unnormalized reward because if the reward magnitude is small , then , according to the paper , the policy becomes almost uniform . Due to the above reasons , the reward magnitude of SAC is much higher than the magnitude of rewards used by A2C , TRPO , and PPO . Thus , the policy network loss and the value network loss have larger magnitude than those of A2C , TRPO , and PPO , so the appropriate regularization strengths become higher . Considering the SAC \u2019 s much larger reward magnitude , in our preliminary experiments , we selected a different range of hyperparameters for SAC before we run the whole experiments . Q4.Minor comment A4 . In the revision , we have added brief descriptions of each algorithm in Appendix A . Thank you again for your review ! If you have any further questions we are happy to answer . [ 1 ] Peter Henderson , Riashat Islam , Philip Bachman , Joelle Pineau , Doina Precup , and David Meger.Deep reinforcement learning that matters . In Thirty-Second AAAI Conference on Artificial Intelli-gence , 2018 . [ 2 ] Islam , R. , Henderson , P. , Gomrokchi , M. , and Precup , D. ( 2017 ) . Reproducibility of benchmarked deep reinforcement learning tasks for continuous control . In ICML 2017 Reproducibility in Machine Learning Workshop ."}, "2": {"review_id": "B1lqDertwr-2", "review_text": "This paper investigates the use of conventional regularizers for neural networks in the reinforcement learning setting. Contrary to the standard practice of foregoing regularizers in deep RL, the paper finds that their addition can improve the performance of policy gradient algorithms on a standard suite of continuous control tasks. Various regularizers are tried, including l2/l1 regularization, entropy regularization and dropout in a combination with a few standard deep RL algorithms such as TRPO, PPO and SAC. Other experiments also verify the impact of these regularizers on the sensitivity of other hyperparameters and whether regularization should be applied to the value or policy networks. Overall, I find this paper to be a solid empirical study of regularization in deep reinforcement learning. The experiments are thorough, with various aspects being examined in more detail. I find several of the findings interesting, such as the importance of regularizing solely the policy network and that batch norm/dropout are effective for off-policy methods but not on-policy ones. There were certain points which warranted some further clarification. I would be willing to increase my score based on the authors' response to the following points: 1) In section 6, the last two sentences (\"For A2C, TRPO, and PPO ... so further regularization is unnecessary.\") are unclear to me. - \"rewards are already normalized using running mean filters.\" I thought that rewards are also normalized for SAC, so I'm not sure how this could explain the difference between the on-policy algorithms and the off-policy ones. - \"mitigates the overestimation bias...further regularization is unncessary.\" Could you clarify the connection between regularization and overestimation bias? Related to this point, in section 2 of the paper, it is written that \"L2 regularization is applied to the critic Q network because it tends to have overestimation bias (Fujimoto et al., 2018)\" but I was not able to find such an explanation in the cited paper though I may have missed it. 2) In section 7, in the paragraph on BN/Dropout, could you clarify the point starting from \"1) For both BN and dropout,...\"? In particular, which discrepancy between the sampling policy and the optimization policy is being referred to here? 3) Did you consider trying weight decay (\"Fixing Weight Decay Regularization in Adam\", Loschilov et al. 2018) as a regularizer? Given the success of L2 regularization, it could be possible that weight decay is even more effective. 4) For the hyperparameter sensitivity plots, where one hyperparameter is varied at a time, why are the step sizes for the policy and value networks not included in these experiments? They are usually a critical hyperparameter. Minor comments and typos: - On p.5, when defining \"hurting\", perhaps it could be better to choose a looser definition such as \"\\mu_r < \\mu_b\" or \"\\mu_r - \\sigma_r < \\mu_b - \\sigma_b\". This way, there could be a larger distinction between the most effective methods. Currently, both l2 and entropy regularization achieve 0.0% and the next best two regularizers are also under 10%. - In abstract: \"regularizing the policy network is typically enough.\" Rephrase perhaps? The experiments seem to show that applying a regularizer to only the policy network is better than on both. - In abstract: \"large improvement\" -> \"large improvements\" - p.2, par. 2: \"those regularizations\" -> \"those regularizers\" - p.3, Weight Clipping: \"This greatly stablizes\" -> \"This greatly stabilizes\". This sentence could be rephrased since \"This\" seems to refer to only weight clipping, but is not the only change in WGANs. - p.3, Dropout: \"regularization technique\" -> \"regularization techniques\" - p.4, par. 1: \"due to more stochasticity\" -> \"due to increased stochasticity\" - p.4, 2nd to last par.: \"during policy update\" -> \"during policy updates\" - p.5, 2nd to last par.: \"sometimes help\" -> \"sometimes helps\", \"easier ones baseline is\" -> \"easier ones the baseline is\" - p.8, 2nd to last par.: \"it naturally accepts\" -> \"they naturally accept\", \"been shown effective\" -> \"been shown to be effective\" - p.8, last sentence: \"policy network without the value network.\" -> \"policy network but not the value network.\" ", "rating": "6: Weak Accept", "reply_text": "Q3.Weight Decay ( Loschilov et al.2018 ) and L2 regularization . A3.Following your suggestion , we implemented \u201c fixed weight decay \u201d ( AdamW in the paper ) following Loschilov et al.2018 and compared it with baseline and L2 regularization . We evaluated them with PPO on Humanoid and HumanoidStandup . Similar to L2 , we briefly tune the strength of weight decay in AdamW and the optimal one is used . The results are shown in Appendix J ( Figure 9 ) . Interestingly , we found that while both L2 regularization and AdamW can significantly improve the performance over baseline , the performance of AdamW tends to be slightly lower than the performance of L2 regularization . Q4.Step size changes in hyperparameters sensitivity plots . A4.We have added experiments on step size ( learning rate ) variation in Figure 2 , Section 5 . We find that L2 , L1 and weight clipping can consistently improve baseline and make the algorithm less sensitive to learning rate changes . We would like to mention that learning rate is an important hyperparameter we vary in our original experiments in section 5 ( Table 4 and 5 ) . ( More hyperparameter sampling details in Table 11 , Appendix E ) Q4 . Minor Comments and typos Thanks for your detailed comments ! We have revised the draft in the revision . ( 1 ) .Looser definitions of \u201c hurting \u201d . We have added the resulting percentages with definition of hurting being $ \\mu_r < \\mu_b $ in the same paragraph . The results are 11.1 % for L2 , 16.7 % for L1 , 22.2 % for weight clipping , 55.6 % for dropout , 72.2 % for BN , and 16.7 % for entropy . For reference , if we define hurting $ \\mu_r - \\sigma_r < \\mu_b - \\sigma_b $ , the results are 5.6 % for L2 , 16.7 % for L1 , 19.4 % for weight clipping , 55.6 % for dropout , 69.4 % for BN , and 13.9 % for entropy . We observe similar trends among different methods with different definitions , and we still observe that regularization rarely hurts , except for BN and dropout ( for off-policy algorithms ) . ( 2 ) .Rephrase of \u201c only regularizing policy network \u201d . We have replaced \u201c is typically enough \u201d to \u201c is typically the best option \u201d to reflect that it is better than regularizing both policy and value network . ( 3 ) - ( 4 ) .We have corrected the typos accordingly . ( 5 ) .Weight clipping . We have corrected the typo , and changed the sentence to \u201c This plays an important role in stabilizing the training of GANs \u201d to indicate it is not the sole factor . ( 6 ) - ( 11 ) .We have corrected the typos accordingly . Thanks again for your review ! We hope our response addresses your concerns . Any further questions or suggestions are welcome . [ 1 ] https : //github.com/haarnoja/sac"}}