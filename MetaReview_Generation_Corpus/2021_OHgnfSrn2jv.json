{"year": "2021", "forum": "OHgnfSrn2jv", "title": "Efficient Wasserstein Natural Gradients for Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "This paper explores the Wasserstein natural gradient in the context of reinforcement learning. R5 rated the paper marginally below the acceptance threshold, but is not very confident about the correctness of his/her assessment. His/her main criticism was the experimental evaluation. This concern was shared by a confident R1. R1 found the paper well structured and that it contains encouraging empirical results, but low technical novelty and (initially) insufficient experiments. His/her initial recommendation was reject, but following an extensive discussion and improvements of the manuscript by the authors, he/she was more convinced about the empirical significance and applicability of the method, and raised his/her score to 6, indicating that the interpretation and presentation improved but that the paper might be interesting only to a moderate number of readers. A confident R2 found this paper very good, although only providing a short review. Two other unfinished or not sufficiently confident reports were not taken into account. Weighing the reports by contents, confidence, and participation in the discussion, the paper scores marginally above the acceptance threshold. In view of the authors' responses, I am discounting R5's criticism about lack of comparison with the PPO baseline. I personally consider the paper very well written, that it presents a natural and potentially useful application of the Wasserstein natural gradient to the context of reinforcement learning, and enjoyed the discussion of behavioral geometry. I am recommending a borderline accept. However, I also appreciate the concern of the referees about the limited technical innovation and how some of the strengths of the method could be presented more convincingly. Please take these comments carefully into consideration when preparing the final version of the paper. ", "reviews": [{"review_id": "OHgnfSrn2jv-0", "review_text": "# # # Summary This paper proposes to use natural gradient instead of standard gradient to optimize a regularized objective with the regularization being the Wasserstein distance between the so-called behaviour distributions for the previous policy and new policy . It then combines this Wasserstein gradient descent with Policy Gradient and Evolutionary Strategies . Experiments conducted in OpenAI and Roboschool show some promising results for this combination . # # # Strong points : - Clarity : The paper is well structured - Empirical significance : The empirical results seem promising where it shows that Wasserstein gradient descent could be a good choice to constraint the behaviour changes efficiently without sacrificing too much computational cost . I personally like that the authors used Roboschool ( RS ) as a benchmark for continuous control tasks as it is , as opposed to Mujuco , a free simulator software ( thus it is more accessible and reproducible for everyone ) . # # # Weak points : - Novelty : the work however has low technical novelty where it combines several known results into a new framework . In particular , the idea of constraining behaviour policy changes and the efficient way to estimate Wasserstein gradient descent are all known and off-the-shelf . Adopting Wasserstein gradient descent to RL constraint update seems straightforward that does not require any significant technical challenge . The interpretation of the framework also seems straightforward , e.g. , it is of course that updating along the Wasserstein natural gradient would incorporate the local geometry of parameterization and help overcome some ill-conditioning issues where KL has . - Empirical significance : The empirical results though promising are not strong given that this is mostly an empirical work . In particular , the present work presents the experiments for PG case in only 4 environments which I think insufficient to make a reliable conclusion about its empirical significance . # # # Questions for the authors - In Section 2 : \u201c Reusing trajectories can reduce the computational cost but drastically increases the variance of the gradient estimator \u201d . Could the authors elaborate on why reusing trajectories drastically increases the variance of the gradient estimator ? - Fig.1 ( c ) : Have all algorithms been initialized at the same initial point ? Also , according to Fig.1 ( c ) that it seems that FNGD has a \u2018 right \u2019 convergence when it converges to the point where \\sigma=0 and \\mu = midpoint , why in the last paragraph of page 4 , the authors conclude that \u201c FNG remains far away from optimum \u201d ? What do I miss here ? - What is the difference between W2-penality and WNGD in Figure 1 ? - On page 5 , \u201c The Wasserstein penalty Equation ( 4 ) encourages global proximity between updates q\u03b8k \u201d . What does globality here refer to while Eq ( 6 ) holds only locally ? # # # Minor comments - The second term of Eq . ( 5 ) : Shouldn \u2019 t it be f_u instead of f there ? - Eq . ( 8 ) : \\argmax_ { u } - The second last sentence at the end of page 4 : cite - > \\cite { sth } - It seems that \\mu and \\sigma in Fig.1 ( c ) have not defined explicitly anywhere . In the capture of Fig.1. , it writes \\theta = ( mu , v ) , so there is a chance of inconsistent notations here ? - On page 5 , \u201c To avoid slowing-down , there is an intricate balance between the step-size and penalty \u03b2 that needs to be maintained Schulman et al . ( 2017 ) \u201d : \\cite - > \\citep # # # My initial recommendation Given the weak and strong points above , I vote for rejecting for this current form . # # # My finial recommendation After the discussion and revision , the authors have presented more convincingly and more clearly the empirical significance and applicability of their method . I highly recommend the authors to highlight the lastest discussion in the final paper , especially the ill-conditioned argument , as it is highly relevant to the practitioners . I think this paper can be interesting for a moderate number of readers , especially the use of the open-sourced Roboschool could also increase its reproduciability . I agree to increas my score to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your detailed comments and for the insightful feedback . We are happy to hear that you found the approach promising and the paper to be clear and well structured . We hope the following response addresses your concerns . 1.Novelty : \u201c The work however has low technical novelty where it combines several known results into a new framework \u201d . We respectfully disagree . This work indeed relies on three important and very recent ideas : \u201c Behavioral embeddings for RL \u201d from Pacchiano et al . ( 2020 ) , the \u201c Wasserstein Natural Gradient ( WNG ) \u201d ( Li 2019 ) and the scalable estimator of the WNG from ( Arbel 2020 ) . Our main contribution is to build on those works to propose general and flexible methods for Policy gradient and Evolution Strategies with broad applicability in RL . Note also that WNG is a rather novel method that remains under-explored in machine learning literature . Our work shows that this new tool can be useful for RL and black-box methods like ES . Moreover , as a second contribution , we also extended previous work for estimating WNG in ( Arbel 2020 ) to cases where only the score function is available . We thus believe the methods we proposed are novel and of general interest to the RL community . Using the Wasserstein Natural Gradient directly preconditions the \u2018 usual \u2019 gradient of the objective to incorporate local geometry and help overcome ill-conditioning . While this might seem straightforward now , we are not aware of any prior work that exploited this fact in RL ( Please see also answer the point ( 4 ) below about Globality vs Locality ) . As a comparison , we do not believe that TRPO was straightforward before it was introduced and showed to yield improvement in RL . 2.Technical challenge : \u201c Adopting Wasserstein gradient descent to RL constraint update seems straightforward that does not require any significant technical challenge. \u201d Thank you , we indeed tried our best to make the algorithms seem easy to use off-the-shelf . By doing so , we hope it will reach a wider community . This keeping in mind that the proposed methods rely on several advanced mathematical concepts such as optimal transport , differential geometry , kernel methods . We believe we provided an accessible and simple presentation of those concepts . 3.Empirical Significance : We are very glad to hear that you see our initial results as promising . We also thank you for encouraging us to strengthen our empirical results . We have now added new results for the policy gradient methods on two new Roboschool environments ( Ant and Reacher ) to our revised submission . WNG-based methods outperform baselines on both . We also performed eigenspectra analysis further confirming consistency with the theoretical prediction that WNG-based approaches deliver larger benefits on ill-conditioned problems . We believe that these additional experiments provide further support for the reliability and empirical significance of our approach ."}, {"review_id": "OHgnfSrn2jv-1", "review_text": "Amari 's Natural Gradient has been very successfully applied for policy optimization , e.g.in a recent line of work by Schulman et al.These benefit of using these natural gradients that restrict the change in KL divergence between successive policies was more stable and faster convergence . However two distributions may differ a lot in terms of KL divergence but because of the dynamics of the MDP they may still have almost the same behavior , therefore recent work has focused on using the Wasserstein distance to measure the divergence between successive policies which naturally leads to `` Wasserstein Natural Descent '' . In this paper the authors build upon recent work on Kernelized Wasserstein NGD by making it more widely applicable and scalable . Moreover they present a good empirical comparison between KL-NGD and Wasserstein-NGD on a combination of pedagogical toy problems and some standard RL benchmarks from OpenAI gym . Overall this paper will be a good contribution to the conference and I recommend acceptance . * * Corrections and suggestion for improving presentation * * 1 . [ Citations ] Page 2 , third paragraph from bottom cites Schulman 2015 instead of Schulman 2017 for PPO . Page 4 last paragraph has a citation missing and Li and Zhao `` Wasserstein Information Matrix '' paper is cited twice . 2.I think it will be better to write down that equation ( 5 ) defines $ f_u $ . Also should n't the $ 1/2 $ factor in equation ( 5 ) be actually $ \\beta / 2 $ ? 3.On page 3 it is said that `` the penalty only accounts for global proximity in behavior .... `` in reference to equation ( 4 ) , but PPO is not implemented with a single value of $ \\beta $ , i.e.the strength of the penalty typically varies as optimization goes on . Why ca n't the same be done for the weight of the $ W_2 $ penalty ?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for the positive evaluation and suggestions for improving the paper ! We are glad to read that you find the paper would be a good contribution to ICLR . We incorporated the suggestions and clarifications in the revised version as follows : 1 . Thank you for pointing out those -- we have adjusted the manuscript accordingly . The missing citation was the work of Tang & Agrawal ( 2019 ) on implicit policies . 2.Thank you , it is indeed clearer this way . We added the following sentence at the end of the first paragraph of section 3 : \u201c Hence , the optimal function $ f_u $ solving Equation 5 defines the \\textbf { optimal } vector field $ \\nabla_x f_ { u } ( x ) $ . \u201d We could also use $ \\beta/2 $ in equation 5 . This has the effect of rescaling the WNG by $ 1/\\beta $ . Since this rescaling can be absorbed by the step-size during optimization , we preferred to keep $ \\beta=1 $ so that we recover the WIM as defined in Li & Zhao ( 2019 ) . 3.This is a very good point ! Thank you for pointing this out ! Indeed $ \\beta $ could also be adapted in the case of W_2 penalty . We clarified this in the revised version by changing the last paragraph of section 2 and by saying : \u201c This procedure is highly accurate when the Wasserstein distance changes slowly between successive updates , as ensured when $ \\beta $ is large . At the same time , larger values for $ \\beta $ also mean that the policy is updated using smaller steps , which can impact convergence . An optimal trade-off between speed of convergence and precision of the estimated Wasserstein distance can be achieved using an adaptive choice of $ \\beta $ as done in the case of PPO ( Schulman et.al.2017 ) .For a finite value of $ \\beta $ , the penalty accounts for \\emph { global } proximity in behavior and does n't explicitly exploit the local geometry induced by the BEM , which can further improve convergence . We introduce an efficient method that explicitly exploits the local geometry induced by the BEM through the Wasserstein Natural gradient ( WNG ) leading to gains in performance at a reduced computational cost . \u201d"}, {"review_id": "OHgnfSrn2jv-2", "review_text": "The paper introduces methods for reinforcement learning based on a Wasserstein Natural Gradients ( WNG ) , an approach to measuring similarity between policies . The methods are based on Policy Gradients and Evolutionary Strategies and add policy similarity term based on WNG instead of KL constraint as in TRPO . The paper is well written and easy to follow ( the conclusion section is missing though ) . Although the method is interesting , I think that the current experimental evaluation has significant flaws : the method does not demonstrate the state-of-the-art performance and significantly improves over the baselines on a small number of tasks . I believe that the paper will significantly benefit from comparisons with stronger baselines such as PPO . Moreover , the experiments performed in the paper ( figure 2 ) demonstrate that the approach only marginally outperforms the baselines on the harder HalfCheetah and Hopper tasks that raises concerns regarding the generality of the approach . The paper proposed an interesting approach to policy constraints in RL but the experimental evaluation is not sufficient .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your encouraging and constructive comments . We are happy to hear that you find the method interesting and the paper well written and easy to follow . We hope the following additions and clarifications address your current concerns : 1 . Experimental evaluation : Thank you for encouraging us to strengthen the empirical evaluation of the method . We performed additional policy gradient experiments on the Roboschool Ant and Reacher tasks which are now included in the revised version . The Ant task is the highest dimensional task to which we applied policy gradients . Our methods outperforms the other ones on both Ant and Reacher , which we hope further underscores the potential of our approach . 2. \u201c The paper will significantly benefit from comparisons with stronger baselines such as PPO. \u201d : We absolutely agree . We have now clarified in the revised version of the paper that the results in Figure 2 already includes PPO with Clipped Surrogate Objective that was labeled as \u2018 KL \u2019 and is now labeled as PPO ( Clip ) for clarity . We apologize for the confusion this created . We would like to emphasize that this method was already shown to outperform TRPO and KL-penalty on various tasks in ( Schulman et al.2020 ) .We would also like to clarify that our experimental setting is similar to the one in Pacchiano et al . ( 2020 ) and reproduces the baselines from that paper . 3.Improvement on the harder HalfCheetah and Hopper tasks . While the improvement on those tasks is indeed smaller compared to the other two tasks , we do not believe that HalfCheetah and Hopper are the hardest tasks by at least two criteria : dimension of the observation and action spaces and hardness of initial improvement . Dimension : For instance , Walker2d has observation and action spaces that are twice as large as Hopper ( and roughly the same as HalfCheetah ) , yet the improvement of our approach on Walker2d is significant . Difficulty of initial improvement : All methods showed a fast initial improvement on both HalfCheetah and Hopper , whereas initial progress on Walker2d and InvertedDoublePendulum was more challenging . In both tasks where initial progress was difficult , the gains for WNG were most evident . 4.When to expect the most improvement with WNG ? To investigate why WNG-based methods deliver bigger gains on some tasks compared to others , we appealed to the prior work of Arbel et al . ( 2020 ) on WNG which shows that the highest improvements are obtained when the problem is ill-conditioned . In Figure 3 , we found that indeed , the problems on which WNG showed less improvement compared to baselines were those with better conditioning . 5.Conclusion : We have now included a conclusion which is as follows : \u201c Explicit regularization using divergence measures between policy representations has been a common theme in recent work on policy optimization for RL . While prior works have previously focused on the KL divergence , Pacchiano et al . ( 2020 ) showed that a Wasserstein regularizer over behavioral distributions provided a powerful alternative framework . Both approaches implicitly define a form of natural gradient , depending on which divergence measure is chosen . Through the introduction of WNPG and WNES , we demonstrate that directly estimating the natural gradient of the un-regularized objective can deliver greater performance at lower computational cost . These algorithms represent novel extensions of previous work on the WNG to problems where the reparameterization trick is not available , as well as to black-box methods like ES . Moreover , using the WNG in conjunction with a WD penalty allows such penalty to take advantage of the local geometry induced by WNG , further improving performance . We also provide a novel comparison between the WNG and FNG , showing that the former has significant advantages on certain problems . We believe this framework opens up a number of avenues for future work . Developing a principled way to identify useful behavioral embeddings for a given RL task would allow to get the highest benefit form WNPG and WNES . From a theoretical perspective , it would be useful to characterize convergence boost granted by the combination of explicit regularization and the corresponding natural gradient approach. \u201d ( Schulman et al.2017 ) : Proximal Policy Optimization Algorithms , https : //arxiv.org/abs/1707.06347 ( Pacchiano et al.2020 ) : Learning to Score Behaviors for Guided Policy Optimization , https : //proceedings.icml.cc/static/paper_files/icml/2020/2630-Paper.pdf"}], "0": {"review_id": "OHgnfSrn2jv-0", "review_text": "# # # Summary This paper proposes to use natural gradient instead of standard gradient to optimize a regularized objective with the regularization being the Wasserstein distance between the so-called behaviour distributions for the previous policy and new policy . It then combines this Wasserstein gradient descent with Policy Gradient and Evolutionary Strategies . Experiments conducted in OpenAI and Roboschool show some promising results for this combination . # # # Strong points : - Clarity : The paper is well structured - Empirical significance : The empirical results seem promising where it shows that Wasserstein gradient descent could be a good choice to constraint the behaviour changes efficiently without sacrificing too much computational cost . I personally like that the authors used Roboschool ( RS ) as a benchmark for continuous control tasks as it is , as opposed to Mujuco , a free simulator software ( thus it is more accessible and reproducible for everyone ) . # # # Weak points : - Novelty : the work however has low technical novelty where it combines several known results into a new framework . In particular , the idea of constraining behaviour policy changes and the efficient way to estimate Wasserstein gradient descent are all known and off-the-shelf . Adopting Wasserstein gradient descent to RL constraint update seems straightforward that does not require any significant technical challenge . The interpretation of the framework also seems straightforward , e.g. , it is of course that updating along the Wasserstein natural gradient would incorporate the local geometry of parameterization and help overcome some ill-conditioning issues where KL has . - Empirical significance : The empirical results though promising are not strong given that this is mostly an empirical work . In particular , the present work presents the experiments for PG case in only 4 environments which I think insufficient to make a reliable conclusion about its empirical significance . # # # Questions for the authors - In Section 2 : \u201c Reusing trajectories can reduce the computational cost but drastically increases the variance of the gradient estimator \u201d . Could the authors elaborate on why reusing trajectories drastically increases the variance of the gradient estimator ? - Fig.1 ( c ) : Have all algorithms been initialized at the same initial point ? Also , according to Fig.1 ( c ) that it seems that FNGD has a \u2018 right \u2019 convergence when it converges to the point where \\sigma=0 and \\mu = midpoint , why in the last paragraph of page 4 , the authors conclude that \u201c FNG remains far away from optimum \u201d ? What do I miss here ? - What is the difference between W2-penality and WNGD in Figure 1 ? - On page 5 , \u201c The Wasserstein penalty Equation ( 4 ) encourages global proximity between updates q\u03b8k \u201d . What does globality here refer to while Eq ( 6 ) holds only locally ? # # # Minor comments - The second term of Eq . ( 5 ) : Shouldn \u2019 t it be f_u instead of f there ? - Eq . ( 8 ) : \\argmax_ { u } - The second last sentence at the end of page 4 : cite - > \\cite { sth } - It seems that \\mu and \\sigma in Fig.1 ( c ) have not defined explicitly anywhere . In the capture of Fig.1. , it writes \\theta = ( mu , v ) , so there is a chance of inconsistent notations here ? - On page 5 , \u201c To avoid slowing-down , there is an intricate balance between the step-size and penalty \u03b2 that needs to be maintained Schulman et al . ( 2017 ) \u201d : \\cite - > \\citep # # # My initial recommendation Given the weak and strong points above , I vote for rejecting for this current form . # # # My finial recommendation After the discussion and revision , the authors have presented more convincingly and more clearly the empirical significance and applicability of their method . I highly recommend the authors to highlight the lastest discussion in the final paper , especially the ill-conditioned argument , as it is highly relevant to the practitioners . I think this paper can be interesting for a moderate number of readers , especially the use of the open-sourced Roboschool could also increase its reproduciability . I agree to increas my score to 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your detailed comments and for the insightful feedback . We are happy to hear that you found the approach promising and the paper to be clear and well structured . We hope the following response addresses your concerns . 1.Novelty : \u201c The work however has low technical novelty where it combines several known results into a new framework \u201d . We respectfully disagree . This work indeed relies on three important and very recent ideas : \u201c Behavioral embeddings for RL \u201d from Pacchiano et al . ( 2020 ) , the \u201c Wasserstein Natural Gradient ( WNG ) \u201d ( Li 2019 ) and the scalable estimator of the WNG from ( Arbel 2020 ) . Our main contribution is to build on those works to propose general and flexible methods for Policy gradient and Evolution Strategies with broad applicability in RL . Note also that WNG is a rather novel method that remains under-explored in machine learning literature . Our work shows that this new tool can be useful for RL and black-box methods like ES . Moreover , as a second contribution , we also extended previous work for estimating WNG in ( Arbel 2020 ) to cases where only the score function is available . We thus believe the methods we proposed are novel and of general interest to the RL community . Using the Wasserstein Natural Gradient directly preconditions the \u2018 usual \u2019 gradient of the objective to incorporate local geometry and help overcome ill-conditioning . While this might seem straightforward now , we are not aware of any prior work that exploited this fact in RL ( Please see also answer the point ( 4 ) below about Globality vs Locality ) . As a comparison , we do not believe that TRPO was straightforward before it was introduced and showed to yield improvement in RL . 2.Technical challenge : \u201c Adopting Wasserstein gradient descent to RL constraint update seems straightforward that does not require any significant technical challenge. \u201d Thank you , we indeed tried our best to make the algorithms seem easy to use off-the-shelf . By doing so , we hope it will reach a wider community . This keeping in mind that the proposed methods rely on several advanced mathematical concepts such as optimal transport , differential geometry , kernel methods . We believe we provided an accessible and simple presentation of those concepts . 3.Empirical Significance : We are very glad to hear that you see our initial results as promising . We also thank you for encouraging us to strengthen our empirical results . We have now added new results for the policy gradient methods on two new Roboschool environments ( Ant and Reacher ) to our revised submission . WNG-based methods outperform baselines on both . We also performed eigenspectra analysis further confirming consistency with the theoretical prediction that WNG-based approaches deliver larger benefits on ill-conditioned problems . We believe that these additional experiments provide further support for the reliability and empirical significance of our approach ."}, "1": {"review_id": "OHgnfSrn2jv-1", "review_text": "Amari 's Natural Gradient has been very successfully applied for policy optimization , e.g.in a recent line of work by Schulman et al.These benefit of using these natural gradients that restrict the change in KL divergence between successive policies was more stable and faster convergence . However two distributions may differ a lot in terms of KL divergence but because of the dynamics of the MDP they may still have almost the same behavior , therefore recent work has focused on using the Wasserstein distance to measure the divergence between successive policies which naturally leads to `` Wasserstein Natural Descent '' . In this paper the authors build upon recent work on Kernelized Wasserstein NGD by making it more widely applicable and scalable . Moreover they present a good empirical comparison between KL-NGD and Wasserstein-NGD on a combination of pedagogical toy problems and some standard RL benchmarks from OpenAI gym . Overall this paper will be a good contribution to the conference and I recommend acceptance . * * Corrections and suggestion for improving presentation * * 1 . [ Citations ] Page 2 , third paragraph from bottom cites Schulman 2015 instead of Schulman 2017 for PPO . Page 4 last paragraph has a citation missing and Li and Zhao `` Wasserstein Information Matrix '' paper is cited twice . 2.I think it will be better to write down that equation ( 5 ) defines $ f_u $ . Also should n't the $ 1/2 $ factor in equation ( 5 ) be actually $ \\beta / 2 $ ? 3.On page 3 it is said that `` the penalty only accounts for global proximity in behavior .... `` in reference to equation ( 4 ) , but PPO is not implemented with a single value of $ \\beta $ , i.e.the strength of the penalty typically varies as optimization goes on . Why ca n't the same be done for the weight of the $ W_2 $ penalty ?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for the positive evaluation and suggestions for improving the paper ! We are glad to read that you find the paper would be a good contribution to ICLR . We incorporated the suggestions and clarifications in the revised version as follows : 1 . Thank you for pointing out those -- we have adjusted the manuscript accordingly . The missing citation was the work of Tang & Agrawal ( 2019 ) on implicit policies . 2.Thank you , it is indeed clearer this way . We added the following sentence at the end of the first paragraph of section 3 : \u201c Hence , the optimal function $ f_u $ solving Equation 5 defines the \\textbf { optimal } vector field $ \\nabla_x f_ { u } ( x ) $ . \u201d We could also use $ \\beta/2 $ in equation 5 . This has the effect of rescaling the WNG by $ 1/\\beta $ . Since this rescaling can be absorbed by the step-size during optimization , we preferred to keep $ \\beta=1 $ so that we recover the WIM as defined in Li & Zhao ( 2019 ) . 3.This is a very good point ! Thank you for pointing this out ! Indeed $ \\beta $ could also be adapted in the case of W_2 penalty . We clarified this in the revised version by changing the last paragraph of section 2 and by saying : \u201c This procedure is highly accurate when the Wasserstein distance changes slowly between successive updates , as ensured when $ \\beta $ is large . At the same time , larger values for $ \\beta $ also mean that the policy is updated using smaller steps , which can impact convergence . An optimal trade-off between speed of convergence and precision of the estimated Wasserstein distance can be achieved using an adaptive choice of $ \\beta $ as done in the case of PPO ( Schulman et.al.2017 ) .For a finite value of $ \\beta $ , the penalty accounts for \\emph { global } proximity in behavior and does n't explicitly exploit the local geometry induced by the BEM , which can further improve convergence . We introduce an efficient method that explicitly exploits the local geometry induced by the BEM through the Wasserstein Natural gradient ( WNG ) leading to gains in performance at a reduced computational cost . \u201d"}, "2": {"review_id": "OHgnfSrn2jv-2", "review_text": "The paper introduces methods for reinforcement learning based on a Wasserstein Natural Gradients ( WNG ) , an approach to measuring similarity between policies . The methods are based on Policy Gradients and Evolutionary Strategies and add policy similarity term based on WNG instead of KL constraint as in TRPO . The paper is well written and easy to follow ( the conclusion section is missing though ) . Although the method is interesting , I think that the current experimental evaluation has significant flaws : the method does not demonstrate the state-of-the-art performance and significantly improves over the baselines on a small number of tasks . I believe that the paper will significantly benefit from comparisons with stronger baselines such as PPO . Moreover , the experiments performed in the paper ( figure 2 ) demonstrate that the approach only marginally outperforms the baselines on the harder HalfCheetah and Hopper tasks that raises concerns regarding the generality of the approach . The paper proposed an interesting approach to policy constraints in RL but the experimental evaluation is not sufficient .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your encouraging and constructive comments . We are happy to hear that you find the method interesting and the paper well written and easy to follow . We hope the following additions and clarifications address your current concerns : 1 . Experimental evaluation : Thank you for encouraging us to strengthen the empirical evaluation of the method . We performed additional policy gradient experiments on the Roboschool Ant and Reacher tasks which are now included in the revised version . The Ant task is the highest dimensional task to which we applied policy gradients . Our methods outperforms the other ones on both Ant and Reacher , which we hope further underscores the potential of our approach . 2. \u201c The paper will significantly benefit from comparisons with stronger baselines such as PPO. \u201d : We absolutely agree . We have now clarified in the revised version of the paper that the results in Figure 2 already includes PPO with Clipped Surrogate Objective that was labeled as \u2018 KL \u2019 and is now labeled as PPO ( Clip ) for clarity . We apologize for the confusion this created . We would like to emphasize that this method was already shown to outperform TRPO and KL-penalty on various tasks in ( Schulman et al.2020 ) .We would also like to clarify that our experimental setting is similar to the one in Pacchiano et al . ( 2020 ) and reproduces the baselines from that paper . 3.Improvement on the harder HalfCheetah and Hopper tasks . While the improvement on those tasks is indeed smaller compared to the other two tasks , we do not believe that HalfCheetah and Hopper are the hardest tasks by at least two criteria : dimension of the observation and action spaces and hardness of initial improvement . Dimension : For instance , Walker2d has observation and action spaces that are twice as large as Hopper ( and roughly the same as HalfCheetah ) , yet the improvement of our approach on Walker2d is significant . Difficulty of initial improvement : All methods showed a fast initial improvement on both HalfCheetah and Hopper , whereas initial progress on Walker2d and InvertedDoublePendulum was more challenging . In both tasks where initial progress was difficult , the gains for WNG were most evident . 4.When to expect the most improvement with WNG ? To investigate why WNG-based methods deliver bigger gains on some tasks compared to others , we appealed to the prior work of Arbel et al . ( 2020 ) on WNG which shows that the highest improvements are obtained when the problem is ill-conditioned . In Figure 3 , we found that indeed , the problems on which WNG showed less improvement compared to baselines were those with better conditioning . 5.Conclusion : We have now included a conclusion which is as follows : \u201c Explicit regularization using divergence measures between policy representations has been a common theme in recent work on policy optimization for RL . While prior works have previously focused on the KL divergence , Pacchiano et al . ( 2020 ) showed that a Wasserstein regularizer over behavioral distributions provided a powerful alternative framework . Both approaches implicitly define a form of natural gradient , depending on which divergence measure is chosen . Through the introduction of WNPG and WNES , we demonstrate that directly estimating the natural gradient of the un-regularized objective can deliver greater performance at lower computational cost . These algorithms represent novel extensions of previous work on the WNG to problems where the reparameterization trick is not available , as well as to black-box methods like ES . Moreover , using the WNG in conjunction with a WD penalty allows such penalty to take advantage of the local geometry induced by WNG , further improving performance . We also provide a novel comparison between the WNG and FNG , showing that the former has significant advantages on certain problems . We believe this framework opens up a number of avenues for future work . Developing a principled way to identify useful behavioral embeddings for a given RL task would allow to get the highest benefit form WNPG and WNES . From a theoretical perspective , it would be useful to characterize convergence boost granted by the combination of explicit regularization and the corresponding natural gradient approach. \u201d ( Schulman et al.2017 ) : Proximal Policy Optimization Algorithms , https : //arxiv.org/abs/1707.06347 ( Pacchiano et al.2020 ) : Learning to Score Behaviors for Guided Policy Optimization , https : //proceedings.icml.cc/static/paper_files/icml/2020/2630-Paper.pdf"}}