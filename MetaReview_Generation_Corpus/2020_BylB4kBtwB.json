{"year": "2020", "forum": "BylB4kBtwB", "title": "Retrieving Signals in the Frequency Domain with Deep Complex Extractors", "decision": "Reject", "meta_review": "The paper discusses audio source separation with complex NNs.  The approach is good and may increase an area of research.  But the experimental section is very weak and needs to be improved to merit publication.", "reviews": [{"review_id": "BylB4kBtwB-0", "review_text": "This paper proposes a new method for source separation, by using deep learning UNets, complex-valued representations and the Fourier domain. Concretely, their contribution is : i) a complex-valued convolutional version of the Feature-Wise Linear Modulation, able to optimise the parameters needed to create multiple separated candidates for each of signal sources that are then combined using signal averaging; ii) the design of a loss that takes into account magnitude and phase while being scale and time invariant. It was then tested and compared with real-valued versions, and also some state-of-the-art methods. Overall, I think this paper is of good quality and proposes an interesting method for this crucial task of source separation. However, I found the paper too dense and difficult to read (even if well written), and it looks like a re-submission from a journal paper of more than 8 pages. I would suggest the authors to shrink the paper so it *really* fits into the 8-pages (without important figures or important implementation details in the appendices), maybe at the cost of leaving some parts (such as old related works) out of the paper. The experiments are important here, and it is too bad that the comparison with state-of-the-art is just in the last paragraph, while the results do not seem to show any improvements compared to the other methods. The computational time might be very important here, as the claim is that FFT reduces time computation, but I did not had time to go through all the appendices. Positive aspects: - The work is well documented and motivated, and I found that the reflexion leading to the method is of good quality. - Concretely, I found interesting the use of the FiLM, originally designed for another application, for minimizing the SNR of the signal sources. The motivation/proof is quite clear too. - Equally, the motivation for the design of the new loss is clear and interesting. - I also found important the experiments, that shows in the same table the difference between the method without the complex-valued part and with different parameter values. Questions and remarks: - I have to recall that I am not an expert on source separation and complex-valued deep learning. Yet, I have had difficulties in understanding the structure of the method, even if the different parts were clearly explained. The figure 1 is very useful, but I found it not clear enough and too small. I went to find some informations in the appendices, but there are too much crucial information there and I did not have time to go through all of it. - The use of the U-net architecture is not explained (just some citations are given). What is supposed to be the output of it? - When you say 'to be more rigorous, we can assume in the cse of speech separation that, for each speaker, there exists an impulse response such that when it is convolved with the clean speech of the speaker, it allows to reconstruct the mix' : why can we be sure that it is always possible, and why is it more rigourous? - Why are the additive noises epsilon_i supposed to have the same E(|epsilon_i|^2|) ? Even if they are uncorrelated, what is the hypothesis behind that? - In the CSimLoss, why (i) is the real part negative and the imaginary part positive; (ii) is the imaginary part squared? - have you tested with a higher lambda_imag (as the larger is now the best)? - It looks from the end of the paper that the method is still not achieving better results than the state-of-the-art. I agree with the authors as it might not be the scope of the paper, but then what is it? If it's time computation, it is not shown in the paper. If it is just a methodology, what would be required in the future to beat the best method? - In the results, table 1, in the last 4 lines: it looks from 1st and 2nd line that the new loss CSimLoss is not very different from the L2 (9.88 compared to 9.87). The best result, in the 4th line, cannot be compared to the 3rd line as both the loss and the number of transforms are different. I then found those values in the appendices, but it would be best to show fewer parameters varying in the main paper, but show some results that can be easily compared. - What is the importance of the first paragraph in 2.1? I was not aware of the holographic reduced representations, but I don't understand it more now, and I don't see why explaining that for 15 lines. Small remarks: - 'deep complex valued models have *just* started to gain momentum'... with citations beginning in 2014, I would not say 'just'. - 'in the frequncy domain is then, ...' --> frequency + no coma - Figure 2 is in the appendix, while in the text it is not said so. I was lost. This figure should not be in the appendix as the appendix should not have key elements, but just details that are not important for the understanding of the paper. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the useful feedback and appreciate the encouraging comments . We have considered the comments and tried to address them below . Reviewer 1 : `` 1- I have to recall that I am not an expert on source separation and complex-valued deep learning . Yet , I have had difficulties in understanding the structure of the method , even if the different parts were clearly explained . The figure 1 is very useful , but I found it not clear enough and too small . I went to find some informations in the appendices , but there are too much crucial information there and I did not have time to go through all of it . '' The U-Net is not the contribution of the paper and it has been widely used in inverse problems such as image segmentation and signal reconstruction . This is why details of the residual upsampling and downsampling blocks incorporated in the U-Net are in the appendix in section A.3 and not in the main text . Sections A.4 : DATA PRE-PROCESSING AND TRAINING DETAILS contains just the details about the implementation , the optimizer and the standard processing of the WSJ dataset ( Hershey et al , 2015 ) . It is not about the scientific contributions but more about the technical details and that is why it is in the appendix . We have provided our source code so the reader can have a better grasp of our method and can reproduce our work . Reviewer 1 : `` 2- The use of the U-net architecture is not explained ( just some citations are given ) . What is supposed to be the output of it ? '' In section 5 in the main text explained that the U-Net architecture we used is similar to \u201c the complex-valued U-Net architecture used in Dedmari et al . ( 2018 ) who reported state-of-the-art results in MRI reconstruction using complex-valued raw input \u201d . That design \u2019 s success is why we selected it as a base . The U-Net \u2019 s output is an intermediate representation that the extractor mechanism learns to use to generate clean speech candidates . Reviewer 1 : `` 3- When you say 'to be more rigorous , we can assume in the case of speech separation that , for each speaker , there exists an impulse response such that when it is convolved with the clean speech of the speaker , it allows to reconstruct the mix ' : why can we be sure that it is always possible , and why is it more rigorous ? '' This information is translated in equation ( 3 ) which is just the application of the equation y = s * r + epsilon , in the context of speech separation . Similar assumptions have been also been made and reported in speech-related problems such as in https : //arxiv.org/pdf/1705.10874.pdf Where noisy signals are the result y ( t ) = s ( t ) \u2217 r ( t ) + n ( t ) . If we want to apply that assumption to speech separation , then , the analogue of the noisy constant signal y ( t ) becomes the mix containing all speeches , for each ith speaker that will be a clean speech s_ { i } ( t ) an impulse response r_ { i } ( t ) and an additive noise n_ { i } ( t ) . Now all of these assumptions correct ? Of course not as George Box states : \u201c All models are wrong but some of them are useful \u201d . It is a wrong assumption but a useful one as it allowed us to leverage the convolution theorem and the linearity of the Fourier transform in order to deduce an extraction mechanism based on FiLM and signal averaging allowing us to increase the signal-to-noise-ratio and so , the quality of signal retrieval in the frequency domain . Reviewer 1 : `` 4- Why are the additive noises epsilon_i supposed to have the same E ( |epsilon_i|^2| ) ? Even if they are uncorrelated , what is the hypothesis behind that ? `` : We are assuming that the noise components have the same mean 0 and the same variance sigma . E [ |epsilon_i|^2| ] = E [ epsilon_i^2 ] = E [ ( epsilon_i - 0 ) ^ { 2 } ] = E [ ( epsilon_i - E [ epsilon_i ] ) ^ { 2 } ] = Var [ epsilon_i ] = sigma . This is why the sum over i from 1 to N of E ( |epsilon_i|^2| ) is equal to N times E ( |epsilon_i|^2| ) = N sigma . Reviewer 1 : `` 5- In the CSimLoss , why ( i ) is the real part negative and the imaginary part positive ; ( ii ) is the imaginary part squared ? `` : We have shown in section A.1 in the appendix and in section 4 that : a ) When the real part of the normalized inner product is maximized , the match in amplitude between the estimate and the target is maximized . b ) When the imaginary part of the normalized inner product is 0 , the match in phase between the estimate and the target is maximized . Deep learning frameworks are built around minimization of some objective function . We formulate the CSimLoss so that its minimization maximizes the match between estimate and target . To do this , we minimize the negation of its real part and minimize the square of its imaginary part . The minimum of the real part \u2019 s negation is achieved at -1 , and the minimum of the imaginary part \u2019 s square is achieved at $ 0^2 = 0 $ , which corresponds to our needs ."}, {"review_id": "BylB4kBtwB-1", "review_text": "This work researches the deep complex-valued neural networks. Specifically, it proposes a new signal extraction mechanism that operates in frequency domain and applies to address the speech separation issue. Also, a function is proposed to explicitly consider both the magnitude and phase information of a signal. Related work on learning representation in frequency domain and speech separation is well introduced. Theoretical analysis is conducted to show the motivation and connection to signal processing. The architecture of the deep neural networks is presented in details, with the elaboration of the complex mask generation. Experimental study is conducted on a benchmark dataset to compare the proposed complex networks with those using real-part values only to demonstrate the improvement. The rating is 3: Weak Reject considering that the novelty is limited and the experimental study is weak. 1. The significance of the theoretical analysis in Eq.(1) to Eq.(4) needs to be better explained. Currently, they seem to be some straightforward results in the field of signal processing; 2. The proposed CSimLoss is interesting. However, its effectiveness seems to be limited as demonstrated in Table 1. It can be found that the CSimLoss in some cases is only comparable (or even inferior) to the L2freq loss; 3. The mask generation proposed in Section 6 conceptually is largely an attention mechanism that has been widely applied in deep networks; 4. The experimental comparison in Table 1 is limited, although some improvements have been demonstrated. This work shall also make a comparison with some of the existing methods on speech separation as described in Section 2.2. 5. It was mentioned in the last paragraph of Section 7 that (Shi et al. 2019) uses different data preparation than this paper. Can this paper use the same data preparation as (Shi et al. 2019) and perform some comparisons? 6. Why did (Shi et al. 2019) achieve better SDR (12.1 vs. 11.3) than the proposed method using standard setup? 7. What if the mechanism of mask generation is also applied to Real U-Net? How much improvement can this bring? ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the useful feedback . We will attempt here to highlight those aspects of the paper we believe are novel , and defend our experimental study \u2019 s setup . Reviewer 3 : `` 1 . The significance of the theoretical analysis in Eq . ( 1 ) to Eq . ( 4 ) needs to be better explained . Currently , they seem to be some straightforward results in the field of signal processing ; Answer by the authors '' : Section 3 and Equations ( 1 ) to ( 4 ) are related to signal processing , but serve to justify the use of FiLM and signal averaging by demonstrating how a combination of both increases the Signal-To-Noise ratio ( SNR ) of retrieved signals and the quality of retrieval . By elaborating such a demonstration , we make a bridge between the proposed extraction approach , that combines FiLM and signal averaging , and the literature in signal processing . Such a connection allows us to show that the proposed approach is a principled method for signal extraction in the frequency domain . More precisely , in section 3 , we explain how : 1- Using Film makes sense in the context of signal retrieval in the frequency domain as it learns a scaling ( \u0393 ) and a shifting representation ( B ) of a noisy signal F ( y ) in order to retrieve the clean signal F ( s ) ; 2- Approximating F ( s ) by performing the operation of signal averaging allows one to increase the signal to noise ratio of the retrieved signal . We provide the following explanation to help understand the method : Equation ( 1 ) gives the expression of the Fourier transform of a noisy signal y . By leveraging the linearity of the Fourier Transform and Convolution Theorem we get equation ( 1 ) . Equation ( 2 ) is just the expression of the clean signal in terms of the Fourier transforms of the noisy signal , the impulse response and the noise component . In equation ( 2 ) , [ 1 / F ( r ) ] and - [ F ( epsilon ) / F ( r ) ] are respectively scaling and shifting representations of F ( y ) . Now , Film ( Perez et al. , 2017 ) is a mechanism that allows to infer a scaling ( \u0393 ) and a shifting representation ( B ) given an input representation . This is why we use FiLM as it allows to infer \u0393 = [ 1 / F ( r ) ] and B = - [ F ( epsilon ) / F ( r ) ] given the noisy mix and the output of the U-Net . Equations ( 1 ) and ( 2 ) allow us to express F ( s ) in terms of \u0393 and B . Equation ( 3 ) is just the application of equation ( 2 ) in the context of speech separation where multiple speakers are involved in the mix and we have to retrieve the clean speech of each of the distinct speakers . We assume then , that , for each speaker , there exists an impulse response r_ { i } and an additive noise epsilon_ { i } such that they allow to reconstruct the original constant mix . This leads to the expression of Equation ( 3 ) . Now , indeed the text between Equation ( 3 ) and Equation ( 4 ) is related to a very well-known result in signal processing which explains how signal averaging increases the signal to noise ratio of a given signal by a factor of N. This text provides to readers unfamiliar with signal averaging and signal processing a clear motivation for using signal averaging in the context of signal retrieval in the frequency domain . Equation ( 4 ) then gives the expression of F ( s ) when such an operation is implemented . Reviewer 3 : `` 2 . The proposed CSimLoss is interesting . However , its effectiveness seems to be limited as demonstrated in Table 1 . It can be found that the CSimLoss in some cases is only comparable ( or even inferior ) to the L2freq loss ; You have has also mentioned in your 4th remark : 4 . The experimental comparison in Table 1 is limited , although some improvements have been demonstrated . `` : Table 1 provides a synopsis of the most important results obtained for the task of speech separation . A more exhaustive set of experiments are provided in the appendix . As mentioned in the beginning of section 7 , the complete results and the extended empirical analysis can be found in the appendix in section A.5 where the list of all the experiments is contained in big tables ( Tables 2 and 3 ) . Also , as mentioned in the analysis in section 7 , and as it can be observed from Tables 2 and 3 , our extraction mechanism is particularly well suited for being paired with the CSimLoss objective . We draw this conclusion because when the proposed extraction mechanism is not used the CSimLoss performs comparably to the L2freq loss . However , when the extraction mechanism is introduced ( FiLm with multiple transformation , signal averaging and dropout ) , the CSimLoss significantly outperforms the L2freq loss and yields the best results ."}, {"review_id": "BylB4kBtwB-2", "review_text": "The authors propose complex valued neural networks to perform audio source separation in the Fourier domain. The adapt a well known U-Net architecture to the task by introducing a complex-valued FiLM layer and a new complex similarity loss that explicitly takes magnitude and phase into account. They motivate the use of complex values well and demonstrate performance and parameter efficiency improvements over real-valued baselines. Importantly, they do not need to perform spetrogram inversion because their network works natively in the complex domain. Despite the quantitative improvements over spectral models, they still slightly underperform the ConvTasNet baseline that operates directly in the waveform domain (which was slightly misleading to not include in the table). The authors perform an extensive hyperparameter search to tune the model and provide sufficient detail to reproduce their experiments. While the results did not improve upon the best baseline, they do provide further evidence to the value of using complex-valued neural networks to handle complex-valued data (where phase and synchronicity matter), which I believe will be of value to the ICLR community, and thus I lean slightly in favor of acceptance. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the useful feedback and appreciate the encouraging comments . To address quickly existing state of the art methods , we have added a Table 4 to our paper , summarizing models in the literature and including ConvTasNet . One important clarification that Table 4 provides is that the various methods use different data preparations and parameters . In particular , ConvTasNet stands out for its use of a non-standard preprocessing of the data , unlike that of the other methods and likely to improve its headline SDR score . Our ConvTasNet reproduction with the standard data preprocessing protocol yielded a significantly lower score . ConvTasNet also stands out for its use of smaller windows and hop lengths , which favour it relative to other papers ( Yu et al , 2017 ) . We have used window sizes and hop lengths more typical of the other papers , facilitating comparisons ."}], "0": {"review_id": "BylB4kBtwB-0", "review_text": "This paper proposes a new method for source separation, by using deep learning UNets, complex-valued representations and the Fourier domain. Concretely, their contribution is : i) a complex-valued convolutional version of the Feature-Wise Linear Modulation, able to optimise the parameters needed to create multiple separated candidates for each of signal sources that are then combined using signal averaging; ii) the design of a loss that takes into account magnitude and phase while being scale and time invariant. It was then tested and compared with real-valued versions, and also some state-of-the-art methods. Overall, I think this paper is of good quality and proposes an interesting method for this crucial task of source separation. However, I found the paper too dense and difficult to read (even if well written), and it looks like a re-submission from a journal paper of more than 8 pages. I would suggest the authors to shrink the paper so it *really* fits into the 8-pages (without important figures or important implementation details in the appendices), maybe at the cost of leaving some parts (such as old related works) out of the paper. The experiments are important here, and it is too bad that the comparison with state-of-the-art is just in the last paragraph, while the results do not seem to show any improvements compared to the other methods. The computational time might be very important here, as the claim is that FFT reduces time computation, but I did not had time to go through all the appendices. Positive aspects: - The work is well documented and motivated, and I found that the reflexion leading to the method is of good quality. - Concretely, I found interesting the use of the FiLM, originally designed for another application, for minimizing the SNR of the signal sources. The motivation/proof is quite clear too. - Equally, the motivation for the design of the new loss is clear and interesting. - I also found important the experiments, that shows in the same table the difference between the method without the complex-valued part and with different parameter values. Questions and remarks: - I have to recall that I am not an expert on source separation and complex-valued deep learning. Yet, I have had difficulties in understanding the structure of the method, even if the different parts were clearly explained. The figure 1 is very useful, but I found it not clear enough and too small. I went to find some informations in the appendices, but there are too much crucial information there and I did not have time to go through all of it. - The use of the U-net architecture is not explained (just some citations are given). What is supposed to be the output of it? - When you say 'to be more rigorous, we can assume in the cse of speech separation that, for each speaker, there exists an impulse response such that when it is convolved with the clean speech of the speaker, it allows to reconstruct the mix' : why can we be sure that it is always possible, and why is it more rigourous? - Why are the additive noises epsilon_i supposed to have the same E(|epsilon_i|^2|) ? Even if they are uncorrelated, what is the hypothesis behind that? - In the CSimLoss, why (i) is the real part negative and the imaginary part positive; (ii) is the imaginary part squared? - have you tested with a higher lambda_imag (as the larger is now the best)? - It looks from the end of the paper that the method is still not achieving better results than the state-of-the-art. I agree with the authors as it might not be the scope of the paper, but then what is it? If it's time computation, it is not shown in the paper. If it is just a methodology, what would be required in the future to beat the best method? - In the results, table 1, in the last 4 lines: it looks from 1st and 2nd line that the new loss CSimLoss is not very different from the L2 (9.88 compared to 9.87). The best result, in the 4th line, cannot be compared to the 3rd line as both the loss and the number of transforms are different. I then found those values in the appendices, but it would be best to show fewer parameters varying in the main paper, but show some results that can be easily compared. - What is the importance of the first paragraph in 2.1? I was not aware of the holographic reduced representations, but I don't understand it more now, and I don't see why explaining that for 15 lines. Small remarks: - 'deep complex valued models have *just* started to gain momentum'... with citations beginning in 2014, I would not say 'just'. - 'in the frequncy domain is then, ...' --> frequency + no coma - Figure 2 is in the appendix, while in the text it is not said so. I was lost. This figure should not be in the appendix as the appendix should not have key elements, but just details that are not important for the understanding of the paper. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the useful feedback and appreciate the encouraging comments . We have considered the comments and tried to address them below . Reviewer 1 : `` 1- I have to recall that I am not an expert on source separation and complex-valued deep learning . Yet , I have had difficulties in understanding the structure of the method , even if the different parts were clearly explained . The figure 1 is very useful , but I found it not clear enough and too small . I went to find some informations in the appendices , but there are too much crucial information there and I did not have time to go through all of it . '' The U-Net is not the contribution of the paper and it has been widely used in inverse problems such as image segmentation and signal reconstruction . This is why details of the residual upsampling and downsampling blocks incorporated in the U-Net are in the appendix in section A.3 and not in the main text . Sections A.4 : DATA PRE-PROCESSING AND TRAINING DETAILS contains just the details about the implementation , the optimizer and the standard processing of the WSJ dataset ( Hershey et al , 2015 ) . It is not about the scientific contributions but more about the technical details and that is why it is in the appendix . We have provided our source code so the reader can have a better grasp of our method and can reproduce our work . Reviewer 1 : `` 2- The use of the U-net architecture is not explained ( just some citations are given ) . What is supposed to be the output of it ? '' In section 5 in the main text explained that the U-Net architecture we used is similar to \u201c the complex-valued U-Net architecture used in Dedmari et al . ( 2018 ) who reported state-of-the-art results in MRI reconstruction using complex-valued raw input \u201d . That design \u2019 s success is why we selected it as a base . The U-Net \u2019 s output is an intermediate representation that the extractor mechanism learns to use to generate clean speech candidates . Reviewer 1 : `` 3- When you say 'to be more rigorous , we can assume in the case of speech separation that , for each speaker , there exists an impulse response such that when it is convolved with the clean speech of the speaker , it allows to reconstruct the mix ' : why can we be sure that it is always possible , and why is it more rigorous ? '' This information is translated in equation ( 3 ) which is just the application of the equation y = s * r + epsilon , in the context of speech separation . Similar assumptions have been also been made and reported in speech-related problems such as in https : //arxiv.org/pdf/1705.10874.pdf Where noisy signals are the result y ( t ) = s ( t ) \u2217 r ( t ) + n ( t ) . If we want to apply that assumption to speech separation , then , the analogue of the noisy constant signal y ( t ) becomes the mix containing all speeches , for each ith speaker that will be a clean speech s_ { i } ( t ) an impulse response r_ { i } ( t ) and an additive noise n_ { i } ( t ) . Now all of these assumptions correct ? Of course not as George Box states : \u201c All models are wrong but some of them are useful \u201d . It is a wrong assumption but a useful one as it allowed us to leverage the convolution theorem and the linearity of the Fourier transform in order to deduce an extraction mechanism based on FiLM and signal averaging allowing us to increase the signal-to-noise-ratio and so , the quality of signal retrieval in the frequency domain . Reviewer 1 : `` 4- Why are the additive noises epsilon_i supposed to have the same E ( |epsilon_i|^2| ) ? Even if they are uncorrelated , what is the hypothesis behind that ? `` : We are assuming that the noise components have the same mean 0 and the same variance sigma . E [ |epsilon_i|^2| ] = E [ epsilon_i^2 ] = E [ ( epsilon_i - 0 ) ^ { 2 } ] = E [ ( epsilon_i - E [ epsilon_i ] ) ^ { 2 } ] = Var [ epsilon_i ] = sigma . This is why the sum over i from 1 to N of E ( |epsilon_i|^2| ) is equal to N times E ( |epsilon_i|^2| ) = N sigma . Reviewer 1 : `` 5- In the CSimLoss , why ( i ) is the real part negative and the imaginary part positive ; ( ii ) is the imaginary part squared ? `` : We have shown in section A.1 in the appendix and in section 4 that : a ) When the real part of the normalized inner product is maximized , the match in amplitude between the estimate and the target is maximized . b ) When the imaginary part of the normalized inner product is 0 , the match in phase between the estimate and the target is maximized . Deep learning frameworks are built around minimization of some objective function . We formulate the CSimLoss so that its minimization maximizes the match between estimate and target . To do this , we minimize the negation of its real part and minimize the square of its imaginary part . The minimum of the real part \u2019 s negation is achieved at -1 , and the minimum of the imaginary part \u2019 s square is achieved at $ 0^2 = 0 $ , which corresponds to our needs ."}, "1": {"review_id": "BylB4kBtwB-1", "review_text": "This work researches the deep complex-valued neural networks. Specifically, it proposes a new signal extraction mechanism that operates in frequency domain and applies to address the speech separation issue. Also, a function is proposed to explicitly consider both the magnitude and phase information of a signal. Related work on learning representation in frequency domain and speech separation is well introduced. Theoretical analysis is conducted to show the motivation and connection to signal processing. The architecture of the deep neural networks is presented in details, with the elaboration of the complex mask generation. Experimental study is conducted on a benchmark dataset to compare the proposed complex networks with those using real-part values only to demonstrate the improvement. The rating is 3: Weak Reject considering that the novelty is limited and the experimental study is weak. 1. The significance of the theoretical analysis in Eq.(1) to Eq.(4) needs to be better explained. Currently, they seem to be some straightforward results in the field of signal processing; 2. The proposed CSimLoss is interesting. However, its effectiveness seems to be limited as demonstrated in Table 1. It can be found that the CSimLoss in some cases is only comparable (or even inferior) to the L2freq loss; 3. The mask generation proposed in Section 6 conceptually is largely an attention mechanism that has been widely applied in deep networks; 4. The experimental comparison in Table 1 is limited, although some improvements have been demonstrated. This work shall also make a comparison with some of the existing methods on speech separation as described in Section 2.2. 5. It was mentioned in the last paragraph of Section 7 that (Shi et al. 2019) uses different data preparation than this paper. Can this paper use the same data preparation as (Shi et al. 2019) and perform some comparisons? 6. Why did (Shi et al. 2019) achieve better SDR (12.1 vs. 11.3) than the proposed method using standard setup? 7. What if the mechanism of mask generation is also applied to Real U-Net? How much improvement can this bring? ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the useful feedback . We will attempt here to highlight those aspects of the paper we believe are novel , and defend our experimental study \u2019 s setup . Reviewer 3 : `` 1 . The significance of the theoretical analysis in Eq . ( 1 ) to Eq . ( 4 ) needs to be better explained . Currently , they seem to be some straightforward results in the field of signal processing ; Answer by the authors '' : Section 3 and Equations ( 1 ) to ( 4 ) are related to signal processing , but serve to justify the use of FiLM and signal averaging by demonstrating how a combination of both increases the Signal-To-Noise ratio ( SNR ) of retrieved signals and the quality of retrieval . By elaborating such a demonstration , we make a bridge between the proposed extraction approach , that combines FiLM and signal averaging , and the literature in signal processing . Such a connection allows us to show that the proposed approach is a principled method for signal extraction in the frequency domain . More precisely , in section 3 , we explain how : 1- Using Film makes sense in the context of signal retrieval in the frequency domain as it learns a scaling ( \u0393 ) and a shifting representation ( B ) of a noisy signal F ( y ) in order to retrieve the clean signal F ( s ) ; 2- Approximating F ( s ) by performing the operation of signal averaging allows one to increase the signal to noise ratio of the retrieved signal . We provide the following explanation to help understand the method : Equation ( 1 ) gives the expression of the Fourier transform of a noisy signal y . By leveraging the linearity of the Fourier Transform and Convolution Theorem we get equation ( 1 ) . Equation ( 2 ) is just the expression of the clean signal in terms of the Fourier transforms of the noisy signal , the impulse response and the noise component . In equation ( 2 ) , [ 1 / F ( r ) ] and - [ F ( epsilon ) / F ( r ) ] are respectively scaling and shifting representations of F ( y ) . Now , Film ( Perez et al. , 2017 ) is a mechanism that allows to infer a scaling ( \u0393 ) and a shifting representation ( B ) given an input representation . This is why we use FiLM as it allows to infer \u0393 = [ 1 / F ( r ) ] and B = - [ F ( epsilon ) / F ( r ) ] given the noisy mix and the output of the U-Net . Equations ( 1 ) and ( 2 ) allow us to express F ( s ) in terms of \u0393 and B . Equation ( 3 ) is just the application of equation ( 2 ) in the context of speech separation where multiple speakers are involved in the mix and we have to retrieve the clean speech of each of the distinct speakers . We assume then , that , for each speaker , there exists an impulse response r_ { i } and an additive noise epsilon_ { i } such that they allow to reconstruct the original constant mix . This leads to the expression of Equation ( 3 ) . Now , indeed the text between Equation ( 3 ) and Equation ( 4 ) is related to a very well-known result in signal processing which explains how signal averaging increases the signal to noise ratio of a given signal by a factor of N. This text provides to readers unfamiliar with signal averaging and signal processing a clear motivation for using signal averaging in the context of signal retrieval in the frequency domain . Equation ( 4 ) then gives the expression of F ( s ) when such an operation is implemented . Reviewer 3 : `` 2 . The proposed CSimLoss is interesting . However , its effectiveness seems to be limited as demonstrated in Table 1 . It can be found that the CSimLoss in some cases is only comparable ( or even inferior ) to the L2freq loss ; You have has also mentioned in your 4th remark : 4 . The experimental comparison in Table 1 is limited , although some improvements have been demonstrated . `` : Table 1 provides a synopsis of the most important results obtained for the task of speech separation . A more exhaustive set of experiments are provided in the appendix . As mentioned in the beginning of section 7 , the complete results and the extended empirical analysis can be found in the appendix in section A.5 where the list of all the experiments is contained in big tables ( Tables 2 and 3 ) . Also , as mentioned in the analysis in section 7 , and as it can be observed from Tables 2 and 3 , our extraction mechanism is particularly well suited for being paired with the CSimLoss objective . We draw this conclusion because when the proposed extraction mechanism is not used the CSimLoss performs comparably to the L2freq loss . However , when the extraction mechanism is introduced ( FiLm with multiple transformation , signal averaging and dropout ) , the CSimLoss significantly outperforms the L2freq loss and yields the best results ."}, "2": {"review_id": "BylB4kBtwB-2", "review_text": "The authors propose complex valued neural networks to perform audio source separation in the Fourier domain. The adapt a well known U-Net architecture to the task by introducing a complex-valued FiLM layer and a new complex similarity loss that explicitly takes magnitude and phase into account. They motivate the use of complex values well and demonstrate performance and parameter efficiency improvements over real-valued baselines. Importantly, they do not need to perform spetrogram inversion because their network works natively in the complex domain. Despite the quantitative improvements over spectral models, they still slightly underperform the ConvTasNet baseline that operates directly in the waveform domain (which was slightly misleading to not include in the table). The authors perform an extensive hyperparameter search to tune the model and provide sufficient detail to reproduce their experiments. While the results did not improve upon the best baseline, they do provide further evidence to the value of using complex-valued neural networks to handle complex-valued data (where phase and synchronicity matter), which I believe will be of value to the ICLR community, and thus I lean slightly in favor of acceptance. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the useful feedback and appreciate the encouraging comments . To address quickly existing state of the art methods , we have added a Table 4 to our paper , summarizing models in the literature and including ConvTasNet . One important clarification that Table 4 provides is that the various methods use different data preparations and parameters . In particular , ConvTasNet stands out for its use of a non-standard preprocessing of the data , unlike that of the other methods and likely to improve its headline SDR score . Our ConvTasNet reproduction with the standard data preprocessing protocol yielded a significantly lower score . ConvTasNet also stands out for its use of smaller windows and hop lengths , which favour it relative to other papers ( Yu et al , 2017 ) . We have used window sizes and hop lengths more typical of the other papers , facilitating comparisons ."}}