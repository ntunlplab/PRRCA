{"year": "2020", "forum": "SygeY1SYvr", "title": "Are Few-shot Learning Benchmarks Too Simple ?", "decision": "Reject", "meta_review": "The paper is interested in assessing the difficulty of popular few-shot classification benchmarks (Omniglot and miniImageNet). A clustering-based meta-learning method is proposed (called Centroid Network), on which a metric is built (gap between the performance of Prototypical Networks and Centroid Networks). As noted by several reviewers, the proposed metric (critical for the paper) is however not motivated enough, nor convincing enough - after discussion, the logic in the metric reasoning seems to remain flawed.\n", "reviews": [{"review_id": "SygeY1SYvr-0", "review_text": "This paper introduces a new method for learning to cluster without labels at meta-evaluation time and show that this method does as well as supervised methods on benchmarks with consistent class semantics. The authors propose a new metric for measuring the simplicity of a few-shot learning benchmark and demonstrate that it is possible to achieve high performance on Omniglot and miniImageNet with their unsupervised method, resulting in a high value of this criterion, whereas the Meta-Dataset is much more difficult. The paper is well written and generally very clear. I appreciate that the authors have highlighted the limitations of both their clustering method (that it requires more assumptions than CCNs) and their benchmark. The centroid method itself seems to draw heavily on pre-existing work, but uses a new similarity metric that improves performance beyond the current state-of-the-art on few-shot clustering tasks. The authors acknowledge that the approximate CSCC metric they define is not consistent across architectures and hyperparameters. It is also a fairly simple metric, but nonetheless represents a novel contribution. Overall I feel that the paper introduces a well-defined problem and makes a step toward quantifying and resolving it. The experiments do a thorough job supporting the arguments of the authors. I have only minor issues that could help with the clarity of this paper: I wasn\u2019t sure what was meant by \u201crelabeling\u201d the query set predictions in the text below Figure 2. I would have appreciated some discussion as to why Sinkhorn distance might be expected to improve performance .", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for their positive review and constructive feedback . We have added explanations relating to the advantages of Sinkhorn K-Means to the Appendix . * Relabeling the query set Any clustering is permutation invariant . Therefore , there are many equally correct ways to label the support set and the query set , which is why we find the optimal permutation which matches the ground truth cluster indices with the predicted cluster indices . Specifically for Figure 2 : - cluster indices are predicted for the query set shapes during step 3 . { yellow square } gets assigned to cluster A , { red square , green triangle , yellow triangle } get assigned to cluster B . - the optimal permutation shows that cluster A matches Class 2 , and cluster B matches Class 1 - therefore , { yellow square } gets relabeled to Class 2 , { red square , green triangle , yellow triangle } get relabeled to class 1 - the unsupervised accuracy can be computed by comparing the predicted and ground truth classes 1 , 2 . * Why is Sinkhorn K-means expected to improve performance ? There are mainly two reasons why Sinkhorn K-Means improves performance compared to K-Means : - Sinkhorn K-Means is particularly well adapted to the few-shot clustering and unsupervised few-shot classification problems because it strictly enforces the fact that the classes have to follow a given distribution ( e.g.balanced ) , whereas K-Means does not . - Sinkhorn K-Means is likely to converge better than K-means due to the regularization factor of the Sinkhorn distance . To illustrate the second point , consider the limit case where the regularization factor of Sinkhorn distance goes to infinity . Then , the assignments in Sinkhorn K-Means become uniform ( each cluster is assigned equally to all points ) , and all the centroids converge to the average of all the points , which in this case is a global minimum . This is by no means a proof , but this example suggests that for large enough regularization , Sinkhorn K-Means will converge better ."}, {"review_id": "SygeY1SYvr-1", "review_text": "The paper is concerned with few-shot classification, both its benchmarks and method used to tackle it. The scope of the few-shot classification problem can be set relatively widely, depending on what data is available at what stage. In general few-shot classification is an important ability of intelligent systems and arguably an area in which biological systems outperform current AI systems the most. The paper makes a number of contributions. (1) It suggests an approach to do a specific type of clustering and compares it favorably to the existing literature. In a specific sense the approach does not use supervised labels (\u201cwithout labels at meta-evaluation time\u201d). (2) It applies that approach to currently existing datasets and achieves \u201csurprisingly high accuracies\u201d in that setting, with the implication that this shows a weakness in these datasets when used for benchmarking (\u201ctoo easy\u201d). (3) It further suggests a metric, dubbed \u201cclass semantics consistency criterion\u201d, that aims to quantify this shortcoming of current benchmarks on these datasets. (4) It assesses a specific meta-dataset using that metric, confirming it is harder in this sense, at least in specific settings. My assessment of the paper is mildly negative; however this is an assessment with low confidence given that I am no expert on few-shot classification or related areas. While the authors first example (the \u201cMongolian\u201d alphabet of the Omniglot dataset and geometric shapes falling into different categories) illustrates the problem space well and is indeed quite intuitive, the same cannot be said about either the specific setting they consider nor the metric they propose. It\u2019s not immediately clear that the other approaches from the literature they compare their method to were conceived for the setting considered here, or indeed optimized for it. The authors do show good accuracy on clustering Omniglot characters without using labels and thus indeed demonstrate a high amount of class semantics consistency for that dataset. The results on miniImageNet are less clear-cut, and the results of the evaluation of the meta-dataset appear to depend on the specific setting considered. This makes it unclear to what extent the proposed metric is general and predictive. To their credit, the authors state that in future work they are looking to make their metric \u201cmore interpretable and less dependent on the backbone architectures\u201d. I believe the paper might benefit from being given additional attention. A streamlined and more accessible version might well be an important contribution in the future.", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for taking the time to review our paper . We address the reviewer \u2019 s concerns below . * It \u2019 s not immediately clear that the other approaches from the literature they compare their method to were conceived for the setting considered here , or indeed optimized for it . Supervised Few-Shot Classification literature : - Our main contribution is to compare CentroidNets on unsupervised few-shot classification vs. ProtoNets on supervised few-shot classification . - ProtoNets were conceived and optimized precisely for the supervised few-shot classification problem ( often just called few-shot learning ) . - Our comparison is fair because unsupervised few-shot classification is strictly harder than supervised few-shot classification . Few-shot clustering literature : - Our main contribution does n't lie in our comparison with CCN ( Hsu et al.2017 ) .- The comparison is here mostly to confirm that our approach is reasonable ( to have a point of comparison ) . - We are honest and open about the fact that our method is less flexible than CCN . - See page 8 \u201c However , we wish to point out that Centroid Networks are less flexible than CCNs , as they require specifying the number of clusters and making an assumption on the sizes of the clusters [ ... ] \u201d * The results on miniImageNet are less clear-cut Indeed , the gap between supervised and unsupervised accuracies is bigger on miniImageNet . - This can be due to the higher visual difficulty of miniImageNet . - This can also be due to the lower class semantic consistency . We do point out that the unsupervised accuracy of our method ( 55.3 % ) is still impressive , considering that it is almost equal to the performance of earlier supervised few-shot classification methods with the same architecture ( 56.2 % for MatchingNets without fine-tuning ) . * The results of the evaluation of the meta-dataset appear to depend on the specific setting considered Just like the usual supervised accuracy in few-shot learning , the unsupervised accuracy and CSCC are dependent on the task distribution considered . Therefore , it is perfectly normal that the numbers are different for Meta-Dataset in the [ Train on ILSVRC ] vs. [ Train on All datasets ] setting , because they define different task distributions ( and are therefore different benchmarks ) . * This makes it unclear to what extent the proposed metric is general and predictive . In order to better address the reviewer \u2019 s concern , we ask the reviewer to clarify what they mean exactly by \u201c general \u201d and \u201c predictive \u201d . Maybe with a concrete example illustrating what these properties are ?"}, {"review_id": "SygeY1SYvr-2", "review_text": "<Paper summary> The authors argue that the popular benchmark datasets, Omniglot and miniImageNet, are too simple to evaluate supervised few-shot classification methods due to their insufficient variety of class semantics. To validate this, the authors proposed clustering-based meta-learning method, called Centroid Network. Although it does not utilize supervision information during meta-evaluation, it can achieve high accuracies on Omniglot and miniImageNet. The authors also proposed a new metric to quantify the difficulty of meta-learning for few-shot classification. <Review summary> Although the main claim of this paper seems correct, it is not sufficiently supported by theory or experiments. My score is actually on the border line, but I currently vote for \"weak reject, because some points in the paper are ambiguous yet. Given clarifications in an author response, I would be willing to increase the score. <Details> * Strength + The paper is well-organized. Especially, the examples shown in the introduction greatly help understanding of what the authors argue in this paper. + A novel study on quantifying the difficulty of meta-learning. + The proposed CentroidNet performs well in the experiments. * Weakness and concerns - Does CentroidNet really work without labels during \"meta-validation\"? As far as I understand, ground truth clusters of the support set defined by the labels are required to compute the accuracies. Therefore, the labels seem to be required to validate the performance of the model. I think it should be \"meta-test.\" - The authors state \"The most intuitive way to train ..., we did not have much success with this approach\" in 5.3, but it is counter-intuitive. If the class semantics are similar among episodes, \"the most intuitive way\" should work, because it can learn the common semantics via meta-training. Further discussion about why it does not work is required. - The high performance of CentroidNet does not support the claim on the insufficient variety of the class semantics. According to ablation study, adopting Sinkhorn K-means is the most important factor to improve the performance. It means that adopting weighted average like in [R1] can also improve the performance of ProtoNet, which results in substantial difference in the performance between ProtoNet and CentroidNet that can deny the claim. - The definition of CSCC is not convincing. First, I could not get the meaning of \"unsupervised Bayes accuracy\" (supervised Bayes accuracy means 1 - Bayes error rate, right?). Second, CSCC seems to mainly quantify the importance of the supervision information during meta-learning, which is not directly related to the difficulty of few-shot learning problem. Intuitively, difficult few-shot learning problems should lead to lower supervised Bayes accuracy, which does not necessarily decrease CSCC. Third, what we can induce via comparing CSCC is not clarified in theory. The discussion in 6.3 is too subjective and specific for the case of training with ILSVRC/all datasets. - This paper lacks citing some closely related works [R1, R2]. [R1] \"Infinite Mixture Prototypes for Few-Shot Learning,\" ICML2019 [R2] \"A Closer Look at Few-shot Classification,\" ICLR2019 * Minor concerns that do not have an impact on the score - Another arXiv paper related to this work: \"Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML\" ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for their thorough review and raising several valid points . We have done our best to answer them and we will improve the main paper accordingly ( we have added some points to the appendix already ) . We hope that the reviewer will reconsider their score if we have addressed their concerns . * `` CentroidNets uses a variety of tricks to improve performance , therefore it is unfair to not also consider tricks ( such as R1 ) to improve Protonet performance '' [ Rephrased from Reviewer : \u201c The high performance of CentroidNet does not support the claim on the insufficient variety of the class semantics [ ... ] ] Indeed , Sinkhorn K-Means is a key component in the performance of CentroidNets . However , it is not obvious that using Sinkhorn K-Means would be an unfair advantage compared to Prototypical Networks , for two reasons : - In CentroidNets , we use Sinkhorn k-Means to attempt to recover the hidden class labels , i.e.to infer the ground-truth labels . In contrast , ProtoNets has direct access to the ground-truth labels ( which incidentally turn out to be hard assignments and lead to unweighted averages ) . - In CentroidNets , we run Sinkhorn k-Means on representations which were learned with the ProtoNet loss , i.e. , they were by construction designed to be averaged without weights . However , in order to best address the reviewer \u2019 s concern , we go further and run new experiments on miniImageNet . This time we constrain the centroids to be unweighted averages of the data points . To do so , starting from the soft weights , we reassign each point only to its closest centroid , and compute the unweighted averages . The comparison between ProtoNets and CentroidNets is now fair in the sense that both prototypes and centroids use unweighted averages . - Unsupervised Accuracy on miniImageNet is 0.5508 +/- 0.0072 for weighted average and 0.5497 +/- 0.0072 for unweighted average . The difference is not significant . - Clustering Accuracy on miniImageNet is 0.6421 +/- 0.0069 for weighted average and 0.6417 +/- 0.0069 for unweighted average . The difference is also not significant . We \u2019 ll be happy to add these results to the paper , if the review thinks them valuable . Therefore , the new experiment suggests that using weighted averages does not bring an unfair advantage , and therefore does not invalidate our comparison . More generally , instead of trying to tune ProtoNets and CentroidNets as well as possible , we try to use comparable models for ProtoNets and CentroidNets ( same architecture , nearly same representation ) . * What is unsupervised Bayes accuracy ? We define the unsupervised Bayes accuracy of an unsupervised few-shot classification task distribution as the highest achievable unsupervised accuracy . Just like the usual Bayes error is limited by label noise , the unsupervised Bayes accuracy is limited by cluster-semantic noise of a task . For illustration , consider the following unsupervised few-shot classification task distribution : - Uniformly sample a random dimension 1 < = j < = D ( hidden to the algorithm ) - Sample ( iid , probability=\u00bd ) random binary vectors ( x_i ) of dimension D ( shown to the algorithm ) and split them between support and query set . - Assign binary labels y = x_j to each vector ( x_i ) ( hidden to algorithm ) . - The goal is to cluster the support set and associate query set points with the support clusters . Because the algorithm does not know which dimension j was sampled ( i.e.the class semantic ) , it does not know how to cluster the support set . Therefore , it is just as good to make random predictions on the query set . Therefore the unsupervised Bayes accuracy is 0.5 Now , consider the same task distribution , except the dimension index j is always fixed to 1 . After meta-training , the algorithm can learn a representation mapping each vector to the value of its first dimension only . The support set can be clustered by grouping all 1s together , and all 0s together . Each query point can then be unambiguously assigned to one of the clusters . The resulting unsupervised Bayes accuracy is 1 . Both task distributions would become equivalent if the algorithm had access to the class semantics j . Therefore , the two unsupervised few-shot tasks differ in difficulty only because of the uncertainty/variability on class semantics , and this is reflected in the difference in unsupervised Bayes accuracy . If this example is deemed helpful , we \u2019 ll be happy to add it to the paper ."}], "0": {"review_id": "SygeY1SYvr-0", "review_text": "This paper introduces a new method for learning to cluster without labels at meta-evaluation time and show that this method does as well as supervised methods on benchmarks with consistent class semantics. The authors propose a new metric for measuring the simplicity of a few-shot learning benchmark and demonstrate that it is possible to achieve high performance on Omniglot and miniImageNet with their unsupervised method, resulting in a high value of this criterion, whereas the Meta-Dataset is much more difficult. The paper is well written and generally very clear. I appreciate that the authors have highlighted the limitations of both their clustering method (that it requires more assumptions than CCNs) and their benchmark. The centroid method itself seems to draw heavily on pre-existing work, but uses a new similarity metric that improves performance beyond the current state-of-the-art on few-shot clustering tasks. The authors acknowledge that the approximate CSCC metric they define is not consistent across architectures and hyperparameters. It is also a fairly simple metric, but nonetheless represents a novel contribution. Overall I feel that the paper introduces a well-defined problem and makes a step toward quantifying and resolving it. The experiments do a thorough job supporting the arguments of the authors. I have only minor issues that could help with the clarity of this paper: I wasn\u2019t sure what was meant by \u201crelabeling\u201d the query set predictions in the text below Figure 2. I would have appreciated some discussion as to why Sinkhorn distance might be expected to improve performance .", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for their positive review and constructive feedback . We have added explanations relating to the advantages of Sinkhorn K-Means to the Appendix . * Relabeling the query set Any clustering is permutation invariant . Therefore , there are many equally correct ways to label the support set and the query set , which is why we find the optimal permutation which matches the ground truth cluster indices with the predicted cluster indices . Specifically for Figure 2 : - cluster indices are predicted for the query set shapes during step 3 . { yellow square } gets assigned to cluster A , { red square , green triangle , yellow triangle } get assigned to cluster B . - the optimal permutation shows that cluster A matches Class 2 , and cluster B matches Class 1 - therefore , { yellow square } gets relabeled to Class 2 , { red square , green triangle , yellow triangle } get relabeled to class 1 - the unsupervised accuracy can be computed by comparing the predicted and ground truth classes 1 , 2 . * Why is Sinkhorn K-means expected to improve performance ? There are mainly two reasons why Sinkhorn K-Means improves performance compared to K-Means : - Sinkhorn K-Means is particularly well adapted to the few-shot clustering and unsupervised few-shot classification problems because it strictly enforces the fact that the classes have to follow a given distribution ( e.g.balanced ) , whereas K-Means does not . - Sinkhorn K-Means is likely to converge better than K-means due to the regularization factor of the Sinkhorn distance . To illustrate the second point , consider the limit case where the regularization factor of Sinkhorn distance goes to infinity . Then , the assignments in Sinkhorn K-Means become uniform ( each cluster is assigned equally to all points ) , and all the centroids converge to the average of all the points , which in this case is a global minimum . This is by no means a proof , but this example suggests that for large enough regularization , Sinkhorn K-Means will converge better ."}, "1": {"review_id": "SygeY1SYvr-1", "review_text": "The paper is concerned with few-shot classification, both its benchmarks and method used to tackle it. The scope of the few-shot classification problem can be set relatively widely, depending on what data is available at what stage. In general few-shot classification is an important ability of intelligent systems and arguably an area in which biological systems outperform current AI systems the most. The paper makes a number of contributions. (1) It suggests an approach to do a specific type of clustering and compares it favorably to the existing literature. In a specific sense the approach does not use supervised labels (\u201cwithout labels at meta-evaluation time\u201d). (2) It applies that approach to currently existing datasets and achieves \u201csurprisingly high accuracies\u201d in that setting, with the implication that this shows a weakness in these datasets when used for benchmarking (\u201ctoo easy\u201d). (3) It further suggests a metric, dubbed \u201cclass semantics consistency criterion\u201d, that aims to quantify this shortcoming of current benchmarks on these datasets. (4) It assesses a specific meta-dataset using that metric, confirming it is harder in this sense, at least in specific settings. My assessment of the paper is mildly negative; however this is an assessment with low confidence given that I am no expert on few-shot classification or related areas. While the authors first example (the \u201cMongolian\u201d alphabet of the Omniglot dataset and geometric shapes falling into different categories) illustrates the problem space well and is indeed quite intuitive, the same cannot be said about either the specific setting they consider nor the metric they propose. It\u2019s not immediately clear that the other approaches from the literature they compare their method to were conceived for the setting considered here, or indeed optimized for it. The authors do show good accuracy on clustering Omniglot characters without using labels and thus indeed demonstrate a high amount of class semantics consistency for that dataset. The results on miniImageNet are less clear-cut, and the results of the evaluation of the meta-dataset appear to depend on the specific setting considered. This makes it unclear to what extent the proposed metric is general and predictive. To their credit, the authors state that in future work they are looking to make their metric \u201cmore interpretable and less dependent on the backbone architectures\u201d. I believe the paper might benefit from being given additional attention. A streamlined and more accessible version might well be an important contribution in the future.", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for taking the time to review our paper . We address the reviewer \u2019 s concerns below . * It \u2019 s not immediately clear that the other approaches from the literature they compare their method to were conceived for the setting considered here , or indeed optimized for it . Supervised Few-Shot Classification literature : - Our main contribution is to compare CentroidNets on unsupervised few-shot classification vs. ProtoNets on supervised few-shot classification . - ProtoNets were conceived and optimized precisely for the supervised few-shot classification problem ( often just called few-shot learning ) . - Our comparison is fair because unsupervised few-shot classification is strictly harder than supervised few-shot classification . Few-shot clustering literature : - Our main contribution does n't lie in our comparison with CCN ( Hsu et al.2017 ) .- The comparison is here mostly to confirm that our approach is reasonable ( to have a point of comparison ) . - We are honest and open about the fact that our method is less flexible than CCN . - See page 8 \u201c However , we wish to point out that Centroid Networks are less flexible than CCNs , as they require specifying the number of clusters and making an assumption on the sizes of the clusters [ ... ] \u201d * The results on miniImageNet are less clear-cut Indeed , the gap between supervised and unsupervised accuracies is bigger on miniImageNet . - This can be due to the higher visual difficulty of miniImageNet . - This can also be due to the lower class semantic consistency . We do point out that the unsupervised accuracy of our method ( 55.3 % ) is still impressive , considering that it is almost equal to the performance of earlier supervised few-shot classification methods with the same architecture ( 56.2 % for MatchingNets without fine-tuning ) . * The results of the evaluation of the meta-dataset appear to depend on the specific setting considered Just like the usual supervised accuracy in few-shot learning , the unsupervised accuracy and CSCC are dependent on the task distribution considered . Therefore , it is perfectly normal that the numbers are different for Meta-Dataset in the [ Train on ILSVRC ] vs. [ Train on All datasets ] setting , because they define different task distributions ( and are therefore different benchmarks ) . * This makes it unclear to what extent the proposed metric is general and predictive . In order to better address the reviewer \u2019 s concern , we ask the reviewer to clarify what they mean exactly by \u201c general \u201d and \u201c predictive \u201d . Maybe with a concrete example illustrating what these properties are ?"}, "2": {"review_id": "SygeY1SYvr-2", "review_text": "<Paper summary> The authors argue that the popular benchmark datasets, Omniglot and miniImageNet, are too simple to evaluate supervised few-shot classification methods due to their insufficient variety of class semantics. To validate this, the authors proposed clustering-based meta-learning method, called Centroid Network. Although it does not utilize supervision information during meta-evaluation, it can achieve high accuracies on Omniglot and miniImageNet. The authors also proposed a new metric to quantify the difficulty of meta-learning for few-shot classification. <Review summary> Although the main claim of this paper seems correct, it is not sufficiently supported by theory or experiments. My score is actually on the border line, but I currently vote for \"weak reject, because some points in the paper are ambiguous yet. Given clarifications in an author response, I would be willing to increase the score. <Details> * Strength + The paper is well-organized. Especially, the examples shown in the introduction greatly help understanding of what the authors argue in this paper. + A novel study on quantifying the difficulty of meta-learning. + The proposed CentroidNet performs well in the experiments. * Weakness and concerns - Does CentroidNet really work without labels during \"meta-validation\"? As far as I understand, ground truth clusters of the support set defined by the labels are required to compute the accuracies. Therefore, the labels seem to be required to validate the performance of the model. I think it should be \"meta-test.\" - The authors state \"The most intuitive way to train ..., we did not have much success with this approach\" in 5.3, but it is counter-intuitive. If the class semantics are similar among episodes, \"the most intuitive way\" should work, because it can learn the common semantics via meta-training. Further discussion about why it does not work is required. - The high performance of CentroidNet does not support the claim on the insufficient variety of the class semantics. According to ablation study, adopting Sinkhorn K-means is the most important factor to improve the performance. It means that adopting weighted average like in [R1] can also improve the performance of ProtoNet, which results in substantial difference in the performance between ProtoNet and CentroidNet that can deny the claim. - The definition of CSCC is not convincing. First, I could not get the meaning of \"unsupervised Bayes accuracy\" (supervised Bayes accuracy means 1 - Bayes error rate, right?). Second, CSCC seems to mainly quantify the importance of the supervision information during meta-learning, which is not directly related to the difficulty of few-shot learning problem. Intuitively, difficult few-shot learning problems should lead to lower supervised Bayes accuracy, which does not necessarily decrease CSCC. Third, what we can induce via comparing CSCC is not clarified in theory. The discussion in 6.3 is too subjective and specific for the case of training with ILSVRC/all datasets. - This paper lacks citing some closely related works [R1, R2]. [R1] \"Infinite Mixture Prototypes for Few-Shot Learning,\" ICML2019 [R2] \"A Closer Look at Few-shot Classification,\" ICLR2019 * Minor concerns that do not have an impact on the score - Another arXiv paper related to this work: \"Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML\" ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for their thorough review and raising several valid points . We have done our best to answer them and we will improve the main paper accordingly ( we have added some points to the appendix already ) . We hope that the reviewer will reconsider their score if we have addressed their concerns . * `` CentroidNets uses a variety of tricks to improve performance , therefore it is unfair to not also consider tricks ( such as R1 ) to improve Protonet performance '' [ Rephrased from Reviewer : \u201c The high performance of CentroidNet does not support the claim on the insufficient variety of the class semantics [ ... ] ] Indeed , Sinkhorn K-Means is a key component in the performance of CentroidNets . However , it is not obvious that using Sinkhorn K-Means would be an unfair advantage compared to Prototypical Networks , for two reasons : - In CentroidNets , we use Sinkhorn k-Means to attempt to recover the hidden class labels , i.e.to infer the ground-truth labels . In contrast , ProtoNets has direct access to the ground-truth labels ( which incidentally turn out to be hard assignments and lead to unweighted averages ) . - In CentroidNets , we run Sinkhorn k-Means on representations which were learned with the ProtoNet loss , i.e. , they were by construction designed to be averaged without weights . However , in order to best address the reviewer \u2019 s concern , we go further and run new experiments on miniImageNet . This time we constrain the centroids to be unweighted averages of the data points . To do so , starting from the soft weights , we reassign each point only to its closest centroid , and compute the unweighted averages . The comparison between ProtoNets and CentroidNets is now fair in the sense that both prototypes and centroids use unweighted averages . - Unsupervised Accuracy on miniImageNet is 0.5508 +/- 0.0072 for weighted average and 0.5497 +/- 0.0072 for unweighted average . The difference is not significant . - Clustering Accuracy on miniImageNet is 0.6421 +/- 0.0069 for weighted average and 0.6417 +/- 0.0069 for unweighted average . The difference is also not significant . We \u2019 ll be happy to add these results to the paper , if the review thinks them valuable . Therefore , the new experiment suggests that using weighted averages does not bring an unfair advantage , and therefore does not invalidate our comparison . More generally , instead of trying to tune ProtoNets and CentroidNets as well as possible , we try to use comparable models for ProtoNets and CentroidNets ( same architecture , nearly same representation ) . * What is unsupervised Bayes accuracy ? We define the unsupervised Bayes accuracy of an unsupervised few-shot classification task distribution as the highest achievable unsupervised accuracy . Just like the usual Bayes error is limited by label noise , the unsupervised Bayes accuracy is limited by cluster-semantic noise of a task . For illustration , consider the following unsupervised few-shot classification task distribution : - Uniformly sample a random dimension 1 < = j < = D ( hidden to the algorithm ) - Sample ( iid , probability=\u00bd ) random binary vectors ( x_i ) of dimension D ( shown to the algorithm ) and split them between support and query set . - Assign binary labels y = x_j to each vector ( x_i ) ( hidden to algorithm ) . - The goal is to cluster the support set and associate query set points with the support clusters . Because the algorithm does not know which dimension j was sampled ( i.e.the class semantic ) , it does not know how to cluster the support set . Therefore , it is just as good to make random predictions on the query set . Therefore the unsupervised Bayes accuracy is 0.5 Now , consider the same task distribution , except the dimension index j is always fixed to 1 . After meta-training , the algorithm can learn a representation mapping each vector to the value of its first dimension only . The support set can be clustered by grouping all 1s together , and all 0s together . Each query point can then be unambiguously assigned to one of the clusters . The resulting unsupervised Bayes accuracy is 1 . Both task distributions would become equivalent if the algorithm had access to the class semantics j . Therefore , the two unsupervised few-shot tasks differ in difficulty only because of the uncertainty/variability on class semantics , and this is reflected in the difference in unsupervised Bayes accuracy . If this example is deemed helpful , we \u2019 ll be happy to add it to the paper ."}}