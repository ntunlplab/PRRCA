{"year": "2020", "forum": "SJxSDxrKDr", "title": "Adversarial Training and Provable Defenses: Bridging the Gap", "decision": "Accept (Talk)", "meta_review": "The reviewers develop a novel technique for training neural networks that are provably robust to adversarial attacks, by combining provable defenses using convex relaxations with latent adversarial attacks that lie in the gap between the convex relaxation and the true realizable set of activations at a layer of the network. The authors show that the resulting procedure is computationally efficient and able to train neural networks to attain SOTA provable robustness to adversarial attacks.\n\nThe paper is well written and clearly explains an interesting idea, backed by thorough experiments. The reviewers were in consensus on acceptance and relatively minor concerns were clearly addressed in the rebuttal phase.\n\nHence, I strongly recommend acceptance.", "reviews": [{"review_id": "SJxSDxrKDr-0", "review_text": "Summary: the paper introduces a novel protocol for training neural networks that aims at leveraging the empirical benefits of adversarial training while allowing to certify the robustness of the network using the convex relation approach introduced by Wong & Kolter. The key ingredient is a novel algorithm for layer-wise adversarial (re-)training via convex relaxations. On CIFAR-10, the proposed protocol yields new state-of-the-art performance for certifying robustness against L_inf perturbations less than 2/255, and comparable performance over existing methods for perturbations less than 8/255 (where the comparison excludes randomized-smoothing based approaches as proposed by Cohen et al.). The proposed methodology seems original and novel. The concept of latent adversarial examples, the layer-wise provable optimization techniques and the sparse representation trick are interesting in their own regard and could be valuable ingredients for future work in this direction. The improvement over the state-of-the-art on CIFAR-10 for perturbations less than 2/255 is significant (although I wouldn't call it substantial). For perturbations less than 8/255 the picture is less clear. The authors' explanation that they couldn't achieve state-of-the-art certified robustness because of smaller network capacity makes sense, however, it also highlights that their protocol doesn't scale as well as previous approaches. I am not concerned about the missing comparison with randomized smoothing-based approaches (I find the rationale provided in Section 2 convincing). The discussion of the relatively weak performance of previous provable defenses on page 3 is a bit vague, e.g. the statement that \"the way these methods construct the loss makes the relationship between the loss and the network parameters significantly more complex than in standard training\", thus causing the \"resulting optimization problem to be more difficult\". To me, these are one and the same thing, and a bit more rigour in the argumentation would be advisable here, in my opinion. ------------- I acknowledge I have read the authors' response and also the other reviews/comments which confirm my opinion that this paper is worthy to be published at ICLR.", "rating": "8: Accept", "reply_text": "Thank you for your feedback . Below we answer the main concerns . Q : Could you discuss your claims about relatively weak performance of previous provable defenses more rigorously ? \u2192 Yes , we now clarify what exactly we meant there . Note that we offer only intuition and have no theoretical proof of the claim itself . When we say that the relationship between the loss function and parameters is more complex , this can be quantified in terms of the number of mathematical operations . For instance , propagating through a linear layer with N inputs and N outputs requires O ( N^2 ) operations for standard training , O ( N^2 ) operations for interval training and O ( N^3 ) for training with linear relaxations from prior work . Our next sentence refers to the difficulty of minimizing this loss function . Intuitively , while there are exceptions , we can expect that a loss which involves more mathematical operations to be more difficult to minimize . Q : The authors ' explanation that they could n't achieve state-of-the-art certified robustness because of smaller network capacity makes sense , however , it also highlights that their protocol does n't scale as well as previous approaches . \u2192 We have now managed to scale our approach to larger networks using the approach of Wong et al . ( 2018 ) to statistically estimate bounds during training . Please see our main points above for a more detailed description ."}, {"review_id": "SJxSDxrKDr-1", "review_text": " Summary: This paper provides a promising new general training methodology to obtain provably robust neural networks (towards adversarial input perturbations). The paper provides promising experimental results on CIFAR-10 by obtaining state-of-the-art certified accuracy while also simultaneously improving clean accuracy. The paper is overall well-written and the algorithm is clearly described. Important questions to be answered: I find the need to clarify my understanding and request for more information in order to make a decision. --Methodology/motivation for the method: I am trying to understand abstractly what the proposed layerwise training is trying to optimize. To be concrete, let's compare to the relaxation of Wong and Kolter (which this paper uses in the instantiation of layerwise adversarial training). What's the exact difference? One way to view this is the following: The same training objective, but a different way to optimize. The new proposal to train involves freezing weights until one layer iteratively starting from the input layer. It is possible that this kind of training provides some inductive bias in finding better solutions. Is this an appropriate understanding? However, the paper\u2019s experimental results unfortunately change the certification procedure. In other words, they haven\u2019t evaluated the same training objective as that of Wong and Kolter. Hence, it\u2019s not clear if the gains are from the better networks, or better certification method, or network being better suited for certification by the method used. The phrase \u201csame relaxation\u201d is not appropriately used. Their certification procedure uses a different (and tighter) relaxation. --Effect on latent adversarial examples: I am unable to understand why this training procedure would reduce the number of latent adversarial examples. The definition of latent adversarial examples seems to suggest that it\u2019s the gap between the actual set of activations corresponding to the input perturbations and the convex hull. However, the proposed layerwise adversarial training procedure involves replacing the actual set S_i with the convex hull C_i when freezing things till below i-1. I do not follow how the proposed method tries to make C_i = S_i. Implicitly the optimization objective does try to make C_i small because the bounds being optimized are tighter when C_i is small. But this is true even for normal certified training, and not sure what changes in the new training procedure. Specific experimental results that would help: --Certified accuracy on using the same LP based certification procedure used in Wong and Kolter with the new layerwise trained networks --The paper\u2019s own certification procedure (a combination of previous methods) on the network from Wong and Kolter or a note on why that doesn\u2019t apply (if it doesn\u2019t) --The paper currently provides only one data point to suggest this training method is superior. Would be good to try SVHN or MNIST. MNIST is perhaps \u201cessentially\u201d solved for small \\eps. But would be good to see if the training method offers gains at larger \\eps. In general, would be good to see more consistent gains. --The paper reports results on first 1000 examples of CIFAR10 test set. From my personal experience, there is a lot of variability in the robustness of test examples when evaluated on 1000 random test instances. Especially since the paper doesn't take a random subset, it might be good to make sure the gains are consistent on some other subset. The Wong et al. baseline is evaluated on the entire test set for example, and hence might not be a fair comparison? What's the Wong et al. accuracy on just the first 1000 test exampes Overall, I am leaning towards accept but need some conceptual and empirical clarification from the authors (detailed above). ", "rating": "6: Weak Accept", "reply_text": "Thank you for your feedback . Below we answer the main concerns . Q : Why do you change the certification procedure to one that is different from what was used in training ? Can you apply your certification procedure on the networks from Wong & Kolter ? \u2192 This is because during certification we can afford to use a more expensive certification method based on a complete verifier . We believe this approach , leveraging complete verifiers after training , is now standard and was already applied by Gowal et al.and Xiao et al.Other works have in fact already certified some of the networks from Wong et al.using complete verifiers . These results can be found in the works of Tjeng et al.and Salman et al.The certified robustness obtained using these verifiers is indeed slightly higher than the one in Wong et al.However , the best result of Wong et al. , presented in our Table 1 , is achieved with a residual network which is too large to apply our certification procedure . Q : I do not follow how the proposed method tries to make C_i = S_i . Could you elaborate the effect your training method has on latent adversarial examples ? \u2192 We will try to clarify this point here . We also fixed a typo in Line 11 of Algorithm 1 ( we freeze layer l + 1 instead of l ) which might have caused some misunderstanding . First , we would like to clarify the definition of latent adversarial examples which you characterized as \u201c the gap between the actual set of activations corresponding to the input perturbations and the convex hull \u201d . This is actually not correct . It is possible that for a network there is a ( large ) gap between the actual set and the convex relaxation and at the same time no latent adversarial example exists . This would happen if the network behaves correctly on the entire difference between the convex relaxation and the true region . Training networks to behave this way is precisely the aim of our method . At the i-th stage of training , layers 1 , 2 , \u2026 , i of the network are frozen . This means that both regions S_i and C_i are fixed ( which means our training is not trying to make S_i = C_i ) . Now , our training method performs two steps . First , it tries to find a latent adversarial example in the difference between C_i and S_i . Second , it updates the non-frozen parameters of the network so that the loss induced by the latent adversarial example is minimized . After enough such updates , this loss will decrease enough so that no latent adversarial example is remaining . Finally , as mentioned earlier in the main points , we remark that we found a regularizer similar to Xiao et al.useful to increase the performance . This regularizer is indeed trying to make C_i = S_i by inducing a loss on the volume of region C_i . However , our training itself has a different goal , as explained above . Q : The paper currently provides only one data point to suggest this training method is superior . Would be good to try SVHN or MNIST . MNIST is perhaps \u201c essentially \u201d solved for small \\eps . But would be good to see if the training method offers gains at larger \\eps . In general , would be good to see more consistent gains . \u2192 To further evaluate the performance of our method we evaluated our method on MNIST and SVHN datasets and included the results in Appendix C. Please see the main response for the summary . Q : Could there be significant variability in results due to the fact that only 1000 images from the test set were certified ? \u2192 To check the amount of variability , we certified another random subset of 1000 images , with little difference in the results . Please see the main response for the results of this experiment ."}, {"review_id": "SJxSDxrKDr-2", "review_text": "This paper was very clearly written and easy to follow. Kudos to the authors. In particular, the experimental evaluation section was exceptionally clear. Thanks to the authors for making the paper so easy to review. The \u201cMain Contributions\u201d section was excellent as well as it allows the reader to quickly understand what the paper is claiming. The introduction & related work section was very clear, and seemed to quickly get the reader up to speed. Minor critiques: - It\u2019s not clear to me that the network size is actually as impressive an improvement as is implied. Barring an extensive hyper-parameter search that demonstrated that this network architecture is the smallest possible that could achieve the presented results, I strongly suspect that applying techniques from papers like EfficientNet [1] or MobileNet would allow the authors of Mirman et. al (2018) to reduce the number of parameters required to achieve their results. I don\u2019t think this takes away from the paper, though- the results are strong despite that. I would encourage the authors to weaken the claims that the only better network is 5 times larger. - In general, I would have liked to see more evaluation- e.g. I would have liked to see more results with a variety of perturbations (2 through 8, not just 2 & 8), and on a variety of datasets. Questions to the authors: - How robust is the algorithm to architecture choice? - What happens if you change the test set? E.g. instead of evaluating on the first 1000 images, what if you evaluate on another random subset? Does that make a difference? I\u2019m concerned that the subset of the test set the authors are using for evaluation isn\u2019t representative of the entire test set. - Is the current architecture the largest network that can be run? I would be interested in seeing how network size affects the performance of your technique. - What hyper-parameter tuning did you do? What other network architectures did you try? - How do the comparison methods compare in terms of training time/machines used? E.g. do all the methods reported in Table 1 use similar amounts of computing power? Overall, this is a great paper, with some interesting results presented in a tight, clear manner. While I would like to see more experiments on larger datasets- e.g. ImageNet- the results seem solid and absolutely worthy of publication. [1]: https://arxiv.org/abs/1905.11946v2 [2]: https://arxiv.org/abs/1704.04861", "rating": "8: Accept", "reply_text": "Thank you for your feedback . Below we answer the main concerns . Q : It \u2019 s not clear to me that the network size is actually as impressive an improvement as is implied . I strongly suspect that applying techniques from papers like EfficientNet [ 1 ] or MobileNet would allow the authors of Mirman et . al ( 2018 ) to reduce the number of parameters required to achieve their results . I would encourage the authors to weaken the claims that the only better network is 5 times larger . \u2192 We agree that approaches from prior work could benefit from the techniques you mentioned to further reduce sizes of their networks . Our claim was referring only to the networks that were reported in the respective papers . However , as we have now also scaled our approach to larger networks , we modified the sentence and no longer claim that better networks are at least 5 times larger . Q : In general , I would have liked to see more evaluation- e.g.I would have liked to see more results with a variety of perturbations ( 2 through 8 , not just 2 & 8 ) , and on a variety of datasets . \u2192 To further evaluate the performance of our method we evaluated on MNIST and SVHN datasets and included results in Appendix C. Please see the main response in the summary . We chose perturbation values 2 and 8 because other values were not evaluated in prior work so we could not compare . Due to time constraints , we were unable to evaluate other perturbation values , however we will do so for the next version . Q : What hyper-parameter tuning did you do ? What other network architectures did you try ? \u2192 For training hyper-parameters we chose number of steps and step size of PGD to be the same as in Madry et al . ( 2018 ) .We also chose L_1 regularization factor the same as Xiao et al.We always used batch size 50 . The only two parameters we tuned were the epsilon used for training and the factor for ReLU stability regularization . As training is relatively costly , we experimented with a few different values and chose the one which minimizes adversarial loss in the final layer , on the training set . Q : Is the current architecture the largest network that can be run ? I would be interested in seeing how network size affects the performance of your technique . \u2192 As mentioned before , we managed to scale our approach to larger networks . Compared to the smaller network used at submission time , we improved the accuracy by 4 % and certified robustness by 2.2 % on CIFAR-10 with 2/255 perturbation . For the final revision we will evaluate the method on a wider range of architectures , however at the moment this was not possible due to the limited rebuttal time period . Q : How do the comparison methods compare in terms of training time/machines used ? E.g.do all the methods reported in Table 1 use similar amounts of computing power ? \u2192 Methods in Table 1 are very different in terms of computing power . While it is hard to directly compare them , Gowal et al.report their method takes 3.5 seconds per epoch and Wong et al.takes 2 minutes per epoch on the MNIST dataset . On MNIST , our method takes 2.5 minutes per epoch while training the first layer , 5 minutes per epoch for the second layer and 10 minutes per epoch for the third layer . Our CIFAR-10 networks take roughly 1 day to train on 1 GeForce RTX 2080 Ti GPU ."}], "0": {"review_id": "SJxSDxrKDr-0", "review_text": "Summary: the paper introduces a novel protocol for training neural networks that aims at leveraging the empirical benefits of adversarial training while allowing to certify the robustness of the network using the convex relation approach introduced by Wong & Kolter. The key ingredient is a novel algorithm for layer-wise adversarial (re-)training via convex relaxations. On CIFAR-10, the proposed protocol yields new state-of-the-art performance for certifying robustness against L_inf perturbations less than 2/255, and comparable performance over existing methods for perturbations less than 8/255 (where the comparison excludes randomized-smoothing based approaches as proposed by Cohen et al.). The proposed methodology seems original and novel. The concept of latent adversarial examples, the layer-wise provable optimization techniques and the sparse representation trick are interesting in their own regard and could be valuable ingredients for future work in this direction. The improvement over the state-of-the-art on CIFAR-10 for perturbations less than 2/255 is significant (although I wouldn't call it substantial). For perturbations less than 8/255 the picture is less clear. The authors' explanation that they couldn't achieve state-of-the-art certified robustness because of smaller network capacity makes sense, however, it also highlights that their protocol doesn't scale as well as previous approaches. I am not concerned about the missing comparison with randomized smoothing-based approaches (I find the rationale provided in Section 2 convincing). The discussion of the relatively weak performance of previous provable defenses on page 3 is a bit vague, e.g. the statement that \"the way these methods construct the loss makes the relationship between the loss and the network parameters significantly more complex than in standard training\", thus causing the \"resulting optimization problem to be more difficult\". To me, these are one and the same thing, and a bit more rigour in the argumentation would be advisable here, in my opinion. ------------- I acknowledge I have read the authors' response and also the other reviews/comments which confirm my opinion that this paper is worthy to be published at ICLR.", "rating": "8: Accept", "reply_text": "Thank you for your feedback . Below we answer the main concerns . Q : Could you discuss your claims about relatively weak performance of previous provable defenses more rigorously ? \u2192 Yes , we now clarify what exactly we meant there . Note that we offer only intuition and have no theoretical proof of the claim itself . When we say that the relationship between the loss function and parameters is more complex , this can be quantified in terms of the number of mathematical operations . For instance , propagating through a linear layer with N inputs and N outputs requires O ( N^2 ) operations for standard training , O ( N^2 ) operations for interval training and O ( N^3 ) for training with linear relaxations from prior work . Our next sentence refers to the difficulty of minimizing this loss function . Intuitively , while there are exceptions , we can expect that a loss which involves more mathematical operations to be more difficult to minimize . Q : The authors ' explanation that they could n't achieve state-of-the-art certified robustness because of smaller network capacity makes sense , however , it also highlights that their protocol does n't scale as well as previous approaches . \u2192 We have now managed to scale our approach to larger networks using the approach of Wong et al . ( 2018 ) to statistically estimate bounds during training . Please see our main points above for a more detailed description ."}, "1": {"review_id": "SJxSDxrKDr-1", "review_text": " Summary: This paper provides a promising new general training methodology to obtain provably robust neural networks (towards adversarial input perturbations). The paper provides promising experimental results on CIFAR-10 by obtaining state-of-the-art certified accuracy while also simultaneously improving clean accuracy. The paper is overall well-written and the algorithm is clearly described. Important questions to be answered: I find the need to clarify my understanding and request for more information in order to make a decision. --Methodology/motivation for the method: I am trying to understand abstractly what the proposed layerwise training is trying to optimize. To be concrete, let's compare to the relaxation of Wong and Kolter (which this paper uses in the instantiation of layerwise adversarial training). What's the exact difference? One way to view this is the following: The same training objective, but a different way to optimize. The new proposal to train involves freezing weights until one layer iteratively starting from the input layer. It is possible that this kind of training provides some inductive bias in finding better solutions. Is this an appropriate understanding? However, the paper\u2019s experimental results unfortunately change the certification procedure. In other words, they haven\u2019t evaluated the same training objective as that of Wong and Kolter. Hence, it\u2019s not clear if the gains are from the better networks, or better certification method, or network being better suited for certification by the method used. The phrase \u201csame relaxation\u201d is not appropriately used. Their certification procedure uses a different (and tighter) relaxation. --Effect on latent adversarial examples: I am unable to understand why this training procedure would reduce the number of latent adversarial examples. The definition of latent adversarial examples seems to suggest that it\u2019s the gap between the actual set of activations corresponding to the input perturbations and the convex hull. However, the proposed layerwise adversarial training procedure involves replacing the actual set S_i with the convex hull C_i when freezing things till below i-1. I do not follow how the proposed method tries to make C_i = S_i. Implicitly the optimization objective does try to make C_i small because the bounds being optimized are tighter when C_i is small. But this is true even for normal certified training, and not sure what changes in the new training procedure. Specific experimental results that would help: --Certified accuracy on using the same LP based certification procedure used in Wong and Kolter with the new layerwise trained networks --The paper\u2019s own certification procedure (a combination of previous methods) on the network from Wong and Kolter or a note on why that doesn\u2019t apply (if it doesn\u2019t) --The paper currently provides only one data point to suggest this training method is superior. Would be good to try SVHN or MNIST. MNIST is perhaps \u201cessentially\u201d solved for small \\eps. But would be good to see if the training method offers gains at larger \\eps. In general, would be good to see more consistent gains. --The paper reports results on first 1000 examples of CIFAR10 test set. From my personal experience, there is a lot of variability in the robustness of test examples when evaluated on 1000 random test instances. Especially since the paper doesn't take a random subset, it might be good to make sure the gains are consistent on some other subset. The Wong et al. baseline is evaluated on the entire test set for example, and hence might not be a fair comparison? What's the Wong et al. accuracy on just the first 1000 test exampes Overall, I am leaning towards accept but need some conceptual and empirical clarification from the authors (detailed above). ", "rating": "6: Weak Accept", "reply_text": "Thank you for your feedback . Below we answer the main concerns . Q : Why do you change the certification procedure to one that is different from what was used in training ? Can you apply your certification procedure on the networks from Wong & Kolter ? \u2192 This is because during certification we can afford to use a more expensive certification method based on a complete verifier . We believe this approach , leveraging complete verifiers after training , is now standard and was already applied by Gowal et al.and Xiao et al.Other works have in fact already certified some of the networks from Wong et al.using complete verifiers . These results can be found in the works of Tjeng et al.and Salman et al.The certified robustness obtained using these verifiers is indeed slightly higher than the one in Wong et al.However , the best result of Wong et al. , presented in our Table 1 , is achieved with a residual network which is too large to apply our certification procedure . Q : I do not follow how the proposed method tries to make C_i = S_i . Could you elaborate the effect your training method has on latent adversarial examples ? \u2192 We will try to clarify this point here . We also fixed a typo in Line 11 of Algorithm 1 ( we freeze layer l + 1 instead of l ) which might have caused some misunderstanding . First , we would like to clarify the definition of latent adversarial examples which you characterized as \u201c the gap between the actual set of activations corresponding to the input perturbations and the convex hull \u201d . This is actually not correct . It is possible that for a network there is a ( large ) gap between the actual set and the convex relaxation and at the same time no latent adversarial example exists . This would happen if the network behaves correctly on the entire difference between the convex relaxation and the true region . Training networks to behave this way is precisely the aim of our method . At the i-th stage of training , layers 1 , 2 , \u2026 , i of the network are frozen . This means that both regions S_i and C_i are fixed ( which means our training is not trying to make S_i = C_i ) . Now , our training method performs two steps . First , it tries to find a latent adversarial example in the difference between C_i and S_i . Second , it updates the non-frozen parameters of the network so that the loss induced by the latent adversarial example is minimized . After enough such updates , this loss will decrease enough so that no latent adversarial example is remaining . Finally , as mentioned earlier in the main points , we remark that we found a regularizer similar to Xiao et al.useful to increase the performance . This regularizer is indeed trying to make C_i = S_i by inducing a loss on the volume of region C_i . However , our training itself has a different goal , as explained above . Q : The paper currently provides only one data point to suggest this training method is superior . Would be good to try SVHN or MNIST . MNIST is perhaps \u201c essentially \u201d solved for small \\eps . But would be good to see if the training method offers gains at larger \\eps . In general , would be good to see more consistent gains . \u2192 To further evaluate the performance of our method we evaluated our method on MNIST and SVHN datasets and included the results in Appendix C. Please see the main response for the summary . Q : Could there be significant variability in results due to the fact that only 1000 images from the test set were certified ? \u2192 To check the amount of variability , we certified another random subset of 1000 images , with little difference in the results . Please see the main response for the results of this experiment ."}, "2": {"review_id": "SJxSDxrKDr-2", "review_text": "This paper was very clearly written and easy to follow. Kudos to the authors. In particular, the experimental evaluation section was exceptionally clear. Thanks to the authors for making the paper so easy to review. The \u201cMain Contributions\u201d section was excellent as well as it allows the reader to quickly understand what the paper is claiming. The introduction & related work section was very clear, and seemed to quickly get the reader up to speed. Minor critiques: - It\u2019s not clear to me that the network size is actually as impressive an improvement as is implied. Barring an extensive hyper-parameter search that demonstrated that this network architecture is the smallest possible that could achieve the presented results, I strongly suspect that applying techniques from papers like EfficientNet [1] or MobileNet would allow the authors of Mirman et. al (2018) to reduce the number of parameters required to achieve their results. I don\u2019t think this takes away from the paper, though- the results are strong despite that. I would encourage the authors to weaken the claims that the only better network is 5 times larger. - In general, I would have liked to see more evaluation- e.g. I would have liked to see more results with a variety of perturbations (2 through 8, not just 2 & 8), and on a variety of datasets. Questions to the authors: - How robust is the algorithm to architecture choice? - What happens if you change the test set? E.g. instead of evaluating on the first 1000 images, what if you evaluate on another random subset? Does that make a difference? I\u2019m concerned that the subset of the test set the authors are using for evaluation isn\u2019t representative of the entire test set. - Is the current architecture the largest network that can be run? I would be interested in seeing how network size affects the performance of your technique. - What hyper-parameter tuning did you do? What other network architectures did you try? - How do the comparison methods compare in terms of training time/machines used? E.g. do all the methods reported in Table 1 use similar amounts of computing power? Overall, this is a great paper, with some interesting results presented in a tight, clear manner. While I would like to see more experiments on larger datasets- e.g. ImageNet- the results seem solid and absolutely worthy of publication. [1]: https://arxiv.org/abs/1905.11946v2 [2]: https://arxiv.org/abs/1704.04861", "rating": "8: Accept", "reply_text": "Thank you for your feedback . Below we answer the main concerns . Q : It \u2019 s not clear to me that the network size is actually as impressive an improvement as is implied . I strongly suspect that applying techniques from papers like EfficientNet [ 1 ] or MobileNet would allow the authors of Mirman et . al ( 2018 ) to reduce the number of parameters required to achieve their results . I would encourage the authors to weaken the claims that the only better network is 5 times larger . \u2192 We agree that approaches from prior work could benefit from the techniques you mentioned to further reduce sizes of their networks . Our claim was referring only to the networks that were reported in the respective papers . However , as we have now also scaled our approach to larger networks , we modified the sentence and no longer claim that better networks are at least 5 times larger . Q : In general , I would have liked to see more evaluation- e.g.I would have liked to see more results with a variety of perturbations ( 2 through 8 , not just 2 & 8 ) , and on a variety of datasets . \u2192 To further evaluate the performance of our method we evaluated on MNIST and SVHN datasets and included results in Appendix C. Please see the main response in the summary . We chose perturbation values 2 and 8 because other values were not evaluated in prior work so we could not compare . Due to time constraints , we were unable to evaluate other perturbation values , however we will do so for the next version . Q : What hyper-parameter tuning did you do ? What other network architectures did you try ? \u2192 For training hyper-parameters we chose number of steps and step size of PGD to be the same as in Madry et al . ( 2018 ) .We also chose L_1 regularization factor the same as Xiao et al.We always used batch size 50 . The only two parameters we tuned were the epsilon used for training and the factor for ReLU stability regularization . As training is relatively costly , we experimented with a few different values and chose the one which minimizes adversarial loss in the final layer , on the training set . Q : Is the current architecture the largest network that can be run ? I would be interested in seeing how network size affects the performance of your technique . \u2192 As mentioned before , we managed to scale our approach to larger networks . Compared to the smaller network used at submission time , we improved the accuracy by 4 % and certified robustness by 2.2 % on CIFAR-10 with 2/255 perturbation . For the final revision we will evaluate the method on a wider range of architectures , however at the moment this was not possible due to the limited rebuttal time period . Q : How do the comparison methods compare in terms of training time/machines used ? E.g.do all the methods reported in Table 1 use similar amounts of computing power ? \u2192 Methods in Table 1 are very different in terms of computing power . While it is hard to directly compare them , Gowal et al.report their method takes 3.5 seconds per epoch and Wong et al.takes 2 minutes per epoch on the MNIST dataset . On MNIST , our method takes 2.5 minutes per epoch while training the first layer , 5 minutes per epoch for the second layer and 10 minutes per epoch for the third layer . Our CIFAR-10 networks take roughly 1 day to train on 1 GeForce RTX 2080 Ti GPU ."}}