{"year": "2021", "forum": "9QLRCVysdlO", "title": "BiPointNet: Binary Neural Network for Point Clouds", "decision": "Accept (Poster)", "meta_review": "The authors propose techniques to deal with binarization of 3D point clouds and propose EMA and layer wise scale recovery that improve results across the board for PointNet style models.\nAn accept.", "reviews": [{"review_id": "9QLRCVysdlO-0", "review_text": "This paper proposes a method to apply binary networks on point clouds . To my knowledge this is the first time that this is attempted so unquestionably the topic of the paper is interesting . From what the authors show a vanilla BNN ( XNOR-Net ) applied to point clouds does not give very good results and for this reason the authors identify solutions that boil down to applying a shift and a scaling . This is really my main problem with the paper : the proposed methods are too simple and the accompanying theory does not look to be so convincing in order to theoretically support the contributions which are just a simple shift and scaling . Actually the authors show that one of the variants of their method can be reduced to average pooling which does not require some sophisticated explanation to convince the reader why it works . Moreover the proposed learnable layer-wise scaling factor is not new and was previously usedat a ) layer-level ( Towards Accurate Binary Convolutional Neural Network , Lin et al , NeurIPS \u2019 17 ) and b ) channel-level ( XNOR-Net++ : Improved Binary Neural Networks , Bulat & Tzimiropoulos , BMVC \u2019 19 ) . In fact , the problem itself is known since at least 2016 , where in the ( XNOR-Net : ImageNet Classification Using Binary Convolutional Neural Networks , Rastegari etal , ECCV \u2019 16 ) identifies this problem and proposes an analytically-computed scaling factor . Other issues : \u201c even global pooling provides strong recognition performance . However , this practice poses challenges for binarization \u201d \u2013 there is no justification provided of why pooling may pose issues for binarization . Avg and max-pooling is used with success in contemporary binary networks . Existing BNNs \u201c are not readily transferable to point clouds. \u201d \u2013 In the paper it is mentioned that this is shown and evaluated in the method section . However , many of the listed methods are not in fact evaluated . This is especially important for methods that also learn to recover the scaling factors . Given that for image classification , at least for ResNet the last layer before the linear classifier is a global pooling operation , how does the proposed EMA changes the results when applied to image classification , on a ResNet18 on Imagenet ? Are there any improvements measurable in that case too ? \u201c Despite that model binarization has been studied extensively in 2D vision tasks ( Krizhevsky et al.,2012 ; Simonyan & Zisserman , 2014 ; Szegedy et al. , 2015 ; Girshick et al. , 2014 ; Girshick , 2015 ; Russakovsky et al. , 2015 ; Wang et al. , 2019b ) '' \u2013 the cited works don \u2019 t support the author statement , since none them perform binarization . $ \\textbf { Final Rating } $ Based on the authors ' responses during the rebuttal period , I do n't believe that the paper makes a sufficient contribution for ICLR . Hence I will stick to my original score .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the feedback and comments . We respond to the concerns below : * * Q1 * * : The proposed methods are too simple and the accompanying theory is not convincing . One of the variants can be reduced to average pooling . * * * * A1 * * : Regarding \u201c too simple \u201d : We regard the simplicity of our approach as a \u201c merit \u201d rather than a disadvantage , which makes our approach easily deployable in practice . Reviewer 2 , 3 , and 4 have provided positive feedback on the significance and plausibility of our method . In fact , to pursue \u201c extreme compression and acceleration \u201d , simplicity is critical for the performance of the binarized model . To this end , we carefully design our techniques to be efficient and lightweight : mean shifting and scaling lead to minimum computational overhead and additional storage . We also highlight that the LSR uses only one parameter per layer , further reduces model storage and enhances fast computation . Regarding our theory : PointNet is an architecture that is fundamentally different from Convolutional Neural Networks used in 2D images . We have shown in the paper that 2D binarization methods are NOT readily transferable to 3D ( Sec 3 ) . Therefore , a new theory is needed to instruct the binarization of PointNet . Our EMA theory plays this role . Instructed by our theory , we have invented EMA-max , which outperforms existing baselines and one of our own variants , EMA-avg ( average pooling ) , as shown in Table 1 and 2 . This is aligned with the fact that max pooling outperforms average pooling in the full precision PointNet . For LSR , we prove in Theorem 2 that the number of feature channels causes a scale distortion for 3D . Through detailed evaluation , we identify two detriments of the scale distortion : first , scale sensitive structures are invalidated ( Figure 4 indicates without scale recovery , the transformed point cloud has a large scaling error ) ; second , optimization is hindered ( Figure 3c shows without scale recovery , the majority of the gradient is truncated ) . Therefore , we not only provide complete theoretical proofs , but supplement them with empirical evidence and in-depth analyses . * * Q2 * * : The proposed learnable layer-wise scaling factor is not new and was previously used in existing works . * * A2 * * : The 3D point clouds exhibit a fundamentally different data structure compared to 2D images . Hence , the problems associated with model binarization in these two domains are not the same : we identify that the * scale distortion * imposes a prominent impact on binarized PointNet ( Section 3.3 ) whereas existing methods aim to address the * quantization error * problem in CNNs used for 2D vision tasks . We design LSR to directly tackle the scale distortion whereas existing binarization methods for CNNs are less effective due to the misalignment of the optimization target and the problem . Specifically , the layerwise scaling factor in our LSR is initialized with the ratio of standard deviation statistics between the output features of FP32 and binary networks ( $ \\alpha_0 = \\frac { \\sigma ( \\mathbf A\\otimes \\mathbf W ) } { \\sigma ( \\mathbf { B_a } \\odot \\mathbf { B_w } ) } $ ) , which aims to recover the layerwise scale of the output features of binarized layers to that of FP32 layers . In contrast , the optimization target of XNOR-Net is minimizing the absolute ( quantization ) error ( $ \\mathop { \\arg\\min } \\limits_ { \\alpha , \\textbf { B } } ||\\mathbf { W } -\\alpha \\mathbf { B } ||^ { 2 } $ ) , which aims to find the optimal approximation of FP32 output features . Moreover , recent methods with scaling factors such as Bi-Real Net and IR-Net , just use one set of factors to approximate the weights only for efficient inference . They do not provide means to adapt to the scale distortion of the input activations . In addition to XNOR-Net , IR-Net , Bi-Real Net that we have already evaluated in Table 2 , we have followed the reviewer \u2019 s recommendation to include ABC-Net and XNOR-Net++ , which apply analytically-computed layerwise scaling factor and learnable scaling factor respectively , in our experiments ( see results below ) . The results are supportive of our theory . First , XNOR-Net outperforms more recent methods such as Bi-Real Net and IR-Net , but these recent methods perform better than XNOR-Net in 2D vision tasks . This discrepancy highlights that there are different challenges of model binarization in CNNs for 2D vision and PointNet for 3D vision . Second , despite that XNOR-Net minimizes absolute quantization error of both the input activations and weight parameters , LSR is able to achieve the best accuracy using only one parameter per layer , demonstrating that LSR is more effective than existing methods in tackling the scale distortion problem that is critical for 3D point clouds . Note that the binarized models without our EMA do not converge in the training process ( results are as low as 4.1 % ) , which shows that the EMA is necessary for binarized PointNet ."}, {"review_id": "9QLRCVysdlO-1", "review_text": "The paper proposes a binarization approach for efficient deep learning on point clouds , called BiPointNet . The authors claim that the immense performance drop of binarized models is caused by the aggregation-induced feature homogenization and scale distortion . The authors propose Entropy-Maximizing Aggregation ( EMA ) and Layer-wise Scale Recovery ( LSR ) to reduce the side-effects of binarization . Experiment results demonstrate that the proposed BIPointNet is able to achieve state-of-the-art results and gives an impressive speedup and storage saving . Besides the major contribution of the paper , the writing of the paper is concise and the illustrations are clear . However , the paper mainly focuses on PointNet-kind of structure , such as PointNet++ , and DGCNN , etc . It would be better if the authors could give more discussion on the application of the EMA and LSR on more advanced methods , such as KPConv and PointConv , etc .", "rating": "7: Good paper, accept", "reply_text": "We would like to first express our deep appreciation for your insightful comments . Our response to your suggestion can be found below : * * Q1 * * : Give more discussion on the application of the EMA and LSR on more advanced methods . * * A1 * * : Our paper evaluates the proposed method on three representative network architectures ( PointNet for Pointwise MLP Networks , PointCNN for Convolution-based Networks , DGCNN for Graph-based Networks [ 1 ] ) ; We agree that PointConv and KPConv are very important work , the computationally intensive operations in these architectures can be significantly accelerated by the binarization module with our LSR ( such as the MLP layers in PointConv ) , and the EMA can be applied to aggregators ( such as the max-pooling in KPConv ) to avoid feature homogenization . We further add more discussion in the revised version of our paper . In fact , we have implemented our BiPointNet on PointConv : our method achieves good results , outperforming XNOR-Net by 4.8 % with much fewer parameters . We are in the process of implementing our methods on more architectures and try our best to add these results in the revised version . |Base Model & nbsp ; & nbsp ; & nbsp ; & nbsp ; |Method & nbsp ; & nbsp ; & nbsp ; & nbsp ; |Bit-width & nbsp ; & nbsp ; & nbsp ; & nbsp ; |OA| |-|| -- | -- | | PointConv| FP32| 32/32| 90.8 % | | | XNOR-Net & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | 1/1 | 83.1 % | | | Ours | 1/1 | 87.9 % | [ 1 ] Guo et . al. , Deep Learning for 3D Point Clouds : A Survey , TPAMI 2020"}, {"review_id": "9QLRCVysdlO-2", "review_text": "This paper proposes a method for learning binary neural networks on point cloud inputs . They provide an entropy analysis of the binarized distributions as well as an offset transform to maximize entropy . Then they analyze the scale of binary activations and propose a learnable scaling to reduce the effects of scale distortion . They show that their method is competitive with other binary neural network methods and even full precision methods . Finally they show performant speed and storage results on a Raspberry Pi . Strengths : - Strong empirical results backed by theoretical analysis - Experiments are comprehensive and show competitive results on accuracy and speed Concerns : - If speed vs accuracy is the main trade-off , I would like to see a more thorough evaluation of all the models and baselines on a speed vs accuracy tradeoff plot Given the strong empirical and theoretical results , I would recommend an accept . I would still like to see the authors strengthen their paper with a more detailed speed/accuracy trade off .", "rating": "7: Good paper, accept", "reply_text": "We would like to first express our deep appreciation for your insightful comments . Our response to your suggestion can be found below : * * Q1 * * : More thorough evaluation of all the models and baselines on a speed vs accuracy tradeoff plot . * * A1 * * : We agree that a speed vs accuracy tradeoff plot is helpful . We thus implement and evaluate more quantization methods based on PointNet architecture on ARM devices , and complete a speed vs accuracy trade-off scatter plot . Since we can not add this plot in our response , we present our accuracy vs speed results on ARM devices in the table below : | CPU & nbsp ; & nbsp ; | Method & nbsp ; & nbsp ; & nbsp ; & nbsp ; | Acc . ( % ) & nbsp ; & nbsp ; & nbsp ; & nbsp ; | Time cost ( ms ) | | -- ||-|-| | A72 | FP32 | 88.2 | 67.3 | | | BNN | 16.2 | 5.5 | | | IR-Net | 63.5 | 5.5 | | | Bi-Real Net & nbsp ; & nbsp ; & nbsp ; & nbsp ; | 77.5 | 5.5 | | | ABC-Net | 77.8 | 9.2 | | | XNOR-Net | 81.9 | 9.7 | | | Ours | 86.4 | 5.5 | | A53 | FP32 | 88.2 | 131.8 | | | BNN | 16.2 | 9.0 | | | IR-Net | 63.5 | 9.0 | | | Bi-Real Net | 77.5 | 9.0 | | | ABC-Net | 77.8 | 15.6 | | | XNOR-Net | 81.9 | 15.7 | | | Ours | 86.4 | 9.0 | The results show that our BiPointNet outperforms others in both speed and accuracy , and the speed of our BiPointNet is much faster than the FP32 model with only a small drop of accuracy . And the speed vs accuracy trade-off plot will be added in the revised version . Note that through optimization of implementation on ARM devices , the models with fixed scaling factor ( IR-Net , Bi-Real Net , and ours ) can be as fast as the BNN baseline with no scaling factors . However , models that use dynamically computed scaling factors ( XNOR-Net and ABC-Net ) suffer extra computational burdens . Besides , more complicated base models , such as PointCNN , DGCNN and PointConv , contain operations that are difficult to implement and optimize on ARM devices ( such as KNN and FPS ) , especially within the short response time . Nevertheless , we are in the progress of implementing our method on more architectures for 3D point clouds and make them easy to deploy on resource-limited devices ."}, {"review_id": "9QLRCVysdlO-3", "review_text": "This paper proposes a method for binarization of neural networks of 3d point clouds . Two modules of entropy maximum aggregation and layer-wise scale recovery are proposed to conquer the problems of discrimination loss induced by feature homogenization and scale imbalance , which are caused by model binarization . The authors provide theoretical analysis about the proposed method . Experiments on various backbones and tasks demonstrate the effectiveness of the proposed method . A practical implantation of BiPointNet on ARM also demonstrates significant speedups over PointNet and large memory savings . Strength : 1 . Binarization of CNN models designed for 2D images has been studied in the past years , this paper extends this problem into 3D point cloud models . The authors show that the existing methods for binarization of 2D CNN models can not work well on this new problem . 2.For this new problem , the authors analysis its performance degradation based on PointNet and proposed effective solutions . 3.Experiments on several tasks show that the proposed method can obtain highly compact models with acceptable accuracy degradation . Experiments on other backbones also show that the proposed method is general , although its analysis is based on PointNet . For the weakness , I only have some minor comments . 1.The discussion on related work could be enlarged . For example , the following papers are well known point cloud networks proposed recently ( a ) PointConv : Deep Convolutional Networks on 3D Point Clouds . CVPR 2019 ( b ) Relation-Shape Convolutional Neural Network for Point Cloud Analysis . CVPR 2019 ( c ) ShellNet : Efficient Point Cloud Convolutional Neural Networks using Concentric Shells Statistics . ICCV 2019 Mixed precision quantization is also an active direction after binarization of neural networks , it could also be mentioned as a possible improvement in the future . ( d ) Mixed Precision Quantization of Convnets via Differentiable Neural Architecture Search . ICLR 2019 ( e ) Search What You Want : Barrier Panelty NAS for Mixed Precision Quantization . ECCV 2020 . 2.Some references miss publication type , i.e. , conference or journal and where they are published . 3.Figure 1 can be improved . It is unclear how LSR works in the whole framework .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We are deeply grateful for the reviewer \u2019 s support of our work and we thank the reviewer for the constructive and helpful suggestions . We provide additional discussions below : * * Q1 * * : The discussion on related work could be enlarged . * * A1 * * : We will add more discussions on the latest works on point clouds , including those mentioned by the reviewer . Our paper evaluates the proposed method on three representative network architectures ( PointNet for Pointwise MLP Networks , PointCNN for Convolution-based Networks , DGCNN for Graph-based Networks [ 1 ] ) ; we agree that PointConv , RS-CNN , and ShellNet are also important works to be discussed in the revised version of our paper . Moreover , despite the limited response time , we have implemented various binarization methods based on PointConv , and our method also achieves outstanding results : ours outperform XNOR-Net by 4.8 % with much fewer parameters . We are working on implementing our method on more base models . |Base Model & nbsp ; & nbsp ; & nbsp ; & nbsp ; |Method & nbsp ; & nbsp ; & nbsp ; & nbsp ; |Bit-width & nbsp ; & nbsp ; & nbsp ; & nbsp ; |OA| |-|| -- | -- | | PointConv| FP32| 32/32| 90.8 % | | | XNOR-Net & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | 1/1 | 83.1 % | | | Ours | 1/1 | 87.9 % | We will also add more discussion on mixed precision quantization in the final version . Mixed precision quantization is also a popular approach to network compression and acceleration . Unlike binarization that pursues extreme compression and acceleration , mixed-precision quantization achieves a balance between speed and accuracy by adjusting the quantization accuracy of different layers . We are also interested to explore mixed precision quantization for point cloud model acceleration in our future work . [ 1 ] Guo et . al. , Deep Learning for 3D Point Clouds : A Survey , TPAMI 2020 * * Q2 * * : Some references miss publication type . * * A2 * * : We will carefully correct our references in the revised version . * * Q3 * * : Figure 1 can be improved . * * A3 * * : We will improve Figure 1 and add related explanations in the revised version . The proposed LSR is applied to the bi-linear layers ( which form the BiMLPs in Figure 1 ) in our BiPointNet , the learnable layerwise scaling factors are applied to recover scales of the features obtained by XNOR-Bitcount operation ( $ \\mathbf Z = \\alpha ( \\mathbf { B_a } \\odot \\mathbf { B_w } ) $ ) ."}], "0": {"review_id": "9QLRCVysdlO-0", "review_text": "This paper proposes a method to apply binary networks on point clouds . To my knowledge this is the first time that this is attempted so unquestionably the topic of the paper is interesting . From what the authors show a vanilla BNN ( XNOR-Net ) applied to point clouds does not give very good results and for this reason the authors identify solutions that boil down to applying a shift and a scaling . This is really my main problem with the paper : the proposed methods are too simple and the accompanying theory does not look to be so convincing in order to theoretically support the contributions which are just a simple shift and scaling . Actually the authors show that one of the variants of their method can be reduced to average pooling which does not require some sophisticated explanation to convince the reader why it works . Moreover the proposed learnable layer-wise scaling factor is not new and was previously usedat a ) layer-level ( Towards Accurate Binary Convolutional Neural Network , Lin et al , NeurIPS \u2019 17 ) and b ) channel-level ( XNOR-Net++ : Improved Binary Neural Networks , Bulat & Tzimiropoulos , BMVC \u2019 19 ) . In fact , the problem itself is known since at least 2016 , where in the ( XNOR-Net : ImageNet Classification Using Binary Convolutional Neural Networks , Rastegari etal , ECCV \u2019 16 ) identifies this problem and proposes an analytically-computed scaling factor . Other issues : \u201c even global pooling provides strong recognition performance . However , this practice poses challenges for binarization \u201d \u2013 there is no justification provided of why pooling may pose issues for binarization . Avg and max-pooling is used with success in contemporary binary networks . Existing BNNs \u201c are not readily transferable to point clouds. \u201d \u2013 In the paper it is mentioned that this is shown and evaluated in the method section . However , many of the listed methods are not in fact evaluated . This is especially important for methods that also learn to recover the scaling factors . Given that for image classification , at least for ResNet the last layer before the linear classifier is a global pooling operation , how does the proposed EMA changes the results when applied to image classification , on a ResNet18 on Imagenet ? Are there any improvements measurable in that case too ? \u201c Despite that model binarization has been studied extensively in 2D vision tasks ( Krizhevsky et al.,2012 ; Simonyan & Zisserman , 2014 ; Szegedy et al. , 2015 ; Girshick et al. , 2014 ; Girshick , 2015 ; Russakovsky et al. , 2015 ; Wang et al. , 2019b ) '' \u2013 the cited works don \u2019 t support the author statement , since none them perform binarization . $ \\textbf { Final Rating } $ Based on the authors ' responses during the rebuttal period , I do n't believe that the paper makes a sufficient contribution for ICLR . Hence I will stick to my original score .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the feedback and comments . We respond to the concerns below : * * Q1 * * : The proposed methods are too simple and the accompanying theory is not convincing . One of the variants can be reduced to average pooling . * * * * A1 * * : Regarding \u201c too simple \u201d : We regard the simplicity of our approach as a \u201c merit \u201d rather than a disadvantage , which makes our approach easily deployable in practice . Reviewer 2 , 3 , and 4 have provided positive feedback on the significance and plausibility of our method . In fact , to pursue \u201c extreme compression and acceleration \u201d , simplicity is critical for the performance of the binarized model . To this end , we carefully design our techniques to be efficient and lightweight : mean shifting and scaling lead to minimum computational overhead and additional storage . We also highlight that the LSR uses only one parameter per layer , further reduces model storage and enhances fast computation . Regarding our theory : PointNet is an architecture that is fundamentally different from Convolutional Neural Networks used in 2D images . We have shown in the paper that 2D binarization methods are NOT readily transferable to 3D ( Sec 3 ) . Therefore , a new theory is needed to instruct the binarization of PointNet . Our EMA theory plays this role . Instructed by our theory , we have invented EMA-max , which outperforms existing baselines and one of our own variants , EMA-avg ( average pooling ) , as shown in Table 1 and 2 . This is aligned with the fact that max pooling outperforms average pooling in the full precision PointNet . For LSR , we prove in Theorem 2 that the number of feature channels causes a scale distortion for 3D . Through detailed evaluation , we identify two detriments of the scale distortion : first , scale sensitive structures are invalidated ( Figure 4 indicates without scale recovery , the transformed point cloud has a large scaling error ) ; second , optimization is hindered ( Figure 3c shows without scale recovery , the majority of the gradient is truncated ) . Therefore , we not only provide complete theoretical proofs , but supplement them with empirical evidence and in-depth analyses . * * Q2 * * : The proposed learnable layer-wise scaling factor is not new and was previously used in existing works . * * A2 * * : The 3D point clouds exhibit a fundamentally different data structure compared to 2D images . Hence , the problems associated with model binarization in these two domains are not the same : we identify that the * scale distortion * imposes a prominent impact on binarized PointNet ( Section 3.3 ) whereas existing methods aim to address the * quantization error * problem in CNNs used for 2D vision tasks . We design LSR to directly tackle the scale distortion whereas existing binarization methods for CNNs are less effective due to the misalignment of the optimization target and the problem . Specifically , the layerwise scaling factor in our LSR is initialized with the ratio of standard deviation statistics between the output features of FP32 and binary networks ( $ \\alpha_0 = \\frac { \\sigma ( \\mathbf A\\otimes \\mathbf W ) } { \\sigma ( \\mathbf { B_a } \\odot \\mathbf { B_w } ) } $ ) , which aims to recover the layerwise scale of the output features of binarized layers to that of FP32 layers . In contrast , the optimization target of XNOR-Net is minimizing the absolute ( quantization ) error ( $ \\mathop { \\arg\\min } \\limits_ { \\alpha , \\textbf { B } } ||\\mathbf { W } -\\alpha \\mathbf { B } ||^ { 2 } $ ) , which aims to find the optimal approximation of FP32 output features . Moreover , recent methods with scaling factors such as Bi-Real Net and IR-Net , just use one set of factors to approximate the weights only for efficient inference . They do not provide means to adapt to the scale distortion of the input activations . In addition to XNOR-Net , IR-Net , Bi-Real Net that we have already evaluated in Table 2 , we have followed the reviewer \u2019 s recommendation to include ABC-Net and XNOR-Net++ , which apply analytically-computed layerwise scaling factor and learnable scaling factor respectively , in our experiments ( see results below ) . The results are supportive of our theory . First , XNOR-Net outperforms more recent methods such as Bi-Real Net and IR-Net , but these recent methods perform better than XNOR-Net in 2D vision tasks . This discrepancy highlights that there are different challenges of model binarization in CNNs for 2D vision and PointNet for 3D vision . Second , despite that XNOR-Net minimizes absolute quantization error of both the input activations and weight parameters , LSR is able to achieve the best accuracy using only one parameter per layer , demonstrating that LSR is more effective than existing methods in tackling the scale distortion problem that is critical for 3D point clouds . Note that the binarized models without our EMA do not converge in the training process ( results are as low as 4.1 % ) , which shows that the EMA is necessary for binarized PointNet ."}, "1": {"review_id": "9QLRCVysdlO-1", "review_text": "The paper proposes a binarization approach for efficient deep learning on point clouds , called BiPointNet . The authors claim that the immense performance drop of binarized models is caused by the aggregation-induced feature homogenization and scale distortion . The authors propose Entropy-Maximizing Aggregation ( EMA ) and Layer-wise Scale Recovery ( LSR ) to reduce the side-effects of binarization . Experiment results demonstrate that the proposed BIPointNet is able to achieve state-of-the-art results and gives an impressive speedup and storage saving . Besides the major contribution of the paper , the writing of the paper is concise and the illustrations are clear . However , the paper mainly focuses on PointNet-kind of structure , such as PointNet++ , and DGCNN , etc . It would be better if the authors could give more discussion on the application of the EMA and LSR on more advanced methods , such as KPConv and PointConv , etc .", "rating": "7: Good paper, accept", "reply_text": "We would like to first express our deep appreciation for your insightful comments . Our response to your suggestion can be found below : * * Q1 * * : Give more discussion on the application of the EMA and LSR on more advanced methods . * * A1 * * : Our paper evaluates the proposed method on three representative network architectures ( PointNet for Pointwise MLP Networks , PointCNN for Convolution-based Networks , DGCNN for Graph-based Networks [ 1 ] ) ; We agree that PointConv and KPConv are very important work , the computationally intensive operations in these architectures can be significantly accelerated by the binarization module with our LSR ( such as the MLP layers in PointConv ) , and the EMA can be applied to aggregators ( such as the max-pooling in KPConv ) to avoid feature homogenization . We further add more discussion in the revised version of our paper . In fact , we have implemented our BiPointNet on PointConv : our method achieves good results , outperforming XNOR-Net by 4.8 % with much fewer parameters . We are in the process of implementing our methods on more architectures and try our best to add these results in the revised version . |Base Model & nbsp ; & nbsp ; & nbsp ; & nbsp ; |Method & nbsp ; & nbsp ; & nbsp ; & nbsp ; |Bit-width & nbsp ; & nbsp ; & nbsp ; & nbsp ; |OA| |-|| -- | -- | | PointConv| FP32| 32/32| 90.8 % | | | XNOR-Net & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | 1/1 | 83.1 % | | | Ours | 1/1 | 87.9 % | [ 1 ] Guo et . al. , Deep Learning for 3D Point Clouds : A Survey , TPAMI 2020"}, "2": {"review_id": "9QLRCVysdlO-2", "review_text": "This paper proposes a method for learning binary neural networks on point cloud inputs . They provide an entropy analysis of the binarized distributions as well as an offset transform to maximize entropy . Then they analyze the scale of binary activations and propose a learnable scaling to reduce the effects of scale distortion . They show that their method is competitive with other binary neural network methods and even full precision methods . Finally they show performant speed and storage results on a Raspberry Pi . Strengths : - Strong empirical results backed by theoretical analysis - Experiments are comprehensive and show competitive results on accuracy and speed Concerns : - If speed vs accuracy is the main trade-off , I would like to see a more thorough evaluation of all the models and baselines on a speed vs accuracy tradeoff plot Given the strong empirical and theoretical results , I would recommend an accept . I would still like to see the authors strengthen their paper with a more detailed speed/accuracy trade off .", "rating": "7: Good paper, accept", "reply_text": "We would like to first express our deep appreciation for your insightful comments . Our response to your suggestion can be found below : * * Q1 * * : More thorough evaluation of all the models and baselines on a speed vs accuracy tradeoff plot . * * A1 * * : We agree that a speed vs accuracy tradeoff plot is helpful . We thus implement and evaluate more quantization methods based on PointNet architecture on ARM devices , and complete a speed vs accuracy trade-off scatter plot . Since we can not add this plot in our response , we present our accuracy vs speed results on ARM devices in the table below : | CPU & nbsp ; & nbsp ; | Method & nbsp ; & nbsp ; & nbsp ; & nbsp ; | Acc . ( % ) & nbsp ; & nbsp ; & nbsp ; & nbsp ; | Time cost ( ms ) | | -- ||-|-| | A72 | FP32 | 88.2 | 67.3 | | | BNN | 16.2 | 5.5 | | | IR-Net | 63.5 | 5.5 | | | Bi-Real Net & nbsp ; & nbsp ; & nbsp ; & nbsp ; | 77.5 | 5.5 | | | ABC-Net | 77.8 | 9.2 | | | XNOR-Net | 81.9 | 9.7 | | | Ours | 86.4 | 5.5 | | A53 | FP32 | 88.2 | 131.8 | | | BNN | 16.2 | 9.0 | | | IR-Net | 63.5 | 9.0 | | | Bi-Real Net | 77.5 | 9.0 | | | ABC-Net | 77.8 | 15.6 | | | XNOR-Net | 81.9 | 15.7 | | | Ours | 86.4 | 9.0 | The results show that our BiPointNet outperforms others in both speed and accuracy , and the speed of our BiPointNet is much faster than the FP32 model with only a small drop of accuracy . And the speed vs accuracy trade-off plot will be added in the revised version . Note that through optimization of implementation on ARM devices , the models with fixed scaling factor ( IR-Net , Bi-Real Net , and ours ) can be as fast as the BNN baseline with no scaling factors . However , models that use dynamically computed scaling factors ( XNOR-Net and ABC-Net ) suffer extra computational burdens . Besides , more complicated base models , such as PointCNN , DGCNN and PointConv , contain operations that are difficult to implement and optimize on ARM devices ( such as KNN and FPS ) , especially within the short response time . Nevertheless , we are in the progress of implementing our method on more architectures for 3D point clouds and make them easy to deploy on resource-limited devices ."}, "3": {"review_id": "9QLRCVysdlO-3", "review_text": "This paper proposes a method for binarization of neural networks of 3d point clouds . Two modules of entropy maximum aggregation and layer-wise scale recovery are proposed to conquer the problems of discrimination loss induced by feature homogenization and scale imbalance , which are caused by model binarization . The authors provide theoretical analysis about the proposed method . Experiments on various backbones and tasks demonstrate the effectiveness of the proposed method . A practical implantation of BiPointNet on ARM also demonstrates significant speedups over PointNet and large memory savings . Strength : 1 . Binarization of CNN models designed for 2D images has been studied in the past years , this paper extends this problem into 3D point cloud models . The authors show that the existing methods for binarization of 2D CNN models can not work well on this new problem . 2.For this new problem , the authors analysis its performance degradation based on PointNet and proposed effective solutions . 3.Experiments on several tasks show that the proposed method can obtain highly compact models with acceptable accuracy degradation . Experiments on other backbones also show that the proposed method is general , although its analysis is based on PointNet . For the weakness , I only have some minor comments . 1.The discussion on related work could be enlarged . For example , the following papers are well known point cloud networks proposed recently ( a ) PointConv : Deep Convolutional Networks on 3D Point Clouds . CVPR 2019 ( b ) Relation-Shape Convolutional Neural Network for Point Cloud Analysis . CVPR 2019 ( c ) ShellNet : Efficient Point Cloud Convolutional Neural Networks using Concentric Shells Statistics . ICCV 2019 Mixed precision quantization is also an active direction after binarization of neural networks , it could also be mentioned as a possible improvement in the future . ( d ) Mixed Precision Quantization of Convnets via Differentiable Neural Architecture Search . ICLR 2019 ( e ) Search What You Want : Barrier Panelty NAS for Mixed Precision Quantization . ECCV 2020 . 2.Some references miss publication type , i.e. , conference or journal and where they are published . 3.Figure 1 can be improved . It is unclear how LSR works in the whole framework .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We are deeply grateful for the reviewer \u2019 s support of our work and we thank the reviewer for the constructive and helpful suggestions . We provide additional discussions below : * * Q1 * * : The discussion on related work could be enlarged . * * A1 * * : We will add more discussions on the latest works on point clouds , including those mentioned by the reviewer . Our paper evaluates the proposed method on three representative network architectures ( PointNet for Pointwise MLP Networks , PointCNN for Convolution-based Networks , DGCNN for Graph-based Networks [ 1 ] ) ; we agree that PointConv , RS-CNN , and ShellNet are also important works to be discussed in the revised version of our paper . Moreover , despite the limited response time , we have implemented various binarization methods based on PointConv , and our method also achieves outstanding results : ours outperform XNOR-Net by 4.8 % with much fewer parameters . We are working on implementing our method on more base models . |Base Model & nbsp ; & nbsp ; & nbsp ; & nbsp ; |Method & nbsp ; & nbsp ; & nbsp ; & nbsp ; |Bit-width & nbsp ; & nbsp ; & nbsp ; & nbsp ; |OA| |-|| -- | -- | | PointConv| FP32| 32/32| 90.8 % | | | XNOR-Net & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; | 1/1 | 83.1 % | | | Ours | 1/1 | 87.9 % | We will also add more discussion on mixed precision quantization in the final version . Mixed precision quantization is also a popular approach to network compression and acceleration . Unlike binarization that pursues extreme compression and acceleration , mixed-precision quantization achieves a balance between speed and accuracy by adjusting the quantization accuracy of different layers . We are also interested to explore mixed precision quantization for point cloud model acceleration in our future work . [ 1 ] Guo et . al. , Deep Learning for 3D Point Clouds : A Survey , TPAMI 2020 * * Q2 * * : Some references miss publication type . * * A2 * * : We will carefully correct our references in the revised version . * * Q3 * * : Figure 1 can be improved . * * A3 * * : We will improve Figure 1 and add related explanations in the revised version . The proposed LSR is applied to the bi-linear layers ( which form the BiMLPs in Figure 1 ) in our BiPointNet , the learnable layerwise scaling factors are applied to recover scales of the features obtained by XNOR-Bitcount operation ( $ \\mathbf Z = \\alpha ( \\mathbf { B_a } \\odot \\mathbf { B_w } ) $ ) ."}}