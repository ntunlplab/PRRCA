{"year": "2020", "forum": "SygcSlHFvS", "title": "On Understanding Knowledge Graph Representation", "decision": "Reject", "meta_review": "The paper proposes a set of conditions that enable a mapping from word embeddings to relation embeddings in knowledge graphs. Then, using recent results about pointwise mutual information word embeddings, the paper provides insights to the latent space of relations, enabling a categorization of relations of entities in a knowledge graph. Empirical experiments on recent knowledge graph models (TransE, DistMult, TuckER and MuRE) are interpreted in light of the predictions coming from the proposed set of conditions.\n\nThe authors responded to reviewer comments well, providing significant updates during the discussion period. Unfortunately, the reviewers did not engage further after their original reviews, and so it is hard to tell whether they agreed that the changes resolved all their questions.\n\nOverall, the paper provides much needed analysis for understanding of the latent space of relations on knowledge graphs. Unfortunately, the original submission did not clearly present the ideas, and it is unclear whether the updated version addresses all the concerns. The paper in its current state is therefore not yet suitable for publication at ICLR.", "reviews": [{"review_id": "SygcSlHFvS-0", "review_text": "This paper proposes to provide a detailed study on the explainability of link prediction (LP) models by utilizing a recent interpretation of word embeddings. More specifically, the authors categorize the relations in KG into three categories (R, S, C) using the correlation between the semantic relation between two words and the geometric relationship between their embeddings. The authors utilize this categorization to provide a better understanding of LP models\u2019 performance through several experiments. This paper reads well and the results appear sound. I personally believe that works on better understanding KGC models are a very essential direction which is mostly ignored in this field of study. Moreover, the provided experiments support the authors\u2019 intuition and arguments. As for the drawbacks, I find the technical novelty of the paper is somewhat limited, as the proposed method consists of a mostly straightforward combination of existing methods. Further, I believe this work needs more experimental results and decisive conclusions identifying future directions to achieve better performance on link prediction. My concerns are as follows: \u2022 I am wondering about the reason for omitting Max/Avg path for two of the relations in WN18RR? Further, the average of 15.2 for the shortest path between entities with \u201calso_see\u201d relation appears to be a mistake? \u2022 Was there any specific reason in choosing WN18RR and NELL-995 KGs for the experiments? \u2022 It would be interesting to see the length of paths between entities for train and test data separately. \u2022 I suggest providing a statistical significance evaluation for each experiment to better validate the conclusions. \u2022 I find the provided study in section 4.2 very similar to the triple classification task in KGs. Can you elaborate on the differences and potential advantages of your setting? \u2022 I am wondering how you identified the \u201cOther True\u201d triples for WN18RR KG in section 4.2 experiments? On overall, although I find the proposed study very interesting and enlightening, I believe that the paper needs more experimental results and decisive conclusions. ", "rating": "6: Weak Accept", "reply_text": "Re \u201c decisive conclusions \u201d : we agree that the conclusions of the paper are insufficiently clear and have improved the paper to address . Decisive conclusions that we make : 1 ) previous understanding of how semantic relations are encoded between PMI-based word embeddings for a few relations ( e.g.similarity , analogies , etc - Allen & Hospedale ( 2019 ) , Allen et al . ( 2019 ) ) is extended to derive the difference between word embeddings for the general relations of knowledge graphs , which translate into linear algebraic mappings . From their mappings , relations can be categorised into 3 types and components of the mappings ( e.g.projection matrix , translation vector ) relate to meaningful/interpretable semantic aspects of the relation ( e.g.relatedness between entities , entity-specific features ) . 2 ) that PMI-based word embeddings and knowledge graph entity embeddings show commonality to their latent structure \u2014 despite the significant differences between their training data and methodology . We demonstrate this by : ( i ) deriving properties of the relation mappings ( based on word embeddings ) , e.g.vector norm , matrix symmetry/effective rank , and identifying those in actual knowledge graph representations ; and ( ii ) showing that the relative performance of knowledge graph models for each relation type accords with how well a model \u2019 s architecture satisfies the corresponding relation conditions ( based on word embeddings ) . 3 ) that stand-alone classification performance should be evaluated for future models since the task itself may be of more practical use than ranking metrics , and it provides novel insight into model performance . Overall , we provide an important step towards a theoretical understanding of the latent structure of knowledge graph representations . In terms of practical use , our results : provide understanding as to which model is most appropriate for a new dataset ( e.g.if relations were known , a priori , to be symmetric ) ; suggest that different aspects of relations ( e.g.type , strength of relatedness ) could be quantitatively evaluated ; and indicate where future research effort might be directed ( e.g.type C relations ) . Furthermore , whilst whether multiplicative ( e.g.DistMult ) or additive ( e.g.TransE ) link prediction models are superior has been an open question , we now provide theoretical justification that the answer is both ( e.g.as in MuRE ) ."}, {"review_id": "SygcSlHFvS-1", "review_text": "Summary: The paper attempts to understand the latent structure underlying knowledge graph embedding methods. The work can be seen as an extension of understanding of PMI-based word embedding methods. They categorize knowledge graph relations into three categories based on their relation conditions: Relatedness (R), Specialisation (S), and Context-shift (C). For each category, they evaluate a representative of different types of knowledge graph embedding methods. Through results, they demonstrate that a model\u2019s ability to represent a specific relation type depends on the limitations imposed by the model architecture with respect to satisfying the necessary relation conditions. Questions: 1. The results in Tables 3 and 4 demonstrate that MuRE is the most effective method for handling different types of relations but then how come its performance on FB15k-237 (.336 MRR) is significantly lower than other methods like TuckER (.358 MRR). Can you provide an explanation? 2. In Section 3.2, the authors list 4 predictions (P1-4). It would be great if authors could provide some more reasoning behind coming with these predictions. 3. In Section 4.2, it is stated that \u201cranking based metrics like MRR and hits@k are flawed if entities are related to more than k others\u201d. It would be great if the authors could give an example to make it more clear. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and the time taken for it , below we address each of your points in turn . 1 ) The difference in performance on the FB15k-237 ( FB ) dataset is due to multi-task learning ( as discussed in the TuckER paper , Balaevic et al . ( 2019b ) ) .Amongst the models considered , TuckER is the only one to share parameters between relations , which is encouraged within its core tensor due to its low-rank . As such , on datasets with many relations and relatively few instances per relation , e.g.FB and NELL , the multi-task learning ability of TuckER offers material advantage . We mention this ( briefly ) in respect of the the NELL dataset in section 4.1 , which we have italicised to make more clear . For avoidance of doubt , this is not our reason for omitting results for the FB dataset , which is because the vast majority of FB relations are found to be of type C , whereas we analyse performance differences between relation types and hence use WN and NELL , which have a broader variety of relations by type . 2 ) we agree that rationale for the predictions could be more clear and have improved clarity ( e.g.the start of Sec 3.2 and key contributions in Sec 1 ) , to present the predictions is a clearer light . To expand on their rationale : P1 : Type R is a special case of type S , which is a special case of type C ( Section 3.1 ) . Hence type S , for example , subsumes type R and ( all else being equal ) a type S relation requires more parameters to be learned than one of type R and , in that sense , is `` harder to learn \u2019 \u2019 . Further to this , the \u201c shape \u201d of the relation-mapping changes between relation types : type R require only a multiplicative ( matrix ) component , type S an extra additive ( vector ) component and type C a further additive component . Whether the architecture of a model supports a relation \u2019 s \u201c shape \u201d ( or , in the language of P1 , meet the \u201c relation conditions ) is expected to affect performance . P2 : Since type R relations are ( by definition ) symmetric , their relation matrices must also be . P3 : Since no additive components ( \u201c offset vectors \u201d ) are required for relatedness relations ( type R ) , any vector norms for those relations are predicted to be small . P4 : The strength ( s ) of the relatedness of a relation r is defined as |S| where S is the set of context words that both entities must co-occur with similarly for r to hold . In the full rank space of PMI vectors each word corresponds to a dimension , thus s is also the dimensionality of a common PMI vector component ( i.e.the component in the dimensions of S ) . That common component can be tested for with a projection matrix of rank s. In the lower dimension of embeddings , the dimensionality of S is obscured , but is anticipated to be reflected in the eigenvalues of the relation matrix due to its relationship with the projection matrix ( Section 3.2 ) . 3 ) As an example of how hits @ k metrics can be flawed : consider computing hits @ 3 for a model with a dataset whose entities include { UK , London , Edinburgh , Brighton , Manchester , York , Birmingham } , training set contains ( UK , contains_city , Edinburgh ) , ( UK , contains_city , Manchester ) and test set contains ( UK , contains_city , London ) . Each test triple is evaluated by removing an entity and ranking the score assigned to that \u201c true \u201d entity amongst all entities in the dataset , excluding other known true triples ( i.e. \u201c filtering \u201d ) . When the test triple ( UK , contains_city , London ) is evaluated by removing London , let the top scoring entities be , in descending order : Edinburgh , Brighton , Manchester , York , London , Birmingham , ... After removing Edinburgh and Manchester ( as known \u201c true \u201d s ) , the order is : Brighton , York , London , Birmingham , ... And the sought answer ( London ) appears in the top k ( i.e.3 ) , and contributes to hits @ k metric . However , if ( UK , contains_city , Edinburgh ) happened to not be in the dataset , the order would be : Edinburgh , Brighton , York , London , Birmingham , ... And the sought answer would not appear in the top k , even though all top 5 answers are correct . This demonstrates that ranking metrics for 1-to-many , many-to-many or many-to-1 relations can be affected ( arbitrarily ) by unknown true answers , which can not be tested for or evaluated ( without further annotation ) , since they are by definition unknown . Further , such instances are always assumed to exist since the central aim of link prediction is to predict them , i.e.true facts that are not previously known ."}, {"review_id": "SygcSlHFvS-2", "review_text": " There has been a large family of knowledge base models developed in the recent years, aiming to encode both entities and relations in a latent space, so the entities can be \u201clinked\u201d via a relation-specific mapping. The paper focuses on understanding these entity embeddings (and geometric embedding relationships), built on top of the connections with PMI-based word embeddings. The paper categorizes all the relations into 3 types (1) related, (2) specialisation, and (3) context shift, and examines some relations in WordNet and NELL, and then empirically evaluates the performance of different types of models and draws the correlation of the results and intuitive understanding of different types of relations. To me, this paper is more like providing some intuitive explanations of existing KG embeddings methods and their performance (not really theoretical justifications). It was an interesting read and I appreciate the authors trying to understanding the latent structure that has been encoded in these models. However, I am just not that sure how many take-aways we can get from this study. I am wondering how loose this categorization is , esp. for the important relations in practice. I\u2019d be also interested in seeing more results on Freebase (and possibly Wikidata) as those KG embeddings are usually more useful. As indicated in the Appendix, the paper mentiosns most of FB15k-237 datasets are in type C, so I am just not sure how many R/S relations are actually there. Also, according to Table 3 and Table 4, I am not sure if there are any surprising findings from there. It seems that there is some randomness/noise, but MuRE generally works better than the others. It is true that DistMult works well on the R-type relations but it is not consistent between WN and NELL. It\u2019d be useful to show results on more relations (and aggregated results in each category). It'd be really great if the paper actually provides some insights on we can further improve these entity embeddings according to this categorization. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and the time taken for it , below we address each of your points in turn . * Re \u201c take-aways \u201d : we agree that we have not made this sufficiently clear and have updated the paper accordingly ( in particular introduction , part of results and conclusion ) . To summarise , the key take-aways from our work are : 1 ) that the previous understanding of how semantic relations are encoded between PMI-based word embeddings for a few relations ( e.g.similarity , analogies , etc - Allen & Hospedales ( 2019 ) , Allen et al . ( 2019 ) ) is extended to derive the difference between word embeddings for the general relations of knowledge graphs , which translate into linear algebraic mappings . From their mappings , relations can be categorised into 3 types and components of the mappings ( e.g.projection matrix , translation vector ) related to meaningful/interpretable semantic aspects of the relation ( e.g.relatedness between entities , entity-specific features ) . 2 ) that PMI-based word embeddings and knowledge graph entity embeddings show commonality to their latent structure - despite the significant differences between their training data and methodology . We demonstrate this by : ( i ) deriving properties of the relation mappings ( based on word embeddings ) , e.g.vector norm , matrix symmetry/effective rank , and identifying those in actual knowledge graph representations ; and ( ii ) showing that the relative performance of knowledge graph models for each relation type accords with how well a model \u2019 s architecture satisfies the corresponding relation conditions ( based on word embeddings ) . 3 ) that stand-alone classification performance should be evaluated for future models since the task itself may be of more practical use than ranking metrics , and it provides novel insight into model performance . Overall , we provide an important step towards a theoretical understanding of the latent structure of knowledge graph representations . In terms of practical use , our results : provide understanding as to which model is most appropriate for a new dataset ( e.g.if relations were known , a priori , to be symmetric ) ; suggest that different aspects of relations ( e.g.type , strength of relatedness ) could be quantitatively evaluated ; and indicate where future research effort might be directed ( e.g.type C relations ) . * Re `` FB15k-237 dataset '' ( FB ) : The prevalence of type C relations in FB does not contradict or weaken our results , it means only that FB is less useful for demonstrating the differences between model performance across relation types relative to datasets that contain a broader spread of relations by type ( e.g.WN , NELL ) . We agree that this is insufficiently clear and move the explanation from Appx B to Sec 3.1 . * Re \u201c Table 3 & 4 findings \u201d : we agree these are insufficiently clear and have updated the paper ( Sec 3.2 , to more clearly motivate the experiments , and Sec 4 ) . Key findings of Tables 3 and 4 are : ( i ) that MuRE \u2019 s advantage over other models largely corresponds to type S/C relations , fitting prediction P1 since those relations require both additive and multiplicative components of the loss function ( no other model has both ) ; ( ii ) that , between MuRE_I and DistMult , multiplicative-only DistMult ( typically ) performs better for type R relations , which require a multiplicative component only ; whereas additive-only MuRE_I performs best for type C/S relations ( see Table 3 ) , which require an additive component ( but may also require a multiplicative component , explaining the inconsistency in Table 4 ) ; and ( iii ) performance of TuckER is comparable to MuRE_I/DistMult models for datasets with few relations ( e.g.WN ) , but is more comparable to MuRE for datasets with many relations ( e.g.NELL , FB ) when multi-task learning of relations provides material benefit ( as discussed in the TuckER paper , Balazevic et al . ( 2019b ) ) . All findings accord with prediction P1 made based on the latent structure of PMI-based word embeddings ( relation conditions ) . As a corollary , whilst multiplicative and additive link prediction models have historically jostled for superiority and which is better remained an open question , we justify why the answer is both ( e.g.as in MuRE ) . * Re \u201c DistMult on type R relations \u201d : DistMult outperforms other models for type R relations , except for the \u201c also_see \u201d ( AS ) and \u201c derivationally_related_form \u201d ( DRF ) relations ( Table 3 ) . - AS has a high Krackhardt score and path length indicating a tree structure and obscuring results since the models considered are not suited to hierarchical relations ( as discussed in the MuRE paper , Balazevic et al ( 2019a ) ) . - DistMult can be fully expressed by MuRE and thus only outperforms when MuRE \u2019 s additional parameters may allow overfitting . DRF has an abundance of data ( 34 % of all training examples ) , whereby any overfitting is reduced , explaining why DistMult does not outperform MuRE ."}], "0": {"review_id": "SygcSlHFvS-0", "review_text": "This paper proposes to provide a detailed study on the explainability of link prediction (LP) models by utilizing a recent interpretation of word embeddings. More specifically, the authors categorize the relations in KG into three categories (R, S, C) using the correlation between the semantic relation between two words and the geometric relationship between their embeddings. The authors utilize this categorization to provide a better understanding of LP models\u2019 performance through several experiments. This paper reads well and the results appear sound. I personally believe that works on better understanding KGC models are a very essential direction which is mostly ignored in this field of study. Moreover, the provided experiments support the authors\u2019 intuition and arguments. As for the drawbacks, I find the technical novelty of the paper is somewhat limited, as the proposed method consists of a mostly straightforward combination of existing methods. Further, I believe this work needs more experimental results and decisive conclusions identifying future directions to achieve better performance on link prediction. My concerns are as follows: \u2022 I am wondering about the reason for omitting Max/Avg path for two of the relations in WN18RR? Further, the average of 15.2 for the shortest path between entities with \u201calso_see\u201d relation appears to be a mistake? \u2022 Was there any specific reason in choosing WN18RR and NELL-995 KGs for the experiments? \u2022 It would be interesting to see the length of paths between entities for train and test data separately. \u2022 I suggest providing a statistical significance evaluation for each experiment to better validate the conclusions. \u2022 I find the provided study in section 4.2 very similar to the triple classification task in KGs. Can you elaborate on the differences and potential advantages of your setting? \u2022 I am wondering how you identified the \u201cOther True\u201d triples for WN18RR KG in section 4.2 experiments? On overall, although I find the proposed study very interesting and enlightening, I believe that the paper needs more experimental results and decisive conclusions. ", "rating": "6: Weak Accept", "reply_text": "Re \u201c decisive conclusions \u201d : we agree that the conclusions of the paper are insufficiently clear and have improved the paper to address . Decisive conclusions that we make : 1 ) previous understanding of how semantic relations are encoded between PMI-based word embeddings for a few relations ( e.g.similarity , analogies , etc - Allen & Hospedale ( 2019 ) , Allen et al . ( 2019 ) ) is extended to derive the difference between word embeddings for the general relations of knowledge graphs , which translate into linear algebraic mappings . From their mappings , relations can be categorised into 3 types and components of the mappings ( e.g.projection matrix , translation vector ) relate to meaningful/interpretable semantic aspects of the relation ( e.g.relatedness between entities , entity-specific features ) . 2 ) that PMI-based word embeddings and knowledge graph entity embeddings show commonality to their latent structure \u2014 despite the significant differences between their training data and methodology . We demonstrate this by : ( i ) deriving properties of the relation mappings ( based on word embeddings ) , e.g.vector norm , matrix symmetry/effective rank , and identifying those in actual knowledge graph representations ; and ( ii ) showing that the relative performance of knowledge graph models for each relation type accords with how well a model \u2019 s architecture satisfies the corresponding relation conditions ( based on word embeddings ) . 3 ) that stand-alone classification performance should be evaluated for future models since the task itself may be of more practical use than ranking metrics , and it provides novel insight into model performance . Overall , we provide an important step towards a theoretical understanding of the latent structure of knowledge graph representations . In terms of practical use , our results : provide understanding as to which model is most appropriate for a new dataset ( e.g.if relations were known , a priori , to be symmetric ) ; suggest that different aspects of relations ( e.g.type , strength of relatedness ) could be quantitatively evaluated ; and indicate where future research effort might be directed ( e.g.type C relations ) . Furthermore , whilst whether multiplicative ( e.g.DistMult ) or additive ( e.g.TransE ) link prediction models are superior has been an open question , we now provide theoretical justification that the answer is both ( e.g.as in MuRE ) ."}, "1": {"review_id": "SygcSlHFvS-1", "review_text": "Summary: The paper attempts to understand the latent structure underlying knowledge graph embedding methods. The work can be seen as an extension of understanding of PMI-based word embedding methods. They categorize knowledge graph relations into three categories based on their relation conditions: Relatedness (R), Specialisation (S), and Context-shift (C). For each category, they evaluate a representative of different types of knowledge graph embedding methods. Through results, they demonstrate that a model\u2019s ability to represent a specific relation type depends on the limitations imposed by the model architecture with respect to satisfying the necessary relation conditions. Questions: 1. The results in Tables 3 and 4 demonstrate that MuRE is the most effective method for handling different types of relations but then how come its performance on FB15k-237 (.336 MRR) is significantly lower than other methods like TuckER (.358 MRR). Can you provide an explanation? 2. In Section 3.2, the authors list 4 predictions (P1-4). It would be great if authors could provide some more reasoning behind coming with these predictions. 3. In Section 4.2, it is stated that \u201cranking based metrics like MRR and hits@k are flawed if entities are related to more than k others\u201d. It would be great if the authors could give an example to make it more clear. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and the time taken for it , below we address each of your points in turn . 1 ) The difference in performance on the FB15k-237 ( FB ) dataset is due to multi-task learning ( as discussed in the TuckER paper , Balaevic et al . ( 2019b ) ) .Amongst the models considered , TuckER is the only one to share parameters between relations , which is encouraged within its core tensor due to its low-rank . As such , on datasets with many relations and relatively few instances per relation , e.g.FB and NELL , the multi-task learning ability of TuckER offers material advantage . We mention this ( briefly ) in respect of the the NELL dataset in section 4.1 , which we have italicised to make more clear . For avoidance of doubt , this is not our reason for omitting results for the FB dataset , which is because the vast majority of FB relations are found to be of type C , whereas we analyse performance differences between relation types and hence use WN and NELL , which have a broader variety of relations by type . 2 ) we agree that rationale for the predictions could be more clear and have improved clarity ( e.g.the start of Sec 3.2 and key contributions in Sec 1 ) , to present the predictions is a clearer light . To expand on their rationale : P1 : Type R is a special case of type S , which is a special case of type C ( Section 3.1 ) . Hence type S , for example , subsumes type R and ( all else being equal ) a type S relation requires more parameters to be learned than one of type R and , in that sense , is `` harder to learn \u2019 \u2019 . Further to this , the \u201c shape \u201d of the relation-mapping changes between relation types : type R require only a multiplicative ( matrix ) component , type S an extra additive ( vector ) component and type C a further additive component . Whether the architecture of a model supports a relation \u2019 s \u201c shape \u201d ( or , in the language of P1 , meet the \u201c relation conditions ) is expected to affect performance . P2 : Since type R relations are ( by definition ) symmetric , their relation matrices must also be . P3 : Since no additive components ( \u201c offset vectors \u201d ) are required for relatedness relations ( type R ) , any vector norms for those relations are predicted to be small . P4 : The strength ( s ) of the relatedness of a relation r is defined as |S| where S is the set of context words that both entities must co-occur with similarly for r to hold . In the full rank space of PMI vectors each word corresponds to a dimension , thus s is also the dimensionality of a common PMI vector component ( i.e.the component in the dimensions of S ) . That common component can be tested for with a projection matrix of rank s. In the lower dimension of embeddings , the dimensionality of S is obscured , but is anticipated to be reflected in the eigenvalues of the relation matrix due to its relationship with the projection matrix ( Section 3.2 ) . 3 ) As an example of how hits @ k metrics can be flawed : consider computing hits @ 3 for a model with a dataset whose entities include { UK , London , Edinburgh , Brighton , Manchester , York , Birmingham } , training set contains ( UK , contains_city , Edinburgh ) , ( UK , contains_city , Manchester ) and test set contains ( UK , contains_city , London ) . Each test triple is evaluated by removing an entity and ranking the score assigned to that \u201c true \u201d entity amongst all entities in the dataset , excluding other known true triples ( i.e. \u201c filtering \u201d ) . When the test triple ( UK , contains_city , London ) is evaluated by removing London , let the top scoring entities be , in descending order : Edinburgh , Brighton , Manchester , York , London , Birmingham , ... After removing Edinburgh and Manchester ( as known \u201c true \u201d s ) , the order is : Brighton , York , London , Birmingham , ... And the sought answer ( London ) appears in the top k ( i.e.3 ) , and contributes to hits @ k metric . However , if ( UK , contains_city , Edinburgh ) happened to not be in the dataset , the order would be : Edinburgh , Brighton , York , London , Birmingham , ... And the sought answer would not appear in the top k , even though all top 5 answers are correct . This demonstrates that ranking metrics for 1-to-many , many-to-many or many-to-1 relations can be affected ( arbitrarily ) by unknown true answers , which can not be tested for or evaluated ( without further annotation ) , since they are by definition unknown . Further , such instances are always assumed to exist since the central aim of link prediction is to predict them , i.e.true facts that are not previously known ."}, "2": {"review_id": "SygcSlHFvS-2", "review_text": " There has been a large family of knowledge base models developed in the recent years, aiming to encode both entities and relations in a latent space, so the entities can be \u201clinked\u201d via a relation-specific mapping. The paper focuses on understanding these entity embeddings (and geometric embedding relationships), built on top of the connections with PMI-based word embeddings. The paper categorizes all the relations into 3 types (1) related, (2) specialisation, and (3) context shift, and examines some relations in WordNet and NELL, and then empirically evaluates the performance of different types of models and draws the correlation of the results and intuitive understanding of different types of relations. To me, this paper is more like providing some intuitive explanations of existing KG embeddings methods and their performance (not really theoretical justifications). It was an interesting read and I appreciate the authors trying to understanding the latent structure that has been encoded in these models. However, I am just not that sure how many take-aways we can get from this study. I am wondering how loose this categorization is , esp. for the important relations in practice. I\u2019d be also interested in seeing more results on Freebase (and possibly Wikidata) as those KG embeddings are usually more useful. As indicated in the Appendix, the paper mentiosns most of FB15k-237 datasets are in type C, so I am just not sure how many R/S relations are actually there. Also, according to Table 3 and Table 4, I am not sure if there are any surprising findings from there. It seems that there is some randomness/noise, but MuRE generally works better than the others. It is true that DistMult works well on the R-type relations but it is not consistent between WN and NELL. It\u2019d be useful to show results on more relations (and aggregated results in each category). It'd be really great if the paper actually provides some insights on we can further improve these entity embeddings according to this categorization. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your review and the time taken for it , below we address each of your points in turn . * Re \u201c take-aways \u201d : we agree that we have not made this sufficiently clear and have updated the paper accordingly ( in particular introduction , part of results and conclusion ) . To summarise , the key take-aways from our work are : 1 ) that the previous understanding of how semantic relations are encoded between PMI-based word embeddings for a few relations ( e.g.similarity , analogies , etc - Allen & Hospedales ( 2019 ) , Allen et al . ( 2019 ) ) is extended to derive the difference between word embeddings for the general relations of knowledge graphs , which translate into linear algebraic mappings . From their mappings , relations can be categorised into 3 types and components of the mappings ( e.g.projection matrix , translation vector ) related to meaningful/interpretable semantic aspects of the relation ( e.g.relatedness between entities , entity-specific features ) . 2 ) that PMI-based word embeddings and knowledge graph entity embeddings show commonality to their latent structure - despite the significant differences between their training data and methodology . We demonstrate this by : ( i ) deriving properties of the relation mappings ( based on word embeddings ) , e.g.vector norm , matrix symmetry/effective rank , and identifying those in actual knowledge graph representations ; and ( ii ) showing that the relative performance of knowledge graph models for each relation type accords with how well a model \u2019 s architecture satisfies the corresponding relation conditions ( based on word embeddings ) . 3 ) that stand-alone classification performance should be evaluated for future models since the task itself may be of more practical use than ranking metrics , and it provides novel insight into model performance . Overall , we provide an important step towards a theoretical understanding of the latent structure of knowledge graph representations . In terms of practical use , our results : provide understanding as to which model is most appropriate for a new dataset ( e.g.if relations were known , a priori , to be symmetric ) ; suggest that different aspects of relations ( e.g.type , strength of relatedness ) could be quantitatively evaluated ; and indicate where future research effort might be directed ( e.g.type C relations ) . * Re `` FB15k-237 dataset '' ( FB ) : The prevalence of type C relations in FB does not contradict or weaken our results , it means only that FB is less useful for demonstrating the differences between model performance across relation types relative to datasets that contain a broader spread of relations by type ( e.g.WN , NELL ) . We agree that this is insufficiently clear and move the explanation from Appx B to Sec 3.1 . * Re \u201c Table 3 & 4 findings \u201d : we agree these are insufficiently clear and have updated the paper ( Sec 3.2 , to more clearly motivate the experiments , and Sec 4 ) . Key findings of Tables 3 and 4 are : ( i ) that MuRE \u2019 s advantage over other models largely corresponds to type S/C relations , fitting prediction P1 since those relations require both additive and multiplicative components of the loss function ( no other model has both ) ; ( ii ) that , between MuRE_I and DistMult , multiplicative-only DistMult ( typically ) performs better for type R relations , which require a multiplicative component only ; whereas additive-only MuRE_I performs best for type C/S relations ( see Table 3 ) , which require an additive component ( but may also require a multiplicative component , explaining the inconsistency in Table 4 ) ; and ( iii ) performance of TuckER is comparable to MuRE_I/DistMult models for datasets with few relations ( e.g.WN ) , but is more comparable to MuRE for datasets with many relations ( e.g.NELL , FB ) when multi-task learning of relations provides material benefit ( as discussed in the TuckER paper , Balazevic et al . ( 2019b ) ) . All findings accord with prediction P1 made based on the latent structure of PMI-based word embeddings ( relation conditions ) . As a corollary , whilst multiplicative and additive link prediction models have historically jostled for superiority and which is better remained an open question , we justify why the answer is both ( e.g.as in MuRE ) . * Re \u201c DistMult on type R relations \u201d : DistMult outperforms other models for type R relations , except for the \u201c also_see \u201d ( AS ) and \u201c derivationally_related_form \u201d ( DRF ) relations ( Table 3 ) . - AS has a high Krackhardt score and path length indicating a tree structure and obscuring results since the models considered are not suited to hierarchical relations ( as discussed in the MuRE paper , Balazevic et al ( 2019a ) ) . - DistMult can be fully expressed by MuRE and thus only outperforms when MuRE \u2019 s additional parameters may allow overfitting . DRF has an abundance of data ( 34 % of all training examples ) , whereby any overfitting is reduced , explaining why DistMult does not outperform MuRE ."}}