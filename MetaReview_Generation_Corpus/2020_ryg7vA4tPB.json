{"year": "2020", "forum": "ryg7vA4tPB", "title": "Rigging the Lottery: Making All Tickets Winners", "decision": "Reject", "meta_review": "A somewhat new approach to growing sparse networks.  Experimental validation is good, focussing on ImageNet and CIFAR-10, plus experiments on language modelling.  Though efficient in computation and storage size, the approach does not have a theoretical foundation.  That does not agree with the intended scope of ICLR.  I strongly suggest the authors submit elsewhere.", "reviews": [{"review_id": "ryg7vA4tPB-0", "review_text": "This paper proposes a pruning technique called RigL, which performs sparse initialization of the network weights, and allows the network to grow weights during training. The sparse initialization makes the pruning algorithm to be memory- and computation- efficient, unlike existing models that starts with dense network weights. The authors train RigL with three different sparsity distributions that considers the number of input and output nodes, and validate them on various deep convnets for training on ImageNet, on which the model outperforms several existing sparsification methods and even dense counterparts. Pros - The proposed model, RigL, is memory- and computation- efficient, and thus allows to train a large network in an efficient manner. - RigL obtains impressive sparsification performance, even yielding sparse networks that outperform their dense counterparts. Cons - The idea of starting from a small, sparse network and expanding it is not novel. DEN [Yoon et al. 18] proposed the same bottom-up approach with sparsely initialized networks, while they allowed to increase the number of neurons at each layer and focused more on continual learning. The authors should compare the two methods both conceptually and experimentally. - The method is more like a set of heuristics rather than a principled approach, which makes it less appealing. This is not really an issue if the paper includes extensive experimental validation and in-depth analysis, but this is not the case. - The experimental validation is largely lacking, as the authors only perform experiments on ImageNet and do not compare against recent state-of-the-art Bayesian sparsification methods (SBP, VIB, L0-regularization). Without such extensive experimental validation, it is uncertain whether the result will generalize, given the highly empirical nature of the work. In sum, although I believe that the paper proposes a very practical method that is easy to implement and is promising, due to lack of experimental validation against a similar approach, state-of-the-art sparsification methods, and results on more datasets, I temporarily provide the rating of weak reject. I may change my opinion if the authors provide those results during the rebuttal period. [Yoon et al. 18] Lifelong learning with dynamically expandable networks, ICLR 2018", "rating": "3: Weak Reject", "reply_text": "We would like to thank Reviewer 1 and respond to their concerns below : ( 1 ) Thank you for the reference to DEN , it is an interesting work . However , there are some important differences between our work and DEN : ( a ) As emphasized in its abstract and in the introduction DEN attacks the problem of catastrophic forgetting in the continual learning setting and grows new neurons during training to efficiently learn each task in sequence . In contrast , we fix the number of available neurons in each layer and focus on sparse training of neural networks for a single task . In theory we could compare the cost of training with these two different approaches , however : ( b ) When extending continual learning ( as used by DEN ) to the split dataset regime it is non-trivial to match the performance of a standard training regime ( as used by RigL ) . For example , see http : //openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Large_Scale_Incremental_Learning_CVPR_2019_paper.pdf . We are not aware of work that is able to do this successfully , and would appreciate references so that we can possibly make a proper comparison . ( 2 ) We have added two new datasets and architectures to address concerns about the lack of experimental validation . We pushed some of the analysis to the Appendix due to length constraints . We would like to point out Appendix B ( Effect of Sparsity Distribution on Other Methods ) , C ( Effect of Momentum Coefficient for SNFS ) , D ( Existence of Lottery Tickets ) , E ( Effect of Update Schedules ) , F ( Alternative Update Schedules ) and H ( Additional Plots and Experiments for CIFAR-10 ) for extended analysis of our results . We have an explanation based on a Taylor series approximation why magnitude pruning and largest gradient growth is optimal ( at least greedily ) in Appendix F ( now Appendix A ) . Unfortunately we had originally forgotten to mention this in the main text , but we have now updated the last paragraph of Section 4 with a short discussion and a pointer to the appendix . ( 3 ) Regarding SBP ( we assume you mean \u201c Structured Bayesian Pruning \u201d https : //arxiv.org/abs/1705.07283 ) , VIB ( assuming \u201c Variational Information Bottleneck \u201d https : //arxiv.org/pdf/1802.10399.pdf ) , and L0-regularization ( https : //arxiv.org/pdf/1712.01312.pdf ) - we note that these techniques function as an architecture search over channel count distributions and are somewhat orthogonal to our own work . SBP and VIB do _at least_ as much work during training as training the large model they start with ( often quite a bit more as there are non-trivial overheads associated with these techniques ) . The resulting architectures can be trained from scratch ( https : //arxiv.org/abs/1810.05270 ) . Once these techniques find an improved architecture it could be trained with _weight_ sparsity using our technique . L0 does slightly reduce the cost during training , but the difference is not very large . According to Figure 4 of their paper , the cost is decreased from ~3.5 to 3.25 ( e11 ) flops by the end of training , or a reduction of 7 % . The total decrease over the course of training would be even less ."}, {"review_id": "ryg7vA4tPB-1", "review_text": "Overview: The paper is dedicated to developing a more efficient and powerful dense-to-sparse training method. In order to break the limits of the size of the largest trainable sparse model to that of the largest trainable dense model, the author proposes a dynamic method that updates the network topology via parameter magnitudes and infrequent gradient calculation. In the experiments parts, they conduct extensive studies to show the proposed approach can surpass the previous sota with ResNet-50, MobileNet v1 and v2 on the imagenet 2012. What's more, the author also provides some intuitive explanation about why allowing topology change during the optimization is beneficial. Strength Bullets: 1. Due to the dynamic network topology, the paper's methods exactly achieve the memory and computation efficient. i) Required memory is only proportional to the size of the sparse model. ii) The amount of computation is proportional to the number of nonzero parameters in the model. 2. The author performs detailed comparison experiments among different sparsity distribution and different pruning methods. And the results overcome the previous state-of-the-art results. 3. Fig 5 shows some interesting insight. It suggests that static sparse training may be stuck at some local minima which are isolated from improved solutions. However, the dynamic update has a big chance to avoid this problem. Weakness Bullets: 1. The author claims that the ticket in the paper does not rely on a \"lucky\" initialization. But it doesn't exclude the possibility that starting from the original initial conditions may give a better performance. Even if the connection is dynamic, we can still record the initial point for each weight. It will be better the author can provide related analysis. 2. In my opinion, in order to prove dynamic pruning is better than static methods, the author needs to provide a comparison with the previous sota method in it's own setting. i.e. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks Recommendation: I think this paper is a novel work. Although it has some flaws in the experiment design, the motivation and experiment results are conniving enough. So, this is a weak accept.", "rating": "6: Weak Accept", "reply_text": "We appreciate Reviewer 3 's comments and address the weakness bullets below : ( 1 ) We add experiments that demonstrate that choosing the lottery initialization in this setting does not improve the results ( Appendix D ) . There are no special tickets . Rewiring RigL solution to the initial conditions gets 70.82 % average accuracy , which is only 0.2 % better than ` Static ` training . Results demonstrate that training with a fixed topology is significantly worse than training with RigL and that RigL does not benefit from starting again with the final topology and original initialization - training for twice as long is more effective than restarting the training . ( 2 ) We aren \u2019 t quite sure what experiments you want to see when you say compare to Lottery \u201c in its original setting \u201d . If we included lottery ticket experiments , the cost of doing Iterative Magnitude Pruning ( 5 times for 80 % sparse and 10 times for 90 % sparse ) would put the cost far off the scale of Figure 2 ( approximately all the way to the edge of the page ) . We also like to point out that , for all of our experiments , we include pruning results which are approximately an upper bound on the performance of lottery tickets at much lower computational cost ."}, {"review_id": "ryg7vA4tPB-2", "review_text": "This paper proposes a method for training sparse network without first training a dense network (e.g. the lottery ticket hypothesis or distillation). The method involves a combination of dynamic pruning of weights coupled with a dynamic \"growing\" of new weights given a novel criterion based on the magnitude of the gradient of the loss. As a result, networks can stay sparse throughout training and testing, leading to a large reduction in computational cost. The paper's approach of dynamically changing the topology of networks is an interesting and motivated idea that seems to work rather well. I also appreciate the experiments on MobileNet, a setting where one expects investigations into sparse network architectures to have significant application. Relatedly, I appreciate the importance of the fact that the computational cost of training and evaluating the network is proportional to the sparse model size, which is not normally true for masked dense models. Overall, I found the paper to be very clear and of high quality, and thus I find this to be an interest addition to investigations into the lottery ticket hypothesis. As the authors state, the novelty of their method is that they use the gradients with the highest magnitudes to grow connections. This is somewhat intuitive given the role gradients play in gradient descent based optimization, but I was wondering if they had any further intuition as to why this is the right criterion? In section 4.3, I was a bit confused by Figure 5. My understanding is that many paths between loss landscape minima follow nonlinear paths -- why is it at all significant that there's a linear barrier? Why are only quadric and cubic Bezier curves used, rather than a more general path finding algorithm? Overall, this is a nice paper that should be accepted to ICLR.", "rating": "6: Weak Accept", "reply_text": "We would like to thank Reviewer 2 for their time and address the points raised : ( 1 ) Appendix F ( now Appendix A ) \u201c Effect of Mask Updates on the Energy Landscape \u201d contains an explanation based on a Taylor series approximation of why magnitude pruning and largest gradient growth is optimal ( at least greedily ) . Unfortunately we did not originally reference this in the main text . We have updated the last paragraph of Section 4 with a short discussion and a pointer as the following : `` ... RigL first removes connections with the smallest magnitudes since removing these connections have been shown to have a minimal effect on the loss . Next , it activates connections with the high gradients , since these connections are expected to decrease the loss fastest . We hypothesize in Appendix A that RigL escapes bad critical points by replacing saddle directions with high gradient dimensions . '' ( 2 ) The linear path having a high loss is only to demonstrate that the connectivity is not simple - it is the expected result . We note that prior work ( https : //arxiv.org/pdf/1802.10026.pdf ) was able to use a quadratic Bezier curve to connect minima in dense networks , so it seemed a reasonable choice . Given N anchor points , the Bezier curves are capable of representing a more general path than the other technique we are aware of for connecting minima ( http : //proceedings.mlr.press/v80/draxler18a/draxler18a.pdf ) . In both cases we must solve an optimization problem involving all N anchor points simultaneously . We run out of memory when attempting to use a 4th order curve ."}], "0": {"review_id": "ryg7vA4tPB-0", "review_text": "This paper proposes a pruning technique called RigL, which performs sparse initialization of the network weights, and allows the network to grow weights during training. The sparse initialization makes the pruning algorithm to be memory- and computation- efficient, unlike existing models that starts with dense network weights. The authors train RigL with three different sparsity distributions that considers the number of input and output nodes, and validate them on various deep convnets for training on ImageNet, on which the model outperforms several existing sparsification methods and even dense counterparts. Pros - The proposed model, RigL, is memory- and computation- efficient, and thus allows to train a large network in an efficient manner. - RigL obtains impressive sparsification performance, even yielding sparse networks that outperform their dense counterparts. Cons - The idea of starting from a small, sparse network and expanding it is not novel. DEN [Yoon et al. 18] proposed the same bottom-up approach with sparsely initialized networks, while they allowed to increase the number of neurons at each layer and focused more on continual learning. The authors should compare the two methods both conceptually and experimentally. - The method is more like a set of heuristics rather than a principled approach, which makes it less appealing. This is not really an issue if the paper includes extensive experimental validation and in-depth analysis, but this is not the case. - The experimental validation is largely lacking, as the authors only perform experiments on ImageNet and do not compare against recent state-of-the-art Bayesian sparsification methods (SBP, VIB, L0-regularization). Without such extensive experimental validation, it is uncertain whether the result will generalize, given the highly empirical nature of the work. In sum, although I believe that the paper proposes a very practical method that is easy to implement and is promising, due to lack of experimental validation against a similar approach, state-of-the-art sparsification methods, and results on more datasets, I temporarily provide the rating of weak reject. I may change my opinion if the authors provide those results during the rebuttal period. [Yoon et al. 18] Lifelong learning with dynamically expandable networks, ICLR 2018", "rating": "3: Weak Reject", "reply_text": "We would like to thank Reviewer 1 and respond to their concerns below : ( 1 ) Thank you for the reference to DEN , it is an interesting work . However , there are some important differences between our work and DEN : ( a ) As emphasized in its abstract and in the introduction DEN attacks the problem of catastrophic forgetting in the continual learning setting and grows new neurons during training to efficiently learn each task in sequence . In contrast , we fix the number of available neurons in each layer and focus on sparse training of neural networks for a single task . In theory we could compare the cost of training with these two different approaches , however : ( b ) When extending continual learning ( as used by DEN ) to the split dataset regime it is non-trivial to match the performance of a standard training regime ( as used by RigL ) . For example , see http : //openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Large_Scale_Incremental_Learning_CVPR_2019_paper.pdf . We are not aware of work that is able to do this successfully , and would appreciate references so that we can possibly make a proper comparison . ( 2 ) We have added two new datasets and architectures to address concerns about the lack of experimental validation . We pushed some of the analysis to the Appendix due to length constraints . We would like to point out Appendix B ( Effect of Sparsity Distribution on Other Methods ) , C ( Effect of Momentum Coefficient for SNFS ) , D ( Existence of Lottery Tickets ) , E ( Effect of Update Schedules ) , F ( Alternative Update Schedules ) and H ( Additional Plots and Experiments for CIFAR-10 ) for extended analysis of our results . We have an explanation based on a Taylor series approximation why magnitude pruning and largest gradient growth is optimal ( at least greedily ) in Appendix F ( now Appendix A ) . Unfortunately we had originally forgotten to mention this in the main text , but we have now updated the last paragraph of Section 4 with a short discussion and a pointer to the appendix . ( 3 ) Regarding SBP ( we assume you mean \u201c Structured Bayesian Pruning \u201d https : //arxiv.org/abs/1705.07283 ) , VIB ( assuming \u201c Variational Information Bottleneck \u201d https : //arxiv.org/pdf/1802.10399.pdf ) , and L0-regularization ( https : //arxiv.org/pdf/1712.01312.pdf ) - we note that these techniques function as an architecture search over channel count distributions and are somewhat orthogonal to our own work . SBP and VIB do _at least_ as much work during training as training the large model they start with ( often quite a bit more as there are non-trivial overheads associated with these techniques ) . The resulting architectures can be trained from scratch ( https : //arxiv.org/abs/1810.05270 ) . Once these techniques find an improved architecture it could be trained with _weight_ sparsity using our technique . L0 does slightly reduce the cost during training , but the difference is not very large . According to Figure 4 of their paper , the cost is decreased from ~3.5 to 3.25 ( e11 ) flops by the end of training , or a reduction of 7 % . The total decrease over the course of training would be even less ."}, "1": {"review_id": "ryg7vA4tPB-1", "review_text": "Overview: The paper is dedicated to developing a more efficient and powerful dense-to-sparse training method. In order to break the limits of the size of the largest trainable sparse model to that of the largest trainable dense model, the author proposes a dynamic method that updates the network topology via parameter magnitudes and infrequent gradient calculation. In the experiments parts, they conduct extensive studies to show the proposed approach can surpass the previous sota with ResNet-50, MobileNet v1 and v2 on the imagenet 2012. What's more, the author also provides some intuitive explanation about why allowing topology change during the optimization is beneficial. Strength Bullets: 1. Due to the dynamic network topology, the paper's methods exactly achieve the memory and computation efficient. i) Required memory is only proportional to the size of the sparse model. ii) The amount of computation is proportional to the number of nonzero parameters in the model. 2. The author performs detailed comparison experiments among different sparsity distribution and different pruning methods. And the results overcome the previous state-of-the-art results. 3. Fig 5 shows some interesting insight. It suggests that static sparse training may be stuck at some local minima which are isolated from improved solutions. However, the dynamic update has a big chance to avoid this problem. Weakness Bullets: 1. The author claims that the ticket in the paper does not rely on a \"lucky\" initialization. But it doesn't exclude the possibility that starting from the original initial conditions may give a better performance. Even if the connection is dynamic, we can still record the initial point for each weight. It will be better the author can provide related analysis. 2. In my opinion, in order to prove dynamic pruning is better than static methods, the author needs to provide a comparison with the previous sota method in it's own setting. i.e. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks Recommendation: I think this paper is a novel work. Although it has some flaws in the experiment design, the motivation and experiment results are conniving enough. So, this is a weak accept.", "rating": "6: Weak Accept", "reply_text": "We appreciate Reviewer 3 's comments and address the weakness bullets below : ( 1 ) We add experiments that demonstrate that choosing the lottery initialization in this setting does not improve the results ( Appendix D ) . There are no special tickets . Rewiring RigL solution to the initial conditions gets 70.82 % average accuracy , which is only 0.2 % better than ` Static ` training . Results demonstrate that training with a fixed topology is significantly worse than training with RigL and that RigL does not benefit from starting again with the final topology and original initialization - training for twice as long is more effective than restarting the training . ( 2 ) We aren \u2019 t quite sure what experiments you want to see when you say compare to Lottery \u201c in its original setting \u201d . If we included lottery ticket experiments , the cost of doing Iterative Magnitude Pruning ( 5 times for 80 % sparse and 10 times for 90 % sparse ) would put the cost far off the scale of Figure 2 ( approximately all the way to the edge of the page ) . We also like to point out that , for all of our experiments , we include pruning results which are approximately an upper bound on the performance of lottery tickets at much lower computational cost ."}, "2": {"review_id": "ryg7vA4tPB-2", "review_text": "This paper proposes a method for training sparse network without first training a dense network (e.g. the lottery ticket hypothesis or distillation). The method involves a combination of dynamic pruning of weights coupled with a dynamic \"growing\" of new weights given a novel criterion based on the magnitude of the gradient of the loss. As a result, networks can stay sparse throughout training and testing, leading to a large reduction in computational cost. The paper's approach of dynamically changing the topology of networks is an interesting and motivated idea that seems to work rather well. I also appreciate the experiments on MobileNet, a setting where one expects investigations into sparse network architectures to have significant application. Relatedly, I appreciate the importance of the fact that the computational cost of training and evaluating the network is proportional to the sparse model size, which is not normally true for masked dense models. Overall, I found the paper to be very clear and of high quality, and thus I find this to be an interest addition to investigations into the lottery ticket hypothesis. As the authors state, the novelty of their method is that they use the gradients with the highest magnitudes to grow connections. This is somewhat intuitive given the role gradients play in gradient descent based optimization, but I was wondering if they had any further intuition as to why this is the right criterion? In section 4.3, I was a bit confused by Figure 5. My understanding is that many paths between loss landscape minima follow nonlinear paths -- why is it at all significant that there's a linear barrier? Why are only quadric and cubic Bezier curves used, rather than a more general path finding algorithm? Overall, this is a nice paper that should be accepted to ICLR.", "rating": "6: Weak Accept", "reply_text": "We would like to thank Reviewer 2 for their time and address the points raised : ( 1 ) Appendix F ( now Appendix A ) \u201c Effect of Mask Updates on the Energy Landscape \u201d contains an explanation based on a Taylor series approximation of why magnitude pruning and largest gradient growth is optimal ( at least greedily ) . Unfortunately we did not originally reference this in the main text . We have updated the last paragraph of Section 4 with a short discussion and a pointer as the following : `` ... RigL first removes connections with the smallest magnitudes since removing these connections have been shown to have a minimal effect on the loss . Next , it activates connections with the high gradients , since these connections are expected to decrease the loss fastest . We hypothesize in Appendix A that RigL escapes bad critical points by replacing saddle directions with high gradient dimensions . '' ( 2 ) The linear path having a high loss is only to demonstrate that the connectivity is not simple - it is the expected result . We note that prior work ( https : //arxiv.org/pdf/1802.10026.pdf ) was able to use a quadratic Bezier curve to connect minima in dense networks , so it seemed a reasonable choice . Given N anchor points , the Bezier curves are capable of representing a more general path than the other technique we are aware of for connecting minima ( http : //proceedings.mlr.press/v80/draxler18a/draxler18a.pdf ) . In both cases we must solve an optimization problem involving all N anchor points simultaneously . We run out of memory when attempting to use a 4th order curve ."}}