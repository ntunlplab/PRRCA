{"year": "2019", "forum": "r1lyTjAqYX", "title": "Recurrent Experience Replay in Distributed Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "The paper proposes a new distributed DQN algorithm that combines recurrent neural networks with distributed prioritized replay memory. The authors systematically compare three types of initialization strategies for training the recurrent models. The thorough investigation is cited as a valuable contribution by all reviewers, with reviewer 1 noting that the study would be of interest to \"anyone using recurrent networks on RL tasks\". Empirical results on Atari and DMLab are impressive.\n\nThe reviewers noted several weaknesses in their original reviews. These included issues of clarity, a need for more detailed ablation studies, and need to more carefully document the empirical setup. A further question was raised on whether the empirical results could be complemented with theoretical or conceptual insights.\n\nThe authors carefully addressed all concerns raised during the reviewing and rebuttal period. They took exceptional care to clarify their writing, document experiment details, and ran a large set of additional experiments as suggested by the reviewers. The AC feels that the review period for the paper was particularly productive and would like to thank the reviewers and authors.\n\nThe reviewers and AC agree that the paper makes a significant contribution to the field and should be accepted.", "reviews": [{"review_id": "r1lyTjAqYX-0", "review_text": "Summary: Leveraging on recent advances on distributed training of RL agents, the paper proposes the analysis of RNN-based RL agents with experience replay (i.e., integrating the time dependencies through RNN). Precisely, the authors empirically compare a state-of-the-art training strategy (called zero start state) with three proposed training strategies (namely; zero-state with burn-in, stored-state and stored-state with burn-in). By comparing these different strategies through a proposed metric (Q-value discrepancy), the authors conclude on the effectiveness of the stored-state with burn-in strategy which they consider for the training of their proposed Recurrent Replay Distributed DQN (R2D2) agent. The proposed analysis is well-motivated and has lead to significant results w.r.t. the state-of-the-art performances of RL agents. Major concerns: My major concerns are three-fold: - The authors do not provide enough details about some \"informal\" experiments which are sometimes important to convince the reader about the relevance of the suggested insights (e.g., line 3 page 5). Beyond this point, the paper is generally hard to follow and reorganizing some sections (e.g., sec. 2.3 should appear after sec. 3 as it contains a lot of technical details) would certainly make the reading of the paper easier. - Hausknecht & Stone (2015) have proposed two training strategies (zero-state and Replaying whole episode trajectories see sec. 3 page 3). The authors should clarify why they did not considered the other states in their study. - The authors present results (mainly, fig. 2 and fig. 3) suggesting that the proposed R2D2 agent outperform the agents Ape-X and IMPALA, where R2D2 is trained using the aforementioned stored-state with burn-in strategy. It is not clear which are the considered training strategies adopted for the (compared to) state-of-the-art agents (Ape-X and IMPALA). The authors should clarify more precisely this point. Minor concerns: - The authors compare the different strategies only in terms of their proposed Q-value discrepancy metric. It could be interesting to consider other metrics in order to evaluate the ability of the methods on common aspects. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for raising these concerns . We have attempted to address them in this revision . \u201c The authors do not provide enough details about some `` informal '' experiments ... \u201d We have now significantly revised our LSTM training analysis to include a more detailed study that shows representation drift measured by both parameter lag ( number of updates since experience was generated ) and the q-value discrepancy , and for the same runs the mean episodic return , some of which is contained in the appendix . Additionally , we now have results as we vary burn-in from zero to 20 and up to 40 steps . We think this improves the section quite a bit , but we are still looking at edits to the paper to improve clarity further . \u201c Beyond this point , the paper is generally hard to follow and reorganizing some sections ( e.g. , sec.2.3 should appear after sec.3 as it contains a lot of technical details ) ... \u201d We have moved one of these sections to the appendix and tried to improve the flow of the paper . Please let us know if this makes for a clearer read . \u201c Hausknecht & Stone ( 2015 ) have proposed two training strategies ( zero-state and Replaying whole episode trajectories see sec.3 page 3 ) . The authors should clarify\u2026 \u201d Hausknecht and Stone ( 2015 ) argued that the two strategies performed similarly in their experiments . We agree that a more thorough investigation of whole-trajectory-based training would be valuable , with special attention to sample correlation , variance and optimization settings . However , this seems to exceed the scope of the paper . \u201c The authors present results ( mainly , fig.2 and fig . 3 ) suggesting that the proposed R2D2 agent outperform the agents Ape-X and IMPALA , where R2D2 is trained using the aforementioned stored-state with burn-in strategy . It is not clear\u2026 \u201d In all R2D2 experiments outside of the initial analysis section ( where we specifically study these methods ) we used the stored-state with 40-step burn-in method . We have attempted to make this more explicit in the revision . Ape-X does not use an RNN , and IMPALA is perhaps most like the `` entire episode trajectories '' approach in Hausknecht and Stone , but due to using the mostly on-policy actor-critic is hard to compare directly . To avoid any confusion , the reported Ape-X and IMPALA results are not our own reruns of the algorithms , but taken from their respective publications or private communication with the authors . \u201c The authors compare the different strategies only in terms of their proposed Q-value discrepancy metric . It could be interesting to consider other metrics in order to evaluate the ability of the methods on common aspects. \u201d As mentioned above , we have attempted to significantly improve this by including more information ."}, {"review_id": "r1lyTjAqYX-1", "review_text": "In this submission, the authors investigate using recurrent networks in distributed DQN with prioritized experience replay on the Atari and DMLab benchmarks. They experiment with several strategies to initialize the recurrent state when processing a sub-sequence sampled from the replay buffer: the best one consists in re-using the initial state computed when the sequence was originally played (even if it may now be outdated) but not doing any network update during the first k steps of the sequence (\u201cburn-in\u201d period). Using this scheme with LSTM units on top of traditional convolutional layers, along with a discount factor gamma = 0.997, leads to a significant improvement on Atari over the previous state-of-the-art, and competitive performance on DMLab. The proposed technique (dubbed R2D2) is not particularly original (it is essentially \u201cjust\u201d using RNNs in Ape-X), but experiments are thorough, investigating several important aspects related to recurrence and memory to validate the approach. These findings are definitely quite relevant to anyone using recurrent networks on RL tasks. The results on Atari are particularly impressive and should be of high interest to researchers working on this benchmark. The fact that the same network architecture and hyper-parameters also work pretty well on DMLab is encouraging w.r.t. the generality of the method. I do have a couple of important concerns though. The first one is that a few potentially important changes were made to the \u201ctraditional\u201d settings typically used on Atari, which makes it difficult to perform a fair comparison to previously published results. Using gamma = 0.997 could by itself provide a significant boost, as hinted by results from \u201cMeta-Gradient Reinforcement Learning\u201d (where increasing gamma improved results significantly compared to the usual 0.99). Other potentially impactful changes are the absence of reward clipping (replaced with a rescaling scheme) and episodes not ending with life loss: I am not sure whether these make the task easier or harder, but they certainly change it to some extent (the \u201cdespite this\u201d above 5.1 suggests it would be harder, but this is not shown empirically). Fortunately, this concern is partially alleviated by Section 6 that shows feedforward networks do not perform as well as recurrent ones, but this is only verified on 5 games: a full benchmark comparison would have been more reassuring (as well as running R2D2 with more \u201cstandard\u201d Atari settings, even if it would mean using different hyper-parameters on DMLab). The second important issue I see is that the authors do not seem to plan to share their code to reproduce their results. Given how time consuming and costly it is to run such experiments, and all potentially tricky implementation details (especially when dealing with recurrent networks), making this code available would be tremendously helpful to the research community (particularly since this paper claims a new SOTA on Atari). I am not giving too much weight to this issue in my review score since (unfortunately) the ICLR reviewer guidelines do not explicitly mention code sharing as a criterion, but I strongly hope the authors will consider it. Besides the above, I have a few additional small questions: 1. \u201cWe also found no benefit from using the importance weighting that has been typically applied with prioritized replay\u201d: this is potentially surprising since this could be \u201cwrong\u201d, mathematically speaking. Do you think this is because of the lack of stochasticity in the environments? (I know Atari is deterministic, but I am not sure about DMLab) 2. Fig. 3 (left) shows R2D2 struggling on some DMLab tasks. Do you have any idea why? The caption of Table 3 in the Appendix suggests the absence of specific reward clipping may be an issue for some tasks, but have you tried adding it back? I also wonder if maybe training a unique network per task may make DMLab harder, since IMPALA has shown some transfer learning occurring between DMLab tasks? (although the comparison might be to the \u201cdeep experts\u201d version of IMPALA \u2014 this is not clear in Fig. 3 \u2014 in which case this last question would be irrelevant) 3. In Table 1, where do the IMPALA (PBT) numbers on DMLab come from? Looking at the current arxiv version of their paper, their Fig. 4 shows it goes above 70% in mean capped score, while your Table 1 reports only 61.5%. I also can\u2019t find a median score being reported on DMLab in their paper, did you try to compute it from their Fig. 9? And why don\u2019t you report their results on Atari? 4. Table 4\u2019s caption mentions \u201c30 no-op starts\u201d but you actually used the standard \u201crandom starts\u201d setting, right? (not a fixed number of 30 no-ops) And finally a few minor comments / suggestions: - In the equation at bottom of p. 2, it seems like theta and theta- (the target network) have been accidentally swapped (at least compared to the traditional double DQN formula) - At top of p. 3 I guess \\bar{delta}_i is the mean of the delta_i\u2019s, but then the index i should be removed - In Fig. 1 (left) please clarify which training phase these stats are computed on (whole training? beginning / middle / end?) - p. 4, \u201cthe true stored recurrent states at each step\u201d: \u201ctrue\u201d is a bit misleading here as it can be interpreted as \u201cthe states one would obtain by re-processing the whole episode from scratch with the current network\u201d => I would suggest to remove it, or to change it (e.g. \u201cpreviously\u201d). By the way, I think it would have been interesting to also compare to these states recomputed \u201cfrom scratch\u201d, since they are the actual ground truth. - I think you should mention in Table 1\u2019s caption that the PBT IMPALA is a single network trained to solve all tasks - Typo at bottom of p. 7, \u201cIndeed, Table 1 that even...\u201d Update: score updated to 8 (from 7) following discussion below", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your comments and in particular your concerns around the use of importance weighting . We took your concerns to heart and have ( as we discuss below ) included it and rerun the experiments . \u201c The fact that the same network architecture and hyper-parameters also work pretty well on DMLab is encouraging w.r.t.the generality of the method. \u201d We want to thank the reviewer for making note of this aspect . It is something we consider particularly noteworthy considering common problems with robustness in deep RL . \u201c \u2026 a couple of important concerns though . The first one is that a few potentially important changes were made to the \u201c traditional \u201d settings typically used on Atari , which makes it difficult to perform a fair comparison to previously published results. \u201d This is a very reasonable concern and we have run a more thorough set of ablations on R2D2 which have now been included in the latest revision . These are not 100 % completed yet , but are far enough along to give a clear picture . Specifically , we are taking your recommendations and comparing R2D2 with ( 1 ) Feed-forward only ( already included , but now also done over all 57 Atari games ) , ( 2 ) Reward clipping but no value function rescaling , ( 3 ) Smaller discount ( gamma = 0.99 ) , and ( 4 ) end-episode on life-loss enabled . Only the last one is not included in the current revision , but will be included before revisions close . \u201c The second important issue I see is that the authors do not seem to plan to share their code to reproduce their results . Given how time consuming and costly it is to run such experiments , and all potentially tricky implementation details ( especially when dealing with recurrent networks ) , making this code available would be tremendously helpful to the research community ( particularly since this paper claims a new SOTA on Atari ) ... I strongly hope the authors will consider it. \u201d Distributed training , in particular , is an area where publically available open source code goes a long way towards reproducibility and progress in the field . Although we are not able to immediately release the code , we believe that we will be able to make the source available in the future . \u201c 1. \u201c We also found no benefit from using the importance weighting that has been typically applied with prioritized replay \u201d : this is potentially surprising since this could be \u201c wrong \u201d , mathematically speaking . Do you think this is because of the lack of stochasticity in the environments ? ( I know Atari is deterministic , but I am not sure about DMLab ) \u201d Thank you for pointing this out ! This does make sense and we agree that in principle the lack of importance weighting when using prioritized replay is not well supported . We have now included it in the algorithm and rerun almost all of our experiments ( ablations are still in progress ) . We have not found it necessary to retune hyper-parameters to support this change , and in fact found that re-introducing importance weighting did stabilise training on some of the DMLab levels and slightly improved overall performance . \u201c 2.Fig.3 ( left ) shows R2D2 struggling on some DMLab tasks . Do you have any idea why ? \u201c We do not use the asymmetric reward clipping of IMPALA and believe that this clipping is most helpful in some of the language tasks . We are in the process of running a very small test in which we add the asymmetric clipping to verify , but do not plan to add it to the algorithm itself in the interest of generality . Additionally , one of the larger benefits of IMPALA PBT is the use of population-based training , and we suspect this is another reason for IMPALA occasionally out-performing R2D2 . \u201c 3.In Table 1 , where do the IMPALA ( PBT ) numbers on DMLab come from ? \u201d We obtained the IMPALA results data from the authors of the cited paper : \u201c Multi-task Deep Reinforcement Learning with PopArt \u201d . However , after further discussion we believe the best approach would be to rerun IMPALA on our same hardware and training regime . Until this completes we will use the provided data , but again , we hope to replace this before the final revision . \u201c And finally a few minor comments / suggestions : \u201d Thank you for these comments and suggestions , we have made the corresponding edits to clarify things and fix mistakes . We should also mention that while doing this we fixed a bug that limited Atari training episode times to 14 minutes ( 50K frames ) instead of 30 minutes ( 108K frames ) , this slightly improves some of our Atari results ."}, {"review_id": "r1lyTjAqYX-2", "review_text": "This paper investigates the use of recurrent NNs in distributted RL settings as a clear improvement of the feed-forward NN variations in partially observed environments. The authors present \"R2DR\" algorithm as a A+B approach from previous works (actually, R2D2 is an Ape-X-like agent using LTSM), as well as an empirical study of a number of ways for training RNN from replay in terms of the effects of parameter lag (and potential alleviating actions) and sample-afficiency. The results presented show impressive performance in Atari-57 and DMLab-30 benchmarks. In summary, this is a very nice paper in which the authors attack a challenging task and empirically confirm that RNN agents generalise far better when scaling up through parallelisation and distributed training allows them to benefit from huge experience. The results obtained in ALE and DMLab improves significantly upon the SOTA works, showing that the trend-line in those benchmarks seem to have been broken. Furthermore, the paper presents their approach/analyses in a well-structured manner and sufficient clarity to retrace the essential contribution. The background and results are well-contextualised with relevant related work. My only major comments are that I\u2019m a bit skeptical about the lack of a more thorough (theoretical) analysis supporting their empirical findings (what gives me food for thought is that LSTM helps that much on even fully observable games such as Ms. Pacman); and the usual caveats regarding evaluation: evaluation conditions aren't well standardized so the different systems (Ape-X, IMPALA, Reactor, Rainbow, AC3 Gorilla, C51, etc.) aren't all comparable. These sort of papers would benefit from a more formal/comprehensive evaluation by means of an explicit enumeration of all the dimensions relevant for their analysis: the data, the knowledge, the software, the hardware, manipulation, computation and, of course, performance, etc. However only some of then are (partially) provided. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments and suggestions . \u201c ... only major comments are that I \u2019 m a bit skeptical about the lack of a more thorough ( theoretical ) analysis supporting their empirical findings ( what gives me food for thought is that LSTM helps that much on even fully observable games such as Ms. Pacman ) ; \u201d We do not have theoretical contributions to add in our rebuttal , but would like to offer some additional resources for understanding the empirical findings . In addition to the more thorough reporting of results now included , as well as now including 3 seeds in all R2D2-based experiments , we have uploaded videos of the agent \u2019 s learned policy on a handful of Atari games . What we observe in these videos is that R2D2 has learned to leverage memory in Atari in unexpected ways . That is , Atari * does * directly benefit from long-term memory in several specific cases . For example , in MsPacman the agent learns to precisely time the ghosts \u2019 vulnerability in order to obtain well timed multi-ghost-meals , yielding much higher scores . Please note these videos are uploaded anonymously to youtube and marked unlisted , which should prevent us from inferring any geographic information about reviewers who view them . MsPacman R2D2 : https : //youtu.be/eexCo9wHqfU R2D2 Feed-Forward : https : //youtu.be/stI08CZlKqo SeaQuest R2D2 : https : //youtu.be/5Umrkdis8OY R2D2 Feed-Forward : https : //youtu.be/8o1LcK_3S3U QBert R2D2 : https : //youtu.be/UUn_vXj89Ps R2D2 Feed-Forward : https : //youtu.be/UUn_vXj89Ps \u201c and the usual caveats regarding evaluation\u2026 \u201d To this end we have run additional ablations on our architectural choices and are in the process of rerunning IMPALA on the same hardware and training regime as we used for R2D2 . We have also attempted to clarify our exact evaluation regime by pointing out the 30-minute episode timeout on Atari , the fact that our results are final scores ( not max-over-training as have been reported for e.g.Ape-X ) , and other evaluation details ."}], "0": {"review_id": "r1lyTjAqYX-0", "review_text": "Summary: Leveraging on recent advances on distributed training of RL agents, the paper proposes the analysis of RNN-based RL agents with experience replay (i.e., integrating the time dependencies through RNN). Precisely, the authors empirically compare a state-of-the-art training strategy (called zero start state) with three proposed training strategies (namely; zero-state with burn-in, stored-state and stored-state with burn-in). By comparing these different strategies through a proposed metric (Q-value discrepancy), the authors conclude on the effectiveness of the stored-state with burn-in strategy which they consider for the training of their proposed Recurrent Replay Distributed DQN (R2D2) agent. The proposed analysis is well-motivated and has lead to significant results w.r.t. the state-of-the-art performances of RL agents. Major concerns: My major concerns are three-fold: - The authors do not provide enough details about some \"informal\" experiments which are sometimes important to convince the reader about the relevance of the suggested insights (e.g., line 3 page 5). Beyond this point, the paper is generally hard to follow and reorganizing some sections (e.g., sec. 2.3 should appear after sec. 3 as it contains a lot of technical details) would certainly make the reading of the paper easier. - Hausknecht & Stone (2015) have proposed two training strategies (zero-state and Replaying whole episode trajectories see sec. 3 page 3). The authors should clarify why they did not considered the other states in their study. - The authors present results (mainly, fig. 2 and fig. 3) suggesting that the proposed R2D2 agent outperform the agents Ape-X and IMPALA, where R2D2 is trained using the aforementioned stored-state with burn-in strategy. It is not clear which are the considered training strategies adopted for the (compared to) state-of-the-art agents (Ape-X and IMPALA). The authors should clarify more precisely this point. Minor concerns: - The authors compare the different strategies only in terms of their proposed Q-value discrepancy metric. It could be interesting to consider other metrics in order to evaluate the ability of the methods on common aspects. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for raising these concerns . We have attempted to address them in this revision . \u201c The authors do not provide enough details about some `` informal '' experiments ... \u201d We have now significantly revised our LSTM training analysis to include a more detailed study that shows representation drift measured by both parameter lag ( number of updates since experience was generated ) and the q-value discrepancy , and for the same runs the mean episodic return , some of which is contained in the appendix . Additionally , we now have results as we vary burn-in from zero to 20 and up to 40 steps . We think this improves the section quite a bit , but we are still looking at edits to the paper to improve clarity further . \u201c Beyond this point , the paper is generally hard to follow and reorganizing some sections ( e.g. , sec.2.3 should appear after sec.3 as it contains a lot of technical details ) ... \u201d We have moved one of these sections to the appendix and tried to improve the flow of the paper . Please let us know if this makes for a clearer read . \u201c Hausknecht & Stone ( 2015 ) have proposed two training strategies ( zero-state and Replaying whole episode trajectories see sec.3 page 3 ) . The authors should clarify\u2026 \u201d Hausknecht and Stone ( 2015 ) argued that the two strategies performed similarly in their experiments . We agree that a more thorough investigation of whole-trajectory-based training would be valuable , with special attention to sample correlation , variance and optimization settings . However , this seems to exceed the scope of the paper . \u201c The authors present results ( mainly , fig.2 and fig . 3 ) suggesting that the proposed R2D2 agent outperform the agents Ape-X and IMPALA , where R2D2 is trained using the aforementioned stored-state with burn-in strategy . It is not clear\u2026 \u201d In all R2D2 experiments outside of the initial analysis section ( where we specifically study these methods ) we used the stored-state with 40-step burn-in method . We have attempted to make this more explicit in the revision . Ape-X does not use an RNN , and IMPALA is perhaps most like the `` entire episode trajectories '' approach in Hausknecht and Stone , but due to using the mostly on-policy actor-critic is hard to compare directly . To avoid any confusion , the reported Ape-X and IMPALA results are not our own reruns of the algorithms , but taken from their respective publications or private communication with the authors . \u201c The authors compare the different strategies only in terms of their proposed Q-value discrepancy metric . It could be interesting to consider other metrics in order to evaluate the ability of the methods on common aspects. \u201d As mentioned above , we have attempted to significantly improve this by including more information ."}, "1": {"review_id": "r1lyTjAqYX-1", "review_text": "In this submission, the authors investigate using recurrent networks in distributed DQN with prioritized experience replay on the Atari and DMLab benchmarks. They experiment with several strategies to initialize the recurrent state when processing a sub-sequence sampled from the replay buffer: the best one consists in re-using the initial state computed when the sequence was originally played (even if it may now be outdated) but not doing any network update during the first k steps of the sequence (\u201cburn-in\u201d period). Using this scheme with LSTM units on top of traditional convolutional layers, along with a discount factor gamma = 0.997, leads to a significant improvement on Atari over the previous state-of-the-art, and competitive performance on DMLab. The proposed technique (dubbed R2D2) is not particularly original (it is essentially \u201cjust\u201d using RNNs in Ape-X), but experiments are thorough, investigating several important aspects related to recurrence and memory to validate the approach. These findings are definitely quite relevant to anyone using recurrent networks on RL tasks. The results on Atari are particularly impressive and should be of high interest to researchers working on this benchmark. The fact that the same network architecture and hyper-parameters also work pretty well on DMLab is encouraging w.r.t. the generality of the method. I do have a couple of important concerns though. The first one is that a few potentially important changes were made to the \u201ctraditional\u201d settings typically used on Atari, which makes it difficult to perform a fair comparison to previously published results. Using gamma = 0.997 could by itself provide a significant boost, as hinted by results from \u201cMeta-Gradient Reinforcement Learning\u201d (where increasing gamma improved results significantly compared to the usual 0.99). Other potentially impactful changes are the absence of reward clipping (replaced with a rescaling scheme) and episodes not ending with life loss: I am not sure whether these make the task easier or harder, but they certainly change it to some extent (the \u201cdespite this\u201d above 5.1 suggests it would be harder, but this is not shown empirically). Fortunately, this concern is partially alleviated by Section 6 that shows feedforward networks do not perform as well as recurrent ones, but this is only verified on 5 games: a full benchmark comparison would have been more reassuring (as well as running R2D2 with more \u201cstandard\u201d Atari settings, even if it would mean using different hyper-parameters on DMLab). The second important issue I see is that the authors do not seem to plan to share their code to reproduce their results. Given how time consuming and costly it is to run such experiments, and all potentially tricky implementation details (especially when dealing with recurrent networks), making this code available would be tremendously helpful to the research community (particularly since this paper claims a new SOTA on Atari). I am not giving too much weight to this issue in my review score since (unfortunately) the ICLR reviewer guidelines do not explicitly mention code sharing as a criterion, but I strongly hope the authors will consider it. Besides the above, I have a few additional small questions: 1. \u201cWe also found no benefit from using the importance weighting that has been typically applied with prioritized replay\u201d: this is potentially surprising since this could be \u201cwrong\u201d, mathematically speaking. Do you think this is because of the lack of stochasticity in the environments? (I know Atari is deterministic, but I am not sure about DMLab) 2. Fig. 3 (left) shows R2D2 struggling on some DMLab tasks. Do you have any idea why? The caption of Table 3 in the Appendix suggests the absence of specific reward clipping may be an issue for some tasks, but have you tried adding it back? I also wonder if maybe training a unique network per task may make DMLab harder, since IMPALA has shown some transfer learning occurring between DMLab tasks? (although the comparison might be to the \u201cdeep experts\u201d version of IMPALA \u2014 this is not clear in Fig. 3 \u2014 in which case this last question would be irrelevant) 3. In Table 1, where do the IMPALA (PBT) numbers on DMLab come from? Looking at the current arxiv version of their paper, their Fig. 4 shows it goes above 70% in mean capped score, while your Table 1 reports only 61.5%. I also can\u2019t find a median score being reported on DMLab in their paper, did you try to compute it from their Fig. 9? And why don\u2019t you report their results on Atari? 4. Table 4\u2019s caption mentions \u201c30 no-op starts\u201d but you actually used the standard \u201crandom starts\u201d setting, right? (not a fixed number of 30 no-ops) And finally a few minor comments / suggestions: - In the equation at bottom of p. 2, it seems like theta and theta- (the target network) have been accidentally swapped (at least compared to the traditional double DQN formula) - At top of p. 3 I guess \\bar{delta}_i is the mean of the delta_i\u2019s, but then the index i should be removed - In Fig. 1 (left) please clarify which training phase these stats are computed on (whole training? beginning / middle / end?) - p. 4, \u201cthe true stored recurrent states at each step\u201d: \u201ctrue\u201d is a bit misleading here as it can be interpreted as \u201cthe states one would obtain by re-processing the whole episode from scratch with the current network\u201d => I would suggest to remove it, or to change it (e.g. \u201cpreviously\u201d). By the way, I think it would have been interesting to also compare to these states recomputed \u201cfrom scratch\u201d, since they are the actual ground truth. - I think you should mention in Table 1\u2019s caption that the PBT IMPALA is a single network trained to solve all tasks - Typo at bottom of p. 7, \u201cIndeed, Table 1 that even...\u201d Update: score updated to 8 (from 7) following discussion below", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your comments and in particular your concerns around the use of importance weighting . We took your concerns to heart and have ( as we discuss below ) included it and rerun the experiments . \u201c The fact that the same network architecture and hyper-parameters also work pretty well on DMLab is encouraging w.r.t.the generality of the method. \u201d We want to thank the reviewer for making note of this aspect . It is something we consider particularly noteworthy considering common problems with robustness in deep RL . \u201c \u2026 a couple of important concerns though . The first one is that a few potentially important changes were made to the \u201c traditional \u201d settings typically used on Atari , which makes it difficult to perform a fair comparison to previously published results. \u201d This is a very reasonable concern and we have run a more thorough set of ablations on R2D2 which have now been included in the latest revision . These are not 100 % completed yet , but are far enough along to give a clear picture . Specifically , we are taking your recommendations and comparing R2D2 with ( 1 ) Feed-forward only ( already included , but now also done over all 57 Atari games ) , ( 2 ) Reward clipping but no value function rescaling , ( 3 ) Smaller discount ( gamma = 0.99 ) , and ( 4 ) end-episode on life-loss enabled . Only the last one is not included in the current revision , but will be included before revisions close . \u201c The second important issue I see is that the authors do not seem to plan to share their code to reproduce their results . Given how time consuming and costly it is to run such experiments , and all potentially tricky implementation details ( especially when dealing with recurrent networks ) , making this code available would be tremendously helpful to the research community ( particularly since this paper claims a new SOTA on Atari ) ... I strongly hope the authors will consider it. \u201d Distributed training , in particular , is an area where publically available open source code goes a long way towards reproducibility and progress in the field . Although we are not able to immediately release the code , we believe that we will be able to make the source available in the future . \u201c 1. \u201c We also found no benefit from using the importance weighting that has been typically applied with prioritized replay \u201d : this is potentially surprising since this could be \u201c wrong \u201d , mathematically speaking . Do you think this is because of the lack of stochasticity in the environments ? ( I know Atari is deterministic , but I am not sure about DMLab ) \u201d Thank you for pointing this out ! This does make sense and we agree that in principle the lack of importance weighting when using prioritized replay is not well supported . We have now included it in the algorithm and rerun almost all of our experiments ( ablations are still in progress ) . We have not found it necessary to retune hyper-parameters to support this change , and in fact found that re-introducing importance weighting did stabilise training on some of the DMLab levels and slightly improved overall performance . \u201c 2.Fig.3 ( left ) shows R2D2 struggling on some DMLab tasks . Do you have any idea why ? \u201c We do not use the asymmetric reward clipping of IMPALA and believe that this clipping is most helpful in some of the language tasks . We are in the process of running a very small test in which we add the asymmetric clipping to verify , but do not plan to add it to the algorithm itself in the interest of generality . Additionally , one of the larger benefits of IMPALA PBT is the use of population-based training , and we suspect this is another reason for IMPALA occasionally out-performing R2D2 . \u201c 3.In Table 1 , where do the IMPALA ( PBT ) numbers on DMLab come from ? \u201d We obtained the IMPALA results data from the authors of the cited paper : \u201c Multi-task Deep Reinforcement Learning with PopArt \u201d . However , after further discussion we believe the best approach would be to rerun IMPALA on our same hardware and training regime . Until this completes we will use the provided data , but again , we hope to replace this before the final revision . \u201c And finally a few minor comments / suggestions : \u201d Thank you for these comments and suggestions , we have made the corresponding edits to clarify things and fix mistakes . We should also mention that while doing this we fixed a bug that limited Atari training episode times to 14 minutes ( 50K frames ) instead of 30 minutes ( 108K frames ) , this slightly improves some of our Atari results ."}, "2": {"review_id": "r1lyTjAqYX-2", "review_text": "This paper investigates the use of recurrent NNs in distributted RL settings as a clear improvement of the feed-forward NN variations in partially observed environments. The authors present \"R2DR\" algorithm as a A+B approach from previous works (actually, R2D2 is an Ape-X-like agent using LTSM), as well as an empirical study of a number of ways for training RNN from replay in terms of the effects of parameter lag (and potential alleviating actions) and sample-afficiency. The results presented show impressive performance in Atari-57 and DMLab-30 benchmarks. In summary, this is a very nice paper in which the authors attack a challenging task and empirically confirm that RNN agents generalise far better when scaling up through parallelisation and distributed training allows them to benefit from huge experience. The results obtained in ALE and DMLab improves significantly upon the SOTA works, showing that the trend-line in those benchmarks seem to have been broken. Furthermore, the paper presents their approach/analyses in a well-structured manner and sufficient clarity to retrace the essential contribution. The background and results are well-contextualised with relevant related work. My only major comments are that I\u2019m a bit skeptical about the lack of a more thorough (theoretical) analysis supporting their empirical findings (what gives me food for thought is that LSTM helps that much on even fully observable games such as Ms. Pacman); and the usual caveats regarding evaluation: evaluation conditions aren't well standardized so the different systems (Ape-X, IMPALA, Reactor, Rainbow, AC3 Gorilla, C51, etc.) aren't all comparable. These sort of papers would benefit from a more formal/comprehensive evaluation by means of an explicit enumeration of all the dimensions relevant for their analysis: the data, the knowledge, the software, the hardware, manipulation, computation and, of course, performance, etc. However only some of then are (partially) provided. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments and suggestions . \u201c ... only major comments are that I \u2019 m a bit skeptical about the lack of a more thorough ( theoretical ) analysis supporting their empirical findings ( what gives me food for thought is that LSTM helps that much on even fully observable games such as Ms. Pacman ) ; \u201d We do not have theoretical contributions to add in our rebuttal , but would like to offer some additional resources for understanding the empirical findings . In addition to the more thorough reporting of results now included , as well as now including 3 seeds in all R2D2-based experiments , we have uploaded videos of the agent \u2019 s learned policy on a handful of Atari games . What we observe in these videos is that R2D2 has learned to leverage memory in Atari in unexpected ways . That is , Atari * does * directly benefit from long-term memory in several specific cases . For example , in MsPacman the agent learns to precisely time the ghosts \u2019 vulnerability in order to obtain well timed multi-ghost-meals , yielding much higher scores . Please note these videos are uploaded anonymously to youtube and marked unlisted , which should prevent us from inferring any geographic information about reviewers who view them . MsPacman R2D2 : https : //youtu.be/eexCo9wHqfU R2D2 Feed-Forward : https : //youtu.be/stI08CZlKqo SeaQuest R2D2 : https : //youtu.be/5Umrkdis8OY R2D2 Feed-Forward : https : //youtu.be/8o1LcK_3S3U QBert R2D2 : https : //youtu.be/UUn_vXj89Ps R2D2 Feed-Forward : https : //youtu.be/UUn_vXj89Ps \u201c and the usual caveats regarding evaluation\u2026 \u201d To this end we have run additional ablations on our architectural choices and are in the process of rerunning IMPALA on the same hardware and training regime as we used for R2D2 . We have also attempted to clarify our exact evaluation regime by pointing out the 30-minute episode timeout on Atari , the fact that our results are final scores ( not max-over-training as have been reported for e.g.Ape-X ) , and other evaluation details ."}}