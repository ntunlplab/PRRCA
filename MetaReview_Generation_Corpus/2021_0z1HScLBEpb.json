{"year": "2021", "forum": "0z1HScLBEpb", "title": "UneVEn: Universal Value Exploration for Multi-Agent Reinforcement Learning", "decision": "Reject", "meta_review": "This paper adapts the ideas around universal successor features for decentralised multi-agent environments, with a particular emphasis on deriving better exploration from them. Like most of the reviewers, I think this is indeed a promising research direction. Given the complexity of the endeavour however, it may take a few more steps until the empirical evidence can back up the authors' ambition: the reviewers' consensus on the current version of the paper is that it is not ready for publication yet.", "reviews": [{"review_id": "0z1HScLBEpb-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper studies the problem setting of cooperative multi-agent RL ( coop-MARL ) under centralized training with decentralized execution ( CTDE ) . Additionally , it assumes that the reward function is known and is represented as a weighted linear combination of a basis function . They consider Q-learning with value-decomposition as the basis for tackling such problems and improve upon SOTA methods for coop-MARL in the domain of predator-prey ( PP ) . Specifically , they claim to improve upon monotonic value-decomposition methods such as VDN and QMIX by better addressing the issue of relative overgeneralization which stems from their monotonicity constraint in representing the joint-action value function . This paper has a similar objective as Weighted QMIX , but it achieves it through implicit weighting by learning over a set of related tasks ( reward function weights ) . Their approach is to extend Value-Decomposition Networks ( VDN ) by combining it with a multi-agent version of USFs ( Successor Features + reward-function-based Universal Value Functions ) , called MAUSFs . Then , they use a related-task sampler during exploration to sample simpler tasks and through this , they claim that more importance is placed on better joint actions implicitly . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I think this is , in general , a sound paper and so would like to see it accepted . As a paper to inspire and give insights on issues in coop-MARL through a creative approach , I like this paper . However , regarding the general applicability and its practical usefulness , I 'm not fully convinced yet . Additionally , I have some other concerns which , hopefully , the authors can address during the rebuttal period . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : - The approach is innovative . - The experiments are targeted and illustrative . The zero-generalization experiment is also useful . - Paper is well-written and covers the literature appropriately . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : Please see my questions below . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during the rebuttal period : 1 ) How can we ensure pi_z is optimal on task z ? Is n't the optimality of pi_z on task z based on which the SF is learned a necessary condition for the GPI update to apply ? 2 ) I 'm not fully convinced how in general this method can be expected to improve regarding the issue of relative overgeneralization ? How do we know that these related tasks would bias the monotonic approximation towards `` more important '' joint actions ? Is there a guarantee in the limit of tasks sampled by some distribution D ? 3 ) I have some concerns regarding the general applicability of this approach . To the best of my understanding , this work is limited to domains where the reward function is known and represented as a linear combination of some basis function . To me , this seems like a very challenging issue to overcome as breaking down the reward function based on experienced reward signals is perhaps as hard as learning the optimal action-value function ( loosely speaking ) . So can this approach really lead to more generic extensions that would work on unknown and general reward functions ? 4 ) Why does VDN 's gap w.r.t.UneVEn narrows down ( instead of widening ) in p=0.004 ( nonmonotonic ) vs. p=0 ( monotonic ) ? 5 ) Why does QMIX perform so much worse than VDN in p=0.004 ( in fact , on all tasks in Figures 2 & 3 this is the case ) ? Should n't QMIX 's ability to represent a larger class of decompositions improve performance over VDN ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Minor comments : - Section 1 , paragraph 3 : `` QTRAN ( Son et al. , 2019 ) and WQMIX ( Rashid et al. , 2020a ) addresses ... '' - > address - Remove the first comma in Proposition 1 : `` For , the predator-prey game defined above , ... '' - I think \\pi * _z should be used to indicate the optimal policy on task z is meant .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their valuable feedback . * * Q1 : `` How can we ensure pi_z is optimal on task z ? Is n't the optimality of pi_z on task z based on which the SF is learned a necessary condition for the GPI update to apply ? `` * * A1 : Our method samples tasks related to the target task based on a normal distribution with a predefined variance at the beginning of each episode and performs TD-error updates to each of the related tasks and the target task . This ensures that the learning happens for all sampled related tasks . Using Proposition 1 from [ 1 ] : Given a GPI policy computed for task $ \\vec { w } $ based on a set $ \\nu $ of related tasks , the maximum difference between optimal policy for task $ \\vec { w } $ and the GPI policy for any state-action pair is upper bounded by two major terms : ( a ) minimum of the norm of distance between task $ \\vec { w } $ and any task $ \\vec { z } \\in \\nu $ , ( b ) the approximation error of SFs of task $ \\vec { z } $ . In our case , the first error is quite small as we always sample related tasks in close vicinity of the target task with small variance of 0.1 or 0.2 . The second error is taken care of by learning simultaneously on all related tasks . * * Q2 : `` I 'm not fully convinced how in general this method can be expected to improve regarding the issue of relative overgeneralization ? How do we know that these related tasks would bias the monotonic approximation towards `` more important '' joint actions ? Is there a guarantee in the limit of tasks sampled by some distribution D ? `` * * A2 : Relative overgeneralization in nonmonotonic MARL tasks happens when the number of suboptimal joint actions leading to negative rewards is much greater than the number of optimal joint actions and the employed joint action value function has limited representational capacity [ 2 , 3 ] . MAVEN [ 2 ] ( Theorem 1 and 2 ) show that VDN and QMIX agents ( with both uniform and epsilon greedy policies ) can latch onto suboptimal behaviour early on during learning , due to the monotonicity constraint as it can prevent the network from correctly remembering the true value of the optimal action ( currently perceived as suboptimal ) . These suboptimal approximations in turn affect exploration as suboptimal actions keep getting selected based on maximizing the current suboptimal joint action value function . WQMIX [ 4 ] shows that non-monotonic tasks can be solved with monotonic value functions ( like VDN and QMIX ) if the optimal actions are sampled more often during training . They introduce explicit weighting mechanisms to enable bias towards such important joint actions during learning through an unconstrained critic . We rely on the intuition that executing similar , but simpler tasks solvable by monotonic value functions , changes the action distribution in our favour . For example , while solving the task for higher p = 0.016 , UneVEn could potentially sample tasks with lower penalties like p = 0.004 , 0.003 , etc. , which are easier to solve using a monotonic value function and have similar important joint actions ( see VDN in Figure 4a with p = 0.004 ) . We achieve this by sampling tasks similar to the target task at random and executing the task with the largest Q-value ( Greedy-GPI action selection scheme ) . This increases the chance of sampling the target task 's optimal actions and therefore makes it solvable with a VDN factorization . The Uniform-GPI scheme suffers from action-selection based on harder related tasks as well resulting in more variance during learning ( see Figure 4d with p = 0.016 ) . The ablations in the updated version of the paper ( Figure 5 ) which always picks actions based on the target task i.e.Target-GPI and Target-NOGPI clearly show the importance of action selection based on related tasks as they are unable to solve the tasks . We do not have any theoretical guarantees in the limit of sampled related tasks ."}, {"review_id": "0z1HScLBEpb-1", "review_text": "This paper employs the linear value factorization proposed by VDN and extends universal successor features with GPI to multi-agent reinforcement learning . It also proposes an exploration method , called universal value exploration , that biases an agent \u2019 s action selection by utilizing related tasks . Empirical evaluation demonstrates its outperformance over baselines in predator-prey tasks . However , I have some major concerns about this paper . First , the motivation of this paper is confusing , that is , the monotonic restriction leads to inefficient exploration . Exploration and the expressiveness of value factorization do not have a causal relation . Some exploration strategies can allow VDN and QMIX to learn the optimal strategy even with non-monotonic returns . One of such trivial exploration policies is the optimal ( or nearly optimal ) policy . The proposed MAUSFs approach looks like a simple extension of single-agent successor features to multi-agent settings by using linear value factorization . Are there challenges for such an extension ? I don \u2019 t think the proposed UNEVEN exploration method can address the relative overgeneralization pathology . The relative overgeneralization pathology is caused by the limited function class of some value factorization methods . QTRAN and QPLEX can potentially address this issue , but they may not perform well in some tasks that require more efficient exploration . I don \u2019 t think it is fair to compare UNEVEN with most of the baselines in this paper . It is because UNEVEN uses the linear parameterized representation of reward functions that allows generating related tasks . In contrast , some baselines do not aim to address the exploration problem or do not take additional information . The experimental evaluation can be improved . For example , it can include more complex settings , like SMAC .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for their valuable feedback . * * Q1 : `` First , the motivation of this paper is confusing , that is , the monotonic restriction leads to inefficient exploration . Exploration and the expressiveness of value factorization do not have a causal relation . `` * * A1 : MAVEN [ 1 ] ( Theorem 1 and 2 ) show that VDN and QMIX agents ( with both uniform and epsilon greedy policies ) can latch onto suboptimal behaviour early on during learning , due to the monotonicity constraint as it can prevent the network from correctly remembering the true value of the optimal action ( currently perceived as suboptimal ) . These suboptimal approximations in turn affect exploration as suboptimal actions keep getting selected based on maximizing the current suboptimal joint action value function . WQMIX [ 2 ] shows that non-monotonic tasks can be solved with monotonic value functions ( like VDN and QMIX ) if the optimal actions are sampled more often during training . There is , therefore , a causal relationship between how actions are explored ( e.g.with $ \\epsilon $ -greedy ) and which tasks can be learned by monotonic factorizations like VDN and QMIX . As explained in our related works section , most work on exploration in RL focuses on visiting as much of the state-action space as possible which in turn exacerbates the relative overgeneralization pathology . Therefore , in our case , we sample simpler reward related tasks with similar optimal joint actions as the target task , thereby implicitly biasing the learning towards important joint-actions . * * Q2 : `` Some exploration strategies can allow VDN and QMIX to learn the optimal strategy even with non-monotonic returns . One of such trivial exploration policies is the optimal ( or nearly optimal ) policy . `` * * A2 : The reviewer correctly points out that putting all `` weight '' on the optimal joint actions during learning ( i.e.only sample optimal joint actions ) should allow us to learn any non-monotonic task ( if we ignore function approximation errors ) . The obvious issue is that we don \u2019 t know that exploration policy . Therefore , we propose an exploration approach which samples and learns tasks similar to the target task $ \\vec { w } $ in the hope that their optimal actions are often useful for the target task , too . Our empirical results justify this hope and show that our exploration approach can alleviate the problem of joint action value function trying to represent all joint actions , leading to suboptimal approximations and poor exploration . We don \u2019 t see how an optimal ( or nearly ) optimal policy is a \u201c trivial \u201d exploration policy . * * Q3 : `` The proposed MAUSFs approach looks like a simple extension of single-agent successor features to multi-agent settings by using linear value factorization . Are there challenges for such an extension ? `` * * A3 : The VDN decomposition of centralized USFs is straightforward , however , the insight that our local GPI implicitly performs global GPI over all combinations of sampled agent-policies is novel . The major contribution of the paper is the UneVEn exploration scheme that allows to overcome the monotonic restrictions of a VDN factorization in a way that would be impossible without local SFs . Our approach is novel as follows : 1 . Our approach enables decentralized execution by introducing novel agent-specific SFs while taking advantage of centralized training ( CTDE ) . 2.Our approach enables decentralized GPI which is particularly well suited for MARL , as it allows us to maximize over a combinatorial set of agent policies . 3.We introduce novel action-selection schemes by leveraging VDN-factorized MAUSFs to overcome the representation limitation of VDN/QMIX . 4.Our results along with ablations show that our approach significantly outperforms other methods on non monotonic tasks and shows comparable performance on other large scale tasks ."}, {"review_id": "0z1HScLBEpb-2", "review_text": "The paper develops and evaluates an algorithm for decision making in the CTDE MARL setting ( centralized training and decentralized execution for multiagent reinforcement learning ) . That is , the concern is how to use closely supervised training to produce agents that can work independently toward a common goal . The problem is formalized in the DEC-POMDP ( decentralized partially observable Markov decision process ) setting . The proposed solution uses elements of several recent and promising approaches including value decomposition networks , QMIX ( a nonlinear monotonic combination of individual utility functions ) , linear reward functions , universal value functions , successor features , generalized policy improvement . The novel idea is combining these ideas to create multi-agent universal successor features . The power of the resulting algorithm was demonstrated in a set of predator-prey games with increased difficulty of learning to coordinate due to an increasing penalty value for accidental miscoordination . It was shown that other methods for this problem were not able to handle learning in the setting of high costs for coordination , but the proposed algorithm could . The paper is quite good for what it claims to do . But , my impression is that the contribution is not very large . Specifically , the main demonstration was ( significantly ) improved performance on a predator-prey task . However , I am skeptical that these results will generalize to other domainsthese kinds of problems are quite hard and it 's not clear if ANY algorithm would be expected to solve a diverse collection of real-world challenges . In the context of what the paper was trying to do , additional domains ( 3 distinct ones , say ? ) would go a long way toward characterizing the space of domains where this approach is appropriate . I guess I 'm saying that this algorithm might be THE algorithm you 'd want to use ( it creatively combines a number of promising component ideas ) , but the paper did n't provide an argument for why the reader should be convinced that the positive results generalize . Some detailed comments : `` We now propose two novel action-selection schemes based on related tasks with probability 1-eps , and thereby implicitly weighting joint actions during learning . '' I 'm afraid I was n't able to parse this sentence . Reword ? pg.6 : Maybe putting the graphs on the same y-axis would aid in comparisons between settings ( different values of p ) . `` However , both QMIX and VDN fail to learn on other three higher penalty target tasks '' - > `` However , both QMIX and VDN fail to learn on three other higher penalty target tasks '' ? `` between different action selection '' - > `` between action-selection schemes '' . `` learns a QMIX-factored joint-action value function along with an unrestricted centralized critic and propose '' - > `` learns a QMIX-factored joint-action value function along with an unrestricted centralized critic and proposes '' ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their valuable feedback . We have added additional results on two more domains in the paper : ( a ) Section 5 Domain 1 : a non-monotonic m-step matrix game from [ 1 ] to test how non-monotonicity and exploration interact , and ( b ) Section 5 Domain 3 : Starcraft Micromanagement Challenge ( SMAC ) from [ 2 ] . References : [ 1 ] Mahajan , A. , Rashid , T. , Samvelyan , M. and Whiteson , S. , 2019 . Maven : Multi-agent variational exploration . In Advances in Neural Information Processing Systems ( pp.7613-7624 ) . [ 2 ] Samvelyan , M. , Rashid , T. , de Witt , C.S. , Farquhar , G. , Nardelli , N. , Rudner , T.G. , Hung , C.M. , Torr , P.H. , Foerster , J. and Whiteson , S. , 2019 . The starcraft multi-agent challenge . arXiv preprint arXiv:1902.04043 ."}, {"review_id": "0z1HScLBEpb-3", "review_text": "Some popular methods like VDN and QMIX focus on the monotonic factorization of joint-action value function , which is not realistic in non-monotonic cases when the agent \u2019 s best action depends on other agents \u2019 actions . This phenomenon is common . For example , in the prisoner \u2019 s dilemma the value function can be monotonically decreasing in each of the agent \u2019 s local value function . One of the effect this paper focuses on is that the monotonically factorization lacks the representational capacity to distinguish the values of coordinated and uncoordinated joint actions during exploration . This effect is well explained in the predator-prey game example , where both VDN and QMIX have undesired performance . Recent work like QTRAN and WQMIX tried to address the problem of inefficient exploration caused by monotonic factorization . However , these approaches still rely on inefficient-greedy exploration which may fail on harder tasks e.g. , again , the predator-prey task above with higher value of p. This paper applies universal successor features to the multi-agent setting ( Multi-Agent Universal Successor Features MAUSFs ) . Decentralized agent-specific SFs with VDN enables agents to compute decentralized greedy policies and to perform decentralized local GP . The two components have some good synergy and improves VDN to some extent . The propose Universal Value Exploration ( UneVEn ) can solve tasks with nonmonotonic values . Both the combination of SF and VDN and the UneVEn algorithm are very intuitive and are easy to understand and implement . However , the strength of the results are moderate . As it is very natural to combine the two it suffices to figure out how strong the synergy is between . This is not observed either through theoretical insights or experimental studies . To be specific , the experiments study only the game of predator-prey and do not yet demonstrate the algorithm \u2019 s generalization power beyond this motivating task . It should outperform existing approaches on a wider range of tasks where the monotonicity does not hold , and be at least competitive on tasks where such an assumption hold , to be valuable . The experiment on Zero-shot generalization is interesting , but SF along should be able to present such an effect of transfer learning ( generalization ) . The paper needs to show how the synergy between SF and VDN is really like in the task of transfer . Pro 1.This paper keeps up with the frontier of MARL . It discusses a lot of algorithms proposed recently and provides a detailed background knowledge . 2.It clearly stated the current research gap in value function factorization . Weak points 1 . This paper might not fill the research gap it mentioned very well . For example , in the introduction part , it says the shortages of VDN and QMIX is that they are restricted by monotonic property . However , it states in the UneVEn section that , `` the basic idea is some of the related tasks can be efficiently learned using a monotonic joint-action value function. `` Does it mean that the implementation of UneVEn still relies on monotonic value function ? 2.The author can elaborate more on the experiments from an intuitive view , rather than just stating the experiment data . For example , as the experiment shows , the UneVEn only outperforms as the penalty parameter p get larger and larger . What 's the reason behind it ? How does UneVEn overcome the `` curse '' of suboptimal stuck in VDN and QMIX ? 3.More experiments on general tasks should be provided .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their valuable feedback . * * Q1 : `` This paper might not fill the research gap it mentioned very well . For example , in the introduction part , it says the shortages of VDN and QMIX is that they are restricted by monotonic property . However , it states in the UneVEn section that , \u201c the basic idea is some of the related tasks can be efficiently learned using a monotonic joint-action value function. \u201d Does it mean that the implementation of UneVEn still relies on monotonic value function ? `` * * A1 : WQMIX [ 1 ] shows that non-monotonic tasks can be solved with monotonic value functions ( like VDN and QMIX ) if the optimal actions are sampled more often during training . They introduce explicit weighting mechanisms to enable bias towards such important joint actions during learning through an unconstrained critic . We rely on the intuition that executing similar , but simpler tasks solvable by monotonic value functions , changes the action distribution in our favour . For example , while solving the task for higher p = 0.016 , UneVEn could potentially sample tasks with lower penalties like p = 0.004 , 0.003 , etc. , which are easier to solve using a monotonic value function and have similar important joint-actions ( see VDN in Figure 4a , p = 0.004 ) . We achieve this by sampling tasks similar to the target task at random and executing the task with the largest Q-value ( Greedy-GPI action selection scheme ) . This increases the chance of sampling the target task 's optimal actions and therefore makes it solvable with a VDN factorization . * * Q2 : `` The author can elaborate more on the experiments from an intuitive view , rather than just stating the experiment data . For example , as the experiment shows , the UneVEn only outperforms as the penalty parameter p gets larger and larger . What 's the reason behind it ? How does UneVEn overcome the `` curse '' of suboptimal stuck in VDN and QMIX ? `` * * A2 : As shown in the reward matrix ( Table 1 in Appendix B ) for predator prey , there are numerous joint actions which lead to a penalty and only a single joint action which leads to a successful capture ( reward +1 ) . For VDN and QMIX with simple $ \\epsilon $ -greedy policies , as we increase the penalty , the TD squared loss computed from states in which the prey is surrounded gets dominated by the actions which lead to a penalty ( that are more numerous than the optimal actions ) . This makes it difficult to learn an accurate monotonic approximation that can capture the optimal joint action [ 2 ] . UneVEn tackles this issue by efficient exploration based on related tasks with simpler reward functions but with similar important joint actions . For example , while solving the task for higher p = 0.016 , UneVEn could potentially sample tasks with lower penalties like p = 0.004 , 0.003 , etc. , which can be solved by monotonic value functions and have similar important joint-actions . UneVEn with Greedy-GPI always picks the task with the highest Q-value which should refer to simpler reward related tasks which are solved earlier and have higher Q-values than other higher penalty related tasks . UneVEn with Uniform-GPI might suffer from action-selection based on higher penalty related tasks leading to higher variance during learning ( see Figure 4d , p = 0.016 ) . * * Q3 : `` More experiments on general tasks should be provided . `` * * A3 : We have added additional results on two more domains in the paper : ( a ) Section 5 Domain 1 : a non-monotonic m-step matrix game from [ 2 ] to test how non-monotonicity and exploration interact , and ( b ) Section 5 Domain 3 : Starcraft Micromanagement Challenge ( SMAC ) from [ 3 ] . References : [ 1 ] Rashid , T. , Farquhar , G. , Peng , B. and Whiteson , S. , 2020 . Weighted QMIX : Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning . Advances in Neural Information Processing Systems , 33 . [ 2 ] Mahajan , A. , Rashid , T. , Samvelyan , M. and Whiteson , S. , 2019 . Maven : Multi-agent variational exploration . In Advances in Neural Information Processing Systems ( pp.7613-7624 ) . [ 3 ] Samvelyan , M. , Rashid , T. , de Witt , C.S. , Farquhar , G. , Nardelli , N. , Rudner , T.G. , Hung , C.M. , Torr , P.H. , Foerster , J. and Whiteson , S. , 2019 . The starcraft multi-agent challenge . arXiv preprint arXiv:1902.04043 ."}], "0": {"review_id": "0z1HScLBEpb-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper studies the problem setting of cooperative multi-agent RL ( coop-MARL ) under centralized training with decentralized execution ( CTDE ) . Additionally , it assumes that the reward function is known and is represented as a weighted linear combination of a basis function . They consider Q-learning with value-decomposition as the basis for tackling such problems and improve upon SOTA methods for coop-MARL in the domain of predator-prey ( PP ) . Specifically , they claim to improve upon monotonic value-decomposition methods such as VDN and QMIX by better addressing the issue of relative overgeneralization which stems from their monotonicity constraint in representing the joint-action value function . This paper has a similar objective as Weighted QMIX , but it achieves it through implicit weighting by learning over a set of related tasks ( reward function weights ) . Their approach is to extend Value-Decomposition Networks ( VDN ) by combining it with a multi-agent version of USFs ( Successor Features + reward-function-based Universal Value Functions ) , called MAUSFs . Then , they use a related-task sampler during exploration to sample simpler tasks and through this , they claim that more importance is placed on better joint actions implicitly . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I think this is , in general , a sound paper and so would like to see it accepted . As a paper to inspire and give insights on issues in coop-MARL through a creative approach , I like this paper . However , regarding the general applicability and its practical usefulness , I 'm not fully convinced yet . Additionally , I have some other concerns which , hopefully , the authors can address during the rebuttal period . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : - The approach is innovative . - The experiments are targeted and illustrative . The zero-generalization experiment is also useful . - Paper is well-written and covers the literature appropriately . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : Please see my questions below . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during the rebuttal period : 1 ) How can we ensure pi_z is optimal on task z ? Is n't the optimality of pi_z on task z based on which the SF is learned a necessary condition for the GPI update to apply ? 2 ) I 'm not fully convinced how in general this method can be expected to improve regarding the issue of relative overgeneralization ? How do we know that these related tasks would bias the monotonic approximation towards `` more important '' joint actions ? Is there a guarantee in the limit of tasks sampled by some distribution D ? 3 ) I have some concerns regarding the general applicability of this approach . To the best of my understanding , this work is limited to domains where the reward function is known and represented as a linear combination of some basis function . To me , this seems like a very challenging issue to overcome as breaking down the reward function based on experienced reward signals is perhaps as hard as learning the optimal action-value function ( loosely speaking ) . So can this approach really lead to more generic extensions that would work on unknown and general reward functions ? 4 ) Why does VDN 's gap w.r.t.UneVEn narrows down ( instead of widening ) in p=0.004 ( nonmonotonic ) vs. p=0 ( monotonic ) ? 5 ) Why does QMIX perform so much worse than VDN in p=0.004 ( in fact , on all tasks in Figures 2 & 3 this is the case ) ? Should n't QMIX 's ability to represent a larger class of decompositions improve performance over VDN ? # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Minor comments : - Section 1 , paragraph 3 : `` QTRAN ( Son et al. , 2019 ) and WQMIX ( Rashid et al. , 2020a ) addresses ... '' - > address - Remove the first comma in Proposition 1 : `` For , the predator-prey game defined above , ... '' - I think \\pi * _z should be used to indicate the optimal policy on task z is meant .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their valuable feedback . * * Q1 : `` How can we ensure pi_z is optimal on task z ? Is n't the optimality of pi_z on task z based on which the SF is learned a necessary condition for the GPI update to apply ? `` * * A1 : Our method samples tasks related to the target task based on a normal distribution with a predefined variance at the beginning of each episode and performs TD-error updates to each of the related tasks and the target task . This ensures that the learning happens for all sampled related tasks . Using Proposition 1 from [ 1 ] : Given a GPI policy computed for task $ \\vec { w } $ based on a set $ \\nu $ of related tasks , the maximum difference between optimal policy for task $ \\vec { w } $ and the GPI policy for any state-action pair is upper bounded by two major terms : ( a ) minimum of the norm of distance between task $ \\vec { w } $ and any task $ \\vec { z } \\in \\nu $ , ( b ) the approximation error of SFs of task $ \\vec { z } $ . In our case , the first error is quite small as we always sample related tasks in close vicinity of the target task with small variance of 0.1 or 0.2 . The second error is taken care of by learning simultaneously on all related tasks . * * Q2 : `` I 'm not fully convinced how in general this method can be expected to improve regarding the issue of relative overgeneralization ? How do we know that these related tasks would bias the monotonic approximation towards `` more important '' joint actions ? Is there a guarantee in the limit of tasks sampled by some distribution D ? `` * * A2 : Relative overgeneralization in nonmonotonic MARL tasks happens when the number of suboptimal joint actions leading to negative rewards is much greater than the number of optimal joint actions and the employed joint action value function has limited representational capacity [ 2 , 3 ] . MAVEN [ 2 ] ( Theorem 1 and 2 ) show that VDN and QMIX agents ( with both uniform and epsilon greedy policies ) can latch onto suboptimal behaviour early on during learning , due to the monotonicity constraint as it can prevent the network from correctly remembering the true value of the optimal action ( currently perceived as suboptimal ) . These suboptimal approximations in turn affect exploration as suboptimal actions keep getting selected based on maximizing the current suboptimal joint action value function . WQMIX [ 4 ] shows that non-monotonic tasks can be solved with monotonic value functions ( like VDN and QMIX ) if the optimal actions are sampled more often during training . They introduce explicit weighting mechanisms to enable bias towards such important joint actions during learning through an unconstrained critic . We rely on the intuition that executing similar , but simpler tasks solvable by monotonic value functions , changes the action distribution in our favour . For example , while solving the task for higher p = 0.016 , UneVEn could potentially sample tasks with lower penalties like p = 0.004 , 0.003 , etc. , which are easier to solve using a monotonic value function and have similar important joint actions ( see VDN in Figure 4a with p = 0.004 ) . We achieve this by sampling tasks similar to the target task at random and executing the task with the largest Q-value ( Greedy-GPI action selection scheme ) . This increases the chance of sampling the target task 's optimal actions and therefore makes it solvable with a VDN factorization . The Uniform-GPI scheme suffers from action-selection based on harder related tasks as well resulting in more variance during learning ( see Figure 4d with p = 0.016 ) . The ablations in the updated version of the paper ( Figure 5 ) which always picks actions based on the target task i.e.Target-GPI and Target-NOGPI clearly show the importance of action selection based on related tasks as they are unable to solve the tasks . We do not have any theoretical guarantees in the limit of sampled related tasks ."}, "1": {"review_id": "0z1HScLBEpb-1", "review_text": "This paper employs the linear value factorization proposed by VDN and extends universal successor features with GPI to multi-agent reinforcement learning . It also proposes an exploration method , called universal value exploration , that biases an agent \u2019 s action selection by utilizing related tasks . Empirical evaluation demonstrates its outperformance over baselines in predator-prey tasks . However , I have some major concerns about this paper . First , the motivation of this paper is confusing , that is , the monotonic restriction leads to inefficient exploration . Exploration and the expressiveness of value factorization do not have a causal relation . Some exploration strategies can allow VDN and QMIX to learn the optimal strategy even with non-monotonic returns . One of such trivial exploration policies is the optimal ( or nearly optimal ) policy . The proposed MAUSFs approach looks like a simple extension of single-agent successor features to multi-agent settings by using linear value factorization . Are there challenges for such an extension ? I don \u2019 t think the proposed UNEVEN exploration method can address the relative overgeneralization pathology . The relative overgeneralization pathology is caused by the limited function class of some value factorization methods . QTRAN and QPLEX can potentially address this issue , but they may not perform well in some tasks that require more efficient exploration . I don \u2019 t think it is fair to compare UNEVEN with most of the baselines in this paper . It is because UNEVEN uses the linear parameterized representation of reward functions that allows generating related tasks . In contrast , some baselines do not aim to address the exploration problem or do not take additional information . The experimental evaluation can be improved . For example , it can include more complex settings , like SMAC .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for their valuable feedback . * * Q1 : `` First , the motivation of this paper is confusing , that is , the monotonic restriction leads to inefficient exploration . Exploration and the expressiveness of value factorization do not have a causal relation . `` * * A1 : MAVEN [ 1 ] ( Theorem 1 and 2 ) show that VDN and QMIX agents ( with both uniform and epsilon greedy policies ) can latch onto suboptimal behaviour early on during learning , due to the monotonicity constraint as it can prevent the network from correctly remembering the true value of the optimal action ( currently perceived as suboptimal ) . These suboptimal approximations in turn affect exploration as suboptimal actions keep getting selected based on maximizing the current suboptimal joint action value function . WQMIX [ 2 ] shows that non-monotonic tasks can be solved with monotonic value functions ( like VDN and QMIX ) if the optimal actions are sampled more often during training . There is , therefore , a causal relationship between how actions are explored ( e.g.with $ \\epsilon $ -greedy ) and which tasks can be learned by monotonic factorizations like VDN and QMIX . As explained in our related works section , most work on exploration in RL focuses on visiting as much of the state-action space as possible which in turn exacerbates the relative overgeneralization pathology . Therefore , in our case , we sample simpler reward related tasks with similar optimal joint actions as the target task , thereby implicitly biasing the learning towards important joint-actions . * * Q2 : `` Some exploration strategies can allow VDN and QMIX to learn the optimal strategy even with non-monotonic returns . One of such trivial exploration policies is the optimal ( or nearly optimal ) policy . `` * * A2 : The reviewer correctly points out that putting all `` weight '' on the optimal joint actions during learning ( i.e.only sample optimal joint actions ) should allow us to learn any non-monotonic task ( if we ignore function approximation errors ) . The obvious issue is that we don \u2019 t know that exploration policy . Therefore , we propose an exploration approach which samples and learns tasks similar to the target task $ \\vec { w } $ in the hope that their optimal actions are often useful for the target task , too . Our empirical results justify this hope and show that our exploration approach can alleviate the problem of joint action value function trying to represent all joint actions , leading to suboptimal approximations and poor exploration . We don \u2019 t see how an optimal ( or nearly ) optimal policy is a \u201c trivial \u201d exploration policy . * * Q3 : `` The proposed MAUSFs approach looks like a simple extension of single-agent successor features to multi-agent settings by using linear value factorization . Are there challenges for such an extension ? `` * * A3 : The VDN decomposition of centralized USFs is straightforward , however , the insight that our local GPI implicitly performs global GPI over all combinations of sampled agent-policies is novel . The major contribution of the paper is the UneVEn exploration scheme that allows to overcome the monotonic restrictions of a VDN factorization in a way that would be impossible without local SFs . Our approach is novel as follows : 1 . Our approach enables decentralized execution by introducing novel agent-specific SFs while taking advantage of centralized training ( CTDE ) . 2.Our approach enables decentralized GPI which is particularly well suited for MARL , as it allows us to maximize over a combinatorial set of agent policies . 3.We introduce novel action-selection schemes by leveraging VDN-factorized MAUSFs to overcome the representation limitation of VDN/QMIX . 4.Our results along with ablations show that our approach significantly outperforms other methods on non monotonic tasks and shows comparable performance on other large scale tasks ."}, "2": {"review_id": "0z1HScLBEpb-2", "review_text": "The paper develops and evaluates an algorithm for decision making in the CTDE MARL setting ( centralized training and decentralized execution for multiagent reinforcement learning ) . That is , the concern is how to use closely supervised training to produce agents that can work independently toward a common goal . The problem is formalized in the DEC-POMDP ( decentralized partially observable Markov decision process ) setting . The proposed solution uses elements of several recent and promising approaches including value decomposition networks , QMIX ( a nonlinear monotonic combination of individual utility functions ) , linear reward functions , universal value functions , successor features , generalized policy improvement . The novel idea is combining these ideas to create multi-agent universal successor features . The power of the resulting algorithm was demonstrated in a set of predator-prey games with increased difficulty of learning to coordinate due to an increasing penalty value for accidental miscoordination . It was shown that other methods for this problem were not able to handle learning in the setting of high costs for coordination , but the proposed algorithm could . The paper is quite good for what it claims to do . But , my impression is that the contribution is not very large . Specifically , the main demonstration was ( significantly ) improved performance on a predator-prey task . However , I am skeptical that these results will generalize to other domainsthese kinds of problems are quite hard and it 's not clear if ANY algorithm would be expected to solve a diverse collection of real-world challenges . In the context of what the paper was trying to do , additional domains ( 3 distinct ones , say ? ) would go a long way toward characterizing the space of domains where this approach is appropriate . I guess I 'm saying that this algorithm might be THE algorithm you 'd want to use ( it creatively combines a number of promising component ideas ) , but the paper did n't provide an argument for why the reader should be convinced that the positive results generalize . Some detailed comments : `` We now propose two novel action-selection schemes based on related tasks with probability 1-eps , and thereby implicitly weighting joint actions during learning . '' I 'm afraid I was n't able to parse this sentence . Reword ? pg.6 : Maybe putting the graphs on the same y-axis would aid in comparisons between settings ( different values of p ) . `` However , both QMIX and VDN fail to learn on other three higher penalty target tasks '' - > `` However , both QMIX and VDN fail to learn on three other higher penalty target tasks '' ? `` between different action selection '' - > `` between action-selection schemes '' . `` learns a QMIX-factored joint-action value function along with an unrestricted centralized critic and propose '' - > `` learns a QMIX-factored joint-action value function along with an unrestricted centralized critic and proposes '' ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their valuable feedback . We have added additional results on two more domains in the paper : ( a ) Section 5 Domain 1 : a non-monotonic m-step matrix game from [ 1 ] to test how non-monotonicity and exploration interact , and ( b ) Section 5 Domain 3 : Starcraft Micromanagement Challenge ( SMAC ) from [ 2 ] . References : [ 1 ] Mahajan , A. , Rashid , T. , Samvelyan , M. and Whiteson , S. , 2019 . Maven : Multi-agent variational exploration . In Advances in Neural Information Processing Systems ( pp.7613-7624 ) . [ 2 ] Samvelyan , M. , Rashid , T. , de Witt , C.S. , Farquhar , G. , Nardelli , N. , Rudner , T.G. , Hung , C.M. , Torr , P.H. , Foerster , J. and Whiteson , S. , 2019 . The starcraft multi-agent challenge . arXiv preprint arXiv:1902.04043 ."}, "3": {"review_id": "0z1HScLBEpb-3", "review_text": "Some popular methods like VDN and QMIX focus on the monotonic factorization of joint-action value function , which is not realistic in non-monotonic cases when the agent \u2019 s best action depends on other agents \u2019 actions . This phenomenon is common . For example , in the prisoner \u2019 s dilemma the value function can be monotonically decreasing in each of the agent \u2019 s local value function . One of the effect this paper focuses on is that the monotonically factorization lacks the representational capacity to distinguish the values of coordinated and uncoordinated joint actions during exploration . This effect is well explained in the predator-prey game example , where both VDN and QMIX have undesired performance . Recent work like QTRAN and WQMIX tried to address the problem of inefficient exploration caused by monotonic factorization . However , these approaches still rely on inefficient-greedy exploration which may fail on harder tasks e.g. , again , the predator-prey task above with higher value of p. This paper applies universal successor features to the multi-agent setting ( Multi-Agent Universal Successor Features MAUSFs ) . Decentralized agent-specific SFs with VDN enables agents to compute decentralized greedy policies and to perform decentralized local GP . The two components have some good synergy and improves VDN to some extent . The propose Universal Value Exploration ( UneVEn ) can solve tasks with nonmonotonic values . Both the combination of SF and VDN and the UneVEn algorithm are very intuitive and are easy to understand and implement . However , the strength of the results are moderate . As it is very natural to combine the two it suffices to figure out how strong the synergy is between . This is not observed either through theoretical insights or experimental studies . To be specific , the experiments study only the game of predator-prey and do not yet demonstrate the algorithm \u2019 s generalization power beyond this motivating task . It should outperform existing approaches on a wider range of tasks where the monotonicity does not hold , and be at least competitive on tasks where such an assumption hold , to be valuable . The experiment on Zero-shot generalization is interesting , but SF along should be able to present such an effect of transfer learning ( generalization ) . The paper needs to show how the synergy between SF and VDN is really like in the task of transfer . Pro 1.This paper keeps up with the frontier of MARL . It discusses a lot of algorithms proposed recently and provides a detailed background knowledge . 2.It clearly stated the current research gap in value function factorization . Weak points 1 . This paper might not fill the research gap it mentioned very well . For example , in the introduction part , it says the shortages of VDN and QMIX is that they are restricted by monotonic property . However , it states in the UneVEn section that , `` the basic idea is some of the related tasks can be efficiently learned using a monotonic joint-action value function. `` Does it mean that the implementation of UneVEn still relies on monotonic value function ? 2.The author can elaborate more on the experiments from an intuitive view , rather than just stating the experiment data . For example , as the experiment shows , the UneVEn only outperforms as the penalty parameter p get larger and larger . What 's the reason behind it ? How does UneVEn overcome the `` curse '' of suboptimal stuck in VDN and QMIX ? 3.More experiments on general tasks should be provided .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their valuable feedback . * * Q1 : `` This paper might not fill the research gap it mentioned very well . For example , in the introduction part , it says the shortages of VDN and QMIX is that they are restricted by monotonic property . However , it states in the UneVEn section that , \u201c the basic idea is some of the related tasks can be efficiently learned using a monotonic joint-action value function. \u201d Does it mean that the implementation of UneVEn still relies on monotonic value function ? `` * * A1 : WQMIX [ 1 ] shows that non-monotonic tasks can be solved with monotonic value functions ( like VDN and QMIX ) if the optimal actions are sampled more often during training . They introduce explicit weighting mechanisms to enable bias towards such important joint actions during learning through an unconstrained critic . We rely on the intuition that executing similar , but simpler tasks solvable by monotonic value functions , changes the action distribution in our favour . For example , while solving the task for higher p = 0.016 , UneVEn could potentially sample tasks with lower penalties like p = 0.004 , 0.003 , etc. , which are easier to solve using a monotonic value function and have similar important joint-actions ( see VDN in Figure 4a , p = 0.004 ) . We achieve this by sampling tasks similar to the target task at random and executing the task with the largest Q-value ( Greedy-GPI action selection scheme ) . This increases the chance of sampling the target task 's optimal actions and therefore makes it solvable with a VDN factorization . * * Q2 : `` The author can elaborate more on the experiments from an intuitive view , rather than just stating the experiment data . For example , as the experiment shows , the UneVEn only outperforms as the penalty parameter p gets larger and larger . What 's the reason behind it ? How does UneVEn overcome the `` curse '' of suboptimal stuck in VDN and QMIX ? `` * * A2 : As shown in the reward matrix ( Table 1 in Appendix B ) for predator prey , there are numerous joint actions which lead to a penalty and only a single joint action which leads to a successful capture ( reward +1 ) . For VDN and QMIX with simple $ \\epsilon $ -greedy policies , as we increase the penalty , the TD squared loss computed from states in which the prey is surrounded gets dominated by the actions which lead to a penalty ( that are more numerous than the optimal actions ) . This makes it difficult to learn an accurate monotonic approximation that can capture the optimal joint action [ 2 ] . UneVEn tackles this issue by efficient exploration based on related tasks with simpler reward functions but with similar important joint actions . For example , while solving the task for higher p = 0.016 , UneVEn could potentially sample tasks with lower penalties like p = 0.004 , 0.003 , etc. , which can be solved by monotonic value functions and have similar important joint-actions . UneVEn with Greedy-GPI always picks the task with the highest Q-value which should refer to simpler reward related tasks which are solved earlier and have higher Q-values than other higher penalty related tasks . UneVEn with Uniform-GPI might suffer from action-selection based on higher penalty related tasks leading to higher variance during learning ( see Figure 4d , p = 0.016 ) . * * Q3 : `` More experiments on general tasks should be provided . `` * * A3 : We have added additional results on two more domains in the paper : ( a ) Section 5 Domain 1 : a non-monotonic m-step matrix game from [ 2 ] to test how non-monotonicity and exploration interact , and ( b ) Section 5 Domain 3 : Starcraft Micromanagement Challenge ( SMAC ) from [ 3 ] . References : [ 1 ] Rashid , T. , Farquhar , G. , Peng , B. and Whiteson , S. , 2020 . Weighted QMIX : Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning . Advances in Neural Information Processing Systems , 33 . [ 2 ] Mahajan , A. , Rashid , T. , Samvelyan , M. and Whiteson , S. , 2019 . Maven : Multi-agent variational exploration . In Advances in Neural Information Processing Systems ( pp.7613-7624 ) . [ 3 ] Samvelyan , M. , Rashid , T. , de Witt , C.S. , Farquhar , G. , Nardelli , N. , Rudner , T.G. , Hung , C.M. , Torr , P.H. , Foerster , J. and Whiteson , S. , 2019 . The starcraft multi-agent challenge . arXiv preprint arXiv:1902.04043 ."}}